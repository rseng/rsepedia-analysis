div aligncenterimg srchttpsrawgithubusercontentcomcupycupymasterdocsimagecupy_logo_pxpng widthdiv cupi numpi scipi gpu pypihttpsimgshieldsiopypivcupysvghttpspypipythonorgpypicupi conda versionhttpsimgshieldsiocondavncondaforgecupysvghttpsanacondaorgcondaforgecupi github licensehttpsimgshieldsiogithublicensecupycupysvghttpsgithubcomcupycupi coverallshttpsimgshieldsiocoverallscupycupysvghttpscoverallsiogithubcupycupi gitterhttpsbadgesgitterimcupycommunitysvghttpsgitterimcupycommun twitterhttpsimgshieldsiotwitterfollowcupy_teamlabelcupy_teamhttpstwittercomcupy_team websitehttpscupydev installhttpsdocscupydevenstableinstallhtml tutorialhttpsdocscupydevenstableuser_guidebasichtml exampleshttpsgithubcomcupycupytreemasterexampl documentationhttpsdocscupydevenst api referencehttpsdocscupydevenstablerefer forumhttpsgroupsgooglecomforumforumcupi cupi numpyscipycompat array librari gpuacceler comput python cupi act dropin replacementhttpsdocscupydevenstablereferencecomparisonhtml run exist numpyscipi code nvidia cuda amd rocm platform py import cupi cp x cparangereshap astypef x array dtypefloat xsumaxi array dtypefloat cupi also provid access lowlevel cuda featur pass ndarray exist cuda cc program via rawkernelshttpsdocscupydevenstableuser_guidekernelhtmlrawkernel use streamshttpsdocscupydevenstablereferencecudahtml perform even call cuda runtim apishttpsdocscupydevenstablereferencecudahtmlruntimeapi directli instal wheel precompil binari packag avail linux x_ window amd choos right packag platform platform command cuda pip instal cupycuda cuda pip instal cupycuda cuda pip instal cupycuda cuda pip instal cupycuda cuda pip instal cupycuda cuda pip instal cupycuda cuda pip instal cupycuda rocm pip instal cupyrocm rocm pip instal cupyrocm rocm pip instal cupyrocm rocm support experiment featur refer docshttpsdocscupydevenlatestinstallhtmlusingcupyonamdgpuexperiment detail use f httpspipcupydevpr option instal prereleas eg pip instal cupycuda f httpspipcupydevpr see instal guidehttpsdocscupydevenstableinstallhtml use condaanaconda build sourc run docker use nvidia contain toolkithttpsgithubcomnvidianvidiadock run cupi imag gpu docker run gpu cupycupi inform releas noteshttpsgithubcomcupycupyreleas project use cupyhttpsgithubcomcupycupywikiprojectsusingcupi contribut guidehttpsdocscupydevenstablecontributionhtml licens mit licens see licens file cupi design base numpi api scipi api see docslicense_third_parti file cupi maintain develop prefer network inchttpspreferredjpen commun contributorshttpsgithubcomcupycupygraphscontributor refer ryosuk okuta yuya unno daisuk nishino shohei hido crissman loomi cupi numpycompat librari nvidia gpu calcul proceed workshop machin learn system learningsi thirtyfirst annual confer neural inform process system nip pdfhttplearningsysorgnipsassetspaperspaper_pdf bibtex inproceedingscupy_learningsi author okuta ryosuk unno yuya nishino daisuk hido shohei loomi crissman titl cupi numpycompat librari nvidia gpu calcul booktitl proceed workshop machin learn system learningsi thirtyfirst annual confer neural inform process system nip year url httplearningsysorgnipsassetspaperspaper_pdf cupi code conduct cupi follow numfocu code conducthomepag avail httpsnumfocusorgcodeofconduct instanc abus harass otherwis unaccept behavior may report contact project team dlfwpreferredjp homepag httpsnumfocusorg includ directori file directori directori copi distribut sdist wheel note item start eg cubgit exclud see setuppi detail cub cub folder git submodul cub project includ cub header submodul enabl build cupycudacub modul also easier mainten inform cub see cub project websitehttpnvlabsgithubcomcub jitifi jitifi folder git submodul jitifi project includ jitifi header submodul build cupycudajitifi modul inform jitifi see jitifi repohttpsgithubcomnvidiajitifi dlpack dlpack folder store dlpack header build cupy_coredlpack modul see readmemd therein inform dlpack see dlpack repohttpsgithubcomdmlcdlpack dlpack header header dlpackh download httpsgithubcomdmlcdlpackblobmainincludedlpackdlpackh commit httpsgithubcomdmlcdlpackcommiteabbbbeafebcc file copi thrust project modifi httpthrustgithubiothes file copi cuda toolkit distribut redistribut follow licens httpsdocsnvidiacomcudaarchiveeulanvidiacudatoolkitlicenseagr cuda enabl cuda enhanc compat host fp header latest cuda toolkit releas place cuda folder currenli header cuda ctk pleas refer contribut guidehttpsdocscupydevenstablecontributionhtml cupi rocm docker imag docker run devicedevkfd devicedevdri groupadd video cupycupyrocm sgemm exampl exampl contain implement singleprecis gener matrixmultipl sgemm implement base one magmahttpiclcsutkedumagma demo demo contain script calcul matrix multipl x k b k x n demo run follow command python sgemmpi gpu gpu_id n n k k demo contain exampl work sgemm kernel requir complet interfac culaunchkernel eg grid size size share memori provid cupyelementwisekernel cupi array work regardless underli memori layout thank ndarray abstract case exampl ndarray abstract need use underli memori layout array match one expect kernel sgemm kernel expect input output array fortran contigu memori layout layout enforc cupyasfortranarray dynam compil launch kernel function written cuda c compil cupyrawkernel class use compil cuda code written sgemmcu class take text code name kernel constructor argument instanc callabl cuda code compil invok call compil code cach avoid compil process first time also cuda code modifi python level simpli text exampl c macro determin distribut data thread specifi runtim note extern c need put top kernel call suppli grid size block size share memori size launch kernel function cupyrawkernel object allow call kernel cuda culaunchkernel interfac word control grid size block size share memori size stream level interfac becom straightforward replac host cu call cuda kernel python code kmean exampl exampl contain implement kmean cluster demo demo contain script partit data group use kmean cluster demo run follow command python kmeanspi gpuid gpu_id ncluster n_cluster num num maxit max_it usecustomkernel outputimag output_imag run script environ without matplotlib render eg nongui environ set environment variabl mplbackend agg may requir use matplotlib exampl mplbackendagg python kmeanspi gmm exampl exampl contain implement gaussian mixtur model gmm demo demo contain script partit data group use gaussian mixtur model demo run follow command python gmmpi gpuid gpu_id num num dim dim maxit max_it tol tol outputimag output run script environ without matplotlib render eg nongui environ set environment variabl mplbackend agg may requir use matplotlib exampl mplbackendagg python gmmpi custom user structur exampl folder contain exampl custom user structur cupyrawkernel see httpsdocscupydevenstabletutorialkernelhtmlhttpsdocscupydevenstabletutorialkernelhtml correspond document folder provid three script rank increas complex builtins_vectorspi show use cuda builtin vector float scalar paramet pass valu host array paramet rawkernel packed_matrixpi demonstr creat use templat pack structur rawmodul complex_structpi illustr possibl recurs build complex numpi dtype match devic structur memori layout exampl run simpl python script pythonx example_namepi auto gener edit cupi ci test coverag param test system linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux linux target cudatd cudamultitd cudatd cudamultitd cudatd cudamultitd cudatd cudamultitd cudatd cudamultitd cudatd cudamultitd cudatd cudamultitd rocmtd rocmtd rocmtd cudaslowtd cudaexampletd cudaheadtd cudaxcudapythontd system linux window os ubuntu ubuntu cento cento ws cuda null rocm null nccl null cutensor null cusparselt null cudnn null python pre numpi pre scipi null pre optuna null pre cython pre cudapython null envcupy_acceler null cub cutensor cubcutensor cutensorcub test unit unitmulti unitslow exampl thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpscipreferredjpcupylinuxcuda dlinuxtestscudadockerfil slinuxtestscudash thttpscipreferredjpcupylinuxcudamulti dlinuxtestscudamultidockerfil slinuxtestscudamultish thttpsjenkinspreferredjpjobchainerjobcupy_mastertestrocmlabelmnjmi dlinuxtestsrocmdockerfil slinuxtestsrocmsh thttpsjenkinspreferredjpjobchainerjobcupy_mastertestrocmlabelmnjmi dlinuxtestsrocmdockerfil slinuxtestsrocmsh thttpsjenkinspreferredjpjobchainerjobcupy_mastertestrocmlabelmnjmi dlinuxtestsrocmdockerfil slinuxtestsrocmsh thttpscipreferredjpcupylinuxcudaslow dlinuxtestscudaslowdockerfil slinuxtestscudaslowsh thttpscipreferredjpcupylinuxcudaexampl dlinuxtestscudaexampledockerfil slinuxtestscudaexamplesh thttpscipreferredjpcupylinuxcudahead dlinuxtestscudaheaddockerfil slinuxtestscudaheadsh thttpscipreferredjpcupylinuxcudaxcudapython dlinuxtestscudaxcudapythondockerfil slinuxtestscudaxcudapythonsh cupi ci cupi use two infrastructur gpu test flexci pfnpublicci gcp linuxwindow cuda jenkin onpremis linux cudarocm current test configur manag chainertesthttpgithubcomchainerchainertest contain set script jenkin gradual migrat new tool directori also gradual migrat jenkin flexci better perform eventu jenkin use rocm test directori contain test matrix definit tool gener test environ matrix schemayaml defin possibl valu test axi constraint matrixyaml defin configur matrix generatepi gener test environ dockerfileshel script linux powershel script window matrix schema matrix program also gener coveragemd see configur coverag usag gener linuxtestsdockerfil linuxtestssh coveragemd pip instal pyyaml generatepi schemayaml matrixyaml futur work support gener window test environ test notif gitter gener shuffl test schemayaml support use osprovid python binari packag instead pyenv support coverag report support instal test linux ci script directori contain asset use ci test isol docker develop contributor reproduc environ ci use runsh tool build docker imag run unit test imag current local codebas readonli mount contain use test runsh take target one stage argument run runsh without argument detail usag list stage exampl target cuda stage build docker imag test run unit test runsh cuda build test target rocm stage build docker imag runsh rocm build test target test directori contain dockerfil bootstrap shell script target name prefix exampl target rocm rocmdockerfil rocmsh use test file gener generatepi tool except follow target cudarapid run unit test compon requir rapid eg cupyxscipysparsecsgraph arrayapi run arrayapitestshttpsgithubcomdataapisarrayapitest cupyarray_api modul upgrad guid page cover chang introduc major version user know migrat older releas pleas see also refcompatibility_matrix support environ major version cupi v drop cuda support cuda earlier longer support use cuda later drop nccl v v v support nccl v v v longer support drop python support python longer support drop numpi support numpi longer support chang classcupycudadevic behavior current devic set via use restor exit block current devic set via funccupycudadeviceus reactiv exit devic context manag exist code mix devic block deviceus may get differ result cupi v v codeblock py cupycudadevic cupycudadeviceus pass cupycudadevic cupi v return devic instead devic chang classcupycudastream behavior stream manag perdevic previouli user respons keep current stream consist current cuda devic exampl follow code rais error cupi v earlier codeblock py import cupi cupycudadevic creat stream devic cupycudastream cupycudadevic tri use stream devic cupyarang cuda_error_invalid_handl invalid resourc handl cupi v manag current stream perdevic thu elimin need switch stream everi time activ devic chang use cupi v exampl behav differ whenev stream creat automat associ current devic ignor switch devic earli version tri use devic rais error associ devic howev v ignor default stream devic use instead current stream set via use restor exit block same chang classcupycudadevic current stream set via funccupycudastreamus reactiv exit stream context manag exist code mix stream block streamus may get differ result cupi v v codeblock py cupycudastream cupycudastream cupycudastream suse pass cupycudaget_current_stream cupi v return instead stream share thread classcupycudastream instanc safe share multipl thread achiev cupi v destroy stream cudastreamdestroy stream current stream thread bigendian array automat convert littleendian funccupyarray funccupyasarray variant alway transfer data gpu littleendian byte order previous cupi copi given classnumpyndarray gpu asi regardless endian cupi v bigendian array convert littleendian transfer nativ byte order gpu chang elimin need manual chang array endian creat cupi array baselin api updat baselin api bump numpi scipi numpi scipi cupi v follow upstream product specif baselin version api chang devic synchron detect api funccupyxallow_synchron classcupyxdevicesynchron introduc experiment featur cupi v mark deprec imposs detect synchron reliabl intern api funccupycudacompile_with_cach mark deprec better altern see classcupyrawmodul ad sinc cupi v classcupyrawkernel sinc v longstand histori api never meant public encourag downstream librari user migrat aforement public api see docuser_guidekernel tutori dlpack routin funccupyfromdlpack deprec favor funccupyfrom_dlpack address potenti data race issu new modul modcupyxprofil ad host profil relat api cupi accordingli follow api reloc modul follow old routin deprec funccupyproftimerangedecor funccupyxprofilertime_rang funccupyproftime_rang funccupyxprofilertime_rang funccupycudaprofil funccupyxprofilerprofil funccupyxtimerepeat funccupyxprofilerbenchmark funccupyndarray__pos__ return copi same funccupyposit instead return self note deprec api may remov futur cupi releas updat docker imag cupi offici docker imag see docinstal detail updat use cuda rocm cupi v drop support cuda cuda longer support use cuda later drop support cudnn v nccl v cudnn v earlier nccl v earlier longer support drop support numpi scipi numpi scipi longer support drop support python python longer support cupi v nccl cudnn longer includ wheel nccl cudnn share librair longer includ wheel see httpsgithubcomcupycupyissues_ discuss manual instal instal wheel dont previou instal see docinstal detail cutensor enabl wheel cutensor use instal cupi via wheel cupycudancclcudnn modul need explicit import previous cupycudanccl cupycudacudnn modul automat import sinc cupi v modul need explicitli import ie import cupycudanccl import cupycudacudnn baselin api updat baselin api bump numpi scipi numpi scipi cupi v follow upstream product specif baselin version follow numpi alias python scalar type cupybool cupyint cupyfloat cupycomplex deprec cupybool_ cupyint_ cupyfloat_ cupycomplex_ use instead requir updat docker imag cupi offici docker imag see docinstal detail updat use cuda python cupi v drop support cuda cuda longer support use cuda later drop support numpi scipi numpi earlier scipi earlier longer support updat docker imag cupi offici docker imag see docinstal detail updat use cuda python scipi optuna preinstal cub support compil requir cub modul built default enabl use cub set cupy_acceleratorscub see envvarcupy_acceler detail due chang g later requir build cupi sourc see docinstal detail follow environ variabl longer effect cub_dis use envvarcupy_acceler aforement cub_path longer requir cupi use either cub sourc bundl cuda use cuda later one cupi distribut api chang cupyscatter_add deprec cupi v remov use funccupyxscatter_add instead cupyspars modul deprec remov futur releas use modcupyxscipyspars instead dtype argument funccupyndarraymin funccupyndarraymax remov align numpi specif funccupyallclos return result dim gpu array instead python bool avoid devic synchron classcupyrawmodul delay compil time first call align behavior classcupyrawkernel cupycuda_en flag nccl_enabl nvtx_enabl etc deprec use cupycudaavail flag cupycudancclavail cupycudanvtxavail etc instead chainer_se environ variabl longer effect use cupy_se instead cupi v drop support python start cupi v python longer support reach endoflif eol januari march python minimum python version support cupi v pleas upgrad python version use affect version python later version list docinstal cupi v binari packag ignor ld_library_path prior cupi v ld_library_path environ variabl use overrid cudnn nccl librari bundl binari distribut also known wheel cupi v ld_library_path ignor discoveri cudnn nccl cupi binari distribut alway use librari come packag avoid error caus unexpect overrid cupi v cupyxscipi namespac modcupyxscipi namespac introduc provid cudaen scipi function modcupyspars modul renam modcupyxscipyspars modcupyspars kept alia backward compat drop support cuda cupi v longer support cuda updat docker imag cupi offici docker imag see docinstal detail updat use cuda cudnn use imag may need upgrad nvidia driver host see requir nvidiadock httpsgithubcomnvidianvidiadockerwikicudarequirements_ detail cupi v note version number bump v v align version chainer therefor cupi v exist default memori pool prior cupi v memori pool enabl default cupi use chainer cupi v memori pool enabl default even use cupi without chainer memori pool significantli improv perform mitig overhead memori alloc cpugpu synchron attent monitor gpu memori usag eg use nvidiasmi may notic gpu memori freed even array instanc becom scope expect behavior default memori pool cach alloc memori block access default memori pool instanc use funcget_default_memory_pool funcget_default_pinned_memory_pool access statist free unus memori block cach memori pool codeblock py import cupi cupyndarray dtypecupyfloat mempool cupyget_default_memory_pool perform size actual alloc may becom larger request array size printmempoolused_byt printmempooltotal_byt even array goe scope memori block kept pool none printmempoolused_byt printmempooltotal_byt clear memori block call free_all_block mempoolfree_all_block printmempoolused_byt printmempooltotal_byt even disabl default memori pool code sure cupi oper codeblock py import cupi cupycudaset_allocatornon cupycudaset_pinned_memory_allocatornon comput capabl cupi v requir nvidia gpu comput capabl larger see list cuda gpu httpsdevelopernvidiacomcudagpus_ check gpu support comput capabl cuda stream cuda stream fulli support cupi v cupycudarandomstateset_stream function chang stream use random number gener remov pleas use funccupycudastreamus instead see discuss httpsgithubcomcupycupypull_ detail cupyx namespac cupyx namespac introduc provid featur specif cupi ie featur provid numpi avoid collis futur see docreferenceext list function rule funccupyscatter_add move funccupyxscatter_add funccupyscatter_add still avail alia encourag use funccupyxscatter_add instead updat docker imag cupi offici docker imag see docinstal detail updat use cuda cudnn chang introduc cuda support nvidia pascal gpu use imag may need upgrad nvidia driver host see requir nvidiadock httpsgithubcomnvidianvidiadockerwikicudarequirements_ detail cupi v chang behavior count_nonzero function perform reason funccupycount_nonzero chang return zerodimension classndarray instead int axisnon see discuss httpsgithubcomcupycupypull_ detail _compatibility_matrix compat matrix listtabl headerrow cupi cc _ cuda rocm cutensor nccl cudnn python numpi scipi baselin api spec doc v latest httpsdocscupydevenstableinstallhtml__ v numpi scipi stabl httpsdocscupydevenstableinstallhtml__ v x numpi scipi v httpsdocscupydevenvinstallhtml__ v x x _ numpi scipi v httpsdocscupydevenvinstallhtml__ v x x _ specifi specifi v httpsdocscupydevenvinstallhtml__ v x na na specifi specifi v httpsdocscupydevenvinstallhtml__ v x na na specifi specifi v httpsdocscupydevenvinstallhtml__ v x na na specifi specifi v httpsdocscupydevenvinstallhtml__ cuda comput capabl highli experiment support limit featur _overview overview cupi httpsgithubcomcupycupy__ numpyscipycompat array librari gpuacceler comput python cupi act dropin replac run exist numpyscipi code nvidia cuda httpsdevelopernvidiacomcudatoolkit__ amd rocm httpswwwamdcomengraphicsserverssolutionsrocm__ platform cupi provid ndarray spars matric associ routin gpu devic api numpi scipi ndimension array ndarray doccupyndarray referencendarray data type dtype boolean bool_ integ int int int int uint uint uint uint float float float float complex complex complex support semant ident classnumpyndarray includ basic advanc index broadcast spars matric doccupyxscipyspars referencescipy_spars spars matrix csr_matrix coo_matrix csc_matrix dia_matrix numpi routin docmodulelevel function referenceroutin cupi doclinear algebra function referencelinalg cupylinalg docfast fourier transform referencefft cupyfft docrandom number gener referencerandom cupyrandom scipi routin docdiscret fourier transform referencescipy_fft cupyxscipyfft cupyxscipyfftpack docadvanc linear algebra referencescipy_linalg cupyxscipylinalg docmultidimension imag process referencescipy_ndimag cupyxscipyndimag docspars matric referencescipy_spars cupyxscipyspars docspars linear algebra referencescipy_sparse_linalg cupyxscipysparselinalg docspeci function referencescipy_speci cupyxscipyspeci docsign process referencescipy_sign cupyxscipysign docstatist function referencescipy_stat cupyxscipystat routin back cuda librari cubla cufft cuspars cusolv curand thrust cub cutensor provid best perform also possibl easili implement doccustom cuda kernel user_guidekernel work ndarray use kernel templat quickli defin elementwis reduct oper singl cuda kernel raw kernel import exist cuda cc code justintim transpil jit gener cuda kernel python sourc code kernel fusion fuse multipl cupi oper singl cuda kernel cupi run multigpu cluster environ distribut commun packag modcupyxdistribut provid collect peertop primit ndarray back nccl user need finegrain control perform access doclowlevel cuda featur user_guidecuda_api avail stream event cuda stream perthread default stream support api memori pool customiz memori alloc builtin memori pool profil support profil code use cuda profil nvtx host api bind directli call cuda librari nccl cudnn cutensor cusparselt api python cupi implement standard api data exchang interoper dlpack httpsgithubcomdmlcdlpack__ cuda array interfac httpsnumbareadthedocsioenstablecudacuda_array_interfacehtml__ __array_ufunc__ nep httpsnumpyorgnepsnepufuncoverrideshtml__ __array_function__ nep httpsnumpyorgnepsneparrayfunctionprotocolhtml__ array api standard httpsdataapisorgarrayapilatest__ thank protocol cupi easili docintegr user_guideinteroper numpi pytorch tensorflow mpipi librari support standard amd rocm environ cupi automat translat cuda api call rocm hip hipbla hipfft hipspars hiprand hipcub hipthrust rccl etc allow code written use cupi run nvidia amd gpu without modif project goal goal cupi project provid python user gpu acceler capabl without indepth knowledg underli gpu technolog cupi team focus provid complet numpi scipi api coverag becom full dropin replac well advanc cuda featur maxim perform matur qualiti librari fundament packag project need acceler lab environ largescal cluster cupi numpi scipi gpu modul cupi toctre maxdepth overview instal user_guideindex referenceindex toctre maxdepth caption develop contribut toctre maxdepth caption misc note upgrad licens instal requir nvidia cuda gpu httpsdevelopernvidiacomcudagpus_ comput capabl larger cuda toolkit httpsdevelopernvidiacomcudatoolkit_ v v v v v v v multipl version cuda toolkit instal cupi automat choos one cuda instal see refinstall_cuda detail requir option instal cupi condaforg howev still need compat driver instal gpu see refinstall_cupy_from_conda_forg detail python httpspythonorg_ v v v v note current cupi test ubuntu httpswwwubuntucom_ lt lt x_ cento httpswwwcentosorg_ x_ window server x_ python depend numpyscipycompat api cupi v base numpi scipi test follow version numpi httpsnumpyorg_ v v v v v scipi httpsscipyorg_ option v v v v requir use docreferencescipi cupyxscipi optuna httpsoptunaorg_ option vx requir use refkernel_param_opt note scipi optuna option depend instal automat note instal cupi recommend upgrad setuptool pip python pip instal u setuptool pip addit cuda librari part cuda featur cupi activ correspond librari instal cutensor httpsdevelopernvidiacomcutensor_ v librari acceler tensor oper see docreferenceenviron detail nccl httpsdevelopernvidiacomnccl_ v v v v librari perform collect multigpu multinod comput cudnn httpsdevelopernvidiacomcudnn_ v v v v v librari acceler deep neural network comput cusparselt httpsdocsnvidiacomcudacusparselt_ v librari acceler spars matrixmatrix multipl instal cupi instal cupi pypi wheel precompil binari packag avail linux x_ window amd packag name differ depend cuda toolkit version listtabl headerrow cuda command v pip instal cupycuda v pip instal cupycuda v pip instal cupycuda v pip instal cupycuda v pip instal cupycuda v pip instal cupycuda v pip instal cupycuda note enabl featur provid addit cuda librari cutensor nccl cudnn need instal manual instal cupi via wheel use instal command setup librari case dont previou instal python cupyxtoolsinstall_librari cuda librari cutensor note use pip instal cupycudaxxx f httpspipcupydevpr instal prereleas develop version use wheel pleas care instal multipl cupi packag time packag cupi packag sourc instal conflict pleas make sure one cupi packag cupi cupycudaxx xx cuda version instal pip freez grep cupi _install_cupy_from_conda_forg instal cupi condaforg condaanaconda crossplatform packag manag solut wide use scientif comput field pip instal instruct compat conda environ altern linux x_ ppcle aarchsbsa window cuda driver correctli set also instal cupi condaforg channel conda instal c condaforg cupi conda instal prebuilt cupi binari packag along cuda runtim librari cudatoolkit necessari instal cuda toolkit advanc conda builtin mechan determin instal latest version cudatoolkit support driver howev reason need forceinstal particular cuda version say conda instal c condaforg cupi cudatoolkit note cudnn cutensor nccl avail condaforg option depend follow command instal conda instal c condaforg cupi cudnn cutensor nccl also instal separ need note encount problem cupi instal condaforg pleas feel free report cupyfeedstock httpsgithubcomcondaforgecupyfeedstockissues_ help investig packag issu condaforg recip real issu cupi note instal cuda toolkit nvcc compil might avail cudatoolkit packag condaforg includ nvcc compil toolchain would like use local cuda instal need make sure version cuda toolkit match cudatoolkit avoid surpris _install_cupy_from_sourc instal cupi sourc use wheel packag recommend whenev possibl howev wheel meet requir eg run nonlinux environ want use version cuda cudnn nccl support wheel also build cupi sourc note cupi sourc build requir g later ubuntu run aptget instal g ubuntu cento follow instruct refher install_gcc note instal cupi sourc featur provid addit cuda librari disabl librari avail build time see refinstall_cudnn instruct note upgrad downgrad version cuda toolkit cudnn nccl cutensor may need reinstal cupi see refinstall_reinstal detail instal latest stabl releas version cupi sourc packag httpspypipythonorgpypicupy_ via pip pip instal cupi want instal latest develop version cupi clone git repositori git clone recurs httpsgithubcomcupycupygit cd cupi pip instal note cython later requir build cupi sourc automat instal build process avail uninstal cupi use pip uninstal cupi pip uninstal cupi note use wheel cupi shall replac cupycudaxx xx cuda version number note cupi instal via conda pleas conda uninstal cupi instead upgrad cupi use pip instal u option pip instal u cupi note use wheel cupi shall replac cupycudaxx xx cuda version number _install_reinstal reinstal cupi reinstal cupi pleas uninstal cupi instal reinstal cupi recommend use nocachedir option pip cach previous built binari pip uninstal cupi pip instal cupi nocachedir note use wheel cupi shall replac cupycudaxx xx cuda version number use cupi insid docker provid offici docker imag httpshubdockercomrcupycupy_ use nvidia contain toolkit httpsgithubcomnvidianvidiadocker_ run cupi imag gpu login environ bash run python interpret docker run gpu cupycupi binbash run interpret directli docker run gpu cupycupi usrbinpython faq _install_error pip fail instal cupi pleas make sure use latest setuptool pip pip instal u setuptool pip use vvvv option pip command display log instal pip instal cupi vvvv use sudo instal cupi note sudo command propag environ variabl need pass environ variabl eg cuda_path need specifi insid sudo like sudo cuda_pathoptnvidiacuda pip instal cupi use certain version conda may fail build cupi error g error unrecogn command line option r due bug conda see condaconda httpsgithubcomcondacondaissues_ detail encount problem pleas upgrad conda _install_cudnn instal cudnn nccl recommend instal cudnn nccl use binari packag ie use apt yum provid nvidia want instal targz version cudnn nccl recommend instal cuda_path directori exampl use ubuntu copi h file includ directori file lib directori cp pathtocudnnh cuda_pathinclud cp pathtolibcudnnso cuda_pathlib destin directori depend environ want use cudnn nccl instal anoth directori pleas use cflag ldflag ld_library_path environ variabl instal cupi export cflagsipathtocudnninclud export ldflagslpathtocudnnlib export ld_library_pathpathtocudnnlibld_library_path _install_cuda work custom cuda instal instal cuda nondefault directori multipl cuda version host may need manual specifi cuda instal directori use cupi cupi use first cuda instal directori found follow order cuda_path environ variabl parent directori nvcc command cupi look nvcc command path environ variabl usrlocalcuda exampl build cupi use nondefault cuda directori cuda_path environ variabl cuda_pathoptnvidiacuda pip instal cupi note cuda instal discoveri also perform runtim use rule depend system configur may also need set ld_library_path environ variabl cuda_pathlib runtim cupi alway rais cupycudacompilercompileexcept cupi rais compileexcept almost everyth possibl cupi detect cuda instal system correctli follow error messag commonli observ case nvrtc error fail load builtin catastroph error open sourc file cuda_fph error overload function distinguish return type alon error identifi __half_raw undefin pleas tri set ld_library_path cuda_path environ variabl exampl cuda instal usrlocalcuda export cuda_pathusrlocalcuda export ld_library_pathcuda_pathlibld_library_path also see refinstall_cuda _install_gcc build fail ubuntu cento order build cupi sourc system legaci gcc g earlier need manual set g later configur nvcc environ variabl ubuntu sudo addaptrepositori ppaubuntutoolchainrtest sudo apt updat sudo apt instal g export nvccnvcc compilerbindir gcc cento sudo yum instal centosreleasescl sudo yum instal devtoolsetgccc sourc optrhdevtoolseten export nvccnvcc compilerbindir gcc use cupi amd gpu experiment cupi experiment support amd gpu rocm requir amd gpu support rocm httpsgithubcomradeonopencomputerocmhardwareandsoftwaresupport_ rocm httpsrocmdocsamdcomenlatestindexhtml_ v v v see rocm instal guid httpsrocmdocsamdcomenlatestinstallation_guideinstallationguidehtml_ detail follow rocm librari requir sudo apt instal hipbla hipspars rocspars rocrand rocthrust rocsolv rocfft hipcub rocprim rccl environ variabl build run cupi rocm follow environ variabl effect rocm_hom directori contain rocm softwar eg optrocm docker tri run cupi rocm use docker docker run devicedevkfd devicedevdri groupadd video cupycupyrocm _install_hip instal binari packag wheel precompil binari packag avail linux x_ packag name differ depend rocm version listtabl headerrow rocm command v pip instal cupyrocm v pip instal cupyrocm v pip instal cupyrocm build cupi rocm sourc build cupi sourc set cupy_install_use_hip rocm_hom hcc_amdgpu_target environ variabl hcc_amdgpu_target isa name support gpu run rocminfo use valu display name line eg gfx specifi commasepar list isa multipl gpu differ architectur export cupy_install_use_hip export rocm_homeoptrocm export hcc_amdgpu_targetgfx pip instal cupi note dont specifi hcc_amdgpu_target environ variabl cupi built gpu architectur avail build host behavior specif rocm build build cupi nvidia cuda build result affect host configur limit follow featur avail due limit rocm specif cuda cuda array interfac cutensor handl extrem larg array whose size around bit boundari hip known fail size atom addit fp cupyndarrayscatter_add cupyxscatter_add multigpu fft fft callback random number gener algorithm sever option rawkernelrawmodul api jitifi dynam parallel perthread default stream random gener api cupyrandomgener rocm version older follow featur yet support spars matric cupyxscipyspars cudnn hipdnn hermitiansymmetr eigenvalu solver cupylinalgeigh polynomi root use hermitiansymmetr eigenvalu solver follow featur may work edg case eg combin dtype note investig root caus issu necessarili cupi issu rocm may potenti bug cupyndarray__getitem__ httpsgithubcomcupycupypull_ cupyix_ httpsgithubcomcupycupypull_ polynomi routin httpsgithubcomcupycupypull_ httpsgithubcomcupycupypull_ cupybroadcast httpsgithubcomcupycupypull_ cupyconvolv httpsgithubcomcupycupypull_ cupycorrel httpsgithubcomcupycupypull_ random sampl routin cupyrandom httpsgithubcomcupycupypull_ cupylinalgeinsum cupyxscipyndimag cupyxscipysign httpsgithubcomcupycupypull_ httpsgithubcomcupycupypull_ httpsgithubcomcupycupypull_ _contrib contribut guid guid contribut cupi develop cupi run offici repositori github httpsgithubcomcupycupy_ anyon want regist issu send pull request read document classif contribut sever way contribut cupi commun regist issu send pull request pr send question cupi gitter channel httpsgitterimcupycommunity_ cupi user group httpsgroupsgooglecomforumforumcupy_ stackoverflow httpsstackoverflowcomquestionstaggedcupy_ opensourc extern exampl write post cupi document mainli focus though contribut also appreci develop cycl section explain develop process cupi contribut cupi strongli recommend understand develop cycl version version cupi follow pep httpswwwpythonorgdevpepspep_ part semant version httpssemverorg_ version number consist three four part xyzw x denot major version denot minor version z denot revis number option w denot preleas suffix major minor revis number follow rule semant version prereleas suffix follow pep version string much friendli python ecosystem note major updat basic contain compatibilitybreak chang last releas candid rc strict rule though critic api bug fix major version may add break chang major version backward compat see docuser_guidecompat _contribreleasecycl releas cycl first one track stabl version seri revis updat latest major version second one track develop version seri prereleas upcom major version consid x latest major version z succeed major version timelin updat depict follow tabl date ver x ver ver z week xrc week x ya week x yb week x yrc week za might revis releas date shown leftmost column rel releas xrc particular revisionminor releas made four week previou one major version prereleas upcom major version made time whether releas revis minor determin base content updat note three stabl releas version xxx parallel develop za version treat almostst version z treat develop version critic bug found xxx stop develop version x may releas hotfix version time creat mileston upcom releas github github mileston basic use collect issu pr resolv releas _contribgitbranch git branch master branch use develop prereleas version mean alpha beta rc updat develop master branch branch contain uptod sourc tree includ featur newli ad latest major version stabl version develop individu branch name vn n reflect version number call version branch exampl v v v develop v branch note contributor send pull request basic send master branch chang also appli stabl version core team member appli chang stabl version chang also includ next revis updat chang applic stabl version master branch pleas send version branch basic accept chang latest version branch stabl version develop unless fix critic want make new featur master branch avail current stabl version pleas send backport pr stabl version latest vn branch see next section detail note chang appli branch sent master branch releas stabl version also merg develop version chang also reflect next major version featur backport pr basic backport new featur develop version stabl version desir includ featur current stabl version work backport work welcom contribut case send backport pr latest vn branch note accept featur backport pr older version run qualiti assur workflow eg ci older version ensur pr correctli port rule send backport pr start pr titl prefix backport clarifi origin pr number pr descript someth like backport xxxx option write pr descript motiv backport featur stabl version pleas follow rule creat featur backport pr note pr includ changesaddit api eg bug fix document improv usual backport core dev member also appreci make backport pr contributor though overal develop proce smoothli issu pull request section explain file issu send pull request pr issuepr label issu pr label follow tag bug bug report issu bug fix pr enhanc implement improv without break interfac featur featur request issu implement pr nocompat disrupt backward compat test test fix updat document document fix improv exampl fix improv exampl instal fix instal script contributionwelcom issu request contribut issu categor issu pr multipl tag might label one issuepr note revis releas includ pr featur nocompat categori file issu regist issu write precis explan want cupi bug report must includ necessari suffici condit reproduc bug featur request must includ want want need cupi contain thought realiz featur request though part import discuss warn question usag cupi highli recommend send post cupi gitter channel httpsgitterimcupycommunity_ cupi user group httpsgroupsgooglecomforumforumcupy_ stackoverflow httpsstackoverflowcomquestionstaggedcupy_ instead issu tracker issu tracker place share knowledg practic may suggest place immedi close howto question issu send pull request write code fix issu encourag send pr first start write code forget confirm follow point read refcodingguid reftestingguid check appropri branch send pr follow refcontribgitbranch idea select branch pleas choos master branch particular check branch write code current sourc tree chosen branch start point chang write code includ unit test hope document send pr github write precis explan fix first document code develop read import part pr send pr automat test github action automat test pass core develop start review code note automat pr test includ cpu test note also run continu integr gpu test master branch version branch latest major version sinc servic current run intern server use automat pr test keep server secur plan add new featur modifi exist api recommend open issu discuss design first design discuss need lower cost core develop code review follow consequ discuss send pr smoothli review shorter time even code complet send pull request workinprogress pr put wip prefix pr titl write precis explan pr core develop contributor join discuss proceed pr wip pr also use discuss base concret code _codingguid code guidelin note code guidelin updat v contribut older version read guidelin use pep httpswwwpythonorgdevpepspep_ part openstack style guidelin httpsdocsopenstackorgdeveloperhacking_ relat gener code style basic style guidelin use autopep flake command check code order avoid confus use differ tool version pin version tool instal follow command within top directori cupi repositori pip instal e stylecheck check code autopep pathtoyourcodepi flake pathtoyourcodepi check cython code use flakecython configur file flake configflakecython pathtoyourcythoncodepyx autopep support automat correct python code conform pep style guid autopep inplac pathtoyourcodepi flake command let know part code obey style guidelin send pull request sure check code pass flake check note flake command perfect check style guidelin notcomplet list rule flake check rel import prohibit h import nonmodul symbol prohibit import statement must organ three part standard librari thirdparti librari intern import h addit restrict usag shortcut symbol code base symbol import packag subpackag cupi exampl cupycudadevic shortcut cupycudadevicedevic allow use shortcut cupi librari implement note still use treetest treeexampl directori send pull request code style automat check github action review process start check pass cupi design base numpi api design cupi sourc code document contain origin numpi one pleas note follow write document order identifi overlap part prefer add remark document copi alter origin one also prefer briefli explain specif function short paragraph refer correspond function numpi user read detail document howev possibl includ complet copi document remark user summar way function cupi implement limit amount featur origin one user explicitli describ implement document chang modifi add new cython file pleas make sure pointer type follow guidelin httpsgithubcomcupycupyissues_ pointer void use within cython intptr_t expos python space memori size size_t memori offset ptrdiff_t note increment enforc rule exist code may follow guidelin pleas ensur new contribut _testingguid unit test test one import part code must write test case verifi implement follow test guid note use pytest mock packag test instal write code pip instal pytest mock run test order run unit test repositori root first build cython file place run follow command pip instal e note modifi pxd file run pip instal e must clean cpp file follow command cython automat rebuild file nice git clean fdx cython modul built run unit test run follow command repositori root python pytest cuda must instal run unit test gpu test requir cudnn run order skip unit test requir cudnn specifi mnot cudnn option python pytest pathtoyourtestpi mnot cudnn gpu test involv multipl gpu want run gpu test insuffici number gpu specifi number avail gpu cupy_test_gpu_limit exampl one gpu launch pytest follow command skip multigpu test export cupy_test_gpu_limit python pytest pathtogputestpi follow name convent run test run follow command repositori root python pytest also specifi root directori search test script python pytest testscupy_test run test cupi python pytest testsinstall_test run test instal modul modifi code relat exist unit test must run appropri command test file directori name convent test put treetestscupy_test directori order enabl test runner find test script correctli use special name convent test subdirectori test script name subdirectori test must end _test suffix name test script must start test_ prefix write test modul use appropri path file name test script whose correspond test modul clear exampl want write test modul cupyxyz test script must locat testscupy_testsx_testsy_teststest_zpi write test mani exampl unit test treetest directori read good recommend way learn write test cupi simpli use modunittest packag standard librari test use util modcupytest addit refcodingguid mention follow rule appli test code test class must inherit classunittesttestcas use modunittest featur write test except follow case use assert statement instead selfassert method eg write assert x instead selfassertequalx use pytestrais instead selfassertrais note increment appli style exist test may use old style selfassertrais etc newli written test follow style even patch includ gpurel code test fail without gpu capabl test function requir cuda must tag cupytestingattrgpu import unittest cupytest import attr class testmyfuncunittesttestcas attrgpu def test_my_gpu_funcself function tag gpu decor skip cupy_test_gpu_limit environ variabl set also cupytestingattrcudnn decor let pytest know test depend cudnn test function decor cudnn skip mnot cudnn given test function decor gpu must depend multipl gpu order write test multipl gpu use cupytestingattrmulti_gpu decor instead import unittest cupytest import attr class testmyfuncunittesttestcas attrmulti_gpu specifi number requir gpu def test_my_two_gpu_funcself test requir much time add cupytestingattrslow decor test function decor slow skip mnot slow given import unittest cupytest import attr class testmyfuncunittesttestcas attrslow def test_my_slow_funcself note want specifi two attribut use oper like mnot cudnn slow see detail document pytest httpsdocspytestorgenlatestexamplemarkershtml_ send pull request travisci httpstravisciorgcupycupy_ automat check code meet code guidelin describ sinc travisci support cuda run unit test automat review process start automat check pass note review test code without option check cudarel code note numer unstabl test might caus error irrelev chang case ignor failur go review process worri document ad new featur framework also need document refer note unsur fix document submit pull request without review help fix document appropri document sourc store doc directori httpsgithubcomcupycupytreemasterdocs_ written restructuredtext httpwwwsphinxdocorgenmasterusagerestructuredtextindexhtml_ format build document need instal sphinx httpwwwsphinxdocorg_ pip instal r docsrequirementstxt build document html format local cd doc make html html file gener buildhtml directori open indexhtml browser see render expect note docstr document comment sourc code collect instal cupi modul modifi docstr make sure instal modul eg use pip instal e build document tip develop tip develop hack cupi sourc code instal edit develop recommend use pip e option instal edit mode pip instal e pleas note even e rerun pip instal e regener c sourc use cython modifi cython sourc file eg pyx file use ccach nvcc environ variabl specifi build time use custom command instead nvcc speed rebuild use ccach httpsccachedev_ v later export nvccccach nvcc limit architectur use cupy_nvcc_generate_cod environ variabl reduc build time limit target cuda architectur exampl run cupi build nvidia p v use export cupy_nvcc_generate_codearchcompute_codesm_archcompute_codesm_ see docreferenceenviron descript fullnam underlin currentmodul modul autoclass objnam method block method rubric method special method item __call__ __enter__ __exit__ __getitem__ __setitem__ __len__ __next__ __iter__ __copy__ item all_method item all_attribut automethod item endif endfor ordinari method item method item __init__ automethod item endif endfor special method item __eq__ __ne__ __lt__ __le__ __gt__ __ge__ __nonzero__ __bool__ item all_method automethod item endif endfor endblock atribut block attribut attribut rubric attribut item attribut autoattribut item endfor endif endblock memori manag cupi use memori pool memori alloc default memori pool significantli improv perform mitig overhead memori alloc cpugpu synchron two differ memori pool cupi devic memori pool gpu devic memori use gpu memori alloc pin memori pool nonswapp cpu memori use cputogpu data transfer attent monitor memori usag eg use nvidiasmi gpu memori ps cpu memori may notic memori freed even array instanc becom scope expect behavior default memori pool cach alloc memori block see docreferencecuda detail memori manag api use pin memori conveni also provid highlevel api cupyx namespac includ funccupyxempty_pin funccupyxempty_like_pin funccupyxzeros_pin funccupyxzeros_like_pin return numpi array back pin memori cupi pin memori pool use pin memori alloc pool note cupi v provid reffft plan cach fft_plan_cach could use portion devic memori fft relat function use memori taken releas shrink disabl cach memori pool oper memori pool instanc provid statist memori alloc access default memori pool instanc use funccupyget_default_memory_pool funccupyget_default_pinned_memory_pool also free unus memori block hold memori pool see exampl code detail codeblock py import cupi import numpi mempool cupyget_default_memory_pool pinned_mempool cupyget_default_pinned_memory_pool creat array cpu numpi alloc byte cpu manag cupi memori pool a_cpu numpyndarray dtypenumpyfloat printa_cpunbyt access statist memori pool printmempoolused_byt printmempooltotal_byt printpinned_mempooln_free_block transfer array cpu gpu alloc byte devic memori pool anoth byte pin memori pool alloc pin memori releas transfer complet note actual alloc size may round larger valu request size perform cupyarraya_cpu printanbyt printmempoolused_byt printmempooltotal_byt printpinned_mempooln_free_block array goe scope alloc devic memori releas kept pool futur reus none del printmempoolused_byt printmempooltotal_byt printpinned_mempooln_free_block clear memori pool call free_all_block mempoolfree_all_block pinned_mempoolfree_all_block printmempoolused_byt printmempooltotal_byt printpinned_mempooln_free_block see classcupycudamemorypool classcupycudapinnedmemorypool detail limit gpu memori usag hardlimit amount gpu memori alloc use cupy_gpu_memory_limit environ variabl see docreferenceenviron detail codeblock py set hardlimit gib export cupy_gpu_memory_limit also specifi limit fraction total amount memori gpu gpu gib memori follow equival configur export cupy_gpu_memory_limit import cupi printcupyget_default_memory_poolget_limit also set limit overrid valu specifi via environ variabl use methcupycudamemorypoolset_limit way use differ limit gpu devic codeblock py import cupi mempool cupyget_default_memory_pool cupycudadevic mempoolset_limits gib cupycudadevic mempoolset_limits gib note cuda alloc gpu memori outsid memori pool cuda context librari handl etc depend usag memori may take one hundr mib count limit chang memori pool use memori alloc instead default memori pool pass memori alloc function funccupycudaset_alloc funccupycudaset_pinned_memory_alloc memori alloc function take argument request size byte return classcupycudamemorypoint classcupycudapinnedmemorypoint cupi provid two alloc use manag memori stream order memori gpu see funccupycudamalloc_manag funccupycudamalloc_async respect detail enabl memori pool back manag memori construct new classcupycudamemorypool instanc alloc set funccupycudamalloc_manag follow codeblock py import cupi use manag memori cupycudaset_allocatorcupycudamemorypoolcupycudamalloc_managedmalloc note pass funccupycudamalloc_manag directli funccupycudaset_alloc without construct classcupycudamemorypool instanc memori freed releas back system immedi may may desir stream order memori alloc new featur ad sinc cuda cupi provid experiment interfac similar cupi memori pool stream order memori alloc also allocatesdealloc memori asynchron fromto memori pool streamord fashion key differ builtin featur implement cuda driver nvidia cuda applic processs easili alloc memori pool enabl memori pool manag stream order memori construct new classcupycudamemoryasyncpool instanc codeblock py import cupi use asynchron stream order memori cupycudaset_allocatorcupycudamemoryasyncpoolmalloc creat custom stream cupycudastream would alloc memori asynchron stream cupyempti dtypecupyfloat note case use classcupycudamemorypool class classcupycudamemoryasyncpool take differ input argument classcupycudamemorypool indic pool use pleas refer classcupycudamemoryasyncpool document detail note pass funccupycudamalloc_async directli funccupycudaset_alloc without construct classcupycudamemoryasyncpool instanc devic current memori pool use use stream order memori import maintain correct stream semant use exampl classcupycudastream classcupycudaev api see refcuda_stream_ev detail cupi attempt act smartli upon dealloc memori freed asynchron either stream alloc first attempt current cupi stream second attempt permit stream memori alloc get destroy memori alloc freed addit applicationslibrari intern use cudamalloc cuda default synchron alloc could unexpect interplay stream order memori alloc specif memori freed memori pool might immedi visibl cudamalloc lead potenti outofmemori error case either call methcupycudamemoryasyncpoolfree_all_block manual perform eventstreamdevic synchron retri current classcupycudamemoryasyncpool interfac experiment particular api larg ident classcupycudamemorypool sever pool method requir suffici new driver cours support hardwar cuda version platform due cuda limit even disabl default memori pool code sure cupi oper codeblock py import cupi disabl memori pool devic memori gpu cupycudaset_allocatornon disabl memori pool pin memori cpu cupycudaset_pinned_memory_allocatornon interoper cupi use conjunct librari cuda function construct use cuda stream creat foreign librari cupi see refcuda_stream_ev numpi classcupyndarray implement __array_ufunc__ interfac see nep mechan overrid ufunc httpwwwnumpyorgnepsnepufuncoverrideshtml_ detail enabl numpi ufunc directli oper cupi array __array_ufunc__ featur requir numpi later code python import cupi import numpi arr cupyrandomrandn astypecupyfloat result numpysumarr printtyperesult class cupy_corecorendarray classcupyndarray also implement __array_function__ interfac see nep dispatch mechan numpi high level array function httpwwwnumpyorgnepsneparrayfunctionprotocolhtml_ detail enabl code use numpi directli oper cupi array __array_function__ featur requir numpi later numpi __array_function__ enabl default numba numba httpsnumbapydataorg_ python jit compil numpi support classcupyndarray implement __cuda_array_interface__ cuda array interchang interfac compat numba v later see cuda array interfac httpsnumbareadthedocsioenstablecudacuda_array_interfacehtml_ detail mean pass cupi array kernel jite numba follow simpl exampl code borrow numbanumba httpsgithubcomnumbanumbapull_ code python import cupi numba import cuda cudajit def addx start cudagrid stride cudagrids rangestart xshape stride outi xi yi cupyarang b cupyzeros_likea printout add b printout addit funccupyasarray support zerocopi convers numba cuda array cupi array code python import numpi import numba import cupi x numpyarang type numpyndarray x_numba numbacudato_devicex type numbacudacudadrvdevicearraydevicendarray x_cupi cupyasarrayx_numba type cupyndarray warn __cuda_array_interface__ specifi object lifetim must manag user undefin behavior export object destroy still use consum librari note cupi use two environ variabl control exchang behavior envvarcupy_cuda_array_interface_sync envvarcupy_cuda_array_interface_export_vers mpipi mpi python mpipi httpsmpipyreadthedocsioenlatest_ python wrapper messag pass interfac mpi librari mpi wide use standard highperform interprocess commun recent sever mpi vendor includ mpich open mpi mvapich extend support beyond mpi standard enabl cudaawar pass cuda devic pointer directli mpi call avoid explicit data movement host devic __cuda_array_interface__ mention dlpack data exchang protocol see refdlpack implement cupi mpipi provid experiment support pass cupi array mpi call provid mpipi built cudaawar mpi implement follow simpl exampl code borrow mpipi tutori httpsmpipyreadthedocsioenlatesttutorialhtml_ code python run script n mpi process mpiexec n n python this_scriptpi import cupi mpipi import mpi comm mpicomm_world size commget_s allreduc sendbuf cupyarang dtypei recvbuf cupyempty_likesendbuf commallreducesendbuf recvbuf assert cupyallcloserecvbuf sendbufs new featur ad sinc mpipi see mpipi websit httpsmpipyreadthedocsioenlatest_ inform pytorch pytorch httpspytorchorg_ machin learn framefork provid highperform differenti tensor oper pytorch also support __cuda_array_interface__ zerocopi data exchang cupi pytorch achiev cost caveat pytorch default creat cpu tensor __cuda_array_interface__ properti defin user need ensur tensor alreadi gpu exchang code python import cupi cp import torch convert torch tensor cupi array torchrand devicecuda b cpasarraya b b b array dtypefloat tensor devicecuda check underli memori pointer assert a__cuda_array_interface__data b__cuda_array_interface__data convert cupi array torch tensor cparang b torchas_tensora devicecuda b b tensor devicecuda array assert a__cuda_array_interface__data b__cuda_array_interface__data pytorch also support zerocopi data exchang dlpack see refdlpack code python import cupi import torch torchutilsdlpack import to_dlpack torchutilsdlpack import from_dlpack creat pytorch tensor tx torchrandn cuda convert dlpack tensor dx to_dlpacktx convert cupi array cx cupyfrom_dlpackdx convert back pytorch tensor tx from_dlpackcxtodlpack pytorchpfnextra httpsgithubcompfnetpytorchpfnextras_ librari provid addit integr featur pytorch includ memori pool share stream share code python import cupi import torch import pytorch_pfn_extra ppe perform cupi memori alloc use pytorch memori pool ppecudause_torch_mempool_in_cupi torchcudamemory_alloc arr cupyarang torchcudamemory_alloc chang default stream pytorch cupi stream torchcudastream ppecudastreamstream use custom kernel pytorch dlpack protocol becom simpl implement function pytorch use cupi userdefin kernel exampl pytorch autograd function comput forward backward pass logarithm use classcupyrawkernel code python import cupi import torch cupy_custom_kernel_fwd cupyrawkernel r extern c __global__ void cupy_custom_kernel_fwdconst float x float int size int tid blockdimx blockidxx threadidxx tid size ytid logxtid cupy_custom_kernel_fwd cupy_custom_kernel_bwd cupyrawkernel r extern c __global__ void cupy_custom_kernel_bwdconst float x float gy float gx int size int tid blockdimx blockidxx threadidxx tid size gxtid gytid xtid cupy_custom_kernel_bwd class cupylogtorchautogradfunct staticmethod def forwardctx x ctxinput x enforc contigu array simplifi rawkernel index cupy_x cupyascontiguousarraycupyfrom_dlpackxdetach cupy_i cupyemptycupy_xshap dtypecupy_xdtyp x_size cupy_xs bs cupy_custom_kernel_fwd bs x_size bs bs cupy_x cupy_i x_size ownership devic memori back cupy_i implicitli transfer torch_i oper safe even go scope function torch_i torchfrom_dlpackcupy_i return torch_i staticmethod def backwardctx grad_i enforc contigu array simplifi rawkernel index cupy_input cupyfrom_dlpackctxinputdetachravel cupy_grad_i cupyfrom_dlpackgrad_ydetachravel cupy_grad_x cupyzeroscupy_grad_yshap dtypecupy_grad_ydtyp gy_siz cupy_grad_ys bs cupy_custom_kernel_bwd bs gy_siz bs bs cupy_input cupy_grad_i cupy_grad_x gy_siz ownership devic memori back cupy_grad_x implicitli transfer torch_i oper safe even go scope function torch_grad_x torchfrom_dlpackcupy_grad_x return torch_grad_x note directli feed torchtensor funccupyfrom_dlpack support new dlpack data exchang protocol ad cupi v pytorch earlier version need wrap tensor torchutilsdlpackto_dlpack shown exampl rmm rmm rapid memori manag httpsdocsrapidsaiapirmmstableindexhtml_ provid highli configur memori alloc rmm provid interfac allow cupi alloc memori rmm memori pool instead cupi pool set simpl code python import cupi import rmm cupycudaset_allocatorrmmrmm_cupy_alloc sometim perform alloc may desir rmm provid option switch alloc code python import cupi import rmm rmmreinitializepool_allocatortru also set init pool size etc cupycudaset_allocatorrmmrmm_cupy_alloc inform cupi memori manag see docmemori _dlpack dlpack dlpack httpsgithubcomdmlcdlpack__ specif tensor structur share tensor among framework cupi support import export dlpack data structur funccupyfrom_dlpack funccupyndarraytodlpack simpl exampl code python import cupi creat cupi array cx cupyrandomrandn astypecupyfloat convert dlpack tensor dx cxtodlpack convert back cupi array cx cupyfrom_dlpackdx tensorflow httpswwwtensorfloworg_ also support dlpack zerocopi data exchang cupi tensorflow dlpack possibl code python import tensorflow tf import cupi cp convert tf tensor cupi array tfdevicegpu tfrandomuniform tftensor shape dtypefloat numpi array dtypefloat adevic joblocalhostreplicataskdevicegpu cap tfexperimentaldlpackto_dlpacka b cpfrom_dlpackcap b b array dtypefloat tftensor shape dtypefloat numpi array dtypefloat convert cupi array tf tensor cparang cap atodlpack b tfexperimentaldlpackfrom_dlpackcap bdevic joblocalhostreplicataskdevicegpu b tftensor shape dtypeint numpyarray array awar tensorflow tensor immut latter case chang b reflect cupi array note dlpack v correct approach implicitli requir user ensur convers import export cupi array must happen cudahip stream doubt current cupi stream use fetch exampl call funccupycudaget_current_stream pleas consult framework document access control stream dlpack data exchang protocol obviat usermanag stream dlpack tensor object dlpack data exchang protocol httpsdataapisorgarrayapilatestdesign_topicsdata_interchangehtml_ provid mechan shift respons user librari compliant object classcupyndarray must implement pair method __dlpack__ __dlpack_device__ function funccupyfrom_dlpack accept object return classcupyndarray safe access cupi current stream likewis classcupyndarray export via compliant librari from_dlpack function note cupi use envvarcupy_dlpack_export_vers control handl tensor back cuda manag memori perform best practic gather trick advic improv cupi perform benchmark utterli import first identifi perform bottleneck make attempt optim code help set baselin benchmark cupi provid use util funccupyxprofilerbenchmark time elaps time python function cpu gpu doctest cupyxprofil import benchmark def my_funca return cpsqrtcpsuma axi cprandomrandom printbenchmarkmy_func n_repeat doctest skip my_func cpu us min max us gpu us min max us gpu execut run asynchron respect cpu execut common pitfal gpu program mistakenli measur elaps time use cpu time util pyfunctimeperf_count python standard librari timeit magic ipython knowledg gpu runtim funccupyxprofilerbenchmark address set cuda event refcurrent_stream right function measur synchron end event see refcuda_stream_ev detail sketch done intern funccupyxprofilerbenchmark doctest import time start_gpu cpcudaev end_gpu cpcudaev start_gpurecord start_cpu timeperf_count my_funca end_cpu timeperf_count end_gpurecord end_gpusynchron t_gpu cpcudaget_elapsed_timestart_gpu end_gpu t_cpu end_cpu start_cpu addit funccupyxprofilerbenchmark run warmup run reduc time fluctuat exclud overhead first invoc onetim overhead awar overhead benchmark cupi code context initi may take sever second call cupi function first time process cuda driver creat cuda context first cuda api call cuda applic kernel compil cupi use onthefli kernel synthesi kernel call requir compil kernel code optim dimens dtype given argument send gpu devic execut kernel cupi cach kernel code sent gpu devic within process reduc kernel compil time call compil code also cach directori homecupykernel_cach path overwritten set envvarcupy_cache_dir environ variabl allow reus compil kernel binari across process indepth profil construct mark nvtxroctx rang use funccupyxprofilertime_rang api startstop profil use funccupyxprofilerprofil api use cubcutensor backend reduct routin reduct oper funccupysum funccupyprod funccupyamin funccupyamax funccupyargmin funccupyargmax mani routin built upon cupi ship implement thing work box howev dedic effort acceler routin cub httpsgithubcomnvidiacub_ cutensor httpsdevelopernvidiacomcutensor_ order support perform backend wherev applic start v cupi introduc environ variabl envvarcupy_acceler allow user specifi desir backend order tri exampl consid sum cubic array doctest cupyxprofil import benchmark cprandomrandom dtypecpfloat printbenchmarkasum n_repeat doctest skip sum cpu us min max us gpu us min max us see take ms run gpu howev launch python session use cupy_acceleratorscub python get x speedup free ms doctest printbenchmarkasum n_repeat doctest skip sum cpu us min max us gpu us min max us cub backend ship togeth cupi also acceler routin inclus scan ex funccupycumsum histogram spars matrixvector multipl applic cuda classcupyreductionkernel cutensor offer optim perform binari elementwis ufunc reduct tensor contract cutensor instal set cupy_acceleratorscubcutensor exampl would tri cub first fall back cutensor cub provid need support case backend applic fall back cupi default implement note gener acceler reduct faster could except depend data layout particular cub reduct support reduct contigu axe case recommend perform benchmark determin whether cubcutensor offer better perform overlap work use stream construct use jit compil construct pleas refer refjit_kernel_definit quick introduct prefer float float construct differ cupi numpi interfac cupi design obey numpi howev differ cast behavior float integ cast behavior float integ defin c specif cast neg float unsign integ infin integ one exampl behavior numpi depend cpu architectur result intel cpu nparray dtypenpfloatastypenpuint array dtypeuint cupyarray dtypenpfloatastypenpuint array dtypeuint nparrayfloatinf dtypenpfloatastypenpint array dtypeint cupyarrayfloatinf dtypenpfloatastypenpint array dtypeint random method support dtype argument numpi random valu gener support dtype argument instead alway return float valu support option cupi curand use cupi support float float nprandomrandndtypenpfloat traceback recent call last file stdin line modul typeerror randn got unexpect keyword argument dtype cupyrandomrandndtypenpfloat doctest skip array dtypefloat outofbound indic cupi handl outofbound indic differ default numpi use integ array index numpi handl rais error cupi wrap around x nparray x traceback recent call last file stdin line modul indexerror index bound axi size x cupyarray x x array duplic valu indic cupi __setitem__ behav differ numpi integ array refer locat multipl time case valu actual store undefin exampl cupi cupyzero cupyarang v cupyarangeastypenpfloat ai v doctest skip array numpi store valu correspond last element among element referenc duplic locat a_cpu npzero i_cpu nparang v_cpu nparangeastypenpfloat a_cpui_cpu v_cpu a_cpu array zerodimension array reduct method numpi reduct function eg funcnumpysum return scalar valu eg classnumpyfloat howev cupi counterpart return zerodimension classcupyndarray cupi scalar valu eg classcupyfloat alias numpi scalar valu alloc cpu memori type return would requir synchron gpu cpu want use scalar valu cast return array explicitli typenpsumnparang npint true typecupysumcupyarang cupy_corecorendarray true type promot cupi automat promot dtype classcupyndarray function two operand result dtype determin dtype input differ numpi rule type promot operand contain zerodimension array zerodimension classnumpyndarray treat scalar valu appear operand numpi function may affect dtype output depend valu scalar input nparray dtypenpint nparray dtypenpfloatdtyp dtypefloat nparray dtypenpint nparray dtypenpfloatdtyp dtypefloat cupyarray dtypenpint cupyarray dtypenpfloatdtyp dtypefloat matrix type classnumpymatrix scipi return classnumpymatrix subclass classnumpyndarray dens matric comput spars matric eg coo_matrix ndarray howev cupi return classcupyndarray oper plan provid classnumpymatrix equival cupi use classnumpymatrix longer recommend sinc numpi data type data type cupi array nonnumer like string object see refoverview detail univers function work cupi array scalar unlik numpi univers function cupi work cupi array scalar accept object eg list classnumpyndarray nppowernparang array cupypowercupyarang traceback recent call last file stdin line modul typeerror unsupport type class list random seed array hash scalar like numpi cupi randomst object accept seed either number full numpi array seed nparray rs cupyrandomrandomstateseedse howev unlik numpi array seed hash singl number may commun much entropi underli random number gener nan notanumb handl default cupi reduct function eg funccupysum handl nan complex number differ numpi counterpart j complex npnan complexnpnan complexnpnan npnan a_np npasarraya printa_npmax a_npmin nanj nanj a_cp cpasarraya_np printa_cpmax a_cpmin nanj nanj reason intern reduct perform stride fashion thu ensur proper comparison order follow numpi rule alway propag firstencount nan _udkernel userdefin kernel cupi provid easi way defin three type cuda kernel elementwis kernel reduct kernel raw kernel document describ defin call kernel basic elementwis kernel elementwis kernel defin classcupyelementwisekernel class instanc class defin cuda kernel invok __call__ method instanc definit elementwis kernel consist four part input argument list output argument list loop bodi code kernel name exampl kernel comput squar differ mathfx x defin follow doctest squared_diff cpelementwisekernel float x float float z z x x squared_diff argument list consist commasepar argument definit argument definit consist type specifi argument name name numpi data type use type specifi note n name start underscor _ reserv intern use kernel call either scalar array broadcast doctest x cparang dtypenpfloatreshap cparang dtypenpfloat squared_diffx array dtypefloat squared_diffx array dtypefloat output argument explicitli specifi next input argument doctest z cpempti dtypenpfloat squared_diffx z array dtypefloat typegener kernel type specifi one charact treat type placehold use defin typegener kernel exampl squared_diff kernel made typegener follow doctest squared_diff_gener cpelementwisekernel x z z x x squared_diff_gener type placehold charact kernel definit indic type actual type placehold determin actual argument type elementwisekernel class first check output argument input argument determin actual type output argument given kernel invoc input argument use determin type type placehold use loop bodi code doctest squared_diff_gener cpelementwisekernel x z diff x z diff diff squared_diff_gener one type placehold use kernel definit exampl kernel made gener multipl argument doctest squared_diff_super_gener cpelementwisekernel x x z z z x x squared_diff_super_gener note kernel requir output argument explicitli specifi type z automat determin input argument raw argument specifi elementwisekernel class index broadcast automat use defin elementwis comput hand sometim want write kernel manual index argument tell elementwisekernel class use manual index ad raw keyword preced type specifi use special variabl method _indsiz manual index indic index within loop _indsiz indic total number element appli elementwis oper note repres size broadcast oper exampl kernel add two vector revers one written follow doctest add_revers cpelementwisekernel x raw z z x y_indsiz add_revers note artifici exampl write oper z x without defin new kernel raw argument use like array index oper y_indsiz involv index comput arbitrarili shape strode note raw argument involv broadcast want mark argument raw must specifi size argument invoc defin valu _indsiz textur memori textur object classcupycudatexturetextureobject pass classcupyelementwisekernel type mark uniqu type placehold distinct type use kernel actual datatyp determin popul textur memori textur coordin comput kernel perthread loop index reduct kernel reduct kernel defin classcupyreductionkernel class use defin four part kernel code ident valu valu use initi valu reduct map express use preprocess element reduc reduct express oper reduc multipl map valu special variabl b use operand post map express use transform result reduc valu special variabl use input output written output paramet reductionkernel class automat insert code fragment requir effici flexibl reduct implement exampl l norm along specifi axe written follow doctest lnorm_kernel cpreductionkernel x input param output param x x map b reduc sqrta postreduct map ident valu lnorm kernel name x cparang dtypenpfloatreshap lnorm_kernelx axi array dtypefloat note raw specifi restrict usag axe reduc put head shape mean want use raw specifi least one argument axi argument must contigu increas sequenc integ start like etc note textur memori yet support classcupyreductionkernel raw kernel raw kernel defin classcupyrawkernel class use raw kernel defin kernel raw cuda sourc classcupyrawkernel object allow call kernel cuda culaunchkernel interfac word control grid size block size share memori size stream doctest add_kernel cprawkernelr extern c __global__ void my_addconst float x const float x float int tid blockdimx blockidxx threadidxx ytid xtid xtid my_add x cparang dtypecpfloatreshap x cparang dtypecpfloatreshap cpzero dtypecpfloat add_kernel x x grid block argument array dtypefloat raw kernel oper complexvalu array creat well doctest complex_kernel cprawkernelr includ cupycomplexcuh extern c __global__ void my_funcconst complexfloat x const complexfloat x complexfloat float int tid blockdimx blockidxx threadidxx ytid xtid xtid my_func x cupyarang dtypecupycomplexreshap x jcupyarang dtypecupycomplexreshap cupyzero dtypecupycomplex complex_kernel x x cupyfloat grid block argument array j j j j j j j j j j j j j j j j j j j j j j j j j dtypecomplex note encourag usag complext type complex number avail includ cupycomplexcuh shown cuda code alreadi written use function cucomplexh need make convers set option translate_cucomplextru creat classcupyrawkernel instanc cuda kernel attribut retriev either access attrcupyrawkernelattribut dictionari access classcupyrawkernel object attribut directli latter also use set certain attribut doctest add_kernel cprawkernelr extern c __global__ void my_addconst float x const float x float int tid blockdimx blockidxx threadidxx ytid xtid xtid my_add add_kernelattribut doctest skip max_threads_per_block shared_size_byt const_size_byt local_size_byt num_reg ptx_version binary_vers cache_mode_ca max_dynamic_shared_size_byt preferred_shared_memory_carveout add_kernelmax_dynamic_shared_size_byt doctest skip add_kernelmax_dynamic_shared_size_byt set new valu attribut doctest skip add_kernelmax_dynamic_shared_size_byt doctest skip dynam parallel support classcupyrawkernel need provid link flag dc classcupyrawkernel option argument static cuda devic runtim librari cudadevrt automat discov cupi detail see cuda toolkit documentation_ _cuda toolkit document httpsdocsnvidiacomcudacudacprogrammingguideindexhtmlcompilingandlink access textur surfac memori classcupyrawkernel support via cuda runtim textur surfac object api see document classcupycudatexturetextureobject classcupycudatexturesurfaceobject well cuda c program guid use textur refer api mark deprec cuda toolkit see introduct classcupyrawmodul kernel reli c std librari header type_trait like encount compil error case tri enabl cupi jitifi httpsgithubcomnvidiajitify_ support set jitifytru creat classcupyrawkernel instanc provid basic c std support remedi common error note kernel return valu need pass input array output array argument note use printf cuda kernel may need synchron stream see output use cupycudastreamnullsynchron use default stream note exampl declar kernel extern c block indic c linkag use ensur kernel name mangl retriv name kernel argument python primit type numpi scalar pass kernel valu array argument pointer argument pass cupi ndarray valid perform cupi argument pass kernel includ type number argument especi note pass cupi classcupyndarray dtype match type argument declar function signatur cuda sourc code unless cast array intent exampl cupyfloat cupyuint array must pass argument type float unsign long long respect cupi directli support array nonprimit type float noth prevent cast float void float kernel python primit type int float complex bool map long long doubl cudoublecomplex bool respect numpi scalar numpygener numpi array numpyndarray size one pass kernel valu mean pass valu base numpi type numpyint numpyfloat provid kernel argument match size refer tabl match cupynumpi dtype cuda type cupynumpi type correspond kernel type items byte bool bool int char sign char int short sign short int int sign int int long long sign long long uint unsign char uint unsign short uint unsign int uint unsign long long float half float float float doubl complex float cufloatcomplex complexfloat complex doubl cudoublecomplex complexdoubl cuda standard guarante size fundament type host devic alway match items size_t ptrdiff_t intptr_t uintptr_t long sign long unsign long howev platform depend pass cuda vector builtin float user defin structur kernel argument provid match devicesid kernel paramet type see refcustom_user_struct _custom_user_struct custom user type possibl use custom type composit type structur structur structur kernel argument defin custom numpi dtype respons match host devic structur memori layout cuda standard guarante size fundament type host devic alway match may howev impos devic align requir composit type mean composit type struct member offset may differ might expect kernel argument pass valu cuda driver copi exactli sizeofparam_typ byte start begin numpi object data pointer param_typ paramet type kernel match param_typ memori layout ex size align struct paddingpack defin correspond numpi dtype httpsnumpyorgdocstablereferencearraysdtypeshtml_ builtin cuda vector type int doubl pack structur name member directli defin numpi dtype follow doctest import numpi np name x z type npfloat float npdtypenam name format type arg nprandomrandastypenpfloatviewfloat printarg doctest skip argx printarg doctest skip arg use directli kernel argument need name field may prefer syntax defin pack structur vector matric doctest import numpi np floatx npdtypenam dummi format npfloat arg nprandomrandastypenpfloatviewfloatx printargitems arg repres byte scalar ie numpi array size pass valu kernel kernel paramet pass valu dedic kb memori bank cach broadcast upper bound total kernel paramet size thu kb see link httpsdocsnvidiacomcudacudacprogrammingguideindexhtmlfunctionparameters_ may import note dedic memori bank share devic __constant__ memori space cupi offer helper routin creat user defin composit type composit type howev built recurs use numpi dtype offset items capabl see cupyexamplescustum_struct httpsgithubcomcupycupytreemasterexamplescustom_struct_ exampl advanc usag warn directli pass static array kernel argument type argn syntax n compil time constant signatur __global__ void kernelfloat arg seen __global__ void kernelfloat arg compil want pass five float kernel valu need defin custom structur struct float float val modifi kernel signatur __global__ void kernelfloat arg raw modul deal larg raw cuda sourc load exist cuda binari classcupyrawmodul class handi initi either cuda sourc code path cuda binari accept argument classcupyrawkernel need kernel retriev call methcupyrawmoduleget_funct method return classcupyrawkernel instanc invok discuss doctest loaded_from_sourc r extern c __global__ void test_sumconst float x const float x float unsign int n unsign int tid blockdimx blockidxx threadidxx tid n ytid xtid xtid __global__ void test_multiplyconst float x const float x float unsign int n unsign int tid blockdimx blockidxx threadidxx tid n ytid xtid xtid modul cprawmodulecodeloaded_from_sourc ker_sum moduleget_functiontest_sum ker_tim moduleget_functiontest_multipli n x cparangen dtypecpfloatreshapen n x cponesn n dtypecpfloat cpzerosn n dtypecpfloat ker_sumn n x x n x x assert cpallclosey x x ker_timesn n x x n x x assert cpallclosey x x instruct use complex number classcupyrawkernel also appli classcupyrawmodul cuda kernel need access global symbol constant memori methcupyrawmoduleget_glob method use see document detail cupi also support textur refer api handl textur refer modul retriev name via methcupyrawmoduleget_texref need pass classcupycudatexturetexturerefer along resourc descriptor textur descriptor bind refer array interfac classcupycudatexturetexturerefer meant mimic classcupycudatexturetextureobject help user make transit latter sinc cuda toolkit former mark deprec support c templat kernel classcupyrawmodul addit provid name_express argument list templat special provid correspond kernel gener retriev type doctest code r templatetypenam __global__ void fxt arr int n unsign int tid blockidxx blockdimx threadidxx tid n arrtid arrtid name_exp fxfloat fxdoubl mod cprawmodulecodecod optionsstdc name_expressionsname_exp ker_float modget_functionname_exp compil happen n cparangen dtypecpfloat ker_float n n array dtypefloat ker_doubl modget_functionname_exp cparangen dtypecpfloat ker_doubl n n array note name express use initi classcupyrawmodul instanc retriev kernel origin unmangl kernel name templat paramet unambigu specifi name mangl demangl handl hood user need worri _kernel_fus kernel fusion funccupyfus decor fuse function decor use defin elementwis reduct kernel easili classcupyelementwisekernel classcupyreductionkernel use decor defin squared_diff kernel follow doctest cpfuse def squared_diffx return x x kernel call either scalar numpi array cupi array like origin function doctest x_cp cparang y_cp cparang squared_diffx_cp y_cp array x_np nparang y_np nparang squared_diffx_np y_np array first function call fuse function analyz origin function base abstract inform argument eg dtype ndim creat cach actual cuda kernel second function call input type fuse function call previous cach kernel highli recommend reus decor function instead decor local function defin multipl time funccupyfus also support simpl reduct kernel doctest cpfuse def sum_of_productsx return cpsumx axi specifi kernel name use kernel_nam keyword argument follow doctest cpfusekernel_namesquared_diff def squared_diffx return x x note current funccupyfus fuse simpl elementwis reduct oper routin eg funccupymatmul funccupyreshap support _jit_kernel_definit jit kernel definit classcupyxjitrawkernel decor creat raw cuda kernel python function section python function wrap decor call target function target function consist elementari scalar oper user manag parallel cupi array oper automat parallel oper eg funccupyadd funccupysum support custom kernel base array function desir pleas refer refkernel_fus section basic usag short exampl write classcupyxjitrawkernel copi valu x use gridstrid loop doctest cupyx import jit jitrawkernel def elementwise_copyx size tid jitblockidxx jitblockdimx jitthreadidxx ntid jitgriddimx jitblockdimx rangetid size ntid yi xi size cupyuint x cupyrandomnormalsizes dtypecupyfloat cupyemptys dtypecupyfloat elementwise_copi x size rawkernel style assert x yall elementwise_copi x size numba style assert x yall two kind style launch kernel support see document classcupyxjit_interface_jitrawkernel detail compil defer first function call cupi jit compil infer type argument call time cach compil kernel speed subsequ call see docreferencekernel full list api basic design cupi jit compil gener cuda code via python ast decid use python bytecod analyz target function avoid perforam degrad cuda sourc code gener python bytecod effect optim cuda compil forloop control statement target function fulli transform jump instruct convert target function bytecod type rule type local variabl infer first assign function first assign must done toplevel function word must ifels bodi forloop limit jit work insid python interact interpret repl compil need get sourc code target function fast fourier transform cupi cupi cover full fast fourier transform fft function provid numpi modcupyfft subset scipi modcupyxscipyfft addit highlevel api use cupi provid addit featur access advanc routin cufft_ offer nvidia gpu control better perform behavior fft routin featur experiment subject chang deprec remov see doccompat may absent hipfft_rocfft_ target amd gpu _cufft httpsdocsnvidiacomcudacufftindexhtml _hipfft httpshipfftreadthedocsioenlatest _rocfft httpsrocfftreadthedocsioenlatest _scipy_fft_backend scipi fft backend sinc scipi v backend mechan provid user regist differ fft backend use scipi api perform actual transform target backend cupi modcupyxscipyfft modul onetim usag context manag funcscipyfftset_backend use codeblock python import cupi cp import cupyxscipyfft cufft import scipyfft cprandomrandomastypecpcomplex scipyfftset_backendcufft b scipyfftffta equival cufftffta howev usag tediou altern user regist backend funcscipyfftregister_backend funcscipyfftset_global_backend avoid use context manag codeblock python import cupi cp import cupyxscipyfft cufft import scipyfft scipyfftset_global_backendcufft cprandomrandomastypecpcomplex b scipyfftffta equival cufftffta note pleas refer scipi fft documentation_ inform note use backend togeth explicit plan argument requir scipi version higher see creat fft plan _scipi fft document httpsdocsscipyorgdocscipyreferenceffthtmlbackendcontrol usermanag fft plan perform reason user may wish creat reus manag fft plan cupi provid highlevel experiment api funccupyxscipyfftpackget_fft_plan need user specifi transform perform would highlevel fft api plan gener base input codeblock python import cupi cp cupyxscipyfft import get_fft_plan cprandomrandom astypecpcomplex plan get_fft_plana axe value_typecc batch cc transform return plan use either explicitli argument modcupyxscipyfft api codeblock python import cupyxscipyfft rest argument must match use gener plan cupyxscipyfftffta axe planplan context manag modcupyfft api codeblock python plan argument must match use gener plan cpfftffta axe _fft_plan_cach fft plan cach howev occas user may want manag fft plan moreov plan could also reus intern cupi routin usermanag plan would applic therefor start cupi v provid builtin plan cach enabl default plan cach done per devic per thread basi retriev funccupyfftconfigget_plan_cach api codeblock python import cupi cp cach cpfftconfigget_plan_cach cacheshow_info cufft plan cach devic cach enabl true current max size count current max memsiz unlimit byte hit miss count cach plan recent use first perform transform would gener plan cach cprandomrandom cpfftfftna axe cacheshow_info hit cufft plan cach devic cach enabl true current max size count current max memsiz unlimit byte hit miss count cach plan recent use first key c none plan type plannd memori usag perform transform plan look cach reus cpfftfftna axe cacheshow_info hit cufft plan cach devic cach enabl true current max size count current max memsiz unlimit byte hit miss count cach plan recent use first key c none plan type plannd memori usag clear cach cacheclear cpfftconfigshow_plan_cache_info cacheshow_info devic cufft plan cach info devic cufft plan cach devic cach enabl true current max size count current max memsiz unlimit byte hit miss count cach plan recent use first return classcupyfft_cacheplancach object method finer control set cach size either count memori usag size set cach disabl pleas refer document detail note shown fft plan associ work area alloc outofmemori error happen one may want inspect clear limit plan cach note plan return funccupyxscipyfftpackget_fft_plan cach fft callback cufft_ provid fft callback merg pre andor post process kernel fft routin reduc access global memori capabl support experiment cupi user need suppli custom load andor store kernel string set context manag via funccupyfftconfigset_cufft_callback note load store kernel pointer name d_loadcallbackptr d_storecallbackptr codeblock python import cupi cp load callback overwrit input array code r __device__ cufftcomplex cb_convertinputc void datain size_t offset void callerinfo void sharedptr cufftcomplex x xx xy return x __device__ cufftcallbackloadc d_loadcallbackptr cb_convertinputc cprandomrandom astypecpcomplex fftn call use callback cpfftconfigset_cufft_callbackscb_loadcod b cpfftfftna axe use c cpfftfftncponesshapeashap dtypecpcomplex axe result agre assert cpallcloseb c static plan also cach distinct nocallback counterpart cpfftconfigget_plan_cacheshow_info note intern featur requir recompil python modul distinct pair load store kernel therefor first invoc slow cost amort callback reus subsequ calcul compil modul cach disk default posit homecupycallback_cach chang environ variabl cupy_cache_dir multigpu fft cupi current provid two kind experiment support multigpu fft warn use multipl gpu perform fft guarante perform rule thumb transform fit gpu avoid use multipl first kind support highlevel funccupyfftfft funccupyfftifft api requir input array resid one particip gpu multigpu calcul done hood end calcul result resid devic start current complextocomplex cc transform support complextor cr realtocomplex rc transform funccupyfftrfft friend transform either batch batch size batch size codeblock python import cupi cp cpfftconfiguse_multi_gpu true cpfftconfigset_cufft_gpu use gpu shape batch size dtype cpcomplex cprandomrandomshapeastypedtyp resid gpu b cpfftffta comput gpu resid gpu need perform dd transform ex funccupyfftfftn instead ex funccupyfftfft would like still work particular use case loop transform axe hood exactli done numpi could lead suboptim perform second kind usag use lowlevel privat cupi api need construct classcupycudacufftpland object use program cc cufft_ use approach input array resid host classnumpyndarray size much larger singl gpu accommod one main reason run multigpu fft codeblock python import numpi np import cupi cp need touch cpfftconfig use lowlevel api shape dtype npcomplex nprandomrandomshapeastypedtyp resid cpu lenshap batch nx shape elif lenshap batch shape nx shape comput via cufft cufft_typ cpcudacufftcufft_cc singleprecis cc plan cpcudacufftplandnx cufft_typ batch devic out_cp npempty_likea output cpu planffta out_cp cufftcufft_forward out_np numpyfftffta use numpi fft npfftfft alway return npcomplex dtype numpycomplex out_np out_npastypedtyp check result assert npallcloseout_cp out_np rtole atol use case pleas consult cufft_ document multigpu transform detail note multigpu plan cach autogener via highlevel api manual gener via lowlevel api halfprecis fft cufft_ provid cufftxtmakeplanmani cufftxtexec routin support wide rang fft need includ bit index halfprecis fft cupi provid experiment support capabl via new though privat classcupycudacufftxtplannd api halfprecis fft support hardwar twice fast singleprecis counterpart numpi yet provid necessari infrastructur halfprecis complex number ie numpycomplex though step featur current bit involv common case codeblock python import cupi cp import numpi np shape input array shape idtyp odtyp edtyp e numpycomplex futur store inputoutput array fp array twice long complex yet avail cprandomrandomshap shape shapeastypecpfloat cpempty_likea fft cufft plan cpcudacufftxtplanndshap shape shapeshap idtyp shape shapeshap odtyp shape edtyp orderc last_axi last_sizenon planffta cpcudacufftcufft_forward fft numpi a_np cpasnumpyaastypenpfloat upcast a_np a_npviewnpcomplex out_np npfftfftna_np axe out_np npascontiguousarrayout_npastypenpcomplex downcast out_np out_npviewnpfloat out_np out_npastypenpfloat dont worri accruaci probabl lost lot cast printok cpmeancpabsout cpasarrayout_np els ok bit index support highlevel fft api plan futur cupi releas user guid user guid provid overview cupi explain import featur detail found refcupi api refer cupy_refer toctre maxdepth basic kernel cuda_api fft memori perform interoper differ compat access cuda function _cuda_stream_ev stream event section discuss basic usag cuda stream event api refer pleas see refstream_event_api role cuda program model pleas refer cuda program guide_ cupi provid highlevel python api classcupycudastream classcupycudaev creat stream event respect data copi kernel launch enqueu onto refcurrent_stream queri via funccupycudaget_current_stream chang either set context manag doctest import numpi np a_np nparang cpcudastream a_cp cpasarraya_np hd transfer stream b_cp cpsuma_cp kernel launch stream assert cpcudaget_current_stream fall back previou stream use default stream go scope use methcupycudastreamus method doctest cpcudastream suse subsequ oper done steam doctest ellipsi stream devic b_np cpasnumpyb_cp assert cpcudaget_current_stream cpcudastreamnullus fall back default null stream stream devic assert cpcudastreamnul cpcudaget_current_stream event creat either manual methcupycudastreamrecord method classcupycudaev object use time gpu activ via funccupycudaget_elapsed_tim set interstream depend doctest e cpcudaev erecord a_cp b_cp a_cp e cpcudaget_current_streamrecord set stream order cpcudastream swait_event a_cp guarante updat copi start a_np cpasnumpya_cp time esynchron cpcudaget_elapsed_time e includ comput time copi time like classcupycudadevic object classcupycudastream classcupycudaev object also use synchron note cupi classcupycudastream object manag per thread per devic basi note nvidia gpu two stream singleton object objcupycudastreamnul objcupycudastreamptd refer legaci default stream perthread default stream respect cupi use former default userdefin stream use chang behavior set environ variabl cupy_cuda_per_thread_default_stream see refenviron applic amd gpu _cuda program guid httpsdocsnvidiacomcudacudacprogrammingguideindexhtml interoper stream creat python librari cupi provid classcupycudaexternalstream api wrap exist stream pointer given python int case stream lifetim manag cupi addit need make sure classcupycudaexternalstream object use devic stream creat either manual explicitli set option device_id argument creat classcupycudaexternalstream object otherwis use like classcupycudastream object cuda driver runtim api construct pleas see refruntime_api api refer api compat polici document express design polici compat cupi api develop team obey polici decid add extend chang api behavior document written user develop user decid level depend cupi implement code base document develop read document creat pull request contain chang interfac note document may contain ambigu level support compat version backward compat updat cupi classifi three level major minor revis type distinct level backward compat major updat contain disrupt chang break backward compat minor updat contain addit extens api keep backward compat support revis updat contain improv api implement without chang api specif note support full backward compat almost infeas pythonbas api sinc way complet hide implement detail process break backward compat deprec drop prepar api may deprec minor updat case deprec note ad api document api implement chang fire deprec warn possibl anoth way reimplement function previous written use deprec api api may mark drop futur case drop state document major version number api plan drop api implement chang fire futur warn possibl actual drop done follow step make api deprec point user use deprec api new applic code mark api drop futur must done minor updat differ deprec major version announc updat drop api consequ take least two minor version drop api first deprec api chang prepar api may mark chang futur chang without backward compat case chang state document version number api plan chang api implement chang fire futur warn certain usag actual chang done follow step announc api chang futur point actual version chang need accur announc mark api chang futur version number plan chang point user use mark api new applic code major updat announc updat chang api support backward compat section defin backward compat minor updat must maintain document interfac cupi offici api document mani applic written base document featur support backward compat document featur word code base document featur run correctli minorrevis updat version develop encourag use appar name object implement detail exampl attribut outsid document api one underscor prefix name _undocumented_behavior undocu behavior behavior cupi implement state document undefin undocu behavior guarante stabl differ minorrevis version minor updat may contain chang undocu behavior exampl suppos api x ad minor updat previou version attempt use x caus attributeerror behavior state document undefin thu ad api x minor version permiss revis updat may also contain chang undefin behavior typic exampl bug fix anoth exampl improv implement may chang intern object structur shown document consequ even revis updat support compat pickl unless full layout pickl object clearli document document error compat basic determin base document though sometim contain error may make api confus assum document alway stronger implement therefor may fix document error updat may break compat regard document note develop must fix document implement function time revis updat bug fix chang complet break backward compat want fix bug side first fix document fit implement start api chang procedur describ object attribut properti object attribut properti sometim replac minor updat break user code except code depend attribut properti implement function method method may replac callabl attribut keep compat paramet return valu minor updat break user code except code depend method callabl attribut implement except warn specif rais except consid part standard backward compat except rais futur version correct usag document allow unless api chang process complet hand warn may ad minor updat api mean minor updat keep backward compat warn instal compat instal process anoth concern compat support environment compat follow way chang depend librari forc modif exist environ must done major updat chang includ follow case drop support version depend librari eg drop cudnn v ad new mandatori depend eg ad hpi setup_requir support option packageslibrari may done minor updat eg support hpi option featur note instal compat guarante featur cupi correctli run support environ may contain bug occur certain environ bug fix updat basic cupi currentmodul cupi section learn follow thing basic classcupyndarray concept current devic hostdevic devicedevic array transfer basic cupyndarray cupi gpu array backend implement subset numpi interfac follow code cp abbrevi cupi follow standard convent abbrevi numpi np doctest import numpi np import cupi cp classcupyndarray class core cupi replac class numpi classnumpyndarray doctest x_gpu cparray x_gpu instanc classcupyndarray one see cupi syntax ident numpi main differ classcupyndarray classnumpyndarray cupi array alloc current devic talk later array manipul also done way similar numpi take euclidean norm aka l norm exampl numpi funcnumpylinalgnorm function calcul cpu doctest x_cpu nparray l_cpu nplinalgnormx_cpu use cupi perform calcul gpu similar way doctest x_gpu cparray l_gpu cplinalgnormx_gpu cupi implement mani function classcupyndarray object see refrefer cupy_refer support subset numpi api knowledg numpi help util cupi featur therefor recommend familiar numpi document httpsnumpyorgdocstableindexhtml_ current devic cupi concept current devic default gpu devic alloc manipul calcul etc array take place suppos id current devic case follow code would creat array x_on_gpu gpu doctest x_on_gpu cparray switch anoth gpu devic use classcupycudadevic context manag doctest cpcudadevic x_on_gpu cparray x_on_gpu cparray cupi oper except multigpu featur devicetodevic copi perform current activ devic gener cupi function expect array devic current one pass array store noncurr devic may work depend hardwar configur gener discourag may perform note array devic current devic mismatch cupi function tri establish peertop memori access httpsdocsnvidiacomcudacudacprogrammingguideindexhtmlpeertopeermemoryaccess_ pp current devic directli read array anoth devic note pp avail topolog permit pp unavail attempt fail valueerror cupyndarraydevic attribut indic devic array alloc doctest cpcudadevic x cparray xdevic cuda devic note one devic avail explicit devic switch need _current_stream current stream associ concept current devic current stream help avoid explicitli pass stream everi singl oper keep api python userfriendli cupi cuda oper data transfer see refdatatransferbas section kernel launch enqueu onto current stream queu task stream execut serial asynchron respect host default current stream cupi cuda null stream ie stream also known legaci default stream uniqu per devic howev possibl chang current stream use classcupycudastream api pleas see doccuda_api exampl current stream cupi retriev use funccupycudaget_current_stream worth note cupi current stream manag per thread per devic basi mean differ python thread differ devic current stream null stream differ _datatransferbas data transfer move array devic funccupyasarray use move classnumpyndarray list object pass funcnumpyarray current devic doctest x_cpu nparray x_gpu cpasarrayx_cpu move data current devic funccupyasarray accept classcupyndarray mean transfer array devic function doctest cpcudadevic x_gpu_ cpndarray creat array gpu cpcudadevic x_gpu_ cpasarrayx_gpu_ move array gpu note funccupyasarray copi input array possibl put array current devic return input object copi array situat use funccupyarray copytru actual funccupyasarray equival cupyarrayarr dtype copyfals move array devic host move devic array host done funccupyasnumpi follow doctest x_gpu cparray creat array current devic x_cpu cpasnumpyx_gpu move array host also use methcupyndarrayget doctest x_cpu x_gpuget memori manag check docmemori detail descript memori manag cupi use memori pool write cpugpu agnost code cupi compat numpi make possibl write cpugpu agnost code purpos cupi implement funccupyget_array_modul function return refer modcupi argument resid gpu modnumpi otherwis exampl cpugpu agnost function comput logp doctest stabl implement log expx def softplusx xp cpget_array_modulex xp standard usag commun printus xp__name__ return xpmaximum x xplogpxpexpabsx need manipul cpu gpu array explicit data transfer may requir move locat either cpu gpu purpos cupi implement two sister method call funccupyasnumpi funccupyasarray exampl demonstr use method doctest x_cpu nparray y_cpu nparray x_cpu y_cpu array x_gpu cpasarrayx_cpu x_gpu y_cpu traceback recent call last typeerror unsupport type class numpyndarray cpasnumpyx_gpu y_cpu array cpasnumpyx_gpu cpasnumpyy_cpu array x_gpu cpasarrayy_cpu array cpasarrayx_gpu cpasarrayy_cpu array funccupyasnumpi method return numpi array array host wherea funccupyasarray method return cupi array array current devic method accept arbitrari input mean appli data locat either host devic convert array miscellan routin hint numpi api refer miscellan routin httpsnumpyorgdocstablereferenceroutinesotherhtml_ currentmodul cupi memori rang autosummari toctre gener shares_memori may_share_memori util autosummari toctre gener show_config matlablik function autosummari toctre gener binari oper hint numpi api refer binari oper httpsnumpyorgdocstablereferenceroutinesbitwisehtml_ currentmodul cupi elementwis bit oper autosummari toctre gener bitwise_and bitwise_or bitwise_xor invert left_shift right_shift bit pack autosummari toctre gener packbit unpackbit output format autosummari toctre gener binary_repr index routin hint numpi api refer index routin httpsnumpyorgdocstablereferenceroutinesindexinghtml_ currentmodul cupi gener index array autosummari toctre gener c_ r_ nonzero indic mask_indic tril_indic tril_indices_from triu_indic triu_indices_from ix_ ravel_multi_index unravel_index diag_indic diag_indices_from indexinglik oper autosummari toctre gener take take_along_axi choos compress diag diagon select libstride_tricksas_strid insert data array autosummari toctre gener place put putmask fill_diagon iter array autosummari toctre gener flatit orphan dlpack helper autosummari toctre gener cupyfromdlpack time rang autosummari toctre gener cupyproftimerangedecor cupyproftime_rang time helper autosummari toctre gener cupyxtimerepeat devic synchron detect warn api deprec cupi v remov futur releas autosummari toctre gener cupyxallow_synchron cupyxdevicesynchron window function hint numpi api refer window function httpsnumpyorgdocstablereferenceroutineswindowhtml_ currentmodul cupi variou window autosummari toctre gener bartlett blackman ham han kaiser modul cupyxscipyndimag multidimension imag process modcupyxscipyndimag hint scipi api refer multidimension imag process scipyndimag httpsdocsscipyorgdocscipyreferencendimagehtml_ filter autosummari toctre gener convolv convolv correl correl gaussian_filt gaussian_filterd gaussian_gradient_magnitud gaussian_laplac generic_filt generic_filterd generic_gradient_magnitud generic_laplac laplac maximum_filt maximum_filterd median_filt minimum_filt minimum_filterd percentile_filt prewitt rank_filt sobel uniform_filt uniform_filterd fourier filter autosummari toctre gener fourier_ellipsoid fourier_gaussian fourier_shift fourier_uniform interpol autosummari toctre gener affine_transform map_coordin rotat shift spline_filt spline_filterd zoom measur autosummari toctre gener center_of_mass extrema histogram label labeled_comprehens maximum maximum_posit mean median minimum minimum_posit standard_devi sum_label varianc morpholog autosummari toctre gener binary_clos binary_dil binary_eros binary_fill_hol binary_hit_or_miss binary_open binary_propag black_tophat generate_binary_structur grey_clos grey_dil grey_eros grey_open iterate_structur morphological_gradi morphological_laplac white_tophat opencv mode modcupyxscipyndimag support addit mode opencv given function perform like cvwarpaffin httpsdocsopencvorgmasterdadgroup__imgproc__transformhtmlgadeefcdddbcaea_ cvresiz httpsdocsopencvorgmasterdadgroup__imgproc__transformhtmlgaaeffedced_ exampl code python import cupyxscipyndimag import cupi cp import cv im cvimreadtodo pl fill imag path trans_mat cpey trans_mat trans_mat smaller_shap imshap imshap smaller cpzerossmaller_shap prealloc memori resiz imag cupyxscipyndimageaffine_transformim trans_mat output_shapesmaller_shap outputsmal modeopencv cvimwritesmallerjpg cpasnumpysmal smaller imag save local logic function hint numpi api refer logic function httpsnumpyorgdocstablereferenceroutineslogichtml_ currentmodul cupi truth valu test autosummari toctre gener uniond array content autosummari toctre gener isfinit isinf isnan array type test autosummari toctre gener iscomplex iscomplexobj isfortran isreal isrealobj isscalar logic oper autosummari toctre gener logical_and logical_or logical_not logical_xor comparison autosummari toctre gener allclos isclos array_equ array_equiv greater greater_equ less less_equ equal not_equ function program hint numpi api refer function program httpsnumpyorgdocstablereferenceroutinesfunctionalhtml_ currentmodul cupi note classcupyvector appli jit compil given python function see refjit_kernel_definit detail autosummari toctre gener apply_along_axi vector piecewis custom kernel autosummari toctre gener cupyelementwisekernel cupyreductionkernel cupyrawkernel cupyrawmodul cupyfus jit kernel definit autosummari toctre gener cupyxjitrawkernel cupyxjitthreadidx cupyxjitblockdim cupyxjitblockidx cupyxjitgriddim cupyxjitgrid cupyxjitgrids cupyxjitlaneid cupyxjitwarps cupyxjitsyncthread cupyxjitsyncwarp cupyxjitshfl_sync cupyxjitshfl_up_sync cupyxjitshfl_down_sync cupyxjitshfl_xor_sync cupyxjitshared_memori cupyxjitatomic_add cupyxjitatomic_sub cupyxjitatomic_exch cupyxjitatomic_min cupyxjitatomic_max cupyxjitatomic_inc cupyxjitatomic_dec cupyxjitatomic_ca cupyxjitatomic_and cupyxjitatomic_or cupyxjitatomic_xor cupyxjit_interface_jitrawkernel kernel binari memoiz autosummari toctre gener cupymemo cupyclear_memo array api function section full list implement api detail document see array api specif httpsdataapisorgarrayapilatestapi_specificationindexhtml_ automodul cupyarray_api member orphan document move docscipy_spars comparison tabl list numpi scipi api correspond cupi implement cupi column denot cupi implement provid yet welcom contribut function includ comparison_tablerstinc modul cupyrandom random sampl modcupyrandom differ modcupyrandom modnumpyrandom function modcupyrandom support dtype option exist correspond numpi api option enabl gener float valu directli without space overhead funccupyrandomdefault_rng use xorwow bit gener default random state serial see descript detail cupi guarante number gener use across major version mean number gener modcupyrandom new major version may previou one even seed distribut use currentmodul cupyrandom new random gener api hint numpi api refer random sampl numpyrandom httpsnumpyorgdocstablereferencerandom_ random gener hint numpi api refer random gener httpsnumpyorgdocstablereferencerandomgeneratorhtml_ autosummari toctre gener default_rng gener bit gener hint numpi api refer bit gener httpsnumpyorgdocstablereferencerandombit_generatorsindexhtml_ autosummari toctre gener bitgener cupi provid follow bit gener implement autosummari toctre gener xorwow mrgka philoxx legaci random gener hint numpi api refer legaci random gener httpsnumpyorgdocstablereferencerandomlegacyhtml_ numpi refer httpsnumpyorgdocreferenceroutinesrandomhtml_ autosummari toctre gener randomst function modcupyrandom autosummari toctre gener beta binomi byte chisquar choic dirichlet exponenti f gamma geometr gumbel hypergeometr laplac logist lognorm logseri multinomi multivariate_norm negative_binomi noncentral_chisquar noncentral_f normal pareto permut poisson power rand randint randn random random_integ random_sampl ranf rayleigh sampl seed shuffl standard_cauchi standard_exponenti standard_gamma standard_norm standard_t triangular uniform vonmis wald weibul zipf cupi provid cupyrandomget_st cupyrandomset_st time use follow cupyspecif api instead note function use classcupyrandomrandomst instanc repres intern state serial autosummari toctre gener get_random_st set_random_st modul cupyxscipystat statist function modcupyxscipystat hint scipi api refer statist function scipystat httpsdocsscipyorgdocscipyreferencestatshtml_ summari statist autosummari toctre gener trim_mean entropi routin numpi follow page describ numpycompat routin function cover subset numpi routin httpsnumpyorgdocstablereferenceroutineshtml_ toctre maxdepth creation manipul binari dtype fft function index io linalg logic math misc pad polynomi random set sort statist test window modul cupyxscipyfft discret fourier transform modcupyxscipyfft hint scipi api refer discret fourier transform scipyfft httpsdocsscipyorgdocscipyreferenceffthtml_ seealso docuser_guidefft fast fourier transform fft autosummari toctre gener fft ifft fft ifft fftn ifftn rfft irfft rfft irfft rfftn irfftn hfft ihfft hfft ihfft hfftn ihfftn discret cosin sine transform dst dct autosummari toctre gener dct idct dctn idctn dst idst dstn idstn helper function autosummari toctre gener fftshift ifftshift fftfreq rfftfreq next_fast_len code compat featur fft modul cupi fft function modul take advantag exist cufft plan return funccupyxscipyfftpackget_fft_plan acceler comput plan either pass explicitli via keywordonli plan argument use context manag one except dct dst transform current support plan argument boolean switch cupyfftconfigenable_nd_plan also affect fft function modul see docfft switch neglect plan manual use funccupyxscipyfftpackget_fft_plan like scipyfft fft function modul option argument overwrite_x default fals semant scipyfft set true input array x overwritten arbitrarili reason inplac fft desir user alway reassign input follow manner x cupyxscipyfftpackfftx overwrite_xtru cupyxscipyfft modul also use backend scipyfft eg instal scipyfftset_backendcupyxscipyfft allow scipyfft work numpi cupi array inform see refscipy_fft_backend boolean switch datacupyfftconfiguse_multi_gpu also affect fft function modul see docfft moreov switch honor plan manual use funccupyxscipyfftpackget_fft_plan type ii iii dct dst transform implement type iv transform current unavail cupyspecif function cupyspecif function place cupyx namespac todokmaehashi use modul cupyx autosummari toctre gener cupyxrsqrt cupyxscatter_add cupyxscatter_max cupyxscatter_min cupyxempty_pin cupyxempty_like_pin cupyxzeros_pin cupyxzeros_like_pin profil util autosummari toctre gener cupyxprofilerbenchmark cupyxprofilertime_rang cupyxprofilerprofil dlpack util helper function creat classcupyndarray either dlpack tensor object support dlpack data exchang protocol detail see refdlpack autosummari toctre gener cupyfrom_dlpack _kernel_param_opt automat kernel paramet optim modcupyxoptim modul cupyxoptim autosummari toctre gener cupyxoptimizingoptim lowlevel cuda support _device_manag devic manag autosummari toctre gener cupycudadevic memori manag autosummari toctre gener cupyget_default_memory_pool cupyget_default_pinned_memory_pool cupycudamemori cupycudamemoryasync cupycudamanagedmemori cupycudaunownedmemori cupycudapinnedmemori cupycudamemorypoint cupycudapinnedmemorypoint cupycudamalloc_manag cupycudamalloc_async cupycudaalloc cupycudaalloc_pinned_memori cupycudaget_alloc cupycudaset_alloc cupycudausing_alloc cupycudaset_pinned_memory_alloc cupycudamemorypool cupycudamemoryasyncpool cupycudapinnedmemorypool cupycudapythonfunctionalloc cupycudacfunctionalloc memori hook autosummari toctre gener cupycudamemoryhook cupycudamemory_hooksdebugprinthook cupycudamemory_hookslineprofilehook _stream_event_api stream event autosummari toctre gener cupycudastream cupycudaexternalstream cupycudaget_current_stream cupycudaev cupycudaget_elapsed_tim _graph_api graph autosummari toctre gener cupycudagraph textur surfac memori autosummari toctre gener cupycudatexturechannelformatdescriptor cupycudatexturecudaarray cupycudatextureresourcedescriptor cupycudatexturetexturedescriptor cupycudatexturetextureobject cupycudatexturesurfaceobject cupycudatexturetexturerefer profil autosummari toctre gener cupycudaprofil cupycudaprofileriniti cupycudaprofilerstart cupycudaprofilerstop cupycudanvtxmark cupycudanvtxmarkc cupycudanvtxrangepush cupycudanvtxrangepushc cupycudanvtxrangepop nccl autosummari toctre gener cupycudancclncclcommun cupycudancclget_build_vers cupycudancclget_vers cupycudancclget_unique_id cupycudancclgroupstart cupycudancclgroupend _runtime_api runtim api cupi wrap cuda runtim api provid nativ cuda oper pleas check cuda runtim api document httpsdocsnvidiacomcudacudaruntimeapiindexhtml_ use function autosummari toctre gener cupycudaruntimedrivergetvers cupycudaruntimeruntimegetvers cupycudaruntimegetdevic cupycudaruntimegetdeviceproperti cupycudaruntimedevicegetattribut cupycudaruntimedevicegetbypcibusid cupycudaruntimedevicegetpcibusid cupycudaruntimedevicegetdefaultmempool cupycudaruntimedevicegetmempool cupycudaruntimedevicesetmempool cupycudaruntimemempooltrimto cupycudaruntimegetdevicecount cupycudaruntimesetdevic cupycudaruntimedevicesynchron cupycudaruntimedevicecanaccessp cupycudaruntimedeviceenablepeeraccess cupycudaruntimedevicegetlimit cupycudaruntimedevicesetlimit cupycudaruntimemalloc cupycudaruntimemallocmanag cupycudaruntimemallocdarray cupycudaruntimemallocarray cupycudaruntimemallocasync cupycudaruntimehostalloc cupycudaruntimehostregist cupycudaruntimehostunregist cupycudaruntimefre cupycudaruntimefreehost cupycudaruntimefreearray cupycudaruntimefreeasync cupycudaruntimememgetinfo cupycudaruntimememcpi cupycudaruntimememcpyasync cupycudaruntimememcpyp cupycudaruntimememcpypeerasync cupycudaruntimememcpyd cupycudaruntimememcpydasync cupycudaruntimememcpydfromarray cupycudaruntimememcpydfromarrayasync cupycudaruntimememcpydtoarray cupycudaruntimememcpydtoarrayasync cupycudaruntimememcpyd cupycudaruntimememcpydasync cupycudaruntimememset cupycudaruntimememsetasync cupycudaruntimememprefetchasync cupycudaruntimememadvis cupycudaruntimepointergetattribut cupycudaruntimestreamcr cupycudaruntimestreamcreatewithflag cupycudaruntimestreamdestroy cupycudaruntimestreamsynchron cupycudaruntimestreamaddcallback cupycudaruntimestreamqueri cupycudaruntimestreamwaitev cupycudaruntimelaunchhostfunc cupycudaruntimeeventcr cupycudaruntimeeventcreatewithflag cupycudaruntimeeventdestroy cupycudaruntimeeventelapsedtim cupycudaruntimeeventqueri cupycudaruntimeeventrecord cupycudaruntimeeventsynchron cupycudaruntimeipcgetmemhandl cupycudaruntimeipcopenmemhandl cupycudaruntimeipcclosememhandl cupycudaruntimeipcgeteventhandl cupycudaruntimeipcopeneventhandl modul cupyxscipysign signal process modcupyxscipysign hint scipi api refer signal process scipysign httpsdocsscipyorgdocscipyreferencesignalhtml_ convolut autosummari toctre gener convolv correl fftconvolv oaconvolv convolv correl sepfird choose_conv_method filter autosummari toctre gener order_filt medfilt medfiltd wiener _environ environ variabl runtim environ variabl cupi use runtim envvar cuda_path path directori contain cuda parent directori contain nvcc use default nvcc found usrlocalcuda use see refinstall_cuda detail envvar cupy_cache_dir default homecupykernel_cach path directori store kernel cach see docuser_guideperform detail envvar cupy_cache_save_cuda_sourc default set cuda sourc file save along compil binari cach directori debug purpos note sourc file save compil binari alreadi store cach envvar cupy_cache_in_memori default set envvarcupy_cache_dir envvarcupy_cache_save_cuda_sourc ignor cach memori environ variabl allow reduc disk io igno nvcc set compil backend envvar cupy_dump_cuda_source_on_error default set cuda kernel compil fail cupi dump cuda kernel code standard error envvar cupy_cuda_compile_with_debug default set cuda kernel compil debug inform devicedebug generatelineinfo envvar cupy_gpu_memory_limit default unlimit amount memori alloc devic valu specifi absolut byte fraction eg total memori gpu see docuser_guidememori detail envvar cupy_se set seed random number gener envvar cupy_experimental_slice_copi default set follow syntax enabl cupy_ndarray numpy_ndarray envvar cupy_acceler default acceler commasepar string backend name cub cutensor indic acceler backend use cupi oper prioriti acceler disabl default envvar cupy_tf default set allow cuda librari use tensor core tf comput bit float point comput envvar cupy_cuda_array_interface_sync default control cupi behavior consum set stream synchron perform devic array provid extern librari implement cuda array interfac consum cupi detail see synchronization_ requir cuda array interfac v document envvar cupy_cuda_array_interface_export_vers default control cupi behavior produc set cupi stream data oper export thu consum anoth librari perform stream synchron detail see synchronization_ requir cuda array interfac v document envvar cupy_dlpack_export_vers default control cupi dlpack support current set valu smaller would disguis manag memori normal devic memori enabl data exchang librari updat dlpack support wherea start cuda manag memori correctli recogn valid devic type envvar nvcc default nvcc defin compil use compil cuda sourc note cupi kernel built nvrtc environ variabl effect classcupyrawkernelclasscupyrawmodul nvcc backend use cub acceler envvar cupy_cuda_per_thread_default_stream default set cupi use cuda perthread default stream effect caus host thread automat execut stream unless cuda default null stream usercr stream specifi set default cuda default null stream use unless perthread default stream ptd usercr stream specifi envvar cupy_compile_with_ptx default default cupi directli compil kernel sass cubin support cuda enhanc compat httpsdocsnvidiacomdeploycudacompatibility_ set cupi instead compil kernel ptx let cuda driver assembl sass ptx option effect cuda later cupi alway compil ptx earlier cuda version also option appli nvrtc select compil backend nvcc backend alway compil sass cubin cuda toolkit environ variabl addit environ variabl list cuda program cuda environ variabl list cuda toolkit documentation_ also honor note envvarcupy_acceler envvarnvcc environ variabl set g later requir runtim host compil pleas refer refinstall_cupy_from_sourc detail instal g _cuda toolkit document httpsdocsnvidiacomcudacudacprogrammingguideindexhtmlenvvar _synchron httpsnumbareadthedocsioenlatestcudacuda_array_interfacehtmlsynchron instal environ variabl use instal build cupi sourc envvar cutensor_path path cutensor root directori contain lib includ directori experiment envvar cupy_install_use_hip default set cupi built amd rocm platform experiment build rocm support see refinstall_hip detail envvar cupy_use_cuda_python default set cupi built use cuda python httpsgithubcomnvidiacudapython_ envvar cupy_nvcc_generate_cod build cupi particular cuda architectur exampl cupy_nvcc_generate_codearchcompute_codesm_ specifi multipl arch concaten arch string semicolon current specifi automat detect current instal gpu architectur build time set default support architectur envvar cupy_num_build_job default enabl disabl parallel build set number process use build extens parallel envvar cupy_num_nvcc_thread default enabl disabl nvcc parallel compil set number thread use compil file use nvcc addit environ variabl envvarcuda_path envvarnvcc also respect build time ndimension array classndarray cupyndarray classcupyndarray cupi counterpart numpi classnumpyndarray provid intuit interfac fixeds multidimension array resid cuda devic basic concept ndarray pleas refer numpi document httpsnumpyorgdocstablereferencearraysndarrayhtml_ todokmaehashi use currentmodul cupi autosummari toctre gener cupyndarray convers tofrom numpi array classcupyndarray classnumpyndarray implicitli convert mean numpi function take classcupyndarray input vice versa convert classnumpyndarray classcupyndarray use funccupyarray funccupyasarray convert classcupyndarray classnumpyndarray use funccupyasnumpi methcupyndarrayget note convert classcupyndarray classnumpyndarray incur data transfer host cpu devic gpu devic costli term perform todokmaehashi use currentmodul cupi autosummari toctre gener cupyarray cupyasarray cupyasnumpi code compat featur classcupyndarray design interchang classnumpyndarray term code compat much possibl occasion need know whether array your handl classcupyndarray classnumpyndarray one exampl invok modulelevel function funccupysum funcnumpysum situat funccupyget_array_modul use autosummari toctre gener cupyget_array_modul autosummari toctre gener cupyxscipyget_array_modul modul cupyxscipyspeci special function modcupyxscipyspeci hint scipi api refer special function scipyspeci httpsdocsscipyorgdocscipyreferencespecialhtml_ bessel function autosummari toctre gener j j yn raw statist function seealso modcupyxscipystat autosummari toctre gener ndtr logit expit log_expit inform theori function autosummari toctre gener entr rel_entr kl_div huber pseudo_hub gamma relat function autosummari toctre gener gamma gammaln gammainc gammaincinv gammaincc gammainccinv psi polygamma digamma poch error function fresnel integr autosummari toctre gener erf erfc erfcx erfinv erfcinv legendr function autosummari toctre gener lpmv sph_harm special function autosummari toctre gener zeta conveni function autosummari toctre gener cbrt exp exp radian cosdg sindg tandg cotdg logp expm round xlogi xlogpi sinc polynomi hint numpi api refer polynomi httpsnumpyorgdocstablereferenceroutinespolynomialshtml_ power seri modcupypolynomialpolynomi hint numpi api refer power seri numpypolynomialpolynomi httpsnumpyorgdocstablereferenceroutinespolynomialspolynomialhtml_ misc function modul cupypolynomialpolynomi autosummari toctre gener polyvand polycompanion polyutil hint numpi api refer polyutil httpsnumpyorgdocstablereferenceroutinespolynomialspolyutilshtml_ function modul cupypolynomialpolyutil autosummari toctre gener as_seri trimseq trimcoef polyd hint numpi api refer polyd httpsnumpyorgdocstablereferenceroutinespolynomialspolydhtml_ currentmodul cupi basic autosummari toctre gener polyd polyv root fit autosummari toctre gener polyfit arithmet autosummari toctre gener polyadd polysub polymul modul cupylinalg linear algebra modcupylinalg hint numpi api refer linear algebra numpylinalg httpsnumpyorgdocstablereferenceroutineslinalghtml_ seealso docscipy_linalg currentmodul cupi matrix vector product autosummari toctre gener dot vdot inner outer matmul tensordot einsum linalgmatrix_pow kron decomposit autosummari toctre gener linalgcholeski linalgqr linalgsvd matrix eigenvalu autosummari toctre gener linalgeigh linalgeigvalsh norm number autosummari toctre gener linalgnorm linalgdet linalgmatrix_rank linalgslogdet trace solv equat invert matric autosummari toctre gener linalgsolv linalgtensorsolv linalglstsq linalginv linalgpinv linalgtensorinv orphan cufft plan cach autoclass cupyfft_cacheplancach member undocmemb modul cupyxscipylinalg linear algebra modcupyxscipylinalg hint scipi api refer linear algebra scipylinalg httpsdocsscipyorgdocscipyreferencelinalghtml_ basic autosummari toctre gener solve_triangular tril triu decomposit autosummari toctre gener lu lu_factor lu_solv special matric autosummari toctre gener block_diag circul companion convolution_matrix dft fiedler fiedler_companion hadamard hankel helmert hilbert kron lesli toeplitz tri distribut follow page describ api use easili perform commun differ process cupi modul cupyxdistribut autosummari toctre gener init_process_group ncclbackend modul cupyxscipysparsecsgraph compress spars graph routin modcupyxscipysparsecsgraph note csgraph modul use pylibcugraph backend need instal pylibcugraph packag httpsanacondaorgrapidsaipylibcugraph rapidsai conda channel use featur list page note current csgraph modul support amd rocm platform hint scipi api refer compress spars graph routin scipysparsecsgraph httpsdocsscipyorgdocscipyreferencesparsecsgraphhtml_ content autosummari toctre gener connected_compon statist hint numpi api refer statist httpsnumpyorgdocstablereferenceroutinesstatisticshtml_ currentmodul cupi order statist autosummari toctre gener amin amax nanmin nanmax ptp percentil quantil averag varianc autosummari toctre gener median averag mean std var nanmedian nanmean nanstd nanvar correl autosummari toctre gener corrcoef correl cov histogram autosummari toctre gener histogram histogramd histogramdd bincount digit set routin hint numpi api refer set routin httpsnumpyorgdocstablereferenceroutinessethtml_ currentmodul cupi make proper set autosummari toctre gener uniqu boolean oper autosummari toctre gener ind isin pad array hint numpi api refer pad array httpsnumpyorgdocstablereferenceroutinespaddinghtml_ currentmodul cupi autosummari toctre gener pad univers function classcupyufunc hint numpi api refer univers function numpyufunc httpsnumpyorgdocstablereferenceufuncshtml_ currentmodul cupi cupi provid univers function aka ufunc support variou elementwis oper cupi ufunc support follow featur numpi one broadcast output type determin cast rule cupi ufunc current provid method reduc accumul reduceat outer ufunc autosummari toctre gener ufunc avail ufunc math oper autosummari toctre gener add subtract multipli matmul divid logaddexp logaddexp true_divid floor_divid neg posit power remaind mod fmod absolut rint sign conj conjug exp exp log log log expm logp sqrt squar cbrt reciproc gcd lcm trigonometr function autosummari toctre gener sin co tan arcsin arcco arctan arctan hypot sinh cosh tanh arcsinh arccosh arctanh degre radian degrad raddeg bittwiddl function autosummari toctre gener bitwise_and bitwise_or bitwise_xor invert left_shift right_shift comparison function autosummari toctre gener greater greater_equ less less_equ not_equ equal logical_and logical_or logical_xor logical_not maximum minimum fmax fmin float function autosummari toctre gener isfinit isinf isnan signbit copysign nextaft modf ldexp frexp fmod floor ceil trunc ufuncat current cupi support ufunc gener howev funccupyxscatter_add substitut addat behav ident gener univers function currentmodul cupyx addit regular ufunc cupi also provid wrapper class convert regular cupi function gener univers function numpi httpsnumpyorgdocstablereferencecapigeneralizedufuncshtml_ allow automat use keyword argument axe order dtype without need explicitli implement wrap function autosummari toctre gener generalizedufunc modul cupyxscipysparselinalg spars linear algebra modcupyxscipysparselinalg hint scipi api refer spars linear algebra scipysparselinalg httpsdocsscipyorgdocscipyreferencesparselinalghtml_ abstract linear oper autosummari toctre gener linearoper aslinearoper matrix norm autosummari toctre gener norm solv linear problem direct method linear equat system autosummari toctre gener spsolv spsolve_triangular factor iter method linear equat system autosummari toctre gener cg gmre cg minr iter method leastsquar problem autosummari toctre gener lsqr lsmr matrix factor eigenvalu problem autosummari toctre gener eigsh lobpcg singular valu problem autosummari toctre gener svd complet incomplet lu factor autosummari toctre gener splu spilu superlu array api complaint object classcupyarray_api_array_objectarray wrapper class built upon classcupyndarray enforc strict complainc array api standard see document httpsdataapisorgarrayapilatestapi_specificationarray_objecthtml_ detail object construct directli rather use one creation function httpsdataapisorgarrayapilatestapi_specificationcreation_functionshtml_ funccupyarray_apiasarray currentmodul cupyarray_api_array_object autosummari toctre gener array modul cupyfft discret fourier transform modcupyfft hint numpi api refer discret fourier transform numpyfft httpsnumpyorgdocstablereferenceroutinesffthtml_ seealso docscipy_fft docuser_guidefft standard fft autosummari toctre gener fft ifft fft ifft fftn ifftn real fft autosummari toctre gener rfft irfft rfft irfft rfftn irfftn hermitian fft autosummari toctre gener hfft ihfft helper routin autosummari toctre gener fftfreq rfftfreq fftshift ifftshift cupyspecif api see descript detail autosummari toctre gener configset_cufft_callback configset_cufft_gpu configget_plan_cach configshow_plan_cache_info normal default normal norm backward none direct transform unscal invers transform scale mathn keyword argument norm forward exact opposit backward direct transform scale mathn invers transform unscal final keyword argument norm ortho transform scale mathsqrtn code compat featur fft function numpi alway return numpyndarray type numpycomplex numpyfloat cupi function follow behavior return numpycomplex numpyfloat type input numpyfloat numpyfloat numpycomplex intern cupyfft alway gener cufft plan see cufft documentation_ detail correspond desir transform possibl ndimension plan use oppos appli separ plan axi transform use ndimension plan provid better perform multidimension transform requir gpu memori separ plan user disabl ndimension plan set cupyfftconfigenable_nd_plan fals abil adjust plan type deviat numpi api use precomput fft plan moreov automat plan gener suppress use exist plan return funccupyxscipyfftpackget_fft_plan context manag deviat numpi final use highlevel numpylik fft api list intern cufft plan cach possibl reus plan cach retriev funccupyfftconfigget_plan_cach current statu queri funccupyfftconfigshow_plan_cache_info finer control plan cach see docplan_cach multigpu fft modcupyfft use multipl gpu enabl disabl featur set datacupyfftconfiguse_multi_gpu true fals next set number gpu particip gpu id use function funccupyfftconfigset_cufft_gpu limit list cufft documentation_ appli particular use one gpu guarante better perform _cufft document httpsdocsnvidiacomcudacufftindexhtml array creation routin hint numpi api refer array creation routin httpsnumpyorgdocstablereferenceroutinesarraycreationhtml_ currentmodul cupi one zero autosummari toctre gener empti empty_lik eye ident one ones_lik zero zeros_lik full full_lik exist data autosummari toctre gener array asarray asanyarray ascontiguousarray copi frombuff fromfil fromfunct fromit fromstr loadtxt numer rang autosummari toctre gener arang linspac logspac meshgrid mgrid ogrid build matric autosummari toctre gener diag diagflat tri tril triu vander data type routin hint numpi api refer data type routin httpsnumpyorgdocstablereferenceroutinesdtypehtml_ currentmodul cupi autosummari toctre gener can_cast result_typ common_typ csvtabl align left promote_typ alia funcnumpypromote_typ min_scalar_typ alia funcnumpymin_scalar_typ objsctyp alia funcnumpyobjsctyp creat data type csvtabl align left dtype alia classnumpydtyp format_pars alia classnumpyformat_pars data type inform csvtabl align left finfo alia classnumpyfinfo iinfo alia classnumpyiinfo machar alia classnumpymachar data type test csvtabl align left issctyp alia funcnumpyissctyp issubdtyp alia funcnumpyissubdtyp issubsctyp alia funcnumpyissubsctyp issubclass_ alia funcnumpyissubclass_ find_common_typ alia funcnumpyfind_common_typ miscellan csvtabl align left typenam alia funcnumpytypenam sctypechar alia funcnumpysctypechar mintypecod alia funcnumpymintypecod _cupy_refer api refer refgenindex refmodindex currentmodul cupi numpyscipycompat api ndarray ufunc routin scipi omit modul name prefix api list follow convens numpyscipi document cupyspecif api use fullyqualifi name toctre maxdepth ndarray ufunc routin scipi ext cuda kernel distribut environ comparison array_api input output hint numpi api refer input output httpsnumpyorgdocstablereferenceroutinesiohtml_ currentmodul cupi numpi binari file npi npz autosummari toctre gener load save savez savez_compress text file autosummari toctre gener loadtxt savetxt genfromtxt fromstr string format autosummari toctre gener arraystr array_repr array_str format_float_posit basen represent autosummari toctre gener binary_repr base_repr python array api support python array api standard httpsdataapisorgarrayapilatest_ aim provid coher set api array tensor librari develop commun build upon solv api fragment issu across commun offer concret function signatur semant scope coverag enabl write backendagnost code better portabl cupi provid experiment support base numpi nep httpsnumpyorgnepsneparrayapistandardhtml_ turn base draft standard final function access modcupyarray_api namespac key differ numpi cupi gpuonli librari therefor cupi user awar potenti devic manag httpsdataapisorgarrayapilatestdesign_topicsdevice_supporthtml_ issu regular cupi code gputous specifi via classcupycudadevic object see refdevice_manag toctre maxdepth array_api_funct array_api_array orphan document move docscipy_stat orphan document move docscipy_sign orphan document move docscipy_fftpack orphan document move docscipy_speci routin scipi follow page describ scipycompat routin function cover subset scipi routin httpsdocsscipyorgdocscipyreferenceapireference_ modul cupyxscipi toctre maxdepth scipy_fft scipy_fftpack scipy_linalg scipy_ndimag scipy_sign scipy_spars scipy_sparse_linalg scipy_sparse_csgraph scipy_speci scipy_stat modul cupytest test support modcupytest hint numpi api refer test support numpytest httpsnumpyorgdocstablereferenceroutinestestinghtml_ assert hint api accept classnumpyndarray classcupyndarray autosummari toctre gener assert_array_almost_equ assert_allclos assert_array_almost_equal_nulp assert_array_max_ulp assert_array_equ assert_array_less cupyspecif api assert autosummari toctre gener assert_array_list_equ numpycupi consist check follow decor test consist cupi function correspond numpi one autosummari toctre gener numpy_cupy_allclos numpy_cupy_array_almost_equ numpy_cupy_array_almost_equal_nulp numpy_cupy_array_max_ulp numpy_cupy_array_equ numpy_cupy_array_list_equ numpy_cupy_array_less parameter dtype test follow decor offer standard way parameter test respect singl combin dtype autosummari toctre gener for_dtyp for_all_dtyp for_float_dtyp for_signed_dtyp for_unsigned_dtyp for_int_dtyp for_complex_dtyp for_dtypes_combin for_all_dtypes_combin for_signed_dtypes_combin for_unsigned_dtypes_combin for_int_dtypes_combin parameter order test follow decor offer standard way parameter test order autosummari toctre gener for_ord for_cf_ord mathemat function hint numpi api refer mathemat function httpsnumpyorgdocstablereferenceroutinesmathhtml_ currentmodul cupi trigonometr function autosummari toctre gener sin co tan arcsin arcco arctan hypot arctan degre radian unwrap degrad raddeg hyperbol function autosummari toctre gener sinh cosh tanh arcsinh arccosh arctanh round autosummari toctre gener around round_ rint fix floor ceil trunc sum product differ autosummari toctre gener prod sum nanprod nansum cumprod cumsum nancumprod nancumsum diff gradient ediffd cross trapz expon logarithm autosummari toctre gener exp expm exp log log log logp logaddexp logaddexp special function autosummari toctre gener sinc float point routin autosummari toctre gener signbit copysign frexp ldexp nextaft ration routin autosummari toctre gener lcm gcd arithmet oper autosummari toctre gener add reciproc posit neg multipli divid power subtract true_divid floor_divid fmod mod modf remaind divmod handl complex number autosummari toctre gener angl real imag conj conjug miscellan autosummari toctre gener convolv clip sqrt cbrt squar absolut fab sign maximum minimum fmax fmin nan_to_num interp modul cupyxscipyfftpack legaci discret fourier transform modcupyxscipyfftpack note scipi version modscipyfft recommend modscipyfftpack consid use modcupyxscipyfft instead hint scipi api refer legaci discret fourier transform scipyfftpack httpsdocsscipyorgdocscipyreferencefftpackhtml_ fast fourier transform fft autosummari toctre gener fft ifft fft ifft fftn ifftn rfft irfft get_fft_plan code compat featur fft modul cupi fft function modul take advantag exist cufft plan return funccupyxscipyfftpackget_fft_plan accelar comput plan either pass explicitli via plan argument use context manag argument plan current experiment interfac may chang futur version funccupyxscipyfftpackget_fft_plan function counterpart scipyfftpack boolean switch datacupyfftconfigenable_nd_plan also affect fft function modul see docfft switch neglect plan manual use funccupyxscipyfftpackget_fft_plan like scipyfftpack fft function modul option argument overwrite_x default fals semant scipyfftpack set true input array x overwritten arbitrarili reason inplac fft desir user alway reassign input follow manner x cupyxscipyfftpackfftx overwrite_xtru boolean switch datacupyfftconfiguse_multi_gpu also affect fft function modul see docfft moreov switch honor plan manual use funccupyxscipyfftpackget_fft_plan sort search count hint numpi api refer sort search count httpsnumpyorgdocstablereferenceroutinessorthtml_ currentmodul cupi sort autosummari toctre gener sort lexsort argsort msort sort_complex partit argpartit seealso funccupyndarraysort search autosummari toctre gener argmax nanargmax argmin nanargmin argwher nonzero flatnonzero searchsort extract count autosummari toctre gener count_nonzero orphan document move docscipy_ndimag modul cupyxscipyspars spars matric modcupyxscipyspars hint scipi api refer spars matric scipyspars httpsdocsscipyorgdocscipyreferencesparsehtml_ cupi support spars matric use cuspars httpsdevelopernvidiacomcusparse_ matric interfac scipi spars matric httpsdocsscipyorgdocscipyreferencesparsehtml_ convers tofrom scipi spars matric cupyxscipysparse_matrix scipysparse_matrix implicitli convert mean scipi function take cupyxscipysparse_matrix object input vice versa convert scipi spars matric cupi pass constructor cupi spars matrix class convert cupi spars matric scipi use funcget cupyxscipysparsespmatrixget method cupi spars matrix class note convert cupi scipi incur data transfer host cpu devic gpu devic costli term perform convers tofrom cupi ndarray convert cupi ndarray cupi spars matric pass constructor cupi spars matrix class convert cupi spars matric cupi ndarray use toarray cupi spars matrix instanc eg funccupyxscipysparsecsr_matrixtoarray convert cupi ndarray cupi spars matric incur data transfer copi insid gpu devic content spars matrix class autosummari toctre gener coo_matrix csc_matrix csr_matrix dia_matrix spmatrix function build spars matric autosummari toctre gener eye ident kron kronsum diag spdiag tril triu bmat hstack vstack rand random spars matrix tool autosummari toctre gener find identifi spars matric autosummari toctre gener isspars isspmatrix isspmatrix_csc isspmatrix_csr isspmatrix_coo isspmatrix_dia submodul autosummari csgraph compress spars graph routin linalg spars linear algebra routin except classscipysparsesparseefficiencywarn classscipysparsesparsewarn array manipul routin hint numpi api refer array manipul routin httpsnumpyorgdocstablereferenceroutinesarraymanipulationhtml_ currentmodul cupi basic oper autosummari toctre gener copyto shape chang array shape autosummari toctre gener reshap ravel seealso attrcupyndarrayflat funccupyndarrayflatten transposelik oper autosummari toctre gener moveaxi rollaxi swapax transpos seealso attrcupyndarrayt chang number dimens autosummari toctre gener atleast_d atleast_d atleast_d broadcast broadcast_to broadcast_array expand_dim squeez chang kind array autosummari toctre gener asarray asanyarray asfarray asfortranarray ascontiguousarray asarray_chkfinit requir join array autosummari toctre gener concaten stack vstack hstack dstack column_stack row_stack split array autosummari toctre gener split array_split dsplit hsplit vsplit tile array autosummari toctre gener tile repeat ad remov element autosummari toctre gener append resiz uniqu trim_zero rearrang element autosummari toctre gener flip fliplr flipud reshap roll rot