# EvoMaster: A Tool For Automatically Generating System-Level Test Cases


![](docs/img/carl-cerstrand-136810_compressed.jpg  "Photo by Carl Cerstrand on Unsplash")

[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.evomaster/evomaster-client-java/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.evomaster/evomaster-client-java)
[![javadoc](https://javadoc.io/badge2/org.evomaster/evomaster-client-java-controller/javadoc.svg)](https://javadoc.io/doc/org.evomaster/evomaster-client-java-controller)
![CI](https://github.com/EMResearch/EvoMaster/workflows/CI/badge.svg)
[![codecov](https://codecov.io/gh/EMResearch/EvoMaster/branch/master/graph/badge.svg)](https://codecov.io/gh/EMResearch/EvoMaster)



### Summary 

_EvoMaster_ ([www.evomaster.org](http://evomaster.org)) is the first (2016) open-source AI-driven tool 
that automatically *generates* system-level test cases
for web/enterprise applications.
This is related to [Fuzzing](https://en.wikipedia.org/wiki/Fuzzing).
Not only _EvoMaster_ can generate inputs that find program crashes, but also it generates small effective test suites that can be used for _regression testing_.

_EvoMaster_ is an AI driven tool.
In particular, internally it uses an [Evolutionary Algorithm](https://en.wikipedia.org/wiki/Evolutionary_algorithm) 
and [Dynamic Program Analysis](https://en.wikipedia.org/wiki/Dynamic_program_analysis)  to be 
able to generate effective test cases.
The approach is to *evolve* test cases from an initial population of 
random ones, trying to maximize measures like code coverage and fault detection.
_EvoMaster_ uses several kinds of AI heuristics to improve performance even further, 
building on decades of research in the field of [Search-Based Software Testing](https://en.wikipedia.org/wiki/Search-based_software_engineering).


__Key features__:

* At the moment, _EvoMaster_ targets RESTful APIs compiled to 
  JVM (e.g., Java and Kotlin). We support JDK __8__ and the major LTS versions after that (currently JDK __17__). Might work on other JVM versions, but we provide __NO__ support for it.

* We provide installers for the main operating systems: Windows (`.msi`), 
  OSX (`.dmg`) and Linux (`.deb`). We also provide an uber-fat JAR file.

* The REST APIs must provide a schema in [OpenAPI/Swagger](https://swagger.io) 
  format (either _v2_ or _v3_).

* The tool generates _JUnit_ (version 4 or 5) tests, written in either Java or Kotlin.

* _Fault detection_: _EvoMaster_ can generate tests cases that reveal faults/bugs in the tested applications.
  Different heuristics are employed, like checking for 500 status codes and mismatches from the API schemas. 

* Self-contained tests: the generated tests do start/stop the application, binding to an ephemeral port.
  This means that the generated tests can be used for _regression testing_ (e.g., added to the Git repository
  of the application, and run with any build tool such as Maven and Gradle). 

* Advanced _whitebox_ heuristics: _EvoMaster_ analyses the bytecode of the tested applications, and uses
  several heuristics such as _testability transformations_ and _taint analysis_ to be able to generate 
  more effective test cases. 

* SQL handling: _EvoMaster_ can intercept and analyse all communications done with SQL databases, and use
  such information to generate higher code coverage test cases. Furthermore, it can generate data directly
  into the databases, and have such initialization automatically added in the generated tests. 
  At the moment, _EvoMaster_ supports _Postgres_, _MySQL_ and _H2_  databases.  

* _Blackbox_ testing mode: can run on any API (regardless of its programming language), 
  as long as an OpenAPI schema is provided. However, results will be worse than whitebox testing (e.g., due
  to lack of bytecode analysis).

* _Authentication_: we support auth based on authentication headers and cookies. 

__Known limitations__:

* To be used for _whitebox_ testing, users need to write a [driver manually](docs/write_driver.md).
  We recommend to try _blackbox_ mode first (should just need a few minutes to get it up and running) to get
  an idea of what _EvoMaster_ can do for you.  

* Execution time: to get good results, you might need to run the search for several hours. 
  We recommend to first try the search for 10 minutes, just to get an idea of what type of tests can be generated.
  But, then, you should run _EvoMaster_ for something like between 1 and 24 hours (the longer the better, but
  it is unlikely to get better results after 24 hours).
  
* External services (e.g., other RESTful APIs): currently there is no support for them (e.g., to automatically mock them).
  It is work in progress.
  
* NoSQL databases (e.g., MongoDB): currently no support. It is work in progress. 

* Failing tests: the tests generated by _EvoMaster_ should all pass, and not fail, even when they detect a fault.
  In those cases, comments/test-names would point out that a test is revealing a possible fault, while still passing.
  However, in some cases the generated tests might fail. This is due to the so called _flaky_ tests, e.g., when
  a test has assertions based on the time clock (e.g., dates and timestamps). 
  There is ongoing effort to address this problem, but it is still not fully solved.   

<!--### Videos---> 
<!-- 
<div>Icons made by <a href="https://www.flaticon.com/authors/freepik" title="Freepik">Freepik</a> from <a href="https://www.flaticon.com/" title="Flaticon">www.flaticon.com</a></div> 
-->


### Videos

![](docs/img/video-player-flaticon.png)

* A [short video](https://youtu.be/3mYxjgnhLEo) (5 minutes)
shows the use of _EvoMaster_ on one of the 
case studies in [EMB](https://github.com/EMResearch/EMB). 

* This [13-minute video](https://youtu.be/ORxZoYw7LnM)
  shows how to write a white-box driver for EvoMaster, for the
  [rest-api-example](https://github.com/EMResearch/rest-api-example). 

* How to [Download and Install EvoMaster on Windows 10](https://youtu.be/uh_XzGxws9o), using its _.msi_ installer. 
 
<!---
### Hiring

Each year we usually have funding for _postdoc_ and _PhD student_ positions to work on this project (in Oslo, Norway).
For more details on current vacancies, see our group page at [AISE Lab](https://emresearch.github.io/).
--->



### Documentation

* [Example of generated tests](docs/example.md)
* [Download and Install EvoMaster](docs/download.md)
* [Build EvoMaster from source](docs/build.md)
* [Console options](docs/options.md)
* [OpenApi/Swagger Schema](docs/openapi.md)
* [Using EvoMaster for Black-Box Testing (easier to setup, but worse results)](docs/blackbox.md)
* [Using EvoMaster for White-Box Testing (harder to setup, but better results)](docs/whitebox.md)
    * [Write an EvoMaster Driver for White-Box Testing](docs/write_driver.md)
    * [Dealing with JDKs above version 8](docs/jdks.md)
* [Console output](docs/console_output.md)  
* [Library dependencies for the generated tests](docs/library_dependencies.md)
* [How to contribute](docs/contribute.md)
    * [Technical notes for developers contributing to EvoMaster](docs/for_developers.md)
* Troubleshooting
    * [Windows and networking](docs/troubleshooting/windows.md)
* More Info
    * [Academic papers related to EvoMaster](docs/publications.md)
    * [Slides of presentations/seminars](docs/presentations.md)
    * [Replicating studies](docs/replicating_studies.md)






### Funding

_EvoMaster_ has been funded by: 
* 2020-2025: a 2 million Euro grant by the European Research Council (ERC),
as part of the *ERC Consolidator* project 
<i>Using Evolutionary Algorithms to Understand and Secure Web/Enterprise Systems</i>.
*  2018-2021: a 7.8 million Norwegian Kroner grant  by the Research Council of Norway (RCN), 
as part of the Frinatek project <i>Evolutionary Enterprise Testing</i>.  


<img src="https://github.com/EMResearch/EvoMaster/blob/master/docs/img/LOGO_ERC-FLAG_EU_.jpg?raw=true" width="200" >


This project has received funding from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No 864972).


### License
_EvoMaster_'s source code is released under the LGPL (v3) license.
For a list of the used third-party libraries, you can directly see the root [pom.xml](./pom.xml) file.
For a list of code directly imported (and then possibly modified/updated) from 
other open-source projects, see [here](./docs/reused_code.md).


### ![](https://www.yourkit.com/images/yklogo.png)

YourKit supports open source projects with its full-featured Java Profiler.
YourKit, LLC is the creator of 
<a href="https://www.yourkit.com/java/profiler/">YourKit Java Profiler</a>
and 
<a href="https://www.yourkit.com/.net/profiler/">YourKit .NET Profiler</a>,
innovative and intelligent tools for profiling Java and .NET applications.


---
name: Bug report
about: Bug report
title: ''
labels: ''
assignees: ''

---

If there are stacktraces of the errors, please paste them here.

Other important info:
- version of EvoMaster (EM) used 
- how EM is run (eg, if from JAR or from one of its OS installers)
- version of applicable runtimes (eg, JVM, NodeJS and .Net). For Java, can paste the output of `java --version`
- command-line options used to run EM
# White-Box Testing


In white-box testing, the internal details of the _system under test_ (SUT) are known.
There is the need to able to access to the source code (or bytecode, for JVM languages) of the SUT.
This is usually not a problem when testing is done by the developers of the SUT themselves. 


A white-box test approach can aim at maximizing the code coverage of the SUT. 
This is helpful in at least two ways:

*   _Fault Detection_: the higher the code coverage, the more likely it is to find a bug in the SUT.
    A bug can only manifest itself if the faulty statements are executed at least once.
*   _Regression Testing_: even if no fault is found, the generated tests can still be useful to check
    later on for regression faults. And for fault detection, the higher code coverage the better.        


To measure code coverage, the SUT needs to be _instrumented_, by putting probes in it.
In JVM languages, this can be done automatically by intercepting the class loaders, and then
use libraries like ASM to manipulate the bytecode of SUT classes at runtime. 


But measuring code coverage alone is not enough to generate high coverage test cases.
Consider this trivial code snippet:

`if(x==42){//...` 

Using a black-box approach in which inputs are randomly generated, a test input would only have 1
single probability out of 2 at the power of 32 (i.e., around 4 billion possibilities in a 32-bit number)
 to cover the `then` branch of that `if` statement.
But, a static/dynamic analysis of the code would simply point out to use the value `42` for `x`.

This is a trivial example, but predicates in the source code can be arbitrarily complex, for
example involving regular expressions and the results of accesses to SQL databases.
_EvoMaster_ uses several different heuristics and code analysis techniques to maximize code coverage 
using an  evolutionary algorithm.
In the academic literature, this is referred as _Search-Based Software Testing_.
The interested reader is encouraged to look at our [academic papers](publications.md) 
to learn more about these technical details. 


These static and dynamic code analyses do require accessing the source code, and instrument it
before the SUT is started. 
But this can be done together when the SUT is instrumented to measure its code coverage.
All the instrumentations and code analyses are automatically performed by _EvoMaster_ with a 
library we provide (e.g., on Maven Central for JVM languages).

A user needs to provide a script/class (called _driver_) in which the SUT is _started_, with instrumentations provided
by our library.
This must be done manually, as each different frameworks (e.g., Spring and DropWizard) has its
own way to start and package an application. 
Once a user has to provide a driver to _start_ the SUT, adding the options to _stop_ and _reset_
the SUT should not be much extra work.
Once this is done, the  test cases  automatically generated by _EvoMaster_ become _self-contained_,
as they can use such driver.
For example, they can _start_ the SUT before the tests, _reset_ its state at each test execution to make
them independent, and then finally _stop_ the SUT after all tests are completed.

We explain [how to write such script/class in this other document](write_driver.md).
To check it out before spending time writing one, you can look at the
[EMB repository](https://github.com/EMResearch/EMB) and search for classes called 
`EmbeddedEvoMasterController`.
Start one of those directly from your IDE.
This will start the controller server (binding by default on port `40100`) for one of the SUTs in the
EMB collection.
The controller server is responsible to handle the start/reset/stop of the SUT.  
Once it is up and running, you can generate test cases  for it by running _EvoMaster_ from
command-line with: 

```
java -jar evomaster.jar
```
  
By default, _EvoMaster_ will try to connect to a controller server that is listening on port 40100.
Its first step will be to tell it to start the SUT with  all the required instrumentations.
Then, it will finally start an evolutionary algorithm to evolve test cases, and measure their fitness
when executed against the SUT. 
To see which options to use when running _EvoMaster_ (e.g., for how long to run the evolution),
see the [main options](options.md).  
  


# Publications

The development of _EvoMaster_ is rooted in academia.
Here, you can find the PDFs of all the academic publications based on _EvoMaster_. 
Furthermore, slides of presentations can be  found [here](presentations.md).
These can be useful if you want to know more on how _EvoMaster_ works internally,
e.g., details on the Many Independent Objective (MIO) algorithm.

To help to replicate previous studies, for most of these papers we also provide the scripts used to setup the experiments.
This explained in more details [here](replicating_studies.md).
Also, some of these papers provides full replication packages, which are linked directly in the papers (and not stored in this repository).

### 2022

* M. Zhang, A. Belhadi, A. Arcuri
  *JavaScript Instrumentation for Search-Based Software Testing: A Study with RESTful APIs*.
  IEEE International Conference on Software Testing, Validation and Verification (ICST). (*to appear*)
  [[PDF](publications/2022_icst.pdf)]
  

### 2021

* B. Marculescu, M. Zhang, A. Arcuri.
  *On the faults found in REST APIs by Automated Test Generation*.
  ACM Transactions on Software Engineering and Methodology (TOSEM). (*to appear*)
  [[PDF](publications/2021_tosem_faults.pdf)]
  [[Scripts](exp/2021_tosem_faults.py)]

* A. Martin-Lopez, A. Arcuri, S. Segura, A. Ruiz-Cortes.
  *Black-Box and White-Box Test Case Generation for RESTful APIs: Enemies or Allies?*
  IEEE International Symposium on Software Reliability Engineering (ISSRE). (*to appear*)
  [[PDF](publications/2021_issre.pdf)]
 
* M. Zhang, A. Arcuri.
  *Enhancing Resource-based Test Case Generation For RESTful APIs with SQL Handling*.
  Symposium on Search-based Software Engineering (SSBSE). 
  [[PDF](publications/2021_ssbse.pdf)]
  [[Scripts](exp/2021_ssbse.py)]

* A. Arcuri, J.P. Galeotti.
  *Enhancing Search-Based Testing With Testability Transformations For Existing APIs*.
  ACM Transactions on Software Engineering and Methodology (TOSEM).
  [[PDF](publications/2021_tosem_tt.pdf)]
  [[Scripts](exp/2021_tosem_tt.py)]

* M. Zhang, A. Arcuri.
  *Adaptive Hypermutation for Search-Based System Test  Generation: A Study on REST APIs with EvoMaster*.
  ACM Transactions on Software Engineering and Methodology (TOSEM). 
  [[PDF](publications/2021_tosem_hm.pdf)] [[Scripts](exp/2021_tosem_hm.py)]

* M. Zhang, B. Marculescu, A. Arcuri.
  *Resource and Dependency based Test Case Generation for RESTful Web Services*.
  Empirical Software Engineering (EMSE). 
  [[PDF](publications/2021_emse.pdf)]
  [[Scripts](exp/2021_tosem_tt.py)]

* A. Arcuri, J.P. Galeotti, B. Marculescu, M. Zhang.
  *EvoMaster: A Search-Based System Test Generation Tool*.
  The Journal of Open Source Software (JOSS).
  [[PDF](publications/2021_joss.pdf)]

### 2020

* A. Arcuri.
  *Automated Blackbox and Whitebox Testing of RESTful APIs with EvoMaster*.
  IEEE Software.
  [[PDF](publications/2020_sw.pdf)]
  [[Scripts](exp/2020_sw.py)]

* A. Arcuri, J.P. Galeotti.
  *Handling SQL Databases in Automated System Test Generation*. 
  ACM Transactions on Software Engineering and Methodology (TOSEM).
  [[PDF](publications/2020_tosem.pdf)] [[Scripts](exp/2020_tosem.py)]

* A. Arcuri, J.P. Galeotti.
  *Testability Transformations For Existing APIs*.
  IEEE International Conference on Software Testing, Validation and Verification (ICST).
  [[PDF](publications/2020_icst.pdf)]
  [[Scripts](exp/2020_icst.py)]

### 2019

* M. Zhang, B. Marculescu, A. Arcuri. 
     *Resource-based Test Case Generation for RESTful Web Services*.
     ACM Genetic and Evolutionary Computation Conference (GECCO).
     [[PDF](publications/2019_gecco_resources.pdf)] [[Scripts](exp/2019_gecco_resource.py)]

* A. Arcuri, J.P. Galeotti. 
     *SQL Data Generation to Enhance Search-Based System Testing*.
     ACM Genetic and Evolutionary Computation Conference (GECCO).
     [[PDF](publications/2019_gecco_sql.pdf)]
     [[Scripts](exp/2019_gecco_sql.py)]

* A. Arcuri. *RESTful API Automated Test Case Generation with EvoMaster*.
     ACM Transactions on Software Engineering and Methodology (TOSEM).
     [[PDF](publications/2019_tosem.pdf)]    [[Scripts](exp/2019_tosem.py)]

  
### 2018


* A. Arcuri. *Test Suite Generation with the Many Independent Objective (MIO) Algorithm*.
     Information and Software Technology (IST).
     [[PDF](publications/2018_ist.pdf)]    [[Scripts](exp/2018_ist.py)]

* A. Arcuri. *EvoMaster: Evolutionary Multi-context Automated System Test Generation*.
     IEEE International Conference on Software Testing, Validation and Verification (ICST).
     [[PDF](publications/2018_icst.pdf)]

     
* A. Arcuri. *An Experience Report On Applying Software Testing Academic 
               Results In Industry: We Need Usable Automated Test Generation*.
   Empirical Software Engineering (EMSE).
   [[PDF](publications/2018_emse.pdf)]                           


### 2017

* A. Arcuri. *RESTful API Automated Test Case Generation*.
  IEEE International Conference on Software Quality, Reliability & Security (QRS).
  [[PDF](publications/2017_qrs.pdf)] 
  [[Scripts](exp/2017_qrs.sh)]
  
* A. Arcuri. *Many Independent Objective (MIO) Algorithm for Test Suite Generation*.
  Symposium on Search-based Software Engineering (SSBSE).
  **Best paper award**.
  [[PDF](publications/2017_ssbse.pdf)] [[Scripts](exp/2017_ssbse.sh)]
  
# OpenAPI Schema

To test a RESTful API, _EvoMaster_ requires the presence of a schema.
There are different ways to write API schemata, where [OpenAPI](https://www.openapis.org/) is 
arguably the most common and used. 
Such format (previously called `Swagger`) is supported by _EvoMaster_, both *v2* and *v3*.

If your API does not have such a schema, you have two options:
1. Write it by hand.
2. Use a tool to automatically generate it from your source code.

This latter option depends on which language and frameworks you are using to implement your API.
For example, for *Spring* applications, you can look at [SpringDoc](https://github.com/springdoc/springdoc-openapi).
Adding a schema then is as easy as just adding such dependency to the classpath.
Then, the schema will be accessible on the endpoint `/v3/api-docs`.  # Download and Install EvoMaster


_EvoMaster_ is composed of two main components:

* *Core*: the main program executable, packaged in a `evomaster.jar` file.
* *Driver*: library used to control and instrument the application under test.
            There is going to be one library per supported language/environment,
            like the JVM.
            Note: the driver is __NOT__ needed for [Black-Box Testing](./blackbox.md). 


The latest release of the `evomaster.jar` executable  can be downloaded from GitHub
 on the [releases page](https://github.com/EMResearch/EvoMaster/releases).
Alternatively, it can be built from [source code](./build.md).

Note: it does not matter where you download the jar file (e.g., your home folder, or the folder
of your project), as long as you can easily access it from a command-line terminal (e.g.,
to be able to execute `java -jar` on it).

Besides an uber-fat jar, since version 1.2.0 we also provide installers for Windows/OSX/Linux.
Note: the installers are built with `jpackage`, that currently does not support 
updating the [PATH environment variable](https://stackoverflow.com/questions/67784565/jpackage-update-path-environment-variable).
This means that, unless you want to type the full absolute path of where _EvoMaster_
gets installed each time you want to use it, you will need to update the PATH environmental variable by hand.
By default, _EvoMaster_ will get installed at:
* On Windows: `C:\Program Files\evomaster\evomaster.exe`
* On OSX: `/Applications/evomaster.app/Contents/MacOS/evomaster`
* On Linux: ` /opt/evomaster/bin/evomaster`


Regarding the _driver_ library, it depends on the language/environment.
For example, the JVM support is available from [Maven Central](https://mvnrepository.com/artifact/org.evomaster). 
If your are building such library from [source code](./build.md), then make sure to
use the Maven `install` option to copy it over your local `~/.m2` repository.
 
# Example

![](img/evomaster_console.png)


The following code is an example of one test that was automatically
generated by _EvoMaster_ for a REST service called 
"scout-api" (see [EMB repository](https://github.com/EMResearch/EMB)).
The generated test uses the [RestAssured](https://github.com/rest-assured/rest-assured) library.

```
@Test
public void test_36_with500() throws Exception {
        
   String location_media_files = "";
        
   String id_0 = given().accept("application/json")
                .header("Authorization", "ApiKey moderator") // moderator
                .contentType("application/json")
                .body(" { " + 
                    " \"uri\": \"hXf3e8B3ikGtuGjT\", " + 
                    " \"name\": \"nkeUfXVTC\", " + 
                    " \"copy_right\": \"\" " + 
                    " } ")
                .post(baseUrlOfSut + "/api/v1/media_files")
                .then()
                .statusCode(200)
                .assertThat()
                .contentType("application/json")
                .body("'uri'", containsString("hXf3e8B3ikGtuGjT"))
                .body("'name'", containsString("nkeUfXVTC"))
                .body("'copy_right'", containsString(""))
                .extract().body().path("id").toString();
                
   location_media_files = "/api/v1/media_files/" + id_0;
        
   given().accept("*/*")
                .header("Authorization", "ApiKey moderator") // moderator
                .get(resolveLocation(location_media_files, baseUrlOfSut + "/api/v1/media_files/1579038228/file"))
                .then()
                .statusCode(500) // se/devscout/scoutapi/resource/MediaFileResource_268_downloadFile
                .assertThat()
                .contentType("application/json")
                .body("'code'", numberMatches(500.0));
}
```

In this automatically generated test, a new resource is first created with a _POST_ command.
The _id_ of this newly generated resource is then extracted from the _POST_ response, and used in the URL
of a following _GET_ request on a sub-resource.
Such _GET_ request does break the backend, as it returns a __500__ HTTP status code.
The last line executed in the business logic of the backend is then printed as comment, to help debugging this fault.    


The generated tests are self-contained, i.e., they 
start/stop the REST server by themselves:

```
    private static SutHandler controller = new em.embedded.se.devscout.scoutapi.EmbeddedEvoMasterController();
    private static String baseUrlOfSut;
    
    
    @BeforeClass
    public static void initClass() {
        baseUrlOfSut = controller.startSut();
        assertNotNull(baseUrlOfSut);
        RestAssured.urlEncodingEnabled = false;
    }
    
    
    @AfterClass
    public static void tearDown() {
        controller.stopSut();
    }
    
    
    @Before
    public void initTest() {
        controller.resetStateOfSUT();
    }
```

The ability of starting/resetting/stopping the tested application is critical for using the generated 
tests in _Continuous Integration_ (e.g., Jenkins, Travis and CircleCI).
However, it requires to write a [_driver_](write_driver.md) to tell _EvoMaster_ how to do 
such start/reset/stop.
  

A generated test is not only going to be a sequence of HTTP calls toward a running application.
_EvoMaster_ can also set up the _environment_ of the application, like automatically adding all the
needed data into a SQL database.

# Build EvoMaster


To compile the project, you need Maven and the JDK (either __8__ or __11__).

Use the Maven command:

`mvn  clean install -DskipTests`

This should create an `evomaster.jar` executable under the `core/target` folder,
and install the driver library in your local `~/.m2` repository.

Note: if you get an error from the *shade-plugin*, then make sure to use
`clean` in your Maven commands.
Furthermore, if you decide to do not skip the tests, then you will need to have
_Docker_ installed and running on your machine.


Note: we have also started initial support for other languages, e.g., JavaScript and C#.
To build everything, you can use the script `buildAll.sh`.
However, besides the JDK, you will need other tools as well installed on your machine (e.g., NodeJS and DotNet Core). # Dealing With JDKs Above Version 8

## Driver/Controller Class

Java __9__ broke backward compatibility in an awful way.
And each new JDK version seems breaking even more stuff :-(.
One painful change was that self-attachment of Java-Agents (needed for bytecode instrumentation)
is now forbidden by default.
When for example starting the driver with a JDK __9+__ (e.g., JDK __11__), you need to add the VM option
`-Djdk.attach.allowAttachSelf=true`, otherwise the process will crash.   
For example, in IntelliJ IDEA:

![](img/intellij_jdk11_jvm_options.png)

Note: there is a hacky workaround for this "_feature_"
(i.e., as done in [ByteBuddy](https://github.com/raphw/byte-buddy/issues/295)),
but it is not implemented. 
There is no much point in implementing such feature, as JDK __17__ broke more stuff as well, and we would still need to set some JVM parameters. 
To use JDK __17__, besides passing `-Djdk.attach.allowAttachSelf=true`, you will also need:

`--add-opens java.base/java.util.regex=ALL-UNNAMED --add-opens java.base/java.net=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED`

isn't it lovely? 


## evomaster.jar

The recommended way to use _EvoMaster_ is via its installer, e.g.,
_.msi_ for Windows.
However, you can still use its _jar_ file directly, if you really want.
Unfortunately, though, if you use JDK _17_ or above, you have to deal
with `--add-opens` shenanigans. 

To see how to set it up, look at the usage of `--add-opens` in  [makeExecutable.sh](../makeExecutable.sh) script. 


# EvoMaster Driver
    
To generate tests for [white-box testing](whitebox.md), you need an _EvoMaster Driver_ up and running before 
executing `evomaster.jar`.
These drivers have to be built manually for each system under test (SUT).
See the [EMB repository](https://github.com/EMResearch/EMB) for a set of existing SUTs with drivers.

To build a client driver in Java (or any JVM language), you need to import the
_EvoMaster_ Java client library. For example, in Maven:

```
<dependency>
   <groupId>org.evomaster</groupId>
   <artifactId>evomaster-client-java-controller</artifactId>
   <scope>test</scope>
   <version>LATEST</version>
</dependency>
```

In Gradle, it would be:

`testCompile('org.evomaster:evomaster-client-java-controller:LATEST')`.

The placeholder `LATEST` needs to be replaced with an actual version number (e.g.,
`1.0.0` or `1.0.0-SNAPSHOT`).
For the latest version, check [Maven Central Repository](https://mvnrepository.com/artifact/org.evomaster/evomaster-client-java-controller).
The latest version number should also appear at the top of the main readme page.
If you are compiling directly from the _EvoMaster_ source code, make sure to use `mvn install` to 
install the snapshot version `x.y.z-SNAPSHOT` of the Java client into your local Maven repository 
(e.g., under *~/.m2*). 
For the actual `x.y.z-SNAPSHOT` version number, you need to look at the root `pom.xml` file in the project.
If you are using Gradle, you can for example check on this [SO question](https://stackoverflow.com/questions/6122252/gradle-alternate-to-mvn-install) 
to see how to do something equivalent to `mvn install`. 

Note: the core application `evomaster.jar` is independent of the driver library, and it contains none of 
the driver's classes.

Note: you might also need to import some [other libraries](library_dependencies.md)
(e.g., *RestAssured* when generating tests for REST APIs runninng on the JVM).

Once the client library is imported, you need to create a class that extends either
`org.evomaster.client.java.controller.EmbeddedSutController`
 or
 `org.evomaster.client.java.controller.ExternalSutController`.
Both these classes extend `SutController`.
The difference is on whether the SUT is started in the same JVM of the _EvoMaster_
driver (*embedded*), or in a separated JVM (*external*).
 
The easiest approach (which we recommend) is to use the *embedded* version, especially when dealing with
frameworks like Spring and DropWizard. 
However, when the presence of the _EvoMaster_ client library gives side-effects (although 
its third-party libraries are shaded, side-effects might still happen),
or when it is not possible (or too complicated) to start the SUT directly (e.g., JEE),
it is better to use the *external* version.
The requirement is that there should be a single, self-executable uber/fat jar for the SUT 
(e.g., Wildfly Swarm).
It can be possible to handle WAR files (e.g., by using Payara), 
but currently we have not tried it out yet.

Once a class is written that extends either `EmbeddedSutController` or
`ExternalSutController`, there are a few abstract methods that need to
be implemented.
For example, those methods specify how to start the SUT, how it should be stopped,
and how to reset its state.
The _EvoMaster_ Java client library also provides further utility classes to help
writing those controllers/drivers.
For example, `org.evomaster.client.java.controller.db.DbCleaner` helps in resetting
the state of a database (if any is used by the SUT).

Note: when implementing a new class, most IDEs (e.g., IntelliJ) have the function 
to automatically generate empty 
stubs for all the abstract methods in its super-classes. 
Also, all the concrete (i.e., non-abstract) methods in  `EmbeddedSutController`
and `ExternalSutController` are marked as `final`, to prevent overriding them by mistake
and so breaking the driver's internal functionalities. 

Each of the abstract methods you need to implement does provide Javadocs.
How to read those Javadocs depend on your IDE settings (e.g., hovering the mouse over a method declaration).
You can also browse them online [here](https://javadoc.io/doc/org.evomaster/evomaster-client-java-controller).



Once a class `X` that is a descendant of `SutController` is written, you need
to be able to start the _EvoMaster_ driver, by using the 
`org.evomaster.client.java.controller.InstrumentedSutStarter`
class. 
For example, in the source code of the class `X`, you could add:
 
```
public static void main(String[] args){

   SutController controller = new X();
   InstrumentedSutStarter starter = new InstrumentedSutStarter(controller);

   starter.start();
}
```

At this point, once this driver is started (e.g., by right-clicking on it in
an IDE to run it as a Java process),
then you can use `evomaster.jar` to finally generate test cases.
Note that it is also possible to run the driver from command-line, like any other Java program with a `main` function.
However, in such case, you will need to package an uber jar file (e.g., using plugins like `maven-shade-plugin` and `maven-assembly-plugin`).  


__WARNING__: Java 9 broke backward compatibility. 
And each new JDK version seems breaking more stuff :-(.
To deal with recent versions of the JDK, see [here for details](./jdks.md).





## TCP Ports

When writing an _EvoMaster_ driver, there are 2 TCP ports that you need to consider:

* the port of the driver itself, whose default value is 40100. This can be changed when instantiating
  a `SutController`. However, the _EvoMaster_ core process would need to be informed of this different
  port value (e.g., by using the `--sutControllerPort` option).

* the port of the SUT. In general, you will want to set up an ephemeral port (i.e., a free, un-used one) 
  for this (e.g., by using the value 0, and then read back in the driver which port was actually 
  assigned to the server).   

## Starting The Application

How to start/reset/stop the SUT depends on the chosen framework used to implement the SUT. 
To implement an _EvoMaster Driver_ class, you need check the JavaDocs of the extended super class,
e.g., `EmbeddedSutController`, and the existing examples in 
[EMB](https://github.com/EMResearch/EMB).


As _SpringBoot_ is nowadays the most common way to implement enterprise systems on the JVM, here we provide
some discussions / walk-through on how to write a driver for it that extends `EmbeddedSutController`,
using as reference the [driver for the *features-service* SUT in EMB](https://github.com/EMResearch/EMB/blob/master/em/embedded/rest/features-service/src/main/java/em/embedded/org/javiermf/features/EmbeddedEvoMasterController.java).


To programmatically start a _SpringBoot_ application (needed to implement `startSut()`), you can use `SpringApplication.run`,
and save the resulting `ConfigurableApplicationContext` in variable (e.g., `ctx`).
This will be useful when needing to override the methods `isSutRunning()` and `stopSut()`,
as you can just implement them with  `ctx.isRunning()` and `ctx.stop()`.


When starting the SUT, there is one important configuration that you want to change: the binding port, as you want to use 0 for ephemeral ports (to avoid port conflicts).

Note: since version _1.3.0_, there is no longer the need to configure `P6Spy`.
  
For a SUT like `features-service`, this can be done with:

```
ctx = SpringApplication.run(Application.class, new String[]{
                "--server.port=0"                
      });
```      

The actual chosen port can then be extracted with:

```
protected int getSutPort() {
        return (Integer) ((Map) ctx.getEnvironment()
                .getPropertySources().get("server.ports").getSource())
                .get("local.server.port");
}
``` 

Finally, the `startSut()` method must return the URL of where the SUT is listening on.
When running tests locally, this is as simple as returning `"http://localhost:" + getSutPort()`.


Note that you need to make sure you can run your application programmatically, regardless of EvoMaster. A simple way is to check if the following works:

```
public static void main(String[] args) {
    SpringApplication.run(Application.class, args);
}
```

The issue could arise when using `spring-boot-maven-plugin` to start the application, and there are some classpath problems in your application.

## SQL Databases

If the application is using a SQL database, you must configure `getConnection()` and `getDatabaseDriverName()`,
instead of leaving their returned values as `null`.
For example, if you are using _H2_, then the driver name would be `org.h2.Driver`.
In _SpringBoot_, you can extract a connection object in the `startSut()` method (and save it in a variable),
by simply using:

```
JdbcTemplate jdbc = ctx.getBean(JdbcTemplate.class);
connection = jdbc.getDataSource().getConnection();
```

Test cases must be __independent__ from each other.
Otherwise, you could get different results based on their execution order.
To enforce such independence, you must clean the state of the SUT in the `resetStateOfSUT()` method.
In theory, RESTful APIs should be _stateless_.
If indeed stateless, resetting the state would be just a matter of cleaning the database.
For this purpose, we provide the `DbCleaner` utility class 
(used to delete data without recreating the database schema).
There might be some tables that you might not want to clean, like for example if you are using 
_FlyWay_ to handle schema migrations.
These tables can be skipped, for example: 

```
public void resetStateOfSUT() {
   DbCleaner.clearDatabase_H2(connection, Arrays.asList("schema_version"));
}
```

where the content of the table `schema_version` is left untouched.

If your application uses some caches, those might be reset at each test execution.
However, an easier approach could be to just start the SUT without the caches, for example using
the option `--spring.cache.type=NONE`.

Whenever possible, it would be best to use an embedded database such as _H2_.
However, if you need to rely on a specific database such as _Postgres_, we recommend starting
it with _Docker_.  
In Java, this can be done with libraries such as [TestContainers](https://github.com/testcontainers/testcontainers-java/) (which you will need to import in Maven/Gradle).
In your driver, you can then have code like:

```
private static final GenericContainer postgres = new GenericContainer("postgres:9")
            .withExposedPorts(5432)
            .withEnv("POSTGRES_HOST_AUTH_METHOD","trust")
            .withTmpFs(Collections.singletonMap("/var/lib/postgresql/data", "rw"));
```
 
Then, the database can be started in `startSut()` with `postgres.start()`,
and stopped in `stopSut()` with `postgres.stop()`.
Then, the URL to connect to the database can be something like:

```
String host = postgres.getContainerIpAddress();
int port = postgres.getMappedPort(5432);
String url = "jdbc:postgresql://"+host+":"+port+"/postgres"
```

You can then tell Spring to use such URL with the parameter `--spring.datasource.url`.

Note: the `withTmpFs` configuration is very important, and it is database dependent. 
A database running in _Docker_ will still write on your hard-drive, which is an unnecessary,
time-consuming overhead. 
The idea then is to mount the folder, in which the database writes, directly in RAM.   

For an example, you can look at the E2E tests in EvoMaster, like the class [com.foo.spring.rest.postgres.SpringRestPostgresController](https://github.com/EMResearch/EvoMaster/blob/master/e2e-tests/spring-rest-postgres/src/test/kotlin/com/foo/spring/rest/postgres/SpringRestPostgresController.kt). 

## Code Coverage  
 
When _EvoMaster_ evolves test cases, it tries to maximize code coverage in the SUT.
But there is no much point in trying to maximize code coverage of the third-party libraries,
like Spring, Hibernate, Tomcat, etc.
Therefore, in the `getPackagePrefixesToCover()` you need to specify the common package prefix for your
business logic. 
In the case of the `features-service` SUT, this was `org.javiermf.features`.


## REST OpenApi/Swagger Schema

To test a RESTful API, in the the `getProblemInfo()`, you need to return an instance of the
`RestProblem` class.
Here, you need to specify where the _OpenApi_ schema is found, and whether any endpoint should be skipped,
i.e., not generating test cases for.
This latter option is useful for example to skip the _SpringBoot Actuator_ endpoints (if any is present).  
If your RESTful API does not have an _OpenApi/Swagger_ schema, this can be automatically added by using
libraries such as [SpringDoc](https://github.com/springdoc/springdoc-openapi).


## Security

The SUT might require authenticated requests (e.g., when _Spring Security_ is used).
How to do it must be specified in the `getInfoForAuthentication()`.
We support auth based on authentication headers and cookies.
Unfortunately, at the moment we do not support OAuth (we will in the future).

The `org.evomaster.client.java.controller.AuthUtils` can be used to simplify the creation of such
configuration objects, e.g., by using methods like `getForDefaultSpringFormLogin()`.
Consider the following example from the `proxyprint` case study 
in the [EMB repository](https://github.com/EMResearch/EMB).

```
@Override
public List<AuthenticationDto> getInfoForAuthentication() {
        return Arrays.asList(
                AuthUtils.getForBasic("admin","master","1234"),
                AuthUtils.getForBasic("consumer","joao","1234"),
                AuthUtils.getForBasic("manager","joaquim","1234"),
                AuthUtils.getForBasic("employee","mafalda","1234")
        );
}
```

Here, auth is done with [RFC-7617](https://tools.ietf.org/html/rfc7617) _Basic_.
Four different users are defined.
When _EvoMaster_ generates test cases, it can decide to use some of those auth credentials, and
generate the valid HTTP headers for them. 
In case of cookies, _EvoMaster_ is able to first make a login request, store the cookie, and then use such
cookie in the following HTTP calls in its generated tests.   


Although _EvoMaster_ can read and analyze the content of a SQL database, it cannot reverse-engineer the
hashed passwords. 
These must be provided with `getInfoForAuthentication()`.
If such auth info is stored in a SQL database, and you are resetting the state of such database in the
`resetStateOfSUT()` method, you will need there to recreate the login/password credentials as well. 
You could write such auth setup in a `init_db.sql` SQL script file, and then 
in `resetStateOfSUT()` execute:

```
DbCleaner.clearDatabase_H2(connection);
SqlScriptRunnerCached.runScriptFromResourceFile(connection,"/init_db.sql");
```     

Note: at the moment _EvoMaster_ is not able to register new users on the fly with HTTP requests, 
and use such info to authenticate its following requests. 

# Release Procedure

Here it is described the list of steps on how to make a new release for `EvoMaster`.
However, a new release of `EvoMaster` should be done only by the project manager.

In the following, with `???` we refer to secrets (e.g., passwords).

## Pre-requisites

In your local `~/.m2` Maven repository, you need to create a `settings.xml` file with the following:
```
<?xml version="1.0" encoding="UTF-8"?>
<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" 
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
<servers>
<server>
      <id>ossrh</id>
      <username>???</username>
      <password>???</password>
    </server>
</servers>
</settings>
```

The username and password are linked to the account that owns the domain `evomaster.org`.

You need to have installed `GPG` on your machine (used to sign files).
Create a public-private keyset with:
```
gpg --gen-key
```

You can list the generated key with:
```
gpg --list-keys
```

Upload the public key with:
```
gpg --keyserver https://keys.openpgp.org --send-keys  ???
```
where `???` needs to be replaced with the id of the generated public key.
You can also read more details [here](https://central.sonatype.org/pages/working-with-pgp-signatures.html).

## Maven Central Release

The Java client needs to be deployed on Maven Central. 
First, you need to set a new version number for the new release.
We use semantic versioning `x.y.z`: `x` for major releases, `y` for minor releases, and 
`z` for patches.
Note: a patch `z` should never break backward compatibility. Breaking changes should only 
happen in  `x` major releases, and avoided if possible in `y` minor releases.
Recall that if there is a breaking change, still all the SUT drivers in EMB would need to be
updated, manually...  

Given a current snapshot version `0.3.1-SNAPSHOT`, a new release could be `0.4.0`, i.e.,
increase minor version `y` by 1, and reset patch version `z` to 0.

To change the version on all `pom.xml` files at once, from project root folder run:
```
mvn versions:set -DnewVersion=x.y.z
```

where `x.y.z` should be substituted with the actual version number, e.g., `0.4.0`.
However, there are other files besides the pom ones that need to be updated, like for example `makeExecutable.sh`.
So, the update of versions should be done with the `version.py` script. E.g.,
```
py version.py x.y.z
```


From project root  folder, execute:
```
mvn  -N -Pdeployment -DskipTests  deploy
cd client-java
mvn clean -Pdeployment -DskipTests  deploy
```

If everything went well, you should be able to see the deployed files at
[https://oss.sonatype.org/](https://oss.sonatype.org/). 
However, it might take some hours before those are in sync with Maven Central,
which you can check at [https://search.maven.org/](https://search.maven.org/).


## GitHub Release

Push the version changes in the `pom.xml` files on Git.
Build the whole `EvoMaster` from project root folder with:
```
mvn clean package -DskipTests
``` 

Make sure to use `package` and __NOT__ `install` (more on this later).
Furthermore, compilation __MUST__ be done with the _lowest_ JDK version currently
supported in _EvoMaster_.

From the [release](https://github.com/EMResearch/EvoMaster/releases) page
on GitHub, create a new release.
It needs to be tagged, with `v` prefix, e.g., `v0.4.0`.
On GitHub, upload the `core/target/evomaster.jar` executable as part of the release 
(there should be an option for _attaching binaries_).

Update: now we are building `.msi`/`.deb`/`.dmg` files as well, as part of GitHub Action CI. Download those from the release commit, and upload them here in the release page. 

## SNAPSHOT Update

Once `EvoMaster` is released on both Maven Central and GitHub, you need to prepare
the next snapshot version, which will have the same version with `z+1` and suffix
`-SNAPSHOT`, e.g, given `0.4.0`, the following snapshot version would 
be `0.4.1-SNAPSHOT`:
```
py version.py 0.4.1-SNAPSHOT
```



## EMB Release

After completing the release of a new version of `EvoMaster`, you need to make a new
release for [https://github.com/EMResearch/EMB](https://github.com/EMResearch/EMB) as well.
The two repositories __MUST__ have their version numbers aligned.

You need to update all `pom.xml` files with the same:
```
mvn versions:set -DnewVersion=x.y.z
```  

However, there are some other places in which the version number needs to be updated as well:

* in the `<evomaster-version>` variable in the root `pom.xml` project file.
* in the `EVOMASTER_VERSION` variable in the `scripts/dist.py` script file.
* in all the case study `pom.xml` files where the `evomaster-client-database-spy`
dependency is used (until we automate this task with a script, you will need to search
for those dependencies manually from your IDE).

To simplify all these previous steps, you can simply run `./scripts/version.py`.

Once those changes are pushed, create a new [release](https://github.com/EMResearch/EMB/releases) 
on GitHub.
Tag it with the same version as `EvoMaster`, but no need to attach/upload any file.

After this is done, update to a new SNAPSHOT version, by replacing __ALL__ the 
occurrences of the release version in the project (e.g., all `pom.xml` and 
`dist.py` files) by using `./scripts/version.py`.  
However, before doing this, it can be good to test the non-SNAPSHOT version of _EvoMaster_.
The reasoning is to force the downloading of all the dependencies from Maven Central,
to check if anything is missing.
And this is why it was important to build the non-SNAPSHOT with `package` instead of `install`. 


## Example Update

Every time we make a new release, we should also update the examples in [https://github.com/EMResearch/rest-api-example](https://github.com/EMResearch/rest-api-example).
This means:
* increase the dependency version of EM in the pom file
* remove/fix any deprecated function in the implemented driver
* regenerate all the tests, using one of built executables (e.g., `evomaster.exe` on Windows)




# Console Output


While running, _EvoMaster_ will output different info on the console logs, like how much time is remaining before completing generating the tests. 
It will also output other info, like in the following example.

```
* Going to save 1 test to src/em
* Evaluated tests: 11
* Evaluated actions: 83
* Needed budget: 53%
* Passed time (seconds): 33
* Execution time per test (ms): Avg=3013,27 , min=723,00 , max=4992,00
* Computation overhead between tests (ms): Avg=1,70 , min=1,00 , max=5,00
```

Here, at the end of the search, it will tell where the test cases are generated (and how many).
During the search, several tests could be evaluated (`Evaluated tests`), and only the ones that contribute to coverage are going to be part of the final test suite.
Furthermore, a `test` could be composed of 1 or more `actions` (e.g., HTTP calls when testing REST APIs).
This implies `Evaluated actions >= Evaluated tests`.

For how long should the search be left running? 
The info `Needed budget` helps with this question, as it keeps track of the last time there was an improvement in the search (e.g., a new line is covered, or a new fault is discovered).
High values (e.g., `>= 90%`) likely mean that, if you leave _EvoMaster_ running for longer, likely you are going to get better results.

Finally, for statistics, we also report how long it takes to evaluate each test case (`Execution time per test`), and what is the computation overhead between 2 test evaluations (`Computation overhead between tests`).
This latter is keeping track of how much _EvoMaster_ needs to run its algorithms and heuristics before deciding of evaluating a new test. 
# Command-Line Options

_EvoMaster_ has several options that can be configured. 
Those can be set on the command-line when _EvoMaster_ is started.

There are 3 types of options:

* __Important__: those are the main options that most users will need to set
                and know about.

* __Internal__: these are low-level tuning options, which most users do not need
                to modify. These were mainly introduced when experimenting with 
                different configurations to maximize the performance of _EvoMaster_.
                
* __Experimental__: these are work-in-progress options, for features still under development
                    and testing.        
         

 The list of available options can also be displayed by using `--help`, e.g.:

 `java -jar evomaster.jar --help`
  
  Options might also have *constraints*, e.g., a numeric value within a defined range,
  or a string being an URL.
  In some cases, strings might only be chosen within a specific set of possible values (i.e., an Enum).
  If any constraint is not satisfied, _EvoMaster_ will fail with an error message.
  
  When used, all options need to be prefixed with a `--`, e.g., `--maxTime`.
  
## Important Command-Line Options

|Options|Description|
|---|---|
|`maxTime`| __String__. Maximum amount of time allowed for the search.  The time is expressed with a string where hours (`h`), minutes (`m`) and seconds (`s`) can be specified, e.g., `1h10m120s` and `72m` are both valid and equivalent. Each component (i.e., `h`, `m` and `s`) is optional, but at least one must be specified.  In other words, if you need to run the search for just `30` seconds, you can write `30s`  instead of `0h0m30s`. **The more time is allowed, the better results one can expect**. But then of course the test generation will take longer. For how long should _EvoMaster_ be left run? The default 1 _minute_ is just for demonstration. __We recommend to run it between 1 and 24 hours__, depending on the size and complexity  of the tested application. *Constraints*: `regex (\s*)((?=(\S+))(\d+h)?(\d+m)?(\d+s)?)(\s*)`. *Default value*: `60s`.|
|`outputFilePrefix`| __String__. The name prefix of generated file(s) with the test cases, without file type extension. In JVM languages, if the name contains '.', folders will be created to represent the given package structure. Also, in JVM languages, should not use '-' in the file name, as not valid symbol for class identifiers. This prefix be combined with the outputFileSuffix to combined the final name. As EvoMaster can split the generated tests among different files, each will get a label, and the names will be in the form prefix+label+suffix. *Constraints*: `regex [-a-zA-Z$_][-0-9a-zA-Z$_]*(.[-a-zA-Z$_][-0-9a-zA-Z$_]*)*`. *Default value*: `EvoMaster`.|
|`outputFileSuffix`| __String__. The name suffix for the generated file(s), to be added before the file type extension. As EvoMaster can split the generated tests among different files, each will get a label, and the names will be in the form prefix+label+suffix. *Constraints*: `regex [-a-zA-Z$_][-0-9a-zA-Z$_]*(.[-a-zA-Z$_][-0-9a-zA-Z$_]*)*`. *Default value*: `Test`.|
|`outputFolder`| __String__. The path directory of where the generated test classes should be saved to. *Default value*: `src/em`.|
|`outputFormat`| __Enum__. Specify in which format the tests should be outputted. If left on `DEFAULT`, then the value specified in the _EvoMaster Driver_ will be used. But a different value must be chosen if doing Black-Box testing. *Valid values*: `DEFAULT, JAVA_JUNIT_5, JAVA_JUNIT_4, KOTLIN_JUNIT_4, KOTLIN_JUNIT_5, JS_JEST, CSHARP_XUNIT`. *Default value*: `DEFAULT`.|
|`blackBox`| __Boolean__. Use EvoMaster in black-box mode. This does not require an EvoMaster Driver up and running. However, you will need to provide further option to specify how to connect to the SUT. *Default value*: `false`.|
|`bbSwaggerUrl`| __String__. When in black-box mode for REST APIs, specify where the Swagger schema can be downloaded from. *Constraints*: `URL`. *Default value*: `""`.|
|`bbTargetUrl`| __String__. When in black-box mode, specify the URL of where the SUT can be reached. In REST, if this is missing, the URL will be inferred from OpenAPI/Swagger schema. In GraphQL, this will point to the entry point of the API. *Constraints*: `URL`. *Default value*: `""`.|
|`ratePerMinute`| __Int__. Rate limiter, of how many actions to do per minute. For example, when making HTTP calls towards an external service, might want to limit the number of calls to avoid bombarding such service (which could end up becoming equivalent to a DoS attack). A value of zero or negative means that no limiter is applied. This is needed only for black-box testing of remote services. *Default value*: `0`.|
|`header0`| __String__. In black-box testing, we still need to deal with authentication of the HTTP requests. With this parameter it is possible to specify a HTTP header that is going to be added to most requests. This should be provided in the form _name:value_. If more than 1 header is needed, use as well the other options _header1_ and _header2_. *Constraints*: `regex (.+:.+)\|(^$)`. *Default value*: `""`.|
|`header1`| __String__. See documentation of _header0_. *Constraints*: `regex (.+:.+)\|(^$)`. *Default value*: `""`.|
|`header2`| __String__. See documentation of _header0_. *Constraints*: `regex (.+:.+)\|(^$)`. *Default value*: `""`.|

## Internal Command-Line Options

|Options|Description|
|---|---|
|`adaptiveGeneSelectionMethod`| __Enum__. Specify a strategy to select genes for mutation adaptively. *Valid values*: `NONE, AWAY_NOIMPACT, APPROACH_IMPACT, APPROACH_LATEST_IMPACT, APPROACH_LATEST_IMPROVEMENT, BALANCE_IMPACT_NOIMPACT, BALANCE_IMPACT_NOIMPACT_WITH_E, ALL_FIXED_RAND`. *Default value*: `APPROACH_IMPACT`.|
|`algorithm`| __Enum__. The algorithm used to generate test cases. *Valid values*: `MIO, RANDOM, WTS, MOSA`. *Default value*: `MIO`.|
|`appendToStatisticsFile`| __Boolean__. Whether should add to an existing statistics file, instead of replacing it. *Default value*: `false`.|
|`archiveGeneMutation`| __Enum__. Whether to enable archive-based gene mutation. *Valid values*: `NONE, SPECIFIED, SPECIFIED_WITH_TARGETS, SPECIFIED_WITH_SPECIFIC_TARGETS, SPECIFIED_WITH_TARGETS_DIRECTION, SPECIFIED_WITH_SPECIFIC_TARGETS_DIRECTION, ADAPTIVE`. *Default value*: `SPECIFIED_WITH_SPECIFIC_TARGETS`.|
|`archiveTargetLimit`| __Int__. Limit of number of individuals per target to keep in the archive. *Constraints*: `min=1.0`. *Default value*: `10`.|
|`avoidNonDeterministicLogs`| __Boolean__. At times, we need to run EvoMaster with printed logs that are deterministic. For example, this means avoiding printing out time-stamps. *Default value*: `false`.|
|`baseTaintAnalysisProbability`| __Double__. Probability to use input tracking (i.e., a simple base form of taint-analysis) to determine how inputs are used in the SUT. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.9`.|
|`bbExperiments`| __Boolean__. Only used when running experiments for black-box mode, where an EvoMaster Driver would be present, and can reset state after each experiment. *Default value*: `false`.|
|`bloatControlForSecondaryObjective`| __Boolean__. Whether secondary objectives are less important than test bloat control. *Default value*: `false`.|
|`coveredTargetFile`| __String__. Specify a file which saves covered targets info regarding generated test suite. *Default value*: `coveredTargets.txt`.|
|`createTests`| __Boolean__. Specify if test classes should be created as output of the tool. Usually, you would put it to 'false' only when debugging EvoMaster itself. *Default value*: `true`.|
|`customNaming`| __Boolean__. Enable custom naming and sorting criteria. *Default value*: `true`.|
|`d`| __Double__. When weight-based mutation rate is enabled, specify a percentage of calculating mutation rate based on a number of candidate genes to mutate. For instance, d = 1.0 means that the mutation rate fully depends on a number of candidate genes to mutate, and d = 0.0 means that the mutation rate fully depends on weights of candidates genes to mutate. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.8`.|
|`doesApplyNameMatching`| __Boolean__. Whether to apply text/name analysis to derive relationships between name entities, e.g., a resource identifier with a name of table. *Default value*: `true`.|
|`e_u1f984`| __Boolean__. QWN0aXZhdGUgdGhlIFVuaWNvcm4gTW9kZQ==. *Default value*: `false`.|
|`enableBasicAssertions`| __Boolean__. Generate basic assertions. Basic assertions (comparing the returned object to itself) are added to the code. NOTE: this should not cause any tests to fail. *Default value*: `true`.|
|`enableTrackEvaluatedIndividual`| __Boolean__. Whether to enable tracking the history of modifications of the individuals with its fitness values (i.e., evaluated individual) during the search. Note that we enforced that set enableTrackIndividual false when enableTrackEvaluatedIndividual is true since information of individual is part of evaluated individual. *Default value*: `true`.|
|`enableWeightBasedMutationRateSelectionForGene`| __Boolean__. Specify whether to enable weight-based mutation selection for selecting genes to mutate for a gene. *Default value*: `true`.|
|`endNumberOfMutations`| __Int__. Number of applied mutations on sampled individuals, by the end of the search. *Constraints*: `min=0.0`. *Default value*: `10`.|
|`endpointFocus`| __String__. Only for debugging. Concentrate search on only one single REST endpoint. *Default value*: `null`.|
|`executiveSummary`| __Boolean__. Generate an executive summary, containing an example of each category of potential fault found.NOTE: This option is only meaningful when used in conjuction with clustering. This is achieved by turning the option --testSuiteSplitType to CLUSTER. *Default value*: `true`.|
|`expandRestIndividuals`| __Boolean__. Enable to expand the genotype of REST individuals based on runtime information missing from Swagger. *Default value*: `true`.|
|`expectationsActive`| __Boolean__. Enable Expectation Generation. If enabled, expectations will be generated. A variable called expectationsMasterSwitch is added to the test suite, with a default value of false. If set to true, an expectation that fails will cause the test case containing it to fail. *Default value*: `true`.|
|`exportCoveredTarget`| __Boolean__. Specify whether to export covered targets info. *Default value*: `false`.|
|`extraHeuristicsFile`| __String__. Where the extra heuristics file (if any) is going to be written (in CSV format). *Default value*: `extra_heuristics.csv`.|
|`extractSqlExecutionInfo`| __Boolean__. Enable extracting SQL execution info. *Default value*: `true`.|
|`feedbackDirectedSampling`| __Enum__. Specify whether when we sample from archive we do look at the most promising targets for which we have had a recent improvement. *Valid values*: `NONE, LAST, FOCUSED_QUICKEST`. *Default value*: `LAST`.|
|`focusedSearchActivationTime`| __Double__. The percentage of passed search before starting a more focused, less exploratory one. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`geneMutationStrategy`| __Enum__. Strategy used to define the mutation probability. *Valid values*: `ONE_OVER_N, ONE_OVER_N_BIASED_SQL`. *Default value*: `ONE_OVER_N_BIASED_SQL`.|
|`geneWeightBasedOnImpactsBy`| __Enum__. Specify a strategy to calculate a weight of a gene based on impacts. *Valid values*: `SORT_COUNTER, SORT_RATIO, COUNTER, RATIO`. *Default value*: `RATIO`.|
|`generateSqlDataWithSearch`| __Boolean__. Enable EvoMaster to generate SQL data with direct accesses to the database. Use a search algorithm. *Default value*: `true`.|
|`heuristicsForSQL`| __Boolean__. Tracking of SQL commands to improve test generation. *Default value*: `true`.|
|`jsControllerPath`| __String__. When generating tests in JavaScript, there is the need to know where the driver is located in respect to the generated tests. *Default value*: `./app-driver.js`.|
|`killSwitch`| __Boolean__. Try to enforce the stopping of SUT business-level code. This is needed when TCP connections timeouts, to avoid thread executions from previous HTTP calls affecting the current one. *Default value*: `true`.|
|`maxActionEvaluations`| __Int__. Maximum number of action evaluations for the search. A fitness evaluation can be composed of 1 or more actions, like for example REST calls or SQL setups. The more actions are allowed, the better results one can expect. But then of course the test generation will take longer. Only applicable depending on the stopping criterion. *Constraints*: `min=1.0`. *Default value*: `1000`.|
|`maxResponseByteSize`| __Int__. Maximum size (in bytes) that EM handles response payloads in the HTTP responses. If larger than that, a response will not be stored internally in EM during the test generation. This is needed to avoid running out of memory. *Default value*: `1000000`.|
|`maxSearchSuiteSize`| __Int__. Define the maximum number of tests in a suite in the search algorithms that evolve whole suites, e.g. WTS. *Constraints*: `min=1.0`. *Default value*: `50`.|
|`maxSqlInitActionsPerMissingData`| __Int__. When generating SQL data, how many new rows (max) to generate for each specific SQL Select. *Constraints*: `min=1.0`. *Default value*: `5`.|
|`maxTestSize`| __Int__. Max number of 'actions' (e.g., RESTful calls or SQL commands) that can be done in a single test. *Constraints*: `min=1.0`. *Default value*: `10`.|
|`maxTimeInSeconds`| __Int__. Maximum number of seconds allowed for the search. The more time is allowed, the better results one can expect. But then of course the test generation will take longer. Only applicable depending on the stopping criterion. If this value is 0, the setting 'maxTime' will be used instead. *Constraints*: `min=0.0`. *Default value*: `0`.|
|`maxlengthOfHistoryForAGM`| __Int__. Specify a maximum length of history when applying archive-based gene mutation. *Default value*: `10`.|
|`minimumSizeControl`| __Int__. Specify minimum size when bloatControlForSecondaryObjective. *Constraints*: `min=0.0`. *Default value*: `2`.|
|`populationSize`| __Int__. Define the population size in the search algorithms that use populations (e.g., Genetic Algorithms, but not MIO). *Constraints*: `min=1.0`. *Default value*: `30`.|
|`probOfApplySQLActionToCreateResources`| __Double__. Specify a probability to apply SQL actions for preparing resources for REST Action. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`probOfArchiveMutation`| __Double__. Specify a probability to enable archive-based mutation. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`probOfEnablingResourceDependencyHeuristics`| __Double__. Specify whether to enable resource dependency heuristics, i.e, probOfEnablingResourceDependencyHeuristics > 0.0. Note that the option is available to be enabled only if resource-based smart sampling is enable. This option has an effect on sampling multiple resources and mutating a structure of an individual. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.95`.|
|`probOfRandomSampling`| __Double__. Probability of sampling a new individual at random. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`probOfSmartSampling`| __Double__. When sampling new test cases to evaluate, probability of using some smart strategy instead of plain random. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.95`.|
|`problemType`| __Enum__. The type of SUT we want to generate tests for, e.g., a RESTful API. If left to DEFAULT, the type will be inferred from the EM Driver. However, in case of ambiguities (e.g., the driver specifies more than one type), then this field must be set with a specific type. This is also the case for Black-Box testing where there is no EM Driver. In this latter case, the system defaults to handle REST APIs. *Valid values*: `DEFAULT, REST`. *Experimental values*: `GRAPHQL, RPC, WEB`. *Default value*: `DEFAULT`.|
|`resourceSampleStrategy`| __Enum__. Specify whether to enable resource-based strategy to sample an individual during search. Note that resource-based sampling is only applicable for REST problem with MIO algorithm. *Valid values*: `NONE, Customized, EqualProbability, Actions, TimeBudgets, Archive, ConArchive`. *Default value*: `ConArchive`.|
|`secondaryObjectiveStrategy`| __Enum__. Strategy used to handle the extra heuristics in the secondary objectives. *Valid values*: `AVG_DISTANCE, AVG_DISTANCE_SAME_N_ACTIONS, BEST_MIN`. *Default value*: `AVG_DISTANCE_SAME_N_ACTIONS`.|
|`seed`| __Long__. The seed for the random generator used during the search. A negative value means the CPU clock time will be rather used as seed. *Default value*: `-1`.|
|`showProgress`| __Boolean__. Whether to print how much search done so far. *Default value*: `true`.|
|`skipFailureSQLInTestFile`| __Boolean__. Whether to skip failed SQL commands in the generated test files. *Default value*: `true`.|
|`snapshotInterval`| __Double__. If positive, check how often, in percentage % of the budget, to collect statistics snapshots. For example, every 5% of the time. *Constraints*: `max=50.0`. *Default value*: `-1.0`.|
|`snapshotStatisticsFile`| __String__. Where the snapshot file (if any) is going to be written (in CSV format). *Default value*: `snapshot.csv`.|
|`specializeSQLGeneSelection`| __Boolean__. Whether to specialize sql gene selection to mutation. *Default value*: `true`.|
|`startNumberOfMutations`| __Int__. Number of applied mutations on sampled individuals, at the start of the search. *Constraints*: `min=0.0`. *Default value*: `1`.|
|`statisticsColumnId`| __String__. An id that will be part as a column of the statistics file (if any is generated). *Default value*: `-`.|
|`statisticsFile`| __String__. Where the statistics file (if any) is going to be written (in CSV format). *Default value*: `statistics.csv`.|
|`stoppingCriterion`| __Enum__. Stopping criterion for the search. *Valid values*: `TIME, FITNESS_EVALUATIONS`. *Default value*: `TIME`.|
|`structureMutationProbability`| __Double__. Probability of applying a mutation that can change the structure of a test. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`sutControllerHost`| __String__. Host name or IP address of where the SUT REST controller is listening on. *Default value*: `localhost`.|
|`sutControllerPort`| __Int__. TCP port of where the SUT REST controller is listening on. *Constraints*: `min=0.0, max=65535.0`. *Default value*: `40100`.|
|`testSuiteFileName`| __String__. DEPRECATED. Rather use _outputFilePrefix_ and _outputFileSuffix_. *Default value*: `""`.|
|`testSuiteSplitType`| __Enum__. Instead of generating a single test file, it could be split in several files, according to different strategies. *Valid values*: `NONE, CLUSTER, CODE`. *Default value*: `CLUSTER`.|
|`tournamentSize`| __Int__. Number of elements to consider in a Tournament Selection (if any is used in the search algorithm). *Constraints*: `min=1.0`. *Default value*: `10`.|
|`useMethodReplacement`| __Boolean__. Apply method replacement heuristics to smooth the search landscape. *Default value*: `true`.|
|`useNonIntegerReplacement`| __Boolean__. Apply non-integer numeric comparison heuristics to smooth the search landscape. *Default value*: `true`.|
|`useTimeInFeedbackSampling`| __Boolean__. Whether to use timestamp info on the execution time of the tests for sampling (e.g., to reward the quickest ones). *Default value*: `true`.|
|`weightBasedMutationRate`| __Boolean__. Whether to enable a weight-based mutation rate. *Default value*: `true`.|
|`writeExtraHeuristicsFile`| __Boolean__. Whether we should collect data on the extra heuristics. Only needed for experiments. *Default value*: `false`.|
|`writeStatistics`| __Boolean__. Whether or not writing statistics of the search process. This is only needed when running experiments with different parameter settings. *Default value*: `false`.|
|`xoverProbability`| __Double__. Probability of applying crossover operation (if any is used in the search algorithm). *Constraints*: `probability 0.0-1.0`. *Default value*: `0.7`.|

## Experimental Command-Line Options

|Options|Description|
|---|---|
|`S1dR`| __Double__. Specify a probability to apply S1dR when resource sampling strategy is 'Customized'. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.25`.|
|`S1iR`| __Double__. Specify a probability to apply S1iR when resource sampling strategy is 'Customized'. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.25`.|
|`S2dR`| __Double__. Specify a probability to apply S2dR when resource sampling strategy is 'Customized'. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.25`.|
|`SMdR`| __Double__. Specify a probability to apply SMdR when resource sampling strategy is 'Customized'. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.25`.|
|`abstractInitializationGeneToMutate`| __Boolean__. During mutation, whether to abstract genes for repeated SQL actions. *Default value*: `false`.|
|`archiveAfterMutationFile`| __String__. Specify a path to save archive after each mutation during search, only useful for debugging. *Default value*: `archive.csv`.|
|`coveredTargetSortedBy`| __Enum__. Specify a format to organize the covered targets by the search. *Valid values*: `NAME, TEST`. *Default value*: `NAME`.|
|`dependencyFile`| __String__. Specify a file that saves derived dependencies. *Default value*: `dependencies.csv`.|
|`doCollectImpact`| __Boolean__. Specify whether to collect impact info that provides an option to enable of collecting impact info when archive-based gene selection is disable. *Default value*: `false`.|
|`dpcTargetTestSize`| __Int__. Specify a max size of a test to be targeted when either DPC_INCREASING or DPC_DECREASING is enabeld. *Default value*: `1`.|
|`employResourceSizeHandlingStrategy`| __Enum__. Specify a strategy to determinate a number of resources to be manipulated throughout the search. *Valid values*: `NONE, RANDOM, DPC`. *Default value*: `NONE`.|
|`enableAdaptiveResourceStructureMutation`| __Boolean__. Specify whether to decide the resource-based structure mutator and resource to be mutated adaptively based on impacts during focused search.Note that it only works when resource-based solution is enabled for solving REST problem. *Default value*: `false`.|
|`enableNLPParser`| __Boolean__. Whether to employ NLP parser to process text. Note that to enable this parser, it is required to build the EvoMaster with the resource profile, i.e., mvn clean install -Presourceexp -DskipTests. *Default value*: `false`.|
|`enableProcessMonitor`| __Boolean__. Whether or not enable a search process monitor for archiving evaluated individuals and Archive regarding an evaluation of search. This is only needed when running experiments with different parameter settings. *Default value*: `false`.|
|`enablePureRPCTestGeneration`| __Boolean__. Whether to generate RPC endpoint invocation which is independent from EM driver. *Default value*: `false`.|
|`enableRPCAssertionWithInstance`| __Boolean__. Whether to generate RPC Assertions based on response instance. *Default value*: `false`.|
|`enableRPCCustomizedResponseTargets`| __Boolean__. Whether to enable customized responses indicating business logic. *Default value*: `false`.|
|`enableRPCExtraResponseTargets`| __Boolean__. Whether to enable extra targets for responses, e.g., regarding nullable response, having extra targets for whether it is null. *Default value*: `false`.|
|`enableTrackIndividual`| __Boolean__. Whether to enable tracking the history of modifications of the individuals during the search. *Default value*: `false`.|
|`enableWriteSnapshotTests`| __Boolean__. Enable to print snapshots of the generated tests during the search in an interval defined in snapshotsInterval. *Default value*: `false`.|
|`errorTextEpsilon`| __Double__. The Distance Metric Error Text may use several values for epsilon.During experimentation, it may be useful to adjust these values. Epsilon describes the size of the neighbourhood used for clustering, so may result in different clustering results.Epsilon should be between 0.0 and 1.0. If the value is outside of that range, epsilon will use the default of 0.8. *Constraints*: `min=0.0, max=1.0`. *Default value*: `0.8`.|
|`exceedTargetsFile`| __String__. Specify a path to save all not covered targets when the number is more than 100. *Default value*: `exceedTargets.txt`.|
|`exportDependencies`| __Boolean__. Specify whether to export derived dependencies among resources. *Default value*: `false`.|
|`exportImpacts`| __Boolean__. Specify whether to export derived impacts among genes. *Default value*: `false`.|
|`generateSqlDataWithDSE`| __Boolean__. Enable EvoMaster to generate SQL data with direct accesses to the database. Use Dynamic Symbolic Execution. *Default value*: `false`.|
|`impactAfterMutationFile`| __String__. Specify a path to save collected impact info after each mutation during search, only useful for debugging. *Default value*: `impactSnapshot.csv`.|
|`impactFile`| __String__. Specify a path to save derived genes. *Default value*: `impact.csv`.|
|`initStructureMutationProbability`| __Double__. Probability of applying a mutation that can change the structure of test's initialization if it has. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.0`.|
|`lastLineEpsilon`| __Double__. The Distance Metric Last Line may use several values for epsilon.During experimentation, it may be useful to adjust these values. Epsilon describes the size of the neighbourhood used for clustering, so may result in different clustering results.Epsilon should be between 0.0 and 1.0. If the value is outside of that range, epsilon will use the default of 0.8. *Constraints*: `min=0.0, max=1.0`. *Default value*: `0.8`.|
|`maxAssertionForDataInCollection`| __Int__. Specify a maximum number of data in a collection to be asserted in generated tests.Note that zero means tht only size of the collection would be asserted and a negative value means all data in the collection should be asserted. *Default value*: `-1`.|
|`maxLengthOfTraces`| __Int__. Specify a maxLength of tracking when enableTrackIndividual or enableTrackEvaluatedIndividual is true. Note that the value should be specified with a non-negative number or -1 (for tracking all history). *Constraints*: `min=-1.0`. *Default value*: `10`.|
|`maxResourceSize`| __Int__. Specify a max size of resources in a test. 0 means the there is no specified restriction on a number of resources. *Constraints*: `min=0.0`. *Default value*: `0`.|
|`maxSizeOfHandlingResource`| __Int__. Specify a maximum number of handling (remove/add) resource size at once, e.g., add 3 resource at most. *Constraints*: `min=0.0`. *Default value*: `0`.|
|`maxSizeOfMutatingInitAction`| __Int__. Specify a maximum number of handling (remove/add) init actions at once, e.g., add 3 init actions at most. *Constraints*: `min=0.0`. *Default value*: `0`.|
|`maxTestSizeStrategy`| __Enum__. Specify a strategy to handle a max size of a test. *Valid values*: `SPECIFIED, DPC_INCREASING, DPC_DECREASING`. *Default value*: `SPECIFIED`.|
|`maximumExistingDataToSampleInDb`| __Int__. Specify a maximum number of existing data in the database to sample when SQL handling is enabled. Note that a negative number means all existing data would be sampled. *Default value*: `-1`.|
|`minRowOfTable`| __Int__. Specify a minimal number of rows in a table that enables selection (i.e., SELECT sql) to prepare resources for REST Action. In other word, if the number is less than the specified, insertion is always applied. *Constraints*: `min=0.0`. *Default value*: `10`.|
|`mutatedGeneFile`| __String__. Specify a path to save mutation details which is useful for debugging mutation. *Default value*: `mutatedGeneInfo.csv`.|
|`mutationTargetsSelectionStrategy`| __Enum__. Specify a strategy to select targets for evaluating mutation. *Valid values*: `FIRST_NOT_COVERED_TARGET, EXPANDED_UPDATED_NOT_COVERED_TARGET, UPDATED_NOT_COVERED_TARGET`. *Default value*: `FIRST_NOT_COVERED_TARGET`.|
|`outputExecutedSQL`| __Enum__. Whether to output executed sql info. *Valid values*: `NONE, ALL_AT_END, ONCE_EXECUTED`. *Default value*: `NONE`.|
|`probOfHandlingLength`| __Double__. Specify a probability of applying length handling. *Default value*: `0.0`.|
|`probOfSelectFromDatabase`| __Double__. Specify a probability that enables selection (i.e., SELECT sql) of data from database instead of insertion (i.e., INSERT sql) for preparing resources for REST actions. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.1`.|
|`probOfSmartInitStructureMutator`| __Double__. Specify a probability of applying a smart structure mutator for initialization of the individual. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.0`.|
|`processFiles`| __String__. Specify a folder to save results when a search monitor is enabled. *Default value*: `process_data`.|
|`processFormat`| __Enum__. Specify a format to save the process data. *Valid values*: `JSON_ALL, TEST_IND, TARGET_TEST_IND`. *Default value*: `JSON_ALL`.|
|`processInterval`| __Double__. Specify how often to save results when a search monitor is enabled, and 0.0 presents to record all evaluated individual. *Constraints*: `min=0.0, max=50.0`. *Default value*: `0.0`.|
|`recordExceededTargets`| __Boolean__. Whether to record targets when the number is more than 100. *Default value*: `false`.|
|`saveArchiveAfterMutation`| __Boolean__. Whether to save archive info after each of mutation, which is typically useful for debugging mutation and archive. *Default value*: `false`.|
|`saveExecutedSQLToFile`| __String__. Specify a path to save all executed sql commands to a file (default is 'sql.txt'). *Default value*: `sql.txt`.|
|`saveImpactAfterMutation`| __Boolean__. Whether to save impact info after each of mutation, which is typically useful debugging impact driven solutions and mutation. *Default value*: `false`.|
|`saveMutationInfo`| __Boolean__. Whether to save mutated gene info, which is typically used for debugging mutation. *Default value*: `false`.|
|`seedTestCases`| __Boolean__. Whether to seed EvoMaster with some initial test cases. These test cases will be used and evolved throughout the search process. *Default value*: `false`.|
|`seedTestCasesFormat`| __Enum__. Format of the test cases seeded to EvoMaster. *Valid values*: `POSTMAN`. *Default value*: `POSTMAN`.|
|`seedTestCasesPath`| __String__. File path where the seeded test cases are located. *Default value*: `postman.postman_collection.json`.|
|`startingPerOfGenesToMutate`| __Double__. Specify a starting percentage of genes of an individual to mutate. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.5`.|
|`structureMutationProFS`| __Double__. Specify a probability of applying structure mutator during the focused search. *Constraints*: `probability 0.0-1.0`. *Default value*: `0.0`.|
|`structureMutationProbStrategy`| __Enum__. Specify a strategy to handle a probability of applying structure mutator during the focused search. *Valid values*: `SPECIFIED, SPECIFIED_FS, DPC_TO_SPECIFIED_BEFORE_FS, DPC_TO_SPECIFIED_AFTER_FS, ADAPTIVE_WITH_IMPACT`. *Default value*: `SPECIFIED`.|
|`treeDepth`| __Int__. Maximum tree depth in mutations/queries to be evaluated;this is to avoid issues when dealing with huge graphs in GraphQL. *Constraints*: `min=1.0`. *Default value*: `11`.|
|`useWeightedSampling`| __Boolean__. When sampling from archive based on targets, decide whether to use weights based on properties of the targets (e.g., a target likely leading to a flag will be sampled less often). *Default value*: `false`.|
|`writeSnapshotTestsIntervalInSeconds`| __Int__. The size (in seconds) of the interval that the snapshots will be printed, if enabled. *Default value*: `3600`.|
# Black-Box Testing

Informally, in *Black-Box Testing* of a RESTful API, we generate test cases without
knowing the internal details of the API.
Still, we need to know the schema of the API to determine which endpoints can be called,
and how.
Otherwise, sending random bits on a TCP socket will unlikely result in any meaningful, 
well formatted HTTP message.

Even with a schema, there is still the issue of generating inputs, like HTTP query parameters
and body payloads (e.g., in JSON/XML).
This can be done at random, but still within the constraints defined in the schema (e.g., the 
structure of the body payloads). 


To do this kind of testing, there is only the need to have an API up and running, and specify
where the schema can be found.
For example, [https://apis.guru](https://apis.guru/) lists many APIs online. 
Such website provides an API itself to query info on existing APIs.
Such small API (only 2 endpoints) can be easily tested by running the following on a command-line: 

```
java -jar core/target/evomaster.jar  --blackBox true --bbSwaggerUrl https://api.apis.guru/v2/openapi.yaml  --outputFormat JAVA_JUNIT_4 --maxTime 30s --ratePerMinute 60
```

The command is doing the following:

* `java -jar core/target/evomaster.jar`: execute the _EvoMaster_ core process. 
  The executable `evomaster.jar` must be either [downloaded](download.md) 
  or [built from source](build.md).
* `--blackBox true`: by default, _EvoMaster_ does white-box testing. Here, we specify that
  we do black-box testing instead.
* `--bbSwaggerUrl ...`: URL of where the Swagger schema is. If the API is running on a different
  host, then such different host would need to be specified with `--bbTargetUrl`.   
* `--outputFormat JAVA_JUNIT_4`: must specify how the tests will be generated, e.g., in Java
  using JUnit 4 in this case. Note: the language of the generated tests is not necessarily related
  to the language in which the tested application is implemented. 
* `--maxTime 30s`: for how long to run the search, i.e., just 30 seconds in this very simple example.
* `--ratePerMinute 60`: avoid doing a DoS attack by bombarding the remote service with too many HTTP calls in quick rapid succession. Limit to max 1 per second (i.e., 60 per minute) in this example.

This command will create the following test suite, in which 2 `GET` calls are executed:

```
public class EvoMasterTest {

    private static String baseUrlOfSut = "https://api.apis.guru";
    
    @BeforeClass
    public static void initClass() {
        RestAssured.enableLoggingOfRequestAndResponseIfValidationFails();
        RestAssured.useRelaxedHTTPSValidation();
        RestAssured.urlEncodingEnabled = false;
        RestAssured.config = RestAssured.config()
            .jsonConfig(JsonConfig.jsonConfig().numberReturnType(JsonPathConfig.NumberReturnType.DOUBLE))
            .redirect(redirectConfig().followRedirects(false));
    }

    
    @Test
    public void test_0() throws Exception {
        
        given().accept("application/json")
                .get(baseUrlOfSut + "/v2/list.json")
                .then()
                .statusCode(200)
                .assertThat()
                .contentType("application/json")
                .body("size()", numberMatches(1605));
    }
    
    
    @Test
    public void test_1() throws Exception {
        
        given().accept("application/json")
                .get(baseUrlOfSut + "/v2/metrics.json")
                .then()
                .statusCode(200)
                .assertThat()
                .contentType("application/json")
                .body("'numAPIs'", numberMatches(1605.0))
                .body("'numEndpoints'", numberMatches(46276.0))
                .body("'numSpecs'", numberMatches(2869.0));
    }
}
```


## AUTH

Since version `1.3.0`, it is possible to specify custom HTTP headers (e.g., to pass auth tokens), using the options from `--header0` to `--header2` (in case more than one HTTP header is needed). 

## WARNING

Black-box testing is easy to do: first download the tool, and then just specify where the Swagger schema
can be found.
However, as it does know nothing about the internal details of the tested application, it is 
unlikely that it will get good results (i.e., in terms of code coverage and detected faults).

The first time you try _EvoMaster_, use black-box testing to get an idea of what _EvoMaster_
could do for you.
However, after an initial trial, we recommend to switch to [white-box testing](whitebox.md),
as it can lead to much, much better results.
# Re-used Code

In some cases, _EvoMaster_ imported (and then modified) code from other open-source projects.
When this happened, it is explicitly stated in the files themselves. 
Still, to give a general overview (and comply to the different licence requirements), they
are listed here:

* _ComputeClassWriter.java_: from [ASM](https://asm.ow2.io/) library. Released under custom INRIA license.

* _RegexEcma262.g4_: from [Antlr examples](https://github.com/antlr/grammars-v4/blob/master/ecmascript/ECMAScript.g4).
  Released under MIT license.
    
* _RegexDistanceUtils.java_: from [EvoSuite](http://www.evosuite.org) unit test generator. 
  Released under GNU Lesser General Public
  
* _RegexDistanceUtilsTest.java_: from [EvoSuite](http://www.evosuite.org) unit test generator. 
  Released under GNU Lesser General Public# Library Dependencies

When *EvoMaster* generates test cases, those test cases can require 
to import some libraries. 
This depends on the target programming language, and on the kind of tests
that are generated. 

When doing white-box testing, you are also required to import the
*EvoMaster-Client* library (as discussed in the documentation for writing 
[white-box drivers](write_driver.md)).
However, such library does **NOT** import the others transitively.
This is done on purpose, as to avoid nasty version mismatching if those
libraries are already used in the application you want to test.
You need to add them manually in your build scripts. 

One possible negative side-effect of this approach is that the generated
tests might not be compatible with the version of the library you
are using.
This should be rare, but it might happen if we were using a deprecated API, and a new version
of the library did remove it. 
As a rule of thumb, we try to update *EvoMaster* to the most recent
versions of those libraries.
If you see such problems, please report them on the issue page.


## REST APIs

### JVM (e.g., Java and Kotlin)    

When generating test cases for REST APIs written in Java/Kotlin,
*EvoMaster* relies on the [RestAssured](https://github.com/rest-assured/rest-assured)
library.

In Maven, this can be imported with:
```
<dependency>
     <groupId>io.rest-assured</groupId>
     <artifactId>rest-assured</artifactId>
     <version>4.3.0</version>
     <scope>test</scope>
</dependency>
```
Recall to check out for recent versions of this library.

A word of advice if you are using Spring: Spring imports transitively
several libraries, including old versions of JUnit and RestAssured
as well. This can mess up the use of RestAssured if you are using 
a different major version compared to the one imported by Spring.
However, it is not too hard to [fix](https://stackoverflow.com/questions/44993615/java-lang-noclassdeffounderror-io-restassured-mapper-factory-gsonobjectmapperfa). 


# NOTES FOR DEVELOPERS


These notes are meant for developers working on _EvoMaster_, and for people making a pull request.
There are several rules of thumb regarding how to write "good code",
but often rules are either too generic and not tailored for a given particular
piece of software (e.g., different kinds of architectures).

The rules of thumb described here in this document are not meant to be either exhaustive nor absolute.
Rigid rules are not substitute for common sense, as they are rather guidelines that can
be ignored in some special cases.
Furthermore, the guidelines need to be _realistic_ and easy to use: there would be no point
to ask for detailed comments on each single method/field and 100% coverage test suites...

These notes also include some explanations and motivations for some of the architectural choices
made in the development of _EvoMaster_.


### Kotlin vs. Java
The core process of _EvoMaster_ is built in _Kotlin_, as we strongly prefer it over _Java_.
However, the client libraries for JDK SUTs (e.g., not just _Java_, but also all other languages that do 
compile to JDK bytecode) are written in _Java_ instead of _Kotlin_.
The main reason is that, being libraries, we do not want to also have to ship the _Kotlin_ runtime 
libraries with them. 


### BRACES { AND SPACES vs. TABS 

Not going to start an holy war here... once made a choice, we just keep it consistent throughout the whole project. 
Regarding opening braces {, the ancient texts state that those shall be on the same line of the code, and not in their own line.
Therefore, the following is right(eous):
```
foo{
}
```
whereas the following blasphemy must be expunged:
```
foo
{
}
```

Note that, at the current moment, we do not use an automated-formatter as part of the build (for several reasons...).

Regarding spaces vs. tabs, I have no idea what is in use. This should be automatically handled by your IDE (and if it doesn't, switch to a better IDE).



### AVOID `System.out` AND `System.err`
_EvoMaster_ uses a logging framework.
For debugging  and logging errors in a class `Foo`, create a logger in the following way.

* for Java: `private static Logger log = LoggerFactory.getLogger(Foo.class);`
* for Kotlin: `companion object { private val log: Logger = LoggerFactory.getLogger(Foo::class.java)}`

It is important to keep the same name `log` to make things consistent among different classes.
If the logging should be part the actual output for the console user, then rather use: 

`LoggingUtil.getInfoLogger()`


### AVOID `String` CONCATENATION IN LOGGERS

Writing something like:

`log.debug("this is not "+ foo + " very " + bar +" efficient")`

is not efficient, as most of the time debug logs are deactivated, and concatenating strings is
expensive. Recall `String` is immutable, and each `+` does create a new `String` object.
The above logging can be rewritten into:

`log.debug("this is not {} very {} efficient", foo, bar)`

Note: not a big deal for _warn_/_error_, as those are/should be rare... but it can become
quite an overhead for _trace_/_debug_/_info_.




### DO NOT USE `System.exit`

Better to throw an exception, as the entry point of _EvoMaster_ does some logging when ends.
Furthermore, `System.exit` becomes problematic when unit testing _EvoMaster_.



### STATIC VARIABLES ARE YOUR ENEMY

Static variables should be either constant or representing transient data (e.g., cache information 
whose presence/missing has only effect on performance, not on functionality).
Having "classes with static state" is usually a poor OO design (an exception to this rule 
is `ExecutionTracer`).
If those are really needed, then you should rather use an _injectable_ singleton service (see next point). 
This is not just to be pedantic, but, really, non-constant static variables make unit testing 
far much harder and lead to code that is more difficult to understand and maintain. 


### `Guice` and `Governator`

To avoid issues with mutable static variables, we use a dependency injection framework.
In particular, we use `Guice`, extended with `Governator` to handle post-construct events.
All injectable services should be singletons, and declared under a package called `*.service` (this
is to make it easy to find out which services are available).

There is no auto-discovery of beans. This is done manually.
The reason is that, depending on configurations, we can have many different context initializations.
For example, the beans used for testing REST APIs would not be needed when testing GraphQL ones.  


### HOW TO WRITE UNIT TEST CASES

Unit tests should be put in the `src/test/java` and `src/test/kotlin` folders, 
following the same package structure as _EvoMaster_ code.
A unit test suite for SUT `org.evomaster.somepackage.Foo` __MUST__ be called `org.evomaster.somepackage.FooTest`.
This is important for several reasons:
- Need to know what class the test case is supposed to unit test by just looking at its name
- Should be easy to identify if a class has a test suite for it
- If in same package, then the test suite can access package/protected fields/methods
- Having `Test` as postfix (instead of a prefix) is useful for when searching for classes by name
- A `Test` postfix is a requirement for _Maven_ to execute the test suite during the build 


### HOW TO WRITE END-TO-END (E2E) TEST CASES

Besides unit tests, it is essential to have E2E ones as well.
Those should be added under the `e2e-tests` module. 
Being _non-deterministic_, we cannot guarantee that _EvoMaster_ can always find a valid solution (e.g., 
create test cases with certain properties).
Furthermore, we cannot run the E2E tests for long time (otherwise the CI builds will take forever).
The idea is to create artificial SUTs that should be _trivial_ to solve when some settings (which we want
to test) are on, and very difficult (if not straight-out infeasible) otherwise.

Note: current version of JUnit 5 is worse than JUnit 4 when dealing with E2E tests.
E.g., there is no handling of _flaky_ tests (in JUnit 4, this was handled by the _Surefire_/_Failsafe_ plugins).
This is the reason why such test executions should be wrapped inside a `handleFlaky` call.   

Also notice that, for JavaScript and C#, E2E tests are different, as run through bash scripts.
This is due to the fact that we have to run 2 separate processes using different technologies (e.g., JVM vs. .Net and NodeJS). 

### AVOID TOO LONG METHODS

Too long methods (e.g., more than 100 lines) should be split, as difficult to understand.
For this task, in _IntelliJ_, you can right-click on a code snippet and choose 
"_Refactor -> Extract -> Function_" 




### WRITE COMMENTS

In the ideal world, each class/method/field would have nice, detailed, appropriate code comments.
But even in such a beautiful world, everything would go to hell at the first code change, as that might
require manually changing most of the code comments.

Cannot really quantify how many comments one should write, but at least it would be good to have:
* brief (1-2 sentences) description of what the class is useful for (just before the class declaration) 
* for fields that are data structures (e.g., collections and arrays) some comments would be useful, as long and detailed 
  variable names are not practical
* for `Map`s, should add a comment stating what is the _key_, and what is the _value_.   

When writing a comment for a class/method/field, use JavaDoc style:
`/**
*/`
In this way, your IDE can show the comments when you hover with the mouse over them.
For C#, besides `/** */`, for single line documentation you can use a triple slash `///`.   
  
  


### IF CANNOT AVOID EXTERNAL SIDE-EFFECTS, DO DOCUMENT IT!!!

If a call on a object has side effects outside the class itself (e.g., writing to disk, add a system hook thread),
then this needs to be documented (see point on how to write comments),
unless it is obvious from the function/class name.  



### PRE AND POST CONDITIONS

* _Pre-conditions_ of `public` methods should throw exceptions explicitly 
  (e.g., `IllegalArgumentException` and `IllegalStateException`).
  Whenever possible, it is worth to write pre-conditions to `public` methods.
* _Pre-conditions_ of `private` methods and _post-conditions_ (both `public` and `private` methods) 
  should use the keyword `assert` in _Java_, and the function `assert()` in _Kotlin_.
  (An exception is when the validation of inputs of a public method is delegated/moved to 
  a `private` method: in this case you could add `throw`.)
  _Post-conditions_ are good, but often are difficult to write.
  Note: a _post-condition_ does not to be complete to be useful (i.e., find bugs). 
  For example, if we have _A && B_, but the writing
  of _B_ is too difficult (or time-consuming), still having just _A_ as _post-condition_ can help  

Note: currently _Kotlin_ does not have lazily evaluated assertions. 
If you are writing a computational expensive check, rather user `Lazy.assert(predicate)`.  
  
  

### FIELDS/CONSTRUCTORS/METHODS ORDER IN A CLASS 

When writing a new class (or re-factoring a current one), fields should come first, 
followed by class constructors and then the other methods.


### NON-DETERMINISM

_EvoMaster_ uses randomized algorithms. Running it twice on the same application can give 
different results. 
This is a problem for testing and debugging _EvoMaster_ itself, as for example the test cases 
will be _flaky_.
To avoid such issues, we must control the source of non-determinism.
All randomness sources __MUST__ come from the `Randomness` class.
Some data-structures could lead to non-deterministic behavior (e.g., iteration over a `Set` does not 
guarantee the order).
This does not seem the case for the default data-structures in Kotlin, but it is definitively 
a problem in Java, e.g., `HashSet` vs. `LinkedHashSet`.

In _EvoMaster_ we do have checks for its determinism. This is achieved by running some E2E tests twice
with verbose logging, and then compare the logs for an _exact_ match.
If some logs are not deterministic (e.g., printing out for how many seconds the search ran), those should
be inside a check for `EMConfig.avoidNonDeterministicLogs`.
  

When running _EvoMaster_ on an application, the _seed_ for the random generator is taken from
the CPU clock.
To make a run deterministic, you will need to use the `--seed` option to specify a constant seed. 


### NAMING CONVENTION
We follow the typical naming convention used in `Java`: class names start in capital letter
(e.g., `class Foo`), whereas we use camel-case for variables and 
methods (e.g., `void fooBar()` and `String helloWorld;`).
Constants in `Java` (but usually not in `Kotlin`, unless they are global public variables in a companion object) 
would be typically in upper-case using snake-case
(e.g., `final String HELLO_WORLD`).
Kebab-case should be avoided for names of classes/methods/variables 
(e.g., no `String hello-world`, which anyway would not compile).

Regarding packages and modules, it is a bit more tricky. In this project, the current
rules are the following (but might change if given arguments for a better approach):
no dashes `-` and no upper-case in the package names, but `-` are fine (and preferable) in module names.
For example, `org.EvoMaster.foo-bar` would be wrong for 2 reasons, which could be fixed
with `org.evomaster.foobar` or `org.evomaster.foo.bar`. 
On the other hand, a Maven module called `foo-bar` would be fine, 
but not `Foo-bar`.
The motivation here is that modules are mapped to folders on the operating system,
and we need to avoid issues with OSs like Windows that are case insensitive, and with `.`
treated as beginning of a file extension. 

All code written for `EvoMaster` must be inside the package `org.evomaster.*`.
Each module must define a subpackage, with a name somehow related to the module itself.
Dashes `-` in the module name would be either stripped or replaced with dots `.`. 
For example, a module called `controller-api` under the module `client-java` could
define a package called `org.evomaster.client.java.controller.api`.
Note that it is imperative that no module defines the same subpackage, as to avoid 
class name conflicts. 

All names should use ASCII letters. Non-ASCII ones like Ã¸ or Ã… must be avoided.



### MAVEN MODULE HIERARCHY

`EvoMaster` is built with `Maven`, with a hierarchy of submodules. 
Given a module `X` declaring a submodule `Y` with `<module>` in its `pom.xml` file,
then `Y` **must** declare `X` as parent with `<parent>`.
Do no break the hierarchy by pointing to a parent outside `EvoMaster` (e.g., 
something like `spring-boot-starter-parent`).
If you need to use such external poms, you can import them as dependency, i.e., specifying
the `<scope>import</scope>` tag. 

When creating a new module, it is also important to add it as a dependency to `report`,
so that aggregated, transitive code coverage can be calculated.


### MAVEN DEPENDENCY VERSION 

All dependency `<version>` tags must be declared in the *root* `pom.xml` file, 
in the `<dependencyManagement>` section.
Submodules *must* not declare a version for a library, and rather refer to the ones in
the root using just `<groupId>` and `<artifactId>` (but possibly overriding some configurations,
like `<scope>`).

Motivation: must have only a single version of a library in `EvoMaster`. Specifying versions
in submodules can lead to duplicated `<version>` declarations with different version numbers.
All version numbers should be easily audited, and so should be in a single file (i.e., the
*root* `pom.xml`).


### THIRD-PARTY LIBRARIES

Adding a new dependency is fine, but few things to consider:

* __NEVER__ ever add a GPL licensed library, unless it is under the so called _classpath exception_.
  Note that LGPL libraries are fine.
 
* When adding a new library, check who is maintaining it, and when was its last update.
  No longer maintained libraries should be avoided. 
  
      
### THIRD-PARTY CODE
  
As a rule of thumb, to avoid possible issues with copyrights and license compliance, 
we should not include code directly from third-party sources.
However, when that happens, it __MUST__ be made clear in the files themselves (e.g.,
with comments in their top, with URLs of the original sources). 
Furthermore, this information should also be added to the [reused_code.md](./reused_code.md) file.  
  
  
### Trello
If you are among the core developers of `EvoMaster`, you should get an invitation to join
[Trello](https://trello.com).
We use it to track activities and assign tasks. Anyone can create new tasks/cards.
Current usage:
- `On going`: tasks that are currently under development. Those must be assigned to at least
   1 person.
- `Done`: tasks that are fully done. We do not delete them, e.g., just in case if need to look at
   them again in the future.
   Even when a task is completed, the moving from `On going` to `Done` should be carried out 
   __only__ during a developer meeting (so it can be demoed or at least discussed).
   Furthermore, a done task should be added on top of the `Done` list.
   In this way, by looking at the top of the list, one can see what were the most recent changes.    
- `Important, to do soon`: high priority tasks which have not been started yet.
- `Issues/bugs`: reported bugs which are not trivial to fix. For developers, better to report them
  here than GitHub issue page.
- `Backlog-*`: different backlogs, divided by topic.


### MAKING A NEW RELEASE
Only the project manager should make a new release, as it requires a password.
Instructions can be found [here](./release.md).


### JDK VERSIONS
At this point, we only support JDK __8__ and the following major LTS versions.
_EvoMaster_ must be built with JDK 8, but still must be able to run it with the most recent LTS JDK.
Can be useful to setup your machine to easily switch between different JDK versions.
For example, if you are using a Mac, in your `~/.profile` configuration, you could have something 
like:
```
export JAVA_HOME_8=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/ 
export JAVA_HOME_11=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home/ 

export JAVA_HOME=$JAVA_HOME_11 
export PATH=$JAVA_HOME/bin:$PATH

alias java8='$JAVA_HOME_8/bin/java'
alias java11='$JAVA_HOME_11/bin/java'
alias mvn8='JAVA_HOME=$JAVA_HOME_8 && mvn'
alias mvn11='JAVA_HOME=$JAVA_HOME_11 && mvn'
```

If you are using Windows, it does not seem there is a simple way to define aliases.
Besides setting up the `JAVA_HOME` environment variable, it can be useful to set up an environment variable for each LTS JDK version, e.g., `JAVA_HOME_8`, `JAVA_HOME_11` and `JAVA_HOME_17` (of course, you will need to install all those JDKs...).
Then, from a bash shell (e.g., Git Bash), you can build with Maven using:

`JAVA_HOME=$JAVA_HOME_11 mvn <your_inputs>`

For example, try it with:

`JAVA_HOME=$JAVA_HOME_11 mvn --version`

You can also call `java` directly with:

`$JAVA_HOME_17/bin/java -version`





 
# Seminars/Presentations

* 2021: *AI-based Automated Test Case Generation
  for REST APIs with EvoMaster*.
  Presentation given at Norsk Helsenett.
  [[PDF](slides/2021_helsenett.pdf)]

* 2021: *Search-Based System Testing with EvoMaster*.
  Tutorial given at SSBSE 2021.
  [[PDF](slides/2021_ssbse.pdf)]

* 2019: *The Many Independent Objective (MIO) Algorithm for Test Suite Generation*.
  Lecture given at ISTI-CNR, Italy.
  [[PDF](slides/2019_mio_pisa.pdf)]  

* 2019: *Testing RESTful Web Services with EvoMaster*.
  Lecture given at the 3rd International Genoa Software Engineering PhD School on
  Automated Functional and Security Testing of Web and Mobile Application,
  Italy.
  [[PDF](slides/2019_evomaster_genova.pdf)]

* 2019: *Using Evolutionary Algorithms to Test Software*.
  Lecture given at Kristiania University College, Norway.
  [[PDF](slides/2019_evolutionary_testing_kristiania.pdf)]

* 2018: *EvoMaster: Evolutionary Multi-context Automated System Testing*. 
  Seminar given at MÃ¤lardalen University, Sweden.
  [[PDF](slides/2018_seminar_evomaster_mdh.pdf)]  

* 2017: *EvoMaster: Evolutionary Multi-context Automated System Testing*. 
  Seminar given at the University of Luxembourg, Luxembourg.
  [[PDF](slides/2017_seminar_evomaster_lux.pdf)]  # How to Contribute

There are many ways in which you can contribute.
If you found _EvoMaster_ of any use, _the easiest
way to show appreciation is to **star** it_.
Issues and feature requests can be reported on
the [issues](https://github.com/EMResearch/EvoMaster/issues) page:  
  
* *Bugs*: as for any bug report, the more detailed
  you can be the better.
  If you are using _EvoMaster_ on an open source project,
  please provide links to it, as then it is much easier
  to reproduce the bugs.
  
* *Documentation*: if you are trying to use _EvoMaster_, but the instructions
  in these notes are not enough to get you started, 
  then it means it is a "bug" in the documentation, which then would need
  to be clarified. 
  
* *Feature Requests*: to improve _EvoMaster_,
  we are very keen to receive feature requests, although of course we cannot
  guarantee when they are going to be implemented, if implemented at all. 
  As researchers, we want to know what are the problems that engineers in industry
  do face, and what could be done to improve _EvoMaster_ to help them.
  
  
* *Pull Requests*: we are keen to receive PRs, as long as you agree
  with the license of _EvoMaster_, and as long as you are allowed by your employer to contribute
  to open-source projects. However, before making a PR, you should read
  the [notes for developers](for_developers.md).  


* *Industry Collaborations*: to evaluate the effectiveness of _EvoMaster_, we need case studies.
  There are some open-source projects that can be used (e.g., which we selected and aggregated in the
  [EMB repository](https://github.com/EMResearch/EMB)).
  But open-source applications are not necessarily representative of software developed in industry.
  Therefore, we "collaborate" with different companies (e.g., [Universitetsforlaget](https://www.universitetsforlaget.no/)),
  to apply _EvoMaster_ on their systems.
      
  * *Benefits for us*: access to the source code of real, industrial systems (of course, under NDAs). 
    It makes easier to publish academic papers, and to get funding from the research councils
    to improve _EvoMaster_ even further.  
  * *Benefits for the industrial collaborators*: 
    
    1. getting priority on new features and bug fixing.
    2. "free" human resources (MSc/PhD students and researchers) that try to break your systems and
        find faults in them.    
    
* *Academic Collaborations*: we are keen to hear from students and researchers that want to 
  collaborate on topics in which _EvoMaster_ can be used on, or extended for.
  For example, it is possible to have visiting PhD students in our lab.
  However, all communications about academic collaborations should not be done here on GitHub,
  but rather by email, directly to Prof. A. Arcuri. 

# Replicating Studies

To foster the progress of scientific research, _replicability_ of studies is very important.
At least in the software testing community, now quite  a few venues __require__ the presence of _replication packages_ to get papers accepted.

To help replicating previous studies for _EvoMaster_, we provide most of the [scripts](exp) used throughout the years in our studies.
Here, we give a high overview of how they can be used. 


First and foremost, we need to make an important disclaimer here.
None of our previous experiments can be replicated with 100% accuracy.
The reason is that _EvoMaster_ deals with networking, which is an unfortunate source of non-determinism (e.g., when idempotent HTTP calls get repeated sporadically).
Furthermore, some of the internal heuristics of our algorithms are time-based: unless repeating the experiments on exactly the same hardware in the same exact conditions, 100% accurate replication of the results cannot be achieved.
However, lot of care has been taken in trying to keep under-control all source of non-determinism in _EvoMaster_.
This means that, given enough repeated experiments with different initializing seeds, average results (e.g., code coverage) should be very similar (although unlikely being 100% the same). 

Another important fact to keep in mind is that _EvoMaster_ is under continuous development, with new releases coming out each year.
However, the release process is unrelated to the published scientific papers, and we have _NOT_ been tagging versions on Git to trace the exact version of _EvoMaster_ used in any specific paper (as that would be a non-negligible time overhead).
As anyway having a 100% accurate replication of the experiments is not possible, we simply suggest  using a recent version of _EvoMaster_ when one wants to re-investigate some of our previous research questions (as most experiment settings will be backward compatible).


## Experiment Scripts

When running experiments for _EvoMaster_, we usually employ the case studies in 
[EMB](https://github.com/EMResearch/EMB), plus some other systems provided by our industrial partners (which of course we cannot provide online).
In each study, we might investigate different settings of _EvoMaster_, and each experiment has to be repeated several times (typically 30) with different random seeds.
Each experiment requires not only to start the process of _EvoMaster's core_, but also the _driver_ process for the tested applications. 
This means that, often, the experiment settings for a scientific study is quite complex.
To help running these experiments, we usually employ a script (written in Python) to configure all these settings.

This script has been evolving throughout the years, to simplify the running of the experiments, from paper to paper, with as little effort as possible. 
A template for one of its most recent versions can be found [in the _scripts_ folder (i.e., _scritps/exp.py_)](../scripts/exp.py).
Here, we briefly discuss how it works. 

Once defined a set of configurations of _EvoMaster_ to experiment with, the _exp.py_ script will generate a set of Bash scripts, in which the experiments will be distributed.
The Python script will take few parameters, like for example:

```
exp.py <cluster> <baseSeed> <dir> <minSeed> <maxSeed> <maxActions> <minutesPerRun> <nJobs>
```

The version of the scripts used in our recent papers do have a description (as comments) for each of these parameters.
But let us briefly summarize them here:

* _cluster_: whether experiments are meant to run on a local machine, or a scientific cluster of computers (the specific settings are for one cluster which is available for researchers in Norway). Unless you have access to such cluster, or want to adapt the script for clusters you have access to, you will likely use __false__ for this parameter.

* _baseSeed_: the starting seed used for initialization of TCP ports (e.g., for the tested applications). Each experiment will use different ports, where _baseSeed_ is the minimum value. As this value is used to setup TCP ports, we recommend a value of at least 1024, and no more than 60 000. The idea is that all (most?) experiments could be run on a single machine, without TCP port clashes. 

* _dir_: where all Bash scripts will be created. _EvoMaster_ is then configured to output all of its data (e.g., the generated test files) into such folder. 

* _minSeed_: used to setup the `--seed` configuration in _EvoMaster_. Although we cannot control all source of non-determinism, we can at least control the seed for the random generator.

* _maxSeed_: the maximum seed used. Experiments will be run for all seeds between _minSeed_ and _maxSeed_ (inclusive). For example, to repeat the experiments 5 times with different seed, could use the configuration _minSeed=0_ and _maxSeed=4_.

* _maxActions_: the search budget, measured in number of actions. For REST APIs, this will be the max number of HTTP calls to execute. Note: to help replicability of the studies, in the experiments we usually do not use time as stopping criterion.

* _minutesPerRun_: this is only needed to setup when running experiments on cluster, to enforce timeouts on hanging processes.

* _nJobs_: the number of Bash scripts to generate. All experiments will be distributed among those scripts. In other words, each script will have one or more experiments, run in sequence. How many scripts to run in parallel depends on your machine specs (e.g., number of CPUs and RAM size).

Once the _exp.py_ script is run, a _runAll.sh_ script will be automatically generated, which helps with the starting of all the Bash scripts in parallel.  

However, _exp.py_ relies on several environment variables, e.g., to specify where different versions of the JDK, EMB and _EvoMaster_ are installed. 
The script will provide descriptive error messages when those variables are not set.


An usage of the script could be:

```
exp.py false 12345 foo 0 4 100000 1 10
```

This will generate 10 Bash scripts in the `foo` folder, with experiments repeated 5 times, having a search budget of 100k HTTP calls per experiment.
These 10 scripts can then be started in parallel with `foo/runAll.sh`.
Once all experiments are finished, the generated `statistics.csv` files can be collected to analyze the results. 
















---
title: 'EvoMaster: A Search-Based System Test Generation Tool'
tags:
  - Java
  - Kotlin
  - SBST
  - search-based software engineering
  - test generation
  - system testing
  - REST
  - evolutionary computation

authors:
  - name: Andrea Arcuri
    orcid: 0000-0003-0799-2930
    affiliation: 1
  - name: Juan Pablo Galeotti
    orcid: 0000-0002-0747-8205
    affiliation: 2
  - name: Bogdan Marculescu
    orcid: 0000-0002-1393-4123
    affiliation: 1
  - name: Man Zhang
    orcid: 0000-0003-1204-9322
    affiliation: 1
affiliations:
 - name: Kristiania University College, Department of Technology, Oslo, Norway
   index: 1
 - name: FCEyN-UBA, and ICC, CONICET-UBA, Depto. de Computaci\'on, Buenos Aires, Argentina
   index: 2
date:  April 2020
bibliography: paper.bib
---


# Statement of Need

Testing web/enterprise applications is complex and expensive when done manually.
Often, software testing takes up to half of the development time and cost for a system. 
So much testing is needed because the cost of software failure is simply
too large: for example, in 2017, 304 software failures (reported in the media) impacted 3.6 billion people and $1.7
trillion in assets worldwide [@tricentis2017]. 
Unfortunately, due to its high cost, software testing is often left incomplete, and only applied partially.


To address this problem, in *Software Engineering* (SE) research a lot of effort has been spent in trying 
to design and implement novel techniques aimed at automating several different tasks, where software testing is among the most studied tasks. 
*Search-Based Software Testing* (SBST) [@harman2012search] casts the problem of software testing as an optimization problem,
aimed at, for example, maximizing code coverage and fault detection.   


Our SBST tool called ``EvoMaster`` addresses these challenges by using evolutionary techniques to 
automatically generate test cases.
It currently focuses on RESTful web services, which are the pillars of modern web and enterprise applications 
[@fielding2000architectural; @allamaraju2010restful].

 
The ``EvoMaster`` tool is aimed  at:
 
* practitioners in industry that want to automatically test their software. 

* researchers that need generated test cases for their studies.


# Tool Summary

``EvoMaster`` [@arcuri2018evomaster]  is a SBST tool 
that automatically *generates* system-level test cases.
Internally, it uses an *Evolutionary Algorithm* 
and *Dynamic Program Analysis*  to be able to generate effective test cases.
The approach is to *evolve* test cases from an initial population of 
random ones, using code coverage and fault detection as fitness function.


Key features:

* At the moment, *EvoMaster* targets RESTful APIs compiled to JVM **8** and **11** bytecode. 

* The APIs must provide a schema in *OpenAPI/Swagger* format (either *v2* or *v3*).

* The tool generates JUnit (version 4 or 5) tests, written in either Java or Kotlin.

* Fault detection: *EvoMaster* can generate tests cases that reveal faults/bugs in the tested applications. Different heuristics are employed, like checking for 500 status codes and mismatches from the API schemas.

* Self-contained tests: the generated tests do start/stop the application, binding to an ephemeral port. This means that the generated tests can be used for *regression testing* (e.g., added to the *Git* repository of the application, and run with any build tool such as *Maven* and *Gradle*).

* Advanced *whitebox* heuristics: *EvoMaster* analyses the bytecode of the tested applications, and uses several heuristics such as *testability transformations* and *taint analysis* to be able to generate more effective test cases.

* SQL handling: *EvoMaster* can intercept and analyse all communications done with SQL databases, and use such information to generate higher code coverage test cases. Furthermore, it can generate data directly into the databases, and have such initialization automatically added in the generated tests. At the moment, *EvoMaster* supports *H2* and *Postgres* databases.

* *Blackbox* testing mode: can run on any API (regardless of its programming language), as long as an *OpenAPI* schema is provided. However, results will be worse than *whitebox* testing (e.g., due to lack of bytecode analysis).


# Published Results

When addressing the testing of real-world web/enterprise applications, there are many challenges. 
The tested code can for example have complex execution flows, where the boolean predicates in *if* and *loop* statements depend on specific input data.
Furthermore, the execution flow could depend on interactions with external entities, such as databases, GUIs and remote web services. 
The search space of all possible test inputs is huge, where only a tiny subset lead to maximize code coverage and detect faults. 


To face and overcome those challenges, *EvoMaster* has been used to experiment with several novel techniques.
These techniques are now integrated in *EvoMaster*, where their best settings (based on empirical studies) are on by default.

This research work led to several publications:
novel search algorithms such as *MIO* [@mio2017; @arcuri2018test],
addressing the white-box testing of RESTful APIs [@arcuri2017restful; @arcuri2019restful],
resource-dependency handling [@zhang2019resource], accesses to SQL databases [@arcuri2019sql],
and novel *testability transformations* [@arcuri2020testability].


# Related Work

In the recent years, different techniques have been proposed for _black-box_ testing 
of RESTful APIs [@restlerICSE2019; @karlsson2020QuickREST; @viglianisi2020resttestgen; @eddouibi2018automatic].
Those present different variants of random testing, enhanced with heuristics based on the information provided in the API schemas.
As those techniques are black-box, they do not access the source-code/bytecode of the tested APIs, and so cannot exploit any such information to improve the generation of test cases.

At the time of this writing, *EvoMaster* appears to be the only tool that can do both
_black-box_ and _white-box_ testing, and that is also released as open-source.
Supporting black-box testing is important, as it is easier to setup and use.
However, white-box testing leads to better results (e.g., higher code coverage and fault detection), as it can exploit more information on the system under test. 


# Acknowledgements
We thank Annibale Panichella for providing a review and fix of our implementation of his MOSA algorithm. 
We also want to thank Agustina Aldasoro for her contributed bug fixes.
This work is funded by the Research Council of Norway (project on Evolutionary Enterprise Testing, grant agreement No 274385), and 
partially by UBACYT-2018 20020170200249BA, PICT-2015-2741.

# References
# Windows and Networking

If you are running _EvoMaster_ on Windows for long periods of time, or if you are running
more than one instance in parallel, you might get error messages regarding running out
of _ephemeral TCP ports_.

This might happen because Windows has much more restrictive default settings compared to 
Linux/Mac when dealing with TCP ports.
If you end up seeing these issues, you might want to use `regedit` to modify 
`HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\TCPIP\Parameters` registry subkeys.
In particular, add new `REG_DWORD` values for:

* `TcpTimedWaitDelay `: can use something like `60`.
* `MaxUserPort`: can use something like `32768`.

As this would impact your whole OS, we recommend doing such changes if and only if you start to see
problems with _EvoMaster_ related to ephemeral ports. 
 