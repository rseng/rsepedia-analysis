# NEWS

## v0.0.3
- Fixed error in `agree_nest` and `agree_reps`; now properly handles missing values
- Remove dependencies on sjstats and cccrm packages

## v0.0.2
- Fixes typos in jamovi/jmv functions
- Adds more descriptive errors to jamovi output
- Remove dontrun from examples in documentation
- Add more details and references to the package's functions
# Comments

- README URLs fixed.
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards
of acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
when an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at arcaldwell49@gmail.com. 
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0,
available at <https://www.contributor-covenant.org/version/2/0/code_of_conduct.html>.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
<https://www.contributor-covenant.org/faq>. Translations are available at <https://www.contributor-covenant.org/translations>.
SimplyAgree R Package
================

<img src="https://raw.githubusercontent.com/arcaldwell49/SimplyAgree/master/images/sticker.png" width="150" height="150" />

*Artwork courtesy of Chelsea Parlett Pelleriti*

<!-- badges: start -->

[![DOI](https://joss.theoj.org/papers/10.21105/joss.04148/status.svg)](https://doi.org/10.21105/joss.04148)
[![Codecov test
coverage](https://codecov.io/gh/arcaldwell49/SimplyAgree/branch/master/graph/badge.svg)](https://app.codecov.io/gh/arcaldwell49/SimplyAgree?branch=master)
[![R-CMD-check](https://github.com/arcaldwell49/SimplyAgree/workflows/R-CMD-check/badge.svg)](https://github.com/arcaldwell49/SimplyAgree/actions)
[![documentation](https://img.shields.io/badge/website-active-blue)](https://aaroncaldwell.us/SimplyAgree/)
<!-- badges: end -->

Please see the package’s
[website](https://aaroncaldwell.us/SimplyAgree/) for updates, vignettes,
and other details about the package.

# Background

`SimplyAgree` is an R package, and [jamovi](https://www.jamovi.org/)
module, created to make agreement and reliability analyses easier for
the average researcher. The functions within this package include simple
tests of agreement (`agree_test`), agreement analysis for nested
(`agree_nest`) and replicate data (`agree_reps`), and provide robust
analyses of reliability (`reli_stats`). In addition, this package
contains a set of functions to help when planning studies looking to
assess measurement agreement (`blandPowerCurve`).

## Installing SimplyAgree

You can install the most up-to-date version of `SimplyAgree` from
[GitHub](https://github.com/arcaldwell49/SimplyAgree) with:

``` r
devtools::install_github("arcaldwell49/SimplyAgree")
```

# Contributing

We are happy to receive bug reports, suggestions, questions, and (most
of all) contributions to fix problems and add features. Pull Requests
for contributions are encouraged.

Here are some simple ways in which you can contribute (in the increasing
order of commitment):

-   Read and correct any inconsistencies in the documentation
-   Raise issues about bugs or wanted features
-   Review code
-   Add new functionality

## Code of Conduct

Please note that the concurve project is released with a [Contributor
Code of
Conduct](https://aaroncaldwell.us/SimplyAgree/CODE_OF_CONDUCT.html). By
contributing to this project, you agree to abide by its terms.

# References

The functions in this package are largely based on the following works:

Lin L (1989). A concordance correlation coefficient to evaluate
reproducibility. *Biometrics* 45: 255 - 268.
<https://doi.org/10.2307/2532051>

Shieh, G. (2019). Assessing agreement between two methods of
quantitative measurements: Exact test procedure and sample size
calculation. *Statistics in Biopharmaceutical Research*, 1-8.
<https://doi.org/10.1080/19466315.2019.1677495>

Parker, R. A., et al (2016). Application of mixed effects limits of
agreement in the presence of multiple sources of variability: exemplar
from the comparison of several devices to measure respiratory rate in
COPD patients. Plos one, 11(12), e0168321.
<https://doi.org/10.1371/journal.pone.0168321>

Zou, G. Y. (2013). Confidence interval estimation for the Bland–Altman
limits of agreement with multiple observations per individual.
*Statistical methods in medical research*, 22(6), 630-642.
<https://doi.org/10.1177/0962280211402548>

Weir, J. P. (2005). Quantifying test-retest reliability using the
intraclass correlation coefficient and the SEM. *The Journal of Strength
& Conditioning Research*, 19(1), 231-240.

Lu, Meng-Jie, et al (2016). “Sample Size for Assessing Agreement between
Two Methods of Measurement by Bland−Altman Method” *The International
Journal of Biostatistics*, 12(2),
<https://doi.org/10.1515/ijb-2015-0039>

King, TS and Chinchilli, VM. (2001). A generalized concordance
correlation coefficient for continuous and categorical data. *Statistics
in Medicine*, 20, 2131:2147.

King, TS, Chinchilli, VM, and Carrasco, JL. (2007). A repeated measures
concordance correlation coefficient. *Statistics in Medicine*, 26,
3095:3113.

Carrasco, JL, et al. (2013). Estimation of the concordance correlation
coefficient for repeated measures using SAS and R. *Computer Methods and
Programs in Biomedicine*, 109, 293-304.
# NEWS

## v0.0.3
- Fixed error in `agree_nest` and `agree_reps`; now properly handles missing values
- Remove dependencies on sjstats and cccrm packages

## v0.0.2
- Fixes typos in jamovi/jmv functions
- Adds more descriptive errors to jamovi output
- Remove dontrun from examples in documentation
- Add more details and references to the package's functions
# httr 1.4.2

* Fix failing test.

* `parse_url()` now refers to RFC3986 for the parsing of the URL's 
  scheme, with a bit more permissive syntax (@ymarcon, #615).

# httr 1.4.1

* Remove the default `cainfo` option on Windows. Providing a CA bundle is not 
  needed anymore because `curl` now uses the native schannel SSL backend.
  For recent versions of libcurl, overriding the CA bundle actually breaks 
  custom trust certificates on corporate networks. (@jeroen, #603)

* `http_status()` now throws the correct error message if http status code is 
  not in the list of known codes (@Javdat, #567).

* `POST()` gains an example on how to use `encode = "raw"` for specific json
  string body (@cderv, #563)

* `RETRY()` now throws the correct error message if an error occurs during the 
  request (@austin3dickey, #581).

* `VERB()` and `RETRY()` now automatically uppercase methods (@patr1ckm, #571).

# httr 1.4.0

## OAuth

OAuth2.0 has been made somewhat more flexible in order to support more websites:

* `init_oauth2.0()` passes `use_basic_auth` onwards, enabling 
   basic authentication for OAuth 2.0 (@peterhartman, #484).

* `oauth2.0_token()` (and `init_oauth2.0()`) gains a `oob_value` argument 
  that allows arbitrary values to be sent for the `request_uri` 
  parameter during OOB flows (@ctrombley, #493).

* `oauth2.0_token()` (and `init_oauth2.0()`) gain a new 
  `query_authorize_extra` parameter make it possible to add extra query
  parameters to the authorization URL. This is needed some APIs (e.g. fitbit)
  (@cosmomeese, #503).

* `oauth_endpoints()` contains updated urls for Yahoo (@ctrombley, #493)
  and Vimeo (#491).

* OAuth 2.0 token refresh gives a more informative error if it fails (#516).

* Prior to token retrieval from on-disk cache, scopes are de-duplicated, 
  sorted, and stripped of names before being hashed. This eliminates a 
  source of hash mismatch that causes new tokens to be requested, even when
  existing tokens had the necessary scope. (@jennybc, #495)

Updates to demos:

* The Facebook OAuth demo now uses device flow (#510). This allows you to
  continue using the FB api from R under their new security policy.

* A new Noun Project demo shows how to use one-legged OAuth1 (@cderv, #548).

* The Vimeo demo has been updated from OAuth 1.0 to 2.0 (#491).

## Minor changes and improvements

* `cache_info()` now handles un-named flags, as illustrated by "private" when
  the server returns "private, max-age = 0".

* `parse_http_date()` gets a better default value for the `failure` argument 
  so that reponses with unparseable dates can be printed without error
  (@shrektan, #544).

* `POST()` now uses 22 digits of precision for `body` list elements by default 
  (@jmwerner, #490)

* `RETRY()` now terminates on any successful request, regardless of the value 
  of `terminate_on`. To return to the previous behaviour, set
  `terminate_on_success = FALSE` (#522).

* In `RETRY()` and `VERB()`, `HEAD` requests now succeed (#478, #499).

* Encoding falls back to UTF-8 if not supplied and content-type parsing
  fails (#500).

* Non-http(s) headers are no longer parsed (@billdenney, #537). This makes it 
  possible to use httr with protocols other than http, although this is not 
  advised, and you're own your own.

# httr 1.3.1

* Re-enable on-disk caching (accidentally disabled in #457) (#475)

# httr 1.3.0

## API changes

* Deprecated `safe_callback()` has been removed.

* `is_interactive` argument to `init_oauth1.0()`, `init_oauth2.0()` and 
  `oauth_listener()` has been deprecated, as the R session does not actually
  need to be interactive.

## New features

* New `set_callback()` and `get_callback()` set and query callback functions 
  that are called right before and after performing an HTTP request 
  (@gaborcsardi, #409)

* `RETRY()` now retries if an error occurs during the request (@asieira, #404),
  and gains two new arguments:

  * `terminate_on` gives you greater control over which status codes should
     it stop retrying. (@asieira, #404) 

  * `pause_min` allows for sub-second delays. (Use with caution! Generally the 
    default is preferred.) (@r2evans)
    
  * If the server returns HTTP status code 429 and specifies a `retry-after` 
    value, that value will now be used instead of exponential backoff with 
    jitter, unless it's smaller than `pause_min`. (@nielsoledam, #472)

## OAuth

* New oauth cache files are always added to `.gitignore` and, if it exists, 
  `.Rbuildignore`. Specifically, this now happens when option 
  `httr_oauth_cache = TRUE` or user specifies cache file name explicitly. 
  (@jennybc, #436)

* `oauth_encode()` now handles UTF-8 characters correctly. 
  (@yutannihilation, #424)

* `oauth_app()` allows you to specify the `redirect_url` if you need to 
  customise it. 
  
* `oauth_service_token()` gains a `sub` parameter so you can request
  access on behalf of another user (#410), and accepts a character vector 
  of `scopes` as was described in the documentation (#389).

* `oauth_signature()` now normalises the URL as described in the OAuth1.0a
  spec (@leeper, #435)

* New `oauth2.0_authorize_url()` and `oauth2.0_access_token()` functions 
  pull out parts of the OAuth process for reuse elsewhere (#457).

* `oauth2.0_token()` gains three new arguments:

    * `config_init` allows you to supply additional config for the initial 
      request. This is needed for some APIs (e.g. reddit) which rate limit 
      based on `user_agent` (@muschellij2, #363).
      
    * `client_credentials`, allows you to use the OAauth2 *Client Credential 
      Grant*. See [RFC 6749](https://tools.ietf.org/html/rfc6749#section-4)
      for details. (@cderv, #384)

    * A `credentials` argument that allows you to customise the auth flow. 
      For advanced used only (#457)

* `is_interactive` argument to `init_oauth1.0()`, `init_oauth2.0()` and 
  `oauth_listener()` has been deprecated, as the R session does not need
  to be interactive.

## Minor bug fixes and improvements
  
* `BROWSER()` prints a message telling you to browse to the URL if called
  in a non-interactive session.

* `find_cert_bundle()` will now correctly find cert bundle in "R_HOME/etc" 
  (@jiwalker-usgs, #386).

* You can now send lists containing `curl::form_data()` in the `body` of
  requests with `encoding = "multipart". This makes it possible to specify the 
  mime-type of individual components (#430).

* `modify_url()` recognises more forms of empty queries. This eliminates a 
  source of spurious trailing `?` and `?=` (@jennybc, #452).
  
* The `length()` method of the internal `path` class is no longer exported 
  (#395).

# httr 1.2.1

* Fix bug with new cache creation code: need to check that 
  cache isn't an empty file.

# httr 1.2.0

## New features

* `oauth_signature()` no longer prepends 'oauth\_'  to additional parameters.
  (@jimhester, #373)

* All `print()` methods now invisibly return `x` (#355).

* `DELETE()` gains a body parameter (#326).

* New `encode = "raw"` allows you to do your own encoding for requests with
  bodies.

* New `http_type()` returns the content/mime type of a request, sans parameters.

## Bug fixes and minor improvements

* No longer uses use custom requests for standard `POST` requests (#356, 
  #357). This has the side-effect of properly following redirects after 
  `POST`, fixing some login issues (eg hadley/rvest#133).
  
* Long deprecated `multipart` argument to `POST()`, `PUT()` and `PATCH()`
  has been removed.

* The cross-session OAuth cache is now created with permission 0600, and should
  give a better error if it can't be created (#365).

* New `RETRY()` function allows you to retry a request multiple times until
  it succeeds (#353).

* The default user agent string is now computed once and cached. This 
  is a small performance improvement, but important for local connections
  (#322, @richfitz).

* `oauth_callback()` gains trailing slash for facebook compatibility (#324).

* `progress()` gains `con` argument to control where progress bar is rendered
  (#359).

* When `use_basic_auth` option is used to obtain a token, token refreshes 
  will now use basic authentication too.

* Suppress unhelpful "No encoding supplied: defaulting to UTF-8." when 
  printing a response (#327).

* All auto parser functions now have consistent arguments. This fixes problem
  where `...` is pass on to another function (#330).

* `parse_media()` can once again parse multiple parameters (#362, #366).

* Correctly cast `config` in `POST()`.

* Fix in readfunction to close connection when done.

# httr 1.1.0

## New features

* `stop_for_status()`, `warn_for_status()` and (new) `message_for_status()`
  replace `message` argument with new `task` argument that optionally describes
  the current task. This allows API wrappers to provide more informative
  error messages on failure (#277, #302). `stop_for_status()` and
  `warn_for_status()` return the response if there were no errors. This 
  makes them easier to use in pipelines (#278).

* `url_ok()` and `url_successful()` have been deprecated in favour of the more
  flexible `http_error()`, which works with urls, responses and integer status
  codes (#299).

## OAuth

* `oauth1.0_token()` gains RSA-SHA1 signature support with the `private_key`
  argument (@nathangoulding, #316).

* `oauth2.0_token()` throws an error if it fails to get an access token (#250)
  and gains two new arguments:

    * `user_params` allows you to pass arbitrary additional parameters to the 
      token access endpoint when acquiring or refreshing a token 
      (@cornf4ke, #312)
    
    * `use_basic_auth` allows you to pick use http authentication when
      getting a token (#310, @grahamrp).

* `oauth_service_token()` checks that its arguments are the correct types 
  (#282) and anways returns a `request` object (#313, @nathangoulding).

* `refresh_oauth2.0()` checks for known OAuth2.0 errors and clears the
  locally cached token in the presense of any (@nathangoulding, #315).

## Bug fixes and minor improvements

* httr no longer bundles `cacert.pem`, and instead it relies on the bundle in 
  openssl. This bundle is only used a last-resort on windows with R <3.2.0.

* Switch to 'openssl' package for hashing, hmac, signatures, and base64.

* httr no longer depends on stringr (#285, @jimhester).

* `build_url()` collapses vector `path` with `/` (#280, @artemklevtsov).

* `content(x)` uses xml2 for XML documents and readr for csv and tsv.

* `content(, type = "text")` defaults to UTF-8 encoding if not otherwise
  specified.

* `has_content()` correctly tests for the presence/absence of body content (#91).

* `parse_url()` correctly parses urls like `file:///a/b/c` work (#309).

* `progress()` returns `TRUE` to fix for 'progress callback must return boolean' 
  warning (@jeroenooms, #252).

* `upload_file()` supports very large files (> 2.5 Gb) (@jeroenooms, #257).

# httr 1.0.0

* httr no longer uses the RCurl package. Instead it uses the curl package, 
  a modern binding to libcurl written by Jeroen Ooms (#172). This should make 
  httr more reliable and prevent the "easy handle already used in multi handle" 
  error. This change shouldn't affect any code that uses httr - all the changes 
  have happened behind the scenes.

* The `oauth_listener` can now listen on a custom IP address and port (the 
  previously hardwired ip:port of `127.0.0.1:1410` is now just the default).
  This permits authentication to work under other settings, such as inside 
  docker containers (which require localhost uses `0.0.0.0` instead). To 
  configure, set the system environmental variables `HTTR_LOCALHOST` and 
  `HTTR_PORT` respectively (@cboettig, #211).

* `POST(encode = 'json')` now automatically turns length-1 vectors into json
  scalars. To prevent this automatic "unboxing", wrap the vector in `I()` 
  (#187).

* `POST()`, `PUT()` and `PATCH()` now drop `NULL` body elements. This is 
  convenient and consistent with the behaviour for url query params.

## Minor improvements and bug fixes

* `cookies` argument to `handle()` is deprecated - cookies are always
  turned on by default.

* `brew_dr()` has been renamed to `httr_dr()` - that's what it should've 
  been in the first place!

* `content(type = "text")` compares encodings in a case-insensitive manner
  (#209).

* `context(type = "auto")` uses a better strategy for text based formats (#209).
  This should allow the `encoding` argument to work more reliably.

* `config()` now cleans up duplicated options (#213).

* Uses `CURL_CA_BUNDLE` environment variable to look for cert bundle on 
  Windows (#223).
  
* `safe_callback()` is deprecated - it's no longer needed with curl.

* `POST()` and `PUT()` now clean up after themselves when uploading a single 
  file (@mtmorgan).

* `proxy()` gains an `auth` argument which allows you to pick the type of
  http authentication used by the proxy (#216).

* `VERB()` gains `body` and `encode` arguments so you can generate 
  arbitrary requests with a body.

* tumblr added as an `oauth_endpoint`.

# httr 0.6.1

* Correctly parse headers with multiple `:`, thanks to @mmorgan (#180).

* In `content()`, if no type is provided to function or specified in headers,
  and we can't guess the type from the extension, we now assume that it's 
  `application/octet-stream` (#181).

* Throw error if `timeout()` is less than 1 ms (#175).

* Improved LinkedIn OAuth demo (#173).

# httr 0.6.0

## New features

* New `write_stream()` allows you to process the response from a server as 
  a stream of raw vectors (#143).

* Suport for Google OAuth2 
  [service accounts](https://developers.google.com/accounts/docs/OAuth2ServiceAccount).
  (#119, thanks to help from @siddharthab).

* `VERB()` allows to you use custom http verbs (#169).

* New `handle_reset()` to allow you to reset the handle if you get the error
  "easy handle already used in multi handle" (#112).

* Uses R6 instead of RC. This makes it possible to extend the OAuth
  classes from outside of httr (#113).

* Now only set `capath` on Windows - system defaults on linux and mac ox 
  seem to be adequate (and in some cases better). I've added a couple of tests
  to ensure that this continues to work in the future.

## Minor improvements and bug fixes

* `vignette("api-packages")` gains more detailed instructions on
  setting environment variables, thanks to @jennybc.

* Add `revoke_all()` to revoke all stored tokens (if possible) (#77).

* Fix for OAuth 2 process when using `options(httr_oob_default = TRUE)`
  (#126, @WillemPaling).

* New `brew_dr()` checks for common problems. Currently checks if your libCurl 
  uses NSS. This is unlikely to work so it gives you some advice on how to 
  fix the problem (thanks to @eddelbuettel for debugging this problem).

* `Content-Type` set to title case to avoid errors in servers which do not
  correctly implement case insensitivity in header names. (#142, #146) thanks
  to Håkon Malmedal (@hmalmedal) and Jim Hester (@jimhester).

* Correctly parse http status when it only contains two components (#162).

* Correctly parse http headers when field name is followed by any amount
  (including none) of white space.

* Default "Accepts" header set to 
  `application/json, text/xml, application/xml, */*`: this should slightly
  increase the likelihood of getting xml back. `application/xml` is correctly
  converted to text before being parsed to `XML::xmlParse()` (#160).

* Make it again possible to override the content type set up by `POST()`
  when sending data (#140).

* New `safe_callback()` function operator that makes R functions safe for
  use as RCurl callbacks (#144).
  
* Added support for passing oauth1 tokens in URL instead of the headers 
  (#145, @bogstag).

* Default to out-of-band credential exchange when `httpuv` isn't installed.
  (#168)

## Deprecated and deleted functions

* `new_token()` has been removed - this was always an internal function
  so you should never have been using it. If you were, switch to creating
  the tokens directly. 

* Deprecate `guess_media()`, and instead use `mime::guess_type()` (#148).

# httr 0.5

* You can now save response bodies directly to disk by using the `write_disk()`
  config. This is useful if you want to capture large files that don't fit in
  memory (#44).

* Default accept header is now "application/json, text/xml, */*" - this should
  encourage servers to send json or xml if they know how.

* `httr_options()` allows you to easily filter the options, e.g. 
  `httr_options("post")`
  
* `POST()` now specifies Curl options more precisely so that Curl know's 
  that you're doing a POST and can respond appropriately to redirects.
  
## Caching

* Preliminary and experimental support for caching with `cache_info()` and
  `rerequest()` (#129). Be aware that this API is likely to change in 
  the future.

* `parse_http_date()` parses http dates according RFC2616 spec.

* Requests now print the time they were made.

* Mime type `application/xml` is automatically parsed with ``XML::xmlParse()`.
  (#128)

## Minor improvements and bug fixes

* Now possible to specify both handle and url when making a request.

* `content(type = "text")` uses `readBin()` instead of `rawToChar()` so
  that strings with embedded NULLs (e.g. WINDOWS-1252) can be re-encoded
  to UTF-8.

* `DELETE()` now returns body of request (#138).

* `headers()` is now a generic with a method for response objects.

* `parse_media()` failed to take into account that media types are 
  case-insenstive - this lead to bad re-encoding for content-types like
  "text/html; Charset=UTF-8"

* Typo which broke `set_cookies()` fixed by @hrbrmstr.

* `url_ok()` works correctly now, instead of always returning `FALSE`,
  a bug since version 0.4 (#133).
  
* Remove redundant arguments `simplifyDataFrame` and `simplifyMatrix` for json parser.

# httr 0.4

## New features

* New `headers()` and `cookies()` functions to extract headers and cookies 
  from responses. Previoulsy internal `status_code()` function now exported
  to extract `status_code()` from responses.

* `POST()`, `PUT()`, and `PATCH()` now use `encode` argument to determine how
  list inputs are encoded. Valid values are "multiple", "form" or "json".
  The `multipart` argument is now deprecated (#103). You can stream a single 
  file from disk with  `upload_file("path/")`. The mime type will be guessed 
  from the extension, or can be supplied explicitly as the second argument to 
  `upload_file()`.

* `progress()` will display a progress bar, useful if you're doing large 
  uploads or downloads (#17).

* `verbose()` now uses a custom debug function so that you can see exactly
  what data is sent to the server. Arguments control exactly what is included,
  and the defaults have been selected to be more helpful for the most common
  cases (#102).

* `with_verbose()` makes it easier to see verbose information when http 
  requests are made within other functions (#87).

## Documentation improvements

* New `quickstart` vignette to help you get up and running with httr.

* New `api-packages` vignette describes how best practices to follow when
  writing R packages that wrap web APIs.

* `httr_options()` lists all known config options, translating between
  their short R names and the full libcurl names. The `curl_doc()` helper
  function allows you to jump directly to the online documentation for an
  option.

## Minor improvements

* `authenticate()` now defaults to `type = "basic"` which is pretty much the
  only type of authentication anyone uses.

* Updated `cacert.pem` to version at 2014-04-22 (#114).

* `content_type()`, `content_type_xml()` and `content_type_json()` make it
  easier to set the content type for `POST` requests (and other requests with
  a body).

* `has_content()` tells you if request has any content associated with it (#91).

* Add `is_interactive()` parameter to `oauth_listener()`, `init_oauth1.0()` and
  `init_oauth2.0()` (#90).

* `oauth_signature()` and `oauth_header()` now exported to make it easier to 
  construct custom authentication for APIs that use only some components of
  the full OAuth process (e.g. 2 legged OAuth).

* NULL `query` parameters are now dropped automatically.

* When `print()`ing a response, httr will only attempt to print the first few 
  lines if it's a text format (i.e. either the main type is text or is
  application/json). It will also truncate each line so that it fits on
  screen - this should hopefully make it easier to see a little bit of the
  content, without filling the screen with gibberish.

* `new_bin()` has been removed: it's easier to see what's going on in 
  examples with `httpbin.org`.

## Bug fixes

* `user_agent()` once again overrides default (closes #97)

* `parse(type = "auto")` returns NULL if no content associated with request 
  (#91).
  
* Better strategy for resetting Curl handles prevents carry-over of error
  status and other problems (#112).

* `set_config()` and `with_config()` now work with `token`s (#111).

# httr 0.3

## OAuth improvements

OAuth 2.0 has recieved a major overhaul in this version. The authentication
dance now works in more environments (including RStudio), and is generally
a little faster. When working on a remote server, or if R's internet connection
is constrained in other ways, you can now use out-of-band authentication,
copying and pasting from any browser to your R session. OAuth tokens from
endpoints that regularly expire access tokens can now be refreshed, and will
be refresh automatically on authentication failure.

httr now uses project (working directory) based caching: every time you
create or refresh a token, a copy of the credentials will be saved in
`.httr-oauth`. You can override this default for individual tokens with the
`cache` parameter, or globally with the `httr_oauth_cache` option. Supply
either a logical vector (`TRUE` = always cache, `FALSE` = never cache,
`NA` = ask), or a string (the path to the cache file).

You should NOT include this cache file in source code control - if you do,
delete it, and reset your access token through the corresponding web interface.
To help, httr will automatically add appropriate entries to `.gitignore` and
`.Rbuildignore`.

These changes mean that you should only ever have to authenticate
once per project, and you can authenticate from any environment in which
you can run R. A big thanks go to Craig Citro (@craigcitro) from google,
who contributed much code and many ideas to make this possible.

* The OAuth token objects are now reference classes, which mean they can be
  updated in place, such as when an access token expires and needs to be
  refreshed. You can manually refresh by calling `$refresh()` on the object.
  You can force reinitialisation (to do the complete dance from
  scratch) by calling `$reinit(force = TRUE)`.

* If a signed OAuth2 request fails with a 401 and the credentials have a
  `refresh_token`, then the OAuth token will be automatically refreshed (#74).

* OAuth tokens are cached locally in a file called `.httr-oauth` (unless
  you opt out). This file should not be included in source code control,
  and httr will automatically add to `.gitignore` and `.Rbuildignore`.
  The caching policy is described in more detail in the help for the
  `Token` class.

* The OAuth2 dance can now be performed without running a local webserver
  (#33, thanks to @craigcitro). To make that the default, set
  `options(httr_oob_default = TRUE)`. This is useful when running R remotely.

* Add support for passing oauth2 tokens in headers instead of the URL, and
  make this the default (#34, thanks to @craigcitro).

* OAuth endpoints can store arbitrary extra urls.

* Use the httpuv webserver for the OAuth dance instead of the built-in
  httpd server (#32, thanks to @jdeboer). This makes the dance work in
  Rstudio, and also seems a little faster. Rook is no longer required.

* `oauth_endpoints()` includes some popular OAuth endpoints.

## Other improvements

* HTTP verbs (`GET()`, `POST()` etc) now pass unnamed arguments to `config()`
  and named arguments to `modify_url()` (#81).

* The placement of `...` in `POST()`, `PATCH()` and `PUT()` has been tweaked
  so that you must always specify `body` and `multipart` arguments with their
  full name. This has always been recommended practice; now it is enforced.

* `httr` includes its own copy of `cacert.pem`, which is more recent than
  the version included in RCurl (#67).

* Added default user agent which includes versions of Curl, RCurl and httr.

* Switched to jsonlite from rjson.

* Content parsers no longer load packages on to search path.

* `stop_for_status()` now raises errors with useful classes so that you can
  use `tryCatch()` to take different actions depending on the type of error.
  See `http_condition()` for more details.

* httr now imports the methods package so that it works when called with
  Rscript.

* New automatic parsers for mime types `text/tab-separated-values` and
  `text/csv` (#49)

* Add support for `fragment` in url building/parsing (#70, thanks to
  @craigcitro).

* You can suppress the body entirely in `POST()`, `PATCH()` and `PUT()`
  with `body = FALSE`.

## Bug fixes

* If you supply multiple headers of the same name, the value of the most
  recently set header will always be used.

* Urls with missing query param values (e.g. `http://x.com/?q=`) are now
  parsed correctly (#27). The names of query params are now also escaped
  and unescaped correctly when parsing and building urls.

* Default html parser is now `XML::htmlParse()` which is easier to use
  with xpath (#66).

# httr 0.2

* OAuth now uses custom escaping function which is guaranteed to work on all
  platforms (Fixes #21)

* When concatenating configs, concatenate all the headers. (Fixes #19)

* export `hmac_sha1` since so many authentication protocols need this

* `content` will automatically guess what type of output (parsed, text or raw)
  based on the content-type header. It also automatically converts text
  content to UTF-8 (using the charset in the media type) and can guess at mime
  type from extension if server doesn't supply one. Media type and encoding
  can be overridden with the `type` and `encoding` arguments respectively.

* response objects automatically print content type to aid debugging.

* `text_content` has become `context(, "text")` and `parsed_content`
  `content(, "parsed")`. The previous calls are deprecated and will be removed
  in a future version.

* In `oauth_listener`, use existing httpd port if help server has already been
  started. This allows the ouath authentication dance to work if you're in
  RStudio. (Fixes #15).

* add several functions related to checking the status of an http request.
  Those are : `status`, `url_ok` and `url_success` as well as
  `stop_for_status` and `warn_for_status`.

* `build_url`: correctly add params back into full url.

# httr 0.1.1

* Add new default config: use the standard SSL certificate

* Add recommendation to use custom handles with `authenticate`
# ggsci 2.9 (2018-05-13)

## Improvements

- New URL for the documentation website: https://nanx.me/ggsci/.

# ggsci 2.8 (2017-09-30)

## Improvements

- Use system font stack instead of Google Fonts in vignettes to avoid pandoc SSL issue.

# ggsci 2.7 (2017-06-12)

## New Features

Two new discrete color palettes:

- JAMA
- Tron Legacy

One new collection of continuous palettes with 19 color options:

- Material Design

# ggsci 2.4 (2017-03-07)

## New Features

Four new discrete color palettes:

- NEJM
- LocusZoom
- IGV
- Star Trek

# ggsci 2.0 (2016-11-20)

## New Features

Two new discrete color palettes:

- D3.js (v3)
- Futurama (Planet Express)

The first continuous color palette:

- GSEA GenePattern

# ggsci 1.0 (2016-04-01)

## New Features

Eight discrete color palettes:

  - NPG
  - AAAS
  - Lancet
  - JCO
  - UCSCGB
  - UChicago
  - The Simpsons (Springfield)
  - Rick and Morty (Schwifty)
# modelr 0.1.8

* Eliminate direct dplyr dependency in favour of vctrs.
* Reimplement `typical.ordered()` for R-devel compatibility.

# modelr 0.1.7

* Minor documentation fixes and updates for deprecated functions.

# modelr 0.1.6

* R CMD check documentation fix

# modelr 0.1.5

* Fix to `data_grid()` to work with dev tidyr.

# modelr 0.1.4

* `add_predictions()`, `gather_predictions()`, and `spread_predictions()` 
  more carefully pass along `type` parameter in order to avoid problems with
  predict methods that don't deal with `type = NULL` (#92).

# modelr 0.1.3

* `add_predictions()`, `gather_predictions()`, and `spread_predictions()` 
  gain a `type` parameter which is passed through to `stats::predict()`
  (#34, @pmenzel)

* New `crossv_loo()` which implements leave-one-out cross validation (@pmenzel)

* `typical()` no longer ignores missing values in character and factor vectors
  (#80).

# modelr 0.1.2

* `data_grid()` no longer fails with modern tidyr (#58).

* New `mape()` and `rsae()` model quality statistics (@paulponcet, #33).

* `rsquare()` use more robust calculation 1 - SS_res / SS_tot rather 
  than SS_reg / SS_tot (#37).

* `typical()` gains `ordered` and `integer` methods (@jrnold, #44), 
  and `...` argument (@jrnold, #42).

# modelr 0.1.1

* Added a `NEWS.md` file to track changes to the package.

* Fixed R CMD CHECK note

* Updated usage of `reduce()` for upcoming purrr release

* More general `permute()` function

* Add `mse()` function to calculate mean squared error. Written by @bensoltoff, pull request #57
# sjlabelled 1.1.6

## General

* Fix issue in `write_*()` functions that did not work since the last *haven*  update.

# sjlabelled 1.1.5

## General

* Fix CRAN check issues.

# sjlabelled 1.1.4

## General

* Reduce package dependencies.

## Bug fixes

* Fixed issues in `remove_labels()` when factor levels were strings.
* Fixed Namespace issues in vignettes.

# sjlabelled 1.1.3

## Bug fixes

* Fix warning from CRAN checks.

# sjlabelled 1.1.2

## General

* Reduce package dependencies.
* `get_dv_labels()` was renamed to `response_labels()`. `get_dv_labels()` will remain as alias.
* `get_term_labels()` was renamed to `term_labels()`. `get_term_labels()` will remain as alias.

## New functions

* `label_to_colnames()` as a convenient shortcut to set variable labels as column names.

## Changes to functions

* `copy_labels` gets a `...`-argument to copy only specific variables values.
* The `read_*()` functions are now less verbose by default.

## Bug fixes

* Fixed issue in `set_labels()` for character vectors with numeric char-values that are larger than `9`.

# sjlabelled 1.1.1

## Changes to functions

* The `read_*()`-functions get a `drop.labels`-argument, which - if `TRUE` - automatically calls `drop_labels()` on the imported dataset.
* `read_data()` is a shortcut that calls one of the `read_*()`-functions, depending on the file extension.

## Bug fixes

* Fix issue in `as_label()` to prevent replacement recycling when labels were numeric.
* Fix issues with saving character vectors to SPSS files.

# sjlabelled 1.1.0

## New functions

* `remove_label()`, to remove variable labels (and preserve value labels).

## Changes to functions

`convert_case()` gets a `verbose`-argument to toggle warnings and messages on or off.

# sjlabelled 1.0.17

## General

* Reduce package dependencies.
* New package-vignette on quasiquotation.

## New functions

* Re-implement `set_na()`, to define (labelled) `NA`-values in a vector.

## Changes to functions

* `as_label()` gets a `keep.labels`-argument. With this, users can easily convert vector to factors and vice versa, preserving label-attributes.

## Bug fixes

* Fixed bug with argument `use.labels` in `as_numeric()`.

# sjlabelled 1.0.16

## General

* Started adding test-units.
* Minor code revisions to avoid errors during CRAN check for current devel-versions of R.

## New functions

* `val_labels()` as counterpart to `var_labels()`, to set value labels, with support for quasi-quotation (see Examples).

## Changes to functions

* `var_labels()` now supports quasi-quotation (see Examples).

# sjlabelled 1.0.15

## General

* Update code to the new class-attribute `haven_labelled` from the **haven**-package.

## Bug fixes

* Fix issue in `get_term_labels()` that returned wrong object names for factors where factor levels did start with "1".

# sjlabelled 1.0.14

## General

* Reduce package dependencies.

## Bug fixes

* Fix bug in `var_labels()`, where non-existing columns may lead to wrong labelling.

# sjlabelled 1.0.13

## General

* Removed defuncted functions.

## Changes to functions

* `copy_labels()` now also copy labels even if columns in subsetted and original data frame do not completely match.
* Arguments `include.non.labelled` and `include.values` in `get_labels()` are renamed to shorter versions `non.labelled` and `values`. `include.non.labelled` and `include.values` will become softly deprecated.
* The `read_*()`-functions get a `verbose`-argument, to show or hide the progressbar when imported datasets are converted.

## Bug fixes

* Due to changes in the _broom_ and _lmerTest_ packages, tidiers did no longer work for `lmerModLmerTest` objects.

# sjlabelled 1.0.12

## General

* `get_dv_labels()` and `get_term_labels()` now support _clmm_-objects (package **ordinal**) and _stanmvreg_-objects (package **rstanarm**).
* `read_spss()` gets a `enc`-argument for character encoding, which is now supported since haven 1.1.2.
* `get_term_labels()` now returns `NULL` for unsupported models, instead of giving an error.
* `get_dv_labels()` now returns a default string for unsupported models, instead of giving an error.

# sjlabelled 1.0.11

## General

* `as_labelled()` now corrects inconsistent types between labels and variable values.

## Changes to functions

* `get_dv_labels()` gets a `multi.resp`-argument to return each label of a multivariate response model (only for _brmsfit_ objects).
* `get_label()` now also returns name-attribute for empty labels if `x` was a data.frame.

## Bug fixes

* `write_*()`-functions should now properly set labels for negative values.

# sjlabelled 1.0.9

## General

* Deprecated `set_note()` and `get_note()`, because there is already an R base function for this purpose: `comment()`.
* Improved performance of functions, at the cost of removing support for the _foreign_ package. _sjlabelled_ now only supports labelled data from package _haven_.

## Changes to functions

* `get_term_labels()` gets a `prefix`-argument to prefix the returned labels of categorical variable either with the related variable name or label.

## Bug fixes

* Fix issues with retrieving incorrect labels from `get_term_labels()` for models that used unlabelled data in combination with other contrasts than the default option.
* `get_dv_labels()` no longer returns `"NULL"` for multivariate-response-models fitted with _brms_.

# sjlabelled 1.0.8

## General

* Removed `lbl_df()`, because printing tibbles now depends on pkg _pillar_ and was revised substantially, so maintainace of `lbl_df()` is too extensive.

# sjlabelled 1.0.7

## General

* Cross references from `dplyr::select_helpers` were updated to `tidyselect::select_helpers`.
* Replace deprecated arguments in `convert_case()` from call to package *snakecase*

# sjlabelled 1.0.6

## Changes to functions

* `get_dv_labels()` and `get_term_labels()` now support `clm`-objects from package *ordinal*,  `polr`-objects from package *MASS* and `Zelig-relogit`-objects from package *Zelig*.
* `get_dv_labels()` and `get_term_labels()` get a `...`-argument to pass down further arguments to `snakecase::to_any_case()`.
* `convert_case()` is now exported, for usage in other packages as well.
* Remove `protect`-argument from internal case conversion (affects `get_term_labels()` and `get_dv_labels()`), in preparation for forthcoming *snakecase*-package update.

# sjlabelled 1.0.5

## General

* Remove unnecessary imports.
* Revised `lbl_df()` due to changes in the internals of `tibble::trunc_mat()`.

## New functions

* `as_factor()` to convert labelled vectors into factors, preserving labels.

## Changes to functions

* `get_dv_labels()` now supports `brmsfit`-objects from package `brms`.

# sjlabelled 1.0.4

## Changes to functions

* `get_term_labels()` now includes variable names for factors with numeric factor levels only (and not only return the numeric level as term label).

## Bug fixes

* Fixed bug for `as_label()`, when `x` was a character vector and argument `drop.levels` was `TRUE`.
* Fixed issue for *lme* and *gls* objects in `get_term_labels()` and `get_dv_labels()`.

# sjlabelled 1.0.3

## General

*  Changed package imports, so `sjlabelled` no longer requires R version 3.3.3 or higher.

# sjlabelled 1.0.2

## General

* Minor fix to avoid warning when using `as_numeric()`.

## Changes to functions

* `get_label()`, `get_term_labels()` and `get_dv_labels()` get a `case`-argument, to convert labels into any case, using the [snakecase](https://cran.r-project.org/package=snakecase)-package.

# sjlabelled 1.0.1

## General

* Removed function 'var_rename()', which is in pkg 'sjmisc'.

## New functions

* `get_term_labels()` and `get_dv_labels()` to retrieve term labels from regression models.

## Changes to functions

* `as_numeric()` gets a `use.labels`-argument to use value labels as new values if these are numeric.

# sjlabelled 1.0.0

## General

* Initial release. All labelled data utility functions from package *sjmisc* have been moved to this package, which is now dedicated to tools for working with labelled data.
# covr 3.5.0

* `codecov()` now supports GitHub Actions for public repositories without having to specify a token.

* New `to_sonarqube()` function added to support SonarQube generic XML format (@nibant, @Delfic, #413).

# covr 3.4.0

* `codecov()` now supports GitHub Actions.

* New `in_covr()` function added to return true if code is being run by covr (#407).

* `file_coverage()`, `environment_coverage()` and `function_coverage()` now set
  `R_COVR=true`, to be consistent with `package_coverage()` (#407)

# covr 3.3.2

* Fix test failures in the development version of R (4.0.0) (#400)

# covr 3.3.1

* Fix inadvertent regression in return visibility when functions are covered.
  covr versions prior to 3.3.0 surrounded each statement in `{` blocks. covr
  3.3.0 switched to using `({`, but this caused an inadvertent regression, as
  `(` will make the result visible it is the last expression in a function.
  Using `if (TRUE) {` restores the previous behavior. (#391, #392)

# covr 3.3.0

## New Features

* New `azure()` function added to make it easy to use covr on [Azure
  Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/)
  (#370)

* Work around issues related to the new curly curly syntax in rlang (#379, #377, rlang#813)

* Compiled code coverage has been improved, in particular C++ templates now
  contain the merged coverage of all template instances, even if the instances
  were defined in separate compilation units. (#390)

## Bugfixes and minor improvements

* `codecov()` now includes support for the flags field (#365)
* `codecov` now looks `codecov.yml` for token if `CODECOV_TOKEN` envvar is not
  set (@MishaCivey #349).
* `per_line()` now does not track lines with only punctuation such as `}` or `{` (#387)
* `tally_coverage()` now includes compiled code, like it did previously (#384)

* Define the necessary coverage flags for C++14, C++17 and C++20 (#369).

* `to_cobertura()` now works with Cobertura coverage-04.dtd (@samssann, #337).

* [R6](https://github.com/r-lib/R6) class generators prefixed with `.` are now
  included in coverage results (@jameslamb, #356).

* `package_coverage()` gains option `pre_clean`, set to `FALSE` to disable
  cleaning of existing objects before running `package_coverage()` (@jpritikin, #375)

# 3.2.1

* Fix for regression when testing coverage of packages using mclapply (#335).

# 3.2.0

## Breaking changes

* Previously deprecated `shine()` has been removed. Instead use `report()`.

## New Features

* `file_report()` added when viewing coverage for a single file (#308).

* `display_name()` is now exported, which can be useful to filter the coverage
  object by filename.

* `environment_coverage()` added, mainly so it can be used for `devtools::test_coverage_file()`.

* `gitlab()` function added to create a coverage report for GitLab using
  GitLab's internal pages (@surmann, #327, #331).

* The (optional) dependency on shiny has been removed. `report()` can now be
  built with only DT and htmltools installed.

## Bugfixes and minor improvements

* Fix for gcc-8 gcov output producing lines with no coverage counts in them (#328)

* `impute_srcref()` now handles `...` and drop through arguments in switch
  statements (#325).

* `tally_coverage()` now avoids an error when there are NA values in the source
  references (#322).

* `covr(clean = TRUE)` now cleans the temporary library as well (#144)

* `package_coverage()` now returns the end of the file if there is a test error (#319)

* `report()` now handles reports in relative paths with subdirectories correctly (#329)

* `report()` reworked to look more like codecov.io and to display the overall
  coverage (#302, #307).

* DT explicitly loaded early in `report()` so that failures will occur fast if
  it is not installed. (#321, @renkun-ken).

# 3.1.0 #

## Breaking changes

* `shine()` has been deprecated in favor of `report()`.

## New Features

* Add support for `.covrignore` files (#238), to exclude files from the coverage.

* Support future versions of R which do not use parse data by default (#309).

* Allow using `trace_calls()` for manually adding functions to package trace
  that are not found automatically (#295, @mb706).

## Bugfixes

* Fix errors when R is not in the `PATH` (#291)

* Fix line computations when relative paths are being used (#242).

* Fix for Coveralls `Build processing error.` (#285) on pro accounts from
  Travis CI (#306, @kiwiroy).

* Keep attributes of function bodies (#311, @gaborcsardi)

# 3.0.1 #
* Add an RStudio Addin for running a coverage report.

* Never use mcexit fix on windows (#223).

* Fix for a performance regression in parsing and reading parse data (#274).

* Fix `switch` support for packages, which was broken due to a bug in
  how parse data is stored in packages.

* Improve behavior of `switch` coverage, it now supports default values and
  fall through properly.

* Add `-p` flag to gcov command to preserve file paths. Fixes a bug where
  gcov output didn't get reported when multiple compiled source files had
  the same name (#271, @patperry)

# 3.0.0 #
* The covr license has been changed to GPL-3.
* Set environment variable `R_COVR=true` when covr is running (#236, #268).
* Made the gather-and-merge-results step at the end of package_coverage() more memory efficient (#226, @HenrikBengtsson).
* Support code coverage with icc (#247, @QinWang).

# 2.2.2 #
* `filter_not_package_files()` now works if a source reference does not have a filename (#254, @hughjonesd).
* Fix test broken with xml2 v1.1.0
* Filter out non-local filenames from results (#237).
* Vignette rewrite / improvements (#229, @CSJCampbell).
* Fix code that returns `structure(NULL, *)` which is deprecated in R 3.4.0 (#260, #261, @renkun-ken).

# 2.2.1 #
* Fix test broken with DT 0.2

# 2.2.0 #
* Fix tests broken with updated htmlwidgets
* Change report tab title based on filename (Chen Liang).
* Add support for cobertura XML output (@wligtenberg).
* Add mcparallel support by patching `mcparallel:::mcexit()`
  automatically for packages using parallel (#195, @kforner).

# 2.1.0 #
* Add support for GitLab CI (#190, @enbrown).
* Update exclusion documentation to include line_exclusions and function
  exclusions (#191).
* Support coverage of R6 methods (#174).
* Explicitly set default packages (including methods) (#183, #180)
* Set R_LIBS and R_LIBS_SITE as well as R_LIBS_USER (#188).
* Automatically exclude RcppExport files (#170).
* Memoised and Vectorized functions now able to be tracked.

# 2.0.1 #
* Support for filtering by function as well as line.
* Now tracks coverage for RC methods
* Rewrote loading and saving to support parallel code and tests including
  `quit()` calls.
* Made passing code to `function_coverage()` and `package_coverage()` _not_ use
  non-standard evaluation.
* `NULL` statements are analyzed for coverage (#156, @krlmlr).
* Finer coverage analysis for brace-less `if`, `while` and `for` statements (#154, @krlmlr).
* Run any combination of coverage types (#104, #133)
* Remove inconsistencies in line counts between shiny app and services (#129)
* Include header files in gcov output (#112)
* Add support for C++11 (#131)
* Always clean gcov files even on failure (#108)
* zero_coverage works with RStudio markers (#119)
* Remove the devtools dependency

# 1.3.0 #
* Set `.libPaths()` in subprocess to match those in calling process (#140, #147).
* Move devtools dependency to suggests, only needed on windows
* move htmltools to suggests

# Initial Release #
Version 1.7.9
=============

### NEW FEATURES

* [##871](https://github.com/tidyverse/lubridate/issues/893) Add `vctrs` support


### BUG FIXES

* [#890](https://github.com/tidyverse/lubridate/issues/890) Correctly compute year in `quarter(..., with_year = TRUE)`
* [#893](https://github.com/tidyverse/lubridate/issues/893) Fix incorrect parsing of abbreviated months in locales with trailing dot (regression in v1.7.8)
* [#886](https://github.com/tidyverse/lubridate/issues/886) Fix `with_tz()` for POSIXlt objects
* [#887](https://github.com/tidyverse/lubridate/issues/887) Error on invalid numeric input to `month()`
* [#889](https://github.com/tidyverse/lubridate/issues/889) Export new dmonth function

Version 1.7.8
=============

### NEW FEATURES

* (breaking) Year and month durations now assume 365.25 days in a year consistently in conversion and constructors. Particularly `dyears(1) == years(1)` is now `TRUE`.
* Format and print methods for 0-length objects are more consistent.
* New duration constructor `dmonths()` to complement other duration constructors.
*
* `duration()` constructor now accepts `months` and `years` arguments.
* [#629](https://github.com/tidyverse/lubridate/issues/629) Added `format_ISO8601()` methods.
* [#672](https://github.com/tidyverse/lubridate/issues/672) Eliminate all partial argument matches
* [#674](https://github.com/tidyverse/lubridate/issues/674) `as_date()` now ignores the `tz` argument
* [#675](https://github.com/tidyverse/lubridate/issues/675) `force_tz()`, `with_tz()`, `tz<-` convert dates to date-times
* [#681](https://github.com/tidyverse/lubridate/issues/681) New constants `NA_Date_` and `NA_POSIXct_` which parallel built-in primitive constants.
* [#681](https://github.com/tidyverse/lubridate/issues/681) New constructors `Date()` and `POSIXct()` which parallel built-in primitive constructors.
* [#695](https://github.com/tidyverse/lubridate/issues/695) Durations can now be compared with numeric vectors.
* [#707](https://github.com/tidyverse/lubridate/issues/707) Constructors return 0-length inputs when called with no arguments
* [#713](https://github.com/tidyverse/lubridate/issues/713) (breaking) `as_datetime()` always returns a `POSIXct()`
* [#717](https://github.com/tidyverse/lubridate/issues/717) Common generics are now defined in `generics` dependency package.
* [#719](https://github.com/tidyverse/lubridate/issues/719) Negative Durations are now displayed with leading `-`.
* [#829](https://github.com/tidyverse/lubridate/issues/829) `%within%` throws more meaningful messages when applied on unsupported classes
* [#831](https://github.com/tidyverse/lubridate/issues/831) Changing hour, minute or second of Date object now yields POSIXct.
* [#869](https://github.com/tidyverse/lubridate/issues/869) Propagate NAs to all internal components of a Period object

### BUG FIXES

* [#682](https://github.com/tidyverse/lubridate/issues/682) Fix quarter extraction with small `fiscal_start`s.
* [#703](https://github.com/tidyverse/lubridate/issues/703) `leap_year()` works with objects supported by `year()`.
* [#778](https://github.com/tidyverse/lubridate/issues/778) `duration()/period()/make_difftime()` work with repeated units
* `c.Period` concatenation doesn't fail with empty components.
* Honor `exact = TRUE` argument in `parse_date_time2`, which was so far ignored.

Version 1.7.4
=============

### NEW FEATURES

* [#658](https://github.com/tidyverse/lubridate/issues/658) `%within%` now accepts a list of intervals, in which case an instant is checked if it occurs within any of the supplied intervals.

### CHANGES

* [#661](https://github.com/tidyverse/lubridate/issues/661) Throw error on invalid multi-unit rounding.
* [#633](https://github.com/tidyverse/lubridate/issues/633) `%%` on intervals relies on `%m+` arithmetic and doesn't produce NAs when intermediate computations result in non-existent dates.
* `tz()` always returns "UTC" when `tzone` attribute cannot be inferred.

### BUG FIXES

* [#664](https://github.com/tidyverse/lubridate/issues/664) Fix lookup of period functions in `as.period`
* [#649](https://github.com/tidyverse/lubridate/issues/664) Fix system timezone memoization

Version 1.7.3
=============

### BUG FIXES

* [#643](https://github.com/tidyverse/lubridate/issues/643), [#640](https://github.com/tidyverse/lubridate/issues/640), [#645](https://github.com/tidyverse/lubridate/issues/645) Fix faulty caching of system timezone.

Version 1.7.2
=============

### NEW FEATURES

* Durations, Periods and difftimes are now comparable with each other.
* `interval` constructor accepts start and end character vectors in ISO 8601 format
* [#362](https://github.com/tidyverse/lubridate/issues/362) Add support for ISO 8601 formats in interval constructor
* [#622](https://github.com/tidyverse/lubridate/issues/622) Add support for ISO 8601 formats in periods and durations constructor

### CHANGES

* Correct license to the originally intended GPL (>= 2)

### BUG FIXES

* [#605](https://github.com/tidyverse/lubridate/issues/605) Fix wrong ceiling of days during DST transition.
* [#607](https://github.com/tidyverse/lubridate/issues/607) Re-instate `format` argument to `as_date` and `as_datetime` (regression in v1.7.1)
* Fix intersection of intervals with missing values.
* Fix UBSAN errors in update.cpp

Version 1.7.1
=============

### BUG FIXES

* [#575](https://github.com/tidyverse/lubridate/issues/598), [#600](https://github.com/tidyverse/lubridate/issues/600), [#602](https://github.com/tidyverse/lubridate/issues/602) Fix zoneinfo lookup on windows and solaris.
* [#598](https://github.com/tidyverse/lubridate/issues/598) Fix broken parsing of `ymd_hms` strings by `as_date`.
* [#597](https://github.com/tidyverse/lubridate/issues/597) Fix broken parsing of `ymd` strings by `as_datetime`.

Version 1.7.0
=============

### NEW FEATURES

* Reduced memory footprint on `trunc_multi_unit` so that it overwrites the vector argument `x` versus making a new vector `y`.
* [#438](https://github.com/tidyverse/lubridate/issues/438) New function `force_tzs` for "enforcement" of heterogeneous time zones.
* [#438](https://github.com/tidyverse/lubridate/issues/438) New function `local_time` for the retrieval of local day time in different time zones.
* [#560](https://github.com/tidyverse/lubridate/issues/560) New argument `cutoff_2000` for parsing functions to indicate 20th century cutoff for `y` format.
* [#257](https://github.com/tidyverse/lubridate/issues/257) New `week_start` parameter in `wday` and `wday<-` to set week start.
* [#401](https://github.com/tidyverse/lubridate/issues/401) New parameter `locale` in `wday`. Labels of the returned factors (when `label=TRUE`) now respect current locale.
* [#485](https://github.com/tidyverse/lubridate/pull/485) `quarter` gained a new argument `fiscal_start` to address the issue of different fiscal conventions.
* [#492](https://github.com/tidyverse/lubridate/issues/492) New functions `epiweek` and `epiyear`.
* [#508](https://github.com/tidyverse/lubridate/pull/508) New parameter `locale` in `month`. Labels of the returned factors (when `label=TRUE`) now respect current locale.
* [#509](https://github.com/tidyverse/lubridate/issues/509) New parameter `week_start` to `floor_date`, `ceiling_date` and `round_date`.
* [#519](https://github.com/tidyverse/lubridate/issues/519) Support fractional units in duration and period string constructors.
* [#502](https://github.com/tidyverse/lubridate/issues/502) Support rounding to fractions of a seconds.
* [#529](https://github.com/tidyverse/lubridate/issues/529) Internal parser now ignores the case of alpha months (B format)
* [#535](https://github.com/tidyverse/lubridate/issues/535) Rounding to `season` is now supported.
* [#536](https://github.com/tidyverse/lubridate/issues/536) `as_date` and `as_datetime` now understand character vectors.
* New parsing parameters to `parse_date_time` - `train=TRUE` and `drop=FALSE` which allow more refined control of the format guessing. Formats are no longer dropped in the process by default, process which resulted in surprising behavior on several occasions ([#516](https://github.com/tidyverse/lubridate/issues/516),[#308](https://github.com/tidyverse/lubridate/issues/308),[#307](https://github.com/tidyverse/lubridate/issues/307)).

### CHANGES

* [#401](https://github.com/tidyverse/lubridate/issues/401) **[Breaking Change]** Labels returned by `wday` and `month` are now in current locale. The abbreviated labels in English locales have been changed to standard abbreviations (Tues -> Tue, Thurs -> Thu etc.).
* [#469](https://github.com/tidyverse/lubridate/issues/469) Throw warning in `with_tz` on invalid timezone.
* [#572](https://github.com/tidyverse/lubridate/issues/572) `B` and `b` formats no longer match numeric months. This corresponds to the original intent, and was always documented as such.

### BUG FIXES

* [#314](https://github.com/tidyverse/lubridate/issues/314), [#407](https://github.com/tidyverse/lubridate/issues/407), [#499](https://github.com/tidyverse/lubridate/issues/499) Make `days`, `dhours`, `round_date` work when the methods package is not loaded.
* [#543](https://github.com/tidyverse/lubridate/issues/543) Make `wday` work on character inputs as it is the case with all other day accessors.
* [#566](https://github.com/tidyverse/lubridate/issues/566) Comparing durations and periods no-longer infloops.
* [#556](https://github.com/tidyverse/lubridate/issues/556) Fix incorrect scoring of `y` format when it's the last in format order (as in `mdy`).
* [#584](https://github.com/tidyverse/lubridate/issues/584) Fix interval by period division.
* [#559](https://github.com/tidyverse/lubridate/issues/559) Parsing of alpha-months in English locales now drops correctly to low level C parsing. Thus, parsing with multiple orders containing `m` and `b` formats now works correctly.
* [#570](https://github.com/tidyverse/lubridate/issues/570), [#574](https://github.com/tidyverse/lubridate/issues/574) Fix broken `date()` when called with missing argument.
* [#567](https://github.com/tidyverse/lubridate/issues/567) Fix year update and rounding for leap years.
* [#545](https://github.com/tidyverse/lubridate/pull/545) Fix wrong locale selection in stamp.
* [#466](https://github.com/tidyverse/lubridate/pull/466) Fix wrong formats within ymd_h family of functions.
* [#472](https://github.com/tidyverse/lubridate/pull/472) Printing method for duration doesn't throw format error on fractional seconds.
* [#475](https://github.com/tidyverse/lubridate/pull/475) character<> comparisons is no longer slow.
* [#483](https://github.com/tidyverse/lubridate/pull/483) Fix add_duration_to_date error when duration first element is NA.
* [#486](https://github.com/tidyverse/lubridate/issues/486) ceiling_date handles `NA` properly.
* [#491](https://github.com/tidyverse/lubridate/issues/491) `make_datetime` respects `tz` argument and is much faster now.
* [#507](https://github.com/tidyverse/lubridate/issues/507) Period and duration parsers now understand 0 units.
* [#524](https://github.com/tidyverse/lubridate/pull/524) Correctly compute length of period in months (issue #490)
* [#525](https://github.com/tidyverse/lubridate/pull/525) Fix to prevent `day<-`, `minute<-`, etc. from producing an error when length(x) is 0 (issue #517)
* [#530](https://github.com/tidyverse/lubridate/issues/530) `parse_date_time` now throw warnings only for actual parsing errors (input with all NAs are silent)
* [#534](https://github.com/tidyverse/lubridate/issues/534) Fix arithmetics with large numbers
* [#554](https://github.com/tidyverse/lubridate/pull/554) Fix tests when running in non-English locales

Version 1.6.0
=============

### NEW FEATURES

* [#464](https://github.com/tidyverse/lubridate/issues/464) New function `semester` to extract semesters form date-time objects.
* [#459](https://github.com/tidyverse/lubridate/issues/459) Flexible C-level parsing for periods and durations has been implemented; `period` and `duration` constructors now accept string as first argument. Same parsing rules apply to the 'unit' parameter in rounding functions.
* [#459](https://github.com/tidyverse/lubridate/issues/459) Comparison between character vectors and periods/durations is now possible.
* [#287](https://github.com/tidyverse/lubridate/issues/287) C-level and derivative parsers now handle English months (%b and %B formats) irrespective of the current locale.
* [#327](https://github.com/tidyverse/lubridate/issues/327) C-level and derivative parsers now handles English AM/PM indicator irrespective of the current locale.
* [#417](https://github.com/tidyverse/lubridate/issues/417) `hms`, `hm`, `ms` gained new argument `roll=TRUE` which rolls minutes and seconds bigger than 59 towards higher units.
* [#445](https://github.com/tidyverse/lubridate/issues/445) Division of intervals by periods is now accurate.
* [#442](https://github.com/tidyverse/lubridate/issues/442) `round_date`, `floor_date` and `ceiling_date` now support rounding to multiple of units.
* [#422](https://github.com/tidyverse/lubridate/issues/422) New parsing function `yq` for parsing most common version of quarter strings.
* [#422](https://github.com/tidyverse/lubridate/issues/422) New format `q` for parsing quarters in all lubridate parsing functions.
* [#441](https://github.com/tidyverse/lubridate/issues/441) Comparison between POSIXt and Date objects is now possible.
* [#437](https://github.com/tidyverse/lubridate/issues/437) New function `as_datetime` to coerce to POSIXct object. A counterpart of `as_date`.
* [#412](https://github.com/tidyverse/lubridate/issues/412) New function `make_date` to produce Date objects. A counterpart of `make_datetime`.
* [#443](https://github.com/tidyverse/lubridate/issues/443) Behavior of `ceiling_date` for `Date` objects was changed to what most of the users expect. Rounding up by months now produces first day of the next months even for first day of the month.
* [#268](https://github.com/tidyverse/lubridate/issues/268) `round_date`, `ceiling_date`, and `floor_date` now accept "quarter", "bimonth", and "halfyear" as `unit` options.
* [#418](https://github.com/tidyverse/lubridate/issues/418) C level parsing functions understand 24:00:00 in datetime strings.

### CHANGES

* Low letter specs for HMS (hms,hm,ms) in `parse_date_time` and related functions are now deprecated.
* [#445](https://github.com/tidyverse/lubridate/issues/445) No more warning on occasional imprecise period length conversions. Imprecise arithmetics with periods is extensively documented.
* `pretty.*` family of functions were renamed and are no longer exported. If you need to use them, use `lubridate:::pretty_*` versions.
* `change_on_boundary` argument in `ceiling_date` does not allow for global option anymore.
* `as.duration`, `as.numeric` don't show "only estimate" messages on conversion from periods. The occasional approximate conversion is documented and deemed common knowledge.
* `as.numeric` with `unit="month"` now works on duration objects.
* [#403](https://github.com/tidyverse/lubridate/issues/403) Update on `Date` objects now return `POSIXct` instead of `POSIXlt`.
* [#411](https://github.com/tidyverse/lubridate/issues/411) format `mdy` or `myd` beginning with `"January"` or `"Jan"` now parsing correctly
* `here` and `olson_time_zones` were deprecated in favor of `new` and base `OlsonNames` respectively.
* Internally, S4 Compare and Ops generics were cleaned and simplified.
* [#456](https://github.com/tidyverse/lubridate/issues/456) Evaluation output in documentation examples was removed.

### BUG FIXES

* [#479](https://github.com/tidyverse/lubridate/issues/479) Fix the inconsistent behavior in `ceiling_date` when `unit = "week"`
* [#463](https://github.com/tidyverse/lubridate/issues/463) Fix NA subscripting error in %m+% when rollback is involved.
* [#462](https://github.com/tidyverse/lubridate/issues/462) Non-numeric or non-character arguments are disallowed as arguments to `period` and `duration` constructors.
* [#458](https://github.com/tidyverse/lubridate/issues/458) When year is missing in parsing, return consistently year 0.
* [#448](https://github.com/tidyverse/lubridate/issues/448) Correctly handle missing months and days in C parser.
* [#450](https://github.com/tidyverse/lubridate/issues/450) Fix incorrect handling of DST gaps in `date_decimal` and `decimal_date`.
* [#420](https://github.com/tidyverse/lubridate/issues/420) `as.numeric` correctly converts periods to (aproximate) numeric time lengths.

Version 1.5.6
============

### NEW FEATURES

* [#390](https://github.com/tidyverse/lubridate/issues/390) `ceiling_date` gains new argument `change_on_boundary` to allow ceiling on boundary of date-time objects.
* C parser can now produce a list of date-time components suitable for POSIXlt constructors.
* `parse_date_time2` and `fast_strptime` gain new `lt` argument to control type of output.
* [#373](https://github.com/tidyverse/lubridate/issues/373) New `date` and `date<-` additions to the `year`, `month` etc family of accessors.
* [#365](https://github.com/tidyverse/lubridate/issues/365) New very fast datetime constructor `make_datetime` (dropin replacement of `ISOdatetime`).
* [#344](https://github.com/tidyverse/lubridate/issues/344) `force_tz` and `with_tz` can handle data.frames component-wise.
* [#355](https://github.com/tidyverse/lubridate/issues/355) New `as_date` replacement of `as.Date` with more intuitive behavior with non-UTC timezones.
* [#333](https://github.com/tidyverse/lubridate/issues/333) `hms` parsers now handle negative components.

### CHANGES

* [#391](https://github.com/tidyverse/lubridate/issues/391) `ymd` family of functions return `Date` object when `tz` argument is NULL (new default) and POSIXct otherwise.
* [#364](https://github.com/tidyverse/lubridate/issues/364) Remove epoch functions.
* For consistency with `base:strptime` `fast_strptime` now returns `POSIXlt` object. That is, its `lt` argument defaults to `TRUE`.

### BUG FIXES

* `interval` constructor treats timezones correctly and  works with UTC whenever meaningful.
* [#371](https://github.com/tidyverse/lubridate/issues/371) `as.period` correctly computes months with intervals spanning multiple years.
* [#388](https://github.com/tidyverse/lubridate/issues/388) `time_length` and `add_with_rollback` now work correctly with missing intervals.
* [#394](https://github.com/tidyverse/lubridate/issues/394) `fast_strptime` and `parse_date_time2` correctly treat non-UTC time zones.
* [#399](https://github.com/tidyverse/lubridate/issues/399) `floor_date` and `round_date` are not preserving tz component for larger than day units

Version 1.5.0
=============

### NEW FEATURES

* New `time_length` method.
* Added `isoyear` function to line up with `isoweek`.
* [#326](https://github.com/tidyverse/lubridate/issues/326) Added `exact = TRUE` option to `parse_date_time` for faster and much more flexible specification of formats.
* New `simple` argument to `fit_to_timeline` and `update` methods mostly intended for internal use.
* [#315](https://github.com/tidyverse/lubridate/issues/315) Implement `unique` method for `interval` class.
* [#295](https://github.com/tidyverse/lubridate/issues/295) New args `preserve_hms` and `roll_to_first` in `rollback` function.
* [#303](https://github.com/tidyverse/lubridate/issues/303) New `quarter` option in `floor_date` and friends.
* [#348](https://github.com/tidyverse/lubridate/issues/348) New `as.list.Interval` S3 method.
* [#278](https://github.com/tidyverse/lubridate/issues/278) Added settors and accessors for `qday` (quarter day).

### CHANGES

* New maintainer Vitalie Spinu (@vspinu)
* Time span constructors were re-factored; `new_interval`, `new_period`, `new_duration`, `new_difftime` were deprecated in favour of the more powerful `interval`, `period`, `duration` and `make_difftime` functions.
* `eseconds`, `eminutes` etc. were deprecated in favour of `dsecons`, `dminutes` etc.
* Many documentation improvements.
* New testthat conventions are adopted. Tests are now in `test/testthat`.
* Internally `isodate` was replaced with a much faster `parse_date_time2(paste(...))` alternative
* [#325](https://github.com/tidyverse/lubridate/issues/325) `Lubridate`'s `trunc`, `ceiling` and `floor` functions have been optimised and now are relying on R's `trunc.POSIXct` whenever possible.
* [#285](https://github.com/tidyverse/lubridate/issues/285) Algebraic computations with negative periods are behaving asymmetrically with respect to their positive counterparts.
* Made necessary changes to accommodate new zoo-based `fst` objects.

### BUG FIXES

* [#360](https://github.com/tidyverse/lubridate/issues/360) Fix c parser for Z (zulu) indicator.
* [#322](https://github.com/tidyverse/lubridate/issues/322) Explicitly encode formatted string with `enc2utf8`.
* [#302](https://github.com/tidyverse/lubridate/issues/302) Allow parsing long numbers such as 20140911000000 as date+time.
* [#349](https://github.com/tidyverse/lubridate/issues/349) Fix broken interval -> period conversion.
* [#336](https://github.com/tidyverse/lubridate/issues/336) Fix broken interval-> period conversion with negative diffs.
* [#227](https://github.com/tidyverse/lubridate/issues/227) Treat "days" and "years" units specially for pretty.point.
* [#286](https://github.com/tidyverse/lubridate/issues/286) %m+-% correctly handles dHMS period components.
* [#323](https://github.com/tidyverse/lubridate/issues/323) Implement coercion methods for Duration class.
* [#226](https://github.com/tidyverse/lubridate/issues/226) Propagate NAs in `int_standardize`
* [#235](https://github.com/tidyverse/lubridate/issues/235) Fix integer division with months and years.
* [#240](https://github.com/tidyverse/lubridate/issues/240) Make `ceiling_date` skip day light gap.
* [#254](https://github.com/tidyverse/lubridate/issues/254) Don't preprocess a/A formats if expressly specified by the user.
* [#289](https://github.com/tidyverse/lubridate/issues/289) Check for valid day-months combinations in C parser.
* [#306](https://github.com/tidyverse/lubridate/issues/306) When needed double guess with `preproc_wday=T`.
* [#308](https://github.com/tidyverse/lubridate/issues/308) Document sparce format guessing in `parse_date_time`.
* [#313](https://github.com/tidyverse/lubridate/issues/313) Fixed and optimized `fit_to_timeline` function.
* [#311](https://github.com/tidyverse/lubridate/issues/311) Always use UTC in `isoweek` computation
* [#294](https://github.com/tidyverse/lubridate/issues/294) Don't use years in `seconds_to_period`.
* Values on `$<-` assignment for periods are now properly recycled.
* Correctly handle NA subscripting in `round_date`.


Version 1.4.0
=============

### CHANGES

* [#219](https://github.com/tidyverse/lubridate/issues/219) In `interval` use UTC when tzone is missing.
* [#255](https://github.com/tidyverse/lubridate/issues/255) Parse yy > 68 as 19yy to comply with `strptime`.

### BUG FIXES

* [#266](https://github.com/tidyverse/lubridate/issues/266) Include `time-zones.R` in `coercion.R`.
* [#251](https://github.com/tidyverse/lubridate/issues/251) Correct computation of weeks.
* [#262](https://github.com/tidyverse/lubridate/issues/262) Document that month boundary is the first second of the month.
* [#270](https://github.com/tidyverse/lubridate/issues/270) Add check for empty unit names in `standardise_lt_names`.
* [#276](https://github.com/tidyverse/lubridate/issues/276) Perform conversion in `as.period.period` if `unit != NULL`.
* [#284](https://github.com/tidyverse/lubridate/issues/284) Compute periods in `as.period.interval` without recurring to modulo arithmetic.
* [#272](https://github.com/tidyverse/lubridate/issues/272) Update examples for `hms`, `hm` and `ms` for new printing style.
* [#236](https://github.com/tidyverse/lubridate/issues/236) Don't allow zeros in month and day during parsing.
* [#247](https://github.com/tidyverse/lubridate/issues/247) Uninitialized index was mistakenly used in subseting.
* [#229](https://github.com/tidyverse/lubridate/issues/229) `guess_formats` now matches flex regexp first.
* `dmilliseconds` now correctly returns a `Duration` object.
* Fixed setdiff for discontinuous intervals.


Version 1.3.3
=============

### CHANGES

* New low level C parser for numeric formats and two new front-end R functions
  parse_date_time2 and fast_strptime. The achieved speed up is 50-100x as
  compared to standard as.POSIXct and strptime functions.

  The user level parser functions of ymd_hms family drop to these C routines
  whenever plain numeric formats are detected.

### BUG FIXES

* olson_time_zones now supports Solaris OS
* infinite recursion on parsing non-existing leap times was fixed


Version 1.3.2
=============

* Lubridate's s4 methods no longer use the representation argument, which has been deprecated in R 3.0.0 (see ?setClass). As a result, lubridate is no longer backwards compatible with R <3.0.0.


Version 1.3.0
=============

### CHANGES

* v1.3.0. treats math with month and year Periods more consistently. If adding or subtracting n months would result in a non-existent date, lubridate will return an NA instead of a day in the following month or year. For example, `ymd("2013-01-31") + months(1)` will return `NA` instead of `2013-03-04` as in v1.2.0. `ymd("2012-02-29") + years(1)` will also return an `NA`. This rule change helps ensure that date + timespan - timespan = date (or NA). If you'd prefer that such arithmetic just returns the last day of the resulting month, see `%m+%` and `%m-%`.
* update.POSIXct and update.POSIXlt have been rewritten to be 7x faster than their versions in v1.2.0. The speed gain is felt in `force_tz`, `with_tz`, `floor_date`, `ceiling_date`, `second<-`, `minute<-`, `hour<-`, `day<-`, `month<-`, `year<-`, and other functions that rely on update (such as math with Periods).
* lubridate includes a Korean translation provided by http://korea.gnu.org/gnustats/

### NEW FEATURES

* lubridate parser and stamp functions now handle ISO8601 date format (e.g., 2013-01-24 19:39:07.880-06:00, 2013-01-24 19:39:07.880Z)
* lubridate v1.3.0 comes with a new R vignette. see `browseVignettes("lubridate")` to view it.
* The accessors `second`, `minute`, `hour`, `day`, `month`, `year` and the settors `second<-`, `minute<-`, `hour<-`, `day<-`, `month<-`, `year<-` now work on Period class objects
* users can control which messages lubridate returns when parsing and estimating with the global option lubridate.verbose. Run `options(lubridate.verbose = TRUE)` to turn parsing messages on. Run `options(lubridate.verbose = FALSE)` to turn estimation and coercion messages off.
* lubridate parser functions now propagate NA's just as as.POSIXct, strptime and other functions do. Previously lubridate's parse functions would only return an error.
* added [[ and [[<- methods for INterval, Period and Duration class objects
* added `%m+%` and `%m-%` methods for Interval and Duration class objects that throw useful errors.
* `olson_time_zones` retreives a character vector is Olson-style time zone names to use in lubridate
* summary methods for Interval, Period, and Duration classes
* date_decimal converts a date written as a decimal of a year into a POSIXct date-time

### BUG FIXES

* fixed bug in way update.POSIXct and update.POSIXlt handle dates that occur in the fall daylight savings overlap. update will choose the date-time closest to the original date time (on the timeline) when two identical clock times exist due to the DST overlap.
* fixed bugs that created unintuitive results for `as.interval`, `int_overlaps`, `%within%` and the interval methods of `c`, `intersect`, `union`, `setdiff`, and `summary`.
* parse functions, `as.interval`, `as.period` and `as.duration` now handlevectors of NA's without returning errors.
* parsers better handle vectors of input that have more than 100 elements and many NAs
* data frames that contain timespan objects with NAs in thme no longer fail toprint
* `round_date`, `ceiling_date` and `update` now correctly handle input of length zero
* `decimal_date` no longer returns NaN for first second of the year

Version 1.2.0
=============

### CHANGES

* lubridate 1.2.0 is significantly faster than lubridate 1.1.0. This is
largely thanks to a parser rewrite submitted by Vitalie Spinu. Thank you,
Vitalie. Some metrics:
  - parser speed up - 60x faster
  - `with_tz` speed up - 15x faster
  - `force_tz` speed up - 3x faster

* Development for 1.2.0 has also focused on improving the way we work with
months. `rollback` rolls dates back to the last day of the previous month.
provides more options for working with months. `days_in_month` finds the
number of days in a date's month. And, `%m+%` and `%m-%` provide a new way to
### handle unequal month lengths while doing arithmetic. See NEW FEATURES for more
details

* date parsing can now parse multiple date formats within the same vector of
date-times. Parsing can also recognize a greater variety of date-time formats
as well as incomplete (truncated) date-times. Contributed by Vitalie Spinu.
Thank you, Vitalie.

* 1.2.0 introduces a new display format for periods. The display is more math
and international friendly.

* 1.2.0 transforms negative intervals into periods much more gracefully (e.g, -
 3 days instead of -1 years, 11 months, and 27 days)

* S3 update methods are now exported

### NEW FEATURES

* `stamp` allows users to print dates in whatever form they like. Contributed
by Vitalie Spinu. Thank you, Vitalie.

* periods now handle fractional seconds. Contributed by Vitalie Spinu. Thank
you, Vitalie.

* date parsing can now parse multiple date formats within the same vector of
date-times. Parsing can also recognize a greater variety of date-time formats
as well as incomplete (truncated) date-times. Contributed by Vitalie Spinu.
Thank you, Vitalie.

* `sort`, `order`, `rank` and `xtfrm` now work with periods

* `as.period.Interval` accepts a unit argument. `as.period` will convert
intervals into periods no larger than the supplied unit.

* `days_in_month` takes a date, returns the number of days in the date's month.
 Contributed by Richard Cotton. Thank you, Richard.

* `%m+%` and `%m-%` perform addition and subtraction with months (and years)
without rollover at the end of a month. These can be used in place of + and -.
These can't be used with periods smaller than a month, which should be handled
separately. An example of the new behavior:

    ymd("2010-01-31") %m+% months(1)
    # "2010-02-28 UTC"

    ymd("2010-01-31") + months(1)
    # "2010-03-03 UTC"

    ymd("2010-03-31") %m-% months(1)
    # "2010-02-28 UTC"

    ymd("2010-01-31") - months(1)
    # "2010-03-03 UTC"

* `rollback` rolls a date back to the last day of the previous month.

* `quarter` returns the fiscal quarter that a date occurs in. Like `quartes`
in base R, but returns a numeric instead of a character string.

### BUG FIXES

* date parsers now handle NAs

* periods now handle NAs

* `[<-` now correctly updates all elements of a period inside a vector, list,
or data.frame

* `period()` now works with unit = "weeks"

* `ceiling_date` no longer rounds up if a date is already at a ceiling

* the redundant (i.e, repeated) hour of fall daylight savings time now
displays with the correct time zone

* `update.POSIXct` and `update.POSIXlt` handle vectors that sum to zero in the
days argument

* the format method for periods, intervals and duration now accurately
displays objects of length 0.



Version 1.1.0
=============

### CHANGES

* lubridate no longer overwrites base R methods for +, - , *, /, %%, and %/%.
To recreate the previous experience of subtracting two date times to create an
interval, we've added the interval creation function %--%.

* lubridate has moved to an S4 object system. Timespans, Intervals, Durations,
and Periods have each been redefined as an S4 class with its own methods.

* arithmetic operations will no longer perform implicit class changes between
timespans. Users must explicitly state how and when they wish class changes to
occur with as.period(), as.duration(), and as.interval(). This makes code
written with lubridate more robust, as such implicit changes often did not
produce consistent behavior across a variety of operations. It also allows
lubridate to be less chatty with fewer console messages. lubridate does not
need to explain what it is doing, because it no longer attempts to do things
whose outcome would not be clear. On the other hand, arithmetic between
multiple time classes will produce informative error messages.

* the internal structure of lubridate R code has been reorganized at
https://github.com/tidyverse/lubridate to make lubridate more development friendly.


### NEW FEATURES

* intervals are now more useful and lubridate has more ways to manipulate them.
Intervals can be created with %--%; modified with int_shift(), int_flip(), and
int_standardize(); manipulated with intersect(), union(), and setdiff(); and
used in logical tests with int_aligns(), int_overlaps(), and %within%.
lubridate will no longer perform arithmetic between two intervals because the
correct results of such operations is no more obvious than the correct result
of adding two dates. Instead users are encouraged to use the new set
operations or to directly modify intervals with int_start() and int_end(),
which can also be used as settors. lubridate now supports negative intervals
as well as positive intervals. Intervals also now display with a time zone.

* Modulo methods for timespans have been changed to return a timespan. this
allows modulo methods to be used with integer division in an intuitive manner,
e.g. `a = a %/% b * b + a %% b`

Users can still acheive a numerical result by using as.numeric() on input
before performing modulo.

* Periods, durations, and intervals can now all be put into a data frame.

* Periods, durations, and intervals can be intuitively subset with $ and [].
These operations also can be used as settors with <-.

* The parsing functions and the as.period method for intervals are now
slightly faster.

* month<- and wday<- settors accept names as well as numbers

* parsing functions now have a quiet argument to parse without messages and a
tz argument to directly parse times into the desired time zone.

* logical comparison methods now work for period objects.


Version 0.2.6
=============


* use `test_package` to avoid incompatibility with current version of
  `testthat`

* other minor fixes to pass `R CMD check`


Version 0.2.5
=============

* added ymdThms() for parsing ISO 8061 formatted combned dates and times

### BUG FIXES

* removed bug in parsing dates with "T" in them

* modified as.period.interval() to display periods in positive units


Version 0.2.4
=============

* Add citations to JSS article


Version 0.2.3
=============

### NEW FEATURES

* ymd_hms(), hms(), and ms() functions can now parse dates that include
decimal values in the seconds element.

* milliseconds(), microseconds(), nanoseconds(), and picoseconds() create
period objects of the specified lengths. dmilliseconds(), dmicroseconds(),
dnanoseconds(), and dpicoseconds() make duration objects of the specified
lengths.


### BUG FIXES

* lubridate no longer overwrites months(), start(), and end() from base R.
Start and end have been replaced with int_start() and int_end().

* lubridate imports plyr and stringr packages, instead of depending on them.


Version 0.2.2
=============

### NEW FEATURES

* made division, modulo, and integer division operations compatible with
difftimes

* created c() methods for periods and durations


### BUG FIXES

* fixed bug in division, modulo, and integer operations with timespans



Version 0.2.1
=============

### NEW FEATURES

* created parsing functions ymd_hm ymd_h dmy_hms dmy_hm dmy_h mdy_hms mdy_hm
mdy_h ydm_hms ydm_hm ydm_h, which operate in the same way as ymd_hms().

### BUG FIXES

* fixed bug in add_dates(). duration objects can now be successfully added to
numeric objects.


-----------
Version 0.2
===========

### NEW FEATURES

* division between timespans: each timespan class (durations, periods,
intervals) can be divided by other timespans. For example, how many weeks are
there between Halloween and Christmas?: (christmas - halloween) / weeks(1)

* modulo operations between timespans

* duration objects now have their own class and display format separate from
difftimes

* interval objects now use an improved data structure and have a cleaner
display format

* lubridate now loads its own namespace

* math operations now automatically coerce interval objects to duration objects.
Allows intervals to be used "right out of the box" without error messages.

* created start() and end() functions for accessing and changing the boundary
date-times of an interval


* rep() methods for periods, intervals, and durations



### MINOR CHANGES

* added a package help page with functions listed by purpose

* eseconds(), eminutes(), etc. are aliased to dseconds(), dminutes(), etc. to
make it easier to remember they are duration objects.

* changed leap.years() to leap_years() to maintain consistent naming scheme



### BUG FIXES

* rewrote as.period() to create only positive periods.

* fixed rollover bug in update.POSIXct()

* edited make_diff() to display in days when approporiate, not weeks
# sjmisc 2.8.5

## Changes to functions

* `flat_table()` gains a `weights`-argument.

## Bug fixes

* `descr()` calculated wrong percentage of missing values for weighted data.
* Fixed issue in `rec` when `min`, `max`, `lo` or `hi` was used to recode a numeric into a character vector, and the new recode string contained one of these four strings as pattern.

# sjmisc 2.8.4

## Changes to functions

* `descr()` now also calculates the IQR.
* Revised `print()`-method for `frq()`.
* Minor changes to be compatible with forthcoming dplyr-release.

# sjmisc 2.8.3

## New functions

* Added `as.data.frame()` for `frq()`.

## Changes to functions

* `typical_value()` now returns the median for integer-values (instead of mean), to preserve the integer-type of a variable.
* The recode-pattern in `rec()` now also works for character variables with whitespaces.
* `rec()` now warns explicetly for possible non-intended multiple assignment of identical new recode-values.
* Improved printing for `frq()`.
* `merge_imputations()` now returns the plot-object as well.
* `to_numeric()` as alias for `to_value()`.

## Bug fixes

* Fixed warning from CRAN checks.
* Fixed errors from CRAN checks.

# sjmisc 2.8.2

## General

* Alias `find_variables()` (alias for `find_var()`) was renamed to `find_in_data()`, to avoid conflicts with package *insight*.
* `rename_variables()` and `rename_columns()` are aliases for `var_rename()`.

## Changes to functions

* `frq()` now also prints frequencies of logical conditions, e.g. how many values are lower or greater than a certain threshold.
* `frq()` gets a `min.frq`-argument, indicating the minimum frequency for which a value will be shown in the output.
* `descr()` gets a `show` argument to show selected columns only.
* `descr()` gets a `file`-argument to write the output as HTML file.
* `var_rename()` now also accepts a named vector with multiple elements as ellipses-argument.

## Bug fixes

* Fixed erroneously warning in `de_mean()`.
* `merge_df()` now removes columns with identical column names inside a data frame before merging, to avoid errors.
* Fixed issue when printing character vectors in `frq()`, where first element was empty, and vectors were not provided as data frame argument.
* Fixed issue in `word_wrap()` when processing expressions.
* Fixed issue in `rec()` with token `rec = "rev"`, when reversing labelled vectors with more value labels than values.

# sjmisc 2.8.1

## General

* `find_variables()` as alias for `find_var()`.
* Revised docs.

## Bug fixes

* Fixed issue with forthcoming update of the **rlang** package.

# sjmisc 2.8.0

## General

* Some print-methods, especially for grouped data frames, are now more compact.

## New functions

* `reshape_longer()`, as alternative to `to_long()`, probably easier to remember (function and argument-names).

## Bug fixes

* `frq()` displayed labels as `NA` in some situations for grouped data frames with more than one group, when data were not labelled.

# sjmisc 2.7.8

## General

* Reduce package dependencies.
* `str_pos()` was renamed into `str_find()`.
* New package-vignette **Recoding Variables**.

## New functions

* `typical_value()`, which was formerly located in package _sjstats_.

## Changes to functions

* `is_whole()` now automatically removes missing values from vectors.
* `is_empty()` now also checks lists with only `NULL`-elements.

## Bug fixes

* Better handling of factors in `merge_imputations()`, which previously could result in `NA`-values when merging imputed values into one variable.
* Fix issue in `is_empty()` in case the vector had non-missing values, but first element of vector was `NA`.
* Fixed bug in `frq()` for grouped data frame, when grouping variable was a character vector. In this case, group titles were mixed up.
* Fix encoding issues in help-files.

# sjmisc 2.7.7

## New functions

* `tidy_values()` to "clean" values (i.e. remove special chars) of character vectors or levels of factors.
* `add_id()` to quickly add an ID variable to (grouped) data frames.

## Changes to functions

* `frq()` gets a `show.na`-argument, to (automatically) show or hide the information for `NA`-values from the output.
* The `weights`-argument in `frq()` now also accepts vectors, and is not limited to variable names. Note that these vectors must be part of a data frame.
* For recode-functions (like `rec()`, `dicho()`, ...), if `suffix = ""` and `append = TRUE`, existing variables will be replaced by the new, recoded variables.
* Improved performance for `group_str()`.
* `var_rename()` now supports quasi-quotation (see Examples).
* `row_sums()` and `row_means()` now return the input data frame when this data frame only had one column and no row means or sums were calculated. The returned data frame still gets the new variable name defined in `var`.

## Bug fixes

* `complete_cases()` returned an empty vector instead of all indexes if all cases (rows) of a data frame were complete.
* Fix issue with `to_dummy()` for character-vector input.
* Fix issue with missing values in `group_str()`.
* Fix issue with grouped data frames in `frq()` when `grp.strings = TRUE`.

# sjmisc 2.7.6

## Changes to functions

* `frq()` gets a `file` and `encoding` argument, to save the HTML output as file.
* `add_variables()` and `move_columns()` now preserve the attributes of a data frame.

# sjmisc 2.7.5

## General

* Reduce package dependencies.

## New functions

* `de_mean()` to compute group-meaned and de-meaned variables.
* `add_variables()` and `add_case()` to add columns or rows in a convenient way to a data frame.
* `move_columns()` to move one or more columns to another position in a data frame.
* `is_num_chr()` to check whether a character vector has only numeric strings.
* `seq_col()` and `seq_row()` as convenient wrapper to create a regular sequence for column or row numbers.

## Changes to functions

* `descr()` gets a `weights`-argument, to print weighted descriptive statistics.
* The `n`-argument in `row_means()` and `row_sums()` now also may be `Inf`, to compute means or sums only if all values in a row are valid (i.e. non-missing).
* Argument `weight.by` in `frq()` was renamed into `weights`.
* `frq()` gets a `title`-argument, to specify an alternative title to the variable label.

## Bug fixes

* `round_num()` preserves data frame attributes.
* `frq()` printed frequencies of grouping-variable for grouped data frames, when `weights` was not `NULL`.
* Fixed issue with wrong title in `frq()` for grouped data frames, when grouping variable was an unlabelled factor.

# sjmisc 2.7.4

## New functions

* `has_na()` to check if variables or observations in a data frame contain `NA`, `NaN` or `Inf` values. Convenient shortcuts for this function are `complete_cases()`, `incomplete_cases()`, `complete_vars()` and `incomplete_vars()`.
* `total_mean()` to compute the overall mean of all values from all columns in a data frame.
* `prcn()` to convert numeric scalars between 0 and 1 into a character-percentage value.
* `numeric_to_factor()` to convert numeric variables into factors, using associated value labels as factor levels.

## Changes to functions

* `set_na()` now also replaces different values per variable into `NA`.
* Changed behaviour of `row_sums()` and missing values. `row_sums()` gets a `n`-argument and now computes row sums if a row has at least `n` non-missing values.

# sjmisc 2.7.3

## General

* A test-suite was added to the package.
* Updated reference in `CITATION` to the publication in the Journal of Open Source Software.

## New functions

* `is_cross_classified()` to check whether two factors are partially crossed.

## Changes to functions

* `ref_lvl()` now also accepts value labels as value for the `lvl`-argument. Additionally, `ref_lvl()` now also works for factor with non-numeric factor levels and simply returns `relevel(x, ref = lvl)` in such cases.

## Bug fixes

* Fixed encoding issues in `rec()` with direct labelling for certain locales.
* Fixed issue in `count_na()`, which did not print labels of tagged `NA` values since the last revision of `frq()`.
* Fixed issue in `merge_imputation()` for cases where original data frame had less columns than imputed data frames.
* Fixed issue in `find_var()` for fuzzy-matching in all elements (i.e. when `fuzzy = TRUE` and `search = "all"`).

# sjmisc 2.7.2

## New functions

* `round_num()` to round only numeric values in a data frame.

## General

* Improved performance for `merge_df()`. Furthermore, `add_rows()` was added as alias for `merge_df()`.
* `merge_df()` resp. `add_rows()` now create a unique `id`-name instead of dropping the ID-variable, in case `id` has the same name of any existing variables in the provided data frames.
* Improved performance for `descr()` and minor changes to the output.

## Support for `mids`-objects (package _mice_)

Following functions now also work on `mids`-objects, as returned by the `mice()`-function:
* `row_count()`, `row_sums()`, `row_means()`, `rec()`, `dicho()`, `center()`, `std()`, `recode_to()` and `to_long()`.

## Changes to functions

* The `weight.by`-argument in `frq()` now should be a variable name from a variable in `x`, and no longer a separate vector.

## Bug fixes

* `descr()` does not work with character vectors, so these are being removed now.

# sjmisc 2.7.1

## General

* Fix typos and revise outdated paragraphs in vignettes.

## New functions

The recoding and transformation functions get scoped variants, allowing to select variables based on logical conditions described in a function:

* `rec_if()` as scoped variant of `rec()`.
* `dicho_if()` as scoped variant of `dicho()`.
* `center_if()` as scoped variant of `center()`.
* `std_if()` as scoped variant of `std()`.
* `split_var_if()` as scoped variant of `split_var()`.
* `group_var_if()` and `group_label_if()` as scoped variant of `group_var()` and `group_label()`.
* `recode_to_if()` as scoped variant of `recode_to()`.
* `set_na_if()` as scoped variant of `set_na()`.

## Changes to functions

* New function `remove_cols()` as alias for `remove_var()`.
* `std()` gets a new robust-option, `robust = "2sd"`, which divides the centered variables by two standard deviations.
* Slightly improved performance for `set_na()`.

## Bug fixes

* `frq()` now removes empty columns before computing frequencies, because applying `frq()` on empty vectors caused an error.
* `empty_cols()` and `empty_rows()` (and hence, `remove_empty_cols()` and `remove_empty_rows()`) caused an error for data frames with only one column resp. row, or if `x` was a vector and no data frame.
* `frq()` now removes missing values from input when weights are applied, to ensure that input and weights have same length.

# sjmisc 2.7.0

## General

* *Breaking changes*: The `append`-argument in recode and transformation functions like `rec()`, `dicho()`, `split_var()`, `group_var()`, `center()`, `std()`, `recode_to()`, `row_sums()`, `row_count()`, `col_count()` and `row_means()` now defaults to `TRUE`.
* The `print()`-method for `descr()` now accepts a `digits`-argument, to specify the rounding of the output.
* Cross refences from `dplyr::select_helpers` were updated to `tidyselect::select_helpers`.

## New functions

* `is_whole()` as counterpart to `is_float()`.

## Changes to functions

* `frq()` now prints variable names for non-labelled data, adds variable names in braces for labelled data and omits the _label_ column for non-labelled data.
* `frq()` now prints mean and standard deviation in the header line of the output.
* `frq()` now gets a `auto.grp`-argument to automatically group variables with many unique values.
* `frq()` now gets a `show.strings`-argument to omit string variables (character vectors) from being printed as frequency table.
* `frq()` now gets a `grp.strings`-argument to group similar string values in the frequency table.
* `frq()` gets an `out`-argument, to print output to console, or as HTML table in the viewer or web browser.
* `descr()` gets an `out`-argument, to print output to console, or as HTML table in the viewer or web browser.

## Bug fixes

* `is_empty()` returned `TRUE` for single vectors with `NA` being the first element.
* Fix issue where due to a bug during code cleanup, `remove_empty_rows()` did no longer remove empty rows, but columns.

# sjmisc 2.6.3

## General

* Revised examples that used removed methods from other packages.
* Use select-helpers from package *tidyselect*, instead of *dplyr*.
* Beautiful colored output for `frq()`, `descr()` and `flat_table()`.

## Changes to functions

* `rec()` now also recodes doubles with floating points, if a range of values is specified.
* `std()` and `center()` now use `include.fac = FALSE` as default option.
* `std()` gets a `robust`-argument, to divide variables either by standard deviation, or - in case of asymmetrically distributed variables - median absolute deviation or Gini's mean difference.
* `frq()` now shows total and valid N in output.

## Bug fixes

* `center()`, `std()`, `dicho()`, `split_var()` and `group_var()` did not work correctly for grouped data frames.
* `frq()` did not print multiple variables when applied on grouped data frames.

# sjmisc 2.6.2

## Changes to functions

* Arguments `as.df` and `as.varlab` in function `find_var()` are now deprecated. Please use `out` instead.
* `rotate_df()` preserves attributes.
* `is_float()` is now exported as function.

## Bug fixes

* Fixed bug for `to_label()`, when `x` was a character vector and argument `drop.levels` was `TRUE`.

# sjmisc 2.6.1

## General

* Fixed issue with latest tidyr-update on CRAN.

## Bug fixes

* `frq()` did not correctly calculate valid and cumulative percentages when using weights.

# sjmisc 2.6.0

## General
* All labelled-data functions were removed and are now in package *sjlabelled*.

## New functions

* `remove_var()` as pipe-friendly function to remove variables from data frames.
* `var_type()` as pipe-friendly function to determine the type of variables.
* `all_na()` to check whether a vector only consists of NA values.
* `rotate_df()` to rotate data frames (switch columns and rows).
* `shorten_string()`, to shorten strings to a certain maxium number of chars.

## Changes to functions

* Following functions now also work on grouped data frames: `dicho()`, `split_var()`, `group_var()`, `std()` and `center()`.
* Argument `groupcount` in `split_var()`, `group_var()` and `group_labels()` is now named `n`.
* Argument `groupsize` in `group_var()` and `group_labels()` is now named `size`.
* `frq()` gets a revised print-method, which does not print the result to console when captured in an object (i.e., `x <- frq(x)` no longer prints the result).
* `frq()` no longer prints (redundant) labels for factors w/o value label attributes.
* `frq()` adds information about the variable type in the table caption (only for variables with variable labels).
* `frq()` adds information about groups when printing grouped, non-labelled variables.
* `descr()` now also prints information about the variable type.
* `to_character()` now preserves variable labels.

# sjmisc 2.5.0

## General

* **sjmisc** now uses dplyr's [tidyeval-approach](http://dplyr.tidyverse.org/articles/programming.html) to evaluate arguments. This means that the select-helper-functions (like `one_of()` or `contains()`) no longer need to be prefixed with a `~` when used as argument within **sjmisc**-functions.
* All labelled-data functions are now deprecated and will become defunct in future package versions. The labelled-data functions have been moved into a separate package, *sjlabelled*.

## New functions

* `row_count()` to count specific values in a data frame per observation.
* `col_count()` to count specific values in a data frame per variable.
* `str_start()` and `str_end()` to find starting and end indices of patterns inside strings.

## Changes to functions

* The output for `frq()` now always includes a `NA`-row, but no longer prints a value for the `NA`-row.
* `merge_imputations()` gets a `summary`-argument to plot a graphical summary of the quality of the merging process.

## Bug fixes

* `add_columns()` and `replace_columns()` crashed R when no data frame was specified in `...`-ellipses argument.
* `descr()` and `frq()` used wrong variable labels when processing grouped data frames for specific situations, where the grouping variable had no sequences values.
* `descr()` did not work for large data frames, because internally, because `psych::describe()` switched to fast mode by default then (removing columns from the output).
# fs 1.5.0

* The libuv release used by fs was updated to 1.38.1

* `dir_create()` now consults the process umask so the mode during directory creation works like `mkdir` does (#284).

* `fs_path`, `fs_bytes` and `fs_perms` objects are now compatible with vctrs 0.3.0 (#266)

* `fs_path` objects now sort properly when there is a mix of ASCII and unicode elements (#279)


# fs 1.4.2

* `file_info(..., follow = TRUE)`, `is_dir()`, and `is_file()`
  follow relative symlinks in non-current directories (@heavywatal, #280)

* `dir_map()` now grows its internal list safely, the 1.4.0 release introduced an unsafe regression (#268)

* `file_info()` returns a tibble if the tibble package is installed, and subsets work when it is a `data.frame` (#265)

* `path_real()` always fails if the file does not exist. Thus it can no longer
be used to resolve symlinks further up the path hierarchy for files that do not
yet exist. This reverts the feature introduced in 1.2.7 (#144, #221, #231)

# fs 1.4.1

* Fix compilation on Solaris.

# fs 1.4.0

* `[[.fs_path`, `[[.fs_bytes` and `[[.fs_perms` now preserve their classes after subsetting (#254).

* `path_has_parent()` now recycles both the `path` and `parent` arguments (#253).

* `path_ext_set()` now recycles both the `path` and `ext` arguments (#250).

* Internally fs no longer depends on Rcpp

# fs 1.3.2

* fs now passes along `CPPFLAGS` during compilation of libuv, fixing an issue that could
  prevent compilation from source on macOS Catalina. (@kevinushey, #229)

* fs now compiles on alpine linux (#210)

* `dir_create()` now works with absolute paths and `recurse = FALSE` (#204).

* `dir_tree()` now works with paths that need tilde expansion (@dmurdoch, @jennybc, #203).

* `file_info()` now returns file sizes with the proper classes ("fs_bytes" and "numeric"), rather than just "fs_bytes" (#239)

* `get_dirent_type()` gains a `fail` argument (@bellma-lilly, #219)

* `is_dir()`, `is_file()`, `is_file_empty()` and `file_info()` gain a `follow` argument, to follow links and return information about the linked file rather than the link itself (#198)

* `path()` now follows "tidy" recycling rules, namely only consistent or length 1 inputs are recycled. (#238)

* `path()` now errors if the path given or constructed will exceed `PATH_MAX` (#233).

* `path_ext_set()` now works with multiple paths (@maurolepore, #208).

# fs 1.3.1

* Fix missed test with UTF-8 characters, which now passes on a strict Latin-1 locale.

* Fix undefined behavior when casting -1 to `size_t`.

# fs 1.3.0

## Breaking changes

* `dir_ls()`, `dir_map()`, `dir_walk()`, `dir_info()` and `dir_tree()` gain a
  `recurse` argument, which can be either a `TRUE` or `FALSE` (as was supported
  previously) _or_ a number of levels to recurse. The previous argument
  `recursive` has been deprecated.

## New features

* `dir_copy()` gains a `overwrite` argument, to overwrite a given directory
  (@pasipasi123, #193)

## Minor improvements and fixes

* `dir_create()` now throws a more accurate error message when you try to
  create a directory in a non-writeable location (#196).

* `fs_path` objects now always show 10 characters by default when printed in
  tibbles (#191).

* `path_file()`, `path_dir()` and `path_ext()` now return normal character
  vectors rather than tidy paths (#194).

* `path_package()` now works with paths in development packages automatically
  (#175).

* tests now pass successfully when run in strict Latin-1 locale

# fs 1.2.7

## New features

* `file_size()` function added as a helper for `file_info("file")$size` (#171)

* `is_file_empty()` function added to test for empty files` (#171)

* `dir_tree()` function added to print a command line representation of a
  directory tree, analogous to the unix `tree` program (#82).

* Add a comparison vignette to quickly compare base R, fs and shell
  alternatives (@xvrdm, #168).

## Minor improvements and fixes

* `path_ext_set()` and `file_temp()` now treat extensions with a leading `.`
  and those without equally. e.g. `path_ext_set("foo", ext = "bar")` and
  `path_ext_set("foo", ext = ".bar")` both result in "foo.bar"

* Tidy paths are now always returned with uppercase drive letters on Windows (#174).

* `format.bench_bytes()` now works with `str()` in R 3.5.1+ (#155).

* `path_ext()`, `path_ext_remove()`, and `path_ext_set()` now work on paths
  with no extension, and `file_temp()` now prepends a `.` to the file extension
  (#153).

* Link with -pthread by default and fix on BSD systems (#128, #145, #146).

* `file_chown()` can now take a `group_id` parameter as character (@cderv, #162).

* Parameter `browser` in `file_show()` now works as described in the documentation (@GegznaV, #154).

* `path_real()` now works even if the file does not exist, but there are
  symlinks further up the path hierarchy (#144).

* `colourise_fs_path()` now returns paths uncolored if the colors argument /
  `LS_COLORS` is malformed (#135).

# fs 1.2.6

* This is a small bugfix only release.

* `file_move()` now fall back to copying, then removing files when moving files
  between devices (which would otherwise fail) (#131, https://github.com/r-lib/usethis/issues/438).

* Fix for a double free when using `warn = TRUE` (#132)

# fs 1.2.5

* Patch release to fix tests which left files in the R session directory

# fs 1.2.4

## New Features

* New `path_wd()` generates paths from the current working directory (#122).

* New `path_has_parent()` determines if a path has a given parent (#116).

* New `file_touch()` used to change access and modification times for a file (#98).

* `dir_ls()`, `dir_map()`, `dir_walk()`, `dir_info()` and `file_info()` gain a
  `fail` parameter, to signal warnings rather than errors if they are called on
  a path which is unavailable due to permissions or locked resources (#105).

## Minor improvements and fixes

* `path_tidy()` now always includes a trailing slash for the windows root
  directory, e.g. `C:/` (#124).

* `path_ext()`, `path_ext_set()` and `path_ext_remove()` now handle paths with
  non-ASCII characters (#120).

* `fs_path` objects now print (without colors) even if the user does not have
  permission to stat them (#121).

* compatibility with upcoming gcc 8 based Windows toolchain (#119)

# fs 1.2.3

## Features

* Experimental support for `/` and `+` methods for `fs_path` objects (#110).

* `file_create()` and `dir_create()` now take `...`, which is passed to
  `path()` to make construction a little nicer (#80).

## Bugfixes

* `path_ext()`, `path_ext_set()` and `path_ext_remove()` now handle paths with
  directories including hidden files without extensions (#92).

* `file_copy()` now copies files into the directory if the target is a
  directory (#102).

# fs 1.2.2

## Features

* fs no longer needs a C++11 compiler, it now works with compilers which
  support only C++03 (#90).

## Bugfixes

* `fs_path` `fs_bytes` and `fs_perm` objects now use `methods::setOldClass()`
  so that S4 dispatch to their base classes works as intended (#91).

* Fix allocation bug in `path_exists()` using `delete []` when we should have
  used `free()`.

# fs 1.2.1

## Features

* `path_abs()` gains a `start` argument, to specify where the absolute path
  should be calculated from (#87).

## Bugfixes

* `path_ext()` now returns `character()` if given 0 length inputs (#89)

* Fix for a memory issue reported by ASAN and valgrind.

# fs 1.2.0

## Breaking changes

* `path_expand()` and `path_home()` now use `USERPROFILE` or
  `HOMEDRIVE`/`HOMEPATH` as the user home directory on Windows. This differs
  from the definition used in `path.expand()` but is consistent with
  definitions from other programming environments such as python and rust. This
  is also more compatible with external tools such as git and ssh, both of
  which put user-level files in `USERPROFILE` by default. To mimic R's (and
  previous) behavior there are functions `path_expand_r()` and `path_home_r()`.

* Handling missing values are more consistent. In general `is_*` functions
  always return `FALSE` for missing values, `path_*` functions always propagate
  NA values (NA inputs become NA outputs) and `dir_*` `file_*` and `link_*`
  functions error with NA inputs.

* fs functions now preserve tildes in their outputs. Previously paths were
  always returned with tildes expanded. Users can use `path_expand()` to expand
  tildes if desired.

## Bugfixes

* Fix crash when a files user or group id did not exist in the respective
  database (#84, #58)
* Fix home expansion on systems without readline (#60).
* Fix propagation of NA values in `path_norm()` (#63).

## Features

* `file_chmod()` is now vectorized over both of its arguments (#71).
* `link_create()` now fails silently if an identical link already exists (#77).
* `path_package()` function created as an analog to `system.file()` which
  always fails if the package or file does not exist (#75)

# fs 1.1.0

## Breaking changes

* Tidy paths no longer expand `~`.

* Filesystem modification functions now error for NA inputs. (#48)

* `path()` now returns 0 length output if given any 0 length inputs (#54).

## New features

* Removed the autotool system dependency on non-windows systems.

## Bugfixes

* `dir_delete()` now correctly expands paths (#47).

* `dir_delete()` now correctly deletes hidden files and directories (#46).

* `link_path()` now checks for an error before trying to make a string,
  avoiding a crash (#43).

* libuv return paths now marked as UTF-8 strings in C code, fixing encoding
  issues on windows. (#42)

* `dir_copy()` now copies the directory inside the target if the target is a
  directory (#51).

* `dir_copy()` now works correctly with absolute paths and no longer removes
  files when `overwrite = TRUE`.

# fs 1.0.0

* Removed the libbsd system dependency on linux
* Initial release
* Added a `NEWS.md` file to track changes to the package.
# reprex 0.3.0

* The `crayon.enabled` option is explicitly set to `FALSE` when rendering the reprex (#238, #239).

* Expression input is once again captured via `substitute()` (as opposed to `rlang::enexpr()`), which is more favorable for reprexes involving tidy eval (#241).

* New venue "html" to render HTML fragments, useful for pasting in sites without markdown but that allow HTML (#236 @cwickham).

* The YAML of reprex's template has been updated in light of the stricter YAML parser used in Pandoc >= 2.2.2.

* `rlang::set_attrs()` has been soft-deprecated and is no longer used internally.

# reprex 0.2.1

* The reprex ad is formatted as superscript for `venue = "gh"` and `venue = "so"`, i.e. it is more subtle (#201).

* New experimental venue "rtf" produces syntax highlighted snippets suitable for pasting into presentation software such as Keynote or PowerPoint. This venue is discussed in [an article](https://reprex.tidyverse.org/articles/articles/rtf.html) (#26).

* Arguments `opts_chunk` and `opts_knit` have been removed from `reprex()`. The same effect has always been achievable via roxygen comments in the reprex code and the examples have always demonstrated this. Overriding knitr options doesn't seem to come up often enough in real-world `reprex()` usage to justify these arguments.

* Internal file system operations use the [fs](https://fs.r-lib.org) package. This should not make any user-facing changes in reprex and we definitely want to know if it does.

# reprex 0.2.0

reprex has a website: <https://reprex.tidyverse.org>. It includes a contributed article from @njtierney (#103).

reprex has moved to the [tidyverse Organization](https://github.com/tidyverse). It is installed as part of the [tidyverse meta-package](https://www.tidyverse.org) and is [suggested to those seeking help](https://www.tidyverse.org/help/). 

`reprex()` gains several arguments and many arguments can now be controlled via an option, in case a user wants their own defaults.

The new `reprex_selection()` add-in reprexes the current selection, with venue controlled by the option `reprex.venue`. It can be handy to bind to a keyboard shortcut (#84 @hadley).

If reprex can't write to the user's clipboard (e.g. on RStudio server or Unix-like systems lacking xclip or xsel), it offers to open the output file for manual copy.

## Option-controlled arguments for custom defaults

These look like `reprex(..., arg = opt(DEFAULT), ...)` in the help file. This is shorthand for `arg = getOption("reprex.arg", DEFAULT)`, i.e. the option `reprex.arg` is consulted and, if unset, the documented default is used. Allows user to define their own default behaviour (#116).

## New arguments to `reprex()`:

  * `advertise`: toggles inclusion of a footer that describes when and how the reprex was created, e.g., "Created on 2017-11-16 by the reprex package (v0.1.1.9000)". Defaults to `TRUE` (#121, #69).
  * `style`: requests code restyling via the newly-Suggested styler package. styler can cope with tidyeval syntactical sugar, e.g. `df %>% group_by(!! group_var)`. Defaults to `FALSE` (#108, #94).
  * `tidyverse_quiet`: affords control of the startup message of the tidyverse meta-package. Defaults to `TRUE`, i.e. suppresses the message (important special case of #70, #100).
  * `std_out_err`: appends output sent to stdout and stderr by the reprex rendering process. This can be necessary to reveal output if the reprex spawns child processes or has `system()` calls. Defaults to `FALSE` (#90, #110).
  * `render`: determines if the reprex is actually rendered or just returns after producing the templated `.R` file. For internal testing.

## Venues

  * Line wrapping is preserved from source via a Pandoc option (#145 @jimhester, #175).

  * `venue = "gh"` now targets CommonMark as the standard for GitHub Flavored Markdown (#77).
  
  * `venue = "so"` has appropriate whitespace at the start.

  * `venue = "ds"` is a new value, corresponding to <https://www.discourse.org>, which is the platform behind [community.rstudio.com](https://community.rstudio.com). This is currently just an alias for the default `"gh"` GitHub venue, because the formatting appears to be compatible. Adding the `"ds"` value so Discourse can be documented and to guard against the possibility that some formatting is actually unique.
  
## Other changes

  * The `keep.source` option is set to `TRUE` when rendering the reprex, so reprexes involving srcrefs should work (#152).
  
  * The "undo" functions (`reprex_invert()`, `reprex_clean()`, `reprex_rescue()`) handle `input` and `outfile` like `reprex()` does. The `outfile` argument is new (#129, #68).

  * The default value for knitr's `upload.fun` is now set according to the venue. It is `knitr::imgur_upload()` for all venues except `"r"`, where it is `identity` (#125).

  * The HTML preview should appear in the RStudio Viewer more consistently, especially on Windows (#75 @yutannihilation).
  
  * More rigorous use of UTF-8 encoding (#76 @yutannihilation).

  * Expression input handling has been refactored. As a result, formatR is no longer Suggested. Trailing comments -- inline and on their own line -- are also now retained (#89, #91, #114, @jennybc and @jimhester).

  * Custom prompts are now escaped when used in regexes (#98, #99 @jimhester). Embedded newlines are now escaped.

# reprex 0.1.2

This was a non-functioning release created by CRAN maintainers by commenting out lines of code relating to the clipboard.

# reprex 0.1.1

  * pandoc added to SystemRequirements.

# reprex 0.1.0

  * `outfile = NA` causes outfiles to be left in working directory. Filenames will be based on the `input` file, if there was one.

  * `reprex()` strips any leading prompts from input code.

  * Added functions `reprex_clean()`, `reprex_invert()`, and `reprex_rescue()` in order to go backwards, i.e. recover source from a wild-caught reprex.

  * `venue = "R"` (or `"r"`) can be used to get an R script back, augmented with commented output.

  * `comment` argument added to specify prefix for commented output.

  * Added an RStudio addin, accessed via "Render reprex".

  * `input` argument to `reprex()` and friends handles code as string, character vector, or file path.

  * The reprex is rendered via `callr::r_safe()` and is thus run in a clean, separate R process, eliminating any leakage of objects or loaded packages to/from the calling session.

  * `reprex()` gains optional arguments `opts_chunk` and `opts_knit`, taking named list as input, in order to supplement or override default knitr chunk and package options, respectively. (#33)
    - This made the explicit `upload.fun` argument unnecessary, so it's gone. The `upload.fun` option defaults to `knitr::imgur_upload`, which means figures produced by the reprex will be uploaded to [imgur.com](http://imgur.com) and the associated image syntax will be put into the Markdown, e.g. `![](http://i.imgur.com/QPU5Cg9.png)`. (#15 @paternogbc)
    
  * Order of `reprex()` arguments has changed.

  * `reprex()` gains the `si` argument to request that `devtools::session_info()` or `sessionInfo()` be appended to reprex code (#6 @dgrtwo). When `si = TRUE` and `venue = "gh"` (the default), session info is wrapped in a collapsible details tag. See [an example](https://github.com/tidyverse/reprex/issues/55) (#55).

  * Reprex code can be provided as an R expression. (#6 @dgrtwo, #35)
  
  * `reprex()` uses clipboard functionality from [`clipr`](https://CRAN.R-project.org/package=clipr) and thus should work on Windows and suitably prepared Unix-like systems, in addition to Mac OS. (#16 @mdlincoln)

# reprex 0.0.0.9000

  * I tweeted about this and some people actually used it!
# sjstats 0.18.

## General

* Effect size computation functions (like `eta_sq()`) now internally call the related functions from the *effectsize* package.
* Remove packages from "Suggest" that have been removed from CRAN.

# sjstats 0.17.9

## Bug fixes

* Fixed documentation for `chisq_gof()`.
* Fixed issue in `anova_stats()` with incorrect effect sizes for certain Anova types (that included an intercept).

# sjstats 0.17.8

## Deprecated and defunct

_sjstats_ is being re-structured, and many functions are re-implemented in new packages that are part of a new project called **easystats**.

Therefore, following functions are now deprecated:

* `cohens_f()`, please use `effectsize::cohens_f()`.
* `std_beta()`, please use `effectsize::standardize_parameters()`.
* `tidy_stan()`, please use `parameters::model_parameters()`.
* `scale_weights()`, please use `parameters::rescale_weights()`.
* `robust()`, please use `parameters::standard_error_robust()`.

## General

* Functions for weighted statistics with prefix `wtd_*()` have been renamed to `weighted_*()`.
* `svy_md()` was renamed to `survey_median()`.
* `mannwhitney()` is an alias for `mwu()`.
* `means_by_group()` is an alias for `grpmean()`.

# sjstats 0.17.7

## Deprecated and defunct

_sjstats_ is being re-structured, and many functions are re-implemented in new packages that are part of a new project called **easystats**. The aim of **easystats** is to provide a unifying and consistent framework to tame, discipline and harness the scary R statistics and their pesky models.

Therefore, following functions are now deprecated:

* `p_value()`, please use `parameters::p_value()`
* `se()`, please use `parameters::standard_error()`

## General

* Revise some functions to cope with the forthcoming _insight_ update.

# sjstats 0.17.6

## General

* Minor revisions to meet the changes in the forthcoming update from *tidyr*.
* `design_effect()` is an alias for `deff()`.
* `samplesize_mixed()` is an alias for `smpsize_lmm()`.
* `crosstable_statistics()` is an alias for `xtab_statistics()`.

## New functions

* `svyglm.zip()` to fit zero-inflated Poisson models for survey-designs.

## Changes to functions

* `phi()` and `cramer()` can now compute confidence intervals.
* `tidy_stan()` removes prior parameters from output.
* `tidy_stan()` now also prints the probability of direction.

## Bug fixes

* Fix bug with wrong computation in `odds_to_rr()`.

# sjstats 0.17.5

## New functions

* `epsilon_sq()`, to compute epsilon-squared effect-size.

## Deprecated and defunct

_sjstats_ is being re-structured, and many functions are re-implemented in new packages that are part of a new project called **easystats**. The aim of **easystats** is to provide a unifying and consistent framework to tame, discipline and harness the scary R statistics and their pesky models.

Therefore, following functions are now deprecated:

* `link_inverse()`, please use `insight::link_inverse()`
* `model_family()`, please use `insight::model_info()`
* `model_frame()`, please use `insight::get_data()`
* `pred_vars()`, please use `insight::find_predictors()`
* `re_grp_var()`, please use `insight::find_random()`
* `grp_var()`, please use `insight::find_random()`
* `resp_val()`, please use `insight::get_response()`
* `resp_var()`, please use `insight::find_response()`
* `var_names()`, please use `insight::clean_names()`
* `overdisp()`, please use `performance::check_overdispersion()`
* `zero_count()`, please use `performance::check_zeroinflation()`
* `converge_ok()`, please use `performance::check_convergence()`
* `is_singular()`, please use `performance::check_singularity()`
* `reliab_test()`, please use `performance::item_reliability()`
* `split_half()`, please use `performance::item_split_half()`
* `predictive_accurarcy()`, please use `performance::performance_accuracy()`
* `cronb()`, please use `performance::cronbachs_alpha()`
* `difficulty()`, please use `performance::item_difficulty()`
* `mic()`, please use `performance::item_intercor()`
* `pca()`, please use `parameters::principal_components()`
* `pca_rotate()`, please use `parameters::principal_components()`
* `r2()`, please use `performance::r2()`
* `icc()`, please use `performance::icc()`
* `rmse()`, please use `performance::rmse()`
* `rse()`, please use `performance::rse()`
* `mse()`, please use `performance::mse()`
* `hdi()`, please use `bayestestR::hdi()`
* `cred_int()`, please use `bayestestR::ci()`
* `rope()`, please use `bayestestR::rope()`
* `n_eff()`, please use `bayestestR::effective_sample()`
* `equi_test()`, please use `bayestestR::equivalence_test()`
* `multicollin()`, please use `performance::check_collinearity()`
* `normality()`, please use `performance::check_normality()`
* `autocorrelation()`, please use `performance::check_autocorrelation()`
* `heteroskedastic()`, please use `performance::check_heteroscedasticity()`
* `outliers()`, please use `performance::check_outliers()`

## Changes to functions

* Anova-stats functions (like `eta_sq()`) get a `method`-argument to define the method for computing confidence intervals from bootstrapping.

## Bug fixes

* In some situations, `smpsize_lmm()` could result in negative sample-size recommendations. This was fixed, and a warning is now shown indicating that the parameters for the power-calculation should be modified.
* Fixed issue with wrong calculated effect size `r` in `mwu()` if group-factor contained more than two groups.

# sjstats 0.17.4

## General

* Following models/objects are now supported by model-information functions like `model_family()`, `link_inverse()` or `model_frame()`: `MixMod` (package **GLMMadaptive**), **MCMCglmm**, `mlogit` and `gmnl`.
* Reduce package dependencies.

## New functions

* `cred_int()`, to compute uncertainty intervals of Bayesian models. Mimics the behaviour and style of `hdi()` and is thus a convenient complement to functions like `posterior_interval()`.

## Changes to functions

* `equi_test()` now finds better defaults for models with binomial outcome (like logistic regression models).
* `r2()` for mixed models now also should work properly for mixed models fitted with **rstanarm**.
* `anova_stats()` and alike (e.g. `eta_sq()`) now all preserve original term names.
* `model_family()` now returns `$is_count = TRUE`, when model is a count-model, and `$is_beta = TRUE` for models with beta-family.
* `pred_vars()` checks that return value has only unique values.
* `pred_vars()` gets a `zi`-argument to return the variables from a model's zero-inflation-formula.

## Bug fixes

* Fix minor issues in `wtd_sd()` and `wtd_mean()` when weight was `NULL` (which usually shoudln't be the case anyway).
* Fix potential issue with `deparse()`, cutting off very long formulas in various functions.
* Fix encoding issues in help-files.

# sjstats 0.17.3

## General

* Export `dplyr::n()`, to meet forthcoming changes in dplyr 0.8.0.

## Changes to functions

* `boot_ci()` gets a `ci.lvl`-argument.
* The `rotation`-argument in `pca_rotate()` now supports all rotations from `psych::principal()`.
* `pred_vars()` gets a `fe.only`-argument to return only fixed effects terms from mixed models, and a `disp`-argument to return the variables from a model's dispersion-formula.
* `icc()` for Bayesian models gets a `adjusted`-argument, to calculate adjusted and conditional ICC (however, only for Gaussian models).
* For `icc()` for non-Gaussian Bayes-models, a message is printed that recommends setting argument `ppd` to `TRUE`.
* `resp_val()` and `resp_var()` now also work for **brms**-models with additional response information (like `trial()` in formula).
* `resp_var()` gets a `combine`-argument, to return either the name of the matrix-column or the original variable names for matrix-columns.
* `model_frame()` now also returns the original variables for matrix-column-variables.
* `model_frame()` now also returns the variable from the dispersion-formula of **glmmTMB**-models.
* `model_family()` and `link_inverse()` now supports **glmmPQL**, **felm** and **lm_robust**-models.
* `anova_stats()` and alike (`omeqa_sq()` etc.) now support gam-models from package **gam**.
* `p_value()` now supports objects of class `svyolr`.

## Bug fixes

* Fix issue with `se()` and `get_re_var()` for objects returned by `icc()`.
* Fix issue with `icc()` for Stan-models.
* `var_names()` did not clear terms with log-log transformation, e.g. `log(log(y))`.
* Fix issue in `model_frame()` for models with splines with only one column.

# sjstats 0.17.2

## General

* Revised help-files for `r2()` and `icc()`, also by adding more references.

## New functions

* `re_grp_var()` to find group factors of random effects in mixed models.

## Changes to functions

* `omega_sq()` and `eta_sq()` give more informative messages when using non-supported objects.
* `r2()` and `icc()` give more informative warnings and messages.
* `tidy_stan()` supports printing simplex parameters of monotonic effects of **brms** models.
* `grpmean()` and `mwu()` get a `file` and `encoding` argument, to save the HTML output as file.

## Bug fixes

* `model_frame()` now correctly names the offset-columns for terms provided as `offset`-argument (i.e. for models where the offset was not specified inside the formula).
* Fixed issue with `weights`-argument in `grpmean()` when variable name was passed as character vector.
* Fixed issue with `r2()` for **glmmTMB** models with `ar1` random effects structure.

# sjstats 0.17.1

## New functions

* `wtd_chisqtest()` to compute a weighted Chi-squared test.
* `wtd_median()` to compute the weighted median of variables.
* `wtd_cor()` to compute weighted correlation coefficients of variables.

## Changes to functions

* `mediation()` can now cope with models from different families, e.g. if the moderator or outcome is binary, while the treatment-effect is continuous.
* `model_frame()`, `link_inverse()`, `pred_vars()`, `resp_var()`, `resp_val()`, `r2()` and `model_family()` now support `clm2`-objects from package **ordinal**.
* `anova_stats()` gives a more informative message for non-supported models or ANOVA-options.

## Bug fixes

* Fixed issue with `model_family()` and `link_inverse()` for models fitted with `pscl::hurdle()` or `pscl::zeroinfl()`.
* Fixed issue with wrong title in `grpmean()` for grouped data frames, when grouping variable was an unlabelled factor.
* Fix issue with `model_frame()` for **coxph**-models with polynomial or spline-terms.
* Fix issue with `mediation()` for logical variables.

# sjstats 0.17.0

## General

* Reduce package dependencies.

## New functions

* `wtd_ttest()` to compute a weighted t-test.
* `wtd_mwu()` to compute a weighted Mann-Whitney-U or Kruskal-Wallis test.

## Changes to functions

* `robust()` was revised, getting more arguments to specify different types of covariance-matrix estimation, and handling these more flexible.
* Improved `print()`-method for `tidy_stan()` for _brmsfit_-objects with categorical-families.
* `se()` now also computes standard errors for relative frequencies (proportions) of a vector.
* `r2()` now also computes r-squared values for _glmmTMB_-models from `genpois`-families.
* `r2()` gives more precise warnings for non-supported model-families.
* `xtab_statistics()` gets a `weights`-argument, to compute measures of association for contingency tables for weighted data.
* The `statistics`-argument in `xtab_statistics()` gets a `"fisher"`-option, to force Fisher's Exact Test to be used.
* Improved variance calculation in `icc()` for generalized linear mixed models with Poisson or negative binomial families.
* `icc()` gets an `adjusted`-argument, to calculate the adjusted and conditional ICC for mixed models.
* To get consistent argument names across functions, argument `weight.by` is now deprecated and renamed into `weights`.

## Bug fixes

* Fix issues with effect size computation for repeated-measure Anova when using bootstrapping to compute confidence intervals.
* `grpmean()` now also adjusts the `n`-columm for weighted data.
* `icc()`, `re_var()` and  `get_re_var()` now correctly compute the random-effect-variances for models with multiple random slopes per random effect term (e.g., `(1 + rs1 + rs2 | grp)`).
* Fix issues in `tidy_stan()`, `mcse()`, `hdi()` and `n_eff()` for `stan_polr()`-models.
* Plotting `equi_test()` did not work for intercept-only models.
# parameters 0.8.1

## New supported models

* `metaplus` (*metaplus*), `glht` (*multcomp*), `glmm`  (*glmm*), `manova` (*stats*), `crq` and `crqs` (*quantreg*)
* Improved support for models from the *rms*  package.

## Changes to functions

* Improved parameters formatting for ordered factors in `model_parameters()` (and `format_parameters()`).
* Argument `df_method` can now also be applied to GLMs, to allow calculation of confidence intervals based on Wald-approximation, not profiled confidence intervals. This speeds up computation of CIs for models fit to large data sets.
* Improved `select_parameters()` for mixed models, and revised docs and associated vignette.

## Bug fixes

* Allow `threshold` to be passed to `efa_to_cfa()` when the model is from `factor_analysis()`.
* Allow correlation matrix to be passed to `factor_analysis()`.
* Fix CRAN check issues.
* Fix issue in `model_parameters()` for models with non-estimable parameters or statistics.
* Fix issue in `model_parameters()` for *plm* models with only one parameter.
* Fix issue in `check_heterogeneity()` in case no predictor would cause heterogeneity bias.
* Make sure *clubSandwich* is used conditionally in all places, to properly pass CRAN checks.

# parameters 0.8.0

## New supported models

* `robmixglm` (*robmixglm*), `betaor`, `betamfx`, `logitor`, `poissonirr`, `negbinirr`, `logitmfx`, `probitmfx`, `poissonmfx`, `negbinmfx` (*mfx*), partial support `emmGrid` (*emmeans*)

## Changes to functions

### `simulate_parameters()` and `simulate_model()`

* has a nicer `print()` method.
* now also simulate parameters from the dispersion model for *glmmTMB* objects.
* gets a `verbose` argument, to show or hide warnings and messages.

## Bug fixes

* fix issue with rank deficient models.

# parameters 0.7.0

## General

* We changed the computation of confidence intervals or standard errors, so these are now based on a t-distribution with degrees of freedom and not normal distribution assuming infinite degrees of freedom. This was implemented for most functions before and only affects few functions (like `equivalence_test()` or CIs for standardized parameters from `model_parameters()` when standardization method was `"posthoc"`).

## New supported models

* `averaging` (*MuMIn*), `bayesx` (*R2BayesX*), `afex_aov` (*afex*)

## New functions

* `check_heterogeneity()` as a small helper to find variables that have a within- and between-effect related to a grouping variable (and thus, may result in heterogeneity bias, see [this vignette](https://easystats.github.io/parameters/articles/demean.html)).

## Changes to functions

### `equivalence_test()`

* gains a `rule` argument, so equivalence testing can be based on different approaches.
* for mixed models gains an `effect` argument, to perform equivalence testing on random effects.
* gains a `p_values` argument, to calculate p-values for the equivalence test.
* now supports more frequentist model objects.

### `describe_distribution()`

* now works on grouped data frames.
* gains `ci` and `iterations` arguments, to compute confidence intervals based on bootstrapping.
* gains a `iqr` argument, to compute the interquartile range.
* `SE` column was removed.

### `model_parameters()`

* `model_parameters()` for Stan-models (*brms*, *rstanarm*) gains a `group_level` argument to show or hide parameters for group levels of random effects.
* Improved accuracy of confidence intervals in `model_parameters()` with `standardize = "basic"` or `standardize = "posthoc"`.
* `model_parameters.merMod()` no longer passes `...` down to bootstrap-functions (i.e. when `bootstrap = TRUE`), as this might conflict with `lme4::bootMer()`.
* For ordinal models (like `MASS::polr()` or `ordinal::clm()`), a `Component` column is added, indicating intercept categories (`"alpha"`) and estimates (`"beta"`).
* The `select`-argument from `print.parameters_model()` now gets a `"minimal"`-option as shortcut to print coefficients, confidence intervals and p-values only.

### Other changes

* `parameters_table()` and `print.parameters_model()` now explicitly get arguments to define the digits for decimal places used in output.
* `ci()`, `standard_error()`, `p_value()` and `model_parameters()` for *glmmTMB* models now also works for dispersion models.

## Bug fixes

* Fixed issue in `equivalence_test()` for mixed models.
* Fixed bug for `model_parameters.anova(..., eta_squared = "partial")` when called with non-mixed models.
* Fixed issue with wrong degrees of freedom in `model_parameters()` for *gam* models.
* Fixed issue with unused arguments in `model_parameters()`.

# parameters 0.6.1

## General

* Remove 'Zelig' from suggested packages, as it was removed from CRAN.

## Changes to functions

### model_parameters()

* `model_parameters()` now also transforms standard errors when `exponentiate = TRUE`.
* `model_parameters()` for `anova()` from mixed models can now also compute effect sizes like eta squared.
* `model_parameters()` for `aov()` gains a `type`-argument to compute type-1, type-2 or type-3 sums of squares.
* `model_parameters()` for Bayesian models gains a `standardize` argument, to return standardized parameters from the posterior distribution.
* Improved `print()` method for `model_parameters()` for nested `aov()` (repeated measurements).
* You can now control whether `demean()` should add attributes to indicate within- and between-effects. This is only relevant for the `print()`-method of `model_parameters()`.

## Bug fixes

* Fixed `model_parameters()` for `anova()` from *lmerTest* models.

# parameters 0.6.0

## Breaking changes

- Alias `model_bootstrap()` was removed, please use `bootstrap_model()`.
- Alias `parameters_bootstrap()` was removed, please use `bootstrap_parameters()`.
- Alias `model_simulate()` was removed, please use `simulate_model()`.
- Alias `parameters_simulate()` was removed, please use `simulate_parameters()`.
- Alias `parameters_selection()` was removed, please use `select_parameters()`.
- Alias `parameters_reduction()` was removed, please use `reduce_parameters()`.
- Functions `DDR()`, `ICA()` and `cmds()` are no longer exported, as these were intended to be used internally by `reduce_parameters()` only.
- `skewness()` and `kurtosis()` always return a data frame.

## New supported models

- Added support for `arima` (*stats*), `bife` (*bife*), `bcplm` and `zcpglm` (*cplm*)

## Changes to functions

### model_parameters()

- Improved print-method for `model_parameters.brmsfit()`.
- Improved print-method for `model_parameters.merMod()` when fitting REWB-Models (see `demean()`).
- Improved efficiency for `model_parameters()` (for linear mixed models) when `df_method = "kenward"`.
- `model_parameters()` gets a `p_adjust`-argument, to adjust p-values for multiple comparisons.
- Minor improvements for `cluster_analysis()` when `method = "kmeans"` and `force = TRUE` (factors now also work for kmeans-clustering).

### p_value(), ci() and standard_error()

- `p_value_kenward()`, `se_kenward()` etc. now give a warning when model was not fitted by REML.
- Added `ci()`, `standard_error()` and `p_value()` for *lavaan* and *blavaan* objects.
- Added `standard_error()` for *brmsfit* and *stanreg* objects.

### Other changes

- Run certain tests only locally, to reduce duration of CRAN checks.
- `skewness()`, `kurtosis()` and `smoothness()` get an `iteration` argument, to set the numbers of bootstrap replicates for computing standard errors.
- Improved print-method for `factor_analysis()`.
- `demean()` now additionally converts factors with more than 2 levels to dummy-variables (binary), to mimic *panelr*-behaviour.

## Bug fixes

- Fixed minor issue with the `print()`-method for `model_parameters.befa()`.
- Fixed issues in `model_parameters()` (for linear mixed models) with wrong order of degrees of freedom when `df_method` was different from default.
- Fixed issues in `model_parameters()` (for linear mixed models) with accuracy of p-values when `df_method = "kenward`.
- Fixed issues in `model_parameters()` with wrong test statistic for *lmerModLmerTest* models.
- Fixed issue in `format_parameters()` (which is used to format output of `model_parameters()`) for factors, when variable name was also part of factor levels.
- Fixed issue in `degrees_of_freedem()` for *logistf*-models, which unintentionally printed the complete model summary.
- Fixed issue in `model_parameters()` for *mlm* models.
- Fixed issue in `random_parameters()` for uncorrelated random effects.

# parameters 0.5.0

## Breaking changes

- `skewness()` now uses a different method to calculate the skewness by default. Different methods can be selected using the `type`-argument.
- `kurtosis()` now uses a different method to calculate the skewness by default. Different methods can be selected using the `type`-argument.

## New supported models

- Added support for `cglm` (*cglm*), `DirichletRegModel` (*DirichletReg*)

## General

- Added new vignettes on 'Standardized Model Parameters' and 'Robust Estimation of Standard Errors', and vignettes are now also published on CRAN.
- Improved handling of robust statistics in `model_parameters()`. This should now work for more models than before.
- Improved accuracy of `ci.merMod()` for `method = "satterthwaite"` and `method = "kenward"`.
- `select_parameters()` for *stanreg* models, which was temporarily removed due to the CRAN removal of package **projpred**, is now re-implemented.

## New functions

- `dof_betwithin()` to compute degrees of freedom based on a between-within approximation method (and related to that, `p_value_*()` and `se_*()` for this method were added as well).
- `random_parameters()` that returns information about the random effects such as variances, R2 or ICC.
- `closest_component()` as a small helper that returns the component index for each variable in a data frame that was used in `principal_components()`.
- `get_scores()` as a small helper to extract scales and calculate sum scores from a principal component analysis (PCA, `principal_components()`).

## Changes to functions

- `n_clusters()` gets the option `"M3C"` for the `package`-argument, so you can try to determine the number of cluster by using the `M3C::M3C()` function.
- The `print()`-method for `model_parameters()` gets a `select`-argument, to print only selected columns of the parameters table.
- `model_parameters()` for meta-analysis models has an improved `print()`-method for subgroups (see examples in `?model_parameters.rma`).
- `model_parameters()` for mixed models gets a `details`-argument to additionally print information about the random effects.
- `model_parameters()` now accepts the `df_method`-argument for more (mixed) models.
- The Intercept-parameter in `model_parameters()` for meta-analysis models was renamed to `"Overall"`.
- `skewness()` gets a `type`-argument, to compute different types of skewness.
- `kurtosis()` gets a `type`-argument, to compute different types of skewness.
- `describe_distribution()` now also works on data frames and gets a nicer print-method.

## Bug fixes

- Fixed issue in `model_parameters()` when `robust = TRUE`, which could sometimes mess up order of the statistic column.
- Fixed issues in `model_parameters()` with wrong `df` for `lme`-models.
- Fixed issues in `model_parameters.merMod()` when `df_method` was not set to default.
- Fixed issues in `model_parameters.merMod()` and `model_parameters.gee()` when `robust = TRUE`.
- Fixed issues with *coxph* models with only one parameter.
- Fixed issue in `format_p()` when argument `digits` was `"apa"`.
- Fixed issues in `model_parameters()` for `zeroinfl`-models.

# parameters 0.4.1

## Bug fixes

- Fix CRAN check issues, caused by removal of package 'projpred'.

# parameters 0.4.0

## Breaking changes

- The column for degrees of freedom in `model_parameters()` was renamed from `df_residuals` to `df_error` for regression model objects, because these degrees of freedom actually were not always referring to _residuals_ - we consider `df_error` as a more generic name.
- `model_parameters()` for standardized parameters (i.e. `standardize` is not `NULL`) only returns standardized coefficients, CI and standard errors (and not both, unstandardized and standardized values).
- `format_ci()` was removed and re-implemented in the **insight** package.

## Renaming

- `model_bootstrap()` was renamed to `bootstrap_model()`. `model_bootstrap()` will remain as alias.
- `parameters_bootstrap()` was renamed to `bootstrap_parameters()`. `parameters_bootstrap()` will remain as alias.
- `model_simulate()` was renamed to `simulate_model()`. `model_simulate()` will remain as alias.
- `parameters_simulate()` was renamed to `simulate_parameters()`. `parameters_simulate()` will remain as alias.
- `parameters_selection()` was renamed to `select_parameters()`. `parameters_selection()` will remain as alias.
- `parameters_reduction()` was renamed to `reduce_parameters()`. `parameters_reduction()` will remain as alias.

## New supported models

- Added support for `vgam` (*VGAM*), `cgam`, `cgamm` (*cgam*), `complmrob` (*complmrob*), `cpglm`, `cpglmm` (*cplm*), `fixest` (*fixest*), `feglm` (*alpaca*), `glmx` (*glmx*), `glmmadmb` (*glmmADMB*), `mcmc` (*coda*), `mixor` (*mixor*).
- `model_parameters()` now supports `blavaan` models (*blavaan*).

## General

- Better handling of `clm2`, `clmm2` and `stanmvreg` models.
- Better handling of `psych::omega` models.

## New functions

- `dof_satterthwaite()` and `dof_ml1()` to compute degrees of freedom based on different approximation methods (and related to that, `p_value_*()` and `se_*()` for these methods were added as well).
- `rescale_weights()` to rescale design (probability or sampling) weights for use in multilevel-models without survey-design.

## Changes to functions

- Robust estimation (like `standard_error_robust()` or `ci_robust()`) can now also compute cluster-robust variance-covariance matrices, using the *clubSandwich*  package.
- `model_parameters()` gets a `robust`-argument, to compute robust standard errors, and confidence intervals and p-values based on robust standard errors.
- Arguments `p_method` and `ci_method` in `model_parameters.merMod()` were replaced by a single argument `df_method`.
- `model_parameters.principal()` includes a `MSA` column for objects from `principal_components()`.

## Bug fixes

- Fixed issue in `model_parameters()` with non-typical ordering of coefficients for mixed models.
- Fixed issues with models of class `rlmerMod`.
- Fixed minor issues `model_parameters.BFBayesFactor()`.

# parameters 0.3.0

## Breaking changes

Parts of the **parameter** package are restructured and functions focussing on anything related to effect sizes are now re-implemented in a new package, [**effectsize**](https://github.com/easystats/effectsize). In details, following breaking changes have been made:

- Functions for computing effect sizes (`cohens_f()`, `eta_squared()` etc.) have been removed and are now re-implemented in the **effectsize**-package.
- Functions for converting effect sizes (`d_to_odds()` etc.) have been removed and are now re-implemented in the **effectsize**-package.
- `standardize()` and `normalize()` (and hence, also `parameters_standardize()`) have been removed ;-( and are now re-implemented in the **effectsize**-package.

## New supported models

- Added support for `aareg` (*survival*), `bracl`, `brmultinom` (*brglm2*), `rma` (*metafor*) and `multinom` (*nnet*) to various functions.
- `model_parameters()` for `kmeans`.
- `p_value()`, `ci()`, `standard_error()` and `model_parameters()` now support *flexsurvreg* models (from package **flexsurv**).

## New functions

- `degrees_of_freedom()` to get DoFs.
- `p_value_robust()`, `ci_robust()` and `standard_error_robust()` to compute robust standard errors, and p-values or confidence intervals based on robust standard errors.
- `demean()` to calculate de-meaned and group-meaned variables (centering within groups, for panel-data regression).
- `n_parameters()` to get number of parameters.
- `n_clusters()` to determine the number of clusters to extract.
- `cluster_analysis()` to return group indices based on cluster analysis.
- `cluster_discrimination()` to determine the goodness of classification of cluster groups.
- `check_clusterstructure()` to check the suitability of data for clustering.
- `check_multimodal()` to check if a distribution is unimodal or multimodal.
- Add `plot()`-methods for `principal_components()`.

## Changes to functions

- Added indices of model fit to `n_factors()` ([Finch, 2019](https://doi.org/10.1177/0013164419865769))
- `standard_error()` for mixed models gets an `effects` argument, to return standard errors for random effects.
- The `method`-argument for `ci()` gets a new option, `"robust"`, to compute confidence intervals based on robust standard errors. Furthermore, `ci_wald()` gets a `robust`-argument to do the same.
- `format_p()` gets a `digits`-argument to set the amount of digits for p-values.
- `model_parameters()` now accepts (non-documented) arguments `digits`, `ci_digits` and `p_digits` to change the amount and style of formatting values. See [examples in `model_parameters.default()`](https://easystats.github.io/parameters/reference/model_parameters.default.html).
- Improved `print()` method for `model_parameters()` when used with Bayesian models.
- Added further method (gap-statistic) to `n_clusters()`.

## Bug fixes

- Interaction terms in `model_parameters()` were denoted as nested interaction when one of the interaction terms was surrounded by a function, e.g. `as.factor()`, `log()` or `I()`.
- Fixed bug in `parameters_type()` when a parameter occured multiple times in a model.
- Fixed bug with *multinom*-support.
- Fixed bug in `model_parameters()` for non-estimable GLMs.
- Fixed bug in `p_value()` for *MASS::rlm* models.
- Fixed bug in `reshape_loadings()` when converting loadings from wide to long and back again.

# parameters 0.2.0

## Breaking changes

- `format_value()` and `format_table()` have been removed and are now re-implemented in the **insight** package.

## General

- `parameters()` is an alias for `model_parameters()`.
- `p_value()`, `ci()`, `standard_error()`, `standardize()` and `model_parameters()` now support many more model objects, including mixed models from packages *nlme*, *glmmTMB* or *GLMMadaptive*, zero-inflated models from package *pscl* or other modelling packages. Along with these changes, functions for specific model objects with zero-inflated component get a `component`-argument to return the requested values for the complete model, the conditional (count) component or the zero-inflation component from the model only.

## New functions

- `parameters_simulate()` and `model_simulate()`, as computational faster alternatives to `parameters_bootstrap()` and `model_bootstrap()`.
- `data_partition()` to partition data into a test and a training set.
- `standardize_names()` to standardize column names from data frames, in particular objects returned from `model_parameters()`.
- `se_kenward()` to calculate approximated standard errors for model parameters, based on the Kenward-Roger (1997) approach.

## Changes to functions

- `format_value()` and `format_ci()` get a `width`-argument to set the minimum length of the returned formatted string.
- `format_ci()` gets a `bracket`-argument include or remove brackets around the ci-values.
- `eta_squared()`, `omega_squared()`, `epsilon_squared()` and `cohens_f()` now support more model objects.
- The `print()`-method for `model_parameters()` now better aligns confidence intervals and p-values.
- `normalize()` gets a `include_bounds`-argument, to compress normalized variables so they do not contain zeros or ones.
- The `method`-argument for `ci.merMod()` can now also be `"kenward"` to compute confidence intervals with degrees of freedom based on the Kenward-Roger (1997) approach.

## Bug fixes

- Fixed issue with wrong computation of wald-approximated confidence intervals.
- Fixed issue with wrong computation of degrees of freedom for `p_value_kenward()`.
- `paramerers_standardize()` resp. `standardize()` for model objects now no longer standardizes `log()` terms, count or ratio response variables, or variables of class `Surv` and `AsIs`.

# parameters 0.1.0

- Added a `NEWS.md` file to track changes to the package
# effectsize 0.3.2

## New features

- `eta_squared_posterior()` for estimating Eta Squared for Bayesian models.
- `eta_squared()`, `omega_squared()` and `epsilon_squared()` now works with
  - `ols` / `rms` models.
- `effectsize()` for class `htest` supports `oneway.test(...)`.


## Bug fixes

- Fix minor miss-calculation of Chi-squared for 2*2 table with small samples ( #102 ).
- Fixed miss-calculation of signed rank in `ranktransform()` ( #87 ).
- Fixed bug in `standardize()` for standard objects with non-standard class-attributes (like vectors of class `haven_labelled` or `vctrs_vctr`).  
- Fix `effectsize()` for one sample `t.test(...)` ( #95 ; thanks to pull request by @mutlusun )


# effectsize 0.3.1

## New features

- `standardize_parameters()` now returns CIs ( #72 )
- `eta_squared()`, `omega_squared()` and `epsilon_squared()` now works with
  - `gam` models.
  - `afex` models.
  - `lme` and `anova.lme` objects.
- New function `equivalence_test()` for effect sizes.
- New plotting methods in the `see` package.

# effectsize 0.3.0

## New features

- New general purpose `effectsize()` function.
- Effectsize for differences have CI methods, and return a data frame.
- Effectsize for ANOVA all have CI methods, and none are based on bootstrapping.
- New effect sizes for contingency tables (`phi()` and `cramers_v()`).
- `chisq_to_phi()` / `cramers_v()` functions now support CIs (via the ncp method), and return a data frame.
- `F_to_eta2()` family of functions now support CIs (via the ncp method), and return a data frame.
- `t_to_d()` and `t_to_r()` now support CIs (via the ncp method), and return a data frame.
- `standardize()` for model-objects has a default-method, which usually accepts all models. Exception for model-objects that do not work will be added if missing.
- `standardize.data.frame()` gets `append` and `suffix` arguments, to add (instead of replace) standardized variables to the returned data frame.
- `eta_squared()`, `omega_squared()` and `epsilon_squared()` now works
  - output from `parameters::model_parameters()`.
  - `mlm` models.

## Bug fixes

- Fix `cohens_d()`'s dealing with formula input (#44).
- `sd_pooled()` now returns the... pooled sd (#44).

## Changes

- In `t_to_d()`, argument `pooled` is now `paired`.

# effectsize 0.2.0

## Bug fixes

- `standardize.data.frame()` did not work when variables had missing values.
- Fixed wrong computation in `standardize()` when `two_sd = TRUE`.
- Fixed bug with missing column names in `standardize_parameters()` for models with different components (like count and zero-inflation).

# effectsize 0.1.1

## Changes

- News are hidden in an air of mystery...

# effectsize 0.1.0

## New features

- `standardize_parameters()` and `standardize()` now support models from packages *brglm*, *brglm2*, *mixor*, *fixest*, *cgam*, *cplm*, *cglm*, *glmmadmb* and *complmrob*.

## Bug fixes

- Fix CRAN check issues.
# ggpubr 0.4.0
  

## New features

- New functions added to customize `ggtexttable()` (#125, #129 and #283):
    - `tab_cell_crossout()`: cross out a table cell.
    - `tab_ncol(), tab_nrow()`: returns, respectively, the number of columns and rows in a ggtexttable.
    - `tab_add_hline()`: Creates horizontal lines or separators at the top or the bottom side of a given specified row.
    - `tab_add_vline()`: Creates vertical lines or separators at the right or the left side of a given specified column.
    - `tab_add_border(), tbody_add_border(), thead_add_border()`: Add borders to table; tbody is for table body and thead is for table head.
    - `tab_add_title()` and `tab_add_footnote()` to add titles and footnotes (#243).
- ggpubr functions updated to handle non-standard column names, for example ("A-A") (#229).
- New function `create_aes()` added to create aes mapping from a list. Makes programming easy with ggplot2 (#229).
- New argument `coord.flip` added to support adding p-values onto horizontal ggplots (#179). When adding the p-values to a horizontal ggplot (generated using `coord_flip()`), you need to specify the option `coord.flip = TRUE`.
- New errorbar functions - `median_hilow_()` and `median_q1q3()` -  added ([@davidlorenz, #209](https://github.com/kassambara/ggpubr/issues/209)):
    - `median_hilow_()`: computes the sample median and a selected pair of outer quantiles having equal tail areas. This function is a reformatted version of `Hmisc::smedian.hilow()`. The confidence limits are computed as follow: `lower.limits = (1-ci)/2` percentiles; `upper.limits = (1+ci)/2` percentiles. By default (`ci = 0.95`), the 2.5th and the 97.5th percentiles are used as the lower and the upper confidence limits, respectively. If you want to use the 25th and the 75th percentiles as the confidence limits, then specify `ci = 0.5` or use the function `median_q1q3()`.
    - `median_q1q3()`: computes the sample median and, the 25th and 75th percentiles. Wrapper around the function median_hilow_() using ci = 0.5.
- New function `get_breaks()` added to easily create breaks for numeric axes. Can be used to increase the number of x and y ticks by specifying the option `n`. It's also possible to control axis breaks by specifying a step between ticks. For example, if by = 5, a tick mark is shown on every 5 ([@Chitanda-Satou, #258](https://github.com/kassambara/ggpubr/issues/258)).

   
## Major changes
   
- The following enhancement has been added to `ggscatterhist()` ([@juliechevalier, #176](https://github.com/kassambara/ggpubr/issues/176)):
    - the output of `ggscatterhist()` is now a list of ggplots, containing the main scatter plot (`sp`) and the marginal plots (`xplot` and `yplot`), which can be customized by the end user using the standard ggplot verbs
    - An S3 printing method is now available for an object of class ggscatterhist. The printing method displays the arranged final figure.

## Minor changes

- Now, when creating a box plot with error bars, color and fill argiments are taken into account in the errorbar function (#105).
- New argument `alternative` supported in `stat_cor()` (#276).
- New argument `position` in `ggline()` to make position "dodged" (#52).
- New argument `outlier.shape` in ggboxplot(). Default is 19. To hide outlier, specify outlier.shape = NA. When jitter is added, then outliers will be automatically hidden. 
- Sorting can be now disabled in `ggdotchart()` using the option `sorting = "none"` (#115, #223).
- New argument `weight` added in `gghistogram()` for creating a weighted histogram (#215)
- Now `ggscaterhist()` takes into account the argument `position` in `margin.params` when marginal plot is a histogram (#286). 
- `ggbarplot()` enhanced to better handle the creation of dodged bar plots combined with jitter points ([@aherholt, #176](https://github.com/kassambara/ggpubr/issues/282))
- New argument `bracket.shorten` added in `stat_pvalue_manual()` and `geom_bracket()`. a small numeric value in [0-1] for shortening the with of bracket (#285).
- New argument `bracket.nudge.y` added in `stat_pvalue_manual()` and `geom_bracket()`. Vertical adjustment to nudge brackets by. Useful to move up or move down the bracket. If positive value, brackets will be moved up; if negative value, brackets are moved down ([#281](https://github.com/kassambara/ggpubr/issues/281)).
- New argument `numeric.x.axis` added in `ggerrorplot()`; logical value, If TRUE, x axis will be treated as numeric. Default is FALSE ([#280](https://github.com/kassambara/ggpubr/issues/280)).
- The option `width` is now considered in `ggadd()` for plotting error bars ([#278](https://github.com/kassambara/ggpubr/issues/278)).
- New argument `linetype` in `ggpaired()`.
- `geom_exec()` used in `ggpaired()` to add lines between paired points.
- `ggmaplot()` now supports two input formats (#198):
    1. baseMean | log2FoldChange|padj: Here, we'll use log2(baseMean) as the x-axis variable
    2. baseMeanLog2 | log2FoldChange|padj: here, baseMeanLog2 is assumed to be the mean of logged values; so we'll use it as x-axis variable without any transformation. 
- new arguments added in `ggmaplot()`:
    - `alpha` for controlling point transparency/density ([@apcamargo, #152](https://github.com/kassambara/ggpubr/issues/152)).
    - `label.select` to select specific genes to show on the plot ([@apastore, #70](https://github.com/kassambara/ggpubr/issues/70))
- In `ggadd()` the `fill` argument is considered for jitter points only when the point shape is in 21:25 ([@atakanekiz, #148](https://github.com/kassambara/ggpubr/issues/148)).
- New argument `parse` added in `ggscatter()` and in `ggtext()`. If TRUE, the labels will be parsed into expressions and displayed as described in ?plotmath (#250).
- New argument `stroke` supported in `ggscatter()` and in `ggline()`. Used only for shapes 21-24 to control the thickness of points border ([@bioguy2018, #258](https://github.com/kassambara/ggpubr/issues/236)).
- the `stat_cor()` function code has been simplified. New arguments `p.accuracy` and `r.accuracy` added; a real value specifying the number of decimal places of precision for the p-value and the correlation coefficient, respectively. Default is NULL. Use (e.g.) 0.01 to show 2 decimal places of precision ([@garthtarr, #186](https://github.com/kassambara/ggpubr/issues/186), [@raedevan6, #114](https://github.com/kassambara/ggpubr/issues/114), [#270](https://github.com/kassambara/ggpubr/issues/270)). 
  
  
## Bug fixes
   
- `annotate_figure()` manual updated to show how to use of superscript/subscript in the axis labels (#165). 
- `ggtextable()` now supports further customization when theme is specified (#283).
- the argument `font.family` is now correctly handled by `ggscatter()` (#149)
- `ggpar()` arguments are correctly applied using `ggpie()` (#277).
- `ggscatter()`: When `conf.int = FALSE`, fill color is set to "lightgray" for the regression line confidence band ([@zhan6073, #111](https://github.com/kassambara/ggpubr/issues/111)).
- Now, `gghistogram()` supports the paramter `yticks.by` ([@Chitanda-Satou, #258](https://github.com/kassambara/ggpubr/issues/258)).

   
# ggpubr 0.3.0
  
  
## New features
  
- New functions:
    - `ggsummarystats()` to create  a GGPLOT with summary stats table under the plot ( [#251](https://github.com/kassambara/ggpubr/pull/251)).
    - `clean_table_theme()` to clean the the theme of a table, such as those created by `ggsummarytable()`
- `ggbarplot()` now supports stacked barplots with error bars ([#245](https://github.com/kassambara/ggpubr/pull/245)).
   

   
## Minor changes
   
- New arguments:
    - `vjsut` in `stat_compare_means()` to move the text up or down relative to the bracket.
    - `type` in `geom_bracket()` to specify label type. Can be "text" or "expression" (for parsing plotmath expression); [#253](https://github.com/kassambara/ggpubr/issues/253).
    - `labeller` to the function `facet()`
    - `position` in `get_legend()` to specify legend position
    - `legend.grob` in `ggarrange()` to specify a common legend you want to add onto the combined plot.
- Maintenance adaptation to dplyr new version by removing deprecated functions, such as group_by_, select_, arrange_, etc
  
## Bug fixes
   
- Now, Barplots are correctly labelled when custom labels are specified by users ([@sekharcu, #234](https://github.com/kassambara/ggpubr/issues/234))
  
  
# ggpubr 0.2.5

## Minor changes

- New arguments `cor.coef.name` in the function `stat_cor()`. Can be one of "R" (pearson coef), "rho" (spearman coef) and "tau" (kendall coef). Uppercase and lowercase are allowed ([@andhamel, #216](https://github.com/kassambara/ggpubr/issues/228)).
- New arguments `digits, r.digits, p.digits` in the function `stat_cor()`. Integer indicating the number of decimal places (round) or significant digits (signif) to be used for the correlation coefficient and the p-value ([@raedevan6, #216](https://github.com/kassambara/ggpubr/issues/114)).
- `compare_means()` adapted to tidyr v>= 1.0.0 by specifying cols in the unnest() function ([@Youguang, #216](https://github.com/kassambara/ggpubr/issues/216)).
 

# ggpubr 0.2.4


## Minor change
   
- unnest adapted to tidyr 1.0.0
- `stat_pvalue_manual()` can now handle an rstatix test result containing only one group column.

# ggpubr 0.2.3
  
## New features
   
- New function `stat_central_tendency()` to add central tendency measures (mean, median, mode) to density and histogram plots
- New function `stat_overlay_normal_density()` to overlay normal density plot (with the same mean and SD) to the density distribution of 'x'. 
  
## Minor changes
  
- The option `exact = FALSE` is no longer used when computing correlation in `stat_cor()` ([@tiagochst, #205](https://github.com/kassambara/ggpubr/issues/205))
  
  
## Bug fixes
   
- `ggpie()` keeps now the default order of labels ([@WortJohn, #203](https://github.com/kassambara/ggpubr/pull/203))


# ggpubr 0.2.2
  
## New fatures

- New function `geom_bracket()` for adding brackets with label annotation to a ggplot. Helpers for adding p-value or significance levels to a plot.

## Minor changes

- `compare_means()` has been adapted to tidyr v1.0.0 ([@jennybc, #196](https://github.com/kassambara/ggpubr/pull/196))
- `geom_exec()` now handles `geom_bracket()` arguments
- New arguments `vjust`, `hide.ns`, `step.increase`, `step.group.by`, `color` and `linetype` added in `stat_pvalue_manual()`
- `stat_pvalue_manual()` can now guess automatically the significance label column.
- New argument `show.legend` added to `ggadd()` and `add_summary()` functions.
  
## Bug fixes
   
- Bug fixes in `gghistogram()`. Works now when the x variable is R keyword, such as var, mean, etc. ([#192](https://github.com/kassambara/ggpubr/issues/192))
- In `ggline()`, error bars now react automatically to grouping by line type ([#191](https://github.com/kassambara/ggpubr/issues/191))


# ggpubr 0.2.1
   
## Minor changes

- New arguments `step.increase` added in `stat_compare_means()` to avoid overlap between brackets.
- In `stat_pvalue_manual()` x axis variable is no longer automatically converted into factor. If your x variable is a factor, make sure that it is converted into factor.
- `stat_pvalue_manual()` can automatically handle the output of rstatix tests
- `ggbarplot()` and `ggviolin()` now automatically create error bars by groups when users forget the option `add.params = list(group = )` ([#183](https://github.com/kassambara/ggpubr/issues/183)).  
- Now, `ggarrange()` works when either `ncol = 1` or `nrow = 1` ([@GegznaV, #141](https://github.com/kassambara/ggpubr/issues/144).
- When method = "wilcox.test", the function `compare_means()` set automatically the option `exact = FALSE`. This is no longer the case ([@stemicha, #141](https://github.com/kassambara/ggpubr/issues/141). 
- `stat_pvalue_manual()` now supports dodged grouped plots ([@emcnerny, #104](https://github.com/kassambara/ggpubr/issues/104)).
- the argument `position` is now handled by `ggdotplot()` ([@Adam-JJJJJ, #178](https://github.com/kassambara/ggpubr/issues/178))

## Bug fixes

- Adding points works now for barplots grouped by fill color ([@elenichri](https://github.com/kassambara/ggpubr/issues/173)) 
- `label.sep` argument works now in `ggscatter()` and `stat_cor()` ([@sbbmu, #150](https://github.com/kassambara/ggpubr/issues/150))
- Fix in `ggscatter()` to avoid freezing when the `add` argument is incorrect ([@atakanekiz, #135](https://github.com/kassambara/ggpubr/issues/180)). 
  

# ggpubr 0.2

## Bug fixes
   
- P-value for multiple comparisons by group (stat_compare_means()) are now correctly displayed ([@elisheva100, #135](https://github.com/kassambara/ggpubr/issues/135)).


# ggpubr 0.1.9
  
## Minor changes
   
- ggsci palettes have been updated to add new palettes: nejm, jama, ucscgb, d3, locuszoom, igv, startrek, tron, futurama, simpsons ([@cbrueffer, #118](https://github.com/kassambara/ggpubr/pull/127)


## Bug fixes
   
- The option `ref.group` was only considered when the grouping variable contains more than two levels. In that case, each level is compared against the specified reference group. Now, `ref.group` option is also considereded in two samples mean comparisons ([@OwenDonohoe, #118](https://github.com/kassambara/ggpubr/issues/118))

- Now, `ggqqplot()` reacts to the argument `conf.int.level` ([@vsluydts, #123](https://github.com/kassambara/ggpubr/issues/123)
- Added error bar color is now inherited from the main plot ([@JesseRop, #109](https://github.com/kassambara/ggpubr/issues/109)


# ggpubr 0.1.8
  
  
## New features
 
- New arguments `bxp.errorbar` added to `ggboxplot()` for adding error bars at the top of the box plots ([@j3ypi, #105](https://github.com/kassambara/ggpubr/issues/105).
- New function `stat_pvalue_manual()` for adding p-values generated elswhere ([@achamess, #81](https://github.com/kassambara/ggpubr/issues/81), [@grst, #65](https://github.com/kassambara/ggpubr/issues/65)).


## Minor changes
   
- `alpha`option added to `ggviolin()` [@mtmatter, #77](https://github.com/kassambara/ggpubr/pull/77)
- New argument `bracket.size` added to `stat_compare_means()` [@mtmatter, #43](https://github.com/kassambara/ggpubr/issues/43)
- Now, the function `stat_cor()` supports R^2 as an option [@philament, #32](https://github.com/kassambara/ggpubr/issues/32)
- New argument `position` added in `gghistogram()`. Allowed values include "identity", "stack", "dodge".
- New argument `ci` added in `ggerrorplot()` [@abrar-alshaer, #94](https://github.com/kassambara/ggpubr/issues/94)

## Bug fixes
  
- Now, `ggscatter()` can remove the letter 'a' from the legend, when the argument `show.legend.text = FALSE` specified [@atsyplenkov, #106](https://github.com/kassambara/ggpubr/issues/106).
- Now, adding a `size` option to ggscatter `add.params` is supported [@retrogenomics, #94](https://github.com/kassambara/ggpubr/issues/53).

# ggpubr 0.1.7

## New features

- New function `ggdonutchart()` added.
   
## Minor changes

- Significance levels can be now customized and passed to `stat_compare_means()` ([@jaison75, #45](https://github.com/kassambara/ggpubr/issues/30)).

- Editing pdf size is now supported in `ggexport()` ([@JauntyJJS, #45](https://github.com/kassambara/ggpubr/issues/63)).

## Bug fixes

- In `ggscatterhist()` the x variable was plotted two times, on both the plot x & y margins, instead of having, as expected, a) the x variable on the main plot x margin and 2) the y variable on the main plot y margin. This has been now fixed. 
- In previous version, `ggdotchart()` sorted automatically within groups when the `color` argument is specified, even when groups = NULL. This default behaviour has been now removed. Sorting withi groups is performed only when the argument `group` is specified ([@sfeds, #90](https://github.com/kassambara/ggpubr/issues/90)).
- Now, `yticks.by` and  `xticks.by` work with NAs ([@j3ypi, #89](https://github.com/kassambara/ggpubr/issues/89)).
   
   
# ggpubr 0.1.6

## New features
   
- New function `ggballoonplot()` added to visualize a contingency table.

- `ggdotchart()` can be now used to plot multiple groups with `position = position_dodge()` ([@ManuelSpinola, #45](https://github.com/kassambara/ggpubr/issues/45)).

- New function `ggscatterhist()` to create a scatter plot with marginal histograms, density plots and box plots.

- New theme `theme_pubclean()`: a clean theme without axis lines, to direct more attention to the data.

- New arguments in `ggarrange()` to customize plot labels ([@G-Thomson, #41](https://github.com/kassambara/ggpubr/issues/38)):  
    - font.label
    - label.x and label.y
    - hjust and vjust
    
- New argument `method.args` added to `stat_compare_means()`. A list of additional arguments used for the test method. For example one might use method.args = list(alternative = "greater") for wilcoxon test ([@Nicktz, #41](https://github.com/kassambara/ggpubr/issues/41)).

- New argument `symnum.args` added to `stat_compare_means()`. A list of arguments to pass to the function symnum for symbolic number coding of p-values. For example, `symnum.args <- list(cutpoints = c(0, 0.0001, 0.001, 0.01, 0.05, 1), symbols = c("****", "***", "**", "*", "ns"))`

- New functions `table_cell_font()` and `table_cell_bg()` to easily access and change the text font and the background of `ggtexttable()` cells ([@ProbleMaker, #29](https://github.com/kassambara/ggpubr/issues/29)).
  
- New argument `numeric.x.axis` in `ggline()`. logical. If TRUE, x axis will be treated as numeric. Default is FALSE. ([@mdphan, #35](https://github.com/kassambara/ggpubr/issues/35))

- New argument `lab.nb.digits` in `ggbarplot()`. Integer indicating the number of decimal places (round) to be used ([#28](https://github.com/kassambara/ggpubr/issues/28)). Example: lab.nb.digits = 2.

- New argument `tip.length` in `stat_compare_means()`. Numeric vector with the fraction of total height that the bar goes down to indicate the precise column. Default is 0.03. Can be of same length as the number of comparisons to adjust specifically the tip lenth of each comparison. For example tip.length = c(0.01, 0.03).
   

## Minor changes

- Now `get_legend()` returns NULL when the plot doesn't have legend.  
   
   
## Bug fixes

- Now data argument are supported in  `stat_compare_means()` when the option comparisons are specified ([@emcnerny, #48](https://github.com/kassambara/ggpubr/issues/48))

- Now `compare_means()` returns the same p-values as `stat_compare_means()` ([@wydty, #15](https://github.com/kassambara/ggpubr/issues/34)).
- `stat_compare_means()` now reacts to label = "p.format" when comparisons specified ([#28](https://github.com/kassambara/ggpubr/issues/28)).
- Now, the p.values are displayed correctly when ref.group is not the first group ([@sehufnkjesktgna, #15](https://github.com/kassambara/ggpubr/issues/27)).
 
# ggpubr 0.1.5
  
## Minor changes

- In `ggpar()`, now `legend.title` can be either a character vector, e.g.: legend.title = "Species" or a list, `legend.title = list(color = "Species", linetype = "Species", shape = "Species")`.

- New argument `ellipse.border.remove` in `ggscatter()` to remove ellipse border lines.
   
```r
ggscatter(mtcars, x = "mpg", y = "wt", 
          color = "cyl",
          ellipse = TRUE, mean.point = TRUE, 
          ellipse.border.remove = TRUE)
```

- In `ggscatter`(), the argument `mean.point` now reacts to fill color.
- Support for text justification added in `ggtexttable()` ([@cj-wilson, #15](https://github.com/kassambara/ggpubr/issues/18))

- The function `ggpie()` can now display japanese texts. New argument `font.family` in `ggpie`() and in `ggpar()` ([@tomochan001, #15](https://github.com/kassambara/ggpubr/issues/15)).

- Using time on x axis works know with `ggline()` and `ggbarplot()` ([@jcpsantiago, #15](https://github.com/kassambara/ggpubr/issues/17)).



## Bug fixes
   
- `stat_compare_means()` now reacts to `hide.ns` properly.
- `drawDetails.splitText()` exported so that the function `ggparagraph()` works properly.
- Now, ggpubr functions accept expression for label text
- In `ggbarplot()`, now labels correspond to the true size of bars ([@tdelhomme, #15](https://github.com/kassambara/ggpubr/issues/15)).
- `stat_compare_means()` now keep the default order of factor levels ([@RoKant, #12](https://github.com/kassambara/ggpubr/issues/12)).


# ggpubr 0.1.4

## New features

- New helper functions:
    - `gradient_color()` and `gradient_color()`: change gradient color and fill palettes.
    - `clean_theme()`: remove axis lines, ticks, texts and titles.
    - `get_legend()`: to extract the legend labels from a ggplot object.
    - `as_ggplot()`: Transform the output of `gridExtra::arrangeGrob()` and `gridExtra::grid.arrange()` to a an object of class ggplot.
    - `ggtexttable()`: to draw a textual table.
    - `ggparagraph()`: to draw a paragraph of text.
    - fill_palette() and color_palette() to change the fill and color palette, respectively.
    - `annotate_figure()` to annotate (arranged) ggplots.
    - `text_grob()` to create easily a customized text graphical object. 
    - `background_image()` to add a background image to a ggplot.
    
- New theme function `theme_transparent()` to create a ggplot with transparent background.
 
## Minor changes

- In `gghistogram()`, density curve and rug react to the fill color.
- `ggarrange()`:
    - New  argument `àlign` to specify whether graphs in the grid should be horizontally ("h") or vertically ("v") aligned. 
    - New argument `legend` to remove or specify the legend position when arranging multiple plots.
    - New argument `common.legend` to create a common unique legend for multiple plots.
     

# ggpubr 0.1.3
   
## New features
   
- New functions:
    - `ggarrange()` to arrange multiple ggplots on the same page.
    - `ggexport()` to export one or multiple ggplots to a file (pdf, eps, png, jpeg).
    - `ggpaired()` to plot paired data.
    - `compare_means()` to compare the means of two or multiple groups. Returns a data frame.
    - `stat_compare_means()` to add p-values and significance levels to plots.
    - `stat_cor()` to add correlation coefficients with p-values to a scatter plot.
    - `stat_stars()` to add stars to a scatter plot.
      
      
      
- Now, the argument `y` can be a character vector of multiple variables to plot at once. This might be useful in genomic fields to plot the gene expression levels of multiple genes at once. see `ggboxplot()`, `ggdotplot()`, `ggstripchart()`, `ggviolin()`, `ggbarplot()` and `ggline`.
   
- The argument `x` can be a vector of multiple variables in `gghistogram()`, `ggdensity()`, `ggecdf()` and `ggqqplot()`.
     
     
- New functions to edit ggplot graphical parameters:
    - `font()` to change the appearance of titles and labels.
    - `rotate_x_text()` and `rotate_y_text()` to rotate x and y axis texts.
    - `rotate()` to rotate a ggplot for creating horizontal plot.
    - `set_palette()` or `change_palette()` to change a ggplot color palette.
    - `border()` to add/change border lines around a ggplot.
    - `bgcolor()` to change ggplot panel background color.
    - `rremove()` to remove a specific component from a ggplot.
    - `grids()` to add grid lines.
    - `xscale()` and `yscale()` to change axis scale.
       
       
- New helper functions:
    - `facet()` added to create multi-panel plots ([#5](https://github.com/kassambara/ggpubr/issues/5)).
    - `add_summary()` to add summary statistics.
    - `ggadd()` to add summary statistics or a geometry onto a ggplot.
      
      
- New data set added: `gene_citation`    
     
     
- New arguments in `ggpar()`: `x.text.angle` and `y.text.angle`

      
      
## Major changes
   
- New arguments in ggpubr functions, see `ggboxplot()`, `ggdotplot()`, `ggstripchart()`, `ggviolin()`, `ggbarplot()` and `ggline`:
    - `combine` added to combine multiple y variables on the same graph.
    - `merge` to merge multiple y variables in the same ploting area.
    - `select` to select which item to display.
    - `remove` to remove a specific item from a plot.
    - `order` to order plot items.
    - `label, font.label, label.select, repel, label.rectangle` to add and customize labels
    - `facet.by, panel.labs and short.panel.labs`: support for faceting and customization of plot panels
        
        
- New argument `grouping.vars`  in `ggtext()`. Grouping variables to sort the data by, when the user wants to display the top n up/down labels.
      
      
- New arguments in `theme_pubr()`: 
    - border,
    - margin, 
    - legend,
    - x.text.angle

   
## Minor changes


- Now, the argument `palette` Can be also a numeric vector of length(groups); in this case a basic color palette is created using the function `grDevices::palette()`.
   
# Bug fixes
   
- Now, `ggpar()` reacts to palette when length(palette) = 1 and palette is a color name [#3](https://github.com/kassambara/ggpubr/issues/3).

- `ggmaplot()` now handles situations, where there is only upregulated, or downlegulated gnes.
  

# ggpubr 0.1.2
   
    
## New features
   
- New function `get_palette()` to generate a palette of k colors from ggsci palettes, RColorbrewer palettes and custom color palettes. Useful to extend RColorBrewer and ggsci to support more colors.
  
## Minor changes
   
- Now the `ggpar()` function can handle a list of ggplots.
- Now the default legend position is `right`.
- New argument `show.legend.text` in the `ggscatter()` function. Use show.legend.text = FALSE to hide text in the legend.
- New arguments `title, submain, subtitle, caption, font.submain, font.subtitle, font.caption` in the `ggpar()` function.
- New argument `font.family` in `ggscatter()`.
   
## Bug fixed
   
- The mean within group for `ggdensity` (`gghistogram`) are now shown if data have NA values [@chunkaowang, #1](https://github.com/kassambara/ggpubr/issues/1)
   
   
# ggpubr 0.1.1
   
  
## New features
   
- New function `ggtext()` for textual annotation.
- New argument star.plot in `ggscatter()`. A logical value. If TRUE, a star plot is generated.
- New helper function `geom_exec()`. A helper function used by ggpubr functions to execute any geom_xx functions in ggplot2. Useful only when you want to call a geom_xx function without carrying about the arguments to put in `ggplot2::aes()`.
- New arguments sort.val and top in `ggbarplot()`. 
    - sort.val: a string specifying whether the value should be sorted. Allowed values are "none" (no sorting), "asc" (for ascending) or "desc" (for descending).
    - top: a numeric value specifying the number of top elements to be shown.
- New function `theme_classic2()` added. Classic theme with axis lines.
    
    
## Minor changes

- `ggboxplot()`, `ggviolin()`, `ggdotplot()`, `ggstripchart()`, `gghistogram()`, `ggdensity()`, `ggecdf()` and `ggqqplot()` can now handle one single numeric vector.

```
# Example
ggboxplot(iris$Sepal.Length)
```

- Now, in `gghistogram()`, when add_density = TRUE, y scale remains = "..count..".
- Now, default theme changed to theme_classic2()
- Default point size and line size set to NULL

   

# ggpubr 0.1.0


## Plot one variable - X: Continuous

- ggdensity(): Density plot
- gghistogram(): Histogram plot
- ggecdf(): Empirical cumulative density function
- ggqqplot(): QQ plots


## Plot two variables - X & Y: Discrete X and Continuous Y

- ggboxplot(): Box plot
- ggviolin(): Violin plot
- ggdotplot(): Dot plot
- ggstripchart(): Stripchart (jitter)
- ggbarplot(): Bar plot
- ggline(): Line plot
- ggerrorplot(): Error plot
- ggpie(): Pie chart
- ggdotchart(): Cleveland's dot plots


## Plot two continuous variables

- ggscatter(): Scatter plot
  
  
## Graphical paramters
   
- ggpar(): Change graphical parameters
- show_line_type(): Line types available in R
- show_point_shapes(): Point shapes available in R
- theme_pubr(): Create a publication ready theme
- labs_pubr(): Format only plot labels to a publication ready style
   
   
## Genomics
    
- ggmaplot(): MA-plot from means and log fold changes
   
   
## Data
   
- diff_express: Differential gene expression analysis results
   
   
## Other
   
- desc_statby(): Descriptive statistics by groups
- stat_chull(): Plot convex hull of a set of points
- stat_conf_ellipse(): Plot confidence ellipses
- stat_mean(): Draw group mean points
# xml2 1.3.2

* `read_html()` and `read_xml()` now error if passed strings of length greater than one (#121)

* `read_xml.raw()` had an inadvertent regression in 1.3.0 and is now again fixed (#300)

* Compilation fix on macOS 10.15.4 (@kevinushey, #296)

# xml2 1.3.1

* `read_html()` now again works with HTML files with non-ASCII encodings (#293).

# xml2 1.3.0

* Removes the Rcpp dependency

# xml2 1.2.5

* Fix compilation issue on macOS versions after High Sierra when not using homebrew supplied libxml2

# xml2 1.2.4

* Fix potential dangling pointer with internal `asXmlChar()` function (@michaelquinn32, #287).

* `as_xml_document()` now handles cases with text nodes trailing normal nodes (#274).

* `xml_add_child()` can now create nodes with a `par` attribute. These previously errored due to partial name matching of the `parent` function in the internal `create_node()` function. (@jennybc, #285)

* `libxml2_version()` now returns a semantic version rather than alphanumeric version, so "2.9.10" > "2.9.9" (#277)

# xml2 1.2.2

* Export S4 classes with documentation, so they can be used in other packages
without Warnings (@nuest, #267)

# xml2 1.2.1

## New Features

* xml2 now has a pkgdown site! <http://xml2.r-lib.org> (@jayhesselberth, #211).

* Windows: upgrade to libxml2 2.9.8

* print methods now match the type of document, e.g. `read_html()` prints as
  "{html_document}" rather than "{xml_document}" (#227)

## Bugfixes and Miscellaneous features

* Generic xml2 error are now forwarded as R errors. Previously these errors
  were output to stderr, so could not be suppressed (#209).

* Fix for ICU 59+ defaulting to use char16_t, which is only available in C++11 (#231)

* No longer uses the C connections API

* Better error message when trying to run `download_xml()` without the curl
  package installed (#262)

* xml2 classes are now registered for use with S4 by calling `setOldClass()` (#248)

* Nodes with nested data type definition entities now work without crashing (#241)

* Test failure fixed due to behavior change with relative paths in libxml2
  2.9.9 (#245).

* `read_xml()` now has a better error message when given zero length character
  inputs (#212).

* `read_xml()` and `read_html()` now automatically check if the response
  succeeded before trying to read from a HTTP response (#255).

* `xml_root()` can now create root nodes with namespaces (#239)

* `xml_set_attr()` no longer crashes if you try to set the same namespace on
  the same node multiple times (#253).

* `xml_set_attr()` now recycles the values if needed (#221)

* `xml_structure()` gains a `file` argument, to support writing to a file
  rather than the console (#244).


# xml2 1.2.0

## Breaking changes

* `as_list()` on `xml_document` objects did not properly include the root node
  in the returned list. Previous behavior can be obtained by using
  `as_list()[[1L]]` in place of `as_list()`.

## New Features

* `download_xml()` and `download_html()` helper functions to make it easy to
  download files (#193).

* `xml_attr()` can now set attributes with no value (#198).

* `xml_serialize()` and `xml_unserialize()` now create file connections when
  given character input (#179).

## Bugfixes

* `xml_find_first()` no longer de-duplicates results, so the results are always
  the same length as the inputs (as documented) (#194).

* xml2 can now build using libxml2 2.7.0

* Use Rcpp symbol registration and visibility to prevent symbol conflicts on Linux

* `xml_add_child()` now requires less resources to insert a node when called
  with `.where = 0L` (@heckendorfc, #175).

* Fixed failing examples due to a change in an external resource.

# xml2 1.1.1

* This is a small point release addressing installation issues found with older
  libxml2 versions shipped with RedHat Linux 6 / CentOS 6 (#163, #164).

# xml2 1.1.0

## New Features
* `write_xml()` and `write_html()` now accept connections as well as filenames
  for output. (#157)

* `xml_add_child()` now takes a `.where` argument specifying where to add the
  new children. (#138)

* `as_xml()` generic function to convert R objects to xml. The most important
  method is for lists and enables full roundtrip support for going to and back
  from xml for lists and enables full roundtrip support to and from XML. (#137, #143)

* `xml_new_root()` can be used to create a new document and a root node in one step (#131).

* `xml_add_parent()` inserts a new node between the node and its parent (#129)

* Add `xml_validate()` to validate a document against an xml schema (#31, @jeroenooms).

* Export `xml2_types.h` to allow for extension packages such as xslt.

* `xml_comment()` allows you to add comment nodes to a document. (#111)

* `xml_cdata()` allows you to add CDATA nodes to a document. (#128)

* Add `xml_set_text()` and `xml_set_name()` equivalent to `xml_text<-` and `xml_name<-`. (#130).

* Add `xml_set_attr()` and `xml_set_attrs()` equivalent to `xml_attr<-` and `xml_attrs<-`. (#109, #130)

* Add `write_html()` method (#133).

## Bugfixes

* `xml_new_document()` now explicitly sets the encoding (default UTF-8) (#142)

* Document formatting options for `write_xml()` (#132)

* Add missing methods for xml_missing objects. (#134)

* Bugfix for xml_length.xml_nodeset that caused it to fail unconditionally. (#140)

* `is.na()` now returns `TRUE` for `xml_missing` objects. (#139)

* Trim non-breaking spaces in `xml_text(trim = TRUE)` (#151).

* Allow setting non-character attributes (values are coerced to characters). (@sjp, #117, #122).

* Fixed return value in call to vapply in xml_integer.xml_nodeset. (@ddiez, #146, #147).

* Allow docs missing a root element to be created and printed. (@sjp, #126, #121).

* xml_add_* methods now return invisibly. (@sjp, #124)

* `as_list()` now preserves element names when attributes exist, and escapes
  XML attributes that conflict with special R attributes (@peterfoley, #115).

# xml2 1.0.0

* All C++ functions now use `checked_get()` instead of `get()` where possible,
  so NULL XPtrs properly throw an error rather than crashing. (@jimhester,
  #101, #104).

* `xml_integer()` and `xml_double()` functions to make it easy to extract
  integer and double text from nodes (@jimhester, #97, #99).

* xml2 now supports modification and creation of XML nodes. New functions
  `xml_new_document()`, `xml_new_child()`, `xml_new_sibling()`,
  `xml_set_namespace()`, , `xml_remove()`, `xml_replace()`, `xml_root()`
  and replacement methods for `xml_name()`, `xml_attr()`, `xml_attrs()` and
  `xml_text()` (@jimhester, #9 #76)

* `xml_ns()` now keeps namespace prefixes that point to the same URI
  (@jimhester, #35, #95).

* `read_xml()` and `read_html()` methods added for `httr::response()` objects.
  (@jimhester, #63, #93)

* `xml_child()` function to make selecting children a little easier
  (@jimhester, #23, #94)

* `xml_find_one()` has been deprecated in favor of `xml_find_first()`
  (@jimhester, #58, #92)

* `xml_read()` functions now default to passing the document's namespace
  object. Namespace definitions can now be removed as well as added and
  `xml_ns_strip()` added to remove all default namespaces from a document.
  (@jimhester, #28, #89)

* `xml_read()` gains a `options` argument to control all available parsing
  options, including `HUGE` to turn off limits for parsing very large
  documents and now drops blank text nodes by default, mimicking default
  behavior of XML package. (@jimhester, #49, #62, #85, #88)

* `xml_write()` expands the path on filenames, so directories can be specified
  with '~/' (@jimhester, #86, #80)

* `xml_find_one()` now returns a 'xml_missing' node object if there are 0
  matches (@jimhester, #55, #53, hadley/rvest#82).

* `xml_find_num()`, `xml_find_chr()`, `xml_find_lgl()` functions added to
  return numeric, character and logical results from XPath expressions. (@jimhester, #55)

* `xml_name()` and `xml_text()` always correctly encode returned value as
  UTF-8 (#54).

# xml2 0.1.2

* Improved configure script - now works again on R-devel on windows.

* Compiles with older versions of libxml2.,

# xml2 0.1.1

* Make configure script more cross platform.

* Add `xml_length()` to count the number of children (#32).
# DBI 1.1.0

## New features

- New `DBIConnector` class (#280).

- Specify `immediate` argument to `dbSendQuery()`, `dbGetQuery()`, `dbSendStatement()` and `dbExecute()` (#268).

- Use specification for `dbGetInfo()` (#271).

- `dbUnquoteIdentifier()` now supports `Id()` objects with `catalog` members (#266, @raffscallion). It also handles unquoted identifiers of the form `table`, `schema.table` or `catalog.schema.table`, for compatibility with dbplyr.


## Documentation

- New DBI intro article (#286, @cutterkom).

- Add pkgdown reference index (#288).

- DBI specification on https://dbi.r-dbi.org/dev/articles/spec now comes with a table of contents and code formatting.

- Update examples to refer to `params` instead of `param` (#235).

- Improved documentation for `sqlInterpolate()` (#100). Add usage of `SQL()` to `sqlInterpolate()` examples (#259, @renkun-ken).

- Improve documentation for `Id`.


## Internal

- Add tests for `dbUnquoteIdentifier()` (#279, @baileych).

- `sqlInterpolate()` uses `dbQuoteLiteral()` instead of checking the type of the input.

- Avoid partial argument match in `dbWriteTable()` (#246, @richfitz).


# DBI 1.0.0 (2018-05-02)

## New generics

- New `dbAppendTable()` that by default calls `sqlAppendTableTemplate()` and then `dbExecute()` with a `param` argument, without support for `row.names` argument (#74).
- New `dbCreateTable()` that by default calls `sqlCreateTable()` and then `dbExecute()`, without support for `row.names` argument (#74).
- New `dbCanConnect()` generic with default implementation (#87).
- New `dbIsReadOnly()` generic with default implementation (#190, @anhqle).

## Changes

- `sqlAppendTable()` now accepts lists for the `values` argument, to support lists of `SQL` objects in R 3.1.
- Add default implementation for `dbListFields(DBIConnection, Id)`, this relies on `dbQuoteIdentifier(DBIConnection, Id)` (#75).

## Documentation updates

- The DBI specification vignette is rendered correctly from the installed package (#234).
- Update docs on how to cope with stored procedures (#242, @aryoda).
- Add "Additional arguments" sections and more examples for `dbGetQuery()`, `dbSendQuery()`, `dbExecute()` and `dbSendStatement()`.
- The `dbColumnInfo()` method is now fully specified (#75).
- The `dbListFields()` method is now fully specified (#75).
- The dynamic list of methods in help pages doesn't contain methods in DBI anymore.

## Bug fixes

- Pass missing `value` argument to secondary `dbWriteTable()` call (#737, @jimhester).
- The `Id` class now uses `<Id>` and not `<Table>` when printing.
- The default `dbUnquoteIdentifier()` implementation now complies to the spec.


# DBI 0.8 (2018-02-24)

Breaking changes
----------------

- `SQL()` now strips the names from the output if the `names` argument is unset.
- The `dbReadTable()`, `dbWriteTable()`, `dbExistsTable()`, `dbRemoveTable()`, and `dbListFields()` generics now specialize over the first two arguments to support implementations with the `Id` S4 class as type for the second argument. Some packages may need to update their documentation to satisfy R CMD check again.

New generics
------------

- Schema support: Export `Id()`, new generics `dbListObjects()` and `dbUnquoteIdentifier()`, methods for `Id` that call `dbQuoteIdentifier()` and then forward (#220).
- New `dbQuoteLiteral()` generic. The default implementation uses switchpatch to avoid dispatch ambiguities, and forwards to `dbQuoteString()` for character vectors. Backends may override methods that also dispatch on the second argument, but in this case also an override for the `"SQL"` class is necessary (#172).

Default implementations
-----------------------

- Default implementations of `dbQuoteIdentifier()` and `dbQuoteLiteral()` preserve names, default implementation of `dbQuoteString()` strips names (#173).
- Specialized methods for `dbQuoteString()` and `dbQuoteIdentifier()` are available again, for compatibility with clients that use `getMethod()` to access them (#218).
- Add default implementation of `dbListFields()`.
- The default implementation of `dbReadTable()` now has `row.names = FALSE` as default and also supports `row.names = NULL` (#186).

API changes
-----------

- The `SQL()` function gains an optional `names` argument which can be used to assign names to SQL strings.

Deprecated generics
-------------------

- `dbListConnections()` is soft-deprecated by documentation.
- `dbListResults()` is deprecated by documentation (#58).
- `dbGetException()` is soft-deprecated by documentation (#51).
- The deprecated `print.list.pairs()` has been removed.

Bug fixes
---------

- Fix `dbDataType()` for `AsIs` object (#198, @yutannihilation).
- Fix `dbQuoteString()` and `dbQuoteIdentifier()` to ignore invalid UTF-8 strings (r-dbi/DBItest#156).

Documentation
-------------

- Help pages for generics now contain a dynamic list of methods implemented by DBI backends (#162).
- `sqlInterpolate()` now supports both named and positional variables (#216, @hannesmuehleisen).
- Point to db.rstudio.com (@wibeasley, #209).
- Reflect new 'r-dbi' organization in `DESCRIPTION` (@wibeasley, #207).

Internal
--------

- Using switchpatch on the second argument for default implementations of `dbQuoteString()` and `dbQuoteIdentifier()`.


# DBI 0.7 (2017-06-17)

- Import updated specs from `DBItest`.
- The default implementation of `dbGetQuery()` now accepts an `n` argument and forwards it to `dbFetch()`. No warning about pending rows is issued anymore (#76).
- Require R >= 3.0.0 (for `slots` argument of `setClass()`) (#169, @mvkorpel).


# DBI 0.6-1 (2017-04-01)

- Fix `dbReadTable()` for backends that do not provide their own implementation (#171).


# DBI 0.6 (2017-03-08)

- Interface changes
    - Deprecated `dbDriver()` and `dbUnloadDriver()` by documentation (#21).
    - Renamed arguments to  `sqlInterpolate()` and `sqlParseVariables()` to be more consistent with the rest of the interface, and added `.dots` argument to `sqlParseVariables`. DBI drivers are now expected to implement `sqlParseVariables(conn, sql, ..., .dots)` and `sqlInterpolate(conn, sql, ...)` (#147).

- Interface enhancements
    - Removed `valueClass = "logical"` for those generics where the return value is meaningless, to allow backends to return invisibly (#135).
    - Avoiding using braces in the definitions of generics if possible, so that standard generics can be detected (#146).
    - Added default implementation for `dbReadTable()`.
    - All standard generics are required to have an ellipsis (with test), for future extensibility.
    - Improved default implementation of `dbQuoteString()` and `dbQuoteIdentifier()` (#77).
    - Removed `tryCatch()` call in `dbGetQuery()` (#113).

- Documentation improvements
    - Finalized first draft of DBI specification, now in a vignette.
    - Most methods now draw documentation from `DBItest`, only those where the behavior is not finally decided don't do this yet yet.
    - Removed `max.connections` requirement from documentation (#56).
    - Improved `dbBind()` documentation and example (#136).
    - Change `omegahat.org` URL to `omegahat.net`, the particular document still doesn't exist below the new domain.

- Internal
    - Use roxygen2 inheritance to copy DBI specification to this package.
    - Use `tic` package for building documentation.
    - Use markdown in documentation.


# DBI 0.5-1 (2016-09-09)

- Documentation and example updates.


# DBI 0.5 (2016-08-11, CRAN release)

- Interface changes
    - `dbDataType()` maps `character` values to `"TEXT"` by default (#102).
    - The default implementation of `dbQuoteString()` doesn't call `encodeString()` anymore: Neither SQLite nor Postgres understand e.g. `\n` in a string literal, and all of SQLite, Postgres, and MySQL accept an embedded newline (#121).

- Interface enhancements
    - New `dbSendStatement()` generic, forwards to `dbSendQuery()` by default (#20, #132).
    - New `dbExecute()`, calls `dbSendStatement()` by default (#109, @bborgesr).
    - New `dbWithTransaction()` that calls `dbBegin()` and `dbCommit()`, and `dbRollback()` on failure (#110, @bborgesr).
    - New `dbBreak()` function which allows aborting from within `dbWithTransaction()` (#115, #133).
    - Export `dbFetch()` and `dbQuoteString()` methods.

- Documentation improvements:
    - One example per function (except functions scheduled for deprecation) (#67).
    - Consistent layout and identifier naming.
    - Better documentation of generics by adding links to the class and related generics in the "See also" section under "Other DBI... generics" (#130). S4 documentation is directed to a hidden page to unclutter documentation index (#59).
    - Fix two minor vignette typos (#124, @mdsumner).
    - Add package documentation.
    - Remove misleading parts in `dbConnect()` documentation (#118).
    - Remove misleading link in `dbDataType()` documentation.
    - Remove full stop from documentation titles.
    - New help topic "DBIspec" that contains the full DBI specification (currently work in progress) (#129).
    - HTML documentation generated by `staticdocs` is now uploaded to http://rstats-db.github.io/DBI for each build of the "production" branch (#131).
    - Further minor changes and fixes.

- Internal
    - Use `contains` argument instead of `representation()` to denote base classes (#93).
    - Remove redundant declaration of transaction methods (#110, @bborgesr).


# DBI 0.4-1 (2016-05-07, CRAN release)

- The default `show()` implementations silently ignore all errors.  Some DBI drivers (e.g., RPostgreSQL) might fail to implement `dbIsValid()` or the other methods used.


# DBI 0.4 (2016-04-30)

* New package maintainer: Kirill Müller.

* `dbGetInfo()` gains a default method that extracts the information from
  `dbGetStatement()`, `dbGetRowsAffected()`, `dbHasCompleted()`, and 
  `dbGetRowCount()`. This means that most drivers should no longer need to
  implement `dbGetInfo()` (which may be deprecated anyway at some point) (#55).

* `dbDataType()` and `dbQuoteString()` are now properly exported.

* The default implementation for `dbDataType()` (powered by `dbiDataType()`) now
  also supports `difftime` and `AsIs` objects and lists of `raw` (#70).

* Default `dbGetQuery()` method now always calls `dbFetch()`, in a `tryCatch()`
  block.

* New generic `dbBind()` for binding values to a parameterised query.

* DBI gains a number of SQL generation functions. These make it easier to 
  write backends by implementing common operations that are slightly
  tricky to do absolutely correctly. 
  
    * `sqlCreateTable()` and `sqlAppendTable()` create tables from a data
      frame and insert rows into an existing table. These will power most
      implementations of `dbWriteTable()`. `sqlAppendTable()` is useful
      for databases that support parameterised queries.
      
    * `sqlRownamesToColumn()` and `sqlColumnToRownames()` provide a standard
      way of translating row names to and from the database.
      
    * `sqlInterpolate()` and `sqlParseVariables()` allows databases without
      native parameterised queries to use parameterised queries to avoid
      SQL injection attacks.
      
    * `sqlData()` is a new generic that converts a data frame into a data
      frame suitable for sending to the database. This is used to (e.g.) 
      ensure all character vectors are encoded as UTF-8, or to convert
      R varible types (like factor) to types supported by the database.

    * The `sqlParseVariablesImpl()` is now implemented purely in R, with full
      test coverage (#83, @hannesmuehleisen).

* `dbiCheckCompliance()` has been removed, the functionality is now available
  in the `DBItest` package (#80).

* Added default `show()` methods for driver, connection and results.

* New concrete `ANSIConnection` class and `ANSI()` function to generate a dummy
  ANSI compliant connection useful for testing.

* Default `dbQuoteString()` and `dbQuoteIdentifer()` methods now use 
  `encodeString()` so that special characters like `\n` are correctly escaped.
  `dbQuoteString()` converts `NA` to (unquoted) NULL.

* The initial DBI proposal and DBI version 1 specification are now included as 
  a vignette. These are there mostly for historical interest.

* The new `DBItest` package is described in the vignette.

* Deprecated `print.list.pairs()`.

* Removed unused `dbi_dep()`.



# Version 0.3.1

* Actually export `dbIsValid()` :/

* `dbGetQuery()` uses `dbFetch()` in the default implementation.

# Version 0.3.0

## New and enhanced generics

* `dbIsValid()` returns a logical value describing whether a connection or 
  result set (or other object) is still valid. (#12).

* `dbQuoteString()` and `dbQuoteIdentifier()` to implement database specific
  quoting mechanisms.

* `dbFetch()` added as alias to `fetch()` to provide consistent name. 
  Implementers should define methods for both `fetch()` and `dbFetch()` until 
  `fetch()` is deprecated in 2015. For now, the default method for `dbFetch()` 
  calls `fetch()`.

* `dbBegin()` begins a transaction (#17). If not supported, DB specific
  methods should throw an error (as should `dbCommit()` and `dbRollback()`).

## New default methods

* `dbGetStatement()`, `dbGetRowsAffected()`, `dbHasCompleted()`, and 
  `dbGetRowCount()` gain default methods that extract the appropriate elements
  from `dbGetInfo()`. This means that most drivers should no longer need to
  implement these methods (#13).

* `dbGetQuery()` gains a default method for `DBIConnection` which uses
  `dbSendQuery()`, `fetch()` and `dbClearResult()`.

## Deprecated features

* The following functions are soft-deprecated. They are going away, 
  and developers who use the DBI should begin preparing. The formal deprecation
  process will begin in July 2015, where these function will emit warnings 
  on use.

    * `fetch()` is replaced by `dbFetch()`.

    * `make.db.names()`, `isSQLKeyword()` and `SQLKeywords()`: a black list 
      based approach is fundamentally flawed; instead quote strings and 
      identifiers with `dbQuoteIdentifier()` and `dbQuoteString()`.
  
* `dbGetDBIVersion()` is deprecated since it's now just a thin wrapper
  around `packageVersion("DBI")`.

* `dbSetDataMappings()` (#9) and `dbCallProc()` (#7) are deprecated as no 
  implementations were ever provided.

## Other improvements

* `dbiCheckCompliance()` makes it easier for implementors to check that their
  package is in compliance with the DBI specification.

* All examples now use the RSQLite package so that you can easily try out
  the code samples (#4).

* `dbDriver()` gains a more effective search mechanism that doesn't rely on
  packages being loaded (#1).

* DBI has been converted to use roxygen2 for documentation, and now most
  functions have their own documentation files. I would love your feedback
  on how we could make the documentation better!

# Version 0.2-7

* Trivial changes (updated package fields, daj)

# Version 0.2-6

* Removed deprecated \synopsis in some Rd files (thanks to Prof. Ripley)

# Version 0.2-5

* Code cleanups contributed by Matthias Burger: avoid partial argument
  name matching and use TRUE/FALSE, not T/F.

* Change behavior of make.db.names.default to quote SQL keywords if
  allow.keywords is FALSE.  Previously, SQL keywords would be name
  mangled with underscores and a digit.  Now they are quoted using
  '"'.

# Version 0.2-4

* Changed license from GPL to LPGL

* Fixed a trivial typo in documentation

# Version 0.1-10

* Fixed documentation typos.

# Version 0.1-9

* Trivial changes.

# Version 0.1-8

* A trivial change due to package.description() being deprecated in 1.9.0.

# Version 0.1-7

* Had to do a substantial re-formatting of the documentation
  due to incompatibilities introduced in 1.8.0 S4 method
  documentation. The contents were not changed (modulo fixing 
  a few typos).  Thanks to Kurt Hornik and John Chambers for
  their help.

# Version 0.1-6

* Trivial documentation changes (for R CMD check's sake)

# Version 0.1-5

* Removed duplicated setGeneric("dbSetDataMappings") 

# Version 0.1-4

* Removed the "valueClass" from some generic functions, namely,
  dbListConnections, dbListResults, dbGetException, dbGetQuery,
  and dbGetInfo.  The reason is that methods for these generics
  could potentially return different classes of objects (e.g., 
  the call dbGetInfo(res) could return a list of name-value pairs,
  while dbGetInfo(res, "statement") could be a character vector).

* Added 00Index to inst/doc

* Added dbGetDBIVersion() (simple wrapper to package.description).

# Version 0.1-3

* ??? Minor changes?

# Version 0.1-2

* An implementation based on version 4 classes and methods.
* Incorporated (mostly Tim Keitt's) comments.
# broom 0.7.0

`broom 0.7.0` is a major release with a large number of new tidiers,
soft-deprecations, and planned hard-deprecations of functions and arguments.

### Big picture changes

- We have changed how we report degrees of freedom for `lm` objects 
(#212, #273). This is especially important for instructors in statistics 
courses. Previously the `df` column in `glance.lm()` reported the rank of the 
design matrix. Now it reports degrees of freedom of the numerator for the 
overall F-statistic. This is equal to the rank of the model matrix minus one 
(unless you omit an intercept column), so the new `df` should be the old 
`df` minus one.

- We are moving away from supporting `summary.*()` objects. In particular, we 
have removed `tidy.summary.lm()` as part of a major overhaul of internals. 
Instead of calling `tidy()` on `summary`-like objects, please call `tidy()` 
directly on model objects moving forward.

- We have removed all support for the `quick` argument in `tidy()` methods. 
This is to simplify internals and is for maintainability purposes. We anticipate
this will not influence many users as few people seemed to use it. If this 
majorly cramps your style, let us know, as we are considering a new verb to 
return only model parameters. In the meantime, `stats::coef()` together with 
`tibble::enframe()` provides most of the functionality 
of `tidy(..., quick = TRUE)`.

- All `conf.int` arguments now default to `FALSE`, and all `conf.level` 
arguments now default to `0.95`. This should primarily affect `tidy.survreg()`, 
which previously always returned confidence intervals, although there are 
some others.

- Tidiers for `emmeans`-objects use the arguments `conf.int` and `conf.level` 
instead of relying on the argument names native to 
the `emmeans::summary()`-methods (i.e., `infer` and `level`). 
Similarly, `multcomp`-tidiers now include a call to `summary()` as previous 
behavior was akin to setting the now removed argument `quick = TRUE`. Both 
families of tidiers now use the `adj.p.value` column name when appropriate. 
Finally, `emmeans`-, `multcomp`-, and `TukeyHSD`-tidiers now consistently 
use the column names `contrast` and `null.value` instead 
of `comparison`, `level1` and `level2`, or `lhs` and `rhs` (see #692).

### Deprecations

This release of `broom` soft-deprecates the following functions and tidier 
methods:

- Tidier methods for data frames, rowwise data frames, vectors and matrices
- `bootstrap()`
- `confint_tidy()`
- `fix_data_frame()`
- `finish_glance()`
- `augment.glmRob()`
- `tidy.table()` and `tidy.ftable()` have been deprecated in favor of
`tibble::as_tibble()`
- `tidy.summaryDefault()` and `glance.summaryDefault()` have been deprecated in 
favor of `skimr::skim()`

We have also gone forward with our planned mixed model deprecations, and have 
removed the following methods, which now live in `broom.mixed`:

- `tidy.brmsfit()`
- `tidy.merMod()`, `glance.merMod()`, `augment.merMod()`
- `tidy.lme()`, `glance.lme()`, `augment.lme()`
- `tidy.stanreg()`, `glance.stanreg()`
- `tidyMCMC()`, `tidy.rjags()`, `tidy.stanfit()`

### Minor breaking changes

- `augment.factanal()` now returns a tibble with columns names `.fs1`, `.fs2`, 
  ..., instead of `factor1`, `factor2`, ... (#650)

- We have renamed the output of `augment.htest()`. In particular, we have 
  renamed the `.residuals` column to `.resid` and the `.stdres` to `.std.resid`
  for consistency. These changes will only affect chi-squared tests.

- `tidy.ridgelm()` now always return a `GCV` column and never returns an 
  `xm` column. (#533 by @jmuhlenkamp)

- `tidy.dist()` no longer supports the `upper` argument.

## A refactoring of `augment()` methods

The internals of `augment.*()` methods have largely been overhauled.

- If you pass a dataset to `augment()` via the `data` or `newdata` arguments,
  you are now guaranteed that the augmented dataset will have exactly the same
  number of rows as the original dataset. This differs from previous behavior
  primarily when there are missing values. Previously `augment()` would drop
  rows containing `NA`. This should no longer be the case.

- `augment.*()` methods no longer accept an `na.action` argument.

- In previous versions, several `augment.*()` methods inherited the 
  `augment.lm()` method, but required additions to the `augment.lm()` method
  itself. We have shifted away from this approach in favor of re-implementing
  many `augment.*()` methods as standalone methods making use of internal 
  helper functions. As a result, `augment.lm()` and some related methods have
  deprecated (previously unused) arguments.

- `augment()` tries to give an informative error when `data` isn't the original
  training data.
  
- The `.resid` column in the output of `augment().*` methods is now consistently 
  defined as `y - y_hat`

## New tidiers

* `anova` objects from the `car` package (#754)
* `pam` objects from the `cluster` package (#637 by @abbylsmith)
* `drm` objects from the `drc` package (#574 by @edild)
* `summary_emm` objects from the `emmeans` package (#691 by @crsh)
* `epi.2by2` objects from the `epiR` package (#711)
* `fixest` objects from the `fixest` package (#785 by @karldw)
* `regsubsets` objects from the `leaps` package (#535)
* `lm.beta` objects from the `lm.beta` package (#545 by @mattle24)
* `rma` objects from the `metafor` package (#674 by @malcolmbarrett, @softloud)
* `mfx`, `logitmfx`, `negbinmfx`, `poissonmfx`, `probitmfx`, and `betamfx` 
objects from the`mfx` package (#700 by @grantmcdermott)
* `lmrob` and `glmrob` objects from the `robustbase` package (#205, #505)
* `sarlm` objects from the `spatialreg` package (#847 by @gregmacfarlane 
and @petrhrobar)
* `speedglm` objects from the `speedglm` package (#685)
* `svyglm` objects from the `survey` package (#611)
* `systemfit` objects from the `systemfit` package (by @jaspercooper)
* We have restored a simplified version of `glance.aov()`, which now contains 
  only the following columns: `logLik`, `AIC`, `BIC, deviance`, `df.residual`, 
  `nobs` (see #212). Note that `tidy.aov()` gives more complete information about 
  degrees of freedom in an `aov` object.

## Improvements to existing tidiers

- `tidy.felm()` now has a `robust = TRUE/FALSE` option that supports robust 
  and cluster standard errors. (#781 by @kuriwaki)

- Make `.fitted` values respect `type.predict` argument of `augment.clm()`. 
  (#617)

- Return factor rather than numeric class predictions in `.fitted` of 
  `augment.polr()`. (#619) Add an option to return `p.values` in `tidy.polr()`. 
  (#833 by @LukasWallrich)

- `tidy.kmeans()` now uses the names of the input variables in the output by
  default. Set `col.names = NULL` to recover the old behavior.

- Previously, F-statistics for weak instruments were returned through 
  `glance.ivreg()`. F-statistics are now returned through 
  `tidy.ivreg(instruments = TRUE)`. Default is `tidy.ivreg(instruments = FALSE)`.
  `glance.ivreg()` still returns Wu-Hausman and Sargan test statistics.

- `glance.biglm()` now returns a `df.residual` column.

- `tidy.prcomp()` argument `matrix` gained new options `"scores"`, 
  `"loadings"`, and `"eigenvalues"`. (#557 by @GegznaV)

- `tidy_optim()` now provides the standard error if the Hessian is present. 
  (#529 by @billdenney)

- `tidy.htest()` column names are now run through `make.names()` to ensure
  syntactic correctness. (#549 by @karissawhiting)

- `tidy.lmodel2()` now returns a `p.value` column. (#570)

- `tidy.lsmobj()` gained a `conf.int` argument for consistency with other 
  tidiers.

- `tidy.polr()` now returns p-values if `p.values` is set to TRUE and the 
  model does not contain factors with more than two levels.

- `tidy.zoo()` now doesn't change column names that have spaces or other
  special characters (previously they were converted to `data.frame` friendly
  column names by `make.names`.)

- `glance.lavaan()` now uses lavaan extractor functions instead of
  subsetting the fit object manually. (#835)
  
- `glance.lm()` no longer errors when only an intercept is provided
  as an explanatory variable. (#865)

### Bug fixes

- Bug fix for `tidy.survreg()` when `robust` is set to `TRUE` in model
fitting (#842, #728)
- Bug fixes in `glance.lavaan()`: address confidence interval error
(#577) and correct reported `nobs` and `norig` (#835)
- Bug fix in muhaz tidiers to ensure output is always a `tibble` (#824)
- Several `glance.*()` methods have been refactored in order to return 
a one-row tibble even when the model matrix is rank-deficient (#823)
- Bug fix to return confidence intervals correct in `tidy.drc()` (#798)
- Added default methods for objects that subclass `glm` and `lm` in order to
error more informatively. (#749, #736, #708, #186)
- Bug fix to allow `augment.kmeans()` to work with masked data (#609)
- Bug fix to allow `augment.Mclust()` to work on univariate data (#490)
- Bug fix to allow `tidy.htest()` to supports equal variances (#608)
- Bug fix to better allow `tidy.boot()` to support confidence intervals (#581)
- Bug fix for `tidy.polr()` when passed `conf.int = TRUE` (#498)

### Other changes

- Many `glance()` methods now return a `nobs` column, which contains the 
number of data points used to fit the model! (#597 by @vincentarelbundock)

- `tidy()` no longer checks for a log or logit link when `exponentiate = TRUE`,
and we have refactored to remove extraneous `exponentiate` arguments. If you 
set `exponentiate = TRUE`, we assume you know what you are doing and that you 
want exponentiated coefficients (and confidence intervals if `conf.int = TRUE`) 
regardless of link function.

- We now use `rlang::arg_match()` when possible instead of `arg.match()` to give
  more informative errors on argument mismatches.

- The package's site has moved from https://broom.tidyverse.org/ to
  https://broom.tidymodels.org/.

- Revised several vignettes and moved them to the tidymodels.org website. The
  existing vignettes will now simply link to the revised versions.

- Many improvements to consistency and clarity of documentation.

- Various warnings resulting from changes to the tidyr API in v1.0.0 have
  been fixed. (#870)
  
- Removed dependencies on reshape2 and superseded functions in dplyr.

- All documentation now links to help files rather than topics.

## For developers and contributors

- Moved core tests to the `modeltests` package.
  
- Generally, after this release, the broom dev team will first ask that
  attempts to add tidier methods supporting a model object are first
  directed to the model-owning package. An article describing best practices 
  in doing so can be found on the {tidymodels} website at 
  https://www.tidymodels.org/learn/develop/broom/, and we will continue
  adding additional resources to that article as we develop them. In the case
  that the maintainer is uninterested in taking on the tidier methods, please
  note this in your issue or PR.

- Added a new vignette discussing how to implement new tidier methods in 
  non-broom packages.

# broom 0.5.6

- Fix failing CRAN checks to due `tibble 3.0.0` release. Removed 
`xergm` dependency.

# broom 0.5.5

- Remove tidiers for robust package and drop robust dependency (temporarily)

# broom 0.5.4

- Fixes failing CRAN checks as the joineRML package has been removed from CRAN

# broom 0.5.3

- Fixes failing CRAN checks due to new matrix classing in R 4.0.0

# broom 0.5.2

- Fixes failing CRAN checks

- Changes to accommodate ergm 3.10 release. `tidy.ergm()` no longer
  has a `quick` argument. The old default of `quick = FALSE` is
  now the only option.

# broom 0.5.1

- `tidy()`, `glance()` and `augment()` are now re-exported from the
  [generics](https://github.com/r-lib/generics) package.

# broom 0.5.0

Tidiers now return `tibble::tibble()`s. This release also includes several new
tidiers, new vignettes and a large number of bug fixes. We've also begun to more
rigorously define tidier specifications: we've laid part of the groundwork for
stricter and more consistent tidying, but the new tidier specifications are not
yet complete. These will appear in the next release.

Additionally, users should note that we are in the process of migrating tidying
methods for mixed models and Bayesian models to `broom.mixed`. `broom.mixed` is
not on CRAN yet, but all mixed model and Bayesian tidiers will be deprecated
once `broom.mixed` is on CRAN. No further development of mixed model tidiers
will take place in `broom`.

## Breaking changes

Almost all tidiers should now return `tibble`s rather than `data.frame`s.
Deprecated tidying methods, Bayesian and mixed model tidiers still return
`data.frame`s.

**Users** are mostly to experience issues when using `augment` in situations
where tibbles are stricter than data frames. For example, specifying model
covariates as a matrix object will now error:

```r
library(broom)
library(quantreg)

fit <- rq(stack.loss ~ stack.x, tau = .5)
broom::augment(fit)
#> Error: Column `stack.x` must be a 1d atomic vector or a list
```

This is because the default `data` argument `data = model.frame(fit)` cannot be
coerced to `tibble`.

Another consequence of this is that `augment.survreg` and `augment.coxph` from
the `survival` package now require that the user explicitly passes data to
either the `data` or `newdata` arguments.

These restrictions will be relaxed in an upcoming release of `broom` pending
support for matrix-columns in tibbles.

**Developers** are likely to experience issues:

- subsetting tibbles with `[`, which returns a tibble rather than a vector.

- setting rownames on tibbles, which is deprecated.

- using matrix and vector tidiers, now deprecated.

- handling the additional tibble classes `tbl_df` and `tbl` beyond the
  `data.frame` class

- linking to defunct documentation files -- broom recently moved all tidiers to
  a `roxygen2` template based documentation system.

## New vignettes

This version of `broom` includes several new vignettes:

- `vignette("available-methods", package = "broom")` contains a table detailing
  which tidying methods are available

- `vignette("adding-tidiers", package = "broom")` is an *in-progress* guide for
  contributors on how to add new tidiers to broom

- `vignette("glossary", package = "broom")` contains tables describing
  acceptable argument names and column names for the *in-progress* new
  specification.

Several old vignettes have also been updated:

- `vignette("bootstrapping", package = "broom")` now relies on the `rsample`
  package and a `tidyr::nest`-`purrr::map`-`tidyr::unnest` workflow. This is now
  the recommended workflow for working with multiple models, as opposed to the
  old `dplyr::rowwise`-`dplyr::do` based workflow.

## Deprecations

- Matrix and vector tidiers have been deprecated in favor of `tibble::as_tibble`
  and `tibble::enframe`

- Dataframe tidiers and rowwise dataframe tidiers have been deprecated

- `bootstrap()` has been deprecated in favor of the
  [`rsample`](https://tidymodels.github.io/rsample/)

- `inflate` has been removed from `broom`

## Other changes

- The `alpha` argument has been removed from `quantreg` tidy methods

- The `separate.levels` argument has been removed from `tidy.TukeyHSD`. To
  obtain the effect of `separate.levels = TRUE`, users may `tidyr::separate`
  after tidying. This is consistent with the `multcomp` tidier behavior.

- The `fe.error` argument was removed from `tidy.felm`. When fixed effects are
  tidier, their standard errors are now always included.

- The `diag` argument in `tidy.dist` has been renamed `diagonal`

- Advice to help beginners make PRs (#397 by @karldw)

- `glance` support for `arima` objects fit with `method = "CSS"` (#396 by @josue-rodriguez)

- A bug fix to re-enable tidying `glmnet` objects with `family = multinomial`
  (#395 by @erleholgersen)

- A bug fix to allow tidying `quantreg` intercept only models (#378 by @erleholgersen)

- A bug fix for `aovlist` objects (#377 by @mvevans89)

- Support for `glmnetUtils` objects (#352 by @Hong-Revo)

- A bug fix to allow `tidy_emmeans` to handle column names with dashes (#351 by @bmannakee)

- `augment.felm` no longer returns `.fe_` and `.comp` columns

- Support saved formulas in `augment.felm` (#347 by @ShreyasSingh)

- `confint_tidy` now drops rows of all `NA` (#345 by @atyre2)

- A new tidier for `caret::confusionMatrix` objects (#344 by @mkuehn10)

- Tidiers for `Kendall::Kendall` objects (#343 by @cimentadaj)

- A new tidying method for `car::durbinWatsonTest` objects (#341 by @mkuehn10)

- `glance` throws an informative error for `quantreg:rq` models fit with
  multiple `tau` values (#338 by @bfgray3)

- `tidy.glmnet` gains the ability to retain zero-valued coefficients with a
  `return_zeros` argument that defaults to `FALSE` (#337 by @bfgray3)

- `tidy.manova` now retains a `Residuals` row (#334 by @jarvisc1)

- Tidiers for `ordinal::clm`, `ordinal::clmm`, `survey::svyolr` and `MASS::polr`
  ordinal model objects (#332 by @larmarange)

- Support for `anova` objects from `car::Anova` (#325 by @mariusbarth)

- Tidiers for `tseries::garch` models (#323 by @wilsonfreitas)

- Removed dependency on `psych` package (#313 by @nutterb)

- Improved error messages (#303 by @michaelweylandt)

- Compatibility with new `rstanarm` and `loo` packages (#298 by @jgabry)

- Support for tidying lists return by `irlba::irlba`

- A truly huge increase in unit tests (#267 by @dchiu911)

- Bug fix for `tidy.prcomp` when missing labels (#265 by @corybrunson)

- Added a `pkgdown` site at https://broom.tidyverse.org/ (#260 by @jayhesselberth)

- Added tidiers for `AER::ivreg` models (#247 by @hughjonesd)

- Added tidiers for the `lavaan` package (#233 by @puterleat)

- Added `conf.int` argument to `tidy.coxph` (#220 by @larmarange)

- Added `augment` method for chi-squared tests (#138 by @larmarange)

- changed default se.type for `tidy.rq` to match that of
  `quantreg::summary.rq()` (#404 by @ethchr)

- Added argument `quick` for `tidy.plm` and `tidy.felm` (#502 and #509 by @MatthieuStigler)

- Many small improvements throughout

## Contributors

Many many thanks to all the following for their thoughtful comments on design,
bug reports and PRs! The community of broom contributors has been kind,
supportive and insightful and I look forward to working you all again!

[@atyre2](https://github.com/atyre2),
[@batpigandme](https://github.com/batpigandme),
[@bfgray3](https://github.com/bfgray3),
[@bmannakee](https://github.com/bmannakee),
[@briatte](https://github.com/briatte),
[@cawoodjm](https://github.com/cawoodjm),
[@cimentadaj](https://github.com/cimentadaj),
[@dan87134](https://github.com/dan87134), [@dgrtwo](https://github.com/dgrtwo),
[@dmenne](https://github.com/dmenne), [@ekatko1](https://github.com/ekatko1),
[@ellessenne](https://github.com/ellessenne),
[@erleholgersen](https://github.com/erleholgersen),
[@ethchr](https://github.com/ethchr),
[@Hong-Revo](https://github.com/Hong-Revo),
[@huftis](https://github.com/huftis),
[@IndrajeetPatil](https://github.com/IndrajeetPatil),
[@jacob-long](https://github.com/jacob-long),
[@jarvisc1](https://github.com/jarvisc1),
[@jenzopr](https://github.com/jenzopr), [@jgabry](https://github.com/jgabry),
[@jimhester](https://github.com/jimhester),
[@josue-rodriguez](https://github.com/josue-rodriguez),
[@karldw](https://github.com/karldw), [@kfeilich](https://github.com/kfeilich),
[@larmarange](https://github.com/larmarange),
[@lboller](https://github.com/lboller),
[@mariusbarth](https://github.com/mariusbarth),
[@michaelweylandt](https://github.com/michaelweylandt),
[@mine-cetinkaya-rundel](https://github.com/mine-cetinkaya-rundel),
[@mkuehn10](https://github.com/mkuehn10),
[@mvevans89](https://github.com/mvevans89),
[@nutterb](https://github.com/nutterb),
[@ShreyasSingh](https://github.com/ShreyasSingh),
[@stephlocke](https://github.com/stephlocke),
[@strengejacke](https://github.com/strengejacke),
[@topepo](https://github.com/topepo),
[@willbowditch](https://github.com/willbowditch),
[@WillemSleegers](https://github.com/WillemSleegers),
[@wilsonfreitas](https://github.com/wilsonfreitas), and
[@MatthieuStigler](https://github.com/MatthieuStigler)

# broom 0.4.4

* Fixed gam tidiers to work with "Gam" objects, due to an update in gam 1.15.
  This fixes failing CRAN tests

* Improved test coverage (thanks to #267 from Derek Chiu)

# broom 0.4.3

* Changed the deprecated `dplyr::failwith` to `purrr::possibly`

* `augment` and `glance` on NULLs now return an empty data frame

* Deprecated the `inflate()` function in favor of `tidyr::crossing`

* Fixed confidence intervals in the gmm tidier (thanks to #242 from David
  Hugh-Jones)

* Fixed a bug in bootstrap tidiers (thanks to #167 from Jeremy Biesanz)

* Fixed tidy.lm with `quick = TRUE` to return terms as character rather than
  factor (thanks to #191 from Matteo Sostero)

* Added tidiers for `ivreg` objects from the AER package (thanks to #245 from
  David Hugh-Jones)

* Added tidiers for `survdiff` objects from the survival package (thanks to #147
  from Michał Bojanowski)

* Added tidiers for `emmeans` from the emmeans package (thanks to #252 from
  Matthew Kay)

* Added tidiers for `speedlm` and `speedglm` from the speedglm package (#685,
  thanks to #248 from David Hugh-Jones)

* Added tidiers for `muhaz` objects from the muhaz package (thanks to #251 from
  Andreas Bender)

* Added tidiers for `decompose` and `stl` objects from stats (thanks to #165
  from Aaron Jacobs)

# broom 0.4.2

* Added tidiers for `lsmobj` and `ref.grid` objects from the lsmeans package

* Added tidiers for `betareg` objects from the betareg package

* Added tidiers for `lmRob` and `glmRob` objects from the robust package

* Added tidiers for `brms` objects from the brms package (thanks to #149 from
  Paul Buerkner)

* Fixed tidiers for orcutt 2.0

* Changed `tidy.glmnet` to filter out rows where estimate == 0.

* Updates to `rstanarm` tidiers (thanks to #177 from Jonah Gabry)

* Fixed issue with survival package 2.40-1 (thanks to #180 from Marcus Walz)

# broom 0.4.1

* Added AppVeyor, codecov.io, and code of conduct

* Changed name of "NA's" column in summaryDefault output to "na"

* Fixed `tidy.TukeyHSD` to include `term` column. Also added `separate.levels`
  argument, with option to separate `comparison` into `level1` and `level2`

* Fixed `tidy.manova` to use correct column name for test (previously, always
  `pillai`)

* Added `kde_tidiers` to tidy kernel density estimates

* Added `orcutt_tidiers` to tidy the results of `cochrane.orcutt` orcutt
  package

* Added `tidy.dist` to tidy the distance matrix output of `dist` from the stats
  package

* Added `tidy` and `glance` for `lmodel2` objects from the lmodel2 package

* Added tidiers for `poLCA` objects from the poLCA package

* Added tidiers for sparse matrices from the Matrix package

* Added tidiers for `prcomp` objects

* Added tidiers for `Mclust` objects from the Mclust package

* Added tidiers for `acf` objects

* Fixed to be compatible with dplyr 0.5, which is being submitted to CRAN

# broom 0.4.0

* Added tidiers for geeglm, nlrq, roc, boot, bgterm, kappa, binWidth, binDesign,
  rcorr, stanfit, rjags, gamlss, and mle2 objects.

* Added `tidy` methods for lists, including u, d, v lists from `svd`, and x, y,
  z lists used by `image` and `persp`

* Added `quick` argument to `tidy.lm`, `tidy.nls`, and `tidy.biglm`, to create a
  smaller and faster version of the output.

* Changed `rowwise_df_tidiers` to allow the original data to be saved as a list
  column, then provided as a column name to `augment`. This required removing
  `data` from the `augment` S3 signature. Also added `tests-rowwise.R`

* Fixed various issues in ANOVA output

* Fixed various issues in lme4 output

* Fixed issues in tests caused by dev version of ggplot2

# broom 0.3.7

* Added tidiers for "plm" (panel linear model) objects from the plm package.

* Added `tidy.coeftest` for coeftest objects from the lmtest package.

* Set up `tidy.lm` to work with "mlm" (multiple linear model) objects (those
  with multiple response columns).

* Added `tidy` and `glance` for "biglm" and "bigglm" objects from the biglm
  package.

* Fixed bug in `tidy.coxph` when one-row matrices are returned

* Added `tidy.power.htest`

* Added `tidy` and `glance` for `summaryDefault` objects

* Added tidiers for "lme" (linear mixed effects models) from the nlme package

* Added `tidy` and `glance` for `multinom` objects from the nnet package.

# broom 0.3.6

* Fixed bug in `tidy.pairwise.htest`, which now can handle cases where the
  grouping variable is numeric.

* Added `tidy.aovlist` method. This added `stringr` package to IMPORTS to trim
  whitespace from the beginning and end of the `term` and `stratum` columns.
  This also required adjusting `tidy.aov` so that it could handle strata that
  are missing p-values.

* Set up `glance.lm` to work with `aov` objects along with `lm` objects.

* Added `tidy` and `glance` for matrix objects, with `tidy.matrix` converting a
  matrix to a data frame with rownames included, and `glance.matrix` returning
  the same result as `glance.data.frame`.

* Changed DESCRIPTION Authors@R to new format

# broom 0.3.5

* Fixed small bug in `felm` where the `.fitted` and `.resid` columns were
  matrices rather than vectors.

* Added tidiers for `rlm` (robust linear model) and `gam` (generalized additive
  model) objects, including adjustments to "lm" tidiers in order to handle them.
  See `?rlm_tidiers` and `?gam_tidiers` for more.

* Removed rownames from `tidy.cv.glmnet` output

# broom 0.3.4

* The behavior of `augment`, particularly with regard to missing data and the
  `na.exclude` argument, has through the use of the `augment_columns` function
  been made consistent across the following models:

    * `lm`

    * `glm`

    * `nls`

    * `merMod` (`lme4`)

    * `survreg` (`survival`)

    * `coxph` (`survival`)

Unit tests in `tests/testthat/test-augment.R` were added to ensure consistency
across these models.

* `tidy`, `augment` and `glance` methods were added for `rowwise_df` objects,
  and are set up to apply across their rows. This allows for simple patterns
  such as:

regressions <- mtcars %>% group_by(cyl) %>% do(mod = lm(mpg ~ wt, .))
regressions %>% tidy(mod) regressions %>% augment(mod)

See `?rowwise_df_tidiers` for more.

* Added `tidy` and `glance` methods for `Arima` objects, and `tidy` for
  `pairwise.htest` objects.

* Fixes for CRAN: change package description to title case, removed NOTES,
  mostly by adding `globals.R` to declare global variables.

* This is the original version published on CRAN.

# broom 0.3

* Tidiers have been added for S3 objects from the following packages:

    * `lme4`

    * `glmnet`

    * `survival`

    * `zoo`

    * `felm`

    * `MASS` (`ridgelm` objects)

* `tidy` and `glance` methods for data.frames have also been added, and
  `augment.data.frame` produces an error (rather than returning the same
  data.frame).

* `stderror` has been changed to `std.error` (affects many functions) to be
  consistent with broom's naming conventions for columns.

* A function `bootstrap` has been added based on [this
  example](https://github.com/hadley/dplyr/issues/269), to perform the common
  use case of bootstrapping models.

# broom 0.2

* Added "augment" S3 generic and various implementations. "augment" does
  something different from tidy: it adds columns to the original dataset,
  including predictions, residuals, or cluster assignments. This was originally
  described as "fortify" in ggplot2.

* Added "glance" S3 generic and various implementations. "glance" produces a
  *one-row* data frame summary, which is necessary for tidy outputs with values
  like R^2 or F-statistics.

* Re-wrote intro broom vignette/README to introduce all three methods.

* Wrote a new kmeans vignette.

* Added tidying methods for multcomp, sp, and map objects (from
  fortify-multcomp, fortify-sp, and fortify-map from ggplot2).

* Because this integrates substantial amounts of ggplot2 code (with permission),
  added Hadley Wickham as an author in DESCRIPTION.
# rex 1.2.0

## Rex Version 1.1.2.9000 ##

* `%>%` is no longer imported and then re-exported from rex

## Rex Version 1.1.2 ##

* Updating tests to work with testthat version 1.0.2.9000.

* Add `m`, `matches` and `s`, `substitutes` aliases for `re_matches` and
  `re_substitutes`.

## Rex Version 1.1.1 ##

* Vignette tweak for ggplot2 2.0.0
* Only print startup message some of the time.
* Move register for magrittr pipe to `.onLoad()`

## Rex Version 1.0.1 ##

* Work around ggplot2 bug with windows fonts

## Rex Version 1.0.0 ##

* Include the capture results even if `locations = TRUE`
* Add `:` operator for character ranges
* Remove duplicate regex functino
* Don't re-compute missing names
* Reduce code duplication
* Add examples for lookarounds

## Rex Version 0.2.0 ##

### Enhancements

* Add a newline shortcut
* add register_shortcuts to allow use of rex in external packages without
  spurious NOTES.

## Rex Version 0.1.1 ##

### Enhancements

* re_matches now has a "locations" argument, which returns the start and end
  locations of the match or capture(s).
* Simplify regular expressions generated from 'some_of' functions.

### Bug fixes

* backslashes ("\\") are now properly escaped.

### Misc

* Improve Rex mode documentation (#21 @Ironholds)
* Improve Log parsing Vignette copy and Title (#18, #20 @Ironholds)
* Add links to GitHub and issues page in DESCRIPTION

## Rex Version 0.1.0 ##

Initial release
# tidyverse 1.3.0

* The tidyverse now has an associated paper at the 
  [Journal of Open Source Software](http://joss.theoj.org/) that you can
  use to cite the tidyverse if you use it in a paper - see 
  `citation("tidyverse")` for details.

* Eliminate repeats in the package list when loading an odd number of 
  packages (#94, #100, @dchiu911)

* Packages attached from same library they were initially loaded from 
  (#171, @gabrocsardi)

* If conflicted package is loaded, omit display of conflicts. 
  This includes fix to `tidyverse_conflicts()` to avoid accidentally
  triggering conflited shims (#136).

* `tidyverse_deps()` now succeeds even if a dependency of a depedency 
  is missing (#152, @PoGibas). It no longer includes dependencies that
  are needed by the tidyverse package but are not strictly part of the
  tidyverse (i.e. cli, crayon, and rstudioapi).

* `tidyverse_deps()` and `tidyverse_update()` gain a new `repos` argument
  that gets passed to the base function `available.packages()` (@zkamvar, #82)

* `tidyverse_packages()` corrently returns packages names (#93, #106, @coatless)

* `tidyverse_sitrep()` gives you a situtation report on your install of 
  the tidyverse (#203)

# tidyverse 1.2.1

* Require modern versions of all packages (#85)

* Work with RStudio 1.0 and earlier (#88).

# tidyverse 1.2.0

## Changes to tidyverse membership

* stringr and forcats have been added to the core tidyverse, so they are
  attached by `library(tidyverse)`.

* reprex joins the tidyverse to make it easier to create reproducible
  examples (#47)

## Other improvements

* On attach, tidyverse now makes better use of the horizontal space, 
  printing packages and versions in two columns (#59). It only prints
  packages that it attaches, not packages that you've already attached.
  Development versions are highlighted in red.
  
    You can now suppress this startup message by setting 
    `options(tidyverse.quiet = TRUE)`

* `tidyverse_conflicts()` now prints all conflicts that involve at least
  one tidyverse package; Previously it only omitted any intra-tidyverse
  conflicts (#26). I've also tweaked the display of conflicts to hopefully 
  make it more clear which function is the "winner".

* `tidyverse_update()` now just gives you the code you need to update the 
  packges, since in general it's not possible to update packages that are
  already loaded.

* feather is now _actually_ in suggests.

# tidyverse 1.1.1

* Moved feather from Imports to Suggests - feather is part of the tidyverse
  but it's installation requirements (C++11 + little-endian) make it painful
  in many scenarios (#36).

# tidyverse 1.1.0

* Added a `NEWS.md` file to track changes to the package.

* Membership changes:
  
  * Removed DBI (since very different API, #16)
  * Added feather (#15)

* `tidyverse_deps()` and `tidyverse_packages()` are now exported so you can
  more easily see the make up of the tidyverse, and what package versions
  you have (#18, #23)

* `suppressPackageStartupMessages()` now suppresses all messages during
   loading (#19). `suppressPackageStartupMessages()` is called automatically
   for all tidyverse packages (#27).
# insight 0.9.0

## New supported model classes

* `BGGM` (*BGGM*), `metaplus` (*metaplus*), `glht` (*multcomp*), `glmm` (*glmm*), improved support for `manova` (*stats*)

## New functions

* Value formatting functions `format_bf()`, `format_pd()`, `format_p()`, `format_rope()` and `format_number()` were moved from package *parameters* to *insight*.

## Changes to functions

* `get_variance()` now also returns the corrlation among random slopes.
* `get_variance()` now also (partially) supports `brmsfit` models.
* `get_parameters()` for models that return (posterior or simulated) samples of model parameters gains a `summary`-argument, which - if `TRUE` - returns a point-estimate (mean of samples) instead of the full samples.
* `format_p()` returns `"> .999"` for p-values equal to or greater than 0.999.

## Bug fixes

* Fixed issue in `find_formula()` that did not properly work for models with random effects in formula (in *lme4* notation), when random effects were in between fixed effects parts.
* `get_variance()` did not return variance components for random effects for null-models with random slopes.
* Fixed issue with `get_variance()` for `lme`-models with categorical random slope.
* Fixed issue that occured since R 4.0.0 in `find_weights()` when function call had no `weights`-argument.
* Fixed issue in `get_data()` for models with `cbind()`-response variables and matrix-like variables in the model frame (e.g. when using `poly()`).
* Fixed issues with `PROreg::BBmm()`, due to changes in latest package update.

# insight 0.8.5

## New supported model classes

* `robmixglm` (*robmixglm*), `betamfx`, `logitmfx`, `poissonmfx`, `probitmfx`, `negbinmfx`, `betaor`, `logitor`, `poissonirr`, `negbinirr` (*mfx*), partial support for *emmGrid*,  *stanfit* and *bayesQR*.

## Changes to functions

* `get_varcov.glmmTMB()` now also returns the variance-covariance matrix for the dispersion model.
* `model_info()` returns `$is_dispersion = TRUE` for *glmmTMB* objects with dispersion model.
* `clean_names()` now also removes mathematical operations (like `100 * log(x)`, which will return `"x"`).
* `format_ci()` gains a `missing` argument, as option how to print missing values.
* `format_value()` now uses `NA_character_` as missing if `missing = NA`.
* `format_value()` also converts small numbers with many decimals into scientific notation.

# insight 0.8.4

## General

* *HRQoL* was removed from suggested packages, as it was removed from CRAN.
* Better support for dispersion models in *glmmTMB*.

## Changes to functions

* `null_model()` now also works for non-mixed models.
* `get_variance()` now also computes variance components for models (from mixed models packages) without random effects.
* Improved support for `afex_aov` and `aovlist` (i.e. Anova with error term).

## Bug fixes

* Fixed some issues with deparsings `NULL` strings under R 4.0.0.
* Fixed accuracy in `get_variance()` for models from Gamma family.
* Fixed edge case in `clean_names()`.
* Fixed issues with `find_formula.lme()` under R 4.0.0.
* Fixed issues with examples from `clean_names()` under R-devel.

# insight 0.8.3

## General

* The function to calculate null-models for mixed effects models is now exported (`null_model()`.)

## New supported model classes

* `arima` (*stats*), `averaging` (*MuMIn*)

## Changes to functions

* Improve family detection in `model_info()`, `link_inverse()` and `link_function()` for *MCMCglmm*.
* Minor revisions to meet changes in *mlogit* package.
* Improve support for *bayesx* and *BBmm* models.

## Bug fixes

* Fixed issue in `find_parameters()` and `clean_parameters()` for *brmsfit* models with specific variable name patterns.
* Fixed issue in `format_ci()` when confidence interval only contained `NA`s and `width` was set to `"auto"`.
* Fixed issue in `find_formula()` for mixed models when formula contained parentheses in the non-random parts, around a certain set of predictors.
* Fixed issue in `get_priors.BFBayesFactor()` for `BFMetat` class.
* Fixed issue in `clean_parameters.BFBayesFactor()` when model contained interaction terms and these were assigned to the "extra" component.

# insight 0.8.2

## Breaking changes

* `model_info()` now only returns `TRUE` for `$is_ordinal`, when model is an ordinal or cumulative link model. In past versions, `$is_ordinal` was also `TRUE` for multinomial models.

## New supported model classes

* `bife` (*bife*), `bcplm` and `zcpglm` (*cplm*)
 
## General

* Improved support for `clogit`-models.

## Bug fixes

* Fixed issue in `find_weights()` for `merMod` models.
* Fixed issue in `get_data()` for models with weights, when weights also contained missing data.
* Fixed issue in `get_data()` for mixed models with complex offset-terms.
* Fixed issue in `get_statistic()` for *zeroinfl*  models with theta-coefficients.
* Fixed issue in `get_statistic()` for *lmerModLmerTest*  models with.
* Fixed issue in `find_parameters()` for *brmsfit*  models for rare situations where a specific pattern of variables names, when used as random effects, did not properly separate fixed from random effects in the return value.
* Fixed issue related to CRAN checks.

# insight 0.8.1

## New supported model classes

* `cglm` (*cglm*), `DirichletRegModel` (*DirichletReg*).

## General

* Improved efficiency of `find_parameters()` and `get_parameters()` for mixed models with large samples and many random effects, and only fixed effects where requested.

## Changes to functions

* `model_info()` now returns `$is_multinomial` for multinomial (but not ordinal or cumulative) link models.
* `format_value()` gets an `as_percent` argument to format values as percentages.

## Bug fixes

* Fixed issue in `get_data()` for *clmm2*-models.
* Fixed issue in `get_data()` for models that used the `lspline()`-function.
* Fixed issue in `get_statistic()` for *multinom* models.
* Fixed issue in `get_priors()` for *stanreg*  models with flat intercept priors.
* Fixed tests that failed due to latest **fixest** update.

# insight 0.8.0

## New supported model classes

* `brglm` (*brglm*), `cgam`, `cgamm` (*cgam*), `cpglm`, `cpglmm` (*cplm*), `feglm` (*apaca*), `glmmadmb` (*glmmADMB*), `glmx` (*glmx*), partial support for `mcmc` (*coda*), `mixor` (*mixor*), `MANOVA`, `RM` (*MANOVA.RM*).

## General

* Better handling of `clm2`, `clmm2` and `rqss` models.

## New functions

* `format_ci()` (re-implemented and slightly enhanced from _parameters_), to format confidence/credible intervals.

## Changes to functions

* `find_parameters()` now also works for `BFBayesFactor` objects.
* Suppress non-informative warning in `get_data()` for model data with weights.
* `format_value()` automatically uses scientific notation for *very* large numbers (> 1e+5). Furthermore, the check for integer values was made more robust, to avoid warnings when checking *very* large numbers for integer type.
* Improved `find_parameters()`, `get_parameters()` and `clean_parameters()` for `BFBayesFactor`-objects.
* `get_priors()` now works for `stanmvreg` objects.
* Other minor improvements.

## Bug fixes

* Better detect Tweedie-models in `model_info()`.
* Fixed issue in `find_random_slopes()` for *panelr*-models with multiple random-effect parts.
* Fixed issues with `zerotrunc` models.
* Fixed issues with `brmsfit` models with correlated random effects.
* Fixed issue with edge-cases in `clean_names()`.
* Fixed issue with breaking changes with latest *brms*-update.
* Further minor bug fixes.

# insight 0.7.1

## New supported model classes

* `complmrob` (*complmrob*), `fixest` (*fixest*), `mclogit` and `mmclogit` (*mclogit*).

## Bug fixes

* Fixed bug in `find_formula()` for mixed models, when random effects are written before any fixed effects terms (like `social ~ (1|school) + open + extro`).
* Fixed bug in `model_info()` for *VGAM* models, where logit-link was not always correctly identified.
* Fixed issue in `get_priors()` for *brmsfit* models, where parameters of conditional and zero-inflated model components had identical names. This caused errors in `bayestestR::simulate_prior()`.
* Fixed CRAN check issue.

# insight 0.7.0

## Breaking changes

* In order to unify column names across easystats-packages, `get_parameters()` and `get_priors()` now return column names according to our naming conventions (i.e. capitalized column names).
* `model_info()` returned both `$is_zeroinf` and `$is_zero_inflated` for zero-inflated models. Now `$is_zeroinf` is softly deprecated, so `model_info()` will return `$is_zero_inflated` only in future updates.

## New supported model classes

* `aareg` (*survival*), `brmultinom` and `bracl` (*brglm2*), and `wbgee` (*panelr*). Furthermore, for different model-types from *panelr* models (within-between, between, etc.) are now also supported.
* Preliminary support for `rma` models (*metafor*).

## Changes to functions

* `get_statistic()` supports `multinom` models (*nnet*).
* `link_inverse()` gets a `what`-argument, to return the link-inverse function for specific distribution parameters from **gamls** models.

## Bug fixes

* Fixed edge case for `find_weights()`.
* Fixed bug in `get_statistic()` for *glmmTMB* models that won't return any data.

# insight 0.6.0

## New supported model classes

* `bayesx` (*R2BayesX*), `bamlss` (*bamlss*) and `flexsurvreg` (*flexsurv*). Note that support for these models is still somewhat experimental.
* Support for *lavaan* and *blavaan* was added, but only applies to some of the functions: `get_data()`, `get_parameters()`, `find_parameters()`, `clean_parameters()`, `find_algorithm()` and `get_priors()` (the two latter only for *blavaan*).

## New functions

* `get_statistic()` to return the test statistic of model estimates.
* `get_varcov()` to return the variance-covariance matrix for models.
* `supported_models()` to print a list of supported models.

## Changes to functions

* `model_info()` now returns the element `is_survival` for survival models.
* `model_info()` now returns the element `is_truncated` for truncated regression, or *brmsfit* models with `trunc()` as additional response part.
* `model_info()` now recognizes beta and beta inflated families from package *gamlss*.
* Better support for nonlinear quantile regression (`quantreg::nlrq()`).
* Better support for nonlinear mixed models (`lme4::nlmer()`). Note that model-specification requires the random term to be written in parentheses, i.e. `(slope | group)`.

## Bug fixes

* Fixed issues in `get_data()`, `find_parameters()` and `get_parameters()` for *gamlss* models.
* Fixed issue in `get_data()` for *plm* models, where the `index`-argument was used in the `plm()`-function call.
* Fixed issue in `get_data()`, `find_predictors()` and `find_variables()` for *brmsfit*  multi-membership-models.
* `is_model()` did not recognize objects of class `anova` and `manova`.
* `model_info()` now correctly recognizes censored regression models from *brmsfit*.
* Fixed issues in `find_parameters()` and `get_parameters()` with *multinom* models.
* Fixed issues in `clean_names()` for cases where variable transformations where made in specific patterns, like `log(test/10)`.

# insight 0.5.0

## Breaking Changes

* The previous `is_model()` function has been renamed to `is_model_supported()` since it was unclear if the function checked the entered object was a model or a supported model in *insight*. The new `is_model()` function checks if the entered object is a model object, while `is_model_supported()` checks if a supported model object.

## New functions

* `find_statistic()` to return the test statistic of a regression model.
* `format_value()` and `format_table()` as utility-functions to format (model) output, especially for tabular output.
* `color_if()` as utility-function to add color formatting to values, depending on certain conditions.

## General

* Make extraction of model family information more stable for gam-objects.

## Changes to functions

* `find_parameters()` and `get_parameters()` now also support objects of class `sim` and `sim.merMod` (from `arm::sim()`).
* `get_variance()` now also supports models of class *clmm*.
* `find_predictors()` and `find_variables()` now include the Euclidean distance matrix for spatial models from *glmmTMB* (returned as random effects element, or more precise, as random slope).

## Bug fixes

* `find_formula()` now extracts group factors of random effects for *gamlss* models.
* `find_parameters()` and `get_parameters()` no longer show `NA` coefficients from group factors of random effects for *gamlss* models.
* `find_parameters()` and `get_parameters()` did not work for multivariate response models of class *brmsfit* when argument `parameters` was specified.
* `get_data()` dropped value and variable label attributes, when model frame contained matrix variables (like splines).
* `get_priors()` swapped column names `location` and `scale` for *brmsfit* -objects.
* `get_parameters()` did not work for *glmmTMB* models without zero-inflation component.
* `find_predictors()` did not remove parentheses from terms in multiple nested random effects.
* Better support for *gam* models (package *mgcv*) with `ziplss` or `mvn` families.

# insight 0.4.1

## Changes to functions

* `get_variance()` now supports models with Gamma-family.
* `get_weights()` and `find_weights()` now work for *brms*-models.

## Bug fixes

* Fix CRAN-check issues due to recent update from the *panelr*-package.

# lazyeval 0.2.2

* Fix protection issues from rchk reports.


# lazyeval 0.2.1

This is a maintenance release. The lazyeval package is no longer
developed as the tidyverse is switching to tidy evaluation.

* Use new registration system.

* Switch from `SET_NAMED()` to `MARK_NOT_MUTABLE()` in prevision of an
  API change in R core

* No longer check the type of the sides of the formula.


# lazyeval 0.2.0

## Formula-based lazy evaluation

Lazyeval has a  new system for lazy-eval based on formulas, described in depth in the new `lazyeval` vignette. This system is still a little experimental - it hasn't seen much use outside of the vignette, so it certainly may change a little in the future. However, long-term goal is to use these tools across all of my packages (ggplot2, tidyr, dplyr, etc), and I am fairly confident that this is a robust system that won't need major changes.

There are three key components:

* `f_eval()` evaluates a formula in the environment where it was defined. 
  If supplied, values are first looked for in an optional `data` argument. 
  Pronouns `.data` and `.env` can be used to resolve ambiguity in this case.
  (#43). Longer forms `f_eval_rhs()` and `f_eval_lhs()` emphasise the side
  of the formula that you want to evaluate (#64).
  
* `f_interp()` provides a full quasiquoting system using `uq()` for unquote
  and `uqs()` for unquote-splice (#36).

* `f_capture()` and `dots_capture()` make it easy to turn promises
  and `...` into explicit formulas. These should be used sparingly, as
  generally lazy-eval is preferred to non-standard eval.
  
* For functions that work with `...`, `f_list()` and `as_f_list()` make it
  possible to use the evaluated LHS of a formula to name the elements of a 
  list (#59).

The core components are accompanied by a number of helper functions:

* Identify a formula with `is_formula()`.

* Create a formula from a quoted call and an environment with `f_new()`.

* "Unwrap" a formula removing one level from the stack of parent environments 
  with `f_unwrap()`.
  
* Get or set either side of a formula with `f_rhs()` or `f_lhs()`, and
  the environment with `f_env()`.
  
* Convert to text/label with `f_text()` and `f_label()`.

I've also added `expr_find()`, `expr_text()` and `expr_label()` explicitly to find the expression associated with a function argument, and label it for output (#58). This is one of the primary uses cases for NSE. `expr_env()` is a similar helper that returns the environment associated with a promise (#67).

## Fixes to existing functions

* `lazy_dots()` gains `.ignore_empty` argument to drop extra arguments (#32).

* `interp.formula()` only accepts single-sided formulas (#37).

* `interp()` accepts an environment in `.values` (#35).

* `interp.character()` always produes a single string, regardless of
  input length (#27).

* Fixed an infinite loop in `lazy_dots(.follow_symbols = TRUE)` (#22, #24)

* `lazy()` now fails with an informative error when it is applied on
  an object that has already been evaluated (#23, @lionel-).

* `lazy()` no longer follows the expressions of lazily loaded objects
  (#18, @lionel-).

# lazyeval 0.1.10

* `as.lazy_dots()` gains a method for NULL, returning a zero-length
  list.

* `auto_names()` no longer truncates symbols (#19, #20)
# cowplot 1.0.0

## Breaking changes

In many ways, cowplot now behaves more appropriately and plays nicer with
the R environment and other R packages. However, this means that several
breaking changes were introduced:

- The default ggplot2 theme is no longer changed upon loading. You can add
  `theme_set(theme_cowplot())` to your code to restore the old behavior.
- The package ggplot2 needs to be loaded separately from cowplot, it is not
  automatically attached.
- All themes now use parameters `font_size` and `font_family`. Previously,
  `theme_nothing()` and `theme_map()` used `base_size` and `base_family`.
- The function `cowplot::ggsave()` was renamed to `cowplot::ggsave2()`,
  so that the ggplot2 version of ggsave() is no longer masked by the
  cowplot version.

Other breaking changes:
- The defaults for `save_plot()` were changed somewhat. This may require
  adjustment if you have code depending on it.

## New features

- New functions `rectangle_key_glyph()` and `circle_key_glyph()`
  make it possible to generate customized legend glyphs.
- Improved alignment of plots. Plots can now be aligned in a greedy
  manner, which improves the appearance of aligned plots with axis
  labels of very different sizes.
- The new functions `stamp()`, `stamp_good()`, `stamp_bad()`,
  `stamp_ugly()`, `stamp_wrong()` allow labeling of plots as good,
  bad, ugly, wrong, etc.
- Added a function `as_grob()` that can convert base plots, lattice plots,
  and ggplot2 plots into grobs that can then be drawn using standard
  grid approaches.
- Much improved handling of base R plots, thanks to improvements in
  grid and gridGraphics. Requires R >= 3.6.0 and gridGraphics >= 0.4.
- Added a function `set_null_device()` to customize the null graphics
  device that is used in `ggdraw()`, `plot_grid()`, etc. There is no
  one null device that always works, so customization is needed.
- Added several new themes, including `theme_minimal_grid()`,
  `theme_minimal_hgrid()`, and `theme_minimal_vgrid()`. Also,
  `theme_cowplot()` is now also available as `theme_half_open()`.
- Expanded functions for extracting plot components from ggplots via
  `get_plot_component()` and several wrapper functions for getting
  titles, axes, panels, and so on. This also lets `get_panel()`
  extract from plots with more than one panel. `get_panel_component()`
  allows you to further extract components like geoms from the
  plot panel (@malcolmbarrett, #111).

## Minor changes

- Minor tweaks to various legend layout parameters so that legends look
  better as the underlying theme font size is changed.
- Various minor fixes to `theme_cowplot()` and derived themes.
- Change `background_grid()` defaults so they match the
  new grid themes.
- Renamed `plot_to_gtable()` to `as_gtable()`.
- All functions now understand both spellings of color/colour.
- `get_legend()` now returns NULL if there is no legend, instead of raising
  an error.


# cowplot 0.9.4

- Fix CRAN check errors

# cowplot 0.9.3

- Fix regression tests to work with ggplot2 3.0.0

# cowplot 0.9.2

- Rewritten cowplot::ggsave function that calls ggplot2::ggsave
- More robust handling of R graphics device weirdness/plots popping up in the wrong places

# cowplot 0.9.1

- Make examples and vignettes fail gracefully when magick package is not installed

# cowplot 0.9.0

- Added a theme for maps, `theme_map()`. Code provided by Spencer Fox, https://github.com/sjfox.
- Added `axis_canvas()` function and related functions to make
  marginal plots and plot annotations simpler
- Now export the `plot_to_gtable` function which converts most
  anything into a gtable for further use with cowplot
- Added `inherit.aes = FALSE` to draw functions where needed
- Added examples to various draw functions
- Added draw_image() function to draw images onto plots

# cowplot 0.8.0

- The function plot_grid can now also handle base-R (graphics) plots. Code provided by https://github.com/flying-sheep.
- More sophisticated plot alignments of complex plots are now possible. Code provided by Spencer Fox, https://github.com/sjfox.
- Plot labels can now be styled. In particular, they follow the theme settings, e.g. if the theme uses a different font than default. This closes issue #37.
- The positioning of plot labels in `plot_grid` can now be controlled with additional position parameters `label_x` and `label_y`. This closes issue #32.
- Problems with elements from globally set themes leaking into the plot-grid background have been fixed. This closes issues #60, #63, #66.


# cowplot 0.7.0

- This version of cowplot has been prepared for the upcoming release of ggplot2 2.2.0. As a result of this upcoming switch, the function switch_axis_position() has been removed. Alternative axes will be natively supported by ggplot2 2.2.0.
- As of this version, cowplot requires R >= 3.3.0. This dependency was added because R 3.3 fixes a critical problem with lists of units.

# cowplot 0.6.3

- Adds a get_legend() function that extracts the legend from a plot. Code provided by https://github.com/marcus1487
- Fixes bug that creates an empty page on some plot devices. Fix provided by https://github.com/marcus1487
- Fixes bug that hardcoded panel border line-type and size. Fix provided by Spencer Fox, https://github.com/sjfox
- Adds the option of custom axes on the drawing layer. Code provided by https://github.com/zaczap
- Separates out the alignment function from plot_grid(). Will likely be more useful further down the line. Code provided by Spencer Fox, https://github.com/sjfox

# cowplot 0.6.2

- Updated plot_grid vignette so the tutorial on aligning plots of different types works again. This stopped working at some point and was removed from 0.6.1.

# cowplot 0.6.1

- Added new convenience function draw_figure_label() to label figures with labels such as "Figure 1".
- Fixes in switch_axis_position() so that rotated axis labels work.
- Fix missing axis lines in ggplot2 2.1

# cowplot 0.6.0

Major changes:
- Now requires ggplot2 version 2.0.0 or higher. Use cowplot 0.5.0 with older versions of ggplot2.
- Because of the dependency on ggplot2 2.0.0, the default design is changed. No more bold face for axis labels
- Add auto-generation of labels in plot_grid()
- Add vignettes describing plot annotations and shared legends among plots

# cowplot 0.5.0

Major changes:
- Fix label positioning in plot_grid() so it is not affected by the scale parameter
- Add draw_label() function which can draw both text and plotmath expressions
- Add parameters hjust and vjust to plot_grid() to allow fine-tuning of label position
- Add annotations underneath plot, via add_sub() function

Other changes:
- Improve vignettes

# cowplot 0.4.0

Major changes:
- Added a function switch_axis_position() which can move/copy the x and/or y axis of a
  plot to the other side
- plot_grid() can now align graphs
- plot_grid() can now make grids with varying column widths and row heights

Other changes:
- Various improvements in the documentation
- Code has been separated into multiple files for easier maintenance

# cowplot 0.3.1

Fix Vignette title

# cowplot 0.3.0

First complete implementation ready for initial release
# bayestestR 0.8.0

## New functions

- `mediation()`, to compute average direct and average causal mediation effects of multivariate response models (`brmsfit`, `stanmvreg`).

## Bug fixes

- `bayesfactor_parameters()` works with `R<3.6.0`.

# bayestestR 0.7.0

## General

- Preliminary support for *stanfit* objects.
- Added support for *bayesQR* objects.

## Changes to functions

- `weighted_posteriors()` can now be used with data frames.
- Revised `print()` for `describe_posterior()`.
- Improved value formatting for Bayesfactor functions.

## Bug fixes

- Link transformation are now taken into account for `emmeans` objets. E.g., in `describe_posterior()`.
- Fix `diagnostic_posterior()` when algorithm is not "sampling".
- Minor revisions to some documentations.
- Fix CRAN check issues for win-old-release.

# bayestestR 0.6.0

## Changes to functions

- `describe_posterior()` now also works on `effectsize::standardize_posteriors()`.
- `p_significance()` now also works on `parameters::simulate_model()`.
- `rope_range()` supports more (frequentis) models.

## Bug fixes

- Fixed issue with `plot()` `data.frame`-methods of `p_direction()` and `equivalence_test()`.
- Fix check issues for forthcoming insight-update.

# bayestestR 0.5.3

## General

- Support for *bcplm* objects (package **cplm**)

## Changes to functions

- `estimate_density()` now also works on grouped data frames.

## Bug fixes

- Fixed bug in `weighted_posteriors()` to properly weight Intercept-only `BFBayesFactor` models.
- Fixed bug in `weighted_posteriors()` when models have very low posterior probability ( #286 ).
- Fixed bug in `describe_posterior()`, `rope()` and `equivalence_test()` for *brmsfit* models with monotonic effect.
- Fixed issues related to latest changes in `as.data.frame.brmsfit()` from the *brms* package.

# bayestestR 0.5.0

## General

- Added `p_pointnull()` as an alias to `p_MAP()`.
- Added `si()` function to compute support intervals.
- Added `weighted_posteriors()` for generating posterior samples averaged across models.
- Added `plot()`-method for `p_significance()`.
- `p_significance()` now also works for *brmsfit*-objects.
- `estimate_density()` now also works for *MCMCglmm*-objects.
- `equivalence_test()` gets `effects` and `component` arguments for *stanreg* and *brmsfit*  models, to print specific model components.
- Support for *mcmc* objects (package **coda**)
- Provide more distributions via `distribution()`.
- Added `distribution_tweedie()`.
- Better handling of `stanmvreg` models for `describe_posterior()`, `diagnostic_posterior()` and `describe_prior()`.

## Breaking changes

- `point_estimate()`: argument `centrality` default value changed from 'median' to 'all'.
- `p_rope()`, previously as exploratory index, was renamed as `mhdior()` (for *Max HDI inside/outside ROPE*), as `p_rope()` will refer to `rope(..., ci = 1)` ( #258 )

## Bug fixes

- Fixed mistake in description of `p_significance()`.
- Fixed error when computing BFs with `emmGrid` based on some non-linear models ( #260 ).
- Fixed wrong output for percentage-values in `print.equivalence_test()`.
- Fixed issue in `describe_posterior()` for `BFBayesFactor`-objects with more than one model.

# bayestestR 0.4.0

## New functions / features

- `convert_bayesian_to_frequentist()` Convert (refit) Bayesian model as frequentist
- `distribution_binomial()` for perfect binomial distributions
- `simulate_ttest()` Simulate data with a mean difference
- `simulate_correlation()` Simulate correlated datasets
- `p_significance()` Compute the probability of Practical Significance (ps)
- `overlap()` Compute overlap between two empirical distributions
- `estimate_density()`: `method = "mixture"` argument added for mixture density estimation

## Bug fixes

- Fixed bug in `simulate_prior()` for stanreg-models when `autoscale` was set to `FALSE`

# bayestestR 0.3.0

## General

- revised `print()`-methods for functions like `rope()`, `p_direction()`, `describe_posterior()` etc., in particular for model objects with random effects and/or zero-inflation component

## New functions / features

- `check_prior()` to check if prior is informative
- `simulate_prior()` to simulate model's priors as distributions
- `distribution_gamma()` to generate a (near-perfect or random) Gamma distribution
- `contr.bayes` function for orthogonal factor coding (implementation from Singmann & Gronau's [`bfrms`](https://github.com/bayesstuff/bfrms/), used for proper prior estimation when factor have 3 levels or more. See Bayes factor vignette
## Changes to functions

- Added support for `sim`, `sim.merMod` (from `arm::sim()`) and `MCMCglmm`-objects to many functions (like `hdi()`, `ci()`, `eti()`, `rope()`, `p_direction()`, `point_estimate()`, ...)
- `describe_posterior()` gets an `effects` and `component` argument, to include the description of posterior samples from random effects and/or zero-inflation component.
- More user-friendly warning for non-supported models in `bayesfactor()`-methods

## Bug fixes

- Fixed bug in `bayesfactor_inclusion()` where the same interaction sometimes appeared more than once (#223)
- Fixed bug in `describe_posterior()` for *stanreg* models fitted with fullrank-algorithm

# bayestestR 0.2.5

## Breaking changes

- `rope_range()` for binomial model has now a different default (-.18; .18 ; instead of -.055; .055)
- `rope()`: returns a proportion (between 0 and 1) instead of a value between 0 and 100
- `p_direction()`: returns a proportion (between 0.5 and 1) instead of a value between 50 and 100 ([#168](https://github.com/easystats/bayestestR/issues/168))
- `bayesfactor_savagedickey()`: `hypothesis` argument replaced by `null` as part of the new `bayesfactor_parameters()` function

## New functions / features

- `density_at()`, `p_map()` and `map_estimate()`: `method` argument added
- `rope()`: `ci_method` argument added
- `eti()`: Computes equal-tailed intervals
- `reshape_ci()`: Reshape CIs between wide/long
- `bayesfactor_parameters()`: New function, replacing `bayesfactor_savagedickey()`, allows for computing Bayes factors against a *point-null* or an *interval-null*
- `bayesfactor_restricted()`: Function for computing Bayes factors for order restricted models

## Minor changes

## Bug fixes

- `bayesfactor_inclusion()` now works with `R < 3.6`.

# bayestestR 0.2.2

## Breaking changes

- `equivalence_test()`: returns capitalized output (e.g., `Rejected` instead of `rejected`)
- `describe_posterior.numeric()`: `dispersion` defaults to `FALSE` for consistency with the other methods

## New functions / features

- `pd_to_p()` and `p_to_pd()`: Functions to convert between probability of direction (pd) and p-value
- Support of `emmGrid` objects: `ci()`, `rope()`, `bayesfactor_savagedickey()`, `describe_posterior()`, ...


## Minor changes

- Improved tutorial 2

## Bug fixes

- `describe_posterior()`: Fixed column order restoration
- `bayesfactor_inclusion()`: Inclusion BFs for matched models are more inline with JASP results.

# bayestestR 0.2.0

## Breaking changes

- plotting functions now require the installation of the `see` package
- `estimate` argument name in `describe_posterior()` and `point_estimate()` changed to `centrality`
- `hdi()`, `ci()`, `rope()` and `equivalence_test()` default `ci` to `0.89`
- `rnorm_perfect()` deprecated in favour of `distribution_normal()`
- `map_estimate()` now returns a single value instead of a dataframe and the `density` parameter has been removed. The MAP density value is now accessible via `attributes(map_output)$MAP_density`

## New functions / features

- `describe_posterior()`, `describe_prior()`, `diagnostic_posterior()`: added wrapper function
- `point_estimate()` added function to compute point estimates
- `p_direction()`: new argument `method` to compute pd based on AUC
- `area_under_curve()`: compute AUC
- `distribution()` functions have been added
- `bayesfactor_savagedickey()`, `bayesfactor_models()` and `bayesfactor_inclusion()` functions has been added
- Started adding plotting methods (currently in the [`see`](https://github.com/easystats/see) package) for `p_direction()` and `hdi()`
- `probability_at()` as alias for `density_at()`
- `effective_sample()` to return the effective sample size of Stan-models
- `mcse()` to return the Monte Carlo standard error of Stan-models

## Minor changes

- Improved documentation
- Improved testing
- `p_direction()`: improved printing
- `rope()` for model-objects now returns the HDI values for all parameters as attribute in a consistent way
- Changes legend-labels in `plot.equivalence_test()` to align plots with the output of the `print()`-method (#78)

## Bug fixes

- `hdi()` returned multiple class attributes (#72)
- Printing results from `hdi()` failed when `ci`-argument had fractional parts for percentage values (e.g. `ci = .995`).
- `plot.equivalence_test()` did not work properly for *brms*-models (#76).

# bayestestR 0.1.0

- CRAN initial publication and [0.1.0 release](https://github.com/easystats/bayestestR/releases/tag/v0.1.0)
- Added a `NEWS.md` file to track changes to the package
# Version 0.6.0

- Support plotmath expression and add new parameter `parse` to function. Fixes issue #64.
  Thanks to @IndrajeetPatil for the idea.

# Version 0.5.0

- Fix typos in README.md (thanks to @SMargell)
- map_signif_level can now take a user-supplied function to format the
p-value (PR #52, thanks to @ilia-kats)


# Version 0.4.0

- Fix bug that stoped textsize from working
- Add manual=TRUE mode, which allows the parameters to be given as a data.frame

# Version 0.3.0

- Simplify setting brackets at custom locations with xmin, xmax and y_position
- Extend documentation
- Bug fixes

# Version 0.2.0

- Fixed bug, when `alpha()` from another package was loaded (issue #2)
- Added additional parameters to customize bracket linetype, textsize, size (issue #5)
- Fixed bug when annotation was identical for different brackets (issue #6)
- Fixed bug when multiple comparisons referenced the same block (issue #8)


# Initial Release (0.1.0)

The package has been made publicly available on CRAN: https://CRAN.R-project.org/package=ggsignif
# dbplyr 1.4.4

* Internally `DBI::dbExecute()` now uses `immediate = TRUE`; this improves
  support for session-scoped temporary tables in MS SQL (@krlmlr, #438).

* Subqueries with `ORDER BY` use `TOP 9223372036854775807` instead of 
  `TOP 100 PERCENT` on SQL Server for compatibility with Azure Data Warehouse 
  (#337, @alexkyllo).

* `escape()` now supports `blob` vectors using new `sql_escape_raw()` 
  generic. It enables using [blob](https://blob.tidyverse.org/) variables in 
  dplyr verbs, for example to filter nvarchar values by UTF-16 blobs
  (see https://github.com/r-dbi/DBI/issues/215#issuecomment-356376133). 
  (@okhoma, #433)

* Added `setOldClass()` calls for `"ident"` and `"ident_q"` classes for 
  compatibility with dplyr 1.0.0 (#448, @krlmlr).

* Postgres `str_detect()` translation uses same argument names as stringr,
  and gains a `negate` argument (#444).

* `semi_join()` and `anti_join()` now correctly support the `sql_on` argument 
  (#443, @krlmlr).

# dbplyr 1.4.3

* dbplyr now uses RPostgres (instead of RPostgreSQL) and RMariaDB (instead of 
  RMySQL) for its internal tests and data functions (#427).

* The Date and POSIXt methods for `escape()` now use exported 
  `sql_escape_date()` and `sql_escape_datetime()` generics to allow backend
  specific formatting of date and datetime literals. These are used to
  provide methods for Athena and Presto backends (@OssiLehtinen, #384, #391).

* `first()`, `last()`, `nth()`, `lead()` and `lag()` now respect the
  `window_frame()` (@krlmlr, #366).

* SQL server: new translations for `str_flatten()` (@PauloJhonny, #405).

* SQL server: temporary datasets are now session-local, not global (#401).

* Postgres: correct `str_detect()`, `str_replace()` and `str_replace_all()` 
  translation (@shosaco, #362).

# dbplyr 1.4.2

* Fix bug when partially evaluating unquoting quosure containing a single 
  symbol (#317)

* Fixes for rlang and dpylr compatibility.

# dbplyr 1.4.1

Minor improvements to SQL generation

* `x %in% y` strips names of `y` (#269).

* Enhancements for scoped verbs (`mutate_all()`, `summarise_if()`,
  `filter_at()` etc) (#296, #306).

* MS SQL use `TOP 100 PERCENT` as stop-gap to allow subqueries with 
  `ORDER BY` (#277).

* Window functions now translated correctly for Hive (#293, @cderv).

# dbplyr 1.4.0

## Breaking changes

* ``Error: `con` must not be NULL``: If you see this error, it probably means 
  that you have forgotten to pass `con` down to a dbplyr function. 
  Previously, dbplyr defaulted to using `simulate_dbi()` which introduced
  subtle escaping bugs. (It's also possible I have forgotten to pass it 
  somewhere that the dbplyr tests don't pick up, so if you can't figure it 
  out, please let me know).

* Subsetting (`[[`, `$`, and `[`) functions are no longer evaluated locally. 
  This makes the translation more consistent and enables useful new idioms 
  for modern databases (#200).

## New features

* MySQL/MariaDB (https://mariadb.com/kb/en/library/window-functions/) 
  and SQLite (https://www.sqlite.org/windowfunctions.html) translations gain 
  support for window functions, available in Maria DB 10.2, MySQL 8.0, and 
  SQLite 3.25 (#191).

* Overall, dplyr generates many fewer subqueries:

  * Joins and semi-joins no longer add an unneeded subquery (#236). This is
    facilitated by the new `bare_identifier_ok` argument to `sql_render()`;
    the previous argument was called `root` and confused me.
  
  * Many sequences of `select()`, `rename()`, `mutate()`, and `transmute()` can
    be collapsed into a single query, instead of always generating a subquery
    (#213).

* New `vignette("sql")` describes some advantages of dbplyr over SQL (#205) and 
  gives some advice about writing literal SQL inside of dplyr, when you need 
  to (#196).

* New `vignette("reprex")` gives some hints on creating reprexes that work 
  anywhere (#117). This is supported by a new `tbl_memdb()` that matches the 
  existing `tbl_lazy()`.

* All `..._join()` functions gain an `sql_on` argument that allows specifying
  arbitrary join predicates in SQL code (#146, @krlmlr).

## SQL translations

* New translations for some lubridate functions: `today()`, `now()`, 
  `year()`, `month()`, `day()`, `hour()`, `minute()`,
  `second()`, `quarter()`, `yday()` (@colearendt, @derekmorr). Also added new 
  translation for `as.POSIXct()`.

* New translations for stringr functions: `str_c()`, `str_sub()`, 
  `str_length()`, `str_to_upper()`, `str_to_lower()`, and `str_to_title()`
  (@colearendt). Non-translated stringr functions throw a clear error.

* New translations for bitwise operations: `bitwNot()`, `bitwAnd()`, `bitwOr()`,
  `bitwXor()`, `bitwShiftL()`, and `bitwShiftR()`. Unlike the base R functions, 
  the translations do not coerce arguments to integers (@davidchall, #235).

* New translation for `x[y]` to `CASE WHEN y THEN x END`. This enables 
  `sum(a[b == 0])` to work as you expect from R (#202). `y` needs to be
  a logical expression; if not you will likely get a type error from your 
  database.

* New translations for `x$y` and `x[["y"]]` to `x.y`, enabling you to index
  into nested fields in databases that provide them (#158).

* The `.data` and `.env` pronouns of tidy evaluation are correctly translated 
  (#132).

* New translation for `median()` and `quantile()`. Works for all ANSI compliant
  databases (SQL Server, Postgres, MariaDB, Teradata) and has custom 
  translations for Hive. Thanks to @edavidaja for researching the SQL variants! 
  (#169)
  
* `na_if()` is correct translated to `NULLIF()` (rather than `NULL_IF`) (#211).

* `n_distinct()` translation throws an error when given more than one argument.
  (#101, #133).

* New default translations for `paste()`, `paste0()`, and the hyperbolic 
  functions (these previously were only available for ODBC databases).

* Corrected translations of `pmin()` and `pmax()` to `LEAST()` and `GREATEST()` 
  for ANSI compliant databases (#118), to `MIN()` and `MAX()` for SQLite, and 
  to an error for SQL server.

* New translation for `switch()` to the simple form of `CASE WHEN` (#192).

### SQL simulation

SQL simulation makes it possible to see what dbplyr will translate SQL to,
without having an active database connection, and is used for testing and
generating reprexes.

* SQL simulation has been overhauled. It now works reliably, is better 
  documented, and always uses ANSI escaping (i.e. `` ` `` for field 
  names and `'` for strings).

* `tbl_lazy()` now actually puts a `dbplyr::src` in the `$src` field. This
  shouldn't affect any downstream code unless you were previously working
  around this weird difference between `tbl_lazy` and `tbl_sql` classes.
  It also includes the `src` class in its class, and when printed,
  shows the generated SQL (#111).

## Database specific improvements

* MySQL/MariaDB

  * Translations also applied to connections via the odbc package 
    (@colearendt, #238)
  
  * Basic support for regular expressions via `str_detect()` and  
    `str_replace_all()` (@colearendt, #168).

  * Improved translation for `as.logical(x)` to `IF(x, TRUE, FALSE)`.

* Oracle

  * New custom translation for `paste()` and `paste0()` (@cderv, #221)

* Postgres

  * Basic support for regular expressions via `str_detect()` and  
    `str_replace_all()` (@colearendt, #168).

* SQLite

  * `explain()` translation now generates `EXPLAIN QUERY PLAN` which
    generates a higher-level, more human friendly explanation.

* SQL server

  * Improved translation for `as.logical(x)` to `CAST(x as BIT)` (#250).
    
  * Translates `paste()`, `paste0()`, and `str_c()` to `+`.

  * `copy_to()` method applies temporary table name transformation
    earlier so that you can now overwrite temporary tables (#258).

  * `db_write_table()` method uses correct argument name for 
    passing along field types (#251).

## Minor improvements and bug fixes

* Aggregation functions only warn once per session about the use of 
  `na.rm = TRUE` (#216).

* table names generated by `random_table_name()` have the prefix 
  "dbplyr_", which makes it easier to find them programmatically 
  (@mattle24, #111)

* Functions that are only available in a windowed (`mutate()`) query now
  throw an error when called in a aggregate (`summarise()`) query (#129)

* `arrange()` understands the `.by_group` argument, making it possible
  sort by groups if desired. The default is `FALSE` (#115)

* `distinct()` now handles computed variables like `distinct(df, y = x + y)` 
  (#154).

* `escape()`, `sql_expr()` and `build_sql()` no longer accept `con = NULL` as 
  a shortcut for `con = simulate_dbi()`. This made it too easy to forget to 
  pass `con` along, introducing extremely subtle escaping bugs. `win_over()`
  gains a `con` argument for the same reason.
  
* New `escape_ansi()` always uses ANSI SQL 92 standard escaping (for use 
  in examples and documentation).

* `mutate(df, x = NULL)` drops `x` from the output, just like when working with
  local data frames (#194).

* `partial_eval()` processes inlined functions (including rlang lambda 
  functions). This makes dbplyr work with more forms of scoped verbs like
  `df %>% summarise_all(~ mean(.))`, `df %>% summarise_all(list(mean))` (#134).

* `sql_aggregate()` now takes an optional argument `f_r` for passing to
  `check_na_rm()`. This allows the warning to show the R function name rather 
  than the SQL function name (@sverchkov, #153).

* `sql_infix()` gains a `pad` argument for the rare operator that doesn't
  need to be surrounded by spaces.

* `sql_prefix()` no longer turns SQL functions into uppercase, allowing for 
  correct translation of case-sensitive SQL functions (#181, @mtoto).

* `summarise()` gives a clear error message if you refer to a variable 
  created in that same `summarise()` (#114).

* New `sql_call2()` which is to `rlang::call2()` as `sql_expr()` is to 
  `rlang::expr()`.

* `show_query()` and `explain()` use `cat()` rather than message.

* `union()`, `union_all()`, `setdiff()` and `intersect()` do a better job
  of matching columns across backends (#183).

# dbplyr 1.3.0

* Now supports for dplyr 0.8.0 (#190) and R 3.1.0

## API changes

* Calls of the form `dplyr::foo()` are now evaluated in the database, 
  rather than locally (#197).

* `vars` argument to `tbl_sql()` has been formally deprecated; it hasn't 
  actually done anything for a while (#3254).

* `src` and `tbl` objects now include a class generated from the class of 
  the underlying connection object. This makes it possible for dplyr backends 
  to implement different behaviour at the dplyr level, when needed. (#2293)

## SQL translation

* `x %in% y` is now translated to `FALSE` if `y` is empty (@mgirlich, #160).

* New `as.integer64(x)` translation to `CAST(x AS BIGINT)` (#3305)

* `case_when` now translates with a ELSE clause if a formula of the form 
  `TRUE~<RHS>` is provided . (@cderv, #112)

* `cummean()` now generates `AVG()` not `MEAN()` (#157)

* `str_detect()` now uses correct parameter order (#3397)

* MS SQL
    * Cumulative summary functions now work (#157)
    * `ifelse()` uses `CASE WHEN` instead of `IIF`; this allows more complex 
       operations, such as `%in%`, to work properly (#93)

* Oracle
    * Custom `db_drop_table()` now only drops tables if they exist (#3306)
    * Custom `setdiff()` translation  (#3493)
    * Custom `db_explain()` translation (#3471)

* SQLite
  * Correct translation for `as.numeric()`/`as.double()` (@chris-park, #171).

* Redshift 
  * `substr()` translation improved (#3339)

## Minor improvements and bug fixes

* `copy_to()` will only remove existing table when `overwrite = TRUE` and the
  table already exists, eliminating a confusing "NOTICE" from PostgreSQL 
  (#3197).

* `partial_eval()` handles unevaluated formulas (#184).

* `pull.tbl_sql()` now extracts correctly from grouped tables (#3562).

* `sql_render.op()` now correctly forwards the `con` argument (@kevinykuo, #73).

# dbplyr 1.2.2

* R CMD check fixes

# dbplyr 1.2.1

* Forward compatibility fixes for rlang 0.2.0

# dbplyr 1.2.0

## New top-level translations

* New translations for 
  
    * MS Access (#2946) (@DavisVaughan)
    * Oracle, via odbc or ROracle (#2928, #2732, @edgararuiz)
    * Teradata. 
    * Redshift.

* dbplyr now supplies appropriate translations for the RMariaDB and 
  RPostgres packages (#3154). We generally recommend using these packages
  in favour of the older RMySQL and RPostgreSQL packages as they are
  fully DBI compliant and tested with DBItest.

## New features

* `copy_to()` can now "copy" tbl_sql in the same src, providing another
  way to cache a query into a temporary table (#3064). You can also 
  `copy_to` tbl_sqls from another source, and `copy_to()` will automatically
  collect then copy.

* Initial support for stringr functions: `str_length()`, `str_to_upper()`,
  `str_to_lower()`, `str_replace_all()`, `str_detect()`, `str_trim()`. 
  Regular expression support varies from database to database, but most 
  simple regular expressions should be ok.

## Tools for developers

* `db_compute()` gains an `analyze` argument to match `db_copy_to()`.

* New `remote_name()`, `remote_con()`, `remote_src()`, `remote_query()` and 
  `remote_query_plan()` provide a standard API for get metadata about a 
  remote tbl (#3130, #2923, #2824).

* New `sql_expr()` is a more convenient building block for low-level SQL
  translation (#3169).

* New `sql_aggregate()` and `win_aggregate()` for generating SQL and windowed
  SQL functions for aggregates. These take one argument, `x`, and warn if 
  `na.rm` is not `TRUE` (#3155). `win_recycled()` is equivalent to 
  `win_aggregate()` and has been soft-deprecated.
  
* `db_write_table` now needs to return the table name

## Minor improvements and bug fixes

* Multiple `head()` calls in a row now collapse to a single call. This avoids 
  a printing problem with MS SQL (#3084).

* `escape()` now works with integer64 values from the bit64 package (#3230)

* `if`, `ifelse()`, and `if_else()` now correctly scope the false condition
  so that it only applies to non-NULL conditions (#3157)

* `ident()` and `ident_q()` handle 0-length inputs better, and should
  be easier to use with S3 (#3212)

* `in_schema()` should now work in more places, particularly in `copy_to()` 
   (#3013, @baileych)

* SQL generation for joins no longer gets stuck in a endless loop if you
  request an empty suffix (#3220).

* `mutate()` has better logic for splitting a single mutate into multiple
  subqueries (#3095).

* Improved `paste()` and `paste0()` support in MySQL, PostgreSQL (#3168),
  and RSQLite (#3176). MySQL and PostgreSQL gain support for `str_flatten()` 
  which behaves like `paste(x, collapse = "-")` (but for technical reasons 
  can't be implemented as a straightforward translation of `paste()`).

* `same_src.tbl_sql()` now performs correct comparison instead of always 
  returning `TRUE`. This means that `copy = TRUE` once again allows you to
  perform cross-database joins (#3002).

* `select()` queries no longer alias column names unnecessarily 
  (#2968, @DavisVaughan).

* `select()` and `rename()` are now powered by tidyselect, 
  fixing a few renaming bugs (#3132, #2943, #2860).

* `summarise()` once again performs partial evaluation before database 
  submission (#3148).

* `test_src()` makes it easier to access a single test source.

## Database specific improvements

*   MS SQL
  
    * Better support for temporary tables (@Hong-Revo)
    
    * Different translations for filter/mutate contexts for: `NULL` evaluation
      (`is.na()`, `is.null()`), logical operators (`!`, `&`, `&&`, `|`, `||`),
      and comparison operators (`==`, `!=`, `<`, `>`, `>=`, `<=`)

*   MySQL: `copy_to()` (via `db_write_table()`) correctly translates logical 
    variables to integers (#3151).
  
*   odbc: improved `n()` translation in windowed context.

*   SQLite: improved `na_if` translation (@cwarden)

*   PostgreSQL: translation for `grepl()` added (@zozlak)

*   Oracle: changed VARVHAR to VARCHAR2 datatype (@washcycle, #66)

# dbplyr 1.1.0

## New features

* `full_join()` over non-overlapping columns `by = character()` translated to
  `CROSS JOIN` (#2924).

* `case_when()` now translates to SQL "CASE WHEN" (#2894)

* `x %in% c(1)` now generates the same SQL as `x %in% 1` (#2898).

* New `window_order()` and `window_frame()` give you finer control over 
  the window functions that dplyr creates (#2874, #2593).

* Added SQL translations for Oracle (@edgararuiz).

## Minor improvements and bug fixes

* `x %in% c(1)` now generates the same SQL as `x %in% 1` (#2898).

* `head(tbl, 0)` is now supported (#2863). 

* `select()`ing zero columns gives a more information error message (#2863).

* Variables created in a join are now disambiguated against other variables
  in the same table, not just variables in the other table (#2823).

* PostgreSQL gains a better translation for `round()` (#60).

* Added custom `db_analyze_table()` for MS SQL, Oracle, Hive and Impala (@edgararuiz)

* Added support for `sd()` for aggregate and window functions (#2887) (@edgararuiz) 

* You can now use the magrittr pipe within expressions,
  e.g. `mutate(mtcars, cyl %>% as.character())`.

* If a translation was supplied for a summarise function, but not for the
  equivalent windowed variant, the expression would be translated to `NULL`
  with a warning. Now `sql_variant()` checks that all aggregate functions 
  have matching window functions so that correct translations or clean errors
  will be generated (#2887)

# dbplyr 1.0.0

## New features

* `tbl()` and `copy_to()` now work directly with DBI connections (#2423, #2576), 
  so there is no longer a need to generate a dplyr src. 
  
    ```R
    library(dplyr)

    con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
    copy_to(con, mtcars)
    
    mtcars2 <- tbl(con, "mtcars")
    mtcars2
    ```

* `glimpse()` now works with remote tables (#2665)

* dplyr has gained a basic SQL optimiser, which collapses certain nested
  SELECT queries into a single query (#1979). This will improve query
  execution performance for databases with less sophisticated query optimisers,
  and fixes certain problems with ordering and limits in subqueries (#1979).
  A big thanks goes to @hhoeflin for figuring out this optimisation.

* `compute()` and `collapse()` now preserve the "ordering" of rows.
  This only affects the computation of window functions, as the rest
  of SQL does not care about row order (#2281).

* `copy_to()` gains an `overwrite` argument which allows you to overwrite
  an existing table. Use with care! (#2296)

* New `in_schema()` function makes it easy to refer to tables in schema:
  `in_schema("my_schema_name", "my_table_name")`.

## Deprecated and defunct

* `query()` is no longer exported. It hasn't been useful for a while
  so this shouldn't break any code.

## Verb-level SQL generation

* Partial evaluation occurs immediately when you execute a verb (like 
  `filter()` or `mutate()`) rather than happening when the query is executed 
  (#2370).

* `mutate.tbl_sql()` will now generate as many subqueries as necessary so 
  that you can refer to variables that you just created (like in mutate
  with regular dataframes) (#2481, #2483).

* SQL joins have been improved:

    * SQL joins always use the `ON ...` syntax, avoiding `USING ...` even for 
      natural joins. Improved handling of tables with columns of the same name 
      (#1997, @javierluraschi). They now generate SQL more similar to what you'd
      write by hand, eliminating a layer or two of subqueries (#2333)
      
    * [API] They now follow the same rules for including duplicated key variables
      that the data frame methods do, namely that key variables are only
      kept from `x`, and never from `y` (#2410)
      
    * [API] The `sql_join()` generic now gains a `vars` argument which lists
      the variables taken from the left and right sides of the join. If you
      have a custom `sql_join()` method, you'll need to update how your
      code generates joins, following the template in `sql_join.generic()`.
      
    * `full_join()` throws a clear error when you attempt to use it with a
      MySQL backend (#2045)
      
    * `right_join()` and `full_join()` now return results consistent with
      local data frame sources when there are records in the right table with
      no match in the left table. `right_join()` returns values of `by` columns
      from the right table. `full_join()` returns coalesced values of `by` 
      columns from the left and right tables (#2578, @ianmcook)

*   `group_by()` can now perform an inline mutate for database backends (#2422).

*   The SQL generation set operations (`intersect()`, `setdiff()`, `union()`, 
    and `union_all()`) have been considerably improved. 
  
    By default, the component SELECT are surrounded with parentheses, except on
    SQLite. The SQLite backend will now throw an error if you attempt a set operation
    on a query that contains a LIMIT, as that is not supported in SQLite (#2270).
    
    All set operations match column names across inputs, filling in non-matching
    variables with NULL (#2556).

*   `rename()` and `group_by()` now combine correctly (#1962)

*   `tbl_lazy()` and `lazy_tbl()` have been exported. These help you test
    generated SQL with out an active database connection.

*   `ungroup()` correctly resets grouping variables (#2704).

## Vector-level SQL generation

* New `as.sql()` safely coerces an input to SQL.

* More translators for `as.character()`, `as.integer()` and `as.double()` 
  (#2775).

* New `ident_q()` makes it possible to specifier identifiers that do not
  need to be quoted.

* Translation of inline scalars:

    * Logical values are now translated differently depending on the backend.
      The default is to use "true" and "false" which is the SQL-99 standard,
      but not widely support. SQLite translates to "0" and "1" (#2052).

    * `Inf` and `-Inf` are correctly escaped

    * Better test for whether or not a double is similar to an integer and 
      hence needs a trailing 0.0 added (#2004).

    * Quoting defaults to `DBI::dbEscapeString()` and `DBI::dbQuoteIdentifier()`
      respectively.

* `::` and `:::` are handled correctly (#2321)

* `x %in% 1` is now correctly translated to `x IN (1)` (#511).

* `ifelse()` and `if_else()` use correct argument names in SQL translation 
  (#2225).

* `ident()` now returns an object with class `c("ident", "character")`. It
   no longer contains "sql" to indicate that this is not already escaped.
   
* `is.na()` and `is.null()` gain extra parens in SQL translation to preserve
  correct precedence (#2302).

* [API] `log(x, b)` is now correctly translated to the SQL `log(b, x)` (#2288).
  SQLite does not support the 2-argument log function so it is translated
  to `log(x) / log(b)`.

* `nth(x, i)` is now correctly translated to `nth_value(x, i)`.

* `n_distinct()` now accepts multiple variables (#2148).

* [API] `substr()` is now translated to SQL, correcting for the difference
  in the third argument. In R, it's the position of the last character,
  in SQL it's the length of the string (#2536).

* `win_over()` escapes expression using current database rules.

## Backends

* `copy_to()` now uses `db_write_table()` instead of `db_create_table()` and 
  `db_insert_into()`. `db_write_table.DBIConnection()` uses `dbWriteTable()`.

* New `db_copy_to()`, `db_compute()` and `db_collect()` allow backends to 
  override the entire database process behind `copy_to()`, `compute()` and 
  `collect()`. `db_sql_render()` allow additional control over the SQL
  rendering process.

* All generics whose behaviour can vary from database to database now 
  provide a DBIConnection method. That means that you can easily scan
  the NAMESPACE to see the extension points.

* `sql_escape_logical()` allows you to control the translation of
  literal logicals (#2614).

* `src_desc()` has been replaced by `db_desc()` and  now dispatches on the 
  connection, eliminating the last method that required dispatch on the class 
  of the src.

* `win_over()`, `win_rank()`, `win_recycled()`, `win_cumulative()`,
  `win_current_group()` and `win_current_order()` are now exported. This
  should make it easier to provide customised SQL for window functions
  (#2051, #2126).
  
*  SQL translation for Microsoft SQL Server (@edgararuiz)

*  SQL translation for Apache Hive (@edgararuiz)

*  SQL translation for Apache Impala (@edgararuiz)


## Minor bug fixes and improvements

* `collect()` once again defaults to return all rows in the data (#1968).
  This makes it behave the same as `as.data.frame()` and `as_tibble()`.

* `collect()` only regroups by variables present in the data (#2156)

* `collect()` will automatically LIMIT the result to the `n`, the number of 
  rows requested. This will provide the query planner with more information
  that it may be able to use to improve execution time (#2083).

* `common_by()` gets a better error message for unexpected inputs (#2091)

* `copy_to()` no longer checks that the table doesn't exist before creation,
  instead preferring to fall back on the database for error messages. This
  should reduce both false positives and false negative (#1470)

* `copy_to()` now succeeds for MySQL if a character column contains `NA` 
   (#1975, #2256, #2263, #2381, @demorenoc, @eduardgrebe).

* `copy_to()` now returns it's output invisibly (since you're often just
   calling for the side-effect).

* `distinct()` reports improved variable information for SQL backends. This
  means that it is more likely to work in the middle of a pipeline (#2359).

* Ungrouped `do()` on database backends now collects all data locally first
  (#2392).

* Call `dbFetch()` instead of the deprecated `fetch()` (#2134).
  Use `DBI::dbExecute()` for non-query SQL commands (#1912)

* `explain()` and `show_query()` now invisibly return the first argument,
  making them easier to use inside a pipeline.

* `print.tbl_sql()` displays ordering (#2287) and prints table name, if known.

* `print(df, n = Inf)` and `head(df, n = Inf)` now work with remote tables 
  (#2580).

* `db_desc()` and `sql_translate_env()` get defaults for DBIConnection.

* Formatting now works by overriding the `tbl_sum()` generic instead of `print()`. This means that the output is more consistent with tibble, and that `format()` is now supported also for SQL sources (tidyverse/dbplyr#14).

## Lazy ops

* [API] The signature of `op_base` has changed to `op_base(x, vars, class)`

* [API] `translate_sql()` and `partial_eval()` have been refined:

    * `translate_sql()` no longer takes a vars argument; instead call
      `partial_eval()` yourself. 
    
    * Because it no longer needs the environment `translate_sql()_` now
      works with a list of dots, rather than a `lazy_dots`.
      
    * `partial_eval()` now takes a character vector of variable names
      rather than a tbl.
      
    * This leads to a simplification of the `op` data structure: 
      dots is now a list of expressions rather than a `lazy_dots`.

* [API] `op_vars()` now returns a list of quoted expressions. This
  enables escaping to happen at the correct time (i.e. when the connection
  is known).
# rstatix 0.6.0
   
## Minor changes
   
- Adapted to upcoming broom v0.7.0 release (#49)
- New argument `stack` added in `get_y_position()` to compute p-values y position for stacked bar plots ([#48](https://github.com/kassambara/rstatix/issues/48)).
- `wilcox_test()`: Now, if `detailed = TRUE`,  an estimate of the location parameter (Only present if argument detailed = TRUE). This corresponds to the pseudomedian (for one-sample case) or to the difference of the location parameter (for two-samples case) ([#45](https://github.com/kassambara/rstatix/issues/45)).

## Bug fixes
   
- `anova_test()` function: Changing R default contrast setting (`contr.treatment`) into orthogonal contrasts (`contr.sum`) to have comparable results to SPSS when users define the model using formula (@benediktclaus, [#40](https://github.com/kassambara/rstatix/issues/40)).
- Now, the option `type = "quantile"` of `get_summary_stats()` works properly (@Boyoron, [#39](https://github.com/kassambara/rstatix/issues/39)).


# rstatix 0.5.0
  
## New features
   
- New functions added for easy data frame manipulation. These functions are internally used in the `rstatix` and the `ggpubr` package and makes it easy to program with tidyverse packages using non standard evaluation.
      - df_select
      - df_arrange
      - df_group_by
      - df_nest_by
      - df_split_by
      - df_unite
      - df_get_var_names
      - df_label_both
      - df_label_value

## Minor changes

- Now, in `freq_table()` the option `na.rm` removes only missing values in the variables used to create the frequency table (@JuhlinF, [#25](https://github.com/kassambara/rstatix/issues/25)).
- Missing values are now correctly handled in `anova_test()` (@benediktclaus, [#31](https://github.com/kassambara/rstatix/issues/31))
- Maintenance for adapting to the future dplyr 1.0.0 version [#32](https://github.com/kassambara/rstatix/issues/32)
  
## Bug fixes
  
- An informative message is now displayed when users try to apply Hedge's correction when computing the Cohen's D for one sample test (@GegznaV, [#36](https://github.com/kassambara/rstatix/issues/36)).
- Bug fixes in the `games_howell_test()` function : the t-statistic is now calculated using the **absolute** mean difference between groups (@GegznaV, [#37](https://github.com/kassambara/rstatix/issues/37)).
- x position is now correctly computed when when making custom comparisons (@barrel0luck, [#28](https://github.com/kassambara/rstatix/issues/28)).
   
   
# rstatix 0.4.0

## New features
   
- The `cohens_d()` function now supports Hedge's correction. New argument `hedge.correction` added . logical indicating whether apply the Hedges correction by multiplying the usual value of Cohen's d by `(N-3)/(N-2.25)` (for unpaired t-test) and by `(n1-2)/(n1-1.25)` for paired t-test; where N is the total size of the two groups being compared (N = n1 + n2) (@IndrajeetPatil, [#9](https://github.com/kassambara/rstatix/issues/9)).
  
## Minor changes
  
- Now, the function `cohens_d()` outputs values with directionality. The absolute value is no longer returned. It can now be positive or negative depending on the data (@narunpat, [#9](https://github.com/kassambara/rstatix/issues/13)).

## Bug fixes
  
- The value of `mu` is now considered when calculating `cohens_d()` for one sample t-test (@mllewis, [#22](https://github.com/kassambara/rstatix/issues/22)).
- The function `tukey_hsd()` now handles situation where minus `-` symbols are present in factor levels (@IndrajeetPatil, [#19](https://github.com/kassambara/rstatix/issues/19)).
  
# rstatix 0.3.1

## Minor changes

- tidyr > 1.0.0 now required
- know, `identify_outliers` returns a basic data frame instead of tibble when nrow = 0 (for nice printing)
- new argument `detailed` added in `dunn_test()`. If TRUE, then estimate and method columns are shown in the results.



# rstatix 0.3.0

## New features
   
- `prop_test()`, `pairwise_prop_test()` and `row_wise_prop_test()`. Performs one-sample and two-samples z-test of proportions. Wrappers around the R base function `prop.test()` but have the advantage of performing pairwise and row-wise z-test of two proportions, the post-hoc tests following a significant chi-square test of homogeneity for 2xc and rx2 contingency tables. 
- `fisher_test()`, `pairwise_fisher_test()` and `row_wise_fisher_test()`: Fisher's exact test for count data. Wrappers around the R base function `fisher.test()` but have the advantage of performing pairwise and row-wise fisher tests, the post-hoc tests following a significant chi-square test of homogeneity for 2xc and rx2 contingency tables. 
- `chisq_test()`, `pairwise_chisq_gof_test()`, `pairwise_chisq_test_against_p()` : Chi-square test for count data.
- `binom_test()`, `pairwise_binom_test()`, `pairwise_binom_test_against_p()` and `multinom_test()`: performs exact binomial and multinomial tests. Alternative to the chi-square test of goodness-of-fit-test when the sample.
- `counts_to_cases()`: converts a contingency table or a data frame of counts into a data frame of individual observations.
- New functions `mcnemar_test()` and `cochran_qtest()` for comparing two ore more related proportions.
- `prop_trend_test()`: Performs chi-squared test for trend in proportion. This test is also known as Cochran-Armitage trend test.


## Minor changes

- Now `get_test_label()` and `get_pwc_label()` return expression by default
- Unit testing and spelling check added
- Code rewritten to adapt tidyr 1.0.0


# rstatix 0.2.0

     
## Minor changes 
   
- `get_anova_table()` supports now an object of class `grouped_anova_test`
- ANOVA table is now correctly returned when `correction = "none"` for repeated measures ANOVA
- `NAs` are now automatically removed before quantile computation for identifying outliers (@IndrajeetPatil, [#10](https://github.com/kassambara/rstatix/issues/10)).
- Unquoted factor variable name is now supported in factor manipulation functions: `set_ref_level()`, `reorder_levels()` and `make_valid_levels()`
- New argument `model` added in the function `emmeans_test()`
- Adapting to tidyr v1.0.0 (@jennybc, [#6](https://github.com/kassambara/rstatix/issues/6))
   
  
## New features
  
- New function `welch_anova_test()`: Welch one-Way ANOVA test. A wrapper around the base function `stats::oneway.test()`. This is is an alternative to the standard one-way ANOVA in the situation where the homogeneity of variance assumption is violated.
- New function `friedman_effsize()`, computes the effect size of Friedman test using the Kendall's W value.
- New function `friedman_test()`, provides a pipe-friendly framework to perform a Friedman rank sum test, which is the non-parametric alternative to the one-way repeated measures ANOVA test.
- New function `games_howell_test()`: Performs Games-Howell test, which is used to compare all possible combinations of group differences when the assumption of homogeneity of variances is violated.
- New function `kruskal_effsize()` for computing effect size for Kruskal-Wallis test.
- New functions added to round and format p-values: `p_round(), p_format(), p_mark_significant()`.
- New function `wilcox_effsize()` added for computing effect size (r) for wilcoxon test.
- New function `get_anova_table()` added to extract ANOVA table from `anova_test()` results. Can apply sphericity correction automatically in the case of within-subject (repeated measures) designs.
- New functions added to extract information from statistical tests: `get_anova_label()`
- New function `emmeans_test()` added for pairwise comparisons of estimated marginal means.
   
   
## Minor changes
  
- the unnecessary column `comparison` removed from `tukey_hsd()` results (breaking change).
- New column `n` (sample count) added to statistical tests results: `t_test()`, `wilcox_test()`, `sign_test()`, `dunn_test()` and `kruskal_test()` (@ShixiangWang, [#4](https://github.com/kassambara/rstatix/issues/4)).
- `rstatix_test` class added to `anova_test()` results
- the results of `kruskal_test()` is now an object of class `rstatix_test` that has an attribute named **args** for holding the test arguments.
- In `get_y_position()`, y positions and test data are merged now for grouped plots.
- New argument `y.trans` added in `get_y_position()` for y scale transformation.
- significance column added in `tukey_hsd()` results.
- `adjust_pvalue()` now supports grouped data

## Bug fixes
  
- `detailed` arguments correctly propagated when grouped stats are performed

# rstatix 0.1.1
   
   
## New features
  
- New function `get_pvalue_position` added to autocompute p-value positions for plotting significance using ggplot2.
- New function `get_comparisons()` added to create a list of possible pairwise comparisons between groups.
- New function `dunn_test()` added for multiple pairwise comparisons following Kruskal-Wallis test.
- New function `sign_test()` added.

   
## Minor changes
   
- `get_summary_stats()` now supports type = "min", "max", "mean" or "median"
- the results of `t_test()`, `wilcox_test()`, `dunn_test()` and `sign_test()` are now an object of class `rstatix_test` that has an attribute named **args** for holding the test arguments.
- The results of `cohens_d()` is now a data frame containing the Cohen's d and the magnitude.

## Bug fixes
  
- the argument `detatiled` is now passed to `compare_pairs()`.

# rstatix 0.1.0

First release
# performance 0.4.8

## General

* Removed suggested packages that have been removed from CRAN.

## Changes to functions

* `icc()` now also computes a "classical" ICC for `brmsfit` models. The former way of calculating an "ICC" for `brmsfit` models is now available as new function called `variance_decomposition()`.

## Bug fixes

* Fix issue with new version of *bigutilsr* for `check_outliers()`.
* Fix issue with model order in `performance_lrt()`.

# performance 0.4.7

## General

* Support for models from package *mfx*.

## Changes to functions

* `model_performance.rma()` now includes results from heterogeneity test for meta-analysis objects.
* `check_normality()` now also works for mixed models (with the limitation that studentized residuals are used).
* `check_normality()` gets an `effects`-argument for mixed models, to check random effects for normality.

## Bug fixes

* Fixed issue in `performance_accuracy()` for binomial models when response variable had non-numeric factor levels.
* Fixed issues in `performance_roc()`, which printed 1 - AUC instead of AUC.

# performance 0.4.6

## General

* Minor revisions to `model_performance()` to meet changes in *mlogit* package.
* Support for `bayesx` models.

## Changes to functions

* `icc()` gains a `by_group` argument, to compute ICCs per different group factors in mixed models with multiple levels or cross-classified design.
* `r2_nakagawa()` gains a `by_group` argument, to compute explained variance at different levels (following the variance-reduction approach by Hox 2010).
* `performance_lrt()` now works on *lavaan* objects.

## Bug fixes

* Fix issues in some functions for models with logical dependent variable.
* Fix bug in `check_itemscale()`, which caused multiple computations of skewness statistics.
* Fix issues in `r2()` for *gam* models.

# performance 0.4.5

## General

* `model_performance()` and `r2()` now support *rma*-objects from package *metafor*, *mlm* and *bife* models.

## Changes to functions

* `compare_performance()` gets a `bayesfactor` argument, to include or exclude the Bayes factor for model comparisons in the output.
* Added `r2.aov()`.

## Bug fixes

* Fixed issue in `performance_aic()` for models from package *survey*, which returned three different AIC values. Now only the AIC value is returned.
* Fixed issue in `check_collinearity()` for *glmmTMB* models when zero-inflated formula only had one predictor.
* Fixed issue in `check_model()` for *lme* models.
* Fixed issue in `check_distribution()` for *brmsfit* models.
* Fixed issue in `check_heteroscedasticity()` for *aov* objects.
* Fixed issues for *lmrob* and *glmrob* objects.

# performance 0.4.4

## General

* Removed `logLik.felm()`, because this method is now implemented in the *lfe* package.
* Support for `DirichletRegModel` models.

## New functions

* `check_itemscale()` to describe various measures of internal consistencies for scales which were built from several items from a PCA, using `parameters::principal_components()`.
* `r2_efron()` to compute Efron's pseudo R2.

## Bug fixes

* Fixed issue in documentation of `performance_score()`.

# performance 0.4.3

## General

* Support for `mixor`, `cpglm` and `cpglmm` models.

## New functions

* `performance_aic()` as a small wrapper that returns the AIC. It is a generic function that also works for some models that don't have a AIC method (like Tweedie models).
* `performance_lrt()` as a small wrapper around `anova()` to perform a Likelihood-Ratio-Test for model comparison.

## Bug fixes

* Fix issues with CRAN checks.

## Changes to functions

* `model_performance()` now calculates AIC for Tweedie models.

# performance 0.4.2

## General

* Support for `bracl`, `brmultinom`, `fixest`, `glmx`, `glmmadmb`, `mclogit`, `mmclogit`, `vgam` and `vglm` models.
* `model_performance()` now supports *plm* models.
* `r2()` now supports *complmrob* models.
* `compare_performance()` now gets a `plot()`-method (requires package **see**).

## Changes to functions

* `compare_performance()` gets a `rank`-argument, to rank models according to their overall model performance.
* `compare_performance()` has a nicer `print()`-method now.
* Verbosity for `compare_performance()` was slightly adjusted.
* `model_performance()`-methods for different objects now also have a `verbose`-argument.

## Minor changes

* `check_collinearity()` now no longer returns backticks in row- and column names.

## Bug fixes

* Fixed issue in `r2()` for `wbm`-models with cross-level interactions.
* `plot()`-methods for `check_heteroscedasticity()` and `check_homogeneity()` now work without requiring to load package *see* before.
* Fixed issues with models of class `rlmerMod`.

# performance 0.4.0

## General

* `performance()` is an alias for `model_performance()`.

## Deprecated and Defunct

* `principal_components()` was removed and re-implemented in the **parameters**-package. Please use `parameters::principal_components()` now.

## Changes to functions

* `check_outliers()` now also works on data frames.
* Added more methods to `check_outliers()`.
* `performance_score()` now also works on `stan_lmer()` and `stan_glmer()` objects.
* `check_singularity()` now works with models of class *clmm*.
* `r2()` now works with models of class *clmm*, *bigglm* and *biglm*.
* `check_overdispersion()` for mixed models now checks that model family is Poisson.

## Bug fixes

* Fixed bug in `compare_performance()` that toggled a warning although models were fit from same data.
* Fixed bug in `check_model()` for *glmmTMB* models that occurred when checking for outliers.

# performance 0.3.0

## General

* Many `check_*()`-methods now get a `plot()`-method. Package **see** is required for plotting.
* `model_performance()` gets a preliminary `print()`-method.

## Breaking changes

* The attribute for the standard error of the Bayesian R2 (`r2_bayes()`) was renamed from `std.error` to `SE` to be in line with the naming convention of other easystats-packages.
* `compare_performance()` now shows the Bayes factor when all compared models are fit from the same data. Previous behaviour was that the BF was shown when models were of same class.

## Changes to functions

* `model_performance()` now also works for *lavaan*-objects.
* `check_outliers()` gets a `method`-argument to choose the method for detecting outliers. Furthermore, two new methods (Mahalanobis Distance and Invariant Coordinate Selection) were implemented.
* `check_model()` now performs more checks for GLM(M)s and other model objects.
* `check_model()` gets a `check`-argument to plot selected checks only.
* `r2_nakagawa()` now returns r-squared for models with singular fit, where no random effect variances could be computed. The r-squared then does not take random effect variances into account. This behaviour was changed to be in line with `MuMIn::r.squaredGLMM()`, which returned a value for models with singular fit.
* `check_distribution()` now detects negative binomial and zero-inflated distributions. Furthermore, attempt to improve accuracy.
* `check_distribution()` now also accepts a numeric vector as input.
* `compare_performance()` warns if models were not fit from same data.

## New check-functions

* `check_homogeneity()` to check models for homogeneity of variances.

## Bug fixes

* Fixed issues with `compare_performance()` and row-ordering.
* Fixed issue in `check_collinearity()` for zero-inflated models, where the zero-inflation component had not enough model terms to calculate multicollinearity.
* Fixed issue in some `check_*()` and `performance_*()` functions for models with binary outcome, when outcome variable was a factor.

# performance 0.2.0

## General

* `r2()` now works for more regression models.
* `r2_bayes()` now works for multivariate response models.
* `model_performance()` now works for more regression models, and also includes the log-loss, proper scoring rules and percentage of correct predictions as new metric for models with binary outcome.

## New performance-functions

* `performance_accuracy()`, which calculates the predictive accuracy of linear or logistic regression models.
* `performance_logloss()` to compute the log-loss of models with binary outcome. The log-loss is a proper scoring function comparable to the `rmse()`.
* `performance_score()` to compute the logarithmic, quadratic and spherical proper scoring rules.
* `performance_pcp()` to calculate the percentage of correct predictions for models with binary outcome.
* `performance_roc()`, to calculate ROC-curves.
* `performance_aicc()`, to calculate the second-order AIC (AICc).

## New check-functions

* `check_collinearity()` to calculate the variance inflation factor and check model predictors for multicollinearity.
* `check_outliers()` to check models for influential observations.
* `check_heteroscedasticity()` to check models for (non-)constant error variance.
* `check_normality()` to check models for (non-)normality of residuals.
* `check_autocorrelation()` to check models for auto-correlated residuals.
* `check_distribution()` to classify the distribution of a model-family using machine learning.

## New indices-functions

* `r2_mckelvey()` to compute McKelvey and Zavoinas R2 value.
* `r2_zeroinflated()` to compute R2 for zero-inflated (non-mixed) models.
* `r2_xu()` as a crude R2 measure for linear (mixed) models.

## Breaking changes

* `model_performance.stanreg()` and `model_performance.brmsfit()` now only return one R2-value and its standard error, instead of different (robust) R2 measures and credible intervals.
* `error_rate()` is now integrated in the `performance_pcp()`-function.

## Changes to functions

* `model_performance.stanreg()` and `model_performance.brmsfit()` now also return the _WAIC_ (widely applicable information criterion).
* `r2_nakagawa()` now calculates the full R2 for mixed models with zero-inflation.
* `icc()` now returns `NULL` and no longer stops when no mixed model is provided.
* `compare_performance()` now shows the Bayes factor when all compared models are of same class.
* Some functions get a `verbose`-argument to show or suppress warnings.

## Bug fixes

* Renamed `r2_coxnell()` to `r2_coxsnell()`.
* Fix issues in `r2_bayes()` and `model_performance()` for ordinal models resp. models with cumulative link (#48).
* `compare_performance()` did not sort the `name`-column properly, if the columns `class` and `name` were not in the same alphabetical order (#51).
# NEWS

## v0.0.2
- Fixes typos in jamovi/jmv functions
- Adds more descriptive errors to jamovi output
- Remove dontrun from examples in documentation
# rvest 0.3.6

* Remove failing example

# rvest 0.3.5

* Use web archive to fix broken example.

# rvest 0.3.4

* Remove unneeded `read_xml.response()` method (#242).

# rvest 0.3.3

* Fix `R CMD check` failure

* `submit_request()` now checks for empty form-field-types to select the
   correct submit fields (@rentrop, #159)

# rvest 0.3.2

* Fixes to `follow_link()` and `back()` to correctly manage session history.

* If you're using xml2 1.0.0, `html_node()` will now return a "missing node".

* Parse rowspans and colspans effectively by filling using repetition from 
  left to right (for colspan) and top to bottom (rowspan) (#111)

* Updated a few examples and demos where the website structure has
  changed.

* Made compatible with both xml2 0.1.2 and 1.0.0.

# rvest 0.3.1

* Fix invalid link for SSA example.

* Parse `<options>` that don't have value attribute (#85).

* Remove all remaining uses of `html()` in favor of `read_html()` 
  (@jimhester, #113).

# rvest 0.3.0

* rvest has been rewritten to take advantage of the new xml2 package. xml2 
  provides a fresh binding to libxml2, avoiding many of the work-arounds 
  previously needed for the XML package. Now rvest depends on the xml2 
  package, so all the xml functions are available, and rvest adds a thin 
  wrapper for html. 
  
* A number of functions have change names. The old versions still work,
  but are deprecated and will be removed in rvest 0.4.0.
  
  * `html_tag()` -> `html_name()`
  * `html()` -> `read_html()`

* `html_node()` now throws an error if there are no matches, and a warning
  if there's more than one match. I think this should make it more likely to
  fail clearly when the structure of the page changes.

* `xml_structure()` has been moved to xml2. New `html_structure()` (also in 
  xml2) highlights id and class attributes (#78).

* `submit_form()` now works with forms that use GET (#66).

* `submit_request()` (and hence `submit_form()`) is now case-insensitive, 
  and so will find `<input type=SUBMIT>` as well as`<input type="submit">`.
  
* `submit_request()` (and hence `submit_form()`) recognizes forms with 
  `<input type="image">` as a valid form submission button.
  
# rvest 0.2.0

## New features

* `html()` and `xml()` pass `...` on to `httr::GET()` so you can more
  finely control the request (#48).

* Add xml support: parse with `xml()`, then work with using `xml_node()`,
  `xml_attr()`, `xml_attrs()`, `xml_text()` and `xml_tag()` (#24).

* `xml_structure()`: new function that displays the structure (i.e. tag
  and attribute names) of a xml/html object (#10).

## Bug fixes

* `follow_link()` now accepts css and xpath selectors. (#38, #41, #42)

* `html()` does a better job of dealing with encodings (passing the
  problem on to `XML::parseHTML()`) instead of trying to do it itself 
  (#25, #50).

* `html_attr()` returns default value when input is NULL (#49)

* Add missing `html_node()` method for session.

* `html_nodes()` now returns an empty list if no elements are found (#31).

* `submit_form()` converts relative paths to absolute URLs (#52).
  It also deals better with 0-length inputs (#29).
# blob 1.2.1

- Inline prettyunits.
- `vec_ptype2.hms.default()` forwards to `vec_default_ptype2()` for compatibility with vctrs 0.2.1.


# blob 1.2.0

## Breaking changes

- The `blob` class is now based on `list_of(raw())` from the vctrs package (#11). This adds support for `vec_cast()` and `vec_ptype2()`. Some operations (such as subset assignment) are now stricter. The `new_blob()` constructor permits safe and fast construction of `blob` objects from a list, and `validate_blob()` checks an existing object for conformity with the rules.

- The new `is_blob()` deprecates the existing `is.blob()`. `as.blob()` is deprecated in favor of `vec_cast()` or the new `as_blob()` (which is just a thin wrapper around `vec_cast()`).

- Indexing a vector of blobs out of bounds now raises an error. Use `NA` as index to create a `NULL` blob.


# blob 1.1.1 (2018-03-24)

- Now suggesting *pillar* instead of importing *tibble*, and using colored
  formatting with the *prettyunits* package with `B` instead of `b` as units
  (#7, #9).

- The blob class can now be used for S4 dispatch.

- Calling `c()` on blob objects returns a blob.


# blob 1.1.0 (2017-06-17)

- New maintainer: Kirill Müller.

- Added `as.blob.blob()`and `as.data.frame.blob()` methods (#3).

- Size of very large blobs is displayed correctly.


# blob 1.0.0

- Initial release.
---
title: 'SimplyAgree: An R package and jamovi Module for Simplifying Agreement and Reliability Analyses'
tags:
  - R
  - statistics
  - reliability
  - agreement
  - Bland-Altman
authors:
  - name: Aaron R. Caldwell
    orcid: 0000-0002-4541-6283
    affiliation: "1, 2" # (Multiple affiliations must be quoted)
affiliations:
 - name: United States Army Research Institute of Environmental Medicine, Natick, MA, the United States of America
   index: 1
 - name: Oak Ridge Institute of Science and Education, Oak Ridge, TN, the United States of America
   index: 2
citation_author: Caldwell
year: 2021
bibliography: paper.bib
csl: apa.csl
journal: JOSS
---

# Summary

Accurate and reliable measurements are critical to quantitative research efforts. Based on citation counts of @bland1986 and @weir2005 alone^[@bland1986 and @weir2005 have greater than 50000 and 4000 citations, respectively, at the time of writing this manuscript. Many of these publications are original research investigations.], researchers appear to highly value methods to quantify the accuracy and reliability of measurement tools used for research. This article introduces the `SimplyAgree` R package and jamovi module as user-friendly solutions for estimating agreement and reliability [@R-base; @jamovi] for continuous measurements. Updates and additional details on `SimplyAgree` can be found on the package's [website](https://aaroncaldwell.us/SimplyAgree). `SimplyAgree` was created for applied physiologists to use when evaluating different physiological measurements (e.g, comparing the measurement of oxygen consumption between two competing gas analyzers). However, the functions within `SimplyAgree` can be utilized by any researcher wanting to evaluate agreement between two continuous measurements, or for evaluating the reliability of a continuous measure^[Discrete and categorical measurements are not supported at the time of writing this manuscript].

# Statement of Need

A number of new methods have been developed in the past three decades to improve the calculation of the limits of agreement [@shieh2019;  @lin1989; @zou2011] and other measures of measurement reliability [@weir2005; @carrasco2013]. However, to the author's best knowledge, statistical software &mdash; particularly open source software &mdash; to implement these statistical analyses are lacking. While some software may provide the limits of agreement analysis outlined by Bland & Altman [-@bland1986; -@bland1999], few, if any, account for multiple observations within the same research subject [@zou2011] or include hypothesis tests of agreement [@shieh2019]. `blandr` [@blandr] and `BlandAltmanLeh` [@BlandAltmanLeh] exist to aid the creation of Bland-Altman type plots, but do not offer the tests detailed by @shieh2019 or @zou2011. `MethComp` [@MethComp] is the most comprehensive with regards to limits of agreement but also lacks the methods outlined by @shieh2019. The `cccrm` package [@carrasco2013] does already provide concordance correlation coefficients as output, and is the basis of the concordance calculations within `SimplyAgree`. Lastly, `psych` [@psych] and `ICC` [@ICC] provide some measures of reliability but lack the ability to calculate standard error of the measurement. Moreover, only `blandr` has a jamovi module but the capabilities are limited to just the traditional Bland-Altman limits. 

`SimplyAgree` is also created to be easy to use for those with limited programming experience. The output from the functions is intentionally verbose and typically provides more than 1 measure of agreement or reliability. The hope is that researchers who use these functions will not limit their interpretations of the results to 1 numeric value, but instead look at the totality of the results (estimates and confidence intervals) for their interpretation. 

Additionally, many researchers may not have the skills necessary to write the code, from scratch, in order to implement many of the newest techniques. @jamovi is a open source statistical platform that provides a graphical user interface (GUI), and therefore is an accessible source for researchers, or even students, without coding experience. Therefore, a jamovi module of `SimplyAgree` was also created in order to reach those researchers who may not have the coding expertise required to effectively use the R package.

# Current R Capabilities

The R package `SimplyAgree`, currently v0.0.2 on the comprehensive R archive network (CRAN), implements a number of useful agreement and reliability analyses.

The current release of the R package can be downloaded directly from CRAN in R:

```
install.packages("SimplyAgree")
```

Or, the developmental version, can be downloaded from GitHub:

```
devtools::install_github("arcaldwell49/SimplyAgree")
```
There are 2 vignettes that document the major functions within the package that can be found on the package's website (https://aaroncaldwell.us/SimplyAgree). Overall, there are 6 fundamental functions, all with generic `plot` and `print` methods, within the R package:

1. `agree_test`: Simple Test of Agreement. This is function performs agreement analyses on two vectors of the same length, and is designed for analyses that were described by Bland & Altman [-@bland1986; -@bland1999]. In addition to providing the traditional Bland-Altman limits of agreement, the function provides a hypothesis test [@shieh2019], and provides the concordance correlation coefficient [@lin1989].

2. `agree_reps`: Test of Agreement for Replicate Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, does not vary. In addition, the concordance correlation coefficient, calculated by U-statistics, is also provided in the output [@carrasco2013].

3. `agree_nest`: Test of Agreement for Nested Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, *does* vary. Similar to the replicate data function, the concordance correlation coefficient, calculated by U-statistics, is provided in the output [@carrasco2013].

4. `loa_mixed`: Bootstrapped Limits of Agreement for Nested Data. This function calculates limits of agreement using a non-parametric bootstrap method, and can allow the underlying mean to vary (replicate data) or not (nested data).

5. `blandPowerCurve`: Power Analysis for Bland-Altman Limits of Agreement. This function implements the formula outlined by @lu2016. This allows for power calculations for the @bland1999 limits of agreement. The function `find_n` can then be used to find the sample size at which adequate power (defined by the user) is achieved.

6. `reli_stats`: Reliability Statistics. This function calculates and provides as output the statistics outlined by @weir2005. This includes an array of intraclass correlation coefficients, the coefficient of variation, and the standard error of measurement.


# Current jamovi Capabilities

The jamovi module can be added to the jamovi directly from the "add module" tab in the GUI.

![How to add a module in jamovi.](module_button.PNG)

The `SimplyAgree` module is then available on the main menu, and within it there are three analysis options.

![SimplyAgree in jamovi.](simplyagree_button.PNG)

The three analysis options essentially enable jamovi users to complete some of the same analyses available in the R package.

1. The simple agreement analysis incorporates the `agree_test` function. Users have the option of including the concordance correlation coefficient, and plots of the data.

![Sample Output from the Simple Agreement Analysis.](simple_agreement.PNG)

2. The nested/replicate agreement analysis uses the `agree_nest` and `agree_reps` function to perform the analyses. The `agree_reps` function is used if "Assume underlying value does not vary?" is selected; otherwise `agree_nest` is used.

![Sample Output from the Nested/Replicate Agreement Analysis.](nested_agreement.PNG)


3. The reliability analysis utilizes `reli_stats` to calculate reliability statistics.

![Sample Output from the Reliability Analsyis.](reliability.PNG)


# Acknowledgements

I would like the thank Ashley Akerman for his kind feedback during the development of `SimplyAgree`. 

The opinions or assertions contained herein are the private views of the author and are not to be construed as official or reflecting the views of the Army or the Department of Defense. Any citations of commercial organizations and trade names in this report do not constitute an official Department of the Army endorsement of approval of the products or services of these organizations. No authors have any conflicts of interest to disclose. Approved for public release; distribution is unlimited.

# References
---
# Example from https://joss.readthedocs.io/en/latest/submitting.html
title: 'SimplyAgree: An R package and jamovi Module for Simplifying Agreement and Reliability Analyses'
tags:
  - R
  - statistics
  - reliability
  - agreement
  - Bland-Altman
authors:
  - name: Aaron R. Caldwell
    orcid: 0000-0002-4541-6283
    affiliation: "1, 2" # (Multiple affiliations must be quoted)
affiliations:
 - name: United States Army Research Institute of Environmental Medicine
   index: 1
 - name: Oak Ridge Institute of Science and Education
   index: 2
citation_author: Caldwell
year: 2021
bibliography: paper.bib
csl: apa.csl
journal: JOSS
---

# Summary

Accurate and reliable measurements are critical to quantitative research efforts. Based on citation counts, researchers highly value methods to quantify the accuracy and reliability of the measurement tools [@bland1986; @weir2005]. This article introduces the `SimplyAgree` R package and jamovi module as user-friendly solutions to estimating agreement and reliability [@R-base; @jamovi].

# Statement of Need

A number of new methods have been developed in the past three decades to improve the calculation of the limits of agreement [@shieh2019;  @lin1989; @zou2011] and other measures of measurement reliability [@weir2005; @carrasco2013]. However, to author's best knowledge, statistical software &mdash; particularly open source software &mdash; to implement these tools is lacking. While some software may provide the agreement analysis outlined by Bland & Altman [-@bland1986; -@bland1999], few, if any, account account for multiple observations within the same research subject [@zou2011] or include hypothesis tests of agreement [@shieh2019]. Many researchers may not have the skills necessary to write statistical programming code in order to implement many of the newest techniques. @jamovi is a open source statistical platform that provides a graphical user interface (GUI), and therefore is an accessible source for researchers without coding experience. Therefore, a jamovi module of `SimplyAgree` was also created in order to reach those researchers who may not have the coding expertise required to effectively use the R package.

# Current R Capabilities

The R package `SimplyAgree`, currently v0.0.2 on the comprehensive R archive network (CRAN), implements a number of useful agreement and reliability analyses that may be useful for researchers to evaluate their quantitative measurements.

The current release of the R package can be downloaded directly from CRAN in R:

```
install.packages("SimplyAgree")
```

Or, the developmental version, can be downloaded from GitHub:

```
devtools::install_github("arcaldwell49/SimplyAgree")
```
There are 2 vignettes that document the major functions within the package that can be found on the package's website (https://aaroncaldwell.us/SimplyAgree). Overall, there are 6 fundamental functions within the R package:

1. `agree_test`: Simple Test of Agreement. This is function performs agreement analyses on two vectors of the same length, and is designed for analyses that were described by Bland & Altman [-@bland1986; -@bland1999]. In addition to providing the traditional Bland-Altman limits of agreement, the function provides a hypothesis test [@shieh2019], and provides the concordance correlation coefficient [@lin1989].

2. `agree_reps`: Test of Agreement for Replicate Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, does not vary. In addition, the concordance correlation coefficient, calculated by U-statistics, is also provided in the output [@carrasco2013].

3. `agree_nest`: Test of Agreement for Nested Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, *does* vary. Similar to the replicate data function, the concordance correlation coefficient, calculated by U-statistics, is provided in the output [@carrasco2013].

4. `loa_mixed`: Bootstrapped Limits of Agreement for Nested Data. This function calculates limits of agreement using a non-parametric bootstrap method, and can allow the underlying mean to vary (replicate data) or not (nested data).

5. `blandPowerCurve`: Power Analysis for Bland-Altman Limits of Agreement. This function implements the formula outlined by @lu

6. `reli_stats`: Reliability Statistics.


# Current jamovi Capabilities

The jamovi module can be added to the jamovi directly from the "add module" tab in the GUI.

![**Figure 1**: How to add a module in jamovi.](module_button.PNG)

The `SimplyAgree` module is then available on the main menu, and within it there are three analysis options.

![**Figure 2**: SimplyAgree in jamovi.](simplyagree_button.PNG)

The three analysis options essentially enable jamovi users to complete some of the same analyses available in the R package.

1.

![**Figure 3**: Sample Output from the Simple Agreement Analysis.](simple_agreement.PNG)

2. 

![**Figure 4**:Sample Output from the Nested/Replicate Agreement Analysis.](nested_agreement.PNG)


3.

![**Figure 5**: Sample Output from the Reliability Analsyis.](reliability.PNG)


# Acknowledgements

I would like the thank Ashley Akerman for his kind feedback during the development of this package. 

The opinions or assertions contained herein are the private views of the author and are not to be construed as official or reflecting the views of the Army or the Department of Defense. Any citations of commercial organizations and trade names in this report do not constitute an official Department of the Army endorsement of approval of the products or services of these organizations. No authors have any conflicts of interest to disclose. Approved for public release; distribution is unlimited.

# References
---
title: "SimplyAgree R Package"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(SimplyAgree)
#![Sticker](https://raw.githubusercontent.com/arcaldwell49/SimplyAgree/master/images/sticker.png)
```


<img src="https://raw.githubusercontent.com/arcaldwell49/SimplyAgree/master/images/sticker.png" width="150" height="150" />

*Artwork courtesy of Chelsea Parlett Pelleriti*

<!-- badges: start -->
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04148/status.svg)](https://doi.org/10.21105/joss.04148)
[![Codecov test coverage](https://codecov.io/gh/arcaldwell49/SimplyAgree/branch/master/graph/badge.svg)](https://app.codecov.io/gh/arcaldwell49/SimplyAgree?branch=master)
[![R-CMD-check](https://github.com/arcaldwell49/SimplyAgree/workflows/R-CMD-check/badge.svg)](https://github.com/arcaldwell49/SimplyAgree/actions)
[![documentation](https://img.shields.io/badge/website-active-blue)](https://aaroncaldwell.us/SimplyAgree/)
<!-- badges: end -->

Please see the package's [website](https://aaroncaldwell.us/SimplyAgree/) for updates, vignettes, and other details about the package.

# Background

`SimplyAgree` is an R package, and [jamovi](https://www.jamovi.org/) module, created to make agreement and reliability analyses easier for the average researcher. The functions within this package include simple tests of agreement (`agree_test`), agreement analysis for nested (`agree_nest`) and replicate data (`agree_reps`), and provide robust analyses of reliability (`reli_stats`). In addition, this package contains a set of functions to help when planning studies looking to assess measurement agreement (`blandPowerCurve`).

## Installing SimplyAgree

You can install the most up-to-date version of `SimplyAgree` from
[GitHub](https://github.com/arcaldwell49/SimplyAgree) with:

``` r
devtools::install_github("arcaldwell49/SimplyAgree")
```

# Contributing

We are happy to receive bug reports, suggestions, questions, and (most
of all) contributions to fix problems and add features. Pull Requests
for contributions are encouraged.

Here are some simple ways in which you can contribute (in the increasing
order of commitment):

-   Read and correct any inconsistencies in the documentation
-   Raise issues about bugs or wanted features
-   Review code
-   Add new functionality


## Code of Conduct

Please note that the concurve project is released with a [Contributor
Code of
Conduct](https://aaroncaldwell.us/SimplyAgree/CODE_OF_CONDUCT.html). By
contributing to this project, you agree to abide by its terms.

# References

The functions in this package are largely based on the following works:

Lin L (1989). A concordance correlation coefficient to evaluate reproducibility. *Biometrics* 45: 255 - 268. <https://doi.org/10.2307/2532051>

Shieh, G. (2019). Assessing agreement between two methods of quantitative measurements: Exact test procedure and sample size calculation. *Statistics in Biopharmaceutical Research*, 1-8. <https://doi.org/10.1080/19466315.2019.1677495>

Parker, R. A., et al (2016). Application of mixed effects limits of agreement in the presence of multiple sources of variability: exemplar from the comparison of several devices to measure respiratory rate in COPD patients. Plos one, 11(12), e0168321. <https://doi.org/10.1371/journal.pone.0168321>

Zou, G. Y. (2013). Confidence interval estimation for the Bland–Altman limits of agreement with multiple observations per individual. *Statistical methods in medical research*, 22(6), 630-642. <https://doi.org/10.1177/0962280211402548>

Weir, J. P. (2005). Quantifying test-retest reliability using the intraclass correlation coefficient and the SEM. *The Journal of Strength & Conditioning Research*, 19(1), 231-240.

Lu, Meng-Jie, et al (2016). "Sample Size for Assessing Agreement between Two Methods of Measurement by Bland−Altman Method" *The International Journal of Biostatistics*, 12(2),  <https://doi.org/10.1515/ijb-2015-0039>

King, TS and Chinchilli, VM. (2001). A generalized concordance correlation coefficient for continuous and categorical data. *Statistics in Medicine*, 20, 2131:2147.

King, TS, Chinchilli, VM, and Carrasco, JL. (2007). A repeated measures concordance correlation coefficient. *Statistics in Medicine*, 26, 3095:3113.

Carrasco, JL, et al. (2013). Estimation of the concordance correlation coefficient for repeated measures using SAS and R. *Computer Methods and Programs in Biomedicine*, 109, 293-304.
---
title: "Best practices for API packages"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Best practices for API packages}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(httr)
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

So you want to write an R client for a web API? This document walks through the key issues involved in writing API wrappers in R. If you're new to working with web APIs, you may want to start by reading "[An introduction to APIs](https://zapier.com/learn/apis)" by zapier.

## Overall design

APIs vary widely. Before starting to code, it is important to understand how the API you are working with handles important issues so that you can implement a complete and coherent R client for the API. 

The key features of any API are the structure of the requests and the structure of the responses. An HTTP request consists of the following parts:

1. HTTP verb (`GET`, `POST`, `DELETE`, etc.)
1. The base URL for the API
1. The URL path or endpoint
1. URL query arguments (e.g., `?foo=bar`)
1. Optional headers
1. An optional request body

An API package needs to be able to generate these components in order to perform the desired API call, which will typically involve some sort of authentication. 

For example, to request that the GitHub API provides a list of all issues for the httr repo, we send an HTTP request that looks like:

```
-> GET /repos/hadley/httr HTTP/1.1
-> Host: api.github.com
-> Accept: application/vnd.github.v3+json
```

Here we're using a `GET` request to the host `api.github.com`. The url is `/repos/hadley/httr`, and we send an accept header that tells GitHub what sort of data we want.

In response to this request, the API will return an HTTP response that includes:

1. An HTTP status code.
1. Headers, key-value pairs.
1. A body typically consisting of XML, JSON, plain text, HTML, 
   or some kind of binary representation. 
   
An API client needs to parse these responses, turning API errors into R errors, and return a useful object to the end user. For the previous HTTP request, GitHub returns:

```
<- HTTP/1.1 200 OK
<- Server: GitHub.com
<- Content-Type: application/json; charset=utf-8
<- X-RateLimit-Limit: 5000
<- X-RateLimit-Remaining: 4998
<- X-RateLimit-Reset: 1459554901
<- 
<- {
<-   "id": 2756403,
<-   "name": "httr",
<-   "full_name": "hadley/httr",
<-   "owner": {
<-     "login": "hadley",
<-     "id": 4196,
<-     "avatar_url": "https://avatars.githubusercontent.com/u/4196?v=3",
<-     ...
<-   },
<-   "private": false,
<-   "html_url": "https://github.com/hadley/httr",
<-   "description": "httr: a friendly http package for R",
<-   "fork": false,
<-   "url": "https://api.github.com/repos/hadley/httr",
<-   ...
<-   "network_count": 1368,
<-   "subscribers_count": 64
<- }
```

Designing a good API client requires identifying how each of these API features is used to compose a request and what type of response is expected for each. It's best practice to insulate the end user from *how* the API works so they only need to understand how to use an R function, not the details of how APIs work. It's your job to suffer so that others don't have to!

## First steps

### Send a simple request

First, find a simple API endpoint that doesn't require authentication: this lets you get the basics working before tackling the complexities of authentication. For this example, we'll use the list of httr issues which requires sending a GET request to `repos/hadley/httr`:

```{R}
library(httr)
github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  GET(url)
}

resp <- github_api("/repos/hadley/httr")
resp
```

### Parse the response

Next, you need to take the response returned by the API and turn it into a useful object. Any API will return an HTTP response that consists of headers and a body. While the response can come in multiple forms (see above), two of the most common structured formats are XML and JSON. 

Note that while most APIs will return only one or the other, some, like the colour lovers API, allow you to choose which one with a url parameter:

```{r}
GET("http://www.colourlovers.com/api/color/6B4106?format=xml")
GET("http://www.colourlovers.com/api/color/6B4106?format=json")
```

Others use [content negotiation](http://en.wikipedia.org/wiki/Content_negotiation) to determine what sort of data to send back. If the API you're wrapping does this, then you'll need to include one of `accept_json()` and `accept_xml()` in your request. 

If you have a choice, choose json: it's usually much easier to work with than xml.

Most APIs will return most or all useful information in the response body, which can be accessed using `content()`. To determine what type of information is returned, you can use `http_type()`

```{r}
http_type(resp)
```

I recommend checking that the type is as you expect in your helper function. This will ensure that you get a clear error message if the API changes:

```{r}
github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  resp
}
```

NB: some poorly written APIs will say the content is type A, but it will actually be type B. In this case you should complain to the API authors, and until they fix the problem, simply drop the check for content type.

Next we need to parse the output into an R object. httr provides some default parsers with `content(..., as = "auto")` but I don't recommend using them inside a package. Instead it's better to explicitly parse it yourself:

1. To parse json, use `jsonlite` package.
1. To parse xml, use the `xml2` package. 

```{r}
github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
}
```

### Return a helpful object

Rather than simply returning the response as a list, I think it's a good practice to make a simple S3 object. That way you can return the response and parsed object, and provide a nice print method. This will make debugging later on much much much more pleasant.

```{r}
github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
  
  structure(
    list(
      content = parsed,
      path = path,
      response = resp
    ),
    class = "github_api"
  )
}

print.github_api <- function(x, ...) {
  cat("<GitHub ", x$path, ">\n", sep = "")
  str(x$content)
  invisible(x)
}

github_api("/users/hadley")
```

The API might return invalid data, but this should be rare, so you can just rely on the parser to provide a useful error message.

### Turn API errors into R errors

Next, you need to make sure that your API wrapper throws an error if the request failed. Using a web API introduces additional possible points of failure into R code aside from those occurring in R itself. These include:

- Client-side exceptions
- Network / communication exceptions
- Server-side exceptions

You need to make sure these are all converted into regular R errors. You can figure out if there's a problem with `http_error()`, which checks the HTTP status code. Status codes in the 400 range usually mean that you've done something wrong. Status codes in the 500 range typically mean that something has gone wrong on the server side.

Often the API will provide information about the error in the body of the response: you should use this where available. If the API returns special errors for common problems, you might want to provide more detail in the error. For example, if you run out of requests and are [rate limited](https://developer.github.com/v3/#rate-limiting) you might want to tell the user how long to wait until they can make the next request (or even automatically wait that long!).

```{r, error = TRUE}
github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  
  resp <- GET(url)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
  
  if (http_error(resp)) {
    stop(
      sprintf(
        "GitHub API request failed [%s]\n%s\n<%s>", 
        status_code(resp),
        parsed$message,
        parsed$documentation_url
      ),
      call. = FALSE
    )
  }
  
  structure(
    list(
      content = parsed,
      path = path,
      response = resp
    ),
    class = "github_api"
  )
}
github_api("/user/hadley")
```

> Some poorly written APIs will return different types of response based on 
> whether or not the request succeeded or failed. If your API does this you'll 
> need to make your request function check the `status_code()` before parsing 
> the response.

For many APIs, the common approach is to retry API calls that return something in the 500 range. However, when doing this, it's **extremely** important to make sure to do this with some form of exponential backoff: if something's wrong on the server-side, hammering the server with retries may make things worse, and may lead to you exhausting quota (or hitting other sorts of rate limits). A common policy is to retry up to 5 times, starting at 1s, and each time doubling and adding a small amount of jitter (plus or minus up to, say, 5% of the current wait time). 

### Set a user agent

While we're in this function, there's one important header that you should set for every API wrapper: the user agent. The user agent is a string used to identify the client. This is most useful for the API owner as it allows them to see who is using the API. It's also useful for you if you have a contact on the inside as it often makes it easier for them to pull your requests from their logs and see what's going wrong. If you're hitting a commercial API, this also makes it easier for internal R advocates to see how many people are using their API via R and hopefully assign more resources.

A good default for an R API package wrapper is to make it the URL to your GitHub repo:

```{r}
ua <- user_agent("http://github.com/hadley/httr")
ua

github_api <- function(path) {
  url <- modify_url("https://api.github.com", path = path)
  
  resp <- GET(url, ua)
  if (http_type(resp) != "application/json") {
    stop("API did not return json", call. = FALSE)
  }
  
  parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
  
  if (status_code(resp) != 200) {
    stop(
      sprintf(
        "GitHub API request failed [%s]\n%s\n<%s>", 
        status_code(resp),
        parsed$message,
        parsed$documentation_url
      ),
      call. = FALSE
    )
  }
  
  structure(
    list(
      content = parsed,
      path = path,
      response = resp
    ),
    class = "github_api"
  )
}
```

### Passing parameters

Most APIs work by executing an HTTP method on a specified URL with some additional parameters. These parameters can be specified in a number of ways, including in the URL path, in URL query arguments, in HTTP headers, and in the request body itself.  These parameters can be controlled using httr functions:

1. URL path: `modify_url()`
2. Query arguments: The `query` argument to `GET()`, `POST()`, etc.
3. HTTP headers: `add_headers()` 
4. Request body: The `body` argument to `GET()`, `POST()`, etc.

[RESTful APIs](https://en.wikipedia.org/wiki/Representational_state_transfer) also use the HTTP verb to communicate arguments (e.g., `GET` retrieves a file, `POST` adds a file, `DELETE` removes a file, etc.). We can use the helpful [httpbin service](http://httpbin.org) to show how to send arguments in each of these ways.

```{r, eval = FALSE}
# modify_url
POST(modify_url("https://httpbin.org", path = "/post"))

# query arguments
POST("http://httpbin.org/post", query = list(foo = "bar"))

# headers
POST("http://httpbin.org/post", add_headers(foo = "bar"))

# body
## as form
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "form")
## as json
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "json")
```

Many APIs will use just one of these forms of argument passing, but others will use multiple of them in combination. Best practice is to insulate the user from how and where the various arguments are used by the API and instead simply expose relevant arguments via R function arguments, some of which might be used in the URL, in the headers, in the body, etc.

If a parameter has a small fixed set of possible values that are allowed by the API, you can use list them in the default arguments and then use `match.arg()` to ensure that the caller only supplies one of those values. (This also allows the user to supply the short unique prefixes.)

```{r}
f <- function(x = c("apple", "banana", "orange")) {
  match.arg(x)
}
f("a")
```

It is good practice to explicitly set default values for arguments that are not required to `NULL`. If there is a default value, it should be the first one listed in the vector of allowed arguments. 

## Authentication

Many APIs can be called without any authentication (just as if you called them in a web browser). However, others require authentication to perform particular requests or to avoid rate limits and other limitations. The most common forms of authentication are OAuth and HTTP basic authentication:

1. *"Basic" authentication*: This requires a username and password (or 
    sometimes just a username). This is passed as part of the HTTP request. 
    In httr, you can do: 
    `GET("http://httpbin.org", authenticate("username", "password"))`

2.  *Basic authentication with an API key*: An alternative provided by many APIs 
    is an API "key" or "token" which is passed as part of the request. It is 
    better than a username/password combination because it can be 
    regenerated independent of the username and password. 
    
    This API key can be specified in a number of different ways: in a URL query
    argument, in an HTTP header such as the `Authorization` header, or in an
    argument inside the request body.
    
3. *OAuth*: OAuth is a protocol for generating a user- or session-specific
    authentication token to use in subsequent requests. (An early standard, 
    OAuth 1.0, is not terribly common any more. See `oauth1.0_token()` for 
    details.) The current OAuth 2.0 standard is very common in modern web apps. 
    It involves a round trip between the client and server to establish if the 
    API client has the authority to access the data. See `oauth2.0_token()`. 
    It's ok to publish the app ID and app "secret" - these are not actually
    important for security of user data. 

> Some APIs describe their authentication processes inaccurately, so
> care needs to be taken to understand the true authentication mechanism
> regardless of the label used in the API docs.

It is possible to specify the key(s) or token(s) required for basic or OAuth authentication in a number of different ways. You may also need some way to preserve user credentials between function calls so that end users do not need to specify them each time. A good start is to use an environment variable. Here is an example of how to write a function that checks for the presence of a GitHub personal access token and errors otherwise:

```{r}
github_pat <- function() {
  pat <- Sys.getenv('GITHUB_PAT')
  if (identical(pat, "")) {
    stop("Please set env var GITHUB_PAT to your github personal access token",
      call. = FALSE)
  }

  pat
}
```

## Pagination (handling multi-page responses)

One particularly frustrating aspect of many APIs is dealing with paginated responses. This is common in APIs that offer search functionality and have the potential to return a very large number of responses. Responses might be paginated because there is a large number of response elements or because elements are updated frequently. Often they will be sorted by an explicit or implicit argument specified in the request. 

When a response is paginated, the API response will typically respond with a header or value specified in the body that contains one of the following:

1. The total number of pages of responses
2. The total number of response elements (with multiple elements per page)
3. An indicator for whether any further elements or pages are available.
4. A URL containing the next page

These values can then be used to make further requests. This will either involve specifying a specific page of responses or specifying a "next page token" that returns the next page of results. How to deal with pagination is a difficult question and a client could implement any of the following:

1. Return one page only by default with an option to return additional specific pages
2. Return a specified page (defaulting to 1) and require the end user to handle pagination
3. Return all pages by writing an internal process of checking for further pages and combining the results

The choice of which to use depends on your needs and goals and the rate limits of the API.

### Rate limiting

Many APIs are rate limited, which means that you can only send a certain number of requests per hour. Often if your request is rate limited, the error message will tell you how long you should wait before performing another request. You might want to expose this to the user, or even include a call to `Sys.sleep()` that waits long enough.

For example, we could implement a `rate_limit()` function that tells you how many calls against the github API are available to you.

```{r}
rate_limit <- function() {
  github_api("/rate_limit")
}
rate_limit()
```

After getting the first version working, you'll often want to polish the output to be more user friendly. For this example, we can parse the unix timestamps into more useful date types.

```{r}
rate_limit <- function() {
  req <- github_api("/rate_limit")
  core <- req$content$resources$core

  reset <- as.POSIXct(core$reset, origin = "1970-01-01")
  cat(core$remaining, " / ", core$limit,
    " (Resets at ", strftime(reset, "%H:%M:%S"), ")\n", sep = "")
}

rate_limit()
```
---
title: "Managing secrets"
author: "Hadley Wickham"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Managing secrets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
library(httr)
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

## Introduction

This document gives you the basics on securely managing secrets. Most of this document is not directly related to httr, but it's common to have some secrets to manage whenever you are using an API.

What is a secret? Some secrets are short alphanumeric sequences:

* Passwords are clearly secrets, e.g. the second argument to `authenticate()`.
  Passwords are particularly important because people (ill-advisedly) often 
  use the same password in multiple places.

* Personal access tokens (e.g. [github][github-token]) should be kept secret:
  they are basically equivalent to a user name password combination, but 
  are slightly safer because you can have multiple tokens for different 
  purposes and it's easy to invalidate one token without affecting the others.

Surprisingly, the "client secret" in an `oauth_app()` is __not__ a secret. It's not equivalent to a password, and if you are writing an API wrapper package, it should be included in the package. (If you don't believe me, here are [google's comments on the topic][google-secret].)

Other secrets are files:

* The JSON web token (jwt) used for server-to-server OAuth 
  (e.g. [google][google-server]) is a secret because it's equivalent to a
  personal access token.

* The `.httr-oauth` file is a secret because it stores OAuth access tokens.

The goal of this vignette is to give you the tools to manage these secrets in a secure way. We'll start with best practices for managing secrets locally, then talk about sharing secrets with selected others (including travis), and finish with the challenges that CRAN presents.

Here, I assume that the main threat is accidentally sharing your secrets when you don't want to. Protecting against a committed attacker is much harder. And if someone has already hacked your computer to the point where they can run code, there's almost nothing you can do. If you're concerned about those scenarios, you'll need to take a more comprehensive approach that's outside the scope of this document.

## Locally

Working with secret files locally is straightforward because it's ok to store them in your project directory as long as you take three precautions:

* Ensure the file is only readable by you, not by any other user on the 
  system. You can use the R function `Sys.chmod()` to do so:

    ```{r, eval = FALSE}
    Sys.chmod("secret.file", mode = "0400")
    ```
    
    It's good practice to verify this setting by examining the file metadata 
    with your local filesystem GUI tools or commands.

* If you use git: make sure the files are listed in `.gitignore` so they don't
  accidentally get included in a public repository.
  
* If you're making a package: make sure they are listed in `.Rbuildignore` 
  so they don't accidentally get included in a public R package.
  
httr proactively takes all of these steps for you whenever it creates a `.httr-oauth` file.

The main remaining risk is that you might share the entire directory (i.e. zipping and emailing, or in a public dropbox directory). If you're worried about this scenario, store your secret files outside of the project directory. If you do this, make sure to provide a helper function to locate the file and provide an informative message if it's missing.

```{r}
my_secrets <- function() {
  path <- "~/secrets/secret.json"
  if (!file.exists(path)) {
    stop("Can't find secret file: '", path, "'")
  }
  
  jsonlite::read_json(path)
}
```

Storing short secrets is harder because it's tempting to record them as a variable in your R script. This is a bad idea, because you end up with a file that contains a mix of secret and public code. Instead, you have three options:

* Ask for the secret each time.
* Store in an environment variable.
* Use the keyring package. 

Regardless of how you store them, to use your secrets you will still need to read them into R variables. Be careful not to expose them by printing them or saving them to a file.

### Ask each time

For scripts that you only use every now and then, a simple solution is to simply ask for the password each time the script is run. If you use RStudio an easy and secure way to request a password is with the rstudioapi package:

```{r, eval = FALSE}
password <- rstudioapi::askForPassword()
```

If you don't use RStudio, use a more general solution like the [getPass](https://github.com/wrathematics/getPass) package.

You should __never__ type your password into the R console: this will typically be stored in the `.Rhistory` file, and it's easy to accidentally share without realising it.

### Environment variables

Asking each time is a hassle, so you might want to store the secret across sessions. One easy way to do that is with environment variables. Environment variables, or __envvars__ for short, are a cross platform way of passing information to processes.

For passing envvars to R, you can list name-value pairs in a file called `.Renviron` in your home directory. The easiest way to edit it is to run:

```{r, eval = FALSE}
file.edit("~/.Renviron")
```

The file looks something like

```
VAR1 = value1
VAR2 = value2
```

```{r, include = FALSE}
Sys.setenv("VAR1" = "value1")
```

And you can access the values in R using `Sys.getenv()`:

```{r}
Sys.getenv("VAR1")
```

Note that `.Renviron` is only processed on startup, so you'll need to restart R to see changes.

These environment variables will be available in every running R process, and can easily be read by any other program on your computer to access that file directly. For more security, use the keyring package. 

### Keyring

The [keyring](https://github.com/r-lib/keyring) package provides a way to store (and retrieve) data in your OS's secure secret store. Keyring has a simple API:

```{r, eval = FALSE}
keyring::key_set("MY_SECRET")
keyring::key_get("MY_SECRET")
```

By default, keyring will use the system keyring. This is unlocked by default when you log in, which means while the password is stored securely pretty much any process can access it.

If you want to be even more secure, you can create custom keyring and keep it locked. That will require you to enter a password every time you want to access your secret. 

```{r, eval = FALSE}
keyring::keyring_create("httr")
keyring::key_set("MY_SECRET", keyring = "httr")
```

Note that accessing the key always unlocks the keyring, so if you're being really careful, make sure to lock it again afterwards.

```{r, eval = FALSE}
keyring::keyring_lock("httr")
```

You might wonder if we've actually achieved anything here because we still need to enter a password! However, that one password lets you access every secret, and you can control how often you need to re-enter it by manually locking and unlocking the keyring.

## Sharing with others

By and large, managing secrets on your own computer is straightforward. The challenge comes when you need to share them with selected others:

* You may need to share a secret with me so that I can run your reprex
  and figure out what is wrong with httr.
  
* You might want to share a secret amongst a group of developers all working
  on the same GitHub project.
  
* You might want to automatically run authenticated tests on travis.

To make this work, all the techniques in this section rely on __public key cryptography__. This is a type of asymmetric encryption where you use a public key to produce content that can only be decrypted by the holder of the matching private key.

### Reprexes

The most common place you might need to share a secret is to generate a reprex. First, do everything you can do eliminate the need to share a secret:

* If it is an http problem, make sure to run all requests with `verbose()`.
* If you get an R error, make sure to include `traceback()`.

If you're lucky, that will be sufficient information to fix the problem.

Otherwise, you'll need to encrypt the secret so you can share it with me. The easiest way to do so is with the following snippet:

```{r, eval = FALSE}
library(openssl)
library(jsonlite)
library(curl)

encrypt <- function(secret, username) {
  url <- paste("https://api.github.com/users", username, "keys", sep = "/")

  resp <- httr::GET(url)
  httr::stop_for_status(resp)
  pubkey <- httr::content(resp)[[1]]$key

  opubkey <- openssl::read_pubkey(pubkey)
  cipher <- openssl::rsa_encrypt(charToRaw(secret), opubkey)
  jsonlite::base64_enc(cipher)
}
  
cipher <- encrypt("<username>\n<password>", "hadley")
cat(cipher)
```

Then I can run the following code on my computer to access it:

```{r, eval = FALSE}
decrypt <- function(cipher, key = openssl::my_key()) {
  cipherraw <- jsonlite::base64_dec(cipher)
  rawToChar(openssl::rsa_decrypt(cipherraw, key = key))
}

decrypt(cipher)
#> username
#> password
```

Change your password before and after you share it with me or anyone else. 

### GitHub

If you want to share secrets with a group of other people on GitHub, use the [secret](https://github.com/gaborcsardi/secret) or [cyphr](https://github.com/richfitz/cyphr) packages.

### Travis

The easiest way to handle short secrets is to use environment variables. You'll set in your `.Renviron` locally and in the settings pane on travis. That way you can use `Sys.getenv()` to access in both places. It's also possible to set encrypted env vars in your `.travis.yml`: see [the documentation][travis-envvar] for details.

Regardless of how you set it, make sure you have a helper to retrieve the value. A good error message will save you a lot of time when debugging problems!

```{r}
my_secret <- function() {
  val <- Sys.getenv("SECRET")
  if (identical(val, "")) {
    stop("`SECRET` env var has not been set")
  }
  val
}
```

Note that encrypted data is not available in pull requests in forks. Typically you'll need to check PRs locally once you've confirmed that the code isn't actively malicious.

To share secret files on travis, see <https://docs.travis-ci.com/user/encrypting-files/>. Basically you will encrypt the file locally and check it in to git. Then you'll add a decryption step to your `.travis.yml` which makes it decrypts it for each run. 

Be careful to not accidentally expose the secret on travis. An easy way to accidentally expose the secret is to print it out so that it's captured in the log. Don't do that!

## CRAN 

There is no way to securely share information with arbitrary R users, including CRAN. That means that if you're developing a package, you need to make sure that `R CMD check` passes cleanly even when authentication is not available. This tends to primarily affect the documentation, vignettes, and tests.

### Documentation

Like any R package, an API client needs clear and complete documentation of all functions. Examples are particularly useful but may need to be wrapped in `\donttest{}` to avoid challenges of authentication, rate limiting, lack of network access, or occasional API server down time.

### Vignettes

Vignettes pose additional challenges when an API requires authentication, because you don't want to bundle your own credentials with the package! However, you can take advantage of the fact that the vignette is built locally, and only checked by CRAN. In a setup chunk, do:

```{r}
NOT_CRAN <- identical(tolower(Sys.getenv("NOT_CRAN")), "true")
knitr::opts_chunk$set(purl = NOT_CRAN)
```

And then use `eval = NOT_CRAN` in any chunk that requires access to a secret.

### Testing

Use `testthat::skip()` to automatically skip tests that require authentication. I typically will wrap this into a little helper function that I call at the start of every test requiring auth.

```{r}
skip_if_no_auth <- function() {
  if (identical(Sys.getenv("MY_SECRET"), "")) {
    skip("No authentication available")
  }
}
```

[google-secret]: https://developers.google.com/identity/protocols/OAuth2#installed
[google-server]: https://developers.google.com/identity/protocols/OAuth2ServiceAccount
[github-token]: https://github.com/blog/1509-personal-api-tokens
[travis-envvar]: https://docs.travis-ci.com/user/environment-variables/
---
title: "Getting started with httr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with httr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
library(httr)
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

# httr quickstart guide

The goal of this document is to get you up and running with httr as quickly as possible. httr is designed to map closely to the underlying http protocol. I'll try and explain the basics in this intro, but I'd also recommend "[HTTP: The Protocol Every Web Developer Must Know][http-tutorial]" or "[HTTP made really easy](http://www.jmarshall.com/easy/http/)".

This vignette (and parts of the httr API) derived from the excellent "[Requests quickstart guide](https://requests.readthedocs.io/en/master/user/quickstart/)" by Kenneth Reitz. Requests is a python library similar in spirit to httr.  

There are two important parts to http: the __request__, the data sent to the server, and the __response__, the data sent back from the server. In the first section, you'll learn about the basics of constructing a request and accessing the response. In the second and third sections, you'll dive into more details of each.

## httr basics

To make a request, first load httr, then call `GET()` with a url:

```{r}
library(httr)
r <- GET("http://httpbin.org/get")
```

This gives you a response object. Printing a response object gives you some useful information: the actual url used (after any redirects), the http status, the file (content) type, the size, and if it's a text file, the first few lines of output.

```{r}
r
```

You can pull out important parts of the response with various helper methods, or dig directly into the object:

```{r}
status_code(r)
headers(r)
str(content(r))
```

I'll use `httpbin.org` throughout this introduction. It accepts many types of http request and returns json that describes the data that it received. This makes it easy to see what httr is doing.

As well as `GET()`, you can also use the `HEAD()`, `POST()`, `PATCH()`, `PUT()` and `DELETE()` verbs. You're probably most familiar with `GET()` and `POST()`: `GET()` is used by your browser when requesting a page, and `POST()` is (usually) used when submitting a form to a server. `PUT()`, `PATCH()` and `DELETE()` are used most often by web APIs.

## The response 

The data sent back from the server consists of three parts: the status line, the headers and the body. The most important part of the status line is the http status code: it tells you whether or not the request was successful. I'll show you how to access that data, then how to access the body and headers.

### The status code

The status code is a three digit number that summarises whether or not the request was successful (as defined by the server that you're talking to). You can access the status code along with a descriptive message using `http_status()`:

```{r}
r <- GET("http://httpbin.org/get")
# Get an informative description:
http_status(r)

# Or just access the raw code:
r$status_code
```

A successful request always returns a status of 200. Common errors are 404 (file not found) and 403 (permission denied). If you're talking to web APIs you might also see 500, which is a generic failure code (and thus not very helpful). If you'd like to learn more, the most memorable guides are the [http status cats](https://www.flickr.com/photos/girliemac/sets/72157628409467125).

You can automatically throw a warning or raise an error if a request did not succeed:

```{r}
warn_for_status(r)
stop_for_status(r)
```

I highly recommend using one of these functions whenever you're using httr inside a function (i.e. not interactively) to make sure you find out about errors as soon as possible.

### The body

There are three ways to access the body of the request, all using `content()`:

*   `content(r, "text")` accesses the body as a character vector:

    ```{r}
    r <- GET("http://httpbin.org/get")
    content(r, "text")
    ```

    httr will automatically decode content from the server using the encoding 
    supplied in the `content-type` HTTP header. Unfortunately you can't always 
    trust what the server tells you, so you can override encoding if needed:

    ```{r, eval = FALSE}
    content(r, "text", encoding = "ISO-8859-1")
    ```

    If you're having problems figuring out what the correct encoding 
    should be, try `stringi::stri_enc_detect(content(r, "raw"))`.

*   For non-text requests, you can access the body of the request as a 
    raw vector:

    ```{r}
    content(r, "raw")
    ```
    
    This is exactly the sequence of bytes that the web server sent, so this is
    the highest fidelity way of saving files to disk:
    
    ```{r, eval = FALSE}
    bin <- content(r, "raw")
    writeBin(bin, "myfile.txt")
    ```

*   httr provides a number of default parsers for common file types:

    ```{r}
    # JSON automatically parsed into named list
    str(content(r, "parsed"))
    ```
    
    See `?content` for a complete list.
    
    These are convenient for interactive usage, but if you're writing an API
    wrapper, it's best to parse the text or raw content yourself and check it
    is as you expect. See the API wrappers vignette for more details.

### The headers

Access response headers with `headers()`:

```{r}
headers(r)
```

This is basically a named list, but because http headers are case insensitive, indexing this object ignores case:

```{r}
headers(r)$date
headers(r)$DATE
```

### Cookies

You can access cookies in a similar way:

```{r}
r <- GET("http://httpbin.org/cookies/set", query = list(a = 1))
cookies(r)
```

Cookies are automatically persisted between requests to the same domain:

```{r}
r <- GET("http://httpbin.org/cookies/set", query = list(b = 1))
cookies(r)
```

## The request

Like the response, the request consists of three pieces: a status line, headers and a body. The status line defines the http method (GET, POST, DELETE, etc) and the url. You can send additional data to the server in the url (with the query string), in the headers (including cookies) and in the body of `POST()`, `PUT()` and `PATCH()` requests.

### The url query string

A common way of sending simple key-value pairs to the server is the query string: e.g. `http://httpbin.org/get?key=val`. httr allows you to provide these arguments as a named list with the `query` argument. For example, if you wanted to pass `key1=value1` and `key2=value2` to `http://httpbin.org/get` you could do:

```{r}
r <- GET("http://httpbin.org/get", 
  query = list(key1 = "value1", key2 = "value2")
)
content(r)$args
```

Any `NULL` elements are automatically dropped from the list, and both keys and values are escaped automatically.

```{r}
r <- GET("http://httpbin.org/get", 
  query = list(key1 = "value 1", "key 2" = "value2", key2 = NULL))
content(r)$args
```

### Custom headers

You can add custom headers to a request with `add_headers()`:

```{r}
r <- GET("http://httpbin.org/get", add_headers(Name = "Hadley"))
str(content(r)$headers)
```

(Note that `content(r)$header` retrieves the headers that httpbin received. `headers(r)` gives the headers that it sent back in its response.)

## Cookies

Cookies are simple key-value pairs like the query string, but they persist across multiple requests in a session (because they're sent back and forth every time). To send your own cookies to the server, use `set_cookies()`:

```{r}
r <- GET("http://httpbin.org/cookies", set_cookies("MeWant" = "cookies"))
content(r)$cookies
```

Note that this response includes the `a` and `b` cookies that were added by the server earlier.

### Request body

When `POST()`ing, you can include data in the `body` of the request. httr allows you to supply this in a number of different ways. The most common way is a named list:

```{r}
r <- POST("http://httpbin.org/post", body = list(a = 1, b = 2, c = 3))
```

You can use the `encode` argument to determine how this data is sent to the server:

```{r}
url <- "http://httpbin.org/post"
body <- list(a = 1, b = 2, c = 3)

# Form encoded
r <- POST(url, body = body, encode = "form")
# Multipart encoded
r <- POST(url, body = body, encode = "multipart")
# JSON encoded
r <- POST(url, body = body, encode = "json")
```

To see exactly what's being sent to the server, use `verbose()`. Unfortunately due to the way that `verbose()` works, knitr can't capture the messages, so you'll need to run these from an interactive console to see what's going on.

```{r, eval = FALSE}
POST(url, body = body, encode = "multipart", verbose()) # the default
POST(url, body = body, encode = "form", verbose())
POST(url, body = body, encode = "json", verbose())
```

`PUT()` and `PATCH()` can also have request bodies, and they take arguments identically to `POST()`.

You can also send files off disk:

```{r, eval = FALSE}
POST(url, body = upload_file("mypath.txt"))
POST(url, body = list(x = upload_file("mypath.txt")))
```

(`upload_file()` will guess the mime-type from the extension - using the `type` argument to override/supply yourself.)

These uploads stream the data to the server: the data will be loaded in R in chunks then sent to the remote server. This means that you can upload files that are larger than memory.

See `POST()` for more details on the other types of thing that you can send: no body, empty body, and character and raw vectors.

##### Built with

```{r}
sessionInfo()
```

[http-tutorial]:http://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177
---
title: "Scientific Journal and Sci-Fi Themed<br>Color Palettes for ggplot2"
author: "Nan Xiao <<https://nanx.me>><br>
         Miaozhu Li <<http://miaozhu.li>>"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
    css: ggsci.css
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2}
---

# Introduction

> My eyes were finally opened and I understood nature.
>
> I learned at the same time to love it.
>
> --- Claude Monet

`ggsci` offers a collection of high-quality color palettes inspired by
colors used in scientific journals, data visualization libraries,
science fiction movies, and TV shows. The color palettes in `ggsci`
are available as `ggplot2` scales. For all the color palettes,
the corresponding scales are named as:

  * `scale_color_palname()`
  * `scale_fill_palname()`

We also provided aliases, such as `scale_colour_palname()` for
`scale_color_palname()`. All available color palettes are
summarized in the table below.

+-----------------+------------------------------+--------------------------------+----------------------+
| Name            | Scales                       | Palette Types                  | Palette Generator    |
+=================+==============================+================================+======================+
| NPG             | `scale_color_npg()`          | `"nrc"`                        | `pal_npg()`          |
|                 | `scale_fill_npg()`           |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| AAAS            | `scale_color_aaas()`         | `"default"`                    | `pal_aaas()`         |
|                 | `scale_fill_aaas()`          |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| NEJM            | `scale_color_nejm()`         | `"default"`                    | `pal_nejm()`         |
|                 | `scale_fill_nejm()`          |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Lancet          | `scale_color_lancet()`       | `"lanonc"`                     | `pal_lancet()`       |
|                 | `scale_fill_lancet()`        |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| JAMA            | `scale_color_jama()`         | `"default"`                    | `pal_jama()`         |
|                 | `scale_fill_jama()`          |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| JCO             | `scale_color_jco()`          | `"default"`                    | `pal_jco()`          |
|                 | `scale_fill_jco()`           |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| UCSCGB          | `scale_color_ucscgb()`       | `"default"`                    | `pal_ucscgb()`       |
|                 | `scale_fill_ucscgb()`        |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| D3              | `scale_color_d3()`<br>       | `"category10"`                 | `pal_d3()`           |
|                 | `scale_fill_d3()`            | `"category20"`                 |                      |
|                 |                              | `"category20b"`                |                      |
|                 |                              | `"category20c"`                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| LocusZoom       | `scale_color_locuszoom()`    | `"default"`                    | `pal_locuszoom()`    |
|                 | `scale_fill_locuszoom()`     |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| IGV             | `scale_color_igv()`          | `"default"`<br>                | `pal_igv()`          |
|                 | `scale_fill_igv()`           | `"alternating"`                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| UChicago        | `scale_color_uchicago()`     | `"default"`<br>                | `pal_uchicago()`     |
|                 | `scale_fill_uchicago()`      | `"light"`<br>                  |                      |
|                 |                              | `"dark"`                       |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Star Trek       | `scale_color_startrek()`     | `"uniform"`                    | `pal_startrek()`     |
|                 | `scale_fill_startrek()`      |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Tron Legacy     | `scale_color_tron()`         | `"legacy"`                     | `pal_tron()`         |
|                 | `scale_fill_tron()`          |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Futurama        | `scale_color_futurama()`     | `"planetexpress"`              | `pal_futurama()`     |
|                 | `scale_fill_futurama()`      |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Rick and Morty  | `scale_color_rickandmorty()` | `"schwifty"`                   | `pal_rickandmorty()` |
|                 | `scale_fill_rickandmorty()`  |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| The Simpsons    | `scale_color_simpsons()`     | `"springfield"`                | `pal_simpsons()`     |
|                 | `scale_fill_simpsons()`      |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| GSEA            | `scale_color_gsea()`         | `"default"`                    | `pal_gsea()`         |
|                 | `scale_fill_gsea()`          |                                |                      |
+-----------------+------------------------------+--------------------------------+----------------------+
| Material Design | `scale_color_material()`     | `"red"` `"pink"`<br>           | `pal_material()`     |
|                 | `scale_fill_material()`      | `"purple"` `"deep-purple"`<br> |                      |
|                 |                              | `"indigo"` `"blue"`<br>        |                      |
|                 |                              | `"light-blue"` `"cyan"`<br>    |                      |
|                 |                              | `"teal"` `"green"`<br>         |                      |
|                 |                              | `"light-green"` `"lime"`<br>   |                      |
|                 |                              | `"yellow"` `"amber"`<br>       |                      |
|                 |                              | `"orange"` `"deep-orange"`<br> |                      |
|                 |                              | `"brown"` `"grey"`<br>         |                      |
|                 |                              | `"blue-grey"`                  |                      |
+-----------------+------------------------------+--------------------------------+----------------------+

# Discrete Color Palettes

We will use scatterplots with smooth curves, and bar plots to demonstrate
the discrete color palettes in `ggsci`.

```{r}
library("ggsci")
library("ggplot2")
library("gridExtra")

data("diamonds")

p1 = ggplot(subset(diamonds, carat >= 2.2),
       aes(x = table, y = price, colour = cut)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "loess", alpha = 0.05, size = 1, span = 1) +
  theme_bw()

p2 = ggplot(subset(diamonds, carat > 2.2 & depth > 55 & depth < 70),
       aes(x = depth, fill = cut)) +
  geom_histogram(colour = "black", binwidth = 1, position = "dodge") +
  theme_bw()
```

## NPG

The NPG palette is inspired by the plots in the journals published by
<emph>Nature Publishing Group</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_npg = p1 + scale_color_npg()
p2_npg = p2 + scale_fill_npg()
grid.arrange(p1_npg, p2_npg, ncol = 2)
```

## AAAS

The AAAS palette is inspired by the plots in the journals published by
<emph>American Association for the Advancement of Science</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_aaas = p1 + scale_color_aaas()
p2_aaas = p2 + scale_fill_aaas()
grid.arrange(p1_aaas, p2_aaas, ncol = 2)
```

## NEJM

The NEJM palette is inspired by the plots in
<emph>The New England Journal of Medicine</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_nejm = p1 + scale_color_nejm()
p2_nejm = p2 + scale_fill_nejm()
grid.arrange(p1_nejm, p2_nejm, ncol = 2)
```

## Lancet

The Lancet palette is inspired by the plots in <emph>Lancet</emph> journals,
such as <emph>Lancet Oncology</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_lancet = p1 + scale_color_lancet()
p2_lancet = p2 + scale_fill_lancet()
grid.arrange(p1_lancet, p2_lancet, ncol = 2)
```

## JAMA

The JAMA palette is inspired by the plots in
<emph>The Journal of the American Medical Association</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_jama = p1 + scale_color_jama()
p2_jama = p2 + scale_fill_jama()
grid.arrange(p1_jama, p2_jama, ncol = 2)
```

## JCO

The JCO palette is inspired by the the plots in
<emph>Journal of Clinical Oncology</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_jco = p1 + scale_color_jco()
p2_jco = p2 + scale_fill_jco()
grid.arrange(p1_jco, p2_jco, ncol = 2)
```

## UCSCGB

The UCSCGB palette is from the colors used by
[UCSC Genome Browser](https://genome.ucsc.edu) for
representing chromosomes. This palette has been intensively
used in visualizations produced by [Circos](http://circos.ca).

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_ucscgb = p1 + scale_color_ucscgb()
p2_ucscgb = p2 + scale_fill_ucscgb()
grid.arrange(p1_ucscgb, p2_ucscgb, ncol = 2)
```

## D3

The D3 palette is from the categorical colors used by
[D3.js](https://d3js.org) (version 3.x and before).
There are four palette types (`category10`, `category20`,
`category20b`, `category20c`) available.

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_d3 = p1 + scale_color_d3()
p2_d3 = p2 + scale_fill_d3()
grid.arrange(p1_d3, p2_d3, ncol = 2)
```

## LocusZoom

The LocusZoom palette is based on the colors used by
[LocusZoom](http://locuszoom.org/).

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_locuszoom = p1 + scale_color_locuszoom()
p2_locuszoom = p2 + scale_fill_locuszoom()
grid.arrange(p1_locuszoom, p2_locuszoom, ncol = 2)
```

## IGV

The IGV palette is from the colors used by
[Integrative Genomics Viewer](http://software.broadinstitute.org/software/igv/)
for representing chromosomes. There are two palette types
(`default`, `alternating`) available.

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_igv_default = p1 + scale_color_igv()
p2_igv_default = p2 + scale_fill_igv()
grid.arrange(p1_igv_default, p2_igv_default, ncol = 2)
```

## UChicago

The UChicago palette is based on
[the colors](https://news.uchicago.edu/sites/default/files/attachments/_uchicago.identity.guidelines.pdf)
used by the <emph>University of Chicago</emph>.
There are three palette types (`default`, `light`, `dark`) available.

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_uchicago = p1 + scale_color_uchicago()
p2_uchicago = p2 + scale_fill_uchicago()
grid.arrange(p1_uchicago, p2_uchicago, ncol = 2)
```

## Star Trek

This palette is inspired by the (uniform) colors in <emph>Star Trek</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_startrek = p1 + scale_color_startrek()
p2_startrek = p2 + scale_fill_startrek()
grid.arrange(p1_startrek, p2_startrek, ncol = 2)
```

## Tron Legacy

This palette is inspired by the colors used in <emph>Tron Legacy</emph>.
It is suitable for displaying data when using a dark theme:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_tron = p1 + theme_dark() + theme(
    panel.background = element_rect(fill = "#2D2D2D"),
    legend.key = element_rect(fill = "#2D2D2D")) +
  scale_color_tron()
p2_tron = p2 + theme_dark() + theme(
    panel.background = element_rect(fill = "#2D2D2D")) +
  scale_fill_tron()
grid.arrange(p1_tron, p2_tron, ncol = 2)
```

## Futurama

This palette is inspired by the colors used in the TV show <emph>Futurama</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_futurama = p1 + scale_color_futurama()
p2_futurama = p2 + scale_fill_futurama()
grid.arrange(p1_futurama, p2_futurama, ncol = 2)
```

## Rick and Morty

This palette is inspired by the colors used in the TV show <emph>Rick and Morty</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_rickandmorty = p1 + scale_color_rickandmorty()
p2_rickandmorty = p2 + scale_fill_rickandmorty()
grid.arrange(p1_rickandmorty, p2_rickandmorty, ncol = 2)
```

## The Simpsons

This palette is inspired by the colors used in the TV show
<emph>The Simpsons</emph>:

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p1_simpsons = p1 + scale_color_simpsons()
p2_simpsons = p2 + scale_fill_simpsons()
grid.arrange(p1_simpsons, p2_simpsons, ncol = 2)
```

# Continuous Color Palettes

We will use a correlation matrix visualization (a special type of heatmap)
to demonstrate the continuous color palettes in `ggsci`.

```{r}
library("reshape2")

data("mtcars")
cor = cor(unname(cbind(mtcars, mtcars, mtcars, mtcars)))
cor_melt = melt(cor)

p3 = ggplot(cor_melt,
            aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(colour = "black", size = 0.3) +
  theme_bw() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())
```

## GSEA

The GSEA palette (continuous) is inspired by the heatmaps generated by
[GSEA GenePattern](https://software.broadinstitute.org/cancer/software/genepattern/).

```{r, fig.width = 10.67, fig.height = 4, out.width = 800, out.height = 300, dpi = 150}
p3_gsea     = p3 + scale_fill_gsea()
p3_gsea_inv = p3 + scale_fill_gsea(reverse = TRUE)
grid.arrange(p3_gsea, p3_gsea_inv, ncol = 2)
```

## Material Design

The <emph>Material Design</emph> color palettes are from the [material design
color guidelines](https://material.io/guidelines/style/color.html).

We generate a random matrix first:

```{r}
library("reshape2")

set.seed(42)
k = 9
x = diag(k)
x[upper.tri(x)] = runif(sum(1:(k - 1)), 0, 1)
x_melt = melt(x)

p4 = ggplot(x_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(colour = "black", size = 0.3) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_bw() + theme(
    legend.position = "none", plot.background = element_blank(),
    axis.line = element_blank(), axis.ticks = element_blank(),
    axis.text.x = element_blank(), axis.text.y = element_blank(),
    axis.title.x = element_blank(), axis.title.y = element_blank(),
    panel.background = element_blank(), panel.border = element_blank(),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

Plot the matrix with the 19 material design color palettes:

```{r, fig.width = 10.67, fig.height = 7.12, out.width = 800, out.height = 533, dpi = 150}
grid.arrange(
  p4 + scale_fill_material("red"),         p4 + scale_fill_material("pink"),
  p4 + scale_fill_material("purple"),      p4 + scale_fill_material("deep-purple"),
  p4 + scale_fill_material("indigo"),      p4 + scale_fill_material("blue"),
  p4 + scale_fill_material("light-blue"),  p4 + scale_fill_material("cyan"),
  p4 + scale_fill_material("teal"),        p4 + scale_fill_material("green"),
  p4 + scale_fill_material("light-green"), p4 + scale_fill_material("lime"),
  p4 + scale_fill_material("yellow"),      p4 + scale_fill_material("amber"),
  p4 + scale_fill_material("orange"),      p4 + scale_fill_material("deep-orange"),
  p4 + scale_fill_material("brown"),       p4 + scale_fill_material("grey"),
  p4 + scale_fill_material("blue-grey"),
  ncol = 6)
```

From the figure above, we can see that even though an identical matrix
was visualized by all plots, some palettes are more preferrable
than the others because our eyes are more sensitive to the changes
of their saturation levels.

# Non-ggplot2 Graphics

To apply the color palettes in `ggsci` to other graphics systems
(such as base graphics and lattice graphics), simply use the
palette generator functions in the table above. For example:

```{r, fig.width = 6.67, fig.height = 6.67, out.width = 500, out.height = 500, dpi = 150}
mypal = pal_npg("nrc", alpha = 0.7)(9)
mypal

library("scales")
show_col(mypal)
```

You will be able to use the generated hex color codes for such
graphics systems accordingly. The transparent level of the
entire palette is easily adjustable via the argument `"alpha"`
in every generator or scale function.

# Discussion

Please note some of the palettes might not be the best choice for certain
purposes, such as color-blind safe, photocopy safe, or print friendly.
If you do have such considerations, you might want to check out
color palettes like [ColorBrewer](http://colorbrewer2.org)
and [viridis](https://cran.r-project.org/package=viridis).

The color palettes in this package are solely created for research purposes.
The authors are not responsible for the usage of such palettes.
---
title: "Using quasiquotation to add variable and value labels"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using quasiquotation to add variable and value labels}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")

if (!requireNamespace("sjmisc", quietly = TRUE) ||
    !requireNamespace("rlang", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

Labelling data is typically a task for end-users and is applied in own scripts or functions rather than in packages. However, sometimes it can be useful for both end-users and package developers to have a flexible way to add variable and value labels to their data. In such cases, [quasiquotation](https://adv-r.hadley.nz/quasiquotation.html) is helpful.

This vignette demonstrate how to use quasiquotation in _sjlabelled_ to label your data.

## Adding value labels to variables using quasiquotation

Usually, `set_labels()` can be used to add value labels to variables. The syntax of this function is easy to use, and `set_labels()` allows to add value labels to multiple variables at once, if these variables share the same value labels.

In the following examples, we will use the `frq()` function, that shows an extra **label**-column containing _value labels_, if the data is labelled. If the data has _no_ value labels, this column is not shown in the output. 

```{r message=FALSE, warning=FALSE}
library(sjlabelled)
library(sjmisc) # for frq()-function
library(rlang)

# unlabelled data
dummies <- data.frame(
  dummy1 = sample(1:3, 40, replace = TRUE),
  dummy2 = sample(1:3, 40, replace = TRUE),
  dummy3 = sample(1:3, 40, replace = TRUE)
)

# set labels for all variables in the data frame
test <- set_labels(dummies, labels = c("low", "mid", "hi"))

attr(test$dummy1, "labels")

frq(test, dummy1)

# and set same value labels for two of three variables
test <- set_labels(
  dummies, dummy1, dummy2,
  labels = c("low", "mid", "hi")
)

frq(test)
```

`val_labels()` does the same job as `set_labels()`, but in a different way. While `set_labels()` requires variables to be specified in the  `...`-argument, and labels in the `labels`-argument, `val_labels()` requires both to be specified in the `...`.

`val_labels()` requires _named_ vectors as argument, with the _left-hand side_ being the name of the variable that should be labelled, and the _right-hand side_ containing the labels for the values.

```{r message=FALSE, warning=FALSE}
test <- val_labels(dummies, dummy1 = c("low", "mid", "hi"))
attr(test$dummy1, "labels")

# remaining variables are not labelled
frq(test)
```

Unlike `set_labels()`, `val_labels()` allows the user to add _different_ value labels to different variables in one function call. Another advantage, or difference, of `val_labels()` is it's flexibility in defining variable names and value labels by using quasiquotation.

### Add labels that are stored in a vector

To use quasiquotation, we need the **rlang** package to be installed and loaded. Now we can have labels in a character vector, and use `!!` to unquote this vector.

```{r message=FALSE, warning=FALSE}
labels <- c("low_quote", "mid_quote", "hi_quote")
test <- val_labels(dummies, dummy1 = !! labels)
attr(test$dummy1, "labels")
```

### Define variable names that are stored in a vector

The same can be done with the names of _variables_ that should get new value labels. We then need `!!` to unquote the variable name and `:=` as assignment.

```{r message=FALSE, warning=FALSE}
variable <- "dummy2"
test <- val_labels(dummies, !! variable := c("lo_var", "mid_var", "high_var"))

# no value labels
attr(test$dummy1, "labels")

# value labels
attr(test$dummy2, "labels")
```

### Both variable names and value labels are stored in a vector

Finally, we can combine the above approaches to be flexible regarding both variable names and value labels.

```{r message=FALSE, warning=FALSE}
variable <- "dummy3"
labels <- c("low", "mid", "hi")
test <- val_labels(dummies, !! variable := !! labels)
attr(test$dummy3, "labels")
```

## Adding variable labels using quasiquotation

`set_label()` is the equivalent to `set_labels()` to add variable labels to a variable. The equivalent to `val_labels()` is `var_labels()`, which works in the same way as `val_labels()`. In case of _variable_ labels, a `label`-attribute is added to a vector or factor (instead of a `labels`-attribute, which is used for _value_ labels).

The following examples show how to use `var_labels()` to add variable labels to the data. We demonstrate this function without further explanation, because it is actually very similar to `val_labels()`.


```{r message=FALSE, warning=FALSE}
dummy <- data.frame(
  a = sample(1:4, 10, replace = TRUE),
  b = sample(1:4, 10, replace = TRUE),
  c = sample(1:4, 10, replace = TRUE)
)

# simple usage
test <- var_labels(dummy, a = "first variable", c = "third variable")

attr(test$a, "label")
attr(test$b, "label")
attr(test$c, "label")

# quasiquotation for labels
v1 <- "First variable"
v2 <- "Second variable"
test <- var_labels(dummy, a = !! v1, b = !! v2)

attr(test$a, "label")
attr(test$b, "label")
attr(test$c, "label")

# quasiquotation for variable names
x1 <- "a"
x2 <- "c"
test <- var_labels(dummy, !! x1 := "First", !! x2 := "Second")

attr(test$a, "label")
attr(test$b, "label")
attr(test$c, "label")

# quasiquotation for both variable names and labels
test <- var_labels(dummy, !! x1 := !! v1, !! x2 := !! v2)

attr(test$a, "label")
attr(test$b, "label")
attr(test$c, "label")
```

## Conclusion

As we have demonstrated, `var_labels()` and `val_labels()` are one of the most flexible and easy-to-use ways to add value and variable labels to our data. Another advantage is the consistent design of all functions in **sjlabelled**, which allows seamless integration into pipe-workflows.
---
title: "Working with Labelled Data"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Working with Labelled Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

This vignette shows a small example how functions to work with labelled data can be implemented in a typical data visualization workflow.

# Labelled Data

In software like SPSS, it is common to have value and variable labels as variable attributes. Variable values, even if categorical, are mostly numeric. In R, however, you may use labels as values directly:

```{r}
factor(c("low", "high", "mid", "high", "low"))
```

Reading SPSS-data with **haven** or **sjlabelled** keeps the numeric values for variables and adds the value and variable labels as attributes. See following example from the sample-dataset efc, which is part of the **sjlabelled**-package:

```{r}
library(sjlabelled)
data(efc)
str(efc$e42dep)
```

While all plotting and table functions of the [sjPlot-package](https://cran.r-project.org/package=sjPlot) make use of these attributes, many packages and/or functions do not consider these attributes, e.g. R base graphics:

```{r warning=FALSE, fig.height=6, fig.width=7}
library(sjlabelled)
data(efc)
barplot(
  table(efc$e42dep, efc$e16sex), 
  beside = T, 
  legend.text = T
)
```

As you can see in the above figure, the plot has neither axis nor legend labels.

# Adding value labels as factor values

`as_label()` is a sjlabelled-function that converts a numeric variable into a factor and sets attribute-value-labels as factor levels. When using factors with valued levels, the bar plot will be labelled.

```{r warning=FALSE, fig.height=6, fig.width=7}
barplot(
  table(sjlabelled::as_label(efc$e42dep),
        sjlabelled::as_label(efc$e16sex)), 
  beside = TRUE, 
  legend.text = TRUE
)
```

# Getting and setting value and variable labels

There are four functions that let you easily set or get value and variable labels of either a single vector or a complete data frame:

  * `get_label()` to get variable labels
  * `get_labels()` to get value labels
  * `set_label()` to set variable labels (add them as vector attribute)
  * `set_labels()` to set value labels (add them as vector attribute)

With this function, you can easily add titles to plots dynamically, i.e. depending on the variable that is plotted.

```{r warning=FALSE, fig.height=6, fig.width=7}
barplot(
  table(sjlabelled::as_label(efc$e42dep),
        sjlabelled::as_label(efc$e16sex)), 
  beside = TRUE, 
  legend.text = TRUE,
  main = get_label(efc$e42dep)
)
```

# Restore labels from subsetted data

The base `subset()` function drops label attributes (or vector attributes in general) when subsetting data. In the sjlabelled-package, there are handy functions to deal with this problem: `copy_labels()` and `remove_labels()`.

`copy_labels()` adds back labels to a subsetted data frame based on the original data frame. And `remove_labels()` removes all label attributes.


## Losing labels during subset

```{r}
efc.sub <- subset(efc, subset = e16sex == 1, select = c(4:8))
str(efc.sub)
```

## Add back labels

```{r, message=FALSE}
efc.sub <- copy_labels(efc.sub, efc)
str(efc.sub)
```

# Conclusion

When working with labelled data, especially when working with data sets imported from other software packages, it comes very handy to make use of the label attributes. The **sjlabelled**-package supports this feature and offers useful functions for these tasks.
---
title: "Labelled Data and the sjlabelled-Package"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Labelled Data and the sjlabelled-Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
if (!requireNamespace("sjmisc", quietly = TRUE) ||
    !requireNamespace("haven", quietly = TRUE) ||
    !requireNamespace("magrittr", quietly = TRUE) ||
    !requireNamespace("dplyr", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

This package provides functions to read and write data between R and other statistical software packages like _SPSS_, _SAS_ or _Stata_ and to work with labelled data; this includes easy ways to get and set label attributes, to convert labelled vectors into factors (and vice versa), or to deal with multiple declared missing values etc.

This vignette gives an overview of functions to work with labelled data.

# Labelled Data

_Labelled data_ (or labelled vectors) is a common data structure in other statistical environments to store meta-information about variables, like variable names, value labels or multiple defined missing values.

Labelled data not only extends **R**'s capabilities to deal with proper value _and_ variable labels, but also facilitates the representation of different types of missing values, like in other statistical software packages. Typically, in R, multiple declared missings cannot be represented in a similar way, like in 'SPSS' or 'SAS', with the regular missing values. However, the **haven**-package introduced `tagged_na` values, which can do this. Tagged NA's work exactly like regular R missing values except that they store one additional byte of information: a tag, which is usually a letter ("a" to "z") or also may be a character number ("0" to "9"). This allows to indicate different missings.

Functions of **sjlabelled** do not necessarily require vectors of class `labelled` or `haven_labelled`. The `labelled` class, implemented by the packages **haven** and **labelled**, may cause troubles with other packages, thus it's only intended as being an intermediate data structure that should be converted to common R classes. However, coercing a `labelled` vector to other classes (like factor or numeric) typically means that meta information like value and variable label attributes are lost. Actually, there is no need to drop these attributes for non-`labelled`-class vectors. Functions like `lm()` simply copy these attributes to the data that is included in the returned object. Packages like **sjPlot** support labelled data for easily annotated data visualization. **sjlabelled** supports working with _labelled data_ and offers functions to benefit from these features.

**Note:** Since package-version 2.0 of the **haven**-package, the `labelled`-class attribute was changed to `haven_labelled`, to avoid interferences with the **Hmisc**-package.

## Labelled Data in haven and labelled

The **labelled**-package is intended to support `labelled` / `haven_labelled` metadata structures, thus the data structure of labelled vectors in **haven** and **labelled** is the same.

Labelled data in this format stores information about value labels, variable names and multiple defined missing values. However, _variable names_ are only part of this information if data was imported with one of **haven**'s read-functions. Adding a variable label attribute is (at least up to version 1.0.0) not possible via the `labelled()`-constructor method.

```{r}
library(haven)
x <- labelled(
  c(1:3, tagged_na("a", "c", "z"), 4:1),
  c("Agreement" = 1, "Disagreement" = 4, "First" = tagged_na("c"),
    "Refused" = tagged_na("a"), "Not home" = tagged_na("z"))
  )

print(x)
```

A `labelled` vector can either be a numeric or character vector. Conversion to factors copies the value labels as factor levels, but drops the label attributes and missing information:

```{r}
is.na(x)

as_factor(x)

is.na(as_factor(x))
```

## Labelled Data in sjlabelled

**sjlabelled** supports label attributes in **haven**-style (`label` and `labels`). You're not restricted to the `labelled` class for vectors when working with **sjlabelled** and labelled data. Hence, you can have vectors of common R classes and still use information like variable or value labels.

```{r message=FALSE}
library(sjlabelled)
# sjlabelled-sample data, an atomic vector with label attributes
data(efc)
str(efc$e16sex)
```

# Value Labels

## Getting value labels

The `get_labels()`-method is a generic method to return value labels of a vector or data frame.
```{r}
get_labels(efc$e42dep)
```

You can prefix the value labels with the associated values or return them as named vector with the `values` argument.
```{r}
get_labels(efc$e42dep, values = "p")
```

`get_labels()` also returns "labels" of factors, even if the factor has no label attributes.
```{r}
x <- factor(c("low", "mid", "low", "hi", "mid", "low"))
get_labels(x)
```

To ensure that labels are only returned for vectors with label-attribute, use the `attr.only` argument.
```{r}
x <- factor(c("low", "mid", "low", "hi", "mid", "low"))
get_labels(x, attr.only = TRUE)
```

If a vector has a label attribute, only these labels are returned. Non-labelled values are excluded from the output by default...
```{r}
# get labels, including tagged NA values
x <- labelled(
  c(1:3, tagged_na("a", "c", "z"), 4:1),
  c("Agreement" = 1, "Disagreement" = 4, "First" = tagged_na("c"),
    "Refused" = tagged_na("a"), "Not home" = tagged_na("z"))
)
get_labels(x)
```

... however, you can add non-labelled values to the return value as well, using the `non.labelled` argument.
```{r}
get_labels(x, non.labelled = TRUE)
```

Tagged missing values can also be included in the output, using the `drop.na` argument.

```{r}
get_labels(x, values = "n", drop.na = FALSE)
```


## Getting labelled values

The `get_values()` method returns the values for labelled values (i.e. values that have an associated label). We still use the vector `x` from the above examples.
```{r}
print(x)

get_values(x)
```

With the `drop.na` argument you can omit those values from the return values that are defined as missing.
```{r}
get_values(x, drop.na = TRUE)
```

## Setting value labels

With `set_labels()` you can add label attributes to any vector.
```{r}
x <- sample(1:4, 20, replace = TRUE)

# return new labelled vector
x <- set_labels(x, labels = c("very low", "low", "mid", "hi"))
x
```

If more labels than values are given, only as many labels elements are used as values are present.
```{r}
x <- c(2, 2, 3, 3, 2)
x <- set_labels(x, labels = c("a", "b", "c"))
x
```

However, you can force to use all labels, even for values that are not in the vector, using the `force.labels` argument.
```{r}
x <- c(2, 2, 3, 3, 2)
x <- set_labels(
  x, 
  labels = c("a", "b", "c"), 
  force.labels = TRUE
)
x
```

For vectors with more unique values than labels, additional labels for non-labelled values are added.
```{r}
x <- c(1, 2, 3, 2, 4, NA)
x <- set_labels(x, labels = c("yes", "maybe", "no"))
x
```

Use `force.values` to add only those labels that have been passed as argument.
```{r}
x <- c(1, 2, 3, 2, 4, NA)
x <- set_labels(
  x, 
  labels = c("yes", "maybe", "no"),
  force.values = FALSE
)
x
```

To add explicit labels for values (without adding more labels than wanted and without dropping labels for values that do not appear in the vector), use a named vector of labels as argument. The arguments `force.values` and `force.labels` are ignored when using named vectors.
```{r}
x <- c(1, 2, 3, 2, 4, 5)
x <- set_labels(
  x, 
  labels = c("strongly agree" = 1, 
             "totally disagree" = 4, 
             "refused" = 5,
             "missing" = 9)
)
x
```

If you want to set different value labels for a complete data frame, if you provide the labels as a `list`. For each variable in the data frame, provide a list element with value labels as character vector. Note that the length of the list must be equal to the number of variables (columns) in the data frame.
```{r}
tmp <- data.frame(
  a = c(1, 2, 3),
  b = c(1, 2, 3),
  c = c(1, 2, 3)
)

labels <- list(
  c("one", "two", "three"),
  c("eins", "zwei", "drei"),
  c("un", "dos", "tres")
)

tmp <- set_labels(tmp, labels = labels)
str(tmp)
```

You can use `set_labels()` within a pipe-workflow with _dplyr_.
```{r message=FALSE}
library(dplyr)
library(sjmisc) # for frq()
data(efc)

efc %>% 
  select(c82cop1, c83cop2, c84cop3) %>% 
  set_labels(labels = c("not often" = 1, "very often" = 4)) %>% 
  frq()
```

# Variable Labels

## Getting variable labels

The `get_label()`-method returns the variable label of a vector or all variable labels from a data frame.
```{r}
get_label(efc$e42dep)

get_label(efc, e42dep, e16sex, e15relat)
```

If a vector has no variable label, `NULL` is returned. However, `get_label()` also allows returning a standard value instead of `NULL`, in case the vector has no label attribute. This is useful to combine with `deparse(substitute())` in function calls, so - for instance - the name of the vector can be used as default value if no variable labels are present.

```{r}
dummy <- c(1, 2, 3)
testit <- function(x) get_label(x, def.value = deparse(substitute(x)))
# returns name of vector, if it has no variable label
testit(dummy)
```

If you want human-readable labels, you can use the `case`-argument, which will pass the labels to a string parser in the [snakecase-package](https://cran.r-project.org/package=snakecase).

```{r}
data(iris)

# returns no labels, because iris-data is not labelled
get_label(iris)

# returns the column name as default labels, if data is not labelled
get_label(iris, def.value = colnames(iris))

# labels are parsed in a readable way
get_label(iris, def.value = colnames(iris), case = "parsed")
```

## Setting variable labels

The `set_label()` function adds the variable label attribute to a vector. You can either return a new vector, or label an existing vector
```{r}
x <- sample(1:4, 10, replace = TRUE)

# return new vector
x <- set_label(x, label = "Dummy-variable")
str(x)

# label existing vector
set_label(x) <- "Another Dummy-variable"
str(x)
```

`set_label()` can also set variable labels for a data frame. In this case, the variable attributes get an additional `name` attribute with the vector's name. This makes it easier to see which label belongs to which vector.
```{r}
x <- data.frame(
  a = sample(1:4, 10, replace = TRUE),
  b = sample(1:4, 10, replace = TRUE),
  c = sample(1:4, 10, replace = TRUE)
)
x <- set_label(x, label = c("Variable A",
                            "Variable B",
                            "Variable C"))

str(x)

get_label(x)
```                     

An alternative to `set_label()` is `var_labels()`, which also works within pipe-workflows. `var_labels()` requires named vectors as arguments to match the column names of the input, and set the associated variable labels.

```{r}
x <- data.frame(
  a = sample(1:4, 10, replace = TRUE),
  b = sample(1:4, 10, replace = TRUE),
  c = sample(1:4, 10, replace = TRUE)
)

library(magrittr) # for pipe
x %>% 
  var_labels(
    a = "Variable A",
    b = "Variable B",
    c = "Variable C"
  ) %>% 
  str()
```                     

# Missing Values

## Defining missing values

`set_na()` converts values of a vector or of multiple vectors in a data frame into `NA`s. With `as.tag = TRUE`, `set_na()` creates tagged `NA` values, which means that these missing values get an information tag and a value label (which is, by default, the former value that was converted to NA). You can either return a new vector/data frame, or set `NA`s into an existing vector/data frame.

```{r}
x <- sample(1:8, 100, replace = TRUE)
# show value distribution
table(x)

# set value 1 and 8 as tagged missings
x <- set_na(x, na = c(1, 8), as.tag = TRUE)
x

# show value distribution, including missings
table(x, useNA = "always")

# now let's see, which NA's were "1" and which were "8"
print_tagged_na(x)

x <- factor(c("a", "b", "c"))
x

# set NA into existing vector
x <- set_na(x, na = "b", as.tag = TRUE)
x
```

## Getting missing values

The `get_na()` function returns all tagged NA values. We still use the vector `x` from the previous example.
```{r}
get_na(x)
```

To see the tags of the NA values, use the `as.tag` argument.
```{r}
get_na(x, as.tag = TRUE)
```


## Replacing specific NA with values

While `set_na()` allows you to replace values with (tagged) NA's, `replace_na()` (from package **sjmisc**) allows you to replace either all NA values of a vector or specific tagged NA values with a non-NA value.

```{r}
library(sjmisc) # for replace_na()
data(efc)
str(efc$c84cop3)

efc$c84cop3 <- set_na(efc$c84cop3, na = c(2, 3), as.tag = TRUE)
get_na(efc$c84cop3, as.tag = TRUE)

# this would replace all NA's into "2"
dummy <- replace_na(efc$c84cop3, value = 2)

# labels of former tagged NA's are preserved
get_labels(dummy, drop.na = FALSE, values = "p")
get_na(dummy, as.tag = TRUE)

# No more NA values
frq(dummy)


# In this example, the tagged NA(2) is replaced with value 2
# the new value label for value 2 is "restored NA"
dummy <- replace_na(efc$c84cop3, value = 2, na.label = "restored NA", tagged.na = "2")

# Only one tagged NA remains
get_labels(dummy, drop.na = FALSE, values = "p")
get_na(dummy, as.tag = TRUE)

# Some NA values remain
frq(dummy)
```

## Replacing values labels

With `replace_labels()`, you can replace (change) value labels of labelled values. This can also be used to change the labels of tagged missing values. Make sure to know the missing tag, which can be accessed via `get_na()`.

```{r}
str(efc$c82cop1)

efc$c82cop1 <- set_na(efc$c82cop1, na = c(2, 3), as.tag = TRUE)
get_na(efc$c82cop1, as.tag = TRUE)

efc$c82cop1 <- replace_labels(efc$c82cop1, labels = c("new NA label" = tagged_na("2")))

get_na(efc$c82cop1, as.tag = TRUE)
```
---
title: "How does covr work anyway?"
author: "Jim Hester"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{How does covr work anyway}
 %\VignetteEngine{knitr::rmarkdown}
 \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
library(covr)
```

# Introduction #

The **covr** package provides a framework for measuring unit test coverage.
Unit testing is one of the cornerstones of software development.
Any piece of R code can be thought of as a software application with a certain set of behaviors.
Unit testing means creating examples of how the code should behave _with a definition of the expected output_.
This could include normal use, edge cases, and expected error cases.
Unit testing is commonly facilitated by frameworks such as **testthat** and **RUnit**.
Test _coverage_ is the _proportion_ of the source code that is executed when running these tests.
Code coverage consists of:

* instrumenting the source code so that it reports when it is run,
* executing the unit test code to exercise the source code.

Measuring code coverage allows developers to asses their progress in quality checking their own (or their collaborators) code.
Measuring code coverage allows code consumers to have confidence in the measures taken by the package authors to verify high code quality.
**covr** provides three functions to calculate test coverage.

- `package_coverage()` performs coverage calculation on an R package. (Unit tests must be contained in the `"tests"` directory.)
- `file_coverage()` performs coverage calculation on one or more R scripts by executing one or more R scripts.
- `function_coverage()` performs coverage calculation on a single named function, using an expression provided.

In addition to providing an objective metric of test suite extensiveness, it is often advantageous for developers to have a code level view of their unit tests.
An interface for visually marking code with test coverage results allows a clear box view of the unit test suite.
The clear box view can be accessed using online tools or a local report can be generated using `report()`.

# Instrumenting R Source Code #

## Modifying the call tree ##

The core function in **covr** is `trace_calls()`.
This function was adapted from ideas in [_Advanced R - Walking the Abstract Syntax Tree with
recursive functions_](http://adv-r.had.co.nz/Expressions.html#ast-funs).
This recursive function modifies each of the leaves (atomic or name objects) of
an R expression by applying a given function to them.
If the expression is not a leaf the walker function calls itself recursively on elements of the expression instead.

We can use this same framework to instead insert a trace statement before each
call by replacing each call with a call to a counting function followed by the previous call.
Braces (`{`) in R may seem like language syntax, but
they are actually a Primitive function and you can call them like any other
function.

```{r}
identical(x = { 1 + 2; 3 + 4 },
    y = `{`(1 + 2, 3 + 4))
```
Remembering that braces always return the value of the last evaluated expression, we can call a counting function followed by the previous function
substituting `as.call(recurse(x))` in our function above with.

```{r, eval = FALSE}
`{`(count(), as.call(recurse(x)))
```

## Source References ##

Now that we have a way to add a counting function to any call in the Abstract Syntax Tree
without changing the output we need a way to determine where in the code source
that function came from.
Luckily R has a built-in method to provide this
information in the form of source references.
When `option(keep.source = TRUE)` (the default for interactive sessions), a reference to the source code
for functions is stored along with the function definition.
This reference is used to provide the original formatting and comments for the given function source.
In particular each call in a function contains a `srcref` attribute, which can then be used as a key to count just that call.

The actual source for `trace_calls` is slightly more complicated because we
want to initialize the counter for each call while we are walking the Abstract Syntax Tree and
there are a few non-calls we also want to count.

## Refining Source References ##

Each statement comes with a source reference. Unfortunately, the following is
counted as one statement:

```r
if (x)
 y()
```

To work around this, detailed parse data (obtained from a refined version of
`getParseData`) is analyzed to impute source references at sub-statement level for `if`, `for`, `while` and `switch` constructs.

## Replacing Source In Place ##

After we have our modified function definition, how do we re-define the function
to use the updated definition, and ensure that all other functions which call
the old function also use the new definition? You might try redefining the function directly.

```{r}
f1 <- function() 1

f1 <- function() 2
f1() == 2
```

While this does work for the simple case of calling the new function in the
same environment, it fails if another function calls a function in a different environment.

```{r}
env <- new.env()
f1 <- function() 1
env$f2 <- function() f1() + 1

env$f1 <- function() 2

env$f2() == 3
```

As modifying external environments and correctly restoring them can be tricky
to get correct, we use the C function
[`reassign_function`](https://github.com/r-lib/covr/blob/9753e0e257b053059b85be90ef6eb614a5af9bba/src/reassign.c#L7-L20),
which is also used in `testthat::with_mock`.
This function takes a function name,
environment, old definition, new definition and copies the formals, body,
attributes and environment from the old function to the new function.
This allows you to do an in-place replacement of a given function with a new
function and ensure that all references to the old function will use the new definition.

# Object Orientation #

## S3 Classes ##

R's S3 object oriented classes simply define functions directly in the packages
namespace, so they can be treated the same as any other function.

## S4 Classes ##

S4 methods have a more complicated implementation than S3 classes.
The function definitions are placed in an enclosing environment based on the generic method they implement.
This makes getting the function definition more complicated.

`replacements_S4` first gets all the generic functions for the package environment.
Then for each generic function if finds the mangled meta package name
and gets the corresponding environment from the base environment.
All of the functions within this environment are then traced.

## Reference Classes ##

Similarly to S4 classes reference classes (RC) define their methods in a special environment.
A similar method is used to add the tracing calls to the
class definition.
These calls are then copied to the object methods when the
generator function is run.

# Compiled code #

## Gcov ##

Test coverage of compiled code uses a completely different mechanism than that
of R code. Fortunately we can take advantage of
[Gcov](https://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Gcov.html#Gcov), the
built-in coverage tool for [gcc](https://gcc.gnu.org/) and compatible reports
from [clang](http://clang.llvm.org/) versions 3.5 and greater.

Both of these compilers track execution coverage when given the `--coverage`
flag. In addition it is necessary to turn off compiler optimization `-O0`,
otherwise the coverage output is difficult or impossible to interpret as
multiple lines can be optimized into one, functions can be inlined, etc.

## Makevars ##

R passes flags defined in `PKG_CFLAGS` to the compiler, however it also has
default flags including `-02` (defined in `$R_HOME/etc/Makeconf`), which need to
be overridden. Unfortunately it is not possible to override the default flags
with environment variables (as the new flags are added to the left of the
defaults rather than the right). However if Make variables are defined in
`~/.R/Makevars` they _are_ used in place of the defaults.

Therefore, we need to temporarily add `-O0 --coverage` to
the Makevars file, then restore the previous state after the coverage is run.

## Subprocess ##

The last hurdle to getting compiled code coverage working properly is that the
coverage output is only produced when the running process ends.
Therefore you cannot run the tests and get the results in the same R process.
**covr** runs a separate R process when running tests.
However we need to modify the package code first before running the tests.

**covr** installs the package to be tested in a
temporary directory.
Next, calls are made to the lazy loading code which installs a user hook to modify the code when it is loaded. We also register a finalizer
which prints the coverage counts when the namespace is unloaded or the R process exits.
These output files are then aggregated together to determine the coverage.

This procedure works regardless of the number of child R processes used, so
therefore also works with parallel code.

# Output Formats #

The output format returned by **covr** is an R object  of class "coverage" containing the information gathered when executing the test suite.
It consists of a named list, where the names are colon-delimited information from the source references (the file, line and columns the traced call is from).
The value is the number of times that given expression was called and the source ref of the original call.

```{r}
# an object to analyze
f1 <- function(x) { x + 1 }
# get results with no unit tests
c1 <- function_coverage(fun = f1, code = NULL)
c1
# get results with unit tests
c2 <- function_coverage(fun = f1, code = f1(x = 1) == 2)
c2
```

An `as.data.frame` method is available to make subsetting by various features easy to do.

While **covr** tracks coverage by expression, typically users expect coverage to
be reported by line, so there are functions to convert to line oriented
coverage.

# Codecov.io and Coveralls.io #

[Codecov](https://codecov.io/) and [Coveralls](https://coveralls.io/) are a web services to help you track your code coverage
over time, and ensure that all new code is appropriately covered.

They both have JSON-based APIs to submit and report on coverage. The functions `codecov` and `coveralls` create outputs that can be consumed by these services.

# Prior Art #

## Overview ##

Prior to writing **covr**, there were a handful of coverage tools for R code.
[**R-coverage**](https://web.archive.org/web/20160611114452/http://r2d2.quartzbio.com/posts/r-coverage-docker.html) by Karl Forner and
[**testCoverage**](https://github.com/MangoTheCat/testCoverage) by Tom Taverner, Chris Campbell & Suchen Jin.

## R-coverage ##

**R-coverage** provides a very robust solution by modifying
the R source code to instrument the code for each call.
Unfortunately this requires you to patch the source of the R application itself.
Getting the changes incorporated into the core R distribution would likely be challenging.

## Test Coverage ##

**testCoverage** uses `getParseData`, R's alternate parser (from 3.0) to analyse the R source code.
The package replaces symbols in the code to be tested with a unique identifier.
This is then injected into a tracing function that will report each time the symbol is called.
The first symbol at each level of the expression tree is traced, allowing the coverage of code branches to be checked.
This is a complicated implementation I do not fully
understand, which is one of the reasons I decided to write **covr**.

## Covr ##

**covr** takes an approach in-between the two previous tools.
Function definitions are modified by parsing the abstract syntax tree and inserting trace statements.
These modified definitions are then transparently replaced in-place using C.
This allows us to correctly instrument every call and function in a package without having to resort to alternate parsing or changes to the R source.

# Conclusion #

**covr** provides an accessible framework which will ease the communication of R unit test suites.
**covr** can be integrated with continuous integration services where R developers are working on larger projects, or as part of multi-disciplinary teams.
**covr** aims to be simple to use to make writing high quality code part of every R user's routine.
---
title: "Do more with dates and times in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Do more with dates and times in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

(This vignette is an updated version of the blog post first published at [r-statistics](http://www.r-statistics.com/2012/03/do-more-with-dates-and-times-in-r-with-lubridate-1-1-0/)_

Lubridate is an R package that makes it easier to work with dates and times. Below is a concise tour of some of the things lubridate can do for you. Lubridate was created by Garrett Grolemund and Hadley Wickham, and is now maintained by Vitalie Spinu.

## Parsing dates and times

Getting R to agree that your data contains the dates and times you think it does can be tricky. Lubridate simplifies that. Identify the order in which the year, month, and day appears in your dates. Now arrange "y", "m", and "d" in the same order. This is the name of the function in lubridate that will parse your dates. For example,

```{r}
library(lubridate)
ymd("20110604")
mdy("06-04-2011")
dmy("04/06/2011")
```

Lubridate's parse functions handle a wide variety of formats and separators, which simplifies the parsing process.

If your date includes time information, add h, m, and/or s to the name of the function. `ymd_hms` is probably the most common date time format. To read the dates in with a certain time zone, supply the official name of that time zone in the `tz` argument.

```{r}
arrive <- ymd_hms("2011-06-04 12:00:00", tz = "Pacific/Auckland")
arrive
leave <- ymd_hms("2011-08-10 14:00:00", tz = "Pacific/Auckland")
leave
```

## Setting and Extracting information

Extract information from date times with the functions `second`, `minute`, `hour`, `day`, `wday`, `yday`, `week`, `month`, `year`, and `tz`. You can also use each of these to set (i.e, change) the given information. Notice that this will alter the date time. `wday` and `month` have an optional `label` argument, which replaces their numeric output with the name of the weekday or month.

```{r}
second(arrive)
second(arrive) <- 25
arrive
second(arrive) <- 0

wday(arrive)
wday(arrive, label = TRUE)
```

## Time Zones

There are two very useful things to do with dates and time zones. First, display the same moment in a different time zone. Second, create a new moment by combining an existing clock time with a new time zone. These are accomplished by `with_tz` and `force_tz`.

For example, a while ago I was in Auckland, New Zealand. I arranged to meet the co-author of lubridate, Hadley, over skype at 9:00 in the morning Auckland time. What time was that for Hadley who was back in Houston, TX?

```{r}
meeting <- ymd_hms("2011-07-01 09:00:00", tz = "Pacific/Auckland")
with_tz(meeting, "America/Chicago")
```

So the meetings occurred at 4:00 Hadley's time (and the day before no less). Of course, this was the same actual moment of time as 9:00 in New Zealand. It just appears to be a different day due to the curvature of the Earth.

What if Hadley made a mistake and signed on at 9:00 his time? What time would it then be my time?

```{r}
mistake <- force_tz(meeting, "America/Chicago")
with_tz(mistake, "Pacific/Auckland")
```

His call would arrive at 2:00 am my time! Luckily he never did that.

## Time Intervals

You can save an interval of time as an Interval class object with lubridate. This is quite useful! For example, my stay in Auckland lasted from June 4, 2011 to August 10, 2011 (which we've already saved as arrive and leave). We can create this interval in one of two ways:

```{r}
auckland <- interval(arrive, leave) 
auckland
auckland <- arrive %--% leave
auckland
```

My mentor at the University of Auckland, Chris, traveled to various conferences that year including the Joint Statistical Meetings (JSM). This took him out of the country from July 20 until the end of August.

```{r}
jsm <- interval(ymd(20110720, tz = "Pacific/Auckland"), ymd(20110831, tz = "Pacific/Auckland"))
jsm
```

Will my visit overlap with and his travels? Yes.

```{r}
int_overlaps(jsm, auckland)
```

Then I better make hay while the sun shines! For what part of my visit will Chris be there?

```{r}
setdiff(auckland, jsm)
```

Other functions that work with intervals include `int_start`, `int_end`, `int_flip`, `int_shift`, `int_aligns`, `union`, `intersect`, `setdiff`, and `%within%`.

## Arithmetic with date times

Intervals are specific time spans (because they are tied to specific dates), but lubridate also supplies two general time span classes: Durations and Periods. Helper functions for creating periods are named after the units of time (plural). Helper functions for creating durations follow the same format but begin with a "d" (for duration) or, if you prefer, and "e" (for exact).

```{r}
minutes(2) ## period
dminutes(2) ## duration
```

Why two classes? Because the timeline is not as reliable as the number line. The Duration class will always supply mathematically precise results. A duration year will always equal 365 days. Periods, on the other hand, fluctuate the same way the timeline does to give intuitive results. This makes them useful for modeling clock times. For example, durations will be honest in the face of a leap year, but periods may return what you want:

```{r}
leap_year(2011) ## regular year
ymd(20110101) + dyears(1)
ymd(20110101) + years(1)

leap_year(2012) ## leap year
ymd(20120101) + dyears(1)
ymd(20120101) + years(1)
```

You can use periods and durations to do basic arithmetic with date times. For example, if I wanted to set up a reoccuring weekly skype meeting with Hadley, it would occur on:

```{r}
meetings <- meeting + weeks(0:5)
```

Hadley travelled to conferences at the same time as Chris. Which of these meetings would be affected? The last two.

```{r}
meetings %within% jsm
```

How long was my stay in Auckland?

```{r}
auckland / ddays(1)
auckland / ddays(2)
auckland / dminutes(1)
```

And so on. Alternatively, we can do modulo and integer division. Sometimes this is more sensible than division - it is not obvious how to express a remainder as a fraction of a month because the length of a month constantly changes.

```{r}
auckland %/% months(1)
auckland %% months(1)
```

Modulo with an timespan returns the remainder as a new (smaller) interval. You can turn this or any interval into a generalized time span with `as.period`.

```{r}
as.period(auckland %% months(1))
as.period(auckland)
```

### If anyone drove a time machine, they would crash

The length of months and years change so often that doing arithmetic with them can be unintuitive. Consider a simple operation, `January 31st + one month`. Should the answer be 

1. `February 31st` (which doesn't exist)
2. `March 4th` (31 days after January 31), or
3. `February 28th` (assuming its not a leap year)

A basic property of arithmetic is that `a + b - b = a`. Only solution 1 obeys this property, but it is an invalid date. I've tried to make lubridate as consistent as possible by invoking the following rule *if adding or subtracting a month or a year creates an invalid date, lubridate will return an NA*. This is new with version 1.3.0, so if you're an old hand with lubridate be sure to remember this!

If you thought solution 2 or 3 was more useful, no problem. You can still get those results with clever arithmetic, or by using the special `%m+%` and `%m-%` operators. `%m+%` and `%m-%` automatically roll dates back to the last day of the month, should that be necessary.

```{r}
jan31 <- ymd("2013-01-31")
jan31 + months(0:11)
floor_date(jan31, "month") + months(0:11) + days(31)
jan31 %m+% months(0:11)
```

Notice that this will only affect arithmetic with months (and arithmetic with years if your start date it Feb 29).

## Vectorization

The code in lubridate is vectorized and ready to be used in both interactive settings and within functions. As an example, I offer a function for advancing a date to the last day of the month

```{r}
last_day <- function(date) {
  ceiling_date(date, "month") - days(1)
}
```


## Further Resources

To learn more about lubridate, including the specifics of periods and durations, please read the [original lubridate paper](http://www.jstatsoft.org/v40/i03/). Questions about lubridate can be addressed to the lubridate google group. Bugs and feature requests should be submitted to the [lubridate development page](http://github.com/hadley/lubridate) on github.
---
title: "Recoding Variables"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Recoding Variables}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning = FALSE, comment = "#>")
suppressPackageStartupMessages(library(sjmisc))
```

Data preparation is a common task in research, which usually takes the most amount of time in the analytical process. **sjmisc** is a package with special focus on transformation of _variables_ that fits into the workflow and design-philosophy of the so-called "tidyverse".

Basically, this package complements the **dplyr** package in that **sjmisc** takes over data transformation tasks on variables, like recoding, dichotomizing or grouping variables, setting and replacing missing values, etc. A distinctive feature of **sjmisc** is the support for labelled data, which is especially useful for users who often work with data sets from other statistical software packages like _SPSS_ or _Stata_.

This vignette demonstrate some of the important recoding-functions in **sjmisc**. The examples are based on data from the EUROFAMCARE project, a survey on the situation of family carers of older people in Europe. The sample data set `efc` is part of this package.

```{r message=FALSE}
library(sjmisc)
data(efc)
```

To show the results after recoding variables, the `frq()` function is used to print frequency tables.

## Dichotomization: dividing variables into two groups

`dicho()` dichotomizes variables into "dummy" variables (with 0/1 coding). Dichotomization is either done by median, mean or a specific value (see argument `dich.by`).

Like all recoding-functions in **sjmisc**, `dicho()` returns the complete data frame _including_ the recoded variables, if the first argument is a `data.frame`. If the first argument is a vector, only the recoded variable is returned. See [this vignette](design_philosophy.html) for details about the function-design.

If `dicho()` returns a data frame, the recoded variables have the same name as the original variable, including a suffix `_d`.

```{r}
# age, ranged from 65 to 104, in this output
# grouped to get a shorter table
frq(efc, e17age, auto.grp = 5)

# splitting is done at the median by default:
median(efc$e17age, na.rm = TRUE)

# the recoded variable is now named "e17age_d"
efc <- dicho(efc, e17age)
frq(efc, e17age_d)
```

As `dicho()`, like all recoding-functions, supports [labelled data](https://cran.r-project.org/package=sjlabelled), the variable preserves it variable label (but not the value labels). You can directly define value labels inside the function:

```{r}
x <- dicho(efc$e17age, val.labels = c("young age", "old age"))
frq(x)
```

To split a variable at a different value, use the `dich.by`-argment. The value specified in `dich.by` is _inclusive_, i.e. all values from lowest to and including `dich.by` are recoded into the lower category, while all values _above_ `dich.by` are recoded into the higher category.

```{r}
# split at upper quartile
x <- dicho(
  efc$e17age, 
  dich.by = quantile(efc$e17age, probs = .75, na.rm = TRUE), 
  val.labels = c("younger three quarters", "oldest quarter")
)
frq(x)
```

Since the distribution of values in a dataset may differ for different subgroups, all recoding-functions also work on grouped data frames. In the following example, first, the age-variable `e17age` is dichotomized at the median. Then, the data is grouped by gender (`c161sex`) and the dichotomization is done for each subgroup, i.e. it once relates to the median age in the subgroup of female, and once to the median age in the subgroup of male family carers.

```{r}
data(efc)
x1 <- dicho(efc$e17age)

x2 <- efc %>% 
  dplyr::group_by(c161sex) %>% 
  dicho(e17age) %>% 
  dplyr::pull(e17age_d)

# median age of total sample
frq(x1)

# median age of total sample, with median-split applied
# to distibution of age by subgroups of gender
frq(x2)
```

## Splitting variables into several groups

`split_var()` recodes numeric variables into equal sized groups, i.e. a variable is cut into a smaller number of groups at specific cut points. The amount of groups depends on the `n`-argument and cuts a variable into `n` quantiles.

Similar to `dicho()`, if the first argument in `split_var()` is a data frame, the complete data frame including the new recoded variable(s), with suffix `_g`, is returned.

```{r}
x <- split_var(efc$e17age, n = 3)
frq(x)
```

Unlike dplyr's `ntile()`, `split_var()` never splits a value into two different categories, i.e. you always get a "clean" separation of original categories. In other words: cases that have identical values in a variable will always be recoded into the same group. The following example demonstrates the differences:

```{r}
x <- dplyr::ntile(efc$neg_c_7, n = 3)
# for some cases, value "10" is recoded into category "1",
# for other cases into category "2". Same is true for value "13"
table(efc$neg_c_7, x)

x <- split_var(efc$neg_c_7, n = 3)
# no separation of cases with identical values.
table(efc$neg_c_7, x)
```

`split_var()`, unlike `ntile()`, does therefor not always return exactly equal-sized groups:

```{r}
x <- dplyr::ntile(efc$neg_c_7, n = 3)
frq(x)

x <- split_var(efc$neg_c_7, n = 3)
frq(x)
```

## Recode variables into equal-ranged groups

With `group_var()`, variables can be grouped into equal ranged categories, i.e. a variable is cut into a smaller number of groups, where each group has the same value range. `group_labels()` creates the related value labels.

The range of the groups is defined in the `size`-argument. At the same time, the `size`-argument also defines the _lower bound_ of one of the groups.

For instance, if the lowest value of a variable is 1 and the maximum is 10, and `size  = 5`, then

a) each group will have a range of 5, and 
b) one of the groups will start with the value 5. 

This means, that an equal-ranged grouping will define groups from _0 to 4_, _5 to 9_ and _10-14_. Each of these groups has a range of 5, and one of the groups starts with the value 5.

The group assignment becomes clearer, when `group_labels()` is used in parallel:

```{r}
set.seed(123)
x <- round(runif(n = 150, 1, 10))

frq(x)

frq(group_var(x, size = 5))

group_labels(x, size = 5)

dummy <- group_var(x, size = 5, as.num = FALSE)
levels(dummy) <- group_labels(x, size = 5)
frq(dummy)

dummy <- group_var(x, size = 3, as.num = FALSE)
levels(dummy) <- group_labels(x, size = 3)
frq(dummy)
```

The argument `right.interval` can be used when `size` should indicate the _upper bound_ of a group-range.

```{r}
dummy <- group_var(x, size = 4, as.num = FALSE)
levels(dummy) <- group_labels(x, size = 4)
frq(dummy)

dummy <- group_var(x, size = 4, as.num = FALSE, right.interval = TRUE)
levels(dummy) <- group_labels(x, size = 4, right.interval = TRUE)
frq(dummy)
```

## Flexible recoding of variables

`rec()` recodes old values of variables into new values, and can be considered as a "classsical" recode-function. The recode-pattern, i.e. which new values should replace the old values, is defined in the `rec`-argument. This argument has a specific "syntax":

* **recode pairs**: Each recode pair has to be separated by a ;, e.g. `rec = "1=1; 2=4; 3=2; 4=3"`

* **multiple values**: Multiple old values that should be recoded into a new single value may be separated with comma, e.g. `rec = "1,2=1; 3,4=2"`

* **value range**: A value range is indicated by a colon, e.g. `rec = "1:4=1; 5:8=2"` (recodes all values from 1 to 4 into 1, and from 5 to 8 into 2)

* **value range for doubles**: For double vectors (with fractional part), all values within the specified range are recoded; e.g. `rec = "1:2.5=1;2.6:3=2"` recodes 1 to 2.5 into 1 and 2.6 to 3 into 2, but 2.55 would not be recoded (since it's not included in any of the specified ranges)

* **"min" and "max"**: Minimum and maximum values are indicates by `min` (or `lo`) and `max` (or `hi`), e.g. `rec = "min:4=1; 5:max=2"` (recodes all values from minimum values of x to 4 into 1, and from 5 to maximum values of x into 2) You can also use `min` or `max` to recode a value into the minimum or maximum value of a variable, e.g. `rec = "min:4=1; 5:7=max"` (recodes all values from minimum values of x to 4 into 1, and from 5 to 7 into the maximum value of x).

* **"else"**: All other values, which have not been specified yet, are indicated by else, e.g. `rec = "3=1; 1=2; else=3"` (recodes 3 into 1, 1 into 2 and all other values into 3)

* **"copy"**: The `"else"`-token can be combined with `"copy"`, indicating that all remaining, not yet recoded values should stay the same (are copied from the original value), e.g. `rec = "3=1; 1=2; else=copy"` (recodes 3 into 1, 1 into 2 and all other values like 2, 4 or 5 etc. will not be recoded, but copied.

*  **NA's**: `NA` values are allowed both as old and new value, e.g. `rec = "NA=1; 3:5=NA"` (recodes all `NA` into 1, and all values from 3 to 5 into NA in the new variable)

* **"rev"**: `"rev"` is a special token that reverses the value order.

* **direct value labelling**: Value labels for new values can be assigned inside the recode pattern by writing the value label in square brackets after defining the new value in a recode pair, e.g. `rec = "15:30=1 [young aged]; 31:55=2 [middle aged]; 56:max=3 [old aged]"`

* **non-captured values**: Non-matching values will be set to `NA`, unless captured by the `"else"`- or `"copy"`-token.

Here are some examples:

```{r}
frq(efc$e42dep)

# replace NA with 5
frq(rec(efc$e42dep, rec = "NA=5;else=copy"))

# recode 1 to 2 into 1 and 3 to 4 into 2
frq(rec(efc$e42dep, rec = "1,2=1; 3,4=2"))

# recode 1 to 3 into 4 into 2
frq(rec(efc$e42dep, rec = "min:3=1; 4=2"))

# recode numeric to character, and remaining values
# into the highest value (="hi") of e42dep
frq(rec(efc$e42dep, rec = "1=first;2=2nd;else=hi"))

data(iris)
frq(rec(iris, Species, rec = "setosa=huhu; else=copy", append = FALSE))

# works with mutate
efc %>%
  dplyr::select(e42dep, e17age) %>%
  dplyr::mutate(dependency_rev = rec(e42dep, rec = "rev")) %>%
  head()

# recode multiple variables and set value labels via recode-syntax
dummy <- rec(
  efc, c160age, e17age,
  rec = "15:30=1 [young]; 31:55=2 [middle]; 56:max=3 [old]",
  append = FALSE
)
frq(dummy)
```

## Scoped variants

Where applicable, the recoding-functions in **sjmisc** have "scoped" versions as well, e.g. `dicho_if()` or `split_var_if()`, where transformation will be applied only to those variables that match the logical condition of `predicate`.

## Cheat Sheet

A cheatsheet can be downloaded from the [RStudio cheatsheet collection](https://www.rstudio.com/resources/cheatsheets/).

---
title: "Exploring Data Sets"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploring Data Sets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning = FALSE, comment = "#>")
suppressPackageStartupMessages(library(sjmisc))
```

Tidying up, transforming and exploring data is an important part of data analysis, and you can manage many common tasks in this process with the [tidyverse](http://tidyverse.org/) or related packages. The **sjmisc**-package fits into this workflow, especially when you work with [labelled data](https://cran.r-project.org/package=sjlabelled), because it offers functions for data transformation and labelled data utility functions. This vignette describes typical steps when beginning with data exploration.

The examples are based on data from the EUROFAMCARE project, a survey on the situation of family carers of older people in Europe. The sample data set `efc` is part of this package. Let us see how the family carer's gender and subjective perception of negative impact of care as well as the cared-for person's dependency are associated with the family carer's quality of life.

```{r message=FALSE}
library(sjmisc)
library(dplyr)
data(efc)
```

## Print frequencies with labels

The first thing that may be of interest is probably the distribution of gender. You can plot frequencies for labelled data with `frq()`. This function requires either a vector or data frame as input and prints the variable label as first line, followed by a frequency-table with values, labels, counts and percentages of the vector.

```{r}
frq(efc$c161sex)
```

## Find variables in a data frame

Next, let’s look at the distribution of gender by the cared-for person's dependency. To compute cross tables, you can use `flat_table()`. It requires the data as first argument, followed by any number of variable names.

But first, we need to know the name of the dependency-variable. This is where `find_var()` comes into play. It searches for variables in a data frame by

1. variable names,
2. variable labels, 
3. value labels 
4. or any combination of these. 

By default, it looks for variable name and labels. The function also supports regex-patterns. By default, `find_var()` returns the column-indices, but you can also print a small "summary"" with the `out`-argument.

```{r}
# find all variables with "dependency" in name or label
find_var(efc, "dependency", out = "table")
```

Variable in column 5, named _e42dep_, is what we are looking for.

## Print crosstables with labels

Now we can look at the distribution of gender by dependency:

```{r}
flat_table(efc, e42dep, c161sex)
```

Since the distribution of male and female carers is skewed, let's see the proportions. To compute crosstables with row or column percentages, use the `margin`-argument:

```{r}
flat_table(efc, e42dep, c161sex, margin = "col")
```

## Recoding variables

Next, we need the negatice impact of care (*neg_c_7*) and want to create three groups: low, middle and high negative impact. We can easily recode and label vectors with `rec()`. This function does not only recode vectors, it also allows direct labelling of categories inside the recode-syntax (this is optional, you can also use the `val.labels`-argument). We now recode *neg_c_7* into a new variable _burden_. The cut-points are a bit arbitrary, for the sake of demonstration.

```{r}
efc$burden <- rec(
  efc$neg_c_7,
  rec = c("min:9=1 [low]; 10:12=2 [moderate]; 13:max=3 [high]; else=NA"),
  var.label = "Subjective burden",
  as.num = FALSE # we want a factor
)
# print frequencies
frq(efc$burden)
```

You can see the variable _burden_ has a variable label ("Subjective burden"), which was set inside `rec()`, as well as three values with labels ("low", "moderate" and "high"). From the lowest value in *neg_c_7* to 9 were recoded into 1, values 10 to 12 into 2 and values 13 to the highest value in *neg_c_7* into 3. All remaining values are set to missing (`else=NA` – for details on the recode-syntax, see `?rec`).

## Grouped data frames

How is burden distributed by gender? We can group the data and print frequencies using `frq()` for this as well, as this function also accepts grouped data frames. Frequencies for grouped data frames first print the group-details (variable name and category), followed by the frequency table. Thanks to labelled data, the output is easy to understand.

```{r}
efc %>% 
  select(burden, c161sex) %>% 
  group_by(c161sex) %>% 
  frq()
```

## Nested data frames

Let's investigate the association between quality of life and burden across the different dependency categories, by fitting linear models for each category of _e42dep_. We can do this using _nested data frames_. `nest()` from the **tidyr**-package can create subsets of a data frame, based on grouping criteria, and create a new _list-variable_, where each element itself is a data frame (so it’s nested, because we have data frames inside a data frame).

In the following example, we group the data by _e42dep_, and "nest" the groups. Now we get a data frame with two columns: First, the grouping variable (_e42dep_) and second, the datasets (subsets) for each country as data frame, stored in the list-variable _data_. The data frames in the subsets (in _data_) all contain the selected variables _burden_, _c161sex_ and *quol_5* (quality of life).

```{r}
# convert variable to labelled factor, because we then 
# have the labels as factor levels in the output
efc$e42dep <- to_label(efc$e42dep, drop.levels = T)
efc %>%
  select(e42dep, burden, c161sex, quol_5) %>%
  group_by(e42dep) %>%
  tidyr::nest()
```

## Get coefficients of nested models

Using `map()` from the **purrr**-package, we can iterate this list and apply any function on each data frame in the list-variable "data". We want to apply the `lm()`-function to the list-variable, to run linear models for all "dependency-datasets". The results of these linear regressions are stored in another list-variable, _models_ (created with `mutate()`). To quickly access and look at the coefficients, we can use `spread_coef()`.

```{r}
efc %>%
  select(e42dep, burden, c161sex, quol_5) %>%
  group_by(e42dep) %>%
  tidyr::nest() %>% 
  na.omit() %>%       # remove nested group for NA
  arrange(e42dep) %>% # arrange by order of levels
  mutate(models = purrr::map(
    data, ~ 
    lm(quol_5 ~ burden + c161sex, data = .))
  ) %>%
  spread_coef(models)
```

We see that higher burden is associated with lower quality of life, for all dependency-groups. The `se` and `p.val`-arguments add standard errors and p-values to the output. `model.term` returns the statistics only for a specific term. If you specify a `model.term`, arguments `se` and `p.val` automatically default to `TRUE`.

```{r}
efc %>%
  select(e42dep, burden, c161sex, quol_5) %>%
  group_by(e42dep) %>%
  tidyr::nest() %>% 
  na.omit() %>%       # remove nested group for NA
  arrange(e42dep) %>% # arrange by order of levels
  mutate(models = purrr::map(
    data, ~ 
    lm(quol_5 ~ burden + c161sex, data = .))
  ) %>%
  spread_coef(models, burden3)
```
---
title: "The Design Philosophy of Functions in sjmisc"
author: "Daniel Lüdecke"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The Design Philosophy of Functions in sjmisc}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>"
)
options(max.print = 1000)
suppressPackageStartupMessages(library(sjmisc))
```

Basically, this package complements the _dplyr_ package in that _sjmisc_ takes over data transformation tasks on variables, like recoding, dichotomizing or grouping variables, setting and replacing missing values, etc. The data transformation functions also support labelled data.

# The design of data transformation functions

The design of data transformation functions in this package follows, where appropriate, the _tidyverse-approach_, with the first argument of a function always being the data (either a data frame or vector), followed by variable names that should be processed by the function. If no variables are specified as argument, the function applies to the complete data that was indicated as first function argument.

## The data-argument

A major difference to dplyr-functions like `select()` or `filter()` is that the data-argument (the first argument of each function), may either be a _data frame_ or a _vector_. The returned object for each function _equals the type of the data-argument_:

  * If the data-argument is a vector, the function returns a vector.
  * If the data-argument is a data frame, the function returns a data frame.

```{r}
library(sjmisc)
data(efc)

# returns a vector
x <- rec(efc$e42dep, rec = "1,2=1; 3,4=2")
str(x)

# returns a data frame
rec(efc, e42dep, rec = "1,2=1; 3,4=2", append = FALSE) %>% head()
```

This design-choice is mainly due to compatibility- and convenience-reasons. It does not affect the usual "tidyverse-workflow" or when using pipe-chains.

## The ...-ellipses-argument

The selection of variables specified in the `...`-ellipses-argument is powered by dplyr's `select()` and tidyselect's `select_helpers()`. This means, you can use existing functions like `:` to select a range of variables, or also use tidyselect's `select_helpers`, like `contains()` or `one_of()`.

```{r echo=FALSE, message=FALSE}
library(dplyr)
```
```{r collapse=TRUE}
# select all variables with "cop" in their names, and also
# the range from c161sex to c175empl
rec(
  efc, contains("cop"), c161sex:c175empl, 
  rec = "0,1=0; else=1", 
  append = FALSE
) %>% head()

# center all variables with "age" in name, variable c12hour
# and all variables from column 19 to 21
center(efc, c12hour, contains("age"), 19:21, append = FALSE) %>% head()
```

## The function-types

There are two types of function designs:

### coercing/converting functions

Functions like `to_factor()` or `to_label()`, which convert variables into other types or add additional information like variable or value labels as attribute, typically _return the complete data frame_ that was given as first argument _without any new variables_. The variables specified in the `...`-ellipses argument are converted (overwritten), all other variables remain unchanged.

```{r}
x <- efc[, 3:5]

x %>% str()

to_factor(x, e42dep, e16sex) %>% str()
```

### transformation/recoding functions

Functions like `rec()` or `dicho()`, which transform or recode variables, by default add _the transformed or recoded variables_ to the data frame, so they return the new variables _and_ the original data as combined data frame. To return _only the transformed and recoded variables_ specified in the `...`-ellipses argument, use argument `append = FALSE`.

```{r}
# complete data, including new columns
rec(efc, c82cop1, c83cop2, rec = "1,2=0; 3:4=2", append = TRUE) %>% head()

# only new columns
rec(efc, c82cop1, c83cop2, rec = "1,2=0; 3:4=2", append = FALSE) %>% head()
```

These variables usually get a suffix, so you can bind these variables as new columns to a data frame, for instance with `add_columns()`. The function `add_columns()` is useful if you want to bind/add columns within a pipe-chain _to the end_ of a data frame.

```{r}
efc %>% 
  rec(c82cop1, c83cop2, rec = "1,2=0; 3:4=2", append = FALSE) %>% 
  add_columns(efc) %>% 
  head()
```

If `append = TRUE` and `suffix = ""`, recoded variables will replace (overwrite) existing variables.

```{r}
# complete data, existing columns c82cop1 and c83cop2 are replaced
rec(efc, c82cop1, c83cop2, rec = "1,2=0; 3:4=2", append = TRUE, suffix = "") %>% head()
```

## sjmisc and dplyr

The functions of **sjmisc** are designed to work together seamlessly with other packages from the tidyverse, like **dplyr**. For instance, you can use the functions from **sjmisc** both within a pipe-worklflow to manipulate data frames, or to create new variables with `mutate()`:

```{r}
efc %>% 
  select(c82cop1, c83cop2) %>% 
  rec(rec = "1,2=0; 3:4=2") %>% 
  head()

efc %>% 
  select(c82cop1, c83cop2) %>% 
  mutate(
    c82cop1_dicho = rec(c82cop1, rec = "1,2=0; 3:4=2"),
    c83cop2_dicho = rec(c83cop2, rec = "1,2=0; 3:4=2")
  ) %>% 
  head()
```

This makes it easy to adapt the **sjmisc** functions to your own workflow.
---
title: "Comparison of fs functions, base R, and shell commands"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{base vs fs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Most of the functionality is **fs** can be approximated with functions in **base R**
or in a command line shell. The table at the end of this vignette can be used
as a translation aid between these three methods.

**fs** functions smooth over some of the idiosyncrasies of file handling with
base R functions:

* Vectorization. All **fs** functions are vectorized, accepting multiple paths
  as input. Base functions are inconsistently vectorized.

* Predictable return values that always convey a path. All **fs** functions
  return a character vector of paths, a named integer or a logical vector, where
  the names give the paths. Base return values are more varied: they are often
  logical or contain error codes which require downstream processing.

* Explicit failure. If **fs** operations fail, they throw an error. Base
  functions tend to generate a warning and a system dependent error code. This
  makes it easy to miss a failure.

* UTF-8 all the things. **fs** functions always convert input paths to UTF-8 and
  return results as UTF-8. This gives you path encoding consistency across OSes.
  Base functions rely on the native system encoding.

* Naming convention. **fs** functions use a consistent naming convention.
  Because base R's functions were gradually added over time there are a number
  of different conventions used (e.g. `path.expand()` vs `normalizePath()`;
  `Sys.chmod()` vs `file.access()`).

## Directory functions

| fs                                 | base                                                                    | shell                                |
| ---                                | ---                                                                     | ---                                  |
| `dir_ls("path")`                   | `list.files("path")`                                                    | `ls path`                            |
| `dir_info("path")`                 | `do.call(rbind, lapply(list.files("path"), file.info))`                 | `ls -al path`                        |
| `dir_copy("path", "new-path")`     | `dir.create("new-path"); file.copy("path", "new-path", recursive=TRUE)` | `cp path new-path`                   |
| `dir_create("path")`               | `dir.create("path")`                                                    | `mkdir path`                         |
| `dir_delete("path")`               | `unlink("path", recursive = TRUE)`                                      | `rm -rf path`                        |
| `dir_exists("path")`               | `dir.exists("path")`                                                    | `if [ -d "path" ]; then ... ; fi`    |
| ~~`dir_move()`~~ (see `file_move`) | `file.rename("path", "new-path")`                                       | `mv path new-path`                   |
| `dir_map("path", fun)`             | *No direct equivalent*                                                  | `for file in $(ls path); do ...; done` |
| `dir_tree("path")`                 | *No direct equivalent*                                                  | `tree path`                          |


## File functions

| fs                                          | base                              | shell                             |
| ---                                         | ---                               | ---                               |
| `file_chmod("path", "mode")`                | `Sys.chmod("path", "mode")`       | `chmod mode path`                 |
| `file_chown("path", "user_id", "group_id")` | *No direct equivalent*            | `chown options path `             |
| `file_copy("path", "new-path")`             | `file.copy("path", "new-path")`   | `cp path new-path`                |
| `file_create("new-path")`                   | `file.create("new-path")`         | `touch new-path`                  |
| `file_delete("path")`                       | `unlink("path")`                  | `rm path`                         |
| `file_exists("path")`                       | `file.exists("path")`             | `if [ -f "path" ]; then ... ; fi` |
| `file_info("path")`                         | `file.info("path")`               | `ls -al path`                     |
| `file_move("path", "new-path")`             | `file.rename("path", "new-path")` | `mv path new-path`                |
| `file_show("path")`                         | `browseURL("path")`               | `open path`                       |
| `file_touch()`                              | *No direct equivalent*            | `touch path`                      |
| `file_temp()`                               | `tempfile()`                      | `mktemp`                          |
| `file_test()`                               | *No direct equivalent*            | `if [ -d "path" ]; then ...; fi`  |

## Path functions

| fs                                                   | base                                              | shell                         |
| ---                                                  | ---                                               | ---                           |
| `path("top_dir", "nested_dir", "file", ext = "ext")` | `file.path("top_dir", "nested_dir", "file.ext")`  | `top_dir/nested_dir/file.ext` |
| `path_expand("~/path")`                              | `path.expand()`                                   | `realpath -m -s ~/path`       |
| `path_dir("path")`                                   | `dirname("path")`                                 | `dirname path`                |
| `path_file("path")`                                  | `basename("path")`                                | `basename path`               |
| `path_home()`                                        | `path.expand("~")`                                | `$HOME`                       |
| `path_package("pkgname", "dir", "file")`             | `system.file("dir", "file", package = "pkgname")` | *No direct equivalent*        |
| `path_norm("path")`                                  | `normalizePath()`                                 | `realpath`                    |
| `path_real("path")`                                  | `normalizePath(mustWork = TRUE)`                  | `realpath`                    |
| `path_rel("path/foo", "path/bar")`                   | *No direct equivalent*                            | *No direct equivalent*        |
| `path_common(c("path/foo", "path/bar", "path/baz"))` | *No direct equivalent*                            | *No direct equivalent*        |
| `path_ext_remove("path")`                            | `sub("\\.[a-zA-Z0-9]*$", "", "path")`             | *No direct equivalent*        |
| `path_ext_set("path", "new_ext")`                    | `sub("\\.[a-zA-Z0-9]*$", "new_ext", "path")`      | *No direct equivalent*        |
| `path_sanitize("path")`                              | *No direct equivalent*                            | *No direct equivalent*        |
---
title: "Reprex do's and don'ts"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reprex do's and don'ts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(reprex)
```

If you're asking for R help, reporting a bug, or requesting a new feature, you're more likely to succeed if you include a good reprex.

## Main requirements

**Use the smallest, simplest, most [built-in data](https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html) possible.**

  - Think: `iris` or `mtcars`. Bore me.
  - If you must make some objects, minimize their size and complexity.
  - Many of the functions and packages you already use to import data from delimited files also offer a way to create a small data frame "inline":
    - `read.table()` and friends have a `text` argument. Example: `read.csv(text = "a,b\n1,2\n3,4")`.
    - `tibble::tribble()` lets you use a natural and readable layout. Example:
    
            tibble::tribble(
              ~ a, ~ b,
                1,   2,
                3,   4
            )
            #> # A tibble: 2 x 2
            #>       a     b
            #>   <dbl> <dbl>
            #> 1     1     2
            #> 2     3     4
  - Get just a bit of something with `head()` or by indexing with the result of `sample()`. If anything is random, consider using `set.seed()` to make it repeatable.
  - `dput()` is a good way to get the code to create an object you have lying around, if you simply cannot make do with built-in or simulated data. Copy and paste the *result* of this into your reprex.
  - Look at official examples and try to write in that style. Consider adapting one.

**Include commands on a strict "need to run" basis.**

  - Ruthlessly strip out anything unrelated to the specific matter at hand.
  - Include every single command that is required, e.g. loading specific packages via `library(foo)`.

**Consider including so-called "session info"**, i.e. your OS and versions of R and add-on packages, if it's conceivable that it matters.

  - Use `reprex(..., si = TRUE)` for this.

**Whitespace rationing is not in effect.**

  - Use good [coding style](http://style.tidyverse.org).
  - Use `reprex(..., style = TRUE)` to request automated styling of your code.

**Pack it in, pack it out, and don't take liberties with other people's computers.** You are asking people to run this code!

  - Don't start with `rm(list = ls())`. It is anti-social to clobber other people's workspaces.
  - Don't start with `setwd("C:\Users\jenny\path\that\only\I\have")`, because it won't work on anyone else's computer.
  - Don't mask built-in functions, i.e. don't define a new function named `c` or `mean`.
  - If you change options, store original values at the start, do your thing, then restore them:
    ``` r
    opar <- par(pch = 19)
    <blah blah blah>
    par(opar)
    ```
  - If you create files, delete them when you're done:
    ``` r
    write(x, "foo.txt")
    <blah blah blah>
    file.remove("foo.txt")
    ```
  - Don't delete files or objects that you didn't create in the first place.
  - Take advantage of R's built-in ability to create temporary files and directories. Read up on [`tempfile()` and `tempdir()`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/tempfile.html).
  
## This seems like a lot of work!
  
Yes, creating a great reprex requires work. You are asking other people to do work too. It's a partnership.

80% of the time you will solve your own problem in the course of writing an excellent reprex. YMMV.
  
The remaining 20% of the time, you will create a reprex that is more likely to elicit the desired behavior in others.

## Further reading:

[How to make a great R reproducible example?](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example/16532098) thread on StackOverflow

[How to write a reproducible example](http://adv-r.had.co.nz/Reproducibility.html) from Hadley Wickham's [Advanced R book](http://adv-r.had.co.nz)

## Package philosophy

The reprex code:

  * Must run and, therefore, should be run **by the person posting**. No faking it.

  * Should be easy for others to digest, so **they don't necessarily have to run it**. You are encouraged to include selected bits of output. :scream:

  * Should be easy for others to copy + paste + run, **if and only if they so choose**. Don't let inclusion of output break executability.

Accomplished like so:

  * Use `rmarkdown::render()` to run the code and capture output that you would normally see on your screen. This is done in a separate R process, via [callr](https://cran.r-project.org/package=callr), to guarantee it is self-contained.

  * Use chunk option `comment = "#>"` to include the output while retaining executability.

## Other work

If I had known about [`formatR::tidy_eval()`](http://yihui.name/formatR/), I probably would never had made reprex! But alas I did not. AFAICT here are the main differences:

  * `reprex()` accepts an expression as primary input, in addition to code on the clipboard, in a character vector, or in a file.
  * `reprex()` runs the reprex in a separate R process, via [callr](https://cran.r-project.org/package=callr). `tidy_eval()` uses the existing R process and offers an `envir` argument.
  * `reprex()` writes the code to a `.R` file and calls `rmarkdown::render()`. `tidy_eval()` runs the code line-by-line via `capture.output(eval(..., envir = envir))`.
  * `reprex()` uploads figures to imgur and inserts the necessary link.
<!--
%\VignetteEngine{knitr::docco_linear}
%\VignetteIndexEntry{An Introduction to the corrplot package}
-->

An Introduction to **corrplot** Package
=======================================

```{r setup, include=FALSE}
set.seed(0) # we need reproducible results
knitr::opts_chunk$set(
  out.extra = 'style="display:block; margin: auto"',
  fig.align = "center",
  fig.path = "webimg/",
  dev = "png")
```

Introduction
------------
The **corrplot** package is a graphical display of a correlation matrix, 
confidence interval. It also contains some algorithms to do matrix reordering. 
In addition, corrplot is good at details, including choosing color, text labels,
color labels, layout, etc.


Visualization methods
----------------------------
There are seven visualization methods (parameter `method`) in **corrplot** package, named `"circle"`, `"square"`, `"ellipse"`, `"number"`, `"shade"`, `"color"`, `"pie"`.

> Positive correlations are displayed in blue and negative correlations in red
> color. Color intensity and the size of the circle are proportional to the
> correlation coefficients.

```{r methods}
library(corrplot)
M <- cor(mtcars)
corrplot(M, method = "circle")
corrplot(M, method = "square")
corrplot(M, method = "ellipse")
corrplot(M, method = "number") # Display the correlation coefficient
corrplot(M, method = "shade")
corrplot(M, method = "color")
corrplot(M, method = "pie")
```


Layout
-----------------------------
There are three layout types (parameter `type`):
- `"full"` (default) : display full **correlation matrix**
- `"upper"` : display upper triangular of the **correlation matrix**
- `"lower"` : display lower triangular of the **correlation matrix**

```{r layout}
corrplot(M, type = "upper")
corrplot(M, type = "upper")
```

`corrplot.mixed()` is a wrapped function for mixed visualization style.
```{r mixed}
corrplot.mixed(M)
corrplot.mixed(M, lower.col = "black", number.cex = .7)
corrplot.mixed(M, lower = "ellipse", upper = "circle")
corrplot.mixed(M, lower = "square", upper = "circle", tl.col = "black")
```


Reorder a correlation matrix
----------------------------
The correlation matrix can be reordered according to the correlation
coefficient. This is important to identify the hidden structure and pattern in
the matrix. There are four methods in corrplot (parameter `order`), named 
`"AOE"`, `"FPC"`, `"hclust"`, `"alphabet"`.  More algorithms can be found in 
[seriation](cran.r-project.org/package=seriation) package.

You can also reorder the matrix "manually" via function `corrMatOrder()`.

  - `"AOE"` is for the angular order of the eigenvectors. It is calculated from 
    the order of the angles $a_i$,
    
    $$
    a_i = 
    \begin{cases}
    			\tan (e_{i2}/e_{i1}), & \text{if $e_{i1}>0$;}
    			 \newline
    			\tan (e_{i2}/e_{i1}) + \pi, & \text{otherwise.}
    \end{cases}			
    $$
    
    where $e_1$ and $e_2$ are the largest two eigenvalues of the correlation 
    matrix.
    See [Michael Friendly (2002)](http://www.datavis.ca/papers/corrgram.pdf)
    for details.
  
  - `"FPC"` for the first principal component order.
  
  - `"hclust"` for hierarchical clustering order, and `"hclust.method"` for the
    agglomeration method to be used. `"hclust.method"` should be one of
    `"ward"`, `"single"`, `"complete"`, `"average"`, `"mcquitty"`, `"median"` or
    `"centroid"`.

  - `"alphabet"` for alphabetical order.

```{r order}
corrplot(M, order = "AOE")
corrplot(M, order = "hclust")
corrplot(M, order = "FPC")
corrplot(M, order = "alphabet")
```

If using `"hclust"`, `corrplot()` can  draw rectangles around the chart of corrrlation matrix based on the results of  hierarchical clustering.

```{r rectangles}
corrplot(M, order = "hclust", addrect = 2)
corrplot(M, order = "hclust", addrect = 3)
```
```{r hclust-lightblue}
# Change background color to lightblue
corrplot(M, type = "upper", order = "hclust",
         col = c("black", "white"), bg = "lightblue")
```


Using different color spectra
------------------------------
As shown in the above section, the color of the correlogram can be customized.
The function `colorRampPalette()` is very convenient for generating color
spectrum.

```{r color}
col1 <- colorRampPalette(c("#7F0000", "red", "#FF7F00", "yellow", "white",
                           "cyan", "#007FFF", "blue", "#00007F"))
col2 <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", "#F4A582",
                           "#FDDBC7", "#FFFFFF", "#D1E5F0", "#92C5DE",
                           "#4393C3", "#2166AC", "#053061"))
col3 <- colorRampPalette(c("red", "white", "blue"))	
col4 <- colorRampPalette(c("#7F0000", "red", "#FF7F00", "yellow", "#7FFF7F",
                           "cyan", "#007FFF", "blue", "#00007F"))
whiteblack <- c("white", "black")

## using these color spectra
corrplot(M, order = "hclust", addrect = 2, col = col1(100))
corrplot(M, order = "hclust", addrect = 2, col = col2(50))
corrplot(M, order = "hclust", addrect = 2, col = col3(20))
corrplot(M, order = "hclust", addrect = 2, col = col4(10))
corrplot(M, order = "hclust", addrect = 2, col = whiteblack, bg = "gold2")
```

You can also use the standard color palettes (package `grDevices`)
```{r hclust-stdcolors}
corrplot(M, order = "hclust", addrect = 2, col = heat.colors(100))
corrplot(M, order = "hclust", addrect = 2, col = terrain.colors(100))
corrplot(M, order = "hclust", addrect = 2, col = cm.colors(100))
corrplot(M, order = "hclust", addrect = 2, col = gray.colors(100))
```

Other option would be to use `RcolorBrewer` package.
```{r hclust-rcolorbrewer}
library(RColorBrewer)

corrplot(M, type = "upper", order = "hclust",
         col = brewer.pal(n = 8, name = "RdBu"))
corrplot(M, type = "upper", order = "hclust",
         col = brewer.pal(n = 8, name = "RdYlBu"))
corrplot(M, type = "upper", order = "hclust",
         col = brewer.pal(n = 8, name = "PuOr"))
```



Changing color and rotation of text labels and legend
-----------------------------------------------------
Parameter `cl.*` is for color legend, and `tl.*` if for text legend. For the
text label, `tl.col` (text label color) and `tl.srt` (text label string 
rotation) are used to change text colors and rotations.

Here are some examples.
```{r color-label}
## remove color legend and text legend 
corrplot(M, order = "AOE", cl.pos = "n", tl.pos = "n")  

## bottom  color legend, diagonal text legend, rotate text label
corrplot(M, order = "AOE", cl.pos = "b", tl.pos = "d", tl.srt = 60)

## a wider color legend with numbers right aligned
corrplot(M, order = "AOE", cl.ratio = 0.2, cl.align = "r")

## text labels rotated 45 degrees
corrplot(M, type = "lower", order = "hclust", tl.col = "black", tl.srt = 45)
```


Dealing with a non-correlation matrix
-------------------------------------
```{r non-corr}
corrplot(abs(M),order = "AOE", col = col3(200), cl.lim = c(0, 1))
## visualize a  matrix in [-100, 100]
ran <- round(matrix(runif(225, -100,100), 15))
corrplot(ran, is.corr = FALSE, method = "square")
## a beautiful color legend 
corrplot(ran, is.corr = FALSE, method = "ellipse", cl.lim = c(-100, 100))
```

If your matrix is rectangular, you can adjust the aspect ratio with the
`win.asp` parameter to make the matrix rendered as a square.
```{r non-corr-asp}
ran <- matrix(rnorm(70), ncol = 7)
corrplot(ran, is.corr = FALSE, win.asp = .7, method = "circle")
```

Dealing with missing (NA) values
--------------------------------
By default, **corrplot** renders NA values as `"?"` characters. Using `na.label`
parameter, it is possible to use a different value (max. two characters are
supported).

```{r NAs}
M2 <- M
diag(M2) = NA
corrplot(M2)
corrplot(M2, na.label = "o")
corrplot(M2, na.label = "NA")
```


Using "plotmath" expressions in labels
--------------------------------------
Since version `0.78`, it is possible to use
[plotmath](https://www.rdocumentation.org/packages/grDevices/topics/plotmath)
expression in variable names. To activate plotmath rendering, prefix your label
with one of the characters `":"`, `"="` or `"$"`.

```{r plotmath}
M2 <- M[1:5,1:5]
colnames(M2) <- c("alpha", "beta", ":alpha+beta", ":a[0]", "=a[beta]")
rownames(M2) <- c("alpha", "beta", NA, "$a[0]", "$ a[beta]")
corrplot(M2)
```


Combining correlogram with the significance test
------------------------------------------------
```{r test}
res1 <- cor.mtest(mtcars, conf.level = .95)
res2 <- cor.mtest(mtcars, conf.level = .99)

## specialized the insignificant value according to the significant level
corrplot(M, p.mat = res1$p, sig.level = .2)
corrplot(M, p.mat = res1$p, sig.level = .05)
corrplot(M, p.mat = res1$p, sig.level = .01)

## leave blank on no significant coefficient
corrplot(M, p.mat = res1$p, insig = "blank")

## add p-values on no significant coefficient
corrplot(M, p.mat = res1$p, insig = "p-value")

## add all p-values
corrplot(M, p.mat = res1$p, insig = "p-value", sig.level = -1)

## add cross on no significant coefficient 
corrplot(M, p.mat = res1$p, order = "hclust", insig = "pch", addrect = 3)
```


Visualize confidence interval
-----------------------------
```{r ci}
corrplot(M, low = res1$lowCI, upp = res1$uppCI, order = "hclust",
         rect.col = "navy", plotC = "rect", cl.pos = "n")
corrplot(M, p.mat = res1$p, low = res1$lowCI, upp = res1$uppCI,
         order = "hclust", pch.col = "red", sig.level = 0.01,
         addrect = 3, rect.col = "navy", plotC = "rect", cl.pos = "n")
```

```{r ci_with_label}
res1 <- cor.mtest(mtcars, conf.level = .95)

corrplot(M, p.mat = res1$p, insig = "label_sig",
         sig.level = c(.001, .01, .05), pch.cex = .9, pch.col = "white")
corrplot(M, p.mat = res1$p, method = "color",
         insig = "label_sig", pch.col = "white")
corrplot(M, p.mat = res1$p, method = "color", type = "upper",
         sig.level = c(.001, .01, .05), pch.cex = .9,
         insig = "label_sig", pch.col = "white", order = "AOE")
corrplot(M, p.mat = res1$p, insig = "label_sig", pch.col = "white",
         pch = "p<.05", pch.cex = .5, order = "AOE")
```

Customize the correlogram
-------------------------
```{r pmat}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(mtcars)$p
head(p.mat[, 1:5])

# Specialized the insignificant value according to the significant level
corrplot(M, type = "upper", order = "hclust", 
         p.mat = p.mat, sig.level = 0.01)

# Leave blank on no significant coefficient
corrplot(M, type = "upper", order = "hclust", 
         p.mat = p.mat, sig.level = 0.01, insig = "blank")
```
In the above figure, correlations with **p-value > 0.01** are considered as
insignificant. In this case the correlation coefficient values are leaved blank
or crosses are added.

```{r customized}
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color", col = col(200),
         type = "upper", order = "hclust", number.cex = .7,
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "black", tl.srt = 90, # Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag = FALSE)
```

**Note:** Some of the plots were taken from [this blog].

[this blog]: http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram

Explore Large Feature Matrices
------------------------------
```{r large_matrix}

# generating large feature matrix (cols=features, rows=samples)
num_features <- 60 # how many features
num_samples <- 300 # how many samples
DATASET <- matrix(runif(num_features * num_samples),
               nrow = num_samples, ncol = num_features)

# setting some dummy names for the features e.g. f23
colnames(DATASET) <- paste0("f", 1:ncol(DATASET))

# let's make 30% of all features to be correlated with feature "f1"
num_feat_corr <- num_features * .3
idx_correlated_features <- as.integer(seq(from = 1,
                                          to = num_features,
                                          length.out = num_feat_corr))[-1]
for (i in idx_correlated_features) {
  DATASET[,i] <- DATASET[,1] + runif(num_samples) # adding some noise
}

corrplot(cor(DATASET), diag = FALSE, order = "FPC",
         tl.pos = "td", tl.cex = 0.5, method = "color", type = "upper")
```
---
title: "Structural Models (EFA, CFA, SEM...)"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, efa, cfa, factor analysis, sem, fa, pca, how many factors, n factors]
vignette: >
  %\VignetteIndexEntry{Structural Models (EFA, CFA, SEM, ...)}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("see", quietly = TRUE) ||
    !requireNamespace("lavaan", quietly = TRUE) ||
    !requireNamespace("performance", quietly = TRUE) ||
    !requireNamespace("psych", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

set.seed(333)
```


# How to perform a Factor Analysis (FA)

The difference between PCA and EFA can be quite hard to intuitively grasp as their output is very familiar. The idea is that PCA aims at extracting the most variance possible from all variables of the dataset, whereas EFA aims at creating consistent factors from the dataset without desperately trying to represent all the variables. 

This is why PCA is popular for feature reduction, as it will try to best represent the variance contained in the original data, minimizing the loss of information. On the other hand, EFA is usually in the context of exploring the latent dimensions that might be hidden in the observed variables, without necessary striving at representing the whole dataset.

To illustrate EFA, let us use the [International Personality Item Pool](https://ipip.ori.org/) data available in the [`psych`](https://www.personality-project.org/r/html/bfi.html) package. It includes 25 personality self report items. The authors built these items following the **big 5** personality structure.

## Factor Structure (Sphericity and KMO)

The first step is to test the dataset for factor analysis suitability. Two existing methods are the **Bartlett's Test of Sphericity** and the **Kaiser, Meyer, Olkin (KMO) Measure of Sampling Adequacy (MSA)**. The former tests whether a matrix is significantly different from an identity matrix. This statistical test for the presence of correlations among variables, providing the statistical probability that the correlation matrix has significant correlations among at least some of variables. As for factor analysis to work, some relationships between variables are needed, thus, a significant Bartlett's test of sphericity is required, say *p* < .001. The latter was introduced by Kaiser (1970) as the Measure of Sampling Adequacy (MSA), later modified by Kaiser and Rice (1974). The Kaiser-Meyer-Olkin (KMO) statistic, which can vary from 0 to 1, indicates the degree to which each variable in a set is predicted without error by the other variables. A value of 0 indicates that the sum of partial correlations is large relative to the sum correlations, indicating factor analysis is likely to be inappropriate. A KMO value close to 1 indicates that the sum of partial correlations is not large relative to the sum of correlations and so factor analysis should yield distinct and reliable factors.

Both tests can be performed by using the `check_factorstructure()` function.

```{r message=FALSE, warning=FALSE}
library(parameters)
library(dplyr)
library(psych)

# Load the data
data <- psych::bfi[, 1:25]  # Select only the 25 first columns corresponding to the items
data <- na.omit(data)  # remove missing values

# Check factor structure
check_factorstructure(data)
```



## Exploratory Factor Analysis (EFA)

Now that we are confident that our dataset is appropriate, we will explore a factor structure made of 5 latent variables, corresponding to the items' authors theory of personality.

```{r message=FALSE, warning=FALSE}
# Fit an EFA
efa <- psych::fa(data, nfactors = 5) %>% 
  model_parameters(sort = TRUE, threshold = "max")
efa
```

As we can see, the 25 items nicely spread on the 5 latent factors, the famous **big 5**. Based on this model, we can now predict back the scores for each individual for these new variables:

```{r message=FALSE, warning=FALSE, eval=FALSE}
predict(efa, names = c("Neuroticism", "Conscientiousness", "Extraversion", "Agreeableness", "Opennness"))
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
head(predict(efa, names = c("Neuroticism", "Conscientiousness", "Extraversion", "Agreeableness", "Opennness")), 5)
```

## How many factors to retain in Factor Analysis (FA)


When running a **factor analysis (FA)**, one often needs to specify **how many components** (or latent variables) to retain or to extract. This decision is often motivated or supported by some statistical indices and procedures aiming at finding the optimal number of factors. 

Interestingly, a huge amount of methods exist to statistically address this issue, giving sometimes very different results... Unfortunately, there is no consensus on **which method to use**, or which is the best.


### The Method Agreement procedure

The Method Agreement procedure, first implemented in the [`psycho`](https://neuropsychology.github.io/psycho.R/2018/05/24/n_factors.html) package [@makowski2018psycho], proposes to rely on the consensus of methods, rather than on one method in particular.


This procedure can be easily used via the `n_factors()` function, re-implemented and improved in the [**parameters**](https://github.com/easystats/parameters) package. One can provide a dataframe, and the function will run a large number of routines and return the optimal number of factors based on the higher consensus.  


```{r message=FALSE, warning=FALSE}
n <- n_factors(data)
n
```

Interestingly, the smallest nubmer of factors that most methods suggest is 6... Which is consistent whith the newer models of personality (e.g., HEXACO).

More details, as well as a summary table can be obtained as follows:

```{r message=FALSE, warning=FALSE}
as.data.frame(n)
summary(n)
```


A plot can also be obtained (the `see` package must be loaded):

```{r message=FALSE, warning=FALSE}
library(see)

plot(n) + theme_modern()
```


## Confirmatory Factor Analysis (CFA)

We've seen above that while an EFA with 5 latent variables works great on our dataset, a structure with 6 latent factors might in fact be more appropriate. How can we **statistically test** if that's actually the case? This can be done using Confirmatory Factor Analysis (CFA), that bridges factor analysis with Structural Equation Modelling (SEM).

However, in order to do that cleanly, EFA should be independent from CFA, in the sense that the factor structure should be explored on a **"training" set**, and then tested (or "confirmed") on a **test set**. In other words, the dataset used for exploration and confirmation is not the same. Note that this procedure is also standard in the field of machine learning.

### Partition the data

The data can be easily split into two sets with the `data_partition()` function, through which we will use 70\% of the sample as training and the rest as test.

```{r message=FALSE, warning=FALSE}
partitions <- data_partition(data, training_proportion = 0.7)
training <- partitions$training
test <- partitions$test
```

### Create CFA structures out of EFA models

In the next step, we will run two EFA models on the training set, specifying 5 and 6 latent factors respectively, that we will then transform into CFA structures. 

```{r message=FALSE, warning=FALSE}
structure_big5 <- psych::fa(training, nfactors = 5) %>% 
  efa_to_cfa()
structure_big6 <- psych::fa(training, nfactors = 6)  %>% 
  efa_to_cfa()

# Investigate how a model looks
structure_big5
```

As we can see, a structure is just a string encoding how the **manifest variables** (the observed variables) are integrated into latent variables.


### Fit and Compare models

We can finally with that structure to the test set using the `lavaan` package, and compare these models together:

```{r message=FALSE, warning=FALSE}
library(lavaan)
library(performance)

big5 <- lavaan::cfa(structure_big5, data = test)
big6 <- lavaan::cfa(structure_big6, data = test)

performance::compare_performance(big5, big6)
```

All in all, it seems that the big 5 structure remains quite reliable.


# References
---
title: "Feature Reduction (PCA, cMDS, ICA...)"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable extraction, feature extraction, dimension extraction]
vignette: >
  %\VignetteIndexEntry{Feature Reduction (PCA, cMDS, ICA, ...)}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("psych", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(dplyr)
}

set.seed(333)
```




Also known as [**feature extraction** or **dimension reduction**](https://en.wikipedia.org/wiki/Feature_extraction) in machine learning, the goal of variable reduction is to **reduce the number of predictors** by derivating, from a set of measured data, new variables intended to be informative and non-redundant. This method can be used to **simplify models**, which can benefit model interpretation, shorten fitting time, and improve generalization (by reducing overfitting).


## Quick and Exploratory Method

Let's start by fitting a multiple regression with the `attitude` dataset, available is base R, to predict the overall **rating** by employees of their organization with the remaining variables (handling of employee **complaints**, special **privileges**, opportunity of **learning**, **raises**, a feedback considered too **critical** and opportunity of **advancement**).

```{r message=FALSE, warning=FALSE}
model <- lm(rating ~ ., data = attitude)
parameters(model)
```

We can explore a reduction of the number of parameters with the `reduce_parameters()` function. 

```{r message=FALSE, warning=FALSE}
newmodel <- reduce_parameters(model)
parameters(newmodel)
```

This quickly *hints* toward the fact that the model could be represented via **two "latent" dimensions**, one correlated with all the positive things that a company has to offer, and the other one related to the amount of negative critiques received by the employees. These two dimensions have a positive and negative relationship with the company rating, respectively.

> What does `reduce_parameters()` exactly do?

This function performs a reduction in the parameters space (the number of variables). It starts by creating a new set of variables, based on a given method (the default method is "**PCA**", but other are available via the `method` argument, such as "**cMDS**", "**DRR**" or "**ICA**"). Then, it names this new dimensions using the original variables that *correlate* the most with it. For instance, a variable named 'V1_0.97/V4_-0.88' means that the V1 and the V4 variables correlate maximally (with respective coefficients of .97 and -.88) with this dimension.

```{r message=FALSE, warning=FALSE}
reduce_parameters(model, method = "cMDS") %>% 
  parameters()
```

A different method (**Classical Multidimensional Scaling - cMDS**) suggests that negative critiques do not have a significant impact on the rating, and that the lack of opportunities of career advancement is a separate dimension with an importance on its own.

Although this function can be useful in exploratory data analysis, it's best to perform the dimension reduction step in a **separate and dedicated stage**, as this is a very important process in the data analysis workflow.


## Principal Component Analysis (PCA)

PCA is a widely used procedure that lies in-between dimension reduction and structural modelling. Indeed, one of the way of reducing the number of predictors is to extract a new set of uncorrelated variables that will *represent* variance of your initial dataset. But how the original variables relate between themselves can also be a question on its own.

We can apply the `principal_components()` function to do the the predictors of the model:

```{r message=FALSE, warning=FALSE}
pca <- principal_components(insight::get_predictors(model), n = "auto")
pca
```

The `principal_component()` function automatically selected one component (if the number of components is not specified, this function uses [`n_factors()`](https://easystats.github.io/parameters/reference/n_factors.html) to estimate the optimal number to keep) and returned the **loadings**, i.e., the relationship with all of the original variables. 

As we can see here, it seems that our new component captured the essence (more than half of the total variance present in the original dataset) of all our other variables together. We can **extract** the values of this component for each of our observation using the `predict()` method and add in the response variable of our initial dataset.

```{r message=FALSE, warning=FALSE}
newdata <- predict(pca)
newdata$rating <- attitude$rating
```

We can know update the model with this new component:
```{r message=FALSE, warning=FALSE}
update(model, rating ~ PC1, data = newdata) %>% 
  parameters()
```


### Using the `psych` package for PCA

You can also use different packages for models, such as [`psych`](https://cran.r-project.org/package=psych) [@revelle2018] or [`FactoMineR`](http://factominer.free.fr/) for PCA or Exploratory Factor Analysis (EFA), as it allows for more flexibility, control and details when running such procedures. Thus, the functions from this package are **fully supported** by `parameters` through the `model_parameters()` function.

As such, the above analysis can be fully reproduced as follows:

```{r message=FALSE, warning=FALSE}
library(psych)

# Fit the PCA
pca <- psych::principal(attitude, nfactors = 1) %>% 
  model_parameters()
pca
```

*Note:* By default, `psych::principal()` uses a **varimax** rotation to extract rotated components, possibly leading to discrepancies in the results.

Finally, refit the model:

```{r message=FALSE, warning=FALSE}
df <- cbind(attitude, predict(pca))

update(model, rating ~ PC1, data = df) %>% 
  model_parameters()
```

# References---
title: "Robust Estimation of Standard Errors, Confidence Intervals and p-values"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable selection, feature selection]
vignette: >
  %\VignetteIndexEntry{Robust Estimation of Standard Errors}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("sandwich", quietly = TRUE) ||
    !requireNamespace("lme4", quietly = TRUE) ||
    !requireNamespace("clubSandwich", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(dplyr)
}

set.seed(333)
```

The [`model_parameters()`](https://easystats.github.io/parameters/articles/model_parameters.html) function also allows the computation of standard errors, confidence intervals and p-values based on robust covariance matrix estimation from model parameters. Robust estimation is based on the packages **sandwich** and **clubSandwich**, so all models supported by either of these packages work with `model_parameters()` when `robust = TRUE`.

## Classical Regression Models

### Robust Covariance Matrix Estimation from Model Parameters

By default, when `model_parameters(robust = TRUE)`, it internally calls `sandwich::vcovHC(type = "HC3")`. However, there are three arguments that allow for choosing different methods and options of robust estimation: `vcov_estimation`, `vcov_type` and `vcov_args` (see [`?standard_error_robust`](https://easystats.github.io/parameters/reference/standard_error_robust.html) for further details).

Let us start with a simple example, which uses a heteroskedasticity-consistent covariance matrix estimation with estimation-type "HC3" (i.e. `sandwich::vcovHC(type = "HC3")` is called):

```{r}
data(iris)
model <- lm(Petal.Length ~ Sepal.Length * Species + Sepal.Width, data = iris)

# model parameters, where SE, CI and p-values are based on robust estimation
mp <- model_parameters(model, robust = TRUE)
mp

# compare standard errors to result from sandwich-package
mp$SE
unname(sqrt(diag(sandwich::vcovHC(model))))
```

### Cluster-Robust Covariance Matrix Estimation (sandwich)

If another covariance matrix estimation is required, use the `vcov_estimation`-argument. This argument needs the suffix for the related `vcov*()`-functions as value, i.e. `vcov_estimation = "CL"` would call `sandwich::vcovCL()`, or `vcov_estimation = "HAC"` would call `sandwich::vcovHAC()`.

The specific estimation type can be changed with `vcov_type`. E.g., `sandwich::vcovCL()` accepts estimation types HC0 to HC3. In the next example, we use a clustered covariance matrix estimation with HC1-estimation type.

```{r}
# change estimation-type
mp <- model_parameters(model, robust = TRUE, vcov_estimation = "CL", vcov_type = "HC1")
mp

# compare standard errors to result from sandwich-package
mp$SE
unname(sqrt(diag(sandwich::vcovCL(model))))
```

Usually, clustered covariance matrix estimation is used when there is a cluster-structure in the data. The variable indicating the cluster-structure can be defined in `sandwich::vcovCL()` with the `cluster`-argument. In `model_parameters()`, additional arguments that should be passed down to functions from the **sandwich** package can be specified in `vcov_args`:

```{r}
iris$cluster <- factor(rep(LETTERS[1:8], length.out = nrow(iris)))
# change estimation-type, defining additional arguments
mp <- model_parameters(
  model, 
  robust = TRUE, 
  vcov_estimation = "CL", 
  vcov_type = "HC1",
  vcov_args = list(cluster = iris$cluster)
)
mp

# compare standard errors to result from sandwich-package
mp$SE
unname(sqrt(diag(sandwich::vcovCL(model, cluster = iris$cluster))))
```

### Cluster-Robust Covariance Matrix Estimation (clubSandwich)

Cluster-robust estimation of the variance-covariance matrix can also be achieved using `clubSandwich::vcovCR()`. Thus, when `vcov_estimation = "CR"`, the related function from the **clubSandwich** package is called. Note that this function _requires_ the specification of the `cluster`-argument.

```{r}
# create fake-cluster-variable, to demonstrate cluster robust standard errors
iris$cluster <- factor(rep(LETTERS[1:8], length.out = nrow(iris)))

# cluster-robust estimation
mp <- model_parameters(
  model, 
  robust = TRUE, 
  vcov_estimation = "CR", 
  vcov_type = "CR1", 
  vcov_args = list(cluster = iris$cluster)
)
mp

# compare standard errors to result from clubSsandwich-package
mp$SE
unname(sqrt(diag(clubSandwich::vcovCR(model, type = "CR1", cluster = iris$cluster))))
```

### Robust Covariance Matrix Estimation on Standardized Model Parameters

Finally, robust estimation can be combined with standardization. However, robust covariance matrix estimation only works for `standardize = "refit"`.

```{r}
# model parameters, robust estimation on standardized model
model_parameters(model, standardize = "refit", robust = TRUE)
```

## Mixed Models

### Robust Covariance Matrix Estimation for Mixed Models

For linear mixed models, that by definition have a clustered ("hierarchical" or multilevel) structure in the data, it is also possible to estimate a cluster-robust covariance matrix. This is possible due to the **clubSandwich** package, thus we need to define the same arguments as in the above example.

```{r}
library(lme4)
data(iris)
set.seed(1234)
iris$grp <- as.factor(sample(1:3, nrow(iris), replace = TRUE))

# fit example model
model <- lme4::lmer(
  Sepal.Length ~ Species * Sepal.Width + Petal.Length + (1 | grp),
  data = iris
)

# normal model parameters, like from 'summary()'
model_parameters(model)

# model parameters, cluster robust estimation for mixed models
model_parameters(
  model, 
  robust = TRUE, 
  vcov_estimation = "CR", 
  vcov_type = "CR1", 
  vcov_args = list(cluster = iris$grp)
)
```

### Robust Covariance Matrix Estimation on Standardized Mixed Model Parameters

Again, robust estimation can be combined with standardization for linear mixed models as well, which in such cases also only works for `standardize = "refit"`.

```{r}
# model parameters, cluster robust estimation on standardized mixed model
model_parameters(
  model, 
  standardize = "refit",
  robust = TRUE, 
  vcov_estimation = "CR", 
  vcov_type = "CR1", 
  vcov_args = list(cluster = iris$grp)
)
```
---
title: "Summary of Model Parameters"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable selection, feature selection]
vignette: >
  %\VignetteIndexEntry{Summary of Model Parameters}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("BayesFactor", quietly = TRUE) ||
    !requireNamespace("lme4", quietly = TRUE) ||
    !requireNamespace("metafor", quietly = TRUE) ||
    !requireNamespace("lavaan", quietly = TRUE) ||
    !requireNamespace("brms", quietly = TRUE) ||
    !requireNamespace("psych", quietly = TRUE) ||
    !requireNamespace("rstanarm", quietly = TRUE) ||
    !requireNamespace("FactoMineR", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(dplyr)
}

set.seed(333)
```

The `model_parameters()` function (also accessible via the shortcut `parameters()`) allows you to extract the parameters and their characteristics from various models in a consistent way. It can be considered as a lightweight alternative to [`broom::tidy()`](https://github.com/tidymodels/broom), with some notable differences: 

- The names of the returned data frame are **specific** to their content. For instance, the column containing the statistic is named following the statistic name, i.e., *t*, *z*, etc., instead of a generic name such as *statistic* (**however**, you can get standardized (generic) column names using [`standardize_names()`](https://easystats.github.io/parameters/reference/standardize_names.html)).
- It is able to compute or extract indices not available by default, such as ***p*-values**, **CIs**, etc.
- It includes **feature engineering** capabilities, including parameters [**bootstrapping**](https://easystats.github.io/parameters/reference/bootstrap_model.html).


## Correlations and *t*-tests

### Frequentist

```{r, warning=FALSE, message=FALSE}
cor.test(iris$Sepal.Length, iris$Sepal.Width) %>% 
  parameters()
```

```{r, warning=FALSE, message=FALSE}
t.test(mpg ~ vs, data = mtcars) %>% 
  parameters()
```


### Bayesian

```{r, warning=FALSE, message=FALSE}
library(BayesFactor)

BayesFactor::correlationBF(iris$Sepal.Length, iris$Sepal.Width) %>% 
  parameters()
```

```{r, warning=FALSE, message=FALSE}
BayesFactor::ttestBF(formula = mpg ~ vs, data = mtcars) %>% 
  parameters()
```


## ANOVAs

Indices of effect size for ANOVAs, such as partial and non-partial versions of `eta_squared()`, `epsilon_sqared()` or `omega_squared()`, were moved to the [**effectsize**-package](https://easystats.github.io/effectsize/). However, **parameters** uses these function to compute such indices for parameters summaries.

### Simple

```{r, warning=FALSE, message=FALSE}
aov(Sepal.Length ~ Species, data = iris) %>%
  parameters(omega_squared = "partial", eta_squared = "partial", epsilon_squared = "partial")
```

### Repeated measures

`parameters()` (resp. its alias `model_parameters()`) also works on repeated measures ANOVAs, whether computed from `aov()` or from a mixed model.

```{r, warning=FALSE, message=FALSE}
aov(mpg ~ am + Error(gear), data = mtcars) %>%
  parameters()
```


## Regressions (GLMs, Mixed Models, GAMs, ...)

`parameters()` (resp. its alias `model_parameters()`) was mainly built with regression models in mind. It works for many types of models and packages, including mixed models and Bayesian models.

### GLMs

```{r, warning=FALSE, message=FALSE}
glm(vs ~ poly(mpg, 2) + cyl, data = mtcars) %>% 
  parameters()
```

### Mixed Models

```{r, warning=FALSE, message=FALSE}
library(lme4)

lmer(Sepal.Width ~ Petal.Length + (1|Species), data = iris) %>% 
  parameters()
```

### Mixed Model with Zero-Inflation Model

```{r, warning=FALSE, message=FALSE}
library(GLMMadaptive)
library(glmmTMB)
data("Salamanders")
model <- mixed_model(
  count ~ spp + mined,
  random = ~1 | site,
  zi_fixed = ~spp + mined,
  family = zi.negative.binomial(), 
  data = Salamanders
)
parameters(model)
```

### Mixed Models with Dispersion Model

```{r, warning=FALSE, message=FALSE}
library(glmmTMB)
sim1 <- function(nfac = 40, nt = 100, facsd = 0.1, tsd = 0.15, mu = 0, residsd = 1) {
  dat <- expand.grid(fac = factor(letters[1:nfac]), t = 1:nt)
  n <- nrow(dat)
  dat$REfac <- rnorm(nfac, sd = facsd)[dat$fac]
  dat$REt <- rnorm(nt, sd = tsd)[dat$t]
  dat$x <- rnorm(n, mean = mu, sd = residsd) + dat$REfac + dat$REt
  dat
}
set.seed(101)
d1 <- sim1(mu = 100, residsd = 10)
d2 <- sim1(mu = 200, residsd = 5)
d1$sd <- "ten"
d2$sd <- "five"
dat <- rbind(d1, d2)
model <- glmmTMB(x ~ sd + (1 | t), dispformula =  ~ sd, data = dat)

parameters(model)
```

### Bayesian Models

`model_parameters()` works fine with Bayesian models from the **rstanarm** package...

```{r, warning=FALSE, message=FALSE, eval = FALSE}
library(rstanarm)

stan_glm(mpg ~ wt * cyl, data = mtcars) %>% 
  parameters()
```
```{r, warning=FALSE, message=FALSE, echo = FALSE}
library(rstanarm)

stan_glm(mpg ~ wt * cyl, data = mtcars, iter = 500, chains = 2, refresh = 0) %>% 
  parameters()
```

... as well as for (more complex) models from the **brms** package. For more complex models, other model components can be printed using the arguments `effects` and `component` arguments.

```{r, warning=FALSE, message=FALSE, eval=FALSE}
library(brms)
data(fish)
set.seed(123)
model <- brm(bf(
   count ~ persons + child + camper + (1 | persons),
   zi ~ child + camper + (1 | persons)
 ),
 data = fish,
 family = zero_inflated_poisson()
)
parameters(model, component = "conditional")
#> Parameter   | Median |         89% CI |     pd | % in ROPE | ESS |  Rhat
#> ------------------------------------------------------------------------
#> b_Intercept |  -0.87 | [-1.49, -0.08] | 96.80% |     4.80% |  78 | 1.000
#> b_persons   |   0.84 | [ 0.60,  1.06] |   100% |        0% |  75 | 0.997
#> b_child     |  -1.16 | [-1.32, -1.00] |   100% |        0% | 107 | 1.027
#> b_camper1   |   0.74 | [ 0.52,  0.91] |   100% |        0% | 224 | 0.993

parameters(model, effects = "all", component = "all")
#> # Fixed Effects (Count Model) 
#> 
#> Parameter   | Median |         89% CI |     pd | % in ROPE | ESS |  Rhat
#> ------------------------------------------------------------------------
#> (Intercept) |  -0.87 | [-1.49, -0.08] | 96.80% |     4.80% |  78 | 1.000
#> persons     |   0.84 | [ 0.60,  1.06] |   100% |        0% |  75 | 0.997
#> child       |  -1.16 | [-1.32, -1.00] |   100% |        0% | 107 | 1.027
#> camper1     |   0.74 | [ 0.52,  0.91] |   100% |        0% | 224 | 0.993
#> 
#> # Fixed Effects (Zero-Inflated Model) 
#> 
#> Parameter   | Median |         89% CI |     pd | % in ROPE | ESS |  Rhat
#> ------------------------------------------------------------------------
#> (Intercept) |  -0.76 | [-1.66,  0.51] | 87.20% |    10.40% |  98 | 0.992
#> child       |   1.87 | [ 1.37,  2.43] |   100% |        0% | 262 | 0.999
#> camper1     |  -0.83 | [-1.44, -0.22] | 99.20% |     0.80% | 168 | 0.997
#> 
#> # Random Effects (Count Model) 
#> 
#> Parameter | Median |        89% CI |     pd | % in ROPE | ESS |  Rhat
#> ---------------------------------------------------------------------
#> persons.1 |  -0.01 | [-0.40, 0.35] | 59.20% |    57.60% |  80 | 1.012
#> persons.2 |   0.03 | [-0.15, 0.33] | 61.60% |    60.80% |  88 | 0.994
#> persons.3 |  -0.02 | [-0.38, 0.11] | 63.20% |    64.80% |  66 | 1.008
#> persons.4 |   0.00 | [-0.51, 0.29] | 51.20% |    62.40% |  76 | 0.992
#> 
#> # Random Effects (Zero-Inflated Model) 
#> 
#> Parameter | Median |         89% CI |     pd | % in ROPE | ESS |  Rhat
#> ----------------------------------------------------------------------
#> persons.1 |   1.38 | [ 0.58,  2.66] | 97.60% |     1.60% | 108 | 0.992
#> persons.2 |   0.27 | [-0.62,  1.40] | 68.80% |    13.60% | 100 | 1.002
#> persons.3 |  -0.11 | [-1.36,  0.86] | 60.80% |    16.80% |  96 | 0.993
#> persons.4 |  -1.19 | [-2.62, -0.31] | 95.20% |     0.80% | 115 | 0.992
```

## Structural Models (PCA, EFA, CFA, SEM...)

The **parameters** package extends the support to structural models.

### Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) 

```{r, warning=FALSE, message=FALSE}
library(psych)

psych::pca(mtcars, nfactors = 3) %>% 
  parameters()
```

```{r, warning=FALSE, message=FALSE, eval = FALSE}
library(FactoMineR)

FactoMineR::FAMD(iris, ncp = 3) %>% 
  parameters()
```
```{r, warning=FALSE, message=FALSE, echo = FALSE}
library(FactoMineR)

FactoMineR::FAMD(iris, ncp = 3, graph = FALSE) %>% 
  parameters()
```


### Confirmatory Factor Analysis (CFA) and Structural Equation Models (SEM)

#### Frequentist

```{r, warning=FALSE, message=FALSE}
library(lavaan)

model <- lavaan::cfa(' visual  =~ x1 + x2 + x3
                       textual =~ x4 + x5 + x6
                       speed   =~ x7 + x8 + x9 ', 
                       data = HolzingerSwineford1939)

model_parameters(model)
```

#### Bayesian

`blavaan` to be done.

## Meta-Analysis

`parameters()` also works for `rma`-objects from the **metafor** package.

```{r, warning=FALSE, message=FALSE}
library(metafor)

mydat <- data.frame(
  effectsize = c(-0.393, 0.675, 0.282, -1.398),
  standarderror = c(0.317, 0.317, 0.13, 0.36)
)

rma(yi = effectsize, sei = standarderror, method = "REML", data = mydat) %>% 
  model_parameters()
```

## Plotting Model Parameters

There is a `plot()`-method implemented in the [**see**-package](https://easystats.github.io/see/). Several examples are shown [in this vignette](https://easystats.github.io/see/articles/parameters.html).
---
title: "Analysing Longitudinal or Panel Data"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
vignette: >
  %\VignetteIndexEntry{Analysing Longitudinal or Panel Data}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  eval = TRUE
)

if (!requireNamespace("lme4", quietly = TRUE) ||
    !requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("ggplot2", quietly = TRUE) ||
    !requireNamespace("see", quietly = TRUE) ||
    !requireNamespace("lfe", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(lme4)
  library(lfe)
}

set.seed(333)
```

This vignette explains the rational behind the [`demean()`](https://easystats.github.io/parameters/reference/demean.html) function. We give recommendations how to analyze multilevel or hierarchical data structures, when macro-indicators (or level-2 predictors, or higher-level units, or more general: _group-level predictors_) are used as covariates and the model suffers from **heterogeneity bias** [@bell_explaining_2015].

# Sample data used in this vignette

```{r}
library(parameters)
data("qol_cancer")
```

* Variables:
  * `QoL`      : Response (quality of life of patient)
  * `phq4`     : Patient Health Questionnaire, **time-varying** variable
  * `hospital` : Location of treatment, **time-invariant** variable, co-variate
  * `education`: Educational level, **time-invariant** variable, co-variate
  * `ID`       : patient ID
  * `time`     : time-point of measurement

# Heterogeneity bias

Heterogeneity bias occurs when group-level predictors vary within and across groups, and hence
fixed effects may correlate with group (or random) effects. This is a typical situation when analyzing longitudinal or panel data: Due to the repeated measurements of persons, the "person" (or subject-ID) is now a level-2 variable. Predictors at level-1 ("fixed effects"), e.g.  self-rated health or income, now have an effect at level-1 ("within"-effect) and at higher-level units (level-2, the subject-level, which is the "between"-effect) (see also [this posting](https://shouldbewriting.netlify.com/posts/2019-10-21-accounting-for-within-and-between-subject-effect/)). This inevitably leads to correlating fixed effects and error terms - which, in turn, results in biased estimates, because both the within- *and* between-effect are captured in *one* estimate.

You can check if your model may suffer from heterogeneity bias using the `check_heterogeneity()` function:

```{r}
check_heterogeneity(qol_cancer, select = c("phq4", "education"), group = "ID")
```

# Adressing heterogeneity bias: the Fixed Effects Regression (FE) approach

Fixed effects regression models (FE) are a popular approach for panel data analysis in particular in econometrics and considered as gold standard. To avoid the problem of heterogeneity bias, in FE all higher-level variance (and thus, any between-effects), "are controlled out using the higher-level entities themselves, included in the model as dummy variables" [@bell_explaining_2015]. As a consequence, FE models are only able estimate _within-effects_.

To remove between-effects and only model within-effects, the data needs some preparation: _de-meaning_. De-meaning, or _person-mean centering_, or _centering within clusters_, takes away the higher-level mean from the regression equation, and as such, FE avoids estimating a parameter for each higher-level unit.

## Computing the de-meaned and group-meaned variables

```{r}
qol_cancer <- cbind(
  qol_cancer,
  demean(qol_cancer, select = c("phq4", "QoL"), group = "ID")
)
```

Now we have:

  * `phq4_between`: time-varying variable with the mean of `phq4` across all time-points, for each patient (ID).
  * `phq4_within`: the de-meaned time-varying variable `phq4`.

A FE model is a classical linear model, where

 * Intercept is removed
  * time-invariant predictors are not allowed to be included
  * the group-level factor is included as predictor
  * time-varying predictors are de-meaned ("person-mean centered", indicating the "within-subject" effect)

```{r}
fe_model1 <- lm(
  QoL ~ 0 + time + phq4_within + ID,
   data = qol_cancer
)
# we use only the first two rows, because the remaining rows are
# the estimates for "ID", which is not of interest here...
model_parameters(fe_model1)[1:2, ]


# instead of removing the intercept, we could also use the 
# de-meaned response...
fe_model2 <- lm(
  QoL_within ~ time + phq4_within + ID,
   data = qol_cancer
)
model_parameters(fe_model2)[2:3, ]


# we compare the results with those from the "lfe"-package for panel data
library(lfe)
fe_model3 <- felm(
  QoL ~ time + phq4 | ID,
   data = qol_cancer
)
model_parameters(fe_model3)
```

As we can see, the _within-effect_ of PHQ-4 is `-3.66`, hence the mean of the change for an average individual case in our sample (or, the "net" effect), is `-3.66`.

But what about the between-effect? How do people with higher PHQ-4 score differ from people with lower PHQ-4 score? Or what about educational inequalities? Do higher educated people have a higher PHQ-4 score than lower educated people?

This question cannot be answered with FE regression. But: "Can one fit a multilevel model with varying intercepts (or coefficients) when the units and predictors correlate? The answer is yes. And the solution is simple." [@bafumi_fitting_2006]

# Adressing heterogeneity bias: the Mixed Model approach

Mixed models include different levels of sources of variability (i.e. error terms at each level). Predictors used at level-1 that are varying across higher-level units will thus have residual errors at both level-1 and higher-level units. "Such covariates contain two parts: one that is specific to the higher-level entity that does not vary between occasions, and one that represents the difference between occasions, within higher-level entities" [@bell_explaining_2015]. Hence, the error terms will be correlated with the covariate, which violates one of the assumptions of mixed models (iid, independent and identically distributed error terms) - also known and described above as _heterogeneity bias_.

But how can this issue be addressed outside the FE framework?

There are several ways how to address this using a mixed models approach: 

  * Correlated group factors and predictors are no problem anyway, because [partial pooling](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/) allows estimates of units o borrow strength from the whole sample and shrink toward a common mean (@shor_bayesian_2007).
  * If predictor and group factors correlate, one can remove this correlation by group-meaning [or "mean within clusters", @bafumi_fitting_2006; @gelman_data_2007, Chap. 12.6.].
  * When time-varying predictors are "decomposed" into their time-varying and time-invariant components (demeaning), then mixed models can model **both** within- and between-subject effects [@bell_fixed_2019] - this approach is essentially a further development of a long-known recommendation by Mundlak [@mundlak_pooling_1978].

For now, we will follow the last recommendation and use the within- and between-version of `phq4`.

```{r}
library(lme4)
mixed_1 <- lmer(
  QoL ~ time + phq4_within + phq4_between + (1 | ID),
   data = qol_cancer
)
model_parameters(mixed_1)

# compare to FE-model
model_parameters(fe_model1)[1:2, ]
```

As we can see, the estimates and standard errors are identical. The argument _against_ the use of mixed models, i.e. that using mixed models for panel data will yield biased estimates and standard errors, is based on an incorrect model specification [@mundlak_pooling_1978]. As such, when the (mixed) model is properly specified, the estimator of the mixed model is identical to the 'within' (i.e. FE) estimator.

As a consequence, we cannot only use the above specified mixed model for panel data, we can even specify more complex models including within-effects, between-effects or random effects variation. A mixed models approach can model the causes of endogeneity explicitly by including the (separated) within- and between-effects of time-varying fixed effects and including time-constant fixed effects.

```{r}
mixed_2 <- lmer(
  QoL ~ time + phq4_within + phq4_between + education + (1 + time | ID),
   data = qol_cancer
)
model_parameters(mixed_2)
```

For more complex models, within-effects will naturally change slightly and are no longer identical to simpler FE models. This is no "bias", but rather the result of building more complex models: FE models lack information of variation in the group-effects or between-subject effects. Furthermore, FE models cannot include random slopes, which means that fixed effects regressions are neglecting "cross-cluster differences in the effects of lower-level controls (which) reduces the precision of estimated context effects, resulting in (...) low statistical power" [@heisig_costs_2017].

# Conclusion: Complex Random Effects Within-Between Models

Depending on the structure of the data, the best approach to analyzing panel data would be a so called "complex random effects within-between" model [@bell_fixed_2019]:

```{r echo=FALSE}
f <- "y<sub>it</sub> = &beta;<sub>0</sub> + &beta;<sub>1W</sub> (x<sub>it</sub> - &#x035E;x<sub>i</sub>) + &beta;<sub>2B</sub> &#x035E;x<sub>i</sub> + &beta;<sub>3</sub> z<sub>i</sub> + &upsilon;<sub>i0</sub> + &upsilon;<sub>i1</sub> (x<sub>it</sub> - &#x035E;x<sub>i</sub>) + &epsilon;<sub>it</sub>"
knitr::asis_output(f)
```

```{r echo=FALSE}
f <- "<ul><li>x<sub>it</sub> - &#x035E;x<sub>i</sub> is the de-meaned predictor, <em>phq4_within</em></li><li>&#x035E;x<sub>i</sub> is the group-meaned predictor, <em>phq4_between</em></li><li>&beta;<sub>1W</sub> is the coefficient for phq4_within (within-subject)</li><li>&beta;<sub>2B</sub> is the coefficient for phq4_between (bewteen-subject)</li><li>&beta;<sub>3</sub> is the coefficient for time-constant predictors, such as `hospital` or `education` (bewteen-subject)</li></ul>"
knitr::asis_output(f)
```

In R-code, the model is written down like this:

```{r}
rewb <- lmer(
  QoL ~ time + phq4_within + phq4_between + education + 
    (1 + time | ID) + (1 + phq4_within | ID),
   data = qol_cancer
)
```

**What about time-constant predictors?**

After demeaning time-varying predictors, "at the higher level, the mean term is no longer constrained by Level 1 effects, so it is free to account for all the higher-level variance associated with that variable" [@bell_explaining_2015].

Thus, _time-constant **categorical**_ predictors, that only have a between-effect, can be simply included as fixed effects predictor (since they’re not constrained by level-1 effects).
Time-constant _continuous_ **group-level predictors** (for instance, GDP of countries) should be group-meaned, to have a proper "between"-effect [@gelman_data_2007, Chap. 12.6.].


The benefit of this kind of model is that you have information on within-, between- and other time-constant (i.e. between) effects or group-level predictors...

```{r}
model_parameters(rewb)
```

... but you can also model the variation of (group) effects across time (and probably space), and you can even include more higher-level units (e.g. nested design or cross-classified design with more than two levels):

```{r}
random_parameters(rewb)
```

**What about imbalanced groups, i.e. large differences in N per group?**

See little example after this visual example...

# A visual example

First, we generate some fake data that implies a linear relationship between outcome and independent variable. The objective is that the amount of typing errors depends on how fast (typing speed) you can type, however, the more typing experience you have, the faster you can type. Thus, the outcome measure is "amount of typing errors", while our predictor is "typing speed". Furthermore, we have repeated measurements of people with different "typing experience levels".

The results show that we will have two sources of variation: Overall, more experienced typists make less mistakes (group-level pattern). When typing faster, typists make more mistakes (individual-level pattern).


```{r}
library(ggplot2)
library(dplyr)
library(see)

set.seed(123)
n <- 5
b <- seq(1, 1.5, length.out = 5)
x <- seq(2, 2 * n, 2)

d <- do.call(rbind, lapply(1:n, function(i) {
  data.frame(x = seq(1, n, by = .2),
             y = 2 * x[i] + b[i] * seq(1, n, by = .2) + rnorm(21),
             grp = as.factor(2 * i))
}))

d <- d %>%
  group_by(grp) %>%
  mutate(x = rev(15 - (x + 1.5 * as.numeric(grp)))) %>%
  ungroup()

labs <- c("very slow", "slow", "average", "fast", "very fast")
levels(d$grp) <- rev(labs)

d <- cbind(d, demean(d, c("x", "y"), group = "grp"))
```

Let's look at the raw data...

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_point(colour = "#555555", size = 2.5, alpha = .5) +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

## Model 1: Linear relationship between typing errors and typing speed

We can now assume a (linear) relationship between typing errors and typing speed.

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_point(colour = "#555555", size = 2.5, alpha = .5) +
  geom_smooth(method = "lm", se = F, colour = "#555555") +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

Looking at the coefficients, we have following model with a coefficient of `-1.92`.

```{r}
m1 <- lm(y ~ x, data = d)
model_parameters(m1)
```

However, we have ignored the clustered structure in our data, in this example due to repeated measurements.

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_point(mapping = aes(colour = grp), size = 2.5, alpha = .5) +
  geom_smooth(method = "lm", se = F, colour = "#555555") +
  see::scale_color_flat() +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

## Model 2: Within-subject effect of typing speed

A fixed effects regression (FE-regression) would now remove all between-effects and include only the within-effects as well as the group-level indicator. 

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_smooth(mapping = aes(colour = grp), method = "lm", se = FALSE) +
  geom_point(mapping = aes(colour = grp), size = 2.2, alpha = .6) +
  see::scale_color_flat() +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

This returns the coefficient of the "within"-effect, which is `1.2`, with a standard error of `0.07`. Note that the FE-model does *not* take the variation *between* subjects into account, thus resulting in (possibly) biased estimates, and biased standard errors.

```{r}
m2 <- lm(y ~ 0 + x_within + grp, data = d)
model_parameters(m2)[1, ]
```

## Model 3: Between-subject effect of typing speed

To understand, why the above model 1 (`m1`) returns a biased estimate, which is a "weighted average" of the within- and between-effects, let us look at the between-effect now.

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_point(mapping = aes(colour = grp), size = 2.2, alpha = .6) +
  geom_smooth(mapping = aes(x = x_between, y = y_between), method = "lm", se = F, colour = "#444444") +
  see::scale_color_flat() +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

As we can see, the between-effect is `-2.93`, which is different from the `-1.92` estimated in the model `m1`.

```{r}
m3 <- lm(y ~ x_between, data = d)
model_parameters(m3)
```

## Model 4: Mixed model with within- and between-subjects

Since FE-models can only model within-effects, we now use a mixed model with within- and between-effects.

```{r echo=FALSE}
ggplot(d, aes(x, y)) +
  geom_smooth(mapping = aes(colour = grp), method = "lm", se = FALSE) +
  geom_point(mapping = aes(colour = grp), size = 2.2, alpha = .6) +
  geom_smooth(mapping = aes(x = x_between, y = y_between), method = "lm", se = F, colour = "#444444") +
  see::scale_color_flat() +
  see::theme_modern() +
  labs(x = "Typing Speed", y = "Typing Errors", colour = "Type Experience")
```

We see, the estimate for the within-effects is *not* biased. Furthermore, we get the correct between-effect as well (standard errors differ, because the variance in the grouping structure is more accurately taken into account).

```{r}
m4 <- lmer(y ~ x_between + x_within + (1 | grp), data = d)
model_parameters(m4)
```

## Model 5: Complex Random-Effects Within-Between Model

Finally, we can also take the variation between subjects into account by adding a random slope. This model can be called a complex "REWB" (random-effects within-between) model. Due to the variation between subjects, we get larger standard errors for the within-effect.

```{r}
m5 <- lmer(y ~ x_between + x_within + (1 + x_within | grp), data = d)
model_parameters(m5)
```

# Balanced versus imbalanced groups

The "simple" linear slope of the between-effect (and also from the within-effect) is (almost) identical in "classical" linear regression compared to linear mixed models when the groups are balanced, i.e. when the number of observation per group is similar or the same.

Whenever group size is imbalanced, the "simple" linear slope will be adjusted. This leads to different estimates for between-effects between classical and mixed models regressions due to shrinkage - i.e. for larger variation of group sizes we find stronger regularization of estimates.

Hence, for mixed models with larger differences in number of observation per random effects group, the between-effect will differ from the between-effect calculated by "classical" regression models. However, this shrinkage is a desired property of mixed models and usually improves the estimates.

```{r}
set.seed(123)
n <- 5
b <- seq(1, 1.5, length.out = 5)
x <- seq(2, 2 * n, 2)

d <- do.call(rbind, lapply(1:n, function(i) {
  data.frame(x = seq(1, n, by = .2),
             y = 2 * x[i] + b[i] * seq(1, n, by = .2) + rnorm(21),
             grp = as.factor(2 * i))
}))

# create imbalanced groups
d$grp[sample(which(d$grp == 8), 10)] <- 6
d$grp[sample(which(d$grp == 4), 8)] <- 2
d$grp[sample(which(d$grp == 10), 9)] <- 6

d <- d %>%
  group_by(grp) %>%
  mutate(x = rev(15 - (x + 1.5 * as.numeric(grp)))) %>%
  ungroup()

labs <- c("very slow", "slow", "average", "fast", "very fast")
levels(d$grp) <- rev(labs)

d <- cbind(d, demean(d, c("x", "y"), group = "grp"))

# Between-subject effect of typing speed
m1 <- lm(y ~ x_between, data = d)
model_parameters(m1)

# Between-subject effect of typing speed, accounting for goup structure
m2 <- lmer(y ~ x_between + (1 | grp), data = d)
model_parameters(m2)
```

# References

---
title: "Selection of Model Parameters"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable selection, feature selection]
vignette: >
  %\VignetteIndexEntry{Parameters Selection}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("performance", quietly = TRUE) ||
    !requireNamespace("rstanarm", quietly = TRUE) ||
    !requireNamespace("lme4", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(dplyr)
}

set.seed(333)
```

Also known as [**feature selection**](https://en.wikipedia.org/wiki/Feature_selection) in machine learning, the goal of variable selection is to **identify a subset of predictors** to **simplify models**. This can benefit model interpretation, shorten fitting time, and improve generalization (by reducing overfitting).

There are many different methods. The one that is appropriate for a given problem depends on the model type, the data, the objective and the theoretical rationale.

The `parameters` package implements a helper that will **automatically pick a method deemed appropriate for the provided model**, run the variables selection and return the **optimal formula**, which you can then re-use to update the model.

## Simple linear regression

### Fit a powerful model

If you are familiar with R and the formula interface, you know of the possibility of including a dot (`.`) in the formula, signifying **"all the remaining variables"**. Curiously, few are aware of the possibility of additionally easily adding **all the interaction terms**. This can be achieved using the `.*.` notation.

Let's try that with the linear regression predicting **Sepal.Length** with the [`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset, included by default in R.

```{r message=FALSE, warning=FALSE}
model <- lm(Sepal.Length ~ .*., data = iris)
summary(model)
```

***Wow, that's a lot of parameters! And almost none of them is significant...***

Which is ***weird***, considering that **gorgeous R2! 0.882!** *I wish I had that in my research...*

### Too many parameters?

As you might know, having a **model that is too performant is not always a good thing**. For instance, it can be a marker of [**overfitting**](https://en.wikipedia.org/wiki/Overfitting): the model corresponds too closely to a particular set of data, and may therefore fail to predict future observations reliably. In multiple regressions, in can also fall under the [**Freedman's paradox**](https://en.wikipedia.org/wiki/Freedman%27s_paradox): some predictors that have actually no relation to the dependent variable being predicted will be **spuriously found to be statistically significant**.

Let's run a few checks using the [**performance**](https://github.com/easystats/performance) package:
```{r message=FALSE, warning=FALSE}
library(performance)

check_normality(model)
check_heteroscedasticity(model)
check_autocorrelation(model)
check_collinearity(model)
```

The main issue of the model seems to be the high [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). This suggests that our model might not be able to give valid results about any individual predictor, nor tell which predictors are redundant with respect to others.

### Parameters selection

Time to do some variables selection. This can be easily done using the `select_parameters()` function, that will **automatically select the best variables** and update the model accordingly. One way of using that is in a tidy pipeline (using [`%>%`](https://cran.r-project.org/package=magrittr/README.html)), using this output to update a new model.


```{r message=FALSE, warning=FALSE}
lm(Sepal.Length ~ .*., data = iris) %>% 
  select_parameters() %>% 
  summary()
```


That's still a lot of parameters, but as you can see, but almost all of them are now significant, and the R2 did not change much. 

Although appealing, please note that these automated selection methods are [**quite criticized**](https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df), and should not be used in place of **theoretical** or **hypothetical** reasons (*i.e.*, you should have justified hypotheses about the parameters of your model).



## Mixed and Bayesian models

For simple linear regressions as above, the selection is made using the `step()` function (available in base R). This performs a [**stepwise**](https://en.wikipedia.org/wiki/Stepwise_regression) selection. However, this procedures is not available for other types of models, such as **mixed** or **Bayesian** models.

### Mixed models

For mixed models (of class `merMod`), stepwise selection is based on `cAIC4::stepcAIC()`. This step function only searches the "best" model based on the _random effects structure_, i.e. `select_parameters()` adds or excludes random effects until the cAIC can't be improved further.

```{r message=FALSE, warning=FALSE}
library(lme4)
data("qol_cancer")

# multiple models are checked, however, initial models
# already seems to be the best one...
lmer(
  QoL ~ time + phq4 + age + (1 + time | hospital / ID),
  data = qol_cancer
) %>% 
  select_parameters() %>%
  summary()
```

### Bayesian models

For Bayesian models, `select_parameters()` uses the **projpred** package to select the best parameters for the model.

```{r message=FALSE, warning=FALSE}
library(rstanarm)
model <- stan_glm(
  mpg ~ ., data = mtcars,
  iter = 500, refresh = 0, verbose = FALSE
)
select_parameters(model, cross_validation = TRUE)
```
---
title: "Standardized Model Parameters"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable selection, feature selection]
vignette: >
  %\VignetteIndexEntry{Standardized Model Parameters}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
options(digits = 2)
knitr::opts_chunk$set(comment = "#>")

if (!requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("lme4", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(parameters)
  library(dplyr)
}

set.seed(333)
```

The [`model_parameters()`](https://easystats.github.io/parameters/articles/model_parameters.html) function (also accessible via the shortcut `parameters()`) can be used to calculate standardized model parameters, too, via the `standardize`-argument. There are different methods of standardizing model parameters: `"refit"`, `"posthoc"`, `"smart"` and `"basic"` (see [`?effectsize::standardize_parameters`](https://easystats.github.io/effectsize/reference/standardize_parameters.html) for further details).

## Standardization by re-fitting the model

`standardize = "refit"` is based on a complete model re-fit with a standardized version of data. Hence, this method is equal to standardizing the variables before fitting the model. It is the most accurate (Neter et al., 1989), but it is also the most computationally costly and long (especially for heavy models such as, for instance, for Bayesian models). This method is particularly recommended for complex models that include interactions or transformations (e.g., polynomial or spline terms).

When `standardize = "refit"`, `model_parameters()` internally calls [`effectsize::standardize()`](https://easystats.github.io/effectsize/reference/standardize.html) to standardize the data that was used to fit the model and updates the model with the standardized data. Note that `effectsize::standardize()` tries to detect which variables should be standardized and which not. For instance, having a `log(x)` in the model formula would exclude `x` from being standardized, because `x` might get negative values, and thus `log(x)` would no longer be defined. Factors will also be not standardized. Response variables will be standardized, if appropriate.

```{r}
library(lme4)
data(iris)
set.seed(1234)
iris$grp <- as.factor(sample(1:3, nrow(iris), replace = TRUE))

# fit example model
model <- lme4::lmer(
  Sepal.Length ~ Species * Sepal.Width + Petal.Length + (1 | grp),
  data = iris
)

# classic model parameters
model_parameters(model)

# standardized model parameters
model_parameters(model, standardize = "refit")
```

The second output is identical to following:

```{r}
# standardize continuous variables manually
model2 <- lme4::lmer(
  scale(Sepal.Length) ~ Species * scale(Sepal.Width) + scale(Petal.Length) + (1 | grp),
  data = iris
)
model_parameters(model2)
```

## Post-hoc standardization

`standardize = "posthoc"` aims at emulating the results obtained by `"refit"` without refitting the model. The coefficients are divided by the standard deviation of the outcome (which becomes their expression 'unit'). Then, the coefficients related to numeric variables are additionally multiplied by the standard deviation of the related terms, so that they correspond to changes of 1 SD of the predictor (e.g., "a change in 1 SD of x is related to a change of 0.24 of the SD of y"). This does not apply to binary variables or factors, so the coefficients are still related to changes in levels.

This method is not accurate and tends to give aberrant results when interactions are specified. However, this method of standardization is the "classic" result obtained by many statistical packages when standardized coefficients are requested.

When `standardize = "posthoc"`, `model_parameters()` internally calls [`effectsize::standardize_parameters(method = "posthoc")`](https://easystats.github.io/effectsize/reference/standardize_parameters.html). Test statistic and p-values are not affected, i.e. they are the same as if no standardization would be applied.

```{r}
model_parameters(model, standardize = "posthoc")
```

`standardize = "basic"` also applies post-hoc standardization, however, factors are converted to numeric, which means that it also scales the coefficient by the standard deviation of model's matrix' parameter of factor levels (transformed to integers) or binary predictors.

```{r}
model_parameters(model, standardize = "basic")
```

## Smart standardization

`standardize = "smart"` is similar to `standardize = "posthoc"` in that it does not involve model re-fitting. The difference is that the SD of the response is computed on the relevant section of the data. For instance, if a factor with 3 levels A (the intercept), B and C is entered as a predictor, the effect corresponding to B vs. A will be scaled by the variance of the response at the intercept only. As a results, the coefficients for effects of factors are similar to a Glass' delta.

```{r}
model_parameters(model, standardize = "smart")
```
---
title: "Effect Sizes for Logistic Models"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, logistic, regression, glm, binomial]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Effect sizes for logistic models}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

pkgs <- c("effectsize", "bayestestR", "ggplot2", "see", "parameters", "modelbased")
if (!all(sapply(pkgs, requireNamespace))) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

# Odds Ratio

*This vignette needs some love.*

<!-- With binary predictors & with continuous predictors. -->

<!-- ## Risk Ratios -->

<!-- ## Converting to something more familiar (r/d) -->

<!-- # Standerdized Coefficiants -->

# Validate a *t*-test with Logistic Regression

Let's start off with a simple example. We will simulate 100 observations of a normally distributed outcome variable (mean = 0 and SD = 1) and a grouping variable made of *zeros* and *ones*. Importantly, the mean difference between these two groups (zeros *vs.* ones) is of 1.

```{r message=FALSE, warning=FALSE}
data <- bayestestR::simulate_difference(n = 100,
                                        d = 1,
                                        names = c("Group", "Outcome"))

summary(data)
```


Now, as we are interested in the difference between these two groups, we can first investigate it using a *t*-test.

```{r message=FALSE, warning=FALSE}
library(ggplot2)

ggplot(data, aes(x=Group, y=Outcome, fill=Group)) +
  geom_boxplot() +
  see::theme_modern()

ttest <- t.test(Outcome ~ Group, data=data, var.equal=TRUE)
ttest_pars <- parameters::parameters(ttest)
ttest_pars
```

As we can see, this confirms our simulation specifications, the difference is indeed of 1.

## Cohen's *d*

Let's compute now, a traditional Cohen's *d* using the `effectsize` package. While this *d* should be close to 1, it should theoretically be a tiny bit larger, because it takes into account the (pooled) SD of the whole variable `x` (across the groups), which because of the difference is a bit larger than 1.

```{r message=FALSE, warning=FALSE}
sd(data$Outcome)
```

We can compute the Cohen's *d* as follows:

```{r message=FALSE, warning=FALSE}
effectsize::cohens_d(data$Outcome, data$Group)
```

As expected, it's pretty close to 1 times the SD of the sample. Interestingly, one can estimate the Cohen's *d* directly from the result of the *t*-test, using the *t* statistic. We can convert it to a *d* using the `effectsize` package:

```{r message=FALSE, warning=FALSE}
effectsize::t_to_d(ttest_pars$t, df_error = ttest_pars$df)
```

Fortunately, they are quite close.

## The Complimentry Logistic Model

Another way of investigating these differences is through the lens of a logistic regression. The main difference is that here, the group variable `y` becomes the outcome and `x` the predictor. Let's fit such model and investigate the parameters: 


```{r message=FALSE, warning=FALSE}
model <- glm(Group ~ Outcome, data = data,
             family = "binomial")

parameters::parameters(model)
```

How to interpret this output? The coefficients of a logistic model are expressed in log-odds, which is a metric of probability. Using the [**modelbased**](https://github.com/easystats/modelbased) package, one can easily visualize this model:


```{r message=FALSE, warning=FALSE}
data_grid <- modelbased::estimate_link(model)

ggplot(data_grid, aes(x = Outcome, y = Predicted)) +
  geom_ribbon(aes(ymin = CI_low, ymax = CI_high), alpha = 0.2) +
  geom_line(color = "red", size = 1) + 
  see::theme_modern()
```

We can see that the probability of `y` being 1 (vs. 0) increases as `x` increases. This is another way of saying that there is a difference of `x` between the two groups of `y`. We can visualize all of our this together as follows:


```{r message=FALSE, warning=FALSE}

ggplot(data, aes(x=Group, y=Outcome, fill=Group)) +
  geom_boxplot() +
  geom_jitter(width = 0.1) + 
  # add vertical regression line
  geom_line(data = data_grid, 
            aes(x = Predicted + 1, y = Outcome, fill = NA), 
            color = "red", size = 1) +
  see::theme_modern()
```

You can notice that the red predicted probability line passes through `x=0` when `y=0.5`. This means that when `x=0`, the probability of the two groups is equal: it is the "middle" of the difference between them.

<!-- # Odds ratios to Cohen's *d* -->

<!-- There is *in theory* a formula to convert these log-odds ratios (the unit of the coefficients in logistic models) to standardized differences (like Cohen's *d*). The formula is: $$d = logodds * (\sqrt{3} / \pi)$$ -->


<!-- Let's see: -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- logodds <- parameters::parameters(model)$Coefficient[2] -->

<!-- effectsize::logodds_to_d(logodds) -->
<!-- ``` -->

<!-- Hum, it seems a bit far from the value computed above! Let's investigate. -->

<!-- # Simulation -->

<!-- We will simulate different datasets with a varying *difference* and *sample size* (number of observations). For each of this dataset, we will compute the "true" Cohen's *d*, as well as the *d* obtained from the conversion of the logistic coefficient. Note that the simulation takes about 30 s to run. -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- df <- data.frame() -->
<!-- for(d in seq(0.1, 2, length.out=20)){ -->
<!--   for(n in exp(seq(log(20), log(150), length.out=20))){ -->
<!--     data <- bayestestR::simulate_difference(n=n, d=-d, names=c("y", "x")) -->
<!--     params <- parameters::parameters(glm(y ~ x, data=data, family="binomial"))[2, ] -->
<!--     params$d <- d -->
<!--     params$n <- n -->
<!--     params$d_cohen <- effectsize::cohens_d(data$x, data$y)$Cohens_d -->
<!--     df <- rbind(df, params) -->
<!--   } -->
<!-- } -->
<!-- df$logodds <- df$Coefficient -->
<!-- df$d_from_coef <- abs(effectsize::logodds_to_d(df$logodds)) -->
<!-- ``` -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- df %>% -->
<!--   ggplot(aes(x=d_cohen, y=d_from_coef, alpha=n)) + -->
<!--   see::geom_point2(size=2) + -->
<!--   geom_abline(slope=1) + -->
<!--   coord_fixed(ratio = 1, xlim = c(0, 3), ylim = c(0, 3)) + -->
<!--   see::theme_modern() -->
<!-- ``` -->


<!-- Oops, it seems like this formula doesn't work well, as it tends to underestimate the Cohen's *d* (and this tendency interacts with the sample size). -->

<!-- # Adjusted metric -->

<!-- Can we come up with a better conversion algorithm than `d = logodds * 0.55` (`0.55` being the result of `sqrt(3) / pi`)? Let's try. We will fit a model to predict the Cohen's *d* based on the logodds and the sample size, and plot the prediction. -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(d_cohen ~ logodds * n, data=df) -->

<!-- df$d_from_coef_adjusted1 <- modelbased::estimate_response(model)$Predicted    -->

<!-- ggplot(df, aes(x=d_cohen, y=d_from_coef_adjusted1, alpha=n)) + -->
<!--   see::geom_point2(size=2) + -->
<!--   geom_abline(slope=1) + -->
<!--   coord_fixed(ratio = 1, xlim = c(0, 3), ylim = c(0, 3)) + -->
<!--   see::theme_modern() -->
<!-- ``` -->

<!-- Seems better already! But if we look at the residuals, we can see that there is still a non-linear relationship with the sample size: -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- df$resid <- residuals(model) -->

<!-- ggplot(df, aes(x=n, y=resid, alpha=d_from_coef_adjusted1)) + -->
<!--   see::geom_point2(size=2)  + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   see::theme_modern() -->
<!-- ``` -->


<!-- Can we further try to eliminate this effect? -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(d_cohen ~ logodds * n * log(n), data=df) -->
<!-- df$d_from_coef_adjusted2 <- modelbased::estimate_response(model)$Predicted    -->

<!-- ggplot(df, aes(x=d_cohen, y=abs(d_from_coef_adjusted2), alpha=n)) + -->
<!--   see::geom_point2(size=2) + -->
<!--   geom_abline(slope=1) + -->
<!--   coord_fixed(ratio = 1, xlim = c(0, 3), ylim = c(0, 3)) + -->
<!--   see::theme_modern() -->
<!-- ``` -->

<!-- It seems like adding the log of the sample size improves the relationship! What is the formula of this new **log-odds** to ***d*** conversion? -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- formula <- insight::get_parameters(model) %>%  -->
<!--   mutate(Coef = paste0(round(Estimate, 5), " * (", Parameter, ")"), -->
<!--          Coef = gsub(":", " * ", Coef), -->
<!--          Coef = gsub(" * ((Intercept))", "", Coef, fixed=TRUE)) %>%  -->
<!--   pull(Coef) %>%  -->
<!--   paste(collapse = " + ") -->

<!-- print(formula) -->
<!-- ``` -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- newconversion <- function(logodds, n){ -->
<!--   0.87341 + 0.75933 * (logodds) + 0.02705 * (n) + -0.35727 * (log(n)) + 0.03714 * (logodds * n) + -0.61004 * (logodds * log(n)) + -0.00418 * (n * log(n)) + -0.00563 * (logodds * n * log(n)) -->
<!-- } -->

<!-- df$d_from_coef_adjusted <- newconversion(df$logodds, df$n) -->

<!-- ggplot(df, aes(x=d_cohen, y=d_from_coef_adjusted)) + -->
<!--   geom_point(size=1) + -->
<!--   geom_abline(slope=1) + -->
<!--   coord_fixed(ratio = 1, xlim = c(0, 3), ylim = c(0, 3)) + -->
<!--   see::theme_modern() -->
<!-- ``` -->


# References---
title: "Parameters Standardization"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, standardization, effect size, cohen d, standardized coefficients]
vignette: >
  %\VignetteIndexEntry{Parameters standardization}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(comment = ">")
options(digits = 2)
options(knitr.kable.NA = '')

pkgs <- c("effectsize", "dplyr", "parameters", "correlation")
if (!all(sapply(pkgs, requireNamespace))) {
  knitr::opts_chunk$set(eval = FALSE)
}

set.seed(333)
```

# Introduction

Standardizing parameters (*i.e.*, coefficients) can allow for their comparison within and between models, variables and studies. Moreover, as it returns coefficients expressed in terms of **change of variance** (for instance, coefficients expressed in terms of SD of the response variable), it can allow for the usage of [effect size interpretation guidelines](https://easystats.github.io/effectsize/articles/interpret.html), such as the famous Cohen's (1988) rules of thumb.

However, standardizing the model's parameters should *not* be automatically and mindlessly done: for some research fields, particular variables or types of studies (*e.g.*, replications), it sometimes makes more sense to keep, use and interpret the original parameters, especially if they are well known or easily understood.

Critically, **parameters standardization is not a trivial process**. Different techniques exist, that can lead to drastically different results. Thus, it is critical that the standardization method is explicitly documented and detailed.

**`parameters` include different techniques of parameters standardization**, described below [@bring1994standardize;@menard2004six;@gelman2008scaling;@schielzeth2010simple;@menard2011standards].

# How to interpret standardized coefficients?


## Measure of association (correlation *r*)

```{r, warning=FALSE, message=FALSE}
library(effectsize)
library(dplyr)

lm(Sepal.Length ~ Petal.Length, data = iris) %>% 
  standardize_parameters()
```

Standardizing the coefficient of this simple linear regression gives a value of `0.87`, but did you know that for a simple regression this is actually the **same as a correlation**? Thus, you can eventually apply some (*in*)famous interpretation guidelines (e.g., Cohen's rules of thumb).

```{r, warning=FALSE, message=FALSE}
library(parameters)

cor.test(iris$Sepal.Length, iris$Petal.Length) %>% 
  model_parameters()
```


What happens in the case of **multiple continuous variables**? As in each effect in a regression model is "adjusted" for the other ones, we might expect coefficients to be somewhat alike to **partial correlations**. Let's first start by computing the partial correlation between **Sepal.Length** and 3 other remaining variables.


```{r, warning=FALSE, message=FALSE}
df <- iris[, 1:4]  # Remove the Species factor
correlation::correlation(df, partial = TRUE)[1:3, 1:3] # Select the rows of interest
```


Now, let's apply another method to obtain effect sizes for frequentist regressions, based on the statistic values. We will convert the *t*-value (and its degrees of freedom, *df*) into a partial correlation coefficient *r*.


```{r, warning=FALSE, message=FALSE}
model <- lm(Sepal.Length ~ ., data = df)
params <- model_parameters(model)

t_to_r(params$t[2:4], params$df_error[2:4])
```

Wow, the retrieved correlations coefficients from the regression model are **exactly** the same as the partial correlations!

However, note that in multiple regression standardizing the parameters in not quite the same as computing the (partial) correlation, due to... math :(^[in fact, they are more closely related to the semi-partial correlations.]

```{r, warning=FALSE, message=FALSE}
model %>% 
  standardize_parameters() 
```

## Standardized differences

How does it work in the case of differences, when **factors** are entered and differences between a given level and a reference level (the intercept)? You might have heard that it is similar to a **Cohen's *d***. Well, let's see.

```{r, warning=FALSE, message=FALSE}
# Select portion of data containing the two levels of interest
data <- iris[iris$Species %in% c("setosa", "versicolor"), ]

lm(Sepal.Length ~ Species, data = data) %>% 
  standardize_parameters()
```

This linear model suggests that the *standardized* difference between the *versicolor* level of Species and the *setosa* level (the reference level - the intercept) is of 1.12 standard deviation of `Sepal.Length` (because the response variable was standardized, right?). Let's compute the **Cohen's *d*** between these two levels:

```{r, warning=FALSE, message=FALSE}
cohens_d(Sepal.Length ~ Species, data = data) 
```

***It is very different!*** Why? How? Both differences should be expressed in units of SD! But which SDs? Different SDs!

When looking at the difference between groups as a **slope**, the standardized parameter is the difference between the means in $SD_{Sepal.Length}$. That is, the *slope* between `setosa` and `versicolor` is a change of 1.45 $SD_{Sepal.Length}$.

However, when looking a the difference as a distance between two populations, Cohen's d is the distance between the means in units of [**pooled SDs**](https://easystats.github.io/effectsize/reference/sd_pooled.html). That is, the *distance* between `setosa` and `versicolor` is of 2.1 SDs of each of the groups (here assumed to be equal).

Note that you can get a proximity of Cohen's d with by converting the $t$ statistic from the regression model via `t_to_d()`:
```{r}
(parameters <- lm(Sepal.Length ~ Species, data = data) %>% 
  model_parameters())
t_to_d(10.52, df_error = 98)
```


It is also interesting to note that using the *smart* method when standardizing parameters will give you indices equivalent to **Glass' delta**, which difference is expressed in terms of SD of the intercept (the "reference" factor levels).

```{r, warning=FALSE, message=FALSE}
lm(Sepal.Length ~ Species, data = data) %>%
  standardize_parameters(method = "smart")
```

```{r, warning=FALSE, message=FALSE}
glass_delta(data$Sepal.Length[data$Species=="versicolor"],
            data$Sepal.Length[data$Species=="setosa"])
# glass_delta takes SD from second group
```

***... So note that some standardized differences are difference than others! :)***

# Standardization methods

<sub>To be added...</sub>



<!-- ### **"refit"**: Re-fitting the model with standardized data -->

<!-- **This method is based on a complete model re-fit with a standardized version of data**. Hence, this method is equal to standardizing the variables before fitting the model. It is the "purest" and the most accurate (Neter et al., 1989), but it is also the most computationally costly and long (especially for heavy models such as, for instance, for Bayesian models). This method is particularly recommended for complex models that include interactions or transformations (e.g., polynomial or spline terms). -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(effectsize) -->

<!-- data <- iris -->
<!-- model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) -->

<!-- standardize_parameters(model, method="refit") -->
<!-- ``` -->

<!-- The `robust` (default to `FALSE`) argument enables a **robust standardization of data**, *i.e.*, based on the **median** and **MAD** instead of the **mean** and **SD**. -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- standardize_parameters(model, method="refit", robust=TRUE) -->
<!-- ``` -->


<!-- This method is very flexible as it can be applied to all types of models (linear, logistic...). -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- data$binary <- ifelse(data$Sepal.Width > 3, 1, 0) -->
<!-- model <- glm(binary ~ Species + Sepal.Length, data = data, family="binomial") -->
<!-- standardize_parameters(model, method="refit") -->
<!-- ``` -->

<!-- ### **"posthoc"**: Refit without refitting -->

<!-- Post-hoc standardization of the parameters, aiming at emulating the results obtained by "refit" without refitting the model. The coefficients are divided by the standard deviation (or MAD if `robust`) of the outcome (which becomes their expression 'unit'). Then, the coefficients related to numeric variables are additionally multiplied by the standard deviation (or MAD if `robust`) of the related terms, so that they correspond to changes of 1 SD of the predictor (e.g., "A change in 1 SD of *x* is related to a change of 0.24 of the SD of *y*). This does not apply to binary variables or factors, so the coefficients are still related to changes in levels. This method is not accurate and tend to give aberrant results when interactions are specified. -->

<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) -->
<!-- standardize_parameters(model, method="posthoc") -->
<!-- ``` -->

<!-- ### **"smart"**: Standardization of Model's parameters with Adjustment, Reconnaissance and Transformation -->

<!-- Similar to `method = "posthoc"` in that it does not involve model refitting. The difference is that the SD of the response is computed on the relevant section of the data. For instance, if a factor with 3 levels A (the intercept), B and C is entered as a predictor, the effect corresponding to B vs. A will be scaled by the variance of the response at the intercept only. As a results, the coefficients for effects of factors are similar to a Glass' *delta*. -->


<!-- ```{r warning=FALSE, message=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) -->
<!-- standardize_parameters(model, method="smart") -->
<!-- ``` -->

<!-- ### **"basic"**: Raw scaling of the model frame -->

<!-- This method is similar to `method = "posthoc"`, but treats all variables as continuous: it also scales the coefficient by the standard deviation of model's matrix' parameter of factors levels (transformed to integers) or binary predictors. Although being inappropriate for these cases, this method is the one implemented by default in other software packages, such as `lm.beta::lm.beta()`. -->

<!-- ## Methods Comparison -->

<!-- We will use the "refit" method as the baseline. We will then compute the differences between these standardized parameters and the ones provided by the other functions. The **bigger the (absolute) number, the worse it is**. -->

<!-- > **SPOILER ALERT: the standardization implemented in `effectsize` is the most accurate and the most flexible.** -->

<!-- ### Convenience function -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(effectsize) -->
<!-- library(lm.beta) -->
<!-- library(MuMIn) -->

<!-- comparison <- function(model, robust=FALSE){ -->
<!--   out <- standardize_parameters(model, method="refit", robust=robust)[1:2] -->

<!--   out$posthoc <- tryCatch({ -->
<!--     out[, 2] - standardize_parameters(model, method="posthoc", robust=robust)[, 2] -->
<!-- }, error = function(error_condition) { -->
<!--     "Error" -->
<!-- }) -->
<!--   out$basic <- tryCatch({ -->
<!--     out[, 2] - standardize_parameters(model, method="basic", robust=robust)[, 2] -->
<!-- }, error = function(error_condition) { -->
<!--     "Error" -->
<!-- }) -->

<!--   out$lm.beta <- tryCatch({ -->
<!--     out[, 2] - lm.beta::lm.beta(model)$standardized.coefficients -->
<!-- }, error = function(error_condition) { -->
<!--     "Error" -->
<!-- }, warning = function(warning_condition) { -->
<!--   "Error" -->
<!-- }) -->

<!--   out$MuMIn <- tryCatch({ -->
<!--     out[, 2] - MuMIn::std.coef(model, partial.sd=FALSE)[, 1] -->
<!-- }, error = function(error_condition) { -->
<!--     "Error" -->
<!-- }) -->

<!--   out[, 2] <- NULL -->
<!--   out -->
<!-- } -->
<!-- ``` -->

<!-- ### Data -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- data <- iris -->
<!-- data$Group_Sepal.Width <- as.factor(ifelse(data$Sepal.Width > 3, "High", "Low")) -->
<!-- data$Binary_Sepal.Width <- as.factor(ifelse(data$Sepal.Width > 3, 1, 0)) -->

<!-- summary(data) -->
<!-- ``` -->

<!-- ### Models with only numeric predictors -->


<!-- #### Linear Model -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- #### Logistic Model -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- glm(Binary_Sepal.Width ~ Petal.Width + Sepal.Length, data=data, family="binomial") -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- #### Linear Mixed Model -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(lme4) -->

<!-- model <- lme4::lmer(Sepal.Length ~ Petal.Width + Sepal.Width + (1|Species), -->
<!--                     data=data) -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- #### Bayesian Models -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(rstanarm) -->

<!-- model <- stan_glm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- For these simple models, **all methods return results equal to the "refit" method** (although the other packages fail). -->


<!-- #### Transformation -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ poly(Petal.Width, 2) + poly(Sepal.Width, 2), data=data) -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- When transformation are involved (e.g., polynomial transformations), **the basic method becomes very unreliable**. -->



<!-- ### Models with factors -->

<!-- #### Linear Model -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width + Group_Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->




<!-- #### Logistic Model -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- glm(Binary_Sepal.Width ~ Petal.Width + Species, data=data, family="binomial") -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- #### Linear Mixed Model -->


<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(lme4) -->

<!-- model <- lme4::lmer(Sepal.Length ~ Petal.Length + Group_Sepal.Width + (1|Species), data=data) -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- #### Bayesian Models -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(rstanarm) -->

<!-- model <- stan_lmer(Sepal.Length ~ Petal.Width + Group_Sepal.Width + (1|Species), -->
<!--                    data=data) -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- When factors are involved, the basic method (that standardizes the numeric transformation of factors) give again different results. -->



<!-- ### Models with interactions -->

<!-- Long story short, coeffcient obtained via **posthoc** standardization (without refitting the model) go berserk when interactions are involved. However, **this is "normal"**: a regression model estimates coefficient between two variables when the other predictors are at 0 (are *fixed* at 0, that people interpret as *"adjusted for"*). When a standardized data is passed (in the *refit* method), the effects and interactions are estimated at the **means** of the other predictors (because 0 is the mean for a standardized variable). Whereas in posthoc standardization, this coefficient correspond to something different (because the 0 corresponds to something different in standardzed and non-standardized data). In other words, when it comes to interaction, passing standardized data results in a different model, which coefficient have an intrinsically different meaning from unstandardized data. And as [for now](https://github.com/easystats/effectsize/issues/6), we are unable to retrieve one from another. -->


<!-- #### Between continuous -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width * Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- #### Between factors -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Species * Group_Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- #### Between factors and continuous -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Width * Group_Sepal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Group_Sepal.Width * Petal.Width, data=data) -->
<!-- comparison(model) -->
<!-- ``` -->


<!-- ## Conclusion -->

<!-- Use `refit` if possible, but if no interactions, can use `posthoc` or `smart`. -->


# References

---
title: "Effect Size from Test Statistics"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, standardization, effect size, cohen d, standardized coefficients]
vignette: >
  %\VignetteIndexEntry{Effect Size from Test Statistics}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(effectsize)

knitr::opts_chunk$set(comment = ">")
options(digits = 2)
options(knitr.kable.NA = '')

pkgs <- c("effectsize", "afex", "lmerTest", "emmeans", "parameters")
if (!all(sapply(pkgs, requireNamespace))) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(afex)
  library(lmerTest)
  library(emmeans)
  library(parameters)
}

set.seed(747)
```

# Introduction

In many real world applications there are no straightforward ways of obtaining standardized effect sizes. However, it is possible to get approximations of most of the effect size indices ($d$, $r$, $\eta^2_p$...) with the use of test statistics. These conversions are based on the idea that **test statistics are a function of effect size and sample size**. Thus information about samples size (or more often of degrees of freedom) is used to reverse-engineer indices of effect size from test statistics. This idea and these functions also power our [***Effect Sizes From Test Statistics*** *shiny app*](https://easystats4u.shinyapps.io/statistic2effectsize/).

The measures discussed here are, in one way or another, ***signal to noise ratios***, with the "noise" representing the unaccounted variance in the outcome variable^[Note that for generalized linear models (Poisson, Logistic...), where the outcome is never on an arbitrary scale, estimates themselves **are** indices of effect size! Thus this vignette is relevant only to general linear models.].

The indices are: 

- Percent variance explained ($\eta^2_p$, $\omega^2_p$, $\epsilon^2_p$).
- Measure of association ($r$).
- Measure of difference ($d$).

## (Partial) Percent Variance Explained

These measures represent the ratio of $Signal^2 / (Signal^2 + Noise^2)$, with the "noise" having all other "signals" partial-ed out (be they of other fixed or random effects). The most popular of these indices is $\eta^2_p$ (Eta; which is equivalent to $R^2$).

The conversion of the $F$- or $t$-statistic is based on @friedman1982simplified.

Let's look at an example:

```{r}
library(afex)

data(md_12.1)

aov_fit <- aov_car(rt ~ angle * noise + Error(id/(angle * noise)),
                   data = md_12.1,
                   anova_table=list(correction = "none", es = "pes"))
aov_fit
```

Let's compare the $\eta^2_p$ (the `pes` column) obtained here with ones recovered from `F_to_eta2()`:

```{r}
library(effectsize)

F_to_eta2(
  f = c(40.72, 33.77, 45.31),
  df = c(2, 1, 2),
  df_error = c(18, 9, 18)
)
```

**They are identical!**^[Note that these are *partial* percent variance explained, and so their sum can be larger than 1.] (except for the fact that `F_to_eta2()` also provides confidence intervals^[Confidence intervals for all indices are estimated using the non-centrality parameter method; These methods search for a the best non-central parameter of the non-central $F$/$t$ distribution for the desired tail-probabilities, and then convert these ncps to the corresponding effect sizes.] :)

In this case we were able to easily obtain the effect size (thanks to `afex`!), but in other cases it might not be as easy, and using estimates based on test statistic offers a good approximation.

For example:

### In Simple Effect and Contrast Analysis

```{r}
library(emmeans)

joint_tests(aov_fit, by = "noise")

F_to_eta2(f = c(5, 79),
          df = 2,
          df_error = 29)
```

We can also use `t_to_eta2()` for contrast analysis:

```{r}
pairs(emmeans(aov_fit, ~ angle))

t_to_eta2(t = c(-5.7, -8.9, -3.2),
          df_error = 18)
```

### In Linear Mixed Models

```{r}
library(lmerTest)

fit_lmm <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)

anova(fit_lmm)

F_to_eta2(45.8, 1, 17)
```

We can also use `t_to_eta2()` for the slope of `Days` (which in this case gives the same result).

```{r}
model_parameters(fit_lmm, df_method = "satterthwaite")

t_to_eta2(6.77, df_error = 17)
```

### Bias-Corrected Indices

Alongside $\eta^2_p$ there are also the less biased $\omega_p^2$ (Omega) and $\epsilon^2_p$ (Epsilon; sometimes called $\text{Adj. }\eta^2_p$, which is equivalent to $R^2_{adj}$; @albers2018power, @mordkoff2019simple).

```{r}
F_to_eta2(45.8, 1, 17)
F_to_epsilon2(45.8, 1, 17)
F_to_omega2(45.8, 1, 17)
```


## Measure of Association

Similar to $\eta^2_p$, $r$ is a signal to noise ratio, and is in fact equal to $\sqrt{\eta^2_p}$ (so it's really a *partial* $r$). It is often used instead of $\eta^2_p$ when discussing the *strength* of association (but I suspect people use it instead of $\eta^2_p$ because it gives a bigger number, which looks better).

### For Slopes

```{r}
model_parameters(fit_lmm, df_method = "satterthwaite")

t_to_r(6.77, df_error = 17)
```

In a fixed-effect linear model, this returns the **partial** correlation. Compare:

```{r}

fit_lm <- lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris) 

model_parameters(fit_lm)

t_to_r(t = c(8.59, 27.57),
       df_error = 147)
```

to:

```{r, eval=require(correlation, quietly = TRUE)}
correlation::correlation(iris[,1:3], partial = TRUE)[1:2, c(1:3,7:8)]
```

### In Contrast Analysis

This measure is also sometimes used in contrast analysis, where it is called the point bi-serial correlation - $r_{pb}$ [@cohen1965some; @rosnow2000contrasts]:

```{r}
pairs(emmeans(aov_fit, ~ angle))

t_to_r(t = c(-5.7, -8.9, -3.2),
       df_error = 18)
```


## Measures of Difference

These indices represent $Signal/Noise$ with the "signal" representing the difference between two means. This is akin to Cohen's $d$, and is a close approximation when comparing two groups of equal size [@wolf1986meta; @rosnow2000contrasts].

These can be useful in contrast analyses.

### Between-Subject Contrasts

```{r}
warp.lm <- lm(breaks ~ tension, data = warpbreaks)

pairs(emmeans(warp.lm,  ~ tension))

t_to_d(t = c(2.5, 3.7, 1.2),
       df_error = 51)

```

### Within-Subject Contrasts

```{r}

pairs(emmeans(aov_fit, ~ angle))

t_to_d(t = c(-5.7,-5.9,-3.2),
       df_error = 18,
       paired = TRUE)

```

(Note `paired = TRUE` to not over estimate the size of the effect; @rosenthal1991meta; @rosnow2000contrasts)


# References
---
title: "Automated Interpretation of Indices of Effect Size"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, rules of thumb, guidelines, interpretation]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Automated interpretation of indices of effect size}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

pkgs <- c("effectsize")
if (!all(sapply(pkgs, requireNamespace))) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

# Why?

The metrics used in statistics (indices of fit, model performance or parameter estimates) can be very abstract. A long experience is required to intuitively ***"feel"*** the meaning of their values. In order to facilitate the understanding of the results they are facing, many scientists use (often implicitly) some set of **rules of thumb**. Thus, in order to validate and standardize such interpretation grids, some authors validated and published them in the form of guidelines. 

One of the most famous interpertation grid was proposed by **Cohen (1988)** for a series of widely used indices, such as the correlation **r** (*r* = .20, small; *r* = .40, moderate and *r* = .60, large) or the **standardized difference** (*Cohen's d*). However, there is now a clear evidence that Cohen's guidelines (which he himself later disavowed; Funder, 2019) are much too stringent and not particularly meaningful taken out of context [@funder2019evaluating]. This led to the emergence of a litterature discussing and creating new sets of rules of thumb.  

Altough **everybody agrees on the fact that effect size interpretation in a study should be justified with a rationale** (and depend on the context, the field, the litterature, the hypothesis, etc.), these pre-baked rules can still nevertheless be useful to give a rough idea or frame of reference to understand scientific results.

The package **`effectsize`** implements such sets of rules of thumb for a variety of indices in a flexible and explicit fashion, helping you understanding and reporting your results in a scientific yet meaningful way. Again, readers should keep in mind that these thresholds, altough *"validated"*, **remain arbitrary**. Thus, their use should be discussed on a case-by-case basis depending on the field, hypotheses, prior results and so on, to avoid their crystalisation, as for the infamous ***p* < .05** example.

Moreovere, some authors suggest the counter-intuitive idea that *very large effects*, especially in the context of psychological research, is likely to be a "gross overestimate that will rarely be found in a large sample or in a replication" [@funder2019evaluating]. They suggest that smaller effect size are worth taking seriously (as they can be potentially consequential), as well as more believable.

# Supported Indices


### Coefficient of determination  (R2)


#### @falk1992primer

```r
interpret_r2(x, rules = "falk1992")
```

- **R2 < 0.1**: Negligible
- **R2 > 0.1**: Adequate



#### @cohen1988statistical

```r
interpret_r2(x, rules = "cohen1988")
```

- **R2 = 0 - 0.02**: Very weak
- **R2 = 0.02 - 0.16**: Weak
- **R2 = 0.16 - 0.26**: Moderate
- **R2 > 0.26**: Substantial




#### @chin1998partial

```r
interpret_r2(x, rules = "chin1998")
```

- **R2 = 0 - 0.19**: Very weak
- **R2 = 0.19 - 0.33**: Weak
- **R2 = 0.33 - 0.67**: Moderate
- **R2 > 0.67**: Substantial

#### @hair2011pls

```r
interpret_r2(x, rules = "hair2011")
```

- **R2 = 0 - 0.25**: Very weak
- **R2 = 0.25 - 0.50**: Weak
- **R2 = 0.50 - 0.75**: Moderate
- **R2 > 0.75**: Substantial



### Correlation *r*


```r
interpret_r(x, rules = "funder2019")
```

- **r = 0 - 0.05**: Very small
- **r = 0.05 - 0.1**: Small
- **r = 0.1 - 0.2**: Medium 
- **r = 0.2 - 0.3**: Large
- **r > 0.4**: Very large



#### @gignac2016effect

Gignac's rules of thumb are actually one of few interpretation grid justified and based on actual data, in this case on the distribution of effect magnitudes in the litterature.

```r
interpret_r(x, rules = "gignac2016")
```

- **r = 0 - 0.1**: Very small
- **r = 0.1 - 0.2**: Small 
- **r = 0.2 - 0.3**: Moderate ("typical")
- **r > 0.3**: Large



#### @cohen1988statistical

```r
interpret_r(x, rules = "cohen1988")
```

- **r = 0 - 0.1**: Very small
- **r = 0.1 - 0.3**: Small
- **r = 0.3 - 0.5**: Moderate
- **r > 0.5**: Large


#### @evans1996straightforward

```r
interpret_r(x, rules = "evans1996")
```

- **r = 0 - 0.2**: Very weak
- **r = 0.2 - 0.4**: Weak
- **r = 0.4 - 0.6**: Moderate
- **r = 0.6 - 0.8**: Strong
- **r > 0.8**: Very strong



### Standardized Difference *d* (Cohen's *d*)

The sandardized difference can be obtained through the standardization of linear model's parameters or data, in which they can be used as indices of effect size.

```r
interpret_d(x, rules = "funder2019")
```

- **r = 0 - 0.1**: Very small
- **r = 0.1 - 0.2**: Small
- **r = 0.2 - 0.4**: Medium 
- **r = 0.4 - 0.6**: Large
- **r > 0.6**: Very large

#### @gignac2016effect

Gignac's rules of thumb are actually one of few interpretation grid justified and based on actual data, in this case on the distribution of effect magnitudes in the litterature.

```r
interpret_d(x, rules = "gignac2016")
```

- **d = 0 - 0.2**: Very small
- **d = 0.2 - 0.4**: Small
- **d = 0.4 - 0.6**: Medium
- **d > 0.6**: Large


#### @cohen1988statistical

```r
interpret_d(x, rules = "cohen1988")
```

- **d = 0 - 0.2**: Very small
- **d = 0.2 - 0.5**: Small
- **d = 0.5 - 0.8**: Medium
- **d > 0.8**: Large

#### @sawilowsky2009new

```r
interpret_d(x, rules = "sawilowsky2009")
```

- **d = 0 - 0.1**: Tiny
- **d = 0.1 - 0.2**: Very small
- **d = 0.2 - 0.5**: Small
- **d = 0.5 - 0.8**: Medium
- **d = 0.8 - 1.2**: Large
- **d = 1.2 - 2**: Very large
- **d > 2**: Huge


### Odds ratio

Odds ratio, and *log* odds ratio, are often found in epidemiological studies. However, they are also the parameters of ***logistic*** regressions, where they can be used as indices of effect size. Note that the (log) odds ratio from logistic regression coefficients are *unstandardized*, as they depend on the scale of the predictor. In order to apply the following guidelines, make sure you [*standardize*](https://easystats.github.io/effectsize/articles/standardize_parameters.html) your predictors!

#### @chen2010big

```r
interpret_odds(x, rules = "chen2010")
```

- **odds = 0 - 1.68**: Very small
- **odds = 1.68 - 3.47**: Small
- **odds = 3.47 - 6.71**: Medium
- **odds > 6.71**: Large

#### @cohen1988statistical

```r
interpret_odds(x, rules = "cohen1988")
```

This converts (log) odds ratio to standardized difference *d* using the following formula [@cohen1988statistical;@sanchez2003effect]:

```r
d <- log_odds * (sqrt(3) / pi)
```


### Omega Squared


The Omega squared is a measure of effect size used in ANOVAs. It is an estimate of how much variance in the response variables are accounted for by the explanatory variables. Omega squared is widely viewed as a lesser biased alternative to eta-squared, especially when sample sizes are small.

#### @field2013discovering

```r
interpret_omega_squared(x, rules = "field2013")
```

- **Omega Squared = 0 - 0.01**: Very small
- **Omega Squared = 0.01 - 0.06**: Small
- **Omega Squared = 0.06 - 0.14**: Medium
- **Omega Squared > 0.14**: Large



### Bayes Factor (BF)

**Bayes factors (BF)** are continuous measures of relative evidence, with a Bayes factor greater than 1 giving evidence in favor of one of the models (the numerator), and a Bayes factor smaller than 1 giving evidence in favor of the other model (the denominator). Yet, it is common to interpret the **magnitude** of relative evidence based on conventions of intervals (presented below), such that the values of a *BF*<sub>10</sub> (comparing the alternative to the null) can be interpreted as:

For human readability, it is recommended to report BFs so that the ratios are larger than 1 - for example, it's harder to understand a *BF*<sub>10</sub>\=0.07 (indicating the data are 0.07 times more probable under the alternative) than a *BF*<sub>01</sub>\=1/0.07\=14.3 (indicating the data are 14.3 times more probable under the null. **BFs** between 0 and 1, indicating evidence *against* the hypothesis, can be converted via `bf = 1 / abs(bf)`.

One can report **Bayes factors** using the following sentence:

> There is a strong evidence against the null hypothesis (BF = 12.2).

#### @jeffreys1961theory

```r
interpret_bf(x, rules = "jeffreys1961")
```

- **bf = 1 - 3**: Anecdotal
- **bf = 3 - 10**: Moderate
- **bf = 10 - 30**: Strong
- **bf = 30 - 100**: Very strong
- **bf > 100**: Extreme

#### @raftery1995bayesian

```r
interpret_bf(x, rules = "raftery1995")
```

- **bf = 1 - 3**: Weak
- **bf = 3 - 20**: Positive
- **bf = 20 - 150**: Strong
- **bf > 150**: Very strong




### Bayesian Convergence Diagnostic (Rthat and Effective Sample Size)


Experts have suggested thresholds value to help interpreting and convergence and sampling quality. As such, `Rhat` should not be larger than 1.1 [@gelman1992inference] or 1.01 [@vehtari2019rank]. An `effective sample size` (ESS) greater than 1,000 is sufficient for stable estimates [@burkner2017brms].


### Other Bayesian Indices (\% in ROPE, *pd*)


The interpretation of Bayesian indices is detailed in [this article](https://easystats.github.io/bayestestR/articles/guidelines.html).



## References---
title: "Data Standardization"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, rules of thumb, guidelines, interpretation]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Data standardization}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)


pkgs <- c("effectsize", "dplyr", "bayestestR", "see", "ggplot2", "parameters", "lme4", "KernSmooth")
if (!all(sapply(pkgs, requireNamespace, quietly = TRUE))) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

# Introduction

To make sense of their data and effects, scientists might want to standardize (Z-score) their variables. They become unitless, expressed only in terms of deviation from an index of centrality (e.g., the mean or the median). However, aside from some benefits, standardization also comes with challenges and issues, that the scientist should be aware of.




## Normal *vs.* Robust

The `effectsize` package offers two methods of standardization. 

**[TO DO]**



## Variable-wise *vs.* Participant-wise 

Standardization is an important step and extra caution is required in **repeated-measures designs**, in which there are three ways of standardizing data:

-   **Variable-wise**: The most common method. A simple scaling of each column.
-   **Participant-wise**: Variables are standardized "within" each participant, *i.e.*, for each participant, by the participant's mean and SD.
-   **Full**: Participant-wise first and then re-standardizing variable-wise.

Unfortunately, the method used is often not explicitly stated. This is an issue as these methods can generate important discrepancies (that can in turn contribute to the reproducibility crisis). Let's investigate these 3 methods.

# The data


We will take the `emotion` dataset in which participants were exposed to negative pictures and had to rate their emotions (**valence**) and the amount of memories associated with the picture (**autobiographical link**). One could make the hypothesis that for young participants with no context of war or violence, the most negative pictures (mutilations) are less related to memories than less negative pictures (involving for example car crashes or sick people). In other words, **we expect a positive relationship between valence** (with high values corresponding to less negativity) **and autobiographical link**.

Let's have a look at the data, averaged by participants:

```{r, fig.width=7, fig.height=4.5, results='hide', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
library(dplyr)

# Download the 'emotion' dataset
load(url("https://raw.github.com/neuropsychology/psycho.R/master/data/emotion.rda"))  
  
# Discard neutral pictures (keep only negative)
df <- emotion %>% 
  filter(Emotion_Condition == "Negative")  

# Summary
df %>% 
  group_by(Participant_ID) %>% 
  summarise(n_Trials = n(),
            Valence_Mean = mean(Subjective_Valence, na.rm=TRUE),
            Valence_SD = sd(Subjective_Valence, na.rm=TRUE),
            Autobiographical_Link_Mean = mean(Autobiographical_Link, na.rm=TRUE),
            Autobiographical_Link_SD = sd(Autobiographical_Link, na.rm=TRUE))
```

As we can see from the means and SDs, there is a lot of variability **between and within** participants (between their means and their individual SD).


# Standardize

We will create three dataframes standardized with each of the three techniques.

```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
library(effectsize)

Z_VariableWise <- df %>% 
  standardize()

Z_ParticipantWise <- df %>% 
  group_by(Participant_ID) %>% 
  standardize() 

Z_Full <- df %>% 
  group_by(Participant_ID) %>% 
  standardize() %>% 
  ungroup() %>% 
  standardize() 
```

# Effect of Standardization 

Let's see how these three standardization techniques affected the **Valence** variable.

## At a general level
```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
# Create a convenient function to print
print_summary <- function(data){
  paste0(
    paste0(deparse(substitute(data)), ": "), 
    paste(round(mean(data[["Subjective_Valence"]]), 3), 
          "+-", 
          round(sd(data[["Subjective_Valence"]]), 3)),
    paste0(" [", 
           round(min(data[["Subjective_Valence"]]), 3),
           ",", 
           round(max(data[["Subjective_Valence"]]), 3),
           "]")
    )
}

# Check the results
print_summary(Z_VariableWise)
print_summary(Z_ParticipantWise)
print_summary(Z_Full)
```

The **means** and the **SD** appear as fairly similar (0 and 1)...

##  Distribution
```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
library(bayestestR)
library(see)

data.frame(VarWise = Z_VariableWise$Subjective_Valence,
           ParWise = Z_ParticipantWise$Subjective_Valence,
           Full = Z_Full$Subjective_Valence) %>% 
  estimate_density(method="kernSmooth") %>% 
  plot() +
  see::theme_modern()
```

and the distributions appear to be similar...


## At a participant level

```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
# Create convenient function
print_participants <- function(data){
  data %>% 
    group_by(Participant_ID) %>% 
    summarise(Mean = mean(Subjective_Valence), 
              SD = sd(Subjective_Valence)) %>% 
    mutate_if(is.numeric, round, 2) %>% 
    head(5) 
    
}

# Check the results
print_participants(Z_VariableWise)
print_participants(Z_ParticipantWise)
print_participants(Z_Full)
```

Ok so there are some differences here....

# Correlation

Let's do a **correlation** between the **variable-wise and participant-wise methods**.


```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
library(ggplot2)

cor.test(Z_VariableWise$Subjective_Valence, Z_ParticipantWise$Subjective_Valence)


data.frame(Original = df$Subjective_Valence,
           VariableWise = Z_VariableWise$Subjective_Valence,
           ParticipantWise = Z_ParticipantWise$Subjective_Valence) %>% 
  ggplot(aes(x=VariableWise, y=ParticipantWise, colour=Original)) +
  geom_point() +
  geom_smooth(method="lm") +
  see::theme_modern()
```


While the three standardization methods roughly present the same characteristics at a general level (mean 0 and SD 1) and a similar distribution, their values are not exactly the same!

# Test

Let's now answer to the original question by investigating the **linear relationship between valence and autobiographical link**. We can do this by running a mixed model with participants entered as random effects.


```{r, fig.width=7, fig.height=4.5, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
library(lme4)
library(parameters)

# Convenient function
print_model <- function(data){
  type_name <- deparse(substitute(data)) 
  lmer(Subjective_Valence ~ Autobiographical_Link + (1|Participant_ID), data=data) %>% 
    parameters() %>% 
    filter(Parameter == "Autobiographical_Link") %>% 
    mutate(Type = type_name,
           Coefficient = round(Coefficient, 3),
           p = round(p, 3)) %>% 
    select(Type, Coefficient, p)
}

# Run the model on all datasets
rbind(print_model(df), 
      print_model(Z_VariableWise),
      print_model(Z_ParticipantWise),
      print_model(Z_Full))
```

As we can see, **variable-wise** standardization only affects **the coefficient** (which is expected, as it changes the unit), but not the **test statistics** (the *p*-value, in this case). However, using **participant-wise** standardization **does** affect the coefficient **and the *p*-value**. No method is better or more justified, and its choice depends on the specific case, context, data and goal.

# Conclusion

1.  **Standardization can be useful in *some* cases and should be justified**
2.  **Variable and Participant-wise standardization methods produce "in appearance" similar data**
3.  **Variable and Participant-wise standardization can lead to different results**
4.  **The choice of the method can strongly influence the results and thus, should be explicitly stated**

We showed here yet another way of **sneakily tweaking the data** that can change the results. To prevent its use for bad practices (e.g., *p*-hacking), we can only support the generalization of open-data, open-analysis and preregistration**


# References---
title: "Converting between Indices of Effect Size"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, effect size, rules of thumb, guidelines, conversion]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Converting between indices of effect size}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

pkgs <- c("effectsize")
if (!all(sapply(pkgs, requireNamespace))) {
  knitr::opts_chunk$set(eval = FALSE)
}
```



The `effectsize` package contains function to convert among indices of effect size. **This vignette needs to be improved! Please contribute by editing the text** [**here**](https://github.com/easystats/effectsize/blob/master/vignettes/convert.Rmd) **to make it better :)**

```{r, warning=FALSE, message=FALSE}
library(effectsize)

convert_d_to_r(d = 1)
```



## References---
title: "Node Modification"
author: "Jim Hester"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Node Modification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(xml2)
library(magrittr)
```

# Modifying Existing XML

Modifying existing XML can be done in xml2 by using the replacement functions
of the accessors. They all have methods for both individual `xml_node` objects
as well as `xml_nodeset` objects. If a vector of values is provided it is
applied piecewise over the nodeset, otherwise the value is recycled.

## Text Modification ##

Text modification only happens on text nodes. If a given node has more than one
text node only the first will be affected. If you want to modify additional
text nodes you need to select them explicitly with `/text()`.

```{r}
x <- read_xml("<p>This is some <b>text</b>. This is more.</p>")
xml_text(x)

xml_text(x) <- "This is some other text."
xml_text(x)

# You can avoid this by explicitly selecting the text node.
x <- read_xml("<p>This is some text. This is <b>bold!</b></p>")
text_only <- xml_find_all(x, "//text()")

xml_text(text_only) <- c("This is some other text. ", "Still bold!")
xml_text(x)
xml_structure(x)
```

## Attribute and Namespace Definition Modification ##

Attributes and namespace definitions are modified one at a time with
`xml_attr()` or all at once with `xml_attrs()`. In both cases using `NULL` as
the value will remove the attribute completely.

```{r}
x <- read_xml("<a href='invalid!'>xml2</a>")
xml_attr(x, "href")

xml_attr(x, "href") <- "https://github.com/r-lib/xml2"
xml_attr(x, "href")

xml_attrs(x) <- c(id = "xml2", href = "https://github.com/r-lib/xml2")
xml_attrs(x)
x

xml_attrs(x) <- NULL
x

# Namespaces are added with as a xmlns or xmlns:prefix attribute
xml_attr(x, "xmlns") <- "http://foo"
x

xml_attr(x, "xmlns:bar") <- "http://bar"
x
```

## Name Modification ##

Node names are modified with `xml_name()`.

```{r}
x <- read_xml("<a><b/></a>")
x
xml_name(x)
xml_name(x) <- "c"
x
```

# Node modification #
All of these functions have a `.copy` argument. If this is set to `FALSE` they
will remove the new node from its location before inserting it into the new
location. Otherwise they make a copy of the node before insertion.

## Replacing existing nodes ##
```{r}
x <- read_xml("<parent><child>1</child><child>2<child>3</child></child></parent>")
children <- xml_children(x)
t1 <- children[[1]]
t2 <- children[[2]]
t3 <- xml_children(children[[2]])[[1]]

xml_replace(t1, t3)
x
```

## Add a sibling ##
```{r}
x <- read_xml("<parent><child>1</child><child>2<child>3</child></child></parent>")
children <- xml_children(x)
t1 <- children[[1]]
t2 <- children[[2]]
t3 <- xml_children(children[[2]])[[1]]

xml_add_sibling(t1, t3)
x

xml_add_sibling(t3, t1, where = "before")
x
```

## Add a child ##
```{r}
x <- read_xml("<parent><child>1</child><child>2<child>3</child></child></parent>")
children <- xml_children(x)
t1 <- children[[1]]
t2 <- children[[2]]
t3 <- xml_children(children[[2]])[[1]]

xml_add_child(t1, t3)
x

xml_add_child(t1, read_xml("<test/>"))
x
```

## Removing nodes ##
The `xml_remove()` can be used to remove a node (and its children) from a
tree. The default behavior is to unlink the node from the tree, but does _not_
free the memory for the node, so R objects pointing to the node are still
valid.

This allows code like the following to work without crashing R

```{r}
x <- read_xml("<foo><bar><baz/></bar></foo>")
x1 <- x %>% xml_children() %>% .[[1]]
x2 <- x1 %>% xml_children() %>% .[[1]]

xml_remove(x1)
rm(x1)
gc()

x2
```
If you are not planning on referencing these nodes again this memory is wasted.
Calling `xml_remove(free = TRUE)` will remove the nodes _and_ free the memory
used to store them.  **Note** In this case _any_ node which previously pointed
to the node or its children will instead be pointing to free memory and may
cause R to crash. xml2 can't figure this out for you, so it's your
responsibility to remove any objects which are no longer valid.

In particular `xml_find_*()` results are easy to overlook, for example

```{r}
x <- read_xml("<a><b /><b><b /></b></a>")
bees <- xml_find_all(x, "//b")
xml_remove(xml_child(x), free = TRUE)
# bees[[1]] is no longer valid!!!
rm(bees)
gc()
```

## Namespaces ##

We want to construct a document with the following namespace layout. (From
http://stackoverflow.com/questions/32939229/creating-xml-in-r-with-namespaces/32941524#32941524).
```xml
<?xml version = "1.0" encoding="UTF-8"?>
<sld xmlns="http://www.o.net/sld"
     xmlns:ogc="http://www.o.net/ogc"
     xmlns:se="http://www.o.net/se"
     version="1.1.0" >
<layer>
<se:Name>My Layer</se:Name>
</layer>
</sld>
```

```{r}
d <- xml_new_root("sld",
    xmlns = "http://www.o.net/sld",
    "xmlns:ogc" = "http://www.o.net/ogc",
    "xmlns:se" = "http://www.o.net/se",
    version = "1.1.0") %>%
  xml_add_child("layer") %>%
  xml_add_child("se:Name", "My Layer") %>%
  xml_root()

d
```
---
title: "Introduction to DBI"
author: "Katharina Brunner"
date: "14 October 2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{"Introduction to DBI"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The {DBI} package defines a common interface between the R and database management systems (DBMS).
Hence the name: DBI stands for **d**ata**b**ase **i**nterface.

Using DBI, developers can focus on the functionalities of their code, instead of setting up the infrastructure depending on the underlying database.
This DBMS-agnostic approach is possible, because DBI works best with several other packages that act as drivers to absorb the peculiarities of the specific DBMSs.

These packages import {DBI} and implement its methods depending on the specific database management system.

Currently, DBI works with the [many different database management systems](https://github.com/r-dbi/DBI/issues/274), e.g.:

* MySQL, using the R-package [RMySQL](https://github.com/r-dbi/RMySQL)
* MariaDB, using the R-package [RMariaDB](https://github.com/r-dbi/RMariaDB)
* Postgres, using and the R-package [RPostgres](https://github.com/r-dbi/RPostgres)
* SQLite, using and the R-package [RSQLite](https://github.com/r-dbi/RSQLite)

DBI offers a set of classes and methods that define what operations are possible and how they are performed:

* connect/disconnect to the DBMS
* create and execute statements in the DBMS
* extract results/output from statements
* error/exception handling
* information (meta-data) from database objects
* transaction management (optional)

## Examples

To showcase DBI capabilities, we create a in-memory RSQLite database 

```{r}
library(DBI)

con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
con
```

The function `dbListTables()` displays the names tables in the remote database.
Since we haven't pushed any data to the database, there are no tables to show.

```{r}
dbListTables(con)
```

We can write the famous data `mtcars` dataset to the remote database by using `dbWriteTable()`.
Calling `dbListTables()` displays the table name:

```{r}

dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)

```

To get all columns names of a remote table, use `dbListFields()`.
It returns a character vector with all column names in the same order as in the database:

```{r}
dbListFields(con, "mtcars")
```
If you want to import database table from the DBMS as a data frame, `dbReadTable()` helps to do that.
Basically, it is the result of the most generic SQL call `SELECT * FROM <name>`.

```{r}
dbReadTable(con, "mtcars")
```

Of course, you can run more specific SQL queries, too.
`dbGetQuery()` is the function to send a query to a database and retrieve the result as a data frame.
Especially when working with large datasets, it is important to free the resources associated with retrieving the result. `dbGetQuery()` cares about this, too.

```{r}
df <- dbGetQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
df
```

Behind the scences, `dbGetQuery()` is a combination of `dbSendQuery()`, `dbFetch()` and `dbClearResult()`.
The following snippet leads to the same result as `dbGetQuery()` above:

```{r}
res <- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
df <- dbFetch(res)
dbClearResult(res)
df
```
When working with large datasets it might be smart to fetch the result step by step, not in one big chunk.
This can be implemented with a `while` loop and a `dbFetch()` call that defines a maximum number of records to retrieve per fetch, here `n = 5`.
There are eleven cars with four cylinders, so we expect two chunks of five rows and one chuck of one row:

```{r}
res <- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
while(!dbHasCompleted(res)){
  chunk <- dbFetch(res, n = 5)
  print(nrow(chunk))
}
```

Again, call `dbClearResult()` and disconnect from the connection with `dbDisconnect()`, when you are done:

```{r}
dbClearResult(res)
dbDisconnect(con)
```


## Further Reading

* An overview on [working with databases in R on Rstudio.com](https://db.rstudio.com/)

---
title: "Implementing a new backend"
author: "Hadley Wickham"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Implementing a new backend}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE}
library(DBI)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

The goal of this document is to help you implement a new backend for DBI. 

If you are writing a package that connects a database to R, I highly recommend that you make it DBI compatible because it makes your life easier by spelling out exactly what you need to do. The consistent interface provided by DBI makes it easier for you to implement the package (because you have fewer arbitrary choices to make), and easier for your users (because it follows a familiar pattern). In addition, the `DBItest` package provides test cases which you can easily incorporate in your package.

I'll illustrate the process using a fictional database called Kazam.

## Getting started

Start by creating a package. It's up to you what to call the package, but following the existing pattern of `RSQLite`, `RMySQL`, `RPostgres` and `ROracle` will make it easier for people to find it. For this example, I'll call my package `RKazam`.

In your `DESCRIPTION`, make sure to include:

```yaml
Imports:
  DBI (>= 0.3.0),
  methods
Suggests:
  DBItest, testthat
```

Importing `DBI` is fine, because your users are not supposed to *attach* your package anyway; the preferred method is to attach `DBI` and use explicit qualification via `::` to access the driver in your package (which needs to be done only once).


## Testing

Why testing at this early stage? Because testing should be an integral part of the software development cycle. Test right from the start, add automated tests as you go, finish faster (because tests are automated) while maintaining superb code quality (because tests also check corner cases that you might not be aware of). Don't worry: if some test cases are difficult or impossible to satisfy, or take too long to run, you can just turn them off.

Take the time now to head over to the `DBItest` vignette. You will find a vast amount of ready-to-use test cases that will help you in the process of implementing your new DBI backend.

```r
vignette("test", package = "DBItest")
```

Add custom tests that are not covered by `DBItest` at your discretion, or enhance `DBItest` and file a pull request if the test is generic enough to be useful for many DBI backends.


## Driver

Start by making a driver class which inherits from `DBIDriver`. This class doesn't need to do anything, it's just used to dispatch other generics to the right method. Users don't need to know about this, so you can remove it from the default help listing with `@keywords internal`:

```{r}
#' Driver for Kazam database.
#' 
#' @keywords internal
#' @export
#' @import DBI
#' @import methods
setClass("KazamDriver", contains = "DBIDriver")
```

The driver class was more important in older versions of DBI, so you should also provide a dummy `dbUnloadDriver()` method.

```{r}
#' @export
#' @rdname Kazam-class
setMethod("dbUnloadDriver", "KazamDriver", function(drv, ...) {
  TRUE
})
```

If your package needs global setup or tear down, do this in the `.onLoad()` and `.onUnload()` functions.

You might also want to add a show method so the object prints nicely:

```{r}
setMethod("show", "KazamDriver", function(object) {
  cat("<KazamDriver>\n")
})
```

Next create `Kazam()` which instantiates this class.

```{r}
#' @export
Kazam <- function() {
  new("KazamDriver")
}

Kazam()
```

## Connection

Next create a connection class that inherits from `DBIConnection`. This should store all the information needed to connect to the database. If you're talking to a C api, this will include a slot that holds an external pointer.

```{r}
#' Kazam connection class.
#' 
#' @export
#' @keywords internal
setClass("KazamConnection", 
  contains = "DBIConnection", 
  slots = list(
    host = "character", 
    username = "character", 
    # and so on
    ptr = "externalptr"
  )
)
```
Now you have some of the boilerplate out of the way, you can start work on the connection. The most important method here is `dbConnect()` which allows you to connect to a specified instance of the database. Note the use of `@rdname Kazam`. This ensures that `Kazam()` and the connect method are documented together.

```{r}
#' @param drv An object created by \code{Kazam()} 
#' @rdname Kazam
#' @export
#' @examples
#' \dontrun{
#' db <- dbConnect(RKazam::Kazam())
#' dbWriteTable(db, "mtcars", mtcars)
#' dbGetQuery(db, "SELECT * FROM mtcars WHERE cyl == 4")
#' }
setMethod("dbConnect", "KazamDriver", function(drv, ...) {
  # ...
  
  new("KazamConnection", host = host, ...)
})
```

* Replace `...` with the arguments needed to connect to your database. You'll
  always need to include `...` in the arguments, even if you don't use it,
  for compatibility with the generic.
  
* This is likely to be where people first come for help, so the examples should show 
  how to connect to the database, and how to query it. (Obviously these examples 
  won't work yet.) Ideally, include examples that can be run right away 
  (perhaps relying on a publicly hosted database), but failing that surround
  in `\dontrun{}` so people can at least see the code.

Next, implement `show()` and `dbDisconnect()` methods. 


## Results

Finally, you're ready to implement the meat of the system: fetching results of a query into a data frame. First define a results class:

```{r}
#' Kazam results class.
#' 
#' @keywords internal
#' @export
setClass("KazamResult", 
  contains = "DBIResult",
  slots = list(ptr = "externalptr")
)
```

Then write a `dbSendQuery()` method. This takes a connection and SQL string as arguments, and returns a result object. Again `...` is needed for compatibility with the generic, but you can add other arguments if you need them.

```{r}
#' Send a query to Kazam.
#' 
#' @export
#' @examples 
#' # This is another good place to put examples
setMethod("dbSendQuery", "KazamConnection", function(conn, statement, ...) {
  # some code
  new("KazamResult", ...)
})
```

Next, implement `dbClearResult()`, which should close the result set and free all resources associated with it:

```{r}
#' @export
setMethod("dbClearResult", "KazamResult", function(res, ...) {
  # free resources
  TRUE
})
```

The hardest part of every DBI package is writing the `dbFetch()` method. This needs to take a result set and (optionally) number of records to return, and create a dataframe. Mapping R's data types to those of your database may require a custom implementation of the `dbDataType()` method for your connection class:

```{r}
#' Retrieve records from Kazam query
#' @export
setMethod("dbFetch", "KazamResult", function(res, n = -1, ...) {
  ...
})

# (optionally)

#' Find the database data type associated with an R object
#' @export
setMethod("dbDataType", "KazamConnection", function(dbObj, obj, ...) {
  ...
})
```

Next, implement `dbHasCompleted()` which should return a `logical` indicating if there are any rows remaining to be fetched.

```{r}
#' @export
setMethod("dbHasCompleted", "KazamResult", function(res, ...) { 
  
})
```

With these four methods in place, you can now use the default `dbGetQuery()` to send a query to the database, retrieve results if available and then clean up. Spend some time now making sure this works with an existing database, or relax and let the `DBItest` package do the work for you.

## SQL methods

You're now on the home stretch, and can make your wrapper substantially more useful by implementing methods that wrap around variations in SQL across databases:

* `dbQuoteString()` and `dbQuoteIdentifer()` are used to safely quote strings
  and identifiers to avoid SQL injection attacks.  Note that the former must be
  vectorized, but not the latter.

* `dbWriteTable()` creates a database table given an R dataframe. I'd recommend
  using the functions prefixed with `sql` in this package to generate the SQL.
  These functions are still a work in progress so please let me
  know if you have problems.
  
* `dbReadTable()`: a simple wrapper around `SELECT * FROM table`. Use 
  `dbQuoteIdentifer()` to safely quote the table name and prevent mismatches
  between the names allowed by R and the database.
  
* `dbListTables()` and `dbExistsTable()` let you determine what tables are
  available. If not provided by your database's API, you may need to generate
  sql that inspects the system tables.
  
* `dbListFields()` shows which fields are available in a given table.

* `dbRemoveTable()` wraps around `DROP TABLE`. Start with `SQL::sqlTableDrop()`.

* `dbBegin()`, `dbCommit()` and `dbRollback()`: implement these three functions
  to provide basic transaction support. This functionality is currently not
  tested in the `DBItest` package.

## Metadata methods

There are a lot of extra metadata methods for result sets (and one for the connection) that you might want to implement. They are described in the following.

* `dbIsValid()` returns if a connection or a result set is open (`TRUE`) or
  closed (`FALSE`). All further methods in this section are valid for result
  sets only.

* `dbGetStatement()` returns the issued query as a character value.

* `dbColumnInfo()` lists the names and types of the result set's columns.

* `dbGetRowCount()` and `dbGetRowsAffected()` returns the number of rows
  returned or altered in a `SELECT` or `INSERT`/`UPDATE` query, respectively.

* `dbBind()` allows using parametrised queries. Take a look at
  `sqlInterpolate()` and `sqlParseVariables()` if your SQL engine doesn't
  offer native parametrised queries.


## Full DBI compliance

By now, your package should implement all methods defined in the DBI specification.  If you want to walk the extra mile, offer a read-only mode that allows your users to be sure that their valuable data doesn't get destroyed inadvertently.
---
title: "A Common Interface to Relational Databases from R and S -- A Proposal"
author: "David James"
date: "March 16, 2000"
output: rmarkdown::html_vignette
bibliography: biblio.bib
vignette: >
  %\VignetteIndexEntry{A Common Interface to Relational Databases from R and S -- A Proposal}
  %\VignetteEngine{knitr::rmarkdown}
---

For too long S and similar data analysis environments have lacked good
interfaces to relational database systems (RDBMS). For the last twenty
years or so these RDBMS have evolved into highly optimized client-server
systems for data storage and manipulation, and currently they serve as
repositories for most of the business, industrial, and research “raw”
data that analysts work with. Other analysis packages, such as SAS, have
traditionally provided good data connectivity, but S and GNU R have
relied on intermediate text files as means of importing data (but see
@R.imp-exp and @R-dbms.) Although this simple approach works well for
relatively modest amounts of mostly static data, it does not scale up to
larger amounts of data distributed over machines and locations, nor does
it scale up to data that is highly dynamic – situations that are
becoming increasingly common.

We want to propose a common interface between R/S and RDBMS that would
allow users to access data stored on database servers in a uniform and
predictable manner irrespective of the database engine. The interface
defines a small set of classes and methods similar in spirit to Python’s
DB-API, Java’s JDBC, Microsoft’s ODBC, Perl’s DBI, etc., but it conforms
to the “whole-object” philosophy so natural in S and R.

# Computing with Distributed Data {#sec:distr}

As data analysts, we are increasingly faced with the challenge of larger
data sources distributed over machines and locations; most of these data
sources reside in relational database management systems (RDBMS). These
relational databases represent a mature client-server distributed
technology that we as analysts could be exploiting more that we’ve done
in the past. The relational technology provides a well-defined standard,
the ANSI SQL-92 @sql92, both for defining and manipulating data in a
highly optimized fashion from virtually any application.

In contrast, S and Splus have provided somewhat limited tools for coping
with the challenges of larger and distributed data sets (Splus does
provide an `import` function to import from databases, but it is quite
limited in terms of SQL facilities). The R community has been more
resourceful and has developed a number of good libraries for connecting
to mSQL, MySQL, PostgreSQL, and ODBC; each library, however, has defined
its own interface to each database engine a bit differently. We think it
would be to everybody’s advantage to coordinate the definition of a
common interface, an effort not unlike those taken in the Python and
Perl communities.

The goal of a common, seamless access to distributed data is a modest
one in our evolution towards a fully distributed computing environment.
We recognize the greater goal of distributed computing as the means to
fully integrate diverse systems – not just databases – into a truly
flexible analysis environment. Good connectivity to databases, however,
is of immediate necessity both in practical terms and as a means to help
us transition from monolithic, self-contained systems to those in which
computations, not only the data, can be carried out in parallel over a
wide number of computers and/or systems @duncan2000. Issues of
reliability, security, location transparency, persistence, etc., will be
new to most of us and working with distributed data may provide a more
gradual change to ease in the ultimate goal of full distributed
computing.

# A Common Interface {#sec:rs-dbi}

We believe that a common interface to databases can help users easily
access data stored in RDBMS. A common interface would describe, in a
uniform way, how to connect to RDBMS, extract meta-data (such as list of
available databases, tables, etc.) as well as a uniform way to execute
SQL statements and import their output into R and S. The current
emphasis is on querying databases and not so much in a full low-level
interface for database development as in JDBC or ODBC, but unlike these,
we want to approach the interface from the “whole-object” perspective
@S4 so natural to R/S and Python – for instance, by fetching all fields
and records simultaneously into a single object.

The basic idea is to split the interface into a front-end consisting of
a few classes and generic functions that users invoke and a back-end set
of database-specific classes and methods that implement the actual
communication. (This is a very well-known pattern in software
engineering, and another good verbatim is the device-independent
graphics in R/S where graphics functions produce similar output on a
variety of different devices, such X displays, Postscript, etc.)

The following verbatim shows the front-end:

```
> mgr <- dbManager("Oracle")  
> con <- dbConnect(mgr, user = "user", passwd = "passwd")
> rs <- dbExecStatement(con, 
          "select fld1, fld2, fld3 from MY_TABLE")
> tbls <- fetch(rs, n = 100)
> hasCompleted(rs)
[1] T
> close(rs)
> rs <- dbExecStatement(con, 
          "select id_name, q25, q50 from liv2")
> res <- fetch(rs)
> getRowCount(rs)
[1] 73
> close(con)
```

Such scripts should work with other RDBMS (say, MySQL) by replacing the
first line with

```
> mgr <- dbManager("MySQL")
```

## Interface Classes {#sec:rs-dbi-classes}

The following are the main RS-DBI classes. They need to be extended by
individual database back-ends (MySQL, Oracle, etc.)

`dbManager`

:   Virtual class[^2] extended by actual database managers, e.g.,
    Oracle, MySQL, Informix.

`dbConnection`

:   Virtual class that captures a connection to a database
    instance[^3].

`dbResult`

:   Virtual class that describes the result of an SQL statement.

`dbResultSet`

:   Virtual class, extends `dbResult` to fully describe the output of
    those statements that produce output records, i.e., `SELECT` (or
    `SELECT`-like) SQL statement.

All these classes should implement the methods `show`, `describe`, and
`getInfo`:

`show`

:   (`print` in R) prints a one-line identification of the object.

`describe`

:   prints a short summary of the meta-data of the specified object
    (like `summary` in R/S).

`getInfo`

:   takes an object of one of the above classes and a string specifying
    a meta-data item, and it returns the corresponding information
    (`NULL` if unavailable).

        > mgr <- dbManager("MySQL")
        > getInfo(mgr, "version")
        > con <- dbConnect(mgr, ...)
        > getInfo(con, "type")  

The reason we implement the meta-data through `getInfo` in this way is
to simplify the writing of database back-ends. We don’t want to
overwhelm the developers of drivers (ourselves, most likely) with
hundreds of methods as in the case of JDBC.

In addition, the following methods should also be implemented:

`getDatabases`

:   lists all available databases known to the `dbManager`.

`getTables`

:   lists tables in a database.

`getTableFields`

:   lists the fields in a table in a database.

`getTableIndices`

:   lists the indices defined for a table in a database.

These methods may be implemented using the appropriate `getInfo` method
above.

In the next few sections we describe in detail each of these classes and
their methods.

### Class `dbManager` {#sec:dbManager}

This class identifies the relational database management system. It
needs to be extended by individual back-ends (Oracle, PostgreSQL, etc.)
The `dbManager` class defines the following methods:

`load`

:   initializes the driver code. We suggest having the generator,
    `dbManager(driver)`, automatically load the driver.

`unload`

:   releases whatever resources the driver is using.

`getVersion`

:   returns the version of the RS-DBI currently implemented, plus any
    other relevant information about the implementation itself and the
    RDBMS being used.

### Class `dbConnection` {#sec:dbConnection}

This virtual class captures a connection to a RDBMS, and it provides
access to dynamic SQL, result sets, RDBMS session management
(transactions), etc. Note that the `dbManager` may or may not allow
multiple simultaneous connections. The methods it defines include:

`dbConnect`

:   opens a connection to the database `dbname`. Other likely arguments
    include `host`, `user`, and `password`. It returns an object that
    extends `dbConnection` in a driver-specific manner (e.g., the MySQL
    implementation creates a connection of class `MySQLConnection` that
    extends `dbConnection`). Note that we could separate the steps of
    connecting to a RDBMS and opening a database there (i.e., opening an
    *instance*). For simplicity we do the 2 steps in this method. If the
    user needs to open another instance in the same RDBMS, just open a
    new connection.

`close`

:   closes the connection and discards all pending work.

`dbExecStatement`

:   submits one SQL statement. It returns a `dbResult` object, and in
    the case of a `SELECT` statement, the object also inherits from
    `dbResultSet`. This `dbResultSet` object is needed for fetching the
    output rows of `SELECT` statements. The result of a non-`SELECT`
    statement (e.g., `UPDATE, DELETE, CREATE, ALTER`, ...) is defined as
    the number of rows affected (this seems to be common among RDBMS).

`commit`

:   commits pending transaction (optional).

`rollback`

:   undoes current transaction (optional).

`callProc`

:   invokes a stored procedure in the RDBMS (tentative). Stored
    procedures are *not* part of the ANSI SQL-92 standard and possibly
    vary substantially from one RDBMS to another. For instance, Oracle
    seems to have a fairly decent implementation of stored procedures,
    but MySQL currently does not support them.

`dbExec`

:   submit an SQL “script” (multiple statements). May be implemented by
    looping with `dbExecStatement`.

`dbNextResultSet`

:   When running SQL scripts (multiple statements), it closes the
    current result set in the `dbConnection`, executes the next
    statement and returns its result set.

### Class `dbResult` {#sec:dbResult}

This virtual class describes the result of an SQL statement (any
statement) and the state of the operation. Non-query statements (e.g.,
`CREATE`, `UPDATE`, `DELETE`) set the “completed” state to 1, while
`SELECT` statements to 0. Error conditions set this slot to a negative
number. The `dbResult` class defines the following methods:

`getStatement`

:   returns the SQL statement associated with the result set.

`getDBConnection`

:   returns the `dbConnection` associated with the result set.

`getRowsAffected`

:   returns the number of rows affected by the operation.

`hasCompleted`

:   was the operation completed? `SELECT`’s, for instance, are not
    completed until their output rows are all fetched.

`getException`

:   returns the status of the last SQL statement on a given connection
    as a list with two members, status code and status description.

### Class `dbResultSet` {#sec:dbResultSet}

This virtual class extends `dbResult`, and it describes additional
information from the result of a `SELECT` statement and the state of the
operation. The `completed` state is set to 0 so long as there are
pending rows to fetch. The `dbResultSet` class defines the following
additional methods:

`getRowCount`

:   returns the number of rows fetched so far.

`getNullOk`

:   returns a logical vector with as many elements as there are fields
    in the result set, each element describing whether the corresponding
    field accepts `NULL` values.

`getFields`

:   describes the `SELECT`ed fields. The description includes field
    names, RDBMS internal data types, internal length, internal
    precision and scale, null flag (i.e., column allows `NULL`’s), and
    corresponding S classes (which can be over-ridden with user-provided
    classes). The current MySQL and Oracle implementations define a
    `dbResultSet` as a named list with the following elements:

    `connection`:

    :   the connection object associated with this result set;

    `statement`:

    :   a string with the SQL statement being processed;

    `description`:

    :   a field description `data.frame` with as many rows as there are
        fields in the `SELECT` output, and columns specifying the
        `name`, `type`, `length`, `precision`, `scale`, `Sclass` of the
        corresponding output field.

    `rowsAffected`:

    :   the number of rows that were affected;

    `rowCount`:

    :   the number of rows so far fetched;

    `completed`:

    :   a logical value describing whether the operation has completed
        or not.

    `nullOk`:

    :   a logical vector specifying whether the corresponding column may
        take NULL values.

    The methods above are implemented as accessor functions to this list
    in the obvious way.

`setDataMappings`

:   defines a conversion between internal RDBMS data types and R/S
    classes. We expect the default mappings to be by far the most common
    ones, but users that need more control may specify a class generator
    for individual fields in the result set. (See Section [sec:mappings]
    for details.)

`close`

:   closes the result set and frees resources both in R/S and the RDBMS.

`fetch`

:   extracts the next `max.rec` records (-1 means all).

## Data Type Mappings {#sec:mappings}

The data types supported by databases are slightly different than the
data types in R and S, but the mapping between them is straightforward:
Any of the many fixed and varying length character types are mapped to
R/S `character`. Fixed-precision (non-IEEE) numbers are mapped into
either doubles (`numeric`) or long (`integer`). Dates are mapped to
character using the appropriate `TO_CHAR` function in the RDBMS (which
should take care of any locale information). Some RDBMS support the type
`CURRENCY` or `MONEY` which should be mapped to `numeric`. Large objects
(character, binary, file, etc.) also need to be mapped. User-defined
functions may be specified to do the actual conversion as follows:

1.  run the query (either with `dbExec` or `dbExecStatement`):

        > rs <- dbExecStatement(con, "select whatever-You-need")

2.  extract the output field definitions

        > flds <- getFields(rs)

3.  replace the class generator in the, say 3rd field, by the user own
    generator:

        > flds[3, "Sclass"]            # default mapping
        [1] "character"

    by

        > flds[3, "Sclass"] <- "myOwnGeneratorFunction"

4.  set the new data mapping prior to fetching

        > setDataMappings(resutlSet, flds)

5.  fetch the rows and store in a `data.frame`

        > data <- fetch(resultSet)

## Open Issues {#sec:open-issues}

We may need to provide some additional utilities, for instance to
convert dates, to escape characters such as quotes and slashes in query
strings, to strip excessive blanks from some character fields, etc. We
need to decide whether we provide hooks so these conversions are done at
the C level, or do all the post-processing in R or S.

Another issue is what kind of data object is the output of an SQL query.
Currently the MySQL and Oracle implementations return data as a
`data.frame`; data frames have the slight inconvenience that they
automatically re-label the fields according to R/S syntax, changing the
actual RDBMS labels of the variables; the issue of non-numeric data
being coerced into factors automatically “at the drop of a hat” (as
someone in s-news wrote) is also annoying.

The execution of SQL scripts is not fully described. The method that
executes scripts could run individual statements without returning until
it encounters a query (`SELECT`-like) statement. At that point it could
return that one result set. The application is then responsible for
fetching these rows, and then for invoking `dbNextResultSet` on the
opened `dbConnection` object to repeat the `dbExec`/`fetch` loop until
it encounters the next `dbResultSet`. And so on. Another (potentially
very expensive) alternative would be to run all statements sequentially
and return a list of `data.frame`s, each element of the list storing the
result of each statement.

Binary objects and large objects present some challenges both to R and
S. It is becoming more common to store images, sounds, and other data
types as binary objects in RDBMS, some of which can be in principle
quite large. The SQL-92 ANSI standard allows up to 2 gigabytes for some
of these objects. We need to carefully plan how to deal with binary
objects – perhaps tentatively not in full generality. Large objects
could be fetched by repeatedly invoking a specified R/S function that
takes as argument chunks of a specified number of raw bytes. In the case
of S4 (and Splus5.x) the RS-DBI implementation can write into an opened
connection for which the user has defined a reader (but can we guarantee
that we won’t overflow the connection?). In the case of R it is not
clear what data type binary large objects (BLOB) should be mapped into.

## Limitations {#sec:limitations}

These are some of the limitations of the current interface definition:

-   we only allow one SQL statement at a time, forcing users to split
    SQL scripts into individual statements;

-   transaction management is not fully described;

-   the interface is heavily biased towards queries, as opposed to
    general purpose database development. In particular we made no
    attempt to define “bind variables”; this is a mechanism by which the
    contents of S objects are implicitly moved to the database during
    SQL execution. For instance, the following embedded SQL statement

          /* SQL */
          SELECT * from emp_table where emp_id = :sampleEmployee

    would take the vector `sampleEmployee` and iterate over each of its
    elements to get the result. Perhaps RS-DBI could at some point in
    the future implement this feature.

# Other Approaches

The high-level, front-end description of RS-DBI is the more critical
aspect of the interface. Details on how to actually implement this
interface may change over time. The approach described in this document
based on one back-end driver per RDBMS is reasonable, but not the only
approach – we simply felt that a simpler approach based on
well-understood and self-contained tools (R, S, and C API’s) would be a
better start. Nevertheless we want to briefly mention a few alternatives
that we considered and tentatively decided against, but may quite
possibly re-visit in the near future.

## Open Database Connectivity (ODBC) {#sec:odbc}

The ODBC protocol was developed by Microsoft to allow connectivity among
C/C++ applications and RDBMS. As you would expect, originally
implementations of the ODBC were only available under Windows
environments. There are various effort to create a Unix implementation
(see [the Unix ODBC](http://www.unixodbc.org/) web-site and @odbc.lj).
This approach looks promising because it allows us to write only one
back-end, instead of one per RDBMS. Since most RDBMS already provide
ODBC drivers, this could greatly simplify development. Unfortunately,
the Unix implementation of ODBC was not mature enough at the time we
looked at it, a situation we expect will change in the next year or so.
At that point we will need to re-evaluate it to make sure that such an
ODBC interface does not penalize the interface in terms of performance,
ease of use, portability among the various Unix versions, etc.

## Java Database Connectivity (JDBC) {#sec:jdbc}

Another protocol, the Java database connectivity, is very well-done and
supported by just about every RDBMS. The issue with JDBC is that as of
today neither S nor R (which are written in C) interfaces cleanly with
Java. There are several efforts (some in a quite fairly advanced state)
to allow S and R to invoke Java methods. Once this interface is widely
available in Splus5x and R we will need to re-visit this issue again and
study the performance, usability, etc., of JDBC as a common back-end to
the RS-DBI.

## CORBA and a 3-tier Architecture {#sec:corba}

Yet another approach is to move the interface to RDBMS out of R and S
altogether into a separate system or server that would serve as a proxy
between R/S and databases. The communication to this middle-layer proxy
could be done through CORBA [@s-corba.98, @corba:siegel.96], Java’s RMI,
or some other similar technology. Such a design could be very flexible,
but the CORBA facilities both in R and S are not widely available yet,
and we do not know whether this will be made available to Splus5 users
from MathSoft. Also, my experience with this technology is rather
limited.

On the other hand, this 3-tier architecture seem to offer the most
flexibility to cope with very large distributed databases, not
necessarily relational.

# Resources {#sec:resources}

The latest documentation and software on the RS-DBI was available at
www.omegahat.net (link dead now: `http://www.omegahat.net/contrib/RS-DBI/index.html`).
The R community has developed interfaces to some databases:
[RmSQL](https://cran.r-project.org/src/contrib/Archive/RmSQL/) is an
interface to the [mSQL](http://www.Hughes.com.au) database written by
Torsten Hothorn; [RPgSQL](http://www.nceas.ucsb.edu/~keitt/R) is an
interface to [PostgreSQL](http://www.postgreSQL.org) and was written by
Timothy H. Keitt; [RODBC](http://www.stats.ox.ac.uk/pub/bdr) is an
interface to ODBC, and it was written by [Michael
Lapsley](mailto:mlapsley@sthelier.sghms.ac.uk). (For more details on all
these see @R.imp-exp.)

The are R and S-Plus interfaces
to
[MySQL](http://www.mysql.org) that follow the propose RS-DBI API
described here; also, there’s an S-Plus interface
SOracle @RS-Oracle to
[Oracle](http://www.oracle.com) (we expect to have an R implementation
soon.)

The idea of a common interface to databases has been successfully
implemented in Java’s Database Connectivity (JDBC)
([www.javasoft.com](http://www.javasoft.com/products/jdbc/index.html)),
in C through the Open Database Connectivity (ODBC)
([www.unixodbc.org](http://www.unixodbc.org/)), in Python’s
Database Application Programming Interface
([www.python.org](http://www.python.org)), and in Perl’s Database
Interface ([www.cpan.org](http://www.cpan.org)).

# Acknowledgements

The R/S database interface came about from suggestions, comments, and
discussions with [John M. Chambers](mailto:jmc@research.bell-labs.com)
and [Duncan Temple Lang](mailto:duncan@research.bell-labs.com) in the
context of the Omega Project for Statistical Computing. [Doug
Bates](mailto:bates@stat.wisc.edu) and [Saikat
DebRoy](mailto:saikat@stat.wisc.edu) ported (and greatly improved) the
first MySQL implementation to R.

# The S Version 4 Definitions

The following code is meant to serve as a detailed description of the
R/S to database interface. We decided to use S4 (instead of R or S
version 3) because its clean syntax help us to describe easily the
classes and methods that form the RS-DBI, and also to convey the
inter-class relationships.

```R
## Define all the classes and methods to be used by an
## implementation of the RS-DataBase Interface.  Mostly, 
## these classes are virtual and each driver should extend
## them to provide the actual implementation.

## Class: dbManager 
## This class identifies the DataBase Management System
## (Oracle, MySQL, Informix, PostgreSQL, etc.)

setClass("dbManager", VIRTUAL)

setGeneric("load", 
   def = function(dbMgr,...) 
   standardGeneric("load")
   )
setGeneric("unload", 
   def = function(dbMgr,...)
   standardGeneric("unload")
   )
setGeneric("getVersion", 
   def = function(dbMgr,...) 
   standardGeneric("getVersion")
   )

## Class: dbConnections 
## This class captures a connection to a database instance. 

setClass("dbConnection", VIRTUAL)

setGeneric("dbConnection", 
   def = function(dbMgr, ...) 
   standardGeneric("dbConnection")
   )
setGeneric("dbConnect", 
   def = function(dbMgr, ...) 
   standardGeneric("dbConnect")
   )
setGeneric("dbExecStatement", 
   def = function(con, statement, ...)
   standardGeneric("dbExecStatement")
   )
setGeneric("dbExec", 
   def = function(con, statement, ...) 
   standardGeneric("dbExec")
   )
setGeneric("getResultSet", 
   def = function(con, ..) 
   standardGeneric("getResultSet")
   )
setGeneric("commit", 
   def = function(con, ...) 
   standardGeneric("commit")
   )
setGeneric("rollback", 
   def = function(con, ...) 
   standardGeneric("rollback")
   )
setGeneric("callProc", 
   def = function(con, ...) 
   standardGeneric("callProc")
   )
setMethod("close", 
   signature = list(con="dbConnection", type="missing"),
   def = function(con, type) NULL
   )

## Class: dbResult
## This is a base class for arbitrary results from the RDBMS 
## (INSERT, UPDATE, DELETE).  SELECTs (and SELECT-like) 
## statements produce "dbResultSet" objects, which extend 
## dbResult.

setClass("dbResult", VIRTUAL)

setMethod("close", 
   signature = list(con="dbResult", type="missing"),
   def = function(con, type) NULL
   )

## Class: dbResultSet
## Note that we define a resultSet as the result of a 
## SELECT  SQL statement.

setClass("dbResultSet", "dbResult")

setGeneric("fetch", 
   def = function(resultSet,n,...) 
   standardGeneric("fetch")
   )
setGeneric("hasCompleted",
   def = function(object, ...) 
   standardGeneric("hasCompleted")
   )
setGeneric("getException",
   def = function(object, ...) 
   standardGeneric("getException")
   )
setGeneric("getDBconnection",
   def = function(object, ...) 
   standardGeneric("getDBconnection")
   )
setGeneric("setDataMappings", 
   def = function(resultSet, ...) 
   standardGeneric("setDataMappings")
   )
setGeneric("getFields",
   def = function(object, table, dbname,  ...) 
   standardGeneric("getFields")
   )
setGeneric("getStatement", 
   def = function(object, ...) 
   standardGeneric("getStatement")
   )
setGeneric("getRowsAffected",
   def = function(object, ...) 
   standardGeneric("getRowsAffected")
   )
setGeneric("getRowCount",
   def = function(object, ...) 
   standardGeneric("getRowCount")
   )
setGeneric("getNullOk",
   def = function(object, ...) 
   standardGeneric("getNullOk")
   )

## Meta-data: 
setGeneric("getInfo", 
   def = function(object, ...) 
   standardGeneric("getInfo")
   )
setGeneric("describe", 
   def = function(object, verbose=F, ...)
   standardGeneric("describe")
   )
setGeneric("getCurrentDatabase",
   def = function(object, ...)
   standardGeneric("getCurrentDatabase")
   )
setGeneric("getDatabases", 
   def = function(object, ...)
   standardGeneric("getDatabases")
   )
setGeneric("getTables", 
   def = function(object, dbname, ...)
   standardGeneric("getTables")
   )
setGeneric("getTableFields", 
   def = function(object, table, dbname, ...)
   standardGeneric("getTableFields")
   )
setGeneric("getTableIndices", 
   def = function(object, table, dbname, ...) 
   standardGeneric("getTableIndices")
   )
```

[^2]: A virtual class allows us to group classes that share some common
    functionality, e.g., the virtual class “`dbConnection`” groups all
    the connection implementations by Informix, Ingres, DB/2, Oracle,
    etc. Although the details will vary from one RDBMS to another, the
    defining characteristic of these objects is what a virtual class
    captures. R and S version 3 do not explicitly define virtual
    classes, but they can easily implement the idea through inheritance.

[^3]: The term “database” is sometimes (confusingly) used both to denote
    the RDBMS, such as Oracle, MySQL, and also to denote a particular
    database instance under a RDBMS, such as “opto” or “sales” databases
    under the same RDBMS.

---
title: "History of DBI"
author: "David James"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{"History of DBI"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The following history of DBI was contributed by David James, the driving force behind the development of DBI, and many of the packages that implement it.

The idea/work of interfacing S (originally S3 and S4) to RDBMS goes back to the mid- and late 1990's in Bell Labs. The first toy interface I did was to implement John Chamber's early concept of "Data Management in S" (1991). The implementation followed that interface pretty closely and immediately showed some of the limitations when dealing with very large databases; if my memory serves me, the issue was the instance-based of the language back then, e.g., if you attached an RDBMS to the `search()` path and then needed to resolve a symbol "foo", you effectively had to bring all the objects in the database to check their mode/class, i.e., the instance object had the metadata in itself as attributes. The experiment showed that the S3 implementation of "data management" was not really suitable to large external RDBMS (probably it was never intended to do that anyway). (Note however, that since then, John and Duncan Temple Lang generalized the data management in S4 a lot, including Duncan's implementation in his RObjectTables package where he considered a lot of synchronization/caching issues relevant to DBI and, more generally, to most external interfaces).

Back then we were working very closely with Lucent's microelectronics manufacturing --- our colleagues there had huge Oracle (mostly) databases that we needed to constantly query via [SQL*Plus](http://en.wikipedia.org/wiki/SQL*Plus). My colleague Jake Luciani was developing advanced applications in C and SQL, and the two of us came up with the first implementation of S3 directly connecting with Oracle.  What I remember is that the Linux [PRO*C](http://en.wikipedia.org/wiki/Pro*C) pre-compiler (that embedded SQL in C code) was very buggy --- we spent a lot of time looking for workarounds and tricks until we got the C interface running.  At the time, other projects within Bell Labs began using MySQL, and we moved to MySQL (with the help of Doug Bates' student Saikat DebRoy, then a summer intern) with no intentions of looking back at the very difficult Oracle interface.  It was at this time that I moved all the code from S3 methods to S4 classes and methods and begun reaching out to the S/R community for suggestions, ideas, etc.  All (most) of this work was on Bell Labs versions of S3 and S4, but I made sure it worked with S-Plus. At some point around 2000 (I don't remember exactly when), I ported all the code to R regressing to S3 methods, and later on (once S4 classes and methods were available in R) I re-implemented everything back to S4 classes and methods in R (a painful back-and-forth). It was at this point that I decided to drop S-Plus altogether.  Around that time, I came across a very early implementation of SQLite and I was quite interested and thought it was a very nice RDBMS that could be used for all kinds of experimentation, etc., so it was pretty easy to implement on top of the DBI.

Within the R community, there were quite a number of people that showed interest on defining a common interface to databases, but only a few folks actually provided code/suggestions/etc.  (Tim Keitt was most active with the dbi/PostgreSQL packages --- he also was considering what he called "proxy" objects, which was reminiscent of what Duncan had been doing).  Kurt Hornick, Vincent Carey, Robert Gentleman, and others provided suggestions/comments/support for the DBI definition. By around 2003, the DBI was more or less implemented as it is today.

I'm sure I'll forget some (most should be in the THANKS sections of the various packages), but the names that come to my mind at this moment are Jake Luciani (ROracle), Don MacQueen and other early ROracle users (super helpful), Doug Bates and his student Saikat DebRoy for RMySQL, Fei Chen (at the time a student of Prof. Ripley) also contributed to RMySQL, Tim Keitt (working on an early S3 interface to PostgrSQL), Torsten Hothorn (worked with mSQL and also MySQL), Prof. Ripley working/extending the RODBC package, in addition to John Chambers and Duncan Temple-Lang who provided very important comments and suggestions.

Actually, the real impetus behind the DBI was always to do distributed statistical computing --- *not* to provide a yet-another import/export mechanism --- and this perspective was driven by John and Duncan's vision and work on inter-system computing, COM, CORBA, etc.  I'm not sure many of us really appreciated (even now) the full extent of those ideas and concepts.  Just like in other languages (C's ODBC, Java's JDBC, Perl's DBI/DBD, Python dbapi), R/S DBI was meant to unify the interfacing to RDBMS so that R/S applications could be developed on top of the DBI and not be hard coded to any one relation database.  The interface I tried to follow the closest was the Python's DBAPI --- I haven't worked on this topic for a while, but I still feel Python's DBAPI is the cleanest and most relevant for the S language.
---
title: "DBI specification"
author: "Kirill Müller"
output: rmarkdown::html_vignette
abstract: >
  The DBI package defines the generic DataBase Interface for R.
  The connection to individual DBMS is provided by other packages that import DBI
  (so-called *DBI backends*).
  This document formalizes the behavior expected by the methods declared in
  DBI and implemented by the individual backends.
  
  To ensure maximum portability and exchangeability, and to reduce the effort
  for implementing a new DBI backend, the DBItest package defines
  a comprehensive set of test cases that test conformance to the DBI
  specification.
  This document is derived from comments in the test definitions of
  the DBItest package.
  Any extensions or updates to the tests will be reflected in
  this document.
vignette: >
  %\VignetteIndexEntry{DBI specification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo = FALSE}
library(magrittr)
library(xml2)
knitr::opts_chunk$set(echo = FALSE)
r <- rprojroot::is_r_package$make_fix_file()
```

```{r error=TRUE}
rd_db <- tools::Rd_db(dir = r())

Links <- tools::findHTMLlinks()

html_topic <- function(name) {
  rd <- rd_db[[paste0(name, ".Rd")]]

  conn <- textConnection(NULL, "w")
  on.exit(close(conn))

  #tools::Rd2HTML(rd, conn, Links = Links)
  tools::Rd2HTML(rd, conn)

  textConnectionValue(conn)
}

xml_topic <- function(name, patcher) {
  html <- html_topic(name)
  x <- read_html(paste(html, collapse = "\n"))

  # No idea why this is necessary when embedding HTML in Markdown
  codes <- x %>% xml_find_all("//code[contains(., '$')]")
  xml_text(codes) <- gsub("[$]", "\\\\$", xml_text(codes))

  xx <- x %>% xml_find_first("/html/body")
  xx %>% xml_find_first("//table") %>% xml_remove()
  xx %>% xml_find_all("//pre") %>% xml_set_attr("class", "r")
  patcher(xx)
}

out_topic <- function(name, patcher = identity) {
  xml <- lapply(name, xml_topic, patcher = patcher)
  sapply(xml, as.character) %>% paste(collapse = "\n")
}

patch_package_doc <- function(x) {
  x %>% xml_find_first("//h3") %>% xml_remove

  remove_see_also_section(x)
  remove_authors_section(x)

  x
}

move_contents_of_usage_section <- function(x) {
  # http://stackoverflow.com/a/3839299/946850
  usage_contents <-
    x %>%
    xml_find_all(
      "//h3[.='Usage']/following-sibling::node() [not(self::h3)] [count(preceding-sibling::h3)=2]")

  usage_text <-
    usage_contents %>%
    xml_find_first("//pre") %>% 
    xml_text

  h3 <- x %>% xml_find_first("//h3")

  intro_text <-
    read_xml(
      paste0(
        "<p>This section describes the behavior of the following method",
        if (length(grep("[(]", strsplit(usage_text, "\n")[[1]])) > 1) "s" else "",
        ":</p>")
    )

  xml_add_sibling(
    h3,
    intro_text,
    .where = "before")
  lapply(usage_contents, xml_add_sibling, .x = h3, .copy = FALSE, .where = "before")

  x %>% xml_find_first("//h3[.='Usage']") %>% xml_remove
  x
}

move_additional_arguments_section <- function(x) {
  # http://stackoverflow.com/a/3839299/946850 and some trial and error
  additional_arguments <- x %>%
    xml_find_all(
      "//h3[.='Additional arguments'] | //h3[.='Additional arguments']/following-sibling::node()[following-sibling::h3]")

  after_arg <- x %>% xml_find_first("//h3[text()='Arguments']/following-sibling::h3")

  lapply(additional_arguments, xml_add_sibling, .x = after_arg, .copy = FALSE, .where = "before")

  x
}

remove_see_also_section <- function(x) {
  # http://stackoverflow.com/a/3839299/946850 and some trial and error
  x %>%
    xml_find_all(
      "//h3[.='See Also'] | //h3[.='See Also']/following-sibling::node()[following-sibling::h3]") %>%
    xml_remove()
  x
}

remove_authors_section <- function(x) {
  # http://stackoverflow.com/a/3839299/946850 and some trial and error
  x %>%
    xml_find_all(
      "//h3[.='Author(s)'] | //h3[.='Author(s)']/following-sibling::node()[following-sibling::h3]") %>%
    xml_remove()
  x
}

patch_method_doc <- function(x) {
  move_contents_of_usage_section(x)
  remove_see_also_section(x)
  move_additional_arguments_section(x)
  x
}

topics <- c(
  "dbDataType",
  "dbConnect",
  "dbDisconnect",
  "dbSendQuery",
  "dbFetch",
  "dbClearResult",
  "dbBind",
  "dbGetQuery",
  "dbSendStatement",
  "dbExecute",
  "dbQuoteString",
  "dbQuoteIdentifier",
  "dbReadTable",
  "dbWriteTable",
  "dbListTables",
  "dbExistsTable",
  "dbRemoveTable",
  "dbListFields",
  "dbIsValid",
  "dbHasCompleted",
  "dbGetStatement",
  "dbGetRowCount",
  "dbGetRowsAffected",
  "dbColumnInfo",
  "transactions",
  "dbWithTransaction"
)

html <- c(
  out_topic("DBI-package", patch_package_doc),
  out_topic(topics, patch_method_doc)
)

temp_html <- tempfile(fileext = ".html")
temp_md <- tempfile(fileext = ".md")

#temp_html <- "out.html"
#temp_md <- "out.md"

#html <- '<html><body><pre class="r">\na\nb\n</pre></body></html>'

writeLines(html, temp_html)
rmarkdown::pandoc_convert(temp_html, "gfm", verbose = FALSE, output = temp_md)
knitr::asis_output(paste(readLines(temp_md), collapse = "\n"))
```
---
title: "A Common Database Interface (DBI)"
author: "R-Databases Special Interest Group"
date: "16 June 2003"
output: rmarkdown::html_vignette
bibliography: biblio.bib
vignette: >
  %\VignetteIndexEntry{A Common Database Interface (DBI)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document describes a common interface between the S language (in
its R and S-Plus implementations) and database management systems
(DBMS). The interface defines a small set of classes and methods similar
in spirit to Perl’s DBI, Java’s JDBC, Python’s DB-API, and Microsoft’s
ODBC.

# Version {#sec:version}

This document describes version 0.1-6 of the database interface API
(application programming interface).

# Introduction {#sec:intro}

The database interface (DBI) separates the connectivity to the DBMS into
a “front-end” and a “back-end”. Applications use only the exposed
“front-end” API. The facilities that communicate with specific DBMS
(Oracle, PostgreSQL, etc.) are provided by “device drivers” that get
invoked automatically by the S language evaluator. The following example
illustrates some of the DBI capabilities:

```R
## Choose the proper DBMS driver and connect to the server

drv <- dbDriver("ODBC")
con <- dbConnect(drv, "dsn", "usr", "pwd")

## The interface can work at a higher level importing tables 
## as data.frames and exporting data.frames as DBMS tables.

dbListTables(con)
dbListFields(con, "quakes")
if(dbExistsTable(con, "new_results"))
   dbRemoveTable(con, "new_results")
dbWriteTable(con, "new_results", new.output)

## The interface allows lower-level interface to the DBMS
res <- dbSendQuery(con, paste(
            "SELECT g.id, g.mirror, g.diam, e.voltage",
            "FROM geom_table as g, elec_measures as e",
            "WHERE g.id = e.id and g.mirrortype = 'inside'",
            "ORDER BY g.diam"))
out <- NULL
while(!dbHasCompleted(res)){
   chunk <- fetch(res, n = 10000)
   out <- c(out, doit(chunk))
}

## Free up resources
dbClearResult(res)
dbDisconnect(con)
dbUnloadDriver(drv)
```

(only the first 2 expressions are DBMS-specific – all others are
independent of the database engine itself).

Individual DBI drivers need not implement all the features we list below
(we indicate those that are optional). Furthermore, drivers may extend
the core DBI facilities, but we suggest to have these extensions clearly
indicated and documented.

The following are the elements of the DBI:

1.  A set of classes and methods (Section [sec:DBIClasses]) that defines
    what operations are possible and how they are defined, e.g.:

    -   connect/disconnect to the DBMS

    -   create and execute statements in the DBMS

    -   extract results/output from statements

    -   error/exception handling

    -   information (meta-data) from database objects

    -   transaction management (optional)

    Some things are left explicitly unspecified, e.g., authentication
    and even the query language, although it is hard to avoid references
    to SQL and relational database management systems (RDBMS).

2.  Drivers

    Drivers are collection of functions that implement the functionality
    defined above in the context of specific DBMS, e.g., mSQL, Informix.

3.  Data type mappings (Section [sec:data-mappings].)

    Mappings and conversions between DBMS data types and R/S objects.
    All drivers should implement the “basic” primitives (see below), but
    may chose to add user-defined conversion function to handle more
    generic objects (e.g., factors, ordered factors, time series,
    arrays, images).

4.  Utilities (Section [sec:utilities].)

    These facilities help with details such as mapping of identifiers
    between S and DBMS (e.g., `_` is illegal in R/S names, and `.` is
    used for constructing compound SQL identifiers), etc.

# DBI Classes and Methods {#sec:DBIClasses}

The following are the main DBI classes. They need to be extended by
individual database back-ends (Sybase, Oracle, etc.) Individual drivers
need to provide methods for the generic functions listed here (those
methods that are optional are so indicated).

*Note: Although R releases prior to 1.4 do not have a formal concept of
classes, we will use the syntax of the S Version 4 classes and methods
(available in R releases 1.4 and later as library `methods`) to convey
precisely the DBI class hierarchy, its methods, and intended behavior.*

The DBI classes are `DBIObject`, `DBIDriver`, `DBIConnection` and
`DBIResult`. All these are *virtual* classes. Drivers define new classes
that extend these, e.g., `PgSQLDriver`, `PgSQLConnection`, and so on.

![Class hierarchy for the DBI. The top two layers are comprised of
virtual classes and each lower layer represents a set of driver-specific
implementation classes that provide the functionality defined by the
virtual classes above.](hierarchy.png)

`DBIObject`:

:   Virtual class[^1] that groups all other DBI classes.

`DBIDriver`:

:   Virtual class that groups all DBMS drivers. Each DBMS driver extends
    this class. Typically generator functions instantiate the actual
    driver objects, e.g., `PgSQL`, `HDF5`, `BerkeleyDB`.

`DBIConnection`:

:   Virtual class that encapsulates connections to DBMS.

`DBIResult`:

:   Virtual class that describes the result of a DBMS query or
    statement.

    [Q: Should we distinguish between a simple result of DBMS statements
    e.g., as `delete` from DBMS queries (i.e., those that generate
    data).]

The methods `format`, `print`, `show`, `dbGetInfo`, and `summary` are
defined (and *implemented* in the `DBI` package) for the `DBIObject`
base class, thus available to all implementations; individual drivers,
however, are free to override them as they see fit.

`format(x, ...)`:

:   produces a concise character representation (label) for the
    `DBIObject` `x`.

`print(x, ...)`/`show(x)`:

:   prints a one-line identification of the object `x`.

`summary(object, ...)`:

:   produces a concise description of the object. The default method for
    `DBIObject` simply invokes `dbGetInfo(dbObj)` and prints the
    name-value pairs one per line. Individual implementations may tailor
    this appropriately.

`dbGetInfo(dbObj, ...)`:

:   extracts information (meta-data) relevant for the `DBIObject`
    `dbObj`. It may return a list of key/value pairs, individual
    meta-data if supplied in the call, or `NULL` if the requested
    meta-data is not available.

    *Hint:* Driver implementations may choose to allow an argument
    `what` to specify individual meta-data, e.g.,
    `dbGetInfo(drv, what = max.connections)`.

In the next few sub-sections we describe in detail each of these classes
and their methods.

## Class `DBIObject` {#sec:DBIObject}

This class simply groups all DBI classes, and thus all extend it.

## Class `DBIDriver` {#sec:DBIDriver}

This class identifies the database management system. It needs to be
extended by individual back-ends (Oracle, PostgreSQL, etc.)

The DBI provides the generator `dbDriver(driverName)` which simply
invokes the function `driverName`, which in turn instantiates the
corresponding driver object.

The `DBIDriver` class defines the following methods:

`driverName`:

:   [meth:driverName] initializes the driver code. The name `driverName`
    refers to the actual generator function for the DBMS, e.g.,
    `RPgSQL`, `RODBC`, `HDF5`. The driver instance object is used with
    `dbConnect` (see page ) for opening one or possibly more connections
    to one or more DBMS.

`dbListConnections(drv, ...)`:

:   list of current connections being handled by the `drv` driver. May
    be `NULL` if there are no open connections. Drivers that do not
    support multiple connections may return the one open connection.

`dbGetInfo(dbObj, ...)`:

:   returns a list of name-value pairs of information about the driver.

    *Hint:* Useful entries could include

    `name`:

    :   the driver name (e.g., `RODBC`, `RPgSQL`);

    `driver.version`:

    :   version of the driver;

    `DBI.version`:

    :   the version of the DBI that the driver implements, e.g.,
        `0.1-2`;

    `client.version`:

    :   of the client DBMS libraries (e.g., version of the `libpq`
        library in the case of `RPgSQL`);

    `max.connections`:

    :   maximum number of simultaneous connections;

    plus any other relevant information about the implementation, for
    instance, how the driver handles upper/lower case in identifiers.

`dbUnloadDriver(driverName)` (optional):

:   frees all resources (local and remote) used by the driver. Returns a
    logical to indicate if it succeeded or not.

## Class `DBIConnection` {#sec:DBIConnection}

This virtual class encapsulates the connection to a DBMS, and it
provides access to dynamic queries, result sets, DBMS session management
(transactions), etc.

*Note:* Individual drivers are free to implement single or multiple
simultaneous connections.

The methods defined by the `DBIConnection` class include:

`dbConnect(drv, ...)`:

:   [meth:dbConnect] creates and opens a connection to the database
    implemented by the driver `drv` (see Section [sec:DBIDriver]). Each
    driver will define what other arguments are required, e.g., `dbname`
    or `dsn` for the database name, `user`, and `password`. It returns
    an object that extends `DBIConnection` in a driver-specific manner
    (e.g., the MySQL implementation could create an object of class
    `MySQLConnection` that extends `DBIConnection`).

`dbDisconnect(conn, ...)`:

:   closes the connection, discards all pending work, and frees
    resources (e.g., memory, sockets). Returns a logical indicating
    whether it succeeded or not.

`dbSendQuery(conn, statement, ...)`:

:   submits one statement to the DBMS. It returns a `DBIResult` object.
    This object is needed for fetching data in case the statement
    generates output (see `fetch` on page ), and it may be used for
    querying the state of the operation; see `dbGetInfo` and other
    meta-data methods on page .

`dbGetQuery(conn, statement, ...)`:

:   submit, execute, and extract output in one operation. The resulting
    object may be a `data.frame` if the `statement` generates output.
    Otherwise the return value should be a logical indicating whether
    the query succeeded or not.

`dbGetException(conn, ...)`:

:   returns a list with elements `errNum` and `errMsg` with the status
    of the last DBMS statement sent on a given connection (this
    information may also be provided by the `dbGetInfo` meta-data
    function on the `conn` object.

    *Hint:* The ANSI SQL-92 defines both a status code and an status
    message that could be return as members of the list.

`dbGetInfo(dbObj, ...)`:

:   returns a list of name-value pairs describing the state of the
    connection; it may return one or more meta-data, the actual driver
    method allows to specify individual pieces of meta-data (e.g.,
    maximum number of open results/cursors).

    *Hint:* Useful entries could include

    `dbname`:

    :   the name of the database in use;

    `db.version`:

    :   the DBMS server version (e.g., “Oracle 8.1.7 on Solaris”;

    `host`:

    :   host where the database server resides;

    `user`:

    :   user name;

    `password`:

    :   password (is this safe?);

    plus any other arguments related to the connection (e.g., thread id,
    socket or TCP connection type).

`dbListResults(conn, ...)`:

:   list of `DBIResult` objects currently active on the connection
    `conn`. May be `NULL` if no result set is active on `conn`. Drivers
    that implement only one result set per connection could return that
    one object (no need to wrap it in a list).

*Note: The following are convenience methods that simplify the
import/export of (mainly) data.frames. The first five methods implement
the core methods needed to `attach` remote DBMS to the S search path.
(For details, see @data-management:1991 [@database-classes:1999].)*

*Hint:* For relational DBMS these methods may be easily implemented
using the core DBI methods `dbConnect`, `dbSendQuery`, and `fetch`, due
to SQL reflectance (i.e., one easily gets this meta-data by querying the
appropriate tables on the RDBMS).

`dbListTables(conn, ...)`:

:   returns a character vector (possibly of zero-length) of object
    (table) names available on the `conn` connection.

`dbReadTable(conn, name, ...)`:

:   imports the data stored remotely in the table `name` on connection
    `conn`. Use the field `row.names` as the `row.names` attribute of
    the output data.frame. Returns a `data.frame`.

    [Q: should we spell out how row.names should be created? E.g., use a
    field (with unique values) as row.names? Also, should `dbReadTable`
    reproduce a data.frame exported with `dbWriteTable`?]

`dbWriteTable(conn, name, value, ...)`:

:   write the object `value` (perhaps after coercing it to data.frame)
    into the remote object `name` in connection `conn`. Returns a
    logical indicating whether the operation succeeded or not.

`dbExistsTable(conn, name, ...)`:

:   does remote object `name` exist on `conn`? Returns a logical.

`dbRemoveTable(conn, name, ...)`:

:   removes remote object `name` on connection `conn`. Returns a logical
    indicating whether the operation succeeded or not.

`dbListFields(conn, name, ...)`:

:   returns a character vector listing the field names of the remote
    table `name` on connection `conn` (see `dbColumnInfo()` for
    extracting data type on a table).

*Note: The following methods deal with transactions and stored
procedures. All these functions are optional.*

`dbCommit(conn, ...)`(optional):

:   commits pending transaction on the connection and returns `TRUE` or
    `FALSE` depending on whether the operation succeeded or not.

`dbRollback(conn, ...)`(optional):

:   undoes current transaction on the connection and returns `TRUE` or
    `FALSE` depending on whether the operation succeeded or not.

`dbCallProc(conn, storedProc, ...)`(optional):

:   invokes a stored procedure in the DBMS and returns a `DBIResult`
    object.

    [Stored procedures are *not* part of the ANSI SQL-92 standard and
    vary substantially from one RDBMS to another.]

    **Deprecated since 2014:**
    
    The recommended way of calling a stored procedure is now
    
    - `dbGetQuery` if a result set is returned and
    - `dbExecute`  for data manipulation and other cases that do not return a result set.

## Class `DBIResult` {#sec:DBIResult}

This virtual class describes the result and state of execution of a DBMS
statement (any statement, query or non-query). The result set `res`
keeps track of whether the statement produces output for R/S, how many
rows were affected by the operation, how many rows have been fetched (if
statement is a query), whether there are more rows to fetch, etc.

*Note: Individual drivers are free to allow single or multiple active
results per connection.*

[Q: Should we distinguish between results that return no data from those
that return data?]

The class `DBIResult` defines the following methods:

`fetch(res, n, ...)`:

:   [meth:fetch] fetches the next `n` elements (rows) from the result
    set `res` and return them as a data.frame. A value of `n=-1` is
    interpreted as “return all elements/rows”.

`dbClearResult(res, ...)`:

:   flushes any pending data and frees all resources (local and remote)
    used by the object `res` on both sides of the connection. Returns a
    logical indicating success or not.

`dbGetInfo(dbObj, ...)`:

:   returns a name-value list with the state of the result set.

    *Hint:* Useful entries could include

    `statement`:

    :   a character string representation of the statement being
        executed;

    `rows.affected`:

    :   number of affected records (changed, deleted, inserted, or
        extracted);

    `row.count`:

    :   number of rows fetched so far;

    `has.completed`:

    :   has the statement (query) finished?

    `is.select`:

    :   a logical describing whether or not the statement generates
        output;

    plus any other relevant driver-specific meta-data.

`dbColumnInfo(res, ...)`:

:   produces a data.frame that describes the output of a query. The
    data.frame should have as many rows as there are output fields in
    the result set, and each column in the data.frame should describe an
    aspect of the result set field (field name, type, etc.)

    *Hint:* The data.frame columns could include

    `field.name`:

    :   DBMS field label;

    `field.type`:

    :   DBMS field type (implementation-specific);

    `data.type`:

    :   corresponding R/S data type, e.g., `integer`;

    `precision`/`scale`:

    :   (as in ODBC terminology), display width and number of decimal
        digits, respectively;

    `nullable`:

    :   whether the corresponding field may contain (DBMS) `NULL`
        values;

    plus other driver-specific information.

`dbSetDataMappings(flds, ...)`(optional):

:   defines a conversion between internal DBMS data types and R/S
    classes. We expect the default mappings (see
    Section [sec:data-mappings]) to be by far the most common ones, but
    users that need more control may specify a class generator for
    individual fields in the result set. [This topic needs further
    discussion.]

*Note: The following are convenience methods that extract information
from the result object (they may be implemented by invoking `dbGetInfo`
with appropriate arguments).*

`dbGetStatement(res, ...)`(optional):

:   returns the DBMS statement (as a character string) associated with
    the result `res`.

`dbGetRowsAffected(res, ...)`(optional):

:   returns the number of rows affected by the executed statement
    (number of records deleted, modified, extracted, etc.)

`dbHasCompleted(res, ...)`(optional):

:   returns a logical that indicates whether the operation has been
    completed (e.g., are there more records to be fetched?).

`dbGetRowCount(res, ...)`(optional):

:   returns the number of rows fetched so far.

# Data Type Mappings {#sec:data-mappings}

The data types supported by databases are different than the data types
in R and S, but the mapping between the “primitive” types is
straightforward: Any of the many fixed and varying length character
types are mapped to R/S `character`. Fixed-precision (non-IEEE) numbers
are mapped into either doubles (`numeric`) or long (`integer`). Notice
that many DBMS do not follow the so-called IEEE arithmetic, so there are
potential problems with under/overflows and loss of precision, but given
the R/S primitive types we cannot do too much but identify these
situations and warn the application (how?).

By default dates and date-time objects are mapped to character using the
appropriate `TO_CHAR` function in the DBMS (which should take care of
any locale information). Some RDBMS support the type `CURRENCY` or
`MONEY` which should be mapped to `numeric` (again with potential round
off errors). Large objects (character, binary, file, etc.) also need to
be mapped. User-defined functions may be specified to do the actual
conversion (as has been done in other inter-systems packages [^2]).

Specifying user-defined conversion functions still needs to be defined.

# Utilities {#sec:utilities}

The core DBI implementation should make available to all drivers some
common basic utilities. For instance:

`dbGetDBIVersion`:

:   returns the version of the currently attached DBI as a string.

`dbDataType(dbObj, obj, ...)`:

:   returns a string with the (approximately) appropriate data type for
    the R/S object `obj`. The DBI can implement this following the
    ANSI-92 standard, but individual drivers may want/need to extend it
    to make use of DBMS-specific types.

`make.db.names(dbObj, snames, ...)`:

:   maps R/S names (identifiers) to SQL identifiers replacing illegal
    characters (as `.`) by the legal SQL `_`.

`SQLKeywords(dbObj, ...)`:

:   returns a character vector of SQL keywords (reserved words). The
    default method returns the list of `.SQL92Keywords`, but drivers
    should update this vector with the DBMS-specific additional reserved
    words.

`isSQLKeyword(dbObj, name, ...)`:

:   for each element in the character vector `name` determine whether or
    not it is an SQL keyword, as reported by the generic function
    `SQLKeywords`. Returns a logical vector parallel to the input object
    `name`.

# Open Issues and Limitations {#sec:open-issues}

There are a number of issues and limitations that the current DBI
conscientiously does not address on the interest of simplicity. We do
list here the most important ones.

Non-SQL:

:   Is it realistic to attempt to encompass non-relational databases,
    like HDF5, Berkeley DB, etc.?

Security:

:   allowing users to specify their passwords on R/S scripts may be
    unacceptable for some applications. We need to consider alternatives
    where users could store authentication on files (perhaps similar to
    ODBC’s `odbc.ini`) with more stringent permissions.

Exceptions:

:   the exception mechanism is a bit too simple, and it does not provide
    for information when problems stem from the DBMS interface itself.
    For instance, under/overflow or loss of precision as we move numeric
    data from DBMS to the more limited primitives in R/S.

Asynchronous communication:

:   most DBMS support both synchronous and asynchronous communications,
    allowing applications to submit a query and proceed while the
    database server process the query. The application is then notified
    (or it may need to poll the server) when the query has completed.
    For large computations, this could be very useful, but the DBI would
    need to specify how to interrupt the server (if necessary) plus
    other details. Also, some DBMS require applications to use threads
    to implement asynchronous communication, something that neither R
    nor S-Plus currently addresses.

SQL scripts:

:   the DBI only defines how to execute one SQL statement at a time,
    forcing users to split SQL scripts into individual statements. We
    need a mechanism by which users can submit SQL scripts that could
    possibly generate multiple result sets; in this case we may need to
    introduce new methods to loop over multiple results (similar to
    Python’s `nextResultSet`).

BLOBS/CLOBS:

:   large objects (both character and binary) present some challenges
    both to R and S-Plus. It is becoming more common to store images,
    sounds, and other data types as binary objects in DBMS, some of
    which can be in principle quite large. The SQL-92 ANSI standard
    allows up to 2 gigabytes for some of these objects. We need to
    carefully plan how to deal with binary objects.

Transactions:

:   transaction management is not fully described.

Additional methods:

:   Do we need any additional methods? (e.g., `dbListDatabases(conn)`,
    `dbListTableIndices(conn, name)`, how do we list all available
    drivers?)

Bind variables:

:   the interface is heavily biased towards queries, as opposed to
    general purpose database development. In particular we made no
    attempt to define “bind variables”; this is a mechanism by which the
    contents of R/S objects are implicitly moved to the database during
    SQL execution. For instance, the following embedded SQL statement

          /* SQL */
          SELECT * from emp_table where emp_id = :sampleEmployee

    would take the vector `sampleEmployee` and iterate over each of its
    elements to get the result. Perhaps the DBI could at some point in
    the future implement this feature.

# Resources {#sec:resources}

The idea of a common interface to databases has been successfully
implemented in various environments, for instance:

Java’s Database Connectivity (JDBC)
([www.javasoft.com](http://www.javasoft.com/products/jdbc/index.html)).

In C through the Open Database Connectivity (ODBC)
([www.unixodbc.org](http://www.unixodbc.org/)).

Python’s Database Application Programming Interface
([www.python.org](http://www.python.org/topics/database)).

Perl’s Database Interface ([dbi.perl.org](http://dbi.perl.org)).

[^1]: A virtual class allows us to group classes that share some common
    characteristics, even if their implementations are radically
    different.

[^2]: Duncan Temple Lang has volunteered to port the data conversion
    code found in R-Java, R-Perl, and R-Python packages to the DBI

---
title: "Introduction to broom"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to broom}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE, message = FALSE)
```

broom: let's tidy up a bit
=====================

The broom package takes the messy output of built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns them into tidy tibbles.

The concept of "tidy data", [as introduced by Hadley Wickham](http://www.jstatsoft.org/v59/i10), offers a powerful framework for data manipulation and analysis. That paper makes a convincing statement of the problem this package tries to solve (emphasis mine):

> **While model inputs usually require tidy inputs, such attention to detail doesn't carry over to model outputs. Outputs such as predictions and estimated coefficients aren't always tidy. This makes it more difficult to combine results from multiple models.** For example, in R, the default representation of model coefficients is not tidy because it does not have an explicit variable that records the variable name for each estimate, they are instead recorded as row names. In R, row names must be unique, so combining coefficients from many models (e.g., from bootstrap resamples, or subgroups) requires workarounds to avoid losing important information. **This knocks you out of the flow of analysis and makes it harder to combine the results from multiple models. I'm not currently aware of any packages that resolve this problem.**

broom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three S3 methods, each of which take common objects produced by R statistical functions (`lm`, `t.test`, `nls`, etc) and convert them into a tibble. broom is particularly designed to work with Hadley's [dplyr](https://github.com/tidyverse/dplyr) package (see the [broom+dplyr](broom_and_dplyr.html) vignette for more).

broom should be distinguished from packages like [reshape2](https://CRAN.R-project.org/package=reshape2) and [tidyr](https://github.com/tidyverse/tidyr), which rearrange and reshape data frames into different forms. Those packages perform critical tasks in tidy data analysis but focus on manipulating data frames in one specific format into another. In contrast, broom is designed to take format that is *not* in a tabular data format (sometimes not anywhere close) and convert it to a tidy tibble.

Tidying model outputs is not an exact science, and it's based on a judgment of the kinds of values a data scientist typically wants out of a tidy analysis (for instance, estimates, test statistics, and p-values). You may lose some of the information in the original object that you wanted, or keep more information than you need. If you think the tidy output for a model should be changed, or if you're missing a tidying function for an S3 class that you'd like, I strongly encourage you to [open an issue](http://github.com/tidymodels/broom/issues) or a pull request.

Tidying functions
-----------------

This package provides three S3 methods that do three distinct kinds of tidying.

* `tidy`: constructs a tibble that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for `multtest` functions.
* `augment`: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.
* `glance`: construct a concise *one-row* summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.

Note that some classes may have only one or two of these methods defined.

Consider as an illustrative example a linear fit on the built-in `mtcars` dataset.

```{r lmfit}
lmfit <- lm(mpg ~ wt, mtcars)
lmfit
summary(lmfit)
```

This summary output is useful enough if you just want to read it. However, converting it to tabular data that contains all the same information, so that you can combine it with other models or do further analysis, is not trivial. You have to do `coef(summary(lmfit))` to get a matrix of coefficients, the terms are still stored in row names, and the column names are inconsistent with other packages (e.g. `Pr(>|t|)` compared to `p.value`).

Instead, you can use the `tidy` function, from the broom package, on the fit:

```{r}
library(broom)
tidy(lmfit)
```

This gives you a tabular data representation. Note that the row names have been moved into a column called `term`, and the column names are simple and consistent (and can be accessed using `$`).

Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use `augment`, which augments the original data with information from the model:

```{r}
augment(lmfit)
```

Note that each of the new columns begins with a `.` (to avoid overwriting any of the original columns).

Finally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the `glance` function:

```{r}
glance(lmfit)
```

This distinction between the `tidy`, `augment` and `glance` functions is explored in a different context in the [k-means vignette](https://www.tidymodels.org/learn/statistics/k-means/).

Other Examples
--------------

### Generalized linear and non-linear models

These functions apply equally well to the output from `glm`:

```{r glmfit}
glmfit <- glm(am ~ wt, mtcars, family = "binomial")
tidy(glmfit)
augment(glmfit)
glance(glmfit)
```

Note that the statistics computed by `glance` are different for `glm` objects than for `lm` (e.g. deviance rather than R^2):

These functions also work on other fits, such as nonlinear models (`nls`):

```{r}
nlsfit <- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))
tidy(nlsfit)
augment(nlsfit, mtcars)
glance(nlsfit)
```

### Hypothesis testing

The `tidy` function can also be applied to `htest` objects, such as those output by popular built-in functions like `t.test`, `cor.test`, and `wilcox.test`.

```{r ttest}
tt <- t.test(wt ~ am, mtcars)
tidy(tt)
```

Some cases might have fewer columns (for example, no confidence interval):

```{r}
wt <- wilcox.test(wt ~ am, mtcars)
tidy(wt)
```

Since the `tidy` output is already only one row, `glance` returns the same output:

```{r}
glance(tt)
glance(wt)
```

`augment` method is defined only for chi-squared tests,  since there is no meaningful sense, for other tests, in which a hypothesis test produces output about each initial data point.

```{r}
chit <- chisq.test(xtabs(Freq ~ Sex + Class, 
                         data = as.data.frame(Titanic)))
tidy(chit)
augment(chit)
```

Conventions
------------

In order to maintain consistency, we attempt to follow some conventions regarding the structure of returned data.

### All functions

* The output of the `tidy`, `augment` and `glance` functions is *always* a tibble.
* The output never has rownames. This ensures that you can combine it with other tidy outputs without fear of losing information (since rownames in R cannot contain duplicates).
* Some column names are kept consistent, so that they can be combined across different models and so that you know what to expect (in contrast to asking "is it `pval` or `PValue`?" every time). The examples below are not all the possible column names, nor will all tidy output contain all or even any of these columns.

### tidy functions

* Each row in a `tidy` output typically represents some well-defined concept, such as one term in a regression, one test, or one cluster/class. This meaning varies across models but is usually self-evident. The one thing each row cannot represent is a point in the initial data (for that, use the `augment` method).
* Common column names include:
    * `term`"" the term in a regression or model that is being estimated.
    * `p.value`: this spelling was chosen (over common alternatives such as `pvalue`, `PValue`, or `pval`) to be consistent with functions in R's built-in `stats` package
    * `statistic` a test statistic, usually the one used to compute the p-value. Combining these across many sub-groups is a reliable way to perform (e.g.) bootstrap hypothesis testing
    * `estimate`
    * `conf.low` the low end of a confidence interval on the `estimate`
    * `conf.high` the high end of a confidence interval on the `estimate`
    * `df` degrees of freedom

### augment functions

* `augment(model, data)` adds columns to the original data.
    * If the `data` argument is missing, `augment` attempts to reconstruct the data from the model (note that this may not always be possible, and usually won't contain columns not used in the model).
* Each row in an `augment` output matches the corresponding row in the original data.
* If the original data contained rownames, `augment` turns them into a column called `.rownames`.
* Newly added column names begin with `.` to avoid overwriting columns in the original data.
* Common column names include:
    * `.fitted`: the predicted values, on the same scale as the data.
    * `.resid`: residuals: the actual y values minus the fitted values
    * `.cluster`: cluster assignments

### glance functions

* `glance` always returns a one-row tibble.
    * The only exception is that `glance(NULL)` returns an empty tibble.
* We avoid including arguments that were *given* to the modeling function. For example, a `glm` glance output does not need to contain a field for `family`, since that is decided by the user calling `glm` rather than the modeling function itself.
* Common column names include:
    * `r.squared` the fraction of variance explained by the model
    * `adj.r.squared` $R^2$ adjusted based on the degrees of freedom
    * `sigma` the square root of the estimated variance of the residuals
---
title: "Available methods"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Available methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The following methods are currently available in `broom`:


```{R echo = FALSE, message = FALSE}
library(broom)
library(dplyr)
library(stringr)

method_df <- function(method_name) {
    m <- as.vector(methods(method_name))
    tibble::tibble(class = str_remove(m, str_c(method_name, "[.]")),
                   !!method_name := "x")
}

method_df("tidy") %>% 
    left_join(method_df("glance")) %>% 
    left_join(method_df("augment")) %>% 
    mutate_all(tidyr::replace_na, "") %>% 
    knitr::kable()
```

---
title: "Adding tidiers to broom"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Adding tidiers to broom}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Adding tidiers to broom

Generally, after this release, the broom dev team will first ask that
attempts to add tidier methods supporting a model object are first
directed to the model-owning package. An article describing best practices 
in doing so can be found on the {tidymodels} website at 
https://www.tidymodels.org/learn/develop/broom/, and we will continue
adding additional resources to that article as we develop them. Some additional
packages that supply themed tidiers are:

- Mixed model tidiers belong in [`broom.mixed`](https://github.com/bbolker/broom.mixed)
- Natural language related tidiers belong in [`tidytext`](https://github.com/juliasilge/tidytext)
- Tree tidiers belong in [`broomstick`](https://github.com/njtierney/broomstick)
- Tidiers for objects from BioConductor belong in [`biobroom`](https://bioconductor.org/packages/release/bioc/html/biobroom.html)

To aid in the process of writing new tidiers, we have provided 
[learning resources](https://www.tidymodels.org/learn/develop/broom) as well 
as lightweight dependencies to re-export tidier generics on the {tidymodels} 
website.

In the case that the maintainer is uninterested in taking on the tidier 
methods, please note this in your issue or PR.
---
title: "kmeans with dplyr and broom"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{kmeans with dplyr and broom}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Tidying k-means clustering

This vignette is now an [article](https://www.tidymodels.org/learn/statistics/k-means/) on the {tidymodels} website.
---
title: "broom and dplyr"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{broom and dplyr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# broom and dplyr

While broom is useful for summarizing the result of a single analysis in a consistent format, it is really designed for high-throughput applications, where you must combine results from multiple analyses. These could be subgroups of data, analyses using different models, bootstrap replicates, permutations, and so on. In particular, it plays well with the `nest/unnest` functions in `tidyr` and the `map` function in `purrr`. First, loading necessary packages and setting some defaults:

```{r}
library(broom)
library(tibble)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)

theme_set(theme_minimal())
```

Let's try this on a simple dataset, the built-in `Orange`. We start by coercing `Orange` to a `tibble`. This gives a nicer print method that will especially useful later on when we start working with list-columns.

```{r}
data(Orange)

Orange <- as_tibble(Orange)
Orange
```

This contains 35 observations of three variables: `Tree`, `age`, and `circumference`. `Tree` is a factor with five levels describing five trees. As might be expected, age and circumference are correlated:

```{r}
cor(Orange$age, Orange$circumference)

ggplot(Orange, aes(age, circumference, color = Tree)) +
  geom_line()
```

Suppose you want to test for correlations individually *within* each tree. You can do this with dplyr's `group_by`:

```{r, message = FALSE, warning = FALSE}
Orange %>% 
  group_by(Tree) %>%
  summarize(correlation = cor(age, circumference))
```

(Note that the correlations are much higher than the aggregated one, and furthermore we can now see it is similar across trees).

Suppose that instead of simply estimating a correlation, we want to perform a hypothesis test with `cor.test`:

```{r}
ct <- cor.test(Orange$age, Orange$circumference)
ct
```

This contains multiple values we could want in our output. Some are vectors of length 1, such as the p-value and the estimate, and some are longer, such as the confidence interval. We can get this into a nicely organized tibble using the `tidy` function:

```{r}
tidy(ct)
```

Often, we want to perform multiple tests or fit multiple models, each on a different part of the data. In this case, we recommend a `nest-map-unnest` workflow. For example, suppose we want to perform correlation tests for each different tree. We start by `nest`ing our data based on the group of interest:

```{r}
nested <- Orange %>% 
  nest(data = -Tree)
```

Then we run a correlation test for each nested tibble using `purrr::map`:

```{r}
nested %>% 
  mutate(test = map(data, ~ cor.test(.x$age, .x$circumference)))
```

This results in a list-column of S3 objects. We want to tidy each of the objects, which we can also do with `map`.

```{r}
nested %>% 
  mutate(
    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col
    tidied = map(test, tidy)
  ) 
```

Finally, we want to unnest the tidied data frames so we can see the results in a flat tibble. All together, this looks like:

```{r}
Orange %>% 
  nest(data = -Tree) %>% 
  mutate(
    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col
    tidied = map(test, tidy)
  ) %>% 
  unnest(tidied)
```

This workflow becomes even more useful when applied to regressions. Untidy output for a regression looks like:

```{r}
lm_fit <- lm(age ~ circumference, data = Orange)
summary(lm_fit)
```

where we tidy these results, we get multiple rows of output for each model:

```{r}
tidy(lm_fit)
```

Now we can handle multiple regressions at once using exactly the same workflow as before:

```{r}
Orange %>%
  nest(data = -Tree) %>% 
  mutate(
    fit = map(data, ~ lm(age ~ circumference, data = .x)),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied)
```

You can just as easily use multiple predictors in the regressions, as shown here on the `mtcars` dataset. We nest the data into automatic and manual cars (the `am` column), then perform the regression within each nested tibble.

```{r}
data(mtcars)
mtcars <- as_tibble(mtcars)  # to play nicely with list-cols
mtcars

mtcars %>%
  nest(data = -am) %>% 
  mutate(
    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),  # S3 list-col
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied)
```

What if you want not just the `tidy` output, but the `augment` and `glance` outputs as well, while still performing each regression only once? Since we're using list-columns, we can just fit the model once and use multiple list-columns to store the tidied, glanced and augmented outputs.

```{r}
regressions <- mtcars %>%
  nest(data = -am) %>% 
  mutate(
    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    augmented = map(fit, augment)
  )

regressions %>% 
  unnest(tidied)

regressions %>% 
  unnest(glanced)

regressions %>% 
  unnest(augmented)
```

By combining the estimates and p-values across all groups into the same tidy data frame (instead of a list of output model objects), a new class of analyses and visualizations becomes straightforward. This includes

- Sorting by p-value or estimate to find the most significant terms across all tests
- P-value histograms
- Volcano plots comparing p-values to effect size estimates

In each of these cases, we can easily filter, facet, or distinguish based on the `term` column. In short, this makes the tools of tidy data analysis available for the *results* of data analysis and models, not just the inputs.
---
title: "Tidy bootstrapping"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tidy bootstrapping}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

# Tidy bootstrapping

This vignette is now an [article](https://www.tidymodels.org/learn/statistics/bootstrap/) on the {tidymodels} website.
---
title: "URL Validation"
author: "Jim Hester"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{URL Validation}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

Consider the task of correctly [validating a URL](https://mathiasbynens.be/demo/url-regex).
From that page two conclusions can be made.

1. Validating URLs require complex regular expressions.
2. Creating a correct regular expression is hard! (only 1 out of 13 regexs were valid for all cases).

Because of this one may be tempted to simply copy the best regex you can find ([gist](https://gist.github.com/dperini/729294)).

The problem with this is that while you can copy it now, what happens later when you find a case that is not handled correctly?  Can you correctly interpret and modify this?
```{r url_parsing_stock, eval=F}
"^(?:(?:http(?:s)?|ftp)://)(?:\\S+(?::(?:\\S)*)?@)?(?:(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0-9\u00a1-\uffff])+)(?:\\.(?:[a-z0-9\u00a1-\uffff](?:-)*)*(?:[a-z0-9\u00a1-\uffff])+)*(?:\\.(?:[a-z0-9\u00a1-\uffff]){2,})(?::(?:\\d){2,5})?(?:/(?:\\S)*)?$"
```

However if you re-create the regex with `rex` it is much easier to understand and modify later if needed.
```{r url_parsing_url}
library(rex)
library(magrittr)

valid_chars <- rex(except_some_of(".", "/", " ", "-"))

re <- rex(
  start,

  # protocol identifier (optional) + //
  group(list("http", maybe("s")) %or% "ftp", "://"),

  # user:pass authentication (optional)
  maybe(non_spaces,
    maybe(":", zero_or_more(non_space)),
    "@"),

  #host name
  group(zero_or_more(valid_chars, zero_or_more("-")), one_or_more(valid_chars)),

  #domain name
  zero_or_more(".", zero_or_more(valid_chars, zero_or_more("-")), one_or_more(valid_chars)),

  #TLD identifier
  group(".", valid_chars %>% at_least(2)),

  # server port number (optional)
  maybe(":", digit %>% between(2, 5)),

  # resource path (optional)
  maybe("/", non_space %>% zero_or_more()),

  end
)
```

We can then validate that it correctly identifies both good and bad URLs. (_IP address validation removed_)

```{r url_parsing_validate}
good <- c("http://foo.com/blah_blah",
  "http://foo.com/blah_blah/",
  "http://foo.com/blah_blah_(wikipedia)",
  "http://foo.com/blah_blah_(wikipedia)_(again)",
  "http://www.example.com/wpstyle/?p=364",
  "https://www.example.com/foo/?bar=baz&inga=42&quux",
  "http://✪df.ws/123",
  "http://userid:password@example.com:8080",
  "http://userid:password@example.com:8080/",
  "http://userid@example.com",
  "http://userid@example.com/",
  "http://userid@example.com:8080",
  "http://userid@example.com:8080/",
  "http://userid:password@example.com",
  "http://userid:password@example.com/",
  "http://➡.ws/䨹",
  "http://⌘.ws",
  "http://⌘.ws/",
  "http://foo.com/blah_(wikipedia)#cite-1",
  "http://foo.com/blah_(wikipedia)_blah#cite-1",
  "http://foo.com/unicode_(✪)_in_parens",
  "http://foo.com/(something)?after=parens",
  "http://☺.damowmow.com/",
  "http://code.google.com/events/#&product=browser",
  "http://j.mp",
  "ftp://foo.bar/baz",
  "http://foo.bar/?q=Test%20URL-encoded%20stuff",
  "http://مثال.إختبار",
  "http://例子.测试",
  "http://-.~_!$&'()*+,;=:%40:80%2f::::::@example.com",
  "http://1337.net",
  "http://a.b-c.de",
  "http://223.255.255.254")

bad <- c(
  "http://",
  "http://.",
  "http://..",
  "http://../",
  "http://?",
  "http://??",
  "http://??/",
  "http://#",
  "http://##",
  "http://##/",
  "http://foo.bar?q=Spaces should be encoded",
  "//",
  "//a",
  "///a",
  "///",
  "http:///a",
  "foo.com",
  "rdar://1234",
  "h://test",
  "http:// shouldfail.com",
  ":// should fail",
  "http://foo.bar/foo(bar)baz quux",
  "ftps://foo.bar/",
  "http://-error-.invalid/",
  "http://-a.b.co",
  "http://a.b-.co",
  "http://0.0.0.0",
  "http://3628126748",
  "http://.www.foo.bar/",
  "http://www.foo.bar./",
  "http://.www.foo.bar./")

all(grepl(re, good) == TRUE)

all(grepl(re, bad) == FALSE)
```

You can now see the power and expressiveness of building regular expressions with `rex`!
---
title: "Server Log Parsing"
author: "Jim Hester"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Server Log Parsing}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

Parsing server log files is a common task in server administration.
[1](http://link.springer.com/article/10.1007/BF03325089),[2](http://stackoverflow.com/search?q=%22Apache+log%22)
Historically R would not be well suited to this and it would be better
performed using a scripting language such as perl. Rex, however, makes this
easy to do and allows you to perform both the data cleaning and analysis in R!

Common server logs consist of space separated fields.

> 198.214.42.14 - - [21/Jul/1995:14:31:46 -0400] "GET /images/ HTTP/1.0" 200 17688

> lahal.ksc.nasa.gov - - [24/Jul/1995:12:42:40 -0400] "GET /images/USA-logosmall.gif HTTP/1.0" 200 234

The logs used in this vignette come from two months of all HTTP requests
to the NASA Kennedy Space Center WWW server in Florida and are freely available
for use. [3](https://web.archive.org/web/20181003084945/http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)

```{r include = FALSE}
library(rex)
library(dplyr)
library(knitr)
library(ggplot2)
library(magrittr)
```

```{r show.warnings=FALSE}
parsed <- scan("NASA.txt", what = "character", sep = "\n") %>%
  re_matches(
    rex(

      # Get the time of the request
      "[",
        capture(name = "time",
          except_any_of("]")
        ),
      "]",

      space, double_quote, "GET", space,

      # Get the filetype of the request if requesting a file
      maybe(
        non_spaces, ".",
        capture(name = "filetype",
          except_some_of(space, ".", "?", double_quote)
        )
      )
    )
  ) %>%
  mutate(filetype = tolower(filetype),
         time = as.POSIXct(time, format="%d/%b/%Y:%H:%M:%S %z"))
```

This gives us a nicely formatted data frame of the time and filetypes of the requests.
```{r echo = FALSE}
kable(head(parsed, n = 10))
```

We can also easily generate a histogram of the filetypes, or a plot of requests over time.
```{r FALSE, fig.show='hold', warning = FALSE, message = FALSE}
ggplot(na.omit(parsed)) + stat_count(aes(x=filetype))
ggplot(na.omit(parsed)) + geom_histogram(aes(x=time)) + ggtitle("Requests over time")
```
---
title: "The tidy tools manifesto"
author: "Hadley Wickham"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The tidy tools manifesto}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document lays out the consistent principles that unify the packages in the tidyverse. The goal of these principles is to provide a uniform interface so that tidyverse packages work together naturally, and once you've mastered one, you have a head start on mastering the others.

This is my first attempt at writing down these principles. That means that this manifesto is both aspirational and likely to change heavily in the future. Currently no packages precisely meet the design goals, and while the underlying ideas are stable, I expect their expression in prose will change substantially as I struggle to make explicit my process and thinking.

There are many other excellent packages that are not part of the tidyverse, because they are designed with a different set of underlying principles. This doesn't make them better or worse, just different. In other words, the complement to the tidyverse is not the **messy**verse, but many other universes of interrelated packages.

There are four basic principles to a tidy API:

1.  Reuse existing data structures.

1.  Compose simple functions with the pipe.

1.  Embrace functional programming.

1.  Design for humans.

## Reuse existing data structures

Where possible, re-use existing data structures, rather than creating custom data structures for your own package. Generally, I think it's better to prefer common existing data structures over custom data structures, even if slightly ill-fitting.

Many R packages (e.g. ggplot2, dplyr, tidyr) work with rectangular datasets made up of observations and variables. If this is true for your package, work with data in either a data frame or [tibble](https://github.com/hadley/tibble/). Assume the data is tidy, with  variables in the columns, and observations in the rows (see [_Tidy Data_](https://www.jstatsoft.org/article/view/v059i10) for more details).

Some packages work at a lower level, focussing on a single type of variable. For example, __stringr__ for strings, __lubridate__ for date/times, and __forcats__ for factors. Generally prefer existing base R vector types, but when this is not possible, create your own using an S3 class built on top of an atomic vector or list. 

If you need "non-standard scoping", where you refer to variables inside a data frame as if they are in the global environment, prefer tidy evaluation over non-standard evaluation. See the examples in [Tidy evaluation](https://tidyeval.tidyverse.org) and [the metaprogramming chapters in Advanced R](https://adv-r.hadley.nz/meta-big-picture.html) for more details.

## Compose simple functions with the pipe

> No matter how complex and polished the individual operations are, 
> it is often the quality of the glue that most directly determines 
> the power of the system. 
>
> --- Hal Abelson

A powerful strategy for solving complex problems is to combine many simple pieces. Each piece should be easily understood in isolation, and have a standard way to combine with other pieces. In R, this strategy plays out by composing single functions with the pipe. The pipe, `%>%`, is a common composition tool that works across all packages.

Some things to bear in mind when writing functions:

*   Strive to keep functions as simple as possible (but no simpler!).
    Generally, each function should do one thing well, and you should be 
    able to describe the purpose of the function in one sentence. 

*   Avoid mixing side-effects with transformations. Ensure each function
    either returns an object, or has a side-effect. Don't do both.
    (This is a slight simplification: functions called primarily for 
    their side-effects should return the primary input invisibly so that
    they can still be combined in a pipeline.)

*   Function names should be verbs. The exception is when many functions 
    use the same verb (typically something like "modify", or "add", or
    "compute"). In that case, avoid duplicating the common verb, and instead
    focus on the noun. A good example of this is ggplot2: almost every
    function adds something to an existing plot.

An advantage of a pipeable API is that it is not compulsory: if you do not like using the pipe, you can compose functions in whatever way you prepare. Compare this to an operator-overloading approach (such as the `+` in ggplot2), or an object-composition approach (such as the `...` approach in httr). 

## Embrace functional programming

R is a functional programming language; embrace it, don't fight it. If you're familiar with an object-oriented language like Python or C#, this is going to take some adjustment. But in the long run you will be much better off working with the language, rather than fighting it.

Generally, this means you should favour:

*   Immutable objects and copy-on-modify semantics. This makes your code
    easier to reason about.

*   The generic functions provided by S3 and S4. These work very naturally
    inside a pipe. If you do need mutable state, try to make it an internal
    implementation detail, rather than exposing it to the user.

*   Tools that abstract over for-loops, like the apply family of functions
    or the map functions in purrr. 

## Design for humans

> Programs must be written for people to read, and only incidentally
> for machines to execute.
> 
> --- Hal Abelson

Design your API primarily so that it is easy to use by humans. Computer efficiency is a secondary concern because the bottleneck in most data analysis is thinking time, not computing time.

*   Invest time in naming your functions. Evocative function names 
    make your API easier to use and remember.

*   Favour explicit, lengthy names, over short, implicit, names. 
    Save the shortest names for the most important operations.

*   Think about how autocomplete can also make an API that's easy to 
    write. Make sure that function families are identified by a 
    common prefix, not a common suffix. This makes autocomplete more
    helpful, as you can jog your memory with the prompts. For smaller
    packages, this may mean that every function has a common prefix 
    (e.g. stringr, xml2, rvest).
---
title: 'Welcome to the Tidyverse'
output:
  rmarkdown::html_vignette:
    keep_md: TRUE
tags:
- data science
- R
author: Hadley Wickham, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D'Agostino McGowan, Romain François, Garrett Grolemund, Alex Hayes, Lionel Henry, Jim Hester, Max Kuhn, Thomas Lin Pedersen, Evan Miller, Stephan Milton Bache, Kirill Müller, Jeroen Ooms, David Robinson, Dana Paige Seidel, Vitalie Spinu, Kohske Takahashi, Davis Vaughan, Claus Wilke, Kara Woo, Hiroaki Yutani
authors:
- name: Hadley Wickham
  orcid: 0000-0003-4757-117X
  affiliation: 1
- name: Mara Averick
  orcid: 0000-0001-9659-6192
  affiliation: 1
- name: Jennifer Bryan
  orcid: 0000-0002-6983-2759
  affiliation: 1
- name: Winston Chang
  orcid: 0000-0002-1576-2126
  affiliation: 1
- name: Lucy D'Agostino McGowan
  orcid: 0000-0001-7297-9359
  affiliation: 8
- name: Romain François
  orcid: 0000-0002-2444-4226
  affiliation: 1
- name: Garrett Grolemund
  orcid: 0000-0002-7765-6011
  affiliation: 1
- name: Alex Hayes
  orcid: 0000-0002-4985-5160
  affiliation: 12
- name: Lionel Henry
  orcid: 0000-0002-8143-5908
  affiliation: 1
- name: Jim Hester
  orcid: 0000-0002-2739-7082
  affiliation: 1
- name: Max Kuhn
  orcid: 0000-0003-2402-136X
  affiliation: 1
- name: Thomas Lin Pedersen
  orcid: 0000-0002-5147-4711
  affiliation: 1
- name: Evan Miller
  affiliation: 13
- name: Stephan Milton Bache
  affiliation: 3
- name: Kirill Müller
  orcid: 0000-0002-1416-3412
  affiliation: 2
- name: Jeroen Ooms
  affiliation: 14
- name: David Robinson
  affiliation: 5
- name: Dana Paige Seidel
  orcid: 0000-0002-9088-5767
  affiliation: 10
- name: Vitalie Spinu
  orcid: 0000-0002-2138-3413
  affiliation: 4
- name: Kohske Takahashi
  orcid: 0000-0001-6076-4828
  affiliation: 9
- name: Davis Vaughan
  orcid: 0000-0003-4777-038X
  affiliation: 1
- name: Claus Wilke
  orcid: 0000-0002-7470-9261
  affiliation: 6
- name: Kara Woo
  orcid: 0000-0002-5125-4188
  affiliation: 7
- name: Hiroaki Yutani
  orcid: 0000-0002-3385-7233
  affiliation: 11
affiliations:
 - name: RStudio
   index: 1
 - name: cynkra
   index: 2
 - name: Redbubble
   index: 3
 - name: Erasmus University Rotterdam
   index: 4
 - name: Flatiron Health
   index: 5
 - name: Department of Integrative Biology, The University of Texas at Austin
   index: 6
 - name: Sage Bionetworks
   index: 7
 - name: Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health
   index: 8
 - name: Chukyo University, Japan
   index: 9
 - name: Department of Environmental Science, Policy, & Management, University of California, Berkeley
   index: 10
 - name: LINE Corporation
   index: 11
 - name: University of Wisconsin, Madison
   index: 12
 - name: None
   index: 13
 - name: University of California, Berkeley
   index: 14
date: 19 November 2019
bibliography: paper.bib
vignette: >
  %\VignetteIndexEntry{Welcome to the tidyverse}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", width = 68)
options(width = 68, cli.unicode = FALSE, cli.width = 68)
```

# Summary

![](tidyverse-logo.png){ width=120px }

At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. 

The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command.

There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor [@Bioconductor; @Huber2015], which provides an ecosystem of packages that support the analysis of high-throughput genomic data. The tidyverse has similar goals to R itself, but any comparison to the R Project [@R-core] is fundamentally challenging as the tidyverse is written in R, and relies on R for its infrastructure; there is no tidyverse without R! That said, the biggest difference is in priorities: base R is highly focussed on stability, whereas the tidyverse will make breaking changes in the search for better interfaces. Another closely related project is data.table [@R-datatable], which provides tools roughly equivalent to the combination of dplyr, tidyr, tibble, and readr. data.table prioritises concision and performance.

This paper describes the tidyverse package, the components of the tidyverse, and some of the underlying design principles. This is a lot of ground to cover in a brief paper, so we focus on a 50,000-foot view showing how all the pieces fit together with copious links to more detailed resources. 

# Tidyverse package

The tidyverse is a collection of packages that can easily be installed with a single "meta"-package, which is called "tidyverse". This provides a convenient way of downloading and installing all tidyverse packages with a single R command:

```{r, eval = FALSE}
install.packages("tidyverse")
```

The core tidyverse includes the packages that you're likely to use in everyday data analyses, and these are attached when you attach the tidyverse package:

```{r}
library(tidyverse)
```

This is a convenient shortcut for attaching the core packages, produces a short report telling you which package versions you're using, and succinctly informs you of any conflicts with previously loaded packages. As of tidyverse version 1.2.0, the core packages include dplyr [@R-dplyr], forcats [@R-forcats], ggplot2 [@ggplot2], purrr [@R-purrr], readr [@R-readr], stringr [@R-stringr], tibble [@R-tibble], and tidyr [@R-tidyr]. 

Non-core packages are installed with `install.packages("tidyverse")`, but are not attached by `library(tidyverse)`. They play more specialised roles, so will be attached by the analyst as needed. The non-core packages are: blob [@R-blob], feather [@R-feather], jsonlite [@R-jsonlite], glue [@R-glue], googledrive [@R-googledrive], haven [@R-haven], hms [@R-hms], lubridate [@R-lubridate], magrittr [@R-magrittr], modelr [@R-modelr], readxl [@R-readxl], reprex [@R-reprex], rvest [@R-rvest], and xml2 [@R-xml2].

The tidyverse package is designed with an eye for teaching: `install.packages("tidyverse")` gets you a "batteries-included" set of 87 packages (at time of writing). This large set of dependencies means that it is not appropriate to use the tidyverse package within another package; instead, we recommend that package authors import only the specific packages that they use.

# Components

How do the component packages of the tidyverse fit together? We use the model of data science tools from "R for Data Science" [@r4ds]:

![](data-science.png){ width=80% }

Every analysis starts with data __import__: if you can't get your data into R, you can't do data science on it! Data import takes data stored in a file, database, or behind a web API, and reads it into a data frame in R. Data import is supported by the core [readr](https://readr.tidyverse.org/) [@R-readr] package for tabular files (like csv, tsv, and fwf). 

Additional non-core packages, such as [readxl](https://readxl.tidyverse.org) [@R-readxl], [haven](https://haven.tidyverse.org) [@R-haven], [googledrive](https://googledrive.tidyverse.org/) [@R-googledrive], and [rvest](https://rvest.tidyverse.org/) [@R-rvest], make it possible to import data stored in other common formats or directly from the web.

Next, we recommend that you __tidy__ your data, getting it into a consistent form that makes the rest of the analysis easier. Most functions in the tidyverse work with tidy data [@tidy-data], where every column is a variable, every row is an observation, and every cell contains a single value. If your data is not already in this form (almost always!), the core [tidyr](https://tidyr.tidyverse.org/) [@R-tidyr] package provides tools to tidy it up.

```{r, include = FALSE}
# Ensure pkgdown links to the right packages.
library(dplyr)
```

Data __transformation__ is supported by the core [dplyr](https://dplyr.tidyverse.org/) [@R-dplyr] package. dplyr provides verbs that work with whole data frames, such as `mutate()` to create new variables, `filter()` to find observations matching given criteria, and `left_join()` and friends to combine multiple tables. dplyr is paired with packages that provide tools for specific column types: 

* [stringr](https://stringr.tidyverse.org) for strings.
* [forcats](https://forcats.tidyverse.org) for factors, R's categorical data 
  type.
* [lubridate](https://lubridate.tidyverse.org) [@R-lubridate] for dates and 
  date-times.
* [hms](https://hms.tidyverse.org/) [@R-hms] for clock times.

There are two main tools for understanding data: __visualisation__ and __modelling__. The tidyverse provides the [ggplot2](https://ggplot2.tidyverse.org/) [@ggplot2] package for visualisation. ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics [@wilkinson2005].

You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. Modelling is outside the scope of this paper, but is part of the closely affiliated [tidymodels](https://github.com/tidymodels) [@R-tidymodels] project, which shares interface design and data structures with the tidyverse.

Finally, you'll need to communicate your results to someone else. __Communication__ is one of the most important parts of data science, but is not included within tidyverse. Instead, we expect people will use other R packages, like [rmarkdown](https://rmarkdown.rstudio.com/) [@R-rmarkdown] and [shiny](https://shiny.rstudio.com/) [@R-shiny], which support dozens of static and dynamic output formats.

Surrounding all these tools is __programming__. Programming is a cross-cutting tool that you use in every part of a data science project. Programming tools in the tidyverse include:

* [purrr](https://purrr.tidyverse.org/) [@R-purrr], which enhances R’s
  functional programming toolkit.

* [tibble](https://tibble.tidyverse.org/) [@R-tibble], which provides
  a modern re-imagining of the venerable data frame, keeping what time has
  proven to be effective, and throwing out what it has not. 

* [reprex](https://reprex.tidyverse.org/) [@R-reprex], which helps 
  programmers get help when they get stuck by easing the creation of 
  reproducible examples.

* [magrittr](https://magrittr.tidyverse.org) [@R-magrittr], which provides 
  the pipe operator, `%>%`, used throughout the tidyverse. The pipe is a tool 
  for function composition, making it easier to solve large problems by breaking 
  them into small pieces.

# Design principles

We are still working to explicitly describe the unifying principles that make the tidyverse consistent, but you can read our latest thoughts at <https://design.tidyverse.org/>. There is one particularly important principle that we want to call out here: the tidyverse is fundamentally __human centred__. That is, the tidyverse is designed to support the activities of a human data analyst, so to be effective tool builders, we must explicitly recognise and acknowledge the strengths and weaknesses of human cognition.

This is particularly important for R, because it’s a language that’s used primarily by non-programmers, and we want to make it as easy as possible for first-time and end-user programmers to learn the tidyverse. We believe deeply in the motivations that lead to the creation of S: "to turn ideas into software, quickly and faithfully" [@programming-with-data]. This means that we spend a lot of time thinking about interface design, and have recently started experimenting with [surveys](https://github.com/hadley/table-shapes) to help guide interface choices. 

Similarly, the tidyverse is not just the collection of packages --- it is also the community of people who use them. We want the tidyverse to be a diverse, inclusive, and welcoming community. We are still developing our skills in this area, but our existing approaches include active use of Twitter to [solicit feedback](https://twitter.com/hadleywickham/status/948722811232751617), announce updates, and generally listen to the community. We also keep users apprised of major upcoming changes through the [tidyverse blog](https://www.tidyverse.org/articles/), run [developer days](https://www.tidyverse.org/articles/2018/08/tidyverse-developer-day/), and support lively discussions on [RStudio community](https://community.rstudio.com).

# Acknowledgments

The tidyverse would not be possible without the immense work of the [R-core team](https://www.r-project.org/contributors.html) who maintain the R language and we are deeply indebted to them. We are also grateful for the financial support of [RStudio, Inc](https://www.rstudio.com/).

# References
---
title: "Getting Started with Accessing Model Information"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{insight}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
if (!requireNamespace("lme4", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}
```

When fitting any statistical model, there are many useful pieces of information that are simultaneously calculated and stored beyond coefficient estimates and general model fit statistics. Although there exist some generic functions to obtain model information and data, many package-specific modeling functions do not provide such methods to allow users to access such valuable information. 

**insight** is an R-package that fills this important gap by providing a suite of functions to support almost any model. The goal of **insight**, then, is to provide tools to provide *easy*, *intuitive*, and *consistent* access to information contained in model objects. These tools aid applied research in virtually any field who fit, diagnose, and present statistical models by streamlining access to every aspect of many model objects via consistent syntax and output.

Built with non-programmers in mind, **insight** offers a broad toolbox for making model and data information easily accessible. While **insight** offers many useful functions for working with and understanding model objects (discussed below), we suggest users start with `model_info()`, as this function provides a clean and consistent overview of model objects (*e.g.*, functional form of the model, the model family, link function, number of observations, variables included in the specification, etc.). With a clear understanding of the model introduced, users are able to adapt other functions for more nuanced exploration of and interaction with virtually any model object.

## Overview of Core Functions

A statistical model is an object describing the relationship between variables. Although there are a lot of *different types* of models, each with their specificities, most of them also share some *common components*. The goal of **insight** is to help you retrieve these components.

The `get_*` prefix extracts *values* (or *data*) associated with model-specific objects (e.g., parameters or variables), while the `find_*` prefix *lists* model-specific objects (e.g., priors or predictors). These are powerful families of functions allowing for great flexibility in use, whether at a high, descriptive level (`find_*`) or narrower level of statistical inspection and reporting (`get_*`). We point users to the package documentation or the complementary package website, https://easystats.github.io/insight/, for a detailed list of the arguments associated with each function as well as the returned values from each function.

```{r out.width="100%", echo=FALSE}
knitr::include_graphics("insight_design_1.png", dpi = 72)
```

## Definition of Model Components

The functions from **insight** address different components of a model. In an effort to avoid confusion about specific "targets" of each function, in this section we provide a short explanation of **insight**'s definitions of regression model components.

### Data

The dataset used to fit the model.

### Parameters

Values estimated or learned from data that capture the relationship between variables. In regression models, these are usually referred to as *coefficients*.

### Response and Predictors

* **response**: the outcome or response variable (dependent variable) of a regression model.
* **predictor**: independent variables of (the _fixed_ part of) a regression model. For mixed models, variables that are only in the _random effects_ part (i.e. grouping factors) of the model are not returned as predictors by default. However, these can be included using additional arguments in the function call, treating predictors are "unique". As such, if a variable appears as a fixed effect and a random slope, it is treated as one (the same) predictor.

```{r out.width="65%", echo=FALSE}
knitr::include_graphics("figure3a.png", dpi = 72)
```

### Variables

Any unique variable names that appear in a regression model, e.g., response variable, predictors or random effects. A "variable" only relates to the unique occurrence of a term, or the term name. For instance, the expression `x + poly(x, 2)` has only the variable `x`.

```{r out.width="80%", echo=FALSE}
knitr::include_graphics("figure3b.png", dpi = 72)
```

### Terms

Terms themselves consist of variable and factor names separated by operators, or involve arithmetic expressions. For instance, the expression `x + poly(x, 2)` has _one_ variable `x`, but _two_ terms `x` and `poly(x, 2)`.

```{r out.width="80%", echo=FALSE}
knitr::include_graphics("figure3c.png", dpi = 72)
```

### Random Effects

* **random slopes**: variables that are specified as random slopes in a mixed effects model.
* **random or grouping factors**: variables that are specified as grouping variables in a mixed effects model.

```{r out.width="65%", echo=FALSE}
knitr::include_graphics("figure3d.png", dpi = 72)
```


## Examples

*Aren't the predictors, terms, and parameters the same thing?*

In some cases, yes. But not in all cases, and sometimes it is useful to have the "bare" variable names (terms), but sometimes it is also useful to have the information about a possible transformation of variables. That is the main reason for having functions that cover similar aspects of a model object (like `find_terms()` and `find_predictors()` or `find_variables()`).

Here are some examples that demonstrate the differences of each function:

```{r echo=TRUE,message=FALSE,warning=FALSE}
library(insight)
library(lme4)
data(sleepstudy)
sleepstudy$mygrp <- sample(1:5, size = 180, replace = TRUE)
sleepstudy$mysubgrp <- NA
sleepstudy$Weeks <- sleepstudy$Days / 7
sleepstudy$cat <- as.factor(sample(letters[1:4], nrow(sleepstudy), replace = TRUE))

for (i in 1:5) {
  filter_group <- sleepstudy$mygrp == i
  sleepstudy$mysubgrp[filter_group] <-
    sample(1:30, size = sum(filter_group), replace = TRUE)
}

model <- lmer(
  Reaction ~ Days + I(Days^2) + log1p(Weeks) + cat +
    (1 | mygrp / mysubgrp) + 
    (1 + Days | Subject),
  data = sleepstudy
)
```


```{r echo=TRUE,message=FALSE,warning=FALSE}
# find the response variable
find_response(model)

# find all predictors, fixed part by default
find_predictors(model)

# find random effects, grouping factors only
find_random(model)

# find random slopes
find_random_slopes(model)

# find all predictors, including random effects
find_predictors(model, effects = "all", component = "all")

# find all terms, including response and random effects
# this is essentially the same as the previous example plus response
find_terms(model)

# find all variables, i.e. also quadratic or log-transformed predictors
find_variables(model)
```

Finally, there is `find_parameters()`. Parameters are also known as *coefficients*, and `find_parameters()` does exactly that: returns the model coefficients.

```{r echo=TRUE,message=FALSE,warning=FALSE}
# find model parameters, i.e. coefficients
find_parameters(model)
```

## Examples of Use Cases in R

We now would like to provide examples of use cases of the **insight** package. These examples probably do not cover typical real-world problems, but serve as illustration of the core idea of this package: The unified interface to access model information. **insight** should help both users and package developers in order to reduce the hassle with the many exceptions from various modelling packages when accessing model information.

### Making Predictions at Specific Values of a Term of Interest

Say, the goal is to make predictions for a certain term, holding remaining co-variates constant. This is  achieved by calling `predict()` and feeding the `newdata`-argument with the values of the term of interest as well as the "constant" values for remaining co-variates. The functions `get_data()` and `find_predictors()` are used to get this information, which then can be used in the call to `predict()`.

In this example, we fit a simple linear model, but it could be replaced by (m)any other models, so this approach is "universal" and applies to many different model objects.

``` r
library(insight)
m <- lm(
  Sepal.Length ~ Species + Petal.Width + Sepal.Width, 
  data = iris
)

dat <- get_data(m)
pred <- find_predictors(m, flatten = TRUE)

l <- lapply(pred, function(x) {
  if (is.numeric(dat[[x]]))
    mean(dat[[x]])
  else
    unique(dat[[x]])
})

names(l) <- pred
l <- as.data.frame(l)

cbind(l, predictions = predict(m, newdata = l))
#>      Species Petal.Width Sepal.Width predictions
#> 1     setosa    1.199333    3.057333    5.101427
#> 2 versicolor    1.199333    3.057333    6.089557
#> 3  virginica    1.199333    3.057333    6.339015
```

### Printing Model Coefficients

The next example should emphasize the possibilities to generalize functions to many different model objects using **insight**. The aim is simply to print coefficients in a complete, human readable sentence.

The first approach uses the functions that are available for some, but obviously not for all models, to access the information about model coefficients.

``` r
print_params <- function(model){
  paste0(
    "My parameters are ",
    paste0(row.names(summary(model)$coefficients),  collapse = ", "),
    ", thank you for your attention!"
  )
}

m1 <- lm(Sepal.Length ~ Petal.Width, data = iris)
print_params(m1)
#> [1] "My parameters are (Intercept), Petal.Width, thank you for your attention!"

# obviously, something is missing in the output
m2 <- mgcv::gam(Sepal.Length ~ Petal.Width + s(Petal.Length), data = iris)
print_params(m2)
#> [1] "My parameters are , thank you for your attention!"
```

As we can see, the function fails for *gam*-models. As the access to models depends on the type of the model in the R ecosystem, we would need to create specific functions for all models types. With **insight**, users can write a function without having to worry about the model type.

``` r
print_params <- function(model){
  paste0(
    "My parameters are ",
    paste0(insight::find_parameters(model, flatten = TRUE),  collapse = ", "),
    ", thank you for your attention!"
  )
}

m1 <- lm(Sepal.Length ~ Petal.Width, data = iris)
print_params(m1)
#> [1] "My parameters are (Intercept), Petal.Width, thank you for your attention!"

m2 <- mgcv::gam(Sepal.Length ~ Petal.Width + s(Petal.Length), data = iris)
print_params(m2)
#> [1] "My parameters are (Intercept), Petal.Width, s(Petal.Length), thank you for your attention!"
```

## Examples of Use Cases in R packages

**insight** is already used by different packages to solve problems that typically occur when the users' inputs are different model objects of varying complexity.

For example, [**ggeffects**](https://strengejacke.github.io/ggeffects), a package that computes and visualizes marginal effects of regression models, requires extraction of the data (`get_data()`) that was used to fit the models, and also the retrieval all model predictors (`find_predictors()`) to decide which covariates are held constant when computing marginal effects. All of this information is required in order to create a data frame for `predict(newdata=<data frame>)`. Furthermore, the models' link-functions (`link_function()`) resp. link-inverse-functions (`link_inverse()`) are required to obtain predictors at the model's response scale.

The [**sjPlot**-package](https://strengejacke.github.io/sjPlot/) creates plots or summary tables from regression models, and uses **insight**-functions to get model-information (`model_info()` or `find_response()`), which is used to build the components of the final plot or table. This information helps, for example, in labeling table columns by providing information on the effect type (odds ratio, incidence rate ratio, etc.) or the different model components, which split plots and tables into the "conditional" and "zero-inflated" parts of a model, in the cases of models with zero-inflation.

[**bayestestR**](https://easystats.github.io/bayestestR/) mainly relies on `get_priors()` and `get_parameters()` to retrieve the necessary information to compute various indices or statistics of Bayesian models (like HDI, Credible Interval, MCSE, effective sample size, Bayes factors, etc.). The advantage of `get_parameters()` in this context is that regardless of the number of parameters the posterior distribution has, the necessary data can be easily accessed from the model objects. There is no need to write original, complicated code or regular expressions.

A last example is the [**performance**-package](https://easystats.github.io/performance/), which provides functions for computing measures to assess model quality. Many of these indices (e.g. check for overdispersion or zero-inflation, predictive accuracy, logloss, RMSE, etc.) require the number of observations (`n_obs()`) or the data from the response-variable (`get_response()`). Again, in this context, functions from **insight** are helpful, because they offer a unified access to this information.
---
title: "Non-standard evaluation"
author: "Hadley Wickham"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Non-standard evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(lazyeval)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

This document describes lazyeval, a package that provides principled tools to perform non-standard evaluation (NSE) in R. You should read this vignette if you want to program with packages like dplyr and ggplot2[^1], or you want a principled way of working with delayed expressions in your own package. As the name suggests, non-standard evaluation breaks away from the standard evaluation (SE) rules in order to do something special. There are three common uses of NSE:

1.  __Labelling__ enhances plots and tables by using the expressions
    supplied to a function, rather than their values. For example, note the
    axis labels in this plot:

    ```{r, fig.width = 4, fig.height = 2.5}
    par(mar = c(4.5, 4.5, 1, 0.5))
    grid <- seq(0, 2 * pi, length = 100)
    plot(grid, sin(grid), type = "l")
    ```

1.  __Non-standard scoping__ looks for objects in places other than the current
    environment. For example, base R has `with()`, `subset()`, and `transform()` 
    that look for objects in a data frame (or list) before the current 
    environment:

    ```{r}
    df <- data.frame(x = c(1, 5, 4, 2, 3), y = c(2, 1, 5, 4, 3))
    
    with(df, mean(x))
    subset(df, x == y)
    transform(df, z = x + y)
    ```

1.  __Metaprogramming__ is a catch-all term that covers all other uses of 
    NSE (such as in `bquote()` and `library()`). Metaprogramming is so called 
    because it involves computing on the unevaluated code in some way.

This document is broadly organised according to the three types of non-standard evaluation described above. The main difference is that after [labelling], we'll take a detour to learn more about [formulas]. You're probably familiar with formulas from linear models (e.g. `lm(mpg ~ displ, data = mtcars)`) but formulas are more than just a tool for modelling: they are a general way of capturing an unevaluated expression. 

The approaches recommended here are quite different to my previous generation of recommendations. I am fairly confident these new approaches are correct, and will not have to change substantially again. The current tools make it easy to solve a number of practical problems that were previously challenging and are rooted in [long-standing theory](http://repository.readscheme.org/ftp/papers/pepm99/bawden.pdf).

[^1]: Currently neither ggplot2 nor dplyr actually use these tools since I've only just figured it out. But I'll be working hard to make sure all my packages are consistent in the near future.

## Labelling

In base R, the classic way to turn an argument into a label is to use `deparse(substitute(x))`:

```{r}
my_label <- function(x) deparse(substitute(x))
my_label(x + y)
```

There are two potential problems with this approach:

1.  For long some expressions, `deparse()` generates a character vector with 
    length > 1:
    
    ```{r}
    my_label({
      a + b
      c + d
    })
    ```

1.  `substitute()` only looks one level up, so you lose the original label if 
    the function isn't called directly:
    
    ```{r}
    my_label2 <- function(x) my_label(x)
    my_label2(a + b)
    ```

Both of these problems are resolved by `lazyeval::expr_text()`:

```{r}
my_label <- function(x) expr_text(x)
my_label2 <- function(x) my_label(x)
   
my_label({
  a + b
  c + d
})
my_label2(a + b)
```

There are two variations on the theme of `expr_text()`:

*   `expr_find()` find the underlying expression. It works similarly to 
    `substitute()` but will follow a chain of promises back up to the original
    expression. This is often useful for [metaprogramming].
  
*   `expr_label()` is a customised version of `expr_text()` that produces 
    labels designed to be used in messages to the user:

    ```{r}
    expr_label(x)
    expr_label(a + b + c)
    expr_label(foo({
      x + y
    }))
    ```

### Exercises

1.  `plot()` uses `deparse(substitute(x))` to generate labels for the x and y
    axes. Can you generate input that causes it to display bad labels?
    Write your own wrapper around `plot()` that uses `expr_label()` to compute
    `xlim` and `ylim`.
    
1.  Create a simple implementation of `mean()` that stops with an informative
    error message if the argument is not numeric:
    
    ```{r, eval = FALSE}
    x <- c("a", "b", "c")
    my_mean(x)
    #> Error: `x` is a not a numeric vector.
    my_mean(x == "a")
    #> Error: `x == "a"` is not a numeric vector.
    my_mean("a")
    #> Error: "a" is not a numeric vector.
    ```

1.  Read the source code for `expr_text()`. How does it work? What additional
    arguments to `deparse()` does it use?

## Formulas

Non-standard scoping is probably the most useful NSE tool, but before we can talk about a solid approach, we need to take a detour to talk about formulas. Formulas are a familiar tool from linear models, but their utility is not limited to models. In fact, formulas are a powerful, general purpose tool, because a formula captures two things: 

1. An unevaluated expression.
1. The context (environment) in which the expression was created.

`~` is a single character that allows you to say: "I want to capture the meaning of this code, without evaluating it right away". For that reason, the formula can be thought of as a "quoting" operator.

### Definition of a formula

Technically, a formula is a "language" object (i.e. an unevaluated expression) with a class of "formula" and an attribute that stores the environment:

```{r}
f <- ~ x + y + z
typeof(f)
attributes(f)
```

The structure of the underlying object is slightly different depending on whether you have a one-sided or two-sided formula:

*   One-sided formulas have length two:

    ```{r}
    length(f)
    # The 1st element is always ~
    f[[1]]
    # The 2nd element is the RHS
    f[[2]]
    ```

*   Two-sided formulas have length three:

    ```{r}
    g <- y ~ x + z
    length(g)
    # The 1st element is still ~
    g[[1]]
    # But now the 2nd element is the LHS
    g[[2]]
    # And the 3rd element is the RHS
    g[[3]]
    ```

To abstract away these differences, lazyeval provides `f_rhs()` and `f_lhs()` to access either side of the formula, and `f_env()` to access its environment:

```{r}
f_rhs(f)
f_lhs(f)
f_env(f)

f_rhs(g)
f_lhs(g)
f_env(g)
```

### Evaluating a formula

A formula captures delays the evaluation of an expression so you can later evaluate it with `f_eval()`:

```{r}
f <- ~ 1 + 2 + 3
f
f_eval(f)
```

This allows you to use a formula as a robust way of delaying evaluation, cleanly separating the creation of the formula from its evaluation. Because formulas capture the code and context, you get the correct result even when a formula is created and evaluated in different places. In the following example, note that the value of `x` inside `add_1000()` is used:

```{r}
x <- 1
add_1000 <- function(x) {
  ~ 1000 + x
}

add_1000(3)
f_eval(add_1000(3))
```

It can be hard to see what's going on when looking at a formula because important values are stored in the environment, which is largely opaque. You can use `f_unwrap()` to replace names with their corresponding values:

```{r}
f_unwrap(add_1000(3))
```

### Non-standard scoping

`f_eval()` has an optional second argument: a named list (or data frame) that overrides values found in the formula's environment. 

```{r}
y <- 100
f_eval(~ y)
f_eval(~ y, data = list(y = 10))

# Can mix variables in environment and data argument
f_eval(~ x + y, data = list(x = 10))
# Can even supply functions
f_eval(~ f(y), data = list(f = function(x) x * 3))
```

This makes it very easy to implement non-standard scoping:

```{r}
f_eval(~ mean(cyl), data = mtcars)
```

One challenge with non-standard scoping is that we've introduced some ambiguity. For example, in the code below does `x` come from `mydata` or the environment?

```{r, eval = FALSE}
f_eval(~ x, data = mydata)
```

You can't tell without knowing whether or not `mydata` has a variable called `x`. To overcome this problem, `f_eval()` provides two pronouns:

* `.data` is bound to the data frame.
* `.env` is bound to the formula environment.

They both start with `.` to minimise the chances of clashing with existing variables.

With these pronouns we can rewrite the previous formula to remove the ambiguity:

```{r}
mydata <- data.frame(x = 100, y = 1)
x <- 10

f_eval(~ .env$x, data = mydata)
f_eval(~ .data$x, data = mydata)
```

If the variable or object doesn't exist, you'll get an informative error:

```{r, error = TRUE}
f_eval(~ .env$z, data = mydata)
f_eval(~ .data$z, data = mydata)
```

### Unquoting

`f_eval()` has one more useful trick up its sleeve: unquoting. Unquoting allows you to write functions where the user supplies part of the formula. For example, the following function allows you to compute the mean of any column (or any function of a column):

```{r}
df_mean <- function(df, variable) {
  f_eval(~ mean(uq(variable)), data = df)
}

df_mean(mtcars, ~ cyl)
df_mean(mtcars, ~ disp * 0.01638)
df_mean(mtcars, ~ sqrt(mpg))
```

To see how this works, we can use `f_interp()` which `f_eval()` calls internally (you shouldn't call it in your own code, but it's useful for debugging). The key is `uq()`: `uq()` evaluates its first (and only) argument and inserts the value into the formula:
    
```{r}
variable <- ~cyl
f_interp(~ mean(uq(variable)))

variable <- ~ disp * 0.01638
f_interp(~ mean(uq(variable)))
```

Unquoting allows you to create code "templates", where you write most of the expression, while still allowing the user to control important components. You can even use `uq()` to change the function being called:

```{r}
f <- ~ mean
f_interp(~ uq(f)(uq(variable)))
```

Note that `uq()` only takes the RHS of a formula, which makes it difficult to insert literal formulas into a call:

```{r}
formula <- y ~ x
f_interp(~ lm(uq(formula), data = df))
```

You can instead use `uqf()` which uses the whole formula, not just the RHS:

```{r}
f_interp(~ lm(uqf(formula), data = df))
```

Unquoting is powerful, but it only allows you to modify a single argument: it doesn't allow you to add an arbitrary number of arguments. To do that, you'll need "unquote-splice", or `uqs()`. The first (and only) argument to `uqs()` should be a list of arguments to be spliced into the call:

```{r}
variable <- ~ x
extra_args <- list(na.rm = TRUE, trim = 0.9)
f_interp(~ mean(uq(variable), uqs(extra_args)))
```

### Exercises

1.  Create a wrapper around `lm()` that allows the user to supply the 
    response and predictors as two separate formulas.
    
1.  Compare and contrast `f_eval()` with `with()`.

1.  Why does this code work even though `f` is defined in two places? (And
    one of them is not a function).

    ```{r}
    f <- function(x) x + 1
    f_eval(~ f(10), list(f = "a"))
    ```

## Non-standard scoping

Non-standard scoping (NSS) is an important part of R because it makes it easy to write functions tailored for interactive data exploration. These functions require less typing, at the cost of some ambiguity and "magic". This is a good trade-off for interactive data exploration because you want to get ideas out of your head and into the computer as quickly as possible. If a function does make a bad guess, you'll spot it quickly because you're working interactively.

There are three challenges to implementing non-standard scoping:

1.  You must correctly delay the evaluation of a function argument, capturing 
    both the computation (the expression), and the context (the environment).
    I recommend making this explicit by requiring the user to "quote" any NSS
    arguments with `~`, and then evaluating explicit with `f_eval()`.
  
1.  When writing functions that use NSS-functions, you need some way to
    avoid the automatic lookup and be explicit about where objects should be
    found. `f_eval()` solves this problem with the `.data.` and `.env` 
    pronouns.

1.  You need some way to allow the user to supply parts of a formula. 
    `f_eval()` solves this with unquoting.

To illustrate these challenges, I will implement a `sieve()` function that works similarly to `base::subset()` or `dplyr::filter()`. The goal of `sieve()` is to make it easy to select observations that match criteria defined by a logical expression. `sieve()` has three advantages over `[`:

1.  It is much more compact when the condition uses many variables, because 
    you don't need to repeat the name of the data frame many times.

1.  It drops rows where the condition evaluates to `NA`, rather than filling 
    them with `NA`s.
    
1.  It always returns a data frame.

The implementation of `sieve()` is straightforward. First we use `f_eval()` to perform NSS. Then we then check that we have a logical vector, replace `NA`s with `FALSE`, and subset with `[`.

```{R}
sieve <- function(df, condition) {
  rows <- f_eval(condition, df)
  if (!is.logical(rows)) {
    stop("`condition` must be logical.", call. = FALSE)
  }
  
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}

df <- data.frame(x = 1:5, y = 5:1)
sieve(df, ~ x <= 2)
sieve(df, ~ x == y)
```

### Programming with `sieve()`

Imagine that you've written some code that looks like this:

```{r, eval = FALSE}
sieve(march, ~ x > 100)
sieve(april, ~ x > 50)
sieve(june, ~ x > 45)
sieve(july, ~ x > 17)
```

(This is a contrived example, but it illustrates all of the important issues you'll need to consider when writing more useful functions.)

Instead of continuing to copy-and-paste your code, you decide to wrap up the common behaviour in a function: 

```{r}
threshold_x <- function(df, threshold) {
  sieve(df, ~ x > threshold)
}
threshold_x(df, 3)
```

There are two ways that this function might fail:

1.  The data frame might not have a variable called `x`. This will fail unless
    there's a variable called `x` hanging around in the global environment:
    
    ```{r, error = TRUE}
    rm(x)
    df2 <- data.frame(y = 5:1)
    
    # Throws an error
    threshold_x(df2, 3)
    
    # Silently gives the incorrect result!
    x <- 5
    threshold_x(df2, 3)
    ```
    
1.  The data frame might have a variable called `threshold`:

    ```{r}
    df3 <- data.frame(x = 1:5, y = 5:1, threshold = 4)
    threshold_x(df3, 3)
    ```

These failures are partiuclarly pernicious because instead of throwing an error they silently produce the wrong answer. Both failures arise because `f_eval()` introduces ambiguity by looking in two places for each name: the supplied data and formula environment. 

To make `threshold_x()` more reliable, we need to be more explicit by using the `.data` and `.env` pronouns:

```{r, error = TRUE}
threshold_x <- function(df, threshold) {
  sieve(df, ~ .data$x > .env$threshold)
}

threshold_x(df2, 3)
threshold_x(df3, 3)
```

Here `.env` is bound to the environment where `~` is evaluated, namely the inside of `threshold_x()`.

### Adding arguments

The `threshold_x()` function is not very useful because it's bound to a specific variable. It would be more powerful if we could vary both the threshold and the variable it applies to. We can do that by taking an additional argument to specify which variable to use. 

One simple approach is to use a string and `[[`:

```{r}
threshold <- function(df, variable, threshold) {
  stopifnot(is.character(variable), length(variable) == 1)
  
  sieve(df, ~ .data[[.env$variable]] > .env$threshold)
}
threshold(df, "x", 4)
```

This is a simple and robust solution, but only allows us to use an existing variable, not an arbitrary expression like `sqrt(x)`.

A more general solution is to allow the user to supply a formula, and use unquoting:

```{r}
threshold <- function(df, variable = ~x, threshold = 0) {
  sieve(df, ~ uq(variable) > .env$threshold)
}

threshold(df, ~ x, 4)
threshold(df, ~ abs(x - y), 2)
```

In this case, it's the responsibility of the user to ensure the `variable` is specified unambiguously. `f_eval()` is designed so that `.data` and `.env` work even when evaluated inside of `uq()`:

```{r}
x <- 3
threshold(df, ~ .data$x - .env$x, 0)
```

### Dot-dot-dot

There is one more tool that you might find useful for functions that take `...`. For example, the code below implements a function similar to `dplyr::mutate()` or `base::transform()`.

```{r}
mogrify <- function(`_df`, ...) {
  args <- list(...)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}
```

(NB: the first argument is a non-syntactic name (i.e. it requires quoting with `` ` ``) so it doesn't accidentally match one of the names of the new variables.)

`transmogrifty()` makes it easy to add new variables to a data frame:

```{r}
df <- data.frame(x = 1:5, y = sample(5))
mogrify(df, z = ~ x + y, z2 = ~ z * 2)
```

One problem with this implementation is that it's hard to specify the names of the generated variables. Imagine you want a function where the name and expression are in separate variables. This is awkward because the variable name is supplied as an argument name to `mogrify()`:

```{r}
add_variable <- function(df, name, expr) {
  do.call("mogrify", c(list(df), setNames(list(expr), name)))
}
add_variable(df, "z", ~ x + y)
```

Lazyeval provides the `f_list()` function to make writing this sort of function a little easier. It takes a list of formulas and evaluates the LHS of each formula (if present) to rename the elements:

```{r}
f_list("x" ~ y, z = ~z)
```

If we tweak `mogrify()` to use `f_list()` instead of `list()`:

```{r}
mogrify <- function(`_df`, ...) {
  args <- f_list(...)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}
```

`add_new()` becomes much simpler:

```{r}
add_variable <- function(df, name, expr) {
  mogrify(df, name ~ uq(expr))
}
add_variable(df, "z", ~ x + y)
```

### Exercises

1.  Write a function that selects all rows of `df` where `variable` is 
    greater than its mean. Make the function more general by allowing the
    user to specify a function to use instead of `mean()` (e.g. `median()`).

1.  Create a version of `mogrify()` where the first argument is `x`?
    What happens if you try to create a new variable called `x`?

## Non-standard evaluation

In some situations you might want to eliminate the formula altogether, and allow the user to type expressions directly. I was once much enamoured with this approach (witness ggplot2, dplyr, ...). However, I now think that it should be used sparingly because explict quoting with `~` leads to simpler code, and makes it more clear to the user that something special is going on.

That said, lazyeval does allow you to eliminate the `~` if you really want to. In this case, I recommend having both a NSE and SE version of the function. The SE version, which takes formuals, should have suffix `_`:

```{r}
sieve_ <- function(df, condition) {
  rows <- f_eval(condition, df)
  if (!is.logical(rows)) {
    stop("`condition` must be logical.", call. = FALSE)
  }
  
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}
```

Then create the NSE version which doesn't need the explicit formula. The key is the use of `f_capture()` which takes an unevaluated argument (a promise) and captures it as a formula:

```{r}
sieve <- function(df, expr) {
  sieve_(df, f_capture(expr))
}
sieve(df, x == 1)
```

If you're familiar with `substitute()` you might expect the same drawbacks to apply. However, `f_capture()` is smart enough to follow a chain of promises back to the original value, so, for example, this code works fine:

```{r}
scramble <- function(df) {
  df[sample(nrow(df)), , drop = FALSE]
}
subscramble <- function(df, expr) {
  scramble(sieve(df, expr))
}
subscramble(df, x < 4)
```

### Dot-dot-dot

If you want a `...` function that doesn't require formulas, I recommend that the SE version take a list of arguments, and the NSE version uses `dots_capture()` to capture multiple arguments as a list of formulas.

```{r}
mogrify_ <- function(`_df`, args) {
  args <- as_f_list(args)
  
  for (nm in names(args)) {
    `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  }
  
  `_df`
}

mogrify <- function(`_df`, ...) {
  mogrify_(`_df`, dots_capture(...))
}
```

### Exercises

1.  Recreate `subscramble()` using `base::subset()` instead of `sieve()`.
    Why does it fail?

## Metaprogramming

The final use of non-standard evaluation is to do metaprogramming. This is a catch-all term that encompasses any function that does computation on an unevaluated expression. You can learn about metaprogrgramming in <http://adv-r.had.co.nz/Expressions.html>, particularly <http://adv-r.had.co.nz/Expressions.html#ast-funs>. Over time, the goal is to move all useful metaprogramming helper functions into this package, and discuss metaprogramming more here.
---
title: "Lazyeval: a new approach to NSE"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lazyeval: a new approach to NSE}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
rownames(mtcars) <- NULL
```

This document outlines my previous approach to non-standard evaluation (NSE). You should avoid it unless you are working with an older version of dplyr or tidyr.

There are three key ideas:

* Instead of using `substitute()`, use `lazyeval::lazy()` to capture both expression
  and environment. (Or use `lazyeval::lazy_dots(...)` to capture promises in `...`)
  
* Every function that uses NSE should have a standard evaluation (SE) escape 
  hatch that does the actual computation. The SE-function name should end with 
  `_`.
  
* The SE-function has a flexible input specification to make it easy for people
  to program with.

## `lazy()`

The key tool that makes this approach possible is `lazy()`, an equivalent to `substitute()` that captures both expression and environment associated with a function argument:

```{r}
library(lazyeval)
f <- function(x = a - b) {
  lazy(x)
}
f()
f(a + b)
```

As a complement to `eval()`, the lazy package provides `lazy_eval()` that uses the environment associated with the lazy object:

```{r}
a <- 10
b <- 1
lazy_eval(f())
lazy_eval(f(a + b))
```

The second argument to lazy eval is a list or data frame where names should be looked up first:

```{r}
lazy_eval(f(), list(a = 1))
```

`lazy_eval()` also works with formulas, since they contain the same information as a lazy object: an expression (only the RHS is used by convention) and an environment:

```{r}
lazy_eval(~ a + b)
h <- function(i) {
  ~ 10 + i
}
lazy_eval(h(1))
```

## Standard evaluation

Whenever we need a function that does non-standard evaluation, always write the standard evaluation version first. For example, let's implement our own version of `subset()`:

```{r}
subset2_ <- function(df, condition) {
  r <- lazy_eval(condition, df)
  r <- r & !is.na(r)
  df[r, , drop = FALSE]
} 

subset2_(mtcars, lazy(mpg > 31))
```

`lazy_eval()` will always coerce it's first argument into a lazy object, so a variety of specifications will work:

```{r}
subset2_(mtcars, ~mpg > 31)
subset2_(mtcars, quote(mpg > 31))
subset2_(mtcars, "mpg > 31")
```

Note that quoted called and strings don't have environments associated with them, so `as.lazy()` defaults to using `baseenv()`. This will work if the expression is self-contained (i.e. doesn't contain any references to variables in the local environment), and will otherwise fail quickly and robustly.

## Non-standard evaluation

With the SE version in hand, writing the NSE version is easy. We just use `lazy()` to capture the unevaluated expression and corresponding environment:

```{r}
subset2 <- function(df, condition) {
  subset2_(df, lazy(condition))
}
subset2(mtcars, mpg > 31)
```

This standard evaluation escape hatch is very important because it allows us to implement different NSE approaches. For example, we could create a subsetting function that finds all rows where a variable is above a threshold:

```{r}
above_threshold <- function(df, var, threshold) {
  cond <- interp(~ var > x, var = lazy(var), x = threshold)
  subset2_(df, cond)
}
above_threshold(mtcars, mpg, 31)
```

Here we're using `interp()` to modify a formula. We use the value of `threshold` and the expression in  by `var`.

## Scoping

Because `lazy()` captures the environment associated with the function argument, we automatically avoid a subtle scoping bug present in `subset()`:
  
```{r}
x <- 31
f1 <- function(...) {
  x <- 30
  subset(mtcars, ...)
}
# Uses 30 instead of 31
f1(mpg > x)

f2 <- function(...) {
  x <- 30
  subset2(mtcars, ...)
}
# Correctly uses 31
f2(mpg > x)
```

`lazy()` has another advantage over `substitute()` - by default, it follows promises across function invocations. This simplifies the casual use of NSE.

```{r, eval = FALSE}
x <- 31
g1 <- function(comp) {
  x <- 30
  subset(mtcars, comp)
}
g1(mpg > x)
#> Error: object 'mpg' not found
```

```{r}
g2 <- function(comp) {
  x <- 30
  subset2(mtcars, comp)
}
g2(mpg > x)
```

Note that `g2()` doesn't have a standard-evaluation escape hatch, so it's not suitable for programming with in the same way that `subset2_()` is. 

## Chained promises

Take the following example:

```{r}
library(lazyeval)
f1 <- function(x) lazy(x)
g1 <- function(y) f1(y)

g1(a + b)
```

`lazy()` returns `a + b` because it always tries to find the top-level promise.

In this case the process looks like this:

1. Find the object that `x` is bound to.
2. It's a promise, so find the expr it's bound to (`y`, a symbol) and the
   environment in which it should be evaluated (the environment of `g()`).
3. Since `x` is bound to a symbol, look up its value: it's bound to a promise.
4. That promise has expression `a + b` and should be evaluated in the global
   environment.
5. The expression is not a symbol, so stop.

Occasionally, you want to avoid this recursive behaviour, so you can use `follow_symbol = FALSE`:

```{r}
f2 <- function(x) lazy(x, .follow_symbols = FALSE)
g2 <- function(y) f2(y)

g2(a + b)
```

Either way, if you evaluate the lazy expression you'll get the same result:

```{r}
a <- 10
b <- 1

lazy_eval(g1(a + b))
lazy_eval(g2(a + b))
```

Note that the resolution of chained promises only works with unevaluated objects. This is because R deletes the information about the environment associated with a promise when it has been forced, so that the garbage collector is allowed to remove the environment from memory in case it is no longer used. `lazy()` will fail with an error in such situations.

```{r, error = TRUE, purl = FALSE}
var <- 0

f3 <- function(x) {
  force(x)
  lazy(x)
}

f3(var)
```
---
title: "Introduction to cowplot"
author: "Claus O. Wilke"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{Introduction to cowplot}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---
```{r message = FALSE, echo = FALSE}
knitr::opts_chunk$set(
  fig.width = 4,
  fig.height = 3
)
```

The cowplot package is a simple add-on to ggplot. It provides various features that help with creating publication-quality figures, such as a set of themes, functions to align plots and arrange them into complex compound figures, and functions that make it easy to annotate plots and or mix plots with images. The package was originally written for internal use in my lab, to provide my students and postdocs with the tools to make high-quality figures for their publications. I have also used the package extensively in my book [Fundamentals of Data Visualization.](https://www.amazon.com/gp/product/1492031089) This introductory vignette provides a brief glance at the key features of the package.

For more complete documentation, read [all vignettes](https://wilkelab.org/cowplot/articles/index.html) and/or the [reference documentation.](https://wilkelab.org/cowplot/reference/index.html)

# Themes

When I first wrote the cowplot package, its primary purpose was to provide a simple and clean theme, similar to ggplot2's `theme_classic()`. Therefore, I wrote the package to automatically load the theme. However, as the package grew in functionality, this choice was increasingly problematic, and therefore as of version 1.0 cowplot does not alter the default ggplot2 theme anymore.

Importantly, the cowplot package now provides a set of complementary themes with different features. I now believe that there isn't one single theme that works for all figures, and therefore I recommend that you always explicitly set a theme for every plot you make.

```{r fig.width = 5.2, message = FALSE}
library(ggplot2)
library(cowplot)

# default ggplot2 theme
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + 
  geom_point()

# classic cowplot theme
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + 
  geom_point() +
  theme_cowplot(12)

# minimal grid theme
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + 
  geom_point() +
  theme_minimal_grid(12)

# minimal horizontal grid theme
ggplot(iris, aes(Sepal.Length, fill = Species)) + 
  geom_density(alpha = 0.5) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.05))) +
  theme_minimal_hgrid(12)
```

However, if you have existing code that depends on the old cowplot behavior, you can call `theme_set(theme_cowplot())` at the top of your script to change the theme globally.

# Arranging plots into a grid

The cowplot package provides the function `plot_grid()` to arrange plots into a grid and label them. 

```{r fig.width = 6}
p1 <- ggplot(mtcars, aes(disp, mpg)) + 
  geom_point()
p2 <- ggplot(mtcars, aes(qsec, mpg)) +
  geom_point()

plot_grid(p1, p2, labels = c('A', 'B'), label_size = 12)
```

While `plot_grid()` was originally written for ggplot2, it also supports other plotting frameworks, such as base graphics.

```{r fig.width = 6}
p3 <- ~plot(mtcars$qsec, mtcars$mpg)

plot_grid(p1, p3, labels = c('A', 'B'), label_size = 12)
```

# Generic plot annotations

The `plot_grid()` is built on top of a generic drawing layer that allows us to capture plots as images and then draw with them or on top of them.

```{r}
p <- ggplot(mtcars, aes(disp, mpg)) + 
  geom_point(size = 1.5, color = "blue") +
  theme_cowplot(12)

logo_file <- system.file("extdata", "logo.png", package = "cowplot")

ggdraw(p) + 
  draw_image(logo_file, x = 1, y = 1, hjust = 1, vjust = 1, width = 0.13, height = 0.2)
```

Here, `ggdraw()` takes a snapshot of the plot and places it at full size into a new drawing canvas. The function `draw_image()` then draws an image on top of the plot.

To create a watermark, we can reverse the order by first setting up an empty drawing canvas, then drawing the image, and then drawing the plot on top.

```{r}
ggdraw() + 
  draw_image(logo_file, scale = 0.5) +
  draw_plot(p)
```
---
title: "1. Initiation to Bayesian models"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Example 1: Initiation to Bayesian models}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(insight)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

set.seed(333)

if (!requireNamespace("rstanarm", quietly = TRUE) ||
    !requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("ggplot2", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

format_percent <- function(x, digits = 0, ...) {
  paste0(format_value(x*100, digits = digits, ...), "%")
}
```

Now that you've read the [**Get started**](https://easystats.github.io/bayestestR/articles/bayestestR.html) section, let's dive in the **subtleties of Bayesian modelling using R**.

## Loading the packages

Once you've [installed](https://easystats.github.io/bayestestR/articles/bayestestR.html#bayestestr-installation) the necessary packages, we can load `rstanarm` (to fit the models), `bayestestR` (to compute useful indices) and `insight` (to access the parameters).

```{r message=FALSE, warning=FALSE}
library(rstanarm)
library(bayestestR)
library(insight)
```

## Simple linear model (*aka* a regression)

We will begin by conducting a simple linear regression to test the relationship between `Petal.Length` (our predictor, or *independent*, variable) and `Sepal.Length` (our response, or *dependent*, variable) from the [`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset which is included by default in R. 


### Fitting the model


Let's start by fitting the **frequentist** version of the model, just to have a reference point:

```{r message=FALSE, warning=FALSE}
model <- lm(Sepal.Length ~ Petal.Length, data=iris)
summary(model)
```

In this model, the linear relationship between `Petal.Length` and `Sepal.Length` is **positive and significant** (beta = 0.41, *t*(148) = 21.6, *p* < .001). This means that for each one-unit increase in `Petal.Length` (the predictor), you can expect `Sepal.Length` (the response) to increase by **0.41**. This effect can be visualized by plotting the predictor values on the `x` axis and the response values as `y` using the `ggplot2` package:

```{r message=FALSE, warning=FALSE}
library(ggplot2)  # Load the package

# The ggplot function takes the data as argument, and then the variables 
# related to aesthetic features such as the x and y axes.
ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +
  geom_point() +  # This adds the points
  geom_smooth(method="lm") # This adds a regression line
```

Now let's fit a **Bayesian version** of the model by using the `stan_glm` function in the `rstanarm` package:

```{r message=FALSE, warning=FALSE, eval=FALSE}
model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA, results='hide'}
library(rstanarm)
set.seed(333)

model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris)
```

You can see the sampling algorithm being run. 

### Extracting the posterior

Once it is done, let us extract the parameters (*i.e.*, coefficients) of the model.

```{r message=FALSE, warning=FALSE, eval=FALSE}
posteriors <- insight::get_parameters(model)

head(posteriors)  # Show the first 6 rows
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
posteriors <- insight::get_parameters(model)

head(posteriors)  # Show the first 6 rows
```

As we can see, the parameters take the form of a lengthy dataframe with two columns, corresponding to the `intercept` and the effect of `Petal.Length`. These columns contain the **posterior distributions** of these two parameters. In simple terms, the posterior distribution is a set of different plausible values for each parameter.

#### About posterior draws

Let's look at the length of the posteriors.

```{r message=FALSE, warning=FALSE}
nrow(posteriors)  # Size (number of rows)
```

> **Why is the size 4000, and not more or less?**

First of all, these observations (the rows) are usually referred to as **posterior draws**. The underlying idea is that the Bayesian sampling algorithm (*e.g.*, **Monte Carlo Markov Chains - MCMC**) will *draw* from the hidden true posterior distribution. Thus, it is through these posterior draws that we can estimate the underlying true posterior distribution. **Therefore, the more draws you have, the better your estimation of the posterior distribution**. However, increased draws also means longer computation time.

If we look at the documentation (`?sampling`) for the rstanarm `"sampling"` algorithm used by default in the model above, we can see several parameters that influence the number of posterior draws. By default, there are **4** `chains` (you can see it as distinct sampling runs), that each create **2000** `iter` (draws). However, only half of these iterations are kept, as half are used for `warm-up` (the convergence of the algorithm). Thus, the total is **`4 chains * (2000 iterations - 1000 warm-up) = 4000`** posterior draws. We can change that, for instance:

```{r message=FALSE, warning=FALSE, eval=FALSE}
model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris, chains = 2, iter = 1000, warmup = 250)
 
nrow(insight::get_parameters(model))  # Size (number of rows)
```
 
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA, echo=FALSE}
model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris, chains = 2, iter = 1000, warmup = 250, refresh = 0)
nrow(insight::get_parameters(model))  # Size (number of rows)
```

In this case, as would be expected, we have **`2 chains * (1000 iterations - 250 warm-up) = 1500`** posterior draws. But let's keep our first model with the default setup (as it has more draws).

#### Visualizing the posterior distribution

Now that we've understood where these values come from, let's look at them. We will start by visualizing the posterior distribution of our parameter of interest, the effect of `Petal.Length`.


```{r message=FALSE, warning=FALSE}
ggplot(posteriors, aes(x = Petal.Length)) +
  geom_density(fill = "orange")
```

This distribution represents the [probability](https://en.wikipedia.org/wiki/Probability_density_function) (the y axis) of different effects (the x axis). The central values are more probable than the extreme values. As you can see, this distribution ranges from about **0.35 to 0.50**, with the bulk of it being at around **0.41**.

> **Congrats! You've just described your posterior distribution.**

And this is at the heart of Bayesian analysis. We don't need *p*-values, *t*-values or degrees of freedom: **everything is there**, within this posterior distribution.

Our description above is consistent with the values obtained from the frequentist regression (which resulted in a beta of **0.41**). This is reassuring! Indeed, **in most cases a Bayesian analysis does not drastically change the results** or their interpretation. Rather, it makes the results more interpretable and intuitive, and easier to understand and describe.

We can now go ahead and **precisely characterize** this posterior distribution.

### Describing the Posterior

Unfortunately, it is often not practical to report the whole posterior distributions as graphs. We need to find a **concise way to summarize it**. We recommend to describe the posterior distribution with **3 elements**:

1. A **point-estimate** which is a one-value summary (similar to the *beta* in frequentist regressions).
2. A **credible interval** representing the associated uncertainty.
3. Some **indices of significance**, giving information about the relative importance of this effect.


#### Point-estimate

**What single value can best represent my posterior distribution?**

Centrality indices, such as the *mean*, the *median* or the *mode* are usually used as point-estimates - but what's the difference between them? Let's answer this by first inspecting the **mean**:

```{r message=FALSE, warning=FALSE}
mean(posteriors$Petal.Length)
```

This is close to the frequentist beta. But as we know, the mean is quite sensitive to outliers or extremes values. Maybe the **median** could be more robust?

```{r message=FALSE, warning=FALSE}
median(posteriors$Petal.Length)
```

Well, this is **very close to the mean** (and identical when rounding the values). Maybe we could take the **mode**, that is, the *peak* of the posterior distribution? In the Bayesian framework, this value is called the **Maximum A Posteriori (MAP)**. Let's see:

```{r message=FALSE, warning=FALSE}
map_estimate(posteriors$Petal.Length)
```

**They are all very close!** Let's visualize these values on the posterior distribution:

```{r message=FALSE, warning=FALSE}
ggplot(posteriors, aes(x = Petal.Length)) +
  geom_density(fill = "orange") +
  # The mean in blue
  geom_vline(xintercept=mean(posteriors$Petal.Length), color="blue", size=1) +
  # The median in red
  geom_vline(xintercept=median(posteriors$Petal.Length), color="red", size=1) +
  # The MAP in purple
  geom_vline(xintercept=map_estimate(posteriors$Petal.Length), color="purple", size=1)
```

Well, all these values give very similar results. Thus, **we will choose the median**, as this value has a direct meaning from a probabilistic perspective: **there is 50\% chance that the true effect is higher and 50\% chance that the effect is lower** (as it divides the distribution in two equal parts).


#### Uncertainty

Now that the have a point-estimate, we have to **describe the uncertainty**. We could compute the range:

```{r message=FALSE, warning=FALSE}
range(posteriors$Petal.Length)
```

But does it make sense to include all these extreme values? Probably not. Thus, we will compute a [**credible interval**](https://easystats.github.io/bayestestR/articles/credible_interval.html). Long story short, it's kind of similar to a frequentist **confidence interval**, but easier to interpret and easier to compute — *and it makes more sense*.

We will compute this **credible interval** based on the [Highest Density Interval (HDI)](https://easystats.github.io/bayestestR/articles/credible_interval.html#different-types-of-cis). It will give us the range containing the 89\% most probable effect values. **Note that we will use 89\% CIs instead of 95\%** CIs (as in the frequentist framework), as the 89\% level gives more [stable results](https://easystats.github.io/bayestestR/articles/credible_interval.html#why-is-the-default-89) [@kruschke2014doing] and reminds us about the arbitrarity of such conventions [@mcelreath2018statistical].

```{r message=FALSE, warning=FALSE}
hdi(posteriors$Petal.Length, ci=0.89)
```

Nice, so we can conclude that **the effect has 89\% chance of falling within the `[0.38, 0.44]` range**. We have just computed the two most important pieces of information for describing our effects. 

#### Effect significance

However, in many scientific fields it not sufficient to simply describe the effects. Scientists also want to know if this effect has significance in practical or statistical terms, or in other words, whether the effect is important. For instnace, is the effect different from 0? So how do we **assess the *significance* of an effect**. How can we do this?

Well, in this particular case, it is very eloquent: **all possible effect values (*i.e.*, the whole posterior distribution) are positive and over 0.35, which is already substantial evidence the effect is not zero**.

But still, we want some objective decision criterion, to say if **yes or no the effect is 'significant'**.  One approach, similar to the frequentist framework, would be to see if the **Credible Interval** contains 0. If it is not the case, that would mean that our **effect is 'significant'**. 

But this index is not very fine-grained, isn't it? **Can we do better? Yes.**


## A linear model with a categorical predictor

Imagine for a moment you are interested in how the weight of chickens varies depending on two different **feed types**. For this exampe, we will start by selecting from the `chickwts` dataset (available in base R) two feed types of interest for us (*we do have peculiar interests*): **meat meals** and **sunflowers**.

### Data preparation and model fitting

```{r message=FALSE, warning=FALSE}
library(dplyr)

# We keep only rows for which feed is meatmeal or sunflower
data <- chickwts %>% 
  filter(feed %in% c("meatmeal", "sunflower"))
```


Let's run another Bayesian regression to predict the **weight** with the **two types of feed type**.

```{r message=FALSE, warning=FALSE, eval=FALSE}
model <- stan_glm(weight ~ feed, data=data)
```
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA, results='hide'}
model <- stan_glm(weight ~ feed, data=data)
```

### Posterior description


```{r message=FALSE, warning=FALSE}
posteriors <- insight::get_parameters(model)

ggplot(posteriors, aes(x=feedsunflower)) +
  geom_density(fill = "red")
```

This represents the **posterior distribution of the difference between `meatmeal` and `sunflowers`**. Seems that the difference is rather **positive** (the values seems concentrated on the right side of 0)... Eating sunflowers makes you more fat (*at least, if you're a chicken*). But, **by how much?** Let us compute the **median** and the **CI**:

```{r message=FALSE, warning=FALSE}
median(posteriors$feedsunflower)
hdi(posteriors$feedsunflower)
```

It makes you fat by around 51 grams (the median). However, the uncertainty is quite high: **there is 89\% chance that the difference between the two feed types is between 14 and 91.**

> **Is this effect different from 0?**

### ROPE Percentage

Testing whether this distribution is different from 0 doesn't make sense, as 0 is a single value (*and the probability that any distribution is different from a single value is infinite*). 

However, one way to assess **significance** could be to define an area around 0, which will consider as *practically equivalent* to zero (*i.e.*, absence of, or negligible, effect). This is called the [**Region of Practical Equivalence (ROPE)**](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html), and is one way of testing the significance of parameters.

**How can we define this region?**


> ***Driing driiiing***

-- ***The easystats team speaking. How can we help?***

-- ***I am Prof. Sanders. An expert in chicks... I mean chickens. Just calling to let you know that based on my expert knowledge, an effect between -20 and 20 is negligible. Bye.***


Well, that's convenient. Now we know that we can define the ROPE as the `[-20, 20]` range. All effects within this range are considered as *null* (negligible). We can now compute the **proportion of the 89\% most probable values (the 89\% CI) which are not null**, *i.e.*, which are outside this range. 

```{r message=FALSE, warning=FALSE}
rope(posteriors$feedsunflower, range = c(-20, 20), ci=0.89)
```

**5\% of the 89\% CI can be considered as null**. Is that a lot? Based on our [**guidelines**](https://easystats.github.io/bayestestR/articles/guidelines.html), yes, it is too much. **Based on this particular definition of ROPE**, we conclude that this effect is not significant (the probability of being negligible is too high).

Although, to be honest, I have **some doubts about this Prof. Sanders**. I don't really trust **his definition of ROPE**. Is there a more **objective** way of defining it?

```{r echo=FALSE, fig.cap="Prof. Sanders giving default values to define the Region of Practical Equivalence (ROPE).", fig.align='center', out.width="75%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/profsanders.png")
```

**Yes.** One of the practice is for instance to use the **tenth (`1/10 = 0.1`) of the standard deviation (SD)** of the response variable, which can be considered as a "negligible" effect size [@cohen1988statistical].

```{r message=FALSE, warning=FALSE}
rope_value <- 0.1 * sd(data$weight)
rope_range <- c(-rope_value, rope_value)
rope_range
```

Let's redefine our ROPE as the region within the `[-6.2, 6.2]` range. **Note that this can be directly obtained by the `rope_range` function :)**

```{r message=FALSE, warning=FALSE}
rope_value <- rope_range(model)
rope_value
```

Let's recompute the **percentage in ROPE**:

```{r message=FALSE, warning=FALSE}
rope(posteriors$feedsunflower, range = rope_range, ci=0.89)
```

With this reasonable definition of ROPE, we observe that the 89\% of the posterior distribution of the effect does **not** overlap with the ROPE. Thus, we can conclude that **the effect is significant** (in the sense of *important* enough to be noted).


### Probability of Direction (pd)

Maybe we are not interested in whether the effect is non-negligible. Maybe **we just want to know if this effect is positive or negative**. In this case, we can simply compute the proportion of the posterior that is positive, no matter the "size" of the effect. 

```{r message=FALSE, warning=FALSE}
n_positive <- posteriors %>% 
  filter(feedsunflower > 0) %>% # select only positive values
  nrow() # Get length
n_positive / nrow(posteriors) * 100
```

We can conclude that **the effect is positive with a probability of 98\%**. We call this index the [**Probability of Direction (pd)**](https://easystats.github.io/bayestestR/articles/probability_of_direction.html). It can, in fact, be computed more easily with the following:

```{r message=FALSE, warning=FALSE}
p_direction(posteriors$feedsunflower)
```

Interestingly, it so happens that **this index is usually highly correlated with the frequentist *p*-value**. We could almost roughly infer the corresponding *p*-value with a simple transformation:

```{r message=FALSE, warning=FALSE, eval=TRUE}
pd <- 97.82
onesided_p <- 1 - pd / 100  
twosided_p <- onesided_p * 2
twosided_p
```

If we ran our model in the frequentist framework, we should approximately observe an effect with a *p*-value of `r round(twosided_p, digits=3)`. **Is that true?**

#### Comparison to frequentist


```{r message=FALSE, warning=FALSE}
lm(weight ~ feed, data=data) %>% 
  summary()
```

The frequentist model tells us that the difference is **positive and significant** (beta = 52, p = 0.04). 

**Although we arrived to a similar conclusion, the Bayesian framework allowed us to develop a more profound and intuitive understanding of our effect, and of the uncertainty of its estimation.**



## All with one function

And yet, I agree, it was a bit **tedious** to extract and compute all the indices. **But what if I told you that we can do all of this, and more, with only one function?**

> **Behold, `describe_posterior`!**

This function computes all of the adored mentioned indices, and can be run directly on the model:
```{r message=FALSE, warning=FALSE}
describe_posterior(model, test = c("p_direction","rope","bayesfactor"))
```

**Tada!** There we have it! The **median**, the **CI**, the **pd** and the **ROPE percentage**!

Understanding and describing posterior distributions is just one aspect of Bayesian modelling... **Are you ready for more?** [**Click here**](https://easystats.github.io/bayestestR/articles/example2_GLM.html) to see the next example.

## References
---
title: "Get Started with Bayesian Analysis"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Get Started with Bayesian Analysis}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("rstanarm", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)
```


## Why use the Bayesian Framework?

The Bayesian framework for statistics is quickly gaining in popularity among scientists, associated with the general shift towards **open and honest science**. Reasons to prefer this approach are **reliability**, **accuracy** (in noisy data and small samples), the possibility of introducing **prior knowledge** into the analysis and, critically, **results intuitiveness** and their **straightforward interpretation** [@andrews2013prior; @etz2016bayesian; @kruschke2010believe; @kruschke2012time; @wagenmakers2018bayesian]. 

In general, the frequentist approach has been associated with the focus on null hypothesis testing, and the misuse of *p*-values has been shown to critically contribute to the reproducibility crisis of psychological science [@chambers2014instead; @szucs2016empirical]. There is a general agreement that the generalization of the Bayesian approach is one way of overcoming these issues [@benjamin2018redefine; @etz2016bayesian].

Once we agreed that the Bayesian framework is the right way to go, you might wonder *what* is the Bayesian framework. 

**What's all the fuss about?**

## What is the Bayesian Framework?

Adopting the Bayesian framework is more of a shift in the paradigm than a change in the methodology. Indeed, all the common statistical procedures (t-tests, correlations, ANOVAs, regressions, ...) can be achieved using the Bayesian framework. One of the core difference is that in the **frequentist view** (the "classic" statistics, with *p* and *t* values, as well as some weird *degrees of freedom*), **the effects are fixed** (but unknown) and **data are random**. On the other hand, in the Bayesian inference process, instead of having estimates of the "true effect", the probability of different effects *given the observed data* is computed, resulting in a distribution of possible values for the parameters, called the ***posterior distribution***. 

The uncertainty in Bayesian inference can be summarized, for instance, by the **median** of the distribution, as well as a range of values of the posterior distribution that includes the 95\% most probable values (the 95\% ***credible* interval**). *Cum grano salis*, these are considered the counterparts to the point-estimate and confidence interval in a frequentist framework. To illustrate the difference of interpretation, the Bayesian framework allows to say *"given the observed data, the effect has 95\% probability of falling within this range"*, while the frequentist less straightforward alternative would be *"when repeatedly computing confidence intervals from data of this sort, there is a 95\% probability that the effect falls within a given range"*. In essence, the Bayesian sampling algorithms (such as MCMC sampling) return a probability distribution (*the posterior*) of an effect that is compatible with the observed data. Thus, an effect can be described by [characterizing its posterior distribution](https://easystats.github.io/bayestestR/articles/guidelines.html) in relation to its centrality (point-estimates), uncertainty, as well as existence and significance


In other words, omitting the maths behind it, we can say that:

- The frequentist bloke tries to estimate "the **real effect**". For instance, the "real" value of the correlation between *x* and *y*. Hence, frequentist models return a "**point-estimate**" (*i.e.*, a single value) of the "real" correlation (*e.g.*, r = 0.42) estimated under a number of obscure assumptions (at a minimum, considering that the data is sampled at random from a "parent", usually normal distribution).
- **The Bayesian master assumes no such thing**. The data are what they are. Based on this observed data (and a **prior** belief about the result), the Bayesian sampling algorithm (sometimes referred to for example as **MCMC** sampling) returns a probability distribution (called **the posterior**) of the effect that is compatible with the observed data. For the correlation between *x* and *y*, it will return a distribution that says, for example, "the most probable effect is 0.42, but this data is also compatible with correlations of 0.12 and 0.74".
- To characterize our effects, **no need of *p* values** or other cryptic indices. We simply describe the posterior distribution of the effect. For example, we can report the median, the [89% *Credible* Interval](https://easystats.github.io/bayestestR/articles/credible_interval.html) or [other indices](https://easystats.github.io/bayestestR/articles/guidelines.html).


```{r echo=FALSE, fig.cap="Accurate depiction of a regular Bayesian user estimating a credible interval.", fig.align='center', out.width="50%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/bayesianMaster.jpg")
```


*Note: Altough the very purpose of this package is to advocate for the use of Bayesian statistics, please note that there are serious arguments supporting frequentist indices (see for instance [this thread](https://discourse.datamethods.org/t/language-for-communicating-frequentist-results-about-treatment-effects/934/16)). As always, the world is not black and white (p \< .001).*

**So... how does it work?**

## A simple example

### BayestestR Installation

You can install `bayestestR` along with the whole [**easystats**](https://github.com/easystats/easystats) suite by running the following:

```{r eval=FALSE, message=FALSE, warning=FALSE}
install.packages("devtools")
devtools::install_github("easystats/easystats")
```

<!-- Now that it's done, we can *load* the packages we need, in this case [`report`](https://github.com/easystats/report), which combines `bayestestR` and other packages together. -->

<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- library(report)  # Calling library(...) enables you to use the package's functions -->
<!-- ``` -->


Let's also install and load the [`rstanarm`](https://mc-stan.org/rstanarm/), that allows fitting Bayesian models, as well as [`bayestestR`](https://github.com/easystats/bayestestR), to describe them.

```{r message=FALSE, warning=FALSE, eval=FALSE}
install.packages("rstanarm")
library(rstanarm)
```



### Traditional linear regression

Let's start by fitting a simple frequentist linear regression (the `lm()` function stands for *linear model*) between two numeric variables, `Sepal.Length` and `Petal.Length` from the famous [`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset, included by default in R.

```{r message=FALSE, warning=FALSE, eval=FALSE}
model <- lm(Sepal.Length ~ Petal.Length, data=iris)
summary(model)
```
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
library(dplyr)

lm(Sepal.Length ~ Petal.Length, data=iris) %>% 
  summary()
```

<!-- ```{r message=FALSE, warning=FALSE, eval=FALSE} -->
<!-- model <- lm(Sepal.Length ~ Petal.Length, data=iris) -->
<!-- report(model) -->
<!-- ``` -->
<!-- ```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA} -->
<!-- library(dplyr) -->

<!-- lm(Sepal.Length ~ Petal.Length, data=iris) %>%  -->
<!--   report() %>%  -->
<!--   to_text(width=100) -->
<!-- ``` -->

This analysis suggests that there is a **significant** (*whatever that means*) and **positive** (with a coefficient of `0.41`) linear relationship between the two variables. 

*Fitting and interpreting **frequentist models is so easy** that it is obvious that people use it instead of the Bayesian framework... right?*

**Not anymore.**


### Bayesian linear regression

<!-- ```{r message=FALSE, warning=FALSE, eval=FALSE} -->
<!-- model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris) -->
<!-- report(model) -->
<!-- ``` -->
<!-- ```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA, results='hide'} -->
<!-- library(rstanarm) -->
<!-- set.seed(333) -->

<!-- model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris) -->
<!-- ``` -->
<!-- ```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA} -->
<!-- model %>%  -->
<!--   report() %>%  -->
<!--   to_text(width=100) -->
<!-- ``` -->

```{r message=FALSE, warning=FALSE, eval=FALSE}
model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris)
describe_posterior(model)
```
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
library(rstanarm)
library(bayestestR)
set.seed(333)

model <- stan_glm(Sepal.Length ~ Petal.Length, data=iris, refresh = 0)
knitr::kable(describe_posterior(model), digits=2)
```


**That's it!** You fitted a Bayesian version of the model by simply using [`stan_glm()`](https://mc-stan.org/rstanarm/reference/stan_glm.html) instead of `lm()` and described the posterior distributions of the parameters. The conclusion that we can drawn, for this example, are very similar. The effect (*the median of the effect's posterior distribution*) is about `0.41`, and it can be also be considered as *significant* in the Bayesian sense (more on that later).

**So, ready to learn more?** Check out the [**next tutorial**](https://easystats.github.io/bayestestR/articles/example1.html)!

## References


---
title: "In-Depth 1: Comparison of Point-Estimates"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{In-Depth 1: Comparison of Point-Estimates}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("see", quietly = TRUE) || 
    !requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("ggplot2", quietly = TRUE) ||
    !requireNamespace("tidyr", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)
```


This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

# Effect Point-Estimates in the Bayesian Framework


## Introduction 

One of the main difference between the Bayesian and the frequentist frameworks is that the former returns a probability distribution of each effect (*i.e.*, parameter of interest of a model, such as a regression slope) instead of a single value. However, there is still a need and demand, for reporting or use in further analysis, for a single value (**point-estimate**) that best characterise the underlying posterior distribution.

There are three main indices used in the literature for effect estimation: the **mean**, the **median** or the **MAP** (Maximum A Posteriori) estimate (roughly corresponding to the mode (the "peak") of the distribution). Unfortunately, there is no consensus about which one to use, as no systematic comparison has ever been done.

In the present work, we will compare these three point-estimates of effect between themselves, as well as with the widely known **beta**, extracted from a comparable frequentist model. With this comparison, we expect to draw bridges and relationships between the two frameworks, helping and easing the transition for the public.


## Experiment 1: Relationship with Error (Noise) and Sample Size


### Methods


The simulation aimed at modulating the following characteristics:

- **Model type**: linear or logistic.
- **"True" effect** (original regression coefficient from which data is drawn): Can be 1 or 0 (no effect).
- **Sample size**: From 20 to 100 by steps of 10.
- **Error**: Gaussian noise applied to the predictor with SD uniformly spread between 0.33 and 6.66 (with 1000 different values).

We generated a dataset for each combination of these characteristics, resulting in a total of `2 * 2 * 9 * 1000 = 36000` Bayesian and frequentist models. The code used for generation is avaible [here](https://easystats.github.io/circus/articles/bayesian_indices.html) (please note that it takes usually several days/weeks to complete).

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(see)
library(parameters)

df <- read.csv("https://raw.github.com/easystats/circus/master/data/bayesSim_study1.csv")
```




### Results


#### Sensitivity to Noise

```{r, message=FALSE, warning=FALSE}
df %>%
  select(error, true_effect, outcome_type, Coefficient, Median, Mean, MAP) %>%
  gather(estimate, value, -error, -true_effect, -outcome_type) %>%
  mutate(temp = as.factor(cut(error, 10, labels = FALSE))) %>% 
  group_by(temp) %>% 
  mutate(error_group = round(mean(error), 1)) %>% 
  ungroup() %>% 
  filter(value < 6) %>% 
  ggplot(aes(x = error_group, y = value, fill = estimate, group = interaction(estimate, error_group))) +
  # geom_hline(yintercept = 0) +
  # geom_point(alpha=0.05, size=2, stroke = 0, shape=16) +
  # geom_smooth(method="loess") +
  geom_boxplot(outlier.shape=NA) +
  theme_modern() +
  scale_fill_manual(values = c("Coefficient" = "#607D8B", "MAP" = "#795548", "Mean" = "#FF9800", "Median" = "#FFEB3B"),
                    name = "Index") +
  ylab("Point-estimate") +
  xlab("Noise") +
  facet_wrap(~ outcome_type * true_effect, scales="free") 
```


#### Sensitivity to Sample Size

```{r, message=FALSE, warning=FALSE}
df %>%
  select(sample_size, true_effect, outcome_type, Coefficient, Median, Mean, MAP) %>%
  gather(estimate, value, -sample_size, -true_effect, -outcome_type) %>%
  mutate(temp = as.factor(cut(sample_size, 10, labels = FALSE))) %>% 
  group_by(temp) %>% 
  mutate(size_group = round(mean(sample_size))) %>% 
  ungroup() %>% 
  filter(value < 6) %>% 
  ggplot(aes(x = size_group, y = value, fill = estimate, group = interaction(estimate, size_group))) +
  # geom_hline(yintercept = 0) +
  # geom_point(alpha=0.05, size=2, stroke = 0, shape=16) +
  # geom_smooth(method="loess") +
  geom_boxplot(outlier.shape=NA) +
  theme_modern() +
  scale_fill_manual(values = c("Coefficient" = "#607D8B", "MAP" = "#795548", "Mean" = "#FF9800", "Median" = "#FFEB3B"),
                    name = "Index") +
  ylab("Point-estimate") +
  xlab("Sample size") +
  facet_wrap(~ outcome_type * true_effect, scales="free")
```



#### Statistical Modelling

We fitted a (frequentist) multiple linear regression to statistically test the the predict the presence or absence of effect with the estimates as well as their interaction with noise and sample size.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
df %>%
  select(sample_size, error, true_effect, outcome_type, Coefficient, Median, Mean, MAP) %>%
  tidyr::pivot_longer(c(-sample_size, -error, -true_effect, -outcome_type), names_to="estimate") %>%
  glm(true_effect ~ outcome_type / estimate / value, data=., family="binomial") %>%
  parameters::parameters(df_method="wald") %>%
  select(Parameter, Coefficient, p) %>%
  filter(stringr::str_detect(Parameter, 'outcome_type'),
         stringr::str_detect(Parameter, ':value')) %>%
  arrange(desc(Coefficient)) %>% 
  knitr::kable(digits=2) 
```

|Parameter                                    | Coefficient|  p|
|:--------------------------------------------|-----------:|--:|
|outcome_typelinear:estimateMean:value        |       10.85|  0|
|outcome_typelinear:estimateMedian:value      |       10.84|  0|
|outcome_typelinear:estimateMAP:value         |       10.72|  0|
|outcome_typelinear:estimateCoefficient:value |       10.54|  0|
|outcome_typebinary:estimateMAP:value         |        4.39|  0|
|outcome_typebinary:estimateMedian:value      |        4.28|  0|
|outcome_typebinary:estimateMean:value        |        4.21|  0|
|outcome_typebinary:estimateCoefficient:value |        3.87|  0|

<!-- REMOVE THIS TABLE ONCE NEW PARAMETERS IS ON CRAN SO IT CAN BE GENERATED ON THE RUN-->

This suggests that, in order to delineate between the presence and the absence of an effect, compared to the frequentist's beta:

- For linear models, the **Mean** was the better predictor, closely followed by the **Median**, the **MAP** and the frequentist **Coefficient**.
- For logistic models, the **MAP** was the better predictor, followed by the **Median**, the **Mean** and, behind, the frequentist **Coefficient**.

Overall, the **median** seems to be appears as a safe and approriate choice, maintaining a a high performance accross different types of models.

<!-- ```{r, message=FALSE, warning=FALSE} -->
<!-- df %>% -->
<!--   select(sample_size, error, true_effect, outcome_type, beta, Median, Mean, MAP) %>% -->
<!--   gather(estimate, value, -sample_size, -error, -true_effect, -outcome_type) %>% -->
<!--   glm(true_effect ~ outcome_type / value * estimate * sample_size * error, data=., family="binomial") %>% -->
<!--   broom::tidy() %>% -->
<!--   select(term, estimate, p=p.value) %>% -->
<!--   filter(stringr::str_detect(term, 'outcome_type'), -->
<!--          stringr::str_detect(term, ':value')) %>% -->
<!--   mutate( -->
<!--     sample_size = stringr::str_detect(term, 'sample_size'), -->
<!--     error = stringr::str_detect(term, 'error'), -->
<!--     term = stringr::str_remove(term, "estimate"), -->
<!--     term = stringr::str_remove(term, "outcome_type"), -->
<!--     p = paste0(sprintf("%.2f", p), ifelse(p < .001, "***", ifelse(p < .01, "**", ifelse(p < .05, "*", ""))))) %>% -->
<!--   arrange(sample_size, error, term) %>%  -->
<!--   select(-sample_size, -error) %>%  -->
<!--   knitr::kable(digits=2)  -->
<!-- ``` -->


<!-- This suggests that, in order to delineate between the presence and the absence of an effect, compared to the frequentist's beta: -->

<!-- - For linear Models; -->

<!--   - The **mean**, followed closely by the **median**, and the **MAP** estimate had a superior performance, altough not significantly. -->
<!--   - The **mean**, followed closely by the **median**, and the **MAP** estimate, were less affected by noise, altough not significantly. -->
<!--   - No difference for the sensitivity to sample size was found. -->

<!-- - For logistic models: -->

<!--   - The **MAP** estimate, followed by the **median** and the **mean**, estimate had a superior performance. -->
<!--   - The **MAP** estimate, followed by the **median**, and the **mean**, were less affected by noise, altough not significantly. -->
<!--   - The **MAP** estimate, followed by the **mean**, and the **median**, were less affected by sample size, altough not significantly. -->


## Experiment 2: Relationship with Sampling Characteristics



### Methods

The simulation aimed at modulating the following characteristics:

- **Model type**: linear or logistic.
- **"True" effect** (original regression coefficient from which data is drawn): Can be 1 or 0 (no effect).
- **draws**: from 10 to 5000 by step of 5 (1000 iterations).
- **warmup**: Ratio of warmup iterations. from 1/10 to 9/10 by step of 0.1 (9 iterations).

We generated 3 datasets for each combination of these characteristics, resulting in a total of `2 * 2 * 8 * 40 * 9 * 3 = 34560` Bayesian and frequentist models. The code used for generation is avaible [here](https://easystats.github.io/circus/articles/bayesian_indices.html) (please note that it takes usually several days/weeks to complete).

```{r message=FALSE, warning=FALSE}
df <- read.csv("https://raw.github.com/easystats/circus/master/data/bayesSim_study2.csv")
```

### Results


#### Sensitivity to number of iterations

```{r, message=FALSE, warning=FALSE}
df %>%
  select(iterations, true_effect, outcome_type, beta, Median, Mean, MAP) %>%
  gather(estimate, value, -iterations, -true_effect, -outcome_type) %>%
  mutate(temp = as.factor(cut(iterations, 5, labels = FALSE))) %>% 
  group_by(temp) %>% 
  mutate(iterations_group = round(mean(iterations), 1)) %>% 
  ungroup() %>% 
  filter(value < 6) %>%
  ggplot(aes(x = iterations_group, y = value, fill = estimate, group = interaction(estimate, iterations_group))) +
  geom_boxplot(outlier.shape=NA) +
  theme_classic() +
  scale_fill_manual(values = c("beta" = "#607D8B", "MAP" = "#795548", "Mean" = "#FF9800", "Median" = "#FFEB3B"),
                    name = "Index") +
  ylab("Point-estimate of the true value 0\n") +
  xlab("\nNumber of Iterations") +
  facet_wrap(~ outcome_type * true_effect, scales="free") 
```


#### Sensitivity to warmup ratio

```{r, message=FALSE, warning=FALSE}
df %>%
  mutate(warmup = warmup / iterations) %>% 
  select(warmup, true_effect, outcome_type, beta, Median, Mean, MAP) %>%
  gather(estimate, value, -warmup, -true_effect, -outcome_type) %>%
  mutate(temp = as.factor(cut(warmup, 3, labels = FALSE))) %>% 
  group_by(temp) %>% 
  mutate(warmup_group = round(mean(warmup), 1)) %>% 
  ungroup() %>% 
  filter(value < 6) %>% 
  ggplot(aes(x = warmup_group, y = value, fill = estimate, group = interaction(estimate, warmup_group))) +
  geom_boxplot(outlier.shape=NA) +
  theme_classic() +
  scale_fill_manual(values = c("beta" = "#607D8B", "MAP" = "#795548", "Mean" = "#FF9800", "Median" = "#FFEB3B"),
                    name = "Index") +
  ylab("Point-estimate of the true value 0\n") +
  xlab("\nNumber of Iterations") +
  facet_wrap(~ outcome_type * true_effect, scales="free") 
```

## Experiment 3: Relationship with Priors Specification


## Discussion

Conclusions can be found in the [guidelines section](https://easystats.github.io/bayestestR/articles/guidelines.html).
---
title: "3. Become a Bayesian master"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Example 3: Become a Bayesian master}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
knitr::opts_chunk$set(dpi=150)
options(digits=2)

set.seed(333)
```

```{r echo=FALSE, fig.cap="Yoda Bayes (896 BBY - 4 ABY).", fig.align='center', out.width="80%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/YodaBayes.jpg")
```




## Mixed Models

TO BE CONTINUED.

### Priors

TO BE CONTINUED.


## What's next?

The journey to become a true Bayesian master is not over. It is merely the beginning. It is now time to leave the `bayestestR` universe and apply the Bayesian framework in a variety of other statistical contexts: 

- [**Marginal means**](https://easystats.github.io/modelbased/articles/estimate_means.html)
- [**Contrast analysis**](https://easystats.github.io/modelbased/articles/estimate_contrasts.html)
- [**Testing Contrasts from Bayesian Models with 'emmeans' and 'bayestestR'**](https://easystats.github.io/blog/posts/bayestestr_emmeans/)
---
title: "Reporting Guidelines"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  %\VignetteIndexEntry{Reporting Guidelines}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)
```


This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541
- Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \& Lüdecke, D. (2019). *Indices of Effect Existence and Significance in the Bayesian Framework*. Frontiers in Psychology 2019;10:2767. [10.3389/fpsyg.2019.02767](https://doi.org/10.3389/fpsyg.2019.02767)

---

# Reporting Guidelines

## How to describe and report the parameters of a model

A Bayesian analysis returns a posterior distribution for each parameter (or *effect*). To minimally describe these distributions, we recommend reporting a point-estimate of [centrality](https://en.wikipedia.org/wiki/Central_tendency) as well as information characterizing the estimation uncertainty (the [dispersion](https://en.wikipedia.org/wiki/Statistical_dispersion)). Additionally, one can also report indices of effect existence and/or significance.

Based on the previous [**comparison of point-estimates**](https://easystats.github.io/bayestestR/articles/indicesEstimationComparison.html) and [**indices of effect existence**](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full), we can draw the following recommendations.

#### **Centrality**

We suggest reporting the [**median**](https://easystats.github.io/bayestestR/reference/point_estimate.html) as an index of centrality, as it is more robust compared to the [mean](https://easystats.github.io/bayestestR/reference/point_estimate.html) or the [MAP estimate](https://easystats.github.io/bayestestR/reference/map_estimate.html). However, in case of severly skewed posterior distributions, the MAP estimate could be a good alternative.
 

#### **Uncertainty** 

The [**89\% Credible Interval (CI)**](https://easystats.github.io/bayestestR/articles/credible_interval.html) appears as a reasonable range to characterize the uncertainty related to the estimation, being more stable than higher thresholds (such as 90\% and 95\%). We also recommend computing the CI based on the [HDI](https://easystats.github.io/bayestestR/reference/hdi.html) rather than [quantiles](https://easystats.github.io/bayestestR/reference/ci.html), favouring probable, - over central - values. 

Note that a CI based on the quantile (equal-tailed interval) might be more appropriate in case of transformations (for instance when transforming log-odds to probabilities). Otherwise, intervals that originally do not cover the null might cover it after transformation (see [here](https://easystats.github.io/bayestestR/articles/credible_interval.html#different-types-of-cis)).


#### **Existence** 

```{r echo=FALSE, fig.cap="Reviewer 2 (circa a long time ago in a galaxy far away).", fig.align='center', out.width="60%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/watto.jpg")
```


The Bayesian framework can neatly delineate and quantify different aspects of hypothesis testing, such as effect *existence* and *significance*. The most straightforward index to describe effect existence is the [**Probability of Direction (pd)**](https://easystats.github.io/bayestestR/articles/probability_of_direction.html), representing the certainty associated with the most probable direction (positive or negative) of the effect. This index is easy to understand, simple to interpret, straightforward to compute, robust to model characteristics and independent from the scale of the data. 

Moreover, it is strongly correlated with the frequentist ***p*-value**, and can thus be used to draw parallels and give some reference to readers non-familiar with Bayesian statistics. A **two-sided *p*-value** of respectively `.1`, `.05`, `.01` and `.001` would correspond approximately to a ***pd*** of 95\%, 97.5\%, 99.5\% and 99.95\%. Thus, for convenience, we suggest the following reference values as an interpretation helpers:

  - *pd* **\<= 95\%** ~ *p* \> .1: uncertain
  - *pd* **\> 95\%** ~ *p* \< .1: possibly existing
  - *pd* **\> 97\%**: likely existing
  - *pd* **\> 99\%**: probably existing
  - *pd* **\> 99.9\%**: certainly existing

#### **Significance** 

The percentage in **ROPE** is a index of **significance** (in its primary meaning), informing us whether a parameter is related - or not - to a non-negligible change (in terms of magnitude) in the outcome. We suggest reporting the **percentage of the full posterior distribution** (the *full* ROPE) instead of a given proportion of CI, in the ROPE, which appears as more sensitive (especially to delineate highly significant effects). Rather than using it as a binary, all-or-nothing decision criterion, such as suggested by the original [equivalence test](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html#equivalence-test), we recommend using the percentage as a continuous index of significance. However, based on [simulation data](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full), we suggest the following reference values as an interpretation helpers:


  - **\> 99\%** in ROPE: negligible (we can accept the null hypothesis)
  - **\> 97.5\%** in ROPE: probably negligible
  - **\<= 97.5\%** \& **\>= 2.5\%** in ROPE: undecided significance
  - **\< 2.5\%** in ROPE: probably significant
  - **\< 1\%** in ROPE: significant (we can reject the null hypothesis)


*Note that extra caution is required as its interpretation highly depends on other parameters such as sample size and ROPE range (see [here](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html#sensitivity-to-parameters-scale))*.


#### **Template Sentence** 

Based on these suggestions, a template sentence for minimal reporting of a parameter based on its posterior distribution could be:

- "the effect of *X* has a probability of ***pd*** of being *negative* (Median = ***median***, 89\% CI [ ***HDI<sub>low</sub>*** , ***HDI<sub>high</sub>*** ] and can be considered as *significant* (***ROPE***\% in ROPE)."

## How to compare different models

Altough it can also be used to assess effect existence and signficance, the **Bayes factor (BF)** is a versatile index that can be used to directly compare different models (or data generation processes). The [Bayes factor](https://easystats.github.io/bayestestR/articles/bayes_factors.html) is a ratio, informing us by how much more (or less) likely the observed data are under two compared models - usually a model with an effect vs. a model *without* the effect. Depending on the specifications of the null model (whether it is a point-estimate (e.g., **0**) or an interval), the Bayes factor could be used both in the context of effect existence and significance.

In general, a Bayes factor greater than 1 giving evidence in favour of one of the models, and a Bayes factor smaller than 1 giving evidence in favour of the other model. Several rules of thumb exist to help the interpretation (see [here](https://easystats.github.io/report/articles/interpret_metrics.html#bayes-factor-bf)), with **\> 3** being one common treshold to categorize non-anecdotal evidence.
 

#### **Template Sentence** 

When reporting Bayes factors (BF), one can use the following sentence: 

- "There is *moderate evidence* in favour of an *absence* of effect of *x* (BF = *BF*)."





*Note: If you have any advice, opinion or such, we encourage you to let us know by opening an [discussion thread](https://github.com/easystats/bayestestR/issues) or making a pull request.*
---
title: "Bayes Factors"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, bayes factors]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Bayes Factors}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the following:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541
- Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \& Lüdecke, D. (2019). *Indices of Effect Existence and Significance in the Bayesian Framework*. Retrieved from [10.3389/fpsyg.2019.02767](https://doi.org/10.3389/fpsyg.2019.02767)

---

```{r setup, include=FALSE}
if (!requireNamespace("rstanarm", quietly = TRUE) ||
      !requireNamespace("BayesFactor", quietly = TRUE) ||
      !requireNamespace("emmeans", quietly = TRUE) ||
      !requireNamespace("logspline", quietly = TRUE) ||
      !requireNamespace("lme4", quietly = TRUE) ||
      !requireNamespace("ggplot2", quietly = TRUE) ||
      !requireNamespace("see", quietly = TRUE)
      ) {
  knitr::opts_chunk$set(eval = FALSE)
} else {
  library(knitr)
  library(insight)
  library(bayestestR)
  library(rstanarm)
  library(BayesFactor)
  library(emmeans)
  
  library(ggplot2)
  library(see)
  
  options(knitr.kable.NA = '',
          digits = 2)
  opts_chunk$set(echo = TRUE,
                 comment = ">",
                 message = FALSE,
                 warning = FALSE,
                 dpi = 150)
  theme_set(theme_modern())
  set.seed(4)
}
```

The adoption of the Bayesian framework for applied statistics, especially in the social and psychological sciences, seems to be developing in two distinct directions. One of the key topics marking their separation is their opinion about **the Bayes factor**. In short, some authors (e.g., the "Amsterdam school", led by [Wagenmakers](https://www.bayesianspectacles.org/)) advocate its use and emphasize its qualities as a statistical index, while others point to its limits and prefer, instead, the precise description of posterior distributions (using [CIs](https://easystats.github.io/bayestestR/reference/hdi.html), [ROPEs](https://easystats.github.io/bayestestR/reference/rope.html), etc.). 

**bayestestR** does not take a side in this debate, rather offering tools to help you in whatever analysis you want to achieve. Instead, it strongly supports the notion of an *informed choice:* **discover the methods, try them, understand them, learn about them, and decide for yourself**.

Having said that, here's an introduction to Bayes factors :)


# The Bayes Factor

**Bayes factors (BFs) are indices of *relative* evidence of one "model" over another**, which can be used in the Bayesian framework as alternatives to classical (frequentist) hypothesis testing indices (such as $p-values$).

According to Bayes' theorem, we can update prior probabilities of some model $M$ ($P(M)$) to posterior probabilities ($P(M|D)$) after observing some datum $D$ by accounting for the probability of observing that datum given the model ($P(D|M)$, also known as the *likelihood*):

$$
P(M|D) = \frac{P(D|M)\times P(M)}{P(D)}
$$

Using this equation, We can compare the probability-odds of two models:

$$
\frac{P(M_1|D)}{P(M_2|D)} = \frac{P(D|M_1)}{P(D|M_2)} \times \frac{P(M_1)}{P(M_2)}
$$
Where the left-most term are the *posterior odds*, the right-most term are the *prior odds*, and the middle term is the *Bayes factor*:

$$
BF_{12}=\frac{P(D|M_1)}{P(D|M_2)}
$$

Thus, Bayes factors can be seen either as a ratio quantifying ***the relative probability of some observed data by two models*** as they can be computed by comparing the marginal likelihoods of the two models, or as ***the degree by which some prior beliefs about the relative credibility of two models are to be updated*** as they can be computed by dividing posterior odds by prior odds, as we will soon demonstrate.

Here we provide functions for computing Bayes factors in two different applications: **testing single parameters (coefficients) within a model** and **comparing statistical models themselves**.

## Testing Models' Parameters with Bayes Factors {#bayesfactor_parameters}

A ***Bayes factor for a single parameter*** can be used to answer the question:

> **Given the observed data, has the null hypothesis of an absence of an effect become more, or less credible?**


```{r deathsticks_fig, echo=FALSE, fig.cap="Bayesian analysis of the Students' (1908) Sleep data set.", fig.align='center', out.width="80%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/deathsticks.jpg")
```

Let's use the Students' (1908) Sleep data set (`data("sleep")`), in which **people took some drug** and where the researchers measured the **extra hours of sleep** that they slept afterwards. We will try answering the following question: 

*given the observed data, has the hypothesis that the drug (the effect of `group`) **has no effect** on the numbers of hours of **extra sleep** (variable `extra`) become more of less credible?*

```{r sleep_boxplot, echo=FALSE}
ggplot(sleep, aes(x = group, y = extra, fill= group)) +
  geom_boxplot() +
  theme_classic()
```

The **bloxplot** suggests that the second group has a higher number of hours of extra sleep. *By how much?* Let's fit a simple [Bayesian linear model](https://easystats.github.io/bayestestR/articles/example1_GLM.html), with a prior of $b_{group} \sim N(0, 3)$:

```{r rstanarm_model, eval = FALSE}
library(rstanarm)

model <- stan_glm(extra ~ group, data = sleep,
                  prior = normal(0, 3, autoscale = FALSE))
```

```{r, echo=FALSE}
model <- stan_glm(extra ~ group, data = sleep,
                  prior = normal(0, 3, autoscale = FALSE),
                  refresh = 0)
```

### Testing against a null-*region*

One way of operationlizing the null-hypothesis is by setting a null region, such that an effect that falls within this interval would be practically equivalent to the the null [@kruschke2010believe]. In our case, that means defining a range of effects we would consider equal to the drug having no effect at all. We can then compute the prior probability of the drug's effect falling *within this null-region*, and the prior probability of the drug's effect falling *outside the null-region* to get our *prior odds*. Say any effect smaller than an hour of extra sleep is practically equivalent to being no effect at all, we would define our prior odds as:

$$
\frac
{P(b_{drug} \in [-1, 1])}
{P(b_{drug} \notin [-1, 1])}
$$

Given our prior has a normal distribution centered at 0 hours with a scale (an SD) of 2.5 hours, our priors would look like this:

```{r, echo=FALSE}
null <- c(-1,1)
xrange <- c(-10,10)

x_vals <- seq(xrange[1], xrange[2], length.out = 400)
d_vals <- dnorm(x_vals, sd = 3)
in_null <- null[1] < x_vals & x_vals < null[2]
range_groups <- rep(0, length(x_vals))
range_groups[!in_null & x_vals < 0] <- -1
range_groups[!in_null & x_vals > 0] <- 1

ggplot(mapping = aes(x_vals, d_vals, fill = in_null, group = range_groups)) +
  geom_area(color = "black", size = 1) +
  scale_fill_flat(name = "", labels = c("Alternative", "Null")) + 
  labs(x = "Drug effect", y = "Density") + 
  theme_modern() + 
  theme(legend.position = c(0.2, 0.8))

pnull <- diff(pnorm(null, sd = 2.5))
prior_odds <- (1 - pnull) / pnull
```

and the prior odds would be 2.2.

By looking at the posterior distribution, can now compute the posterior probability of the drug's effect falling *within the null-region*, and the posterior probability of the drug's effect falling *outside the null-region* to get our *posterior odds*:

$$
\frac
{P(b_{drug} \in [-1,1] | Data)}
{P(b_{drug} \notin [-1,1] | Data)}
$$

```{r rstanarm_fit, echo=FALSE}

model_prior <- bayestestR:::.update_to_priors.stanreg(model)
posterior <- insight::get_parameters(model)$group2
prior <- insight::get_parameters(model_prior)$group2

f_post <- logspline::logspline(posterior)

d_vals_post <- logspline::dlogspline(x_vals,f_post)

ggplot(mapping = aes(x_vals, d_vals_post, fill = in_null, group = range_groups)) +
  geom_area(color = "black", size = 1) +
  scale_fill_flat(name = "", labels = c("Alternative", "Null")) + 
  labs(x = "Drug effect", y = "Density") + 
  theme_modern() + 
  theme(legend.position = c(0.2, 0.8))

My_first_BF <- bayesfactor_parameters(model, null = c(-1,1), prior = model_prior)

BF <- My_first_BF$BF[2]
post_odds <- prior_odds * BF

med_post <- point_estimate(posterior)$Median
```

We can see that the center of the posterior distribution has shifted away from 0 (to ~1.5). Likewise, the posterior odds are 2 - which seems to favor **the effect being non-null**, but... *does this mean the data support the alternative over the null?* Hard to say, since even before the data were observed, the priors already favored the alternative - so we need to take our priors into account here!

Let's compute the Bayes factor as the change from the prior odds to the posterior odds: $BF_{10} = Odds_{posterior} / Odds_{prior} = 0.9$! This BF indicates that the data provide 1/0.9 = 1.1 times more evidence for the effect of the drug being practically nothing than it does for the drug having some clinically significant effect. Thus, although the center of distribution has shifted away from 0, and the posterior distribution seems to favor a non-null effect of the drug, it seems that given the observed data, the probability mass has *overall* shifted closer to the null interval, making the values in the null interval more probable! [see *Non-overlapping Hypotheses* in @morey2011bayesinterval]

Note that **interpretation guides** for Bayes factors can be found [**here**](https://easystats.github.io/report/articles/interpret_metrics.html#bayes-factor-bf).  
  
All of this can be achieved with the function `bayesfactor_parameters()`, which computes a Bayes factor for each of the model's parameters:

```{r, eval=FALSE}
My_first_BF <- bayesfactor_parameters(model, null = c(-1, 1))
My_first_BF
```

```{r, echo=FALSE}
print(My_first_BF)
```

We can also plot using the `see` package:

```{r}
library(see)
plot(My_first_BF)
```

### Testing against the *point*-null (0)

**What if we don't know what region would be practically equivalent to 0?** Or if we just want the null to be exactly zero? Not a problem - as the width of null region shrinks to a point, the change from the prior probability to the posterior probability of the null can be estimated by comparing the the density of the null value between the two distributions.^[Note that as the width of null interval shrinks to zero, the prior probability and posterior probability of the alternative tends towards 1.00.] This ratio is called the **Savage-Dickey ratio**, and has the added benefit of also being an approximation of a Bayes factor comparing the estimated model against the a model in which the parameter of interest has been restricted to a point-null:

> "[...] the Bayes factor for $H_0$ versus $H_1$ could be obtained by analytically integrating out the model parameter $\theta$. However, the Bayes factor may likewise be obtained by only considering $H_1$, and dividing the height of the posterior for $\theta$ by the height of the prior for $\theta$, at the point of interest." [@wagenmakers2010bayesian]

```{r, eval=FALSE}
My_second_BF <- bayesfactor_parameters(model, null = 0)
My_second_BF
```

```{r, echo=FALSE}
My_second_BF <- bayesfactor_parameters(
  data.frame(group2 = posterior),
  data.frame(group2 = prior),
  null = 0)

print(My_second_BF)
```

```{r}
plot(My_second_BF)
```

### One-sided tests

We can also conduct a directional test (a "one sided" or "one tailed" test) if we have a prior hypotheses about the direction of the effect. This can be done by setting an order restriction on the prior distribution (and thus also on the posterior distribution) of the alternative [@morey2014simple]. For example, if we have a prior hypothesis that the effect of the drug is an *increase* in the number of sleep hours, the alternative will be restricted to the region to the right of the null (point or interval):

```{r savagedickey_one_sided, eval=FALSE}
test_group2_right <- bayesfactor_parameters(model, direction = ">")
test_group2_right
```

```{r prior_n_post_plot_one_sided, echo=FALSE}
test_group2_right <- bayesfactor_parameters(
  data.frame(group2 = posterior),
  data.frame(group2 = prior),
  null = 0,
  direction = ">"
)

BF <- test_group2_right$BF

print(test_group2_right)
```

```{r}
plot(test_group2_right)
```

As we can see, given that we have an *a priori* assumption about the direction of the effect (*that the effect is positive*), **the presence of an effect is 2.8 times more likely than the absence of an effect**, given the observed data (or that the data are 2.8 time more probable under $H_1$ than $H_0$). This indicates that, given the observed data, and a priori hypothesis, the posterior mass has shifted away from the null value, giving some evidence against the null (note that a Bayes factor of 2.8 is still considered quite [weak evidence](https://easystats.github.io/report/articles/interpret_metrics.html#bayes-factor-bf)).

**NOTE**: See the *Testing Contrasts* appendix below.

### Support intervals {#si}

So far we've seen that Bayes factors quantify relative support between competing hypotheses. However, we can also ask:

> **Upon observing the data, the credibility of which of the parameter’s values has increased (or decreased)?**

For example, we've seen that the point null has become somewhat less credible after observing the data, but we might also ask *which values have gained some credibility given the observed data?*. The resulting range of values is called **the support interval** as it indicates which values are supported by the data [@wagenmakers2018SI]. We can do this by once again comparing the prior and posterior distributions and checking where the posterior densities are higher than the prior densities. This can be achieved with the `si()` function:

```{r, eval=FALSE}
my_first_si <- si(model, BF = 1)
my_first_si
```

```{r, echo=FALSE}
my_first_si <- si(data.frame(group2 = posterior),
                  data.frame(group2 = prior),
                  BF = 1)

print(my_first_si)
```

The argument `BF = 1` indicates that we want the interval to contain values that have gained support by a factor of at least 1 (that is, any support at all).

Visually, we can see that the credibility of all the values within this interval has increased (and likewise the credibility of all the values outside this interval has decreased):

```{r}
plot(my_first_si)
```

We can also see the this support interval (just barely) excludes the point null (0) - whose credibility we've already seen has decreased by the observed data. This emphasizes the relationship between the support interval and the Bayes factor:

> "The interpretation of such intervals would be analogous to how a frequentist confidence interval contains all the parameter values that would not have been rejected if tested at level $\alpha$. For instance, a BF = 1/3 support interval encloses all values of theta for which the updating factor is not stronger than 3 against." [@wagenmakers2018SI]

Thus, the choice of BF (the level of support the interval should indicate) depends on what we want our interval to represent:

- A $BF = 1$ contains values whose credibility has merely not decreased by observing the data.  
- A $BF > 1$ contains values who received more impressive support from the data.  
- A $BF < 1$ contains values whose credibility has *not* been impressively decreased by observing the data. Testing against values outside this interval will produce a Bayes factor larger than $1/BF$ in support of the alternative.

## Comparing Models using Bayes Factors {#bayesfactor_models}

Bayes factors can also be used to compare statistical models, for which they answer the question:

> **Under which model are the the observed data more probable?**

In other words, which model is more likely to have produced the observed data? This is usually done by comparing the marginal likelihoods of two models. In such a case, the Bayes factor is a measure of the *relative* evidence of one of the compared models over the other.

Let's use Bayes factors for model comparison to find a model that best describes the length of an iris' sepal using the `iris` data set.

### For Bayesian models (`brms` and `rstanarm`)

**Note: In order to compute Bayes factors for Bayesian models, non-default arguments must be added upon fitting:**
  
  - `brmsfit` models **must** have been fitted with `save_all_pars = TRUE`
  - `stanreg` models **must** have been fitted with a defined `diagnostic_file`.
  
Let's first fit 5 Bayesian regressions with `brms` to predict `Sepal.Length`:

```{r brms_disp, eval=FALSE}
library(brms)

m0 <- brm(Sepal.Length ~ 1, # intercept only model
          data = iris, save_all_pars = TRUE)
m1 <- brm(Sepal.Length ~ Petal.Length,
          data = iris, save_all_pars = TRUE)
m2 <- brm(Sepal.Length ~ Species,
          data = iris, save_all_pars = TRUE)
m3 <- brm(Sepal.Length ~ Species + Petal.Length,
          data = iris, save_all_pars = TRUE)
m4 <- brm(Sepal.Length ~ Species * Petal.Length,
          data = iris, save_all_pars = TRUE)
```

We can now compare these models with the `bayesfactor_models()` function, using the `denominator` argument to specify which model all models will be compared against (in this case, the intercept-only model):

```{r brms_models_disp, eval=FALSE}
library(bayestestR)
comparison <- bayesfactor_models(m1, m2, m3, m4, denominator = m0)
comparison
```

```{r brms_models_print, echo=FALSE}
comparison <- structure(
  list(
    Model = c(
      "Petal.Length",
      "Species",
      "Species + Petal.Length",
      "Species * Petal.Length",
      "1"
    ),
    BF = c(3.44736e+44, 5.628679e+29, 7.121386e+55, 9.149948e+55, 1)
  ),
  class = c("bayesfactor_models", "see_bayesfactor_models", "data.frame"),
  row.names = c(NA, -5L),
  denominator = 5L,
  BF_method = "marginal likelihoods (bridgesampling)", unsupported_models = FALSE
)

print(comparison)
```

We can see that the full model is the best model - with $BF_{\text{m0}}=9\times 10^{55}$ compared to the null (intercept only).

Due to the transitive property of Bayes factors, we can easily change the reference model to the main effects model:

```{r update_models1}
update(comparison, reference = 3)
```

As we can see, though the full model is the best, there is hardly any evidence that it is preferable to the main effects model.

We can also change the reference model to the `Species` model:

```{r update_models2}
update(comparison, reference = 2)
```

Notice that in the Bayesian framework the compared models *do not* need to be nested models, as happened here when we compared the `Petal.Length`-only model to the `Species`-only model (something that cannot be done in the frequentists framework, where compared models must be nested in one another).

**NOTE:** In order to correctly and precisely estimate Bayes Factors, you always need the 4 P's: **P**roper **P**riors ^[[Robert, 2016](https://doi.org/10.1016/j.jmp.2015.08.002); [Kass & Raftery, 1993](https://doi.org/10.1080/01621459.1995.10476572); [Fernández, Ley, & Steel, 2001](https://doi.org/10.1016/S0304-4076(00)00076-2)], and a **P**lentiful **P**osterior ^[[Gronau, Wagenmakers, Heck, & Matzke, 2019](https://doi.org/10.1007/s11336-018-9648-3)].

### For Frequentist models via the BIC approximation 

It is also possible to compute Bayes factors for the comparison of frequentist models. This is done by comparing BIC measures, allowing a Bayesian comparison of non-nested frequentist models [@wagenmakers2007practical]. Let's try it out on some **linear mixed models**:


```{r lme4_models, eval=FALSE}
library(lme4)

m0 <- lmer(Sepal.Length ~ (1 | Species), data = iris)
m1 <- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
m2 <- lmer(Sepal.Length ~ Petal.Length + (Petal.Length | Species), data = iris)
m3 <- lmer(Sepal.Length ~ Petal.Length + Petal.Width + (Petal.Length | Species), data = iris)
m4 <- lmer(Sepal.Length ~ Petal.Length * Petal.Width + (Petal.Length | Species), data = iris)

bayesfactor_models(m1, m2, m3, m4, denominator = m0)
```

```{r, echo=FALSE}
structure(list(Model = c(
  "Petal.Length + (1 | Species)",
  "Petal.Length + (Petal.Length | Species)", 
  "Petal.Length + Petal.Width + (Petal.Length | Species)",
  "Petal.Length * Petal.Width + (Petal.Length | Species)", 
  "1 + (1 | Species)"),
  BF = c(8.24027869011648e+24, 4.7677519818206e+23, 
         1.52492156042604e+22, 5.93045520305254e+20, 1)),
  class = c("bayesfactor_models", "see_bayesfactor_models", "data.frame"),
  row.names = c(NA, -5L),
  denominator = 5L,
  BF_method = "BIC approximation", unsupported_models = FALSE)
```

### Order restricted models {#bayesfactor_restricted}

As stated above when discussing one-sided hypothesis tests, we can create new models by imposing order restrictions on a given model. For example, consider the following model, in which we predict the length of an iris' sepal from the length of its petal, as well as from its species, with a prior of $b_{petal} \sim N(0,2)$ $b_{versicolors}\ \&\  b_{virginica} \sim N(0,1.2)$:

```{r, eval=FALSE}
iris_model <- stan_glm(Sepal.Length ~ Species + Petal.Length,
                       data = iris,
                       prior = normal(0, c(2, 1.2, 1.2), autoscale = FALSE))
```

```{r, echo=FALSE}
iris_model <- stan_glm(Sepal.Length ~ Species + Petal.Length,
                       data = iris,
                       prior = normal(0, c(2, 1.2, 1.2), autoscale = FALSE),
                       refresh = 0)
```

These priors are **unrestricted** - that is, all values between $-\infty$ and $\infty$ of all parameters in the model have some non-zero credibility (no matter how small; this is true for both the prior and posterior distribution). Subsequently, *a priori* the ordering of the parameters relating to the iris species can have any ordering, such that (a priori) setosa can have larger sepals than virginica, but it is also possible for virginica to have larger sepals than setosa!

Does it make sense to let our priors cover all of these possibilities? That depends on our *prior* knowledge or hypotheses. For example, even a novice botanist will assume that it is unlikely that petal length will be *negatively* associated with sepal length - an iris with longer petals is likely larger, and thus will also have a longer sepal. And an expert botanist will perhaps assume that setosas have smaller sepals than both versicolors and virginica. These priors can be formulated as **restricted** priors [@morey_2015_blog; @morey2011bayesinterval]:

1. The novice botanist: $b_{petal} > 0$
2. The expert botanist: $b_{versicolors} > 0\ \&\ b_{virginica} > 0$

By testing these restrictions on prior and posterior samples, we can see how the probabilities of the restricted distributions change after observing the data. This can be achieved with `bayesfactor_restricted()`, that compute a Bayes factor for these restricted model vs the unrestricted model. Let's first specify these restrictions as logical conditions:

```{r}
botanist_hypotheses <- c(
  "Petal.Length > 0",
  "(Speciesversicolor > 0) & (Speciesvirginica > 0)"
)
```

Let's test these hypotheses:
```{r, eval=FALSE}
botanist_BFs <- bayesfactor_restricted(iris_model, hypothesis = botanist_hypotheses)
botanist_BFs
```

```{r, echo=FALSE}

model_prior <- bayestestR:::.update_to_priors.stanreg(iris_model)

botanist_BFs <- bayesfactor_restricted(iris_model, prior = model_prior, 
                                       hypothesis = botanist_hypotheses)
print(botanist_BFs)
```

We can see that the novice botanist's hypothesis gets a Bayes factor of ~2, indicating the data provides twice as much evidence for a model in which petal length is restricted to be positively associated with sepal length than for a model with not such restriction.

What about our expert botanist? He seems to have failed miserably, with a BF favoring the *unrestricted* model many many times over ($BF\gg1,000$). How is this possible? It seems that when *controlling for petal length*, versicolor and virginica actually have shorter sepals!

```{r plot_iris, echo=FALSE}
ggplot(iris, aes(Petal.Length, Sepal.Length, color = Species)) + 
  geom_point() + 
  scale_color_flat() + 
  theme(legend.position = c(0.2, 0.8))
```

Note that these Bayes factors compare the restricted model to the unrestricted model. If we wanted to compare the restricted model to the null model, we could use the transitive property of Bayes factors like so:

$$
BF_{restricted / NULL} = \frac
{BF_{restricted / un-restricted}}
{BF_{un-restricted / NULL}}
$$

**Because these restrictions are on the prior distribution, they are only appropriate for testing pre-planned (*a priori*) hypotheses, and should not be used for any post hoc comparisons [@morey_2015_blog].**

**NOTE**: See the *Specifying Correct Priors for Factors with More Than 2 Levels* appendix below.

## Bayesian Model Averaging

In the previous section we discussed the direct comparison of two models to determine if an effect is supported by the data. However, in many cases there are too many models to consider or perhaps it is not straightforward which models we should compare to determine if an effect is supported by the data. For such cases we can use Bayesian model averaging (BMA) to determine the support provided by the data for a parameter or term across many models.

### Inclusion Bayes factors {#bayesfactor_inclusion}

Inclusion Bayes factors answer the question:

> **Are the observed data more probable under models with a particular predictor, than they are under models without that particular predictor?**

In other words, on average - are models with predictor $X$ more likely to have produced the observed data than models without predictor $X$?^[A model without predictor $X$ can be thought of as a model in which the parameter(s) of the predictor have been restricted to a null-point of 0.]

Since each model has a prior probability, it is possible to sum the prior probability of all models that include a predictor of interest (the *prior inclusion probability*), and of all models that do not include that predictor (the *prior exclusion probability*). After the data are observed, and each model is assigned a posterior probability, we can similarly consider the sums of the posterior models' probabilities to obtain the *posterior inclusion probability* and the *posterior exclusion probability*. Once again, the change from prior inclusion odds to the posterior inclusion odds is the Inclusion Bayes factor ["$BF_{Inclusion}$"; @clyde2011bayesian].

Lets use the `brms` example from above:

```{r inclusion_brms}
bayesfactor_inclusion(comparison)
```

If we examine the interaction term's inclusion Bayes factor, we can see that across all 5 models, a model with the interaction term (`Species:Petal.Length`) is *on average* 5 times more likely than a model without the interaction term. **Note** that `Species`, a factor represented in the model with several parameters, gets a single Bayes factor - inclusion Bayes factors are given per predictor!

We can also compare only matched models - such that averaging is done only across models that (1) do not include any interactions with the predictor of interest; (2) for interaction predictors, averaging is done only across models that contain the main effect from which the interaction predictor is comprised (see explanation for why you might want to do this [here](https://www.cogsci.nl/blog/interpreting-bayesian-repeated-measures-in-jasp)).

```{r inclusion_brms2}
bayesfactor_inclusion(comparison, match_models = TRUE)
```

#### Comparison with JASP

`bayesfactor_inclusion()` is meant to provide Bayes Factors per predictor, similar to JASP's *Effects* option. Let's compare the two:

1. Across all models:

```{r JASP_all}
library(BayesFactor)
data(ToothGrowth)
ToothGrowth$dose <- as.factor(ToothGrowth$dose)

BF_ToothGrowth <- anovaBF(len ~ dose*supp, ToothGrowth, progress = FALSE)

bayesfactor_inclusion(BF_ToothGrowth)
```

```{r JASP_all_fig, echo=FALSE}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/JASP1.PNG")
```

2. Across matched models:

```{r JASP_matched}
bayesfactor_inclusion(BF_ToothGrowth, match_models = TRUE)
```


```{r JASP_matched_fig, echo=FALSE}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/JASP2.PNG")
```

3. With Nuisance Effects:

We'll add `dose` to the null model in JASP, and do the same in `R`:

```{r JASP_Nuisance}
BF_ToothGrowth_against_dose <- BF_ToothGrowth[3:4]/BF_ToothGrowth[2] # OR: 
# update(bayesfactor_models(BF_ToothGrowth),
#        subset = c(4, 5),
#        reference = 3)
BF_ToothGrowth_against_dose


bayesfactor_inclusion(BF_ToothGrowth_against_dose)
```

```{r JASP_Nuisance_fig, echo=FALSE}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/JASP3.PNG")
```

### Averaging posteriors {#weighted_posteriors}

Similar to how we can average evidence for a predictor across models, we can also average the posterior estimate across models. One situation in which this is useful in **situations where Bayes factors seem to support a null effect, yet the *HDI* of the alternative excludes the null value** (also see `si()` described above). For example, looking at Motor *Trend Car Road Tests* (`data(mtcars)`), we would naturally predict miles/gallon (`mpg`) from transition type (`am`) and weight (`wt`), but what about number of carburetors (`carb`)? Is this a good predictor?

We can determine this by comparing the following models:

```{r, eval=FALSE}
mod <- stan_glm(mpg ~ wt + am,            
                data = mtcars,
                prior = normal(0, c(10,10), autoscale = FALSE),
                diagnostic_file = file.path(tempdir(), "df1.csv"))

mod_carb <- stan_glm(mpg ~ wt + am + carb,            
                     data = mtcars,
                     prior = normal(0, c(10,10,20), autoscale = FALSE),
                     diagnostic_file = file.path(tempdir(), "df0.csv"))

bayesfactor_models(mod_carb, denominator = mod)
```

```{r, echo=FALSE}
mod <- stan_glm(mpg ~ wt + am,            
                data = mtcars,
                prior = normal(0, c(10,10), autoscale = FALSE),
                diagnostic_file = file.path(tempdir(), "df1.csv"),
                refresh = 0)

mod_carb <- stan_glm(mpg ~ wt + am + carb,            
                     data = mtcars,
                     prior = normal(0, c(10,10,20), autoscale = FALSE),
                     diagnostic_file = file.path(tempdir(), "df0.csv"),
                     refresh = 0)

BF_carb <- bayesfactor_models(mod_carb, denominator = mod, verbose = FALSE)
BF <- BF_carb$BF[1]
print(BF_carb)

```

It seems that the model without `carb` as a predictor is $1/BF=1.2$ times more likely than the model *with* `carb` as a predictor. We might then assume that in the latter model, the HDI will include the point-null value of 0 effect, to also indicate the credibility of the null in the posterior. However, this is not the case:

```{r}
hdi(mod_carb, ci = .95)
```

How can this be? By estimating the HDI of the effect for `carb` in the full model, we are acting under the assumption that that model is correct. However, as we've just seen, both models are practically tied, and in fact it was the no-`carb` model, in which the effect for `carb` is fixed at 0, that was slightly more supported by the data. If this is the case **why limit our estimation of the effect just to one model?** [@van2019cautionary].

Using Bayesian model averaging, we can combine the posteriors samples from several models, weighted by the models' marginal likelihood (done via the `bayesfactor_models()` function). If some parameter is part of some of the models but is missing from others, it is assumed to be fixed a 0 (which can also be seen as a method of applying shrinkage to our estimates). This results in a posterior distribution across several models, which we can now treat like any posterior distribution, and estimate the HDI. We can do this with the `weighted_posteriors()` function:

```{r}
BMA_draws <- weighted_posteriors(mod, mod_carb)

BMA_hdi <- hdi(BMA_draws, ci = .95)
BMA_hdi

plot(BMA_hdi)
```

We can see that across both models under consideration, the posterior of the `carb` effect is almost equally weighted between the alternative model and the null model - as represented by about half of the posterior mass concentrated at 0 - which makes sense as both models were almost equally supported by the data. We can also see that across both models, that now **the HDI does contain 0**. Thus we have resolved the conflict between the Bayes factor and the HDI [@rouder2018bayesian]!

**Note** that parameters might play different roles across different models; For example, the parameter `A` plays a different role in the model `Y ~ A + B` (where it is a main effect) than it does in the model `Y ~ A + B + A:B` (where it is a simple effect). In many cases centering of predictors (mean subtracting for continuous variables, and effects coding via `contr.sum` or orthonormal coding via [`contr.bayes`](https://easystats.github.io/bayestestR/reference/contr.bayes.html) for factors) can in some cases reduce this issue.

## Appendices

### Testing contrasts (with `emmeans` / `modelbased`)

Besides testing parameter `bayesfactor_parameters()` can be used to test any estimate based on the prior and posterior distribution of the estimate. One way to achieve this is with a mix of `bayesfactor_parameters()` + [**`emmeans`**](https://cran.r-project.org/package=emmeans) to [test Bayesian contrasts](https://easystats.github.io/blog/posts/bayestestr_emmeans/).

For example, in the `sleep` example from above, we can estimate the group means and the difference between them:

```{r, echo=FALSE}
set.seed(1)
```

```{r}
library(emmeans)

groups <- emmeans(model, ~ group)
group_diff <- pairs(groups)

(groups_all <- rbind(groups, group_diff))

# pass the original model via prior
bayesfactor_parameters(groups_all, prior = model)
```

That is strong evidence for the mean of group 1 being 0, and for group 2 for not being 0, but hardly any evidence for the difference between them being not 0. Conflict? Uncertainty? That is the Bayesian way!

We can also use the `easystats`' [**`modelbased`**](https://cran.r-project.org/package=modelbased) package to compute Bayes factors for contrasts:

```{r, echo=FALSE}
set.seed(1)
```

```{r, eval=FALSE}
library(modelbased)

estimate_contrasts(model, test = "bf")
```

**NOTE**: See the *Specifying Correct Priors for Factors with More Than 2 Levels* section below.

### Specifying correct priors for factors {#contr_bayes}

This section introduces the biased priors obtained when using the common *effects* factor coding (`contr.sum`) or dummy factor coding (`contr.treatment`), and the solution of using orthonormal factor coding (`contr.bayes`) [as outlined in @rouder2012default, section 7.2]. Specifically, ***special care should be taken when working with factors which have 3 or more levels***.

#### Contrasts (and marginal means)

The *effects* factor coding commonly used in factorial analysis carries a hidden bias when it is applies to Bayesian priors. For example, if we want to test all pairwise differences between 3 levels of the same factor, we would expect all *a priori* differences to have the same distribution, but...

For our example, we will be test all ***prior*** pairwise differences between the 3 species in the `iris` data-set.

```{r, eval=FALSE}
df <- iris
contrasts(df$Species) <- contr.sum

fit_sum <- stan_glm(Sepal.Length ~ Species, data = df,
                    prior = normal(0, c(1, 1), autoscale = FALSE),
                    prior_PD = TRUE, # sample priors
                    family = gaussian())

pairs_sum <- pairs(emmeans(fit_sum, ~ Species))
pairs_sum
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
df <- iris
contrasts(df$Species)[, ] <- contr.sum(3)

fit_sum <- stan_glm(Sepal.Length ~ Species, data = df,
                    prior = normal(0, c(1, 1), autoscale = FALSE),
                    prior_PD = TRUE, # sample priors
                    family = gaussian(),
                    refresh = 0)

pairs_sum <- pairs(emmeans(fit_sum, ~ Species))

em_pairs_samples <- as.data.frame(as.matrix(emmeans::as.mcmc.emmGrid(pairs_sum, names = FALSE)))

print(pairs_sum)

ggplot(stack(em_pairs_samples), aes(x = values, fill = ind)) + 
  geom_density(size = 1) + 
  facet_grid(ind ~ .) + 
  labs(x = "prior difference values") + 
  theme(legend.position = "none")
```

We can see that the though the prior estimate for all 3 pairwise contrasts is ~0, the scale / HDI is much more narrow for the prior of the `setosa - versicolor` contrast!

***What happened???***  
This is caused by an inherent bias in the priors introduced by the *effects* coding (it's even worse with the default treatment coding, because the prior for the intercept is usually drastically different from the effect's parameters). **And since it affects the priors, this bias will also bias the the Bayes factors over / understating evidence for some contrasts over others!**

The solution is to use *orthonormal* factor coding, a-la `contr.bayes`, which can either specify this factor coding per-factor:

```{r, eval=FALSE}
contrasts(df$Species) <- contr.bayes
```

Or you can set it globally:

```{r, eval=FALSE}
options(contrasts = c('contr.bayes', 'contr.poly'))
```

Let's again estimate the ***prior*** differences:

```{r, eval=FALSE}
fit_bayes <- stan_glm(Sepal.Length ~ Species, data = df,
                      prior = normal(0, c(1, 1), autoscale = FALSE),
                      prior_PD = TRUE, # sample priors
                      family = gaussian())

pairs_bayes <- pairs(emmeans(fit_bayes, ~ Species))
pairs_bayes
```

```{r, echo=FALSE}
contrasts(df$Species)[, ] <- contr.bayes(3)

fit_bayes <- stan_glm(Sepal.Length ~ Species, data = df,
                      prior = normal(0, c(1, 1), autoscale = FALSE),
                      prior_PD = TRUE, # sample priors
                      family = gaussian(),
                      refresh = 0)

pairs_bayes <- pairs(emmeans(fit_bayes, ~ Species))

em_pairs_samples <- as.data.frame(as.matrix(emmeans::as.mcmc.emmGrid(pairs_bayes, names = FALSE)))

print(pairs_bayes)

ggplot(stack(em_pairs_samples), aes(x = values, fill = ind)) + 
  geom_density(size = 1) + 
  facet_grid(ind ~ .) + 
  labs(x = "prior difference values") + 
  theme(legend.position = "none")
```


We can see that using this coding scheme, we have equal priors on all pairwise contrasts.

#### Order restrictions

This bias also affect order restrictions involving 3 or more levels. For example, if we want to test an order restriction among A, B, and C, the *a priori* probability of obtaining the order A > C > B is 1/6 (reach back to *intro to stats* year 1), but...

For our example, we will be interested in the following order restrictions in the `iris` data-set (each line is a separate restriction):

```{r}
hyp <- c(
  # comparing 2 levels
  "setosa < versicolor",
  "setosa < virginica",
  "versicolor < virginica",
  
  # comparing 3 (or more) levels
  "setosa    < virginica  & virginica  < versicolor",
  "virginica < setosa     & setosa     < versicolor",
  "setosa    < versicolor & versicolor < virginica"
)
```

With the default factor coding, this looks like this:

```{r, eval=FALSE}
contrasts(df$Species) <- contr.sum

fit_sum <- stan_glm(Sepal.Length ~ Species, data = df,
                    prior = normal(0, c(1, 1), autoscale = FALSE),
                    family = gaussian())

em_sum <- emmeans(fit_sum, ~ Species) # the posterior marginal means
  
bayesfactor_restricted(em_sum, fit_sum, hypothesis = hyp)
```

```{r, echo=FALSE}
contrasts(df$Species)[, ] <- contr.sum(3)

fit_sum <- stan_glm(Sepal.Length ~ Species, data = df,
                    prior = normal(0, c(1, 1), autoscale = FALSE),
                    family = gaussian(),
                    refresh = 0)

em_sum <- emmeans(fit_sum, ~ Species) # the posterior marginal means
  
bayesfactor_restricted(em_sum, fit_sum, hypothesis = hyp)
```

***What happened???***   

1. The comparison of 2 levels all have a prior of ~0.5, as expected.  
2. The comparison of 3 levels has different priors, depending on the order restriction - i.e. **some orders are *a priori* more likely than others!!!**  

Again, this is solved by using the *orthonormal* factor coding (from above).

```{r, eval=FALSE}
contrasts(df$Species) <- contr.bayes

fit_bayes <- stan_glm(Sepal.Length ~ Species, data = df,
                      prior = normal(0, c(1, 1), autoscale = FALSE),
                      family = gaussian())

em_bayes <- emmeans(fit_sum, ~ Species) # the posterior marginal means

bayesfactor_restricted(em_bayes, fit_sum, hypothesis = hyp)
```

```{r, echo=FALSE}
contrasts(df$Species)[, ] <- contr.bayes(3)

fit_bayes <- stan_glm(Sepal.Length ~ Species, data = df,
                      prior = normal(0, c(1, 1), autoscale = FALSE),
                      family = gaussian(),
                      refresh = 0)

em_bayes <- emmeans(fit_bayes, ~ Species) # the posterior marginal means
  
bayesfactor_restricted(em_bayes, fit_bayes, hypothesis = hyp)
```

#### Conclusion

When comparing the results from the two factor coding schemes, we find:  
1. In both cases, the estimated (posterior) means are quite similar (if not identical).  
2. The priors and Bayes factors differ between the two schemes.  
3. Only with `contr.bayes`, the prior distribution of the difference or the order of 3 (or more) means is balanced.

# References---
title: "Probability of Direction (pd)"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Probability of Direction (pd)}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541
- Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \& Lüdecke, D. (2019). *Indices of Effect Existence and Significance in the Bayesian Framework*. Frontiers in Psychology 2019;10:2767. [10.3389/fpsyg.2019.02767](https://doi.org/10.3389/fpsyg.2019.02767)

---

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("see", quietly = TRUE) ||
    !requireNamespace("dplyr", quietly = TRUE) ||
    !requireNamespace("ggplot2", quietly = TRUE) ||
    !requireNamespace("tidyr", quietly = TRUE) ||
    !requireNamespace("logspline", quietly = TRUE) ||
    !requireNamespace("KernSmooth", quietly = TRUE) ||
    !requireNamespace("tidyr", quietly = TRUE) ||
    !requireNamespace("GGally", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

set.seed(333)
```

# What is the *pd?*

The **Probability of Direction (pd)** is an index of **effect existence**, ranging from 50\% to 100\%, representing the certainty with which an effect goes in a particular direction (*i.e.*, is positive or negative). 

Beyond its **simplicity of interpretation, understanding and computation**, this index also presents other interesting properties:

- It is **independent from the model**: It is solely based on the posterior distributions and does not require any additional information from the data or the model.
- It is **robust** to the scale of both the response variable and the predictors.
- It is strongly correlated with the frequentist ***p*-value**, and can thus be used to draw parallels and give some reference to readers non-familiar with Bayesian statistics.

However, this index is not relevant to assess the magnitude and importance of an effect (the meaning of "significance"), which is better achieved through other indices such as the [ROPE percentage](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html). In fact, indices of significance and existence are totally independent. You can have an effect with a *pd* of **99.99\%**, for which the whole posterior distribution is concentrated within the `[0.0001, 0.0002]` range. In this case, the effect is **positive with a high certainty**, but also **not significant** (*i.e.*, very small).

Indices of effect existence, such as the *pd*, are particularly useful in exploratory research or clinical studies, for which the focus is to make sure that the effect of interest is not in the opposite direction (for clinical studies, that a treatment is not harmful). However, once the effect's direction is confirmed, the focus should shift toward its significance, including a precise estimation of its magnitude, relevance and importance.

# Relationship with the *p*-value

In most cases, it seems that the *pd* has a direct correspondence with the frequentist **one-sided *p*-value** through the formula: $$p_{one-sided} = 1-p_d$$ Similarly, the **two-sided *p*-value** (the most commonly reported one) is equivalent through the formula: $$p_{two-sided} = 2*(1-p_d)$$ Thus, the two-sided *p*-value of respectively **.1**, **.05**, **.01** and **.001** would correspond approximately to a *pd* of **95\%**, **97.5\%**, **99.5\%** and **99.95\%** .


```{r message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Correlation between the frequentist p-value and the probability of direction (pd)", fig.align='center'}
library(dplyr)
library(tidyr)
library(ggplot2)
library(see)

read.csv("https://raw.github.com/easystats/easystats/master/publications/makowski_2019_bayesian/data/data.csv") %>% 
  mutate(effect_existence = ifelse(true_effect == 1, "Presence of true effect", "Absence of true effect"),
         p_direction = p_direction * 100) %>% 
  ggplot(aes(x=p_direction, y=p_value, color=effect_existence)) +
  geom_point2(alpha=0.1) +
  geom_segment(aes(x=95, y=Inf, xend=95, yend=0.1), color="black", linetype="longdash") +
  geom_segment(aes(x=-Inf, y=0.1, xend=95, yend=0.1), color="black", linetype="longdash") +
  geom_segment(aes(x=97.5, y=Inf, xend=97.5, yend=0.05), color="black", linetype="dashed") +
  geom_segment(aes(x=-Inf, y=0.05, xend=97.5, yend=0.05), color="black", linetype="dashed") +
  theme_modern() +
  scale_y_reverse(breaks = c(0.05, round(seq(0, 1, length.out = 11), digits=2))) +
  scale_x_continuous(breaks = c(95, 97.5, round(seq(50, 100, length.out = 6)))) +
  scale_color_manual(values=c("Presence of true effect"="green", "Absence of true effect"="red")) +
  theme(legend.title = element_blank()) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  xlab("Probability of Direction (pd)") +
  ylab("Frequentist p-value")
```



> **But if it's like the *p*-value, it must be bad because the *p*-value is bad [*insert reference to the reproducibility crisis*].**

In fact, this aspect of the reproducibility crisis might have been misunderstood. Indeed, it is not that the *p*-value is an intrinsically bad or wrong. Instead, it is its **misuse**, **misunderstanding** and **misinterpretation** that fuels the decay of the situation. For instance, the fact that the **pd** is highly correlated with the *p*-value suggests that the latter is more an index of effect *existence* than *significance* (*i.e.*, "worth of interest"). The Bayesian version, the **pd**, has an intuitive meaning and makes obvious the fact that **all thresholds are arbitrary**. Additionally, the **mathematical and interpretative transparency** of the **pd**, and its reconceptualisation as an index of effect existence, offers a valuable insight into the characterization of Bayesian results. Moreover, its concomitant proximity with the frequentist *p*-value makes it a perfect metric to ease the transition of psychological research into the adoption of the Bayesian framework.

# Methods of computation

The most **simple and direct** way to compute the **pd** is to 1) look at the median's sign, 2) select the portion of the posterior of the same sign and 3) compute the percentage that this portion represents. This "simple" method is the most straightforward, but its precision is directly tied to the number of posterior draws.

The second approach relies on [**density estimation**](https://easystats.github.io/bayestestR/reference/estimate_density.html). It starts by estimating the density function (for which many methods are available), and then computing the [**area under the curve**](https://easystats.github.io/bayestestR/reference/area_under_curve.html) (AUC) of the density curve on the other side of 0. The density-based method could hypothetically be considered as more precise, but strongly depends on the method used to estimate the density function.

# Methods comparison

Let's compare the 4 available methods, the **direct** method and 3 **density-based** methods differing by their density estimation algorithm (see [`estimate_density`](https://easystats.github.io/bayestestR/reference/estimate_density.html)).

## Correlation

Let's start by testing the proximity and similarity of the results obtained by different methods.

```{r message=FALSE, warning=FALSE, fig.align='center'}
library(bayestestR)
library(logspline)
library(KernSmooth)

# Compute the correlations
data <- data.frame()
for(the_mean in runif(25, 0, 4)){
  for(the_sd in runif(25, 0.5, 4)){
    x <- rnorm(100, the_mean, abs(the_sd))
    data <- rbind(data,
      data.frame("direct" = pd(x),
                 "kernel" = pd(x, method="kernel"),
                 "logspline" = pd(x, method="logspline"),
                 "KernSmooth" = pd(x, method="KernSmooth")
                 ))
  }
}
data <- as.data.frame(sapply(data, as.numeric))

# Visualize the correlations
library(ggplot2)
library(GGally)

GGally::ggpairs(data) +
  theme_classic()
```

All methods give are highly correlated and give very similar results. That means that the method choice is not a drastic game changer and cannot be used to tweak the results too much.

## Accuracy

To test the accuracy of each methods, we will start by computing the **direct *pd*** from a very dense distribution (with a large amount of observations). This will be our baseline, or "true" *pd*. Then, we will iteratively draw smaller samples from this parent distribution, and we will compute the *pd* with different methods. The closer this estimate is from the reference one, the better.

```{r message=FALSE, warning=FALSE}
data <- data.frame()
for(i in 1:25){
  the_mean <- runif(1, 0, 4)
  the_sd <- abs(runif(1, 0.5, 4))
  parent_distribution <- rnorm(100000, the_mean, the_sd)
  true_pd <- pd(parent_distribution)
  
  for(j in 1:25){
    sample_size <- round(runif(1, 25, 5000))
    subsample <- sample(parent_distribution, sample_size)
    data <- rbind(data,
      data.frame("sample_size" = sample_size, 
                 "true" = true_pd,
                 "direct" = pd(subsample) - true_pd,
                 "kernel" = pd(subsample, method="kernel")- true_pd,
                 "logspline" = pd(subsample, method="logspline") - true_pd,
                 "KernSmooth" = pd(subsample, method="KernSmooth") - true_pd
                 ))
  }
}
data <- as.data.frame(sapply(data, as.numeric))
```

```{r message=FALSE, warning=FALSE, fig.align='center'}
library(tidyr)
library(dplyr)

data %>% 
  tidyr::gather(Method, Distance, -sample_size, -true) %>% 
  ggplot(aes(x=sample_size, y = Distance, color = Method, fill= Method)) +
  geom_point(alpha=0.3, stroke=0, shape=16) +
  geom_smooth(alpha=0.2) +
  geom_hline(yintercept=0) +
  theme_classic() +
  xlab("\nDistribution Size")
```

The "Kernel" based density methods seems to consistently underestimate the *pd*. Interestingly, the "direct" method appears as being the more reliable, even in the case of small number of posterior draws.


## Can the pd be 100\%?

`p = 0.000` is coined as one of the term to avoid when reporting results [@lilienfeld2015fifty], even if often displayed by statistical software. The rationale is that for every probability distribution, there is no value with a probability of exactly 0. There is always some infinitesimal probability associated with each data point, and the `p = 0.000` returned by software is due to approximations related, among other, to finite memory hardware.

One could apply this rationale for the *pd*: since all data points have a non-null probability density, then the *pd* (a particular portion of the probability density) can *never* be 100\%. While this is an entirely valid point, people using the *direct* method might argue that their *pd* is based on the posterior draws, rather than on the theoretical, hidden, true posterior distribution (which is only approximated by the posterior draws). These posterior draws represent a finite sample for which `pd = 100%` is a valid statement. ---
title: "Region of Practical Equivalence (ROPE)"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test, rope, equivalence test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Region of Practical Equivalence (ROPE)}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("rstanarm", quietly = TRUE) || !requireNamespace("see", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

set.seed(333)
```


# What is the *ROPE?*

Unlike a frequentist approach, Bayesian inference is not based on statistical significance, where effects are tested against "zero". Indeed, the Bayesian framework offers a probabilistic view of the parameters, allowing assessment of the uncertainty related to them. Thus, rather than concluding that an effect is present when it simply differs from zero, we would conclude that the probability of being outside a specific range that can be considered as **"practically no effect"** (*i.e.*, a negligible magnitude) is sufficient. This range is called the **region of practical equivalence (ROPE)**.

Indeed, statistically, the probability of a posterior distribution being different from 0 does not make much sense (the probability of it being different from a single point being infinite). Therefore, the idea underlining ROPE is to let the user define an area around the null value enclosing values that are **equivalent to the null** value for practical purposes [@kruschke2010believe; @kruschke2012time; @kruschke2014doing].

# Equivalence Test


The ROPE, being a region corresponding to a "null" hypothesis, is used for the **equivalence test**, to test whether a parameter is **significant** (in the sense of *important* enough to be cared about). This test is usually based on the **"HDI+ROPE decision rule"** [@kruschke2014doing; @kruschke2018bayesian] to check whether parameter values should be accepted or rejected against an explicitly formulated "null hypothesis" (*i.e.*, a ROPE). In other words, it checks the percentage of Credible Interval (CI) that is the null region (the ROPE). If this percentage is sufficiently low, the null hypothesis is rejected. If this percentage is sufficiently high, the null hypothesis is accepted.




# Credible interval in ROPE *vs* full posterior in ROPE

Using the ROPE and the HDI as Credible Interval, Kruschke (2018) suggests using the percentage of the 95\% HDI that falls within the ROPE as a decision rule. However, as the 89\% HDI [is considered a better choice](https://easystats.github.io/bayestestR/articles/credible_interval.html) [@kruschke2014doing; @mcelreath2014rethinking; @mcelreath2018statistical], `bayestestR` provides by default the percentage of the 89\% HDI that falls within the ROPE.

However, [*simulation studies data*](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full) suggest that using the percentage of the full posterior distribution, instead of a CI, might be more sensitive (especially do delineate highly significant effects). Thus, we recommend that the user considers using the ***full* ROPE** percentage (by setting `ci = 1`), which will return the portion of the entire posterior distribution in the ROPE.

# What percentage in ROPE to accept or to reject?

If the HDI is completely outside the ROPE, the "null hypothesis" for this parameter is "rejected". If the ROPE completely covers the HDI, *i.e.*, all most credible values of a parameter are inside the region of practical equivalence, the null hypothesis is accepted. Else, it’s unclear whether the null hypothesis should be accepted or rejected. 

If the **full ROPE** is used (*i.e.*, 100\% of the HDI), then the null hypothesis is rejected or accepted if the percentage of the posterior within the ROPE is smaller than to 2.5\% or greater than 97.5\%. Desirable results are low proportions inside the ROPE  (the closer to zero the better).


# How to define the ROPE range?

Kruschke (2018) suggests that the ROPE could be set, by default, to a range from `-0.1` to `0.1` of a standardized parameter (negligible effect size according to Cohen, 1988).

- For **linear models (lm)**, this can be generalised to: $$[-0.1*SD_{y}, 0.1*SD_{y}]$$.
- For **logistic models**, the parameters expressed in log odds ratio can be converted to standardized difference through the formula: $$\sqrt{3}/\pi$$, resulting in a range of `-0.055` to `-0.055`. For other models with binary outcome, it is strongly recommended to manually specify the rope argument. Currently, the same default is applied that for logistic models.
- For **t-tests**, the standard deviation of the response is used, similarly to linear models (see above).
- For **correlations**, `-0.05, 0.05` is used, *i.e.*, half the value of a negligible correlation as suggested by Cohen's (1988) rules of thumb.
- For all other models, `-0.1, 0.1` is used to determine the ROPE limits, but it is strongly advised to specify it manually.

# Sensitivity to parameter's scale

It is important to consider **the unit (*i.e.*, the scale) of the predictors** when using an index based on the ROPE, as the correct interpretation of the ROPE as representing a region of practical equivalence to zero is dependent on the scale of the predictors. Indeed, unlike other indices (such as the [`pd`](https://easystats.github.io/bayestestR/articles/probability_of_direction.html)), the percentage in **ROPE** depend on the unit of its parameter. In other words, as the ROPE represents a fixed portion of the response's scale, its proximity with a coefficient depends on the scale of the coefficient itself. 


For instance, if we consider a simple regression `growth ~ time`, modelling the development of **Wookies babies**, a negligible change (the ROPE) is less than **54 cm**. If our `time` variable is **expressed in days**, we will find that the coefficient (representing the growth **by day**) is of about **10 cm** (*the median of the posterior of the coefficient is 10*). Which we would consider as **negligible**. However, if we decide to express the `time` variable **in years**, the coefficient will be scaled by this transformation (as it will now represent the growth **by year**). The coefficient will now be around **3550** cm (`10 * 355`), which we would now consider as **significant**.


```{r message=FALSE, warning=FALSE, eval=FALSE}
library(rstanarm)
library(bayestestR)
library(see)

data <- iris  # Use the iris data
model <- stan_glm(Sepal.Length ~ Sepal.Width, data=data)  # Fit model
```
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=">"}
library(rstanarm)
library(bayestestR)
library(see)
set.seed(333)

data <- iris  # Use the iris data
model <- stan_glm(Sepal.Length ~ Sepal.Width, data=data, refresh = 0)
```


```{r echo=TRUE, message=FALSE, warning=FALSE, comment=">"}
# Compute indices
pd <- p_direction(model)
percentage_in_rope <- rope(model, ci=1)

# Visualise the pd
plot(pd)
pd

# Visualise the percentage in ROPE
plot(percentage_in_rope)
percentage_in_rope
```

We can see that the *pd* and the percentage in ROPE of the linear relationship between **Sepal.Length** and **Sepal.Width** are respectively of about `92.95%` and `15.95%`, corresponding to an **uncertain** and **not significant** effect. What happen if we scale our predictor? 



```{r message=FALSE, warning=FALSE, eval=FALSE}
data$Sepal.Width_scaled <- data$Sepal.Width / 100  # Divide predictor by 100
model <- stan_glm(Sepal.Length ~ Sepal.Width_scaled, data=data)  # Fit model
```
```{r echo=FALSE, message=FALSE, warning=FALSE, comment=">"}
set.seed(333)
data$Sepal.Width_scaled <- data$Sepal.Width / 100
model <- stan_glm(Sepal.Length ~ Sepal.Width_scaled, data=data, refresh = 0)
```

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=">"}
# Compute indices
pd <- p_direction(model)
percentage_in_rope <- rope(model, ci=1)

# Visualise the pd
plot(pd)
pd

# Visualise the percentage in ROPE
plot(percentage_in_rope)
percentage_in_rope
```


As you can see, by simply dividing the predictor by 100, we **drastically** changed the conclusion related to the **percentage in ROPE** (which became very close to `0`): the effect could now be **interpreted as being significant**. Thus, we recommend paying close attention to the unit of the predictors when selecting the ROPE range (*e.g.*, what coefficient would correspond to a small effect?), and when reporting or reading ROPE results.


# Multicollinearity: Non-independent covariates

When **parameters show strong correlations**, *i.e.*, when covariates are not independent, the joint parameter distributions may shift towards or away from the ROPE. Collinearity invalidates ROPE and hypothesis testing based on univariate marginals, as the probabilities are conditional on independence. Most problematic are parameters that only have partial overlap with the ROPE region. In case of collinearity, the (joint) distributions  of these parameters may either get an increased or decreased ROPE, which means that inferences based on ROPE are inappropriate [@kruschke2014doing].

The `equivalence_test()` and `rope()` functions perform a simple check for pairwise correlations between parameters, but as there can be collinearity between more than two variables, a first step to check the assumptions of this hypothesis testing is to look at different pair plots. An even more sophisticated check is the projection predictive variable selection [@piironen2017comparison].



---
title: "Credible Intervals (CI)"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test, ci, credible interval]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Credible Intervals (CI)}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
options(digits=2)

if (!requireNamespace("ggplot2", quietly = TRUE) || !requireNamespace("dplyr", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

set.seed(333)
```


# What is a *Credible* Interval?

Credible intervals are an important concept in Bayesian statistics. Its core purpose is to describe and summarise **the uncertainty** related to your parameters. In this regards, it could appear as quite similar to the frequentist **Confidence Intervals**. However, while their goal is similar, **their statistical definition annd meaning is very different**. Indeed, while the latter is obtained through a complex algorithm full of rarely-tested assumptions and approximations, the credible intervals are fairly straightforward to compute. 

As the Bayesian inference returns a distribution of possible effect values (the posterior), the credible interval is just the range containing a particular percentage of probable values. For instance, the 95\% credible interval is simply the central portion of the posterior distribution that contains 95\% of the values. 

Note that this drastically improve the interpretability of the Bayesian interval compared to the frequentist one. Indeed, the Bayesian framework allows us to say *"given the observed data, the effect has 95% probability of falling within this range"*, while the frequentist less straightforward alternative (the 95\% ***Confidence* Interval**) would be "*there is a 95\% probability that when computing a confidence interval from data of this sort, the effect falls within this range*".


# Why is the default 89\%?

Naturally, when it came about choosing the CI level to report by default, **people started using 95\%**, the arbitrary convention used in the **frequentist** world. However, some authors suggested that 95\% might not be the most apppropriate for Bayesian posterior distributions, potentially lacking stability if not enough posterior samples are drawn [@kruschke2014doing]. 

The proposition was to use 90\% instead of 95\%. However, recently, McElreath (2014, 2018) suggested that if we were to use arbitrary thresholds in the first place, why not use 89\% as this value has the additional argument of being a prime number.

Thus, by default, the CIs are computed with 89\% intervals (`ci = 0.89`), deemed to be more stable than, for instance, 95\% intervals [@kruschke2014doing]. An effective sample size (ESS; see [here](https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html)) of at least 10.000 is recommended if 95\% intervals should be computed (Kruschke, 2014, p. 183ff). Moreover, 89 is the highest **prime number** that does not exceed the already unstable 95\% threshold. What does it have to do with anything? *Nothing*, but it reminds us of the total arbitrarity of any of these conventions [@mcelreath2018statistical].

# Different types of CIs

The reader might notice that `bayestestR` provides **two methods** to compute credible intervals, the **Highest Density Interval (HDI)** (`hdi()`) and the **Equal-tailed Interval (ETI)** (`eti()`). These methods can also be changed via the `method` argument of the `ci()` function. What is the difference? Let's see:

```{r warning=FALSE, message=FALSE}
library(bayestestR)
library(dplyr)
library(ggplot2)

# Generate a normal distribution
posterior <- distribution_normal(1000)

# Compute HDI and ETI
ci_hdi <- ci(posterior, method = "HDI")
ci_eti <- ci(posterior, method = "ETI")

# Plot the distribution and add the limits of the two CIs
posterior %>% 
  estimate_density(extend=TRUE) %>% 
  ggplot(aes(x=x, y=y)) +
  geom_area(fill="orange") +
  theme_classic() +
  # HDI in blue
  geom_vline(xintercept=ci_hdi$CI_low, color="royalblue", size=3) +
  geom_vline(xintercept=ci_hdi$CI_high, color="royalblue", size=3) +
  # Quantile in red
  geom_vline(xintercept=ci_eti$CI_low, color="red", size=1) +
  geom_vline(xintercept=ci_eti$CI_high, color="red", size=1)
```

> **These are exactly the same...**

But is it also the case for other types of distributions?

```{r warning=FALSE, message=FALSE}
library(bayestestR)
library(dplyr)
library(ggplot2)

# Generate a beta distribution
posterior <- distribution_beta(1000, 6, 2)

# Compute HDI and Quantile CI
ci_hdi <- ci(posterior, method = "HDI")
ci_eti <- ci(posterior, method = "ETI")

# Plot the distribution and add the limits of the two CIs
posterior %>% 
  estimate_density(extend=TRUE) %>% 
  ggplot(aes(x=x, y=y)) +
  geom_area(fill="orange") +
  theme_classic() +
  # HDI in blue
  geom_vline(xintercept=ci_hdi$CI_low, color="royalblue", size=3) +
  geom_vline(xintercept=ci_hdi$CI_high, color="royalblue", size=3) +
  # ETI in red
  geom_vline(xintercept=ci_eti$CI_low, color="red", size=1) +
  geom_vline(xintercept=ci_eti$CI_high, color="red", size=1)
```

> **The difference is strong with this one.**

Contrary to the **HDI**, for which all points within the interval have a higher probability density than points outside the interval, the **ETI** is **equal-tailed**. This means that a 90\% interval has 5\% of the distribution on either side of its limits. It indicates the 5th percentile and the 95th percentile. In symmetric distributions, the two methods of computing credible intervals, the ETI and the HDI, return similar results.

This is not the case for skewed distributions. Indeed, it is possible that parameter values in the ETI have lower credibility (are less probable) than parameter values outside the ETI. This property seems undesirable as a summary of the credible values in a distribution.

On the other hand, the ETI range does *not* change when transformations are applied to the distribution (for instance, for a log odds scale to probabilities): the lower and higher bounds of the transformed distribution will correspond to the transformed lower and higher bounds of the original distribution. On the contrary, applying transformations to the distribution will change the resulting HDI. Thus, for instance if exponentiated credible intervals are required, it is recommended to calculate the ETI.

# The Support Interval

Unlike the HDI and the ETI, which look at the posterior distribution, the **Support Interval (SI)** provides information regarding the change in the credability of values from the prior to the posterior - in other words, it indicates which values of a parameter are have gained support by the observed data by some factor greater or equal to *k* [@wagenmakers2018SI].

```{r warning=FALSE, message=FALSE}
prior <- distribution_normal(1000, mean = 0, sd = 1)
posterior <- distribution_normal(1000, mean = .5, sd = .3)

si_1 <- si(posterior, prior, BF = 1)
si_3 <- si(posterior, prior, BF = 3)

ggplot(mapping = aes(x=x, y=y)) +
  theme_classic() +
  # The posterior
  geom_area(fill = "orange",
            data = estimate_density(posterior, extend = TRUE)) +
  # The prior
  geom_area(color = "black", fill = NA, size = 1, linetype = "dashed",
            data = estimate_density(prior, extend = TRUE)) +
  # BF = 1 SI in blue
  geom_vline(xintercept=si_1$CI_low, color="royalblue", size=1) +
  geom_vline(xintercept=si_1$CI_high, color="royalblue", size=1) +
  # BF = 3 SI in red
  geom_vline(xintercept=si_3$CI_low, color="red", size=1) +
  geom_vline(xintercept=si_3$CI_high, color="red", size=1)
```

Between the blue lines are values that received *some* support by the data (this is a BF = 1 SI), which received at least "moderate" support from the data.

From the presepctive of the Savage-Dickey Bayes factor, testing against a point null hypothesis for any value within the support interval will yeild a Bayes factor smaller than 1/`BF`. 

# References
---
title: "2. Confirmation of Bayesian skills"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{Example 2: Confirmation of Bayesian skills}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

This vignette can be referred to by citing the package:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541

---

```{r message=FALSE, warning=FALSE, include=FALSE}
if (!requireNamespace("see", quietly = TRUE) ||
      !requireNamespace("dplyr", quietly = TRUE) ||
      !requireNamespace("ggplot2", quietly = TRUE) ||
      !requireNamespace("performance", quietly = TRUE) ||
      !requireNamespace("BayesFactor", quietly = TRUE) ||
      !requireNamespace("rstanarm", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

data(iris)
library(knitr)
library(bayestestR)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">")
knitr::opts_chunk$set(dpi=150)
options(digits=2)

set.seed(333)
```


Now that [**describing and understanding posterior distributions**](https://easystats.github.io/bayestestR/articles/example1.html) of linear regressions has no secrets for you, we will take one step back and study some simpler models: **correlations** and ***t*-tests**.

But before we do that, let us take a moment to remind ourselves and appreciate the fact that **all basic statistical pocedures** such as correlations, *t*-tests, ANOVAs or Chisquare tests ***are* linear regressions** (we strongly recommend  [this excellent demonstration](https://lindeloev.github.io/tests-as-linear/)). Nevertheless, these simple models will be the occasion to introduce a more complex index, such as the **Bayes factor**.

## Correlations

### Frequentist version 

Let us start, again, with a **frequentist correlation** between two continuous variables, the **width** and the **length** of the sepals of some flowers. The data is available in R as the `iris`  dataset (the same that was used in the [previous tutorial](https://easystats.github.io/bayestestR/articles/example1.html)). 

We will compute a Pearson's correlation test, store the results in an object called `result`, then display it:

```{r message=FALSE, warning=FALSE}
result <- cor.test(iris$Sepal.Width, iris$Sepal.Length)
result
```

As you can see in the output, the test that we did actually compared two hypotheses: the **null hypothesis** (*h0*; no correlation) with the **alternative hypothesis** (*h1*; a non-null correlation). Based on the *p*-value, the null hypothesis cannot be rejected: the correlation between the two variables is **negative but not significant** (r = -.12, p > .05).

### Bayesian correlation 

To compute a Bayesian correlation test, we will need the [`BayesFactor`](https://richarddmorey.github.io/BayesFactor/) package (you can install it by running `install.packages("BayesFactor")`). We can then load this package, compute the correlation using the `correlationBF()` function and store the results in a similar fashion. 
 
```{r message=FALSE, warning=FALSE, results='hide'}
library(BayesFactor)
result <- correlationBF(iris$Sepal.Width, iris$Sepal.Length)
```

Now, let us run our `describe_posterior()` function on that:

```{r message=FALSE, warning=FALSE, eval=FALSE}
describe_posterior(result)
```
```{r echo=FALSE}
structure(list(Parameter = "rho", Median = -0.114149129692488, 
    CI = 89, CI_low = -0.240766308855643, CI_high = 0.00794997655649642, 
    pd = 91.6, ROPE_CI = 89, ROPE_low = -0.1, ROPE_high = 0.1, 
    ROPE_Percentage = 42.0949171581017, BF = 0.509017511647702, 
    Prior_Distribution = "cauchy", Prior_Location = 0, Prior_Scale = 0.333333333333333), row.names = 1L, class = "data.frame")
```


We see again many things here, but the important indices for now are the **median** of the posterior distribution, `-.11`. This is (again) quite close to the frequentist correlation. We could, as previously, describe the [**credible interval**](https://easystats.github.io/bayestestR/articles/credible_interval.html), the [**pd**](https://easystats.github.io/bayestestR/articles/probability_of_direction.html) or the [**ROPE percentage**](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html), but we will focus here on another index provided by the Bayesian framework, the **Bayes factor (BF)**.

### Bayes factor (BF)

We said previously that a correlation test actually compares two hypotheses, a null (absence of effect) with an altnernative one (presence of an effect). The [**Bayes factor (BF)**](https://easystats.github.io/bayestestR/articles/bayes_factors.html) allows the same comparison and determines **under which of two models the observed data are more probable**: a model with the effect of interest, and a null model without the effect of interest. We can use `bayesfactor()` to specifically compute the Bayes factor comparing those models:

```{r message=FALSE, warning=FALSE}
bayesfactor(result)
```

We got a *BF* of `0.51`. What does it mean?

Bayes factors are **continuous measures of relative evidence**, with a Bayes factor greater than 1 giving evidence in favour of one of the models (often referred to as *the numerator*), and a Bayes factor smaller than 1 giving evidence in favour of the other model (*the denominator*). 

> **Yes, you heard things right, evidence in favour of the null!**

That's one of the reason why the Bayesian framework is sometimes considered as superior to the frequentist framework. Remember from your stats lessons, that the ***p*-value can only be used to reject *h0***, but not *accept* it. With the **Bayes factor**, you can measure **evidence against - and in favour of - the null**. 

BFs representing evidence for the alternative against the null can be reversed using $BF_{01}=1/BF_{10}$ (the *01* and *10* correspond to *h0* against *h1* and *h1* against *h0*, respectively) to provide evidence of the null againtt the alternative. This improves human readability in cases where the BF of the alternative against the null is smaller than 1 (i.e., in support of the null).

In our case, `BF = 1/0.51 = 2`, indicates that the data are **2 times more probable under the null compared to the alternative hypothesis**, which, though favouring the null, is considered only [anecdotal evidence against the null](https://easystats.github.io/report/articles/interpret_metrics.html#bayes-factor-bf).

We can thus conclude that there is **anecdotal evidence in favour of an absence of correlation between the two variables (r<sub>median</sub> = 0.11, BF = 0.51)**, which is a much more informative statement that what we can do with frequentist statistics.

**And that's not all!**

### Visualise the Bayes factor

In general, **pie charts are an absolute no-go in data visualisation**, as our brain's perceptive system heavily distorts the information presented in such way. Nevertheless, there is one exeption: pizza charts.

It is an intuitive way of interpreting the strength of evidence provided by BFs as an amount of surprise. 


```{r echo=FALSE, fig.cap="Wagenmakers' pizza poking analogy. From the great 'www.bayesianspectacles.org' blog.", fig.align='center', out.width="80%"}
knitr::include_graphics("https://github.com/easystats/easystats/raw/master/man/figures/bayestestR/LetsPokeAPizza.jpg")
```


Such "pizza plots" can be directly created through the [`see`](https://github.com/easystats/see) visualisation companion package for easystats (you can install it by running `install.packages("see")`):

```{r message=FALSE, warning=FALSE}
library(see)

plot(bayesfactor(result)) +
  scale_fill_pizza()
```

So, after seeing this pizza, how much would you be suprised by the outcome of a blinded poke? 


## *t*-tests

***"I know that I know nothing, and especially not if *versicolor* and *virginica* differ in terms of Sepal.Width"*, famously said Socrates**. Time to finally answer this answer this crucial question!

### Versicolor *vs.* virginica

Bayesian *t*-tests can be performed in a very similar way to correlations. As we are particularly interested in two levels of the `Species` factor, *versicolor* and *virginica*. We will start by filtering out from `iris` the non-relevant observations corresponding to the *setosa* specie, and we will then visualise the observations and the distribution of the `Sepal.Width` variable. 

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)

# Select only two relevant species
data <- iris %>% 
  filter(Species != "setosa") %>% 
  droplevels()

# Visualise distributions and observations
data %>% 
  ggplot(aes(x = Species, y = Sepal.Width, fill = Species)) +
  geom_violindot(fill_dots = "black", size_dots = 1) +
  scale_fill_material() +
  theme_modern()
```

It *seems* (visually) that *virgnica* flowers have, on average, a slightly higer width of sepals. Let's assess this difference statistically by using the `ttestBF` in the `BayesFactor` package.

### Compute the Bayesian *t*-test

```{r message=FALSE, warning=FALSE}
result <- BayesFactor::ttestBF(formula = Sepal.Width ~ Species, data = data)
describe_posterior(result)
```

From the indices, we can say that the difference of `Sepal.Width` between *virginica* and *versicolor* has a probability of **100% of being negative** [*from the pd and the sign of the median*] (median = -0.19, 89% CI [-0.29, -0.092]). The data provides a **strong evidence against the null hypothesis** (BF = 18).

Keep that in mind as we will see another way of investigating this question.


## Logistic Model

A hypothesis for which one uses a *t*-test can also be tested using a binomial model (*e.g.*, a **logistic model**). Indeed, it is possible to reformulate the following hypothesis, "*there is an important difference in this variable between the two groups*" by "*this variable is able to discriminate between (or classify) the two groups*". However, these models are much more powerful than a regular *t*-test.

In the case of the difference of `Sepal.Width` between *virginica* and *versicolor*, the question becomes, *how well can we classify the two species using only* `Sepal.Width`. 

### Fit the model

```{r message=FALSE, warning=FALSE, eval=FALSE}
library(rstanarm)

model <- stan_glm(Species ~ Sepal.Width, data = data, family = "binomial")
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rstanarm)

model <- stan_glm(Species ~ Sepal.Width, data = data, family = "binomial", refresh = 0)
```

### Visualise the model

Using the [`estimate`](https://github.com/easystats/estimate) package. **Wait until estimate is on CRAN**.


### Performance and Parameters

TO DO.

```{r message=FALSE, warning=FALSE}
library(performance)

model_performance(model)
```

```{r message=FALSE, warning=FALSE}
describe_posterior(model, test = c("pd", "ROPE", "BF"))
```



### Visualise the indices

TO DO.

```{r message=FALSE, warning=FALSE}
# plot(rope(result))
```


### Diagnostic Indices

About diagnostic indices such as Rhat and ESS.


---
title: "In-Depth 2: Comparison of Indices of Effect Existence and Significance"
output:
  github_document:
    toc: true
    toc_depth: 3
    fig_width: 10.08
    fig_height: 6
  word_document:
    toc: true
    toc_depth: 3
    fig_width: 10.08
    fig_height: 6
    df_print: "kable"
    highlight: "pygments"
    reference_docx: templates/Template_Frontiers.docx
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    fig_width: 10.08
    fig_height: 6
tags: [r, bayesian, posterior, test]
vignette: >
  \usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{In-Depth 2: Comparison of Indices of Effect Existence}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
csl: apa.csl
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(comment=">", dpi=75)
options(digits=2)
```


This vignette can be referred to by citing the following:

- Makowski, D., Ben-Shachar, M. S., \& Lüdecke, D. (2019). *bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework*. Journal of Open Source Software, 4(40), 1541. https://doi.org/10.21105/joss.01541
- Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \& Lüdecke, D. (2019). *Indices of Effect Existence and Significance in the Bayesian Framework*. Frontiers in Psychology 2019;10:2767. [10.3389/fpsyg.2019.02767](https://doi.org/10.3389/fpsyg.2019.02767)

---

# Indices of Effect *Existence* and *Significance* in the Bayesian Framework

A comparison of different Bayesian indices (*pd*, *BFs*, ROPE etc.) is accessible [**here**](https://doi.org/10.3389/fpsyg.2019.02767).---
title: "ggsignif intro"
author: "Constantin Ahlmann-Eltze"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ggsignif intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This package provides an extension for the popular ggplot2 package.

Often people are looking for a way to quickly annotate if the difference between 2 groups in a plot is
significantly different 
([look](http://stackoverflow.com/questions/17084566/put-stars-on-ggplot-barplots-and-boxplots-to-indicate-the-level-of-significanc) [at](http://stackoverflow.com/questions/14958159/indicating-the-statistically-significant-difference-in-bar-graph-using-r) [all](http://stackoverflow.com/questions/29263046/how-to-draw-the-boxplot-with-significant-level) [those](http://stackoverflow.com/questions/29186435/showing-significance-relationships-in-a-ggplot2-bar-graph) [questions](https://groups.google.com/forum/#!topic/ggplot2/ntVlMqTc6PI)). 
The usual answer is along the lines of just manually adding lines and annotation to the plot.

But I believe that a fundamental strength of ggplot is that it allows one to quickly make advanced plots by combining layers of visualization, which can encapsulate complex statistical methods (`geom_smooth`, `geom_contour` etc.).

So I created this package which provides a single layer `geom_signif` which can calculate the significance of a difference between groups and add the annotation to the plot in a single line.

# How to use it?

First load both packages

```{r}
library(ggplot2)
library(ggsignif)
```

Second step: plot your data

```{r fig.width = 6, fig.height = 4.8}
ggplot(iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot() +
  geom_signif(comparisons = list(c("versicolor", "virginica")), 
              map_signif_level=TRUE)
```

That's it, it is really simple!

## Advanced options

Sometimes one might need more advanced control over the display. To specify exactly where
the bracket is drawn use the `y_position`, `xmin` and `xmax` parameters combined with
a custom `annotations`. This is always necessary if `geom_signif` is combined with
another layer that uses `position="dodge"`, because it changes the location of the 
visual elements without updating the data.

```{r fig.width = 6, fig.height = 4.8}
dat <- data.frame(Group = c("S1", "S1", "S2", "S2"),
                  Sub   = c("A", "B", "A", "B"),
                  Value = c(3,5,7,8))  

ggplot(dat, aes(Group, Value)) +
  geom_bar(aes(fill = Sub), stat="identity", position="dodge", width=.5) +
  geom_signif(y_position=c(5.3, 8.3), xmin=c(0.8, 1.8), xmax=c(1.2, 2.2),
              annotation=c("**", "NS"), tip_length=0) +
  geom_signif(comparisons=list(c("S1", "S2")),
              y_position = 9.3, tip_length = 0, vjust=0.2) +
  scale_fill_manual(values = c("grey80", "grey20"))
    
```

For more detailed documentation of the available parameters see the man page for the geom_signif function (`> ?geom_signif`).

# Bugs, Comments or Questions?

If you have any problems with the package, just file an issue at https://github.com/const-ae/ggsignif.
















---
title: "Writing SQL with dbplyr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Writing SQL with dbplyr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette discusses why you might use dbplyr instead of writing SQL yourself, and what to do when dbplyr's built-in translations can't create the SQL that you need.

```{r setup, message = FALSE}
library(dplyr)
library(dbplyr)

mf <- memdb_frame(x = 1, y = 2)
```

## Why use dbplyr?

One simple nicety of dplyr is that it will automatically generate subqueries if you want to use a freshly created variable in `mutate()`: 

```{r}
mf %>% 
  mutate(
    a = y * x, 
    b = a ^ 2,
  ) %>% 
  show_query()
```

In general, it's much easier to work iteratively in dbplyr. You can easily give intermediate queries names, and reuse them in multiple places. Or if you have a common operation that you want to do to many queries, you can easily wrap it up in a function. It's also easy to chain `count()` to the end of any query to check the results are about what you expect.

## What happens when dbplyr fails?

dbplyr aims to translate the most common R functions to their SQL equivalents, allowing you to ignore the vagaries of the SQL dialect that you're working with, so you can focus on the data analysis problem at hand. But different backends have different capabilities, and sometimes there are SQL functions that don't have exact equivalents in R. In those cases, you'll need to write SQL code directly. This section shows you how you can do so.

### Prefix functions

Any function that dbplyr doesn't know about will be left as is:

```{r}
mf %>% 
  mutate(z = foofify(x, y)) %>% 
  show_query()
```

Because SQL functions are general case insensitive, I recommend using upper case when you're using SQL functions in R code. That makes it easier to spot that you're doing something unusual:

```{r}
mf %>% 
  mutate(z = FOOFIFY(x, y)) %>% 
  show_query()
```

### Infix functions

As well as prefix functions (where the name of the function comes before the arguments), dbplyr also translates infix functions. That allows you to use expressions like `LIKE` which does a limited form of pattern matching:

```{r}
mf %>% 
  filter(x %LIKE% "%foo%") %>% 
  show_query()
```

Or use `||` for string concatenation (note that backends should translate `paste()` and `paste0()` for you):

```{r}
mf %>% 
  transmute(z = x %||% y) %>% 
  show_query()
```

### Special forms

SQL functions tend to have a greater variety of syntax than R. That means there are a number of expressions that can't be translated directly from R code. To insert these in your own queries, you can use literal SQL inside `sql()`:

```{r}
mf %>% 
  transmute(factorial = sql("x!")) %>% 
  show_query()

mf %>% 
  transmute(factorial = sql("CAST(x AS FLOAT)")) %>% 
  show_query()
```

Note that you can use `sql()` at any depth inside the expression:

```{r}
mf %>% 
  filter(x == sql("ANY VALUES(1, 2, 3)")) %>% 
  show_query()
```
---
title: "Introduction to dbplyr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to dbplyr}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
options(tibble.print_min = 6L, tibble.print_max = 6L, digits = 3)
```

As well as working with local in-memory data stored in data frames, dplyr also works with remote on-disk data stored in databases. This is particularly useful in two scenarios:

* Your data is already in a database.
  
* You have so much data that it does not all fit into memory simultaneously
  and you need to use some external storage engine.
  
(If your data fits in memory there is no advantage to putting it in a database: it will only be slower and more frustrating.)

This vignette focuses on the first scenario because it's the most common. If you're using R to do data analysis inside a company, most of the data you need probably already lives in a database (it's just a matter of figuring out which one!). However, you will learn how to load data in to a local database in order to demonstrate dplyr's database tools. At  the end, I'll also give you a few pointers if you do need to set up your own database.

## Getting started

To use databases with dplyr you need to first install dbplyr:

```{r, eval = FALSE}
install.packages("dbplyr")
```

You'll also need to install a DBI backend package. The DBI package provides a common interface that allows dplyr to work with many different databases using the same code. DBI is automatically installed with dbplyr, but you need to install a specific backend for the database that you want to connect to.

Five commonly used backends are:

*   [RMariaDB](https://CRAN.R-project.org/package=RMariaDB) 
    connects to MySQL and MariaDB

*   [RPostgres](https://CRAN.R-project.org/package=RPostgres) 
    connects to Postgres and Redshift.

*   [RSQLite](https://github.com/rstats-db/RSQLite) embeds a SQLite database.

*   [odbc](https://github.com/rstats-db/odbc#odbc) connects to many commercial
    databases via the open database connectivity protocol.

*   [bigrquery](https://github.com/rstats-db/bigrquery) connects to Google's
    BigQuery.

If the database you need to connect to is not listed here, you'll need to do some investigation (i.e. googling) yourself.

In this vignette, we're going to use the RSQLite backend which is automatically installed when you install dbplyr. SQLite is a great way to get started with databases because it's completely embedded inside an R package. Unlike most other systems, you don't need to setup a separate database server. SQLite is great for demos, but is surprisingly powerful, and with a little practice you can use it to easily work with many gigabytes of data.

## Connecting to the database

To work with a database in dplyr, you must first connect to it, using `DBI::dbConnect()`. We're not going to go into the details of the DBI package here, but it's the foundation upon which dbplyr is built. You'll need to learn more about if you need to do things to the database that are beyond the scope of dplyr.

```{r setup, message = FALSE}
library(dplyr)
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
```

The arguments to `DBI::dbConnect()` vary from database to database, but the first argument is always the database backend. It's `RSQLite::SQLite()` for RSQLite, `RMariaDB::MariaDB()` for RMariaDB, `RPostgres::Postgres()` for RPostgres, `odbc::odbc()` for odbc, and `bigrquery::bigquery()` for BigQuery. SQLite only needs one other argument: the path to the database. Here we use the special string `":memory:"` which causes SQLite to make a temporary in-memory database.

Most existing databases don't live in a file, but instead live on another server. That means in real-life that your code will look more like this:

```{r, eval = FALSE}
con <- DBI::dbConnect(RMariaDB::MariaDB(), 
  host = "database.rstudio.com",
  user = "hadley",
  password = rstudioapi::askForPassword("Database password")
)
```

(If you're not using RStudio, you'll need some other way to securely retrieve your password. You should never record it in your analysis scripts or type it into the console. [Securing Credentials](https://db.rstudio.com/best-practices/managing-credentials) provides some best practices.)

Our temporary database has no data in it, so we'll start by copying over `nycflights13::flights` using the convenient `copy_to()` function. This is a quick and dirty way of getting data into a database and is useful primarily for demos and other small jobs.

```{r}
copy_to(con, nycflights13::flights, "flights",
  temporary = FALSE, 
  indexes = list(
    c("year", "month", "day"), 
    "carrier", 
    "tailnum",
    "dest"
  )
)
```

As you can see, the `copy_to()` operation has an additional argument that allows you to supply indexes for the table. Here we set up indexes that will allow us to quickly process the data by day, carrier, plane, and destination. Creating the right indices is key to good database performance, but is unfortunately beyond the scope of this article.

Now that we've copied the data, we can use `tbl()` to take a reference to it:

```{r}
flights_db <- tbl(con, "flights")
```

When you print it out, you'll notice that it mostly looks like a regular tibble:

```{r}
flights_db 
```

The main difference is that you can see that it's a remote source in a SQLite database.

## Generating queries

To interact with a database you usually use SQL, the Structured Query Language. SQL is over 40 years old, and is used by pretty much every database in existence. The goal of dbplyr is to automatically generate SQL for you so that you're not forced to use it. However, SQL is a very large language and dbplyr doesn't do everything. It focusses on `SELECT` statements, the SQL you write most often as an analyst.

Most of the time you don't need to know anything about SQL, and you can continue to use the dplyr verbs that you're already familiar with:

```{r}
flights_db %>% select(year:day, dep_delay, arr_delay)

flights_db %>% filter(dep_delay > 240)

flights_db %>% 
  group_by(dest) %>%
  summarise(delay = mean(dep_time))
```

However, in the long-run, I highly recommend you at least learn the basics of SQL. It's a valuable skill for any data scientist, and it will help you debug problems if you run into problems with dplyr's automatic translation. If you're completely new to SQL you might start with this [codeacademy tutorial](https://www.codecademy.com/learn/learn-sql). If you have some familiarity with SQL and you'd like to learn more, I found [how indexes work in SQLite](http://www.sqlite.org/queryplanner.html) and [10 easy steps to a complete understanding of SQL](http://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql) to be particularly helpful.

The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible:

* It never pulls data into R unless you explicitly ask for it.

* It delays doing any work until the last possible moment: it collects together
  everything you want to do and then sends it to the database in one step.

For example, take the following code:

```{r}
tailnum_delay_db <- flights_db %>% 
  group_by(tailnum) %>%
  summarise(
    delay = mean(arr_delay),
    n = n()
  ) %>% 
  arrange(desc(delay)) %>%
  filter(n > 100)
```

Surprisingly, this sequence of operations never touches the database. It's not until you ask for the data (e.g. by printing `tailnum_delay`) that dplyr generates the SQL and requests the results from the database. Even then it tries to do as little work as possible and only pulls down a few rows.

```{r}
tailnum_delay_db
```

Behind the scenes, dplyr is translating your R code into SQL. You can see the SQL it's generating with `show_query()`:

```{r}
tailnum_delay_db %>% show_query()
```

If you're familiar with SQL, this probably isn't exactly what you'd write by hand, but it does the job. You can learn more about the SQL translation in `vignette("translation-verb")` and `vignette("translation-function")`.

Typically, you'll iterate a few times before you figure out what data you need from the database. Once you've figured it out, use `collect()` to pull all the data down into a local tibble:

```{r}
tailnum_delay <- tailnum_delay_db %>% collect()
tailnum_delay
```

`collect()` requires that database does some work, so it may take a long time to complete. Otherwise, dplyr tries to prevent you from accidentally performing expensive query operations:

* Because there's generally no way to determine how many rows a query will 
  return unless you actually run it, `nrow()` is always `NA`.

* Because you can't find the last few rows without executing the whole 
  query, you can't use `tail()`.

```{r, error = TRUE}
nrow(tailnum_delay_db)

tail(tailnum_delay_db)
```

You can also ask the database how it plans to execute the query with `explain()`. The output is database dependent, and can be esoteric, but learning a bit about it can be very useful because it helps you understand if the database can execute the query efficiently, or if you need to create new indices.

## Creating your own database

If you don't already have a database, here's some advice from my experiences setting up and running all of them. SQLite is by far the easiest to get started with. PostgreSQL is not too much harder to use and has a wide range of built-in functions. In my opinion, you shouldn't bother with MySQL/MariaDB: it's a pain to set up, the documentation is subpar, and it's less featureful than Postgres. Google BigQuery might be a good fit if you have very large data, or if you're willing to pay (a small amount of) money to someone who'll look after your database.

All of these databases follow a client-server model - a computer that connects to the database and the computer that is running the database (the two may be one and the same but usually isn't). Getting one of these databases up and running is beyond the scope of this article, but there are plenty of tutorials available on the web.

### MySQL/MariaDB

In terms of functionality, MySQL lies somewhere between SQLite and PostgreSQL. It provides a wider range of [built-in functions](http://dev.mysql.com/doc/refman/5.0/en/functions.html). It gained support for window functions in 2018.

### PostgreSQL

PostgreSQL is a considerably more powerful database than SQLite. It has a much wider range of [built-in functions](http://www.postgresql.org/docs/current/static/functions.html), and is generally a more featureful database.

### BigQuery

BigQuery is a hosted database server provided by Google. To connect, you need to provide your `project`, `dataset` and optionally a project for `billing` (if billing for `project` isn't enabled). 

It provides a similar set of functions to Postgres and is designed specifically for analytic workflows. Because it's a hosted solution, there's no setup involved, but if you have a lot of data, getting it to Google can be an ordeal (especially because upload support from R is not great currently). (If you have lots of data, you can [ship hard drives](<https://cloud.google.com/storage/docs/offline-media-import-export>)!)
---
title: "Verb translation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Verb translation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


There are two parts to dbplyr SQL translation: translating dplyr verbs, and translating expressions within those verbs. This vignette describes how entire verbs are translated; `vignette("translate-function")` describes how individual expressions within those verbs are translated.

All dplyr verbs generate a `SELECT` statement. To demonstrate we'll make a temporary database with a couple of tables

```{r, message = FALSE}
library(dplyr)

con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
flights <- copy_to(con, nycflights13::flights)
airports <- copy_to(con, nycflights13::airports)
```

## Single table verbs

*  `select()` and `mutate()` modify the `SELECT` clause:

    ```{r}
    flights %>%
      select(contains("delay")) %>%
      show_query()
    
    flights %>%
      select(distance, air_time) %>%  
      mutate(speed = distance / (air_time / 60)) %>%
      show_query()
    ```
    
    (As you can see here, the generated SQL isn't always as minimal as you
    might generate by hand.)
    
*  `filter()` generates a `WHERE` clause:
    
    ```{r}
    flights %>% 
      filter(month == 1, day == 1) %>%
      show_query()
    ```

*  `arrange()` generates an `ORDER BY` clause:

    ```{r}
    flights %>% 
      arrange(carrier, desc(arr_delay)) %>%
      show_query()
    ```

*  `summarise()` and `group_by()` work together to generate a `GROUP BY` clause:

    ```{r}
    flights %>%
      group_by(month, day) %>%
      summarise(delay = mean(dep_delay)) %>%
      show_query()
    ```

## Dual table verbs

| R                | SQL
|------------------|------------------------------------------------------------
| `inner_join()`   | `SELECT * FROM x JOIN y ON x.a = y.a`
| `left_join()`    | `SELECT * FROM x LEFT JOIN y ON x.a = y.a`
| `right_join()`   | `SELECT * FROM x RIGHT JOIN y ON x.a = y.a`
| `full_join()`    | `SELECT * FROM x FULL JOIN y ON x.a = y.a`
| `semi_join()`    | `SELECT * FROM x WHERE EXISTS (SELECT 1 FROM y WHERE x.a = y.a)`
| `anti_join()`    | `SELECT * FROM x WHERE NOT EXISTS (SELECT 1 FROM y WHERE x.a = y.a)`
| `intersect(x, y)`| `SELECT * FROM x INTERSECT SELECT * FROM y`
| `union(x, y)`    | `SELECT * FROM x UNION SELECT * FROM y`
| `setdiff(x, y)`  | `SELECT * FROM x EXCEPT SELECT * FROM y`

`x` and `y` don't have to be tables in the same database. If you specify `copy = TRUE`, dplyr will copy the `y` table into the same location as the `x` variable. This is useful if you've downloaded a summarised dataset and determined a subset of interest that you now want the full data for. You can use `semi_join(x, y, copy = TRUE)` to upload the indices of interest to a temporary table in the same database as `x`, and then perform a efficient semi join in the database. 

If you're working with large data, it maybe also be helpful to set `auto_index = TRUE`. That will automatically add an index on the join variables to the temporary table.

## Behind the scenes

The verb level SQL translation is implemented on top of `tbl_lazy`, which basically tracks the operations you perform in a pipeline (see `lazy-ops.R`). Turning that into a SQL query takes place in three steps:

* `sql_build()` recurses over the lazy op data structure building up query 
  objects (`select_query()`, `join_query()`, `set_op_query()` etc) 
  that represent the different subtypes of `SELECT` queries that we might
  generate.

* `sql_optimise()` takes a pass over these SQL objects, looking for potential
  optimisations. Currently this only involves removing subqueries where
  possible.

* `sql_render()` calls an SQL generation function (`sql_select()`, `sql_join()`, 
  `sql_subquery()`, `sql_semijoin()` etc) to produce the actual SQL.
  Each of these functions is a generic, taking the connection as an argument,
  so that the details can be customised for different databases.
---
title: "Adding a new DBI backend"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Adding a new DBI backend}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)
```

This document describes how to add a new SQL backend to dbplyr. To begin:

* Ensure that you have a DBI compliant database backend. If not, you'll need
  to first create it by following the instructions in 
  `vignette("backend", package = "DBI")`.
  
* You'll need a working knowledge of S3. Make sure that you're 
  [familiar with the basics](http://adv-r.had.co.nz/OO-essentials.html#s3) 
  before you start.
 
This document is still a work in progress, but it will hopefully get you started. I'd also strongly recommend reading the bundled source code for [SQLite](https://github.com/tidyverse/dbplyr/blob/master/R/backend-sqlite.R), [MySQL](https://github.com/tidyverse/dbplyr/blob/master/R/backend-mysql.R), and [PostgreSQL](https://github.com/tidyverse/dbplyr/blob/master/R/backend-postgres.R).

## First steps

For interactive exploitation, attach dplyr and DBI. If you're creating a package, you'll need to import dplyr and DBI.

```{r setup, message = FALSE}
library(dplyr)
library(DBI)
```

Check that you can create a tbl from a connection, like:

```{r}
con <- DBI::dbConnect(RSQLite::SQLite(), path = ":memory:")
DBI::dbWriteTable(con, "mtcars", mtcars)

tbl(con, "mtcars")
```

If you can't, this likely indicates some problem with the DBI methods. Use [DBItest](https://github.com/rstats-db/DBItest) to narrow down the problem.

Now is a good time to implement a method for `db_desc()`. This should briefly describe the connection, typically formatting the information returned from `dbGetInfo()`. This is what dbplyr does for Postgres connections:

```{r}
#' @export
db_desc.PostgreSQLConnection <- function(x) {
  info <- dbGetInfo(x)
  host <- if (info$host == "") "localhost" else info$host

  paste0("postgres ", info$serverVersion, " [", info$user, "@",
    host, ":", info$port, "/", info$dbname, "]")
}
```

## Copying, computing, collecting and collapsing

Next, check that `copy_to()`, `collapse()`, `compute()`, and `collect()` work.

* If `copy_to()` fails, it's likely you need a method for `db_write_table()`,
  `db_create_indexes()` or `db_analyze()`.

* If `collapse()` fails, your database has a non-standard way of constructing 
  subqueries. Add a method for `sql_subquery()`.
  
* If `compute()` fails, your database has a non-standard way of saving queries
  in temporary tables. Add a method for `db_save_query()`.

## SQL translation

Make sure you've read `vignette("translation-verb")` so you have the lay of the land. 

### Verbs

Check that SQL translation for the key verbs work:

* `summarise()`, `mutate()`, `filter()` etc: powered by `sql_select()`
* `left_join()`, `inner_join()`: powered by `sql_join()`
* `semi_join()`, `anti_join()`: powered by `sql_semi_join()`
* `union()`, `intersect()`, `setdiff()`: powered by `sql_set_op()`

### Vectors

Finally, you may have to provide custom R -> SQL translation at the vector level by providing a method for `sql_translate_env()`. This function should return an object created by `sql_variant()`. See existing methods for examples.

---
title: "Function translation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Function translation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)
```

There are two parts to dbplyr SQL translation: translating dplyr verbs, and translating expressions within those verbs. This vignette describes how individual expressions (function calls) are translated; `vignette("translate-verb")` describes how entire verbs are translated.


```{r, message = FALSE}
library(dbplyr)
library(dplyr)
```

`dbplyr::translate_sql()` powers translation of individual function calls, and I'll use it extensively in this vignette to show what's happening. You shouldn't need to use it ordinary code as dbplyr takes care of the translation automatically.

```{r}
translate_sql((x + y) / 2)
```

`translate_sql()` takes an optional `con` parameter. If not supplied, this causes dplyr to generate (approximately) SQL-92 compliant SQL. If supplied, dplyr uses `sql_translate_env()` to look up a custom environment which makes it possible for different databases to generate slightly different SQL: see `vignette("new-backend")` for more details.  You can use the various simulate helpers to see the translations used by different backends:

```{r}
translate_sql(x ^ 2L)
translate_sql(x ^ 2L, con = simulate_sqlite())
translate_sql(x ^ 2L, con = simulate_access())
```

Perfect translation is not possible because databases don't have all the functions that R does. The goal of dplyr is to provide a semantic rather than a literal translation: what you mean, rather than precisely what is done. In fact, even for functions that exist both in databases and R, you shouldn't expect results to be identical; database programmers have different priorities than R core programmers. For example, in R in order to get a higher level of numerical accuracy, `mean()` loops through the data twice. R's `mean()` also provides a `trim` option for computing trimmed means; this is something that databases do not provide. 

If you're interested in how `translate_sql()` is implemented, the basic techniques that underlie the implementation of `translate_sql()` are described in ["Advanced R"](http://adv-r.hadley.nz/translation.html). 

## Basic differences

The following examples work through some of the basic differences between R and SQL.

*   `"` and `'` mean different things

    ```{r}
    # In SQLite variable names are escaped by double quotes:
    translate_sql(x)
    # And strings are escaped by single quotes
    translate_sql("x")
    ```
    
*   And some functions have different argument orders:
    
    ```{r}
    translate_sql(substr(x, 5, 10))
    translate_sql(log(x, 10))
    ```

*   R and SQL have different defaults for integers and reals. 
    In R, 1 is a real, and 1L is an integer. In SQL, 1 is an integer, and 1.0 is a real
    
    ```{r}
    translate_sql(1)
    translate_sql(1L)
    ```

## Known functions

### Mathematics

* basic math operators: `+`, `-`, `*`, `/`, `^`
* trigonometry: `acos()`, `asin()`, `atan()`, `atan2()`, `cos()`, `cot()`, `tan()`, `sin()`
* hypergeometric: `cosh()`, `coth()`, `sinh()`, `tanh()`
* logarithmic: `log()`, `log10()`, `exp()`
* misc: `abs()`, `ceiling()`, `sqrt()`, `sign()`, `round()`

## Modulo arithmetic

dbplyr translates `%%` and `%/%` to their SQL equivalents but note that they are not precisely the same: most databases use truncated division where the modulo operator takes the sign of the dividend, where R using the mathematically preferred floored division with the modulo sign taking the sign of the divisor.

```{r}
df <- tibble(
  x = c(10L, 10L, -10L, -10L), 
  y = c(3L, -3L, 3L, -3L)
)
mf <- tbl_memdb(df)

df %>% mutate(x %% y, x %/% y)

mf %>% mutate(x %% y, x %/% y)
```


### Logical comparisons

* logical comparisons: `<`, `<=`, `!=`, `>=`, `>`, `==`, `%in%`
* boolean operations: `&`, `&&`, `|`, `||`, `!`, `xor()`

### Aggregation

All database provide translation for the basic aggregations: `mean()`, `sum()`, `min()`, `max()`, `sd()`, `var()`. Databases automatically drop NULLs (their equivalent of missing values) whereas in R you have to ask nicely. The aggregation functions warn you about this important difference:

```{r}
translate_sql(mean(x))
translate_sql(mean(x, na.rm = TRUE))
```

Note that, by default, `translate()` assumes that the call is inside a `mutate()` or `filter()` and generates a window translation. If you want to see the equivalent `summarise()`/aggregation translation, use `window = FALSE`:

```{r}
translate_sql(mean(x, na.rm = TRUE), window = FALSE)
```

### Conditional evaluation

`if` and `switch()` are translate to `CASE WHEN`:

```{r}
translate_sql(if (x > 5) "big" else "small")
translate_sql(switch(x, a = 1L, b = 2L, 3L))
```
  
### String manipulation

### Date/time

* string functions: `tolower`, `toupper`, `trimws`, `nchar`, `substr`
* coerce types: `as.numeric`, `as.integer`, `as.character`

## Unknown functions

Any function that dplyr doesn't know how to convert is left as is. This means that database functions that are not covered by dplyr can be used directly via `translate_sql()`. Here a couple of examples that will work with [SQLite](http://www.sqlite.org/lang_corefunc.html):

```{r}
translate_sql(glob(x, y))
translate_sql(x %like% "ab%")
```

See `vignette("sql")` for more details.

## Window functions

Things get a little trickier with window functions, because SQL's window functions are considerably more expressive than the specific variants provided by base R or dplyr. They have the form `[expression] OVER ([partition clause] [order clause] [frame_clause])`:

* The __expression__ is a combination of variable names and window functions.
  Support for window functions varies from database to database, but most
  support the ranking functions, `lead`, `lag`, `nth`, `first`,
  `last`, `count`, `min`, `max`, `sum`, `avg` and `stddev`.

* The __partition clause__ specifies how the window function is broken down
  over groups. It plays an analogous role to `GROUP BY` for aggregate functions,
  and `group_by()` in dplyr. It is possible for different window functions to 
  be partitioned into different groups, but not all databases support it, and
  neither does dplyr.
  
* The __order clause__ controls the ordering (when it makes a difference).
  This is important for the ranking functions since it specifies which 
  variables to rank by, but it's also needed for cumulative functions and lead.
  Whenever you're thinking about before and after in SQL, you must always tell 
  it which variable defines the order. If the order clause is missing when 
  needed, some databases fail with an error message while others return 
  non-deterministic results.
  
* The __frame clause__ defines which rows, or __frame__, that are passed 
  to the window function, describing which rows (relative to the current row)
  should be included. The frame clause provides two offsets which determine
  the start and end of frame. There are three special values: -Inf means
  to include all preceding rows (in SQL, "unbounded preceding"), 0 means the
  current row ("current row"), and Inf means all following rows ("unbounded
  following"). The complete set of options is comprehensive, but fairly 
  confusing, and is summarised visually below.

    ```{r echo = FALSE, out.width = "100%"}
    knitr::include_graphics("windows.png", dpi = 300)
    ```

    Of the many possible specifications, there are only three that commonly
    used. They select between aggregation variants:

    * Recycled: `BETWEEN UNBOUND PRECEEDING AND UNBOUND FOLLOWING`
    
    * Cumulative: `BETWEEN UNBOUND PRECEEDING AND CURRENT ROW`
    
    * Rolling: `BETWEEN 2 PRECEEDING AND 2 FOLLOWING`
    
    dplyr generates the frame clause based on whether your using a recycled
    aggregate or a cumulative aggregate.
    
To see how individual window functions are translated to SQL, we can again use `translate_sql()`:

```{r}
translate_sql(mean(G))
translate_sql(rank(G))
translate_sql(ntile(G, 2))
translate_sql(lag(G))
```

If the tbl has been grouped or arranged previously in the pipeline, then dplyr will use that information to set the "partition by" and "order by" clauses. For interactive exploration, you can achieve the same effect by setting the `vars_group` and `vars_order` arguments to `translate_sql()`

```{r}
translate_sql(cummean(G), vars_order = "year")
translate_sql(rank(), vars_group = "ID")
```

There are some challenges when translating window functions between R and SQL, because dplyr tries to keep the window functions as similar as possible to both the existing R analogues and to the SQL functions. This means that there are three ways to control the order clause depending on which window function you're using:

* For ranking functions, the ordering variable is the first argument: `rank(x)`,
  `ntile(y, 2)`. If omitted or `NULL`, will use the default ordering associated 
  with the tbl (as set by `arrange()`).

* Accumulating aggregates only take a single argument (the vector to aggregate).
  To control ordering, use `order_by()`.
  
* Aggregates implemented in dplyr (`lead`, `lag`, `nth_value`, `first_value`, 
  `last_value`) have an `order_by` argument. Supply it to override the
  default ordering.

The three options are illustrated in the snippet below:

```{r, eval = FALSE}
mutate(players,
  min_rank(yearID),
  order_by(yearID, cumsum(G)),
  lead(G, order_by = yearID)
)
```

Currently there is no way to order by multiple variables, except by setting the default ordering with `arrange()`. This will be added in a future release. 

---
title: "Reprexes for dbplyr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{reprex}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

If you're reporting a bug in dbplyr, it is much easier for me to help you if you can supply a [reprex](https://reprex.tidyverse.org) that I can run on my computer. Creating reprexes for dbplyr is particularly challenging because you are probably using a database that you can't share with me. Fortunately, in many cases you can still demonstrate the problem even if I don't have the complete dataset, or even access to the database system that you're using.

This vignette outlines three approaches for creating reprexes that will work anywhere:

* Use `memdb_frame()`/`tbl_memdb()` to easily create datasets that live in an 
  in-memory SQLite database.
  
* Use `lazy_frame()`/`tbl_lazy()` to simulate SQL generation of dplyr pipelines.

* Use `translate_sql()` to simulate SQL generation of columnar expression.

```{r setup, message = FALSE}
library(dplyr)
library(dbplyr)
```

## Using `memdb_frame()`

The first place to start is with SQLite. SQLite is particularly appealing because it's completely embedded instead an R package so doesn't have any external dependencies. SQLite is designed to be small and simple, so it can't demonstrate all problems, but it's easy to try out and a great place to start.

You can easily create a SQLite in-memory database table using `memdb_frame()`:

```{r}
mf <- memdb_frame(g = c(1, 1, 2, 2, 2), x = 1:5, y = 5:1)
mf

mf %>% 
  group_by(g) %>% 
  summarise_all(mean, na.rm = TRUE)
```

Reprexes are easiest to understand if you create very small custom data, but if you do want to use an existing data frame you can use `tbl_memdb()`:

```{r}
mtcars_db <- tbl_memdb(mtcars)
mtcars_db %>% 
  group_by(cyl) %>% 
  summarise(n = n()) %>% 
  show_query()
```

## Translating verbs

Many problems with dbplyr come down to incorrect SQL generation. Fortunately, it's possible to generate SQL without a database using `lazy_frame()` and `tbl_lazy()`. Both take an `con` argument which takes a database "simulator" like `simulate_postgres()`, `simulate_sqlite()`, etc.

```{r}
x <- c("abc", "def", "ghif")

lazy_frame(x = x, con = simulate_postgres()) %>% 
  head(5) %>% 
  show_query()

lazy_frame(x = x, con = simulate_mssql()) %>% 
  head(5) %>% 
  show_query()
```

If you isolate the problem to incorrect SQL generation, it would be very helpful if you could also suggest more appropriate SQL.

## Translating individual expressions

In some cases, you might be able to track the problem down to incorrect translation for a single column expression. In that case, you can make your reprex even simpler with `translate_sql()`:

```{r}
translate_sql(substr(x, 1, 2), con = simulate_postgres())
translate_sql(substr(x, 1, 2), con = simulate_sqlite())
```
---
title: "Univariate Polynomials in R"
author: "Bill Venables"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    includes:
      in_header: header.tex
  html_document: null
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "",
                      fig.height = 7,
                      fig.width = 9,
                      fig.align = "center",
                      out.height = "0.25\\textheight")
library(polynom)
setHook("plot.new",
        list(las = function() par(las = 1),
             pch = function() par(pch = 20)),
        "append")
```

## Preamble

The `polynom` package is an R collection of functions to implement a class for
univariate polynomial manipulations.  It is based on the corresponding S
package by Bill Venables `<Bill.Venables@gmail.com>`, and was
adapted to R by Kurt Hornik `<Kurt.Hornik@R-project.org>` and Martin
Maechler `<maechler@stat.math.ethz.ch>`.

This document is based on the original 'NOTES', with minor updates.

# A Univariate Polynomial Class for R

## Introduction and summary

The following started as a straightforward programming exercise in
operator overloading, but seems to be more generally useful.  The goal
is to write a polynomial class, that is a suite of facilities that allow
operations on polynomials: addition, subtraction, multiplication,
"division", remaindering, printing, plotting, and so forth, to be
conducted using the same operators and functions, and hence with the
same ease, as ordinary arithmetic, plotting, printing, and so on.

The class is limited to univariate polynomials, and so they may
therefore be uniquely defined by their numeric coefficient vector.
Coercing a polynomial to numeric yields this coefficient vector as a
numeric vector.

For reasons of simplicity it is limited to REAL polynomials; handling
polynomials with complex coefficients would be a simple extension.
Dealing with polynomials with polynomial coefficients, and hence
multivariate polynomials, would be feasible, though a major undertaking
and the result would be very slow and of rather limited usefulness and
efficiency.

## General orientation

The function `polynomial()` creates an object of class `polynomial` from a
numeric coefficient vector.  Coefficient vectors are assumed to apply to
the powers of the carrier variable in increasing order, that is, in the
*truncated power series* form, and in the same form as required by
`polyroot()`, the system function for computing zeros of polynomials.  (As
a matter or terminology, the *zeros* of the polynomial $P(x)$ are the same
as the *roots* of equation $P(x) = 0$.)

Polynomials may also be created by specifying a set of (x, y) pairs and
constructing the Lagrange interpolation polynomial that passes through
them (`poly.calc(x, y)`).  If `y` is a matrix, an interpolation polynomial
is calculated for each column and the result is a list of polynomials
(of class `polylist`).

The third way polynomials are commonly generated is via its zeros using
`poly.calc(z)`, which creates the monic polynomial of lowest degree with
the values in `z` as its zeros.

The core facility provided is the group method function
`Ops.polynomial()`, which allows arithmetic operations to be performed on
polynomial arguments using ordinary arithmetic operators.

## Notes

 1. `+`, `-` and `*` have their obvious meanings for polynomials.
 
 2. `^` is limited to non-negative integer powers.
 
 3. `/` returns the polynomial quotient.  If division is not exact the
    remainder is discarded, (but see 4.)
 
 4. `%%` returns the polynomial remainder, so that if all arguments are
    polynomials, `p1 * (p2 / p1) + p2 %% p1` is the same polynomial as `p2`,
    provided `p1` is not the zero polynomial.
 
 5. If numeric vectors are used in polynomial arithmetic they are
    coerced to polynomial, which could be a source of surprise.  In the
    case of scalars, though, the result is natural.
 
 6. Some logical operations are allowed, but not always very
    satisfactorily.  `==` and `!=` mean exact equality or not,
    respectively, however `<`, `<=`, `>`, `>=`, `!`, `|` and `&` are not
    allowed at all and cause stops in the calculation. 
 
 7. Most Math group functions are disallowed with polynomial arguments.
    The only exceptions are `ceiling`, `floor`, `round`, `trunc`, and
    `signif`.
 
 8. Summary group functions are not implemented, apart from `sum` and `prod`.
 
 9. Polynomials may be evaluated at specific x values either directly
    using `predict(p, x)`, or indirectly using `as.function(p)`, which
    creates a function to evaluate the polynomial, and then using the
    result.
 
10. The print method for polynomials can be slow and is a bit
    pretentious.  The plotting methods (`plot(p)`, `lines(p)`, `points(p)`)
    are fairly nominal, but may prove useful.

## Examples

1.  Find the Hermite polynomials up to degree 5 and plot them.
    Also plot their derivatives and integrals on separate plots. 

    The polynomials in question satisfy 
$$ 
    \begin{aligned}
    He_0(x) &= 1,\\
    He_1(x) &= x,\\
    He_n(x) &= x  He_{n-1}(x) - (n - 1)  He_{n-2}(x), \qquad n = 2, 3, \ldots
    \end{aligned}
$$

```{r}
He <- list(polynomial(1), polynomial(0:1))
x <- polynomial()
for (n in 3:6) {
  He[[n]] <- x * He[[n-1]] - (n-2) * He[[n-2]] ## R indices start from 1, not 0
}
He <- as.polylist(He)
plot(He)
plot(deriv(He))
plot(integral(He))
```


2.  Find the orthogonal polynomials on $x = (0, 1, 2, 4)$ and
    construct R functions to evaluate them at arbitrary $x$ values.

```{r}
x <- c(0,1,2,4)
(op <- poly.orth(x))
(fop <- lapply(op, as.function))
(P <- sapply(fop, function(f) f(x)))
zapsmall(crossprod(P))     ### Verify orthonormality
```

3. Miscellaneous computations using polynomial arithmetic.

```{r}
(p1 <- poly.calc(1:6))
(p2 <- change.origin(p1, 3))
predict(p1, 0:7)
predict(p2, 0:7)
predict(p2, 0:7 - 3)
(p3 <- (p1 - 2 * p2)^2)         # moderate arithmetic expression.
fp3 <- as.function(p3)          # should have 1, 2, 3 as zeros
fp3(0:4)

```

4. Polynomials can be numerically fragile. This can easily lead to surprising numerical problems.

```{r}
x <- 80:89
y <- c(487, 370, 361, 313, 246, 234, 173, 128, 88, 83)

p <- poly.calc(x, y)        ## leads to catastropic numerical failure!
predict(p, x) - y

p1 <- poly.calc(x - 84, y)  ## changing origin fixes the problem
predict(p1, x - 84) - y

plot(p1, xlim = c(80, 89) - 84, xlab = "x - 84")
points(x - 84, y, col = "red", cex = 2)

#### Can we now write the polynomial in "raw" form?

p0 <- as.function(p1)(polynomial() - 84) ## attempt to change the origin back to zero 
                                         ## leads to problems again
plot(p0, xlim = c(80, 89))
points(x, y, col = "red", cex = 2)  ## major numerical errors due to finite precision

```

---
title: "SelectorGadget"
author: "Hadley Wickham"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SelectorGadget}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
embed_png <- function(path, dpi = NULL) {
  meta <- attr(png::readPNG(path, native = TRUE, info = TRUE), "info")
  if (!is.null(dpi)) meta$dpi <- rep(dpi, 2)

  knitr::asis_output(paste0(
    "<img src='", path, "'",
    " width=", round(meta$dim[1] / (meta$dpi[1] / 96)),
    " height=", round(meta$dim[2] / (meta$dpi[2] / 96)),
    " />"
  ))
}

knitr::opts_chunk$set(comment = "#>", collapse = TRUE)

```

```{r results="asis", echo=FALSE}
# directly adding css to output html without ruining css style https://stackoverflow.com/questions/29291633/adding-custom-css-tags-to-an-rmarkdown-html-document
cat("
<style>
img {
border: 0px;
outline: 0 ;
}
</style>
")
```

SelectorGadget is a JavaScript bookmarklet that allows you to interactively figure out what css selector you need to extract desired components from a page. 

## Installation

To install it, open this page in your browser, and then drag the following link to your bookmark bar: <a href="javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();">SelectorGadget</a>.

## Use

To use it, open the page 

1. Click on the element you want to select. SelectorGadget will make a first
   guess at what css selector you want. It's likely to be bad since it only
   has one example to learn from, but it's a start. Elements that match the
   selector will be highlighted in yellow.

2. Click on elements that shouldn't be selected. They will turn red.
   Click on elements that *should* be selected. They will turn green.

3. Iterate until only the elements you want are selected.  SelectorGadget
   isn't perfect and sometimes won't be able to find a useful css selector. 
   Sometimes starting from a different element helps.

For example, imagine we want to find the actors listed on an IMDB movie page, e.g. [The Lego Movie](http://www.imdb.com/title/tt1490017/).

1.  Navigate to the page and scroll to the actors list.

    ```{r, echo = FALSE}
    embed_png("selectorgadget-1-s.png")
    ```

2. Click on the SelectorGadget link in the bookmarks. The SelectorGadget
   console will appear at the bottom of the screen, and element currently
   under the mouse will be highlighted in orange.
   
    ```{r, echo = FALSE}
    embed_png("selectorgadget-2-s.png")
    ```

3.  Click on the element you want to select (the name of an actor). The 
    element you selected will be highlighted in green. SelectorGadget guesses
    which css selector you want (`a` in this case), and highlights
    all matches in yellow (see total count equal to 592 as indicated on 
    on the "Clear" button). This seems to be a little too excessive.

    ```{r, echo = FALSE}
    embed_png("selectorgadget-3-s.png")
    ```

4.  Scroll around the document to find elements that you don't want to match 
    and click on them. For example, we don't to match the character the actor 
    contributed to, so we click on it and it turns red. The css selector updates to 
    `.primary_photo+ td a`.

    ```{r, echo = FALSE}
    embed_png("selectorgadget-4-s.png")
    embed_png("selectorgadget-5-s.png")
    ```

Once we've determined the css selector, we can use it in R to extract the values we want:

```{r}
library(rvest)
lego_url <- "http://www.imdb.com/title/tt1490017/"
html <- read_html(lego_url)
cast <- html_nodes(html, ".primary_photo+ td a")
length(cast)
cast[1:2]
```

Finally, we can extract the text from the selected HTML nodes.

Looking carefully at this output, we see twice as many matches as we expected. That's because we've selected both the table cell and the text inside the cell. We can experiment with selectorgadget to find a better match or look at the html directly.

```{r}
html_text(cast, trim = TRUE)
```

Let's say we're also interested in extracting the links to the actors' pages. We can access html attributes of the selected nodes using `html_attrs()`.   

```{r}
cast_attrs <- html_attrs(cast)
length(cast_attrs)
cast_attrs[1:2]
```

As we can see there's only one attribute called `href` which contains relative url to the actor's page. We can extract it using `html_attr()`, indicating the name of the attribute of interest. Relative urls can be turned to absolute urls using `url_absolute()`.

```{r}
cast_rel_urls <- html_attr(cast, "href")
length(cast_rel_urls)
cast_rel_urls[1:2]

cast_abs_urls <- html_attr(cast, "href") %>% 
  url_absolute(lego_url)
cast_abs_urls[1:2]
```

---
title: "Harvesting the web with rvest"
author: "Dmytro Perepolkin"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Harvesting the web with rvest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
embed_png <- function(path, dpi = NULL) {
  meta <- attr(png::readPNG(path, native = TRUE, info = TRUE), "info")
  if (!is.null(dpi)) meta$dpi <- rep(dpi, 2)

  knitr::asis_output(paste0(
    "<img src='", path, "'",
    " width=", round(meta$dim[1] / (meta$dpi[1] / 96)),
    " height=", round(meta$dim[2] / (meta$dpi[2] / 96)),
    " />"
  ))
}

knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

```{r results="asis", echo=FALSE}
# directly adding css to output html without ruining css style https://stackoverflow.com/questions/29291633/adding-custom-css-tags-to-an-rmarkdown-html-document
cat("
<style>
img {
border: 0px;
outline: 0 ;
}
</style>
")
```


## Introduction to HTML elements

HTML stand for "Hyper Text Markup Language". HTML page consists of series of elements which browsers use to interpret how to display the content. HTML tags are names of the elements surrounded by angle brackets like so: `<tagname> content goes here... </tagname>`. Most HTML tags come in pairs and consist of opening and a closing tag, known as _start tag_ and _end tag_, where the _end tag_ name is preceded by the forward slash `/`.

Below is a visualization of a simple HTML page structure:

```{r, echo=FALSE}
embed_png("harvesting-web-1-s.png")
```

It is possible to define HTML attributes inside HTML tags. Attributes provide additional information about HTML elements, such as hyperlinks for text, and width and height for images. Attributes are always defined in the start tag and come in `name="value"` pairs, like so: `<a href="https://www.example.com">This is a link</a>`

You can learn more about HTML tags and attributes from online resources, such as [W3schools](https://www.w3schools.com/html/default.asp)

## CSS Selectors

CSS is a language that describes how HTML elements should be displayed. One of the ways to define useful shortcuts for selecting HTML elements to style is **CSS selectors**. CSS selectors represent *patterns* for locating HTML elements. They can be used not only for styling, but also for extracting the content of these elements. SelectorGadget (see `vignette("selectorgadget")`) implements a quick way of plucking HTML elements using CSS selectors.

Lets have a look at the IMDB page for the [Lego Movie](https://www.imdb.com/title/tt1490017/) and inspect the content of the page behind the fist entry in the Cast table. You can right-click the element you want to inspect and select Inspect or Inspect Element, depending on your browser. 

```{r, echo=FALSE}
embed_png("harvesting-web-2-s.png")
```

This will open Developer Tools with the Elements tab containing full HTML content of the page in the tree view, focused on the element you chose to inspect.

```{r, echo=FALSE}
embed_png("harvesting-web-3-s.png")
```

Two of very common HTML attributes are `class` and `id`. They are used for grouping and identifying HTML tags. In the screenshot above you can find examples of both of them. 

The tags containing `class` attribute can be selected using `.` followed by the name of the class. For example, you can open SelectorGadget and try selecting tags with classes "odd"(or "even") like so:

```{r, echo=FALSE}
embed_png("harvesting-web-4-s.png")
```

In order to search inside specific tag, selectors can be separated by space. Here, for example, we are locating tags with class "character" inside tags with class "odd" (i.e. we are searching for tags with class "character" among the descendants of tags with class "odd").

```{r, echo=FALSE}
embed_png("harvesting-web-5-s.png")
```

Another useful selector is for tags with a special attribute "id". Selector for the tags with this attribute can be composed with symbol `#` prepending the attribute value. This selector can be combined with another one as in the example below.

```{r, echo=FALSE}
embed_png("harvesting-web-6-s.png")
```

Other tags can be simply identified by name. SelectorGadget will usually do a fairly good job guessing CSS selector combination, but you can always inspect the page and adjust CSS selector as needed. Here's more realistic example of CSS selector suggested by SelectorGadget (from `vignette("selectogadget")`) which might be interpreted as: "Select tag `a` which is descendant of a tag `td` immediately following a tag with a class set to `.primary_photo`".

```{r, echo=FALSE}
embed_png("harvesting-web-7-s.png")
```

The tag with `+` sign indicates that we are interested not in the one identified by the selector, but the one "immediately following" it. In the example above, the tag containing the actor name does not have the "class" attribute, but it is located between (on the same level with) two other `td` tags: `td` tag with class set to `primary_photo` and `td` tag with class set to `character`. Therefore, SelectorGadget suggested the `+` syntax to locate the exact tag we need.

Overview of other useful CSS selectors can be found online, for example [here](https://www.w3schools.com/cssref/css_selectors.asp)

## Extracting HTML elements with CSS

Once the required section of the HTML document is located, it can be extracted with `rvest`. Let's look at the IMDB page for the [Lego Movie](https://www.imdb.com/title/tt1490017/) and extract the names of the characters the actors play. 

```{r}
library(rvest)
lego_url <- "http://www.imdb.com/title/tt1490017/"
html <- read_html(lego_url)

characters <- html_nodes(html, ".cast_list .character")

length(characters)

characters[1:2]
html_text(characters, trim=TRUE)
```

`rvest` contains a few handy functions for accessing different attributes of the currently selected nodes. For example, we can access names of the selected tags with `html_name()`:
```{r}
html_nodes(html,".cast_list") %>% 
  html_name()
```
Most of the time we want to use `html_nodes()` (returning `xml nodeset`) to make sure we capture all matches, but in those cases when we know that there is only one single match or when we only care about the first match, we can use `html_node()`, which returns single `xml node`.

```{r}
html_node(html,".cast_list")
```

If the name of the current tag is `table` (both as single `xml node`, but also as `xml nodeset` of tables) it can usually be parsed into a data.frame automatically (either as a single data.frame or as a list of data.frames, for `xml node` and `xml nodeset`, respectively).

```{r}
html_node(html,".cast_list") %>% 
  html_table() %>% 
  head()
```

The most basic content in HTML is text. It can be located between the opening and the closing tag of the "tag bracket" (sometimes along with other "descendant" tags). The text (from current and all "children" tags) can be extracted with `html_text()`. Lets extract text out of the first character node.

```{r}
html_node(html, ".cast_list .character") %>% 
  html_text()
```

Finally attributes of HTML tags can be extracted using `html_attrs()`(or `html_attr()` for extracting a single attribute from a node/nodeset, given its name). Let's fetch urls of the actors' pages.

```{r}
html_nodes(html, ".cast_list .character") %>% 
  html_children() %>% 
  html_attr("href")
```

## Extracting HTML elements with xpath

Alternative way of extracting HTML elements is using `xpath` argument in `html_node()` or `html_nodes()`, which allows specifying expressions to extract individual nodes or nodesets. `XPath` (XML Path Language) is a query language for selecting nodes from an XML document. It is much more powerful than CSS selectors, but its syntax is also more terse.

Say we want to extract text from the character column of the Cast table, but only the text without hyperlinks. If we look closer, we will see that the first `td` node of class `character` has two children nodes `a` interleaved with plain text in the following sequence:

- node `a` containing word "Batman" with hyperlink
- text "/" (forward slash sign)
- node `a` containing words "Bruce Wayne" with another hyperlink
- text "(voice)"

```{r, echo=FALSE}
embed_png("harvesting-web-8-s.png")
```

The following `rvest` code will separate "children" text from "parent" text and return only text from the "child" node(s).

```{r}
html_node(html, ".cast_list .character") %>% 
  html_children() %>% 
  html_text()
```

However, there's no suitable "CSS selector" to exclude child nodes and extract only text from the "parent" node. This is the situation where `xpath` selector can be more powerful. In addition to traversing the html/xml tree, `xpath` also has its own "extractor" functions, similar to those of `rvest`. In particular, here will will use `text()` applied to "current node only" (this is the meaning of `./` in the following code). We also use `normalize-space()` function, which drops empty strings.

Note that the `xpath` internal function `text()` has simplified the content, but `html_nodes()` is unaware of it and still returns `xml_nodeset`.

```{r}
html_node(html, ".cast_list .character") %>% 
  html_nodes(xpath="./text()[normalize-space()]")
```

We can now simplify this output and return values in a normal list.
```{r}
html_node(html, ".cast_list .character") %>% 
  html_nodes(xpath="./text()[normalize-space()]") %>% 
  html_text(trim=TRUE)
```

Please, refer to [XPath syntax reference here](https://www.w3schools.com/xml/xpath_syntax.asp) to learn more how to compose and use `xpath` to locate elements inside HTML tree. 
---
title: "Importing and exporting RSA/DSA/EC keys"
date: "`r Sys.Date()`"
output:
  html_document
vignette: >
  %\VignetteIndexEntry{Importing and exporting RSA/DSA/EC keys}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
library(openssl)
```

The `openssl` package implements a modern interface to libssl and libcrypto for R. It builds on the new `EVP` api which was introduced in OpenSSL 1.0 and provides a unified API to the various methods and formats. OpenSSL supports three major public key crypto systems:

 - __RSA__ : Most popular method. Supports both encryption and signatures.
 - __DSA__ : Digital Signature Algorithm. Mostly for signatures, not very popular anymore.
 - __ECDSA__ : Elliptic Curve DSA. Supports signatures and encryption via Diffie Hellman. Gaining popularity.

For each type there are several common __formats__ for storing keys and certificates: 

 - [__DER__](#the-der-format): binary format, serialized ASN.1 structure
 - [__PEM__](#the-pem-format): base64 encoded DER + header wrapped in `===`
 - [__SSH__](#the-openssh-format): single line of base64 with a header. Only for pubkeys.
 - [__JWK__](#the-json-web-key-jwk-format): JSON Web key. Stores key data in a JSON object
 
The openssl package automatically detects the format when possible. However being able to recognize the various formats can be useful.

### The DER format

DER is the standard __binary__ format using by protocols for storing and exchanging keys and certificates. It consists of a [serialized ASN.1](https://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One#Example_encoded_in_DER) structure which hold the key's (very large) prime numbers.


```{r}
key <- ec_keygen()
pubkey <- key$pubkey
bin <- write_der(pubkey)
print(bin)
```

To read a DER key use `read_key` or `read_pubkey` with `der = TRUE`. 

```{r}
read_pubkey(bin, der = TRUE)
```

Users typically don't need to worry about the key's underlying primes, but have a look at `key$data` if you are curious.

### The PEM format

In practice the user rarely encounters DER because it is mainly for internal use. When humans exchange keys and certificates they typically use the PEM format. PEM is simply **base64 encoded DER data, plus a header**. The header identifies the key (and possibly encryption) type. 

```{r}
cat(write_pem(pubkey))
cat(write_pem(key, password = NULL))
```

The PEM format allows for protecting private keys with a password. R will prompt you for the password when reading such a protected key.

```{r}
cat(write_pem(key, password = "supersecret"))
```

### The OpenSSH format

For better or worse, OpenSSH uses a custom format for **public keys**. The advantage of this format is that it fits on a single line which is nice for e.g. your `~/.ssh/known_hosts` file. There is no special format for private keys, OpenSSH uses PEM as well.

```{r}
str <- write_ssh(pubkey)
print(str)
```

The `read_pubkey` function will automatically detect if a file contains a `PEM` or `SSH` key.

```{r}
read_pubkey(str)
```

### The JSON Web Key (JWK) format

Yet another recent format to store RSA or EC keys are JSON Web Keys (JWK). JWK is part of the **Javascript Object Signing and Encryption (JOSE)** specification. The `write_jwk` and `read_jwk` functions are implemented in a separate package which uses the `openssl` package.

```{r}
library(jose)
json <- write_jwk(pubkey)
jsonlite::prettify(json)
```

Keys from `jose` and `openssl` are the same.

```{r}
mykey <- read_jwk(json)
identical(mykey, pubkey)
print(mykey)
```
---
title: "Generating Secure Random Numbers in R"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Generating Secure Random Numbers in R}
  \usepackage[utf8]{inputenc}  
output:
  html_document
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(comment = "")
library(openssl)
```

The `rand_bytes` function binds to [RAND_bytes](https://www.openssl.org/docs/manmaster/man3/RAND_bytes.html) in OpenSSL to generate cryptographically strong pseudo-random bytes. See the OpenSSL documentation for what this means. 

```{r}
rnd <- rand_bytes(10)
print(rnd)
```

Bytes are 8 bit and hence can have `2^8 = 256` possible values.

```{r}
as.numeric(rnd)
```

Each random byte can be decomposed into 8 random bits (booleans)

```{r}
x <- rand_bytes(1)
as.logical(rawToBits(x))
```

## Secure Random Numbers

`rand_num` is a simple (2 lines) wrapper to `rand_bytes` to generate random numbers (doubles) between 0 and 1.

```{r}
rand_num(10)
```

To map random draws from [0,1] into a probability density, we can use a [Cumulative Distribution Function](http://en.wikipedia.org/wiki/Cumulative_distribution_function). For example we can combine `qnorm` and `rand_num` to simulate `rnorm`:

```{r}
# Secure rnorm
x <- qnorm(rand_num(1000), mean = 100, sd = 15)
hist(x)
```

Same for discrete distributions:

```{r}
# Secure rbinom
y <- qbinom(rand_num(1000), size = 20, prob = 0.1)
hist(y, breaks = -.5:(max(y)+1))
```
---
title: "Cryptographic Hashing in R"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Cryptographic Hashing in R}
  \usepackage[utf8]{inputenc}  
output:
  html_document
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(comment = "")
library(openssl)
```

The functions `sha1`, `sha256`, `sha512`, `md4`, `md5` and `ripemd160` bind to the respective [digest functions](https://www.openssl.org/docs/manmaster/man1/dgst.html) in OpenSSL's libcrypto. Both binary and string inputs are supported and the output type will match the input type.

```{r}
md5("foo")
md5(charToRaw("foo"))
```

Functions are fully vectorized for the case of character vectors: a vector with n strings will return n hashes.

```{r}
# Vectorized for strings
md5(c("foo", "bar", "baz"))
```

Besides character and raw vectors we can pass a connection object (e.g. a file, socket or url). In this case the function will stream-hash the binary contents of the connection.

```{r}
# Stream-hash a file
myfile <- system.file("CITATION")
md5(file(myfile))
```

Same for URLs. The hash of the [`R-3.1.1-win.exe`](http://cran.us.r-project.org/bin/windows/base/old/3.1.1/R-3.1.1-win.exe) below should match the one in [`md5sum.txt`](http://cran.us.r-project.org/bin/windows/base/old/3.1.1/md5sum.txt) 

```{r eval=FALSE}
# Stream-hash from a network connection
md5(url("http://cran.us.r-project.org/bin/windows/base/old/3.1.1/R-3.1.1-win.exe"))
```

## Compare to digest

Similar functionality is also available in the **digest** package, but with a slightly different interface:

```{r}
# Compare to digest
library(digest)
digest("foo", "md5", serialize = FALSE)

# Other way around
digest(cars, skip = 0)
md5(serialize(cars, NULL))
```
---
title: "Fun with bignum: how RSA encryption works"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Fun with bignum: how RSA encryption works}
  \usepackage[utf8]{inputenc}  
output:
  html_document
---

```{r setup, include=FALSE}
library(openssl)
knitr::opts_chunk$set(echo = TRUE)
```


Primitive types such as `int` or `double` store numbers in exactly 4 or 8 bytes, with finite precision. This suffices for most applications, but cryptography requires arithmetic on very large numbers, without loss of precision. Therefore OpenSSL uses a __bignum__ data type which holds arbitrary sized integers and implements all basic arithmetic and comparison operators such as `+`, `-`, `*`, `^`, `%%`, `%/%`, `==`, `!=`, `<`, `<=`, `>` and `>=`.

One special case, the [__modular exponent__](https://en.wikipedia.org/wiki/Modular_exponentiation) `a^b %% m` can be calculated using `bignum_mod_exp`, even when `b` is too large for calculating `a^b`.

```{r}
# create a bignum
y <- bignum("123456789123456789")
z <- bignum("D41D8CD98F00B204E9800998ECF8427E", hex = TRUE)

# size grows
print(y * z)

# Basic arithmetic
div <- z %/% y
mod <- z %% y
z2 <- div * y + mod
stopifnot(z2 == z)
stopifnot(div < z)
```

RSA involves a public key and a private key. The public key should be known by everyone and is used for encrypting messages. Messages encrypted with the public key can only be decrypted in a reasonable amount of time using the private key. In RSA, this asymmetry is based on the practical difficulty of factoring the product of two large prime numbers.

### RSA key generation

An RSA key-pair is generated as follows (adapted from [wikipedia](https://en.wikipedia.org/wiki/RSA_(cryptosystem))):

 - Choose two distinct prime numbers $p$ and $q$. Keep these secret.
 - Compute the product $n = p*q$. This $n$ value is public and used as the modulus.
 - Compute $\phi(n) = (p − 1)(q − 1)$.
 - Choose an integer $e$ smaller than $\phi(n)$ such that $e$ and $\phi(n)$ are coprime. OpenSSL always uses $65537$.
 - Compute a value for $d$ such that $(d * e)\pmod{\phi(n)} = 1$.
 
OpenSSL has a key generator that does these things for us. 

```{r}
(key <- rsa_keygen(512))
(pubkey <- key$pubkey)
```

Usually we would use `rsa_encrypt` and `rsa_decrypt` to perform the encryption:

```{r}
msg <- charToRaw("hello world")
ciphertext <- rsa_encrypt(msg, pubkey)
rawToChar(rsa_decrypt(ciphertext, key))
```

Let's look at how this works under the hood.

### How RSA encryption works

The `data` field of the private key extracts the underlying bignum integers:

```{r}
key$data
```

You can verify that the equations above hold for this key. The public key is simply a subset of the key which only contains $n$ and $e$:

```{r}
pubkey$data
```

In order to encrypt a message into ciphertext we have to treat the message data as an integer. The message cannot be larger than the key size. For example convert the text `hello world` into an integer:

```{r}
m <- bignum(charToRaw("hello world"))
print(m)
```

To encrypt this message $m$ into ciphertext $c$ we calculate $c = m^e\pmod n$. Using the public key from above:

```{r}
e <- pubkey$data$e
n <- pubkey$data$n
c <- (m ^ e) %% n
print(c)
```

This number represents our encrypted message! It is usually exchanged using base64 notation for human readability:

```{r}
base64_encode(c)
```

The ciphertext can be decrypted using $d$ from the corresponding private key via $m = c^d \pmod{n}$. Note that `c^d` is too large to calculate directly so we need to use `bignum_mod_exp` instead.

```{r}
d <- key$data$d
out <- bignum_mod_exp(c, d, n)
rawToChar(out)
```

The only difference with the actual `rsa_encrypt` and `rsa_decrypt` functions is that these add some additional padding to the data. 
---
# Example from https://joss.readthedocs.io/en/latest/submitting.html
title: 'SimplyAgree: An R package and jamovi Module for Simplifying Agreement and Reliability Analyses'
tags:
  - R
  - statistics
  - reliability
  - agreement
  - Bland-Altman
authors:
  - name: Aaron R. Caldwell
    orcid: 0000-0002-4541-6283
    affiliation: "1, 2" # (Multiple affiliations must be quoted)
affiliations:
 - name: United States Army Research Institute of Environmental Medicine
   index: 1
 - name: Oak Ridge Institute of Science and Education
   index: 2
citation_author: Caldwell
year: 2021
bibliography: paper.bib
csl: apa.csl
journal: JOSS
output: 
  pdf_document:
    fig_caption: no
---
Aaron R. Caldwell^1,2^

1. United States Army Research Institute of Environmental Medicine, Natick, MA.
2. Oak Ridge Institute of Science and Education, Oak Ridge, TN.

# Summary

Accurate and reliable measurements are critical to quantitative research efforts. Based on citation counts, researchers highly value methods to quantify the accuracy and reliability of the measurement tools [@bland1986; @weir2005]. This article introduces the `SimplyAgree` R package and jamovi module as user-friendly solutions for estimating agreement and reliability [@R-base; @jamovi]. 

# Statement of Need

A number of new methods have been developed in the past three decades to improve the calculation of the limits of agreement [@shieh2019;  @lin1989; @zou2011] and other measures of measurement reliability [@weir2005; @carrasco2013]. However, to author's best knowledge, statistical software &mdash; particularly open source software &mdash; to implement these statistical analyses is lacking. While some software may provide the limits of agreement analysis outlined by Bland & Altman [-@bland1986; -@bland1999], few, if any, account for multiple observations within the same research subject [@zou2011] or include hypothesis tests of agreement [@shieh2019]. Many researchers may not have the skills necessary to write the code, from scratch, in order to implement many of the newest techniques. @jamovi is a open source statistical platform that provides a graphical user interface (GUI), and therefore is an accessible source for researchers without coding experience. Therefore, a jamovi module of `SimplyAgree` was also created in order to reach those researchers who may not have the coding expertise required to effectively use the R package.

# Current R Capabilities

The R package `SimplyAgree`, currently v0.0.2 on the comprehensive R archive network (CRAN), implements a number of useful agreement and reliability analyses.

The current release of the R package can be downloaded directly from CRAN in R:

```
install.packages("SimplyAgree")
```

Or, the developmental version, can be downloaded from GitHub:

```
devtools::install_github("arcaldwell49/SimplyAgree")
```
There are 2 vignettes that document the major functions within the package that can be found on the package's website (https://aaroncaldwell.us/SimplyAgree). Overall, there are 6 fundamental functions, all with generic `plot` and `print` methods, within the R package:

1. `agree_test`: Simple Test of Agreement. This is function performs agreement analyses on two vectors of the same length, and is designed for analyses that were described by Bland & Altman [-@bland1986; -@bland1999]. In addition to providing the traditional Bland-Altman limits of agreement, the function provides a hypothesis test [@shieh2019], and provides the concordance correlation coefficient [@lin1989].

2. `agree_reps`: Test of Agreement for Replicate Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, does not vary. In addition, the concordance correlation coefficient, calculated by U-statistics, is also provided in the output [@carrasco2013].

3. `agree_nest`: Test of Agreement for Nested Data. This function provides the limits of agreement described by @zou2011 for data where the mean, per subject, *does* vary. Similar to the replicate data function, the concordance correlation coefficient, calculated by U-statistics, is provided in the output [@carrasco2013].

4. `loa_mixed`: Bootstrapped Limits of Agreement for Nested Data. This function calculates limits of agreement using a non-parametric bootstrap method, and can allow the underlying mean to vary (replicate data) or not (nested data).

5. `blandPowerCurve`: Power Analysis for Bland-Altman Limits of Agreement. This function implements the formula outlined by @lu2016. This allows for power calculations for the @bland1999 limits of agreement. The function `find_n` can then be used to find the sample size at which adequate power (defined by the user) is achieved.

6. `reli_stats`: Reliability Statistics. This function calculates and provides as output the statistics outlined by @weir2005. This includes an array of intraclass correlation coefficients, the coefficient of variation, and the standard error of measurement.

\newpage

# Current jamovi Capabilities

The jamovi module can be added to the jamovi directly from the "add module" tab in the GUI.

![**Figure 1**: How to add a module in jamovi.](module_button.PNG)

The `SimplyAgree` module is then available on the main menu, and within it there are three analysis options.

![**Figure 2**: SimplyAgree in jamovi.](simplyagree_button.PNG)

The three analysis options essentially enable jamovi users to complete some of the same analyses available in the R package.

1. The simple agreement analysis incorporates the `agree_test` function. Users have the option of including the concordance correlation coefficient, and plots of the data.

![**Figure 3**: Sample Output from the Simple Agreement Analysis.](simple_agreement.PNG)

2. The nested/replicate agreement analysis uses the `agree_nest` and `agree_reps` function to perform the analyses. The `agree_reps` function is used if "Assume underlying value does not vary?" is selected; otherwise `agree_nest` is used.

![**Figure 4**:Sample Output from the Nested/Replicate Agreement Analysis.](nested_agreement.PNG)


3. The reliability analysis utilizes `reli_stats` to calculate reliability statistics.

![**Figure 5**: Sample Output from the Reliability Analsyis.](reliability.PNG)

\newpage

# Acknowledgements

I would like the thank Ashley Akerman for his kind feedback during the development of this package. 

The opinions or assertions contained herein are the private views of the author and are not to be construed as official or reflecting the views of the Army or the Department of Defense. Any citations of commercial organizations and trade names in this report do not constitute an official Department of the Army endorsement of approval of the products or services of these organizations. No authors have any conflicts of interest to disclose. Approved for public release; distribution is unlimited.

# References
---
title: "Re-analysis of an Agreement Study"
output: rmarkdown::html_vignette
bibliography: ref.bib
vignette: >
  %\VignetteIndexEntry{Re-analysis of an Agreement Study}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: inline
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE,warning=FALSE}
library(SimplyAgree)
library(cccrm)
library(tidyverse)
library(ggpubr)
data("temps")
df_temps = temps
```

# Re-analysis of a Previous Study of Agreement

In the study by @ravanelli2020change, they attempted to estimate the effect of varying the time of day (AM or PM) on the measurement of thermoregulatory variables (e.g., rectal and esophageal temperature). In total, participants completed 6 separate trials wherein these variables were measured. While this is a robust study of these variables the analyses focused on ANOVAs and t-tests to determine whether or not the time-of-day (AM or PM). This poses a problem because 1) they were trying to test for equivalence and 2) this is a study of *agreement* not *differences* (See @Lin1989). Due to the latter point, the use of t-test or ANOVAs (F-tests) is rather inappropriate since they provide an answer to different, albeit related, question.

Instead, the authors could test their hypotheses by using tools that  estimate the absolute *agreement* between the AM and PM sessions within each condition. This is rather complicated because we have multiple measurement within each participant. However, between the tools included in `SimplyAgree` and `cccrm` [@cccrm] I believe we can get closer to the right answer.

In order to understand the underlying processes of these functions and procedures I highly recommend reading the statistical literature that documents methods within these functions. For the `cccrm` package please see the work by @carrasco2003, @carrasco2009, and @carrasco2013. The `loa_mixed` function was inspired by the work of @Parker2016 which documented how to implement multi-level models and bootstrapping to estimate the limits of agreement. 

# Concordance

An easy approach to measuring agreement between 2 conditions or measurement tools is through the concordance correlation coefficient (CCC). The CCC essentially provides a single coefficient (values between 0 and 1) that provides an estimate to how closely one measurement is to another. In its simplest form it is a type of intraclass correlation coefficient that takes into account the mean difference between two measurements. In other words, if we were to draw a line of identity on a graph and plot two measurements (X & Y), the closer those points are to the line of identity the higher the CCC (and vice versa).

```{r, fig.cap="Example of the Line of Identity"}
qplot(1,1) + geom_abline(intercept = 0, slope = 1)
```

In the following sections, let us see how well esophageal and rectal temperature are in agreement after exercising in the heat for 1 hour at differing conditions.

## Rectal Temperature

Now, based on the typical thresholds (0.8 can be considered a "good" CCC), neither Trec as a raw value or a change score (Trec delta) is within acceptable degrees of agreement. As I will address later, this may not be an accurate and there are sometimes where there is a low CCC but the expected differences between conditions is acceptable (limits of agreement).

```{r}
ccc_rec.post = cccrm::cccUst(dataset = df_temps,
                            ry = "trec_post",
                            rtime = "trial_condition",
                            rmet = "tod")

ccc_rec.post
```


```{r}
ccc_rec.delta = cccrm::cccUst(dataset = df_temps,
                            ry = "trec_delta",
                            rtime = "trial_condition",
                            rmet = "tod")

ccc_rec.delta
```

Finally, we can visualize the concordance between the two different types of measurements and the respective time-of-day and conditions. From the plot we can see there is clear bias in the raw post exercise values (higher in the PM), but even when "correcting for baseline differences" by calculating the differences scores we can see a higher degree of disagreement between the two conditions. 

```{r pltsrec,fig.cap="Concordance Plots of Rectal Temperature",echo=FALSE,fig.width=8.5,fig.height=5}
df_rec.delta = df_temps %>%
  mutate(id_spec = paste0(id,"_",trial_condition)) %>%
  select(id,id_spec,trec_delta,tod,trial_condition) %>%
  pivot_wider(id_cols = c(id,id_spec,trial_condition),
              names_from = tod,
              values_from = trec_delta) %>%
  mutate(diff = PM - AM,
         Average = (AM + PM)/2)

df_rec.post = df_temps %>%
  mutate(id_spec = paste0(id,"_",trial_condition)) %>%
  select(id,id_spec,trec_post,tod,trial_condition) %>%
  pivot_wider(id_cols = c(id,id_spec,trial_condition),
              names_from = tod,
              values_from = trec_post) %>%
  mutate(diff = PM - AM,
         Average = (AM + PM)/2)

p_rec.delta <- ggplot(df_rec.delta, aes(x=AM, y=PM)) +
  stat_smooth(aes(color=trial_condition), 
              method = "lm",
              formula = 'y ~ x',
              fullrange = TRUE,
              geom='line', alpha=0.5, se=FALSE)+
  scale_x_continuous("AM - Trec (delta)",
                     limits = c(.15,1.1))+
  scale_y_continuous("PM - Trec (delta)",
                     limits = c(.15,1.1))+
  geom_abline(intercept = 0, slope = 1, size = .85,
              alpha = .75) +
  geom_point(aes(color=trial_condition),
             alpha = .75) +
  theme_classic() +
  theme(legend.position = "bottom") +
  scale_color_viridis_d()

p_rec.post <- ggplot(df_rec.post, aes(x=AM, y=PM)) +
  stat_smooth(aes(color=trial_condition), 
              method = "lm",
              formula = 'y ~ x',
              fullrange = TRUE,
              geom='line', alpha=0.75, se=FALSE)+
  scale_x_continuous("AM - Trec (post)",
                     limits = c(36.4,38.5))+
  scale_y_continuous("PM - Trec (post)",
                     limits = c(36.4,38.5))+
  geom_abline(intercept = 0, slope = 1, size = .85,
              alpha = .75) +
  geom_point(aes(color=trial_condition),
             alpha = .75) +
  theme_classic() +
  theme(legend.position = "bottom") +
  scale_color_viridis_d()
ggarrange(p_rec.post,p_rec.delta)
```

# Esophageal Temperature

We can replicate the same analyses for esophageal temperature. From the data and plots below we can see that the post exercise CCC is much improved compared to rectal temperature. However, there is no further improvement when looking at the delta (difference scores) for pre-to-post exercise. 

```{r}
ccc_eso.post = cccrm::cccUst(dataset = df_temps,
                            ry = "teso_post",
                            rtime = "trial_condition",
                            rmet = "tod")

ccc_eso.post
```


```{r}
ccc_eso.delta = cccrm::cccUst(dataset = df_temps,
                            ry = "teso_delta",
                            rtime = "trial_condition",
                            rmet = "tod")

ccc_eso.delta
```

```{r pltseso,fig.cap="Concordance Plots of Esophageal Temperature",echo=FALSE,fig.width=8.5,fig.height=5}
df_eso.delta = df_temps %>%
  mutate(id_spec = paste0(id,"_",trial_condition)) %>%
  select(id,id_spec,teso_delta,tod,trial_condition) %>%
  pivot_wider(id_cols = c(id,id_spec,trial_condition),
              names_from = tod,
              values_from = teso_delta) %>%
  mutate(diff = PM - AM,
         Average = (AM + PM)/2)

df_eso.post = df_temps %>%
  mutate(id_spec = paste0(id,"_",trial_condition)) %>%
  select(id,id_spec,teso_post,tod,trial_condition) %>%
  pivot_wider(id_cols = c(id,id_spec,trial_condition),
              names_from = tod,
              values_from = teso_post) %>%
  mutate(diff = PM - AM,
         Average = (AM + PM)/2)

p_eso.delta <- ggplot(df_eso.delta, aes(x=AM, y=PM)) +
  stat_smooth(aes(color=trial_condition), 
              method = "lm",
              formula = 'y ~ x',
              fullrange = TRUE,
              geom='line', alpha=0.5, se=FALSE)+
  scale_x_continuous("AM - Teso (delta)",
                     limits = c(.15,1.1))+
  scale_y_continuous("PM - Teso (delta)",
                     limits = c(.15,1.1))+
  geom_abline(intercept = 0, slope = 1, size = .85,
              alpha = .75) +
  geom_point(aes(color=trial_condition),
             alpha = .75) +
  theme_classic() +
  theme(legend.position = "bottom") +
  scale_color_viridis_d()

p_eso.post <- ggplot(df_eso.post, aes(x=AM, y=PM)) +
  stat_smooth(aes(color=trial_condition), 
              method = "lm",
              formula = 'y ~ x',
              fullrange = TRUE,
              geom='line', alpha=0.75, se=FALSE)+
  scale_x_continuous("AM - Teso (post)",
                     limits = c(36.4,38.5))+
  scale_y_continuous("PM - Teso (post)",
                     limits = c(36.4,38.5))+
  geom_abline(intercept = 0, slope = 1, size = .85,
              alpha = .75) +
  geom_point(aes(color=trial_condition),
             alpha = .75) +
  theme_classic() +
  theme(legend.position = "bottom") +
  scale_color_viridis_d()
ggarrange(p_eso.post,p_eso.delta)
```

# Limits of Agreement

In addition to the CCC we can use the `loa_mixed` function in order to calculate the "limits of agreement". Typically the 95% Limits of Agreement are calculated which provide the difference between two measuring systems for 95% of future measurements pairs. In order to do that we will need the data in a "wide" format where each measurement (in this case AM and PM) are their own column and then we can calculate a column that is the difference score. Once we have the data in this "wide" format, we can then use the `loa_mixed` function to calculate the average difference (mean bias) and the variance (which determines the limits of agreement).

```{r, echo=FALSE}
rec.post_loa = readr::read_rds("rec_post_loa.rds")
rec.delta_loa = readr::read_rds("rec_delta_loa.rds")

eso.post_loa = readr::read_rds("eso_post_loa.rds")
eso.delta_loa = readr::read_rds("eso_delta_loa.rds")

```

## Rectal Temperature

So we will calculate the limits of agreement using the `loa_mixed` function. We will need to identify the columns with the right information using the `diff`, `condition`, and `id` arguments. We then select the right data set using the `data` argument. Lastly, we specify the specifics of the conditions for how the limits are calculated. For this specific analysis I decided to calculate 95% limits of agreement with 95% confidence intervals, and I will use bias-corrected accelerated (bca) bootstrap confidence intervals.

```{r, eval=FALSE}
rec.post_loa = SimplyAgree::loa_mixed(diff = "diff",
                                     condition = "trial_condition",
                                     id = "id",
                                     data = df_rec.post,
                                     conf.level = .95,
                                     agree.level = .95,
                                     replicates = 199,
                                     type = "bca")
```

When we create a table of the results we can see that CCC and limits of agreement (LoA), at least for Trec post exercise, are providing the same conclusion (poor agreement).

```{r}
knitr::kable(rec.post_loa$loa,
             caption = "LoA: Trec Post Exercise")
```

Furthermore, we can visualize the results with a typical Bland-Altman plot of the LoA.

```{r, fig.cap="Limits of Agreement for Trec Post Exercise",fig.width=7,fig.height=5}
plot(rec.post_loa)
```

Now, when we look at the Delta values for Trec we find that there is much closer agreement (maybe even acceptable agreement) when we look at LoA. However, we cannot say that the average difference would be less than 0.25 which may not be acceptable for some researchers.
```{r}
knitr::kable(rec.delta_loa$loa,
             caption = "LoA: Delta Trec")
```

```{r, fig.cap="Limits of Agreement for Delta Trec",fig.width=7,fig.height=5}
plot(rec.delta_loa)
```

## Esophageal Temperature

We can repeat the process for esophageal temperature. Overall, the results are fairly similar, and while there is better agreement on the delta (change scores), it is still fairly difficult to determine that there is "good" agreement between the AM and PM measurements.

```{r, eval=FALSE}
eso.post_loa = SimplyAgree::loa_mixed(diff = "diff",
                                     condition = "trial_condition",
                                     id = "id",
                                     data = df_eso.post,
                                     conf.level = .95,
                                     agree.level = .95,
                                     replicates = 199,
                                     type = "bca")
```

```{r}
knitr::kable(eso.post_loa$loa,
             caption = "LoA: Teso Post Exercise")
```


```{r, fig.cap="Limits of Agreement for Teso Post Exercise",fig.width=7,fig.height=5}
plot(eso.post_loa)
```


```{r}
knitr::kable(eso.delta_loa$loa,
             caption = "LoA: Delta Teso")
```

```{r, fig.cap="Limits of Agreement for Delta Teso",fig.width=7,fig.height=5}
plot(eso.delta_loa)
```


# References

---
title: "Introduction to SimplyAgree"
author: "Aaron R. Caldwell"
date: "Last Updated: `r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{Introduction to SimplyAgree}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The `SimplyAgree` R package was created to make the process of quantifying measurement agreement, consistency, and reliability. This package expands upon the capabilities of currently available R packages (such as `psych` and `blandr`) by 1) providing support for agreement studies that involve multiple observations (`agree_test` and `agree_reps` functions) 2) provide robust tests of agreement even in simple studies (`shieh_test` output from the `agree_test` function) and 3) a robust set of reliability statistics (`reli_stats` function).

In this vignette I will shortly demonstrate the implementation of each function and *most* of the underlying calculations within each function

```{r}
library(SimplyAgree)
```


# Simple Agreement

## `agree_test`

In the simplest scenario, a study may be conducted to compare one measure (e.g., `x`) and another (e.g., `y`). In this scenario each pair of observations (x and y) are *independent*; meaning that each pair represents one subject/participant. In most cases we have a degree of agreement that we would deem adequate. This may constitute a hypothesis wherein you may believe the agreement between two measurements is within a certain limit (limits of agreement). If this is the goal then the `agree_test` function is what you need to use in this package.

The data for the two measurements are put into the `x` and `y` arguments. If there is a hypothesized limit of agreement then this can be set with the `delta` argument (this is optional). Next, the limit of agreement can be set with the `agree.level` and the confidence level ($1-\alpha$). Once those are set the analysis can be run. Please note, this package has pre-loaded data from the Zou 2013 paper. While data does not conform the assumptions of the test it can be used to test out many of the functions in this package. Since there isn't an *a priori* hypothesis I will not declare a `delta` argument, but I will estimate the 95% confidence intervals for 80% limits of agreement.

```{r}
a1 = agree_test(x = reps$x,
                y = reps$y,
                agree.level = .8)
```

We can then print the general results. These results include the general parameters of the analysis up top, then the results of the Shieh exact test for agreement (no conclusion is included due to the lack of a `delta` argument being set). Then the limits of agreement, with confidence limits, are included. Lastly, Lin's Concordance Correlation Coefficient, another measure of agreement, is also included.

```{r}
print(a1)
```

Next, we can use the generic `plot` function to produce visualizations of agreement. This includes the Bland-Altman plot (`type = 1`) and a line-of-identity plot (`type = 2`).

```{r, fig.width=6, fig.height=6}
plot(a1, type = 1)

plot(a1, type = 2)
```

## Calculations in `agree_test`

### Shieh's test

The hypothesis test procedure is based on the "exact" approach details by @shieh2019. In this procedure the null hypothesis (not acceptable agreement) is rejected if the extreme lower bound and upper bound are within the proposed agreement limits. The agreement limits ($\hat\theta_{EL} \space and \space \hat\theta_{EU}$) are calculated as the following:

$$
\hat\theta_{EL,EU} = \bar{d} \pm \gamma_{1-\alpha}\cdot \frac{S}{\sqrt{N}}
$$

wherein $\bar{d}$ is the mean difference between the two methods, $S$ is the standard deviation of the sample, $N$ is the total number of pairs, and $\gamma_{1-\alpha}$ critical value (which requires a specialized function within `R` to estimate).

### Limits of Agreement

The reported limits of agreement are derived from the work of @bland1986 and @bland1999.

$$
LoA = \bar{d} \pm t_{1-\alpha/2,N-1} \cdot \sqrt{\left[\frac{1}{N}+\frac{(z_{1-\alpha/2})^{2}}{2 \cdot (N-1)} \right] \cdot S^2}
$$
wherein, $t$ is the critical t-value at the given sample size and confidence level, $z$ is the value of the normal distribution at the given alpha level, and $S^2$ is the variance of the difference scores.

### Concordance Correlation Coefficient

The CCC was calculated as outlined by @lin1989 (with later corrections).

$$
\hat\rho_c = \frac{2 \cdot s_{xy}} {s_x^2 + s_y^2+(\bar x-\bar y)^2}
$$
where $s_{xy}$ is the covariance, $s_x^2$ and $s_y^2$ are the variances of x and y respectively, and $(\bar x-\bar y)$ is the difference in the means of x & y.

# Repeated Measures Agreement

In many cases there are multiple measurements taken within subjects when comparing two measurements tools. In some cases the true underlying value will not be expected to vary (i.e., replicates; `agree_reps`), or multiple measurements may be taken within an individual *and* these values are expected to vary (i.e., nested design; `agree_nest`).

The confidence limits on the limits of agreement are based on the "MOVER" method described in detail by @zou2011. However, both functions operate similarly to `agree_test`; the only difference being that the data has to be provided as a `data.frame` in R.

## `agree_reps`

This function is for cases where the underlying values do *not* vary within subjects. This can be considered cases where replicate measure may be taken. For example, a researcher may want to compare the performance of two ELISA assays where measurements are taken in duplicate/triplicate.

So, for this function you will have to provide the data frame object with the `data` argument and the names of the columns containing the first (`x` argument) and second (`y` argument) must then be provided. An additional column indicating the subject identifier (`id`) must also be provided. Again, if there is a hypothesized agreement limit then this could be provided with the `delta` argument.

```{r}
a2 = agree_reps(x = "x",
                y = "y",
                id = "id",
                data = reps,
                agree.level = .8)
```

The results can then be printed. The printing format is very similar to `agree_test`, but notice that 1) the hypothesis test is based on the limits of agreement (MOVER method), 2) the Concordance Correlation Coefficient is calculated via the U-statistics method, 3) the Shieh TOST results are missing because they cannot be estimated for this type of design.

```{r}
print(a2)
```
```{r, fig.width=6, fig.height=6}
plot(a2, type = 1)

plot(a2, type = 2)
```

## `agree_nest`

This function is for cases where the underlying values may vary within subjects. This can be considered cases where there are distinct pairs of data wherein data is collected in different times/conditions within each subject. An example would be measuring blood pressure on two different devices on many people at different time points/days.

The function works almost identically to `agree_reps` but the underlying calculations are different


```{r}
a3 = agree_nest(x = "x",
                y = "y",
                id = "id",
                data = reps,
                agree.level = .8)
```

The printed results (and plots) are very similar to `agree_reps`. However, the CCC result now has a warning because the calculation in this scenario may not be entirely appropriate given the nature of the data.

```{r}
print(a3)
```
```{r, fig.width=6, fig.height=6}
plot(a3, type = 1)

plot(a3, type = 2)
```
### Calculations for `agree_reps` & `agree_nest`

All the calculations for the limits of agreement in these two functions can be found in the article by @zou2011.

In addition, the CCC calculations are derived from the `cccUst` function of the `cccrm` R package. The mathematics for this CCC calculation can be found in the work of @king2007 and @carrasco2009.

# Advanced Agreement

In some cases, the agreement calculations involve comparing two methods within individuals within varying conditions. For example, the "recpre_long" data set within this package contains two measurements of rectal temperature in 3 different conditions (where there is a fixed effect of condition). For this particular case we can use bootstrapping to estimate the limits of agreement.

The `loa_mixed` function can then calculate the limits of agreement. Like the previous functions, the data set must be set with the `data` argument. The `diff` is the column which contains the difference between the two measurements. The `condition` is the column that indicates the different conditions that the measurements were taken within. The `id` is the column containing the subject/participant identifier. The `plot.xaxis`, if utilized, sets the column from which to plot the data on the x-axis. The final two arguments `replicates` and `type` set the requirements for the bootstrapping procedure. **Warning**: This is a computationally heavy procedure and may take a few minutes to complete. An example code can be seen below.

```{r,warning=FALSE,eval=FALSE}
a4 = loa_mixed(data = recpre_long,
               diff = "diff",
               condition = "trial_condition",
               id = "id",
               plot.xaxis = "AM",
               replicates = 199,
               type = "perc")
```


# Test-Retest Reliability

Another feature of this R package is the ability to estimate the reliability of a measurement. This R package allows for the calculation of Intraclass Correlation Coefficients (ICC), various standard errors (SEM, SEE, and SEP), and coefficient of variation. All of the underlying calculations (sans the coefficient of variation) is based on the paper by @weir2005. This is a fairly popular paper within my own field (kinesiology), and hence was the inspiration for creating this function that provides all the calculative approaches included within that manuscript.

For this package, the test-retest reliability statistics can be calculated with the `reli_stats` function. This function allow for data to be input in a long (multiple rows of data for each subject) or in wide (one row for each subject but a column for each item/measure).

For the long data form, the column containing the subject identifier (`id`), item number (`item`), and measurements (`measure`) are provided. In this function I refer to items similar to if we were measuring internal consistency for a questionnaire (which is just a special case of test-retest reliability). So, `item` could also be refer to time points, which is what is typically seen in human performance settings where test-retest reliability may be evaluated over the course of repeated visits to the same laboratory. If `wide` is set to `TRUE` then the columns containing the measurements are provided (e.g., `c("value1","value2","value3")`).

To demonstrate the function, I will create a data set in the wide format.

```{r}
  sf <- matrix(
    c(9,    2,   5,    8,
      6,    1,   3,    2,
      8,    4,   6,    8,
      7,    1,   2,    6,
      10,   5,   6,    9,
      6,   2,   4,    7),
    ncol = 4,
    byrow = TRUE
  )

  colnames(sf) <- paste("J", 1:4, sep = "")
  rownames(sf) <- paste("S", 1:6, sep = "")
  #sf  #example from Shrout and Fleiss (1979)
  dat = as.data.frame(sf)
```


Now, that we have a data set (`dat`), I can use it in the `reli_stats` function.

```{r}
test1 = reli_stats(
  data = dat,
  wide = TRUE,
  col.names = c("J1", "J2", "J3", "J4")
)
```

This function also has generic print and plot functions. The output from print provides the coefficient of variation, standard errors, and a table of various intraclass correlation coefficients. Notice the conclusions about the reliability of the measurement here would vary greatly based on the statistic being reported. What statistic you should report is beyond the current vignette, but is heavily detailed in @weir2005. However, within the table there are columns for `model` and `measures` which describe the model that is being used and the what these different ICCs are intended to measure, respectively.

```{r}
print(test1)
```

Also included in the results is a plot of the measurements across the items (e.g., time points).

```{r, fig.width=6, fig.height=6}
plot(test1)
```

# Power Analysis for Agreement

There are surprisingly few resources for planning a study that attempts to quantify agreement between two methods. Therefore, we have added one function, with hopefully more in the future, to aid in the power analysis for simple agreement studies. The current function is `blandPowerCurve` which constructs a "curve" of power across sample sizes, agreement levels, and confidence levels. This is based on the work of @lu2016.

For this function the user must define the hypothesized limits of agreement (`delta`), mean difference between methods (`mu`), and the standard deviation of the difference scores (`SD`). There is also the option of adjusting the range of sample size (default: `seq(10,100,1)` which is 10 to 100 by 1), the agreement level (default is 95%), and confidence level (default is 95%). The function then produces a data frame of the results. A quick look at the head and we can see that we have low statistical power when the sample size is at the lower end of the range.

```{r}
power_res <- blandPowerCurve(
  samplesizes = seq(10, 100, 1),
  mu = 0.5,
  SD = 2.5,
  delta = c(6,7),
  conf.level = c(.90,.95),
  agree.level = c(.8,.9)
)

head(power_res)
```

We can then find the sample size at which (or closest to which) a desired power level with the `find_n` method for `powerCurve` objects created with the function above.

```{r}
find_n(power_res, power = .8)
```

Additionally we can plot the power curve to see how power changes over different levels of `delta`, `agree.level`, and `conf.level`

```{r,fig.width=6, fig.height=6 }
plot(power_res)
```


# References
\name{NEWS}
\title{NEWS file for the selectr package}
\encoding{UTF-8}

\section{Changes in version 0.4-2}{
  \subsection{MINOR CHANGES}{
    \itemize{

      \item Improve handling of vectors of length > 1 in logical comparison.
      Contributed by Garrick Aden-Buie.

      \item Minor improvements to error message construction.
      Contributed by Michael Chirico.

    }
  }
}

\section{Changes in version 0.4-1}{
  \subsection{BUG FIXES}{
    \itemize{

      \item When the \pkg{R.oo} package is attached, the use of class
      selectors no longer worked. This is due to the use of the \code{Class}
      name for \pkg{R.oo}'s base class object, where \pkg{selectr} was also
      using it (but not exporting) the same name of \code{Class} for
      representing a class selector. Consequently, \pkg{selectr}'s code was
      changed to rename the class to avoid any clashing. Because it was not
      exported, this is purely an internal change. Thanks to
      Francois Lemaire-Sicre for reporting the issue.

    }
  }
}

\section{Changes in version 0.4-0}{
  \subsection{MINOR CHANGES}{
     \itemize{

      \item Large rewrite of internals to use the R6 OO system instead of
      Reference Classes. This does not affect any external facing code as the
      results should be identical to the previous implementation, which is why
      this change is marked as minor. Initial and crude performance testing
      (by running the test suite) indicates that the R6 implementation is
      approximately twice as fast at generating XPath as the Reference Classes
      implementation.

      \item The minimum required version of R for \pkg{selectr} has been
      increased from \code{2.15.2} to \code{3.0} as that is the minimum
      required version of \pkg{R6}.

      \item Minor performance enhancements have been made. Not only is
      \pkg{R6} faster than Reference Classes, the use of string formatting
      has been replaced with string concatenation. Additionally dynamic
      calling of methods via \code{do.call()} has been replaced with direct
      method calls.

     }
  }

  \subsection{BUG FIXES}{
    \itemize{

      \item The issues in previous releases where methods can sometimes be
      missing should now be resolved. The bug appeared to lie in core
      Reference Classes code. By switching to \pkg{R6}, this type of issue
      should no longer be possible.

    }
  }
}


\section{Changes in version 0.3-2}{
  \subsection{MINOR CHANGES}{
     \itemize{

      \item Improved method registration for \pkg{XML} and \pkg{xml2} 
      objects. Avoids checks on each use and is only performed once per
      dependent package load/unload.

     }
  }

  \subsection{BUG FIXES}{
    \itemize{

      \item In some environments, reference class methods were missing at
      runtime. This appears to be due to some internal behaviour in them
      \pkg{methods} package where methods are registered on an objects when
      the \code{$} operator is used for a field or method. Instead, when
      a method is missing, they are manually bound to the object.

    }
  }
}

\section{Changes in version 0.3-1}{
  \subsection{MINOR CHANGES}{
     \itemize{

      \item Enabled partial matching on the translator argument to
      \code{css_to_xpath()}. Instead of defaulting to a generic translator,
      a non-matching argument will be returned with an error.

      \item Introduced many more unit tests via the \pkg{covr} package.
      This enabled dead code to be trimmed and also identified areas of code
      which needed improvement. Minor enhancements include: tolerate
      whitespace within a \code{:not()}, more consistent results returned
      from parser methods, improvements to argument parsing.

     }
  }

  \subsection{BUG FIXES}{
    \itemize{

      \item The \code{|=} attribute matching operator was not being parsed
      correctly for the generic translator.

      \item Handle scenario where a CSS comment is unclosed. Results in
      everything after the comment start to be removed (which may or may
      not result in a valid selector).

    }
  }
}

\section{Changes in version 0.3-0}{
  \subsection{MAJOR CHANGES}{
    \itemize{

      \item Added support for documents from the \pkg{xml2} package.

      \item selectr now also does not strictly depend on the XML
      package. If either the \pkg{XML} or \pkg{xml2} packages are
      present (which are required for the \code{querySelector} methods
      to work) then \code{querySelector} will begin to work for them.
      This also enables selectr to be used for translation-only.

    }
  }

  \subsection{BUG FIXES}{
    \itemize{

      \item Improve support for nth-*(an+b) selectors. Ported from
      cssselect.

    }
  }
}

\section{Changes in version 0.2-3}{
  \subsection{MINOR CHANGES}{
    \itemize{

      \item Code cleanup contributed by Kun Ren (#1).

      \item Updated DESCRIPTION to include URL and BugReports fields.
      Also update email address.

    }
  }
  \subsection{BUG FIXES}{
    \itemize{

      \item Fix behaviour for nth-*(an+b) pseudo-class selectors for
      negative a's. Contributed to cssselect by Paul Tremberth,
      ported to R.

      \item Escape delimiting characters to support new version of the
      stringr package. Probably should have been done in the first place.
      Reported by Hadley Wickham (#5).

    }
  }
}

\section{Changes in version 0.2-2}{
  \subsection{MINOR CHANGES}{
    \itemize{

      \item Corrected licence to BSD 3 clause. This was the licence in
      use previously, but has now been made more explicit.

      \item Removed 'Enhances' field because we import functions from
      \pkg{XML}. This choice is made because \pkg{XML} is a required
      package, rather than an optional package that can be worked with.
      This and the previous change have been made to keep up with recent
      changes in R-devel.

    }
  }
}

\section{Changes in version 0.2-1}{
  \subsection{MINOR FEATURES}{
    \itemize{

      \item Added a 'CITATION' file which cites a technical report on
      the package.

      \item \code{show()} methods are now available on internal objects,
      making interactive extensibility and bug-fixing easier. This is
      simply wrapping the \code{repr()} methods (mirroring the Python
      source) that the same objects have.

    }
  }
  \subsection{BUG FIXES}{
    \itemize{

      \item Use the session character encoding to determine whether to
      run unicode tests. Tests break in non-unicode sessions otherwise.

    }
  }
}

\section{Changes in version 0.2-0}{
  \subsection{NEW FEATURES}{
    \itemize{

      \item Introduced new functions \code{querySelectorNS()} and
      \code{querySelectorAllNS()} to ease the use of namespaces within a
      document. Previously this would have required knowledge of XPath.

    }
  }
  \subsection{BUG FIXES}{
    \itemize{

      \item Fix meaning of \code{:empty}, whitespace is not empty.

      \item Use \code{lang()} for XML documents with the \code{:lang()}
      CSS selector.

      \item \code{|ident} no longer produces a parsing error, but is now
      equivalent to just 'ident'.

    }
  }
}

\section{Changes in version 0.1-1}{
  \subsection{BUG FIXES}{
    \itemize{

      \item Now testing unicode only in non-Windows platforms on package
      check. Output should still be consistent, just depends on the
      current charset being unicode.

    }
  }
}

\section{Changes in version 0.1-0}{
  \subsection{NEW FEATURES}{
    \itemize{

      \item Initial port of the Python 'cssselect' package. Code is very
      literally ported, including the test suite.

      \item Wrapped translation functionality into a single function,
      \code{css_to_xpath()}.

      \item Created two convenience functions, \code{querySelector()} and
      \code{querySelectorAll()}. These mirror the behaviour of the same
      functions present in a web browser. \code{querySelector()} returns a
      node, while \code{querySelectorAll()} returns a list of nodes.

    }
  }
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/blandPowerCurve.R
\name{blandPowerCurve}
\alias{blandPowerCurve}
\title{Estimate power curve for Bland-Altman limits of agreement}
\usage{
blandPowerCurve(
  samplesizes = seq(10, 100, 1),
  mu = 0,
  SD,
  delta,
  conf.level = 0.95,
  agree.level = 0.95
)
}
\arguments{
\item{samplesizes}{vector of samples sizes at which to estimate power.}

\item{mu}{mean of differences}

\item{SD}{standard deviation of differences}

\item{delta}{The threshold below which methods agree/can be considered equivalent, can be in any units. Equivalence Bound for Agreement. More than one delta can be provided.}

\item{conf.level}{the confidence level(s) required. Default is 95\%. More than one confidence level can be provided.}

\item{agree.level}{the agreement level(s) required. Default is 95\%. The proportion of data that should lie between the thresholds, for 95\% limits of agreement this should be 0.95. More than one confidence level can be provided.}
}
\value{
A dataframe is returned containing the power analysis results. The results can then be plotted with the plot.powerCurve function.
}
\description{
This function calculates the power for the Bland-Altman method under varying parameter settings and for a range of sample sizes.
}
\section{references}{

Lu, M. J., et al. (2016). Sample Size for Assessing Agreement between Two Methods of Measurement by Bland-Altman Method. The international journal of biostatistics, 12(2), <https://doi.org/10.1515/ijb-2015-0039>
}

\examples{
\donttest{
powerCurve <- blandPowerCurve(samplesizes = seq(10, 200, 1),
mu = 0,
SD = 3.3,
delta = 8,
conf.level = .95,
agree.level = .95)
# Plot the power curve
plot(powerCurve, type = 1)
# Find at what N power of .8 is achieved
find_n(powerCurve, power = .8)
}
# If the desired power is not found then
## Sample size range must be expanded
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/methods.simple_reli.R
\name{simple_reli-methods}
\alias{simple_reli-methods}
\alias{print.simple_reli}
\alias{plot.simple_reli}
\title{Methods for simple_reli objects}
\usage{
\method{print}{simple_reli}(x, ...)

\method{plot}{simple_reli}(x, ...)
}
\arguments{
\item{x}{object of class \code{simple_reli} as returned from the reli_stats function}

\item{...}{further arguments passed through, see description of return value
for details.
\code{\link{reli_stats}}.}
}
\value{
\describe{
  \item{\code{print}}{Prints short summary of the Limits of Agreement}
  \item{\code{plot}}{Returns a plot of the data points used in the reliability analysis}
}
}
\description{
Methods defined for objects returned from the agree functions.
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jmvagreemulti.h.R
\name{jmvagreemulti}
\alias{jmvagreemulti}
\title{Nested/Replicate Data Agreement Analysis}
\usage{
jmvagreemulti(
  data,
  method1,
  method2,
  id,
  ciWidth = 95,
  agreeWidth = 95,
  testValue = 2,
  CCC = TRUE,
  valEq = FALSE,
  plotbland = FALSE,
  plotcon = FALSE
)
}
\arguments{
\item{data}{Data}

\item{method1}{Name of column containing 1st Vector of data}

\item{method2}{Name of column containing Vector of data}

\item{id}{Name of column containing subject identifier}

\item{ciWidth}{a number between 50 and 99.9 (default: 95), the width of
confidence intervals}

\item{agreeWidth}{a number between 50 and 99.9 (default: 95), the width of
agreement limits}

\item{testValue}{a number specifying the limit of agreement}

\item{CCC}{\code{TRUE} or \code{FALSE} (default), produce CCC table}

\item{valEq}{.}

\item{plotbland}{\code{TRUE} or \code{FALSE} (default), for Bland-Altman
plot}

\item{plotcon}{\code{TRUE} or \code{FALSE} (default), for Line of identity
plot}
}
\value{
A results object containing:
\tabular{llllll}{
  \code{results$text} \tab \tab \tab \tab \tab a preformatted \cr
  \code{results$blandtab} \tab \tab \tab \tab \tab a table \cr
  \code{results$ccctab} \tab \tab \tab \tab \tab a table \cr
  \code{results$plotba} \tab \tab \tab \tab \tab an image \cr
  \code{results$plotcon} \tab \tab \tab \tab \tab an image \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$blandtab$asDF}

\code{as.data.frame(results$blandtab)}
}
\description{
Nested/Replicate Data Agreement Analysis
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jmvreli.h.R
\name{jmvreli}
\alias{jmvreli}
\title{Reliability Analysis}
\usage{
jmvreli(data, vars, ciWidth = 95, desc = FALSE, plots = FALSE)
}
\arguments{
\item{data}{the data as a data frame}

\item{vars}{a list of the column names containing the measurements for
reliability analysis.}

\item{ciWidth}{a number between 50 and 99.9 (default: 95), the width of
confidence intervals}

\item{desc}{\code{TRUE} or \code{FALSE} (default), provide table of
variance components}

\item{plots}{\code{TRUE} or \code{FALSE} (default), plot data}
}
\value{
A results object containing:
\tabular{llllll}{
  \code{results$text} \tab \tab \tab \tab \tab a preformatted \cr
  \code{results$icctab} \tab \tab \tab \tab \tab a table \cr
  \code{results$vartab} \tab \tab \tab \tab \tab a table \cr
  \code{results$plots} \tab \tab \tab \tab \tab an image \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$icctab$asDF}

\code{as.data.frame(results$icctab)}
}
\description{
Reliability Analysis
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agree_reps.R
\name{agree_reps}
\alias{agree_reps}
\title{Tests for Absolute Agreement with Replicates}
\usage{
agree_reps(x, y, id, data, delta, agree.level = 0.95, conf.level = 0.95)
}
\arguments{
\item{x}{Name of column with first measurement}

\item{y}{Name of other column with first measurement}

\item{id}{Column with subject identifier}

\item{data}{Data frame with all data}

\item{delta}{The threshold below which methods agree/can be considered equivalent, can be in any units. Equivalence Bound for Agreement.}

\item{agree.level}{the agreement level required. Default is 95\%. The proportion of data that should lie between the thresholds, for 95\% limits of agreement this should be 0.95.}

\item{conf.level}{the confidence level required. Default is 95\%.}
}
\value{
Returns single list with the results of the agreement analysis.

\describe{
  \item{\code{"loa"}}{a data frame of the limits of agreement including the average difference between the two sets of measurements, the standard deviation of the difference between the two sets of measurements and the lower and upper confidence limits of the difference between the two sets of measurements.}
  \item{\code{"h0_test"}}{Decision from hypothesis test.}
  \item{\code{"identity.plot"}}{Plot of x and y with a line of identity with a linear regression line}
  \item{\code{"bland_alt.plot"}}{Simple Bland-Altman plot. Red line are the upper and lower bounds for shieh test; grey box is the acceptable limits (delta). If the red lines are within the grey box then the shieh test should indicate 'reject h0', or to reject the null hypothesis that this not acceptable agreement between x & y.}
  \item{\code{"ccc.xy"}}{Lin's concordance correlation coefficient and confidence intervals using U-statistics.}
  \item{\code{"conf.level"}}{Returned as input.}
  \item{\code{"agree.level"}}{Returned as input.}

}
}
\description{
agree_nest produces an absolute agreement analysis for data where there is multiple observations per subject but the mean does not vary within subjects as described by Zou (2013). Output mirrors that of agree_test but CCC is calculated via U-statistics.
}
\section{References}{

Zou, G. Y. (2013). Confidence interval estimation for the Bland–Altman limits of agreement with multiple observations per individual. Statistical methods in medical research, 22(6), 630-642.

King, TS and Chinchilli, VM. (2001). A generalized concordance correlation coefficient for continuous and categorical data. Statistics in Medicine, 20, 2131:2147.

King, TS; Chinchilli, VM; Carrasco, JL. (2007). A repeated measures concordance correlation coefficient. Statistics in Medicine, 26, 3095:3113.

Carrasco, JL; Phillips, BR; Puig-Martinez, J; King, TS; Chinchilli, VM. (2013). Estimation of the concordance correlation coefficient for repeated measures using SAS and R. Computer Methods and Programs in Biomedicine, 109, 293-304.
}

\examples{
data('reps')
agree_reps(x = "x", y = "y", id = "id", data = reps, delta = 2)
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/methods.powerCurve.R
\name{powerCurve-methods}
\alias{powerCurve-methods}
\alias{find_n}
\alias{plot.powerCurve}
\title{Methods for powerCurve objects}
\usage{
find_n(x, power = 0.8)

\method{plot}{powerCurve}(x, ...)
}
\arguments{
\item{x}{object of class \code{powerCurve}}

\item{power}{Level of power (value between 0 and 1) for find_n to find the sample size.}

\item{...}{further arguments passed through, see description of return value
for details.
\code{\link{blandPowerCurve}}.}
}
\value{
\describe{
  \item{\code{plot}}{Returns a plot of the limits of agreement (type = 1) or concordance plot (type = 2)}
  \item{\code{find_n}}{Find sample size at which desired power is achieved}
}
}
\description{
Methods defined for objects returned from the powerCurve function.
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/loa_mixed.R
\name{loa_mixed}
\alias{loa_mixed}
\title{Mixed Effects Limits of Agreement}
\usage{
loa_mixed(
  diff,
  condition,
  id,
  data,
  plot.xaxis = NULL,
  delta,
  conf.level = 0.95,
  agree.level = 0.95,
  replicates = 1999,
  type = "bca"
)
}
\arguments{
\item{diff}{column name of the data frame that includes the continuous measurement of interest.}

\item{condition}{column name indicating different conditions subjects were tested under.}

\item{id}{column name indicating the subject/participant identifier}

\item{data}{A data frame containing the variables within the model.}

\item{plot.xaxis}{column name indicating what to plot on the x.axis for the Bland-Altman plots. If this argument is missing or set to NULL then no plot will be produced.}

\item{delta}{The threshold below which methods agree/can be considered equivalent, can be in any units. Equivalence Bound for Agreement.}

\item{conf.level}{the confidence level required. Default is 95\%.}

\item{agree.level}{the agreement level required. Default is 95\%.}

\item{replicates}{the number of bootstrap replicates. Passed on to the boot function. Default is 500.}

\item{type}{A character string representing the type of bootstrap confidence intervals. Only "norm", "basic", "bca", and "perc" currently supported. Bias-corrected and accelerated, bca, is the default. See ?boot::boot.ci for more details.}
}
\value{
Returns single list with the results of the agreement analysis.

\describe{
  \item{\code{"var_comp"}}{Table of variance components}
  \item{\code{"loa"}}{a data frame of the limits of agreement including the average difference between the two sets of measurements, the standard deviation of the difference between the two sets of measurements and the lower and upper confidence limits of the difference between the two sets of measurements.}
  \item{\code{"h0_test"}}{Decision from hypothesis test.}
  \item{\code{"bland_alt.plot"}}{Simple Bland-Altman plot. Red line are the upper and lower bounds for shieh test; grey box is the acceptable limits (delta). If the red lines are within the grey box then the shieh test should indicate 'reject h0', or to reject the null hypothesis that this not acceptable agreement between x & y.}
  \item{\code{"conf.level"}}{Returned as input.}
  \item{\code{"agree.level"}}{Returned as input.}
}
}
\description{
This function allows for the calculation of bootstrapped limits of agreement when there are multiple observations per subject.
}
\section{References}{

Parker, R. A., Weir, C. J., Rubio, N., Rabinovich, R., Pinnock, H., Hanley, J., McLoughan, L., Drost, E.M., Mantoani, L.C., MacNee, W., & McKinstry, B. (2016). "Application of mixed effects limits of agreement in the presence of multiple sources of variability: exemplar from the comparison of several devices to measure respiratory rate in COPD patients". Plos One, 11(12), e0168321. <https://doi.org/10.1371/journal.pone.0168321>
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jmvagree.h.R
\name{jmvagree}
\alias{jmvagree}
\title{Simple Agreement Analysis}
\usage{
jmvagree(
  data,
  method1,
  method2,
  ciWidth = 95,
  agreeWidth = 95,
  testValue = 2,
  CCC = TRUE,
  plotbland = FALSE,
  plotcon = FALSE
)
}
\arguments{
\item{data}{Data}

\item{method1}{Name of column containing 1st Vector of data}

\item{method2}{Name of column containing Vector of data}

\item{ciWidth}{a number between 50 and 99.9 (default: 95), the width of
confidence intervals}

\item{agreeWidth}{a number between 50 and 99.9 (default: 95), the width of
agreement limits}

\item{testValue}{a number specifying the limit of agreement}

\item{CCC}{\code{TRUE} or \code{FALSE} (default), produce CCC table}

\item{plotbland}{\code{TRUE} or \code{FALSE} (default), for Bland-Altman
plot}

\item{plotcon}{\code{TRUE} or \code{FALSE} (default), for Bland-Altman plot}
}
\value{
A results object containing:
\tabular{llllll}{
  \code{results$text} \tab \tab \tab \tab \tab a preformatted \cr
  \code{results$blandtab} \tab \tab \tab \tab \tab a table \cr
  \code{results$ccctab} \tab \tab \tab \tab \tab a table \cr
  \code{results$plotba} \tab \tab \tab \tab \tab an image \cr
  \code{results$plotcon} \tab \tab \tab \tab \tab an image \cr
}

Tables can be converted to data frames with \code{asDF} or \code{\link{as.data.frame}}. For example:

\code{results$blandtab$asDF}

\code{as.data.frame(results$blandtab)}
}
\description{
Simple Agreement Analysis
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reli_stats.R
\name{reli_stats}
\alias{reli_stats}
\title{Reliability Statistics}
\usage{
reli_stats(
  measure,
  item,
  id,
  data,
  wide = FALSE,
  col.names = NULL,
  conf.level = 0.95
)
}
\arguments{
\item{measure}{Name of column containing the measurement of interest}

\item{item}{Name of column containing the items. If this is a test-retest reliability study then this would indicate the time point (e.g., time1,time2, time3, etc.)}

\item{id}{Column with subject identifier}

\item{data}{Data frame with all data}

\item{wide}{Logical value (TRUE or FALSE) indicating if data is in a "wide" format. Default is TRUE.}

\item{col.names}{If wide is equal to TRUE then col.names is a list of the column names containing the measurements for reliability analysis.}

\item{conf.level}{the confidence level required. Default is 95\%.}
}
\value{
Returns single list with the results of the agreement analysis.

\describe{
  \item{\code{"icc"}}{Table of ICC results}
  \item{\code{"lmer"}}{Linear mixed model from lme4}
  \item{\code{"anova"}}{Analysis of Variance table}
  \item{\code{"var_comp"}}{Table of Variance Components}
  \item{\code{"n.id"}}{Number of subjects/participants}
  \item{\code{"n.items"}}{Number of items/time points}
  \item{\code{"cv"}}{Coefficient of Variation}
  \item{\code{"SEM"}}{Standard Error of Measurement}
  \item{\code{"SEE"}}{Standard Error of the Estimate}
  \item{\code{"SEP"}}{Standard Error of Predicitions}
  \item{\code{"plot.reliability"}}{Plot of data points within subjects across items}


}
}
\description{
reli_stats produces reliability statistics desccribed by Weir (2005). This includes intraclass correlation coefficients, the coefficient of variation, and the standard error of meassurement.
}
\section{References}{

Weir, J. P. (2005). Quantifying test-retest reliability using the intraclass correlation coefficient and the SEM. The Journal of Strength & Conditioning Research, 19(1), 231-240.
}

\examples{
data('reps')
reli_stats(data = reps, wide = TRUE, col.names = c("x","y"))

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agree_test.R
\name{agree_test}
\alias{agree_test}
\title{Tests for Absolute Agreement}
\usage{
agree_test(x, y, delta, conf.level = 0.95, agree.level = 0.95)
}
\arguments{
\item{x}{Vector with first measurement}

\item{y}{Vector with second measurement}

\item{delta}{The threshold below which methods agree/can be considered equivalent, can be in any units. Equivalence Bound for Agreement.}

\item{conf.level}{the confidence level required. Default is 95\%.}

\item{agree.level}{the agreement level required. Default is 95\%. The proportion of data that should lie between the thresholds, for 95\% limits of agreement this should be 0.95.}
}
\value{
Returns single list with the results of the agreement analysis.

\describe{
  \item{\code{"shieh_test"}}{The TOST hypothesis test as described by Shieh.}
  \item{\code{"ccc.xy"}}{Lin's concordance correlation coefficient and confidence intervals.}
  \item{\code{"s.shift"}}{Scale shift from x to y.}
  \item{\code{"l.shift"}}{Location shift from x to y.}
  \item{\code{"bias"}}{a bias correction factor that measures how far the best-fit line deviates from a line at 45 degrees. No deviation from the 45 degree line occurs when bias = 1. See Lin 1989, page 258.}
  \item{\code{"loa"}}{Data frame containing the limits of agreement calculations}
  \item{\code{"h0_test"}}{Decision from hypothesis test.}
  \item{\code{"identity.plot"}}{Plot of x and y with a line of identity with a linear regression line}
  \item{\code{"bland_alt.plot"}}{Simple Bland-Altman plot. Red line are the upper and lower bounds for shieh test; grey box is the acceptable limits (delta). If the red lines are within the grey box then the shieh test should indicate 'reject h0', or to reject the null hypothesis that this not acceptable agreement between x & y.}

}
}
\description{
The agree_test function calculates a variety of agreement statistics. The hypothesis test of agreement is calculated by the method described by Shieh (2019). Bland-Altman limits of agreement, and confidence intervals, are also provided (Bland & Altman 1999; Bland & Altman 1986). In addition, the concordance correlation coefficient (CCC; Lin 1989) is also provided.
}
\section{References}{

Shieh (2019). Assessing Agreement Between Two Methods of Quantitative Measurements: Exact Test Procedure and Sample Size Calculation, Statistics in Biopharmaceutical Research, <https://doi.org/10.1080/19466315.2019.1677495>

Bland, J. M., & Altman, D. G. (1999). Measuring agreement in method comparison studies. Statistical methods in medical research, 8(2), 135-160.

Bland, J. M., & Altman, D. (1986). Statistical methods for assessing agreement between two methods of clinical measurement. The lancet, 327(8476), 307-310.

Lawrence, I., & Lin, K. (1989). A concordance correlation coefficient to evaluate reproducibility. Biometrics, 255-268.
}

\examples{
data('reps')
agree_test(x=reps$x, y=reps$y, delta = 2)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/methods.loa_mixed_bs.R
\name{loa_mixed_bs-methods}
\alias{loa_mixed_bs-methods}
\alias{print.loa_mixed_bs}
\alias{plot.loa_mixed_bs}
\title{Methods for loa_mixed_bs objects}
\usage{
\method{print}{loa_mixed_bs}(x, ...)

\method{plot}{loa_mixed_bs}(x, ...)
}
\arguments{
\item{x}{object of class \code{loa_mixed_bs} as returned from \code{loa_mixed}}

\item{...}{further arguments passed through, see description of return value
for details.
\code{\link{loa_mixed}}.}
}
\value{
\describe{
  \item{\code{print}}{Prints short summary of the Limits of Agreement}
  \item{\code{plot}}{Returns a plot of the limits of agreement}
}
}
\description{
Methods defined for objects returned from the loa_mixed functions.
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/methods.simple_agree.R
\name{simple_agree-methods}
\alias{simple_agree-methods}
\alias{print.simple_agree}
\alias{plot.simple_agree}
\title{Methods for simple_agree objects}
\usage{
\method{print}{simple_agree}(x, ...)

\method{plot}{simple_agree}(x, type = 1, ...)
}
\arguments{
\item{x}{object of class \code{simple_agree} as returned from a function starting with 'agree'}

\item{...}{further arguments passed through, see description of return value
for details.
\code{\link{agree_test}}.}

\item{type}{Type of plot to output. Default (1) is Bland-Altman plot while type=2 will produce a line-of-identity plot.}
}
\value{
\describe{
  \item{\code{print}}{Prints short summary of the Limits of Agreement}
  \item{\code{plot}}{Returns a plot of the limits of agreement (type = 1) or concordance plot (type = 2)}
}
}
\description{
Methods defined for objects returned from the agree functions.
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{temps}
\alias{temps}
\alias{recpre_long}
\title{Data}
\format{
A data frame with 60 rows and 10 variables:
\describe{
  \item{id}{Subject identifier}
  \item{trial_num}{order in which the experimental trial was completed}
  \item{trial_condition}{Environmental condition and metabolic heat production}
  \item{tod}{Time of Day}
  \item{trec_pre}{Rectal temperature before the beginning of the trial}
  \item{trec_post}{Rectal temperature at the end of the trial}
  \item{trec_delta}{Change in rectal temperature}
  \item{teso_pre}{Esophageal temperature before the beginning of the trial}
  \item{teso_post}{Esophageal temperature at the end of the trial}
  \item{teso_delta}{Change in esophageal temperature}

}

An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 30 rows and 6 columns.
}
\source{
Ravanelli N, Jay O. The Change in Core Temperature and Sweating Response during Exercise Are Unaffected by Time of Day within the Wake Period. Med Sci Sports Exerc. 2020 Dec 1. doi: 10.1249/MSS.0000000000002575. Epub ahead of print. PMID: 33273272.
}
\usage{
temps

recpre_long
}
\description{
A dataset from a study on the reliability of human body temperature at different times of day before and after exercise.
}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agree_nest.R
\name{agree_nest}
\alias{agree_nest}
\title{Tests for Absolute Agreement with Nested Data}
\usage{
agree_nest(x, y, id, data, delta, agree.level = 0.95, conf.level = 0.95)
}
\arguments{
\item{x}{Name of column with first measurement}

\item{y}{Name of other column with first measurement}

\item{id}{Column with subject identifier}

\item{data}{Data frame with all data}

\item{delta}{The threshold below which methods agree/can be considered equivalent, can be in any units. Equivalence Bound for Agreement.}

\item{agree.level}{the agreement level required. Default is 95\%. The proportion of data that should lie between the thresholds, for 95\% limits of agreement this should be 0.95.}

\item{conf.level}{the confidence level required. Default is 95\%.}
}
\value{
Returns single list with the results of the agreement analysis.

\describe{
  \item{\code{"loa"}}{a data frame of the limits of agreement including the average difference between the two sets of measurements, the standard deviation of the difference between the two sets of measurements and the lower and upper confidence limits of the difference between the two sets of measurements.}
  \item{\code{"h0_test"}}{Decision from hypothesis test.}
  \item{\code{"identity.plot"}}{Plot of x and y with a line of identity with a linear regression line}
  \item{\code{"bland_alt.plot"}}{Simple Bland-Altman plot. Red line are the upper and lower bounds for shieh test; grey box is the acceptable limits (delta). If the red lines are within the grey box then the shieh test should indicate 'reject h0', or to reject the null hypothesis that this not acceptable agreement between x & y.}
  \item{\code{"ccc.xy"}}{Lin's concordance correlation coefficient and confidence intervals using U-statistics. Warning: if underlying value varies this estimate will be inaccurate.}
  \item{\code{"conf.level"}}{Returned as input.}
  \item{\code{"agree.level"}}{Returned as input.}

}
}
\description{
agree_nest produces an absolute agreement analysis for data where there is multiple observations per subject but the mean varies within subjects as described by Zou (2013). Output mirrors that of agree_test but CCC is calculated via U-statistics.
}
\section{References}{

Zou, G. Y. (2013). Confidence interval estimation for the Bland–Altman limits of agreement with multiple observations per individual. Statistical methods in medical research, 22(6), 630-642.

King, TS and Chinchilli, VM. (2001). A generalized concordance correlation coefficient for continuous and categorical data. Statistics in Medicine, 20, 2131:2147.

King, TS; Chinchilli, VM; Carrasco, JL. (2007). A repeated measures concordance correlation coefficient. Statistics in Medicine, 26, 3095:3113.

Carrasco, JL; Phillips, BR; Puig-Martinez, J; King, TS; Chinchilli, VM. (2013). Estimation of the concordance correlation coefficient for repeated measures using SAS and R. Computer Methods and Programs in Biomedicine, 109, 293-304.
}

\examples{
data('reps')
agree_nest(x = "x", y = "y", id = "id", data = reps, delta = 2)
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{reps}
\alias{reps}
\title{reps}
\format{
A data frame with 20 rows with 3 variables
\describe{
  \item{id}{Subject identifier}
  \item{x}{X measurement}
  \item{y}{Y measurement}


}
}
\usage{
reps
}
\description{
A fake data set of a agreement study where both measures have replicates.
}
\keyword{datasets}
