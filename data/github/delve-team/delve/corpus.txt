titl delv neural network featur varianc analysi tag python deep learn machin learn satur pytorch ai author name justin shenkcofirst author orcid affili name mat l richtercofirst author affili orcid name wolf byttner affili orcid affili name visiolab berlin germani index name institut cognit scienc univers osnabrueck osnabrueck germani index name rapid health london england unit kingdom index date august bibliographi paperbib summari design neural network complex task deep neural network often refer black box model littl insight function approxim gain look structur layer output delv tool look neural network repres data represent featur chang throughout train tool enabl deep learn research understand limit suggest improv design network remov ad layer sever tool exist allow analyz neural network train techniqu character focu either data model well level abstract exampl abstract modelori techniqu tool analyz sharp local optima keskarsensitivitygoogl indic gener capabl train model scenario complex dataset model reduc error surfac allow insight differ differ setup less abstract datacentr techniqu gradcam selvaraju et al gradcamgradcamplusplu reduc model set classactiv map overlay individu data point get intuit understand infer process svcca svccasvcca consid modelcentr middl ground term abstract sinc allow compar analysi featur extract specif layer svcca also relev function perspect work sinc use singular valu decomposit core techniqu obtain analysi result anoth modelcentr tool allow layerbylay analysi logist regress probe alain util logist regress train output hidden layer measur linear separ data thu qualiti intermedi solut qualiti classifi model latter great import work sinc logist regress probe often use compar model identifi contribut layer overal perform featurespacesizemattersgoingdeep demonstr satur metric capabl show parameterineffici neural network architectur howev aforement tool signific limit term use practic applic scenario tool use improv perform given model case datacentr tool like gradcam solut propag back data make hard deriv decis regard neural architectur howev biggest concern aforement tool cost comput resourc integr analysi workflow deep learn practition tool like svcca logist regress probe requir complex comput expens procedur need conduct train natur limit techniqu small benchmark primarili academ dataset like cifar featurespac analysi tool use develop deep learningbas model need abl use littl comput workflow overhead possibl ideal analysi done live train progress allow research interrupt potenti longrun train session improv model satur propos shenkthesi later refin featurespac analysi techniqu known author capabl allow identifi parameterineffici setup featurespacesizemattersgoingdeep make satur usabl applic scenario necessari provid easytous framework allow integr tool normal train infer code minim invas chang also necessari comput analysi done onlin part regular forward pass model make integr seamless possibl numer comparison variou method promis avenu futur research model introspect delv tool extract inform base covari matrix data like satur intrins dimension neural network layer emphas practic usabl special attent place low overhead minim invas integr delv exist train infer setup delv hook directli pytorch pytorch model extract necessari inform littl comput memori overhead thank effici covari approxim algorithm enabl user store analyz extract statist without chang current experi workflow make delv easi integr monitor system make interfac easi expand allow user util prefer way monitor experi simpl csvfile folder structur sophist solut like tensorboard tensorflowwhitepap comprehens sourc document provid homepag httpdelvedocsreadthedocsiodelvedocsreadthedocsio statement need research spectral properti neural network represent explod recent year svccasvccagradcamkernelpcaalainfeatureattribut public like svcca featurespac demonstr use interest inform extract spectral analysi latent represent also shown metric like satur shenkthesisspectralanalysi use optim neural network architectur identifi patholog pattern hint ineffici neural network structur main purpos delv provid easi flexibl access type layerbas statist combin eas usag extens delv enabl excit scientif explor machin learn research engin delv alreadi use number scientif public featurespacesizemattersgoingdeep sourc code delv archiv zenodo link doi zenodo overview librari softwar structur sever modul distribut task full detail avail httpsdelvedocsreadthedocsio tensorboardx summarywrit tensorflowwhitepap use effici save artifact like imag statist train minim interrupt varieti layer featur statist observ statist intrins dimension layer satur intrins dimension divid featur space dimension covariancematrix determin covari matrix also known gener varianc trace covari matrix measur varianc data trace diagon matrix anoth way measur dispers data layer satur intrins dimension divid featur space dimension sever layer current support convolut linear lstm addit layer pytorch convtranspos plan futur develop see issu httpsgithubcomdelveteamdelveissu eigendecomposit featur covari matrix comput satur relat metric like intrins dimension requir covari matrix layer output comput covari matrix layer output train evalu set impract naiv sinc would requir hold entir dataset memori would also contradict goal seamless integr exist train loop commonli oper minibatch therefor batchwis approxim algorithm use comput covari matrix onlin train comput covari two variabl use covari approxim algorithm two random variabl x n sampl qx fracsumn_i x_i y_in fracsumn_i x_i sumn_i y_in x_i y_i individu observ respect random variabl x n total number sampl advantag method number seen sampl sum squar sum variabl need store make memori consumpt per layer constant respect size dataset comput qx possibl combin featur obtain covari matrix layer output qz_l z_l z_l layer output entir dataset parallel comput featur combin exploit shape layer output matrix a_l layer l comput sumn_i x_i y_i featur combin layer l calcul run squar sumb_ba_lbt a_lb batch output matric a_lb b b b batch replac fracsumn_i x_i sumn_i y_in outer product bara_l bigotim bara_l sampl mean bara_l run sum output z_lk k n train time divid total number train sampl n formula batchwis approxim covari matrix written like qz_l z_l fracsumb_ba_lbt a_lbn bara_l bigotim bara_l batchwis updat algorithm allow us integr approxim covari matrix part regular forward pass train evalu algorithm use threadsaf common valu store singl comput devic node furthermor allow updat covari matrix asynchron network train distribut manner avoid problem caus round error numer instabl implement algorithm convert default data bit floatingpoint valu default anoth challeng dimension data convolut layer simpl flatten data vector would result high dimension vector comput expens singular valu decomposit direct consequ address issu treat everi kernel posit individu observ turn thdegre outputtensor shape sampl height width filter matrix shape sampl cdot height cdot width filter advantag strategi inform lost keep dimension q manag size option reduc comput requir featur map automat reduc size use linear interpol constant maximum height width sinc inform lost process disabl approxim method describ alongsid satur metric work shenkthesisspectralanalysi refin featurespac refer