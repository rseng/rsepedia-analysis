### Maximum search related terms
Maximum number of words produced for each search term. These words are then trimmed to produce the final vocabulary (the size of which is controlled by `Max Terms`)
### Weighing function
Weighing function of the aggregation step.
### Boost method
Method used to determine the weight given to words produced, before the aggregation step. Two methods are available:

 - Sum similarity -- this method uses the similarity (between the seed word and each word) to determine the weight. The similarities are added for each word, as each word can appear in the results of different seed terms.
 - Counts -- count the number of times a word appears as the result of a seed term.
### Years in interval
Number of years which a single time step will cover.
### Seed concept
This is the initial word to begin the search. Multiple words can be used (separated by commas).
### Words per year
Maximum number of words per year time period which are left after the aggregation step.
### Track direction
Direction in which the concept search is conducted:
 - Forward -- starting from the earliest year and moving forward through time.
 - Backward -- starting from the latest year and moving backward through time.
### Year period
Period of years in which the search will take place. This allows to begin searching at any point in the available time range.
### Word boost
Additional weight given to seed terms when producing the vocabulary for each time step. Higher weights will cause seed terms to stay in the final vocabulary.
### Do cleaning
Apply custom vocabulary cleaning function.
### Algorithm

The vocabulary monitor contains can use two different algorithms for generating vocabularies. These control which words are used as seed terms for each model:

 - The non-adaptive vocabulary generator uses the same seed terms each time to generate the related terms.
 - The adaptive vocabulary generator uses the related terms generated by one semantic model as seed terms for the next semantic model. This adds an additional possibility: it allows for the semantic models to be used in chronological order, or in reverse chronological order -- searching forwards or backwards in time.
### Max terms
The maximum number of words to be included in the vocabulary for each time period.
### Minimum concept similarity
Minimum similarity concepts must have on semantic space. Words whose similarity (with the seed concept) is smaller than this threshold will be ignored.

Also interpreted as Maximum concept distance.
### Weighting function parameter
Parameter controlling the shape weighting function of the aggregation step.
# How does ShiCo work?

Given a set of seed terms, ShiCo uses its semantic models to generate a vocabulary of related terms. This process is done for every one of the semantic models available. This is done using a *Vocabulary Monitor*. The vocabulary monitor generates a vocabulary of related terms and a list of vocabulary links, which explains which of the related terms was generated by which one of the seed terms.

![Overview of vocabulary monitor](./shicoOverviewA.png)

The produced vocabularies and links are then aggregated using a *Vocabulary Aggregator* -- the vocabulary aggregator groups together results from multiple models into a single final vocabulary. This has a smoothing effect on the produced vocabulary.

![Overview of vocabulary aggregator](./shicoOverviewB.png)

## Vocabulary Monitor

The vocabulary monitor uses the given terms to query the semantic models. The models provide a list of *related words* and some measure of how closely related they are to the seed term -- this is interpreted as a *distance* between the seed term and the related word:

![Overview of vocabulary aggregator](./seedWordRelationA.png)

As mentioned before, from each model the vocabulary monitor generates a vocabulary and list of vocabulary links. The vocabulary is a list of terms related to the seed terms -- each term is assigned a *weight*. This weight is calculated in one of two ways:
 - As a count of the number of times the term appears
 - As a sum of the distance between the seed terms and the related term

These weightings start from the assumption that a related term can appear as the result of one or more seed terms, in which case each time it increases the weight assigned to the related term.

![Overview of vocabulary aggregator](./seedWordRelationB.png)

In this way, the weights of each word are the sum of the weight contribution from each seed.

```
weights(word1) = weight(word1,seed1)
weights(word2) = weight(word2,seed1) + weight(word2,seed2)
...
```

The vocabulary monitor limits the number of words in the generated vocabulary, only the *N* words with the highest weights are included in the vocabulary.

```
word1: weights(word1)
word2: weights(word2)
...
wordN: weights(wordN)
-------- cut point
wordN+1: weights(wordN+1)  # dropped
wordN+2: weights(wordN+2)  # dropped
...                        # dropped
```

### Algorithms

The vocabulary monitor contains can use two different algorithms for generating vocabularies. These control which words are used as seed terms for each model.

#### Non-adaptive

The non-adaptive vocabulary generator uses the same seed terms each time to generate the related terms.

![Non Adaptive generator](./vmNonAdaptive.png)

#### Adaptive

The adaptive vocabulary generator uses the related terms generated by one semantic model as seed terms for the next semantic model.

![Adaptive generator](./vmAdaptive.png)

This adds an additional possibility: it allows for the semantic models to be used in chronological order, or in reverse chronological order -- searching forwards or backwards in time.

For more details, please refer to [this paper](#Wevers DH2015).


## Vocabulary Aggregator

A vocabulary aggregator takes a vocabulary produced by a vocabulary monitor and aggregates them over a set time window.

![Vocabulary aggregator](./vaYears.png)

Weighting functions are used to aggregate topics in a year:

Terms inside the 'window', are weighted by a weighting function.

### Gaussian
![Gaussian weighting function](./vaWeightGauss.png)


### JSD
![JSD weighting function](./vaWeightJSD.png)

### Linear
![Linear weighting function](./vaWeightLinear.png)

For more details, please refer to [this paper](#Wevers DH2015).

# References
##### Wevers DH2015
Wevers, M.; Kenter, T. and Huijnen, P. Concepts Through Time: Tracing Concepts in Dutch Newspaper Discourse (1890-1990) using Word Embeddings. In Digital Humanities 2015 (DH2015), 2015.
# What should you do if you want to modify ShiCo?

Be brave! And get in touch if you need help. Pull requests are very welcome.

## Backend

Written in Python.

### Unit testing
If you modify ShiCo back end, make sure to write your unit tests for your code.

To run Python unit tests, run:
```
$ nosetests
```

## Web app

Written in Javascript (Angular).

### Adding hooks

You can add your own custom behaviour to the force directed graphs like this:
```
(function() {
 'use strict';

 angular
   .module('shico')
   .run(runBlock);

 function runBlock(GraphConfigService) {
   GraphConfigService.addForceGraphHook(function(node) {
     node.select('circle').attr('r', function(d) {
       return d.name.length;
     });
   });
 }
})();

```

This snippet modifies the size of the force directed graph nodes, and makes them dependent on the length of the name in the node's data.

## Making a release on GitHub
 - Merge changes on branch `demo`
 - Run `gulp build`
 - Make github release

# How to build your ShiCo models?
This tutorial will show you how to build your own word2vec models for using ShiCo with your own data.

First, we will create some simulated data. This is only for illustration and you should replace this part of the tutorial for something which actually loads your own data.

ShiCo works with the assumption that we have a collection of documents for a number of year. Each document is labeled with one year. A document is represented as a list of sentences contained in the document. Each sentence is in turn a list of words. Any pre-processing required should be applied to the document at this stage -- for example making all words lowercase, removing stop-words, removing unusual symbols, etc. Note that the exact details of the pre-processing are for you to decide.

The following function creates a single random document:


```python
import numpy as np

def buildRandomDoc():
    '''Create a random document with two sentences of an arbitrary length.'''
    # This is my vocabulary of possible words
    vocab = [ 'hello', 'world', 'this', 'is', 'a', 'public', 'service', 'announcement',
             'this', 'is', 'only', 'a', 'test' ]

    doc = []
    for i in range(2):   # Two sentences per document
        # Each sentence in my document will be between 10 and 20 words
        wordsInSent = 10 + np.random.randint(10)

        # Pick words randomly from vocabulary
        sent = np.random.choice(vocab, wordsInSent).tolist()
        doc.append(sent)
    return doc

def printableDocument(doc):
    '''Get printable version of a my document'''
    text = '. '.join(' '.join(sent) for sent in doc)
    return (text[:57]+'...') if len(text)>60 else text

myDoc = buildRandomDoc()
print 'A single document looks like this:\n   ',printableDocument(myDoc)
```

    A single document looks like this:
        public world announcement only only is service is public ...


But of course, you don't want to analyze a single document but rather a collection of them. In practice, we will have a list of documents for each years.

The following snippet of code will create 3 documents for each year between 1950 and 1960. Notice that all years have the same number of documents, which will not always be the case.

Now, this is how our document collection looks like:


```python
for year in range(1950,1960):
    print '%d >'%year
    for n in range(3):
        doc = buildRandomDoc()
        print '  ',printableDocument(doc)
```

    1950 >
       this test announcement a hello public test is hello a tes...
       this service only only this hello hello only a this world...
       is world a a a announcement hello hello world a public is...
    1951 >
       this this world this service this announcement this servi...
       a public a a public only service announcement this announ...
       this public a announcement test a is hello only announcem...
    1952 >
       this a service a test only is is public announcement this...
       is is service this announcement this test a a hello is a ...
       a is this this a hello service a is this. this is this pu...
    1953 >
       world only a public public public this is hello a is serv...
       this service hello announcement is service announcement p...
       this world public a this this service announcement public...
    1954 >
       is only is public is a is a public a a test a test. a onl...
       announcement service public hello public is test a a is t...
       is only this this world hello hello test world announceme...
    1955 >
       only hello world test service service this this test this...
       a this hello test this world hello service a this this wo...
       this world a announcement test this hello a this a only a...
    1956 >
       is announcement public world a test is hello is hello a. ...
       is a hello a test this public service world public is. th...
       test public hello announcement service a a is world publi...
    1957 >
       this public this service hello is is only this this. hell...
       this is a hello a public announcement only this hello a i...
       is announcement world this this world public only announc...
    1958 >
       a only only only hello hello test is announcement test a ...
       test this is this only world this test a is this this a a...
       this service a is public service is a this test a public ...
    1959 >
       public is service this service this announcement a this t...
       service this a test announcement test only announcement s...
       is only only public this a hello world is world a hello p...


# Accessing your data
There are three things we need to be able to do with your data:
 - Know when is the starting year
 - Know when is the end year
 - Retrieve documents for particular years.

So you would need to implement 3 functions:

```python
def getMinYear():
    '''Retrieves the first year on the data set.'''
    pass
```

```python
def getMaxYear():
    '''Retrieves the last year on the data set.'''
    pass
```

```python
def getDocumentsForYear(year):
    '''Retrieves a list of documents for a year specified.'''
    pass
```

These could, for example, load the data from text files, a database, a webservice, etc.

This is our implementation of these functions, but this will be different for your own set of documents:


```python
def getMinYear():
    return 1950  # This is fixed, in our dummy case

def getMaxYear():
    return 1970  # This is fixed, in our dummy case

def getDocumentsForYear(year):
    docs = []
    for n in range(10):  # Generate 10 random documents for each year
        doc = buildRandomDoc()
        docs.append(doc)
    return docs
```

## Ready to go
Once you have implemented these three functions in whichever way is most convenient for you, we are ready to go.

We can do a few checks to make sure our data is fine (you can do these checks with your own data as well).
 - Make sure that for any given year, we get a list of documents
 - Make sure documents are lists of sentences
 - Make sure sentences are in turn lists of words


```python
assert getMinYear() < getMaxYear(), \
    'minYear should be before maxYear'

anyYear = getMinYear()

listOfDocs = getDocumentsForYear(anyYear)
assert isinstance(listOfDocs, list), \
    'We should get a list of documents of any given year'

listOfSentences = listOfDocs[0]
assert isinstance(listOfSentences, list), \
    'Each document should contain a list of sentences'

listOfWords = listOfSentences[0]
assert isinstance(listOfWords, list), \
    'Each sentence should contain a list of words'

word = listOfWords[0]
assert isinstance(word, str), \
    'Words should be strings'
```

Now we can glue together sentences for a given year, and join year ranges as required.

These two functions can remain the same for your own data.


```python
def getSentencesForYear(year):
    '''Return list of lists of strings.
    Return list of sentences in given year.
    Each sentence is a list of words.
    Each word is a string.'''
    docs = getDocumentsForYear(year)

    sentences = []
    for doc in docs:
        for sent in doc:
            sentences.append(sent)
    return sentences

def getSentencesInRange(startY, endY):
    '''Return list of lists of strings.
    Return list of sentences in given year.
    Each sentence is a list of words.
    Each word is a string.'''
    return [ s for year in range(startY, endY)
               for s in getSentencesForYear(year) ]
```

## Train models
Now we can train the models. There are a few settings you can play with:

 - yearsInModel -- each model will cover several years. This controls how many years will be included in each model
 - stepYears -- this controls the year gap between models.
 - modelFolder -- folder where the models will be stored.

**NOTE:** If your models have a lot of data, it could take a while to train. Grab a coffee (or tea!), and wait.


```python
import gensim

yearsInModel = 5
stepYears = 1
modelFolder = 'tempModels'

y0 = getMinYear()
yN = getMaxYear()

for year in range(y0,yN-yearsInModel+1, stepYears):
    startY = year
    endY   = year + yearsInModel
    modelName = modelFolder + '/%d_%d.w2v'%(year,year + yearsInModel)
    print 'Building model: ',modelName

    sentences = getSentencesInRange(startY, endY)
    model = gensim.models.Word2Vec(min_count=1)
    model.build_vocab(sentences)
    model.train(sentences)

    print '...saving'
    model.init_sims(replace=True)
    model.save_word2vec_format(modelName, binary=True)
```

    Building model:  tempModels/1950_1955.w2v
    ...saving
    Building model:  tempModels/1951_1956.w2v
    ...saving
    Building model:  tempModels/1952_1957.w2v
    ...saving
    Building model:  tempModels/1953_1958.w2v
    ...saving
    Building model:  tempModels/1954_1959.w2v
    ...saving
    Building model:  tempModels/1955_1960.w2v
    ...saving
    Building model:  tempModels/1956_1961.w2v
    ...saving
    Building model:  tempModels/1957_1962.w2v
    ...saving
    Building model:  tempModels/1958_1963.w2v
    ...saving
    Building model:  tempModels/1959_1964.w2v
    ...saving
    Building model:  tempModels/1960_1965.w2v
    ...saving
    Building model:  tempModels/1961_1966.w2v
    ...saving
    Building model:  tempModels/1962_1967.w2v
    ...saving
    Building model:  tempModels/1963_1968.w2v
    ...saving
    Building model:  tempModels/1964_1969.w2v
    ...saving
    Building model:  tempModels/1965_1970.w2v
    ...saving


Now that your models have been created, you should now be ready to run your own ShiCo server!
# How to use ShiCo?

This guide will instruct you in the elements for using ShiCo's user interface.

## User interface components

When you first open ShiCo on your browser, you will see a simple search bar:

![Search bar](./searchBar.png)

You can enter one or multiple (comma separated) *seed terms*. These seed terms are the entry point for your concept search. Click *Submit* to begin your search. The results from your search will be displayed in the results panel below the search bar.

The search bar has some additional features:
 - It allows you to modify the search parameters. Click the *+* button to display additional search parameters.
 - It allows you to save the parameters of your current search, or load the parameters of a previous search.

## Search parameters

The following is the list of parameters (with a link to a brief explanation) which can be used to control your concept search:

 - [Max Terms](/webapp/src/help/maxTerms.md)
 - [Max related terms](/webapp/src/help/maxRelatedTerms.md)
 - [Minimum concept similarity](/webapp/src/help/minSim.md)
 - [Word boost](/webapp/src/help/wordBoost.md)
 - [Boost method](/webapp/src/help/boostMethod.md)
 - [Algorithm](/webapp/src/help/algorithm.md)
 - [Track direction](/webapp/src/help/direction.md)
 - [Years in interval](/webapp/src/help/yearsInInterval.md)
 - [Words per year](/webapp/src/help/wordsPerYear.md)
 - [Weighing function](/webapp/src/help/weighFunc.md)
 - [Function shape](/webapp/src/help/wFParam.md)
 - [Do cleaning ?](/webapp/src/help/doCleaning.md) (only shown if your backend uses a cleaning function).
 - [Year period](/webapp/src/help/yearPeriod.md)

## Produced graphics

Once a search is complete, ShiCo displays results in the results panel. Results are displayed using various graphs:

 - Stream graph -- this shows each word of the resulting vocabulary as a stream over time. The stream gets wider or narrower  according to the weight the word is given in the vocabulary.

![Stream graph](./streamGraph.png)

 - Network graphs -- this shows a collection of graphs displaying the resulting vocabulary as a network graph. Words which are related to each other are connected with an arrow. The direction of the arrow indicates which word was the product of which seed word.

![Network graph](./networkGraph.png)

 - Space embedding -- this shows an estimate of the spatial relationship between words in the final vocabulary at every time step. Please keep in mind that these spatial relations are approximate and should be considered with care.

![Space embedding graph](./embeddingGraph.png)

 - Plain text vocabulary -- this shows a text representation of the concept search. This consists, for each time step, of the seed words used and the produced vocabulary.

## Saving and loading search parameters

When you click the *Save parameters* button, a text box with your search parameters will be displayed. Copy these parameters and save them somewhere. Click *Ok* to hide the text box.

When you click the *Load parameters* button, another text box will be displayed. Enter previously saved search parameters in this box and click *Ok* to load the parameters.
# Deploying ShiCo
If you want to run your own instance of ShiCo, there are a few things you will need:

 - A set of word2vec models which your ShiCo instance will use.
 - Run the python back end on your a server (you will need a server with enough memory to hold your word2vec models).
 - Run a web server to serve the front end to the browser.

## Word2vec models

You are welcome to use our [existing w2v models](http://doi.org/10.5281/zenodo.1189328). If you do, please contact us for more details on how the models were build and to know how to cite our work. You can also [create your own](./buildingModels.md) models, based on your own corpus.

## Launching the back end

Once you have downloaded the code (or clone this repo), and install all Python requirements (contained in *requirements.txt*), you can launch the flask server as follows:
```
$ python shico/server/app.py -f "word2vecModels/????_????.w2v"
```

*Note:* loading the word2vec models takes some time and may consume a large amount of memory.

You can check that the server is up and running by connecting to the server using curl (or your web browser):
```
http://localhost:5000/load-settings
```

Alternatively you use [Gunicorn](http://gunicorn.org/), by setting your configuration on *shico/server/config.py* and then running:

```
$ gunicorn --bind 0.0.0.0:8000 --timeout 1200 shico.server.wsgi:app
```

## Launching the front end

The necessary files for serving the front end are located in the *webapp* folder. You will need to edit your configuration file (*webapp/srs/config.json*) to tell the front end where your back end is running. For example, if your backend is running on *localhost* port 5000 as in the example above, you would set your configuration file as follows:

```
{
  "baseURL": "http://localhost:5000"
}
```

If you are familiar with the Javascript world, you can use the *gulp* tasks provided. You can serve your front end as follows (from the *webapp* folder):
```
$ gulp serve
```

You can build a deployable version (minified, uglified, etc) as follows:
```
$ gulp build
```
This will build a deployable version on the *webapp/dist* folder.

## Pre-build deployable version

If you are not familiar with the Javascript world (or just don't feel like building your own deployable version), the *demo* branch of this repository contains a pre-build version of the front end. You can checkout (or download) that branch, and then you are ready to go.

## Serve with your favorite web server

Once you have a *webapp/dist* folder (whether downloaded or self built) you can serve the content of it using your favorite web server. For example, you could use Python SimpleHTTPServer as follows (from the *webapp/dist* folder):
```
$ python -m SimpleHTTPServer
```

## Cleaning functions
In some cases, resulting vocabularies may contain words which we would like to filter. ShiCo offers the possibility of using a *cleaning* function, for filtering vocabularies after they have been generated. To use this option, it is necessary to indicate the name of the cleaning function when starting the ShiCo server. A sample cleaning function is provided (*shico.extras.cleanTermList*). You can use this function as follows:
```
$ python shico/server/app.py -c "shico.extras.cleanTermList"
```

If you are using gunicorn, in your *config.py*, you can set `cleaningFunctionStr` to the name of your cleaning function, for instance:

```
cleaningFunctionStr = "shico.extras.cleanTermList"
```

## Speeding up ShiCo

Current implementation of ShiCo relies on gensim word2vec model `most_similar` function, which in turn requires the calculation of the dot product between two large matrices, via `numpy.dot` function. For this reason, ShiCo greatly benefits from using libraries which accelerate matrix multiplications, such as OpenBLAS. ShiCo has been tested using [Numpy with OpenBLAS](https://hunseblog.wordpress.com/2014/09/15/installing-numpy-and-openblas/), producing a significant increase in speed.
