# <a href="https://numpy.org/"><img alt="NumPy" src="/branding/logo/primary/numpylogo.svg" height="60"></a>

<!--[![Azure Pipelines](https://dev.azure.com/numpy/numpy/_apis/build/status/numpy.numpy?branchName=main)](-->
<!--https://dev.azure.com/numpy/numpy/_build/latest?definitionId=1?branchName=main)-->
<!--[![Actions build_test](https://github.com/numpy/numpy/actions/workflows/build_test.yml/badge.svg)](-->
<!--https://github.com/numpy/numpy/actions/workflows/build_test.yml)-->
<!--[![TravisCI](https://app.travis-ci.com/numpy/numpy.svg?branch=main)](-->
<!--https://app.travis-ci.com/numpy/numpy)-->
<!--[![CircleCI](https://img.shields.io/circleci/project/github/numpy/numpy/main.svg?label=CircleCI)](-->
<!--https://circleci.com/gh/numpy/numpy)-->
<!--[![Codecov](https://codecov.io/gh/numpy/numpy/branch/main/graph/badge.svg)](-->
<!--https://codecov.io/gh/numpy/numpy)-->

[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)

NumPy is the fundamental package for scientific computing with Python.

- **Website:** https://www.numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://www.numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

NumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:

    python -c 'import numpy; numpy.test()'

Code of Conduct
----------------------

NumPy is a community-driven open source project developed by a diverse group of
[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong
commitment to creating an open, inclusive, and positive community. Please read the
[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact
with others in a way that makes our community thrive.

Call for Contributions
----------------------

The NumPy project welcomes your expertise and enthusiasm!

Small improvements or fixes are always appreciated; issues labeled as ["good
first issue"](https://github.com/numpy/numpy/labels/good%20first%20issue)
may be a good starting point. If you are considering larger contributions
to the source code, please contact us through the [mailing
list](https://mail.python.org/mailman/listinfo/numpy-discussion) first.

Writing code isn’t the only way to contribute to NumPy. You can also:
- review pull requests
- help us stay on top of new and old issues
- develop tutorials, presentations, and other educational materials
- maintain and improve [our website](https://github.com/numpy/numpy.org)
- develop graphic design for our brand assets and promotional materials
- translate website content
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

For more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). 
If you’re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by opening a new issue or leaving a
comment on a relevant issue that is already open.

Our preferred channels of communication are all public, but if you’d like to
speak to us in private first, contact our community coordinators at
numpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for
an invitation).

We also have a biweekly community call, details of which are announced on the
mailing list. You are very welcome to join.

If you are new to contributing to open source, [this
guide](https://opensource.guide/how-to-contribute/) helps explain why, what,
and how to successfully get involved.
PocketFFT
---------

This is a heavily modified implementation of FFTPack [1,2], with the following
advantages:

- strictly C99 compliant
- more accurate twiddle factor computation
- very fast plan generation
- worst case complexity for transform sizes with large prime factors is
  `N*log(N)`, because Bluestein's algorithm [3] is used for these cases.


Some code details
-----------------

Twiddle factor computation:

- making use of symmetries to reduce number of sin/cos evaluations
- all angles are reduced to the range `[0; pi/4]` for higher accuracy
- an adapted implementation of `sincospi()` is used, which actually computes
  `sin(x)` and `(cos(x)-1)`.
- if `n` sin/cos pairs are required, the adjusted `sincospi()` is only called
  `2*sqrt(n)` times; the remaining values are obtained by evaluating the
  angle addition theorems in a numerically accurate way.

Parallel invocation:

- Plans only contain read-only data; all temporary arrays are allocated and
  deallocated during an individual FFT execution. This means that a single plan
  can be used in several threads at the same time.

Efficient codelets are available for the factors:

- 2, 3, 4, 5, 7, 11 for complex-valued FFTs
- 2, 3, 4, 5 for real-valued FFTs

Larger prime factors are handled by somewhat less efficient, generic routines.

For lengths with very large prime factors, Bluestein's algorithm is used, and
instead of an FFT of length `n`, a convolution of length `n2 >= 2*n-1`
is performed, where `n2` is chosen to be highly composite.


[1] Swarztrauber, P. 1982, Vectorizing the Fast Fourier Transforms
    (New York: Academic Press), 51
[2] https://www.netlib.org/fftpack/
[3] https://en.wikipedia.org/wiki/Chirp_Z-transform
NumPy has a Code of Conduct, please see: https://numpy.org/code-of-conduct
# Contributing to numpy

## Reporting issues

When reporting issues please include as much detail as possible about your
operating system, numpy version and python version. Whenever possible, please
also include a brief, self-contained code example that demonstrates the problem.

If you are reporting a segfault please include a GDB traceback, which you can
generate by following
[these instructions.](https://github.com/numpy/numpy/blob/main/doc/source/dev/development_environment.rst#debugging)

## Contributing code

Thanks for your interest in contributing code to numpy!

+ If this is your first time contributing to a project on GitHub, please read
through our
[guide to contributing to numpy](https://numpy.org/devdocs/dev/index.html)
+ If you have contributed to other projects on GitHub you can go straight to our
[development workflow](https://numpy.org/devdocs/dev/development_workflow.html)

Either way, please be sure to follow our
[convention for commit messages](https://numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message).

If you are writing new C code, please follow the style described in
``doc/C_STYLE_GUIDE``.

Suggested ways to work on your development version (compile and run
the tests without interfering with system packages) are described in
``doc/source/dev/development_environment.rst``.

### A note on feature enhancements/API changes

If you are interested in adding a new feature to NumPy, consider
submitting your feature proposal to the [mailing list][mail], 
which is the preferred forum for discussing new features and
API changes.

[mail]: https://mail.python.org/mailman/listinfo/numpy-discussion
<!--         ----------------------------------------------------------------
                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!
                ----------------------------------------------------------------

*  FORMAT IT RIGHT:
      http://www.numpy.org/devdocs/dev/development_workflow.html#writing-the-commit-message

*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:
      http://www.numpy.org/devdocs/dev/development_workflow.html#get-the-mailing-list-s-opinion

*  HIT ALL THE GUIDELINES:
      https://numpy.org/devdocs/dev/index.html#guidelines

*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:
      http://www.numpy.org/devdocs/dev/development_workflow.html#getting-your-pr-reviewed
-->
# NumPy Logo Guidelines
These guidelines are meant to help keep the NumPy logo consistent and recognizable across all its uses. They also provide a common language for referring to the logos and their components.

The primary logo is the horizontal option (logomark and text next to each other) and the secondary logo is the stacked version (logomark over text). I’ve also provided the logomark on its own (meaning it doesn’t have text). When in doubt, it’s preferable to use primary or secondary options over the logomark alone.

## Color
The full color options are a combo of two shades of blue, rgb(77, 171, 207) and rgb(77, 119, 207), while light options are rgb(255, 255, 255) and dark options are rgb(1, 50, 67).

Whenever possible, use the full color logos. One color logos (light or dark) are to be used when full color will not have enough contrast, usually when logos must be on colored backgrounds.

## Minimum Size
Please do not make the primary logo smaller than 50px wide, secondary logo smaller than 35px wide, or logomark smaller than 20px wide.

## Logo Integrity
A few other notes to keep in mind when using the logo:
- Make sure to scale the logo proportionally.
- Maintain a good amount of space around the logo. Don’t let it overlap with text, images, or other elements.
- Do not try and recreate or modify the logo. For example, do not use the logomark and then try to write NumPy in another font.
Note that since Python 3.6 the builtin tracemalloc module can be used to
track allocations inside numpy.
Numpy places its CPU memory allocations into the `np.lib.tracemalloc_domain`
domain.
See https://docs.python.org/3/library/tracemalloc.html.

The tool that used to be here has been deprecated.
==================================
A Guide to Masked Arrays in NumPy
==================================

.. Contents::

See http://www.scipy.org/scipy/numpy/wiki/MaskedArray (dead link)
for updates of this document.


History
-------

As a regular user of MaskedArray, I (Pierre G.F. Gerard-Marchant) became
increasingly frustrated with the subclassing of masked arrays (even if
I can only blame my inexperience). I needed to develop a class of arrays
that could store some additional information along with numerical values,
while keeping the possibility for missing data (picture storing a series
of dates along with measurements, what would later become the `TimeSeries
Scikit <http://projects.scipy.org/scipy/scikits/wiki/TimeSeries>`__
(dead link).

I started to implement such a class, but then quickly realized that
any additional information disappeared when processing these subarrays
(for example, adding a constant value to a subarray would erase its
dates). I ended up writing the equivalent of *numpy.core.ma* for my
particular class, ufuncs included. Everything went fine until I needed to
subclass my new class, when more problems showed up: some attributes of
the new subclass were lost during processing. I identified the culprit as
MaskedArray, which returns masked ndarrays when I expected masked
arrays of my class. I was preparing myself to rewrite *numpy.core.ma*
when I forced myself to learn how to subclass ndarrays. As I became more
familiar with the *__new__* and *__array_finalize__* methods,
I started to wonder why masked arrays were objects, and not ndarrays,
and whether it wouldn't be more convenient for subclassing if they did
behave like regular ndarrays.

The new *maskedarray* is what I eventually come up with. The
main differences with the initial *numpy.core.ma* package are
that MaskedArray is now a subclass of *ndarray* and that the
*_data* section can now be any subclass of *ndarray*. Apart from a
couple of issues listed below, the behavior of the new MaskedArray
class reproduces the old one. Initially the *maskedarray*
implementation was marginally slower than *numpy.ma* in some areas,
but work is underway to speed it up; the expectation is that it can be
made substantially faster than the present *numpy.ma*.


Note that if the subclass has some special methods and
attributes, they are not propagated to the masked version:
this would require a modification of the *__getattribute__*
method (first trying *ndarray.__getattribute__*, then trying
*self._data.__getattribute__* if an exception is raised in the first
place), which really slows things down.

Main differences
----------------

 * The *_data* part of the masked array can be any subclass of ndarray (but not recarray, cf below).
 * *fill_value* is now a property, not a function.
 * in the majority of cases, the mask is forced to *nomask* when no value is actually masked. A notable exception is when a masked array (with no masked values) has just been unpickled.
 * I got rid of the *share_mask* flag, I never understood its purpose.
 * *put*, *putmask* and *take* now mimic the ndarray methods, to avoid unpleasant surprises. Moreover, *put* and *putmask* both update the mask when needed.  * if *a* is a masked array, *bool(a)* raises a *ValueError*, as it does with ndarrays.
 * in the same way, the comparison of two masked arrays is a masked array, not a boolean
 * *filled(a)* returns an array of the same subclass as *a._data*, and no test is performed on whether it is contiguous or not.
 * the mask is always printed, even if it's *nomask*, which makes things easy (for me at least) to remember that a masked array is used.
 * *cumsum* works as if the *_data* array was filled with 0. The mask is preserved, but not updated.
 * *cumprod* works as if the *_data* array was filled with 1. The mask is preserved, but not updated.

New features
------------

This list is non-exhaustive...

 * the *mr_* function mimics *r_* for masked arrays.
 * the *anom* method returns the anomalies (deviations from the average)

Using the new package with numpy.core.ma
----------------------------------------

I tried to make sure that the new package can understand old masked
arrays. Unfortunately, there's no upward compatibility.

For example:

>>> import numpy.core.ma as old_ma
>>> import maskedarray as new_ma
>>> x = old_ma.array([1,2,3,4,5], mask=[0,0,1,0,0])
>>> x
array(data =
 [     1      2 999999      4      5],
      mask =
 [False False True False False],
      fill_value=999999)
>>> y = new_ma.array([1,2,3,4,5], mask=[0,0,1,0,0])
>>> y
array(data = [1 2 -- 4 5],
      mask = [False False True False False],
      fill_value=999999)
>>> x==y
array(data =
 [True True True True True],
      mask =
 [False False True False False],
      fill_value=?)
>>> old_ma.getmask(x) == new_ma.getmask(x)
array([True, True, True, True, True])
>>> old_ma.getmask(y) == new_ma.getmask(y)
array([True, True, False, True, True])
>>> old_ma.getmask(y)
False


Using maskedarray with matplotlib
---------------------------------

Starting with matplotlib 0.91.2, the masked array importing will work with
the maskedarray branch) as well as with earlier versions.

By default matplotlib still uses numpy.ma, but there is an rcParams setting
that you can use to select maskedarray instead.  In the matplotlibrc file
you will find::

  #maskedarray : False       # True to use external maskedarray module
                             # instead of numpy.ma; this is a temporary #
                             setting for testing maskedarray.


Uncomment and set to True to select maskedarray everywhere.
Alternatively, you can test a script with maskedarray by using a
command-line option, e.g.::

  python simple_plot.py --maskedarray


Masked records
--------------

Like *numpy.core.ma*, the *ndarray*-based implementation
of MaskedArray is limited when working with records: you can
mask any record of the array, but not a field in a record. If you
need this feature, you may want to give the *mrecords* package
a try (available in the *maskedarray* directory in the scipy
sandbox). This module defines a new class, *MaskedRecord*. An
instance of this class accepts a *recarray* as data, and uses two
masks: the *fieldmask* has as many entries as records in the array,
each entry with the same fields as a record, but of boolean types:
they indicate whether the field is masked or not; a record entry
is flagged as masked in the *mask* array if all the fields are
masked. A few examples in the file should give you an idea of what
can be done. Note that *mrecords* is still experimental...

Optimizing maskedarray
----------------------

Should masked arrays be filled before processing or not?
--------------------------------------------------------

In the current implementation, most operations on masked arrays involve
the following steps:

 * the input arrays are filled
 * the operation is performed on the filled arrays
 * the mask is set for the results, from the combination of the input masks and the mask corresponding to the domain of the operation.

For example, consider the division of two masked arrays::

  import numpy
  import maskedarray as ma
  x = ma.array([1,2,3,4],mask=[1,0,0,0], dtype=numpy.float_)
  y = ma.array([-1,0,1,2], mask=[0,0,0,1], dtype=numpy.float_)

The division of x by y is then computed as::

  d1 = x.filled(0) # d1 = array([0., 2., 3., 4.])
  d2 = y.filled(1) # array([-1.,  0.,  1.,  1.])
  m = ma.mask_or(ma.getmask(x), ma.getmask(y)) # m =
  array([True,False,False,True])
  dm = ma.divide.domain(d1,d2) # array([False,  True, False, False])
  result = (d1/d2).view(MaskedArray) # masked_array([-0. inf, 3., 4.])
  result._mask = logical_or(m, dm)

Note that a division by zero takes place. To avoid it, we can consider
to fill the input arrays, taking the domain mask into account, so that::

  d1 = x._data.copy() # d1 = array([1., 2., 3., 4.])
  d2 = y._data.copy() # array([-1.,  0.,  1.,  2.])
  dm = ma.divide.domain(d1,d2) # array([False,  True, False, False])
  numpy.putmask(d2, dm, 1) # d2 = array([-1.,  1.,  1.,  2.])
  m = ma.mask_or(ma.getmask(x), ma.getmask(y)) # m =
  array([True,False,False,True])
  result = (d1/d2).view(MaskedArray) # masked_array([-1. 0., 3., 2.])
  result._mask = logical_or(m, dm)

Note that the *.copy()* is required to avoid updating the inputs with
*putmask*.  The *.filled()* method also involves a *.copy()*.

A third possibility consists in avoid filling the arrays::

  d1 = x._data # d1 = array([1., 2., 3., 4.])
  d2 = y._data # array([-1.,  0.,  1.,  2.])
  dm = ma.divide.domain(d1,d2) # array([False,  True, False, False])
  m = ma.mask_or(ma.getmask(x), ma.getmask(y)) # m =
  array([True,False,False,True])
  result = (d1/d2).view(MaskedArray) # masked_array([-1. inf, 3., 2.])
  result._mask = logical_or(m, dm)

Note that here again the division by zero takes place.

A quick benchmark gives the following results:

 * *numpy.ma.divide*  : 2.69 ms per loop
 * classical division     : 2.21 ms per loop
 * division w/ prefilling : 2.34 ms per loop
 * division w/o filling   : 1.55 ms per loop

So, is it worth filling the arrays beforehand ? Yes, if we are interested
in avoiding floating-point exceptions that may fill the result with infs
and nans. No, if we are only interested into speed...


Thanks
------

I'd like to thank Paul Dubois, Travis Oliphant and Sasha for the
original masked array package: without you, I would never have started
that (it might be argued that I shouldn't have anyway, but that's
another story...).  I also wish to extend these thanks to Reggie Dugard
and Eric Firing for their suggestions and numerous improvements.


Revision notes
--------------

  * 08/25/2007 : Creation of this page
  * 01/23/2007 : The package has been moved to the SciPy sandbox, and is regularly updated: please check out your SVN version!
Regenerating lapack_lite source
===============================

:Authors: * David M. Cooke <cookedm@physics.mcmaster.ca>
          * Eric Wieser (upgraded lapack version on 2017-03-26)

The ``numpy/linalg/f2c_*.c`` files are ``f2c``'d versions of the LAPACK routines
required by the ``LinearAlgebra`` module, and wrapped by the ``lapack_lite``
module. The scripts in this directory can be used to create these files
automatically from a directory of LAPACK source files.

You'll need `plex 2.0.0dev`_, available from PyPI, installed to do the
appropriate scrubbing. As of writing, **this is only available for python 2.7**,
and is unlikely to ever be ported to python 3.
As a result, all the Python scripts in this directory must remain compatible
with Python 2.7, even though NumPy itself no longer supports this version,
until these scripts are rewritten to use something other than ``plex``.

.. _plex 2.0.0dev: https://pypi.python.org/pypi/plex/

The routines that ``lapack_litemodule.c`` wraps are listed in
``wrapped_routines``, along with a few exceptions that aren't picked up
properly. Assuming that you have an unpacked LAPACK source tree in
``/tmp/lapack-3.x.x``, you generate the new routines in this directory with::

$ ./make_lite.py wrapped_routines /tmp/lapack-3.x.x

This will grab the right routines, with dependencies, put them into the
appropriate ``f2c_*.f`` files, run ``f2c`` over them, then do some scrubbing
similar to that done to generate the CLAPACK_ distribution.

.. _CLAPACK: http://netlib.org/clapack/index.html

The output C files in git use the LAPACK source from the LAPACK_ page, using
version 3.2.2. Unfortunately, newer versions use newer FORTRAN features, which
are increasingly not supported by ``f2c``. As these are found, the patch files
will need to be changed to re-express new constructs with legacy constructs.

.. _LAPACK: http://netlib.org/lapack/index.html
..  -*- rst -*-

================
NumPy benchmarks
================

Benchmarking NumPy with Airspeed Velocity.


Usage
-----

Airspeed Velocity manages building and Python virtualenvs by itself,
unless told otherwise. Some of the benchmarking features in
``runtests.py`` also tell ASV to use the NumPy compiled by
``runtests.py``. To run the benchmarks, you do not need to install a
development version of NumPy to your current Python environment.

Before beginning, ensure that *airspeed velocity* is installed.
By default, `asv` ships with support for anaconda and virtualenv::

    pip install asv
    pip install virtualenv

After contributing new benchmarks, you should test them locally
before submitting a pull request.

To run all benchmarks, navigate to the root NumPy directory at
the command line and execute::

    python runtests.py --bench

where ``--bench`` activates the benchmark suite instead of the
test suite. This builds NumPy and runs  all available benchmarks
defined in ``benchmarks/``. (Note: this could take a while. Each
benchmark is run multiple times to measure the distribution in
execution times.)

To run benchmarks from a particular benchmark module, such as
``bench_core.py``, simply append the filename without the extension::

    python runtests.py --bench bench_core

To run a benchmark defined in a class, such as ``Mandelbrot``
from ``bench_avx.py``::

    python runtests.py --bench bench_avx.Mandelbrot

Compare change in benchmark results to another version/commit/branch::

    python runtests.py --bench-compare v1.6.2 bench_core
    python runtests.py --bench-compare 8bf4e9b bench_core
    python runtests.py --bench-compare main bench_core

All of the commands above display the results in plain text in
the console, and the results are not saved for comparison with
future commits. For greater control, a graphical view, and to
have results saved for future comparison you can run ASV commands
(record results and generate HTML)::

    cd benchmarks
    asv run -n -e --python=same
    asv publish
    asv preview

More on how to use ``asv`` can be found in `ASV documentation`_
Command-line help is available as usual via ``asv --help`` and
``asv run --help``.

.. _ASV documentation: https://asv.readthedocs.io/


Writing benchmarks
------------------

See `ASV documentation`_ for basics on how to write benchmarks.

Some things to consider:

- The benchmark suite should be importable with any NumPy version.

- The benchmark parameters etc. should not depend on which NumPy version
  is installed.

- Try to keep the runtime of the benchmark reasonable.

- Prefer ASV's ``time_`` methods for benchmarking times rather than cooking up
  time measurements via ``time.clock``, even if it requires some juggling when
  writing the benchmark.

- Preparing arrays etc. should generally be put in the ``setup`` method rather
  than the ``time_`` methods, to avoid counting preparation time together with
  the time of the benchmarked operation.

- Be mindful that large arrays created with ``np.empty`` or ``np.zeros`` might
  not be allocated in physical memory until the memory is accessed. If this is
  desired behaviour, make sure to comment it in your setup function. If
  you are benchmarking an algorithm, it is unlikely that a user will be
  executing said algorithm on a newly created empty/zero array. One can force
  pagefaults to occur in the setup phase either by calling ``np.ones`` or
  ``arr.fill(value)`` after creating the array,
This file contains a walkthrough of branching NumPy 1.21.x on Linux.  The
commands can be copied into the command line, but be sure to replace 1.21 and
1.22 by the correct versions. It is good practice to make ``.mailmap`` as
current as possible before making the branch, that may take several weeks.

This should be read together with the general directions in `releasing`.

Branching
=========

Make the branch
---------------

This is only needed when starting a new maintenance branch. Because
NumPy now depends on tags to determine the version, the start of a new
development cycle in the main branch needs an annotated tag. That is done
as follows::

    $ git checkout main
    $ git pull upstream main
    $ git commit --allow-empty -m'REL: Begin NumPy 1.22.0 development'
    $ git push upstream HEAD

If the push fails because new PRs have been merged, do::

    $ git pull --rebase upstream

and repeat the push. Once the push succeeds, tag it::

    $ git tag -a -s v1.22.0.dev0 -m'Begin NumPy 1.22.0 development'
    $ git push upstream v1.22.0.dev0

then make the new branch and push it::

    $ git branch maintenance/1.21.x HEAD^
    $ git push upstream maintenance/1.21.x

Prepare the main branch for further development
-----------------------------------------------

Make a PR branch to prepare main for further development::

    $ git checkout -b 'prepare-main-for-1.22.0-development' v1.22.0.dev0

Delete the release note fragments::

    $ git rm doc/release/upcoming_changes/[0-9]*.*.rst

Create the new release notes skeleton and add to index::

    $ cp doc/source/release/template.rst doc/source/release/1.22.0-notes.rst
    $ gvim doc/source/release/1.22.0-notes.rst  # put the correct version
    $ git add doc/source/release/1.22.0-notes.rst
    $ gvim doc/source/release.rst  # add new notes to notes index
    $ git add doc/source/release.rst

Update ``pavement.py`` and update the ``RELEASE_NOTES`` variable to point to
the new notes::

    $ gvim pavement.py
    $ git add pavement.py

Update ``cversions.txt`` to add current release. There should be no new hash
to worry about at this early point, just add a comment following previous
practice::

    $ gvim numpy/core/code_generators/cversions.txt
    $ git add numpy/core/code_generators/cversions.txt

Check your work, commit it, and push::

    $ git status  # check work
    $ git commit -m'REL: Prepare main for NumPy 1.22.0 development'
    $ git push origin HEAD

Now make a pull request.

.. _NEP25:

======================================
NEP 25 — NA support via special dtypes
======================================

:Author: Nathaniel J. Smith <njs@pobox.com>
:Status: Deferred
:Type: Standards Track
:Created: 2011-07-08

Abstract
========

*Context: this NEP was written as an additional alternative to NEP 12 (NEP 24
is another alternative), which at the time of writing had an implementation
that was merged into the NumPy main branch.*

To try and make more progress on the whole missing values/masked arrays/...
debate, it seems useful to have a more technical discussion of the pieces
which we *can* agree on. This is the second, which attempts to nail down the
details of how NAs can be implemented using special dtype's.

Rationale
---------

An ordinary value is something like an integer or a floating point number. A
missing value is a placeholder for an ordinary value that is for some reason
unavailable. For example, in working with statistical data, we often build
tables in which each row represents one item, and each column represents
properties of that item. For instance, we might take a group of people and
for each one record height, age, education level, and income, and then stick
these values into a table. But then we discover that our research assistant
screwed up and forgot to record the age of one of our individuals. We could
throw out the rest of their data as well, but this would be wasteful; even
such an incomplete row is still perfectly usable for some analyses (e.g., we
can compute the correlation of height and income). The traditional way to
handle this would be to stick some particular meaningless value in for the
missing data,e.g., recording this person's age as 0. But this is very error
prone; we may later forget about these special values while running other
analyses, and discover to our surprise that babies have higher incomes than
teenagers. (In this case, the solution would be to just leave out all the
items where we have no age recorded, but this isn't a general solution; many
analyses require something more clever to handle missing values.) So instead
of using an ordinary value like 0, we define a special "missing" value,
written "NA" for "not available".

There are several possible ways to represent such a value in memory. For
instance, we could reserve a specific value (like 0, or a particular NaN, or
the smallest negative integer) and then ensure that this value is treated
specially by all arithmetic and other operations on our array. Another option
would be to add an additional mask array next to our main array, use this to
indicate which values should be treated as NA, and then extend our array
operations to check this mask array whenever performing computations. Each
implementation approach has various strengths and weaknesses, but here we focus
on the former (value-based) approach exclusively and leave the possible
addition of the latter to future discussion. The core advantages of this
approach are (1) it adds no additional memory overhead, (2) it is
straightforward to store and retrieve such arrays to disk using existing file
storage formats, (3) it allows binary compatibility with R arrays including NA
values, (4) it is compatible with the common practice of using NaN to indicate
missingness when working with floating point numbers, (5) the dtype is already
a place where "weird things can happen" -- there are a wide variety of dtypes
that don't act like ordinary numbers (including structs, Python objects,
fixed-length strings, ...), so code that accepts arbitrary NumPy arrays already
has to be prepared to handle these (even if only by checking for them and
raising an error). Therefore adding yet more new dtypes has less impact on
extension authors than if we change the ndarray object itself.

The basic semantics of NA values are as follows. Like any other value, they
must be supported by your array's dtype -- you can't store a floating point
number in an array with dtype=int32, and you can't store an NA in it either.
You need an array with dtype=NAint32 or something (exact syntax to be
determined). Otherwise, NA values act exactly like any other values. In
particular, you can apply arithmetic functions and so forth to them. By
default, any function which takes an NA as an argument always returns an NA as
well, regardless of the values of the other arguments. This ensures that if we
try to compute the correlation of income with age, we will get "NA", meaning
"given that some of the entries could be anything, the answer could be anything
as well". This reminds us to spend a moment thinking about how we should
rephrase our question to be more meaningful. And as a convenience for those
times when you do decide that you just want the correlation between the known
ages and income, then you can enable this behavior by adding a single argument
to your function call.

For floating point computations, NAs and NaNs have (almost?) identical
behavior. But they represent different things -- NaN an invalid computation
like 0/0, NA a value that is not available -- and distinguishing between these
things is useful because in some situations they should be treated differently.
(For example, an imputation procedure should replace NAs with imputed values,
but probably should leave NaNs alone.) And anyway, we can't use NaNs for
integers, or strings, or booleans, so we need NA anyway, and once we have NA
support for all these types, we might as well support it for floating point too
for consistency.

General strategy
================

NumPy already has a general mechanism for defining new dtypes and slotting them
in so that they're supported by ndarrays, by the casting machinery, by ufuncs,
and so on. In principle, we could implement NA-dtypes just using these existing
interfaces. But we don't want to do that, because defining all those new ufunc
loops etc. from scratch would be a huge hassle, especially since the basic
functionality needed is the same in all cases. So we need some generic
functionality for NAs -- but it would be better not to bake this in as a single
set of special "NA types", since users may well want to define new custom
dtypes that have their own NA values, and have them integrate well the rest of
the NA machinery. Our strategy, therefore, is to avoid the `mid-layer mistake`_
by exposing some code for generic NA handling in different situations, which
dtypes can selectively use or not as they choose.

.. _mid-layer mistake: https://lwn.net/Articles/336262/

Some example use cases:
  1. We want to define a dtype that acts exactly like an int32, except that the
     most negative value is treated as NA.
  2. We want to define a parametrized dtype to represent `categorical data`_,
     and the bit-pattern to be used for NA depends on the number of categories
     defined, so our code needs to play an active role handling it rather than
     simply deferring to the standard machinery.
  3. We want to define a dtype that acts like an length-10 string and supports
     NAs. Since our string may hold arbitrary binary values, we want to actually
     allocate 11 bytes for it, with the first byte a flag indicating whether this
     string is NA and the rest containing the string content.
  4. We want to define a dtype that allows multiple different types of NA data,
     which print differently and can be distinguished by the new ufunc that we
     define called ``is_na_of_type(...)``, but otherwise takes advantage of the
     generic NA machinery for most operations.

.. _categorical data: http://mail.scipy.org/pipermail/numpy-discussion/2010-August/052401.html

dtype C-level API extensions
============================

.. highlight:: c

The `PyArray_Descr`_ struct gains the following new fields::

  void * NA_value;
  PyArray_Descr * NA_extends;
  int NA_extends_offset;

.. _PyArray_Descr: http://docs.scipy.org/doc/numpy/reference/c-api.types-and-structures.html#PyArray_Descr

The following new flag values are defined::

  NPY_NA_AUTO_ARRFUNCS
  NPY_NA_AUTO_CAST
  NPY_NA_AUTO_UFUNC
  NPY_NA_AUTO_UFUNC_CHECKED
  NPY_NA_AUTO_ALL /* the above flags OR'ed together */

The `PyArray_ArrFuncs`_ struct gains the following new fields::

  void (*isna)(void * src, void * dst, npy_intp n, void * arr);
  void (*clearna)(void * data, npy_intp n, void * arr);

.. _PyArray_ArrFuncs: http://docs.scipy.org/doc/numpy/reference/c-api.types-and-structures.html#PyArray_ArrFuncs

We add at least one new convenience macro::

  #define NPY_NA_SUPPORTED(dtype) ((dtype)->f->isna != NULL)

The general idea is that anywhere where we used to call a dtype-specific
function pointer, the code will be modified to instead:

  1. Check for whether the relevant ``NPY_NA_AUTO_...`` bit is enabled, the
     NA_extends field is non-NULL, and the function pointer we wanted to call
     is NULL.
  2. If these conditions are met, then use ``isna`` to identify which entries
     in the array are NA, and handle them appropriately. Then look up whatever
     function we were *going* to call using this dtype on the ``NA_extends``
     dtype instead, and use that to handle the non-NA elements.

For more specifics, see following sections.

Note that if ``NA_extends`` points to a parametrized dtype, then the dtype
object it points to must be fully specified. For example, if it is a string
dtype, it must have a non-zero ``elsize`` field.

In order to handle the case where the NA information is stored in a field next
to the `real' data, the ``NA_extends_offset`` field is set to a non-zero value;
it must point to the location within each element of this dtype where some data
of the ``NA_extends`` dtype is found. For example, if we have are storing
10-byte strings with an NA indicator byte at the beginning, then we have::

  elsize == 11
  NA_extends_offset == 1
  NA_extends->elsize == 10

When delegating to the ``NA_extends`` dtype, we offset our data pointer by
``NA_extends_offset`` (while keeping our strides the same) so that it sees an
array of data of the expected type (plus some superfluous padding). This is
basically the same mechanism that record dtypes use, IIUC, so it should be
pretty well-tested.

When delegating to a function that cannot handle "misbehaved" source data (see
the ``PyArray_ArrFuncs`` documentation for details), then we need to check for
alignment issues before delegating (especially with a non-zero
``NA_extends_offset``). If there's a problem, when we need to "clean up" the
source data first, using the usual mechanisms for handling misaligned data. (Of
course, we should usually set up our dtypes so that there aren't any alignment
issues, but someone screws that up, or decides that reduced memory usage is
more important to them then fast inner loops, then we should still handle that
gracefully, as we do now.)

The ``NA_value`` and ``clearna`` fields are used for various sorts of casting.
``NA_value`` is a bit-pattern to be used when, for example, assigning from
np.NA. ``clearna`` can be a no-op if ``elsize`` and ``NA_extends->elsize`` are
the same, but if they aren't then it should clear whatever auxiliary NA storage
this dtype uses, so that none of the specified array elements are NA.

Core dtype functions
--------------------

The following functions are defined in ``PyArray_ArrFuncs``. The special
behavior described here is enabled by the NPY_NA_AUTO_ARRFUNCS bit in the dtype
flags, and only enabled if the given function field is *not* filled in.

``getitem``: Calls ``isna``. If ``isna`` returns true, returns np.NA.
Otherwise, delegates to the ``NA_extends`` dtype.

``setitem``: If the input object is ``np.NA``, then runs
``memcpy(self->NA_value, data, arr->dtype->elsize);``. Otherwise, calls
``clearna``, and then delegates to the ``NA_extends`` dtype.

``copyswapn``, ``copyswap``: FIXME: Not sure whether there's any special
handling to use for these?

``compare``: FIXME: how should this handle NAs? R's sort function *discards*
NAs, which doesn't seem like a good option.

``argmax``: FIXME: what is this used for? If it's the underlying implementation
for np.max, then it really needs some way to get a skipna argument. If not,
then the appropriate semantics depends on what it's supposed to accomplish...

``dotfunc``: QUESTION: is it actually guaranteed that everything has the same
dtype? FIXME: same issues as for ``argmax``.

``scanfunc``: This one's ugly. We may have to explicitly override it in all of
our special dtypes, because assuming that we want the option of, say, having
the token "NA" represent an NA value in a text file, we need some way to check
whether that's there before delegating. But ``ungetc`` is only guaranteed to
let us put back 1 character, and we need 2 (or maybe 3 if we actually check for
"NA "). The other option would be to read to the next delimiter, check whether
we have an NA, and if not then delegate to ``fromstr`` instead of ``scanfunc``,
but according to the current API, each dtype might in principle use a totally
different rule for defining "the next delimiter". So... any ideas? (FIXME)

``fromstr``: Easy -- check for "NA ", if present then assign ``NA_value``,
otherwise call ``clearna`` and delegate.

``nonzero``: FIXME: again, what is this used for? (It seems redundant with
using the casting machinery to cast to bool.) Probably it needs to be modified
so that it can return NA, though...

``fill``: Use ``isna`` to check if either of the first two values is NA. If so,
then fill the rest of the array with ``NA_value``. Otherwise, call ``clearna``
and then delegate.

``fillwithvalue``: Guess this can just delegate?

``sort``, ``argsort``: These should probably arrange to sort NAs to a
particular place in the array (either the front or the back -- any opinions?)

``scalarkind``: FIXME: I have no idea what this does.

``castdict``, ``cancastscalarkindto``, ``cancastto``: See section on casting
below.

Casting
-------

FIXME: this really needs attention from an expert on NumPy's casting rules. But
I can't seem to find the docs that explain how casting loops are looked up and
decided between (e.g., if you're casting from dtype A to dtype B, which dtype's
loops are used?), so I can't go into details. But those details are tricky and
they matter...

But the general idea is, if you have a dtype with ``NPY_NA_AUTO_CAST`` set,
then the following conversions are automatically allowed:

  * Casting from the underlying type to the NA-type: this is performed by the
  * usual ``clearna`` + potentially-strided copy dance. Also, ``isna`` is
  * called to check that none of the regular values have been accidentally
  * converted into NA; if so, then an error is raised.
  * Casting from the NA-type to the underlying type: allowed in principle, but
    if ``isna`` returns true for any of the values that are to be converted,
    then again, an error is raised. (If you want to get around this, use
    ``np.view(array_with_NAs, dtype=float)``.)
  * Casting between the NA-type and other types that do not support NA: this is
    allowed if the underlying type is allowed to cast to the other type, and is
    performed by combining a cast to or from the underlying type (using the
    above rules) with a cast to or from the other type (using the underlying
    type's rules).
  * Casting between the NA-type and other types that do support NA: if the
    other type has NPY_NA_AUTO_CAST set, then we use the above rules plus the
    usual dance with ``isna`` on one array being converted to ``NA_value``
    elements in the other. If only one of the arrays has NPY_NA_AUTO_CAST set,
    then it's assumed that that dtype knows what it's doing, and we don't do
    any magic. (But this is one of the things that I'm not sure makes sense, as
    per my caveat above.)

Ufuncs
------

All ufuncs gain an additional optional keyword argument, ``skipNA=``, which
defaults to False.

If ``skipNA == True``, then the ufunc machinery *unconditionally* calls
``isna`` for any dtype where NPY_NA_SUPPORTED(dtype) is true, and then acts as
if any values for which isna returns True were masked out in the ``where=``
argument (see miniNEP 1 for the behavior of ``where=``). If a ``where=``
argument is also given, then it acts as if the ``isna`` values had be ANDed out
of the ``where=`` mask, though it does not actually modify the mask. Unlike the
other changes below, this is performed *unconditionally* for any dtype which
has an ``isna`` function defined; the NPY_NA_AUTO_UFUNC flag is *not* checked.

If NPY_NA_AUTO_UFUNC is set, then ufunc loop lookup is modified so that
whenever it checks for the existence of a loop on the current dtype, and does
not find one, then it also checks for a loop on the ``NA_extends`` dtype. If
that loop is found, then it uses it in the normal way, with the exceptions that
(1) it is only called for values which are not NA according to ``isna``, (2) if
the output array has NPY_NA_AUTO_UFUNC set, then ``clearna`` is called on it
before calling the ufunc loop, (3) pointer offsets are adjusted by
``NA_extends_offset`` before calling the ufunc loop. In addition, if
NPY_NA_AUTO_UFUNC_CHECK is set, then after evaluating the ufunc loop we call
``isna`` on the *output* array, and if there are any NAs in the output which
were not in the input, then we raise an error. (The intention of this is to
catch cases where, say, we represent NA using the most-negative integer, and
then someone's arithmetic overflows to create such a value by accident.)

FIXME: We should go into more detail here about how NPY_NA_AUTO_UFUNC works
when there are multiple input arrays, of which potentially some have the flag
set and some do not.

Printing
--------

FIXME: There should be some sort of mechanism by which values which are NA are
automatically repr'ed as NA, but I don't really understand how NumPy printing
works, so I'll let someone else fill in this section.

Indexing
--------

Scalar indexing like ``a[12]`` goes via the ``getitem`` function, so according
to the proposal as described above, if a dtype delegates ``getitem``, then
scalar indexing on NAs will return the object ``np.NA``. (If it doesn't
delegate ``getitem``, of course, then it can return whatever it wants.)

This seems like the simplest approach, but an alternative would be to add a
special case to scalar indexing, where if an ``NPY_NA_AUTO_INDEX`` flag were
set, then it would call ``isna`` on the specified element. If this returned
false, it would call ``getitem`` as usual; otherwise, it would return a 0-d
array containing the specified element. The problem with this is that it breaks
expressions like ``if a[i] is np.NA: ...``. (Of course, there is nothing nearly
so convenient as that for NaN values now, but then, NaN values don't have their
own global singleton.) So for now we stick to scalar indexing just returning
``np.NA``, but this can be revisited if anyone objects.

.. highlight:: python

Python API for generic NA support
=================================

NumPy will gain a global singleton called ``numpy.NA``, similar to None, but with
semantics reflecting its status as a missing value. In particular, trying to
treat it as a boolean will raise an exception, and comparisons with it will
produce ``numpy.NA`` instead of True or False. These basics are adopted from the
behavior of the NA value in the R project. To dig deeper into the ideas,
http://en.wikipedia.org/wiki/Ternary_logic#Kleene_logic provides a starting
point.

Most operations on ``np.NA`` (e.g., ``__add__``, ``__mul__``) are overridden to
unconditionally return ``np.NA``.

The automagic dtype detection used for expressions like ``np.asarray([1, 2,
3])``, ``np.asarray([1.0, 2.0. 3.0])`` will be extended to recognize the
``np.NA`` value, and use it to automatically switch to a built-in NA-enabled
dtype (which one being determined by the other elements in the array). A simple
``np.asarray([np.NA])`` will use an NA-enabled float64 dtype (which is
analogous to what you get from ``np.asarray([])``). Note that this means that
expressions like ``np.log(np.NA)`` will work: first ``np.NA`` will be coerced
to a 0-d NA-float array, and then ``np.log`` will be called on that.

Python-level dtype objects gain the following new fields::

  NA_supported
  NA_value

``NA_supported`` is a boolean which simply exposes the value of the
``NPY_NA_SUPPORTED`` flag; it should be true if this dtype allows for NAs,
false otherwise. [FIXME: would it be better to just key this off the existence
of the ``isna`` function? Even if a dtype decides to implement all other NA
handling itself, it still has to define ``isna`` in order to make ``skipNA=``
work correctly.]

``NA_value`` is a 0-d array of the given dtype, and its sole element contains
the same bit-pattern as the dtype's underlying ``NA_value`` field. This makes
it possible to determine the default bit-pattern for NA values for this type
(e.g., with ``np.view(mydtype.NA_value, dtype=int8)``).

We *do not* expose the ``NA_extends`` and ``NA_extends_offset`` values at the
Python level, at least for now; they're considered an implementation detail
(and it's easier to expose them later if they're needed then unexpose them if
they aren't).

Two new ufuncs are defined: ``np.isNA`` returns a logical array, with true
values where-ever the dtype's ``isna`` function returned true. ``np.isnumber``
is only defined for numeric dtypes, and returns True for all elements which are
not NA, and for which ``np.isfinite`` would return True.

Builtin NA dtypes
=================

The above describes the generic machinery for NA support in dtypes. It's
flexible enough to handle all sorts of situations, but we also want to define a
few generally useful NA-supporting dtypes that are available by default.

For each built-in dtype, we define an associated NA-supporting dtype, as
follows:

* floats: the associated dtype uses a specific NaN bit-pattern to indicate NA
  (chosen for R compatibility)
* complex: we do whatever R does (FIXME: look this up -- two NA floats,
  probably?)
* signed integers: the most-negative signed value is used as NA (chosen for R
  compatibility)
* unsigned integers: the most-positive value is used as NA (no R compatibility
  possible).
* strings: the first byte (or, in the case of unicode strings, first 4 bytes)
  is used as a flag to indicate NA, and the rest of the data gives the actual
  string. (no R compatibility possible)
* objects: Two options (FIXME): either we don't include an NA-ful version, or
  we use np.NA as the NA bit pattern.
* boolean: we do whatever R does (FIXME: look this up -- 0 == FALSE, 1 == TRUE,
  2 == NA?)

Each of these dtypes is trivially defined using the above machinery, and are
what are automatically used by the automagic type inference machinery (for
``np.asarray([True, np.NA, False])``, etc.).

They can also be accessed via a new function ``np.withNA``, which takes a
regular dtype (or an object that can be coerced to a dtype, like 'float') and
returns one of the above dtypes. Ideally ``withNA`` should also take some
optional arguments that let you describe which values you want to count as NA,
etc., but I'll leave that for a future draft (FIXME).

FIXME: If ``d`` is one of the above dtypes, then should ``d.type`` return?

The NEP also contains a proposal for a somewhat elaborate
domain-specific-language for describing NA dtypes. I'm not sure how great an
idea that is. (I have a bias against using strings as data structures, and find
the already existing strings confusing enough as it is -- also, apparently the
NEP version of NumPy uses strings like 'f8' when printing dtypes, while my
NumPy uses object names like 'float64', so I'm not sure what's going on there.
``withNA(float64, arg1=value1)`` seems like a more pleasant way to print a
dtype than "NA[f8,value1]", at least to me.) But if people want it, then cool.

Type hierarchy 
--------------

FIXME: how should we do subtype checks, etc., for NA dtypes? What does
``issubdtype(withNA(float), float)`` return? How about
``issubdtype(withNA(float), np.floating)``?

Serialization
-------------


Copyright
---------

This document has been placed in the public domain.
.. _NEP05:

=======================================
NEP 5 — Generalized Universal Functions
=======================================

:Status: Final

There is a general need for looping over not only functions on scalars
but also over functions on vectors (or arrays), as explained on
http://scipy.org/scipy/numpy/wiki/GeneralLoopingFunctions.  We propose
to realize this concept by generalizing the universal functions
(ufuncs), and provide a C implementation that adds ~500 lines
to the numpy code base.  In current (specialized) ufuncs, the elementary
function is limited to element-by-element operations, whereas the
generalized version supports "sub-array" by "sub-array" operations.
The Perl vector library PDL provides a similar functionality and its
terms are re-used in the following.

Each generalized ufunc has information associated with it that states
what the "core" dimensionality of the inputs is, as well as the
corresponding dimensionality of the outputs (the element-wise ufuncs
have zero core dimensions).  The list of the core dimensions for all
arguments is called the "signature" of a ufunc.  For example, the
ufunc numpy.add has signature ``(),()->()`` defining two scalar inputs
and one scalar output.

Another example is (see the GeneralLoopingFunctions page) the function
``inner1d(a,b)`` with a signature of ``(i),(i)->()``.  This applies the
inner product along the last axis of each input, but keeps the
remaining indices intact.  For example, where ``a`` is of shape ``(3,5,N)``
and ``b`` is of shape ``(5,N)``, this will return an output of shape ``(3,5)``.
The underlying elementary function is called 3*5 times.  In the
signature, we specify one core dimension ``(i)`` for each input and zero core
dimensions ``()`` for the output, since it takes two 1-d arrays and
returns a scalar.  By using the same name ``i``, we specify that the two
corresponding dimensions should be of the same size (or one of them is
of size 1 and will be broadcasted).

The dimensions beyond the core dimensions are called "loop" dimensions.  In
the above example, this corresponds to ``(3,5)``.

The usual numpy "broadcasting" rules apply, where the signature
determines how the dimensions of each input/output object are split
into core and loop dimensions:

#. While an input array has a smaller dimensionality than the corresponding
   number of core dimensions, 1's are prepended to its shape.
#. The core dimensions are removed from all inputs and the remaining
   dimensions are broadcasted; defining the loop dimensions.
#. The output is given by the loop dimensions plus the output core dimensions.



Definitions
-----------

Elementary Function
    Each ufunc consists of an elementary function that performs the
    most basic operation on the smallest portion of array arguments
    (e.g. adding two numbers is the most basic operation in adding two
    arrays).  The ufunc applies the elementary function multiple times
    on different parts of the arrays.  The input/output of elementary
    functions can be vectors; e.g., the elementary function of inner1d
    takes two vectors as input.

Signature
    A signature is a string describing the input/output dimensions of
    the elementary function of a ufunc.  See section below for more
    details.

Core Dimension
    The dimensionality of each input/output of an elementary function
    is defined by its core dimensions (zero core dimensions correspond
    to a scalar input/output).  The core dimensions are mapped to the
    last dimensions of the input/output arrays.

Dimension Name
    A dimension name represents a core dimension in the signature.
    Different dimensions may share a name, indicating that they are of
    the same size (or are broadcastable).

Dimension Index
    A dimension index is an integer representing a dimension name. It
    enumerates the dimension names according to the order of the first
    occurrence of each name in the signature.


Details of Signature
--------------------

The signature defines "core" dimensionality of input and output
variables, and thereby also defines the contraction of the
dimensions.  The signature is represented by a string of the
following format:

* Core dimensions of each input or output array are represented by a
  list of dimension names in parentheses, ``(i_1,...,i_N)``; a scalar
  input/output is denoted by ``()``.  Instead of ``i_1``, ``i_2``,
  etc, one can use any valid Python variable name.
* Dimension lists for different arguments are separated by ``","``.
  Input/output arguments are separated by ``"->"``.
* If one uses the same dimension name in multiple locations, this
  enforces the same size (or broadcastable size) of the corresponding
  dimensions.

The formal syntax of signatures is as follows::

    <Signature>            ::= <Input arguments> "->" <Output arguments>
    <Input arguments>      ::= <Argument list>
    <Output arguments>     ::= <Argument list>
    <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
    <Argument>             ::= "(" <Core dimension list> ")"
    <Core dimension list>  ::= nil | <Dimension name> |
                               <Dimension name> "," <Core dimension list>
    <Dimension name>       ::= valid Python variable name


Notes:

#. All quotes are for clarity.
#. Core dimensions that share the same name must be broadcastable, as
   the two ``i`` in our example above.  Each dimension name typically
   corresponding to one level of looping in the elementary function's
   implementation.
#. White spaces are ignored.

Here are some examples of signatures:

+-------------+------------------------+-----------------------------------+
| add         | ``(),()->()``          |                                   |
+-------------+------------------------+-----------------------------------+
| inner1d     | ``(i),(i)->()``        |                                   |
+-------------+------------------------+-----------------------------------+
| sum1d       | ``(i)->()``            |                                   |
+-------------+------------------------+-----------------------------------+
| dot2d       | ``(m,n),(n,p)->(m,p)`` | matrix multiplication             |
+-------------+------------------------+-----------------------------------+
| outer_inner | ``(i,t),(j,t)->(i,j)`` | inner over the last dimension,    |
|             |                        | outer over the second to last,    |
|             |                        | and loop/broadcast over the rest. |
+-------------+------------------------+-----------------------------------+

C-API for implementing Elementary Functions
-------------------------------------------

The current interface remains unchanged, and ``PyUFunc_FromFuncAndData``
can still be used to implement (specialized) ufuncs, consisting of
scalar elementary functions.

One can use ``PyUFunc_FromFuncAndDataAndSignature`` to declare a more
general ufunc.  The argument list is the same as
``PyUFunc_FromFuncAndData``, with an additional argument specifying the
signature as C string.

Furthermore, the callback function is of the same type as before,
``void (*foo)(char **args, intp *dimensions, intp *steps, void *func)``.
When invoked, ``args`` is a list of length ``nargs`` containing
the data of all input/output arguments.  For a scalar elementary
function, ``steps`` is also of length ``nargs``, denoting the strides used
for the arguments. ``dimensions`` is a pointer to a single integer
defining the size of the axis to be looped over.

For a non-trivial signature, ``dimensions`` will also contain the sizes
of the core dimensions as well, starting at the second entry.  Only
one size is provided for each unique dimension name and the sizes are
given according to the first occurrence of a dimension name in the
signature.

The first ``nargs`` elements of ``steps`` remain the same as for scalar
ufuncs.  The following elements contain the strides of all core
dimensions for all arguments in order.

For example, consider a ufunc with signature ``(i,j),(i)->()``.  In
this case, ``args`` will contain three pointers to the data of the
input/output arrays ``a``, ``b``, ``c``.  Furthermore, ``dimensions`` will be
``[N, I, J]`` to define the size of ``N`` of the loop and the sizes ``I`` and ``J``
for the core dimensions ``i`` and ``j``.  Finally, ``steps`` will be
``[a_N, b_N, c_N, a_i, a_j, b_i]``, containing all necessary strides.
.. _NEP08:

=============================================================
NEP 8 —  A proposal for adding groupby functionality to NumPy
=============================================================

:Author: Travis Oliphant
:Contact: oliphant@enthought.com
:Date: 2010-04-27
:Status: Deferred


Executive summary
=================

NumPy provides tools for handling data and doing calculations in much
the same way as relational algebra allows.  However, the common group-by
functionality is not easily handled.  The reduce methods of NumPy's
ufuncs are a natural place to put this groupby behavior.  This NEP
describes two additional methods for ufuncs (reduceby and reducein) and
two additional functions (segment and edges) which can help add this
functionality.

Example Use Case
================
Suppose you have a NumPy structured array containing information about
the number of purchases at several stores over multiple days.  To be clear, the
structured array data-type is::

  dt = [('year', i2), ('month', i1), ('day', i1), ('time', float),
      ('store', i4), ('SKU', 'S6'), ('number', i4)]

Suppose there is a 1-d NumPy array of this data-type and you would like
to compute various statistics (max, min, mean, sum, etc.) on the number
of products sold, by product, by month, by store, etc.

Currently, this could be done by using reduce methods on the number
field of the array, coupled with in-place sorting, unique with
return_inverse=True and bincount, etc.  However, for such a common
data-analysis need, it would be nice to have standard and more direct
ways to get the results.


Ufunc methods proposed
======================

It is proposed to add two new reduce-style methods to the ufuncs:
reduceby and reducein.  The reducein method is intended to be a simpler
to use version of reduceat, while the reduceby method is intended to
provide group-by capability on reductions.

reducein::

        <ufunc>.reducein(arr, indices, axis=0, dtype=None, out=None)

        Perform a local reduce with slices specified by pairs of indices.

        The reduction occurs along the provided axis, using the provided
        data-type to calculate intermediate results, storing the result into
        the array out (if provided).

        The indices array provides the start and end indices for the
        reduction.  If the length of the indices array is odd, then the
        final index provides the beginning point for the final reduction
        and the ending point is the end of arr.

        This generalizes along the given axis, the behavior:

        [<ufunc>.reduce(arr[indices[2*i]:indices[2*i+1]])
                for i in range(len(indices)/2)]

        This assumes indices is of even length

        Example:
           >>> a = [0,1,2,4,5,6,9,10]
           >>> add.reducein(a,[0,3,2,5,-2])
           [3, 11, 19]

           Notice that sum(a[0:3]) = 3; sum(a[2:5]) = 11; and sum(a[-2:]) = 19

reduceby::

        <ufunc>.reduceby(arr, by, dtype=None, out=None)

        Perform a reduction in arr over unique non-negative integers in by.


        Let N=arr.ndim and M=by.ndim.  Then, by.shape[:N] == arr.shape.
        In addition, let I be an N-length index tuple, then by[I]
        contains the location in the output array for the reduction to
        be stored.  Notice that if N == M, then by[I] is a non-negative
        integer, while if N < M, then by[I] is an array of indices into
        the output array.

        The reduction is computed on groups specified by unique indices
        into the output array. The index is either the single
        non-negative integer if N == M or if N < M, the entire
        (M-N+1)-length index by[I] considered as a whole.


Functions proposed
==================

- segment
- edges
.. _NEP48:

=====================================
NEP 48 — Spending NumPy project funds
=====================================

:Author: Ralf Gommers <ralf.gommers@gmail.com>
:Author: Inessa Pawson <inessa@albuscode.org>
:Author: Stefan van der Walt <stefanv@berkeley.edu>
:Status: Draft
:Type: Informational
:Created: 2021-02-07
:Resolution:


Abstract
--------

The NumPy project has historically never received significant **unrestricted**
funding. However, that is starting to change.  This NEP aims to provide
guidance about spending NumPy project unrestricted funds by formulating a set
of principles about *what* to pay for and *who* to pay. It will also touch on
how decisions regarding spending funds get made, how funds get administered,
and transparency around these topics.


Motivation and Scope
--------------------

NumPy is a fiscally sponsored project of NumFOCUS, a 501(c)(3) nonprofit
organization headquartered in Austin, TX. Therefore, for all legal and
accounting matters the NumPy project has to follow the rules and regulations
for US nonprofits. All nonprofit donations are classified into two categories:
**unrestricted funds** which may be used for any legal purpose appropriate
to the organization and **restricted funds**, monies set aside for a
particular purpose (e.g., project, educational program, etc.).

For the detailed timeline of NumPy funding refer to
:ref:`numpy-funding-history`.

Since its inception and until 2020, the NumPy project has only spent on the order of
$10,000 USD of funds that were not restricted to a particular program.  Project
income of this type has been relying on donations from individuals and, from
mid 2019, recurring monthly contributions from Tidelift. By the end of 2020,
the Tidelift contributions increased to $3,000/month, and there's also a
potential for an increase of donations and grants going directly to the
project. Having a clear set of principles around how to use these funds will
facilitate spending them fairly and effectively. Additionally, it will make it
easier to solicit donations and other contributions.

A key assumption this NEP makes is that NumPy remains a largely
volunteer-driven project, and that the project funds are not enough to employ
maintainers full-time. If funding increases to the point where that assumption
is no longer true, this NEP should be updated.

In scope for this NEP are:

- Principles of spending project funds: what to pay for, and who to pay.
- Describing how NumPy's funds get administered.
- Describing how decisions to spend funds get proposed and made.

Out of scope for this NEP are:

- Making any decisions about spending project funds on a specific project or
  activity.
- Principles for spending funds that are intended for NumPy development, but
  don't fall in the category of NumPy unrestricted funds. This includes most of
  the grant funding, which is usually earmarked for certain
  activities/deliverables and goes to an Institutional Partner rather than
  directly to the NumPy project, and companies or institutions funding specific
  features.
  *Rationale: As a project, we have no direct control over how this work gets
  executed (at least formally, until issues or PRs show up). In some cases, we
  may not even know the contributions were funded or done by an employee on
  work time. (Whether that's the case or not should not change how we approach
  a contribution).  For grants though, we do expect the research/project leader
  and funded team to align their work with the needs of NumPy and be
  receptive to feedback from other NumPy maintainers and contributors.*


Principles of spending project funds
------------------------------------

NumPy will likely always be a project with many times more volunteer
contributors than funded people. Therefore having those funded people operate
in ways that attract more volunteers and enhance their participation experience
is critical. That key principle motivates many of the more detailed principles
given below for what to pay for and whom to pay.

The approach for spending funds will be:

- first figure out what we want to fund,
- then look for a great candidate,
- after that's settled, determine a fair compensation level.

The next sections go into detail on each of these three points.

.. _section-what-to-pay-for:

What to pay for
```````````````

1. Pay for things that are important *and* otherwise won't get done.
   *Rationale: there is way more to be done than there are funds to do all
   those things. So count on interested volunteers or external sponsored work
   to do many of those things.*
2. Plan for sustainability. Don't rely on money always being there.
3. Consider potential positive benefits for NumPy maintainers and contributors,
   maintainers of other projects, end users, and other stakeholders like
   packagers and educators.
4. Think broadly. There's more to a project than code: websites, documentation,
   community building, governance - it's all important.
5. For proposed funded work, include paid time for others to review your work
   if such review is expected to be significant effort - do not just increase
   the load on volunteer maintainers.
   *Rationale: we want the effect of spending funds to be positive for
   everyone, not just for the people getting paid. This is also a matter of
   fairness.*

When considering development work, principle (1) implies that priority should
be giving to (a) the most boring/painful tasks that no one likes doing, and to
necessary structural changes to the code base that are too large to be done by
a volunteer in a reasonable amount of time.

There are also many tasks, activities, and projects outside of
development work that are important and could enhance the project or community
- think of, for example, user surveys, translations, outreach, dedicated
mentoring of newcomers, community organizating, website improvements, and
administrative tasks.

Time of people to perform tasks is also not the only thing that funds can be
used for: expenses for in-person developer meetings or sprints, hosted hardware
for benchmarking or development work, and CI or other software services could
all be good candidates to spend funds on.

Whom to pay
```````````

1. All else being equal, give preference to existing maintainers/contributors.
2. When looking outside of the current team, consider this an opportunity to
   make the project more diverse.
3. Pay attention to the following when considering paying someone:

   - the necessary technical or domain-specific skills to execute the tasks,
   - communication and self-management skills,
   - experience contributing to and working with open source projects.

It will likely depend on the project/tasks whether there's already a clear best
candidate within the NumPy team, or whether we look for new people to get
involved. Before making any decisions, the decision makers (according to the
NumPy governance document - currently that's the Steering Council) should think
about whether an opportunity should be advertised to give a wider group of
people a chance to apply for it.

Compensating fairly
```````````````````

.. note::

   This section on compensating fairly will be considered *Draft* even if this
   NEP as a whole is accepted. Once we have applied the approach outlined here
   at least 2-3 times and we are happy with it, will we remove this note and
   consider this section *Accepted*.

Paying people fairly is a difficult topic, especially when it comes to
distributed teams. Therefore, we will only offer some guidance here. Final
decisions will always have to be considered and approved by the group of people
that bears this responsibility (according to the current NumPy governance
structure, this would be the NumPy Steering Council).

Discussions on remote employee compensation tend to be dominated by two
narratives: "pay local market rates" and "same work -- same pay".

We consider them both extreme:

- "Same work -- same pay" is unfair to people living in locations with a higher
  cost of living. For example, the average rent for a single family apartment
  can differ by a large factor (from a few hundred dollars to thousands of
  dollars per month).
- "Pay local market rates" bakes in existing inequalities between countries
  and makes fixed-cost items like a development machine or a holiday trip
  abroad relatively harder to afford in locations where market rates are lower.

We seek to find a middle ground between these two extremes.

Useful points of reference include companies like GitLab and
Buffer who are transparent about their remuneration policies ([3]_, [4]_),
Google Summer of Code stipends ([5]_), other open source projects that manage
their budget in a transparent manner (e.g., Babel and Webpack on Open
Collective ([6]_, [7]_)), and standard salary comparison sites.

Since NumPy is a not-for-profit project, we also looked to the nonprofit sector
for guidelines on remuneration policies and compensation levels. Our findings
show that most smaller non-profits tend to pay a median salary/wage. We
recognize merit in this approach: applying candidates are likely to have a
genuine interest in open source, rather than to be motivated purely by
financial incentives.

Considering all of the above, we will use the following guidelines for
determining compensation:

1. Aim to compensate people appropriately, up to a level that's expected for
   senior engineers or other professionals as applicable.
2. Establish a compensation cap of $125,000 USD that cannot be exceeded even
   for the residents from the most expensive/competitive locations ([#f-pay]_).
3. For equivalent work and seniority,  a pay differential between locations
   should never be more than 2x.
   For example, if we pay $110,000 USD to a senior-level developer from New
   York, for equivalent work a senior-level developer from South-East Asia
   should be paid at least $55,000 USD. To compare locations, we will use
   `Numbeo Cost of Living calculator <https://www.numbeo.com/cost-of-living/>`__
   (or its equivalent).

Some other considerations:

- Often, compensated work is offered for a limited amount of hours or fixed
  term. In those cases, consider compensation equivalent to a remuneration
  package that comes with permanent employment (e.g., one month of work should
  be compensated by at most 1/12th of a full-year salary + benefits).
- When comparing rates, an individual contractor should typically make 20% more
  than someone who is employed since they have to take care of their benefits
  and accounting on their own.
- Some people may be happy with one-off payments towards a particular
  deliverable (e.g., "triage all open issues for label X for $x,xxx").
  This should be compensated at a lower rate compared to an individual
  contractor. Or they may motivate lower amounts for another reason (e.g., "I
  want to receive $x,xxx to hire a cleaner or pay for childcare, to free up
  time for work on open source).
- When funding someone's time through their employer, that employer may want to
  set the compensation level based on its internal rules (e.g., overhead rates).
  Small deviations from the guidelines in this NEP may be needed in such cases,
  however they should be within reason.
- It's entirely possible that another strategy rather than paying people for
  their time on certain tasks may turn out to be more effective. Anything that
  helps the project and community grow and improve is worth considering.
- Transparency helps. If everyone involved is comfortable sharing their
  compensation levels with the rest of the team (or better make it public),
  it's least likely to be way off the mark for fairness.

We highly recommend that the individuals involved in decision-making about
hiring and compensation peruse the content of the References section of this
NEP. It offers a lot of helpful advice on this topic.


Defining fundable activities and projects
-----------------------------------------

We'd like to have a broader set of fundable ideas that we will prioritize with
input from NumPy team members and the wider community. All ideas will be
documented on a single wiki page. Anyone may propose an idea. Only members of a
NumPy team may edit the wiki page.

Each listed idea must meet the following requirements:

1. It must be clearly scoped: its description must explain the importance to
   the project, referencing the NumPy Roadmap if possible, the items to pay for
   or activities and deliverables, and why it should be a funded activity (see
   :ref:`section-what-to-pay-for`).
2. It must contain the following metadata: title, cost, time duration or effort
   estimate, and (if known) names of the team member(s) to execute or coordinate.
3. It must have an assigned priority (low, medium, or high). This discussion
   can originate at a NumPy community meeting or on the mailing list. However,
   it must be finalized on the mailing list allowing everyone to weigh in.

If a proposed idea has been assigned a high priority level, a decision on
allocating funding for it will be made on the private NumPy Steering Council
mailing list. *Rationale: these will often involve decisions about individuals,
which is typically hard to do in public. This is the current practice that
seems to be working well.*

Sometimes, it may be practical to make a single funding decision ad-hoc (e.g.,
"Here's a great opportunity plus the right person to execute it right now”).
However, this approach to decision-making should be used rarely.


Strategy for spending/saving funds
----------------------------------

There is an expectation from NumPy individual, corporate, and institutional
donors that the funds will be used for the benefit of the project and the
community. Therefore, we should spend available funds, thoughtfully,
strategically, and fairly, as they come in. For emergencies, we should keep a
$10,000 - $15,000 USD reserve which could cover, for example, a year of CI and
hosting services, 1-2 months of full-time maintenance work, or contracting a
consultant for a specific need.


How project funds get administered
----------------------------------

We will first summarize how administering of funds works today, and then
discuss how to make this process more efficient and transparent.

Currently, the project funds are held by NumFOCUS in a dedicated account.
NumFOCUS has a small accounting team, which produces an account overview as a
set of spreadsheets on a monthly basis. These land in a shared drive, typically
with about a one month delay (e.g., the balance and transactions for February
are available at the end of March), where a few NumPy team members can access
them. Expense claims and invoices are submitted through the NumFOCUS website.
Those then show up in another spreadsheet, where a NumPy team member must
review and approve each of them before payments are made. Following NumPy
bylaws, the NumFOCUS finance subcommittee, consisting of five people, meets
every six months to review all the project related transactions. (In practice,
there have been so few transactions that we skipped some of these meetings.)

The existing process is time-consuming and error-prone. More transparency and
automation are desirable.


Transparency about project funds and in decision making
```````````````````````````````````````````````````````

**To discuss: do we want full transparency by publishing our accounts,
transparency to everyone on a NumPy team, or some other level?**

Ralf: I'd personally like it to be fully transparent, like through Open
Collective, so the whole community can see current balance, income and expenses
paid out at any moment in time. Moving to Open Collective is nontrivial,
however we can publish the data elsewhere for now if we'd want to.
*Note: Google Season of Docs this year requires having an Open Collective
account, so this is likely to happen soon enough.*

Stefan/Inessa: at least a summary overview should be fully public, and all
transactions should be visible to the Steering Council. Full transparency of
all transactions is probably fine, but not necessary.

*The options here may be determined by the accounting system and amount of
effort required.*


.. _numpy-funding-history:

NumPy funding – history and current status
------------------------------------------

The NumPy project received its first major funding in 2017. For an overview of
the early history of NumPy (and SciPy), including some institutions sponsoring
time for their employees or contractors to work on NumPy, see [1]_ and [2]_. To
date, NumPy has received four grants:

- Two grants, from the Alfred P. Sloan Foundation and the Gordon and Betty
  Moore Foundation respectively, of about $1.3M combined to the Berkeley
  Institute of Data Science. Work performed during the period 2017-2020;
  PI Stéfan van der Walt.
- Two grants from the Chan Zuckerberg Foundation to NumFOCUS, for a combined
  amount of $335k. Work performed during the period 2020-2021; PI's Ralf
  Gommers (first grant) and Melissa Mendonça (second grant).

From 2012 onwards NumPy has been a fiscally sponsored project of NumFOCUS.
Note that fiscal sponsorship doesn't mean NumPy gets funding, rather that it
can receive funds under the umbrella of a nonprofit. See `NumFOCUS Project
Support <https://numfocus.org/projects-overview>`__ for more details.

Only since 2017 has the NumPy website displayed a "Donate" button, and since
2019 the NumPy repositories have had the GitHub Sponsors button. Before that,
it was possible to donate to NumPy on the NumFOCUS website. The sum total of
donations from individuals to NumPy for 2017-2020 was about $6,100.

From May 2019 onwards, Tidelift has supported NumPy financially as part of
its "managed open source" business model. From May 2019 till July 2020 this was
$1,000/month, and it started steadily growing after that to about $3,000/month
(as of Feb 2021).

Finally, there has been other incidental project income, for example, some book
royalties from Packt Publishing, GSoC mentoring fees from Google, and
merchandise sales revenue through the NumFOCUS web shop. All of these were
small (two or three figure) amounts.

This brings the total amount of project income which did not already have a
spending target to about $35,000. Most of that is recent, from Tidelift.
Over the past 1.5 years we spent about $10,000 for work on the new NumPy
website and Sphinx theme. Those spending decisions were made by the NumPy
Steering Council and announced on the mailing list.

That leaves about $25,000 in available funds at the time of writing, and
that amount is currently growing at a rate of about $3,000/month.


Related Work
------------

See references.  We assume that other open source projects have also developed
guidelines on spending project funds. However, we were unable to find any
examples at the time of writing.


Alternatives
------------

*Alternative spending strategy*: not having cash reserves. The rationale
being that NumPy is important enough that in a real emergency some person or
entity will likely jump in to help out. This is not a responsible approach to
financial stewardship of the project though. Hence, we decided against it.


Discussion
----------



References and Footnotes
------------------------

.. [1] Pauli Virtanen et al., "SciPy 1.0: fundamental algorithms for scientific
       computing in Python", https://www.nature.com/articles/s41592-019-0686-2,
       2020

.. [2] Charles Harris et al., "Array programming with NumPy", https://www.nature.com/articles/s41586-020-2649-2, 2020

.. [3] https://remote.com/blog/remote-compensation

.. [4] https://about.gitlab.com/company/culture/all-remote/compensation/#how-do-you-decide-how-much-to-pay-people

.. [5] https://developers.google.com/open-source/gsoc/help/student-stipends

.. [6] Jurgen Appelo, "Compensation: what is fair?", https://blog.agilityscales.com/compensation-what-is-fair-38a65a822c29, 2016

.. [7] Project Include, "Compensating fairly", https://projectinclude.org/compensating_fairly

.. [#f-pay] This cap is derived from comparing with compensation levels at
            other open source projects (e.g., Babel, Webpack, Drupal - all in
            the $100,000 -- $125,000 range) and Partner Institutions.

- Nadia Eghbal, "Roads and Bridges: The Unseen Labor Behind Our Digital
  Infrastructure", 2016
- Nadia Eghbal, "Working in Public: The Making and Maintenance of Open
  Source", 2020
- https://github.com/nayafia/lemonade-stand
- Daniel Oberhaus, `"The Internet Was Built on the Free Labor of Open Source
  Developers. Is That Sustainable?"
  <https://www.vice.com/en/article/43zak3/the-internet-was-built-on-the-free-labor-of-open-source-developers-is-that-sustainable>`_, 2019
- David Heinemeier Hansson, `"The perils of mixing open source and money" <https://dhh.dk/2013/the-perils-of-mixing-open-source-and-money.html>`_, 2013
- Danny Crichton, `"Open source sustainability" <https://techcrunch.com/2018/06/23/open-source-sustainability/?guccounter=1>`_, 2018
- Nadia Eghbal, "Rebuilding the Cathedral", https://www.youtube.com/watch?v=VS6IpvTWwkQ, 2017
- Nadia Eghbal, "Where money meets open source", https://www.youtube.com/watch?v=bjAinwgvQqc&t=246s, 2017
- Eileen Uchitelle, ""The unbearable vulnerability of open source", https://www.youtube.com/watch?v=VdwO3LQ56oM, 2017 (the inverted triangle, open source is a funnel)
- Dries Buytaert, "Balancing Makers and Takers to scale and sustain Open Source", https://dri.es/balancing-makers-and-takers-to-scale-and-sustain-open-source, 2019
- Safia Abdalla, "Beyond Maintenance", https://increment.com/open-source/beyond-maintenance/, 2019
- Xavier Damman, "Money and Open Source Communities", https://blog.opencollective.com/money-and-open-source-communities/, 2016
- Aseem Sood, "Let's talk about money", https://blog.opencollective.com/lets-talk-about-money/, 2017
- Alanna Irving, "Has your open source community raised money? Here's how to spend it.", https://blog.opencollective.com/has-your-open-source-community-raised-money-heres-how-to-spend-it/, 2017
- Alanna Irving, "Funding open source, how Webpack reached $400k+/year", https://blog.opencollective.com/funding-open-source-how-webpack-reached-400k-year/, 2017
- Alanna Irving, "Babel's rise to financial sustainability", https://blog.opencollective.com/babels-rise-to-financial-sustainability/, 2019
- Devon Zuegel, "The city guide to open source", https://www.youtube.com/watch?v=80KTVu6GGSE, 2020 + blog: https://increment.com/open-source/the-city-guide-to-open-source/

GitHub Sponsors:

- https://github.blog/2019-05-23-announcing-github-sponsors-a-new-way-to-contribute-to-open-source/
- https://github.blog/2020-05-12-github-sponsors-is-out-of-beta-for-sponsored-organizations/
- https://blog.opencollective.com/on-github-sponsors/, 2019
- https://blog.opencollective.com/double-the-love/, 2020
- https://blog.opencollective.com/github-sponsors-for-companies-open-source-collective-for-people/


Copyright
---------

This document has been placed in the public domain.
.. _NEP32:

==================================================
NEP 32 — Remove the financial functions from NumPy
==================================================

:Author: Warren Weckesser <warren.weckesser@gmail.com>
:Status: Final
:Type: Standards Track
:Created: 2019-08-30
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2019-September/080074.html


Abstract
--------

We propose deprecating and ultimately removing the financial functions [1]_
from NumPy.  The functions will be moved to an independent repository,
and provided to the community as a separate package with the name
``numpy_financial``.


Motivation and scope
--------------------

The NumPy financial functions [1]_ are the 10 functions ``fv``, ``ipmt``,
``irr``, ``mirr``, ``nper``, ``npv``, ``pmt``, ``ppmt``, ``pv`` and ``rate``.
The functions provide elementary financial calculations such as future value,
net present value, etc. These functions were added to NumPy in 2008 [2]_.

In May, 2009, a request by Joe Harrington to add a function called ``xirr`` to
the financial functions triggered a long thread about these functions [3]_.
One important point that came up in that thread is that a "real" financial
library must be able to handle real dates.  The NumPy financial functions do
not work with actual dates or calendars.  The preference for a more capable
library independent of NumPy was expressed several times in that thread.

In June, 2009, D. L. Goldsmith expressed concerns about the correctness of the
implementations of some of the financial functions [4]_.  It was suggested then
to move the financial functions out of NumPy to an independent package.

In a GitHub issue in 2013 [5]_, Nathaniel Smith suggested moving the financial
functions from the top-level namespace to ``numpy.financial``.  He also
suggested giving the functions better names.  Responses at that time included
the suggestion to deprecate them and move them from NumPy to a separate
package.  This issue is still open.

Later in 2013 [6]_, it was suggested on the mailing list that these functions
be removed from NumPy.

The arguments for the removal of these functions from NumPy:

* They are too specialized for NumPy.
* They are not actually useful for "real world" financial calculations, because
  they do not handle real dates and calendars.
* The definition of "correctness" for some of these functions seems to be a
  matter of convention, and the current NumPy developers do not have the
  background to judge their correctness.
* There has been little interest among past and present NumPy developers
  in maintaining these functions.

The main arguments for keeping the functions in NumPy are:

* Removing these functions will be disruptive for some users.  Current users
  will have to add the new ``numpy_financial`` package to their dependencies,
  and then modify their code to use the new package.
* The functions provided, while not "industrial strength", are apparently
  similar to functions provided by spreadsheets and some calculators.  Having
  them available in NumPy makes it easier for some developers to migrate their
  software to Python and NumPy.

It is clear from comments in the mailing list discussions and in the GitHub
issues that many current NumPy developers believe the benefits of removing
the functions outweigh the costs.  For example, from [5]_::

    The financial functions should probably be part of a separate package
    -- Charles Harris

    If there's a better package we can point people to we could just deprecate
    them and then remove them entirely... I'd be fine with that too...
    -- Nathaniel Smith

    +1 to deprecate them. If no other package exists, it can be created if
    someone feels the need for that.
    -- Ralf Gommers

    I feel pretty strongly that we should deprecate these. If nobody on numpy’s
    core team is interested in maintaining them, then it is purely a drag on
    development for NumPy.
    -- Stephan Hoyer

And from the 2013 mailing list discussion, about removing the functions from
NumPy::

    I am +1 as well, I don't think they should have been included in the first
    place.
    -- David Cournapeau

But not everyone was in favor of removal::

    The fin routines are tiny and don't require much maintenance once
    written.  If we made an effort (putting up pages with examples of common
    financial calculations and collecting those under a topical web page,
    then linking to that page from various places and talking it up), I
    would think they could attract users looking for a free way to play with
    financial scenarios.  [...]
    So, I would say we keep them.  If ours are not the best, we should bring
    them up to snuff.
    -- Joe Harrington

For an idea of the maintenance burden of the financial functions, one can
look for all the GitHub issues [7]_ and pull requests [8]_ that have the tag
``component: numpy.lib.financial``.

One method for measuring the effect of removing these functions is to find
all the packages on GitHub that use them.  Such a search can be performed
with the ``python-api-inspect`` service [9]_.  A search for all uses of the
NumPy financial functions finds just eight repositories.  (See the comments
in [5]_ for the actual SQL query.)


Implementation
--------------

* Create a new Python package, ``numpy_financial``, to be maintained in the
  top-level NumPy github organization.  This repository will contain the
  definitions and unit tests for the financial functions.  The package will
  be added to PyPI so it can be installed with ``pip``.
* Deprecate the financial functions in the ``numpy`` namespace, beginning in
  NumPy version 1.18. Remove the financial functions from NumPy version 1.20.


Backward compatibility
----------------------

The removal of these functions breaks backward compatibility, as explained
earlier.  The effects are mitigated by providing the ``numpy_financial``
library.


Alternatives
------------

The following alternatives were mentioned in [5]_:

* *Maintain the functions as they are (i.e. do nothing).*
  A review of the history makes clear that this is not the preference of many
  NumPy developers.  A recurring comment is that the functions simply do not
  belong in NumPy.  When that sentiment is combined with the history of bug
  reports and the ongoing questions about the correctness of the functions, the
  conclusion is that the cleanest solution is deprecation and removal.
* *Move the functions from the ``numpy`` namespace to ``numpy.financial``.*
  This was the initial suggestion in [5]_.  Such a change does not address the
  maintenance issues, and doesn't change the misfit that many developers see
  between these functions and NumPy.  It causes disruption for the current
  users of these functions without addressing what many developers see as the
  fundamental problem.


Discussion
----------

Links to past mailing list discussions, and to relevant GitHub issues and pull
requests, have already been given.  The announcement of this NEP was made on
the NumPy-Discussion mailing list on 3 September 2019 [10]_, and on the
PyData mailing list on 8 September 2019 [11]_.  The formal proposal to accept
the NEP was made on 19 September 2019 [12]_; a notification was also sent to
PyData (same thread as [11]_).  There have been no substantive objections.


References and footnotes
------------------------

.. [1] Financial functions,
   https://numpy.org/doc/1.17/reference/routines.financial.html

.. [2] NumPy-Discussion mailing list, "Simple financial functions for NumPy",
   https://mail.python.org/pipermail/numpy-discussion/2008-April/032353.html

.. [3] NumPy-Discussion mailing list, "add xirr to numpy financial functions?",
   https://mail.python.org/pipermail/numpy-discussion/2009-May/042645.html

.. [4] NumPy-Discussion mailing list, "Definitions of pv, fv, nper, pmt, and rate",
   https://mail.python.org/pipermail/numpy-discussion/2009-June/043188.html

.. [5] Get financial functions out of main namespace,
   https://github.com/numpy/numpy/issues/2880

.. [6] NumPy-Discussion mailing list, "Deprecation of financial routines",
   https://mail.python.org/pipermail/numpy-discussion/2013-August/067409.html

.. [7] ``component: numpy.lib.financial`` issues,
   https://github.com/numpy/numpy/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3A%22component%3A+numpy.lib.financial%22+

.. [8] ``component: numpy.lib.financial`` pull requests,
   https://github.com/numpy/numpy/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3A%22component%3A+numpy.lib.financial%22+

.. [9] Quansight-Labs/python-api-inspect,
   https://github.com/Quansight-Labs/python-api-inspect/

.. [10] NumPy-Discussion mailing list, "NEP 32: Remove the financial functions
   from NumPy"
   https://mail.python.org/pipermail/numpy-discussion/2019-September/079965.html

.. [11] PyData mailing list (pydata@googlegroups.com), "NumPy proposal to
   remove the financial functions.
   https://mail.google.com/mail/u/0/h/1w0mjgixc4rpe/?&th=16d5c38be45f77c4&q=nep+32&v=c&s=q

.. [12] NumPy-Discussion mailing list, "Proposal to accept NEP 32: Remove the
   financial functions from NumPy"
   https://mail.python.org/pipermail/numpy-discussion/2019-September/080074.html

Copyright
---------

This document has been placed in the public domain.
.. _NEP14:

=============================================
NEP 14 — Plan for dropping Python 2.7 support
=============================================

:Status: Final
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2017-November/077419.html

The Python core team plans to stop supporting Python 2 in 2020. The NumPy
project has supported both Python 2 and Python 3 in parallel since 2010, and
has found that supporting Python 2 is an increasing burden on our limited
resources; thus, we plan to eventually drop Python 2 support as well. Now that
we're entering the final years of community-supported Python 2, the NumPy
project wants to clarify our plans, with the goal of to helping our downstream
ecosystem make plans and accomplish the transition with as little disruption as
possible.

Our current plan is as follows.

Until **December 31, 2018**, all NumPy releases will fully support both
Python2 and Python3.

Starting on **January 1, 2019**, any new feature releases will support only
Python3.

The last Python2 supporting release will be designated as a long term support
(LTS) release, meaning that we will continue to merge bug fixes and make bug
fix releases for a longer period than usual.  Specifically, it will be
supported by the community until **December 31, 2019**.

On **January 1, 2020** we will raise a toast to Python2, and community support
for the last Python2 supporting release will come to an end. However, it will
continue to be available on PyPI indefinitely, and if any commercial vendors
wish to extend the LTS support past this point then we are open to letting them
use the LTS branch in the official NumPy repository to coordinate that.

If you are a NumPy user who requires ongoing Python2 support in 2020 or later,
then please contact your vendor. If you are a vendor who wishes to continue to
support NumPy on Python2 in 2020+, please get in touch; ideally we'd like you
to get involved in maintaining the LTS before it actually hits end of life so
that we can make a clean handoff.

To minimize disruption, running ``pip install numpy`` on Python 2 will continue
to give the last working release in perpetuity, but after January 1, 2019 it
may not contain the latest features, and after January 1, 2020 it may not
contain the latest bug fixes.

For more information on the scientific Python ecosystem's transition
to Python3 only, see the python3-statement_.

For more information on porting your code to run on Python 3, see the
python3-howto_.

.. _python3-statement: https://python3statement.org/

.. _python3-howto: https://docs.python.org/3/howto/pyporting.html
.. _NEP20:

===============================================================
NEP 20 — Expansion of generalized universal function signatures
===============================================================

:Author: Marten van Kerkwijk <mhvk@astro.utoronto.ca>
:Status: Final
:Type: Standards Track
:Created: 2018-06-10
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-April/077959.html,
             https://mail.python.org/pipermail/numpy-discussion/2018-May/078078.html

.. note:: The proposal to add fixed (i) and flexible (ii) dimensions
          was accepted, while that to add broadcastable (iii) ones was deferred.

Abstract
--------

Generalized universal functions are, as their name indicates, generalization
of universal functions: they operate on non-scalar elements.  Their signature
describes the structure of the elements they operate on, with names linking
dimensions of the operands that should be the same.  Here, it is proposed to
extend the signature to allow the signature to indicate that a dimension (i)
has fixed size; (ii) can be absent; and (iii) can be broadcast.

Detailed description
--------------------

Each part of the proposal is driven by specific needs [1]_.

1. Fixed-size dimensions.  Code working with spatial vectors often explicitly
   is for 2 or 3-dimensional space (e.g., the code from the `Standards Of
   Fundamental Astronomy <http://www.iausofa.org/>`_, which the author hopes
   to wrap using gufuncs for astropy [2]_).  The signature should be able to
   indicate that.  E.g., the signature of a function that converts a polar
   angle to a two-dimensional cartesian unit vector would currently have to be
   ``()->(n)``, with there being no way to indicate that ``n`` has to equal 2.
   Indeed, this signature is particularly annoying since without putting in an
   output argument, the current gufunc wrapper code fails because it cannot
   determine ``n``.  Similarly, the signature for an cross product of two
   3-dimensional vectors has to be ``(n),(n)->(n)``, with again no way to
   indicate that ``n`` has to equal 3.  Hence, the proposal here to allow one
   to give numerical values in addition to variable names.  Thus, angle to
   two-dimensional unit vector would be ``()->(2)``; two angles to
   three-dimensional unit vector ``(),()->(3)``; and that for the cross
   product of two three-dimensional vectors would be ``(3),(3)->(3)``.

2. Possibly missing dimensions.  This part is almost entirely driven by the
   wish to wrap ``matmul`` in a gufunc. ``matmul`` stands for matrix
   multiplication, and if it did only that, it could be covered with the
   signature ``(m,n),(n,p)->(m,p)``. However, it has special cases for when a
   dimension is missing, allowing either argument to be treated as a single
   vector, with the function thus becoming, effectively, vector-matrix,
   matrix-vector, or vector-vector multiplication (but with no
   broadcasting). To support this, it is suggested to allow postfixing a
   dimension name with a question mark to indicate that the dimension does not
   necessarily have to be present.

   With this addition, the signature for ``matmul`` can be expressed as
   ``(m?,n),(n,p?)->(m?,p?)``.  This indicates that if, e.g., the second
   operand has only one dimension, for the purposes of the elementary function
   it will be treated as if that input has core shape ``(n, 1)``, and the
   output has the corresponding core shape of ``(m, 1)``. The actual output
   array, however, has the flexible dimension removed, i.e., it will have
   shape ``(..., m)``.  Similarly, if both arguments have only a single
   dimension, the inputs will be presented as having shapes ``(1, n)`` and
   ``(n, 1)`` to the elementary function, and the output as ``(1, 1)``, while
   the actual output array returned will have shape ``()``. In this way, the
   signature allows one to use a single elementary function for four related
   but different signatures, ``(m,n),(n,p)->(m,p)``, ``(n),(n,p)->(p)``,
   ``(m,n),(n)->(m)`` and ``(n),(n)->()``.

3. Dimensions that can be broadcast. For some applications, broadcasting
   between operands makes sense. For instance, an ``all_equal`` function that
   compares vectors in arrays could have a signature ``(n),(n)->()``, but this
   forces both operands to be arrays, while it would be useful also to check
   that, e.g., all parts of a vector are constant (maybe zero). The proposal
   is to allow the implementer of a gufunc to indicate that a dimension can be
   broadcast by post-fixing the dimension name with ``|1``. Hence, the
   signature for ``all_equal`` would become ``(n|1),(n|1)->()``.  The
   signature seems handy more generally for "chained ufuncs"; e.g., another
   application might be in a putative ufunc implementing ``sumproduct``.

   Another example that arose in the discussion, is of a weighted mean, which
   might look like ``weighted_mean(y, sigma[, axis, ...])``, returning the
   mean and its uncertainty.  With a signature of ``(n),(n)->(),()``, one
   would be forced to always give as many sigmas as there are data points,
   while broadcasting would allow one to give a single sigma for all points
   (which is still useful to calculate the uncertainty on the mean).

Implementation
--------------

The proposed changes have all been implemented [3]_, [4]_, [5]_. These PRs
extend the ufunc structure with two new fields, each of size equal to the
number of distinct dimensions, with ``core_dim_sizes`` holding possibly fixed
sizes, and ``core_dim_flags`` holding flags indicating whether a dimension can
be missing or broadcast.  To ensure we can distinguish between this new
version and previous versions, an unused entry ``reserved1`` is repurposed as
a version number.

In the implementation, care is taken that to the elementary function flagged
dimensions are not treated any differently than non-flagged ones: for
instance, sizes of fixed-size dimensions are still passed on to the elementary
function (but the loop can now count on that size being equal to the fixed one
given in the signature).

An implementation detail to be decided upon is whether it might be handy to
have a summary of all flags. This could possibly be stored in ``core_enabled``
(which currently is a bool), with non-zero continuing to indicate a gufunc,
but specific flags indicating whether or not a gufunc uses fixed, flexible, or
broadcastable dimensions.

With the above, the formal definition of the syntax would become [4]_::

  <Signature>            ::= <Input arguments> "->" <Output arguments>
  <Input arguments>      ::= <Argument list>
  <Output arguments>     ::= <Argument list>
  <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
  <Argument>             ::= "(" <Core dimension list> ")"
  <Core dimension list>  ::= nil | <Core dimension> |
                             <Core dimension> "," <Core dimension list>
  <Core dimension>       ::= <Dimension name> <Dimension modifier>
  <Dimension name>       ::= valid Python variable name | valid integer
  <Dimension modifier>   ::= nil | "|1" | "?"

#. All quotes are for clarity.
#. Unmodified core dimensions that share the same name must have the same size.
   Each dimension name typically corresponds to one level of looping in the
   elementary function's implementation.
#. White spaces are ignored.
#. An integer as a dimension name freezes that dimension to the value.
#. If a name if suffixed with the ``|1`` modifier, it is allowed to broadcast
   against other dimensions with the same name.  All input dimensions
   must share this modifier, while no output dimensions should have it.
#. If the name is suffixed with the ``?`` modifier, the dimension is a core
   dimension only if it exists on all inputs and outputs that share it;
   otherwise it is ignored (and replaced by a dimension of size 1 for the
   elementary function).

Examples of signatures [4]_:

+----------------------------+-----------------------------------+
| Signature                  | Possible use                      |
+----------------------------+-----------------------------------+
| ``(),()->()``              | Addition                          |
+----------------------------+-----------------------------------+
| ``(i)->()``                | Sum over last axis                |
+----------------------------+-----------------------------------+
| ``(i|1),(i|1)->()``        | Test for equality along axis,     |
|                            | allowing comparison with a scalar |
+----------------------------+-----------------------------------+
| ``(i),(i)->()``            | inner vector product              |
+----------------------------+-----------------------------------+
| ``(m,n),(n,p)->(m,p)``     | matrix multiplication             |
+----------------------------+-----------------------------------+
| ``(n),(n,p)->(p)``         | vector-matrix multiplication      |
+----------------------------+-----------------------------------+
| ``(m,n),(n)->(m)``         | matrix-vector multiplication      |
+----------------------------+-----------------------------------+
| ``(m?,n),(n,p?)->(m?,p?)`` | all four of the above at once,    |
|                            | except vectors cannot have loop   |
|                            | dimensions (ie, like ``matmul``)  |
+----------------------------+-----------------------------------+
| ``(3),(3)->(3)``           | cross product for 3-vectors       |
+----------------------------+-----------------------------------+
| ``(i,t),(j,t)->(i,j)``     | inner over the last dimension,    |
|                            | outer over the second to last,    |
|                            | and loop/broadcast over the rest. |
+----------------------------+-----------------------------------+

Backward compatibility
----------------------

One possible worry is the change in ufunc structure.  For most applications,
which call ``PyUFunc_FromDataAndSignature``, this is entirely transparent.
Furthermore, by repurposing ``reserved1`` as a version number, code compiled
against older versions of numpy will continue to work (though one will get a
warning upon import of that code with a newer version of numpy), except if
code explicitly changes the ``reserved1`` entry.

Alternatives
------------

It was suggested instead of extending the signature, to have multiple
dispatch, so that, e.g., ``matmul`` would simply have the multiple signatures
it supports, i.e., instead of ``(m?,n),(n,p?)->(m?,p?)`` one would have
``(m,n),(n,p)->(m,p) | (n),(n,p)->(p) | (m,n),(n)->(m) | (n),(n)->()``.  A
disadvantage of this is that the developer now has to make sure that the
elementary function can deal with these different signatures.  Furthermore,
the expansion quickly becomes cumbersome.  For instance, for the ``all_equal``
signature of ``(n|1),(n|1)->()``, one would have to have five entries:
``(n),(n)->() | (n),(1)->() | (1),(n)->() | (n),()->() | (),(n)->()``.  For
signatures like ``(m|1,n|1,o|1),(m|1,n|1,o|1)->()`` (from the ``cube_equal``
test case in [4]_), it is not even worth writing out the expansion.

For broadcasting, the alternative suffix of ``^`` was suggested (as
broadcasting can be thought of as increasing the size of the array).  This
seems less clear.  Furthermore, it was wondered whether it should not just be
an all-or-nothing flag.  This could be the case, though given the postfix
for flexible dimensions, arguably another postfix is clearer (as is the
implementation).

Discussion
----------

The proposals here were discussed at fair length on the mailing list [6]_,
[7]_.  The main points of contention were whether the use cases were
sufficiently strong. In particular, for frozen dimensions, it was argued that
checks on the right number could be put in loop selection code.  This seems
much less clear for no benefit.

For broadcasting, the lack of examples of elementary functions that might need
it was noted, with it being questioned whether something like ``all_equal``
was best done with a gufunc rather than as a special method on ``np.equal``.
One counter-argument to this would be that there is an actual PR for
``all_equal`` [8]_.  Another that even if one were to use a method, it would
be good to be able to express their signature (just as is possible at least
for ``reduce`` and ``accumulate``).

A final argument was that we were making the gufuncs too complex. This
arguably holds for the dimensions that can be omitted, but that also has the
strongest use case. The frozen dimensions has a very simple implementation and
its meaning is obvious. The ability to broadcast is simple too, once the
flexible dimensions are supported.

References and Footnotes
------------------------

.. [1] Identified needs and suggestions for the implementation are not all by
       the author. In particular, the suggestion for fixed dimensions and
       initial implementation was by Jaime Frio (`gh-5015
       <https://github.com/numpy/numpy/pull/5015>`_), the suggestion of ``?``
       to indicate dimensions can be omitted was by Nathaniel Smith, and the
       initial implementation of that by Matti Picus (`gh-11132
       <https://github.com/numpy/numpy/pull/11132>`_).
.. [2] `wrap ERFA functions in gufuncs
       <https://github.com/astropy/astropy/pull/7502>`_ (`ERFA
       <https://github.com/liberfa/erfa>`_) is the less stringently licensed
       version of `Standards Of Fundamental Astronomy
       <http://www.iausofa.org/>`_
.. [3] `fixed-size and flexible dimensions
       <https://github.com/numpy/numpy/pull/11175>`_
.. [4] `broadcastable dimensions
       <https://github.com/numpy/numpy/pull/11179>`_
.. [5] `use in matmul <https://github.com/numpy/numpy/pull/11133>`_
.. [6] Discusses implementations for ``matmul``:
       https://mail.python.org/pipermail/numpy-discussion/2018-May/077972.html,
       https://mail.python.org/pipermail/numpy-discussion/2018-May/078021.html
.. [7] Broadcasting:
       https://mail.python.org/pipermail/numpy-discussion/2018-May/078078.html
.. [8] `Logical gufuncs <https://github.com/numpy/numpy/pull/8528>`_ (includes
       ``all_equal``)

Copyright
---------

This document has been placed in the public domain.
.. _NEP04:

=========================================================================
NEP 4 — A (third) proposal for implementing some date/time types in NumPy
=========================================================================

:Author: Francesc Alted i Abad
:Contact: faltet@pytables.com
:Author: Ivan Vilata i Balaguer
:Contact: ivan@selidor.net
:Date: 2008-07-30
:Status: Deferred

Executive summary
=================

A date/time mark is something very handy to have in many fields where
one has to deal with data sets.  While Python has several modules that
define a date/time type (like the integrated ``datetime`` [1]_ or
``mx.DateTime`` [2]_), NumPy has a lack of them.

In this document, we are proposing the addition of a series of date/time
types to fill this gap.  The requirements for the proposed types are
two-folded: 1) they have to be fast to operate with and 2) they have to
be as compatible as possible with the existing ``datetime`` module that
comes with Python.


Types proposed
==============

To start with, it is virtually impossible to come up with a single
date/time type that fills the needs of every case of use.  So, after
pondering about different possibilities, we have stuck with *two*
different types, namely ``datetime64`` and ``timedelta64`` (these names
are preliminary and can be changed), that can have different time units
so as to cover different needs.

.. Important:: the time unit is conceived here as metadata that
  *complements* a date/time dtype, *without changing the base type*.  It
  provides information about the *meaning* of the stored numbers, not
  about their *structure*.

Now follows a detailed description of the proposed types.


``datetime64``
--------------

It represents a time that is absolute (i.e. not relative).  It is
implemented internally as an ``int64`` type.  The internal epoch is the
POSIX epoch (see [3]_).  Like POSIX, the representation of a date
doesn't take leap seconds into account.

In time unit *conversions* and time *representations* (but not in other
time computations), the value -2**63 (0x8000000000000000) is interpreted
as an invalid or unknown date, *Not a Time* or *NaT*.  See the section
on time unit conversions for more information.

Time units
~~~~~~~~~~

It accepts different time units, each of them implying a different time
span.  The table below describes the time units supported with their
corresponding time spans.

======== ================ ==========================
      Time unit               Time span (years)
------------------------- --------------------------
  Code       Meaning
======== ================ ==========================
   Y       year             [9.2e18 BC, 9.2e18 AD]
   M       month            [7.6e17 BC, 7.6e17 AD]
   W       week             [1.7e17 BC, 1.7e17 AD]
   B       business day     [3.5e16 BC, 3.5e16 AD]
   D       day              [2.5e16 BC, 2.5e16 AD]
   h       hour             [1.0e15 BC, 1.0e15 AD]
   m       minute           [1.7e13 BC, 1.7e13 AD]
   s       second           [ 2.9e9 BC,  2.9e9 AD]
   ms      millisecond      [ 2.9e6 BC,  2.9e6 AD]
   us      microsecond      [290301 BC, 294241 AD]
   c#      ticks (100ns)    [  2757 BC,  31197 AD]
   ns      nanosecond       [  1678 AD,   2262 AD]
======== ================ ==========================

The value of an absolute date is thus *an integer number of units of the
chosen time unit* passed since the internal epoch.  When working with
business days, Saturdays and Sundays are simply ignored from the count
(i.e. day 3 in business days is not Saturday 1970-01-03, but Monday
1970-01-05).

Building a ``datetime64`` dtype
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation::

  dtype('datetime64[us]')

Using the short string notation::

  dtype('M8[us]')

The default is microseconds if no time unit is specified.  Thus, 'M8' is equivalent to 'M8[us]'


Setting and getting values
~~~~~~~~~~~~~~~~~~~~~~~~~~

The objects with this dtype can be set in a series of ways::

  t = numpy.ones(3, dtype='M8[s]')
  t[0] = 1199164176    # assign to July 30th, 2008 at 17:31:00
  t[1] = datetime.datetime(2008, 7, 30, 17, 31, 01) # with datetime module
  t[2] = '2008-07-30T17:31:02'    # with ISO 8601

And can be get in different ways too::

  str(t[0])  -->  2008-07-30T17:31:00
  repr(t[1]) -->  datetime64(1199164177, 's')
  str(t[0].item()) --> 2008-07-30 17:31:00  # datetime module object
  repr(t[0].item()) --> datetime.datetime(2008, 7, 30, 17, 31)  # idem
  str(t)  -->  [2008-07-30T17:31:00  2008-07-30T17:31:01  2008-07-30T17:31:02]
  repr(t)  -->  array([1199164176, 1199164177, 1199164178],
                      dtype='datetime64[s]')

Comparisons
~~~~~~~~~~~

The comparisons will be supported too::

  numpy.array(['1980'], 'M8[Y]') == numpy.array(['1979'], 'M8[Y]')
  --> [False]

or by applying broadcasting::

  numpy.array(['1979', '1980'], 'M8[Y]') == numpy.datetime64('1980', 'Y')
  --> [False, True]

The next should work too::

  numpy.array(['1979', '1980'], 'M8[Y]') == '1980-01-01'
  --> [False, True]

because the right hand expression can be broadcasted into an array of 2
elements of dtype 'M8[Y]'.

Compatibility issues
~~~~~~~~~~~~~~~~~~~~

This will be fully compatible with the ``datetime`` class of the
``datetime`` module of Python only when using a time unit of
microseconds.  For other time units, the conversion process will lose
precision or will overflow as needed.  The conversion from/to a
``datetime`` object doesn't take leap seconds into account.


``timedelta64``
---------------

It represents a time that is relative (i.e. not absolute).  It is
implemented internally as an ``int64`` type.

In time unit *conversions* and time *representations* (but not in other
time computations), the value -2**63 (0x8000000000000000) is interpreted
as an invalid or unknown time, *Not a Time* or *NaT*.  See the section
on time unit conversions for more information.

Time units
~~~~~~~~~~

It accepts different time units, each of them implying a different time
span.  The table below describes the time units supported with their
corresponding time spans.

======== ================ ==========================
      Time unit               Time span
------------------------- --------------------------
  Code       Meaning
======== ================ ==========================
   Y       year             +- 9.2e18 years
   M       month            +- 7.6e17 years
   W       week             +- 1.7e17 years
   B       business day     +- 3.5e16 years
   D       day              +- 2.5e16 years
   h       hour             +- 1.0e15 years
   m       minute           +- 1.7e13 years
   s       second           +- 2.9e12 years
   ms      millisecond      +- 2.9e9 years
   us      microsecond      +- 2.9e6 years
   c#      ticks (100ns)    +- 2.9e4 years
   ns      nanosecond       +- 292 years
   ps      picosecond       +- 106 days
   fs      femtosecond      +- 2.6 hours
   as      attosecond       +- 9.2 seconds
======== ================ ==========================

The value of a time delta is thus *an integer number of units of the
chosen time unit*.

Building a ``timedelta64`` dtype
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation::

  dtype('timedelta64[us]')

Using the short string notation::

  dtype('m8[us]')

The default is micro-seconds if no default is specified:  'm8' is equivalent to 'm8[us]'


Setting and getting values
~~~~~~~~~~~~~~~~~~~~~~~~~~

The objects with this dtype can be set in a series of ways::

  t = numpy.ones(3, dtype='m8[ms]')
  t[0] = 12    # assign to 12 ms
  t[1] = datetime.timedelta(0, 0, 13000)   # 13 ms
  t[2] = '0:00:00.014'    # 14 ms

And can be get in different ways too::

  str(t[0])  -->  0:00:00.012
  repr(t[1]) -->  timedelta64(13, 'ms')
  str(t[0].item()) --> 0:00:00.012000   # datetime module object
  repr(t[0].item()) --> datetime.timedelta(0, 0, 12000)  # idem
  str(t)     -->  [0:00:00.012  0:00:00.014  0:00:00.014]
  repr(t)    -->  array([12, 13, 14], dtype="timedelta64[ms]")

Comparisons
~~~~~~~~~~~

The comparisons will be supported too::

  numpy.array([12, 13, 14], 'm8[ms]') == numpy.array([12, 13, 13], 'm8[ms]')
  --> [True, True, False]

or by applying broadcasting::

  numpy.array([12, 13, 14], 'm8[ms]') == numpy.timedelta64(13, 'ms')
  --> [False, True, False]

The next should work too::

  numpy.array([12, 13, 14], 'm8[ms]') == '0:00:00.012'
  --> [True, False, False]

because the right hand expression can be broadcasted into an array of 3
elements of dtype 'm8[ms]'.

Compatibility issues
~~~~~~~~~~~~~~~~~~~~

This will be fully compatible with the ``timedelta`` class of the
``datetime`` module of Python only when using a time unit of
microseconds.  For other units, the conversion process will lose
precision or will overflow as needed.


Examples of use
===============

Here it is an example of use for the ``datetime64``::

  In [5]: numpy.datetime64(42, 'us')
  Out[5]: datetime64(42, 'us')

  In [6]: print numpy.datetime64(42, 'us')
  1970-01-01T00:00:00.000042  # representation in ISO 8601 format

  In [7]: print numpy.datetime64(367.7, 'D')  # decimal part is lost
  1971-01-02  # still ISO 8601 format

  In [8]: numpy.datetime('2008-07-18T12:23:18', 'm')  # from ISO 8601
  Out[8]: datetime64(20273063, 'm')

  In [9]: print numpy.datetime('2008-07-18T12:23:18', 'm')
  Out[9]: 2008-07-18T12:23

  In [10]: t = numpy.zeros(5, dtype="datetime64[ms]")

  In [11]: t[0] = datetime.datetime.now()  # setter in action

  In [12]: print t
  [2008-07-16T13:39:25.315  1970-01-01T00:00:00.000
   1970-01-01T00:00:00.000  1970-01-01T00:00:00.000
   1970-01-01T00:00:00.000]

  In [13]: repr(t)
  Out[13]: array([267859210457, 0, 0, 0, 0], dtype="datetime64[ms]")

  In [14]: t[0].item()     # getter in action
  Out[14]: datetime.datetime(2008, 7, 16, 13, 39, 25, 315000)

  In [15]: print t.dtype
  dtype('datetime64[ms]')

And here it goes an example of use for the ``timedelta64``::

  In [5]: numpy.timedelta64(10, 'us')
  Out[5]: timedelta64(10, 'us')

  In [6]: print numpy.timedelta64(10, 'us')
  0:00:00.000010

  In [7]: print numpy.timedelta64(3600.2, 'm')  # decimal part is lost
  2 days, 12:00

  In [8]: t1 = numpy.zeros(5, dtype="datetime64[ms]")

  In [9]: t2 = numpy.ones(5, dtype="datetime64[ms]")

  In [10]: t = t2 - t1

  In [11]: t[0] = datetime.timedelta(0, 24)  # setter in action

  In [12]: print t
  [0:00:24.000  0:00:01.000  0:00:01.000  0:00:01.000  0:00:01.000]

  In [13]: print repr(t)
  Out[13]: array([24000, 1, 1, 1, 1], dtype="timedelta64[ms]")

  In [14]: t[0].item()     # getter in action
  Out[14]: datetime.timedelta(0, 24)

  In [15]: print t.dtype
  dtype('timedelta64[s]')


Operating with date/time arrays
===============================

``datetime64`` vs ``datetime64``
--------------------------------

The only arithmetic operation allowed between absolute dates is the
subtraction::

  In [10]: numpy.ones(3, "M8[s]") - numpy.zeros(3, "M8[s]")
  Out[10]: array([1, 1, 1], dtype=timedelta64[s])

But not other operations::

  In [11]: numpy.ones(3, "M8[s]") + numpy.zeros(3, "M8[s]")
  TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray'

Comparisons between absolute dates are allowed.

Casting rules
~~~~~~~~~~~~~

When operating (basically, only the subtraction will be allowed) two
absolute times with different unit times, the outcome would be to raise
an exception.  This is because the ranges and time-spans of the different
time units can be very different, and it is not clear at all what time
unit will be preferred for the user.  For example, this should be
allowed::

  >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[Y]")
  array([1, 1, 1], dtype="timedelta64[Y]")

But the next should not::

  >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[ns]")
  raise numpy.IncompatibleUnitError  # what unit to choose?


``datetime64`` vs ``timedelta64``
---------------------------------

It will be possible to add and subtract relative times from absolute
dates::

  In [10]: numpy.zeros(5, "M8[Y]") + numpy.ones(5, "m8[Y]")
  Out[10]: array([1971, 1971, 1971, 1971, 1971], dtype=datetime64[Y])

  In [11]: numpy.ones(5, "M8[Y]") - 2 * numpy.ones(5, "m8[Y]")
  Out[11]: array([1969, 1969, 1969, 1969, 1969], dtype=datetime64[Y])

But not other operations::

  In [12]: numpy.ones(5, "M8[Y]") * numpy.ones(5, "m8[Y]")
  TypeError: unsupported operand type(s) for *: 'numpy.ndarray' and 'numpy.ndarray'

Casting rules
~~~~~~~~~~~~~

In this case the absolute time should have priority for determining the
time unit of the outcome.  That would represent what the people wants to
do most of the times.  For example, this would allow to do::

  >>> series = numpy.array(['1970-01-01', '1970-02-01', '1970-09-01'],
  dtype='datetime64[D]')
  >>> series2 = series + numpy.timedelta(1, 'Y')  # Add 2 relative years
  >>> series2
  array(['1972-01-01', '1972-02-01', '1972-09-01'],
  dtype='datetime64[D]')  # the 'D'ay time unit has been chosen


``timedelta64`` vs ``timedelta64``
----------------------------------

Finally, it will be possible to operate with relative times as if they
were regular int64 dtypes *as long as* the result can be converted back
into a ``timedelta64``::

  In [10]: numpy.ones(3, 'm8[us]')
  Out[10]: array([1, 1, 1], dtype="timedelta64[us]")

  In [11]: (numpy.ones(3, 'm8[M]') + 2) ** 3
  Out[11]: array([27, 27, 27], dtype="timedelta64[M]")

But::

  In [12]: numpy.ones(5, 'm8') + 1j
  TypeError: the result cannot be converted into a ``timedelta64``

Casting rules
~~~~~~~~~~~~~

When combining two ``timedelta64`` dtypes with different time units the
outcome will be the shorter of both ("keep the precision" rule).  For
example::

  In [10]: numpy.ones(3, 'm8[s]') + numpy.ones(3, 'm8[m]')
  Out[10]: array([61, 61, 61],  dtype="timedelta64[s]")

However, due to the impossibility to know the exact duration of a
relative year or a relative month, when these time units appear in one
of the operands, the operation will not be allowed::

  In [11]: numpy.ones(3, 'm8[Y]') + numpy.ones(3, 'm8[D]')
  raise numpy.IncompatibleUnitError  # how to convert relative years to days?

In order to being able to perform the above operation a new NumPy
function, called ``change_timeunit`` is proposed.  Its signature will
be::

  change_timeunit(time_object, new_unit, reference)

where 'time_object' is the time object whose unit is to be changed,
'new_unit' is the desired new time unit, and 'reference' is an absolute
date (NumPy datetime64 scalar) that will be used to allow the conversion
of relative times in case of using time units with an uncertain number
of smaller time units (relative years or months cannot be expressed in
days).

With this, the above operation can be done as follows::

  In [10]: t_years = numpy.ones(3, 'm8[Y]')

  In [11]: t_days = numpy.change_timeunit(t_years, 'D', '2001-01-01')

  In [12]: t_days + numpy.ones(3, 'm8[D]')
  Out[12]: array([366, 366, 366],  dtype="timedelta64[D]")


dtype vs time units conversions
===============================

For changing the date/time dtype of an existing array, we propose to use
the ``.astype()`` method.  This will be mainly useful for changing time
units.

For example, for absolute dates::

  In[10]: t1 = numpy.zeros(5, dtype="datetime64[s]")

  In[11]: print t1
  [1970-01-01T00:00:00  1970-01-01T00:00:00  1970-01-01T00:00:00
   1970-01-01T00:00:00  1970-01-01T00:00:00]

  In[12]: print t1.astype('datetime64[D]')
  [1970-01-01  1970-01-01  1970-01-01  1970-01-01  1970-01-01]

For relative times::

  In[10]: t1 = numpy.ones(5, dtype="timedelta64[s]")

  In[11]: print t1
  [1 1 1 1 1]

  In[12]: print t1.astype('timedelta64[ms]')
  [1000 1000 1000 1000 1000]

Changing directly from/to relative to/from absolute dtypes will not be
supported::

  In[13]: numpy.zeros(5, dtype="datetime64[s]").astype('timedelta64')
  TypeError: data type cannot be converted to the desired type

Business days have the peculiarity that they do not cover a continuous
line of time (they have gaps at weekends).  Thus, when converting from
any ordinary time to business days, it can happen that the original time
is not representable.  In that case, the result of the conversion is
*Not a Time* (*NaT*)::

  In[10]: t1 = numpy.arange(5, dtype="datetime64[D]")

  In[11]: print t1
  [1970-01-01  1970-01-02  1970-01-03  1970-01-04  1970-01-05]

  In[12]: t2 = t1.astype("datetime64[B]")

  In[13]: print t2  # 1970 begins in a Thursday
  [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

When converting back to ordinary days, NaT values are left untouched
(this happens in all time unit conversions)::

  In[14]: t3 = t2.astype("datetime64[D]")

  In[13]: print t3
  [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]


Final considerations
====================

Why the ``origin`` metadata disappeared
---------------------------------------

During the discussion of the date/time dtypes in the NumPy list, the
idea of having an ``origin`` metadata that complemented the definition
of the absolute ``datetime64`` was initially found to be useful.

However, after thinking more about this, we found that the combination
of an absolute ``datetime64`` with a relative ``timedelta64`` does offer
the same functionality while removing the need for the additional
``origin`` metadata.  This is why we have removed it from this proposal.

Operations with mixed time units
--------------------------------

Whenever an operation between two time values of the same dtype with the
same unit is accepted, the same operation with time values of different
units should be possible (e.g. adding a time delta in seconds and one in
microseconds), resulting in an adequate time unit.  The exact semantics
of this kind of operations is defined int the "Casting rules"
subsections of the "Operating with date/time arrays" section.

Due to the peculiarities of business days, it is most probable that
operations mixing business days with other time units will not be
allowed.

Why there is not a ``quarter`` time unit?
-----------------------------------------

This proposal tries to focus on the most common used set of time units
to operate with, and the ``quarter`` can be considered more of a derived
unit.  Besides, the use of a ``quarter`` normally requires that it can
start at whatever month of the year, and as we are not including support
for a time ``origin`` metadata, this is not a viable venue here.
Finally, if we were to add the ``quarter`` then people should expect to
find a ``biweekly``, ``semester`` or ``biyearly`` just to put some
examples of other derived units, and we find this a bit too overwhelming
for this proposal purposes.


.. [1] https://docs.python.org/library/datetime.html
.. [2] https://www.egenix.com/products/python/mxBase/mxDateTime
.. [3] https://en.wikipedia.org/wiki/Unix_time


.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 72
.. End:
.. _NEP03:

=====================================================
NEP 3 — Cleaning the math configuration of numpy.core
=====================================================

:Author: David Cournapeau
:Contact: david@ar.media.kyoto-u.ac.jp
:Date: 2008-09-04
:Status: Deferred

Executive summary
=================

Before building numpy.core, we use some configuration tests to gather some
information about available math functions. Over the years, the configuration
became convoluted, to the point it became difficult to support new platforms
easily.

The goal of this proposal is to clean the configuration of the math
capabilities for easier maintenance.

Current problems
================

Currently, the math configuration mainly test for some math functions, and
configure numpy accordingly. But instead of testing each desired function
independently, the current system has been developed more as workarounds
particular platform oddities, using platform implicit knowledge. This is
against the normal philosophy of testing for capabilities only, which is the
autoconf philosophy, which showed the path toward portability (on Unix at
least) [1] This causes problems because modifying or adding configuration on
existing platforms break the implicit assumption, without a clear solution.

For example, on windows, when numpy is built with mingw, it would be nice to
enforce the configuration sizeof(long double) == sizeof(double) because mingw
uses the MS runtime, and the MS runtime does not support long double.
Unfortunately, doing so breaks the mingw math function detection, because of
the implicit assumption that mingw has a configuration sizeof(long double) !=
sizeof(double).

Another example is the testing for set of functions using only one function: if
expf is found, it is assumed that all basic float functions are available.
Instead, each function should be tested independently (expf, sinf, etc...).

Requirements
============

We have two strong requirements:
	- it should not break any currently supported platform
	- it should not make the configuration much slower (1-2 seconds are
	  acceptable)

Proposal
========

We suggest to break any implicit assumption, and test each math function
independently from each other, as usually done by autoconf. Since testing for a
vast set of functions can be time consuming, we will use a scheme similar to
AC_CHECK_FUNCS_ONCE in autoconf, that is test for a set of function at once,
and only in the case it breaks, do the per function check. When the first check
works, it should be as fast as the current scheme, except that the assumptions
are explicitly checked (all functions implied by HAVE_LONGDOUBLE_FUNCS would
be checked together, for example).

Issues
======

Static vs non static ? For basic functions, shall we define them static or not ?

License
=======

This document has been placed in the public domain.

[1]: Autobook here
.. _NEP43:

==============================================================================
NEP 43 — Enhancing the extensibility of UFuncs
==============================================================================

:title: Enhancing the Extensibility of UFuncs
:Author: Sebastian Berg
:Status: Draft
:Type: Standard
:Created: 2020-06-20


.. note::

    This NEP is fourth in a series:

    - :ref:`NEP 40 <NEP40>` explains the shortcomings of NumPy's dtype implementation.

    - :ref:`NEP 41 <NEP41>` gives an overview of our proposed replacement.

    - :ref:`NEP 42 <NEP42>`  describes the new design's datatype-related APIs.

    - NEP 43 (this document) describes the new design's API for universal functions.


******************************************************************************
Abstract
******************************************************************************

The previous NEP 42 proposes the creation of new DTypes which can
be defined by users outside of NumPy itself.
The implementation of NEP 42 will enable users to create arrays with a custom dtype
and stored values.
This NEP outlines how NumPy will operate on arrays with custom dtypes in the future.
The most important functions operating on NumPy arrays are the so called
"universal functions" (ufunc) which include all math functions, such as
``np.add``, ``np.multiply``, and even ``np.matmul``.
These ufuncs must operate efficiently on multiple arrays with
different datatypes.

This NEP proposes to expand the design of ufuncs.
It makes a new distinction between the ufunc which can operate
on many different dtypes such as floats or integers,
and a new ``ArrayMethod`` which defines the efficient operation for
specific dtypes.

.. note::

    Details of the private and external APIs may change to reflect user
    comments and implementation constraints. The underlying principles and
    choices should not change significantly.


******************************************************************************
Motivation and scope
******************************************************************************

The goal of this NEP is to extend universal
functions support the new DType system detailed in NEPs 41 and 42.
While the main motivation is enabling new user-defined DTypes, this will
also significantly simplify defining universal functions for NumPy strings or
structured DTypes.
Until now, these DTypes are not supported by any of NumPy's functions
(such as ``np.add`` or ``np.equal``), due to difficulties arising from
their parametric nature (compare NEP 41 and 42), e.g. the string length.

Functions on arrays must handle a number of distinct steps which are
described in more detail in section "`Steps involved in a UFunc call`_".
The most important ones are:

- Organizing all functionality required to define a ufunc call for specific
  DTypes.  This is often called the "inner-loop".
- Deal with input for which no exact matching implementation is found.
  For example when ``int32`` and ``float64`` are added, the ``int32``
  is cast to ``float64``.  This requires a distinct "promotion" step.

After organizing and defining these, we need to:

- Define the user API for customizing both of the above points.
- Allow convenient reuse of existing functionality.
  For example a DType representing physical units, such as meters,
  should be able to fall back to NumPy's existing math implementations.

This NEP details how these requirements will be achieved in NumPy:

- All DTyper-specific functionality currently part of the ufunc
  definition will be defined as part of a new `ArrayMethod`_ object.
  This ``ArrayMethod`` object will be the new, preferred, way to describe any
  function operating on arrays.

- Ufuncs will dispatch to the ``ArrayMethod`` and potentially use promotion
  to find the correct ``ArrayMethod`` to use.
  This will be described in the `Promotion and dispatching`_ section.

A new C-API will be outlined in each section. A future Python API is
expected to be very similar and the C-API is presented in terms of Python
code for readability.

The NEP proposes a large, but necessary, refactor of the NumPy ufunc internals.
This modernization will not affect end users directly and is not only a necessary
step for new DTypes, but in itself a maintenance effort which is expected to
help with future improvements to the ufunc machinery.

While the most important restructure proposed is the new ``ArrayMethod``
object, the largest long-term consideration is the API choice for
promotion and dispatching.


***********************
Backwards Compatibility
***********************

The general backwards compatibility issues have also been listed
previously in NEP 41.

The vast majority of users should not see any changes beyond those typical
for NumPy releases.
There are three main users or use-cases impacted by the proposed changes:

1. The Numba package uses direct access to the NumPy C-loops and modifies
   the NumPy ufunc struct directly for its own purposes.
2. Astropy uses its own "type resolver", meaning that a default switch over
   from the existing type resolution to a new default Promoter requires care.
3. It is currently possible to register loops for dtype *instances*.
   This is theoretically useful for structured dtypes and is a resolution
   step happening *after* the DType resolution step proposed here.

This NEP will try hard to maintain backward compatibility as much as
possible. However, both of these projects have signaled willingness to adapt
to breaking changes.

The main reason why NumPy will be able to provide backward compatibility
is that:

* Existing inner-loops can be wrapped, adding an indirection to the call but
  maintaining full backwards compatibility.
  The ``get_loop`` function can, in this case, search the existing
  inner-loop functions (which are stored on the ufunc directly) in order
  to maintain full compatibility even with potential direct structure access.
* Legacy type resolvers can be called as a fallback (potentially caching
  the result). The resolver may need to be called twice (once for the DType
  resolution and once for the ``resolve_descriptor`` implementation).
* The fallback to the legacy type resolver should in most cases handle loops
  defined for such structured dtype instances.  This is because if there is no
  other ``np.Void`` implementation, the legacy fallback will retain the old
  behaviour at least initially.

The masked type resolvers specifically will *not* remain supported, but
has no known users (including NumPy itself, which only uses the default
version).

Further, no compatibility attempt will be made for *calling* as opposed
to providing either the normal or the masked type resolver.  As NumPy
will use it only as a fallback.  There are no known users of this
(undocumented) possibility.

While the above changes potentially break some workflows,
we believe that the long-term improvements vastly outweigh this.
Further, packages such as astropy and Numba are capable of adapting so that
end-users may need to update their libraries but not their code.


******************************************************************************
Usage and impact
******************************************************************************

This NEP restructures how operations on NumPy arrays are defined both
within NumPy and for external implementers.
The NEP mainly concerns those who either extend ufuncs for custom DTypes
or create custom ufuncs.  It does not aim to finalize all
potential use-cases, but rather restructure NumPy to be extensible and allow
addressing new issues or feature requests as they arise.


Overview and end user API 
=========================

To give an overview of how this NEP proposes to structure ufuncs,
the following describes the potential exposure of the proposed restructure
to the end user.

Universal functions are much like a Python method defined on the DType of
the array when considering a ufunc with only a single input::

    res = np.positive(arr)

could be implemented (conceptually) as::

    positive_impl = arr.dtype.positive
    res = positive_impl(arr)

However, unlike methods, ``positive_impl`` is not stored on the dtype itself.
It is rather the implementation of ``np.positive`` for a specific DType.
Current NumPy partially exposes this "choice of implementation" using
the ``dtype`` (or more exact ``signature``) attribute in universal functions,
although these are rarely used::

    np.positive(arr, dtype=np.float64)

forces NumPy to use the ``positive_impl`` written specifically for the Float64
DType.

This NEP makes the distinction more explicit, by creating a new object to
represent ``positive_impl``::

    positive_impl = np.positive.resolve_impl((type(arr.dtype), None))
    # The `None` represents the output DType which is automatically chosen.

While the creation of a ``positive_impl`` object and the ``resolve_impl``
method is part of this NEP, the following code::

    res = positive_impl(arr)

may not be implemented initially and is not central to the redesign.

In general NumPy universal functions can take many inputs.
This requires looking up the implementation by considering all of them
and makes ufuncs "multi-methods" with respect to the input DTypes::

    add_impl = np.add.resolve_impl((type(arr1.dtype), type(arr2.dtype), None))

This NEP defines how ``positive_impl`` and ``add_impl`` will be represented
as a new ``ArrayMethod`` which can be implemented outside of NumPy.
Further, it defines how ``resolve_impl`` will implement and solve dispatching
and promotion.

The reasons for this split may be more clear after reviewing the
`Steps involved in a UFunc call`_ section.


Defining a new ufunc implementation
===================================

The following is a mock-up of how a new implementation, in this case
to define string equality, will be added to a ufunc.

.. code-block:: python

    class StringEquality(BoundArrayMethod):
        nin = 1
        nout = 1
        # DTypes are stored on the BoundArrayMethod and not on the internal
        # ArrayMethod, to reference cyles.
        DTypes = (String, String, Bool)

        def resolve_descriptors(self: ArrayMethod, DTypes, given_descrs):
            """The strided loop supports all input string dtype instances
            and always returns a boolean. (String is always native byte order.)

            Defining this function is not necessary, since NumPy can provide
            it by default.

            The `self` argument here refers to the unbound array method, so
            that DTypes are passed in explicitly.
            """
            assert isinstance(given_descrs[0], DTypes[0])
            assert isinstance(given_descrs[1], DTypes[1])
            assert given_descrs[2] is None or isinstance(given_descrs[2], DTypes[2])
            
            out_descr = given_descrs[2]  # preserve input (e.g. metadata)
            if given_descrs[2] is None:
                out_descr = DTypes[2]()

            # The operation is always "no" casting (most ufuncs are)
            return (given_descrs[0], given_descrs[1], out_descr), "no"

        def strided_loop(context, dimensions, data, strides, innerloop_data):
            """The 1-D strided loop, similar to those used in current ufuncs"""
            # dimensions: Number of loop items and core dimensions
            # data: Pointers to the array data.
            # strides: strides to iterate all elements
            n = dimensions[0]  # number of items to loop over
            num_chars1 = context.descriptors[0].itemsize
            num_chars2 = context.descriptors[1].itemsize

            # C code using the above information to compare the strings in
            # both arrays.  In particular, this loop requires the `num_chars1`
            # and `num_chars2`.  Information which is currently not easily
            # available.

    np.equal.register_impl(StringEquality)
    del StringEquality  # may be deleted.


This definition will be sufficient to create a new loop, and the
structure allows for expansion in the future; something that is already
required to implement casting within NumPy itself.
We use ``BoundArrayMethod`` and a ``context`` structure here.  These
are described and motivated in details later. Briefly:

* ``context`` is a generalization of the ``self`` that Python passes to its
  methods.
* ``BoundArrayMethod`` is equivalent to the Python distinction that
  ``class.method`` is a method, while ``class().method`` returns a "bound" method.


Customizing Dispatching and Promotion
=====================================

Finding the correct implementation when ``np.positive.resolve_impl()`` is
called is largely an implementation detail.
But, in some cases it may be necessary to influence this process when no
implementation matches the requested DTypes exactly:

.. code-block:: python

    np.multiple.resolve_impl((Timedelta64, Int8, None))

will not have an exact match, because NumPy only has an implementation for
multiplying ``Timedelta64`` with ``Int64``.
In simple cases, NumPy will use a default promotion step to attempt to find
the correct implementation, but to implement the above step, we will allow
the following:

.. code-block:: python

    def promote_timedelta_integer(ufunc, dtypes):
        new_dtypes = (Timdelta64, Int64, dtypes[-1])
        # Resolve again, using Int64:
        return ufunc.resolve_impl(new_dtypes)

    np.multiple.register_promoter(
        (Timedelta64, Integer, None), promote_timedelta_integer)

Where ``Integer`` is an abstract DType (compare NEP 42).


.. _steps_of_a_ufunc_call:

****************************************************************************
Steps involved in a UFunc call
****************************************************************************

Before going into more detailed API choices, it is helpful to review the
steps involved in a call to a universal function in NumPy.

A UFunc call is split into the following steps:

1. Handle ``__array_ufunc__`` protocol:

   * For array-likes such as a Dask arrays, NumPy can defer the operation.
     This step is performed first, and unaffected by this NEP (compare :ref:`NEP18`).

2. Promotion and dispatching

   * Given the DTypes of all inputs, find the correct implementation.
     E.g. an implementation for ``float64``, ``int64`` or a user-defined DType.

   * When no exact implementation exists, *promotion* has to be performed.
     For example, adding a ``float32`` and a ``float64`` is implemented by
     first casting the ``float32`` to ``float64``.

3. Parametric ``dtype`` resolution:

   * In general, whenever an output DType is parametric the parameters have
     to be found (resolved).
   * For example, if a loop adds two strings, it is necessary to define the
     correct output (and possibly input) dtypes.  ``S5 + S4 -> S9``, while
     an ``upper`` function has the signature ``S5 -> S5``.
   * When they are not parametric, a default implementation is provided
     which fills in the default dtype instances (ensuring for example native
     byte order).

4. Preparing the iteration:

   * This step is largely handled by ``NpyIter`` internally (the iterator).
   * Allocate all outputs and temporary buffers necessary to perform casts.
     This *requires* the dtypes as resolved in step 3.
   * Find the best iteration order, which includes information to efficiently
     implement broadcasting. For example, adding a single value to an array
     repeats the same value.

5. Setup and fetch the C-level function:

   * If necessary, allocate temporary working space.
   * Find the C-implemented, light weight, inner-loop function.
     Finding the inner-loop function can allow specialized implementations
     in the future.
     For example casting currently optimizes contiguous casts and
     reductions have optimizations that are currently handled
     inside the inner-loop function itself.
   * Signal whether the inner-loop requires the Python API or whether
     the GIL may be released (to allow threading).
   * Clear floating point exception flags.

6. Perform the actual calculation:

   * Run the DType specific inner-loop function.
   * The inner-loop may require access to additional data, such as dtypes or
     additional data set in the previous step.
   * The inner-loop function may be called an undefined number of times.

7. Finalize:

   * Free any temporary working space allocated in step 5.
   * Check for floating point exception flags.
   * Return the result.

The ``ArrayMethod`` provides a concept to group steps 3 to 6 and partially 7.
However, implementers of a new ufunc or ``ArrayMethod`` usually do not need to
customize the behaviour in steps 4 or 6 which NumPy can and does provide.
For the ``ArrayMethod`` implementer, the central steps to customize
are step 3 and step 5.  These provide the custom inner-loop function and
potentially inner-loop specific setup.
Further customization is possible and anticipated as future extensions.

Step 2. is promotion and dispatching and will be restructured
with new API to allow customization of the process where necessary.

Step 1 is listed for completeness and is unaffected by this NEP.

The following sketch provides an overview of step 2 to 6 with an emphasize
of how dtypes are handled and which parts are customizable ("Registered")
and which are handled by NumPy:

.. figure:: _static/nep43-sketch.svg
    :figclass: align-center


*****************************************************************************
ArrayMethod
*****************************************************************************

A central proposal of this NEP is the creation of the ``ArrayMethod`` as an object
describing each implementation specific to a given set of DTypes.
We use the ``class`` syntax to describe the information required to create
a new ``ArrayMethod`` object:

.. code-block:: python

    class ArrayMethod:
        name: str  # Name, mainly useful for debugging

        # Casting safety information (almost always "safe", necessary to
        # unify casting and universal functions)
        casting: Casting = "no"

        # More general flags:
        flags: int

        def resolve_descriptors(self,
                Tuple[DTypeMeta], Tuple[DType|None]: given_descrs) -> Casting, Tuple[DType]:
            """Returns the safety of the operation (casting safety) and the
            """
            # A default implementation can be provided for non-parametric
            # output dtypes.
            raise NotImplementedError

        @staticmethod
        def get_loop(Context : context, strides, ...) -> strided_loop_function, flags:
            """Returns the low-level C (strided inner-loop) function which
            performs the actual operation.
            
            This method may initially private, users will be able to provide
            a set of optimized inner-loop functions instead:
            
            * `strided_inner_loop`
            * `contiguous_inner_loop`
            * `unaligned_strided_loop`
            * ...
            """
            raise NotImplementedError

        @staticmethod
        def strided_inner_loop(
                Context : context, data, dimensions, strides, innerloop_data):
            """The inner-loop (equivalent to the current ufunc loop)
            which is returned by the default `get_loop()` implementation."""
            raise NotImplementedError

With ``Context`` providing mostly static information about the function call:

.. code-block:: python

    class Context:
        # The ArrayMethod object itself:
        ArrayMethod : method

        # Information about the caller, e.g. the ufunc, such as `np.add`:
        callable : caller = None
        # The number of input arguments:
        int : nin = 1
        # The number of output arguments:
        int : nout = 1
        # The actual dtypes instances the inner-loop operates on:
        Tuple[DType] : descriptors

        # Any additional information required. In the future, this will
        # generalize or duplicate things currently stored on the ufunc:
        #  - The ufunc signature of generalized ufuncs
        #  - The identity used for reductions

And ``flags`` stored properties, for whether:

* the ``ArrayMethod`` supports unaligned input and output arrays
* the inner-loop function requires the Python API (GIL)
* NumPy has to check the floating point error CPU flags.

*Note: More information is expected to be added as necessary.*


The call ``Context``
====================

The "context" object is analogous to Python's ``self`` that is
passed to all methods.
To understand why the "context" object is necessary and its
internal structure, it is helpful to remember
that a Python method can be written in the following way
(see also the `documentation of __get__
<https://docs.python.org/3.8/reference/datamodel.html#object.__get__>`_):

.. code-block:: python

    class BoundMethod:
        def __init__(self, instance, method):
            self.instance = instance
            self.method = method

        def __call__(self, *args, **kwargs):
            return self.method.function(self.instance, *args, **kwargs)


    class Method:
        def __init__(self, function):
            self.function = function

        def __get__(self, instance, owner=None):
            assert instance is not None  # unsupported here
            return BoundMethod(instance, self)            


With which the following ``method1`` and ``method2`` below, behave identically:

.. code-block:: python

    def function(self):
        print(self)

    class MyClass:
        def method1(self):
            print(self)

        method2 = Method(function)

And both will print the same result:

.. code-block:: python

    >>> myinstance = MyClass()
    >>> myinstance.method1()
    <__main__.MyClass object at 0x7eff65436d00>
    >>> myinstance.method2()
    <__main__.MyClass object at 0x7eff65436d00>

Here ``self.instance`` would be all information passed on by ``Context``.
The ``Context`` is a generalization and has to pass additional information:

* Unlike a method which operates on a single class instance, the ``ArrayMethod``
  operates on many input arrays and thus multiple dtypes.
* The ``__call__`` of the ``BoundMethod`` above contains only a single call
  to a function. But an ``ArrayMethod`` has to call ``resolve_descriptors``
  and later pass on that information to the inner-loop function.
* A Python function has no state except that defined by its outer scope.
  Within C, ``Context`` is able to provide additional state if necessary.

Just as Python requires the distinction of a method and a bound method,
NumPy will have a ``BoundArrayMethod``.
This stores all of the constant information that is part of the ``Context``,
such as:

* the ``DTypes``
* the number of input and output arguments
* the ufunc signature (specific to generalized ufuncs, compare :ref:`NEP20`).

Fortunately, most users and even ufunc implementers will not have to worry
about these internal details; just like few Python users need to know
about the ``__get__`` dunder method.
The ``Context`` object or C-structure provides all necessary data to the
fast C-functions and NumPy API creates the new ``ArrayMethod`` or
``BoundArrayMethod`` as required.


.. _ArrayMethod_specs:

ArrayMethod Specifications
==========================

.. highlight:: c

These specifications provide a minimal initial C-API, which shall be expanded
in the future, for example to allow specialized inner-loops.

Briefly, NumPy currently relies on strided inner-loops and this
will be the only allowed method of defining a ufunc initially.
We expect the addition of a ``setup`` function or exposure of ``get_loop``
in the future.

UFuncs require the same information as casting, giving the following
definitions (see also :ref:`NEP 42 <NEP42>` ``CastingImpl``):

* A new structure to be passed to the resolve function and inner-loop::
  
        typedef struct {
            PyObject *caller;  /* The ufunc object */
            PyArrayMethodObject *method;

            int nin, nout;

            PyArray_DTypeMeta **dtypes;
            /* Operand descriptors, filled in by resolve_desciptors */
            PyArray_Descr **descriptors;

            void *reserved;  // For Potential in threading (Interpreter state)
        } PyArrayMethod_Context
  
  This structure may be appended to include additional information in future
  versions of NumPy and includes all constant loop metadata.

  We could version this structure, although it may be simpler to version
  the ``ArrayMethod`` itself.

* Similar to casting, ufuncs may need to find the correct loop dtype
  or indicate that a loop is only capable of handling certain instances of
  the involved DTypes (e.g. only native byteorder).  This is handled by
  a ``resolve_descriptors`` function (identical to the ``resolve_descriptors``
  of ``CastingImpl``)::

      NPY_CASTING
      resolve_descriptors(
              PyArrayMethodObject *self,
              PyArray_DTypeMeta *dtypes,
              PyArray_Descr *given_dtypes[nin+nout],
              PyArray_Descr *loop_dtypes[nin+nout]);

  The function fills ``loop_dtypes`` based on the given ``given_dtypes``.
  This requires filling in the descriptor of the output(s).
  Often also the input descriptor(s) have to be found, e.g. to ensure native
  byteorder when needed by the inner-loop.

  In most cases an ``ArrayMethod`` will have non-parametric output DTypes
  so that a default implementation can be provided.

* An additional ``void *user_data`` will usually be typed to extend
  the existing ``NpyAuxData *`` struct::
  
        struct {
            NpyAuxData_FreeFunc *free;
            NpyAuxData_CloneFunc *clone;
            /* To allow for a bit of expansion without breaking the ABI */
           void *reserved[2];
        } NpyAuxData;

  This struct is currently mainly used for the NumPy internal casting
  machinery and as of now both ``free`` and ``clone`` must be provided,
  although this could be relaxed.

  Unlike NumPy casts, the vast majority of ufuncs currently do not require
  this additional scratch-space, but may need simple flagging capability
  for example for implementing warnings (see Error and Warning Handling below).
  To simplify this NumPy will pass a single zero initialized ``npy_intp *``
  when ``user_data`` is not set. 
  *Note that it would be possible to pass this as part of Context.*

* The optional ``get_loop`` function will not be public initially, to avoid
  finalizing the API which requires design choices also with casting:

  .. code-block:: C

        innerloop *
        get_loop(
            PyArrayMethod_Context *context,
            int aligned, int move_references,
            npy_intp *strides,
            PyArray_StridedUnaryOp **out_loop,
            NpyAuxData **innerloop_data,
            NPY_ARRAYMETHOD_FLAGS *flags);
  
  ``NPY_ARRAYMETHOD_FLAGS`` can indicate whether the Python API is required
  and floating point errors must be checked. ``move_references`` is used
  internally for NumPy casting at this time.

* The inner-loop function::

    int inner_loop(PyArrayMethod_Context *context, ..., void *innerloop_data);

  Will have the identical signature to current inner-loops with the following
  changes:

  * A return value to indicate an error when returning ``-1`` instead of ``0``.
    When returning ``-1`` a Python error must be set.
  * The new first argument ``PyArrayMethod_Context *`` is used to pass in
    potentially required information about the ufunc or descriptors in a
    convenient way.
  * The ``void *innerloop_data`` will be the ``NpyAuxData **innerloop_data`` as set by
    ``get_loop``.  If ``get_loop`` does not set ``innerloop_data`` an ``npy_intp *``
    is passed instead (see `Error Handling`_ below for the motivation).

  *Note:* Since ``get_loop`` is expected to be private, the exact implementation
  of ``innerloop_data`` can be modified until final exposure.

Creation of a new ``BoundArrayMethod`` will use a ``PyArrayMethod_FromSpec()``
function.  A shorthand will allow direct registration to a ufunc using
``PyUFunc_AddImplementationFromSpec()``.  The specification is expected
to contain the following (this may extend in the future)::

    typedef struct {
        const char *name;  /* Generic name, mainly for debugging */
        int nin, nout;
        NPY_CASTING casting;
        NPY_ARRAYMETHOD_FLAGS flags;
        PyArray_DTypeMeta **dtypes;
        PyType_Slot *slots;
    } PyArrayMethod_Spec;

.. highlight:: python

Discussion and alternatives
===========================

The above split into an ``ArrayMethod`` and ``Context`` and the additional
requirement of a ``BoundArrayMethod`` is a necessary split mirroring the
implementation of methods and bound methods in Python.

One reason for this requirement is that it allows storing the ``ArrayMethod``
object in many cases without holding references to the ``DTypes`` which may
be important if DTypes are created (and deleted) dynamically.
(This is a complex topic, which does not have a complete solution in current
Python, but the approach solves the issue with respect to casting.)

There seem to be no alternatives to this structure.  Separating the
DType-specific steps from the general ufunc dispatching and promotion is
absolutely necessary to allow future extension and flexibility.
Furthermore, it allows unifying casting and ufuncs.

Since the structure of ``ArrayMethod`` and ``BoundArrayMethod`` will be
opaque and can be extended, there are few long-term design implications aside
from the choice of making them Python objects.


``resolve_descriptors``
-----------------------

The ``resolve_descriptors`` method is possibly the main innovation of this
NEP and it is central also in the implementation of casting in NEP 42.

By ensuring that every ``ArrayMethod`` provides ``resolve_descriptors`` we
define a unified, clear API for step 3 in `Steps involved in a UFunc call`_.
This step is required to allocate output arrays and has to happen before
casting can be prepared.

While the returned casting-safety (``NPY_CASTING``) will almost always be
"no" for universal functions, including it has two big advantages:

* ``-1`` indicates that an error occurred. If a Python error is set, it will
  be raised.  If no Python error is set this will be considered an "impossible"
  cast and a custom error will be set. (This distinction is important for the
  ``np.can_cast()`` function, which should raise the first one and return
  ``False`` in the second case, it is not noteworthy for typical ufuncs).
  *This point is under consideration, we may use -1 to indicate
  a general error, and use a different return value for an impossible cast.*
* Returning the casting safety is central to NEP 42 for casting and
  allows the unmodified use of ``ArrayMethod`` there.
* There may be a future desire to implement fast but unsafe implementations.
  For example for ``int64 + int64 -> int32`` which is unsafe from a casting
  perspective. Currently, this would use ``int64 + int64 -> int64`` and then
  cast to ``int32``. An implementation that skips the cast would
  have to signal that it effectively includes the "same-kind" cast and is
  thus not considered "no".


``get_loop`` method
-------------------

Currently, NumPy ufuncs typically only provide a single strided loop, so that
the ``get_loop`` method may seem unnecessary.
For this reason we plan for ``get_loop`` to be a private function initially.

However, ``get_loop`` is required for casting where specialized loops are
used even beyond strided and contiguous loops.
Thus, the ``get_loop`` function must be a full replacement for
the internal ``PyArray_GetDTypeTransferFunction``.

In the future, ``get_loop`` may be made public or a new ``setup`` function
be exposed to allow more control, for example to allow allocating
working memory.
Further, we could expand ``get_loop`` and allow the ``ArrayMethod`` implementer
to also control the outer iteration and not only the 1-D inner-loop.


Extending the inner-loop signature
----------------------------------

Extending the inner-loop signature is another central and necessary part of
the NEP.

**Passing in the Context:**

Passing in the ``Context`` potentially allows for the future extension of
the signature by adding new fields to the context struct.
Furthermore it provides direct access to the dtype instances which
the inner-loop operates on.
This is necessary information for parametric dtypes since for example comparing
two strings requires knowing the length of both strings.
The ``Context`` can also hold potentially useful information such as the
original ``ufunc``, which can be helpful when reporting errors.

In principle passing in Context is not necessary, as all information could be
included in ``innerloop_data`` and set up in the ``get_loop`` function.
In this NEP we propose passing the struct to simplify creation of loops for
parametric DTypes.

**Passing in user data:**

The current casting implementation uses the existing ``NpyAuxData *`` to pass
in additional data as defined by ``get_loop``.
There are certainly alternatives to the use of this structure, but it
provides a simple solution, which is already used in NumPy and public API.

``NpyAyxData *`` is a light weight, allocated structure and since it already
exists in NumPy for this purpose, it seems a natural choice.
To simplify some use-cases (see "Error Handling" below), we will pass a
``npy_intp *innerloop_data = 0`` instead when ``innerloop_data`` is not provided.

*Note:* Since ``get_loop`` is expected to be private initially we can gain
experience with ``innerloop_data`` before exposing it as public API.

**Return value:**

The return value to indicate an error is an important, but currently missing
feature in NumPy. The error handling is further complicated by the way
CPUs signal floating point errors.
Both are discussed in the next section.

Error Handling
""""""""""""""

.. highlight:: c

We expect that future inner-loops will generally set Python errors as soon
as an error is found. This is complicated when the inner-loop is run without
locking the GIL.  In this case the function will have to lock the GIL,
set the Python error and return ``-1`` to indicate an error occurred:::

    int
    inner_loop(PyArrayMethod_Context *context, ..., void *innerloop_data)
    {
        NPY_ALLOW_C_API_DEF

        for (npy_intp i = 0; i < N; i++) {
            /* calculation */

            if (error_occurred) {
                NPY_ALLOW_C_API;
                PyErr_SetString(PyExc_ValueError,
                    "Error occurred inside inner_loop.");
                NPY_DISABLE_C_API
                return -1;
            }
        }
        return 0;
    }

Floating point errors are special, since they require checking the hardware
state which is too expensive if done within the inner-loop function itself.
Thus, NumPy will handle these if flagged by the ``ArrayMethod``.
An ``ArrayMethod`` should never cause floating point error flags to be set
if it flags that these should not be checked. This could interfere when
calling multiple functions; in particular when casting is necessary.

An alternative solution would be to allow setting the error only at the later
finalization step when NumPy will also check the floating point error flags.

We decided against this pattern at this time. It seems more complex and
generally unnecessary.
While safely grabbing the GIL in the loop may require passing in an additional
``PyThreadState`` or ``PyInterpreterState`` in the future (for subinterpreter
support), this is acceptable and can be anticipated.
Setting the error at a later point would add complexity: for instance
if an operation is paused (which can currently happen for casting in particular),
the error check needs to run explicitly ever time this happens.

We expect that setting errors immediately is the easiest and most convenient
solution and more complex solution may be possible future extensions.

Handling *warnings* is slightly more complex: A warning should be
given exactly once for each function call (i.e. for the whole array) even
if naively it would be given many times.
To simplify such a use case, we will pass in ``npy_intp *innerloop_data = 0``
by default which can be used to store flags (or other simple persistent data).
For instance, we could imagine an integer multiplication loop which warns
when an overflow occurred::

    int
    integer_multiply(PyArrayMethod_Context *context, ..., npy_intp *innerloop_data)
    {
        int overflow;
        NPY_ALLOW_C_API_DEF

        for (npy_intp i = 0; i < N; i++) {
            *out = multiply_integers(*in1, *in2, &overflow);

            if (overflow && !*innerloop_data) {
                NPY_ALLOW_C_API;
                if (PyErr_Warn(PyExc_UserWarning,
                        "Integer overflow detected.") < 0) {
                    NPY_DISABLE_C_API
                    return -1;
                }
                *innerloop_data = 1;
                NPY_DISABLE_C_API
        }
        return 0;
    }

*TODO:* The idea of passing an ``npy_intp`` scratch space when ``innerloop_data``
is not set seems convenient, but I am uncertain about it, since I am not
aware of any similar prior art.  This "scratch space" could also be part of
the ``context`` in principle.

.. highlight:: python

Reusing existing Loops/Implementations
======================================

For many DTypes the above definition for adding additional C-level loops will be
sufficient and require no more than a single strided loop implementation
and if the loop works with parametric DTypes, the
``resolve_descriptors`` function *must* additionally be provided.

However, in some use-cases it is desirable to call back to an existing implementation.
In Python, this could be achieved by simply calling into the original ufunc.

For better performance in C, and for large arrays, it is desirable to reuse
an existing ``ArrayMethod`` as directly as possible, so that its inner-loop function
can be used directly without additional overhead.
We will thus allow to create a new, wrapping, ``ArrayMethod`` from an existing
``ArrayMethod``.

This wrapped ``ArrayMethod`` will have two additional methods:

* ``view_inputs(Tuple[DType]: input_descr) -> Tuple[DType]`` replacing the
  user input descriptors with descriptors matching the wrapped loop.
  It must be possible to *view* the inputs as the output.
  For example for ``Unit[Float64]("m") + Unit[Float32]("km")`` this will
  return ``float64 + int32``. The original ``resolve_descriptors`` will
  convert this to ``float64 + float64``.

* ``wrap_outputs(Tuple[DType]: input_descr) -> Tuple[DType]`` replacing the
  resolved descriptors with the desired actual loop descriptors.
  The original ``resolve_descriptors`` function will be called between these
  two calls, so that the output descriptors may not be set in the first call.
  In the above example it will use the ``float64`` as returned (which might
  have changed the byte-order), and further resolve the physical unit making
  the final signature::
  
      Unit[Float64]("m") + Unit[Float64]("m") -> Unit[Float64]("m")

  the UFunc machinery will take care of casting the "km" input to "m".


The ``view_inputs`` method allows passing the correct inputs into the
original ``resolve_descriptors`` function, while ``wrap_outputs`` ensures
the correct descriptors are used for output allocation and input buffering casts.

An important use-case for this is that of an abstract Unit DType
with subclasses for each numeric dtype (which could be dynamically created)::

    Unit[Float64]("m")
    # with Unit[Float64] being the concrete DType:
    isinstance(Unit[Float64], Unit)  # is True

Such a ``Unit[Float64]("m")`` instance has a well-defined signature with
respect to type promotion.
The author of the ``Unit`` DType can implement most necessary logic by
wrapping the existing math functions and using the two additional methods
above.
Using the *promotion* step, this will allow to create a register a single
promoter for the abstract ``Unit`` DType with the ``ufunc``.
The promoter can then add the wrapped concrete ``ArrayMethod`` dynamically
at promotion time, and NumPy can cache (or store it) after the first call.

**Alternative use-case:**

A different use-case is that of a ``Unit(float64, "m")`` DType, where
the numerical type is part of the DType parameter.
This approach is possible, but will require a custom ``ArrayMethod``
which wraps existing loops.
It must also always require two steps of dispatching (one to the ``Unit``
DType and a second one for the numerical type).

Furthermore, the efficient implementation will require the ability to
fetch and reuse the inner-loop function from another ``ArrayMethod``.
(Which is probably necessary for users like Numba, but it is uncertain
whether it should be a common pattern and it cannot be accessible from
Python itself.)


.. _promotion_and_dispatching:

*************************
Promotion and dispatching
*************************

NumPy ufuncs are multi-methods in the sense that they operate on (or with)
multiple DTypes at once.
While the input (and output) dtypes are attached to NumPy arrays,
the ``ndarray`` type itself does not carry the information of which
function to apply to the data.

For example, given the input::

    int_arr = np.array([1, 2, 3], dtype=np.int64)
    float_arr = np.array([1, 2, 3], dtype=np.float64)
    np.add(int_arr, float_arr)

has to find the correct ``ArrayMethod`` to perform the operation.
Ideally, there is an exact match defined, e.g. for ``np.add(int_arr, int_arr)``
the ``ArrayMethod[Int64, Int64, out=Int64]`` matches exactly and can be used.
However, for ``np.add(int_arr, float_arr)`` there is no direct match,
requiring a promotion step.

Promotion and dispatching process
=================================

In general the ``ArrayMethod`` is found by searching for an exact match of
all input DTypes.
The output dtypes should *not* affect calculation, but if multiple registered
``ArrayMethod``\ s match exactly, the output DType will be used to find the
better match.
This will allow the current distinction for ``np.equal`` loops which define
both ``Object, Object -> Bool`` (default) and ``Object, Object -> Object``.

Initially, an ``ArrayMethod`` will be defined for *concrete* DTypes only
and since these cannot be subclassed an exact match is guaranteed.
In the future we expect that ``ArrayMethod``\ s can also be defined for
*abstract* DTypes. In which case the best match is found as detailed below.

**Promotion:**

If a matching ``ArrayMethod`` exists, dispatching is straight forward.
However, when it does not, additional definitions are required to implement
this "promotion":

* By default any UFunc has a promotion which uses the common DType of all
  inputs and dispatches a second time.  This is well-defined for most
  mathematical functions, but can be disabled or customized if necessary.
  For instances ``int32 + float64`` tries again using ``float64 + float64``
  which is the common DType.

* Users can *register* new Promoters just as they can register a
  new ``ArrayMethod``.  These will use abstract DTypes to allow matching
  a large variety of signatures.
  The return value of a promotion function shall be a new ``ArrayMethod``
  or ``NotImplemented``.  It must be consistent over multiple calls with
  the same input to allow caching of the result.

The signature of a promotion function would be::

    promoter(np.ufunc: ufunc, Tuple[DTypeMeta]: DTypes): -> Union[ArrayMethod, NotImplemented]

Note that DTypes may include the output's DType, however, normally the
output DType will *not* affect which ``ArrayMethod`` is chosen.

In most cases, it should not be necessary to add a custom promotion function.
An example which requires this is multiplication with a unit:
in NumPy ``timedelta64`` can be multiplied with most integers,
but NumPy only defines a loop (``ArrayMethod``) for ``timedelta64 * int64``
so that multiplying with ``int32`` would fail.

To allow this, the following promoter can be registered for
``(Timedelta64, Integral, None)``::

    def promote(ufunc, DTypes):
        res = list(DTypes)
        try:
            res[1] = np.common_dtype(DTypes[1], Int64)
        except TypeError:
            return NotImplemented

        # Could check that res[1] is actually Int64
        return ufunc.resolve_impl(tuple(res))

In this case, just as a ``Timedelta64 * int64`` and ``int64 * timedelta64``
``ArrayMethod`` is necessary, a second promoter will have to be registered to
handle the case where the integer is passed first.

**Dispatching rules for ArrayMethod and Promoters:**

Promoter and ``ArrayMethod`` are discovered by finding the best match as
defined by the DType class hierarchy.
The best match is defined if:

* The signature matches for all input DTypes, so that
  ``issubclass(input_DType, registered_DType)``  returns true.
* No other promoter or ``ArrayMethod`` is more precise in any input:
  ``issubclass(other_DType, this_DType)`` is true (this may include if both
  are identical).
* This promoter or ``ArrayMethod`` is more precise in at least one input or
  output DType.

It will be an error if ``NotImplemented`` is returned or if two
promoters match the input equally well.
When an existing promoter is not precise enough for new functionality, a
new promoter has to be added.
To ensure that this promoter takes precedence it may be necessary to define
new abstract DTypes as more precise subclasses of existing ones.

The above rules enable specialization if an output is supplied
or the full loop is specified.  This should not typically be necessary,
but allows resolving ``np.logic_or``, etc. which have both
``Object, Object -> Bool`` and ``Object, Object -> Object`` loops (using the
first by default).


Discussion and alternatives
===========================

Instead of resolving and returning a new implementation, we could also
return a new set of DTypes to use for dispatching.  This works, however,
it has the disadvantage that it is impossible to dispatch to a loop
defined on a different ufunc or to dynamically create a new ``ArrayMethod``.


**Rejected Alternatives:**

In the above the promoters use a multiple dispatching style type resolution
while the current UFunc machinery uses the first
"safe" loop (see also :ref:`NEP 40 <NEP40>`) in an ordered hierarchy.

While the "safe" casting rule is not restrictive enough, we could imagine
using a new "promote" casting rule, or the common-DType logic to find the
best matching loop by upcasting the inputs as necessary.

One downside to this approach is that upcasting alone allows upcasting the
result beyond what is expected by users:
Currently (which will remain supported as a fallback) any ufunc which defines
only a float64 loop will also work for float16 and float32 by *upcasting*::

    >>> from scipy.special import erf
    >>> erf(np.array([4.], dtype=np.float16))  # float16
    array([1.], dtype=float32)

with a float32 result.  It is impossible to change the ``erf`` function to
return a float16 result without changing the result of following code.
In general, we argue that automatic upcasting should not occur in cases
where a less precise loop can be defined, *unless* the ufunc
author does this intentionally using a promotion.

This consideration means that upcasting has to be limited by some additional
method.

*Alternative 1:*

Assuming general upcasting is not intended, a rule must be defined to
limit upcasting the input from ``float16 -> float32`` either using generic
logic on the DTypes or the UFunc itself (or a combination of both).
The UFunc cannot do this easily on its own, since it cannot know all possible
DTypes which register loops.
Consider the two examples:

First (should be rejected):

* Input: ``float16 * float16``
* Existing loop: ``float32 * float32``

Second (should be accepted):

* Input: ``timedelta64 * int32``
* Existing loop: ``timedelta64 * int16``


This requires either:

1. The ``timedelta64`` to somehow signal that the ``int64`` upcast is
   always supported if it is involved in the operation.
2. The ``float32 * float32`` loop to reject upcasting.

Implementing the first approach requires signaling that upcasts are
acceptable in the specific context.  This would require additional hooks
and may not be simple for complex DTypes.

For the second approach in most cases a simple ``np.common_dtype`` rule will
work for initial dispatching, however, even this is only clearly the case
for homogeneous loops.
This option will require adding a function to check whether the input
is a valid upcast to each loop individually, which seems problematic.
In many cases a default could be provided (homogeneous signature).

*Alternative 2:*

An alternative "promotion" step is to ensure that the *output* DType matches
with the loop after first finding the correct output DType.
If the output DTypes are known, finding a safe loop becomes easy.
In the majority of cases this works, the correct output dtype is just::

    np.common_dtype(*input_DTypes)

or some fixed DType (e.g. Bool for logical functions).

However, it fails for example in the ``timedelta64 * int32`` case above since
there is a-priori no way to know that the "expected" result type of this
output is indeed ``timedelta64`` (``np.common_dtype(Datetime64, Int32)`` fails).
This requires some additional knowledge of the timedelta64 precision being
int64. Since a ufunc can have an arbitrary number of (relevant) inputs
it would thus at least require an additional ``__promoted_dtypes__`` method
on ``Datetime64`` (and all DTypes).

A further limitation is shown by masked DTypes.  Logical functions do not
have a boolean result when masked are involved, which would thus require the
original ufunc author to anticipate masked DTypes in this scheme.
Similarly, some functions defined for complex values will return real numbers
while others return complex numbers.  If the original author did not anticipate
complex numbers, the promotion may be incorrect for a later added complex loop.


We believe that promoters, while allowing for an huge theoretical complexity,
are the best solution:

1. Promotion allows for dynamically adding new loops. E.g. it is possible
   to define an abstract Unit DType, which dynamically creates classes to
   wrap other existing DTypes.  Using a single promoter, this DType can
   dynamically wrap existing ``ArrayMethod`` enabling it to find the correct
   loop in a single lookup instead of two.
2. The promotion logic will usually err on the safe side: A newly-added
   loop cannot be misused unless a promoter is added as well.
3. They put the burden of carefully thinking of whether the logic is correct
   on the programmer adding new loops to a UFunc.  (Compared to Alternative 2)
4. In case of incorrect existing promotion, writing a promoter to restrict
   or refine a generic rule is possible.  In general a promotion rule should
   never return an *incorrect* promotion, but if it the existing promotion
   logic fails or is incorrect for a newly-added loop, the loop can add a
   new promoter to refine the logic.

The option of having each loop verify that no upcast occurred is probably
the best alternative, but does not include the ability to dynamically
adding new loops.

The main downsides of general promoters is that they allow a possible
very large complexity.
A third-party library *could* add incorrect promotions to NumPy, however,
this is already possible by adding new incorrect loops.
In general we believe we can rely on downstream projects to use this
power and complexity carefully and responsibly.


***************
User Guidelines
***************

In general adding a promoter to a UFunc must be done very carefully.
A promoter should never affect loops which can be reasonably defined
by other datatypes.  Defining a hypothetical ``erf(UnitFloat16)`` loop
must not lead to ``erf(float16)``.
In general a promoter should fulfill the following requirements:

* Be conservative when defining a new promotion rule. An incorrect result
  is a much more dangerous error than an unexpected error.
* One of the (abstract) DTypes added should typically match specifically with a
  DType (or family of DTypes) defined by your project.
  Never add promotion rules which go beyond normal common DType rules!
  It is *not* reasonable to add a loop for ``int16 + uint16 -> int24`` if
  you write an ``int24`` dtype. The result of this operation was already
  defined previously as ``int32`` and will be used with this assumption.
* A promoter (or loop) should never affect existing loop results.
  This includes adding faster but less precise loops/promoters to replace
  existing ones.
* Try to stay within a clear, linear hierarchy for all promotion (and casting)
  related logic. NumPy itself breaks this logic for integers and floats
  (they are not strictly linear, since int64 cannot promote to float32).
* Loops and promoters can be added by any project, which could be:

  * The project defining the ufunc
  * The project defining the DType
  * A third-party project

  Try to find out which is the best project to add the loop.  If neither
  the project defining the ufunc nor the project defining the DType add the
  loop, issues with multiple definitions (which are rejected) may arise
  and care should be taken that the loop behaviour is always more desirable
  than an error.

In some cases exceptions to these rules may make sense, however, in general
we ask you to use extreme caution and when in doubt create a new UFunc
instead.  This clearly notifies the users of differing rules.
When in doubt, ask on the NumPy mailing list or issue tracker!


**************
Implementation
**************

Implementation of this NEP will entail a large refactor and restructuring
of the current ufunc machinery (as well as casting).

The implementation unfortunately will require large maintenance of the
UFunc machinery, since both the actual UFunc loop calls, as well as the
initial dispatching steps have to be modified.

In general, the correct ``ArrayMethod``, also those returned by a promoter,
will be cached (or stored) inside a hashtable for efficient lookup.


**********
Discussion
**********

There is a large space of possible implementations with many discussions
in various places, as well as initial thoughts and design documents.
These are listed in the discussion of :ref:`NEP 40 <NEP40>` and not repeated here for
brevity.

A long discussion which touches many of these points and points towards
similar solutions can be found in
`the github issue 12518 "What should be the calling convention for ufunc inner loop signatures?" <https://github.com/numpy/numpy/issues/12518>`_


**********
References
**********

Please see NEP 40 and 41 for more discussion and references.


*********
Copyright
*********

This document has been placed in the public domain.
.. _NEP35:

===========================================================
NEP 35 — Array creation dispatching with __array_function__
===========================================================

:Author: Peter Andreas Entschev <pentschev@nvidia.com>
:Status: Final
:Type: Standards Track
:Created: 2019-10-15
:Updated: 2020-11-06
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2021-May/081761.html

Abstract
--------

We propose the introduction of a new keyword argument ``like=`` to all array
creation functions to address one of the shortcomings of ``__array_function__``,
as described by NEP 18 [1]_. The ``like=`` keyword argument will create an
instance of the argument's type, enabling direct creation of non-NumPy arrays.
The target array type must implement the ``__array_function__`` protocol.

Motivation and Scope
--------------------

Many libraries implement the NumPy API, such as Dask for graph
computing, CuPy for GPGPU computing, xarray for N-D labeled arrays, etc. Underneath,
they have adopted the ``__array_function__`` protocol which allows NumPy to understand
and treat downstream objects as if they are the native ``numpy.ndarray`` object.
Hence the community while using various libraries still benefits from a unified
NumPy API. This not only brings great convenience for standardization but also
removes the burden of learning a new API and rewriting code for every new
object. In more technical terms, this mechanism of the protocol is called a
"dispatcher", which is the terminology we use from here onwards when referring
to that.


.. code:: python

    x = dask.array.arange(5)    # Creates dask.array
    np.diff(x)                  # Returns dask.array

Note above how we called Dask's implementation of ``diff`` via the NumPy
namespace by calling ``np.diff``, and the same would apply if we had a CuPy
array or any other array from a library that adopts ``__array_function__``.
This allows writing code that is agnostic to the implementation library, thus
users can write their code once and still be able to use different array
implementations according to their needs.

Obviously, having a protocol in-place is useful if the arrays are created
elsewhere and let NumPy handle them. But still these arrays have to be started
in their native library and brought back. Instead if it was possible to create
these objects through NumPy API then there would be an almost complete
experience, all using NumPy syntax. For example, say we have some CuPy array
``cp_arr``, and want a similar CuPy array with identity matrix. We could still
write the following:

.. code:: python

     x = cupy.identity(3)

Instead, the better way would be using to only use the NumPy API, this could now
be achieved with:

.. code:: python

    x = np.identity(3, like=cp_arr)

As if by magic, ``x`` will also be a CuPy array, as NumPy was capable to infer
that from the type of ``cp_arr``. Note that this last step would not be possible
without ``like=``, as it would be impossible for the NumPy to know the user
expects a CuPy array based only on the integer input.

The new ``like=`` keyword proposed is solely intended to identify the downstream
library where to dispatch and the object is used only as reference, meaning that
no modifications, copies or processing will be performed on that object.

We expect that this functionality will be mostly useful to library developers,
allowing them to create new arrays for internal usage based on arrays passed
by the user, preventing unnecessary creation of NumPy arrays that will
ultimately lead to an additional conversion into a downstream array type.

Support for Python 2.7 has been dropped since NumPy 1.17, therefore we make use
of the keyword-only argument standard described in PEP-3102 [2]_ to implement
``like=``, thus preventing it from being passed by position.

.. _neps.like-kwarg.usage-and-impact:

Usage and Impact
----------------

NumPy users who don't use other arrays from downstream libraries can continue
to use array creation routines without a ``like=`` argument. Using
``like=np.ndarray`` will work as if no array was passed via that argument.
However, this will incur additional checks that will negatively impact
performance.

To understand the intended use for ``like=``, and before we move to more complex
cases, consider the following illustrative example consisting only of NumPy and
CuPy arrays:

.. code:: python

    import numpy as np
    import cupy

    def my_pad(arr, padding):
        padding = np.array(padding, like=arr)
        return np.concatenate((padding, arr, padding))

    my_pad(np.arange(5), [-1, -1])    # Returns np.ndarray
    my_pad(cupy.arange(5), [-1, -1])  # Returns cupy.core.core.ndarray

Note in the ``my_pad`` function above how ``arr`` is used as a reference to
dictate what array type padding should have, before concatenating the arrays to
produce the result. On the other hand, if ``like=`` wasn't used, the NumPy case
would still work, but CuPy wouldn't allow this kind of automatic
conversion, ultimately raising a
``TypeError: Only cupy arrays can be concatenated`` exception.

Now we should look at how a library like Dask could benefit from ``like=``.
Before we understand that, it's important to understand a bit about Dask basics
and how it ensures correctness with ``__array_function__``. Note that Dask can
perform computations on different sorts of objects, like dataframes, bags and
arrays, here we will focus strictly on arrays, which are the objects we can use
``__array_function__`` with.

Dask uses a graph computing model, meaning it breaks down a large problem in
many smaller problems and merges their results to reach the final result. To
break the problem down into smaller ones, Dask also breaks arrays into smaller
arrays that it calls "chunks". A Dask array can thus consist of one or more
chunks and they may be of different types. However, in the context of
``__array_function__``, Dask only allows chunks of the same type; for example,
a Dask array can be formed of several NumPy arrays or several CuPy arrays, but
not a mix of both.

To avoid mismatched types during computation, Dask keeps an attribute ``_meta`` as
part of its array throughout computation: this attribute is used to both predict
the output type at graph creation time, and to create any intermediary arrays
that are necessary within some function's computation. Going back to our
previous example, we can use ``_meta`` information to identify what kind of
array we would use for padding, as seen below:

.. code:: python

    import numpy as np
    import cupy
    import dask.array as da
    from dask.array.utils import meta_from_array

    def my_dask_pad(arr, padding):
        padding = np.array(padding, like=meta_from_array(arr))
        return np.concatenate((padding, arr, padding))

    # Returns dask.array<concatenate, shape=(9,), dtype=int64, chunksize=(5,), chunktype=numpy.ndarray>
    my_dask_pad(da.arange(5), [-1, -1])

    # Returns dask.array<concatenate, shape=(9,), dtype=int64, chunksize=(5,), chunktype=cupy.ndarray>
    my_dask_pad(da.from_array(cupy.arange(5)), [-1, -1])

Note how ``chunktype`` in the return value above changes from
``numpy.ndarray`` in the first ``my_dask_pad`` call to ``cupy.ndarray`` in the
second. We have also renamed the function to ``my_dask_pad`` in this example
with the intent to make it clear that this is how Dask would implement such
functionality, should it need to do so, as it requires Dask's internal tools
that are not of much use elsewhere.

To enable proper identification of the array type we use Dask's utility function
``meta_from_array``, which was introduced as part of the work to support
``__array_function__``, allowing Dask to handle ``_meta`` appropriately. Readers
can think of ``meta_from_array`` as a special function that just returns the
type of the underlying Dask array, for example:

.. code:: python

    np_arr = da.arange(5)
    cp_arr = da.from_array(cupy.arange(5))

    meta_from_array(np_arr)  # Returns a numpy.ndarray
    meta_from_array(cp_arr)  # Returns a cupy.ndarray

Since the value returned by ``meta_from_array`` is a NumPy-like array, we can
just pass that directly into the ``like=`` argument.

The ``meta_from_array`` function is primarily targeted at the library's internal
usage to ensure chunks are created with correct types. Without the ``like=``
argument, it would be impossible to ensure ``my_pad`` creates a padding array
with a type matching that of the input array, which would cause a ``TypeError``
exception to be raised by CuPy, as discussed above would happen to the CuPy case
alone. Combining Dask's internal handling of meta arrays and the proposed
``like=`` argument, it now becomes possible to handle cases involving creation
of non-NumPy arrays, which is likely the heaviest limitation Dask currently
faces from the ``__array_function__`` protocol.

Backward Compatibility
----------------------

This proposal does not raise any backward compatibility issues within NumPy,
given that it only introduces a new keyword argument to existing array creation
functions with a default ``None`` value, thus not changing current behavior.

Detailed description
--------------------

The introduction of the ``__array_function__`` protocol allowed downstream
library developers to use NumPy as a dispatching API. However, the protocol
did not -- and did not intend to -- address the creation of arrays by downstream
libraries, preventing those libraries from using such important functionality in
that context.

The purpose of this NEP is to address that shortcoming in a simple and
straightforward way: introduce a new ``like=`` keyword argument, similar to how
the ``empty_like`` family of functions work. When array creation functions
receive such an argument, they will trigger the ``__array_function__`` protocol,
and call the downstream library's own array creation function implementation.
The ``like=`` argument, as its own name suggests, shall be used solely for the
purpose of identifying where to dispatch.  In contrast to the way
``__array_function__`` has been used so far (the first argument identifies the
target downstream library), and to avoid breaking NumPy's API with regards to
array creation, the new ``like=`` keyword shall be used for the purpose of
dispatching.

Downstream libraries will benefit from the ``like=`` argument without any
changes to their API, given the argument only needs to be implemented by NumPy.
It's still allowed that downstream libraries include the ``like=`` argument,
as it can be useful in some cases, please refer to
:ref:`neps.like-kwarg.implementation` for details on those cases. It will still
be required that downstream libraries implement the ``__array_function__``
protocol, as described by NEP 18 [1]_, and appropriately introduce the argument
to their calls to NumPy array creation functions, as exemplified in
:ref:`neps.like-kwarg.usage-and-impact`.

Related work
------------

Other NEPs have been written to address parts of ``__array_function__``
protocol's limitation, such as the introduction of the ``__duckarray__``
protocol in NEP 30 [3]_, and the introduction of an overriding mechanism called
``uarray`` by NEP 31 [4]_.

.. _neps.like-kwarg.implementation:

Implementation
--------------

The implementation requires introducing a new ``like=`` keyword to all existing
array creation functions of NumPy. As examples of functions that would add this
new argument (but not limited to) we can cite those taking array-like objects
such as ``array`` and ``asarray``, functions that create arrays based on
numerical inputs such as ``range`` and ``identity``, as well as the ``empty``
family of functions, even though that may be redundant, since specializations
for those already exist with the naming format ``empty_like``. As of the
writing of this NEP, a complete list of array creation functions can be
found in [5]_.

This newly proposed keyword shall be removed by the ``__array_function__``
mechanism from the keyword dictionary before dispatching. The purpose for this
is twofold:

1. Simplifies adoption of array creation by those libraries already opting-in
   to implement the ``__array_function__`` protocol, thus removing the
   requirement to explicitly opt-in for all array creation functions; and
2. Most downstream libraries will have no use for the keyword argument, and
   those that do may accomplish so by capturing ``self`` from
   ``__array_function__``.

Downstream libraries thus do not require to include the ``like=`` keyword to
their array creation APIs. In some cases (e.g., Dask), having the ``like=``
keyword can be useful, as it would allow the implementation to identify
array internals. As an example, Dask could benefit from the reference array
to identify its chunk type (e.g., NumPy, CuPy, Sparse), and thus create a new
Dask array backed by the same chunk type, something that's not possible unless
Dask can read the reference array's attributes.

Function Dispatching
~~~~~~~~~~~~~~~~~~~~

There are two different cases to dispatch: Python functions, and C functions.
To permit ``__array_function__`` dispatching, one possible implementation is to
decorate Python functions with ``overrides.array_function_dispatch``, but C
functions have a different requirement, which we shall describe shortly.

The example below shows a suggestion on how the ``asarray`` could be decorated
with ``overrides.array_function_dispatch``:

.. code:: python

    def _asarray_decorator(a, dtype=None, order=None, *, like=None):
        return (like,)

    @set_module('numpy')
    @array_function_dispatch(_asarray_decorator)
    def asarray(a, dtype=None, order=None, *, like=None):
        return array(a, dtype, copy=False, order=order)

Note in the example above that the implementation remains unchanged, the only
difference is the decoration, which uses the new ``_asarray_decorator`` function
to instruct the ``__array_function__`` protocol to dispatch if ``like`` is not
``None``.

We will now look at a C function example, and since ``asarray`` is anyway a
specialization of ``array``, we will use the latter as an example now. As
``array`` is a C function, currently all NumPy does regarding its Python source
is to import the function and adjust its ``__module__`` to ``numpy``. The
function will now be decorated with a specialization of
``overrides.array_function_from_dispatcher``, which shall take care of adjusting
the module too.

.. code:: python

    array_function_nodocs_from_c_func_and_dispatcher = functools.partial(
        overrides.array_function_from_dispatcher,
        module='numpy', docs_from_dispatcher=False, verify=False)

    @array_function_nodocs_from_c_func_and_dispatcher(_multiarray_umath.array)
    def array(a, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,
              like=None):
        return (like,)

There are two downsides to the implementation above for C functions:

1.  It creates another Python function call; and
2.  To follow current implementation standards, documentation should be attached
    directly to the Python source code.

The first version of this proposal suggested the implementation above as one
viable solution for NumPy functions implemented in C. However, due to the
downsides pointed out above we have decided to discard any changes on the Python
side and resolve those issues with a pure-C implementation. Please refer to
[7]_ for details.

Reading the Reference Array Downstream
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As stated in the beginning of :ref:`neps.like-kwarg.implementation` section,
``like=`` is not propagated to the downstream library, nevertheless, it's still
possible to access it. This requires some changes in the downstream library's
``__array_function__`` definition, where the ``self`` attribute is in practice
that passed via ``like=``. This is the case because we use ``like=`` as the
dispatching array, unlike other compute functions covered by NEP-18 that usually
dispatch on the first positional argument.

An example of such use is to create a new Dask array while preserving its
backend type:

.. code:: python

    # Returns dask.array<array, shape=(3,), dtype=int64, chunksize=(3,), chunktype=cupy.ndarray>
    np.asarray([1, 2, 3], like=da.array(cp.array(())))

    # Returns a cupy.ndarray
    type(np.asarray([1, 2, 3], like=da.array(cp.array(()))).compute())

Note how above the array is backed by ``chunktype=cupy.ndarray``, and the
resulting array after computing it is also a ``cupy.ndarray``. If Dask did
not use the ``like=`` argument via the ``self`` attribute from
``__array_function__``, the example above would be backed by ``numpy.ndarray``
instead:

.. code:: python

    # Returns dask.array<array, shape=(3,), dtype=int64, chunksize=(3,), chunktype=numpy.ndarray>
    np.asarray([1, 2, 3], like=da.array(cp.array(())))

    # Returns a numpy.ndarray
    type(np.asarray([1, 2, 3], like=da.array(cp.array(()))).compute())

Given the library would need to rely on ``self`` attribute from
``__array_function__`` to dispatch the function with the correct reference
array, we suggest one of two alternatives:

1. Introduce a list of functions in the downstream library that do support the
   ``like=`` argument and pass ``like=self`` when calling the function; or
2. Inspect whether the function's signature and verify whether it includes the
   ``like=`` argument. Note that this may incur in a higher performance penalty
   and assumes introspection is possible, which may not be if the function is
   a C function.

To make things clearer, let's take a look at how suggestion 2 could be
implemented in Dask. The current relevant part of ``__array_function__``
definition in Dask is seen below:

.. code:: python

    def __array_function__(self, func, types, args, kwargs):
        # Code not relevant for this example here

        # Dispatch ``da_func`` (da.asarray, for example) with *args and **kwargs
        da_func(*args, **kwargs)

And this is how the updated code would look like:

.. code:: python

    def __array_function__(self, func, types, args, kwargs):
        # Code not relevant for this example here

        # Inspect ``da_func``'s  signature and store keyword-only arguments
        import inspect
        kwonlyargs = inspect.getfullargspec(da_func).kwonlyargs

        # If ``like`` is contained in ``da_func``'s signature, add ``like=self``
        # to the kwargs dictionary.
        if 'like' in kwonlyargs:
            kwargs['like'] = self

        # Dispatch ``da_func`` (da.asarray, for example) with args and kwargs.
        # Here, kwargs contain ``like=self`` if the function's signature does too.
        da_func(*args, **kwargs)

Alternatives
------------

Recently a new protocol to replace ``__array_function__`` entirely was proposed
by NEP 37 [6]_, which would require considerable rework by downstream libraries
that adopt ``__array_function__`` already, because of that we still believe the
``like=`` argument is beneficial for NumPy and downstream libraries. However,
that proposal wouldn't necessarily be considered a direct alternative to the
present NEP, as it would replace NEP 18 entirely, upon which this builds.
Discussion on details about this new proposal and why that would require rework
by downstream libraries is beyond the scope of the present proposal.

Discussion
----------

- `Further discussion on implementation and the NEP's content <https://mail.python.org/pipermail/numpy-discussion/2020-August/080919.html>`_
- `Decision to release an experimental implementation in NumPy 1.20.0 <https://mail.python.org/pipermail/numpy-discussion/2020-November/081193.html>`__


References
----------

.. [1] `NEP 18 - A dispatch mechanism for NumPy's high level array functions <https://numpy.org/neps/nep-0018-array-function-protocol.html>`_.

.. [2] `PEP 3102 — Keyword-Only Arguments <https://www.python.org/dev/peps/pep-3102/>`_.

.. [3] `NEP 30 — Duck Typing for NumPy Arrays - Implementation <https://numpy.org/neps/nep-0030-duck-array-protocol.html>`_.

.. [4] `NEP 31 — Context-local and global overrides of the NumPy API <https://github.com/numpy/numpy/pull/14389>`_.

.. [5] `Array creation routines <https://docs.scipy.org/doc/numpy-1.17.0/reference/routines.array-creation.html>`_.

.. [6] `NEP 37 — A dispatch protocol for NumPy-like modules <https://numpy.org/neps/nep-0037-array-module.html>`_.

.. [7] `Implementation's pull request on GitHub <https://github.com/numpy/numpy/pull/16935>`_

Copyright
---------

This document has been placed in the public domain.
.. _NEP45:

=================================
NEP 45 — C style guide
=================================

:Author: Charles Harris <charlesr.harris@gmail.com>
:Status: Accepted
:Type: Process
:Created: 2012-02-26
:Resolution: https://github.com/numpy/numpy/issues/11911

.. highlight:: c

Abstract
--------

This document gives coding conventions for the C code comprising
the C implementation of NumPy.

Motivation and Scope
--------------------

The NumPy C coding conventions are based on Python
`PEP 7 -- Style Guide for C Code <https://www.python.org/dev/peps/pep-0007>`_
by Guido van Rossum with a few added strictures.

Because the NumPy conventions are very close to those in PEP 7, that PEP is
used as a template with the NumPy additions and variations in the appropriate
spots.

Usage and Impact
----------------

There are many C coding conventions and it must be emphasized that the primary
goal of the NumPy conventions isn't to choose the "best," about which there is
certain to be disagreement, but to achieve uniformity.

Two good reasons to break a particular rule:

1. When applying the rule would make the code less readable, even
   for someone who is used to reading code that follows the rules.

2. To be consistent with surrounding code that also breaks it
   (maybe for historic reasons) -- although this is also an
   opportunity to clean up someone else's mess.


Backward compatibility
----------------------

No impact.


Detailed description
--------------------

C dialect
=========

* Use C99 (that is, the standard defined by ISO/IEC 9899:1999).

* Don't use GCC extensions (for instance, don't write multi-line strings
  without trailing backslashes). Preferably break long strings
  up onto separate lines like so::

          "blah blah"
          "blah blah"

  This will work with MSVC, which otherwise chokes on very long
  strings.

* All function declarations and definitions must use full prototypes (that is,
  specify the types of all arguments).

* No compiler warnings with major compilers (gcc, VC++, a few others).

.. note::
   NumPy still produces compiler warnings that need to be addressed.

Code layout
============

* Use 4-space indents and no tabs at all.

* No line should be longer than 80 characters.  If this and the
  previous rule together don't give you enough room to code, your code is
  too complicated -- consider using subroutines.

* No line should end in whitespace.  If you think you need
  significant trailing whitespace, think again; somebody's editor might
  delete it as a matter of routine.

* Function definition style: function name in column 1, outermost
  curly braces in column 1, blank line after local variable declarations::

        static int
        extra_ivars(PyTypeObject *type, PyTypeObject *base)
        {
            int t_size = PyType_BASICSIZE(type);
            int b_size = PyType_BASICSIZE(base);

            assert(t_size >= b_size); /* type smaller than base! */
            ...
            return 1;
        }

  If the transition to C++ goes through it is possible that this form will
  be relaxed so that short class methods meant to be inlined can have the
  return type on the same line as the function name. However, that is yet to
  be determined.

* Code structure: one space between keywords like ``if``, ``for`` and
  the following left parenthesis; no spaces inside the parenthesis; braces
  around all ``if`` branches, and no statements on the same line as the
  ``if``. They should be formatted as shown::

        if (mro != NULL) {
            one_line_statement;
        }
        else {
            ...
        }


        for (i = 0; i < n; i++) {
            one_line_statement;
        }


        while (isstuff) {
            dostuff;
        }


        do {
            stuff;
        } while (isstuff);


        switch (kind) {
            /* Boolean kind */
            case 'b':
                return 0;
            /* Unsigned int kind */
            case 'u':
                ...
            /* Anything else */
            default:
                return 3;
        }


* The return statement should *not* get redundant parentheses::

        return Py_None; /* correct */
        return(Py_None); /* incorrect */

* Function and macro call style: ``foo(a, b, c)``, no space before
  the open paren, no spaces inside the parens, no spaces before
  commas, one space after each comma.

* Always put spaces around the assignment, Boolean, and comparison
  operators.  In expressions using a lot of operators, add spaces
  around the outermost (lowest priority) operators.

* Breaking long lines: If you can, break after commas in the
  outermost argument list.  Always indent continuation lines
  appropriately: ::

        PyErr_SetString(PyExc_TypeError,
                "Oh dear, you messed up.");

  Here appropriately means at least a double indent (8 spaces). It isn't
  necessary to line everything up with the opening parenthesis of the function
  call.

* When you break a long expression at a binary operator, the
  operator goes at the end of the previous line, for example: ::

        if (type > tp_dictoffset != 0 &&
                base > tp_dictoffset == 0 &&
                type > tp_dictoffset == b_size &&
                (size_t)t_size == b_size + sizeof(PyObject *)) {
            return 0;
        }

  Note that the terms in the multi-line Boolean expression are indented so
  as to make the beginning of the code block clearly visible.

* Put blank lines around functions, structure definitions, and
  major sections inside functions.

* Comments go before the code they describe. Multi-line comments should
  be like so: ::

        /*
         * This would be a long
         * explanatory comment.
         */

  Trailing comments should be used sparingly. Instead of ::

        if (yes) { // Success!

  do ::

        if (yes) {
            // Success!

* All functions and global variables should be declared static
  when they aren't needed outside the current compilation unit.

* Declare external functions and variables in a header file.


Naming conventions
==================

* There has been no consistent prefix for NumPy public functions, but
  they all begin with a prefix of some sort, followed by an underscore, and
  are in camel case: ``PyArray_DescrAlignConverter``, ``NpyIter_GetIterNext``.
  In the future the names should be of the form ``Npy*_PublicFunction``,
  where the star is something appropriate.

* Public Macros should have a ``NPY_`` prefix and then use upper case,
  for example, ``NPY_DOUBLE``.

* Private functions should be lower case with underscores, for example:
  ``array_real_get``. Single leading underscores should not be used, but
  some current function names violate that rule due to historical accident.

.. note::

   Functions whose names begin with a single underscore should be renamed at
   some point.


Function documentation
======================

NumPy doesn't have a C function documentation standard at this time, but
needs one. Most NumPy functions are not documented in the code, and that
should change. One possibility is Doxygen with a plugin so that the same
NumPy style used for Python functions can also be used for documenting
C functions, see the files in ``doc/cdoc/``.


Related Work
------------

Based on Van Rossum and Warsaw, :pep:`7`


Discussion
----------

https://github.com/numpy/numpy/issues/11911
recommended that this proposal, which originated as ``doc/C_STYLE_GUIDE.rst.txt``,
be turned into an NEP.


Copyright
---------

This document has been placed in the public domain.
.. _NEP06:

===================================================
NEP 6 — Replacing Trac with a different bug tracker
===================================================

:Author: David Cournapeau, Stefan van der Walt
:Status: Deferred

Some release managers of both numpy and scipy are becoming more and more
dissatisfied with the current development workflow, in particular for bug
tracking. This document is a tentative to explain some problematic scenario,
current trac limitations, and what can be done about it.

Scenario
========

new release
-----------

The workflow for a release is roughly as follows:

	* find all known regressions from last release, and fix them

        * get an idea of all bugs reported since last release

        * triage bugs in regressions/blocker issues/etc..., and assign them in
          the according roadmap, subpackage and maintainers

	* pinging subpackage maintainers

Most of those tasks are quite inefficient in the current trac as used on scipy:

        * it is hard to keep track of issues. In particular, every time one goes
          to trac, we don't really know what's new from what's not. If you
          think of issues as emails, the current situation would be like not
          having read/unread feature.

        * Batch handling of issues: changing characteristics of several issues
          at the same time is difficult, because the only available UI is
          web-based. Command-line based UI are much more efficient for this
          kind of scenario

More generally, making useful reports is very awkward with the currently
deployed trac. Trac 0.11 may solve of those problems, but it has to be much
better than the actually deployed version on scipy website. Finding issues with
patches, old patches, etc... and making reports has to be much more streamlined
that it is now.

subcomponent maintainer
-----------------------

Say you are the maintainer of scipy.foo, then you are mostly interested in
getting bugs concerning scipy.foo only. But it should be easy for the general
team to follow your work - it should also be easy for casual users (e.g. not
developers) to follow some new features development pace.

Review, newcoming code
----------------------

The goal is simple: make the bar as low as possible, and make sure people know
what to do at every step to contribute to numpy or scipy:

        * Right now, patches languish for too long in trac. Of course, lack of
          time is one big reason; but the process of following new contributes
          could be made much simpler

        * It should be possible to be pinged only for reviews one a subset of
          numpy/scipy.

        * It should be possible for people interested in the patches to follow
          its progression. Comments, but also 'mini' timelines could be useful,
          particularly for massive issues (massive from a coding POV).

Current trac limitation
=======================

Note: by trac, we mean the currently deployed one. Some more recent versions
may solve some of the issues.

        * Multi-project support: we have three trac instances, one for scipy,
          one for numpy, one for scikits. Creating accounts, maintaining and
          updating each of them is a maintenance burden. Nobody likes to do
          this kind of work, so anything which can reduce the burden is a plus.
          Also, it happens quite frequently that a bug against numpy is filled
          on scipy trac and vice and versa. You have to handle this manually,
          currently.

        * Clients not based on the web-ui. This can be made through the xmlrpc
          plugin + some clients. In particular, something like
          http://tracexplorer.devjavu.com/ can be interesting for people who
          like IDE. At least one person expressed his desire to have as much
          integration as possible with Eclipse.

        * Powerful queries: it should be possible to quickly find issues
          between two releases, the new issues from a given date, issues with
          patch, issues waiting for reviews, etc... The issues data have to be
          customizable, because most bug-tracker do not support things like
          review, etc... so we need to handle this ourselves (through tags,
          etc...)

        * Marking issues as read/unread. It should also be possible for any
          user to 'mask' issues to ignore them.

        * ticket dependency. This is quite helpful in my experience for big
          features which can be split into several issues. Roadmap can only be
          created by trac admin, and they are kind of heavy-weight.

Possible candidates
===================

Updated trac + plugins
----------------------

Pros:

        * Same system

        * In python, so we can hack it if we want

Cons:

        * Trac is aimed at being basic, and extended with plugins. But most
          plugins are broken, or not up to date. The information on which
          plugins are mature is not easily available.

        * At least the scipy.org trac was slow, and needed to be restarted
          constantly. This is simply not acceptable.

Redmine
-------

Pros:

        * Support most features (except xmlrpc ?). Multi-project, etc...

        * (subjective): I (cdavid) find the out-of-the-box experience with
          redmine much more enjoyable. More information is available easily,
          less clicks, more streamlined. See
          http://www.redmine.org/wiki/redmine/TheyAreUsingRedmine for examples

        * Conversion scripts from trac (no experience with it yet for numpy/scipy).

        * Community seems friendly and gets a lof of features done

Cons:

        * new system, less mature ?

        * in Ruby: since we are a python project, most of dev are familiar with
          python.

        * Wiki integration, etc... ?

Unknown:

        * xmlrpc API
        * performances
        * maintenance cost

Roundup
-------

TODO
.. _NEP44:

===================================================
NEP 44 — Restructuring the NumPy documentation
===================================================

:Author: Ralf Gommers
:Author: Melissa Mendonça
:Author: Mars Lee
:Status: Accepted
:Type: Process
:Created: 2020-02-11
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2020-March/080467.html

Abstract
========

This document proposes a restructuring of the NumPy Documentation, both in form
and content, with the goal of making it more organized and discoverable for
beginners and experienced users.

Motivation and Scope
====================

See `here <https://numpy.org/devdocs/>`_ for the front page of the latest docs.
The organization is quite confusing and illogical (e.g. user and developer docs
are mixed). We propose the following:

- Reorganizing the docs into the four categories mentioned in [1]_, namely *Tutorials*, *How Tos*, *Reference Guide* and *Explanations* (more about this below).
- Creating dedicated sections for Tutorials and How-Tos, including orientation
  on how to create new content;
- Adding an Explanations section for key concepts and techniques that require
  deeper descriptions, some of which will be rearranged from the Reference Guide.

Usage and Impact
================

The documentation is a fundamental part of any software project, especially
open source projects. In the case of NumPy, many beginners might feel demotivated
by the current structure of the documentation, since it is difficult to discover
what to learn (unless the user has a clear view of what to look for in the
Reference docs, which is not always the case).

Looking at the results of a "NumPy Tutorial" search on any search engine also
gives an idea of the demand for this kind of content. Having official high-level
documentation written using up-to-date content and techniques will certainly
mean more users (and developers/contributors) are involved in the NumPy
community.

Backward compatibility
======================

The restructuring will effectively demand a complete rewrite of links and some
of the current content. Input from the community will be useful for identifying
key links and pages that should not be broken.

Detailed description
====================

As discussed in the article [1]_, there are four categories of doc content:

- Tutorials
- How-to guides
- Explanations
- Reference guide

We propose to use those categories as the ones we use (for writing and
reviewing) whenever we add a new documentation section.

The reasoning for this is that it is clearer both for
developers/documentation writers and to users where each piece of
information should go, and the scope and tone of each document. For
example, if explanations are mixed with basic tutorials, beginners
might be overwhelmed and alienated. On the other hand, if the reference
guide contains basic how-tos, it might be difficult for experienced
users to find the information they need, quickly.

Currently, there are many blogs and tutorials on the internet about NumPy or
using NumPy. One of the issues with this is that if users search for this
information they may end up in an outdated (unofficial) tutorial before
they find the current official documentation. This can be especially
confusing, especially for beginners. Having a better infrastructure for the
documentation also aims to solve this problem by giving users high-level,
up-to-date official documentation that can be easily updated.

Status and ideas of each type of doc content
--------------------------------------------

**Reference guide**

NumPy has a quite complete reference guide. All functions are documented, most
have examples, and most are cross-linked well with *See Also* sections. Further
improving the reference guide is incremental work that can be done (and is being
done) by many people. There are, however, many explanations in the reference
guide. These can be moved to a more dedicated Explanations section on the docs.

**How-to guides**

NumPy does not have many how-to's. The subclassing and array ducktyping section
may be an example of a how-to. Others that could be added are:

- Parallelization (controlling BLAS multithreading with ``threadpoolctl``, using
  multiprocessing, random number generation, etc.)
- Storing and loading data (``.npy``/``.npz`` format, text formats, Zarr, HDF5,
  Bloscpack, etc.)
- Performance (memory layout, profiling, use with Numba, Cython, or Pythran)
- Writing generic code that works with NumPy, Dask, CuPy, pydata/sparse, etc.

**Explanations**

There is a reasonable amount of content on fundamental NumPy concepts such as
indexing, vectorization, broadcasting, (g)ufuncs, and dtypes. This could be
organized better and clarified to ensure it's really about explaining the concepts
and not mixed with tutorial or how-to like content.

There are few explanations about anything other than those fundamental NumPy
concepts. 

Some examples of concepts that could be expanded:

- Copies vs. Views;
- BLAS and other linear algebra libraries; 
- Fancy indexing.

In addition, there are many explanations in the Reference Guide, which should be
moved to this new dedicated Explanations section.

**Tutorials**

There's a lot of scope for writing better tutorials. We have a new *NumPy for
absolute beginners tutorial* [3]_ (GSoD project of Anne Bonner). In addition we
need a number of tutorials addressing different levels of experience with Python
and NumPy. This could be done using engaging data sets, ideas or stories. For
example, curve fitting with polynomials and functions in ``numpy.linalg`` could
be done with the Keeling curve (decades worth of CO2 concentration in air
measurements) rather than with synthetic random data.

Ideas for tutorials (these capture the types of things that make sense, they're
not necessarily the exact topics we propose to implement):

- Conway's game of life with only NumPy (note: already in `Nicolas Rougier's book
  <https://www.labri.fr/perso/nrougier/from-python-to-numpy/#the-game-of-life>`_)
- Using masked arrays to deal with missing data in time series measurements
- Using Fourier transforms to analyze the Keeling curve data, and extrapolate it.
- Geospatial data (e.g. lat/lon/time to create maps for every year via a stacked
  array, like `gridMet data <http://www.climatologylab.org/gridmet.html>`_)
- Using text data and dtypes (e.g. use speeches from different people, shape
  ``(n_speech, n_sentences, n_words)``)

The *Preparing to Teach* document [2]_ from the Software Carpentry Instructor
Training materials is a nice summary of how to write effective lesson plans (and
tutorials would be very similar). In addition to adding new tutorials, we also
propose a *How to write a tutorial* document, which would help users contribute
new high-quality content to the documentation.

Data sets
---------

Using interesting data in the NumPy docs requires giving all users access to
that data, either inside NumPy or in a separate package. The former is not the
best idea, since it's hard to do without increasing the size of NumPy
significantly. Even for SciPy there has so far been no consensus on this (see
`scipy PR 8707 <https://github.com/scipy/scipy/pull/8707>`_ on adding a new
``scipy.datasets`` subpackage).

So we'll aim for a new (pure Python) package, named ``numpy-datasets`` or
``scipy-datasets`` or something similar. That package can take some lessons from
how, e.g., scikit-learn ships data sets. Small data sets can be included in the
repo, large data sets can be accessed via a downloader class or function.

Related Work
============

Some examples of documentation organization in other projects:

- `Documentation for Jupyter <https://jupyter.org/documentation>`_
- `Documentation for Python <https://docs.python.org/3/>`_
- `Documentation for TensorFlow <https://www.tensorflow.org/learn>`_

These projects make the intended audience for each part of the documentation
more explicit, as well as previewing some of the content in each section. 

Implementation
==============

Currently, the `documentation for NumPy <https://numpy.org/devdocs/>`_ can be
confusing, especially for beginners. Our proposal is to reorganize the docs in
the following structure:

- For users:
    - Absolute Beginners Tutorial
    - main Tutorials section
    - How Tos for common tasks with NumPy
    - Reference Guide (API Reference)
    - Explanations
    - F2Py Guide
    - Glossary
- For developers/contributors:
    - Contributor's Guide
    - Under-the-hood docs
    - Building and extending the documentation
    - Benchmarking 
    - NumPy Enhancement Proposals
- Meta information
    - Reporting bugs
    - Release Notes
    - About NumPy
    - License

Ideas for follow-up
-------------------

Besides rewriting the current documentation to some extent, it would be ideal
to have a technical infrastructure that would allow more contributions from the
community. For example, if Jupyter Notebooks could be submitted as-is as
tutorials or How-Tos, this might create more contributors and broaden the NumPy
community.

Similarly, if people could download some of the documentation in Notebook
format, this would certainly mean people would use less outdated material for
learning NumPy.

It would also be interesting if the new structure for the documentation makes
translations easier.
      
Discussion
==========

Discussion around this NEP can be found on the NumPy mailing list:

- https://mail.python.org/pipermail/numpy-discussion/2020-February/080419.html

References and Footnotes
========================

.. [1] `What nobody tells you about documentation <https://www.divio.com/blog/documentation/>`_

.. [2] `Preparing to Teach <https://carpentries.github.io/instructor-training/15-lesson-study/index.html>`_ (from the `Software Carpentry <https://software-carpentry.org/>`_ Instructor Training materials)

.. [3] `NumPy for absolute beginners Tutorial <https://numpy.org/devdocs/user/absolute_beginners.html>`_ by Anne Bonner

Copyright
=========

This document has been placed in the public domain.
.. _NEP38:

=============================================================
NEP 38 — Using SIMD optimization instructions for performance
=============================================================

:Author: Sayed Adel, Matti Picus, Ralf Gommers
:Status: Accepted
:Type: Standards
:Created: 2019-11-25
:Resolution: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/PVWJ74UVBRZ5ZWF6MDU7EUSJXVNILAQB/#PVWJ74UVBRZ5ZWF6MDU7EUSJXVNILAQB


Abstract
--------

While compilers are getting better at using hardware-specific routines to
optimize code, they sometimes do not produce optimal results. Also, we would
like to be able to copy binary optimized C-extension modules from one machine
to another with the same base architecture (x86, ARM, or PowerPC) but with
different capabilities without recompiling.

We have a mechanism in the ufunc machinery to `build alternative loops`_
indexed by CPU feature name. At import (in ``InitOperators``), the loop
function that matches the run-time CPU info `is chosen`_ from the candidates.This
NEP proposes a mechanism to build on that for many more features and
architectures.  The steps proposed are to:

- Establish a set of well-defined, architecture-agnostic, universal intrisics
  which capture features available across architectures.
- Capture these universal intrisics in a set of C macros and use the macros
  to build code paths for sets of features from the baseline up to the maximum
  set of features available on that architecture. Offer these as a limited
  number of compiled alternative code paths.
- At runtime, discover which CPU features are available, and choose from among
  the possible code paths accordingly.


Motivation and Scope
--------------------

Traditionally NumPy has depended on compilers to generate optimal code
specifically for the target architecture.
However few users today compile NumPy locally for their machines. Most use the
binary packages which must provide run-time support for the lowest-common
denominator CPU architecture. Thus NumPy cannot take advantage of 
more advanced features of their CPU processors, since they may not be available
on all users' systems.

Traditionally, CPU features have been exposed through `intrinsics`_ which are
compiler-specific instructions that map directly to assembly instructions.
Recently there were discussions about the effectiveness of adding more
intrinsics (e.g., `gh-11113`_ for AVX optimizations for floats).  In the past,
architecture-specific code was added to NumPy for `fast avx512 routines`_ in
various ufuncs, using the mechanism described above to choose the best loop
for the architecture. However the code is not generic and does not generalize
to other architectures.

Recently, OpenCV moved to using `universal intrinsics`_ in the Hardware
Abstraction Layer (HAL) which provided a nice abstraction for common shared
Single Instruction Multiple Data (SIMD) constructs. This NEP proposes a similar
mechanism for NumPy. There are three stages to using the mechanism:

- Infrastructure is provided in the code for abstract intrinsics. The ufunc
  machinery will be extended using sets of these abstract intrinsics, so that
  a single ufunc will be expressed as a set of loops, going from a minimal to
  a maximal set of possibly available intrinsics.
- At compile time, compiler macros and CPU detection are used to turn the
  abstract intrinsics into concrete intrinsic calls. Any intrinsics not
  available on the platform, either because the CPU does not support them
  (and so cannot be tested) or because the abstract intrinsic does not have a
  parallel concrete intrinsic on the platform will not error, rather the
  corresponding loop will not be produced and added to the set of
  possibilities.
- At runtime, the CPU detection code will further limit the set of loops
  available, and the optimal one will be chosen for the ufunc.

The current NEP proposes only to use the runtime feature detection and optimal
loop selection mechanism for ufuncs. Future NEPS may propose other uses for the
proposed solution.

The ufunc machinery already has the ability to select an optimal loop for
specifically available CPU features at runtime, currently used for ``avx2``,
``fma`` and ``avx512f`` loops (in the generated ``__umath_generated.c`` file);
universal intrinsics would extend the generated code to include more loop
variants.

Usage and Impact
----------------

The end user will be able to get a list of intrinsics available for their
platform and compiler. Optionally,
the user may be able to specify which of the loops available at runtime will be
used, perhaps via an environment variable to enable benchmarking the impact of
the different loops. There should be no direct impact to naive end users, the
results of all the loops should be identical to within a small number (1-3?)
ULPs. On the other hand, users with more powerful machines should notice a
significant performance boost.

Binary releases - wheels on PyPI and conda packages
```````````````````````````````````````````````````

The binaries released by this process will be larger since they include all
possible loops for the architecture. Some packagers may prefer to limit the
number of loops in order to limit the size of the binaries, we would hope they
would still support a wide range of families of architectures. Note this
problem already exists in the Intel MKL offering, where the binary package
includes an extensive set of alternative shared objects (DLLs) for various CPU
alternatives.

Source builds
`````````````

See "Detailed Description" below. A source build where the packager knows
details of the target machine could theoretically produce a smaller binary by
choosing to compile only the loops needed by the target via command line
arguments.

How to run benchmarks to assess performance benefits
````````````````````````````````````````````````````

Adding more code which use intrinsics will make the code harder to maintain.
Therefore, such code should only be added if it yields a significant
performance benefit. Assessing this performance benefit can be nontrivial.
To aid with this, the implementation for this NEP will add a way to select
which instruction sets can be used at *runtime* via environment variables.
(name TBD). This ablility is critical for CI code verification.


Diagnostics
```````````

A new dictionary ``__cpu_features__`` will be available to python. The keys are
the available features, the value is a boolean whether the feature is available
or not. Various new private
C functions will be used internally to query available features. These
might be exposed via specific c-extension modules for testing.


Workflow for adding a new CPU architecture-specific optimization
````````````````````````````````````````````````````````````````

NumPy will always have a baseline C implementation for any code that may be
a candidate for SIMD vectorization.  If a contributor wants to add SIMD
support for some architecture (typically the one of most interest to them),
this comment is the beginning of a tutorial on how to do so:
https://github.com/numpy/numpy/pull/13516#issuecomment-558859638

.. _tradeoffs:

As of this moment, NumPy has a number of ``avx512f`` and ``avx2`` and ``fma``
SIMD loops for many ufuncs. These would likely be the first candidates
to be ported to universal intrinsics. The expectation is that the new
implementation may cause a regression in benchmarks, but not increase the
size of the binary. If the regression is not minimal, we may choose to keep
the X86-specific code for that platform and use the universal intrisic code
for other platforms.

Any new PRs to implement ufuncs using intrinsics will be expected to use the
universal intrinsics. If it can be demonstrated that the use of universal
intrinsics is too awkward or is not performant enough, platform specific code
may be accepted as well. In rare cases, a single-platform only PR may be
accepted, but it would have to be examined within the framework of preferring
a solution using universal intrinsics.

The subjective criteria for accepting new loops are:

- correctness: the new code must not decrease accuracy by more than 1-3 ULPs
  even at edge points in the algorithm.
- code bloat: both source code size and especially binary size of the compiled
  wheel.
- maintainability: how readable is the code
- performance: benchmarks must show a significant performance boost

.. _new-intrinsics:

Adding a new intrinsic
~~~~~~~~~~~~~~~~~~~~~~

If a contributor wants to use a platform-specific SIMD instruction that is not
yet supported as a universal intrinsic, then:

1. It should be added as a universal intrinsic for all platforms
2. If it does not have an equivalent instruction on other platforms (e.g.
   ``_mm512_mask_i32gather_ps`` in ``AVX512``), then no universal intrinsic
   should be added and a platform-specific ``ufunc`` or a short helper function
   should be written instead. If such a helper function is used, it must be
   wrapped with the feature macros, and a reasonable non-intrinsic fallback to
   be used by default.

We expect (2) to be the exception. The contributor and maintainers should
consider whether that single-platform intrinsic is worth it compared to using
the best available universal intrinsic based implementation.

Reuse by other projects
```````````````````````

It would be nice if the universal intrinsics would be available to other
libraries like SciPy or Astropy that also build ufuncs, but that is not an
explicit goal of the first implementation of this NEP.

Backward compatibility
----------------------

There should be no impact on backwards compatibility.


Detailed description
--------------------

The CPU-specific are mapped to unversal intrinsics which are
similar for all x86 SIMD variants, ARM SIMD variants etc. For example, the
NumPy universal intrinsic ``npyv_load_u32`` maps to:

*  ``vld1q_u32`` for ARM based NEON
* ``_mm256_loadu_si256`` for x86 based AVX2 
* ``_mm512_loadu_si512`` for x86 based AVX-512

Anyone writing a SIMD loop will use the ``npyv_load_u32`` macro instead of the
architecture specific intrinsic. The code also supplies guard macros for
compilation and runtime, so that the proper loops can be chosen.

Two new build options are available to ``runtests.py`` and ``setup.py``:
``--cpu-baseline`` and ``--cpu-dispatch``.
The absolute minimum required features to compile are defined by
``--cpu-baseline``.  For instance, on ``x86_64`` this defaults to ``SSE3``. The
minimum features will be enabled if the compiler support it. The
set of additional intrinsics that can be detected and used as sets of
requirements to dispatch on are set by ``--cpu-dispatch``. For instance, on
``x86_64`` this defaults to ``[SSSE3, SSE41, POPCNT, SSE42, AVX, F16C, XOP,
FMA4, FMA3, AVX2, AVX512F, AVX512CD, AVX512_KNL, AVX512_KNM, AVX512_SKX,
AVX512_CLX, AVX512_CNL, AVX512_ICL]``. These features are all mapped to a
c-level boolean array ``npy__cpu_have``, and a c-level convenience function
``npy_cpu_have(int feature_id)`` queries this array, and the results are stored
in ``__cpu_features__`` at runtime.

When importing the ufuncs, the available compiled loops' required features are
matched to the ones discovered. The loop with the best match is marked to be
called by the ufunc.

Related Work
------------

- `Pixman`_ is the library used by Cairo and X to manipulate pixels. It uses
  a technique like the one described here to fill a structure with function
  pointers at runtime. These functions are similar to ufunc loops.
- `Eigen`_ is a C++ template library for linear algebra: matrices, vectors,
  numerical solvers, and related algorithms. It is a higher level-abstraction
  than the intrinsics discussed here.
- `xsimd`_ is a header-only C++ library for x86 and ARM that implements the
  mathematical functions used in the algorithms of ``boost.SIMD``.
- `Simd`_ is a high-level image processing and machine learning library with
  optimizations for different platforms.
- OpenCV used to have the one-implementation-per-architecture design, but more
  recently moved to a design that is quite similar to what is proposed in this
  NEP. The top-level `dispatch code`_ includes a `generic header`_ that is
  `specialized at compile time`_ by the CMakefile system.
- `VOLK`_ is a GPL3 library used by gnuradio and others to abstract SIMD
  intrinsics. They offer a set of high-level operations which have been
  optimized for each architecture.
- The C++ Standards Committee has proposed `class templates`_ for portable
  SIMD programming via vector types, and `namespaces`_ for the templates.

Implementation
--------------

Current PRs:

- `gh-13421 improve runtime detection of CPU features <https://github.com/numpy/numpy/pull/13421>`_
- `gh-13516: enable multi-platform SIMD compiler optimizations <https://github.com/numpy/numpy/pull/13516>`_

The compile-time and runtime code infrastructure are supplied by the first PR.
The second adds a demonstration of use of the infrastructure for a loop. Once
the NEP is approved, more work is needed to write loops using the machnisms
provided by the NEP.

Alternatives
------------

A proposed alternative in gh-13516_ is to implement loops for each CPU
architecture separately by hand, without trying to abstract common patterns in
the SIMD intrinsics (e.g., have `loops.avx512.c.src`, `loops.avx2.c.src`,
`loops.sse.c.src`, `loops.vsx.c.src`, `loops.neon.c.src`, etc.). This is more
similar to what PIXMAX does. There's a lot of duplication here though, and the
manual code duplication requires a champion who will be dedicated to
implementing and maintaining that platform's loop code.


Discussion
----------

Most of the discussion took place on the PR `gh-15228`_ to accept this NEP.
Discussion on the mailing list mentioned `VOLK`_ which was added to
the section on related work. The question of maintainability also was raised
both on the mailing list and in `gh-15228`_ and resolved as follows:

- If contributors want to leverage a specific SIMD instruction, will they be
  expected to add software implementation of this instruction for all other
  architectures too? (see the `new-intrinsics`_ part of the workflow).
- On whom does the burden lie to verify the code and benchmarks for all
  architectures? What happens if adding a universal ufunc in place of
  architecture-specific code helps one architecture but harms performance
  on another? (answered in the tradeoffs_ part of the workflow).

References and Footnotes
------------------------

.. _`build alternative loops`: https://github.com/numpy/numpy/blob/v1.17.4/numpy/core/code_generators/generate_umath.py#L50
.. _`is chosen`: https://github.com/numpy/numpy/blob/v1.17.4/numpy/core/code_generators/generate_umath.py#L1038
.. _`gh-11113`: https://github.com/numpy/numpy/pull/11113
.. _`gh-15228`: https://github.com/numpy/numpy/pull/15228
.. _`gh-13516`: https://github.com/numpy/numpy/pull/13516
.. _`fast avx512 routines`: https://github.com/numpy/numpy/pulls?q=is%3Apr+avx512+is%3Aclosed

.. [1] Each NEP must either be explicitly labeled as placed in the public domain (see
   this NEP as an example) or licensed under the `Open Publication License`_.

.. _Open Publication License: https://www.opencontent.org/openpub/

.. _`xsimd`: https://xsimd.readthedocs.io/en/latest/
.. _`Pixman`: https://gitlab.freedesktop.org/pixman
.. _`VOLK`: https://www.libvolk.org/doxygen/index.html
.. _`Eigen`: http://eigen.tuxfamily.org/index.php?title=Main_Page
.. _`Simd`: https://github.com/ermig1979/Simd
.. _`dispatch code`: https://github.com/opencv/opencv/blob/4.1.2/modules/core/src/arithm.dispatch.cpp
.. _`generic header`: https://github.com/opencv/opencv/blob/4.1.2/modules/core/src/arithm.simd.hpp
.. _`specialized at compile time`: https://github.com/opencv/opencv/blob/4.1.2/modules/core/CMakeLists.txt#L3-#L13
.. _`intrinsics`: https://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-intrinsics
.. _`universal intrinsics`: https://docs.opencv.org/master/df/d91/group__core__hal__intrin.html
.. _`class templates`: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0214r8.pdf
.. _`namespaces`: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/n4808.pdf

Copyright
---------

This document has been placed in the public domain. [1]_
===================================
NEP 49 — Data allocation strategies
===================================

:Author: Matti Picus
:Status: Final
:Type: Standards Track
:Created: 2021-04-18
:Resolution: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/YZ3PNTXZUT27B6ITFAD3WRSM3T3SRVK4/#PKYXCTG4R5Q6LIRZC4SEWLNBM6GLRF26


Abstract
--------

The ``numpy.ndarray`` requires additional memory allocations
to hold ``numpy.ndarray.strides``, ``numpy.ndarray.shape`` and
``numpy.ndarray.data`` attributes. These attributes are specially allocated
after creating the python object in ``__new__`` method.

This NEP proposes a mechanism to override the memory management strategy used
for ``ndarray->data`` with user-provided alternatives. This allocation holds
the data and can be very large. As accessing this data often becomes
a performance bottleneck, custom allocation strategies to guarantee data
alignment or pinning allocations to specialized memory hardware can enable
hardware-specific optimizations. The other allocations remain unchanged.

Motivation and Scope
--------------------

Users may wish to override the internal data memory routines with ones of their
own. Two such use-cases are to ensure data alignment and to pin certain
allocations to certain NUMA cores. This desire for alignment was discussed
multiple times on the mailing list `in 2005`_,  and in `issue 5312`_ in 2014,
which led to `PR 5457`_ and more mailing list discussions here_ `and here`_. In
a comment on the issue `from 2017`_, a user described how 64-byte alignment
improved performance by 40x.

Also related is `issue 14177`_ around the use of ``madvise`` and huge pages on
Linux.

Various tracing and profiling libraries like filprofiler_ or `electric fence`_
override ``malloc``.

The long CPython discussion of `BPO 18835`_  began with discussing the need for
``PyMem_Alloc32`` and ``PyMem_Alloc64``.  The early conclusion was that the
cost (of wasted padding) vs. the benefit of aligned memory is best left to the
user, but then evolves into a discussion of various proposals to deal with
memory allocations, including `PEP 445`_ `memory interfaces`_ to
``PyTraceMalloc_Track`` which apparently was explicitly added for NumPy.

Allowing users to implement different strategies via the NumPy C-API will
enable exploration of this rich area of possible optimizations. The intention
is to create a flexible enough interface without burdening normative users.

.. _`issue 5312`: https://github.com/numpy/numpy/issues/5312
.. _`from 2017`: https://github.com/numpy/numpy/issues/5312#issuecomment-315234656
.. _`in 2005`: https://numpy-discussion.scipy.narkive.com/MvmMkJcK/numpy-arrays-data-allocation-and-simd-alignement
.. _`here`: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/YPC5BGPUMKT2MLBP6O3FMPC35LFM2CCH/#YPC5BGPUMKT2MLBP6O3FMPC35LFM2CCH
.. _`and here`: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/IQK3EPIIRE3V4BPNAMJ2ZST3NUG2MK2A/#IQK3EPIIRE3V4BPNAMJ2ZST3NUG2MK2A
.. _`issue 14177`: https://github.com/numpy/numpy/issues/14177
.. _`filprofiler`: https://github.com/pythonspeed/filprofiler/blob/master/design/allocator-overrides.md
.. _`electric fence`: https://github.com/boundarydevices/efence
.. _`memory interfaces`: https://docs.python.org/3/c-api/memory.html#customize-memory-allocators
.. _`BPO 18835`: https://bugs.python.org/issue18835
.. _`PEP 445`: https://www.python.org/dev/peps/pep-0445/

Usage and Impact
----------------

The new functions can only be accessed via the NumPy C-API. An example is
included later in this NEP. The added ``struct`` will increase the size of the
``ndarray`` object. It is a necessary price to pay for this approach. We
can be reasonably sure that the change in size will have a minimal impact on
end-user code because NumPy version 1.20 already changed the object size.

The implementation preserves the use of ``PyTraceMalloc_Track`` to track
allocations already present in NumPy.

Backward compatibility
----------------------

The design will not break backward compatibility. Projects that were assigning
to the ``ndarray->data`` pointer were already breaking the current memory
management strategy and should restore
``ndarray->data`` before calling ``Py_DECREF``. As mentioned above, the change
in size should not impact end-users.

Detailed description
--------------------

High level design
=================

Users who wish to change the NumPy data memory management routines will use
:c:func:`PyDataMem_SetHandler`, which uses a :c:type:`PyDataMem_Handler`
structure to hold pointers to functions used to manage the data memory. In
order to allow lifetime management of the ``context``, the structure is wrapped
in a ``PyCapsule``.

Since a call to ``PyDataMem_SetHandler`` will change the default functions, but
that function may be called during the lifetime of an ``ndarray`` object, each
``ndarray`` will carry with it the ``PyDataMem_Handler``-wrapped PyCapsule used
at the time of its instantiation, and these will be used to reallocate or free
the data memory of the instance. Internally NumPy may use ``memcpy`` or
``memset`` on the pointer to the data memory.

The name of the handler will be exposed on the python level via a
``numpy.core.multiarray.get_handler_name(arr)`` function. If called as
``numpy.core.multiarray.get_handler_name()`` it will return the name of the
handler that will be used to allocate data for the next new `ndarrray`.

The version of the handler will be exposed on the python level via a
``numpy.core.multiarray.get_handler_version(arr)`` function. If called as
``numpy.core.multiarray.get_handler_version()`` it will return the version of the
handler that will be used to allocate data for the next new `ndarrray`.

The version, currently 1, allows for future enhancements to the
``PyDataMemAllocator``. If fields are added, they must be added to the end.


NumPy C-API functions
=====================

.. c:type:: PyDataMem_Handler

    A struct to hold function pointers used to manipulate memory

    .. code-block:: c

        typedef struct {
            char name[127];  /* multiple of 64 to keep the struct aligned */
            uint8_t version; /* currently 1 */
            PyDataMemAllocator allocator;
        } PyDataMem_Handler;

    where the allocator structure is

    .. code-block:: c

        /* The declaration of free differs from PyMemAllocatorEx */
        typedef struct {
            void *ctx;
            void* (*malloc) (void *ctx, size_t size);
            void* (*calloc) (void *ctx, size_t nelem, size_t elsize);
            void* (*realloc) (void *ctx, void *ptr, size_t new_size);
            void (*free) (void *ctx, void *ptr, size_t size);
        } PyDataMemAllocator;

    The use of a ``size`` parameter in ``free`` differentiates this struct from
    the :c:type:`PyMemAllocatorEx` struct in Python. This call signature is
    used internally in NumPy currently, and also in other places for instance
    `C++98 <https://en.cppreference.com/w/cpp/memory/allocator/deallocate>`,
    `C++11 <https://en.cppreference.com/w/cpp/memory/allocator_traits/deallocate>`, and
    `Rust (allocator_api) <https://doc.rust-lang.org/std/alloc/trait.Allocator.html#tymethod.deallocate>`.

    The consumer of the `PyDataMemAllocator` interface must keep track of ``size`` and make sure it is
    consistent with the parameter passed to the ``(m|c|re)alloc``  functions.

    NumPy itself may violate this requirement when the shape of the requested
    array contains a ``0``, so authors of PyDataMemAllocators should relate to
    the ``size`` parameter as a best-guess. Work to fix this is ongoing in PRs
    15780_ and 15788_ but has not yet been resolved. When it is this NEP should
    be revisited.

.. c:function:: PyObject * PyDataMem_SetHandler(PyObject *handler)

   Sets a new allocation policy. If the input value is ``NULL``, will reset
   the policy to the default. Return the previous policy, or
   return NULL if an error has occurred. We wrap the user-provided
   so they will still call the Python and NumPy memory management callback
   hooks. All the function pointers must be filled in, ``NULL`` is not
   accepted.

.. c:function:: const PyObject * PyDataMem_GetHandler()

   Return the current policy that will be used to allocate data for the
   next ``PyArrayObject``. On failure, return ``NULL``.

``PyDataMem_Handler`` thread safety and lifetime
================================================
The active handler is stored in the current :py:class:`~contextvars.Context`
via a :py:class:`~contextvars.ContextVar`. This ensures it can be configured both
per-thread and per-async-coroutine.

There is currently no lifetime management of ``PyDataMem_Handler``.
The user of `PyDataMem_SetHandler` must ensure that the argument remains alive
for as long as any objects allocated with it, and while it is the active handler.
In practice, this means the handler must be immortal.

As an implementation detail, currently this ``ContextVar`` contains a ``PyCapsule``
object storing a pointer to a ``PyDataMem_Handler`` with no destructor,
but this should not be relied upon.

Sample code
===========

This code adds a 64-byte header to each ``data`` pointer and stores information
about the allocation in the header. Before calling ``free``, a check ensures
the ``sz`` argument is correct.

.. code-block:: c

    #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION
    #include <numpy/arrayobject.h>
    NPY_NO_EXPORT void *

    typedef struct {
        void *(*malloc)(size_t);
        void *(*calloc)(size_t, size_t);
        void *(*realloc)(void *, size_t);
        void (*free)(void *);
    } Allocator;

    NPY_NO_EXPORT void *
    shift_alloc(Allocator *ctx, size_t sz) {
        char *real = (char *)ctx->malloc(sz + 64);
        if (real == NULL) {
            return NULL;
        }
        snprintf(real, 64, "originally allocated %ld", (unsigned long)sz);
        return (void *)(real + 64);
    }

    NPY_NO_EXPORT void *
    shift_zero(Allocator *ctx, size_t sz, size_t cnt) {
        char *real = (char *)ctx->calloc(sz + 64, cnt);
        if (real == NULL) {
            return NULL;
        }
        snprintf(real, 64, "originally allocated %ld via zero",
                 (unsigned long)sz);
        return (void *)(real + 64);
    }

    NPY_NO_EXPORT void
    shift_free(Allocator *ctx, void * p, npy_uintp sz) {
        if (p == NULL) {
            return ;
        }
        char *real = (char *)p - 64;
        if (strncmp(real, "originally allocated", 20) != 0) {
            fprintf(stdout, "uh-oh, unmatched shift_free, "
                    "no appropriate prefix\\n");
            /* Make C runtime crash by calling free on the wrong address */
            ctx->free((char *)p + 10);
            /* ctx->free(real); */
        }
        else {
            npy_uintp i = (npy_uintp)atoi(real +20);
            if (i != sz) {
                fprintf(stderr, "uh-oh, unmatched shift_free"
                        "(ptr, %ld) but allocated %ld\\n", sz, i);
                /* This happens when the shape has a 0, only print */
                ctx->free(real);
            }
            else {
                ctx->free(real);
            }
        }
    }

    NPY_NO_EXPORT void *
    shift_realloc(Allocator *ctx, void * p, npy_uintp sz) {
        if (p != NULL) {
            char *real = (char *)p - 64;
            if (strncmp(real, "originally allocated", 20) != 0) {
                fprintf(stdout, "uh-oh, unmatched shift_realloc\\n");
                return realloc(p, sz);
            }
            return (void *)((char *)ctx->realloc(real, sz + 64) + 64);
        }
        else {
            char *real = (char *)ctx->realloc(p, sz + 64);
            if (real == NULL) {
                return NULL;
            }
            snprintf(real, 64, "originally allocated "
                     "%ld  via realloc", (unsigned long)sz);
            return (void *)(real + 64);
        }
    }

    static Allocator new_handler_ctx = {
        malloc,
        calloc,
        realloc,
        free
    };

    static PyDataMem_Handler new_handler = {
        "secret_data_allocator",
        1,
        {
            &new_handler_ctx,
            shift_alloc,      /* malloc */
            shift_zero, /* calloc */
            shift_realloc,      /* realloc */
            shift_free       /* free */
        }
    };

Related Work
------------

This NEP is being tracked by the pnumpy_ project and a `comment in the PR`_
mentions use in orchestrating FPGA DMAs.

Implementation
--------------

This NEP has been implemented in `PR  17582`_.

Alternatives
------------

These were discussed in `issue 17467`_. `PR 5457`_  and `PR 5470`_ proposed a
global interface for specifying aligned allocations.

``PyArray_malloc_aligned`` and friends were added to NumPy with the
`numpy.random` module API refactor. and are used there for performance.

`PR 390`_ had two parts: expose ``PyDataMem_*`` via the NumPy C-API, and a hook
mechanism. The PR was merged with no example code for using these features.

Discussion
----------

The discussion on the mailing list led to the ``PyDataMemAllocator`` struct
with a ``context`` field like :c:type:`PyMemAllocatorEx` but with a different
signature for ``free``.


References and Footnotes
------------------------

.. [1] Each NEP must either be explicitly labeled as placed in the public domain (see
   this NEP as an example) or licensed under the `Open Publication License`_.

.. _Open Publication License: https://www.opencontent.org/openpub/

.. _`PR 17582`: https://github.com/numpy/numpy/pull/17582
.. _`PR 5457`: https://github.com/numpy/numpy/pull/5457
.. _`PR 5470`: https://github.com/numpy/numpy/pull/5470
.. _`15780`: https://github.com/numpy/numpy/pull/15780
.. _`15788`: https://github.com/numpy/numpy/pull/15788
.. _`PR 390`: https://github.com/numpy/numpy/pull/390
.. _`issue 17467`: https://github.com/numpy/numpy/issues/17467
.. _`comment in the PR`: https://github.com/numpy/numpy/pull/17582#issuecomment-809145547
.. _pnumpy: https://quansight.github.io/pnumpy/stable/index.html

Copyright
---------

This document has been placed in the public domain. [1]_
=================================
NEP X — Template and instructions
=================================

:Author: <list of authors' real names and optionally, email addresses>
:Status: <Draft | Active | Accepted | Deferred | Rejected | Withdrawn | Final | Superseded>
:Type: <Standards Track | Process>
:Created: <date created on, in yyyy-mm-dd format>
:Resolution: <url> (required for Accepted | Rejected | Withdrawn)


Abstract
--------

The abstract should be a short description of what the NEP will achieve.

Note that the — in the title is an elongated dash, not -.

Motivation and Scope
--------------------

This section describes the need for the proposed change. It should describe
the existing problem, who it affects, what it is trying to solve, and why.
This section should explicitly address the scope of and key requirements for
the proposed change.

Usage and Impact
----------------

This section describes how users of NumPy will use features described in this
NEP. It should be comprised mainly of code examples that wouldn't be possible
without acceptance and implementation of this NEP, as well as the impact the
proposed changes would have on the ecosystem. This section should be written
from the perspective of the users of NumPy, and the benefits it will provide
them; and as such, it should include implementation details only if
necessary to explain the functionality.

Backward compatibility
----------------------

This section describes the ways in which the NEP breaks backward compatibility.

The mailing list post will contain the NEP up to and including this section.
Its purpose is to provide a high-level summary to users who are not interested
in detailed technical discussion, but may have opinions around, e.g., usage and
impact.

Detailed description
--------------------

This section should provide a detailed description of the proposed change.
It should include examples of how the new functionality would be used,
intended use-cases and pseudo-code illustrating its use.


Related Work
------------

This section should list relevant and/or similar technologies, possibly in other
libraries. It does not need to be comprehensive, just list the major examples of
prior and relevant art.


Implementation
--------------

This section lists the major steps required to implement the NEP.  Where
possible, it should be noted where one step is dependent on another, and which
steps may be optionally omitted.  Where it makes sense, each step should
include a link to related pull requests as the implementation progresses.

Any pull requests or development branches containing work on this NEP should
be linked to from here.  (A NEP does not need to be implemented in a single
pull request if it makes sense to implement it in discrete phases).


Alternatives
------------

If there were any alternative solutions to solving the same problem, they should
be discussed here, along with a justification for the chosen approach.


Discussion
----------

This section may just be a bullet list including links to any discussions
regarding the NEP:

- This includes links to mailing list threads or relevant GitHub issues.


References and Footnotes
------------------------

.. [1] Each NEP must either be explicitly labeled as placed in the public domain (see
   this NEP as an example) or licensed under the `Open Publication License`_.

.. _Open Publication License: https://www.opencontent.org/openpub/


Copyright
---------

This document has been placed in the public domain. [1]_
==================
NEP 36 — Fair play
==================

:Author: Stéfan van der Walt <stefanv@berkeley.edu>
:Status: Accepted
:Type: Informational
:Created: 2019-10-24
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2021-June/081890.html


Abstract
--------

This document sets out Rules of Play for companies and outside
developers that engage with the NumPy project. It covers:

- Restrictions on use of the NumPy name
- How and whether to publish a modified distribution
- How to make us aware of patched versions

Companies and developers will know after reading this NEP what kinds
of behavior the community would like to see, and which we consider
troublesome, bothersome, and unacceptable.

Motivation
----------

Every so often, we learn of NumPy versions modified and circulated by outsiders.
These patched versions can cause problems for the NumPy community
(see, e.g., [#erf]_ and [#CVE-2019-6446]_).
When issues like these arise, our developers waste time identifying
the problematic release, locating alterations, and determining an
appropriate course of action.

In addition, packages on the Python Packaging Index are sometimes
named such that users assume they are sanctioned or maintained by
NumPy.  We wish to reduce the number of such incidents.

During a community call on `October 16th, 2019
<https://github.com/numpy/archive/blob/main/status_meetings/status-2019-10-16.md>`__
the community resolved to draft guidelines to address these matters.

.. [#erf] In December 2018, a
   `bug report <https://github.com/numpy/numpy/issues/12515>`__
   was filed against `np.erf` -- a function that didn't exist in the
   NumPy distribution.  It came to light that a company had published
   a NumPy version with an extended API footprint. After several
   months of discussion, the company agreed to make its patches
   public, and we added a label to the NumPy issue tracker to identify
   issues pertaining to that distribution.

.. [#CVE-2019-6446] After a security issue (CVE-2019-6446) was filed
   against NumPy, distributions put in their own fixes, most often by
   changing a default keyword value. As a result the NumPy API was
   inconsistent across distributions.

Scope
-----

This document aims to define a minimal set of rules that, when
followed, will be considered good-faith efforts in line with the
expectations of the NumPy developers.

Our hope is that developers who feel they need to modify NumPy will
first consider contributing to the project, or use one of several existing
mechanisms for extending our APIs and for operating on
externally defined array objects.

When in doubt, please `talk to us first
<https://numpy.org/community/>`__. We may suggest an alternative; at
minimum, we'll be prepared.

Fair play rules
---------------

1. Do not reuse the NumPy name for projects not developed by the NumPy
   community.

   At time of writing, there are only a handful of ``numpy``-named
   packages developed by the community, including ``numpy``,
   ``numpy-financial``, and ``unumpy``.  We ask that external packages not
   include the phrase ``numpy``, i.e., avoid names such as
   ``mycompany_numpy``.

   To be clear, this rule only applies to modules (package names); it
   is perfectly acceptable to have a *submodule* of your own library
   named ``mylibrary.numpy``.

   NumPy is a trademark owned by NumFOCUS.

2. Do not republish modified versions of NumPy.

   Modified versions of NumPy make it very difficult for the
   developers to address bug reports, since we typically do not know
   which parts of NumPy have been modified.

   If you have to break this rule (and we implore you not
   to!), then make it clear in the ``__version__`` tag that
   you have modified NumPy, e.g.::

     >>> print(np.__version__)
     '1.17.2+mycompany.15`

   We understand that minor patches are often required to make a
   library work inside of a distribution.  E.g., Debian may patch
   NumPy so that it searches for optimized BLAS libraries in the
   correct locations.  This is acceptable, but we ask that no
   substantive changes are made.

3. Do not extend or modify NumPy's API.

   If you absolutely have to break rule two, please do not add
   additional functions to the namespace, or modify the API of
   existing functions.  NumPy's API is already
   quite large, and we are working hard to reduce it where feasible.
   Having additional functions exposed in distributed versions is
   confusing for users and developers alike.

4. *DO* use official mechanism to engage with the API.

   Protocols such as `__array_ufunc__
   <https://numpy.org/neps/nep-0013-ufunc-overrides.html>`__ and
   `__array_function__
   <https://numpy.org/neps/nep-0018-array-function-protocol.html>`__
   were designed to help external packages interact more easily with
   NumPy.  E.g., the latter allows objects from foreign libraries to
   pass through NumPy.  We actively encourage using any of
   these "officially sanctioned" mechanisms for overriding or
   interacting with NumPy.

   If these mechanisms are deemed insufficient, please start a
   discussion on the mailing list before monkeypatching NumPy.

Questions and answers
---------------------

**Q:** We would like to distribute an optimized version of NumPy that
utilizes special instructions for our company's CPU.  You recommend
against that, so what are we to do?

**A:** Please consider including the patches required in the official
NumPy repository.  Not only do we encourage such contributions, but we
already have optimized loops for some platforms available.

**Q:** We would like to ship a much faster version of FFT than NumPy
provides, but NumPy has no mechanism for overriding its FFT routines.
How do we proceed?

**A:** There are two solutions that we approve of: let the users
install your optimizations using a piece of code, such as::

  from my_company_accel import patch_numpy_fft
  patch_numpy_fft()

or have your distribution automatically perform the above, but print a
message to the terminal clearly stating what is happening::

  We are now patching NumPy for optimal performance under MyComp
  Special Platform.  Please direct all bug reports to
  https://mycomp.com/numpy-bugs

If you require additional mechanisms for overriding code, please
discuss this with the development team on the mailing list.

**Q:** We would like to distribute NumPy with faster linear algebra
routines. Are we allowed to do this?

**A:** Yes, this is explicitly supported by linking to a different
version of BLAS.

Discussion
----------

References and footnotes
------------------------

Copyright
---------

This document has been placed in the public domain.
.. _NEP17:

================================
NEP 17 — Split out masked arrays
================================

:Author: Stéfan van der Walt <stefanv@berkeley.edu>
:Status: Rejected
:Type: Standards Track
:Created: 2018-03-22
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-May/078026.html

Abstract
--------

This NEP proposes removing MaskedArray functionality from NumPy, and
publishing it as a stand-alone package.

Detailed description
--------------------

MaskedArrays are a sub-class of the NumPy ``ndarray`` that adds
masking capabilities, i.e. the ability to ignore or hide certain array
values during computation.

While historically convenient to distribute this class inside of NumPy,
improved packaging has made it possible to distribute it separately
without difficulty.

Motivations for this move include:

 * Focus: the NumPy package should strive to only include the
   `ndarray` object, and the essential utilities needed to manipulate
   such arrays.
 * Complexity: the MaskedArray implementation is non-trivial, and imposes
   a significant maintenance burden.
 * Compatibility: MaskedArray objects, being subclasses [1]_ of `ndarrays`,
   often cause complications when being used with other packages.
   Fixing these issues is outside the scope of NumPy development.

This NEP proposes a deprecation pathway through which MaskedArrays
would still be accessible to users, but no longer as part of the core
package.

Implementation
--------------

Currently, a MaskedArray is created as follows::

  from numpy import ma
  ma.array([1, 2, 3], mask=[True, False, True])

This will return an array where the values 1 and 3 are masked (no
longer visible to operations such as `np.sum`).

We propose refactoring the `np.ma` subpackage into a new
pip-installable library called `maskedarray` [2]_, which would be used
in a similar fashion::

  import maskedarray as ma
  ma.array([1, 2, 3], mask=[True, False, True])

For two releases of NumPy, `maskedarray` would become a NumPy
dependency, and would expose MaskedArrays under the existing name,
`np.ma`.  If imported as `np.ma`, a `NumpyDeprecationWarning` will
be raised, describing the impending deprecation with instructions on
how to modify code to use `maskedarray`.

After two releases, `np.ma` will be removed entirely. In order to obtain
`np.ma`, a user will install it via `pip install` or via their package
manager. Subsequently, `importing maskedarray` on a version of NumPy that
includes it intgrally will raise an `ImportError`.

Documentation
`````````````

NumPy's internal documentation refers explicitly to MaskedArrays in
certain places, e.g. `ndarray.concatenate`:

> When one or more of the arrays to be concatenated is a MaskedArray,
> this function will return a MaskedArray object instead of an ndarray,
> but the input masks are *not* preserved. In cases where a MaskedArray
> is expected as input, use the ma.concatenate function from the masked
> array module instead.

Such documentation will be removed, since the expectation is that
users of `maskedarray` will use methods from that package to operate
on MaskedArrays.

Other appearances
~~~~~~~~~~~~~~~~~

Explicit MaskedArray support will be removed from:

- `numpygenfromtext`
- `numpy.libmerge_arrays`, `numpy.lib.stack_arrays`

Backward compatibility
----------------------

For two releases of NumPy, apart from a deprecation notice, there will
be no user visible changes.  Thereafter, `np.ma` will no longer be
available (instead, MaskedArrays will live in the `maskedarray`
package).

Note also that new PEPs on array-like objects may eventually provide
better support for MaskedArrays than is currently available.

Alternatives
------------

After a lively discussion on the mailing list:

- There is support (and active interest in) making a better *new* masked array
  class.
- The new class should be a consumer of the external NumPy API with no special
  status (unlike today where there are hacks across the codebase to support it)
- `MaskedArray` will stay where it is, at least until the new masked array
  class materializes and has been tried in the wild.

References and Footnotes
------------------------

.. [1] Subclassing ndarray,
       https://docs.scipy.org/doc/numpy/user/basics.subclassing.html
.. [2] PyPI: maskedarray, https://pypi.org/project/maskedarray/

Copyright
---------

This document has been placed in the public domain.
.. _NEP11:

==================================
NEP 11 — Deferred UFunc evaluation
==================================

:Author: Mark Wiebe <mwwiebe@gmail.com>
:Content-Type: text/x-rst
:Created: 30-Nov-2010
:Status: Deferred

********
Abstract
********

This NEP describes a proposal to add deferred evaluation to NumPy's
UFuncs.  This will allow Python expressions like
"a[:] = b + c + d + e" to be evaluated in a single pass through all
the variables at once, with no temporary arrays.  The resulting
performance will likely be comparable to the *numexpr* library,
but with a more natural syntax.

This idea has some interaction with UFunc error handling and
the UPDATEIFCOPY flag, affecting the design and implementation,
but the result allows for the usage of deferred evaluation
with minimal effort from the Python user's perspective.

**********
Motivation
**********

NumPy's style of UFunc execution causes suboptimal performance for
large expressions, because multiple temporaries are allocated and
the inputs are swept through in multiple passes.  The *numexpr* library
can outperform NumPy for such large expressions, by doing the execution
in small cache-friendly blocks, and evaluating the whole expression
per element.  This results in one sweep through each input, which
is significantly better for the cache.

For an idea of how to get this kind of behavior in NumPy without
changing the Python code, consider the C++ technique of
expression templates. These can be used to quite arbitrarily
rearrange expressions using
vectors or other data structures, example:

.. code-block:: cpp

    A = B + C + D;

can be transformed into something equivalent to:

.. code-block:: cpp

    for(i = 0; i < A.size; ++i) {
        A[i] = B[i] + C[i] + D[i];
    }

This is done by returning a proxy object that knows how to calculate
the result instead of returning the actual object.  With modern C++
optimizing compilers, the resulting machine code is often the same
as hand-written loops.  For an example of this, see the
`Blitz++ Library <http://www.oonumerics.org/blitz/docs/blitz_3.html>`_.
A more recently created library for helping write expression templates
is `Boost Proto <http://beta.boost.org/doc/libs/1_44_0/doc/html/proto.html>`_.

By using the same idea of returning a proxy object in Python, we
can accomplish the same thing dynamically.  The return object is
an ndarray without its buffer allocated, and with enough knowledge
to calculate itself when needed.  When a "deferred array" is
finally evaluated, we can use the expression tree made up of
all the operand deferred arrays, effectively creating a single new
UFunc to evaluate on the fly.


*******************
Example Python Code
*******************

Here's how it might be used in NumPy.::

    # a, b, c are large ndarrays

    with np.deferredstate(True):

        d = a + b + c
        # Now d is a 'deferred array,' a, b, and c are marked READONLY
        # similar to the existing UPDATEIFCOPY mechanism.

        print d
        # Since the value of d was required, it is evaluated so d becomes
        # a regular ndarray and gets printed.

        d[:] = a*b*c
        # Here, the automatically combined "ufunc" that computes
        # a*b*c effectively gets an out= parameter, so no temporary
        # arrays are needed whatsoever.

        e = a+b+c*d
        # Now e is a 'deferred array,' a, b, c, and d are marked READONLY

        d[:] = a
        # d was marked readonly, but the assignment could see that
        # this was due to it being a deferred expression operand.
        # This triggered the deferred evaluation so it could assign
        # the value of a to d.

There may be some surprising behavior, though.::

    with np.deferredstate(True):

        d = a + b + c
        # d is deferred

        e[:] = d
        f[:] = d
        g[:] = d
        # d is still deferred, and its deferred expression
        # was evaluated three times, once for each assignment.
        # This could be detected, with d being converted to
        # a regular ndarray the second time it is evaluated.

I believe the usage that should be recommended in the documentation
is to leave the deferred state at its default, except when
evaluating a large expression that can benefit from it.::

    # calculations

    with np.deferredstate(True):
        x = <big expression>

    # more calculations

This will avoid surprises which would be cause by always keeping
deferred usage True, like floating point warnings or exceptions
at surprising times when deferred expression are used later.
User questions like "Why does my print statement throw a
divide by zero error?" can hopefully be avoided by recommending
this approach.

********************************
Proposed Deferred Evaluation API
********************************

For deferred evaluation to work, the C API needs to be aware of its
existence, and be able to trigger evaluation when necessary.  The
ndarray would gain two new flag.

    ``NPY_ISDEFERRED``

        Indicates the expression evaluation for this ndarray instance
        has been deferred.

    ``NPY_DEFERRED_WASWRITEABLE``

        Can only be set when ``PyArray_GetDeferredUsageCount(arr) > 0``.
        It indicates that when ``arr`` was first used in a deferred
        expression, it was a writeable array.  If this flag is set,
        calling ``PyArray_CalculateAllDeferred()`` will make ``arr``
        writeable again.

.. note:: QUESTION

    Should NPY_DEFERRED and NPY_DEFERRED_WASWRITEABLE be visible
    to Python, or should accessing the flags from python trigger
    PyArray_CalculateAllDeferred if necessary?

The API would be expanded with a number of functions.

``int PyArray_CalculateAllDeferred()``

    This function forces all currently deferred calculations to occur.

    For example, if the error state is set to ignore all, and
    np.seterr({all='raise'}), this would change what happens
    to already deferred expressions.  Thus, all the existing
    deferred arrays should be evaluated before changing the
    error state.

``int PyArray_CalculateDeferred(PyArrayObject* arr)``

    If 'arr' is a deferred array, allocates memory for it and
    evaluates the deferred expression.  If 'arr' is not a deferred
    array, simply returns success.  Returns NPY_SUCCESS or NPY_FAILURE.

``int PyArray_CalculateDeferredAssignment(PyArrayObject* arr, PyArrayObject* out)``

    If 'arr' is a deferred array, evaluates the deferred expression
    into 'out', and 'arr' remains a deferred array.  If 'arr' is not
    a deferred array, copies its value into out.  Returns NPY_SUCCESS
    or NPY_FAILURE.

``int PyArray_GetDeferredUsageCount(PyArrayObject* arr)``

    Returns a count of how many deferred expressions use this array
    as an operand.

The Python API would be expanded as follows.

 ``numpy.setdeferred(state)``

    Enables or disables deferred evaluation. True means to always
    use deferred evaluation.  False means to never use deferred
    evaluation.  None means to use deferred evaluation if the error
    handling state is set to ignore everything.  At NumPy initialization,
    the deferred state is None.

    Returns the previous deferred state.

``numpy.getdeferred()``

    Returns the current deferred state.

``numpy.deferredstate(state)``

    A context manager for deferred state handling, similar to
    ``numpy.errstate``.


Error Handling
==============

Error handling is a thorny issue for deferred evaluation.  If the
NumPy error state is {all='ignore'}, it might be reasonable to
introduce deferred evaluation as the default, however if a UFunc
can raise an error, it would be very strange for the later 'print'
statement to throw the exception instead of the actual operation which
caused the error.

What may be a good approach is to by default enable deferred evaluation
only when the error state is set to ignore all, but allow user control with
'setdeferred' and 'getdeferred' functions.  True would mean always
use deferred evaluation, False would mean never use it, and None would
mean use it only when safe (i.e. the error state is set to ignore all).

Interaction With UPDATEIFCOPY
=============================

The ``NPY_UPDATEIFCOPY`` documentation states:

    The data area represents a (well-behaved) copy whose information
    should be transferred back to the original when this array is deleted.

    This is a special flag that is set if this array represents a copy
    made because a user required certain flags in PyArray_FromAny and a
    copy had to be made of some other array (and the user asked for this
    flag to be set in such a situation). The base attribute then points
    to the “misbehaved” array (which is set read_only). When the array
    with this flag set is deallocated, it will copy its contents back to
    the “misbehaved” array (casting if necessary) and will reset the
    “misbehaved” array to NPY_WRITEABLE. If the “misbehaved” array was
    not NPY_WRITEABLE to begin with then PyArray_FromAny would have
    returned an error because NPY_UPDATEIFCOPY would not have been possible.

The current implementation of UPDATEIFCOPY assumes that it is the only
mechanism mucking with the writeable flag in this manner.  These mechanisms
must be aware of each other to work correctly.  Here's an example of how
they might go wrong:

1. Make a temporary copy of 'arr' with UPDATEIFCOPY ('arr' becomes read only)
2. Use 'arr' in a deferred expression (deferred usage count becomes one,
   NPY_DEFERRED_WASWRITEABLE is **not** set, since 'arr' is read only)
3. Destroy the temporary copy, causing 'arr' to become writeable
4. Writing to 'arr' destroys the value of the deferred expression

To deal with this issue, we make these two states mutually exclusive.

* Usage of UPDATEIFCOPY checks the ``NPY_DEFERRED_WASWRITEABLE`` flag,
  and if it's set, calls ``PyArray_CalculateAllDeferred`` to flush
  all deferred calculation before proceeding.
* The ndarray gets a new flag ``NPY_UPDATEIFCOPY_TARGET`` indicating
  the array will be updated and made writeable at some point in the
  future.  If the deferred evaluation mechanism sees this flag in
  any operand, it triggers immediate evaluation.

Other Implementation Details
============================

When a deferred array is created, it gets references to all the
operands of the UFunc, along with the UFunc itself.  The
'DeferredUsageCount' is incremented for each operand, and later
gets decremented when the deferred expression is calculated or
the deferred array is destroyed.

A global list of weak references to all the deferred arrays
is tracked, in order of creation.  When ``PyArray_CalculateAllDeferred``
gets called, the newest deferred array is calculated first.
This may release references to other deferred arrays contained
in the deferred expression tree, which then
never have to be calculated.

Further Optimization
====================

Instead of conservatively disabling deferred evaluation when any
errors are not set to 'ignore', each UFunc could give a set
of possible errors it generates.  Then, if all those errors
are set to 'ignore', deferred evaluation could be used even
if other errors are not set to ignore.

Once the expression tree is explicitly stored, it is possible to
do transformations on it.  For example add(add(a,b),c) could
be transformed into add3(a,b,c), or add(multiply(a,b),c) could
become fma(a,b,c) using the CPU fused multiply-add instruction
where available.

While I've framed deferred evaluation as just for UFuncs, it could
be extended to other functions, such as dot().  For example, chained
matrix multiplications could be reordered to minimize the size
of intermediates, or peep-hole style optimizer passes could search
for patterns that match optimized BLAS/other high performance
library calls.

For operations on really large arrays, integrating a JIT like LLVM into
this system might be a big benefit.  The UFuncs and other operations
would provide bitcode, which could be inlined together and optimized
by the LLVM optimizers, then executed.  In fact, the iterator itself
could also be represented in bitcode, allowing LLVM to consider
the entire iteration while doing its optimization.
.. _NEP15:

=====================================
NEP 15 — Merging multiarray and umath
=====================================

:Author: Nathaniel J. Smith <njs@pobox.com>
:Status: Final
:Type: Standards Track
:Created: 2018-02-22
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html

Abstract
--------

Let's merge ``numpy.core.multiarray`` and ``numpy.core.umath`` into a
single extension module, and deprecate ``np.set_numeric_ops``.


Background
----------

Currently, numpy's core C code is split between two separate extension
modules.

``numpy.core.multiarray`` is built from
``numpy/core/src/multiarray/*.c``, and contains the core array
functionality (in particular, the ``ndarray`` object).

``numpy.core.umath`` is built from ``numpy/core/src/umath/*.c``, and
contains the ufunc machinery.

These two modules each expose their own separate C API, accessed via
``import_multiarray()`` and ``import_umath()`` respectively. The idea
is that they're supposed to be independent modules, with
``multiarray`` as a lower-level layer with ``umath`` built on top. In
practice this has turned out to be problematic.

First, the layering isn't perfect: when you write ``ndarray +
ndarray``, this invokes ``ndarray.__add__``, which then calls the
ufunc ``np.add``. This means that ``ndarray`` needs to know about
ufuncs – so instead of a clean layering, we have a circular
dependency. To solve this, ``multiarray`` exports a somewhat
terrifying function called ``set_numeric_ops``. The bootstrap
procedure each time you ``import numpy`` is:

1. ``multiarray`` and its ``ndarray`` object are loaded, but
   arithmetic operations on ndarrays are broken.

2. ``umath`` is loaded.

3. ``set_numeric_ops`` is used to monkeypatch all the methods like
   ``ndarray.__add__`` with objects from ``umath``.

In addition, ``set_numeric_ops`` is exposed as a public API,
``np.set_numeric_ops``.

Furthermore, even when this layering does work, it ends up distorting
the shape of our public ABI. In recent years, the most common reason
for adding new functions to ``multiarray``\'s "public" ABI is not that
they really need to be public or that we expect other projects to use
them, but rather just that we need to call them from ``umath``. This
is extremely unfortunate, because it makes our public ABI
unnecessarily large, and since we can never remove things from it then
this creates an ongoing maintenance burden. The way C works, you can
have internal API that's visible to everything inside the same
extension module, or you can have a public API that everyone can use;
you can't (easily) have an API that's visible to multiple extension
modules inside numpy, but not to external users.

We've also increasingly been putting utility code into
``numpy/core/src/private/``, which now contains a bunch of files which
are ``#include``\d twice, once into ``multiarray`` and once into
``umath``. This is pretty gross, and is purely a workaround for these
being separate C extensions. The ``npymath`` library is also
included in both extension modules.


Proposed changes
----------------

This NEP proposes three changes:

1. We should start building ``numpy/core/src/multiarray/*.c`` and
   ``numpy/core/src/umath/*.c`` together into a single extension
   module.

2. Instead of ``set_numeric_ops``, we should use some new, private API
   to set up ``ndarray.__add__`` and friends.

3. We should deprecate, and eventually remove, ``np.set_numeric_ops``.


Non-proposed changes
--------------------

We don't necessarily propose to throw away the distinction between
multiarray/ and umath/ in terms of our source code organization:
internal organization is useful! We just want to build them together
into a single extension module. Of course, this does open the door for
potential future refactorings, which we can then evaluate based on
their merits as they come up.

It also doesn't propose that we break the public C ABI. We should
continue to provide ``import_multiarray()`` and ``import_umath()``
functions – it's just that now both ABIs will ultimately be loaded
from the same C library. Due to how ``import_multiarray()`` and
``import_umath()`` are written, we'll also still need to have modules
called ``numpy.core.multiarray`` and ``numpy.core.umath``, and they'll
need to continue to export ``_ARRAY_API`` and ``_UFUNC_API`` objects –
but we can make one or both of these modules be tiny shims that simply
re-export the magic API object from where-ever it's actually defined.
(See ``numpy/core/code_generators/generate_{numpy,ufunc}_api.py`` for
details of how these imports work.)


Backward compatibility
----------------------

The only compatibility break is the deprecation of ``np.set_numeric_ops``.


Rejected alternatives
---------------------

Preserve ``set_numeric_ops`` for monkeypatching
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In discussing this NEP, one additional use case was raised for
``set_numeric_ops``: if you have an optimized vector math library
(e.g. Intel's MKL VML, Sleef, or Yeppp), then ``set_numeric_ops`` can
be used to monkeypatch numpy to use these operations instead of
numpy's built-in vector operations. But, even if we grant that this is
a great idea, using ``set_numeric_ops`` isn't actually the best way to
do it. All ``set_numeric_ops`` allows you to do is take over Python's
syntactic operators (``+``, ``*``, etc.) on ndarrays; it doesn't let
you affect operations called via other APIs (e.g., ``np.add``), or
operations that don't have built-in syntax (e.g., ``np.exp``). Also,
you have to reimplement the whole ufunc machinery, instead of just the
core loop. On the other hand, the `PyUFunc_ReplaceLoopBySignature
<https://docs.scipy.org/doc/numpy/reference/c-api.ufunc.html#c.PyUFunc_ReplaceLoopBySignature>`__
API – which was added in 2006 – allows replacement of the inner loops
of arbitrary ufuncs. This is both simpler and more powerful – e.g.
replacing the inner loop of ``np.add`` means your code will
automatically be used for both ``ndarray + ndarray`` as well as direct
calls to ``np.add``. So this doesn't seem like a good reason to not
deprecate ``set_numeric_ops``.


Discussion
----------

* https://mail.python.org/pipermail/numpy-discussion/2018-March/077764.html
* https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html

Copyright
---------

This document has been placed in the public domain.
.. _NEP02:

=================================================================================
NEP 2 — A proposal to build numpy without warning with a big set of warning flags
=================================================================================

:Author: David Cournapeau
:Contact: david@ar.media.kyoto-u.ac.jp
:Date: 2008-09-04
:Status: Deferred

.. highlight:: c

Executive summary
=================

When building numpy and scipy, we are limited to a quite restricted set of
warning compilers, thus missing a large class of potential bugs which could be
detected with stronger warning flags. The goal of this NEP is present the
various methods used to clean the code and implement some policy to make numpy
buildable with a  bigger set of warning flags, while keeping the build warnings
free.

Warning flags
=============

Each compiler detects a different set of potential errors. The baseline will
be gcc -Wall -W -Wextra. Ideally, a complete set would be nice:

.. code-block:: bash

  -W -Wall -Wextra -Wstrict-prototypes -Wmissing-prototypes -Waggregate-return
  -Wcast-align -Wcast-qual -Wnested-externs -Wshadow -Wbad-function-cast
  -Wwrite-strings "

Intel compiler, VS with ``/W3 /Wall``, Sun compilers have extra warnings too.

Kind of warnings
================

C Python extension code tends to naturally generate a lot of spurious warnings.
The goal is to have some facilities to tag some typical C-Python code so that
the compilers do not generate warnings in those cases; the tag process has to
be clean, readable, and be robust. In particular, it should not make the code
more obscure or worse, break working code.

unused parameter
----------------

This one appears often: any python-callable C function takes two arguments,
of which the first is not used for functions (only for methods). One way to
solve it is to tag the function argument with a macro NPY_UNUSED. This macro
uses compiler specific code to tag the variable, and mangle it such as it is
not possible to use it accidentally once it is tagged.

The code to apply compiler specific option could be::

  #if defined(__GNUC__)
          #define __COMP_NPY_UNUSED __attribute__ ((__unused__))
  # elif defined(__ICC)
          #define __COMP_NPY_UNUSED __attribute__ ((__unused__))
  #else
          #define __COMP_NPY_UNUSED
  #endif

The variable mangling would be::

  #define NPY_UNUSED(x) (__NPY_UNUSED_TAGGED ## x) __COMP_NPY_UNUSED

When applied to a variable, one would get::

  int foo(int * NPY_UNUSED(dummy))

expanded to::

   int foo(int * __NPY_UNUSED_TAGGEDdummy __COMP_NPY_UNUSED)

Thus avoiding any accidental use of the variable. The mangling is pure C, and
thuse portable. The per-variable warning disabling is compiler specific.

signed/unsigned comparison
--------------------------

More tricky: not always clear what to do

half-initialized structures
---------------------------

Just put the elements with NULL in it.
.. _NEP23:

=======================================================
NEP 23 — Backwards compatibility and deprecation policy
=======================================================

:Author: Ralf Gommers <ralf.gommers@gmail.com>
:Status: Final
:Type: Process
:Created: 2018-07-14
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2021-January/081423.html


Abstract
--------

In this NEP we describe NumPy's approach to backwards compatibility,
its deprecation and removal policy, and the trade-offs and decision
processes for individual cases where breaking backwards compatibility
is considered.


Motivation and Scope
--------------------

NumPy has a very large user base.  Those users rely on NumPy being stable
and the code they write that uses NumPy functionality to keep working.
NumPy is also actively maintained and improved -- and sometimes improvements
require, or are made easier, by breaking backwards compatibility.
Finally, there are trade-offs in stability for existing users vs. avoiding
errors or having a better user experience for new users.  These competing
needs often give rise to long debates and delay accepting or rejecting
contributions.  This NEP tries to address that by providing a policy as well
as examples and rationales for when it is or isn't a good idea to break
backwards compatibility.

In addition, this NEP can serve as documentation for users about how the NumPy
project treats backwards compatibility, and the speed at which they can expect
changes to be made.

In scope for this NEP are:

- Principles of NumPy's approach to backwards compatibility.
- How to deprecate functionality, and when to remove already deprecated
  functionality.
- Decision making process for deprecations and removals.
- How to ensure that users are well informed about any change.

Out of scope are:

- Making concrete decisions about deprecations of particular functionality.
- NumPy's versioning scheme.


General principles
------------------

When considering proposed changes that are backwards incompatible, the
main principles the NumPy developers use when making a decision are:

1. Changes need to benefit more than they harm users.
2. NumPy is widely used, so breaking changes should be assumed by default to be
   harmful.
3. Decisions should be based on how they affect users and downstream packages
   and should be based on usage data where possible. It does not matter whether
   this use contradicts the documentation or best practices.
4. The possibility of an incorrect result is worse than an error or even crash.

When assessing the costs of proposed changes, keep in mind that most users do
not read the mailing list, do not notice deprecation warnings, and sometimes
wait more than one or two years before upgrading from their old version. And
that NumPy has millions of users, so "no one will do or use this" is likely
incorrect.

Benefits of proposed changes can include improved functionality, usability and
performance, as well as lower maintenance cost and improved future
extensibility.

Fixes for clear bugs are exempt from this backwards compatibility policy.
However, in case of serious impact on users even bug fixes may have to be
delayed for one or more releases. For example, if a downstream library would no
longer build or would give incorrect results.


Strategies related to deprecations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Impact assessment
`````````````````

Getting hard data on the impact of a deprecation of often difficult. Strategies
that can be used to assess such impact include:

- Use a code search engine ([1]_, [2]_) or static ([3]_) or dynamic ([4]_) code
  analysis tools to determine where and how the functionality is used.
- Test prominent downstream libraries against a development build of NumPy
  containing the proposed change to get real-world data on its impact.
- Make a change on the main branch and revert it before release if it
  causes problems.  We encourage other packages to test against
  NumPy's main branch and if that's too burdensome, then at least to
  test pre-releases. This often turns up issues quickly.

Alternatives to deprecations
````````````````````````````

If the impact is unclear or significant, it is often good to consider
alternatives to deprecations. For example, discouraging use in documentation
only, or moving the documentation for the functionality to a less prominent
place or even removing it completely. Commenting on open issues related to it
that they are low-prio or labeling them as "wontfix" will also be a signal to
users, and reduce the maintenance effort needing to be spent.


Implementing deprecations and removals
--------------------------------------

Deprecation warnings are necessary in all cases where functionality
will eventually be removed.  If there is no intent to remove functionality,
then it should not be deprecated. A "please don't use this for new code"
in the documentation or other type of warning should be used instead, and the
documentation can be organized such that the preferred alternative is more
prominently shown.

Deprecations:

- shall include the version number of the release in which the functionality
  was deprecated.
- shall include information on alternatives to the deprecated functionality, or a
  reason for the deprecation if no clear alternative is available. Note that
  release notes can include longer messages if needed.
- shall use ``DeprecationWarning`` by default, and ``VisibleDeprecation``
  for changes that need attention again after already having been deprecated or
  needing extra attention for some reason.
- shall be listed in the release notes of the release where the deprecation is
  first present.
- shall not be introduced in micro (bug fix) releases.
- shall set a ``stacklevel``, so the warning appears to come from the correct
  place.
- shall be mentioned in the documentation for the functionality. A
  ``.. deprecated::`` directive can be used for this.

Examples of good deprecation warnings (also note standard form of the comments
above the warning, helps when grepping):

.. code-block:: python

    # NumPy 1.15.0, 2018-09-02
    warnings.warn('np.asscalar(a) is deprecated since NumPy 1.16.0, use '
                  'a.item() instead', DeprecationWarning, stacklevel=3)

    # NumPy 1.15.0, 2018-02-10
    warnings.warn("Importing from numpy.testing.utils is deprecated "
                  "since 1.15.0, import from numpy.testing instead.",
                  DeprecationWarning, stacklevel=2)

    # NumPy 1.14.0, 2017-07-14
    warnings.warn(
        "Reading unicode strings without specifying the encoding "
        "argument is deprecated since NumPy 1.14.0. Set the encoding, "
        "use None for the system default.",
        np.VisibleDeprecationWarning, stacklevel=2)

.. code-block:: C

        /* DEPRECATED 2020-05-13, NumPy 1.20 */
        if (PyErr_WarnFormat(PyExc_DeprecationWarning, 1,
                matrix_deprecation_msg, ufunc->name, "first") < 0) {
            return NULL;
        }

Removal of deprecated functionality:

- shall be done after at least 2 releases assuming the current 6-monthly
  release cycle; if that changes, there shall be at least 1 year between
  deprecation and removal.
- shall be listed in the release notes of the release where the removal happened.
- can be done in any minor, but not bugfix, release.

For backwards incompatible changes that aren't "deprecate and remove" but for
which code will start behaving differently, a ``FutureWarning`` should be
used. Release notes, mentioning version number and using ``stacklevel`` should
be done in the same way as for deprecation warnings. A ``.. versionchanged::``
directive shall be used in the documentation after the behaviour change was
made to indicate when the behavior changed:

.. code-block:: python

    def argsort(self, axis=np._NoValue, ...):
        """
        Parameters
        ----------
        axis : int, optional
            Axis along which to sort. If None, the default, the flattened array
            is used.

            ..  versionchanged:: 1.13.0
                Previously, the default was documented to be -1, but that was
                in error. At some future date, the default will change to -1, as
                originally intended.
                Until then, the axis should be given explicitly when
                ``arr.ndim > 1``, to avoid a FutureWarning.
        """
        ...
        warnings.warn(
            "In the future the default for argsort will be axis=-1, not the "
            "current None, to match its documentation and np.argsort. "
            "Explicitly pass -1 or None to silence this warning.",
            MaskedArrayFutureWarning, stacklevel=3)


Decision making
---------------

In concrete cases where this policy needs to be applied, decisions are made according
to the `NumPy governance model
<https://docs.scipy.org/doc/numpy/dev/governance/index.html>`_.

All deprecations must be proposed on the mailing list in order to give everyone
with an interest in NumPy development a chance to comment. Removal of
deprecated functionality does not need discussion on the mailing list.


Functionality with more strict deprecation policies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- ``numpy.random`` has its own backwards compatibility policy with additional
  requirements on top of the ones in this NEP, see
  `NEP 19 <http://www.numpy.org/neps/nep-0019-rng-policy.html>`_.
- The file format of ``.npy`` and ``.npz`` files is strictly versioned
  independent of the NumPy version; existing format versions must remain
  backwards compatible even if a newer format version is introduced.


Example cases
-------------

We now discuss a few concrete examples from NumPy's history to illustrate
typical issues and trade-offs.

**Changing the behavior of a function**

``np.histogram`` is probably the most infamous example.
First, a new keyword ``new=False`` was introduced, this was then switched
over to None one release later, and finally it was removed again.
Also, it has a ``normed`` keyword that had behavior that could be considered
either suboptimal or broken (depending on ones opinion on the statistics).
A new keyword ``density`` was introduced to replace it; ``normed`` started giving
``DeprecationWarning`` only in v.1.15.0.  Evolution of ``histogram``::

    def histogram(a, bins=10, range=None, normed=False):  # v1.0.0

    def histogram(a, bins=10, range=None, normed=False, weights=None, new=False):  #v1.1.0

    def histogram(a, bins=10, range=None, normed=False, weights=None, new=None):  #v1.2.0

    def histogram(a, bins=10, range=None, normed=False, weights=None):  #v1.5.0

    def histogram(a, bins=10, range=None, normed=False, weights=None, density=None):  #v1.6.0

    def histogram(a, bins=10, range=None, normed=None, weights=None, density=None):  #v1.15.0
        # v1.15.0 was the first release where `normed` started emitting
        # DeprecationWarnings

The ``new`` keyword was planned from the start to be temporary.  Such a plan
forces users to change their code more than once, which is almost never the
right thing to do.  Instead, a better approach here would have been to
deprecate ``histogram`` and introduce a new function ``hist`` in its place.


**Disallowing indexing with floats**

Indexing an array with floats is asking for something ambiguous, and can be a
sign of a bug in user code.  After some discussion, it was deemed a good idea
to deprecate indexing with floats.  This was first tried for the v1.8.0
release, however in pre-release testing it became clear that this would break
many libraries that depend on NumPy.  Therefore it was reverted before release,
to give those libraries time to fix their code first.  It was finally
introduced for v1.11.0 and turned into a hard error for v1.12.0.

This change was disruptive, however it did catch real bugs in, e.g., SciPy and
scikit-learn.  Overall the change was worth the cost, and introducing it in
the main branch first to allow testing, then removing it again before
a release, is a useful strategy.

Similar deprecations that also look like good examples of
cleanups/improvements:

- removing deprecated boolean indexing (in 2016, see `gh-8312 <https://github.com/numpy/numpy/pull/8312>`__)
- deprecating truth testing on empty arrays (in 2017, see `gh-9718 <https://github.com/numpy/numpy/pull/9718>`__)


**Removing the financial functions**

The financial functions (e.g. ``np.pmt``) had short non-descriptive names, were
present in the main NumPy namespace, and didn't really fit well within NumPy's
scope.  They were added in 2008 after
`a discussion <https://mail.python.org/pipermail/numpy-discussion/2008-April/032353.html>`_
on the mailing list where opinion was divided (but a majority in favor).
The financial functions didn't cause a lot of overhead, however there were
still multiple issues and PRs a year for them which cost maintainer time to
deal with.  And they cluttered up the ``numpy`` namespace.  Discussion on
removing them was discussed in 2013 (gh-2880, rejected) and in 2019
(:ref:`NEP32`, accepted without significant complaints).

Given that they were clearly outside of NumPy's scope, moving them to a
separate ``numpy-financial`` package and removing them from NumPy after a
deprecation period made sense.  That also gave users an easy way to update
their code by doing `pip install numpy-financial`.


Alternatives
------------

**Being more aggressive with deprecations.**

The goal of being more aggressive is to allow NumPy to move forward faster.
This would avoid others inventing their own solutions (often in multiple
places), as well as be a benefit to users without a legacy code base.  We
reject this alternative because of the place NumPy has in the scientific Python
ecosystem - being fairly conservative is required in order to not increase the
extra maintenance for downstream libraries and end users to an unacceptable
level.


Discussion
----------

- `Mailing list discussion on the first version of this NEP in 2018 <https://mail.python.org/pipermail/numpy-discussion/2018-July/078432.html>`__
- `Mailing list discussion on the Dec 2020 update of this NEP <https://mail.python.org/pipermail/numpy-discussion/2020-December/081358.html>`__
- `PR with review comments on the Dec 2020 update of this NEP <https://github.com/numpy/numpy/pull/18097>`__


References and Footnotes
------------------------

- `Issue requesting semantic versioning <https://github.com/numpy/numpy/issues/10156>`__

- `PEP 387 - Backwards Compatibility Policy <https://www.python.org/dev/peps/pep-0387/>`__

.. [1] https://searchcode.com/

.. [2] https://sourcegraph.com/search

.. [3] https://github.com/Quansight-Labs/python-api-inspect

.. [4] https://github.com/data-apis/python-record-api

Copyright
---------

This document has been placed in the public domain.
.. _NEP31:

============================================================
NEP 31 — Context-local and global overrides of the NumPy API
============================================================

:Author: Hameer Abbasi <habbasi@quansight.com>
:Author: Ralf Gommers <rgommers@quansight.com>
:Author: Peter Bell <pbell@quansight.com>
:Status: Draft
:Type: Standards Track
:Created: 2019-08-22


Abstract
--------

This NEP proposes to make all of NumPy's public API overridable via an
extensible backend mechanism.

Acceptance of this NEP means NumPy would provide global and context-local
overrides in a separate namespace, as well as a dispatch mechanism similar
to NEP-18 [2]_. First experiences with ``__array_function__`` show that it
is necessary to be able to override NumPy functions that *do not take an
array-like argument*, and hence aren't overridable via
``__array_function__``. The most pressing need is array creation and coercion
functions, such as ``numpy.zeros`` or ``numpy.asarray``; see e.g. NEP-30 [9]_.

This NEP proposes to allow, in an opt-in fashion, overriding any part of the
NumPy API. It is intended as a comprehensive resolution to NEP-22 [3]_, and
obviates the need to add an ever-growing list of new protocols for each new
type of function or object that needs to become overridable.

Motivation and Scope
--------------------

The primary end-goal of this NEP is to make the following possible:

.. code:: python

    # On the library side
    import numpy.overridable as unp

    def library_function(array):
        array = unp.asarray(array)
        # Code using unumpy as usual
        return array

    # On the user side:
    import numpy.overridable as unp
    import uarray as ua
    import dask.array as da

    ua.register_backend(da) # Can be done within Dask itself

    library_function(dask_array)  # works and returns dask_array

    with unp.set_backend(da):
        library_function([1, 2, 3, 4])  # actually returns a Dask array.

Here, ``backend`` can be any compatible object defined either by NumPy or an
external library, such as Dask or CuPy. Ideally, it should be the module
``dask.array`` or ``cupy`` itself.

These kinds of overrides are useful for both the end-user as well as library
authors. End-users may have written or wish to write code that they then later
speed up or move to a different implementation, say PyData/Sparse. They can do
this simply by setting a backend. Library authors may also wish to write code
that is portable across array implementations, for example ``sklearn`` may wish
to write code for a machine learning algorithm that is portable across array
implementations while also using array creation functions.

This NEP takes a holistic approach: It assumes that there are parts of
the API that need to be overridable, and that these will grow over time. It
provides a general framework and a mechanism to avoid a design of a new
protocol each time this is required. This was the goal of ``uarray``: to
allow for overrides in an API without needing the design of a new protocol.

This NEP proposes the following: That ``unumpy`` [8]_  becomes the
recommended override mechanism for the parts of the NumPy API not yet covered
by ``__array_function__`` or ``__array_ufunc__``, and that ``uarray`` is
vendored into a new namespace within NumPy to give users and downstream
dependencies access to these overrides.  This vendoring mechanism is similar
to what SciPy decided to do for making ``scipy.fft`` overridable (see [10]_).

The motivation behind ``uarray`` is manyfold: First, there have been several
attempts to allow dispatch of parts of the NumPy API, including (most
prominently), the ``__array_ufunc__`` protocol in NEP-13 [4]_, and the
``__array_function__`` protocol in NEP-18 [2]_, but this has shown the need
for further protocols to be developed, including a protocol for coercion (see
[5]_, [9]_). The reasons these overrides are needed have been extensively
discussed in the references, and this NEP will not attempt to go into the
details of why these are needed; but in short: It is necessary for library
authors to be able to coerce arbitrary objects into arrays of their own types,
such as CuPy needing to coerce to a CuPy array, for example, instead of
a NumPy array. In simpler words, one needs things like ``np.asarray(...)`` or
an alternative to "just work" and return duck-arrays.

Usage and Impact
----------------

This NEP allows for global and context-local overrides, as well as
automatic overrides a-la ``__array_function__``.

Here are some use-cases this NEP would enable, besides the 
first one stated in the motivation section:

The first is allowing alternate dtypes to return their
respective arrays.

.. code:: python

    # Returns an XND array
    x = unp.ones((5, 5), dtype=xnd_dtype) # Or torch dtype

The second is allowing overrides for parts of the API.
This is to allow alternate and/or optimised implementations
for ``np.linalg``, BLAS, and ``np.random``.

.. code:: python

    import numpy as np
    import pyfftw # Or mkl_fft

    # Makes pyfftw the default for FFT
    np.set_global_backend(pyfftw)

    # Uses pyfftw without monkeypatching
    np.fft.fft(numpy_array)    

    with np.set_backend(pyfftw) # Or mkl_fft, or numpy
        # Uses the backend you specified
        np.fft.fft(numpy_array)

This will allow an official way for overrides to work with NumPy without
monkeypatching or distributing a modified version of NumPy.

Here are a few other use-cases, implied but not already
stated:

.. code:: python

    data = da.from_zarr('myfile.zarr')
    # result should still be dask, all things being equal
    result = library_function(data)
    result.to_zarr('output.zarr')

This second one would work if ``magic_library`` was built
on top of ``unumpy``.

.. code:: python

    from dask import array as da
    from magic_library import pytorch_predict

    data = da.from_zarr('myfile.zarr')
    # normally here one would use e.g. data.map_overlap
    result = pytorch_predict(data)
    result.to_zarr('output.zarr')

There are some backends which may depend on other backends, for example xarray
depending on `numpy.fft`, and transforming a time axis into a frequency axis,
or Dask/xarray holding an array other than a NumPy array inside it. This would
be handled in the following manner inside code::

    with ua.set_backend(cupy), ua.set_backend(dask.array):
        # Code that has distributed GPU arrays here

Backward compatibility
----------------------

There are no backward incompatible changes proposed in this NEP.

Detailed description
--------------------

Proposals
~~~~~~~~~

The only change this NEP proposes at its acceptance, is to make ``unumpy`` the
officially recommended way to override NumPy, along with making some submodules
overridable by default via ``uarray``. ``unumpy`` will remain a separate
repository/package (which we propose to vendor to avoid a hard dependency, and
use the separate ``unumpy`` package only if it is installed, rather than depend
on for the time being). In concrete terms, ``numpy.overridable`` becomes an
alias for ``unumpy``, if available with a fallback to the a vendored version if
not. ``uarray`` and ``unumpy`` and will be developed primarily with the input
of duck-array authors and secondarily, custom dtype authors, via the usual
GitHub workflow. There are a few reasons for this:

* Faster iteration in the case of bugs or issues.
* Faster design changes, in the case of needed functionality.
* ``unumpy`` will work with older versions of NumPy as well.
* The user and library author opt-in to the override process,
  rather than breakages happening when it is least expected.
  In simple terms, bugs in ``unumpy`` mean that ``numpy`` remains
  unaffected.
* For ``numpy.fft``, ``numpy.linalg`` and ``numpy.random``, the functions in
  the main namespace will mirror those in the ``numpy.overridable`` namespace.
  The reason for this is that there may exist functions in the in these
  submodules that need backends, even for ``numpy.ndarray`` inputs.

Advantanges of ``unumpy`` over other solutions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``unumpy`` offers a number of advantanges over the approach of defining a new
protocol for every problem encountered: Whenever there is something requiring
an override, ``unumpy`` will be able to offer a unified API with very minor
changes. For example:

* ``ufunc`` objects can be overridden via their ``__call__``, ``reduce`` and
  other methods.
* Other functions can be overridden in a similar fashion.
* ``np.asduckarray`` goes away, and becomes ``np.overridable.asarray`` with a
  backend set.
* The same holds for array creation functions such as ``np.zeros``,
  ``np.empty`` and so on.

This also holds for the future: Making something overridable would require only
minor changes to ``unumpy``.

Another promise ``unumpy`` holds is one of default implementations. Default
implementations can be provided for any multimethod, in terms of others. This
allows one to override a large part of the NumPy API by defining only a small
part of it. This is to ease the creation of new duck-arrays, by providing
default implementations of many functions that can be easily expressed in
terms of others, as well as a repository of utility functions that help in the
implementation of duck-arrays that most duck-arrays would require. This would
allow us to avoid designing entire protocols, e.g., a protocol for stacking
and concatenating would be replaced by simply implementing ``stack`` and/or
``concatenate`` and then providing default implementations for everything else
in that class. The same applies for transposing, and many other functions for
which protocols haven't been proposed, such as ``isin`` in terms of ``in1d``,
``setdiff1d`` in terms of ``unique``, and so on.

It also allows one to override functions in a manner which
``__array_function__`` simply cannot, such as overriding ``np.einsum`` with the
version from the ``opt_einsum`` package, or Intel MKL overriding FFT, BLAS
or ``ufunc`` objects. They would define a backend with the appropriate
multimethods, and the user would select them via a ``with`` statement, or
registering them as a backend.

The last benefit is a clear way to coerce to a given backend (via the
``coerce`` keyword in ``ua.set_backend``), and a protocol
for coercing not only arrays, but also ``dtype`` objects and ``ufunc`` objects
with similar ones from other libraries. This is due to the existence of actual,
third party dtype packages, and their desire to blend into the NumPy ecosystem
(see [6]_). This is a separate issue compared to the C-level dtype redesign
proposed in [7]_, it's about allowing third-party dtype implementations to
work with NumPy, much like third-party array implementations. These can provide
features such as, for example, units, jagged arrays or other such features that
are outside the scope of NumPy.

Mixing NumPy and ``unumpy`` in the same file
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Normally, one would only want to import only one of ``unumpy`` or ``numpy``,
you would import it as ``np`` for familiarity. However, there may be situations
where one wishes to mix NumPy and the overrides, and there are a few ways to do
this, depending on the user's style::

    from numpy import overridable as unp
    import numpy as np

or::

    import numpy as np

    # Use unumpy via np.overridable

Duck-array coercion
~~~~~~~~~~~~~~~~~~~

There are inherent problems about returning objects that are not NumPy arrays
from ``numpy.array`` or ``numpy.asarray``, particularly in the context of C/C++
or Cython code that may get an object with a different memory layout than the
one it expects. However, we believe this problem may apply not only to these
two functions but all functions that return NumPy arrays. For this reason,
overrides are opt-in for the user, by using the submodule ``numpy.overridable``
rather than ``numpy``. NumPy will continue to work unaffected by anything in
``numpy.overridable``.

If the user wishes to obtain a NumPy array, there are two ways of doing it:

1. Use ``numpy.asarray`` (the non-overridable version).
2. Use ``numpy.overridable.asarray`` with the NumPy backend set and coercion
   enabled

Aliases outside of the ``numpy.overridable`` namespace
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All functionality in ``numpy.random``, ``numpy.linalg`` and ``numpy.fft``
will be aliased to their respective overridable versions inside
``numpy.overridable``. The reason for this is that there are alternative
implementations of RNGs (``mkl-random``), linear algebra routines (``eigen``,
``blis``) and FFT routines (``mkl-fft``, ``pyFFTW``) that need to operate on
``numpy.ndarray`` inputs, but still need the ability to switch behaviour.

This is different from monkeypatching in a few different ways:

* The caller-facing signature of the function is always the same,
  so there is at least the loose sense of an API contract. Monkeypatching
  does not provide this ability.
* There is the ability of locally switching the backend.
* It has been `suggested <https://mail.python.org/archives/list/numpy-discussion@python.org/message/PS7EN3CRT6XERNTCN56MAYOXFFFEC55G/>`_
  that the reason that 1.17 hasn't landed in the Anaconda defaults channel is
  due to the incompatibility between monkeypatching and ``__array_function__``,
  as monkeypatching would bypass the protocol completely.
* Statements of the form ``from numpy import x; x`` and ``np.x`` would have
  different results depending on whether the import was made before or
  after monkeypatching happened.

All this isn't possible at all with ``__array_function__`` or
``__array_ufunc__``.

It has been formally realised (at least in part) that a backend system is
needed for this, in the `NumPy roadmap <https://numpy.org/neps/roadmap.html#other-functionality>`_.

For ``numpy.random``, it's still necessary to make the C-API fit the one
proposed in `NEP-19 <https://numpy.org/neps/nep-0019-rng-policy.html>`_.
This is impossible for `mkl-random`, because then it would need to be
rewritten to fit that framework. The guarantees on stream
compatibility will be the same as before, but if there's a backend that affects
``numpy.random`` set, we make no guarantees about stream compatibility, and it
is up to the backend author to provide their own guarantees.

Providing a way for implicit dispatch
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It has been suggested that the ability to dispatch methods which do not take
a dispatchable is needed, while guessing that backend from another dispatchable.

As a concrete example, consider the following:

.. code:: python

    with unumpy.determine_backend(array_like, np.ndarray):
        unumpy.arange(len(array_like))

While this does not exist yet in ``uarray``, it is trivial to add it. The need for
this kind of code exists because one might want to have an alternative for the
proposed ``*_like`` functions, or the ``like=`` keyword argument. The need for these
exists because there are functions in the NumPy API that do not take a dispatchable
argument, but there is still the need to select a backend based on a different
dispatchable.

The need for an opt-in module
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The need for an opt-in module is realised because of a few reasons:

* There are parts of the API (like `numpy.asarray`) that simply cannot be
  overridden due to incompatibility concerns with C/Cython extensions, however,
  one may want to coerce to a duck-array using ``asarray`` with a backend set.
* There are possible issues around an implicit option and monkeypatching, such
  as those mentioned above.

NEP 18 notes that this may require maintenance of two separate APIs. However,
this burden may be lessened by, for example, parametrizing all tests over
``numpy.overridable`` separately via a fixture. This also has the side-effect
of thoroughly testing it, unlike ``__array_function__``. We also feel that it
provides an opportunity to separate the NumPy API contract properly from the
implementation.

Benefits to end-users and mixing backends
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Mixing backends is easy in ``uarray``, one only has to do:

.. code:: python

    # Explicitly say which backends you want to mix
    ua.register_backend(backend1)
    ua.register_backend(backend2)
    ua.register_backend(backend3)

    # Freely use code that mixes backends here.

The benefits to end-users extend beyond just writing new code. Old code
(usually in the form of scripts) can be easily ported to different backends
by a simple import switch and a line adding the preferred backend. This way,
users may find it easier to port existing code to GPU or distributed computing.

Related Work
------------

Other override mechanisms
~~~~~~~~~~~~~~~~~~~~~~~~~

* NEP-18, the ``__array_function__`` protocol. [2]_
* NEP-13, the ``__array_ufunc__`` protocol. [3]_
* NEP-30, the ``__duck_array__`` protocol. [9]_

Existing NumPy-like array implementations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Dask: https://dask.org/
* CuPy: https://cupy.chainer.org/
* PyData/Sparse: https://sparse.pydata.org/
* Xnd: https://xnd.readthedocs.io/
* Astropy's Quantity: https://docs.astropy.org/en/stable/units/

Existing and potential consumers of alternative arrays
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Dask: https://dask.org/
* scikit-learn: https://scikit-learn.org/
* xarray: https://xarray.pydata.org/
* TensorLy: http://tensorly.org/

Existing alternate dtype implementations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ``ndtypes``: https://ndtypes.readthedocs.io/en/latest/
* Datashape: https://datashape.readthedocs.io
* Plum: https://plum-py.readthedocs.io/

Alternate implementations of parts of the NumPy API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ``mkl_random``: https://github.com/IntelPython/mkl_random
* ``mkl_fft``: https://github.com/IntelPython/mkl_fft
* ``bottleneck``: https://github.com/pydata/bottleneck
* ``opt_einsum``: https://github.com/dgasmith/opt_einsum

Implementation
--------------

The implementation of this NEP will require the following steps:

* Implementation of ``uarray`` multimethods corresponding to the
  NumPy API, including classes for overriding ``dtype``, ``ufunc``
  and ``array`` objects, in the ``unumpy`` repository, which are usually
  very easy to create.
* Moving backends from ``unumpy`` into the respective array libraries.

Maintenance can be eased by testing over ``{numpy, unumpy}`` via parameterized
tests. If a new argument is added to a method, the corresponding argument
extractor and replacer will need to be updated within ``unumpy``.

A lot of argument extractors can be re-used from the existing implementation
of the ``__array_function__`` protocol, and the replacers can be usually
re-used across many methods.

For the parts of the namespace which are going to be overridable by default,
the main method will need to be renamed and hidden behind a ``uarray`` multimethod.

Default implementations are usually seen in the documentation using the words
"equivalent to", and thus, are easily available.

``uarray`` Primer
~~~~~~~~~~~~~~~~~

**Note:** *This section will not attempt to go into too much detail about
uarray, that is the purpose of the uarray documentation.* [1]_
*However, the NumPy community will have input into the design of
uarray, via the issue tracker.*

``unumpy`` is the interface that defines a set of overridable functions
(multimethods) compatible with the numpy API. To do this, it uses the
``uarray`` library. ``uarray`` is a general purpose tool for creating
multimethods that dispatch to one of multiple different possible backend
implementations. In this sense, it is similar to the ``__array_function__``
protocol but with the key difference that the backend is explicitly installed
by the end-user and not coupled into the array type.

Decoupling the backend from the array type gives much more flexibility to
end-users and backend authors. For example, it is possible to:

* override functions not taking arrays as arguments
* create backends out of source from the array type
* install multiple backends for the same array type

This decoupling also means that ``uarray`` is not constrained to dispatching
over array-like types. The backend is free to inspect the entire set of
function arguments to determine if it can implement the function e.g. ``dtype``
parameter dispatching.

Defining backends
^^^^^^^^^^^^^^^^^

``uarray`` consists of two main protocols: ``__ua_convert__`` and
``__ua_function__``, called in that order, along with ``__ua_domain__``.
``__ua_convert__`` is for conversion and coercion. It has the signature
``(dispatchables, coerce)``, where ``dispatchables`` is an iterable of
``ua.Dispatchable`` objects and ``coerce`` is a boolean indicating whether or
not to force the conversion. ``ua.Dispatchable`` is a simple class consisting
of three simple values: ``type``, ``value``, and ``coercible``.
``__ua_convert__`` returns an iterable of the converted values, or
``NotImplemented`` in the case of failure.

``__ua_function__`` has the signature ``(func, args, kwargs)`` and defines
the actual implementation of the function. It receives the function and its
arguments. Returning ``NotImplemented`` will cause a move to the default
implementation of the function if one exists, and failing that, the next
backend.

Here is what will happen assuming a ``uarray`` multimethod is called:

1. We canonicalise the arguments so any arguments without a default
   are placed in ``*args`` and those with one are placed in ``**kwargs``.
2. We check the list of backends.

   a. If it is empty, we try the default implementation.

3. We check if the backend's ``__ua_convert__`` method exists. If it exists:

   a. We pass it the output of the dispatcher,
      which is an iterable of ``ua.Dispatchable`` objects.
   b. We feed this output, along with the arguments,
      to the argument replacer. ``NotImplemented`` means we move to 3
      with the next backend.
   c. We store the replaced arguments as the new arguments.

4. We feed the arguments into ``__ua_function__``, and return the output, and
   exit if it isn't ``NotImplemented``.
5. If the default implementation exists, we try it with the current backend.
6. On failure,  we move to 3 with the next backend. If there are no more
   backends, we move to 7.
7. We raise a ``ua.BackendNotImplementedError``.

Defining overridable multimethods
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To define an overridable function (a multimethod), one needs a few things:

1. A dispatcher that returns an iterable of ``ua.Dispatchable`` objects.
2. A reverse dispatcher that replaces dispatchable values with the supplied
   ones.
3. A domain.
4. Optionally, a default implementation, which can be provided in terms of
   other multimethods.

As an example, consider the following::

    import uarray as ua

    def full_argreplacer(args, kwargs, dispatchables):
        def full(shape, fill_value, dtype=None, order='C'):
            return (shape, fill_value), dict(
                dtype=dispatchables[0],
                order=order
            )

        return full(*args, **kwargs)

    @ua.create_multimethod(full_argreplacer, domain="numpy")
    def full(shape, fill_value, dtype=None, order='C'):
        return (ua.Dispatchable(dtype, np.dtype),)

A large set of examples can be found in the ``unumpy`` repository, [8]_.
This simple act of overriding callables allows us to override:

* Methods
* Properties, via ``fget`` and ``fset``
* Entire objects, via ``__get__``.

Examples for NumPy
^^^^^^^^^^^^^^^^^^

A library that implements a NumPy-like API will use it in the following
manner (as an example)::

    import numpy.overridable as unp
    _ua_implementations = {}

    __ua_domain__ = "numpy"

    def __ua_function__(func, args, kwargs):
        fn = _ua_implementations.get(func, None)
        return fn(*args, **kwargs) if fn is not None else NotImplemented

    def implements(ua_func):
        def inner(func):
            _ua_implementations[ua_func] = func
            return func

        return inner

    @implements(unp.asarray)
    def asarray(a, dtype=None, order=None):
        # Code here
        # Either this method or __ua_convert__ must
        # return NotImplemented for unsupported types,
        # Or they shouldn't be marked as dispatchable.

    # Provides a default implementation for ones and zeros.
    @implements(unp.full)
    def full(shape, fill_value, dtype=None, order='C'):
        # Code here

Alternatives
------------

The current alternative to this problem is a combination of NEP-18 [2]_,
NEP-13 [4]_ and NEP-30 [9]_ plus adding more protocols (not yet specified)
in addition to it. Even then, some parts of the NumPy API will remain
non-overridable, so it's a partial alternative.

The main alternative to vendoring ``unumpy`` is to simply move it into NumPy
completely and not distribute it as a separate package. This would also achieve
the proposed goals, however we prefer to keep it a separate package for now,
for reasons already stated above.

The third alternative is to move ``unumpy`` into the NumPy organisation and
develop it as a NumPy project. This will also achieve the said goals, and is
also a possibility that can be considered by this NEP. However, the act of
doing an extra ``pip install`` or ``conda install`` may discourage some users
from adopting this method.

An alternative to requiring opt-in is mainly to *not* override ``np.asarray``
and ``np.array``, and making the rest of the NumPy API surface overridable,
instead providing ``np.duckarray`` and ``np.asduckarray``
as duck-array friendly alternatives that used the respective overrides. However,
this has the downside of adding a minor overhead to NumPy calls.

Discussion
----------

* ``uarray`` blogpost: https://labs.quansight.org/blog/2019/07/uarray-update-api-changes-overhead-and-comparison-to-__array_function__/
* The discussion section of NEP-18: https://numpy.org/neps/nep-0018-array-function-protocol.html#discussion
* NEP-22: https://numpy.org/neps/nep-0022-ndarray-duck-typing-overview.html
* Dask issue #4462: https://github.com/dask/dask/issues/4462
* PR #13046: https://github.com/numpy/numpy/pull/13046
* Dask issue #4883: https://github.com/dask/dask/issues/4883
* Issue #13831: https://github.com/numpy/numpy/issues/13831
* Discussion PR 1: https://github.com/hameerabbasi/numpy/pull/3
* Discussion PR 2: https://github.com/hameerabbasi/numpy/pull/4
* Discussion PR 3: https://github.com/numpy/numpy/pull/14389


References and Footnotes
------------------------

.. [1] uarray, A general dispatch mechanism for Python: https://uarray.readthedocs.io

.. [2] NEP 18 — A dispatch mechanism for NumPy’s high level array functions: https://numpy.org/neps/nep-0018-array-function-protocol.html

.. [3] NEP 22 — Duck typing for NumPy arrays – high level overview: https://numpy.org/neps/nep-0022-ndarray-duck-typing-overview.html

.. [4] NEP 13 — A Mechanism for Overriding Ufuncs: https://numpy.org/neps/nep-0013-ufunc-overrides.html

.. [5] Reply to Adding to the non-dispatched implementation of NumPy methods: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/5GUDMALWDIRHITG5YUOCV343J66QSX3U/#5GUDMALWDIRHITG5YUOCV343J66QSX3U

.. [6] Custom Dtype/Units discussion: https://mail.python.org/archives/list/numpy-discussion@python.org/thread/RZYCVT6C3F7UDV6NA6FEV4MC5FKS6RDA/#RZYCVT6C3F7UDV6NA6FEV4MC5FKS6RDA

.. [7] The epic dtype cleanup plan: https://github.com/numpy/numpy/issues/2899

.. [8] unumpy: NumPy, but implementation-independent: https://unumpy.readthedocs.io

.. [9] NEP 30 — Duck Typing for NumPy Arrays - Implementation: https://www.numpy.org/neps/nep-0030-duck-array-protocol.html

.. [10] http://scipy.github.io/devdocs/fft.html#backend-control


Copyright
---------

This document has been placed in the public domain.
.. _NEP47:

========================================
NEP 47 — Adopting the array API standard
========================================

:Author: Ralf Gommers <ralf.gommers@gmail.com>
:Author: Stephan Hoyer <shoyer@gmail.com>
:Author: Aaron Meurer <asmeurer@gmail.com>
:Status: Draft
:Type: Standards Track
:Created: 2021-01-21
:Resolution:


Abstract
--------

We propose to adopt the `Python array API standard`_, developed by the
`Consortium for Python Data API Standards`_. Implementing this as a separate
new namespace in NumPy will allow authors of libraries which depend on NumPy
as well as end users to write code that is portable between NumPy and all
other array/tensor libraries that adopt this standard.

.. note::

    We expect that this NEP will remain in a draft state for quite a while.
    Given the large scope we don't expect to propose it for acceptance any
    time soon; instead, we want to solicit feedback on both the high-level
    design and implementation, and learn what needs describing better in this
    NEP or changing in either the implementation or the array API standard
    itself.


Motivation and Scope
--------------------

Python users have a wealth of choice for libraries and frameworks for
numerical computing, data science, machine learning, and deep learning. New
frameworks pushing forward the state of the art in these fields are appearing
every year. One unintended consequence of all this activity and creativity
has been fragmentation in multidimensional array (a.k.a. tensor) libraries -
which are the fundamental data structure for these fields. Choices include
NumPy, Tensorflow, PyTorch, Dask, JAX, CuPy, MXNet, and others.

The APIs of each of these libraries are largely similar, but with enough
differences that it’s quite difficult to write code that works with multiple
(or all) of these libraries. The array API standard aims to address that
issue, by specifying an API for the most common ways arrays are constructed
and used. The proposed API is quite similar to NumPy's API, and deviates mainly
in places where (a) NumPy made design choices that are inherently not portable
to other implementations, and (b) where other libraries consistently deviated
from NumPy on purpose because NumPy's design turned out to have issues or
unnecessary complexity.

For a longer discussion on the purpose of the array API standard we refer to
the `Purpose and Scope section of the array API standard <https://data-apis.github.io/array-api/latest/purpose_and_scope.html>`__
and the two blog posts announcing the formation of the Consortium [1]_ and
the release of the first draft version of the standard for community review [2]_.

The scope of this NEP includes:

- Adopting the 2021 version of the array API standard
- Adding a separate namespace, tentatively named ``numpy.array_api``
- Changes needed/desired outside of the new namespace, for example new dunder
  methods on the ``ndarray`` object
- Implementation choices, and differences between functions in the new
  namespace with those in the main ``numpy`` namespace
- A new array object conforming to the array API standard
- Maintenance effort and testing strategy
- Impact on NumPy's total exposed API surface and on other future and
  under-discussion design choices
- Relation to existing and proposed NumPy array protocols
  (``__array_ufunc__``, ``__array_function__``, ``__array_module__``).
- Required improvements to existing NumPy functionality

Out of scope for this NEP are:

- Changes in the array API standard itself. Those are likely to come up
  during review of this NEP, but should be upstreamed as needed and this NEP
  subsequently updated.


Usage and Impact
----------------

*This section will be fleshed out later, for now we refer to the use cases given
in* `the array API standard Use Cases section <https://data-apis.github.io/array-api/latest/use_cases.html>`__

In addition to those use cases, the new namespace contains functionality that
is widely used and supported by many array libraries. As such, it is a good
set of functions to teach to newcomers to NumPy and recommend as "best
practice". That contrasts with NumPy's main namespace, which contains many
functions and objects that have been superseded or we consider mistakes - but
that we can't remove because of backwards compatibility reasons.

The usage of the ``numpy.array_api`` namespace by downstream libraries is
intended to enable them to consume multiple kinds of arrays, *without having
to have a hard dependency on all of those array libraries*:

.. image:: _static/nep-0047-library-dependencies.png

Adoption in downstream libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The prototype implementation of the ``array_api`` namespace will be used with
SciPy, scikit-learn, and other libraries of interest that depend on NumPy, in
order to get more experience with the design and find out if any important
parts are missing.

The pattern to support multiple array libraries is intended to be something
like::

    def somefunc(x, y):
        # Retrieves standard namespace. Raises if x and y have different
        # namespaces.  See Appendix for possible get_namespace implementation
        xp = get_namespace(x, y)
        out = xp.mean(x, axis=0) + 2*xp.std(y, axis=0)
        return out

The ``get_namespace`` call is effectively the library author opting in to
using the standard API namespace, and thereby explicitly supporting
all conforming array libraries.


The ``asarray`` / ``asanyarray`` pattern
````````````````````````````````````````

Many existing libraries use the same ``asarray`` (or ``asanyarray``) pattern
as NumPy itself does; accepting any object that can be coerced into a ``np.ndarray``.
We consider this design pattern problematic - keeping in mind the Zen of
Python, *"explicit is better than implicit"*, as well as the pattern being
historically problematic in the SciPy ecosystem for ``ndarray`` subclasses
and with over-eager object creation. All other array/tensor libraries are
more strict, and that works out fine in practice. We would advise authors of
new libraries to avoid the ``asarray`` pattern. Instead they should either
accept just NumPy arrays or, if they want to support multiple kinds of
arrays, check if the incoming array object supports the array API standard
by checking for ``__array_namespace__`` as shown in the example above.

Existing libraries can do such a check as well, and only call ``asarray`` if
the check fails. This is very similar to the ``__duckarray__`` idea in
:ref:`NEP30`.


.. _adoption-application-code:

Adoption in application code
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The new namespace can be seen by end users as a cleaned up and slimmed down
version of NumPy's main namespace. Encouraging end users to use this
namespace like::

    import numpy.array_api as xp

    x = xp.linspace(0, 2*xp.pi, num=100)
    y = xp.cos(x)

seems perfectly reasonable, and potentially beneficial - users get offered only
one function for each purpose (the one we consider best-practice), and they
then write code that is more easily portable to other libraries.


Backward compatibility
----------------------

No deprecations or removals of existing NumPy APIs or other backwards
incompatible changes are proposed.


High-level design
-----------------

The array API standard consists of approximately 120 objects, all of which
have a direct NumPy equivalent. This figure shows what is included at a high level:

.. image:: _static/nep-0047-scope-of-array-API.png

The most important changes compared to what NumPy currently offers are:

- A new array object, ``numpy.array_api.Array`` which:

    - is a thin pure Python (non-subclass) wrapper around ``np.ndarray``,
    - conforms to the casting rules and indexing behavior specified by the
      standard,
    - does not have methods other than dunder methods,
    - does not support the full range of NumPy indexing behavior (see
      :ref:`indexing` below),
    - does not have distinct scalar objects, only 0-D arrays,
    - cannot be constructed directly. Instead array construction functions
      like ``asarray()`` should be used.

- Functions in the ``array_api`` namespace:

    - do not accept ``array_like`` inputs, only ``numpy.array_api`` array
      objects, with Python scalars only being supported in dunder operators on
      the array object,
    - do not support ``__array_ufunc__`` and ``__array_function__``,
    - use positional-only and keyword-only parameters in their signatures,
    - have inline type annotations,
    - may have minor changes to signatures and semantics of individual
      functions compared to their equivalents already present in NumPy,
    - only support dtype literals, not format strings or other ways of
      specifying dtypes,
    - generally may only support a restricted set of dtypes compared to their
      NumPy counterparts.

- DLPack_ support will be added to NumPy,
- New syntax for "device support" will be added, through a ``.device``
  attribute on the new array object, and ``device=`` keywords in array creation
  functions in the ``array_api`` namespace,
- Casting rules will differ from those NumPy currently has. Output dtypes can
  be derived from input dtypes (i.e. no value-based casting), and 0-D arrays
  are treated like >=1-D arrays. Cross-kind casting (e.g., int to float) is
  not allowed.
- Not all dtypes NumPy has are part of the standard. Only boolean, signed and
  unsigned integers, and floating-point dtypes up to ``float64`` are supported.
  Complex dtypes are expected to be added in the next version of the standard.
  Extended precision, string, void, object and datetime dtypes, as well as
  structured dtypes, are not included.

Improvements to existing NumPy functionality that are needed include:

- Add support for stacks of matrices to some functions in ``numpy.linalg``
  that are currently missing such support.
- Add the ``keepdims`` keyword to ``np.argmin`` and ``np.argmax``.
- Add a "never copy" mode to ``np.asarray``.
- Add smallest_normal to ``np.finfo()``.
- DLPack_ support.

Additionally, the ``numpy.array_api`` implementation was chosen to be a
*minimal* implementation of the array API standard. This means that it not
only conforms to all the requirements of the array API, but it explicitly does
not include any APIs or behaviors not explicitly required by it. The standard
itself does not require implementations to be so restrictive, but doing this
with the NumPy array API implementation will allow it to become a canonical
implementation of the array API standard. Anyone who wants to make use of the
array API standard can use the NumPy implementation and be sure that their
code is not making use of behaviors that will not be in other conforming
implementations.

In particular, this means

- ``numpy.array_api`` will only include those functions that are listed in the
  standard. This also applies to methods on the ``Array`` object,
- Functions will only accept input dtypes that are required by the standard
  (e.g., transcendental functions like ``cos`` will not accept integer dtypes
  because the standard only requires them to accept floating-point dtypes),
- Type promotion will only occur for combinations of dtypes required by the
  standard (see the :ref:`dtypes-and-casting-rules` section below),
- Indexing is limited to a subset of possible index types (see :ref:`indexing`
  below).


Functions in the ``array_api`` namespace
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's start with an example of a function implementation that shows the most
important differences with the equivalent function in the main namespace::

    def matmul(x1: Array, x2: Array, /) -> Array:
        """
        Array API compatible wrapper for :py:func:`np.matmul <numpy.matmul>`.
        See its docstring for more information.
        """
        if x1.dtype not in _numeric_dtypes or x2.dtype not in _numeric_dtypes:
            raise TypeError("Only numeric dtypes are allowed in matmul")

        # Call result type here just to raise on disallowed type combinations
        _result_type(x1.dtype, x2.dtype)

        return Array._new(np.matmul(x1._array, x2._array))

This function does not accept ``array_like`` inputs, only
``numpy.array_api.Array``. There are multiple reasons for this. Other array
libraries all work like this. Requiring the user to do coercion of Python
scalars, lists, generators, or other foreign objects explicitly results in a
cleaner design with less unexpected behavior. It is higher-performance---less
overhead from ``asarray`` calls. Static typing is easier. Subclasses will work
as expected. And the slight increase in verbosity because users have to
explicitly coerce to ``ndarray`` on rare occasions seems like a small price to
pay.

This function does not support ``__array_ufunc__`` nor ``__array_function__``.
These protocols serve a similar purpose as the array API standard module itself,
but through a different mechanisms. Because only ``Array`` instances are accepted,
dispatching via one of these protocols isn't useful anymore.

This function uses positional-only parameters in its signature. This makes
code more portable---writing, for instance, ``max(a=a, ...)`` is no longer
valid, hence if other libraries call the first parameter ``input`` rather than
``a``, that is fine. Note that NumPy already uses positional-only arguments
for functions that are ufuncs. The rationale for keyword-only parameters (not
shown in the above example) is two-fold: clarity of end user code, and it
being easier to extend the signature in the future without worrying about the
order of keywords.

This function has inline type annotations. Inline annotations are far easier to
maintain than separate stub files. And because the types are simple, this will
not result in a large amount of clutter with type aliases or unions like in the
current stub files NumPy has.

This function only accepts numeric dtypes (i.e., not ``bool``). It also does
not allow the input dtypes to be of different kinds (the internal
``_result_type()`` function will raise ``TypeError`` on cross-kind type
combinations like ``_result_type(int32, float64)``). This allows the
implementation to be minimal. Preventing combinations that work in NumPy but
are not required by the array API specification lets users of the submodule
know they are not relying on NumPy specific behavior that may not be present
in array API conforming implementations from other libraries.

DLPack support for zero-copy data interchange
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ability to convert one kind of array into another kind is valuable, and
indeed necessary when downstream libraries want to support multiple kinds of
arrays. This requires a well-specified data exchange protocol. NumPy already
supports two of these, namely the buffer protocol (i.e., PEP 3118), and
the ``__array_interface__`` (Python side) / ``__array_struct__`` (C side)
protocol. Both work similarly, letting the "producer" describe how the data
is laid out in memory so the "consumer" can construct its own kind of array
with a view on that data.

DLPack works in a very similar way. The main reasons to prefer DLPack over
the options already present in NumPy are:

1. DLPack is the only protocol with device support (e.g., GPUs using CUDA or
   ROCm drivers, or OpenCL devices). NumPy is CPU-only, but other array
   libraries are not. Having one protocol per device isn't tenable, hence
   device support is a must.
2. Widespread support. DLPack has the widest adoption of all protocols. Only
   NumPy is missing support, and the experiences of other libraries with it
   are positive. This contrasts with the protocols NumPy does support, which
   are used very little---when other libraries want to interoperate with
   NumPy, they typically use the (more limited, and NumPy-specific)
   ``__array__`` protocol.

Adding support for DLPack to NumPy entails:

- Adding a ``ndarray.__dlpack__()`` method which returns a ``dlpack`` C
  structure wrapped in a ``PyCapsule``.
- Adding a ``np._from_dlpack(obj)`` function, where ``obj`` supports
  ``__dlpack__()``, and returns an ``ndarray``.

DLPack is currently a ~200 LoC header, and is meant to be included directly, so
no external dependency is needed. Implementation should be straightforward.


Syntax for device support
~~~~~~~~~~~~~~~~~~~~~~~~~

NumPy itself is CPU-only, so it clearly doesn't have a need for device support.
However, other libraries (e.g. TensorFlow, PyTorch, JAX, MXNet) support
multiple types of devices: CPU, GPU, TPU, and more exotic hardware.
To write portable code on systems with multiple devices, it's often necessary
to create new arrays on the same device as some other array, or to check that
two arrays live on the same device. Hence syntax for that is needed.

The array object will have a ``.device`` attribute which enables comparing
devices of different arrays (they only should compare equal if both arrays are
from the same library and it's the same hardware device). Furthermore,
``device=`` keywords in array creation functions are needed. For example::

    def empty(shape: Union[int, Tuple[int, ...]], /, *,
              dtype: Optional[dtype] = None,
              device: Optional[device] = None) -> Array:
        """
        Array API compatible wrapper for :py:func:`np.empty <numpy.empty>`.
        """
        if device not in ["cpu", None]:
            raise ValueError(f"Unsupported device {device!r}")
        return Array._new(np.empty(shape, dtype=dtype))

The implementation for NumPy is as simple as setting the device attribute to
the string ``"cpu"`` and raising an exception if array creation functions
encounter any other value.

.. _dtypes-and-casting-rules:

Dtypes and casting rules
~~~~~~~~~~~~~~~~~~~~~~~~

The supported dtypes in this namespace are boolean, 8/16/32/64-bit signed and
unsigned integer, and 32/64-bit floating-point dtypes. These will be added to
the namespace as dtype literals with the expected names (e.g., ``bool``,
``uint16``, ``float64``).

The most obvious omissions are the complex dtypes. The rationale for the lack
of complex support in the first version of the array API standard is that several
libraries (PyTorch, MXNet) are still in the process of adding support for
complex dtypes. The next version of the standard is expected to include ``complex64``
and ``complex128`` (see `this issue <https://github.com/data-apis/array-api/issues/102>`__
for more details).

Specifying dtypes to functions, e.g. via the ``dtype=`` keyword, is expected
to only use the dtype literals. Format strings, Python builtin dtypes, or
string representations of the dtype literals are not accepted. This will
improve readability and portability of code at little cost. Furthermore, no
behavior is expected of these dtype literals themselves other than basic
equality comparison. In particular, since the array API does not have scalar
objects, syntax like ``float32(0.0)`` is not allowed (a 0-D array can be
created with ``asarray(0.0, dtype=float32)``).

Casting rules are only defined between different dtypes of the same kind
(i.e., boolean to boolean, integer to integer, or floating-point to
floating-point). This also means omitting integer-uint64 combinations that
would upcast to float64 in NumPy. The rationale for this is that mixed-kind
(e.g., integer to floating-point) casting behaviors differ between libraries.

.. image:: _static/nep-0047-casting-rules-lattice.png

*Type promotion diagram. Promotion between any two types is given by their
join on this lattice. Only the types of participating arrays matter, not their
values. Dashed lines indicate that behavior for Python scalars is undefined on
overflow. The Python scalars themselves are only allowed in operators on the
array object, not inside of functions. Boolean, integer and floating-point
dtypes are not connected, indicating mixed-kind promotion is undefined (for
the NumPy implementation, these raise an exception).*

The most important difference between the casting rules in NumPy and in the
array API standard is how scalars and 0-dimensional arrays are handled. In the
standard, array scalars do not exist and 0-dimensional arrays follow the same
casting rules as higher-dimensional arrays. Furthermore, there is no
value-based casting in the standard. The result type of an operation can be
predicted entirely from its input arrays' dtypes, regardless of their shapes
or values. Python scalars are only allowed in dunder operations (like
``__add__``), and only if they are of the same kind as the array dtype. They
always cast to the dtype of the array, regardless of value. Overflow behavior
is undefined.

See the `Type Promotion Rules section of the array API standard <https://data-apis.github.io/array-api/latest/API_specification/type_promotion.html>`__
for more details.

In the implementation, this means

- Ensuring any operation that would produce an scalar object in NumPy is
  converted to a 0-D array in the ``Array`` constructor,
- Checking for combinations that would apply value-based casting and
  ensuring they promote to the correct type. This can be achieved, e.g., by
  manually broadcasting 0-D inputs (preventing them from participating in
  value-based casting), or by explicitly passing the ``signature`` argument
  to the underlying ufunc,
- In dunder operator methods, manually converting Python scalar inputs to 0-D
  arrays of the matching dtype if they are the same kind, and raising otherwise. For scalars out of
  bounds of the given dtype (for which the behavior is undefined by the spec),
  the behavior of ``np.array(scalar, dtype=dtype)`` is used (either cast or
  raise OverflowError).

.. _indexing:

Indexing
~~~~~~~~

An indexing expression that would return a scalar with ``ndarray``, e.g.
``arr_2d[0, 0]``, will return a 0-D array with the new ``Array`` object. There are
several reasons for this: array scalars are largely considered a design mistake
which no other array library copied; it works better for non-CPU libraries
(typically arrays can live on the device, scalars live on the host); and it's
simply a more consistent design. To get a Python scalar out of a 0-D array, one can
use the builtin for the type, e.g. ``float(arr_0d)``.

The other `indexing modes in the standard <https://data-apis.github.io/array-api/latest/API_specification/indexing.html>`__
do work largely the same as they do for ``numpy.ndarray``. One noteworthy
difference is that clipping in slice indexing (e.g., ``a[:n]`` where ``n`` is
larger than the size of the first axis) is unspecified behavior, because
that kind of check can be expensive on accelerators.

The standard omits advanced indexing (indexing by an integer array), and boolean indexing is limited to a
single n-D boolean array. This is due to those indexing modes not being
suitable for all types of arrays or JIT compilation. Furthermore, some
advanced NumPy indexing semantics, such as the semantics for mixing advanced
and non-advanced indices in a single index, are considered design mistakes in
NumPy. The absence of these more advanced index types does not seem to be
problematic; if a user or library author wants to use them, they can do so
through zero-copy conversion to ``numpy.ndarray``. This will signal correctly
to whomever reads the code that it is then NumPy-specific rather than portable
to all conforming array types.

Being a minimal implementation, ``numpy.array_api`` will explicitly disallow
slices with clipped bounds, advanced indexing, and boolean indices mixed with
other indices.

The array object
~~~~~~~~~~~~~~~~

The array object in the standard does not have methods other than dunder
methods. It also does not allow direct construction, preferring instead array
construction methods like ``asarray``. The rationale for that is that not all
array libraries have methods on their array object (e.g., TensorFlow does
not). It also provides only a single way of doing something, rather than have
functions and methods that are effectively duplicate.

Mixing operations that may produce views (e.g., indexing, ``nonzero``)
in combination with mutation (e.g., item or slice assignment) is
`explicitly documented in the standard to not be supported <https://data-apis.github.io/array-api/latest/design_topics/copies_views_and_mutation.html>`__.
This cannot easily be prohibited in the array object itself; instead this will
be guidance to the user via documentation.

The standard current does not prescribe a name for the array object itself. We
propose to name it ``Array``. This uses proper PEP 8 capitalization for a
class, and does not conflict with any existing NumPy class names. [3]_ Note
that the actual name of the array class does not actually matter that much as
it is not itself included in the top-level namespace, and cannot be directly
constructed.

Implementation
--------------

A prototype of the ``array_api`` namespace can be found in
https://github.com/numpy/numpy/pull/18585. The docstring in its
``__init__.py`` has several important notes about implementation details. The
code for the wrapper functions also contains ``# Note:`` comments everywhere
there is a difference with the NumPy API. The
implementation is entirely in pure Python, and consists primarily of wrapper
classes/functions that pass through to the corresponding NumPy functions after
applying input validation and any changed behavior. One important part that is not
implemented yet is DLPack_ support, as its implementation in ``np.ndarray`` is
still in progress (https://github.com/numpy/numpy/pull/19083).

The ``numpy.array_api`` module is considered experimental. This means that
importing it will issue a ``UserWarning``. The alternative to this was naming
the module ``numpy._array_api``, but the warning was chosen instead so that it
does not become necessary to rename the module in the future, potentially
breaking user code. The module also requires Python 3.8 or greater due to
extensive use of the positional-only argument syntax.

The experimental nature of the module also means that it is not yet mentioned
anywhere in the NumPy documentation, outside of its module docstring and this
NEP. Documentation for the implementation is itself a challenging problem.
Presently every docstring in the implementation simply references the
underlying NumPy function it implements. However, this is not ideal, as the
underlying NumPy function may have different behavior from the corresponding
function in the array API, for instance, additional keyword arguments that are
not present in the array API. It has been suggested that documentation may be
pulled directly from the spec itself, but support for this would require
making some technical changes to the way the spec is written, and so the
current implementation does not yet make any attempt to do this.

The array API specification is accompanied by an in-progress `official test
suite <https://github.com/data-apis/array-api-tests>`_, which is designed to
test conformance of any library to the array API specification. The tests
included with the implementation will therefore be minimal, as the majority of
the behavior will be verified by this test suite. The tests in NumPy itself
for the ``array_api`` submodule will only include testing for behavior not
covered by the array API test suite, for instance, tests that the
implementation is minimal and properly rejects things like disallowed type
combinations. A CI job will be added to the array API test suite repository to
regularly test it against the NumPy implementation. The array API test suite
is designed to be vendored if libraries wish to do that, but this idea was
rejected for NumPy because the time taken by it is significant relative to the
existing NumPy test suite, and because the test suite is itself still
a work in progress.

The dtype objects
~~~~~~~~~~~~~~~~~

We must be able to compare dtypes for equality, and expressions like these must
be possible::

    np.array_api.some_func(..., dtype=x.dtype)

The above implies it would be nice to have ``np.array_api.float32 ==
np.array_api.ndarray(...).dtype``.

Dtypes should not be assumed to have a class hierarchy by users, however we are
free to implement it with a class hierarchy if that's convenient. We considered
the following options to implement dtype objects:

1. Alias dtypes to those in the main namespace, e.g., ``np.array_api.float32 =
   np.float32``.
2. Make the dtypes instances of ``np.dtype``, e.g., ``np.array_api.float32 =
   np.dtype(np.float32)``.
3. Create new singleton classes with only the required methods/attributes
   (currently just ``__eq__``).

It seems like (2) would be easiest from the perspective of interacting with
functions outside the main namespace and (3) would adhere best to the
standard. (2) does not prevent users from accessing NumPy-specific attributes
of the dtype objects like (3) would, although unlike (1), it does disallow
creating scalar objects like ``float32(0.0)``. (2) also keeps only one object
per dtype---with (1), ``arr.dtype`` would be still be a dtype instance. The
implementation currently uses (2).

TBD: the standard does not yet have a good way to inspect properties of a
dtype, to ask questions like "is this an integer dtype?". Perhaps this is easy
enough to do for users, like so::

    def _get_dtype(dt_or_arr):
        return dt_or_arr.dtype if hasattr(dt_or_arr, 'dtype') else dt_or_arr

    def is_floating(dtype_or_array):
        dtype = _get_dtype(dtype_or_array)
        return dtype in (float32, float64)

    def is_integer(dtype_or_array):
        dtype = _get_dtype(dtype_or_array)
        return dtype in (uint8, uint16, uint32, uint64, int8, int16, int32, int64)

However it could make sense to add to the standard. Note that NumPy itself
currently does not have a great for asking such questions, see
`gh-17325 <https://github.com/numpy/numpy/issues/17325>`__.


Feedback from downstream library authors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TODO - this can only be done after trying out some use cases

Leo Fang (CuPy): *"My impression is for CuPy we could simply take this new array object and s/numpy/cupy"*


Related Work
------------

:ref:`NEP37` contains a similar mechanism to retrieve a NumPy-like namespace.
In fact, NEP 37 inspired the (slightly simpler) mechanism in the array API
standard.

Other libraries have adopted large parts of NumPy's API, made changes where
necessary, and documented deviations. See for example
`the jax.numpy documentation <https://jax.readthedocs.io/en/latest/jax.numpy.html>`__
and `Difference between CuPy and NumPy <https://docs.cupy.dev/en/stable/reference/difference.html>`__.
The array API standard was constructed with the help of such comparisons, only
between many array libraries rather than only between NumPy and one other library.


Alternatives
------------

It was proposed to have the NumPy array API implementation as a separate
library from NumPy. This was rejected because keeping it separate will make it
less likely for people to review it, and including it in NumPy itself as an
experimental submodule will make it easier for end users and library authors
who already depend on NumPy to access the implementation.

Appendix - a possible ``get_namespace`` implementation
------------------------------------------------------

The ``get_namespace`` function mentioned in the
:ref:`adoption-application-code` section can be implemented like::

    def get_namespace(*xs):
        # `xs` contains one or more arrays, or possibly Python scalars (accepting
        # those is a matter of taste, but doesn't seem unreasonable).
        namespaces = {
            x.__array_namespace__() if hasattr(x, '__array_namespace__') else None for x in xs if not isinstance(x, (bool, int, float, complex))
        }

        if not namespaces:
            # one could special-case np.ndarray above or use np.asarray here if
            # older numpy versions need to be supported.
            raise ValueError("Unrecognized array input")

        if len(namespaces) != 1:
            raise ValueError(f"Multiple namespaces for array inputs: {namespaces}")

        xp, = namespaces
        if xp is None:
            raise ValueError("The input is not a supported array type")

        return xp


Discussion
----------

- `First discussion on the mailing list about the array API standard <https://mail.python.org/pipermail/numpy-discussion/2020-November/081181.html>`__

- `Discussion of NEP 47 on the mailing list
  <https://mail.python.org/pipermail/numpy-discussion/2021-February/081530.html>`_

- `PR #18585 implementing numpy.array_api
  <https://github.com/numpy/numpy/pull/18585>`_

References and Footnotes
------------------------

.. _Python array API standard: https://data-apis.github.io/array-api/latest

.. _Consortium for Python Data API Standards: https://data-apis.org/

.. _DLPack: https://github.com/dmlc/dlpack

.. [1] https://data-apis.org/blog/announcing_the_consortium/

.. [2] https://data-apis.org/blog/array_api_standard_release/

.. [3] https://github.com/numpy/numpy/pull/18585#discussion_r641370294

Copyright
---------

This document has been placed in the public domain. [1]_
.. _NEP13:

==========================================
NEP 13 — A mechanism for overriding Ufuncs
==========================================

.. currentmodule:: numpy

:Author: Blake Griffith
:Contact: blake.g@utexas.edu
:Date: 2013-07-10

:Author: Pauli Virtanen

:Author: Nathaniel Smith

:Author: Marten van Kerkwijk

:Author: Stephan Hoyer
:Date: 2017-03-31

:Status: Final

Executive summary
=================

NumPy's universal functions (ufuncs) currently have some limited
functionality for operating on user defined subclasses of
:class:`ndarray` using ``__array_prepare__`` and ``__array_wrap__``
[1]_, and there is little to no support for arbitrary
objects. e.g. SciPy's sparse matrices [2]_ [3]_.

Here we propose adding a mechanism to override ufuncs based on the ufunc
checking each of it's arguments for a ``__array_ufunc__`` method.
On discovery of ``__array_ufunc__`` the ufunc will hand off the
operation to the method.

This covers some of the same ground as Travis Oliphant's proposal to
retro-fit NumPy with multi-methods [4]_, which would solve the same
problem. The mechanism here follows more closely the way Python enables
classes to override ``__mul__`` and other binary operations. It also
specifically addresses how binary operators and ufuncs should interact.
(Note that in earlier iterations, the override was called
``__numpy_ufunc__``. An implementation was made, but had not quite the
right behaviour, hence the change in name.)

The ``__array_ufunc__`` as described below requires that any
corresponding Python binary operations (``__mul__`` et al.) should be
implemented in a specific way and be compatible with NumPy's ndarray
semantics. Objects that do not satisfy this cannot override any NumPy
ufuncs.  We do not specify a future-compatible path by which this
requirement can be relaxed --- any changes here require corresponding
changes in 3rd party code.

.. [1] http://docs.python.org/doc/numpy/user/basics.subclassing.html
.. [2] https://github.com/scipy/scipy/issues/2123
.. [3] https://github.com/scipy/scipy/issues/1569
.. [4] https://technicaldiscovery.blogspot.com/2013/07/thoughts-after-scipy-2013-and-specific.html


Motivation
==========

The current machinery for dispatching Ufuncs is generally agreed to be
insufficient. There have been lengthy discussions and other proposed
solutions [5]_, [6]_.

Using ufuncs with subclasses of :class:`ndarray` is limited to
``__array_prepare__`` and ``__array_wrap__`` to prepare the output arguments,
but these don't allow you to for example change the shape or the data of
the arguments. Trying to ufunc things that don't subclass
:class:`ndarray` is even more difficult, as the input arguments tend to
be cast to object arrays, which ends up producing surprising results.

Take this example of ufuncs interoperability with sparse matrices.::

    In [1]: import numpy as np
    import scipy.sparse as sp

    a = np.random.randint(5, size=(3,3))
    b = np.random.randint(5, size=(3,3))

    asp = sp.csr_matrix(a)
    bsp = sp.csr_matrix(b)

    In [2]: a, b
    Out[2]:(array([[0, 4, 4],
                   [1, 3, 2],
                   [1, 3, 1]]),
            array([[0, 1, 0],
                   [0, 0, 1],
                   [4, 0, 1]]))

    In [3]: np.multiply(a, b) # The right answer
    Out[3]: array([[0, 4, 0],
                   [0, 0, 2],
                   [4, 0, 1]])

    In [4]: np.multiply(asp, bsp).todense() # calls __mul__ which does matrix multi
    Out[4]: matrix([[16,  0,  8],
                    [ 8,  1,  5],
                    [ 4,  1,  4]], dtype=int64)

    In [5]: np.multiply(a, bsp) # Returns NotImplemented to user, bad!
    Out[5]: NotImplemented

Returning :obj:`NotImplemented` to user should not happen. Moreover::

    In [6]: np.multiply(asp, b)
    Out[6]: array([[ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>],
                       [ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>],
                       [ <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>,
                        <3x3 sparse matrix of type '<class 'numpy.int64'>'
                    with 8 stored elements in Compressed Sparse Row format>]], dtype=object)

Here, it appears that the sparse matrix was converted to an object array
scalar, which was then multiplied with all elements of the ``b`` array.
However, this behavior is more confusing than useful, and having a
:exc:`TypeError` would be preferable.

This proposal will *not* resolve the issue with scipy.sparse matrices,
which have multiplication semantics incompatible with NumPy arrays.
However, the aim is to enable writing other custom array types that have
strictly ndarray compatible semantics.

.. [5] https://mail.python.org/pipermail/numpy-discussion/2011-June/056945.html

.. [6] https://github.com/numpy/numpy/issues/5844


Proposed interface
==================

The standard array class :class:`ndarray` gains an ``__array_ufunc__``
method and objects can override Ufuncs by overriding this method (if
they are :class:`ndarray` subclasses) or defining their own. The method
signature is::

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs)

Here:

- *ufunc* is the ufunc object that was called.
- *method* is a string indicating how the Ufunc was called, either
  ``"__call__"`` to indicate it was called directly, or one of its
  methods: ``"reduce"``, ``"accumulate"``,  ``"reduceat"``, ``"outer"``,
  or ``"at"``.
- *inputs* is a tuple of the input arguments to the ``ufunc``
- *kwargs* contains any optional or keyword arguments passed to the
  function. This includes any ``out`` arguments, which are always
  contained in a tuple.

Hence, the arguments are normalized: only the required input arguments
(``inputs``) are passed on as positional arguments, all the others are
passed on as a dict of keyword arguments (``kwargs``). In particular, if
there are output arguments, positional are otherwise, that are not
:obj:`None`, they are passed on as a tuple in the ``out`` keyword
argument (even for the ``reduce``, ``accumulate``, and ``reduceat`` methods
where in all current cases only a single output makes sense).

The function dispatch proceeds as follows:

- If one of the input or output arguments implements
  ``__array_ufunc__``, it is executed instead of the ufunc.

- If more than one of the arguments implements ``__array_ufunc__``,
  they are tried in the following order: subclasses before superclasses,
  inputs before outputs, otherwise left to right.

- The first ``__array_ufunc__`` method returning something else than
  :obj:`NotImplemented` determines the return value of the Ufunc.

- If all ``__array_ufunc__`` methods of the input arguments return
  :obj:`NotImplemented`, a :exc:`TypeError` is raised.

- If a ``__array_ufunc__`` method raises an error, the error is
  propagated immediately.

- If none of the input arguments had an ``__array_ufunc__`` method, the
  execution falls back on the default ufunc behaviour.

In the above, there is one proviso: if a class has an
``__array_ufunc__`` attribute but it is identical to
``ndarray.__array_ufunc__``, the attribute is ignored.  This happens for
instances of `ndarray` and for `ndarray` subclasses that did not
override their inherited ``__array_ufunc__`` implementation.


Type casting hierarchy
----------------------

The Python operator override mechanism gives much freedom in how to
write the override methods, and it requires some discipline in order to
achieve predictable results. Here, we discuss an approach for
understanding some of the implications, which can provide input in the
design.

It is useful to maintain a clear idea of what types can be "upcast" to
others, possibly indirectly (e.g. indirect A->B->C is implemented but
direct A->C not). If the implementations of ``__array_ufunc__`` follow a
coherent type casting hierarchy, it can be used to understand results of
operations.

Type casting can be expressed as a `graph <https://en.wikipedia.org/wiki/Graph_theory>`__
defined as follows:

    For each ``__array_ufunc__`` method, draw directed edges from each
    possible input type to each possible output type.

    That is, in each case where ``y = x.__array_ufunc__(a, b, c, ...)``
    does something else than returning ``NotImplemented`` or raising an error,
    draw edges ``type(a) -> type(y)``, ``type(b) -> type(y)``, ...

If the resulting graph is *acyclic*, it defines a coherent type casting
hierarchy (unambiguous partial ordering between types).  In this case,
operations involving multiple types generally predictably produce result
of the "highest" type, or raise a :exc:`TypeError`.  See examples at the
end of this section.

If the graph has cycles, the ``__array_ufunc__`` type casting is not
well-defined, and things such as ``type(multiply(a, b)) !=
type(multiply(b, a))`` or ``type(add(a, add(b, c))) != type(add(add(a,
b), c))`` are not excluded (and then probably always possible).

If the type casting hierarchy is well defined, for each class A, all
other classes that define ``__array_ufunc__`` belong to exactly one of
three groups:

- *Above A*: the types that A can be (indirectly) upcast to in ufuncs.

- *Below A*: the types that can be (indirectly) upcast to A in ufuncs.

- *Incompatible*: neither above nor below A; types for which no
  (indirect) upcasting is possible.

Note that the legacy behaviour of NumPy ufuncs is to try to convert
unknown objects to :class:`ndarray` via :func:`np.asarray`.  This is
equivalent to placing :class:`ndarray` above these objects in the graph.
Since we above defined :class:`ndarray` to return `NotImplemented` for
classes with custom ``__array_ufunc__``, this puts :class:`ndarray`
below such classes in the type hierarchy, allowing the operations to be
overridden.

In view of the above, binary ufuncs describing transitive operations
should aim to define a well-defined casting hierarchy.  This is likely
also a sensible approach to all ufuncs --- exceptions to this should
consider carefully if any surprising behavior results.

.. admonition:: Example

   Type casting hierarchy.

   .. image:: _static/nep0013_image1.png

   The ``__array_ufunc__`` of type A can handle ndarrays returning C,
   B can handle ndarray and D returning B, and C can handle A and B returning C,
   but not ndarrays or D.  The
   result is a directed acyclic graph, and defines a type casting
   hierarchy, with relations ``C > A``, ``C > ndarray``, ``C > B > ndarray``,
   ``C > B > D``. The type A is incompatible with B, D, ndarray,
   and D is incompatible with A and ndarray.  Ufunc
   expressions involving these classes should produce results of the
   highest type involved or raise a :exc:`TypeError`.

.. admonition:: Example

   One-cycle in the ``__array_ufunc__`` graph.

   .. image:: _static/nep0013_image2.png

   In this case, the ``__array_ufunc__`` relations have a cycle of length 1,
   and a type casting hierarchy does not exist. Binary operations are not
   commutative: ``type(a + b) is A`` but ``type(b + a) is B``.

.. admonition:: Example

   Longer cycle in the ``__array_ufunc__`` graph.

   .. image:: _static/nep0013_image3.png

   In this case, the ``__array_ufunc__`` relations have a longer cycle, and a
   type casting hierarchy does not exist. Binary operations are still
   commutative, but type transitivity is lost: ``type(a + (b + c)) is A`` but
   ``type((a + b) + c) is C``.


Subclass hierarchies
--------------------

Generally, it is desirable to mirror the class hierarchy in the ufunc
type casting hierarchy. The recommendation is that an
``__array_ufunc__`` implementation of a class should generally return
`NotImplemented` unless the inputs are instances of the same class or
superclasses.  This guarantees that in the type casting hierarchy,
superclasses are below, subclasses above, and other classes are
incompatible.  Exceptions to this need to check they respect the
implicit type casting hierarchy.

.. note::

   Note that type casting hierarchy and class hierarchy are here defined
   to go the "opposite" directions.  It would in principle also be
   consistent to have ``__array_ufunc__`` handle also instances of
   subclasses. In this case, the "subclasses first" dispatch rule would
   ensure a relatively similar outcome. However, the behavior is then less
   explicitly specified.

Subclasses can be easily constructed if methods consistently use
:func:`super` to pass through the class hierarchy [7]_.  To support
this, :class:`ndarray` has its own ``__array_ufunc__`` method,
equivalent to::

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        # Cannot handle items that have __array_ufunc__ (other than our own).
        outputs = kwargs.get('out', ())
        for item in inputs + outputs:
            if (hasattr(item, '__array_ufunc__') and
                    type(item).__array_ufunc__ is not ndarray.__array_ufunc__):
                return NotImplemented

        # If we didn't have to support legacy behaviour (__array_prepare__,
        # __array_wrap__, etc.), we might here convert python floats,
        # lists, etc, to arrays with
        # items = [np.asarray(item) for item in inputs]
        # and then start the right iterator for the given method.
        # However, we do have to support legacy, so call back into the ufunc.
        # Its arguments are now guaranteed not to have __array_ufunc__
        # overrides, and it will do the coercion to array for us.
        return getattr(ufunc, method)(*items, **kwargs)

Note that, as a special case, the ufunc dispatch mechanism does not call
this `ndarray.__array_ufunc__` method, even for `ndarray` subclasses
if they have not overridden the default `ndarray` implementation. As a
consequence, calling `ndarray.__array_ufunc__` will not result to a
nested ufunc dispatch cycle.

The use of :func:`super` should be particularly useful for subclasses of
:class:`ndarray` that only add an attribute like a unit.  In their
`__array_ufunc__` implementation, such classes can do possible
adjustment of the arguments relevant to their own class, and pass on to
the superclass implementation using :func:`super` until the ufunc is
actually done, and then do possible adjustments of the outputs.

In general, custom implementations of `__array_ufunc__` should avoid
nested dispatch cycles, where one not just calls the ufunc via
``getattr(ufunc, method)(*items, **kwargs)``, but catches possible
exceptions, etc.  As always, there may be exceptions. For instance, for a
class like :class:`MaskedArray`, which only cares that whatever
it contains is an :class:`ndarray` subclass, a reimplementation with
``__array_ufunc__`` may well be more easily done by directly applying
the ufunc to its data, and then adjusting the mask.  Indeed, one can
think of this as part of the class determining whether it can handle the
other argument (i.e., where in the type hierarchy it sits). In this
case, one should return :obj:`NotImplemented` if the trial fails.  So,
the implementation would be something like::

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
        # for simplicity, outputs are ignored here.
        unmasked_items = tuple((item.data if isinstance(item, MaskedArray)
                                else item) for item in inputs)
        try:
            unmasked_result = getattr(ufunc, method)(*unmasked_items, **kwargs)
        except TypeError:
            return NotImplemented
        # for simplicity, ignore that unmasked_result could be a tuple
        # or a scalar.
        if not isinstance(unmasked_result, np.ndarray):
            return NotImplemented
        # now combine masks and view as MaskedArray instance
        ...

As a specific example, consider a quantity and a masked array class
which both override ``__array_ufunc__``, with specific instances ``q``
and ``ma``, where the latter contains a regular array. Executing
``np.multiply(q, ma)``, the ufunc will first dispatch to
``q.__array_ufunc__``, which returns :obj:`NotImplemented` (since the
quantity class turns itself into an array and calls :func:`super`, which
passes on to ``ndarray.__array_ufunc__``, which sees the override on
``ma``). Next, ``ma.__array_ufunc__`` gets a chance. It does not know
quantity, and if it were to just return :obj:`NotImplemented` as well,
an :exc:`TypeError` would result. But in our sample implementation, it
uses ``getattr(ufunc, method)`` to, effectively, evaluate
``np.multiply(q, ma.data)``. This again will pass to
``q.__array_ufunc__``, but this time, since ``ma.data`` is a regular
array, it will return a result that is also a quantity. Since this is a
subclass of :class:`ndarray`, ``ma.__array_ufunc__`` can turn this into
a masked array and thus return a result (obviously, if it was not a
array subclass, it could still return :obj:`NotImplemented`).

Note that in the context of the type hierarchy discussed above this is a
somewhat tricky example, since :class:`MaskedArray` has a strange
position: it is above all subclasses of :class:`ndarray`, in that it can
cast them to its own type, but it does not itself know how to interact
with them in ufuncs.

.. [7] https://rhettinger.wordpress.com/2011/05/26/super-considered-super/

.. _neps.ufunc-overrides.turning-ufuncs-off:

Turning Ufuncs off
------------------

For some classes, Ufuncs make no sense, and, like for some other special
methods such as ``__hash__`` and ``__iter__`` [8]_, one can indicate
Ufuncs are not available by setting ``__array_ufunc__`` to :obj:`None`.
If a Ufunc is called on any operand that sets ``__array_ufunc__ = None``,
it will unconditionally raise :exc:`TypeError`.

In the type casting hierarchy, this makes it explicit that the type is
incompatible relative to :class:`ndarray`.

.. [8] https://docs.python.org/3/reference/datamodel.html#specialnames

Behavior in combination with Python's binary operations
-------------------------------------------------------

The Python operator override mechanism in :class:`ndarray` is coupled to
the ``__array_ufunc__`` mechanism. For the special methods calls such as
``ndarray.__mul__(self, other)`` that Python calls for implementing
binary operations such as ``*`` and ``+``, NumPy's :class:`ndarray`
implements the following behavior:

- If ``other.__array_ufunc__ is None``, :class:`ndarray`
  returns :obj:`NotImplemented`. Control reverts to Python, which in turn
  will try calling a corresponding reflexive method on ``other`` (e.g.,
  ``other.__rmul__``), if present.
- If the ``__array_ufunc__`` attribute is absent on ``other`` and
  ``other.__array_priority__ > self.__array_priority__``, :class:`ndarray`
  also returns :obj:`NotImplemented` (and the logic proceeds as in the
  previous case). This ensures backwards compatibility with old versions
  of NumPy.
- Otherwise, :class:`ndarray` unilaterally calls the corresponding Ufunc.
  Ufuncs never return ``NotImplemented``, so **reflexive methods such
  as** ``other.__rmul__`` **cannot be used to override arithmetic with
  NumPy arrays if** ``__array_ufunc__`` **is set** to any value other than
  ``None``. Instead, their behavior needs to be changed by implementing
  ``__array_ufunc__`` in a fashion consistent with the corresponding Ufunc,
  e.g., ``np.multiply``. See :ref:`neps.ufunc-overrides.list-of-operators`
  for a list of affected operators and their corresponding ufuncs.

A class wishing to modify the interaction with :class:`ndarray` in
binary operations therefore has two options:

1. Implement ``__array_ufunc__`` and follow NumPy semantics for Python
   binary operations (see below).

2. Set ``__array_ufunc__ = None``, and implement Python binary
   operations freely.  In this case, ufuncs called on this argument will
   raise :exc:`TypeError` (see
   :ref:`neps.ufunc-overrides.turning-ufuncs-off`).

Recommendations for implementing binary operations
--------------------------------------------------

For most numerical classes, the easiest way to override binary
operations is thus to define ``__array_ufunc__`` and override the
corresponding Ufunc. The class can then, like :class:`ndarray` itself,
define the binary operators in terms of Ufuncs. Here, one has to take
some care to ensure that one allows for other classes to indicate they
are not compatible, i.e., implementations should be something like::

    def _disables_array_ufunc(obj):
        try:
            return obj.__array_ufunc__ is None
        except AttributeError:
            return False

    class ArrayLike:
        ...
        def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
            ...
            return result

        # Option 1: call ufunc directly
        def __mul__(self, other):
            if _disables_array_ufunc(other):
                return NotImplemented
            return np.multiply(self, other)

        def __rmul__(self, other):
            if _disables_array_ufunc(other):
                return NotImplemented
            return np.multiply(other, self)

        def __imul__(self, other):
            return np.multiply(self, other, out=(self,))

        # Option 2: call into one's own __array_ufunc__
        def __mul__(self, other):
            return self.__array_ufunc__(np.multiply, '__call__', self, other)

        def __rmul__(self, other):
            return self.__array_ufunc__(np.multiply, '__call__', other, self)

        def __imul__(self, other):
            result = self.__array_ufunc__(np.multiply, '__call__', self, other,
                                          out=(self,))
            if result is NotImplemented:
                raise TypeError(...)

To see why some care is necessary, consider another class ``other`` that
does not know how to deal with arrays and ufuncs, and thus has set
``__array_ufunc__`` to :obj:`None`, but does know how to do
multiplication::

    class MyObject:
        __array_ufunc__ = None
        def __init__(self, value):
            self.value = value
        def __repr__(self):
            return "MyObject({!r})".format(self.value)
        def __mul__(self, other):
            return MyObject(1234)
        def __rmul__(self, other):
            return MyObject(4321)

For either option above, we get the expected result::

    mine = MyObject(0)
    arr = ArrayLike([0])

    mine * arr    # -> MyObject(1234)
    mine *= arr   # -> MyObject(1234)
    arr * mine    # -> MyObject(4321)
    arr *= mine   # -> TypeError

Here, in the first and second example, ``mine.__mul__(arr)`` gets called
and the result arrives immediately.  In the third example, first
``arr.__mul__(mine)`` is called. In option (1), the check on
``mine.__array_ufunc__ is None`` will succeed and thus
:obj:`NotImplemented` is returned, which causes ``mine.__rmul__(arg)``
to be executed.  In option (2), it is presumably inside
``arr.__array_ufunc__`` that it becomes clear that the other argument
cannot be dealt with, and again :obj:`NotImplemented` is returned,
causing control to pass to ``mine.__rmul__``.

For the fourth example, with the in-place operators, we have here
followed :class:`ndarray` and ensure we never return
:obj:`NotImplemented`, but rather raise a :exc:`TypeError`. In
option (1) this happens indirectly: we pass to ``np.multiply``, which
in turn immediately raises :exc:`TypeError`, because one of its operands
(``out[0]``) disables Ufuncs.  In option (2), we pass directly to
``arr.__array_ufunc__``, which will return :obj:`NotImplemented`, which
we catch.

.. note:: the reason for not allowing in-place operations to return
   :obj:`NotImplemented` is that these cannot generically be replaced by
   a simple reverse operation: most array operations assume the contents
   of the instance are changed in-place, and do not expect a new
   instance.  Also, what would ``ndarr[:] *= mine`` imply?  Assuming it
   means ``ndarr[:] = ndarr[:] * mine``, as python does by default if
   the ``ndarr.__imul__`` were to return :obj:`NotImplemented`, is
   likely to be wrong.

Now consider what would happen if we had not added checks. For option
(1), the relevant case is if we had not checked whether
``__array_func__`` was set to :obj:`None`.  In the third example,
``arr.__mul__(mine)`` is called, and without the check, this would go to
``np.multiply(arr, mine)``. This tries ``arr.__array_ufunc__``, which
returns :obj:`NotImplemented` and sees that ``mine.__array_ufunc__ is
None``, so a :exc:`TypeError` is raised.

For option (2), the relevant example is the fourth, with ``arr *=
mine``: if we had let the :obj:`NotImplemented` pass, python would have
replaced this with ``arr = mine.__rmul__(arr)``, which is not wanted.

Because the semantics of Ufunc overrides and Python's binary operations
are nearly identical, in most cases options (1) and (2) will
yield the same result with the same implementation of ``__array_ufunc__``.
One exception is the order in which implementations are tried when the
second argument is a subclass of the first argument, due to a Python
bug [9]_ expected to be fixed in Python 3.7.

In general, we recommend adopting option (1), which is the option most
similar to that used by :class:`ndarray` itself. Note that option (1)
is viral, in the sense that any other class that wishes to support binary
operations with your class now must also follow these rules for supporting
binary arithmetic with :class:`ndarray` (i.e., they must either implement
``__array_ufunc__`` or set it to :obj:`None`). We believe this is a good
thing, because it ensures the consistency of ufuncs and arithmetic on all
objects that support them.

To make implementing such array-like classes easier, the mixin class
:class:`~numpy.lib.mixins.NDArrayOperatorsMixin` provides option (1) style
overrides for all binary operators with corresponding Ufuncs. Classes
that wish to implement ``__array_ufunc__`` for compatible versions
of NumPy but that also need to support binary arithmetic with NumPy arrays
on older versions should ensure that ``__array_ufunc__`` can also be used
to implement all binary operations they support.

Finally, we note that we had extensive discussion about whether it might
make more sense to ask classes like ``MyObject`` to implement a full
``__array_ufunc__`` [6]_. In the end, allowing classes to opt out was
preferred, and the above reasoning led us to agree on a similar
implementation for :class:`ndarray` itself. The opt-out mechanism requires
disabling Ufuncs so a class cannot define a Ufuncs to return a different
result than the corresponding binary operations (i.e., if
``np.add(x, y)`` is defined, it should match ``x + y``). Our goal was to
simplify the dispatch logic for binary operations with NumPy arrays
as much as possible, by making it possible to use Python's dispatch rules
or NumPy's dispatch rules, but not some mixture of both at the same time.

.. [9] https://bugs.python.org/issue30140

.. _neps.ufunc-overrides.list-of-operators:

List of operators and NumPy Ufuncs
----------------------------------

Here is a full list of Python binary operators and the corresponding NumPy
Ufuncs used by :class:`ndarray` and
:class:`~numpy.lib.mixins.NDArrayOperatorsMixin`:

====== ============ =========================================
Symbol Operator     NumPy Ufunc(s)
====== ============ =========================================
``<``  ``lt``       :func:`less`
``<=`` ``le``       :func:`less_equal`
``==`` ``eq``       :func:`equal`
``!=`` ``ne``       :func:`not_equal`
``>``  ``gt``       :func:`greater`
``>=`` ``ge``       :func:`greater_equal`
``+``  ``add``      :func:`add`
``-``  ``sub``      :func:`subtract`
``*``  ``mul``      :func:`multiply`
``/``  ``truediv``  :func:`true_divide`
       (Python 3)
``/``  ``div``      :func:`divide`
       (Python 2)
``//`` ``floordiv`` :func:`floor_divide`
``%``  ``mod``      :func:`remainder`
NA     ``divmod``   :func:`divmod`
``**`` ``pow``      :func:`power` [10]_
``<<`` ``lshift``   :func:`left_shift`
``>>`` ``rshift``   :func:`right_shift`
``&``  ``and_``     :func:`bitwise_and`
``^``  ``xor_``     :func:`bitwise_xor`
``|``  ``or_``      :func:`bitwise_or`
``@``  ``matmul``   Not yet implemented as a ufunc [11]_
====== ============ =========================================

And here is the list of unary operators:

====== ============ =========================================
Symbol Operator     NumPy Ufunc(s)
====== ============ =========================================
``-``  ``neg``      :func:`negative`
``+``  ``pos``      :func:`positive` [12]_
NA     ``abs``      :func:`absolute`
``~``  ``invert``   :func:`invert`
====== ============ =========================================

.. [10] class :`ndarray` takes short cuts for ``__pow__`` for the
        cases where the power equals ``1`` (:func:`positive`),
        ``-1`` (:func:`reciprocal`), ``2`` (:func:`square`), ``0`` (an
        otherwise private ``_ones_like`` ufunc), and ``0.5``
        (:func:`sqrt`), and the array is float or complex (or integer
        for square).
.. [11] Because NumPy's :func:`matmul` is not a ufunc, it is
        `currently not possible <https://github.com/numpy/numpy/issues/9028>`_
        to override ``numpy_array @ other`` with ``other`` taking precedence
        if ``other`` implements ``__array_func__``.
.. [12] :class:`ndarray` currently does a copy instead of using this ufunc.

Future extensions to other functions
------------------------------------

Some NumPy functions could be implemented as (generalized) Ufunc, in
which case it would be possible for them to be overridden by the
``__array_ufunc__`` method.  A prime candidate is :func:`~numpy.matmul`,
which currently is not a Ufunc, but could be relatively easily be
rewritten as a (set of) generalized Ufuncs. The same may happen with
functions such as :func:`~numpy.median`, :func:`~numpy.min`, and
:func:`~numpy.argsort`.


.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 72
.. End:

.. _NEP01:

=============================================
NEP 1 — A simple file format for NumPy arrays
=============================================

:Author: Robert Kern <robert.kern@gmail.com>
:Status: Final
:Created: 20-Dec-2007

Abstract
--------

We propose a standard binary file format (NPY) for persisting
a single arbitrary NumPy array on disk.  The format stores all of
the shape and dtype information necessary to reconstruct the array
correctly even on another machine with a different architecture.
The format is designed to be as simple as possible while achieving
its limited goals.  The implementation is intended to be pure
Python and distributed as part of the main numpy package.


Rationale
---------

A lightweight, omnipresent system for saving NumPy arrays to disk
is a frequent need.  Python in general has pickle [1] for saving
most Python objects to disk.  This often works well enough with
NumPy arrays for many purposes, but it has a few drawbacks:

- Dumping or loading a pickle file require the duplication of the
  data in memory.  For large arrays, this can be a showstopper.

- The array data is not directly accessible through
  memory-mapping.  Now that numpy has that capability, it has
  proved very useful for loading large amounts of data (or more to
  the point: avoiding loading large amounts of data when you only
  need a small part).

Both of these problems can be addressed by dumping the raw bytes
to disk using ndarray.tofile() and numpy.fromfile().  However,
these have their own problems:

- The data which is written has no information about the shape or
  dtype of the array.

- It is incapable of handling object arrays.

The NPY file format is an evolutionary advance over these two
approaches.  Its design is mostly limited to solving the problems
with pickles and tofile()/fromfile().  It does not intend to solve
more complicated problems for which more complicated formats like
HDF5 [2] are a better solution.


Use Cases
---------

- Neville Newbie has just started to pick up Python and NumPy.  He
  has not installed many packages, yet, nor learned the standard
  library, but he has been playing with NumPy at the interactive
  prompt to do small tasks.  He gets a result that he wants to
  save.

- Annie Analyst has been using large nested record arrays to
  represent her statistical data.  She wants to convince her
  R-using colleague, David Doubter, that Python and NumPy are
  awesome by sending him her analysis code and data.  She needs
  the data to load at interactive speeds.  Since David does not
  use Python usually, needing to install large packages would turn
  him off.

- Simon Seismologist is developing new seismic processing tools.
  One of his algorithms requires large amounts of intermediate
  data to be written to disk.  The data does not really fit into
  the industry-standard SEG-Y schema, but he already has a nice
  record-array dtype for using it internally.

- Polly Parallel wants to split up a computation on her multicore
  machine as simply as possible.  Parts of the computation can be
  split up among different processes without any communication
  between processes; they just need to fill in the appropriate
  portion of a large array with their results.  Having several
  child processes memory-mapping a common array is a good way to
  achieve this.


Requirements
------------

The format MUST be able to:

- Represent all NumPy arrays including nested record
  arrays and object arrays.

- Represent the data in its native binary form.

- Be contained in a single file.

- Support Fortran-contiguous arrays directly.

- Store all of the necessary information to reconstruct the array
  including shape and dtype on a machine of a different
  architecture.  Both little-endian and big-endian arrays must be
  supported and a file with little-endian numbers will yield
  a little-endian array on any machine reading the file.  The
  types must be described in terms of their actual sizes.  For
  example, if a machine with a 64-bit C "long int" writes out an
  array with "long ints", a reading machine with 32-bit C "long
  ints" will yield an array with 64-bit integers.

- Be reverse engineered.  Datasets often live longer than the
  programs that created them.  A competent developer should be
  able to create a solution in his preferred programming language to
  read most NPY files that he has been given without much
  documentation.

- Allow memory-mapping of the data.

- Be read from a filelike stream object instead of an actual file.
  This allows the implementation to be tested easily and makes the
  system more flexible.  NPY files can be stored in ZIP files and
  easily read from a ZipFile object.

- Store object arrays.  Since general Python objects are
  complicated and can only be reliably serialized by pickle (if at
  all), many of the other requirements are waived for files
  containing object arrays.  Files with object arrays do not have
  to be mmapable since that would be technically impossible.  We
  cannot expect the pickle format to be reverse engineered without
  knowledge of pickle.  However, one should at least be able to
  read and write object arrays with the same generic interface as
  other arrays.

- Be read and written using APIs provided in the numpy package
  itself without any other libraries.  The implementation inside
  numpy may be in C if necessary.

The format explicitly *does not* need to:

- Support multiple arrays in a file.  Since we require filelike
  objects to be supported, one could use the API to build an ad
  hoc format that supported multiple arrays.  However, solving the
  general problem and use cases is beyond the scope of the format
  and the API for numpy.

- Fully handle arbitrary subclasses of numpy.ndarray.  Subclasses
  will be accepted for writing, but only the array data will be
  written out.  A regular numpy.ndarray object will be created
  upon reading the file.  The API can be used to build a format
  for a particular subclass, but that is out of scope for the
  general NPY format.


Format Specification: Version 1.0
---------------------------------

The first 6 bytes are a magic string: exactly "\x93NUMPY".

The next 1 byte is an unsigned byte: the major version number of
the file format, e.g. \x01.

The next 1 byte is an unsigned byte: the minor version number of
the file format, e.g. \x00.  Note: the version of the file format
is not tied to the version of the numpy package.

The next 2 bytes form a little-endian unsigned short int: the
length of the header data HEADER_LEN.

The next HEADER_LEN bytes form the header data describing the
array's format.  It is an ASCII string which contains a Python
literal expression of a dictionary.  It is terminated by a newline
('\n') and padded with spaces ('\x20') to make the total length of
the magic string + 4 + HEADER_LEN be evenly divisible by 16 for
alignment purposes.

The dictionary contains three keys:

    "descr" : dtype.descr
        An object that can be passed as an argument to the
        numpy.dtype() constructor to create the array's dtype.

    "fortran_order" : bool
        Whether the array data is Fortran-contiguous or not.
        Since Fortran-contiguous arrays are a common form of
        non-C-contiguity, we allow them to be written directly to
        disk for efficiency.

    "shape" : tuple of int
        The shape of the array.

For repeatability and readability, this dictionary is formatted
using pprint.pformat() so the keys are in alphabetic order.

Following the header comes the array data.  If the dtype contains
Python objects (i.e. dtype.hasobject is True), then the data is
a Python pickle of the array.  Otherwise the data is the
contiguous (either C- or Fortran-, depending on fortran_order)
bytes of the array.  Consumers can figure out the number of bytes
by multiplying the number of elements given by the shape (noting
that shape=() means there is 1 element) by dtype.itemsize.

Format Specification: Version 2.0
---------------------------------

The version 1.0 format only allowed the array header to have a
total size of 65535 bytes.  This can be exceeded by structured
arrays with a large number of columns.  The version 2.0 format
extends the header size to 4 GiB.  `numpy.save` will automatically
save in 2.0 format if the data requires it, else it will always use
the more compatible 1.0 format.

The description of the fourth element of the header therefore has
become:

    The next 4 bytes form a little-endian unsigned int: the length
    of the header data HEADER_LEN.

Conventions
-----------

We recommend using the ".npy" extension for files following this
format.  This is by no means a requirement; applications may wish
to use this file format but use an extension specific to the
application.  In the absence of an obvious alternative, however,
we suggest using ".npy".

For a simple way to combine multiple arrays into a single file,
one can use ZipFile to contain multiple ".npy" files.  We
recommend using the file extension ".npz" for these archives.


Alternatives
------------

The author believes that this system (or one along these lines) is
about the simplest system that satisfies all of the requirements.
However, one must always be wary of introducing a new binary
format to the world.

HDF5 [2] is a very flexible format that should be able to
represent all of NumPy's arrays in some fashion.  It is probably
the only widely-used format that can faithfully represent all of
NumPy's array features.  It has seen substantial adoption by the
scientific community in general and the NumPy community in
particular.  It is an excellent solution for a wide variety of
array storage problems with or without NumPy.

HDF5 is a complicated format that more or less implements
a hierarchical filesystem-in-a-file.  This fact makes satisfying
some of the Requirements difficult.  To the author's knowledge, as
of this writing, there is no application or library that reads or
writes even a subset of HDF5 files that does not use the canonical
libhdf5 implementation.  This implementation is a large library
that is not always easy to build.  It would be infeasible to
include it in numpy.

It might be feasible to target an extremely limited subset of
HDF5.  Namely, there would be only one object in it: the array.
Using contiguous storage for the data, one should be able to
implement just enough of the format to provide the same metadata
that the proposed format does.  One could still meet all of the
technical requirements like mmapability.

We would accrue a substantial benefit by being able to generate
files that could be read by other HDF5 software.  Furthermore, by
providing the first non-libhdf5 implementation of HDF5, we would
be able to encourage more adoption of simple HDF5 in applications
where it was previously infeasible because of the size of the
library.  The basic work may encourage similar dead-simple
implementations in other languages and further expand the
community.

The remaining concern is about reverse engineerability of the
format.  Even the simple subset of HDF5 would be very difficult to
reverse engineer given just a file by itself.  However, given the
prominence of HDF5, this might not be a substantial concern.

In conclusion, we are going forward with the design laid out in
this document.  If someone writes code to handle the simple subset
of HDF5 that would be useful to us, we may consider a revision of
the file format.


Implementation
--------------

The version 1.0 implementation was first included in the 1.0.5 release of
numpy, and remains available.  The version 2.0 implementation was first
included in the 1.9.0 release of numpy.

Specifically, the file format.py in this directory implements the
format as described here.

    https://github.com/numpy/numpy/blob/main/numpy/lib/format.py


References
----------

[1] https://docs.python.org/library/pickle.html

[2] https://support.hdfgroup.org/HDF5/


Copyright
---------

This document has been placed in the public domain.

.. _NEP22:

===========================================================
NEP 22 — Duck typing for NumPy arrays – high level overview
===========================================================

:Author: Stephan Hoyer <shoyer@google.com>, Nathaniel J. Smith <njs@pobox.com>
:Status: Final
:Type: Informational
:Created: 2018-03-22
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-September/078752.html

Abstract
--------

We outline a high-level vision for how NumPy will approach handling
“duck arrays”. This is an Informational-class NEP; it doesn’t
prescribe full details for any particular implementation. In brief, we
propose developing a number of new protocols for defining
implementations of multi-dimensional arrays with high-level APIs
matching NumPy.


Detailed description
--------------------

Traditionally, NumPy’s ``ndarray`` objects have provided two things: a
high level API for expression operations on homogeneously-typed,
arbitrary-dimensional, array-structured data, and a concrete
implementation of the API based on strided in-RAM storage. The API is
powerful, fairly general, and used ubiquitously across the scientific
Python stack. The concrete implementation, on the other hand, is
suitable for a wide range of uses, but has limitations: as data sets
grow and NumPy becomes used in a variety of new environments, there
are increasingly cases where the strided in-RAM storage strategy is
inappropriate, and users find they need sparse arrays, lazily
evaluated arrays (as in dask), compressed arrays (as in blosc), arrays
stored in GPU memory, arrays stored in alternative formats such as
Arrow, and so forth – yet users still want to work with these arrays
using the familiar NumPy APIs, and re-use existing code with minimal
(ideally zero) porting overhead. As a working shorthand, we call these
“duck arrays”, by analogy with Python’s “duck typing”: a “duck array”
is a Python object which “quacks like” a numpy array in the sense that
it has the same or similar Python API, but doesn’t share the C-level
implementation.

This NEP doesn’t propose any specific changes to NumPy or other
projects; instead, it gives an overview of how we hope to extend NumPy
to support a robust ecosystem of projects implementing and relying
upon its high level API.

Terminology
~~~~~~~~~~~

“Duck array” works fine as a placeholder for now, but it’s pretty
jargony and may confuse new users, so we may want to pick something
else for the actual API functions. Unfortunately, “array-like” is
already taken for the concept of “anything that can be coerced into an
array” (including e.g. list objects), and “anyarray” is already taken
for the concept of “something that shares ndarray’s implementation,
but has different semantics”, which is the opposite of a duck array
(e.g., np.matrix is an “anyarray”, but is not a “duck array”). This is
a classic bike-shed so for now we’re just using “duck array”. Some
possible options though include: arrayish, pseudoarray, nominalarray,
ersatzarray, arraymimic, ...


General approach
~~~~~~~~~~~~~~~~

At a high level, duck array support requires working through each of
the API functions provided by NumPy, and figuring out how it can be
extended to work with duck array objects. In some cases this is easy
(e.g., methods/attributes on ndarray itself); in other cases it’s more
difficult. Here are some principles we’ve found useful so far:


Principle 1: Focus on “full” duck arrays, but don’t rule out “partial” duck arrays
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can distinguish between two classes:

* “full” duck arrays, which aspire to fully implement np.ndarray’s
  Python-level APIs and work essentially anywhere that np.ndarray
  works

* “partial” duck arrays, which intentionally implement only a subset
  of np.ndarray’s API.

Full duck arrays are, well, kind of boring. They have exactly the same
semantics as ndarray, with differences being restricted to
under-the-hood decisions about how the data is actually stored. The
kind of people that are excited about making numpy more extensible are
also, unsurprisingly, excited about changing or extending numpy’s
semantics. So there’s been a lot of discussion of how to best support
partial duck arrays. We've been guilty of this ourself.

At this point though, we think the best general strategy is to focus
our efforts primarily on supporting full duck arrays, and only worry
about partial duck arrays as much as we need to make sure we don't
accidentally rule them out for no reason.

Why focus on full duck arrays? Several reasons:

First, there are lots of very clear use cases. Potential consumers of
the full duck array interface include almost every package that uses
numpy (scipy, sklearn, astropy, ...), and in particular packages that
provide array-wrapping-classes that handle multiple types of arrays,
such as xarray and dask.array. Potential implementers of the full duck
array interface include: distributed arrays, sparse arrays, masked
arrays, arrays with units (unless they switch to using dtypes),
labeled arrays, and so forth. Clear use cases lead to good and
relevant APIs.

Second, the Anna Karenina principle applies here: full duck arrays are
all alike, but every partial duck array is partial in its own way:

* ``xarray.DataArray`` is mostly a duck array, but has incompatible
  broadcasting semantics.
* ``xarray.Dataset`` wraps multiple arrays in one object; it still
  implements some array interfaces like ``__array_ufunc__``, but
  certainly not all of them.
* ``pandas.Series`` has methods with similar behavior to numpy, but
  unique null-skipping behavior.
* scipy’s ``LinearOperator``\s support matrix multiplication and nothing else
* h5py and similar libraries for accessing array storage have objects
  that support numpy-like slicing and conversion into a full array,
  but not computation.
* Some classes may be similar to ndarray, but without supporting the
  full indexing semantics.

And so forth.

Despite our best attempts, we haven't found any clear, unique way of
slicing up the ndarray API into a hierarchy of related types that
captures these distinctions; in fact, it’s unlikely that any single
person even understands all the distinctions. And this is important,
because we have a *lot* of APIs that we need to add duck array support
to (both in numpy and in all the projects that depend on numpy!). By
definition, these already work for ``ndarray``, so hopefully getting
them to work for full duck arrays shouldn’t be so hard, since by
definition full duck arrays act like ``ndarray``. It’d be very
cumbersome to have to go through each function and identify the exact
subset of the ndarray API that it needs, then figure out which partial
array types can/should support it. Once we have things working for
full duck arrays, we can go back later and refine the APIs needed
further as needed. Focusing on full duck arrays allows us to start
making progress immediately.

In the future, it might be useful to identify specific use cases for
duck arrays and standardize narrower interfaces targeted just at those
use cases. For example, it might make sense to have a standard “array
loader” interface that file access libraries like h5py, netcdf, pydap,
zarr, ... all implement, to make it easy to switch between these
libraries. But that’s something that we can do as we go, and it
doesn’t necessarily have to involve the NumPy devs at all. For an
example of what this might look like, see the documentation for
`dask.array.from_array
<http://dask.pydata.org/en/latest/array-api.html#dask.array.from_array>`__.


Principle 2: Take advantage of duck typing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``ndarray`` has a very large API surface area::

    In [1]: len(set(dir(np.ndarray)) - set(dir(object)))
    Out[1]: 138

And this is a huge **under**\estimate, because there are also many
free-standing functions in NumPy and other libraries which currently
use the NumPy C API and thus only work on ``ndarray`` objects. In type
theory, a type is defined by the operations you can perform on an
object; thus, the actual type of ``ndarray`` includes not just its
methods and attributes, but *all* of these functions. For duck arrays
to be successful, they’ll need to implement a large proportion of the
``ndarray`` API – but not all of it. (For example,
``dask.array.Array`` does not provide an equivalent to the
``ndarray.ptp`` method, presumably because no-one has ever noticed or
cared about its absence. But this doesn’t seem to have stopped people
from using dask.)

This means that realistically, we can’t hope to define the whole duck
array API up front, or that anyone will be able to implement it all in
one go; this will be an incremental process. It also means that even
the so-called “full” duck array interface is somewhat fuzzily defined
at the borders; there are parts of the ``np.ndarray`` API that duck
arrays won’t have to implement, but we aren’t entirely sure what those
are.

And ultimately, it isn’t really up to the NumPy developers to define
what does or doesn’t qualify as a duck array. If we want scikit-learn
functions to work on dask arrays (for example), then that’s going to
require negotiation between those two projects to discover
incompatibilities, and when an incompatibility is discovered it will
be up to them to negotiate who should change and how. The NumPy
project can provide technical tools and general advice to help resolve
these disagreements, but we can’t force one group or another to take
responsibility for any given bug.

Therefore, even though we’re focusing on “full” duck arrays, we
*don’t* attempt to define a normative “array ABC” – maybe this will be
useful someday, but right now, it’s not. And as a convenient
side-effect, the lack of a normative definition leaves partial duck
arrays room to experiment.

But, we do provide some more detailed advice for duck array
implementers and consumers below.

Principle 3: Focus on protocols
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Historically, numpy has had lots of success at interoperating with
third-party objects by defining *protocols*, like ``__array__`` (asks
an arbitrary object to convert itself into an array),
``__array_interface__`` (a precursor to Python’s buffer protocol), and
``__array_ufunc__`` (allows third-party objects to support ufuncs like
``np.exp``).

`NEP 16 <https://github.com/numpy/numpy/pull/10706>`_ took a
different approach: we need a duck-array equivalent of
``asarray``, and it proposed to do this by defining a version of
``asarray`` that would let through objects which implemented a new
AbstractArray ABC. As noted above, we now think that trying to define
an ABC is a bad idea for other reasons. But when this NEP was
discussed on the mailing list, we realized that even on its own
merits, this idea is not so great. A better approach is to define a
*method* that can be called on an arbitrary object to ask it to
convert itself into a duck array, and then define a version of
``asarray`` that calls this method.

This is strictly more powerful: if an object is already a duck array,
it can simply ``return self``. It allows more correct semantics: NEP
16 assumed that ``asarray(obj, dtype=X)`` is the same as
``asarray(obj).astype(X)``, but this isn’t true. And it supports more
use cases: if h5py supported sparse arrays, it might want to provide
an object which is not itself a sparse array, but which can be
automatically converted into a sparse array. See NEP <XX, to be
written> for full details.

The protocol approach is also more consistent with core Python
conventions: for example, see the ``__iter__`` method for coercing
objects to iterators, or the ``__index__`` protocol for safe integer
coercion. And finally, focusing on protocols leaves the door open for
partial duck arrays, which can pick and choose which subset of the
protocols they want to participate in, each of which have well-defined
semantics.

Conclusion: protocols are one honking great idea – let’s do more of
those.

Principle 4: Reuse existing methods when possible
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It’s tempting to try to define cleaned up versions of ndarray methods
with a more minimal interface to allow for easier implementation. For
example, ``__array_reshape__`` could drop some of the strange
arguments accepted by ``reshape`` and ``__array_basic_getitem__``
could drop all the `strange edge cases
<http://www.numpy.org/neps/nep-0021-advanced-indexing.html>`__ of
NumPy’s advanced indexing.

But as discussed above, we don’t really know what APIs we need for
duck-typing ndarray. We would inevitably end up with a very long list
of new special methods. In contrast, existing methods like ``reshape``
and ``__getitem__`` have the advantage of already being widely
used/exercised by libraries that use duck arrays, and in practice, any
serious duck array type is going to have to implement them anyway.

Principle 5: Make it easy to do the right thing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Making duck arrays work well is going to be a community effort.
Documentation helps, but only goes so far. We want to make it easy to
implement duck arrays that do the right thing.

One way NumPy can help is by providing mixin classes for implementing
large groups of related functionality at once.
``NDArrayOperatorsMixin`` is a good example: it allows for
implementing arithmetic operators implicitly via the
``__array_ufunc__`` method. It’s not complete, and we’ll want more
helpers like that (e.g. for reductions).

(We initially thought that the importance of these mixins might be an
argument for providing an array ABC, since that’s the standard way to
do mixins in modern Python. But in discussion around NEP 16 we
realized that partial duck arrays also wanted to take advantage of
these mixins in some cases, so even if we did have an array ABC then
the mixins would still need some sort of separate existence. So never
mind that argument.)

Tentative duck array guidelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As a general rule, libraries using duck arrays should insist upon the
minimum possible requirements, and libraries implementing duck arrays
should provide as complete of an API as possible. This will ensure
maximum compatibility. For example, users should prefer to rely on
``.transpose()`` rather than ``.swapaxes()`` (which can be implemented
in terms of transpose), but duck array authors should ideally
implement both.

If you are trying to implement a duck array, then you should strive to
implement everything. You certainly need ``.shape``, ``.ndim`` and
``.dtype``, but also your dtype attribute should actually be a
``numpy.dtype`` object, weird fancy indexing edge cases should ideally
work, etc. Only details related to NumPy’s specific ``np.ndarray``
implementation (e.g., ``strides``, ``data``, ``view``) are explicitly
out of scope.

A (very) rough sketch of future plans
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The proposals discussed so far – ``__array_ufunc__`` and some kind of
``asarray`` protocol – are clearly necessary but not sufficient for
full duck typing support. We expect the need for additional protocols
to support (at least) these features:

* **Concatenating** duck arrays, which would be used internally by other
  array combining methods like stack/vstack/hstack. The implementation
  of concatenate will need to be negotiated among the list of array
  arguments. We expect to use an ``__array_concatenate__`` protocol
  like ``__array_ufunc__`` instead of multiple dispatch.
* **Ufunc-like functions** that currently aren’t ufuncs. Many NumPy
  functions like median, percentile, sort, where and clip could be
  written as generalized ufuncs but currently aren’t. Either these
  functions should be written as ufuncs, or we should consider adding
  another generic wrapper mechanism that works similarly to ufuncs but
  makes fewer guarantees about how the implementation is done.
* **Random number generation** with duck arrays, e.g.,
  ``np.random.randn()``. For example, we might want to add new APIs
  like ``random_like()`` for generating new arrays with a matching
  shape *and* type – though we'll need to look at some real examples
  of how these functions are used to figure out what would be helpful.
* **Miscellaneous other functions** such as ``np.einsum``,
  ``np.zeros_like``, and ``np.broadcast_to`` that don’t fall into any
  of the above categories.
* **Checking mutability** on duck arrays, which would imply that they
  support assignment with ``__setitem__`` and the out argument to
  ufuncs. Many otherwise fine duck arrays are not easily mutable (for
  example, because they use some kinds of sparse or compressed
  storage, or are in read-only shared memory), and it turns out that
  frequently-used code like the default implementation of ``np.mean``
  needs to check this (to decide whether it can re-use temporary
  arrays).

We intentionally do not describe exactly how to add support for these
types of duck arrays here. These will be the subject of future NEPs.


Copyright
---------

This document has been placed in the public domain.
.. _NEP37:

===================================================
NEP 37 — A dispatch protocol for NumPy-like modules
===================================================

:Author: Stephan Hoyer <shoyer@google.com>
:Author: Hameer Abbasi
:Author: Sebastian Berg
:Status: Draft
:Type: Standards Track
:Created: 2019-12-29

Abstract
--------

NEP-18's ``__array_function__`` has been a mixed success. Some projects (e.g.,
dask, CuPy, xarray, sparse, Pint, MXNet) have enthusiastically adopted it.
Others (e.g., JAX) have been more reluctant. Here we propose a new
protocol, ``__array_module__``, that we expect could eventually subsume most
use-cases for ``__array_function__``. The protocol requires explicit adoption
by both users and library authors, which ensures backwards compatibility, and
is also significantly simpler than ``__array_function__``, both of which we
expect will make it easier to adopt.

Why ``__array_function__`` hasn't been enough
---------------------------------------------

There are two broad ways in which NEP-18 has fallen short of its goals:

1. **Backwards compatibility concerns**. `__array_function__` has significant
   implications for libraries that use it:

   - `JAX <https://github.com/google/jax/issues/1565>`_ has been reluctant
     to implement ``__array_function__`` in part because it is concerned about
     breaking existing code: users expect NumPy functions like
     ``np.concatenate`` to return NumPy arrays. This is a fundamental
     limitation of the ``__array_function__`` design, which we chose to allow
     overriding the existing ``numpy`` namespace.
     Libraries like Dask and CuPy have looked at and accepted the backwards
     incompatibility impact of ``__array_function__``; it would still have been
     better for them if that impact didn't exist.

     Note that projects like `PyTorch
     <https://github.com/pytorch/pytorch/issues/22402>`_ and `scipy.sparse
     <https://github.com/scipy/scipy/issues/10362>`_ have also not
     adopted ``__array_function__`` yet, because they don't have a
     NumPy-compatible API or semantics. In the case of PyTorch, that is likely
     to be added in the future. ``scipy.sparse`` is in the same situation as
     ``numpy.matrix``: its semantics are not compatible with ``numpy.ndarray``
     and therefore adding ``__array_function__`` (except to return ``NotImplemented``
     perhaps) is not a healthy idea.
   - ``__array_function__`` currently requires an "all or nothing" approach to
     implementing NumPy's API. There is no good pathway for **incremental
     adoption**, which is particularly problematic for established projects
     for which adopting ``__array_function__`` would result in breaking
     changes.

2. **Limitations on what can be overridden.** ``__array_function__`` has some
   important gaps, most notably array creation and coercion functions:

   - **Array creation** routines (e.g., ``np.arange`` and those in
     ``np.random``) need some other mechanism for indicating what type of
     arrays to create. `NEP 35 <https://numpy.org/neps/nep-0035-array-creation-dispatch-with-array-function.html>`_
     proposed adding optional ``like=`` arguments to functions without
     existing array arguments. However, we still lack any mechanism to
     override methods on objects, such as those needed by
     ``np.random.RandomState``.
   - **Array conversion** can't reuse the existing coercion functions like
     ``np.asarray``, because ``np.asarray`` sometimes means "convert to an
     exact ``np.ndarray``" and other times means "convert to something _like_
     a NumPy array." This led to the `NEP 30
     <https://numpy.org/neps/nep-0030-duck-array-protocol.html>`_ proposal for
     a separate ``np.duckarray`` function, but this still does not resolve how
     to cast one duck array into a type matching another duck array.

Other maintainability concerns that were raised include:

- It is no longer possible to use **aliases to NumPy functions** within
  modules that support overrides. For example, both CuPy and JAX set
  ``result_type = np.result_type`` and now have to wrap use of
  ``np.result_type`` in their own ``result_type`` function instead.
- Implementing **fall-back mechanisms** for unimplemented NumPy functions
  by using NumPy's implementation is hard to get right (but see the
  `version from dask <https://github.com/dask/dask/pull/5043>`_), because
  ``__array_function__`` does not present a consistent interface.
  Converting all arguments of array type requires recursing into generic
  arguments of the form ``*args, **kwargs``.

``get_array_module`` and the ``__array_module__`` protocol
----------------------------------------------------------

We propose a new user-facing mechanism for dispatching to a duck-array
implementation, ``numpy.get_array_module``. ``get_array_module`` performs the
same type resolution as ``__array_function__`` and returns a module with an API
promised to match the standard interface of ``numpy`` that can implement
operations on all provided array types.

The protocol itself is both simpler and more powerful than
``__array_function__``, because it doesn't need to worry about actually
implementing functions. We believe it resolves most of the maintainability and
functionality limitations of ``__array_function__``.

The new protocol is opt-in, explicit and with local control; see
:ref:`appendix-design-choices` for discussion on the importance of these design
features.

The array module contract
=========================

Modules returned by ``get_array_module``/``__array_module__`` should make a
best effort to implement NumPy's core functionality on new array types(s).
Unimplemented functionality should simply be omitted (e.g., accessing an
unimplemented function should raise ``AttributeError``). In the future, we
anticipate codifying a protocol for requesting restricted subsets of ``numpy``;
see :ref:`requesting-restricted-subsets` for more details.

How to use ``get_array_module``
===============================

Code that wants to support generic duck arrays should explicitly call
``get_array_module`` to determine an appropriate array module from which to
call functions, rather than using the ``numpy`` namespace directly. For
example:

.. code:: python

    # calls the appropriate version of np.something for x and y
    module = np.get_array_module(x, y)
    module.something(x, y)

Both array creation and array conversion are supported, because dispatching is
handled by ``get_array_module`` rather than via the types of function
arguments. For example, to use random number generation functions or methods,
we can simply pull out the appropriate submodule:

.. code:: python

    def duckarray_add_random(array):
        module = np.get_array_module(array)
        noise = module.random.randn(*array.shape)
        return array + noise

We can also write the duck-array ``stack`` function from `NEP 30
<https://numpy.org/neps/nep-0030-duck-array-protocol.html>`_, without the need
for a new ``np.duckarray`` function:

.. code:: python

    def duckarray_stack(arrays):
        module = np.get_array_module(*arrays)
        arrays = [module.asarray(arr) for arr in arrays]
        shapes = {arr.shape for arr in arrays}
        if len(shapes) != 1:
            raise ValueError('all input arrays must have the same shape')
        expanded_arrays = [arr[module.newaxis, ...] for arr in arrays]
        return module.concatenate(expanded_arrays, axis=0)

By default, ``get_array_module`` will return the ``numpy`` module if no
arguments are arrays. This fall-back can be explicitly controlled by providing
the ``module`` keyword-only argument. It is also possible to indicate that an
exception should be raised instead of returning a default array module by
setting ``module=None``.

How to implement ``__array_module__``
=====================================

Libraries implementing a duck array type that want to support
``get_array_module`` need to implement the corresponding protocol,
``__array_module__``. This new protocol is based on Python's dispatch protocol
for arithmetic, and is essentially a simpler version of ``__array_function__``.

Only one argument is passed into ``__array_module__``, a Python collection of
unique array types passed into ``get_array_module``, i.e., all arguments with
an ``__array_module__`` attribute.

The special method should either return a namespace with an API matching
``numpy``, or ``NotImplemented``, indicating that it does not know how to
handle the operation:

.. code:: python

    class MyArray:
        def __array_module__(self, types):
            if not all(issubclass(t, MyArray) for t in types):
                return NotImplemented
            return my_array_module

Returning custom objects from ``__array_module__``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``my_array_module`` will typically, but need not always, be a Python module.
Returning a custom objects (e.g., with functions implemented via
``__getattr__``) may be useful for some advanced use cases.

For example, custom objects could allow for partial implementations of duck
array modules that fall-back to NumPy (although this is not recommended in
general because such fall-back behavior can be error prone):

.. code:: python

    class MyArray:
        def __array_module__(self, types):
            if all(issubclass(t, MyArray) for t in types):
                return ArrayModule()
            else:
                return NotImplemented

    class ArrayModule:
        def __getattr__(self, name):
            import base_module
            return getattr(base_module, name, getattr(numpy, name))

Subclassing from ``numpy.ndarray``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All of the same guidance about well-defined type casting hierarchies from
NEP-18 still applies. ``numpy.ndarray`` itself contains a matching
implementation of ``__array_module__``,  which is convenient for subclasses:

.. code:: python

    class ndarray:
        def __array_module__(self, types):
            if all(issubclass(t, ndarray) for t in types):
                return numpy
            else:
                return NotImplemented

NumPy's internal machinery
==========================

The type resolution rules of ``get_array_module`` follow the same model as
Python and NumPy's existing dispatch protocols: subclasses are called before
super-classes, and otherwise left to right. ``__array_module__`` is guaranteed
to be called only  a single time on each unique type.

The actual implementation of `get_array_module` will be in C, but should be
equivalent to this Python code:

.. code:: python

    def get_array_module(*arrays, default=numpy):
        implementing_arrays, types = _implementing_arrays_and_types(arrays)
        if not implementing_arrays and default is not None:
            return default
        for array in implementing_arrays:
            module = array.__array_module__(types)
            if module is not NotImplemented:
                return module
        raise TypeError("no common array module found")

    def _implementing_arrays_and_types(relevant_arrays):
        types = []
        implementing_arrays = []
        for array in relevant_arrays:
            t = type(array)
            if t not in types and hasattr(t, '__array_module__'):
                types.append(t)
                # Subclasses before superclasses, otherwise left to right
                index = len(implementing_arrays)
                for i, old_array in enumerate(implementing_arrays):
                    if issubclass(t, type(old_array)):
                        index = i
                        break
                implementing_arrays.insert(index, array)
        return implementing_arrays, types

Relationship with ``__array_ufunc__`` and ``__array_function__``
----------------------------------------------------------------

These older protocols have distinct use-cases and should remain
===============================================================

``__array_module__`` is intended to resolve limitations of
``__array_function__``, so it is natural to consider whether it could entirely
replace ``__array_function__``. This would offer dual benefits: (1) simplifying
the user-story about how to override NumPy and (2) removing the slowdown
associated with checking for dispatch when calling every NumPy function.

However, ``__array_module__`` and ``__array_function__`` are pretty different
from a user perspective: it requires explicit calls to ``get_array_function``,
rather than simply reusing original ``numpy`` functions. This is probably fine
for *libraries* that rely on duck-arrays, but may be frustratingly verbose for
interactive use.

Some of the dispatching use-cases for ``__array_ufunc__`` are also solved by
``__array_module__``, but not all of them. For example, it is still useful to
be able to define non-NumPy ufuncs (e.g., from Numba or SciPy) in a generic way
on non-NumPy arrays (e.g., with dask.array).

Given their existing adoption and distinct use cases, we don't think it makes
sense to remove or deprecate ``__array_function__`` and ``__array_ufunc__`` at
this time.

Mixin classes to implement ``__array_function__`` and ``__array_ufunc__``
=========================================================================

Despite the user-facing differences, ``__array_module__`` and a module
implementing NumPy's API still contain sufficient functionality needed to
implement dispatching with the existing duck array protocols.

For example, the following mixin classes would provide sensible defaults for
these special methods in terms of ``get_array_module`` and
``__array_module__``:

.. code:: python

    class ArrayUfuncFromModuleMixin:

        def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
            arrays = inputs + kwargs.get('out', ())
            try:
                array_module = np.get_array_module(*arrays)
            except TypeError:
                return NotImplemented

            try:
                # Note this may have false positive matches, if ufunc.__name__
                # matches the name of a ufunc defined by NumPy. Unfortunately
                # there is no way to determine in which module a ufunc was
                # defined.
                new_ufunc = getattr(array_module, ufunc.__name__)
            except AttributeError:
                return NotImplemented

            try:
                callable = getattr(new_ufunc, method)
            except AttributeError:
                return NotImplemented

            return callable(*inputs, **kwargs)

    class ArrayFunctionFromModuleMixin:

        def __array_function__(self, func, types, args, kwargs):
            array_module = self.__array_module__(types)
            if array_module is NotImplemented:
                return NotImplemented

            # Traverse submodules to find the appropriate function
            modules = func.__module__.split('.')
            assert modules[0] == 'numpy'
            for submodule in modules[1:]:
                module = getattr(module, submodule, None)
            new_func = getattr(module, func.__name__, None)
            if new_func is None:
                return NotImplemented

            return new_func(*args, **kwargs)

To make it easier to write duck arrays, we could also add these mixin classes
into ``numpy.lib.mixins`` (but the examples above may suffice).

Alternatives considered
-----------------------

Naming
======

We like the name ``__array_module__`` because it mirrors the existing
``__array_function__`` and ``__array_ufunc__`` protocols. Another reasonable
choice could be ``__array_namespace__``.

It is less clear what the NumPy function that calls this protocol should be
called (``get_array_module`` in this proposal). Some possible alternatives:
``array_module``, ``common_array_module``, ``resolve_array_module``,
``get_namespace``, ``get_numpy``, ``get_numpylike_module``,
``get_duck_array_module``.

.. _requesting-restricted-subsets:

Requesting restricted subsets of NumPy's API
============================================

Over time, NumPy has accumulated a very large API surface, with over 600
attributes in the top level ``numpy`` module alone. It is unlikely that any
duck array library could or would want to implement all of these functions and
classes, because the frequently used subset of NumPy is much smaller.

We think it would be useful exercise to define "minimal" subset(s) of NumPy's
API, omitting rarely used or non-recommended functionality. For example,
minimal NumPy might include ``stack``, but not the other stacking functions
``column_stack``, ``dstack``, ``hstack`` and ``vstack``. This could clearly
indicate to duck array authors and users what functionality is core and what
functionality they can skip.

Support for requesting a restricted subset of NumPy's API would be a natural
feature to include in  ``get_array_function`` and ``__array_module__``, e.g.,

.. code:: python

    # array_module is only guaranteed to contain "minimal" NumPy
    array_module = np.get_array_module(*arrays, request='minimal')

To facilitate testing with NumPy and use with any valid duck array library,
NumPy itself would return restricted versions of the ``numpy`` module when
``get_array_module`` is called only on NumPy arrays. Omitted functions would
simply not exist.

Unfortunately, we have not yet figured out what these restricted subsets should
be, so it doesn't make sense to do this yet. When/if we do, we could either add
new keyword arguments to ``get_array_module`` or add new top level functions,
e.g., ``get_minimal_array_module``. We would also need to add either a new
protocol patterned off of ``__array_module__`` (e.g.,
``__array_module_minimal__``), or could add an optional second argument to
``__array_module__`` (catching errors with ``try``/``except``).

A new namespace for implicit dispatch
=====================================

Instead of supporting overrides in the main `numpy` namespace with
``__array_function__``, we could create a new opt-in namespace, e.g.,
``numpy.api``, with versions of NumPy functions that support dispatching. These
overrides would need new opt-in protocols, e.g., ``__array_function_api__``
patterned off of ``__array_function__``.

This would resolve the biggest limitations of ``__array_function__`` by being
opt-in and would also allow for unambiguously overriding functions like
``asarray``, because ``np.api.asarray`` would always mean "convert an
array-like object."  But it wouldn't solve all the dispatching needs met by
``__array_module__``, and would leave us with supporting a considerably more
complex protocol both for array users and implementors.

We could potentially implement such a new namespace *via* the
``__array_module__`` protocol. Certainly some users would find this convenient,
because it is slightly less boilerplate. But this would leave users with a
confusing choice: when should they use `get_array_module` vs.
`np.api.something`. Also, we would have to add and maintain a whole new module,
which is considerably more expensive than merely adding a function.

Dispatching on both types and arrays instead of only types
==========================================================

Instead of supporting dispatch only via unique array types, we could also
support dispatch via array objects, e.g., by passing an ``arrays`` argument as
part of the ``__array_module__`` protocol. This could potentially be useful for
dispatch for arrays with metadata, such provided by Dask and Pint, but would
impose costs in terms of type safety and complexity.

For example, a library that supports arrays on both CPUs and GPUs might decide
on which device to create a new arrays from functions like ``ones`` based on
input arguments:

.. code:: python

    class Array:
        def __array_module__(self, types, arrays):
            useful_arrays = tuple(a in arrays if isinstance(a, Array))
            if not useful_arrays:
                return NotImplemented
            prefer_gpu = any(a.prefer_gpu for a in useful_arrays)
            return ArrayModule(prefer_gpu)

    class ArrayModule:
        def __init__(self, prefer_gpu):
            self.prefer_gpu = prefer_gpu

        def __getattr__(self, name):
            import base_module
            base_func = getattr(base_module, name)
            return functools.partial(base_func, prefer_gpu=self.prefer_gpu)

This might be useful, but it's not clear if we really need it. Pint seems to
get along OK without any explicit array creation routines (favoring
multiplication by units, e.g., ``np.ones(5) * ureg.m``), and for the most part
Dask is also OK with existing ``__array_function__`` style overrides (e.g.,
favoring ``np.ones_like`` over ``np.ones``). Choosing whether to place an array
on the CPU or GPU could be solved by `making array creation lazy
<https://github.com/google/jax/pull/1668>`_.

.. _appendix-design-choices:

Appendix: design choices for API overrides
------------------------------------------

There is a large range of possible design choices for overriding NumPy's API.
Here we discuss three major axes of the design decision that guided our design
for ``__array_module__``.

Opt-in vs. opt-out for users
============================

The ``__array_ufunc__`` and ``__array_function__`` protocols provide a
mechanism for overriding NumPy functions *within NumPy's existing namespace*.
This means that users need to explicitly opt-out if they do not want any
overridden behavior, e.g., by casting arrays with ``np.asarray()``.

In theory, this approach lowers the barrier for adopting these protocols in
user code and libraries, because code that uses the standard NumPy namespace is
automatically compatible. But in practice, this hasn't worked out. For example,
most well-maintained libraries that use NumPy follow the best practice of
casting all inputs with ``np.asarray()``, which they would have to explicitly
relax to use ``__array_function__``. Our experience has been that making a
library compatible with a new duck array type typically requires at least a
small amount of work to accommodate differences in the data model and operations
that can be implemented efficiently.

These opt-out approaches also considerably complicate backwards compatibility
for libraries that adopt these protocols, because by opting in as a library
they also opt-in their users, whether they expect it or not. For winning over
libraries that have been unable to adopt ``__array_function__``, an opt-in
approach seems like a must.

Explicit vs. implicit choice of implementation
==============================================

Both ``__array_ufunc__`` and ``__array_function__`` have implicit control over
dispatching: the dispatched functions are determined via the appropriate
protocols in every function call. This generalizes well to handling many
different types of objects, as evidenced by its use for implementing arithmetic
operators in Python, but it has an important downside for **readability**:
it is not longer immediately evident to readers of code what happens when a
function is called, because the function's implementation could be overridden
by any of its arguments.

The **speed** implications are:

- When using a *duck-array type*, ``get_array_module`` means type checking only
  needs to happen once inside each function that supports duck typing, whereas
  with ``__array_function__`` it happens every time a NumPy function is called.
  Obvious it's going to depend on the function, but if a typical duck-array
  supporting function calls into other NumPy functions 3-5 times this is a factor
  of 3-5x more overhead.
- When using *NumPy arrays*, ``get_array_module`` is one extra call per
  function (``__array_function__`` overhead remains the same), which means a
  small amount of extra overhead.

Explicit and implicit choice of implementations are not mutually exclusive
options. Indeed, most implementations of NumPy API overrides via
``__array_function__`` that we are familiar with (namely, Dask, CuPy and
Sparse, but not Pint) also include an explicit way to use their version of
NumPy's API by importing a module directly (``dask.array``, ``cupy`` or
``sparse``, respectively).

Local vs. non-local vs. global control
======================================

The final design axis is how users control the choice of API:

- **Local control**, as exemplified by multiple dispatch and Python protocols for
  arithmetic, determines which implementation to use either by checking types
  or calling methods on the direct arguments of a function.
- **Non-local control** such as `np.errstate
  <https://docs.scipy.org/doc/numpy/reference/generated/numpy.errstate.html>`_
  overrides behavior with global-state via function decorators or
  context-managers. Control is determined hierarchically, via the inner-most
  context.
- **Global control** provides a mechanism for users to set default behavior,
  either via function calls or configuration files. For example, matplotlib
  allows setting a global choice of plotting backend.

Local control is generally considered a best practice for API design, because
control flow is entirely explicit, which makes it the easiest to understand.
Non-local and global control are occasionally used, but generally either due to
ignorance or a lack of better alternatives.

In the case of duck typing for NumPy's public API, we think non-local or global
control would be mistakes, mostly because they **don't compose well**. If one
library sets/needs one set of overrides and then internally calls a routine
that expects another set of overrides, the resulting behavior may be very
surprising. Higher order functions are especially problematic, because the
context in which functions are evaluated may not be the context in which they
are defined.

One class of override use cases where we think non-local and global control are
appropriate is for choosing a backend system that is guaranteed to have an
entirely consistent interface, such as a faster alternative implementation of
``numpy.fft`` on NumPy arrays. However, these are out of scope for the current
proposal, which is focused on duck arrays.
==============
Scope of NumPy
==============

Here, we describe aspects of N-d array computation that are within scope for NumPy development. This is *not* an aspirational definition of where NumPy should aim, but instead captures the status quo—areas which we have decided to continue supporting, at least for the time being.

- **In-memory, N-dimensional, homogeneously typed (single pointer + strided) arrays on CPUs**

  - Support for a wide range of data types
  - Not specialized hardware such as GPUs
  - But, do support wide range of CPUs (e.g. ARM, PowerX)

- **Higher level APIs for N-dimensional arrays**

  - NumPy is a *de facto* standard for array APIs in Python
  - Indexing and fast iteration over elements (ufunc)
  - Interoperability protocols with other data container implementations (like
    :ref:`__array_ufunc__ and __array_function__ <basics.dispatch>`.

- **Python API and a C API** to the ndarray's methods and attributes.

- Other **specialized types or uses of N-dimensional arrays**:

  - Masked arrays
  - Structured arrays (informally known as record arrays)
  - Memory mapped arrays

- Historically, NumPy has included the following **basic functionality
  in support of scientific computation**. We intend to keep supporting
  (but not to expand) what is currently included:

  - Linear algebra
  - Fast Fourier transforms and windowing
  - Pseudo-random number generators
  - Polynomial fitting

- NumPy provides some **infrastructure for other packages in the scientific Python ecosystem**:

  - numpy.distutils (build support for C++, Fortran, BLAS/LAPACK, and other
    relevant libraries for scientific computing)
  - f2py (generating bindings for Fortran code)
  - testing utilities

- **Speed**: we take performance concerns seriously and aim to execute
  operations on large arrays with similar performance as native C
  code. That said, where conflict arises, maintenance and portability take
  precedence over performance. We aim to prevent regressions where
  possible (e.g., through asv).
.. _NEP46:

=====================================
NEP 46 — NumPy sponsorship guidelines
=====================================

:Author: Ralf Gommers <ralf.gommers@gmail.com>
:Status: Accepted
:Type: Process
:Created: 2020-12-27
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2021-January/081424.html


Abstract
--------

This NEP provides guidelines on how the NumPy project will acknowledge
financial and in-kind support.


Motivation and Scope
--------------------

In the past few years, the NumPy project has gotten significant financial
support, as well as dedicated work time for maintainers to work on NumPy. There
is a need to acknowledge that support - it's the right thing to do, it's
helpful when looking for new funding, and funders and organizations expect or
require it, Furthermore, having a clear policy for how NumPy acknowledges
support is helpful when searching for new support. Finally, this policy may
help set reasonable expectations for potential funders.

This NEP is aimed at both the NumPy community - who can use it as a guideline
when looking for support on behalf of the project and when acknowledging
existing support - and at past, current and prospective sponsors, who often
want or need to know what they get in return for their support other than a
healthier NumPy.

The scope of this proposal includes:

- direct financial support, employers providing paid time for NumPy maintainers
  and regular contributors, and in-kind support such as free hardware resources or
  services,
- where and how NumPy acknowledges support (e.g., logo placement on the website),
- the amount and duration of support which leads to acknowledgement, and
- who in the NumPy project is responsible for sponsorship related topics, and
  how to contact them.


How NumPy will acknowledge support
----------------------------------

There will be two different ways to acknowledge financial and in-kind support:
one to recognize significant active support, and another one to recognize
support received in the past and smaller amounts of support.

Entities who fall under "significant active supporter" we'll call Sponsor.
The minimum level of support given to NumPy to be considered a Sponsor are:

- $30,000/yr for unrestricted financial contributions (e.g., donations)
- $60,000/yr for financial contributions for a particular purpose (e.g., grants)
- $100,000/yr for in-kind contributions (e.g., time for employees to contribute)

We define support being active as:

- for a one-off donation: it was received within the previous 12 months,
- for recurring or financial or in-kind contributions: they should be ongoing.

After support moves from "active" to "inactive" status, the acknowledgement
will be left in its place for at least another 6 months. If appropriate, the
funding team can discuss opportunities for renewal with the sponsor. After
those 6 months, acknowledgement may be moved to the historical overview. The
exact timing of this move is at the discretion of the funding team, because
there may be reasons to keep it in the more prominent place for longer.

The rationale for the above funding levels is that unrestricted financial
contributions are typically the most valuable for the project, and the hardest
to obtain.  The opposite is true for in-kind contributions. The dollar value of
the levels also reflect that NumPy's needs have grown to the point where we
need multiple paid developers in order to effectively support our user base and
continue to move the project forward. Financial support at or above these
levels is needed to be able to make a significant difference.

Sponsors will get acknowledged through:

- a small logo displayed on the front page of the NumPy website
- prominent logo placement on https://numpy.org/about/
- logos displayed in talks about NumPy by maintainers
- announcements of the sponsorship on the NumPy mailing list and the numpy-team
  Twitter account

In addition to Sponsors, we already have the concept of Institutional Partner
(defined in NumPy's
`governance document <https://numpy.org/devdocs/dev/governance/index.html>`__),
for entities who employ a NumPy maintainer and let them work on NumPy as part
of their official duties. The governance document doesn't currently define a
minimum amount of paid maintainer time needed to be considered for partnership.
Therefore we propose that level here, roughly in line with the sponsorship
levels:

- 6 person-months/yr of paid work time for one or more NumPy maintainers or
  regular contributors to any NumPy team or activity

Institutional Partners get the same benefits as Sponsors, in addition to what
is specified in the NumPy governance document.

Finally, a new page on the website (https://numpy.org/funding/, linked from the
About page) will be added to acknowledge all current and previous sponsors,
partners, and any other entities and individuals who provided $5,000 or more of
financial or in-kind support. This page will include relevant details of
support (dates, amounts, names, and purpose); no logos will be used on this
page. The rationale for the $5,000 minimum level is to keep the amount of work
maintaining the page reasonable; the level is the equivalent of, e.g., one GSoC
or a person-week's worth of engineering time in a Western country, which seems
like a reasonable lower limit.


Implementation
--------------

The following content changes need to be made:

- Add a section with small logos towards the bottom of the `numpy.org
  <https://numpy.org/>`__ website.
- Create a full list of historical and current support and deploy it to
  https://numpy.org/funding.
- Update the NumPy governance document for changes to Institutional Partner
  eligibility requirements and benefits.
- Update https://numpy.org/about with details on how to get in touch with the
  NumPy project about sponsorship related matters (see next section).


NumPy Funding Team
~~~~~~~~~~~~~~~~~~

At the moment NumPy has only one official body, the Steering Council, and no
good way to get in touch with either that body or any person or group
responsible for funding and sponsorship related matters. The way this is
typically done now is to somehow find the personal email of a maintainer, and
email them in private. There is a need to organize this more transparently - a
potential sponsor isn't likely to inquire through the mailing list, nor is it
easy for a potential sponsor to know if they're reaching out to the right
person in private.

https://numpy.org/about/ already says that NumPy has a "funding and grants"
team. However that is not the case. We propose to organize this team, name team
members on it, and add the names of those team members plus a dedicated email
address for the team to the About page.


Status before this proposal
---------------------------

Acknowledgement of support
~~~~~~~~~~~~~~~~~~~~~~~~~~

At the time of writing (Dec 2020), the logos of the four largest financial
sponsors and two institutional partners are displayed on
https://numpy.org/about/. The `Nature paper about NumPy <https://www.nature.com/articles/s41586-020-2649-2>`__
mentions some early funding. No comprehensive list of received funding and
in-kind support is published anywhere.

Decisions on which logos to list on the website have been made mostly by the
website team. Decisions on which entities to recognize as Institutional Partner
have been made by the NumPy Steering Council.


NumPy governance, decision-making, and financial oversight
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*This section is meant as context for the reader, to help put the rest of this
NEP in perspective, and perhaps answer questions the reader has when reading
this as a potential sponsor.*

NumPy has a formal governance structure defined in
`this governance document <https://numpy.org/devdocs/dev/governance/index.html>`__).
Decisions are made by consensus among all active participants in a discussion
(typically on the mailing list), and if consensus cannot be reached then the
Steering Council takes the decision (also by consensus).

NumPy is a sponsored project of NumFOCUS, a US-based 501(c)3 nonprofit.
NumFOCUS administers NumPy funds, and ensures they are spent in accordance with
its mission and nonprofit status. In practice, NumPy has a NumFOCUS
subcommittee (with its members named in the NumPy governance document) who can
authorize financial transactions. Those transactions, for example paying a
contractor for a particular activity or deliverable, are decided on by the
NumPy Steering Council.


Alternatives
------------

*Tiered sponsorship levels.* We considered using tiered sponsorship levels, and
rejected this alternative because it would be more complex, and not necessarily
communicate the right intent - the minimum levels are for us to determine how
to acknowledge support that we receive, not a commercial value proposition.
Entities typically will support NumPy because they rely on the project or want
to help advance it, and not to get brand awareness through logo placement.

*Listing all donations*. Note that in the past we have received many smaller
donations, mostly from individuals through NumFOCUS. It would be great to list
all of those contributions, but given the way we receive information on those
donations right now, that would be quite labor-intensive. If we manage to move
to a more suitable platform, such as `Open Collective <https://opencollective.com/>`__,
in the future, we should reconsider listing all individual donations.


Related Work
------------

Here we provide a few examples of how other projects handle sponsorship
guidelines and acknowledgements.

*Scikit-learn* has a narrow banner with logos at the bottom of
https://scikit-learn.org, and a list of present funding and past sponsors at
https://scikit-learn.org/stable/about.html#funding. Plus a separate section
"Infrastructure support" at the bottom of that same About page.

*Jupyter* has logos of sponsors and institutional partners in two sections on
https://jupyter.org/about. Some subprojects have separate approaches, for
example sponsors are listed (by using the `all-contributors
<https://github.com/all-contributors/all-contributors>`__ bot) in the README for
`jupyterlab-git <https://github.com/jupyterlab/jupyterlab-git>`__. For a recent
discussion on that, see `here <jupyterlab-git acknowledgements discussion>`_.

*NumFOCUS* has a large banner with sponsor logos on its front page at
https://numfocus.org, and a full page with sponsors at different sponsorship
levels listed at https://numfocus.org/sponsors. They also have a
`Corporate Sponsorship Prospectus <https://numfocus.org/blog/introducing-our-newest-corporate-sponsorship-prospectus>`__,
which includes a lot of detail on both sponsorship levels and benefits, as well
as how that helps NumFOCUS-affiliated projects (including NumPy).


Discussion
----------

- `Mailing list thread discussing this NEP <https://mail.python.org/pipermail/numpy-discussion/2020-December/081353.html>`__
- `PR with review of the NEP draft <https://github.com/numpy/numpy/pull/18084>`__


References and Footnotes
------------------------

- `Inside NumPy: preparing for the next decade <https://github.com/numpy/archive/blob/main/content/inside_numpy_presentation_SciPy2019.pdf>`__ presentation at SciPy'19 discussing the impact of the first NumPy grant.
- `Issue  <https://github.com/numpy/numpy/issues/13393>`__ and
  `email <https://mail.python.org/pipermail/numpy-discussion/2019-April/079371.html>`__
  where IBM offered a $5,000 bounty for VSX SIMD support
- `JupyterLab Corporate Engagement and Contribution Guide <https://github.com/jupyterlab/jupyterlab/blob/master/CORPORATE.md>`__


.. _jupyterlab-git acknowledgements discussion: https://github.com/jupyterlab/jupyterlab-git/pull/530


Copyright
---------

This document has been placed in the public domain.
.. _NEP26:

====================================================
NEP 26 — Summary of missing data NEPs and discussion
====================================================

:Author: Mark Wiebe <mwwiebe@gmail.com>, Nathaniel J. Smith <njs@pobox.com>
:Status: Deferred
:Type: Standards Track
:Created: 2012-04-22

*Context*: this NEP was written as summary of the large number of discussions
and proposals (:ref:`NEP12`, :ref:`NEP24`, :ref:`NEP25`), regarding missing data
functionality.

The debate about how NumPy should handle missing data, a subject with
many preexisting approaches, requirements, and conventions, has been long and
contentious. There has been more than one proposal for how to implement
support into NumPy, and there is a testable implementation which is
merged into NumPy's current main. The vast number of emails and differing
points of view has made it difficult for interested parties to understand
the issues and be comfortable with the direction NumPy is going.

Here is our (Mark and Nathaniel's) attempt to summarize the
problem, proposals, and points of agreement/disagreement in a single
place, to help the community move towards consensus.

The NumPy developers' problem
=============================

For this discussion, "missing data" means array elements
which can be indexed (e.g. A[3] in an array A with shape (5,)),
but have, in some sense, no value.

It does not refer to compressed or sparse storage techniques where
the value for A[3] is not actually stored in memory, but still has a
well-defined value like 0.

This is still vague, and to create an actual implementation,
it is necessary to answer such questions as:

* What values are computed when doing element-wise ufuncs.
* What values are computed when doing reductions.
* Whether the storage for an element gets overwritten when marking
  that value missing.
* Whether computations resulting in NaN automatically treat in the
  same way as a missing value.
* Whether one interacts with missing values using a placeholder object
  (e.g. called "NA" or "masked"), or through a separate boolean array.
* Whether there is such a thing as an array object that cannot hold
  missing array elements.
* How the (C and Python) API is expressed, in terms of dtypes,
  masks, and other constructs.
* If we decide to answer some of these questions in multiple ways,
  then that creates the question of whether that requires multiple
  systems, and if so how they should interact.

There's clearly a very large space of missing-data APIs that *could*
be implemented. There is likely at least one user, somewhere, who
would find any possible implementation to be just the thing they
need to solve some problem. On the other hand, much of NumPy's power
and clarity comes from having a small number of orthogonal concepts,
such as strided arrays, flexible indexing, broadcasting, and ufuncs,
and we'd like to preserve that simplicity.

There has been dissatisfaction among several major groups of NumPy users
about the existing status quo of missing data support. In particular,
neither the numpy.ma component nor use of floating-point NaNs as a
missing data signal fully satisfy the performance requirements and
ease of use for these users. The example of R, where missing data
is treated via an NA placeholder and is deeply integrated into all
computation, is where many of these users point to indicate what
functionality they would like. Doing a deep integration of missing
data like in R must be considered carefully, it must be clear it
is not being done in a way which sacrifices existing performance
or functionality.

Our problem is, how can we choose some incremental additions to
NumPy that will make a large class of users happy, be
reasonably elegant, complement the existing design, and that we're
comfortable we won't regret being stuck with in the long term.

Prior art
=========

So a major (maybe *the* major) problem is figuring out how ambitious
the project to add missing data support to NumPy should be, and which
kinds of problems are in scope. Let's start with the
best understood situation where "missing data" comes into play:

"Statistical missing data"
--------------------------

In statistics, social science, etc., "missing data" is a term of art
referring to a specific (but extremely common and important)
situation: we have tried to gather some measurements according to some
scheme, but some of these measurements are missing. For example, if we
have a table listing the height, age, and income of a number of
individuals, but one person did not provide their income, then we need
some way to represent this::

  Person | Height | Age | Income
  ------------------------------
     1   |   63   | 25  | 15000
     2   |   58   | 32  | <missing>
     3   |   71   | 45  | 30000

The traditional way is to record that income as, say, "-99", and
document this in the README along with the data set. Then, you have to
remember to check for and handle such incomes specially; if you
forget, you'll get superficially reasonable but completely incorrect
results, like calculating the average income on this data set as
14967. If you're in one of these fields, then such missing-ness is
routine and inescapable, and if you use the "-99" approach then it's a
pitfall you have to remember to check for explicitly on literally
*every* calculation you ever do. This is, obviously, an unpleasant way
to live.

Let's call this situation the "statistical missing data" situation,
just to have a convenient handle for it. (As mentioned, practitioners
just call this "missing data", and what to do about it is literally an
entire sub-field of statistics; if you google "missing data" then
every reference is on how to handle it.) NumPy isn't going to do
automatic imputation or anything like that, but it could help a great
deal by providing some standard way to at least represent data which
is missing in this sense.

The main prior art for how this could be done comes from the S/S+/R
family of languages. Their strategy is, for each type they support,
to define a special value called "NA". (For ints this is INT_MAX,
for floats it's a special NaN value that's distinguishable from
other NaNs, ...) Then, they arrange that in computations, this
value has a special semantics that we will call "NA semantics".

NA Semantics
------------

The idea of NA semantics is that any computations involving NA
values should be consistent with what would have happened if we
had known the correct value.

For example, let's say we want to compute the mean income, how might
we do this? One way would be to just ignore the missing entry, and
compute the mean of the remaining entries. This gives us (15000 +
30000)/2, or 22500.

Is this result consistent with discovering the income of person 2?
Let's say we find out that person 2's income is 50000. This means
the correct answer is (15000 + 50000 + 30000)/3, or 31666.67,
indicating clearly that it is not consistent. Therefore, the mean
income is NA, i.e. a specific number whose value we are unable
to compute.

This motivates the following rules, which are how R implements NA:

Assignment:
  NA values are understood to represent specific
  unknown values, and thus should have value-like semantics with
  respect to assignment and other basic data manipulation
  operations. Code which does not actually look at the values involved
  should work the same regardless of whether some of them are
  missing. For example, one might write::

    income[:] = income[np.argsort(height)]

  to perform an in-place sort of the ``income`` array, and know that
  the shortest person's income would end up being first. It turns out
  that the shortest person's income is not known, so the array should
  end up being ``[NA, 15000, 30000]``, but there's nothing
  special about NAness here.

Propagation:
  In the example above, we concluded that an operation like ``mean``
  should produce NA when one of its data values was NA.
  If you ask me, "what is 3 plus x?", then my only possible answer is
  "I don't know what x is, so I don't know what 3 + x is either". NA
  means "I don't know", so 3 + NA is NA.

  This is important for safety when analyzing data: missing data often
  requires special handling for correctness -- the fact that you are
  missing information might mean that something you wanted to compute
  cannot actually be computed, and there are whole books written on
  how to compensate in various situations. Plus, it's easy to not
  realize that you have missing data, and write code that assumes you
  have all the data. Such code should not silently produce the wrong
  answer.

  There is an important exception to characterizing this as propagation,
  in the case of boolean values. Consider the calculation::

    v = np.any([False, False, NA, True])

  If we strictly propagate, ``v`` will become NA. However, no
  matter whether we place True or False into the third array position,
  ``v`` will then get the value True. The answer to the question
  "Is the result True consistent with later discovering the value
  that was missing?" is yes, so it is reasonable to not propagate here,
  and instead return the value True. This is what R does::

    > any(c(F, F, NA, T))
    [1] TRUE
    > any(c(F, F, NA, F))
    [1] NA

Other:
  NaN and NA are conceptually distinct. 0.0/0.0 is not a mysterious,
  unknown value -- it's defined to be NaN by IEEE floating point, Not
  a Number. NAs are numbers (or strings, or whatever), just unknown
  ones. Another small but important difference is that in Python, ``if
  NaN: ...`` treats NaN as True (NaN is "truthy"); but ``if NA: ...``
  would be an error.

  In R, all reduction operations implement an alternative semantics,
  activated by passing a special argument (``na.rm=TRUE`` in R).
  ``sum(a)`` means "give me the sum of all the
  values" (which is NA if some of the values are NA);
  ``sum(a, na.rm=True)`` means "give me the sum of all the non-NA
  values".

Other prior art
---------------

Once we move beyond the "statistical missing data" case, the correct
behavior for missing data becomes less clearly defined. There are many
cases where specific elements are singled out to be treated specially
or excluded from computations, and these could often be conceptualized
as involving 'missing data' in some sense.

In image processing, it's common to use a single image together with
one or more boolean masks to e.g. composite subsets of an image. As
Joe Harrington pointed out on the list, in the context of processing
astronomical images, it's also common to generalize to a
floating-point valued mask, or alpha channel, to indicate degrees of
"missingness". We think this is out of scope for the present design,
but it is an important use case, and ideally NumPy should support
natural ways of manipulating such data.

After R, numpy.ma is probably the most mature source of
experience on missing-data-related APIs. Its design is quite different
from R; it uses different semantics -- reductions skip masked values
by default and NaNs convert to masked -- and it uses a different
storage strategy via a separate mask. While it seems to be generally
considered sub-optimal for general use, it's hard to pin down whether
this is because the API is immature but basically good, or the API
is fundamentally broken, or the API is great but the code should be
faster, or what. We looked at some of those users to try and get a
better idea.

Matplotlib is perhaps the best known package to rely on numpy.ma. It
seems to use it in two ways. One is as a way for users to indicate
what data is missing when passing it to be graphed. (Other ways are
also supported, e.g., passing in NaN values gives the same result.) In
this regard, matplotlib treats np.ma.masked and NaN values in the same way
that R's plotting routines handle NA and NaN values. For these purposes,
matplotlib doesn't really care what semantics or storage strategy is
used for missing data.

Internally, matplotlib uses numpy.ma arrays to store and pass around
separately computed boolean masks containing 'validity' information
for each input array in a cheap and non-destructive fashion. Mark's
impression from some shallow code review is that mostly it works
directly with the data and mask attributes of the masked arrays,
not extensively using the particular computational semantics of
numpy.ma. So, for this usage they do rely on the non-destructive
mask-based storage, but this doesn't say much about what semantics
are needed.

Paul Hobson `posted some code`__ on the list that uses numpy.ma for
storing arrays of contaminant concentration measurements. Here the
mask indicates whether the corresponding number represents an actual
measurement, or just the estimated detection limit for a concentration
which was too small to detect. Nathaniel's impression from reading
through this code is that it also mostly uses the .data and .mask
attributes in preference to performing operations on the MaskedArray
directly.

__ https://mail.scipy.org/pipermail/numpy-discussion/2012-April/061743.html

So, these examples make it clear that there is demand for a convenient
way to keep a data array and a mask array (or even a floating point
array) bundled up together and "aligned". But they don't tell us much
about what semantics the resulting object should have with respect to
ufuncs and friends.

Semantics, storage, API, oh my!
===============================

We think it's useful to draw a clear line between use cases,
semantics, and storage. Use cases are situations that users encounter,
regardless of what NumPy does; they're the focus of the previous
section. When we say *semantics*, we mean the result of different
operations as viewed from the Python level without regard to the
underlying implementation.

*NA semantics* are the ones described above and used by R::

  1 + NA = NA
  sum([1, 2, NA]) = NA
  NA | False = NA
  NA | True = True

With ``na.rm=TRUE`` or ``skipNA=True``, this switches to::

  1 + NA = illegal # in R, only reductions take na.rm argument
  sum([1, 2, NA], skipNA=True) = 3

There's also been discussion of what we'll call *ignore
semantics*. These are somewhat underdefined::

  sum([1, 2, IGNORED]) = 3
  # Several options here:
  1 + IGNORED = 1
  #  or
  1 + IGNORED = <leaves output array untouched>
  #  or
  1 + IGNORED = IGNORED

The numpy.ma semantics are::

  sum([1, 2, masked]) = 3
  1 + masked = masked

If either NA or ignore semantics are implemented with masks, then there
is a choice of what should be done to the value in the storage
for an array element which gets assigned a missing value. Three
possibilities are:

* Leave that memory untouched (the choice made in the NEP).
* Do the calculation with the values independently of the mask
  (perhaps the most useful option for Paul Hobson's use-case above).
* Copy whatever value is stored behind the input missing value into
  the output (this is what numpy.ma does. Even that is ambiguous in
  the case of ``masked + masked`` -- in this case numpy.ma copies the
  value stored behind the leftmost masked value).

When we talk about *storage*, we mean the debate about whether missing
values should be represented by designating a particular value of the
underlying data-type (the *bitpattern dtype* option, as used in R), or
by using a separate *mask* stored alongside the data itself.

For mask-based storage, there is also an important question about what
the API looks like for accessing the mask, modifying the mask, and
"peeking behind" the mask.

Designs that have been proposed
===============================

One option is to just copy R, by implementing a mechanism whereby
dtypes can arrange for certain bitpatterns to be given NA semantics.

One option is to copy numpy.ma closely, but with a more optimized
implementation. (Or to simply optimize the existing implementation.)

One option is that described in `NEP12`, for which an implementation
of mask-based missing data exists. This system is roughly:

* There is both bitpattern and mask-based missing data, and both
  have identical interoperable NA semantics.
* Masks are modified by assigning np.NA or values to array elements.
  The way to peek behind the mask or to unmask values is to keep a
  view of the array that shares the data pointer but not the mask pointer.
* Mark would like to add a way to access and manipulate the mask more
  directly, to be used in addition to this view-based API.
* If an array has both a bitpattern dtype and a mask, then assigning
  np.NA writes to the mask, rather than to the array itself. Writing
  a bitpattern NA to an array which supports both requires accessing
  the data by "peeking under the mask".

Another option is that described in `NEP24`, which is to implement
bitpattern dtypes with NA semantics for the "statistical missing data"
use case, and to also implement a totally independent API for masked
arrays with ignore semantics and all mask manipulation done explicitly
through a .mask attribute.

Another option would be to define a minimalist aligned array container
that holds multiple arrays and that can be used to pass them around
together. It would support indexing (to help with the common problem
of wanting to subset several arrays together without their becoming
unaligned), but all arithmetic etc. would be done by accessing the
underlying arrays directly via attributes. The "prior art" discussion
above suggests that something like this holding a .data and a .mask
array might actually be solve a number of people's problems without
requiring any major architectural changes to NumPy. This is similar to
a structured array, but with each field in a separately stored array
instead of packed together.

Several people have suggested that there should be a single system
that has multiple missing values that each have different semantics,
e.g., a MISSING value that has NA semantics, and a separate IGNORED
value that has ignored semantics.

None of these options are necessarily exclusive.

The debate
==========

We both are dubious of using ignored semantics as a default missing
data behavior. **Nathaniel** likes NA semantics because he is most
interested in the "statistical missing data" use case, and NA semantics
are exactly right for that. **Mark** isn't as interested in that use
case in particular, but he likes the NA computational abstraction
because it is unambiguous and well-defined in all cases, and has a
lot of existing experience to draw from.

What **Nathaniel** thinks, overall:

* The "statistical missing data" use case is clear and compelling; the
  other use cases certainly deserve our attention, but it's hard to say what
  they *are* exactly yet, or even if the best way to support them is
  by extending the ndarray object.
* The "statistical missing data" use case is best served by an R-style
  system that uses bitpattern storage to implement NA semantics. The
  main advantage of bitpattern storage for this use case is that it
  avoids the extra memory and speed overhead of storing and checking a
  mask (especially for the common case of floating point data, where
  some tricks with NaNs allow us to effectively hardware-accelerate
  most NA operations). These concerns alone appears to make a
  mask-based implementation unacceptable to many NA users,
  particularly in areas like neuroscience (where memory is tight) or
  financial modeling (where milliseconds are critical). In addition,
  the bit-pattern approach is less confusing conceptually (e.g.,
  assignment really is just assignment, no magic going on behind the
  curtain), and it's possible to have in-memory compatibility with R
  for inter-language calls via rpy2.  The main disadvantage of the
  bitpattern approach is the need to give up a value to represent NA,
  but this is not an issue for the most important data types (float,
  bool, strings, enums, objects); really, only integers are
  affected. And even for integers, giving up a value doesn't really
  matter for statistical problems. (Occupy Wall Street
  notwithstanding, no-one's income is 2**63 - 1. And if it were, we'd
  be switching to floats anyway to avoid overflow.)
* Adding new dtypes requires some cooperation with the ufunc and
  casting machinery, but doesn't require any architectural changes or
  violations of NumPy's current orthogonality.
* His impression from the mailing list discussion, esp. the `"what can
  we agree on?" thread`__, is that many numpy.ma users specifically
  like the combination of masked storage, the mask being easily
  accessible through the API, and ignored semantics. He could be
  wrong, of course. But he cannot remember seeing anybody besides Mark
  advocate for the specific combination of masked storage and NA
  semantics, which makes him nervous.

  __ http://thread.gmane.org/gmane.comp.python.numeric.general/46704
* Also, he personally is not very happy with the idea of having two
  storage implementations that are almost-but-not-quite identical at
  the Python level. While there likely are people who would like to
  temporarily pretend that certain data is "statistically missing
  data" without making a copy of their array, it's not at all clear
  that they outnumber the people who would like to use bitpatterns and
  masks simultaneously for distinct purposes. And honestly he'd like
  to be able to just ignore masks if he wants and stick to
  bitpatterns, which isn't possible if they're coupled together
  tightly in the API.  So he would say the jury is still very much out
  on whether this aspect of the NEP design is an advantage or a
  disadvantage. (Certainly he's never heard of any R users complaining
  that they really wish they had an option of making a different
  trade-off here.)
* R's NA support is a `headline feature`__ and its target audience
  consider it a compelling advantage over other platforms like Matlab
  or Python. Working with statistical missing data is very painful
  without platform support.

  __ http://www.sr.bham.ac.uk/~ajrs/R/why_R.html
* By comparison, we clearly have much more uncertainty about the use
  cases that require a mask-based implementation, and it doesn't seem
  like people will suffer too badly if they are forced for now to
  settle for using NumPy's excellent mask-based indexing, the new
  where= support, and even numpy.ma.
* Therefore, bitpatterns with NA semantics seem to meet the criteria
  of making a large class of users happy, in an elegant way, that fits
  into the original design, and where we can have reasonable certainty
  that we understand the problem and use cases well enough that we'll
  be happy with them in the long run. But no mask-based storage
  proposal does, yet.

What **Mark** thinks, overall:

* The idea of using NA semantics by default for missing data, inspired
  by the "statistical missing data" problem, is better than all the
  other default behaviors which were considered. This applies equally
  to the bitpattern and the masked approach.

* For NA-style functionality to get proper support by all NumPy
  features and eventually all third-party libraries, it needs to be
  in the core. How to correctly and efficiently handle missing data
  differs by algorithm, and if thinking about it is required to fully
  support NumPy, NA support will be broader and higher quality.

* At the same time, providing two different missing data interfaces,
  one for masks and one for bitpatterns, requires NumPy developers
  and third-party NumPy plugin developers to separately consider the
  question of what to do in either case, and do two additional
  implementations of their code. This complicates their job,
  and could lead to inconsistent support for missing data.

* Providing the ability to work with both masks and bitpatterns through
  the same C and Python programming interface makes missing data support
  cleanly orthogonal with all other NumPy features.

* There are many trade-offs of memory usage, performance, correctness, and
  flexibility between masks and bitpatterns. Providing support for both
  approaches allows users of NumPy to choose the approach which is
  most compatible with their way of thinking, or has characteristics
  which best match their use-case. Providing them through the same
  interface further allows them to try both with minimal effort, and
  choose the one which performs better or uses the least memory for
  their programs.

* Memory Usage

  * With bitpatterns, less memory is used for storing a single array
    containing some NAs.

  * With masks, less memory is used for storing multiple arrays that
    are identical except for the location of their NAs. (In this case a
    single data array can be re-used with multiple mask arrays;
    bitpattern NAs would need to copy the whole data array.)

* Performance

  * With bitpatterns, the floating point type can use native hardware
    operations, with nearly correct behavior. For fully correct floating
    point behavior and with other types, code must be written which
    specially tests for equality with the missing-data bitpattern.

  * With masks, there is always the overhead of accessing mask memory
    and testing its truth value. The implementation that currently exists
    has no performance tuning, so it is only good to judge a minimum
    performance level. Optimal mask-based code is in general going to
    be slower than optimal bitpattern-based code.

* Correctness

  * Bitpattern integer types must sacrifice a valid value to represent NA.
    For larger integer types, there are arguments that this is ok, but for
    8-bit types there is no reasonable choice. In the floating point case,
    if the performance of native floating point operations is chosen,
    there is a small inconsistency that NaN+NA and NA+NaN are different.
  * With masks, it works correctly in all cases.

* Generality

  * The bitpattern approach can work in a fully general way only when
    there is a specific value which can be given up from the
    data type. For IEEE floating point, a NaN is an obvious choice,
    and for booleans represented as a byte, there are plenty of choices.
    For integers, a valid value must be sacrificed to use this approach.
    Third-party dtypes which plug into NumPy will also have to
    make a bitpattern choice to support this system, something which
    may not always be possible.

  * The mask approach works universally with all data types.

Recommendations for Moving Forward
==================================

**Nathaniel** thinks we should:

* Go ahead and implement bitpattern NAs.
* *Don't* implement masked arrays in the core -- or at least, not
  yet. Instead, we should focus on figuring out how to implement them
  out-of-core, so that people can try out different approaches without
  us committing to any one approach. And so new prototypes can be
  released more quickly than the NumPy release cycle. And anyway,
  we're going to have to figure out how to experiment with such
  changes out-of-core if NumPy is to continue to evolve without
  forking -- might as well do it now. The existing code can live in
  the main branch, be disabled, or live its own branch -- it'll still be there
  once we know what we're doing.

**Mark** thinks we should:

* The existing code should remain as is, with a global run-time experimental
  flag added which disables NA support by default.

A more detailed rationale for this recommendation is:

* A solid preliminary NA-mask implementation is currently in NumPy
  main. This implementation has been extensively tested
  against scipy and other third-party packages, and has been in main
  in a stable state for a significant amount of time.
* This implementation integrates deeply with the core, providing an
  interface which is usable in the same way R's NA support is. It
  provides a compelling, user-friendly answer to R's NA support.
* The missing data NEP provides a plan for adding bitpattern-based
  dtype support of NAs, which will operate through the same interface
  but allow for the same performance/correctness tradeoffs that R has made.
* Making it very easy for users to try out this implementation, which
  has reasonable feature coverage and performance characteristics, is
  the best way to get more concrete feedback about how NumPy's missing
  data support should look.

Because of its preliminary state, the existing implementation is marked
as experimental in the NumPy documentation. It would be good for this
to remain marked as experimental until it is more fleshed out, for
example supporting struct and array dtypes and with a fuller set of
NumPy operations.

I think the code should stay as it is, except to add a run-time global
NumPy flag, perhaps numpy.experimental.maskna, which defaults to
False and can be toggled to True. In its default state, any NA feature
usage would raise an "ExperimentalError" exception, a measure which
would prevent it from being accidentally used and communicate its
experimental status very clearly.

The `ABI issues`__ seem very tricky to deal with effectively in the 1.x
series of releases, but I believe that with proper implementation-hiding
in a 2.0 release, evolving the software to support various other
ABI ideas that have been discussed is feasible. This is the approach
I like best.

__ http://thread.gmane.org/gmane.comp.python.numeric.general/49485>

**Nathaniel** notes in response that he doesn't really have any
objection to shipping experimental APIs in the main numpy distribution
*if* we're careful to make sure that they don't "leak out" in a way
that leaves us stuck with them. And in principle some sort of "this
violates your warranty" global flag could be a way to do that. (In
fact, this might also be a useful strategy for the kinds of changes
that he favors, of adding minimal hooks to enable us to build
prototypes more easily -- we could have some "rapid prototyping only"
hooks that let prototype hacks get deeper access to NumPy's internals
than we were otherwise ready to support.)

But, he wants to point out two things. First, it seems like we still
have fundamental questions to answer about the NEP design, like
whether masks should have NA semantics or ignore semantics, and there
are already plans to majorly change how NEP masks are exposed and
accessed. So he isn't sure what we'll learn by asking for feedback on
the NEP code in its current state.

And second, given the concerns about their causing (minor) ABI issues,
it's not clear that we could really prevent them from leaking out. (He
looks forward to 2.0 too, but we're not there yet.) So maybe it would
be better if they weren't present in the C API at all, and the hoops
required for testers were instead something like, 'we have included a
hacky pure-Python prototype accessible by typing "import
numpy.experimental.donttrythisathome.NEP" and would welcome feedback'?

If so, then he should mention that he did implement a horribly klugy,
pure Python implementation of the NEP API that works with NumPy
1.6.1. This was mostly as an experiment to see how possible such
prototyping was and to test out a possible ufunc override mechanism,
but if there's interest, the module is available here:
https://github.com/njsmith/numpyNEP

It passes the maskna test-suite, with some minor issues described
in a big comment at the top.

**Mark** responds:

I agree that it's important to be careful when adding new
features to NumPy, but I also believe it is essential that the project
have forward development momentum. A project like NumPy requires
developers to write code for advancement to occur, and obstacles
that impede the writing of code discourage existing developers
from contributing more, and potentially scare away developers
who are thinking about joining in.

All software projects, both open source and closed source, must
balance between short-term practicality and long-term planning.
In the case of the missing data development, there was a short-term
resource commitment to tackle this problem, which is quite immense
in scope. If there isn't a high likelihood of getting a contribution
into NumPy that concretely advances towards a solution, I expect
that individuals and companies interested in doing such work will
have a much harder time justifying a commitment of their resources.
For a project which is core to so many other libraries, only
relying on the good will of selfless volunteers would mean that
NumPy could more easily be overtaken by another project.

In the case of the existing NA contribution at issue, how we resolve
this disagreement represents a decision about how NumPy's
developers, contributors, and users should interact. If we create
a document describing a dispute resolution process, how do we
design it so that it doesn't introduce a large burden and excessive
uncertainty on developers that could prevent them from productively
contributing code?

If we go this route of writing up a decision process which includes
such a dispute resolution mechanism, I think the meat of it should
be a roadmap that potential contributors and developers can follow
to gain influence over NumPy. NumPy development needs broad support
beyond code contributions, and tying influence in the project to
contributions seems to me like it would be a good way to encourage
people to take on tasks like bug triaging/management, continuous
integration/build server administration, and the myriad other
tasks that help satisfy the project's needs. No specific meritocratic,
democratic, consensus-striving system will satisfy everyone, but the
vigour of the discussions around governance and process indicate that
something at least a little bit more formal than the current status
quo is necessary.

In conclusion, I would like the NumPy project to prioritize movement
towards a more flexible and modular ABI/API, balanced with strong
backwards-compatibility constraints and feature additions that
individuals, universities, and companies want to contribute.
I do not believe keeping the NA code in 1.7 as it is, with the small
additional measure of requiring it to be enabled by an experimental
flag, poses a risk of long-term ABI troubles. The greater risk I see
is a continuing lack of developers contributing to the project,
and I believe backing out this code because these worries would create a
risk of reducing developer contribution.


References and Footnotes
------------------------

:ref:`NEP12` describes Mark's NA-semantics/mask implementation/view based mask
handling API.

:ref:`NEP24` ("the alterNEP") was Nathaniel's initial attempt at separating MISSING
and IGNORED handling into bit-patterns versus masks, though there's a bunch
he would change about the proposal at this point.

:ref:`NEP25` ("miniNEP 2") was a later attempt by Nathaniel to sketch out an
implementation strategy for NA dtypes.

A further discussion overview page can be found at:
https://github.com/njsmith/numpy/wiki/NA-discussion-status


Copyright
---------

This document has been placed in the public domain.
.. _NEP42:

==============================================================================
NEP 42 — New and extensible DTypes
==============================================================================

:title: New and extensible DTypes
:Author: Sebastian Berg
:Author: Ben Nathanson
:Author: Marten van Kerkwijk
:Status: Accepted
:Type: Standard
:Created: 2019-07-17
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2020-October/081038.html

.. note::

    This NEP is third in a series:

    - :ref:`NEP40` explains the shortcomings of NumPy's dtype implementation.

    - :ref:`NEP41` gives an overview of our proposed replacement.

    - NEP 42 (this document) describes the new design's datatype-related APIs.

    - :ref:`NEP43` describes the new design's API for universal functions.


******************************************************************************
Abstract
******************************************************************************

NumPy's dtype architecture is monolithic -- each dtype is an instance of  a
single class. There's no principled way to expand it for new dtypes, and the
code is difficult to read and maintain.

As :ref:`NEP 41 <NEP41>` explains, we are proposing a new architecture that is
modular and open to user additions. dtypes will derive from a new ``DType``
class serving as the extension point for new types. ``np.dtype("float64")``
will return an instance of a ``Float64`` class, a subclass of root class
``np.dtype``.

This NEP is one of two that lay out the design and API of this new
architecture. This NEP addresses dtype implementation; :ref:`NEP 43 <NEP43>` addresses
universal functions.

.. note::

    Details of the private and external APIs may change to reflect user
    comments and implementation constraints. The underlying principles and
    choices should not change significantly.


******************************************************************************
Motivation and scope
******************************************************************************

Our goal is to allow user code to create fully featured dtypes for a broad
variety of uses, from physical units (such as meters) to domain-specific
representations of geometric objects. :ref:`NEP 41 <NEP41>` describes a number
of these new dtypes and their benefits.

Any design supporting dtypes must consider:

- How shape and dtype are determined when an array is created
- How array elements are stored and accessed
- The rules for casting dtypes to other dtypes

In addition:

- We want dtypes to comprise a class hierarchy open to new types and to
  subhierarchies, as motivated in :ref:`NEP 41 <NEP41>`.

And to provide this,

- We need to define a user API.

All these are the subjects of this NEP.

- The class hierarchy, its relation to the Python scalar types, and its
  important attributes are described in `nep42_DType class`_.

- The functionality that will support dtype casting is described in `Casting`_.

- The implementation of item access and storage, and the way shape and dtype
  are determined when creating an array, are described in :ref:`nep42_array_coercion`.

- The functionality for users to define their own DTypes is described in
  `Public C-API`_.

The API here and in :ref:`NEP 43 <NEP43>` is entirely on the C side. A Python-side version
will be proposed in a future NEP. A future Python API is expected to be
similar, but provide a more convenient API to reuse the functionality of
existing DTypes. It could also provide shorthands to create structured DTypes
similar to Python's
`dataclasses <https://docs.python.org/3.8/library/dataclasses.html>`_.


******************************************************************************
Backward compatibility
******************************************************************************

The disruption is expected to be no greater than that of a typical NumPy
release.

- The main issues are noted in :ref:`NEP 41 <NEP41>` and will mostly affect
  heavy users of the NumPy C-API.

- Eventually we will want to deprecate the API currently used for creating
  user-defined dtypes.

- Small, rarely noticed inconsistencies are likely to change. Examples:

  - ``np.array(np.nan, dtype=np.int64)`` behaves differently from
    ``np.array([np.nan], dtype=np.int64)`` with the latter raising an error.
    This may require identical results (either both error or both succeed).
  - ``np.array([array_like])`` sometimes behaves differently from
    ``np.array([np.array(array_like)])``
  - array operations may or may not preserve dtype metadata

- Documentation that describes the internal structure of dtypes will need
  to be updated.

The new code must pass NumPy's regular test suite, giving some assurance that
the changes are compatible with existing code.

******************************************************************************
Usage and impact
******************************************************************************

We believe the few structures in this section are sufficient to consolidate
NumPy's present functionality and also to support complex user-defined DTypes.

The rest of the NEP fills in details and provides support for the claim.

Again, though Python is used for illustration, the implementation is a C API only; a
future NEP will tackle the Python API.

After implementing this NEP, creating a DType will be possible by implementing
the following outlined DType base class,
that is further described in `nep42_DType class`_:

.. code-block:: python
    :dedent: 0

    class DType(np.dtype):
        type : type        # Python scalar type
        parametric : bool  # (may be indicated by superclass)

        @property
        def canonical(self) -> bool:
            raise NotImplementedError

        def ensure_canonical(self : DType) -> DType:
            raise NotImplementedError

For casting, a large part of the functionality is provided by the "methods" stored
in ``_castingimpl``

.. code-block:: python
    :dedent: 0

        @classmethod
        def common_dtype(cls : DTypeMeta, other : DTypeMeta) -> DTypeMeta:
            raise NotImplementedError

        def common_instance(self : DType, other : DType) -> DType:
            raise NotImplementedError

        # A mapping of "methods" each detailing how to cast to another DType
        # (further specified at the end of the section)
        _castingimpl = {}

For array-coercion, also part of casting:

.. code-block:: python
    :dedent: 0

        def __dtype_setitem__(self, item_pointer, value):
            raise NotImplementedError

        def __dtype_getitem__(self, item_pointer, base_obj) -> object:
            raise NotImplementedError

        @classmethod
        def __discover_descr_from_pyobject__(cls, obj : object) -> DType:
            raise NotImplementedError

        # initially private:
        @classmethod
        def _known_scalar_type(cls, obj : object) -> bool:
            raise NotImplementedError


Other elements of the casting implementation is the ``CastingImpl``:

.. code-block:: python
    :dedent: 0

    casting = Union["safe", "same_kind", "unsafe"]

    class CastingImpl:
        # Object describing and performing the cast
        casting : casting

        def resolve_descriptors(self, Tuple[DTypeMeta], Tuple[DType|None] : input) -> (casting, Tuple[DType]):
            raise NotImplementedError

        # initially private:
        def _get_loop(...) -> lowlevel_C_loop:
            raise NotImplementedError

which describes the casting from one DType to another. In
:ref:`NEP 43 <NEP43>` this ``CastingImpl`` object is used unchanged to
support universal functions.
Note that the name ``CastingImpl`` here will be generically called
``ArrayMethod`` to accommodate both casting and universal functions.


******************************************************************************
Definitions
******************************************************************************
.. glossary::

   dtype
      The dtype *instance*; this is the object attached to a numpy array.

   DType
      Any subclass of the base type ``np.dtype``.

   coercion
      Conversion of Python types to NumPy arrays and values stored in a NumPy
      array.

   cast
      Conversion of an array to a different dtype.

   parametric type
       A dtype whose representation can change based on a parameter value,
       like a string dtype with a length parameter. All members of the current
       ``flexible`` dtype class are parametric. See
       :ref:`NEP 40 <parametric-datatype-discussion>`.

   promotion
      Finding a dtype that can perform an operation on a mix of dtypes without
      loss of information.

   safe cast
      A cast is safe if no information is lost when changing type.

On the C level we use ``descriptor`` or ``descr`` to mean
*dtype instance*. In the proposed C-API, these terms will distinguish
dtype instances from DType classes.

.. note::
   NumPy has an existing class hierarchy for scalar types, as
   seen :ref:`in the figure <nep-0040_dtype-hierarchy>` of
   :ref:`NEP 40 <NEP40>`, and the new DType hierarchy will resemble it. The
   types are used as an attribute of the single dtype class in the current
   NumPy; they're not dtype classes. They neither harm nor help this work.

.. _nep42_DType class:

******************************************************************************
The DType class
******************************************************************************

This section reviews the structure underlying the proposed DType class,
including the type hierarchy and the use of abstract DTypes.

Class getter
==============================================================================

To create a DType instance from a scalar type users now call
``np.dtype`` (for instance, ``np.dtype(np.int64)``). Sometimes it is
also necessary to access the underlying DType class; this comes up in
particular with type hinting because the "type" of a DType instance is
the DType class. Taking inspiration from type hinting, we propose the
following getter syntax::

    np.dtype[np.int64]

to get the DType class corresponding to a scalar type. The notation
works equally well with built-in and user-defined DTypes.

This getter eliminates the need to create an explicit name for every
DType, crowding the ``np`` namespace; the getter itself signifies the
type. It also opens the possibility of making ``np.ndarray`` generic
over DType class using annotations like::

    np.ndarray[np.dtype[np.float64]]

The above is fairly verbose, so it is possible that we will include
aliases like::

    Float64 = np.dtype[np.float64]

in ``numpy.typing``, thus keeping annotations concise but still
avoiding crowding the ``np`` namespace as discussed above. For a
user-defined DType::

    class UserDtype(dtype): ...

one can do ``np.ndarray[UserDtype]``, keeping annotations concise in
that case without introducing boilerplate in NumPy itself. For a
user-defined scalar type::

    class UserScalar(generic): ...

we would need to add a typing overload to ``dtype``::

    @overload
    __new__(cls, dtype: Type[UserScalar], ...) -> UserDtype

to allow ``np.dtype[UserScalar]``.

The initial implementation probably will return only concrete (not abstract)
DTypes.

*This item is still under review.*


Hierarchy and abstract classes
==============================================================================

We will use abstract classes as building blocks of our extensible DType class
hierarchy.

1. Abstract classes are inherited cleanly, in principle allowing checks like
   ``isinstance(np.dtype("float64"), np.inexact)``.

2. Abstract classes allow a single piece of code to handle a multiplicity of
   input types. Code written to accept Complex objects can work with numbers
   of any precision; the precision of the results is determined by the
   precision of the arguments.

3. There's room for user-created families of DTypes. We can envision an
   abstract ``Unit`` class for physical units, with a concrete subclass like
   ``Float64Unit``. Calling ``Unit(np.float64, "m")`` (``m`` for meters) would
   be equivalent to ``Float64Unit("m")``.

4. The implementation of universal functions in :ref:`NEP 43 <NEP43>` may require
   a class hierarchy.

**Example:** A NumPy ``Categorical`` class would be a match for pandas
``Categorical`` objects, which can contain integers or general Python objects.
NumPy needs a DType that it can assign a Categorical to, but it also needs
DTypes like ``CategoricalInt64`` and ``CategoricalObject`` such that
``common_dtype(CategoricalInt64, String)`` raises an error, but
``common_dtype(CategoricalObject, String)`` returns an ``object`` DType. In
our scheme, ``Categorical`` is an abstract type with ``CategoricalInt64`` and
``CategoricalObject`` subclasses.


Rules for the class structure, illustrated :ref:`below <nep42_hierarchy_figure>`:

1. Abstract DTypes cannot be instantiated. Instantiating an abstract DType
   raises an error, or perhaps returns an instance of a concrete subclass.
   Raising an error will be the default behavior and may be required initially.

2. While abstract DTypes may be superclasses, they may also act like Python's
   abstract base classes (ABC) allowing registration instead of subclassing.
   It may be possible to simply use or inherit from Python ABCs.

3. Concrete DTypes may not be subclassed. In the future this might be relaxed
   to allow specialized implementations such as a GPU float64 subclassing a
   NumPy float64.

The
`Julia language <https://docs.julialang.org/en/v1/manual/types/#man-abstract-types-1>`_
has a similar prohibition against subclassing concrete types.
For example methods such as the later ``__common_instance__`` or
``__common_dtype__`` cannot work for a subclass unless they were designed
very carefully.
It helps avoid unintended vulnerabilities to implementation changes that
result from subclassing types that were not written to be subclassed.
We believe that the DType API should rather be extended to simplify wrapping
of existing functionality.

The DType class requires C-side storage of methods and additional information,
to be implemented by a ``DTypeMeta`` class. Each ``DType`` class is an
instance of ``DTypeMeta`` with a well-defined and extensible interface;
end users ignore it.

.. _nep42_hierarchy_figure:
.. figure:: _static/dtype_hierarchy.svg
    :figclass: align-center


Miscellaneous methods and attributes
==============================================================================

This section collects definitions in the DType class that are not used in
casting and array coercion, which are described in detail below.

* Existing dtype methods (:class:`numpy.dtype`) and C-side fields are preserved.

* ``DType.type`` replaces ``dtype.type``. Unless a use case arises,
  ``dtype.type`` will be deprecated.
  This indicates a Python scalar type which represents the same values as
  the DType. This is the same type as used in the proposed `Class getter`_
  and for `DType discovery during array coercion`_.
  (This can may also be set for abstract DTypes, this is necessary
  for array coercion.)

* A new ``self.canonical`` property generalizes the notion of byte order to
  indicate whether data has been stored in a default/canonical way. For
  existing code, "canonical" will just signify native byte order, but it can
  take on new meanings in new DTypes -- for instance, to distinguish a
  complex-conjugated instance of Complex which stores ``real - imag`` instead
  of ``real + imag``. The ISNBO ("is
  native byte order") flag might be repurposed as the canonical flag.

* Support is included for parametric DTypes. A DType will be deemed parametric
  if it inherits from ParametricDType.

* DType methods may resemble or even reuse existing Python slots. Thus Python
  special slots are off-limits for user-defined DTypes (for instance, defining
  ``Unit("m") > Unit("cm")``), since we may want to develop a meaning for these
  operators that is common to all DTypes.

* Sorting functions are moved to the DType class. They may be implemented by
  defining a method ``dtype_get_sort_function(self, sortkind="stable") ->
  sortfunction`` that must return ``NotImplemented`` if the given ``sortkind``
  is not known.

* Functions that cannot be removed are implemented as special methods.
  Many of these were previously defined part of the :c:type:`PyArray_ArrFuncs`
  slot of the dtype instance (``PyArray_Descr *``) and include functions
  such as ``nonzero``, ``fill`` (used for ``np.arange``), and
  ``fromstr`` (used to parse text files).
  These old methods will be deprecated and replacements
  following the new design principles added.
  The API is not defined here. Since these methods can be deprecated and renamed
  replacements added, it is acceptable if these new methods have to be modified.

* Use of ``kind`` for non-built-in types is discouraged in favor of
  ``isinstance`` checks.  ``kind`` will return the ``__qualname__`` of the
  object to ensure uniqueness for all DTypes. On the C side, ``kind`` and
  ``char`` are set to ``\0`` (NULL character).
  While ``kind`` will be discouraged, the current ``np.issubdtype``
  may remain the preferred method for this type of check.

* A method ``ensure_canonical(self) -> dtype`` returns a new dtype (or
  ``self``) with the ``canonical`` flag set.

* Since NumPy's approach is to provide functionality through unfuncs,
  functions like sorting that will be implemented in DTypes might eventually be
  reimplemented as generalized ufuncs.

.. _nep_42_casting:

******************************************************************************
Casting
******************************************************************************

We review here the operations related to casting arrays:

- Finding the "common dtype," returned by :func:`numpy.promote_types` and
  :func:`numpy.result_type`

- The result of calling :func:`numpy.can_cast`

We show how casting arrays with ``astype(new_dtype)`` will be implemented.

`Common DType` operations
==============================================================================

When input types are mixed, a first step is to find a DType that can hold
the result without loss of information -- a "common DType."

Array coercion and concatenation both return a common dtype instance. Most
universal functions use the common DType for dispatching, though they might
not use it for a result (for instance, the result of a comparison is always
bool).

We propose the following implementation:

-  For two DType classes::

       __common_dtype__(cls, other : DTypeMeta) -> DTypeMeta

   Returns a new DType, often one of the inputs, which can represent values
   of both input DTypes.  This should usually be minimal:
   the common DType of ``Int16`` and ``Uint16`` is ``Int32`` and not ``Int64``.
   ``__common_dtype__``  may return NotImplemented to defer to other and,
   like Python operators, subclasses take precedence (their
   ``__common_dtype__`` method is tried first).

-  For two instances of the same DType::

    __common_instance__(self: SelfT, other : SelfT) -> SelfT

   For nonparametric built-in dtypes, this returns a canonicalized copy of
   ``self``, preserving metadata. For nonparametric user types, this provides
   a default implementation.

-  For instances of different DTypes, for example ``>float64`` and ``S8``,
   the operation is done in three steps:

   1. ``Float64.__common_dtype__(type(>float64), type(S8))``
      returns ``String`` (or defers to ``String.__common_dtype__``).

   2. The casting machinery (explained in detail below) provides the
      information that ``">float64"`` casts to ``"S32"``

   3. ``String.__common_instance__("S8", "S32")`` returns the final ``"S32"``.

The benefit of this handoff is to reduce duplicated code and keep concerns
separate. DType implementations don't need to know how to cast, and the
results of casting can be extended to new types, such as a new string encoding.

This means the implementation will work like this::

    def common_dtype(DType1, DType2):
        common_dtype = type(dtype1).__common_dtype__(type(dtype2))
        if common_dtype is NotImplemented:
            common_dtype = type(dtype2).__common_dtype__(type(dtype1))
            if common_dtype is NotImplemented:
                raise TypeError("no common dtype")
        return common_dtype

    def promote_types(dtype1, dtype2):
        common = common_dtype(type(dtype1), type(dtype2))

        if type(dtype1) is not common:
            # Find what dtype1 is cast to when cast to the common DType
            # by using the CastingImpl as described below:
            castingimpl = get_castingimpl(type(dtype1), common)
            safety, (_, dtype1) = castingimpl.resolve_descriptors(
                    (common, common), (dtype1, None))
            assert safety == "safe"  # promotion should normally be a safe cast

        if type(dtype2) is not common:
            # Same as above branch for dtype1.

        if dtype1 is not dtype2:
            return common.__common_instance__(dtype1, dtype2)

Some of these steps may be optimized for nonparametric DTypes.

Since the type returned by ``__common_dtype__`` is not necessarily one of the
two arguments, it's not equivalent to NumPy's "safe" casting.
Safe casting works for ``np.promote_types(int16, int64)``, which returns
``int64``, but fails for::

    np.promote_types("int64", "float32") -> np.dtype("float64")

It is the responsibility of the DType author to ensure that the inputs
can be safely cast to the ``__common_dtype__``.

Exceptions may apply. For example, casting ``int32`` to
a (long enough) string is  at least at this time  considered "safe".
However ``np.promote_types(int32, String)`` will *not* be defined.

**Example:**

``object`` always chooses ``object`` as the common DType.  For
``datetime64`` type promotion is defined with no other datatype, but if
someone were to implement a new higher precision datetime, then::

   HighPrecisionDatetime.__common_dtype__(np.dtype[np.datetime64])

would return ``HighPrecisionDatetime``, and the casting implementation,
as described below, may need to decide how to handle the datetime unit.


**Alternatives:**

-  We're pushing the decision on common DTypes to the DType classes. Suppose
   instead we could turn to a universal algorithm based on safe casting,
   imposing a total order on DTypes and returning the first type that both
   arguments could cast to safely.

   It would be difficult to devise a reasonable total order, and it would have
   to accept new entries. Beyond that, the approach is flawed because
   importing a type can change the behavior of a program. For example, a
   program requiring the common DType of ``int16`` and ``uint16`` would
   ordinarily get the built-in type ``int32`` as the first match; if the
   program adds ``import int24``, the first match becomes ``int24`` and the
   smaller type might make the program overflow for the first time. [1]_

-  A more flexible common DType could be implemented in the future where
   ``__common_dtype__`` relies on information from the casting logic.
   Since ``__commond_dtype__`` is a method a such a default implementation
   could be added at a later time.

-  The three-step handling of differing dtypes could, of course, be coalesced.
   It would lose the value of splitting in return for a possibly faster
   execution. But few cases would benefit. Most cases, such as array coercion,
   involve a single Python type (and thus dtype).


The cast operation
==============================================================================

Casting is perhaps the most complex and interesting DType operation. It
is much like a typical universal function on arrays, converting one input to a
new output, with two distinctions:

- Casting always requires an explicit output datatype.
- The NumPy iterator API requires access to functions that are lower-level
  than what universal functions currently need.

Casting can be complex and may not implement all details of each input
datatype (such as non-native byte order or unaligned access). So a complex
type conversion might entail 3 steps:

1. The input datatype is normalized and prepared for the cast.
2. The cast is performed.
3. The result, which is in a normalized form, is cast to the requested
   form (non-native byte order).

Further, NumPy provides different casting kinds or safety specifiers:

* ``equivalent``, allowing only byte-order changes
* ``safe``, requiring a type large enough to preserve value
* ``same_kind``, requiring a safe cast or one within a kind, like float64 to float32
* ``unsafe``, allowing any data conversion

and in some cases a cast may be just a view.

We need to support the two current signatures of ``arr.astype``:

- For DTypes: ``arr.astype(np.String)``

  - current spelling ``arr.astype("S")``
  - ``np.String`` can be an abstract DType

- For dtypes: ``arr.astype(np.dtype("S8"))``


We also have two signatures of ``np.can_cast``:

- Instance to class: ``np.can_cast(dtype, DType, "safe")``
- Instance to instance: ``np.can_cast(dtype, other_dtype, "safe")``

On the Python level ``dtype`` is overloaded to mean class or instance.

A third ``can_cast`` signature, ``np.can_cast(DType, OtherDType, "safe")``,may be used
internally but need not be exposed to Python.

During DType creation, DTypes will be able to pass a list of ``CastingImpl``
objects, which can define casting to and from the DType.

One of them should define the cast between instances of that DType. It can be
omitted if the DType has only a single implementation and is nonparametric.

Each ``CastingImpl`` has a distinct DType signature:

  ``CastingImpl[InputDtype, RequestedDtype]``

and implements the following methods and attributes:


* To report safeness,

  ``resolve_descriptors(self, Tuple[DTypeMeta], Tuple[DType|None] : input) -> casting, Tuple[DType]``.

  The ``casting`` output reports safeness (safe, unsafe, or same-kind), and
  the tuple is used for more multistep casting, as in the example below.

* To get a casting function,

  ``get_loop(...) -> function_to_handle_cast (signature to be decided)``

  returns a low-level implementation of a strided casting function
  ("transfer function") capable of performing the
  cast.

  Initially the implementation will be *private*, and users will only be
  able to provide strided loops with the signature.

* For performance, a ``casting`` attribute taking a value of  ``equivalent``, ``safe``,
  ``unsafe``, or ``same-kind``.


**Performing a cast**

.. _nep42_cast_figure:

.. figure:: _static/casting_flow.svg
    :figclass: align-center

The above figure illustrates a multistep
cast of an ``int24`` with a value of ``42`` to a string of length 20
(``"S20"``).

We've picked an example where the implementer has only provided limited
functionality: a function to cast an ``int24`` to an ``S8`` string (which can
hold all 24-bit integers). This means multiple conversions are needed.

The full process is:

1. Call

   ``CastingImpl[Int24, String].resolve_descriptors((Int24, String), (int24, "S20"))``.

   This provides the information that ``CastingImpl[Int24, String]`` only
   implements the cast of ``int24`` to ``"S8"``.

2. Since ``"S8"`` does not match ``"S20"``, use

   ``CastingImpl[String, String].get_loop()``

   to find the transfer (casting) function to convert an ``"S8"`` into an ``"S20"``

3. Fetch the transfer function to convert an ``int24`` to an ``"S8"`` using

   ``CastingImpl[Int24, String].get_loop()``

4. Perform the actual cast using the two transfer functions:

   ``int24(42) -> S8("42") -> S20("42")``.

   ``resolve_descriptors`` allows the implementation for

   ``np.array(42, dtype=int24).astype(String)``

   to call

   ``CastingImpl[Int24, String].resolve_descriptors((Int24, String), (int24, None))``.

   In this case the result of ``(int24, "S8")`` defines the correct cast:

   ``np.array(42, dtype=int24).astype(String) == np.array("42", dtype="S8")``.

**Casting safety**

To compute ``np.can_cast(int24, "S20", casting="safe")``, only the
``resolve_descriptors`` function is required and
is called in the same way as in :ref:`the figure describing a cast <nep42_cast_figure>`.

In this case, the calls to ``resolve_descriptors``, will also provide the
information that ``int24 -> "S8"`` as well as ``"S8" -> "S20"`` are safe
casts, and thus also the ``int24 -> "S20"`` is a safe cast.

In some cases, no cast is necessary. For example, on most Linux systems
``np.dtype("long")`` and ``np.dtype("longlong")`` are different dtypes but are
both 64-bit integers. In this case, the cast can be performed using
``long_arr.view("longlong")``. The information that a cast is a view will be
handled by an additional flag.  Thus the ``casting`` can have the 8 values in
total: the original 4 of ``equivalent``, ``safe``, ``unsafe``, and ``same-kind``,
plus ``equivalent+view``, ``safe+view``, ``unsafe+view``, and
``same-kind+view``. NumPy currently defines ``dtype1 == dtype2`` to be True
only if byte order matches. This functionality can be replaced with the
combination of "equivalent" casting and the "view" flag.

(For more information on the ``resolve_descriptors`` signature see the
:ref:`nep42_C-API` section below and :ref:`NEP 43 <NEP43>`.)


**Casting between instances of the same DType**

To keep down the number of casting
steps, CastingImpl must be capable of any conversion between all instances
of this DType.

In general the DType implementer must include ``CastingImpl[DType, DType]``
unless there is only a singleton instance.

**General multistep casting**

We could implement certain casts, such as ``int8`` to ``int24``,
even if the user provides only an ``int16 -> int24`` cast. This proposal does
not provide that, but future work might find such casts dynamically, or at least
allow ``resolve_descriptors`` to return arbitrary ``dtypes``.

If ``CastingImpl[Int8, Int24].resolve_descriptors((Int8, Int24), (int8, int24))``
returns ``(int16, int24)``, the actual casting process could be extended to include
the ``int8 -> int16`` cast. This adds a step.


**Example:**

The implementation for casting integers to datetime would generally
say that this cast is unsafe (because it is always an unsafe cast).
Its ``resolve_descriptors`` function may look like::

     def resolve_descriptors(self, DTypes, given_dtypes):
        from_dtype, to_dtype = given_dtypes
        from_dtype = from_dtype.ensure_canonical()  # ensure not byte-swapped
        if to_dtype is None:
            raise TypeError("Cannot convert to a NumPy datetime without a unit")
        to_dtype = to_dtype.ensure_canonical()  # ensure not byte-swapped

        # This is always an "unsafe" cast, but for int64, we can represent
        # it by a simple view (if the dtypes are both canonical).
        # (represented as C-side flags here).
        safety_and_view = NPY_UNSAFE_CASTING | _NPY_CAST_IS_VIEW
        return safety_and_view, (from_dtype, to_dtype)

.. note::

    While NumPy currently defines integer-to-datetime casts, with the possible
    exception of the unit-less ``timedelta64`` it may be better to not define
    these casts at all.  In general we expect that user defined DTypes will be
    using custom methods such as ``unit.drop_unit(arr)`` or ``arr *
    unit.seconds``.


**Alternatives:**

- Our design objectives are:
  -  Minimize the number of DType methods and avoid code duplication.
  -  Mirror the implementation of universal functions.

- The decision to use only the DType classes in the first step of finding the
  correct ``CastingImpl`` in addition to defining ``CastingImpl.casting``,
  allows to retain the current default implementation of
  ``__common_dtype__`` for existing user defined dtypes, which could be
  expanded in the future.

- The split into multiple steps may seem to add complexity rather than reduce
  it, but it consolidates the signatures of ``np.can_cast(dtype, DTypeClass)``
  and ``np.can_cast(dtype, other_dtype)``.

  Further, the API guarantees separation of concerns for user DTypes. The user
  ``Int24`` dtype does not have to handle all string lengths if it does not
  wish to do so.  Further, an encoding added to the ``String`` DType would
  not affect the overall cast. The ``resolve_descriptors`` function
  can keep returning the default encoding and the ``CastingImpl[String,
  String]`` can take care of any necessary encoding changes.

- The main alternative is moving most of the information that is here pushed
  into the ``CastingImpl`` directly into methods on the DTypes. But this
  obscures the similarity between casting and universal functions. It does
  reduce indirection, as noted below.

- An earlier proposal defined two methods ``__can_cast_to__(self, other)`` to
  dynamically return ``CastingImpl``. This
  removes the requirement to define all possible casts at DType creation
  (of one of the involved DTypes).

  Such an API could be added later. It resembles Python's ``__getattr__`` in
  providing additional control over attribute lookup.


**Notes:**

``CastingImpl`` is used as a name in this NEP to clarify that it implements
all functionality related to a cast. It is meant to be identical to the
``ArrayMethod`` proposed in NEP 43 as part of restructuring ufuncs to handle
new DTypes. All type definitions are expected to be named ``ArrayMethod``.

The way dispatching works for ``CastingImpl`` is planned to be limited
initially and fully opaque. In the future, it may or may not be moved into a
special UFunc, or behave more like a universal function.


.. _nep42_array_coercion:


Coercion to and from Python objects
==============================================================================

When storing a single value in an array or taking it out, it is necessary to
coerce it -- that is, convert it -- to and from the low-level representation
inside the array.

Coercion is slightly more complex than typical casts. One reason is that a
Python object could itself be a 0-dimensional array or scalar with an
associated DType.

Coercing to and from Python scalars requires two to three
methods that largely correspond to the current definitions:

1. ``__dtype_setitem__(self, item_pointer, value)``

2. ``__dtype_getitem__(self, item_pointer, base_obj) -> object``;
   ``base_obj`` is for memory management and usually ignored; it points to
   an object owning the data. Its only role is to support structured datatypes
   with subarrays within NumPy, which currently return views into the array.
   The function returns an equivalent Python scalar (i.e. typically a NumPy
   scalar).

3. ``__dtype_get_pyitem__(self, item_pointer, base_obj) -> object`` (initially
   hidden for new-style user-defined datatypes, may be exposed on user
   request). This corresponds to the ``arr.item()`` method also used by
   ``arr.tolist()`` and returns Python floats, for example, instead of NumPy
   floats.

(The above is meant for C-API. A Python-side API would have to use byte
buffers or similar to implement this, which may be useful for prototyping.)

When a certain scalar
has a known (different) dtype, NumPy may in the future use casting instead of
``__dtype_setitem__``.

A user datatype is (initially) expected to implement
``__dtype_setitem__`` for its own ``DType.type`` and all basic Python scalars
it wishes to support (e.g. ``int`` and ``float``). In the future a
function ``known_scalar_type`` may be made public to allow a user dtype to signal
which Python scalars it can store directly.


**Implementation:** The pseudocode implementation for setting a single item in
an array from an arbitrary Python object ``value`` is (some
functions here are defined later)::

    def PyArray_Pack(dtype, item_pointer, value):
        DType = type(dtype)
        if DType.type is type(value) or DType.known_scalartype(type(value)):
            return dtype.__dtype_setitem__(item_pointer, value)

        # The dtype cannot handle the value, so try casting:
        arr = np.array(value)
        if arr.dtype is object or arr.ndim != 0:
            # not a numpy or user scalar; try using the dtype after all:
            return dtype.__dtype_setitem__(item_pointer, value)

         arr.astype(dtype)
         item_pointer.write(arr[()])

where the call to ``np.array()`` represents the dtype discovery and is
not actually performed.

**Example:** Current ``datetime64`` returns ``np.datetime64`` scalars and can
be assigned from ``np.datetime64``. However, the datetime
``__dtype_setitem__`` also allows assignment from date strings ("2016-05-01")
or Python integers. Additionally the datetime ``__dtype_get_pyitem__``
function actually returns a Python ``datetime.datetime`` object (most of the
time).


**Alternatives:** This functionality could also be implemented as a cast to and
from the ``object`` dtype.
However, coercion is slightly more complex than typical casts.
One reason is that in general a Python object could itself be a
zero-dimensional array or scalar with an associated DType.
Such an object has a DType, and the correct cast to another DType is already
defined::

    np.array(np.float32(4), dtype=object).astype(np.float64)

is identical to::

    np.array(4, dtype=np.float32).astype(np.float64)

Implementing the first ``object`` to ``np.float64`` cast explicitly,
would require the user to take to duplicate or fall back to existing
casting functionality.

It is certainly possible to describe the coercion to and from Python objects
using the general casting machinery, but the ``object`` dtype is special and
important enough to be handled by NumPy using the presented methods.

**Further issues and discussion:**

- The ``__dtype_setitem__`` function duplicates some code, such as coercion
  from a string.

  ``datetime64`` allows assignment from string, but the same conversion also
  occurs for casting from the string dtype to ``datetime64``.

  We may in the future expose the ``known_scalartype`` function to allow the
  user to implement such duplication.

  For example, NumPy would normally use

  ``np.array(np.string_("2019")).astype(datetime64)``

  but ``datetime64`` could choose to use its ``__dtype_setitem__`` instead
  for performance reasons.

- There is an issue about how subclasses of scalars should be handled. We
  anticipate to stop automatically detecting the dtype for
  ``np.array(float64_subclass)`` to be float64. The user can still provide
  ``dtype=np.float64``. However, the above automatic casting using
  ``np.array(scalar_subclass).astype(requested_dtype)`` will fail. In many
  cases, this is not an issue, since the Python ``__float__`` protocol can be
  used instead.  But in some cases, this will mean that subclasses of Python
  scalars will behave differently.

.. note::

    *Example:* ``np.complex256`` should not use ``__float__`` in its
    ``__dtype_setitem__`` method in the future unless it is a known floating
    point type.  If the scalar is a subclass of a different high precision
    floating point type (e.g. ``np.float128``) then this currently loses
    precision without notifying the user.
    In that case ``np.array(float128_subclass(3), dtype=np.complex256)``
    may fail unless the ``float128_subclass`` is first converted to the
    ``np.float128`` base class.


DType discovery during array coercion
==============================================================================

An important step in the use of NumPy arrays is creation of the array from
collections of generic Python objects.

**Motivation:** Although the distinction is not clear currently, there are two main needs::

    np.array([1, 2, 3, 4.])

needs to guess the correct dtype based on the Python objects inside.
Such an array may include a mix of datatypes, as long as they can be
promoted.
A second use case is when users provide the output DType class, but not the
specific DType instance::

    np.array([object(), None], dtype=np.dtype[np.string_])  # (or `dtype="S"`)

In this case the user indicates that ``object()`` and ``None`` should be
interpreted as strings.
The need to consider the user provided DType also arises for a future
``Categorical``::

    np.array([1, 2, 1, 1, 2], dtype=Categorical)

which must interpret the numbers as unique categorical values rather than
integers.

There are three further issues to consider:

1. It may be desirable to create datatypes associated
   with normal Python scalars (such as ``datetime.datetime``) that do not
   have a ``dtype`` attribute already.

2. In general, a datatype could represent a sequence, however, NumPy currently
   assumes that sequences are always collections of elements
   (the sequence cannot be an element itself).
   An example would be a ``vector`` DType.

3. An array may itself contain arrays with a specific dtype (even
   general Python objects).  For example:
   ``np.array([np.array(None, dtype=object)], dtype=np.String)``
   poses the issue of how to handle the included array.

Some of these difficulties arise because finding the correct shape
of the output array and finding the correct datatype are closely related.

**Implementation:** There are two distinct cases above:

1. The user has provided no dtype information.

2. The user provided a DType class  -- as represented, for example, by ``"S"``
   representing a string of any length.

In the first case, it is necessary to establish a mapping from the Python type(s)
of the constituent elements to the DType class.
Once the DType class is known, the correct dtype instance needs to be found.
In the case of strings, this requires to find the string length.

These two cases shall be implemented by leveraging two pieces of information:

1. ``DType.type``: The current type attribute to indicate which Python scalar
   type is associated with the DType class (this is a *class* attribute that always
   exists for any datatype and is not limited to array coercion).

2. ``__discover_descr_from_pyobject__(cls, obj) -> dtype``: A classmethod that
   returns the correct descriptor given the input object.
   Note that only parametric DTypes have to implement this.
   For nonparametric DTypes using the default instance will always be acceptable.

The Python scalar type which is already associated with a DType through the
``DType.type`` attribute maps from the DType to the Python scalar type.
At registration time, a DType may choose to allow automatically discover for
this Python scalar type.
This requires a lookup in the opposite direction, which will be implemented
using global a mapping (dictionary-like) of::

   known_python_types[type] = DType

Correct garbage collection requires additional care.
If both the Python scalar type (``pytype``) and ``DType`` are created dynamically,
they will potentially be deleted again.
To allow this, it must be possible to make the above mapping weak.
This requires that the ``pytype`` holds a reference of ``DType`` explicitly.
Thus, in addition to building the global mapping, NumPy will store the ``DType`` as
``pytype.__associated_array_dtype__`` in the Python type.
This does *not* define the mapping and should *not* be accessed directly.
In particular potential inheritance of the attribute does not mean that NumPy will use the
superclasses ``DType`` automatically. A new ``DType`` must be created for the
subclass.

.. note::

    Python integers do not have a clear/concrete NumPy type associated right
    now. This is because during array coercion NumPy currently finds the first
    type capable of representing their value in the list of `long`, `unsigned
    long`, `int64`, `unsigned int64`, and `object` (on many machines `long` is
    64 bit).

    Instead they will need to be implemented using an ``AbstractPyInt``. This
    DType class can then provide ``__discover_descr_from_pyobject__`` and
    return the actual dtype which is e.g. ``np.dtype("int64")``. For
    dispatching/promotion in ufuncs, it will also be necessary to dynamically
    create ``AbstractPyInt[value]`` classes (creation can be cached), so that
    they can provide the current value based promotion functionality provided
    by ``np.result_type(python_integer, array)`` [2]_ .

To allow for a DType to accept inputs as scalars that are not basic Python
types or instances of ``DType.type``, we use ``known_scalar_type`` method.
This can allow discovery of a ``vector`` as a scalar (element) instead of a sequence
(for the command ``np.array(vector, dtype=VectorDType)``) even when ``vector`` is itself a
sequence or even an array subclass. This will *not* be public API initially,
but may be made public at a later time.

**Example:** The current datetime DType requires a
``__discover_descr_from_pyobject__`` which returns the correct unit for string
inputs.  This allows it to support::

    np.array(["2020-01-02", "2020-01-02 11:24"], dtype="M8")

By inspecting the date strings. Together with the common dtype
operation, this allows it to automatically find that the datetime64 unit
should be "minutes".


**NumPy internal implementation:** The implementation to find the correct dtype
will work similar to the following pseudocode::

    def find_dtype(array_like):
        common_dtype = None
        for element in array_like:
            # default to object dtype, if unknown
            DType = known_python_types.get(type(element), np.dtype[object])
            dtype = DType.__discover_descr_from_pyobject__(element)

            if common_dtype is None:
                common_dtype = dtype
            else:
                common_dtype = np.promote_types(common_dtype, dtype)

In practice, the input to ``np.array()`` is a mix of sequences and array-like
objects, so that deciding what is an element requires to check whether it
is a sequence.
The full algorithm (without user provided dtypes) thus looks more like::

    def find_dtype_recursive(array_like, dtype=None):
        """
        Recursively find the dtype for a nested sequences (arrays are not
        supported here).
        """
        DType = known_python_types.get(type(element), None)

        if DType is None and is_array_like(array_like):
            # Code for a sequence, an array_like may have a DType we
            # can use directly:
            for element in array_like:
                dtype = find_dtype_recursive(element, dtype=dtype)
            return dtype

        elif DType is None:
            DType = np.dtype[object]

        # dtype discovery and promotion as in `find_dtype` above

If the user provides ``DType``, then this DType will be tried first, and the
``dtype`` may need to be cast before the promotion is performed.

**Limitations:** The motivational point 3. of a nested array
``np.array([np.array(None, dtype=object)], dtype=np.String)`` is currently
(sometimes) supported by inspecting all elements of the nested array.
User DTypes will implicitly handle these correctly if the nested array
is of ``object`` dtype.
In some other cases NumPy will retain backward compatibility for existing
functionality only.
NumPy uses such functionality to allow code such as::

    >>> np.array([np.array(["2020-05-05"], dtype="S")], dtype=np.datetime64)
    array([['2020-05-05']], dtype='datetime64[D]')

which discovers the datetime unit ``D`` (days).
This possibility will not be accessible to user DTypes without an
intermediate cast to ``object`` or a custom function.

The use of a global type map means that an error or warning has to be given if
two DTypes wish to map to the same Python type. In most cases user DTypes
should only be implemented for types defined within the same library to avoid
the potential for conflicts. It will be the DType implementor's responsibility
to be careful about this and use avoid registration when in doubt.

**Alternatives:**

- Instead of a global mapping, we could rely on the scalar attribute
  ``scalar.__associated_array_dtype__``. This only creates a difference in
  behavior for subclasses, and the exact implementation can be undefined
  initially. Scalars will be expected to derive from a NumPy scalar. In
  principle NumPy could, for a time, still choose to rely on the attribute.

- An earlier proposal for the ``dtype`` discovery algorithm used a two-pass
  approach, first finding the correct ``DType`` class and only then
  discovering the parametric ``dtype`` instance. It was rejected as
  needlessly complex. But it would have enabled value-based promotion
  in universal functions, allowing::

    np.add(np.array([8], dtype="uint8"), [4])

  to return a ``uint8`` result (instead of ``int16``), which currently happens for::

    np.add(np.array([8], dtype="uint8"), 4)

  (note the list ``[4]`` instead of scalar ``4``).
  This is not a feature NumPy currently has or desires to support.

**Further issues and discussion:** It is possible to create a DType
such as Categorical, array, or vector which can only be used if ``dtype=DType``
is provided. Such DTypes cannot roundtrip correctly. For example::

    np.array(np.array(1, dtype=Categorical)[()])

will result in an integer array. To get the original ``Categorical`` array
``dtype=Categorical`` will need to be passed explicitly.
This is a general limitation, but round-tripping is always possible if
``dtype=original_arr.dtype`` is passed.


.. _nep42_c-api:

******************************************************************************
Public C-API
******************************************************************************

DType creation
==============================================================================

To create a new DType the user will need to define the methods and attributes
outlined in the `Usage and impact`_ section and detailed throughout this
proposal.

In addition, some methods similar to those in :c:type:`PyArray_ArrFuncs` will
be needed for the slots struct below.

As mentioned in :ref:`NEP 41 <NEP41>`, the interface to define this DType
class in C is modeled after :PEP:`384`: Slots and some additional information
will be passed in a slots struct and identified by ``ssize_t`` integers::

    static struct PyArrayMethodDef slots[] = {
        {NPY_dt_method, method_implementation},
        ...,
        {0, NULL}
    }

    typedef struct{
      PyTypeObject *typeobj;    /* type of python scalar or NULL */
      int flags                 /* flags, including parametric and abstract */
      /* NULL terminated CastingImpl; is copied and references are stolen */
      CastingImpl *castingimpls[];
      PyType_Slot *slots;
      PyTypeObject *baseclass;  /* Baseclass or NULL */
    } PyArrayDTypeMeta_Spec;

    PyObject* PyArray_InitDTypeMetaFromSpec(PyArrayDTypeMeta_Spec *dtype_spec);

All of this is passed by copying.

**TODO:** The DType author should be able to define new methods for the
DType, up to defining a full object, and, in the future, possibly even
extending the ``PyArrayDTypeMeta_Type`` struct. We have to decide what to make
available initially. A solution may be to allow inheriting only from an
existing class: ``class MyDType(np.dtype, MyBaseclass)``. If ``np.dtype`` is
first in the method resolution order, this also prevents an undesirable
override of slots like ``==``.

The ``slots`` will be identified by names which are prefixed with ``NPY_dt_``
and are:

* ``is_canonical(self) -> {0, 1}``
* ``ensure_canonical(self) -> dtype``
* ``default_descr(self) -> dtype`` (return must be native and should normally be a singleton)
* ``setitem(self, char *item_ptr, PyObject *value) -> {-1, 0}``
* ``getitem(self, char *item_ptr, PyObject (base_obj) -> object or NULL``
* ``discover_descr_from_pyobject(cls, PyObject) -> dtype or NULL``
* ``common_dtype(cls, other) -> DType, NotImplemented, or NULL``
* ``common_instance(self, other) -> dtype or NULL``

Where possible, a default implementation will be provided if the slot is
omitted or set to ``NULL``. Nonparametric dtypes do not have to implement:

* ``discover_descr_from_pyobject`` (uses ``default_descr`` instead)
* ``common_instance`` (uses ``default_descr`` instead)
* ``ensure_canonical`` (uses ``default_descr`` instead).

Sorting is expected to be implemented using:

* ``get_sort_function(self, NPY_SORTKIND sort_kind) -> {out_sortfunction, NotImplemented, NULL}``.

For convenience, it will be sufficient if the user implements only:

* ``compare(self, char *item_ptr1, char *item_ptr2, int *res) -> {-1, 0, 1}``


**Limitations:** The ``PyArrayDTypeMeta_Spec`` struct is clumsy to extend (for
instance, by adding a version tag to the ``slots`` to indicate a new, longer
version). We could use a function to provide the struct; it would require
memory management but would allow ABI-compatible extension (the struct is
freed again when the DType is created).


CastingImpl
==============================================================================

The external API for ``CastingImpl`` will be limited initially to defining:

* ``casting`` attribute, which can be one of the supported casting kinds.
  This is the safest cast possible. For example, casting between two NumPy
  strings is of course "safe" in general, but may be "same kind" in a specific
  instance if the second string is shorter. If neither type is parametric the
  ``resolve_descriptors`` must use it.

* ``resolve_descriptors(PyArrayMethodObject *self, PyArray_DTypeMeta *DTypes[2],
  PyArray_Descr *dtypes_in[2], PyArray_Descr *dtypes_out[2], NPY_CASTING *casting_out)
  -> int {0, -1}`` The out
  dtypes must be set correctly to dtypes which the strided loop
  (transfer function) can handle.  Initially the result must have instances
  of the same DType class as the ``CastingImpl`` is defined for. The
  ``casting`` will be set to ``NPY_EQUIV_CASTING``, ``NPY_SAFE_CASTING``,
  ``NPY_UNSAFE_CASTING``, or ``NPY_SAME_KIND_CASTING``.
  A new, additional flag,
  ``_NPY_CAST_IS_VIEW``, can be set to indicate that no cast is necessary and a
  view is sufficient to perform the cast. The cast should return
  ``-1`` when an error occurred. If a cast is not possible (but no error
  occurred), a ``-1`` result should be returned *without* an error set.
  *This point is under consideration, we may use ``-1`` to indicate
  a general error, and use a different return value for an impossible cast.*
  This means that it is *not* possible to inform the user about why a cast is
  impossible.

* ``strided_loop(char **args, npy_intp *dimensions, npy_intp *strides,
  ...) -> int {0, -1}`` (signature will be fully defined in :ref:`NEP 43 <NEP43>`)

This is identical to the proposed API for ufuncs. The additional ``...``
part of the signature will include information such as the two ``dtype``\s.
More optimized loops are in use internally, and
will be made available to users in the future (see notes).

Although verbose, the API will mimic the one for creating a new DType:

.. code-block:: C

    typedef struct{
      int flags;                  /* e.g. whether the cast requires the API */
      int nin, nout;              /* Number of Input and outputs (always 1) */
      NPY_CASTING casting;        /* The "minimal casting level" */
      PyArray_DTypeMeta *dtypes;  /* input and output DType class */
      /* NULL terminated slots defining the methods */
      PyType_Slot *slots;
    } PyArrayMethod_Spec;

The focus differs between casting and general ufuncs.  For example, for casts
``nin == nout == 1`` is always correct, while for ufuncs ``casting`` is
expected to be usually `"no"`.

**Notes:** We may initially allow users to define only a single loop.
Internally NumPy optimizes far more, and this should be made public
incrementally in one of two ways:

* Allow multiple versions, such as:

  * contiguous inner loop
  * strided inner loop
  * scalar inner loop

* Or, more likely, expose the ``get_loop`` function which is passed additional
  information, such as the fixed strides (similar to our internal API).

* The casting level denotes the minimal guaranteed casting level and can be
  ``-1`` if the cast may be impossible.  For most non-parametric casts, this
  value will be the casting level.  NumPy may skip the ``resolve_descriptors``
  call for ``np.can_cast()`` when the result is ``True`` based on this level.

The example does not yet include setup and error handling. Since these are
similar to the UFunc machinery, they  will be defined in :ref:`NEP 43 <NEP43>` and then
incorporated identically into casting.

The slots/methods used will be prefixed with ``NPY_meth_``.


**Alternatives:**

- Aside from name changes and signature tweaks, there seem to be few
  alternatives to the above structure. The proposed API using ``*_FromSpec``
  function is a good way to achieve a stable and extensible API. The slots
  design is extensible and can be changed without breaking binary
  compatibility. Convenience functions can still be provided to allow creation
  with less code.

- One downside is that compilers cannot warn about function-pointer
  incompatibilities.


******************************************************************************
Implementation
******************************************************************************

Steps for implementation are outlined in the Implementation section of
:ref:`NEP 41 <NEP41>`. In brief, we first will rewrite the internals of
casting and array coercion. After that, the new public API will be added
incrementally. We plan to expose it in a preliminary state initially to gain
experience. All functionality currently implemented on the dtypes will be
replaced systematically as new features are added.


******************************************************************************
Alternatives
******************************************************************************

The space of possible implementations is large, so there have been many
discussions, conceptions, and design documents. These are listed in
:ref:`NEP 40 <NEP40>`. Alternatives were also been discussed in the
relevant sections above.


******************************************************************************
References
******************************************************************************

.. [1] To be clear, the program is broken: It should not have stored a value
  in the common DType that was below the lowest int16 or above the highest
  uint16. It avoided overflow earlier by an accident of implementation.
  Nonetheless,  we insist that program behavior not be altered just by
  importing a type.

.. [2] NumPy currently inspects the value to allow the operations::

     np.array([1], dtype=np.uint8) + 1
     np.array([1.2], dtype=np.float32) + 1.

   to return a ``uint8`` or ``float32`` array respectively.  This is
   further described in the documentation for :func:`numpy.result_type`.


******************************************************************************
Copyright
******************************************************************************

This document has been placed in the public domain.
.. _NEP18:

====================================================================
NEP 18 — A dispatch mechanism for NumPy's high level array functions
====================================================================

:Author: Stephan Hoyer <shoyer@google.com>
:Author: Matthew Rocklin <mrocklin@gmail.com>
:Author: Marten van Kerkwijk <mhvk@astro.utoronto.ca>
:Author: Hameer Abbasi <hameerabbasi@yahoo.com>
:Author: Eric Wieser <wieser.eric@gmail.com>
:Status: Final
:Type: Standards Track
:Created: 2018-05-29
:Updated: 2019-05-25
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-August/078493.html

Abstract
--------

We propose the ``__array_function__`` protocol, to allow arguments of NumPy
functions to define how that function operates on them. This will allow
using NumPy as a high level API for efficient multi-dimensional array
operations, even with array implementations that differ greatly from
``numpy.ndarray``.

Detailed description
--------------------

NumPy's high level ndarray API has been implemented several times
outside of NumPy itself for different architectures, such as for GPU
arrays (CuPy), Sparse arrays (scipy.sparse, pydata/sparse) and parallel
arrays (Dask array) as well as various NumPy-like implementations in the
deep learning frameworks, like TensorFlow and PyTorch.

Similarly there are many projects that build on top of the NumPy API
for labeled and indexed arrays (XArray), automatic differentiation
(Autograd, Tangent), masked arrays (numpy.ma), physical units (astropy.units,
pint, unyt), etc. that add additional functionality on top of the NumPy API.
Most of these project also implement a close variation of NumPy's level high
API.

We would like to be able to use these libraries together, for example we
would like to be able to place a CuPy array within XArray, or perform
automatic differentiation on Dask array code. This would be easier to
accomplish if code written for NumPy ndarrays could also be used by
other NumPy-like projects.

For example, we would like for the following code example to work
equally well with any NumPy-like array object:

.. code:: python

    def f(x):
        y = np.tensordot(x, x.T)
        return np.mean(np.exp(y))

Some of this is possible today with various protocol mechanisms within
NumPy.

-  The ``np.exp`` function checks the ``__array_ufunc__`` protocol
-  The ``.T`` method works using Python's method dispatch
-  The ``np.mean`` function explicitly checks for a ``.mean`` method on
   the argument

However other functions, like ``np.tensordot`` do not dispatch, and
instead are likely to coerce to a NumPy array (using the ``__array__``)
protocol, or err outright. To achieve enough coverage of the NumPy API
to support downstream projects like XArray and autograd we want to
support *almost all* functions within NumPy, which calls for a more
reaching protocol than just ``__array_ufunc__``. We would like a
protocol that allows arguments of a NumPy function to take control and
divert execution to another function (for example a GPU or parallel
implementation) in a way that is safe and consistent across projects.

Implementation
--------------

We propose adding support for a new protocol in NumPy,
``__array_function__``.

This protocol is intended to be a catch-all for NumPy functionality that
is not covered by the ``__array_ufunc__`` protocol for universal functions
(like ``np.exp``). The semantics are very similar to ``__array_ufunc__``, except
the operation is specified by an arbitrary callable object rather than a ufunc
instance and method.

A prototype implementation can be found in
`this notebook <https://nbviewer.jupyter.org/gist/shoyer/1f0a308a06cd96df20879a1ddb8f0006>`_.

.. warning::

  The ``__array_function__`` protocol, and its use on particular functions,
  is *experimental*. We plan to retain an interface that makes it possible
  to override NumPy functions, but the way to do so for particular functions
  **can and will change** with little warning. If such reduced backwards
  compatibility guarantees are not accepted to you, do not rely upon overrides
  of NumPy functions for non-NumPy arrays. See "Non-goals" below for more
  details.

.. note::

  Dispatch with the ``__array_function__`` protocol has been implemented but is
  not yet enabled by default:

  - In NumPy 1.16, you need to set the environment variable
    ``NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1`` before importing NumPy to test
    NumPy function overrides.
  - In NumPy 1.17, the protocol will be enabled by default, but can be disabled
    with ``NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0``.
  - Eventually, expect to ``__array_function__`` to always be enabled.

The interface
~~~~~~~~~~~~~

We propose the following signature for implementations of
``__array_function__``:

.. code-block:: python

    def __array_function__(self, func, types, args, kwargs)

-  ``func`` is an arbitrary callable exposed by NumPy's public API,
   which was called in the form ``func(*args, **kwargs)``.
-  ``types`` is a `collection <https://docs.python.org/3/library/collections.abc.html#collections.abc.Collection>`_
   of unique argument types from the original NumPy function call that
   implement ``__array_function__``.
-  The tuple ``args`` and dict ``kwargs`` are directly passed on from the
   original call.

Unlike ``__array_ufunc__``, there are no high-level guarantees about the
type of ``func``, or about which of ``args`` and ``kwargs`` may contain objects
implementing the array API.

As a convenience for ``__array_function__`` implementors, ``types`` provides all
argument types with an ``'__array_function__'`` attribute. This
allows implementors to quickly identify cases where they should defer to
``__array_function__`` implementations on other arguments.
The type of ``types`` is intentionally vague:
``frozenset`` would most closely match intended use, but we may use ``tuple``
instead for performance reasons. In any case, ``__array_function__``
implementations should not rely on the iteration order of ``types``, which
would violate a well-defined "Type casting hierarchy" (as described in
`NEP-13 <https://www.numpy.org/neps/nep-0013-ufunc-overrides.html>`_).

Example for a project implementing the NumPy API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most implementations of ``__array_function__`` will start with two
checks:

1.  Is the given function something that we know how to overload?
2.  Are all arguments of a type that we know how to handle?

If these conditions hold, ``__array_function__`` should return
the result from calling its implementation for ``func(*args, **kwargs)``.
Otherwise, it should return the sentinel value ``NotImplemented``, indicating
that the function is not implemented by these types. This is preferable to
raising ``TypeError`` directly, because it gives *other* arguments the
opportunity to define the operations.

There are no general requirements on the return value from
``__array_function__``, although most sensible implementations should probably
return array(s) with the same type as one of the function's arguments.
If/when Python gains
`typing support for protocols <https://www.python.org/dev/peps/pep-0544/>`_
and NumPy adds static type annotations, the ``@overload`` implementation
for ``SupportsArrayFunction`` will indicate a return type of ``Any``.

It may also be convenient to define a custom decorators (``implements`` below)
for registering ``__array_function__`` implementations.

.. code:: python

    HANDLED_FUNCTIONS = {}

    class MyArray:
        def __array_function__(self, func, types, args, kwargs):
            if func not in HANDLED_FUNCTIONS:
                return NotImplemented
            # Note: this allows subclasses that don't override
            # __array_function__ to handle MyArray objects
            if not all(issubclass(t, MyArray) for t in types):
                return NotImplemented
            return HANDLED_FUNCTIONS[func](*args, **kwargs)

    def implements(numpy_function):
        """Register an __array_function__ implementation for MyArray objects."""
        def decorator(func):
            HANDLED_FUNCTIONS[numpy_function] = func
            return func
        return decorator

    @implements(np.concatenate)
    def concatenate(arrays, axis=0, out=None):
        ...  # implementation of concatenate for MyArray objects

    @implements(np.broadcast_to)
    def broadcast_to(array, shape):
        ...  # implementation of broadcast_to for MyArray objects

Note that it is not required for ``__array_function__`` implementations to
include *all* of the corresponding NumPy function's optional arguments
(e.g., ``broadcast_to`` above omits the irrelevant ``subok`` argument).
Optional arguments are only passed in to ``__array_function__`` if they
were explicitly used in the NumPy function call.

.. note::

    Just like the case for builtin special methods like ``__add__``, properly
    written ``__array_function__`` methods should always return
    ``NotImplemented`` when an unknown type is encountered. Otherwise, it will
    be impossible to correctly override NumPy functions from another object
    if the operation also includes one of your objects.

Necessary changes within the NumPy codebase itself
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This will require two changes within the NumPy codebase:

1. A function to inspect available inputs, look for the
   ``__array_function__`` attribute on those inputs, and call those
   methods appropriately until one succeeds.  This needs to be fast in the
   common all-NumPy case, and have acceptable performance (no worse than
   linear time) even if the number of overloaded inputs is large (e.g.,
   as might be the case for `np.concatenate`).

   This is one additional function of moderate complexity.
2. Calling this function within all relevant NumPy functions.

   This affects many parts of the NumPy codebase, although with very low
   complexity.

Finding and calling the right ``__array_function__``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Given a NumPy function, ``*args`` and ``**kwargs`` inputs, we need to
search through ``*args`` and ``**kwargs`` for all appropriate inputs
that might have the ``__array_function__`` attribute. Then we need to
select among those possible methods and execute the right one.
Negotiating between several possible implementations can be complex.

Finding arguments
'''''''''''''''''

Valid arguments may be directly in the ``*args`` and ``**kwargs``, such
as in the case for ``np.tensordot(left, right, out=out)``, or they may
be nested within lists or dictionaries, such as in the case of
``np.concatenate([x, y, z])``. This can be problematic for two reasons:

1. Some functions are given long lists of values, and traversing them
   might be prohibitively expensive.
2. Some functions may have arguments that we don't want to inspect, even
   if they have the ``__array_function__`` method.

To resolve these issues, NumPy functions should explicitly indicate which
of their arguments may be overloaded, and how these arguments should be
checked. As a rule, this should include all arguments documented as either
``array_like`` or ``ndarray``.

We propose to do so by writing "dispatcher" functions for each overloaded
NumPy function:

- These functions will be called with the exact same arguments that were passed
  into the NumPy function (i.e., ``dispatcher(*args, **kwargs)``), and should
  return an iterable of arguments to check for overrides.
- Dispatcher functions are required to share the exact same positional,
  optional and keyword-only arguments as their corresponding NumPy functions.
  Otherwise, valid invocations of a NumPy function could result in an error when
  calling its dispatcher.
- Because default *values* for keyword arguments do not have
  ``__array_function__`` attributes, by convention we set all default argument
  values to ``None``. This reduces the likelihood of signatures falling out
  of sync, and minimizes extraneous information in the dispatcher.
  The only exception should be cases where the argument value in some way
  effects dispatching, which should be rare.

An example of the dispatcher for ``np.concatenate`` may be instructive:

.. code:: python

    def _concatenate_dispatcher(arrays, axis=None, out=None):
        for array in arrays:
            yield array
        if out is not None:
            yield out

The concatenate dispatcher is written as generator function, which allows it
to potentially include the value of the optional ``out`` argument without
needing to create a new sequence with the (potentially long) list of objects
to be concatenated.

Trying ``__array_function__`` methods until the right one works
'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''

Many arguments may implement the ``__array_function__`` protocol. Some
of these may decide that, given the available inputs, they are unable to
determine the correct result. How do we call the right one? If several
are valid then which has precedence?

For the most part, the rules for dispatch with ``__array_function__``
match those for ``__array_ufunc__`` (see
`NEP-13 <https://www.numpy.org/neps/nep-0013-ufunc-overrides.html>`_).
In particular:

-  NumPy will gather implementations of ``__array_function__`` from all
   specified inputs and call them in order: subclasses before
   superclasses, and otherwise left to right. Note that in some edge cases
   involving subclasses, this differs slightly from the
   `current behavior <https://bugs.python.org/issue30140>`_ of Python.
-  Implementations of ``__array_function__`` indicate that they can
   handle the operation by returning any value other than
   ``NotImplemented``.
-  If all ``__array_function__`` methods return ``NotImplemented``,
   NumPy will raise ``TypeError``.

If no ``__array_function__`` methods exist, NumPy will default to calling its
own implementation, intended for use on NumPy arrays. This case arises, for
example, when all array-like arguments are Python numbers or lists.
(NumPy arrays do have a ``__array_function__`` method, given below, but it
always returns ``NotImplemented`` if any argument other than a NumPy array
subclass implements ``__array_function__``.)

One deviation from the current behavior of ``__array_ufunc__`` is that NumPy
will only call ``__array_function__`` on the *first* argument of each unique
type. This matches Python's
`rule for calling reflected methods <https://docs.python.org/3/reference/datamodel.html#object.__ror__>`_,
and this ensures that checking overloads has acceptable performance even when
there are a large number of overloaded arguments. To avoid long-term divergence
between these two dispatch protocols, we should
`also update <https://github.com/numpy/numpy/issues/11306>`_
``__array_ufunc__`` to match this behavior.

The ``__array_function__`` method on ``numpy.ndarray``
''''''''''''''''''''''''''''''''''''''''''''''''''''''

The use cases for subclasses with ``__array_function__`` are the same as those
with ``__array_ufunc__``, so ``numpy.ndarray`` also defines a
``__array_function__`` method:

.. code:: python

    def __array_function__(self, func, types, args, kwargs):
        if not all(issubclass(t, ndarray) for t in types):
            # Defer to any non-subclasses that implement __array_function__
            return NotImplemented

        # Use NumPy's private implementation without __array_function__
        # dispatching
        return func._implementation(*args, **kwargs)

This method matches NumPy's dispatching rules, so for most part it is
possible to pretend that ``ndarray.__array_function__`` does not exist.
The private ``_implementation`` attribute, defined below in the
``array_function_dispatch`` decorator, allows us to avoid the special cases for
NumPy arrays that were needed in the ``__array_ufunc__`` protocol.

The ``__array_function__`` protocol always calls subclasses before
superclasses, so if any ``ndarray`` subclasses are involved in an operation,
they will get the chance to override it, just as if any other argument
overrides ``__array_function__``. But the default behavior in an operation
that combines a base NumPy array and a subclass is different: if the subclass
returns ``NotImplemented``, NumPy's implementation of the function will be
called instead of raising an exception. This is appropriate since subclasses
are `expected to be substitutable <https://en.wikipedia.org/wiki/Liskov_substitution_principle>`_.

We still caution authors of subclasses to exercise caution when relying
upon details of NumPy's internal implementations. It is not always possible to
write a perfectly substitutable ndarray subclass, e.g., in cases involving the
creation of new arrays, not least because NumPy makes use of internal
optimizations specialized to base NumPy arrays, e.g., code written in C. Even
if NumPy's implementation happens to work today, it may not work in the future.
In these cases, your recourse is to re-implement top-level NumPy functions via
``__array_function__`` on your subclass.

Changes within NumPy functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Given a function defining the above behavior, for now call it
``implement_array_function``, we now need to call that
function from within every relevant NumPy function. This is a pervasive change,
but of fairly simple and innocuous code that should complete quickly and
without effect if no arguments implement the ``__array_function__``
protocol.

To achieve this, we define a ``array_function_dispatch`` decorator to rewrite
NumPy functions. The basic implementation is as follows:

.. code:: python

    def array_function_dispatch(dispatcher, module=None):
        """Wrap a function for dispatch with the __array_function__ protocol."""
        def decorator(implementation):
            @functools.wraps(implementation)
            def public_api(*args, **kwargs):
                relevant_args = dispatcher(*args, **kwargs)
                return implement_array_function(
                    implementation, public_api, relevant_args, args, kwargs)
            if module is not None:
                public_api.__module__ = module
            # for ndarray.__array_function__
            public_api._implementation = implementation
            return public_api
        return decorator

    # example usage
    def _broadcast_to_dispatcher(array, shape, subok=None):
        return (array,)

    @array_function_dispatch(_broadcast_to_dispatcher, module='numpy')
    def broadcast_to(array, shape, subok=False):
        ...  # existing definition of np.broadcast_to

Using a decorator is great! We don't need to change the definitions of
existing NumPy functions, and only need to write a few additional lines
for the dispatcher function. We could even reuse a single dispatcher for
families of functions with the same signature (e.g., ``sum`` and ``prod``).
For such functions, the largest change could be adding a few lines to the
docstring to note which arguments are checked for overloads.

It's particularly worth calling out the decorator's use of
``functools.wraps``:

- This ensures that the wrapped function has the same name and docstring as
  the wrapped NumPy function.
- On Python 3, it also ensures that the decorator function copies the original
  function signature, which is important for introspection based tools such as
  auto-complete.
- Finally, it ensures that the wrapped function
  `can be pickled <http://gael-varoquaux.info/programming/decoration-in-python-done-right-decorating-and-pickling.html>`_.

The example usage illustrates several best practices for writing dispatchers
relevant to NumPy contributors:

- We passed the ``module`` argument, which in turn sets the  ``__module__``
  attribute on the generated function. This is for the benefit of better error
  messages, here for errors raised internally by NumPy when no implementation
  is found, e.g.,
  ``TypeError: no implementation found for 'numpy.broadcast_to'``. Setting
  ``__module__`` to the canonical location in NumPy's public API encourages
  users to use NumPy's public API for identifying functions in
  ``__array_function__``.

- The dispatcher is a function that returns a tuple, rather than an equivalent
  (and equally valid) generator using ``yield``:

  .. code:: python

    # example usage
    def broadcast_to(array, shape, subok=None):
        yield array

  This is no accident: NumPy's implementation of dispatch for
  ``__array_function__`` is fastest when dispatcher functions return a builtin
  sequence type (``tuple`` or ``list``).

  On a related note, it's perfectly fine for dispatchers to return arguments
  even if in some cases you *know* that they cannot have an
  ``__array_function__`` method. This can arise for functions with default
  arguments (e.g., ``None``) or complex signatures. NumPy's dispatching logic
  sorts out these cases very quickly, so it generally is not worth the trouble
  of parsing them on your own.

.. note::

    The code for ``array_function_dispatch`` above has been updated from the
    original version of this NEP to match the actual
    `implementation in NumPy <https://github.com/numpy/numpy/blob/e104f03ac8f65ae5b92a9b413b0fa639f39e6de2/numpy/core/overrides.py>`_.

Extensibility
~~~~~~~~~~~~~

An important virtue of this approach is that it allows for adding new
optional arguments to NumPy functions without breaking code that already
relies on ``__array_function__``.

This is not a theoretical concern. NumPy's older, haphazard implementation of
overrides *within* functions like ``np.sum()`` necessitated some awkward
gymnastics when we decided to add new optional arguments, e.g., the new
``keepdims`` argument is only passed in cases where it is used:

.. code:: python

    def sum(array, ..., keepdims=np._NoValue):
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        return array.sum(..., **kwargs)

For ``__array_function__`` implementors, this also means that it is possible
to implement even existing optional arguments incrementally, and only in cases
where it makes sense. For example, a library implementing immutable arrays
would not be required to explicitly include an unsupported ``out`` argument in
the function signature. This can be somewhat onerous to implement properly,
e.g.,

.. code:: python

    def my_sum(array, ..., out=None):
        if out is not None:
            raise TypeError('out argument is not supported')
        ...

We thus avoid encouraging the tempting shortcut of adding catch-all
``**ignored_kwargs`` to the signatures of functions called by NumPy, which fails
silently for misspelled or ignored arguments.

Performance
~~~~~~~~~~~

Performance is always a concern with NumPy, even though NumPy users have
already prioritized usability over pure speed with their choice of the Python
language itself. It's important that this new ``__array_function__`` protocol
not impose a significant cost in the typical case of NumPy functions acting
on NumPy arrays.

Our `microbenchmark results <https://nbviewer.jupyter.org/gist/shoyer/1f0a308a06cd96df20879a1ddb8f0006>`_
show that a pure Python implementation of the override machinery described
above adds roughly 2-3 microseconds of overhead to each NumPy function call
without any overloaded arguments. For context, typical NumPy functions on small
arrays have a runtime of 1-10 microseconds, mostly determined by what fraction
of the function's logic is written in C. For example, one microsecond is about
the difference in speed between the ``ndarray.sum()`` method (1.6 us) and
``numpy.sum()`` function (2.6 us).

Fortunately, we expect significantly less overhead with a C implementation of
``implement_array_function``, which is where the bulk of the
runtime is. This would leave the ``array_function_dispatch`` decorator and
dispatcher function on their own adding about 0.5 microseconds of overhead,
for perhaps ~1 microsecond of overhead in the typical case.

In our view, this level of overhead is reasonable to accept for code written
in Python. We're pretty sure that the vast majority of NumPy users aren't
concerned about performance differences measured in microsecond(s) on NumPy
functions, because it's difficult to do *anything* in Python in less than a
microsecond.

Use outside of NumPy
~~~~~~~~~~~~~~~~~~~~

Nothing about this protocol that is particular to NumPy itself. Should
we encourage use of the same ``__array_function__`` protocol third-party
libraries for overloading non-NumPy functions, e.g., for making
array-implementation generic functionality in SciPy?

This would offer significant advantages (SciPy wouldn't need to invent
its own dispatch system) and no downsides that we can think of, because
every function that dispatches with ``__array_function__`` already needs
to be explicitly recognized. Libraries like Dask, CuPy, and Autograd
already wrap a limited subset of SciPy functionality (e.g.,
``scipy.linalg``) similarly to how they wrap NumPy.

If we want to do this, we should expose at least the decorator
``array_function_dispatch()`` and possibly also the lower level
``implement_array_function()`` as part of NumPy's public API.

Non-goals
---------

We are aiming for basic strategy that can be relatively mechanistically
applied to almost all functions in NumPy's API in a relatively short
period of time, the development cycle of a single NumPy release.

We hope to get both the ``__array_function__`` protocol and all specific
overloads right on the first try, but our explicit aim here is to get
something that mostly works (and can be iterated upon), rather than to
wait for an optimal implementation. The price of moving fast is that for
now **this protocol should be considered strictly experimental**. We
reserve the right to change the details of this protocol and how
specific NumPy functions use it at any time in the future -- even in
otherwise bug-fix only releases of NumPy. In practice, once initial
issues with ``__array_function__`` are worked out, we will use abbreviated
deprecation cycles as short as a single major NumPy release (e.g., as
little as four months).

In particular, we don't plan to write additional NEPs that list all
specific functions to overload, with exactly how they should be
overloaded. We will leave this up to the discretion of committers on
individual pull requests, trusting that they will surface any
controversies for discussion by interested parties.

However, we already know several families of functions that should be
explicitly exclude from ``__array_function__``. These will need their
own protocols:

-  universal functions, which already have their own protocol.
-  ``array`` and ``asarray``, because they are explicitly intended for
   coercion to actual ``numpy.ndarray`` object.
-  dispatch for methods of any kind, e.g., methods on
   ``np.random.RandomState`` objects.

We also expect that the mechanism for overriding specific functions
that will initially use the ``__array_function__`` protocol can and will
change in the future. As a concrete example of how we expect to break
behavior in the future, some functions such as ``np.where`` are currently
not NumPy universal functions, but conceivably could become universal
functions in the future. When/if this happens, we will change such overloads
from using ``__array_function__`` to the more specialized ``__array_ufunc__``.


Backward compatibility
----------------------

This proposal does not change existing semantics, except for those arguments
that currently have ``__array_function__`` attributes, which should be rare.


Alternatives
------------

Specialized protocols
~~~~~~~~~~~~~~~~~~~~~

We could (and should) continue to develop protocols like
``__array_ufunc__`` for cohesive subsets of NumPy functionality.

As mentioned above, if this means that some functions that we overload
with ``__array_function__`` should switch to a new protocol instead,
that is explicitly OK for as long as ``__array_function__`` retains its
experimental status.

Switching to a new protocol should use an abbreviated version of NumPy's
normal deprecation cycle:

- For a single major release, after checking for any new protocols, NumPy
  should still check for ``__array_function__`` methods that implement the
  given function. If any argument returns a value other than
  ``NotImplemented`` from ``__array_function__``, a descriptive
  ``FutureWarning`` should be issued.
- In the next major release, the checks for ``__array_function__`` will be
  removed.

Separate namespace
~~~~~~~~~~~~~~~~~~

A separate namespace for overloaded functions is another possibility,
either inside or outside of NumPy.

This has the advantage of alleviating any possible concerns about
backwards compatibility and would provide the maximum freedom for quick
experimentation. In the long term, it would provide a clean abstraction
layer, separating NumPy's high level API from default implementations on
``numpy.ndarray`` objects.

The downsides are that this would require an explicit opt-in from all
existing code, e.g., ``import numpy.api as np``, and in the long term
would result in the maintenance of two separate NumPy APIs. Also, many
functions from ``numpy`` itself are already overloaded (but
inadequately), so confusion about high vs. low level APIs in NumPy would
still persist.

Alternatively, a separate namespace, e.g., ``numpy.array_only``, could be
created for a non-overloaded version of NumPy's high level API, for cases
where performance with NumPy arrays is a critical concern. This has most
of the same downsides as the separate namespace.

Multiple dispatch
~~~~~~~~~~~~~~~~~

An alternative to our suggestion of the ``__array_function__`` protocol
would be implementing NumPy's core functions as
`multi-methods <https://en.wikipedia.org/wiki/Multiple_dispatch>`_.
Although one of us wrote a `multiple dispatch
library <https://github.com/mrocklin/multipledispatch>`_ for Python, we
don't think this approach makes sense for NumPy in the near term.

The main reason is that NumPy already has a well-proven dispatching
mechanism with ``__array_ufunc__``, based on Python's own dispatching
system for arithmetic, and it would be confusing to add another
mechanism that works in a very different way. This would also be more
invasive change to NumPy itself, which would need to gain a multiple
dispatch implementation.

It is possible that multiple dispatch implementation for NumPy's high
level API could make sense in the future. Fortunately,
``__array_function__`` does not preclude this possibility, because it
would be straightforward to write a shim for a default
``__array_function__`` implementation in terms of multiple dispatch.

Implementations in terms of a limited core API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The internal implementation of some NumPy functions is extremely simple.
For example:

- ``np.stack()`` is implemented in only a few lines of code by combining
  indexing with ``np.newaxis``, ``np.concatenate`` and the ``shape`` attribute.
- ``np.mean()`` is implemented internally in terms of ``np.sum()``,
  ``np.divide()``, ``.astype()`` and ``.shape``.

This suggests the possibility of defining a minimal "core" ndarray
interface, and relying upon it internally in NumPy to implement the full
API. This is an attractive option, because it could significantly reduce
the work required for new array implementations.

However, this also comes with several downsides:

1. The details of how NumPy implements a high-level function in terms of
   overloaded functions now becomes an implicit part of NumPy's public API. For
   example, refactoring ``stack`` to use ``np.block()`` instead of
   ``np.concatenate()`` internally would now become a breaking change.
2. Array libraries may prefer to implement high level functions differently than
   NumPy. For example, a library might prefer to implement a fundamental
   operations like ``mean()`` directly rather than relying on ``sum()`` followed
   by division. More generally, it's not clear yet what exactly qualifies as
   core functionality, and figuring this out could be a large project.
3. We don't yet have an overloading system for attributes and methods on array
   objects, e.g., for accessing ``.dtype`` and ``.shape``. This should be the
   subject of a future NEP, but until then we should be reluctant to rely on
   these properties.

Given these concerns, we think it's valuable to support explicit overloading of
nearly every public function in NumPy's API. This does not preclude the future
possibility of rewriting NumPy functions in terms of simplified core
functionality with ``__array_function__`` and a protocol and/or base class for
ensuring that arrays expose methods and properties like ``numpy.ndarray``.
However, to work well this would require the possibility of implementing
*some* but not all functions with ``__array_function__``, e.g., as described
in the next section.

Partial implementation of NumPy's API
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With the current design, classes that implement ``__array_function__``
to overload at least one function implicitly declare an intent to
implement the entire NumPy API. It's not possible to implement *only*
``np.concatenate()`` on a type, but fall back to NumPy's default
behavior of casting with ``np.asarray()`` for all other functions.

This could present a backwards compatibility concern that would
discourage libraries from adopting ``__array_function__`` in an
incremental fashion. For example, currently most numpy functions will
implicitly convert ``pandas.Series`` objects into NumPy arrays, behavior
that assuredly many pandas users rely on. If pandas implemented
``__array_function__`` only for ``np.concatenate``, unrelated NumPy
functions like ``np.nanmean`` would suddenly break on pandas objects by
raising TypeError.

Even libraries that reimplement most of NumPy's public API sometimes rely upon
using utility functions from NumPy without a wrapper. For example, both CuPy
and JAX simply `use an alias <https://github.com/numpy/numpy/issues/12974>`_ to
``np.result_type``, which already supports duck-types with a ``dtype``
attribute.

With ``__array_ufunc__``, it's possible to alleviate this concern by
casting all arguments to numpy arrays and re-calling the ufunc, but the
heterogeneous function signatures supported by ``__array_function__``
make it impossible to implement this generic fallback behavior for
``__array_function__``.

We considered three possible ways to resolve this issue, but none were
entirely satisfactory:

1. Change the meaning of all arguments returning ``NotImplemented`` from
   ``__array_function__`` to indicate that all arguments should be coerced to
   NumPy arrays and the operation should be retried. However, many array
   libraries (e.g., scipy.sparse) really don't want implicit conversions to
   NumPy arrays, and often avoid implementing ``__array__`` for exactly this
   reason. Implicit conversions can result in silent bugs and performance
   degradation.

   Potentially, we could enable this behavior only for types that implement
   ``__array__``, which would resolve the most problematic cases like
   scipy.sparse. But in practice, a large fraction of classes that present a
   high level API like NumPy arrays already implement ``__array__``. This would
   preclude reliable use of NumPy's high level API on these objects.

2. Use another sentinel value of some sort, e.g.,
   ``np.NotImplementedButCoercible``, to indicate that a class implementing
   part of NumPy's higher level array API is coercible as a fallback. If all
   arguments return ``NotImplementedButCoercible``, arguments would be coerced
   and the operation would be retried.

   Unfortunately, correct behavior after encountering
   ``NotImplementedButCoercible`` is not always obvious. Particularly
   challenging is the "mixed" case where some arguments return
   ``NotImplementedButCoercible`` and others return ``NotImplemented``.
   Would dispatching be retried after only coercing the "coercible" arguments?
   If so, then conceivably we could end up looping through the dispatching
   logic an arbitrary number of times. Either way, the dispatching rules would
   definitely get more complex and harder to reason about.

3. Allow access to NumPy's implementation of functions, e.g., in the form of
   a publicly exposed ``__skip_array_function__`` attribute on the NumPy
   functions. This would allow for falling back to NumPy's implementation by
   using ``func.__skip_array_function__`` inside ``__array_function__``
   methods, and could also potentially be used to be used to avoid the
   overhead of dispatching. However, it runs the risk of potentially exposing
   details of NumPy's implementations for NumPy functions that do not call
   ``np.asarray()`` internally. See
   `this note <https://mail.python.org/pipermail/numpy-discussion/2019-May/079541.html>`_
   for a summary of the full discussion.

These solutions would solve real use cases, but at the cost of additional
complexity. We would like to gain experience with how ``__array_function__`` is
actually used before making decisions that would be difficult to roll back.

A magic decorator that inspects type annotations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In principle, Python 3 type annotations contain sufficient information to
automatically create most ``dispatcher`` functions. It would be convenient to
use these annotations to dispense with the need for manually writing
dispatchers, e.g.,

.. code:: python

    @array_function_dispatch
    def broadcast_to(array: ArrayLike
                     shape: Tuple[int, ...],
                     subok: bool = False):
        ...  # existing definition of np.broadcast_to

This would require some form of automatic code generation, either at compile or
import time.

We think this is an interesting possible extension to consider in the future. We
don't think it makes sense to do so now, because code generation involves
tradeoffs and NumPy's experience with type annotations is still
`quite limited <https://github.com/numpy/numpy-stubs>`_. Even if NumPy
was Python 3 only (which will happen
`sometime in 2019 <http://www.numpy.org/neps/nep-0014-dropping-python2.7-proposal.html>`_),
we aren't ready to annotate NumPy's codebase directly yet.

Support for implementation-specific arguments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We could allow ``__array_function__`` implementations to add their own
optional keyword arguments by including ``**ignored_kwargs`` in dispatcher
functions, e.g.,

.. code:: python

    def _concatenate_dispatcher(arrays, axis=None, out=None, **ignored_kwargs):
        ...  # same implementation of _concatenate_dispatcher as above

Implementation-specific arguments are somewhat common in libraries that
otherwise emulate NumPy's higher level API (e.g., ``dask.array.sum()`` adds
``split_every`` and ``tensorflow.reduce_sum()`` adds ``name``). Supporting
them in NumPy would be particularly useful for libraries that implement new
high-level array functions on top of NumPy functions, e.g.,

.. code:: python

    def mean_squared_error(x, y, **kwargs):
        return np.mean((x - y) ** 2, **kwargs)

Otherwise, we would need separate versions of ``mean_squared_error`` for each
array implementation in order to pass implementation-specific arguments to
``mean()``.

We wouldn't allow adding optional positional arguments, because these are
reserved for future use by NumPy itself, but conflicts between keyword arguments
should be relatively rare.

However, this flexibility would come with a cost. In particular, it implicitly
adds ``**kwargs`` to the signature for all wrapped NumPy functions without
actually including it (because we use ``functools.wraps``). This means it is
unlikely to work well with static analysis tools, which could report invalid
arguments. Likewise, there is a price in readability: these optional arguments
won't be included in the docstrings for NumPy functions.

It's not clear that this tradeoff is worth it, so we propose to leave this out
for now. Adding implementation-specific arguments will require using those
libraries directly.

Other possible choices for the protocol
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The array function ``__array_function__`` includes only two arguments, ``func``
and ``types``, that provide information about the context of the function call.

``func`` is part of the protocol because there is no way to avoid it:
implementations need to be able to dispatch by matching a function to NumPy's
public API.

``types`` is included because we can compute it almost for free as part of
collecting ``__array_function__`` implementations to call in
``implement_array_function``. We also think it will be used
by many ``__array_function__`` methods, which otherwise would need to extract
this information themselves. It would be equivalently easy to provide single
instances of each type, but providing only types seemed cleaner.

Taking this even further, it was suggested that ``__array_function__`` should be
a ``classmethod``. We agree that it would be a little cleaner to remove the
redundant ``self`` argument, but feel that this minor clean-up would not be
worth breaking from the precedence of ``__array_ufunc__``.

There are two other arguments that we think *might* be important to pass to
``__array_ufunc__`` implementations:

- Access to the non-dispatched implementation (i.e., before wrapping with
  ``array_function_dispatch``) in ``ndarray.__array_function__`` would allow
  us to drop special case logic for that method from
  ``implement_array_function``.
- Access to the ``dispatcher`` function passed into
  ``array_function_dispatch()`` would allow ``__array_function__``
  implementations to determine the list of "array-like" arguments in a generic
  way by calling ``dispatcher(*args, **kwargs)``. This *could* be useful for
  ``__array_function__`` implementations that dispatch based on the value of an
  array attribute (e.g., ``dtype`` or ``units``) rather than directly on the
  array type.

We have left these out for now, because we don't know that they are necessary.
If we want to include them in the future, the easiest way to do so would be to
update the ``array_function_dispatch`` decorator to add them as function
attributes.

Callable objects generated at runtime
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NumPy has some APIs that define callable objects *dynamically*, such as
``vectorize`` and methods on ``random.RandomState`` object. Examples can
also be found in other core libraries in the scientific Python stack, e.g.,
distribution objects in scipy.stats and model objects in scikit-learn. It would
be nice to be able to write overloads for such callables, too. This presents a
challenge for the ``__array_function__`` protocol, because unlike the case for
functions there is no public object in the ``numpy`` namespace to pass into
the ``func`` argument.

We could potentially handle this by establishing an alternative convention
for how the ``func`` argument could be inspected, e.g., by using
``func.__self__`` to obtain the class object and ``func.__func__`` to return
the unbound function object. However, some caution is in order, because
this would immesh what are currently implementation details as a permanent
features of the interface, such as the fact that ``vectorize`` is implemented as a
class rather than closure, or whether a method is implemented directly or using
a descriptor.

Given the complexity and the limited use cases, we are also deferring on this
issue for now, but we are confident that ``__array_function__`` could be
expanded to accommodate these use cases in the future if need be.

Discussion
----------

Various alternatives to this proposal were discussed in a few GitHub issues:

1. `pydata/sparse #1 <https://github.com/pydata/sparse/issues/1>`_
2. `numpy/numpy #11129 <https://github.com/numpy/numpy/issues/11129>`_

Additionally it was the subject of `a blogpost
<http://matthewrocklin.com/blog/work/2018/05/27/beyond-numpy>`_. Following this
it was discussed at a `NumPy developer sprint
<https://scisprints.github.io/#may-numpy-developer-sprint>`_ at the `UC
Berkeley Institute for Data Science (BIDS) <https://bids.berkeley.edu/>`_.

Detailed discussion of this proposal itself can be found on the
`the mailing list <https://mail.python.org/pipermail/numpy-discussion/2018-June/078127.html>`_ and relevant pull requests
(`1 <https://github.com/numpy/numpy/pull/11189>`_,
`2 <https://github.com/numpy/numpy/pull/11303#issuecomment-396638175>`_,
`3 <https://github.com/numpy/numpy/pull/11374>`_)

Copyright
---------

This document has been placed in the public domain.
.. _NEP07:

==================================================================
NEP 7 — A proposal for implementing some date/time types in NumPy
==================================================================

:Author: Travis Oliphant
:Contact: oliphant@enthought.com
:Date: 2009-06-09
:Status: Final

Revised only slightly from the third proposal by

:Author: Francesc Alted i Abad
:Contact: faltet@pytables.com
:Author: Ivan Vilata i Balaguer
:Contact: ivan@selidor.net
:Date: 2008-07-30

Executive summary
=================

A date/time mark is something very handy to have in many fields where
one has to deal with data sets.  While Python has several modules that
define a date/time type (like the integrated ``datetime`` [1]_ or
``mx.DateTime`` [2]_), NumPy has a lack of them.

We are proposing the addition of date/time types to fill this gap.
The requirements for the proposed types are two-fold: 1) they have
to be fast to operate with and 2) they have to be as compatible as
possible with the existing ``datetime`` module that comes with Python.


Types proposed
==============

It is virtually impossible to come up with a single date/time type
that fills the needs of every use case.  As a result, we propose two
general date-time types: 1) ``timedelta64`` -- a relative time and 2)
``datetime64`` -- an absolute time.

Each of these times are represented internally as 64-bit signed
integers that refer to a particular unit (hour, minute, microsecond,
etc.).  There are several pre-defined units as well as the ability to
create rational multiples of these units.  A representation is also
supported such that the stored date-time integer can encode both the
number of a particular unit as well as a number of sequential events
tracked for each unit.

The ``datetime64`` represents an absolute time.  Internally it is
represented as the number of time units between the intended time and
the epoch (12:00am on January 1, 1970 --- POSIX time including its
lack of leap seconds).

.. Important:  The information that provides meaning to the integers stored in
   the date/time dtypes are stored as metadata which is a new feature to be
   added to the dtype object.

Time units
===========

The 64-bit integer time can represent several different basic units as
well as derived units.  The basic units are listed in the following
table:

======== ================ ======================= ==========================
      Time unit               Time span              Time span (years)
------------------------- ----------------------- --------------------------
  Code       Meaning         Relative Time             Absolute Time
======== ================ ======================= ==========================
   Y       year             +- 9.2e18 years         [9.2e18 BC, 9.2e18 AD]
   M       month            +- 7.6e17 years         [7.6e17 BC, 7.6e17 AD]
   W       week             +- 1.7e17 years         [1.7e17 BC, 1.7e17 AD]
   B       business day     +- 3.5e16 years         [3.5e16 BC, 3.5e16 AD]
   D       day              +- 2.5e16 years         [2.5e16 BC, 2.5e16 AD]
   h       hour             +- 1.0e15 years         [1.0e15 BC, 1.0e15 AD]
   m       minute           +- 1.7e13 years         [1.7e13 BC, 1.7e13 AD]
   s       second           +- 2.9e12 years         [ 2.9e9 BC,  2.9e9 AD]
   ms      millisecond      +- 2.9e9 years          [ 2.9e6 BC,  2.9e6 AD]
   us      microsecond      +- 2.9e6 years          [290301 BC, 294241 AD]
   ns      nanosecond       +- 292 years            [  1678 AD,   2262 AD]
   ps      picosecond       +- 106 days             [  1969 AD,   1970 AD]
   fs      femtosecond      +- 2.6 hours            [  1969 AD,   1970 AD]
   as      attosecond       +- 9.2 seconds          [  1969 AD,   1970 AD]
======== ================ ======================= ==========================

A time unit is specified by a string consisting of a base-type given in
the above table

Besides these basic code units, the user can create derived units
consisting of multiples of any basic unit: 100ns, 3M, 15m, etc.

A limited number of divisions of any basic unit can be used to create
multiples of a higher-resolution unit provided the divisor can be
divided evenly into the number of higher-resolution units available.
For example: Y/4 is just short-hand for -> (12M)/4 -> 3M and Y/4 will be
represented after creation as 3M.  The first lower unit found to have an
even divisor will be chosen (up to 3 lower units).  The following
standardized definitions are used in this specific case to find
acceptable divisors

====== ====================
 Code    Interpreted as
====== ====================
Y      12M, 52W, 365D
M      4W, 30D, 720h
W      5B, 7D, 168h, 10080m
B      24h, 1440m, 86400s
D      24h, 1440m, 86400s
h      60m, 3600s
m      60s, 60000ms
====== ====================

s, ms, us, ns, ps, fs (use 1000 and 1000000 of the next two available
lower units respectively).

Finally, a date-time data-type can be created with support for tracking
sequential events within a basic unit: [D]//100, [Y]//4 (notice the
required brackets).  These ``modulo`` event units provide the following
interpretation to the date-time integer:

   * the divisor is the number of events in each period
   * the (integer) quotient is the integer number representing the base units
   * the remainder is the particular event in the period.

Modulo event-units can be combined with any derived units, but brackets
are required.  Thus [100ns]//50 which allows recording 50 events for
every 100ns so that 0 represents the first event in the first 100ns
tick, 1 represents the second event in the first 100ns tick, while 50
represents the first event in the second 100ns tick, and 51 represents
the second event in the second 100ns tick.

To fully specify a date-time type, the time unit string must be
combined with either the string for a datetime64 ('M8') or a
timedelta64 ('m8') using brackets '[]'.  Therefore, a fully-specified
string representing a date-time dtype is 'M8[Y]' or (for a more
complicated example) 'M8[7s/9]//5'.

If a time unit is not specified, then it defaults to [us].  Thus 'M8' is
equivalent to 'M8[us]' (except when modulo event-units are desired --
i.e. you cannot specify 'M8[us]//5' as 'M8//5' or as '//5'

``datetime64``
==============

This dtype represents a time that is absolute (i.e. not relative).  It
is implemented internally as an ``int64`` type.  The integer represents
units from the internal POSIX epoch (see [3]_). Like POSIX, the
representation of a date doesn't take leap seconds into account.

In time unit *conversions* and time *representations* (but not in other
time computations), the value -2**63 (0x8000000000000000) is interpreted
as an invalid or unknown date, *Not a Time* or *NaT*.  See the section
on time unit conversions for more information.

The value of an absolute date is thus *an integer number of units of
the chosen time unit* passed since the epoch.  If the integer is a
negative number, then the magnitude of the integer represents the
number of units prior to the epoch.  When working with business days,
Saturdays and Sundays are simply ignored from the count (i.e. day 3 in
business days is not Saturday 1970-01-03, but Monday 1970-01-05).

Building a ``datetime64`` dtype
--------------------------------

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation::

  dtype('datetime64[us]')

Using the short string notation::

  dtype('M8[us]')

If a time unit is not specified, then it defaults to [us].  Thus 'M8'
is equivalent to 'M8[us]'.


Setting and getting values
---------------------------

The objects with this dtype can be set in a series of ways::

  t = numpy.ones(3, dtype='M8[s]')
  t[0] = 1199164176    # assign to July 30th, 2008 at 17:31:00
  t[1] = datetime.datetime(2008, 7, 30, 17, 31, 01) # with datetime module
  t[2] = '2008-07-30T17:31:02'    # with ISO 8601

And can be get in different ways too::

  str(t[0])  -->  2008-07-30T17:31:00
  repr(t[1]) -->  datetime64(1199164177, 's')
  str(t[0].item()) --> 2008-07-30 17:31:00  # datetime module object
  repr(t[0].item()) --> datetime.datetime(2008, 7, 30, 17, 31)  # idem
  str(t)  -->  [2008-07-30T17:31:00  2008-07-30T17:31:01  2008-07-30T17:31:02]
  repr(t)  -->  array([1199164176, 1199164177, 1199164178],
                      dtype='datetime64[s]')

Comparisons
------------

The comparisons will be supported too::

  numpy.array(['1980'], 'M8[Y]') == numpy.array(['1979'], 'M8[Y]')
  --> [False]

including applying broadcasting::

  numpy.array(['1979', '1980'], 'M8[Y]') == numpy.datetime64('1980', 'Y')
  --> [False, True]

The following should also work::

  numpy.array(['1979', '1980'], 'M8[Y]') == '1980-01-01'
  --> [False, True]

because the right hand expression can be broadcasted into an array of 2
elements of dtype 'M8[Y]'.

Compatibility issues
---------------------

This will be fully compatible with the ``datetime`` class of the
``datetime`` module of Python only when using a time unit of
microseconds.  For other time units, the conversion process will lose
precision or will overflow as needed.  The conversion from/to a
``datetime`` object doesn't take leap seconds into account.


``timedelta64``
===============

It represents a time that is relative (i.e. not absolute).  It is
implemented internally as an ``int64`` type.

In time unit *conversions* and time *representations* (but not in other
time computations), the value -2**63 (0x8000000000000000) is interpreted
as an invalid or unknown time, *Not a Time* or *NaT*.  See the section
on time unit conversions for more information.

The value of a time delta is *an integer number of units of the
chosen time unit*.

Building a ``timedelta64`` dtype
---------------------------------

The proposed ways to specify the time unit in the dtype constructor are:

Using the long string notation::

  dtype('timedelta64[us]')

Using the short string notation::

  dtype('m8[us]')

If a time unit is not specified, then a default of [us] is assumed.
Thus 'm8' and 'm8[us]' are equivalent.

Setting and getting values
---------------------------

The objects with this dtype can be set in a series of ways::

  t = numpy.ones(3, dtype='m8[ms]')
  t[0] = 12    # assign to 12 ms
  t[1] = datetime.timedelta(0, 0, 13000)   # 13 ms
  t[2] = '0:00:00.014'    # 14 ms

And can be get in different ways too::

  str(t[0])  -->  0:00:00.012
  repr(t[1]) -->  timedelta64(13, 'ms')
  str(t[0].item()) --> 0:00:00.012000   # datetime module object
  repr(t[0].item()) --> datetime.timedelta(0, 0, 12000)  # idem
  str(t)     -->  [0:00:00.012  0:00:00.014  0:00:00.014]
  repr(t)    -->  array([12, 13, 14], dtype="timedelta64[ms]")

Comparisons
------------

The comparisons will be supported too::

  numpy.array([12, 13, 14], 'm8[ms]') == numpy.array([12, 13, 13], 'm8[ms]')
  --> [True, True, False]

or by applying broadcasting::

  numpy.array([12, 13, 14], 'm8[ms]') == numpy.timedelta64(13, 'ms')
  --> [False, True, False]

The following should work too::

  numpy.array([12, 13, 14], 'm8[ms]') == '0:00:00.012'
  --> [True, False, False]

because the right hand expression can be broadcasted into an array of 3
elements of dtype 'm8[ms]'.

Compatibility issues
---------------------

This will be fully compatible with the ``timedelta`` class of the
``datetime`` module of Python only when using a time unit of
microseconds.  For other units, the conversion process will lose
precision or will overflow as needed.


Examples of use
===============

Here is an example of use for the ``datetime64``::

  In [5]: numpy.datetime64(42, 'us')
  Out[5]: datetime64(42, 'us')

  In [6]: print numpy.datetime64(42, 'us')
  1970-01-01T00:00:00.000042  # representation in ISO 8601 format

  In [7]: print numpy.datetime64(367.7, 'D')  # decimal part is lost
  1971-01-02  # still ISO 8601 format

  In [8]: numpy.datetime('2008-07-18T12:23:18', 'm')  # from ISO 8601
  Out[8]: datetime64(20273063, 'm')

  In [9]: print numpy.datetime('2008-07-18T12:23:18', 'm')
  Out[9]: 2008-07-18T12:23

  In [10]: t = numpy.zeros(5, dtype="datetime64[ms]")

  In [11]: t[0] = datetime.datetime.now()  # setter in action

  In [12]: print t
  [2008-07-16T13:39:25.315  1970-01-01T00:00:00.000
   1970-01-01T00:00:00.000  1970-01-01T00:00:00.000
   1970-01-01T00:00:00.000]

  In [13]: repr(t)
  Out[13]: array([267859210457, 0, 0, 0, 0], dtype="datetime64[ms]")

  In [14]: t[0].item()     # getter in action
  Out[14]: datetime.datetime(2008, 7, 16, 13, 39, 25, 315000)

  In [15]: print t.dtype
  dtype('datetime64[ms]')

And here it goes an example of use for the ``timedelta64``::

  In [5]: numpy.timedelta64(10, 'us')
  Out[5]: timedelta64(10, 'us')

  In [6]: print numpy.timedelta64(10, 'us')
  0:00:00.000010

  In [7]: print numpy.timedelta64(3600.2, 'm')  # decimal part is lost
  2 days, 12:00

  In [8]: t1 = numpy.zeros(5, dtype="datetime64[ms]")

  In [9]: t2 = numpy.ones(5, dtype="datetime64[ms]")

  In [10]: t = t2 - t1

  In [11]: t[0] = datetime.timedelta(0, 24)  # setter in action

  In [12]: print t
  [0:00:24.000  0:00:01.000  0:00:01.000  0:00:01.000  0:00:01.000]

  In [13]: print repr(t)
  Out[13]: array([24000, 1, 1, 1, 1], dtype="timedelta64[ms]")

  In [14]: t[0].item()     # getter in action
  Out[14]: datetime.timedelta(0, 24)

  In [15]: print t.dtype
  dtype('timedelta64[s]')


Operating with date/time arrays
===============================

``datetime64`` vs ``datetime64``
--------------------------------

The only arithmetic operation allowed between absolute dates is
subtraction::

  In [10]: numpy.ones(3, "M8[s]") - numpy.zeros(3, "M8[s]")
  Out[10]: array([1, 1, 1], dtype=timedelta64[s])

But not other operations::

  In [11]: numpy.ones(3, "M8[s]") + numpy.zeros(3, "M8[s]")
  TypeError: unsupported operand type(s) for +: 'numpy.ndarray' and 'numpy.ndarray'

Comparisons between absolute dates are allowed.

Casting rules
~~~~~~~~~~~~~

When operating (basically, only the subtraction will be allowed) two
absolute times with different unit times, the outcome would be to raise
an exception.  This is because the ranges and time-spans of the different
time units can be very different, and it is not clear at all what time
unit will be preferred for the user.  For example, this should be
allowed::

  >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[Y]")
  array([1, 1, 1], dtype="timedelta64[Y]")

But the next should not::

  >>> numpy.ones(3, dtype="M8[Y]") - numpy.zeros(3, dtype="M8[ns]")
  raise numpy.IncompatibleUnitError  # what unit to choose?


``datetime64`` vs ``timedelta64``
---------------------------------

It will be possible to add and subtract relative times from absolute
dates::

  In [10]: numpy.zeros(5, "M8[Y]") + numpy.ones(5, "m8[Y]")
  Out[10]: array([1971, 1971, 1971, 1971, 1971], dtype=datetime64[Y])

  In [11]: numpy.ones(5, "M8[Y]") - 2 * numpy.ones(5, "m8[Y]")
  Out[11]: array([1969, 1969, 1969, 1969, 1969], dtype=datetime64[Y])

But not other operations::

  In [12]: numpy.ones(5, "M8[Y]") * numpy.ones(5, "m8[Y]")
  TypeError: unsupported operand type(s) for *: 'numpy.ndarray' and 'numpy.ndarray'

Casting rules
~~~~~~~~~~~~~

In this case the absolute time should have priority for determining the
time unit of the outcome.  That would represent what the people wants to
do most of the times.  For example, this would allow to do::

  >>> series = numpy.array(['1970-01-01', '1970-02-01', '1970-09-01'],
  dtype='datetime64[D]')
  >>> series2 = series + numpy.timedelta(1, 'Y')  # Add 2 relative years
  >>> series2
  array(['1972-01-01', '1972-02-01', '1972-09-01'],
  dtype='datetime64[D]')  # the 'D'ay time unit has been chosen


``timedelta64`` vs ``timedelta64``
----------------------------------

Finally, it will be possible to operate with relative times as if they
were regular int64 dtypes *as long as* the result can be converted back
into a ``timedelta64``::

  In [10]: numpy.ones(3, 'm8[us]')
  Out[10]: array([1, 1, 1], dtype="timedelta64[us]")

  In [11]: (numpy.ones(3, 'm8[M]') + 2) ** 3
  Out[11]: array([27, 27, 27], dtype="timedelta64[M]")

But::

  In [12]: numpy.ones(5, 'm8') + 1j
  TypeError: the result cannot be converted into a ``timedelta64``

Casting rules
~~~~~~~~~~~~~

When combining two ``timedelta64`` dtypes with different time units the
outcome will be the shorter of both ("keep the precision" rule).  For
example::

  In [10]: numpy.ones(3, 'm8[s]') + numpy.ones(3, 'm8[m]')
  Out[10]: array([61, 61, 61],  dtype="timedelta64[s]")

However, due to the impossibility to know the exact duration of a
relative year or a relative month, when these time units appear in one
of the operands, the operation will not be allowed::

  In [11]: numpy.ones(3, 'm8[Y]') + numpy.ones(3, 'm8[D]')
  raise numpy.IncompatibleUnitError  # how to convert relative years to days?

In order to being able to perform the above operation a new NumPy
function, called ``change_timeunit`` is proposed.  Its signature will
be::

  change_timeunit(time_object, new_unit, reference)

where 'time_object' is the time object whose unit is to be changed,
'new_unit' is the desired new time unit, and 'reference' is an absolute
date (NumPy datetime64 scalar) that will be used to allow the conversion
of relative times in case of using time units with an uncertain number
of smaller time units (relative years or months cannot be expressed in
days).

With this, the above operation can be done as follows::

  In [10]: t_years = numpy.ones(3, 'm8[Y]')

  In [11]: t_days = numpy.change_timeunit(t_years, 'D', '2001-01-01')

  In [12]: t_days + numpy.ones(3, 'm8[D]')
  Out[12]: array([366, 366, 366],  dtype="timedelta64[D]")


dtype vs time units conversions
===============================

For changing the date/time dtype of an existing array, we propose to use
the ``.astype()`` method.  This will be mainly useful for changing time
units.

For example, for absolute dates::

  In[10]: t1 = numpy.zeros(5, dtype="datetime64[s]")

  In[11]: print t1
  [1970-01-01T00:00:00  1970-01-01T00:00:00  1970-01-01T00:00:00
   1970-01-01T00:00:00  1970-01-01T00:00:00]

  In[12]: print t1.astype('datetime64[D]')
  [1970-01-01  1970-01-01  1970-01-01  1970-01-01  1970-01-01]

For relative times::

  In[10]: t1 = numpy.ones(5, dtype="timedelta64[s]")

  In[11]: print t1
  [1 1 1 1 1]

  In[12]: print t1.astype('timedelta64[ms]')
  [1000 1000 1000 1000 1000]

Changing directly from/to relative to/from absolute dtypes will not be
supported::

  In[13]: numpy.zeros(5, dtype="datetime64[s]").astype('timedelta64')
  TypeError: data type cannot be converted to the desired type

Business days have the peculiarity that they do not cover a continuous
line of time (they have gaps at weekends).  Thus, when converting from
any ordinary time to business days, it can happen that the original time
is not representable.  In that case, the result of the conversion is
*Not a Time* (*NaT*)::

  In[10]: t1 = numpy.arange(5, dtype="datetime64[D]")

  In[11]: print t1
  [1970-01-01  1970-01-02  1970-01-03  1970-01-04  1970-01-05]

  In[12]: t2 = t1.astype("datetime64[B]")

  In[13]: print t2  # 1970 begins in a Thursday
  [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

When converting back to ordinary days, NaT values are left untouched
(this happens in all time unit conversions)::

  In[14]: t3 = t2.astype("datetime64[D]")

  In[13]: print t3
  [1970-01-01  1970-01-02  NaT  NaT  1970-01-05]

Necessary changes to NumPy
==========================

In order to facilitate the addition of the date-time data-types a few changes
to NumPy were made:

Addition of metadata to dtypes
------------------------------

All data-types now have a metadata dictionary. It can be set using the
metadata keyword during construction of the object.

Date-time data-types will place the word "__frequency__" in the meta-data
dictionary containing a 4-tuple with the following parameters.

(basic unit string (str),
 number of multiples (int),
 number of sub-divisions (int),
 number of events (int)).

Simple time units like 'D' for days will thus be specified by ('D', 1, 1, 1) in
the "__frequency__" key of the metadata.  More complicated time units (like '[2W/5]//50') will be indicated by ('D', 2, 5, 50).

The "__frequency__" key is reserved for metadata and cannot be set with a
dtype constructor.


Ufunc interface extension
-------------------------

ufuncs that have datetime and timedelta arguments can use the Python API
during ufunc calls (to raise errors).

There is a new ufunc C-API call to set the data for a particular
function pointer (for a particular set of data-types) to be the list of arrays
passed in to the ufunc.

Array Interface Extensions
--------------------------

The array interface is extended to both handle datetime and timedelta
typestr (including extended notation).

In addition, the typestr element of the __array_interface__ can be a tuple
as long as the version string is 4.  The tuple is
('typestr', metadata dictionary).

This extension to the typestr concept extends to the descr portion of
the __array_interface__.  Thus, the second element in the tuple of a
list of tuples describing a data-format can itself be a tuple of
('typestr', metadata dictionary).


Final considerations
====================

Why the fractional time and events: [3Y/12]//50
-----------------------------------------------

It is difficult to come up with enough units to satisfy every need.  For
example, in C# on Windows the fundamental tick of time is 100ns.
Multiple of basic units are simple to handle.  Divisors of basic units
are harder to handle arbitrarily, but it is common to mentally think of
a month as 1/12 of a year, or a day as 1/7 of a week.  Therefore, the
ability to specify a unit in terms of a fraction of a "larger" unit was
implemented.

The event notion (//50) was added to solve a use-case of a commercial
sponsor of this NEP.  The idea is to allow timestamp to carry both event
number and timestamp information.  The remainder carries the event
number information, while the quotient carries the timestamp
information.


Why the ``origin`` metadata disappeared
---------------------------------------

During the discussion of the date/time dtypes in the NumPy list, the
idea of having an ``origin`` metadata that complemented the definition
of the absolute ``datetime64`` was initially found to be useful.

However, after thinking more about this, we found that the combination
of an absolute ``datetime64`` with a relative ``timedelta64`` does offer
the same functionality while removing the need for the additional
``origin`` metadata.  This is why we have removed it from this proposal.

Operations with mixed time units
--------------------------------

Whenever an operation between two time values of the same dtype with the
same unit is accepted, the same operation with time values of different
units should be possible (e.g. adding a time delta in seconds and one in
microseconds), resulting in an adequate time unit.  The exact semantics
of this kind of operations is defined int the "Casting rules"
subsections of the "Operating with date/time arrays" section.

Due to the peculiarities of business days, it is most probable that
operations mixing business days with other time units will not be
allowed.


.. [1] https://docs.python.org/library/datetime.html
.. [2] https://www.egenix.com/products/python/mxBase/mxDateTime
.. [3] https://en.wikipedia.org/wiki/Unix_time


.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 72
.. End:
.. _NEP40:

================================================
NEP 40 — Legacy datatype implementation in NumPy
================================================

:title: Legacy Datatype Implementation in NumPy
:Author: Sebastian Berg
:Status: Final
:Type: Informational
:Created: 2019-07-17


.. note::

    This NEP is first in a series:

    - NEP 40 (this document) explains the shortcomings of NumPy's dtype implementation.

    - :ref:`NEP 41 <NEP41>` gives an overview of our proposed replacement.

    - :ref:`NEP 42 <NEP42>` describes the new design's datatype-related APIs.

    - NEP 43 describes the new design's API for universal functions.



Abstract
--------

As a preparation to further NumPy enhancement proposals 41, 42, and 43. This
NEP details the current status of NumPy datatypes as of NumPy 1.18.
It describes some of the technical aspects and concepts that
motivated the other proposals.
For more general information most readers should begin by reading :ref:`NEP 41 <NEP41>`
and use this document only as a reference or for additional details.


Detailed Description
--------------------

This section describes some central concepts and provides a brief overview
of the current implementation of dtypes as well as a discussion.
In many cases subsections will be split roughly to first describe the
current implementation and then follow with an "Issues and Discussion" section.

.. _parametric-datatype-discussion:

Parametric Datatypes
^^^^^^^^^^^^^^^^^^^^

Some datatypes are inherently *parametric*. All ``np.flexible`` scalar
types are attached to parametric datatypes (string, bytes, and void).
The class ``np.flexible`` for scalars is a superclass for the data types of
variable length (string, bytes, and void).
This distinction is similarly exposed by the C-Macros
``PyDataType_ISFLEXIBLE`` and ``PyTypeNum_ISFLEXIBLE``.
This flexibility generalizes to the set of values which can be represented
inside the array.
For instance, ``"S8"`` can represent longer strings than ``"S4"``.
The parametric string datatype thus also limits the values inside the array
to a subset (or subtype) of all values which can be represented by string
scalars.

The basic numerical datatypes are not flexible (do not inherit from
``np.flexible``). ``float64``, ``float32``, etc. do have a byte order, but the described
values are unaffected by it, and it is always possible to cast them to the
native, canonical representation without any loss of information.

The concept of flexibility can be generalized to parametric datatypes.
For example the private ``PyArray_AdaptFlexibleDType`` function also accepts the
naive datetime dtype as input to find the correct time unit.
The datetime dtype is thus parametric not in the size of its storage,
but instead in what the stored value represents.
Currently ``np.can_cast("datetime64[s]", "datetime64[ms]", casting="safe")``
returns true, although it is unclear that this is desired or generalizes
to possible future data types such as physical units.

Thus we have data types (mainly strings) with the properties that:

1. Casting is not always safe (``np.can_cast("S8", "S4")``)
2. Array coercion should be able to discover the exact dtype, such as for
   ``np.array(["str1", 12.34], dtype="S")`` where NumPy discovers the
   resulting dtype as ``"S5"``.
   (If the dtype argument is omitted the behaviour is currently ill defined [gh-15327]_.)
   A form similar to ``dtype="S"`` is ``dtype="datetime64"`` which can
   discover the unit: ``np.array(["2017-02"], dtype="datetime64")``.

This notion highlights that some datatypes are more complex than the basic
numerical ones, which is evident in the complicated output type discovery
of universal functions.


Value Based Casting
^^^^^^^^^^^^^^^^^^^

Casting is typically defined between two types:
A type is considered to cast safely to a second type when the second type
can represent all values of the first without loss of information.
NumPy may inspect the actual value to decide
whether casting is safe or not.

This is useful for example in expressions such as::

    arr = np.array([1, 2, 3], dtype="int8")
    result = arr + 5
    assert result.dtype == np.dtype("int8")
    # If the value is larger, the result will change however:
    result = arr + 500
    assert result.dtype == np.dtype("int16")

In this expression, the python value (which originally has no datatype) is
represented as an ``int8`` or ``int16`` (the smallest possible data type).

NumPy currently does this even for NumPy scalars and zero-dimensional arrays,
so that replacing ``5`` with ``np.int64(5)`` or ``np.array(5, dtype="int64")``
in the above expression will lead to the same results, and thus ignores the
existing datatype. The same logic also applies to floating-point scalars,
which are allowed to lose precision.
The behavior is not used when both inputs are scalars, so that
``5 + np.int8(5)`` returns the default integer size (32 or 64-bit) and not
an ``np.int8``.

While the behaviour is defined in terms of casting and exposed by
``np.result_type`` it is mainly important for universal functions
(such as ``np.add`` in the above examples).
Universal functions currently rely on safe casting semantics to decide which
loop should be used, and thus what the output datatype will be.


Issues and Discussion
"""""""""""""""""""""

There appears to be some agreement that the current method is
not desirable for values that have a datatype,
but may be useful for pure python integers or floats as in the first
example.
However, any change of the datatype system and universal function dispatching
must initially fully support the current behavior.
A main difficulty is that for example the value ``156`` can be represented
by ``np.uint8`` and ``np.int16``.
The result depends on the "minimal" representation in the context of the
conversion (for ufuncs the context may depend on the loop order).


The Object Datatype
^^^^^^^^^^^^^^^^^^^

The object datatype currently serves as a generic fallback for any value
which is not otherwise representable.
However, due to not having a well-defined type, it has some issues,
for example when an array is filled with Python sequences::

    >>> l = [1, [2]]
    >>> np.array(l, dtype=np.object_)
    array([1, list([2])], dtype=object)  # a 1d array

    >>> a = np.empty((), dtype=np.object_)
    >>> a[...] = l
    ValueError: assignment to 0-d array  # ???
    >>> a[()] = l
    >>> a
    array(list([1, [2]]), dtype=object)

Without a well-defined type, functions such as ``isnan()`` or ``conjugate()``
do not necessarily work, but can work for a :class:`decimal.Decimal`.
To improve this situation it seems desirable to make it easy to create
``object`` dtypes that represent a specific Python datatype and stores its object
inside the array in the form of pointer to python ``PyObject``.
Unlike most datatypes, Python objects require garbage collection.
This means that additional methods to handle references and
visit all objects must be defined.
In practice, for most use-cases it is sufficient to limit the creation of such
datatypes so that all functionality related to Python C-level references is
private to NumPy.

Creating NumPy datatypes that match builtin Python objects also creates a few problems
that require more thoughts and discussion.
These issues do not need to solved right away:

* NumPy currently returns *scalars* even for array input in some cases, in most
  cases this works seamlessly. However, this is only true because the NumPy
  scalars behave much like NumPy arrays, a feature that general Python objects
  do not have.
* Seamless integration probably requires that ``np.array(scalar)`` finds the
  correct DType automatically since some operations (such as indexing) return
  the scalar instead of a 0D array.
  This is problematic if multiple users independently decide to implement
  for example a DType for ``decimal.Decimal``.


Current ``dtype`` Implementation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Currently ``np.dtype`` is a Python class with its instances being the
``np.dtype(">float64")``, etc. instances.
To set the actual behaviour of these instances, a prototype instance is stored
globally and looked up based on the ``dtype.typenum``. The singleton is used
where possible. Where required it is copied and modified, for instance to change
endianness.

Parametric datatypes (strings, void, datetime, and timedelta) must store
additional information such as string lengths, fields, or datetime units --
new instances of these types are created instead of relying on a singleton.
All current datatypes within NumPy further support setting a metadata field
during creation which can be set to an arbitrary dictionary value, but seems
rarely used in practice (one recent and prominent user is h5py).

Many datatype-specific functions are defined within a C structure called
:c:type:`PyArray_ArrFuncs`, which is part of each ``dtype`` instance and
has a similarity to Python's ``PyNumberMethods``.
For user-defined datatypes this structure is exposed to the user, making
ABI-compatible changes impossible.
This structure holds important information such as how to copy or cast,
and provides space for pointers to functions, such as comparing elements,
converting to bool, or sorting.
Since some of these functions are vectorized operations, operating on more than
one element, they fit the model of ufuncs and do not need to be defined on the
datatype in the future.
For example the ``np.clip`` function was previously implemented using
``PyArray_ArrFuncs`` and is now implemented as a ufunc.

Discussion and Issues
"""""""""""""""""""""

A further issue with the current implementation of the functions on the dtype
is that, unlike methods,
they are not passed an instance of the dtype when called.
Instead, in many cases, the array which is being operated on is passed in
and typically only used to extract the datatype again.
A future API should likely stop passing in the full array object.
Since it will be necessary to fall back to the old definitions for
backward compatibility, the array object may not be available.
However, passing a "fake" array in which mainly the datatype is defined
is probably a sufficient workaround
(see backward compatibility; alignment information may sometimes also be desired).

Although not extensively used outside of NumPy itself, the currently
``PyArray_Descr`` is a public structure.
This is especially also true for the ``PyArray_ArrFuncs`` structure stored in
the ``f`` field.
Due to compatibility they may need to remain supported for a very long time,
with the possibility of replacing them by functions that dispatch to a newer API.

However, in the long run access to these structures will probably have to
be deprecated.


NumPy Scalars and Type Hierarchy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As a side note to the above datatype implementation: unlike the datatypes,
the NumPy scalars currently **do** provide a type hierarchy, consisting of abstract
types such as ``np.inexact`` (see figure below).
In fact, some control flow within NumPy currently uses
``issubclass(a.dtype.type, np.inexact)``.

.. _nep-0040_dtype-hierarchy:

.. figure:: _static/nep-0040_dtype-hierarchy.png

   **Figure:** Hierarchy of NumPy scalar types reproduced from the reference
   documentation. Some aliases such as ``np.intp`` are excluded. Datetime
   and timedelta are not shown.

NumPy scalars try to mimic zero-dimensional arrays with a fixed datatype.
For the numerical (and unicode) datatypes, they are further limited to
native byte order.


Current Implementation of Casting
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

One of the main features which datatypes need to support is casting between one
another using ``arr.astype(new_dtype, casting="unsafe")``, or during execution
of ufuncs with different types (such as adding integer and floating point numbers).

Casting tables determine whether it is possible to cast from one specific type to another.
However, generic casting rules cannot handle the parametric dtypes such as strings.
The logic for parametric datatypes is defined mainly in ``PyArray_CanCastTo``
and currently cannot be customized for user defined datatypes.

The actual casting has two distinct parts:

1. ``copyswap``/``copyswapn`` are defined for each dtype and can handle
   byte-swapping for non-native byte orders as well as unaligned memory.
2. The generic casting code is provided by C functions which know how to
   cast aligned and contiguous memory from one dtype to another
   (both in native byte order).
   These C-level functions can be registered to cast aligned and contiguous memory
   from one dtype to another.
   The function may be provided with both arrays (although the parameter
   is sometimes ``NULL`` for scalars).
   NumPy will ensure that these functions receive native byte order input.
   The current implementation stores the functions either in a C-array
   on the datatype which is cast, or in a dictionary when casting to a user
   defined datatype.

Generally NumPy will thus perform casting as chain of the three functions
``in_copyswapn -> castfunc -> out_copyswapn`` using (small) buffers between
these steps.

The above multiple functions are wrapped into a single function (with metadata)
that handles the cast and is used for example during the buffered iteration used
by ufuncs.
This is the mechanism that is always used for user defined datatypes.
For most dtypes defined within NumPy itself, more specialized code is used to
find a function to do the actual cast
(defined by the private ``PyArray_GetDTypeTransferFunction``).
This mechanism replaces most of the above mechanism and provides much faster
casts for example when the inputs are not contiguous in memory.
However, it cannot be extended by user defined datatypes.

Related to casting, we currently have a ``PyArray_EquivTypes`` function which
indicate that a *view* is sufficient (and thus no cast is necessary).
This function is used multiple places and should probably be part of
a redesigned casting API.


DType handling in Universal functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Universal functions are implemented as instances of the ``numpy.UFunc`` class
with an ordered-list of datatype-specific
(based on the dtype typecode character, not datatype instances) implementations,
each with a signature and a function pointer.
This list of implementations can be seen with ``ufunc.types`` where
all implementations are listed with their C-style typecode signatures.
For example::

    >>> np.add.types
    [...,
     'll->l',
     ...,
     'dd->d',
     ...]

Each of these signatures is associated with a single inner-loop function defined
in C, which does the actual calculation, and may be called multiple times.

The main step in finding the correct inner-loop function is to call a
:c:type:`PyUFunc_TypeResolutionFunc` which retrieves the input dtypes from
the provided input arrays
and will determine the full type signature (including output dtype) to be executed.

By default the ``TypeResolver`` is implemented by searching all of the implementations
listed in ``ufunc.types`` in order and stopping if all inputs can be safely
cast to fit the signature.
This means that if long (``l``) and double (``d``) arrays are added,
numpy will find that the ``'dd->d'`` definition works
(long can safely cast to double) and uses that.

In some cases this is not desirable. For example the ``np.isnat`` universal
function has a ``TypeResolver`` which rejects integer inputs instead of
allowing them to be cast to float.
In principle, downstream projects can currently use their own non-default
``TypeResolver``, since the corresponding C-structure necessary to do this
is public.
The only project known to do this is Astropy, which is willing to switch to
a new API if NumPy were to remove the possibility to replace the TypeResolver.

For user defined datatypes, the dispatching logic is similar,
although separately implemented and limited (see discussion below).


Issues and Discussion
"""""""""""""""""""""

It is currently only possible for user defined functions to be found/resolved
if any of the inputs (or the outputs) has the user datatype, since it uses the
`OO->O` signature.
For example, given that a ufunc loop to implement ``fraction_divide(int, int)
-> Fraction`` has been implemented,
the call ``fraction_divide(4, 5)`` (with no specific output dtype) will fail
because the loop that
includes the user datatype ``Fraction`` (as output) can only be found if any of
the inputs is already a ``Fraction``.
``fraction_divide(4, 5, dtype=Fraction)`` can be made to work, but is inconvenient.

Typically, dispatching is done by finding the first loop that matches. A match
is defined as: all inputs (and possibly outputs) can
be cast safely to the signature typechars (see also the current implementation
section).
However, in some cases safe casting is problematic and thus explicitly not
allowed.
For example the ``np.isnat`` function is currently only defined for
datetime and timedelta,
even though integers are defined to be safely castable to timedelta.
If this was not the case, calling
``np.isnat(np.array("NaT", "timedelta64").astype("int64"))`` would currently
return true, although the integer input array has no notion of "not a time".
If a universal function, such as most functions in ``scipy.special``, is only
defined for ``float32`` and ``float64`` it will currently automatically
cast a ``float16`` silently to ``float32`` (similarly for any integer input).
This ensures successful execution, but may lead to a change in the output dtype
when support for new data types is added to a ufunc.
When a ``float16`` loop is added, the output datatype will currently change
from ``float32`` to ``float16`` without a warning.

In general the order in which loops are registered is important.
However, this is only reliable if all loops are added when the ufunc is first defined.
Additional loops added when a new user datatypes is imported
must not be sensitive to the order in which imports occur.

There are two main approaches to better define the type resolution for user
defined types:

1. Allow for user dtypes to directly influence the loop selection.
   For example they may provide a function which return/select a loop
   when there is no exact matching loop available.
2. Define a total ordering of all implementations/loops, probably based on
   "safe casting" semantics, or semantics similar to that.

While option 2 may be less complex to reason about it remains to be seen
whether it is sufficient for all (or most) use cases.


Adjustment of Parametric output DTypes in UFuncs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A second step necessary for parametric dtypes is currently performed within
the ``TypeResolver``:
the datetime and timedelta datatypes have to decide on the correct parameter
for the operation and output array.
This step also needs to double check that all casts can be performed safely,
which by default means that they are "same kind" casts.

Issues and Discussion
"""""""""""""""""""""

Fixing the correct output dtype is currently part of the type resolution.
However, it is a distinct step and should probably be handled as such after
the actual type/loop resolution has occurred.

As such this step may move from the dispatching step (described above) to
the implementation-specific code described below.


DType-specific Implementation of the UFunc
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once the correct implementation/loop is found, UFuncs currently call
a single *inner-loop function* which is written in C.
This may be called multiple times to do the full calculation and it has
little or no information about the current context. It also has a void
return value.

Issues and Discussion
"""""""""""""""""""""

Parametric datatypes may require passing
additional information to the inner-loop function to decide how to interpret
the data.
This is the reason why currently no universal functions for ``string`` dtypes
exist (although technically possible within NumPy itself).
Note that it is currently possible to pass in the input array objects
(which in turn hold the datatypes when no casting is necessary).
However, the full array information should not be required and currently the
arrays are passed in before any casting occurs.
The feature is unused within NumPy and no known user exists.

Another issue is the error reporting from within the inner-loop function.
There exist currently two ways to do this:

1. by setting a Python exception
2. using the CPU floating point error flags.

Both of these are checked before returning to the user.
However, many integer functions currently can set neither of these errors,
so that checking the floating point error flags is unnecessary overhead.
On the other hand, there is no way to stop the iteration or pass out error
information which does not use the floating point flags or requires to hold
the Python global interpreter lock (GIL).

It seems necessary to provide more control to authors of inner loop functions.
This means allowing users to pass in and out information from the inner-loop
function more easily, while *not* providing the input array objects.
Most likely this will involve:

* Allowing the execution of additional code before the first and after
  the last inner-loop call.
* Returning an integer value from the inner-loop to allow stopping the
  iteration early and possibly propagate error information.
* Possibly, to allow specialized inner-loop selections. For example currently
  ``matmul`` and many reductions will execute optimized code for certain inputs.
  It may make sense to allow selecting such optimized loops beforehand.
  Allowing this may also help to bring casting (which uses this heavily) and
  ufunc implementations closer.

The issues surrounding the inner-loop functions have been discussed in some
detail in the github issue gh-12518_ .

Reductions use an "identity" value.
This is currently defined once per ufunc, regardless of the ufunc dtype signature.
For example  ``0`` is used for ``sum``, or ``math.inf`` for ``min``.
This works well for numerical datatypes, but is not always appropriate for other dtypes.
In general it should be possible to provide a dtype-specific identity to the
ufunc reduction.


Datatype Discovery during Array Coercion
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When calling ``np.array(...)`` to coerce a general Python object to a NumPy array,
all objects need to be inspected to find the correct dtype.
The input to ``np.array()`` are potentially nested Python sequences which hold
the final elements as generic Python objects.
NumPy has to unpack all the nested sequences and then inspect the elements.
The final datatype is found by iterating over all elements which will end up
in the array and:

1. discovering the dtype of the single element:

   * from array (or array like) or NumPy scalar using ``element.dtype``
   * using ``isinstance(..., float)`` for known Python types
     (note that these rules mean that subclasses are *currently* valid).
   * special rule for void datatypes to coerce tuples.

2. Promoting the current dtype with the next elements dtype using
   ``np.promote_types``.
3. If strings are found, the whole process is restarted (see also [gh-15327]_),
   in a similar manner as if ``dtype="S"`` was given (see below).

If ``dtype=...`` is given, this dtype is used unmodified, unless
it is an unspecific *parametric dtype instance* which means "S0", "V0", "U0",
"datetime64", and "timdelta64".
These are thus flexible datatypes without length 0 – considered to be unsized –
and datetimes or timedelta without a unit attached ("generic unit").

In future DType class hierarchy, these may be represented by the class rather
than a special instance, since these special instances should not normally be
attached to an array.

If such a *parametric dtype instance* is provided for example using ``dtype="S"``
``PyArray_AdaptFlexibleDType`` is called and effectively inspects all values
using DType specific logic.
That is:

* Strings will use ``str(element)`` to find the length of most elements
* Datetime64 is capable of coercing from strings and guessing the correct unit.


Discussion and Issues
"""""""""""""""""""""

It seems probable that during normal discovery, the ``isinstance`` should rather
be strict ``type(element) is desired_type`` checks.
Further, the current ``AdaptFlexibleDType`` logic should be made available to
user DTypes and not be a secondary step, but instead replace, or be part of,
the normal discovery.



Related Issues
--------------

``np.save`` currently translates all user-defined dtypes to void dtypes.
This means they cannot be stored using the ``npy`` format.
This is not an issue for the python pickle protocol, although it may require
some thought if we wish to ensure that such files can be loaded securely
without the possibility of executing malicious code
(i.e. without the ``allow_pickle=True`` keyword argument).

The additional existence of masked arrays and especially masked datatypes
within Pandas has interesting implications for interoperability.
Since mask information is often stored separately, its handling requires
support by the container (array) object.
NumPy itself does not provide such support, and is not expected to add it
in the foreseeable future.
However, if such additions to the datatypes within NumPy would improve
interoperability they could be considered even if
they are not used by NumPy itself.


Related Work
------------

* Julia types are an interesting blueprint for a type hierarchy, and define
  abstract and concrete types [julia-types]_.

* In Julia promotion can occur based on abstract types. If a promoter is
  defined, it will cast the inputs and then Julia can then retry to find
  an implementation with the new values [julia-promotion]_.

* ``xnd-project`` (https://github.com/xnd-project) with ndtypes and gumath

  * The ``xnd-project`` is similar to NumPy and defines data types as well
    as the possibility to extend them. A major difference is that it does
    not use promotion/casting within the ufuncs, but instead requires explicit
    definition of ``int32 + float64 -> float64`` loops.



Discussion
----------

There have been many discussions about the current state and what a future
datatype system may look like.
The full list of these discussion is long and some are lost to time,
the following provides a subset for more recent ones:

* Draft NEP by Stephan Hoyer after a developer meeting (was updated on the next developer meeting) https://hackmd.io/6YmDt_PgSVORRNRxHyPaNQ

* List of related documents gathered previously here
  https://hackmd.io/UVOtgj1wRZSsoNQCjkhq1g (TODO: Reduce to the most important
  ones):

  * https://github.com/numpy/numpy/pull/12630
    Matti Picus draft NEP, discusses the technical side of subclassing  more from
    the side of ``ArrFunctions``

  * https://hackmd.io/ok21UoAQQmOtSVk6keaJhw and https://hackmd.io/s/ryTFaOPHE
    (2019-04-30) Proposals for subclassing implementation approach.

  * Discussion about the calling convention of ufuncs and need for more
    powerful UFuncs: https://github.com/numpy/numpy/issues/12518

  * 2018-11-30 developer meeting notes:
    https://github.com/BIDS-numpy/docs/blob/master/meetings/2018-11-30-dev-meeting.md
    and subsequent draft for an NEP: https://hackmd.io/6YmDt_PgSVORRNRxHyPaNQ

    BIDS Meeting on November 30, 2018 and document by Stephan Hoyer about
    what numpy should provide and thoughts of how to get there. Meeting with
    Eric Wieser, Matti Picus, Charles Harris, Tyler Reddy, Stéfan van der
    Walt, and Travis Oliphant.

  * SciPy 2018 brainstorming session with summaries of use cases:
    https://github.com/numpy/numpy/wiki/Dtype-Brainstorming

    Also lists some requirements and some ideas on implementations



References
----------

.. _gh-12518: https://github.com/numpy/numpy/issues/12518
.. [gh-15327] https://github.com/numpy/numpy/issues/12518

.. [julia-types] https://docs.julialang.org/en/v1/manual/types/index.html#Abstract-Types-1

.. [julia-promotion] https://docs.julialang.org/en/v1/manual/conversion-and-promotion/



Copyright
---------

This document has been placed in the public domain.
.. _NEP28:

===================================
NEP 28 — numpy.org website redesign
===================================

:Author: Ralf Gommers <ralf.gommers@gmail.com>
:Author: Joe LaChance <joe@boldmetrics.com>
:Author: Shekhar Rajak <shekharrajak.1994@gmail.com>
:Status: Final
:Type: Informational
:Created: 2019-07-16
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2019-August/079889.html


Abstract
--------

NumPy is the fundamental library for numerical and scientific computing with
Python. It is used by millions and has a large team of maintainers and
contributors. Despite that, its `numpy.org <http://numpy.org>`_ website has
never received the attention it needed and deserved. We hope and intend to
change that soon. This document describes ideas and requirements for how to
design a replacement for the current website, to better serve the needs of
our diverse community.

At a high level, what we're aiming for is:

- a modern, clean look
- an easy to deploy static site
- a structure that's easy to navigate
- content that addresses all types of stakeholders
- Possible multilingual translations / i18n

This website serves a couple of roles:

- it's the entry point to the project for new users
- it should link to the documentation (which is hosted separately, now on
  http://docs.scipy.org/ and in the near future on http://numpy.org/doc).
- it should address various aspects of the project (e.g. what NumPy is and
  why you'd want to use it, community, project organization, funding,
  relationship with NumFOCUS and possibly other organizations)
- it should link out to other places, so every type of stakeholder
  (beginning and advanced user, educators, packagers, funders, etc.)
  can find their way


Motivation and Scope
--------------------

The current numpy.org website has almost no content and its design is poor.
This affects many users, who come there looking for information. It also
affects many other aspects of the NumPy project, from finding new contributors
to fundraising.

The scope of the proposed redesign is the top-level numpy.org site, which
now contains only a couple of pages and may contain on the order of ten
pages after the redesign. Changing the documentation (user guide, reference
guide, and some other pages in the NumPy Manual) is out of scope for
this proposal.


Detailed description
--------------------

User Experience
~~~~~~~~~~~~~~~

Besides the NumPy logo, there is little that can or needs to be kept from the
current website. We will rely to a large extent on ideas and proposals by the
designer(s) of the new website.

As reference points we can use the `Jupyter website <https://jupyter.org/>`_,
which is probably the best designed site in our ecosystem, and the
`QuantEcon <https://quantecon.org>`_ and `Julia <https://julialang.org>`_
sites which are well-designed too.

The Website
~~~~~~~~~~~

A static site is a must. There are many high-quality static site generators.
The current website uses Sphinx, however that is not the best choice - it's
hard to theme and results in sites that are too text-heavy due to Sphinx'
primary aim being documentation.

The following should be considered when choosing a static site generator:

1. *How widely used is it?* This is important when looking for help maintaining
   or improving the site. More popular frameworks are usually also better
   maintained, so less chance of bugs or obsolescence.
2. *Ease of deployment.* Most generators meet this criterion, however things
   like built-in support for GitHub Pages helps.
3. *Preferences of who implements the new site.* Everyone has their own
   preferences. And it's a significant amount of work to build a new site.
   So we should take the opinion of those doing the work into account.

Traffic
```````

The current site receives on the order of 500,000 unique visitors per month.
With a redesigned site and relevant content, there is potential for visitor
counts to reach 5-6 million -- a similar level as
`scipy.org <http://scipy.org>`_ or `matplotlib.org <http://matplotlib.org>`_ --
or more.

Possible options for static site generators
```````````````````````````````````````````

1. *Jekyll.* This is a well maintained option with 855 Github contributors,
   with contributions within the last month. Jekyll is written in Ruby, and
   has a simple CLI interface. Jekyll also has a large directory of
   `themes <https://jekyllthemes.io>`__, although a majority cost money.
   There are several themes (`serif <https://jekyllthemes.io/theme/serif>`_,
   `uBuild <https://jekyllthemes.io/theme/ubuild-jekyll-theme>`_,
   `Just The Docs <https://jekyllthemes.io/theme/just-the-docs>`_) that are
   appropriate and free. Most themes are likely responsive for mobile, and
   that should be a requirement. Jekyll uses a combination of liquid templating
   and YAML to render HTML, and content is written in Markdown. i18n
   functionality is not native to Jekyll, but can be added easily.
   One nice benefit of Jekyll is that it can be run automatically by GitHub
   Pages, so deployment via a CI system doesn't need to be implemented.
2. *Hugo.* This is another well maintained option with 554 contributors, with
   contributions within the last month. Hugo is written in Go, and similar to
   Jekyll, has a simple to use CLI interface to generate static sites. Again,
   similar to Jekyll, Hugo has a large directory of
   `themes <https://themes.gohugo.io>`_. These themes appear to be free,
   unlike some of Jekyll's themes.
   (`Sample landing page theme <https://themes.gohugo.io/hugo-hero-theme>`_,
   `docs theme <https://themes.gohugo.io/hugo-whisper-theme>`_). Hugo uses Jade
   as its templating language, and content is also written in Markdown. i18n
   functionality is native to Hugo.
3. *Docusaurus.* Docusaurus is a responsive static site generator made by Facebook.
   Unlike the previous options, Docusaurus doesn't come with themes, and thus we
   would not want to use this for our landing page. This is an excellent docs
   option written in React. Docusaurus natively has support for i18n (via
   Crowdin_), document versioning, and document search.

Both Jekyll and Hugo are excellent options that should be supported into the
future and are good choices for NumPy. Docusaurus has several bonus features
such as versioning and search that Jekyll and Hugo don't have, but is likely
a poor candidate for a landing page - it could be a good option for a
high-level docs site later on though.

Deployment
~~~~~~~~~~

There is no need for running a server, and doing so is in our experience a
significant drain on the time of maintainers.

1. *Netlify.* Using netlify is free until 100GB of bandwidth is used. Additional
   bandwidth costs $20/100GB. They support a global CDN system, which will keep
   load times quick for users in other regions. Netlify also has Github integration,
   which will allow for easy deployment. When a pull request is merged, Netlify
   will automatically deploy the changes. DNS is simple, and HTTPS is also supported.
2. *Github Pages.* Github Pages also has a 100GB bandwidth limit, and is unclear if
   additional bandwidth can be purchased. It is also unclear where sites are deployed,
   and should be assumed sites aren't deployed globally. Github Pages has an easy to
   use CI & DNS, similar to Netlify. HTTPS is supported.
3. *Cloudflare.* An excellent option, additional CI is likely needed for the same
   ease of deployment.

All of the above options are appropriate for the NumPy site based on current
traffic. Updating to a new deployment strategy, if needed, is a minor amount of
work compared to developing the website itself. If a provider such as
Cloudflare is chosen, additional CI may be required, such as CircleCI, to
have a similar deployment to GitHub Pages or Netlify.

Analytics
~~~~~~~~~

It's beneficial to maintainers to know how many visitors are coming to
numpy.org. Google Analytics offers visitor counts and locations. This will
help to support and deploy more strategically, and help maintainers
understand where traffic is coming from.

Google Analytics is free. A script, provided by Google, must be added to the home page.

Website Structure
~~~~~~~~~~~~~~~~~

We aim to keep the first version of the new website small in terms of amount
of content. New pages can be added later on, it's more important right now to
get the site design right and get some essential information up. Note that in
the second half of 2019 we expect to get 1 or 2 tech writers involved in the
project via Google Season of Docs. They will likely help improve the content
and organization of that content.

We propose the following structure:

0. Front page: essentials of what NumPy is (compare e.g. jupyter.org), one or
   a couple key user stories (compare e.g. julialang.org)
1. Install
2. Documentation
3. Array computing
4. Community
5. Learning
6. About Us
7. Contribute
8. Donate

There may be a few other pages, e.g. a page on performance, that are linked
from one of the main pages.

Stakeholder Content
~~~~~~~~~~~~~~~~~~~

This should have as little content as possible *within the site*. Somewhere
on the site we should link out to content that's specific to:

- beginning users (quickstart, tutorial)
- advanced users
- educators
- packagers
- package authors that depend on NumPy
- funders (governance, roadmap)

Translation (multilingual / i18n)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NumPy has users all over the world. Most of those users are not native
English speakers, and many don't speak English well or at all. Therefore
having content in multiple languages is potentially addressing a large unmet
need. It would likely also help make the NumPy project more diverse and
welcoming.

On the other hand, there are good reasons why few projects have a
multi-lingual site. It's potentially a lot of extra work. Extra work for
maintainers is costly - they're already struggling to keep up with the work
load. Therefore we have to very carefully consider whether a multi-lingual
site is feasible and weight costs and benefits.

We start with an assertion: maintaining translations of all documentation, or
even the whole user guide, as part of the NumPy project is not feasible. One
simply has to look at the volume of our documentation and the frequency with
which we change it to realize that that's the case. Perhaps it will be
feasible though to translate just the top-level pages of the website. Those
do not change very often, and it will be a limited amount of content (order
of magnitude 5-10 pages of text).

We propose the following requirements for adding a language:

- The language must have a dedicated maintainer
- There must be a way to validate content changes (e.g. a second
  maintainer/reviewer, or high quality language support in a freely
  available machine translation tool)
- The language must have a reasonable size target audience (to be
  assessed by the NumPy maintainers)

Furthermore we propose a policy for when to remove support for a language again
(preferably by hiding it rather than deleting content). This may be done when
the language no longer has a maintainer, and coverage of translations falls
below an acceptable threshold (say 80%).

Benefits of having translations include:

- Better serve many existing and potential users
- Potentially attract a culturally and geographically more diverse set of contributors

The tradeoffs are:

- Cost of maintaining a more complex code base
- Cost of making decisions about whether or not to add a new language
- Higher cost to making content changes, creates work for language maintainers
- Any content change should be rolled out with enough delay to have translations in place

Can we define a small enough set of pages and content that it makes sense to do this?
Probably yes.

Is there an easy to use tool to maintain translations and add them to the website?
To be discussed - it needs investigating, and may depend on the choice of static site
generator. One potential option is Crowdin_, which is free for open source projects.


Style and graphic design
~~~~~~~~~~~~~~~~~~~~~~~~

Beyond the "a modern, clean look" goal we choose to not specify too much.  A
designer may have much better ideas than the authors of this proposal, hence we
will work with the designer(s) during the implementation phase.

The NumPy logo could use a touch-up.  The logo widely recognized and its colors and
design are good, however the look-and-feel is perhaps a little dated.


Other aspects
~~~~~~~~~~~~~

A search box would be nice to have.  The Sphinx documentation already has a
search box, however a search box on the main site which provides search results
for the docs, the website, and perhaps other domains that are relevant for
NumPy would make sense.


Backward compatibility
----------------------

Given a static site generator is chosen, we will migrate away from Sphinx for
numpy.org (the website, *not including the docs*). The current deployment can
be preserved until a future deprecation date is decided (potentially based on
the comfort level of our new site).

All site generators listed above have visibility into the HTML and Javascript
that is generated, and can continue to be maintained in the event a given
project ceases to be maintained.


Alternatives
------------

Alternatives we considered for the overall design of the website:

1. *Update current site.* A new Sphinx theme could be chosen. This would likely
   take the least amount of resources initially, however, Sphinx does not have
   the features we are looking for moving forward such as i18n, responsive design,
   and a clean, modern look.
   Note that updating the docs Sphinx theme is likely still a good idea - it's
   orthogonal to this NEP though.
2. *Create custom site.* This would take the most amount of resources, and is
   likely to have additional benefit in comparison to a static site generator.
   All features would be able to be added at the cost of developer time.


Discussion
----------

- Pull request for this NEP (with a good amount of discussion): https://github.com/numpy/numpy/pull/14032
- Email about NEP for review: https://mail.python.org/pipermail/numpy-discussion/2019-July/079856.html
- Proposal to accept this NEP: https://mail.python.org/pipermail/numpy-discussion/2019-August/079889.html


References and Footnotes
------------------------
.. _Crowdin: https://crowdin.com/pricing#annual

Copyright
---------

This document has been placed in the public domain.
.. _NEP16:

=============================================================
NEP 16 — An abstract base class for identifying "duck arrays"
=============================================================

:Author: Nathaniel J. Smith <njs@pobox.com>
:Status: Withdrawn
:Type: Standards Track
:Created: 2018-03-06
:Resolution: https://github.com/numpy/numpy/pull/12174

.. note::

    This NEP has been withdrawn in favor of the protocol based approach
    described in
    `NEP 22 <nep-0022-ndarray-duck-typing-overview.html>`__

Abstract
--------

We propose to add an abstract base class ``AbstractArray`` so that
third-party classes can declare their ability to "quack like" an
``ndarray``, and an ``asabstractarray`` function that performs
similarly to ``asarray`` except that it passes through
``AbstractArray`` instances unchanged.


Detailed description
--------------------

Many functions, in NumPy and in third-party packages, start with some
code like::

   def myfunc(a, b):
       a = np.asarray(a)
       b = np.asarray(b)
       ...

This ensures that ``a`` and ``b`` are ``np.ndarray`` objects, so
``myfunc`` can carry on assuming that they'll act like ndarrays both
semantically (at the Python level), and also in terms of how they're
stored in memory (at the C level). But many of these functions only
work with arrays at the Python level, which means that they don't
actually need ``ndarray`` objects *per se*: they could work just as
well with any Python object that "quacks like" an ndarray, such as
sparse arrays, dask's lazy arrays, or xarray's labeled arrays.

However, currently, there's no way for these libraries to express that
their objects can quack like an ndarray, and there's no way for
functions like ``myfunc`` to express that they'd be happy with
anything that quacks like an ndarray. The purpose of this NEP is to
provide those two features.

Sometimes people suggest using ``np.asanyarray`` for this purpose, but
unfortunately its semantics are exactly backwards: it guarantees that
the object it returns uses the same memory layout as an ``ndarray``,
but tells you nothing at all about its semantics, which makes it
essentially impossible to use safely in practice. Indeed, the two
``ndarray`` subclasses distributed with NumPy – ``np.matrix`` and
``np.ma.masked_array`` – do have incompatible semantics, and if they
were passed to a function like ``myfunc`` that doesn't check for them
as a special-case, then it may silently return incorrect results.


Declaring that an object can quack like an array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are two basic approaches we could use for checking whether an
object quacks like an array. We could check for a special attribute on
the class::

  def quacks_like_array(obj):
      return bool(getattr(type(obj), "__quacks_like_array__", False))

Or, we could define an `abstract base class (ABC)
<https://docs.python.org/3/library/collections.abc.html>`__::

  def quacks_like_array(obj):
      return isinstance(obj, AbstractArray)

If you look at how ABCs work, this is essentially equivalent to
keeping a global set of types that have been declared to implement the
``AbstractArray`` interface, and then checking it for membership.

Between these, the ABC approach seems to have a number of advantages:

* It's Python's standard, "one obvious way" of doing this.

* ABCs can be introspected (e.g. ``help(np.AbstractArray)`` does
  something useful).

* ABCs can provide useful mixin methods.

* ABCs integrate with other features like mypy type-checking,
  ``functools.singledispatch``, etc.

One obvious thing to check is whether this choice affects speed. Using
the attached benchmark script on a CPython 3.7 prerelease (revision
c4d77a661138d, self-compiled, no PGO), on a Thinkpad T450s running
Linux, we find::

    np.asarray(ndarray_obj)      330 ns
    np.asarray([])              1400 ns

    Attribute check, success      80 ns
    Attribute check, failure      80 ns

    ABC, success via subclass    340 ns
    ABC, success via register()  700 ns
    ABC, failure                 370 ns

Notes:

* The first two lines are included to put the other lines in context.

* This used 3.7 because both ``getattr`` and ABCs are receiving
  substantial optimizations in this release, and it's more
  representative of the long-term future of Python. (Failed
  ``getattr`` doesn't necessarily construct an exception object
  anymore, and ABCs were reimplemented in C.)

* The "success" lines refer to cases where ``quacks_like_array`` would
  return True. The "failure" lines are cases where it would return
  False.

* The first measurement for ABCs is subclasses defined like::

      class MyArray(AbstractArray):
          ...

  The second is for subclasses defined like::

      class MyArray:
          ...

      AbstractArray.register(MyArray)

  I don't know why there's such a large difference between these.

In practice, either way we'd only do the full test after first
checking for well-known types like ``ndarray``, ``list``, etc. `This
is how NumPy currently checks for other double-underscore attributes
<https://github.com/numpy/numpy/blob/main/numpy/core/src/private/get_attr_string.h>`__
and the same idea applies here to either approach. So these numbers
won't affect the common case, just the case where we actually have an
``AbstractArray``, or else another third-party object that will end up
going through ``__array__`` or ``__array_interface__`` or end up as an
object array.

So in summary, using an ABC will be slightly slower than using an
attribute, but this doesn't affect the most common paths, and the
magnitude of slowdown is fairly small (~250 ns on an operation that
already takes longer than that). Furthermore, we can potentially
optimize this further (e.g. by keeping a tiny LRU cache of types that
are known to be AbstractArray subclasses, on the assumption that most
code will only use one or two of these types at a time), and it's very
unclear that this even matters – if the speed of ``asarray`` no-op
pass-throughs were a bottleneck that showed up in profiles, then
probably we would have made them faster already! (It would be trivial
to fast-path this, but we don't.)

Given the semantic and usability advantages of ABCs, this seems like
an acceptable trade-off.

..
   CPython 3.6 (from Debian)::

       Attribute check, success     110 ns
       Attribute check, failure     370 ns

       ABC, success via subclass    690 ns
       ABC, success via register()  690 ns
       ABC, failure                1220 ns


Specification of ``asabstractarray``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given ``AbstractArray``, the definition of ``asabstractarray`` is simple::

  def asabstractarray(a, dtype=None):
      if isinstance(a, AbstractArray):
          if dtype is not None and dtype != a.dtype:
              return a.astype(dtype)
          return a
      return asarray(a, dtype=dtype)

Things to note:

* ``asarray`` also accepts an ``order=`` argument, but we don't
  include that here because it's about details of memory
  representation, and the whole point of this function is that you use
  it to declare that you don't care about details of memory
  representation.

* Using the ``astype`` method allows the ``a`` object to decide how to
  implement casting for its particular type.

* For strict compatibility with ``asarray``, we skip calling
  ``astype`` when the dtype is already correct. Compare::

      >>> a = np.arange(10)

      # astype() always returns a view:
      >>> a.astype(a.dtype) is a
      False

      # asarray() returns the original object if possible:
      >>> np.asarray(a, dtype=a.dtype) is a
      True


What exactly are you promising if you inherit from ``AbstractArray``?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This will presumably be refined over time. The ideal of course is that
your class should be indistinguishable from a real ``ndarray``, but
nothing enforces that except the expectations of users. In practice,
declaring that your class implements the ``AbstractArray`` interface
simply means that it will start passing through ``asabstractarray``,
and so by subclassing it you're saying that if some code works for
``ndarray``\s but breaks for your class, then you're willing to accept
bug reports on that.

To start with, we should declare ``__array_ufunc__`` to be an abstract
method, and add the ``NDArrayOperatorsMixin`` methods as mixin
methods.

Declaring ``astype`` as an ``@abstractmethod`` probably makes sense as
well, since it's used by ``asabstractarray``. We might also want to go
ahead and add some basic attributes like ``ndim``, ``shape``,
``dtype``.

Adding new abstract methods will be a bit tricky, because ABCs enforce
these at subclass time; therefore, simply adding a new
`@abstractmethod` will be a backwards compatibility break. If this
becomes a problem then we can use some hacks to implement an
`@upcoming_abstractmethod` decorator that only issues a warning if the
method is missing, and treat it like a regular deprecation cycle. (In
this case, the thing we'd be deprecating is "support for abstract
arrays that are missing feature X".)


Naming
~~~~~~

The name of the ABC doesn't matter too much, because it will only be
referenced rarely and in relatively specialized situations. The name
of the function matters a lot, because most existing instances of
``asarray`` should be replaced by this, and in the future it's what
everyone should be reaching for by default unless they have a specific
reason to use ``asarray`` instead. This suggests that its name really
should be *shorter* and *more memorable* than ``asarray``... which
is difficult. I've used ``asabstractarray`` in this draft, but I'm not
really happy with it, because it's too long and people are unlikely to
start using it by habit without endless exhortations.

One option would be to actually change ``asarray``\'s semantics so
that *it* passes through ``AbstractArray`` objects unchanged. But I'm
worried that there may be a lot of code out there that calls
``asarray`` and then passes the result into some C function that
doesn't do any further type checking (because it knows that its caller
has already used ``asarray``). If we allow ``asarray`` to return
``AbstractArray`` objects, and then someone calls one of these C
wrappers and passes it an ``AbstractArray`` object like a sparse
array, then they'll get a segfault. Right now, in the same situation,
``asarray`` will instead invoke the object's ``__array__`` method, or
use the buffer interface to make a view, or pass through an array with
object dtype, or raise an error, or similar. Probably none of these
outcomes are actually desirable in most cases, so maybe making it a
segfault instead would be OK? But it's dangerous given that we don't
know how common such code is. OTOH, if we were starting from scratch
then this would probably be the ideal solution.

We can't use ``asanyarray`` or ``array``, since those are already
taken.

Any other ideas? ``np.cast``, ``np.coerce``?


Implementation
--------------

1. Rename ``NDArrayOperatorsMixin`` to ``AbstractArray`` (leaving
   behind an alias for backwards compatibility) and make it an ABC.

2. Add ``asabstractarray`` (or whatever we end up calling it), and
   probably a C API equivalent.

3. Begin migrating NumPy internal functions to using
   ``asabstractarray`` where appropriate.


Backward compatibility
----------------------

This is purely a new feature, so there are no compatibility issues.
(Unless we decide to change the semantics of ``asarray`` itself.)


Rejected alternatives
---------------------

One suggestion that has come up is to define multiple abstract classes
for different subsets of the array interface. Nothing in this proposal
stops either NumPy or third-parties from doing this in the future, but
it's very difficult to guess ahead of time which subsets would be
useful. Also, "the full ndarray interface" is something that existing
libraries are written to expect (because they work with actual
ndarrays) and test (because they test with actual ndarrays), so it's
by far the easiest place to start.


Links to discussion
-------------------

* https://mail.python.org/pipermail/numpy-discussion/2018-March/077767.html


Appendix: Benchmark script
--------------------------

.. literalinclude:: nep-0016-benchmark.py


Copyright
---------

This document has been placed in the public domain.
=============
NumPy Roadmap
=============

This is a live snapshot of tasks and features we will be investing resources
in. It may be used to encourage and inspire developers and to search for
funding.


Interoperability
----------------

We aim to make it easier to interoperate with NumPy. There are many NumPy-like
packages that add interesting new capabilities to the Python ecosystem, as well
as many libraries that extend NumPy's model in various ways.  Work in NumPy to
facilitate interoperability with all such packages, and the code that uses them,
may include (among other things) interoperability protocols, better duck typing
support and ndarray subclass handling.

The key goal is: *make it easy for code written for NumPy to also work with
other NumPy-like projects.* This will enable GPU support via, e.g, CuPy or JAX,
distributed array support via Dask, and writing special-purpose arrays (either
from scratch, or as a ``numpy.ndarray`` subclass) that work well with SciPy,
scikit-learn and other such packages.

The ``__array_ufunc__`` and ``__array_function__`` protocols are stable, but
do not cover the whole API.  New protocols for overriding other functionality
in NumPy are needed. Work in this area aims to bring to completion one or more
of the following proposals:

- :ref:`NEP30`
- :ref:`NEP31`
- :ref:`NEP35`
- :ref:`NEP37`

In addition we aim to provide ways to make it easier for other libraries to
implement a NumPy-compatible API. This may include defining consistent subsets
of the API, as discussed in `this section of NEP 37
<https://numpy.org/neps/nep-0037-array-module.html#requesting-restricted-subsets-of-numpy-s-api>`__.


Performance
-----------

Improvements to NumPy's performance are important to many users. We have
focused this effort on Universal SIMD (see :ref:`NEP38`) intrinsics which
provide nice improvements across various hardware platforms via an abstraction
layer.  The infrastructure is in place, and we welcome follow-on PRs to add
SIMD support across all relevant NumPy functions.

Other performance improvement ideas include:

- A better story around parallel execution.
- Optimizations in individual functions.
- Reducing ufunc and ``__array_function__`` overhead.

Furthermore we would like to improve the benchmarking system, in terms of coverage,
easy of use, and publication of the results (now
`here <https://pv.github.io/numpy-bench>`__) as part of the docs or website.


Documentation and website
-------------------------

The NumPy `documentation <https://www.numpy.org/devdocs>`__ is of varying
quality. The API documentation is in good shape; tutorials and high-level
documentation on many topics are missing or outdated. See :ref:`NEP44` for
planned improvements. Adding more tutorials is underway in the
`numpy-tutorials repo <https://github.com/numpy/numpy-tutorials>`__.

Our website (https://numpy.org) was completely redesigned recently. We aim to
further improve it by adding translations, more case studies and other
high-level content, and more (see `this tracking issue <https://github.com/numpy/numpy.org/issues/266>`__).


Extensibility
-------------

We aim to make it much easier to extend NumPy. The primary topic here is to
improve the dtype system - see :ref:`NEP41` and related NEPs linked from it.
Concrete goals for the dtype system rewrite are:

- Easier custom dtypes:

  - Simplify and/or wrap the current C-API
  - More consistent support for dtype metadata
  - Support for writing a dtype in Python

- Allow adding (a) new string dtype(s). This could be encoded strings with
  fixed-width storage (e.g., ``utf8`` or ``latin1``), and/or a variable length
  string dtype. The latter could share an implementation with ``dtype=object``,
  but be explicitly type-checked.
  One of these should probably be the default for text data. The current
  string dtype support is neither efficient nor user friendly.


User experience
---------------

Type annotations
````````````````
NumPy 1.20 adds type annotations for most NumPy functionality, so users can use
tools like `mypy`_ to type check their code and IDEs can improve their support
for NumPy. Improving those type annotations, for example to support annotating
array shapes and dtypes, is ongoing.

Platform support
````````````````
We aim to increase our support for different hardware architectures. This
includes adding CI coverage when CI services are available, providing wheels on
PyPI for POWER8/9 (``ppc64le``), providing better build and install
documentation, and resolving build issues on other platforms like AIX.


Maintenance
-----------

- ``MaskedArray`` needs to be improved, ideas include:

  - Rewrite masked arrays to not be a ndarray subclass -- maybe in a separate project?
  - MaskedArray as a duck-array type, and/or
  - dtypes that support missing values

- Fortran integration via ``numpy.f2py`` requires a number of improvements, see
  `this tracking issue <https://github.com/numpy/numpy/issues/14938>`__.
- A backend system for ``numpy.fft`` (so that e.g. ``fft-mkl`` doesn't need to monkeypatch numpy).
- Write a strategy on how to deal with overlap between NumPy and SciPy for ``linalg``.
- Deprecate ``np.matrix`` (very slowly).
- Add new indexing modes for "vectorized indexing" and "outer indexing" (see :ref:`NEP21`).
- Make the polynomial API easier to use.
- Integrate an improved text file loader.
- Ufunc and gufunc improvements, see `gh-8892 <https://github.com/numpy/numpy/issues/8892>`__
  and `gh-11492 <https://github.com/numpy/numpy/issues/11492>`__.


.. _`mypy`: https://mypy.readthedocs.io
.. _NEP00:

===========================
NEP 0 — Purpose and process
===========================

:Author: Jarrod Millman <millman@berkeley.edu>
:Status: Active
:Type: Process
:Created: 2017-12-11


What is a NEP?
--------------

NEP stands for NumPy Enhancement Proposal.  A NEP is a design
document providing information to the NumPy community, or describing
a new feature for NumPy or its processes or environment.  The NEP
should provide a concise technical specification of the feature and a
rationale for the feature.

We intend NEPs to be the primary mechanisms for proposing major new
features, for collecting community input on an issue, and for
documenting the design decisions that have gone into NumPy.  The NEP
author is responsible for building consensus within the community and
documenting dissenting opinions.

Because the NEPs are maintained as text files in a versioned
repository, their revision history is the historical record of the
feature proposal [1]_.


Types
^^^^^

There are three kinds of NEPs:

1. A **Standards Track** NEP describes a new feature or implementation
   for NumPy.

2. An **Informational** NEP describes a NumPy design issue, or provides
   general guidelines or information to the Python community, but does not
   propose a new feature. Informational NEPs do not necessarily represent a
   NumPy community consensus or recommendation, so users and implementers are
   free to ignore Informational NEPs or follow their advice.

3. A **Process** NEP describes a process surrounding NumPy, or
   proposes a change to (or an event in) a process.  Process NEPs are
   like Standards Track NEPs but apply to areas other than the NumPy
   language itself.  They may propose an implementation, but not to
   NumPy's codebase; they require community consensus.  Examples include
   procedures, guidelines, changes to the decision-making process, and
   changes to the tools or environment used in NumPy development.
   Any meta-NEP is also considered a Process NEP.


NEP Workflow
------------

The NEP process begins with a new idea for NumPy.  It is highly
recommended that a single NEP contain a single key proposal or new
idea. Small enhancements or patches often don't need
a NEP and can be injected into the NumPy development workflow with a
pull request to the NumPy `repo`_. The more focused the
NEP, the more successful it tends to be.
If in doubt, split your NEP into several well-focused ones.

Each NEP must have a champion---someone who writes the NEP using the style
and format described below, shepherds the discussions in the appropriate
forums, and attempts to build community consensus around the idea.  The NEP
champion (a.k.a. Author) should first attempt to ascertain whether the idea is
suitable for a NEP. Posting to the numpy-discussion `mailing list`_ is the best
way to go about doing this.

The proposal should be submitted as a draft NEP via a `GitHub pull
request`_ to the ``doc/neps`` directory with the name ``nep-<n>.rst``
where ``<n>`` is an appropriately assigned four-digit number (e.g.,
``nep-0000.rst``). The draft must use the :doc:`nep-template` file.

Once the PR for the NEP is in place, a post should be made to the
mailing list containing the sections up to "Backward compatibility",
with the purpose of limiting discussion there to usage and impact.
Discussion on the pull request will have a broader scope, also including
details of implementation.

At the earliest convenience, the PR should be merged (regardless of
whether it is accepted during discussion).  Additional PRs may be made
by the Author to update or expand the NEP, or by maintainers to set
its status, discussion URL, etc.

Standards Track NEPs consist of two parts, a design document and a
reference implementation.  It is generally recommended that at least a
prototype implementation be co-developed with the NEP, as ideas that sound
good in principle sometimes turn out to be impractical when subjected to the
test of implementation.  Often it makes sense for the prototype implementation
to be made available as PR to the NumPy repo (making sure to appropriately
mark the PR as a WIP).


Review and Resolution
^^^^^^^^^^^^^^^^^^^^^

NEPs are discussed on the mailing list.  The possible paths of the
status of NEPs are as follows:

.. image:: _static/nep-0000.png

All NEPs should be created with the ``Draft`` status.

Eventually, after discussion, there may be a consensus that the NEP
should be accepted – see the next section for details. At this point
the status becomes ``Accepted``.

Once a NEP has been ``Accepted``, the reference implementation must be
completed.  When the reference implementation is complete and incorporated
into the main source code repository, the status will be changed to ``Final``.

To allow gathering of additional design and interface feedback before
committing to long term stability for a language feature or standard library
API, a NEP may also be marked as "Provisional". This is short for
"Provisionally Accepted", and indicates that the proposal has been accepted for
inclusion in the reference implementation, but additional user feedback is
needed before the full design can be considered "Final". Unlike regular
accepted NEPs, provisionally accepted NEPs may still be Rejected or Withdrawn
even after the related changes have been included in a Python release.

Wherever possible, it is considered preferable to reduce the scope of a
proposal to avoid the need to rely on the "Provisional" status (e.g. by
deferring some features to later NEPs), as this status can lead to version
compatibility challenges in the wider NumPy ecosystem.

A NEP can also be assigned status ``Deferred``.  The NEP author or a
core developer can assign the NEP this status when no progress is being made
on the NEP.

A NEP can also be ``Rejected``.  Perhaps after all is said and done it
was not a good idea.  It is still important to have a record of this
fact. The ``Withdrawn`` status is similar---it means that the NEP author
themselves has decided that the NEP is actually a bad idea, or has
accepted that a competing proposal is a better alternative.

When a NEP is ``Accepted``, ``Rejected``, or ``Withdrawn``, the NEP should be
updated accordingly. In addition to updating the status field, at the very
least the ``Resolution`` header should be added with a link to the relevant
thread in the mailing list archives.

NEPs can also be ``Superseded`` by a different NEP, rendering the
original obsolete.  The ``Replaced-By`` and ``Replaces`` headers
should be added to the original and new NEPs respectively.

Process NEPs may also have a status of ``Active`` if they are never
meant to be completed, e.g. NEP 0 (this NEP).


How a NEP becomes Accepted
^^^^^^^^^^^^^^^^^^^^^^^^^^

A NEP is ``Accepted`` by consensus of all interested contributors. We
need a concrete way to tell whether consensus has been reached. When
you think a NEP is ready to accept, send an email to the
numpy-discussion mailing list with a subject like:

  Proposal to accept NEP #<number>: <title>

In the body of your email, you should:

* link to the latest version of the NEP,

* briefly describe any major points of contention and how they were
  resolved,

* include a sentence like: "If there are no substantive objections
  within 7 days from this email, then the NEP will be accepted; see
  NEP 0 for more details."

For an example, see: https://mail.python.org/pipermail/numpy-discussion/2018-June/078345.html

After you send the email, you should make sure to link to the email
thread from the ``Discussion`` section of the NEP, so that people can
find it later.

Generally the NEP author will be the one to send this email, but
anyone can do it – the important thing is to make sure that everyone
knows when a NEP is on the verge of acceptance, and give them a final
chance to respond. If there's some special reason to extend this final
comment period beyond 7 days, then that's fine, just say so in the
email. You shouldn't do less than 7 days, because sometimes people are
travelling or similar and need some time to respond.

In general, the goal is to make sure that the community has consensus,
not provide a rigid policy for people to try to game. When in doubt,
err on the side of asking for more feedback and looking for
opportunities to compromise.

If the final comment period passes without any substantive objections,
then the NEP can officially be marked ``Accepted``. You should send a
followup email notifying the list (celebratory emoji optional but
encouraged 🎉✨), and then update the NEP by setting its ``:Status:``
to ``Accepted``, and its ``:Resolution:`` header to a link to your
followup email.

If there *are* substantive objections, then the NEP remains in
``Draft`` state, discussion continues as normal, and it can be
proposed for acceptance again later once the objections are resolved.

In unusual cases, the `NumPy Steering Council`_ may be asked to decide
whether a controversial NEP is ``Accepted``.


Maintenance
^^^^^^^^^^^

In general, Standards track NEPs are no longer modified after they have
reached the Final state as the code and project documentation are considered
the ultimate reference for the implemented feature.
However, finalized Standards track NEPs may be updated as needed.

Process NEPs may be updated over time to reflect changes
to development practices and other details. The precise process followed in
these cases will depend on the nature and purpose of the NEP being updated.


Format and Template
-------------------

NEPs are UTF-8 encoded text files using the reStructuredText_ format.  Please
see the :doc:`nep-template` file and the reStructuredTextPrimer_ for more
information.  We use Sphinx_ to convert NEPs to HTML for viewing on the web
[2]_.


Header Preamble
^^^^^^^^^^^^^^^

Each NEP must begin with a header preamble.  The headers
must appear in the following order.  Headers marked with ``*`` are
optional.  All other headers are required.

.. code-block:: rst

    :Author: <list of authors' real names and optionally, email addresses>
    :Status: <Draft | Active | Accepted | Deferred | Rejected |
             Withdrawn | Final | Superseded>
    :Type: <Standards Track | Process>
    :Created: <date created on, in dd-mmm-yyyy format>
  * :Requires: <nep numbers>
  * :NumPy-Version: <version number>
  * :Replaces: <nep number>
  * :Replaced-By: <nep number>
  * :Resolution: <url>

The Author header lists the names, and optionally the email addresses
of all the authors of the NEP.  The format of the Author header
value must be

.. code-block:: rst

    Random J. User <address@dom.ain>

if the email address is included, and just

.. code-block:: rst

    Random J. User

if the address is not given.  If there are multiple authors, each should be on
a separate line.


Discussion
----------

- https://mail.python.org/pipermail/numpy-discussion/2017-December/077481.html


References and Footnotes
------------------------

.. [1] This historical record is available by the normal git commands
   for retrieving older revisions, and can also be browsed on
   `GitHub <https://github.com/numpy/numpy/tree/main/doc/neps>`_.

.. [2] The URL for viewing NEPs on the web is
   https://www.numpy.org/neps/.

.. _repo: https://github.com/numpy/numpy

.. _mailing list: https://mail.python.org/mailman/listinfo/numpy-discussion

.. _issue tracker: https://github.com/numpy/numpy/issues

.. _NumPy Steering Council:
   https://docs.scipy.org/doc/numpy/dev/governance/governance.html

.. _`GitHub pull request`: https://github.com/numpy/numpy/pulls

.. _reStructuredText: http://docutils.sourceforge.net/rst.html

.. _reStructuredTextPrimer: http://www.sphinx-doc.org/en/stable/rest.html

.. _Sphinx: http://www.sphinx-doc.org/en/stable/


Copyright
---------

This document has been placed in the public domain.
.. _NEP10:

==============================================
NEP 10 — Optimizing Iterator/UFunc performance
==============================================

:Author: Mark Wiebe <mwwiebe@gmail.com>
:Content-Type: text/x-rst
:Created: 25-Nov-2010
:Status: Final

*****************
Table of contents
*****************

.. contents::

********
Abstract
********

This NEP proposes to replace the NumPy iterator and multi-iterator
with a single new iterator, designed to be more flexible and allow for
more cache-friendly data access.  The new iterator also subsumes much
of the core ufunc functionality, making it easy to get the current
ufunc benefits in contexts which don't precisely fit the ufunc mold.
Key benefits include:

* automatic reordering to find a cache-friendly access pattern
* standard and customizable broadcasting
* automatic type/byte-order/alignment conversions
* optional buffering to minimize conversion memory usage
* optional output arrays, with automatic allocation when unsupplied
* automatic output or common type selection

A large fraction of this iterator design has already been implemented with
promising results.  Construction overhead is slightly greater (a.flat:
0.5 us, nditer(a): 1.4 us and broadcast(a,b): 1.4 us, nditer([a,b]):
2.2 us), but, as shown in an example, it is already possible to improve
on the performance of the built-in NumPy mechanisms in pure Python code
together with the iterator.  One example rewrites np.add, getting a
four times improvement with some Fortran-contiguous arrays, and
another improves image compositing code from 1.4s to 180ms.

The implementation attempts to take into account
the design decisions made in the NumPy 2.0 refactor, to make its future
integration into libndarray relatively simple.

**********
Motivation
**********

NumPy defaults to returning C-contiguous arrays from UFuncs.  This can
result in extremely poor memory access patterns when dealing with data
that is structured differently.  A simple timing example illustrates
this with a more than eight times performance hit from adding
Fortran-contiguous arrays together.  All timings are done using NumPy
2.0dev (Nov 22, 2010) on an Athlon 64 X2 4200+, with a 64-bit OS.::

    In [1]: import numpy as np
    In [2]: a = np.arange(1000000,dtype=np.float32).reshape(10,10,10,10,10,10)
    In [3]: b, c, d = a.copy(), a.copy(), a.copy()

    In [4]: timeit a+b+c+d
    10 loops, best of 3: 28.5 ms per loop

    In [5]: timeit a.T+b.T+c.T+d.T
    1 loops, best of 3: 237 ms per loop

    In [6]: timeit a.T.ravel('A')+b.T.ravel('A')+c.T.ravel('A')+d.T.ravel('A')
    10 loops, best of 3: 29.6 ms per loop

In this case, it is simple to recover the performance by switching to
a view of the memory, adding, then reshaping back.  To further examine
the problem and see how it isn’t always as trivial to work around,
let’s consider simple code for working with image buffers in NumPy.

Image Compositing Example
=========================

For a more realistic example, consider an image buffer.  Images are
generally stored in a Fortran-contiguous order, and the colour
channel can be treated as either a structured 'RGB' type or an extra
dimension of length three.  The resulting memory layout is neither C-
nor Fortran-contiguous, but is easy to work with directly in NumPy,
because of the flexibility of the ndarray.  This appears ideal, because
it makes the memory layout compatible with typical C or C++ image code,
while simultaneously giving natural access in Python. Getting the color
of pixel (x,y) is just ‘image[x,y]’.

The performance of this layout in NumPy turns out to be very poor.
Here is code which creates two black images, and does an ‘over’
compositing operation on them.::

    In [9]: image1 = np.zeros((1080,1920,3), dtype=np.float32).swapaxes(0,1)
    In [10]: alpha1 = np.zeros((1080,1920,1), dtype=np.float32).swapaxes(0,1)
    In [11]: image2 = np.zeros((1080,1920,3), dtype=np.float32).swapaxes(0,1)
    In [12]: alpha2 = np.zeros((1080,1920,1), dtype=np.float32).swapaxes(0,1)
    In [13]: def composite_over(im1, al1, im2, al2):
       ....:     return (im1 + (1-al1)*im2, al1 + (1-al1)*al2)

    In [14]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 3.51 s per loop

If we give up the convenient layout, and use the C-contiguous default,
the performance is about seven times better.::

    In [16]: image1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [17]: alpha1 = np.zeros((1080,1920,1), dtype=np.float32)
    In [18]: image2 = np.zeros((1080,1920,3), dtype=np.float32)
    In [19]: alpha2 = np.zeros((1080,1920,1), dtype=np.float32)

    In [20]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 581 ms per loop

But this is not all, since it turns out that broadcasting the alpha
channel is exacting a performance price as well.  If we use an alpha
channel with 3 values instead of one, we get::

    In [21]: image1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [22]: alpha1 = np.zeros((1080,1920,3), dtype=np.float32)
    In [23]: image2 = np.zeros((1080,1920,3), dtype=np.float32)
    In [24]: alpha2 = np.zeros((1080,1920,3), dtype=np.float32)

    In [25]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 313 ms per loop

For a final comparison, let’s see how it performs when we use
one-dimensional arrays to ensure just a single loop does the
calculation.::

    In [26]: image1 = np.zeros((1080*1920*3), dtype=np.float32)
    In [27]: alpha1 = np.zeros((1080*1920*3), dtype=np.float32)
    In [28]: image2 = np.zeros((1080*1920*3), dtype=np.float32)
    In [29]: alpha2 = np.zeros((1080*1920*3), dtype=np.float32)

    In [30]: timeit composite_over(image1,alpha1,image2,alpha2)
    1 loops, best of 3: 312 ms per loop

To get a reference performance number, I implemented this simple operation
straightforwardly in C (careful to use the same compile options as NumPy).
If I emulated the memory allocation and layout of the Python code, the
performance was roughly 0.3 seconds, very much in line with NumPy’s
performance.  Combining the operations into one pass reduced the time
to roughly 0.15 seconds.

A slight variation of this example is to use a single memory block
with four channels (1920,1080,4) instead of separate image and alpha.
This is more typical in image processing applications, and here’s how
that looks with a C-contiguous layout.::

    In [31]: image1 = np.zeros((1080,1920,4), dtype=np.float32)
    In [32]: image2 = np.zeros((1080,1920,4), dtype=np.float32)
    In [33]: def composite_over(im1, im2):
       ....:     ret = (1-im1[:,:,-1])[:,:,np.newaxis]*im2
       ....:     ret += im1
       ....:     return ret

    In [34]: timeit composite_over(image1,image2)
    1 loops, best of 3: 481 ms per loop

To see the improvements that implementation of the new iterator as
proposed can produce, go to the example continued after the
proposed API, near the bottom of the document.

*************************
Improving Cache-Coherency
*************************

In order to get the best performance from UFunc calls, the pattern of
memory reads should be as regular as possible. Modern CPUs attempt to
predict the memory read/write pattern and fill the cache ahead of time.
The most predictable pattern is for all the inputs and outputs to be
sequentially processed in the same order.

I propose that by default, the memory layout of the UFunc outputs be as
close to that of the inputs as possible.  Whenever there is an ambiguity
or a mismatch, it defaults to a C-contiguous layout.

To understand how to accomplish this, we first consider the strides of
all the inputs after the shapes have been normalized for broadcasting.
By determining whether a set of strides are compatible and/or ambiguous,
we can determine an output memory layout which maximizes coherency.

In broadcasting, the input shapes are first transformed to broadcast
shapes by prepending singular dimensions, then the broadcast strides
are created, where any singular dimension’s stride is set to zero.

Strides may be negative as well, and in certain cases this can be
normalized to fit the following discussion.  If all the strides for a
particular axis are negative or zero, the strides for that dimension
can be negated after adjusting the base data pointers appropriately.

Here's an example of how three inputs with C-contiguous layouts result in
broadcast strides.  To simplify things, the examples use an itemsize of 1.

==================  ========  =======  =======
Input shapes:       (5,3,7)   (5,3,1)  (1,7)
Broadcast shapes:   (5,3,7)   (5,3,1)  (1,1,7)
Broadcast strides:  (21,7,1)  (3,1,0)  (0,0,1)
==================  ========  =======  =======

*Compatible Strides* - A set of strides are compatible if there exists
a permutation of the axes such that the strides are decreasing for every
stride in the set, excluding entries that are zero.

The example above satisfies the definition with the identity permutation.
In the motivation image example, the strides are slightly different if
we separate the colour and alpha information or not.  The permutation
which demonstrates compatibility here is the transposition (0,1).

=============================  =====================  =====================
Input/Broadcast shapes:        Image (1920, 1080, 3)  Alpha (1920, 1080, 1)
Broadcast strides (separate):  (3,5760,1)             (1,1920,0)
Broadcast strides (together):  (4,7680,1)             (4,7680,0)
=============================  =====================  =====================

*Ambiguous Strides* - A set of compatible strides are ambiguous if
more than one permutation of the axes exists such that the strides are
decreasing for every stride in the set, excluding entries that are zero.

This typically occurs when every axis has a 0-stride somewhere in the
set of strides.  The simplest example is in two dimensions, as follows.

==================  =====  =====
Broadcast shapes:   (1,3)  (5,1)
Broadcast strides:  (0,1)  (1,0)
==================  =====  =====

There may, however, be unambiguous compatible strides without a single
input forcing the entire layout, as in this example:

==================  =======  =======
Broadcast shapes:   (1,3,4)  (5,3,1)
Broadcast strides:  (0,4,1)  (3,1,0)
==================  =======  =======

In the face of ambiguity, we have a choice to either completely throw away
the fact that the strides are compatible, or try to resolve the ambiguity
by adding an additional constraint.  I think the appropriate choice
is to resolve it by picking the memory layout closest to C-contiguous,
but still compatible with the input strides.

Output Layout Selection Algorithm
=================================

The output ndarray memory layout we would like to produce is as follows:

===============================  =============================================
Consistent/Unambiguous strides:  The single consistent layout
Consistent/Ambiguous strides:    The consistent layout closest to C-contiguous
Inconsistent strides:            C-contiguous
===============================  =============================================

Here is pseudo-code for an algorithm to compute the permutation for the
output layout.::

    perm = range(ndim) # Identity, i.e. C-contiguous
    # Insertion sort, ignoring 0-strides
    # Note that the sort must be stable, and 0-strides may
    # be reordered if necessary, but should be moved as little
    # as possible.
    for i0 = 1 to ndim-1:
        # ipos is where perm[i0] will get inserted
        ipos = i0
        j0 = perm[i0]
        for i1 = i0-1 to 0:
            j1 = perm[i1]
            ambig, shouldswap = True, False
            # Check whether any strides are ordered wrong
            for strides in broadcast_strides:
                if strides[j0] != 0 and strides[j1] != 0:
                    if strides[j0] > strides[j1]:
                        # Only set swap if it's still ambiguous.
                        if ambig:
                            shouldswap = True
                    else:
                        # Set swap even if it's not ambiguous,
                        # because not swapping is the choice
                        # for conflicts as well.
                        shouldswap = False
                    ambig = False
            # If there was an unambiguous comparison, either shift ipos
            # to i1 or stop looking for the comparison
            if not ambig:
                if shouldswap:
                    ipos = i1
                else:
                    break
        # Insert perm[i0] into the right place
        if ipos != i0:
           for i1 = i0-1 to ipos:
             perm[i1+1] = perm[i1]
           perm[ipos] = j0
    # perm is now the closest consistent ordering to C-contiguous
    return perm

*********************
Coalescing Dimensions
*********************

In many cases, the memory layout allows for the use of a one-dimensional
loop instead of tracking multiple coordinates within the iterator.
The existing code already exploits this when the data is C-contiguous,
but since we're reordering the axes, we can apply this optimization
more generally.

Once the iteration strides have been sorted to be monotonically
decreasing, any dimensions which could be coalesced are side by side.
If for all the operands, incrementing by strides[i+1] shape[i+1] times
is the same as incrementing by strides[i], or strides[i+1]*shape[i+1] ==
strides[i], dimensions i and i+1 can be coalesced into a single dimension.

Here is pseudo-code for coalescing.::

    # Figure out which pairs of dimensions can be coalesced
    can_coalesce = [False]*ndim
    for strides, shape in zip(broadcast_strides, broadcast_shape):
        for i = 0 to ndim-2:
            if strides[i+1]*shape[i+1] == strides[i]:
                can_coalesce[i] = True
    # Coalesce the types
    new_ndim = ndim - count_nonzero(can_coalesce)
    for strides, shape in zip(broadcast_strides, broadcast_shape):
        j = 0
        for i = 0 to ndim-1:
            # Note that can_coalesce[ndim-1] is always False, so
            # there is no out-of-bounds access here.
            if can_coalesce[i]:
                shape[i+1] = shape[i]*shape[i+1]
            else:
                strides[j] = strides[i]
                shape[j] = shape[i]
                j += 1

*************************
Inner Loop Specialization
*************************

Specialization is handled purely by the inner loop function, so this
optimization is independent of the others.  Some specialization is
already done, like for the reduce operation.  The idea is mentioned in
http://projects.scipy.org/numpy/wiki/ProjectIdeas, “use intrinsics
(SSE-instructions) to speed up low-level loops in NumPy.”

Here are some possibilities for two-argument functions,
covering the important cases of add/subtract/multiply/divide.

* The first or second argument is a single value (i.e. a 0 stride
  value) and does not alias the output.  arr = arr + 1; arr = 1 + arr

  * Can load the constant once instead of reloading it from memory every time

* The strides match the size of the data type. C- or
  Fortran-contiguous data, for example

  * Can do a simple loop without using strides

* The strides match the size of the data type, and they are
  both 16-byte aligned (or differ from 16-byte aligned by the same offset)

  * Can use SSE to process multiple values at once

* The first input and the output are the same single value
  (i.e. a reduction operation).

  * This is already specialized for many UFuncs in the existing code

The above cases are not generally mutually exclusive, for example a
constant argument may be combined with SSE when the strides match the
data type size, and reductions can be optimized with SSE as well.

**********************
Implementation Details
**********************

Except for inner loop specialization, the discussed
optimizations significantly affect ufunc_object.c and the
PyArrayIterObject/PyArrayMultiIterObject used to do the broadcasting.
In general, it should be possible to emulate the current behavior where it
is desired, but I believe the default should be to produce and manipulate
memory layouts which will give the best performance.

To support the new cache-friendly behavior, we introduce a new
option ‘K’ (for “keep”) for any ``order=`` parameter.

The proposed ‘order=’ flags become as follows:

===  =====================================================================================
‘C’  C-contiguous layout
‘F’  Fortran-contiguous layout
‘A’  ‘F’ if the input(s) have a Fortran-contiguous layout, ‘C’ otherwise (“Any Contiguous”)
‘K’  a layout equivalent to ‘C’ followed by some permutation of the axes, as close to the layout of the input(s) as possible (“Keep Layout”)
===  =====================================================================================

Or as an enum:

.. code-block:: c

    /* For specifying array memory layout or iteration order */
    typedef enum {
            /* Fortran order if inputs are all Fortran, C otherwise */
            NPY_ANYORDER=-1,
            /* C order */
            NPY_CORDER=0,
            /* Fortran order */
            NPY_FORTRANORDER=1,
            /* An order as close to the inputs as possible */
            NPY_KEEPORDER=2
    } NPY_ORDER;


Perhaps a good strategy is to first implement the capabilities discussed
here without changing the defaults.  Once they are implemented and
well-tested, the defaults can change from ``order='C'`` to ``order='K'``
everywhere appropriate.  UFuncs additionally should gain an ``order=``
parameter to control the layout of their output(s).

The iterator can do automatic casting, and I have created a sequence
of progressively more permissive casting rules.  Perhaps for 2.0, NumPy
could adopt this enum as its preferred way of dealing with casting.

.. code-block:: c

    /* For specifying allowed casting in operations which support it */
    typedef enum {
            /* Only allow identical types */
            NPY_NO_CASTING=0,
            /* Allow identical and byte swapped types */
            NPY_EQUIV_CASTING=1,
            /* Only allow safe casts */
            NPY_SAFE_CASTING=2,
            /* Allow safe casts and casts within the same kind */
            NPY_SAME_KIND_CASTING=3,
            /* Allow any casts */
            NPY_UNSAFE_CASTING=4
    } NPY_CASTING;

Iterator Rewrite
================

Based on an analysis of the code, it appears that refactoring the existing
iteration objects to implement these optimizations is prohibitively
difficult.  Additionally, some usage of the iterator requires modifying
internal values or flags, so code using the iterator would have to
change anyway.  Thus we propose creating a new iterator object which
subsumes the existing iterator functionality and expands it to account
for the optimizations.

High level goals for the replacement iterator include:

* Small memory usage and a low number of memory allocations.
* Simple cases (like flat arrays) should have very little overhead.
* Combine single and multiple iteration into one object.

Capabilities that should be provided to user code:

* Iterate in C, Fortran, or “Fastest” (default) order.
* Track a C-style or Fortran-style flat index if requested
  (existing iterator always tracks a C-style index).  This can be done
  independently of the iteration order.
* Track the coordinates if requested (the existing iterator requires
  manually changing an internal iterator flag to guarantee this).
* Skip iteration of the last internal dimension so that it can be
  processed with an inner loop.
* Jump to a specific coordinate in the array.
* Iterate an arbitrary subset of axes (to support, for example, reduce
  with multiple axes at once).
* Ability to automatically allocate output parameters if a NULL input
  is provided,  These outputs should have a memory layout matching
  the iteration order, and are the mechanism for the ``order='K'``
  support.
* Automatic copying and/or buffering of inputs which do not satisfy
  type/byte-order/alignment requirements.  The caller's iteration inner
  loop should be the same no matter what buffering or copying is done.

Notes for implementation:

* User code must never touch the inside of the iterator. This allows
  for drastic changes of the internal memory layout in the future, if
  higher-performance implementation strategies are found.
* Use a function pointer instead of a macro for iteration.
  This way, specializations can be created for the common cases,
  like when ndim is small, for different flag settings, and when the
  number of arrays iterated is small.  Also, an iteration pattern
  can be prescribed that makes a copy of the function pointer first
  to allow the compiler to keep the function pointer
  in a register.
* Dynamically create the memory layout, to minimize the number of
  cache lines taken up by the iterator (for LP64,
  sizeof(PyArrayIterObject) is about 2.5KB, and a binary operation
  like plus needs three of these for the Multi-Iterator).
* Isolate the C-API object from Python reference counting, so that
  it can be used naturally from C.  The Python object then becomes
  a wrapper around the C iterator.  This is analogous to the
  PEP 3118 design separation of Py_buffer and memoryview.

Proposed Iterator Memory Layout
===============================

The following struct describes the iterator memory.  All items
are packed together, which means that different values of the flags,
ndim, and niter will produce slightly different layouts.

.. code-block:: c

    struct {
        /* Flags indicate what optimizations have been applied, and
         * affect the layout of this struct. */
        uint32 itflags;
        /* Number of iteration dimensions.  If FLAGS_HASCOORDS is set,
         * it matches the creation ndim, otherwise it may be smaller.  */
        uint16 ndim;
        /* Number of objects being iterated.  This is fixed at creation time. */
        uint16 niter;

        /* The number of times the iterator will iterate */
        intp itersize;

        /* The permutation is only used when FLAGS_HASCOORDS is set,
         * and is placed here so its position depends on neither ndim
         * nor niter. */
        intp perm[ndim];

        /* The data types of all the operands */
        PyArray_Descr *dtypes[niter];
        /* Backups of the starting axisdata 'ptr' values, to support Reset */
        char *resetdataptr[niter];
        /* Backup of the starting index value, to support Reset */
        npy_intp resetindex;

        /* When the iterator is destroyed, Py_XDECREF is called on all
           these objects */
        PyObject *objects[niter];

        /* Flags indicating read/write status and buffering
         * for each operand. */
        uint8 opitflags[niter];
        /* Padding to make things intp-aligned again */
        uint8 padding[];

        /* If some or all of the inputs are being buffered */
        #if (flags&FLAGS_BUFFERED)
        struct buffer_data {
            /* The size of the buffer, and which buffer we're on.
             * the i-th iteration has i = buffersize*bufferindex+pos
             */
            intp buffersize;
            /* For tracking position inside the buffer */
            intp size, pos;
            /* The strides for the pointers */
            intp stride[niter];
            /* Pointers to the data for the current iterator position.
             * The buffer_data.value ptr[i] equals either
             * axis_data[0].ptr[i] or buffer_data.buffers[i] depending
             * on whether copying to the buffer was necessary.
             */
            char* ptr[niter];
            /* Functions to do the copyswap and casting necessary */
            transferfn_t readtransferfn[niter];
            void *readtransferdata[niter];
            transferfn_t writetransferfn[niter];
            void *writetransferdata[niter];
            /* Pointers to the allocated buffers for operands
             * which the iterator determined needed buffering
             */
            char *buffers[niter];
        };
        #endif /* FLAGS_BUFFERED */

        /* Data per axis, starting with the most-frequently
         * updated, and in decreasing order after that. */
        struct axis_data {
            /* The shape of this axis */
            intp shape;
            /* The current coordinate along this axis */
            intp coord;
            /* The operand and index strides for this axis */
            intp stride[niter];
            #if (flags&FLAGS_HASINDEX)
                intp indexstride;
            #endif
            /* The operand pointers and index values for this axis */
            char* ptr[niter];
            #if (flags&FLAGS_HASINDEX)
                intp index;
            #endif
        }[ndim];
    };

The array of axis_data structs is ordered to be in increasing rapidity
of increment updates.  If the ``perm`` is the identity, this means it’s
reversed from the C-order.  This is done so data items touched
most often are closest to the beginning of the struct, where the
common properties are, resulting in increased cache coherency.
It also simplifies the iternext call, while making getcoord and
related functions slightly more complicated.

Proposed Iterator API
=====================

The existing iterator API includes functions like PyArrayIter_Check,
PyArray_Iter* and PyArray_ITER_*.  The multi-iterator array includes
PyArray_MultiIter*, PyArray_Broadcast, and PyArray_RemoveSmallest.  The
new iterator design replaces all of this functionality with a single object
and associated API.  One goal of the new API is that all uses of the
existing iterator should be replaceable with the new iterator without
significant effort.

The C-API naming convention chosen is based on the one in the numpy-refactor
branch, where libndarray has the array named ``NpyArray`` and functions
named ``NpyArray_*``.  The iterator is named ``NpyIter`` and functions are
named ``NpyIter_*``.

The Python exposure has the iterator named ``np.nditer``.  One possible
release strategy for this iterator would be to release a 1.X (1.6?) version
with the iterator added, but not used by the NumPy code.  Then, 2.0 can
be release with it fully integrated.  If this strategy is chosen, the
naming convention and API should be finalized as much as possible before
the 1.X release.  The name ``np.iter`` can't be used because it conflicts
with the Python built-in ``iter``.  I would suggest the name ``np.nditer``
within Python, as it is currently unused.

In addition to the performance goals set out for the new iterator,
it appears the API can be refactored to better support some common
NumPy programming idioms.

By moving some functionality currently in the UFunc code into the
iterator, it should make it easier for extension code which wants
to emulate UFunc behavior in cases which don't quite fit the
UFunc paradigm.  In particular, emulating the UFunc buffering behavior
is not a trivial enterprise.

Old -> New Iterator API Conversion
----------------------------------

For the regular iterator:

===============================  =============================================
``PyArray_IterNew``              ``NpyIter_New``
``PyArray_IterAllButAxis``       ``NpyIter_New`` + ``axes`` parameter **or**
                                 Iterator flag ``NPY_ITER_NO_INNER_ITERATION``
``PyArray_BroadcastToShape``     **NOT SUPPORTED** (but could be, if needed)
``PyArrayIter_Check``            Will need to add this in Python exposure
``PyArray_ITER_RESET``           ``NpyIter_Reset``
``PyArray_ITER_NEXT``            Function pointer from ``NpyIter_GetIterNext``
``PyArray_ITER_DATA``            ``NpyIter_GetDataPtrArray``
``PyArray_ITER_GOTO``            ``NpyIter_GotoCoords``
``PyArray_ITER_GOTO1D``          ``NpyIter_GotoIndex``
``PyArray_ITER_NOTDONE``         Return value of ``iternext`` function pointer
===============================  =============================================

For the multi-iterator:

===============================  =============================================
``PyArray_MultiIterNew``         ``NpyIter_MultiNew``
``PyArray_MultiIter_RESET``      ``NpyIter_Reset``
``PyArray_MultiIter_NEXT``       Function pointer from ``NpyIter_GetIterNext``
``PyArray_MultiIter_DATA``       ``NpyIter_GetDataPtrArray``
``PyArray_MultiIter_NEXTi``      **NOT SUPPORTED** (always lock-step iteration)
``PyArray_MultiIter_GOTO``       ``NpyIter_GotoCoords``
``PyArray_MultiIter_GOTO1D``     ``NpyIter_GotoIndex``
``PyArray_MultiIter_NOTDONE``    Return value of ``iternext`` function pointer
``PyArray_Broadcast``            Handled by ``NpyIter_MultiNew``
``PyArray_RemoveSmallest``       Iterator flag ``NPY_ITER_NO_INNER_ITERATION``
===============================  =============================================

For other API calls:

===============================  =============================================
``PyArray_ConvertToCommonType``  Iterator flag ``NPY_ITER_COMMON_DTYPE``
===============================  =============================================


Iterator Pointer Type
---------------------

The iterator structure is internally generated, but a type is still needed
to provide warnings and/or errors when the wrong type is passed to
the API.  We do this with a typedef of an incomplete struct

``typedef struct NpyIter_InternalOnly NpyIter;``


Construction and Destruction
----------------------------

``NpyIter* NpyIter_New(PyArrayObject* op, npy_uint32 flags, NPY_ORDER order, NPY_CASTING casting, PyArray_Descr* dtype, npy_intp a_ndim, npy_intp *axes, npy_intp buffersize)``

    Creates an iterator for the given numpy array object ``op``.

    Flags that may be passed in ``flags`` are any combination
    of the global and per-operand flags documented in
    ``NpyIter_MultiNew``, except for ``NPY_ITER_ALLOCATE``.

    Any of the ``NPY_ORDER`` enum values may be passed to ``order``.  For
    efficient iteration, ``NPY_KEEPORDER`` is the best option, and the other
    orders enforce the particular iteration pattern.

    Any of the ``NPY_CASTING`` enum values may be passed to ``casting``.
    The values include ``NPY_NO_CASTING``, ``NPY_EQUIV_CASTING``,
    ``NPY_SAFE_CASTING``, ``NPY_SAME_KIND_CASTING``, and
    ``NPY_UNSAFE_CASTING``.  To allow the casts to occur, copying or
    buffering must also be enabled.

    If ``dtype`` isn't ``NULL``, then it requires that data type.
    If copying is allowed, it will make a temporary copy if the data
    is castable.  If ``UPDATEIFCOPY`` is enabled, it will also copy
    the data back with another cast upon iterator destruction.

    If ``a_ndim`` is greater than zero, ``axes`` must also be provided.
    In this case, ``axes`` is an ``a_ndim``-sized array of ``op``'s axes.
    A value of -1 in ``axes`` means ``newaxis``. Within the ``axes``
    array, axes may not be repeated.

    If ``buffersize`` is zero, a default buffer size is used,
    otherwise it specifies how big of a buffer to use.  Buffers
    which are powers of 2 such as 512 or 1024 are recommended.

    Returns NULL if there is an error, otherwise returns the allocated
    iterator.

    To make an iterator similar to the old iterator, this should work.

    .. code-block:: c

        iter = NpyIter_New(op, NPY_ITER_READWRITE,
                            NPY_CORDER, NPY_NO_CASTING, NULL, 0, NULL);

    If you want to edit an array with aligned ``double`` code,
    but the order doesn't matter, you would use this.

    .. code-block:: c

        dtype = PyArray_DescrFromType(NPY_DOUBLE);
        iter = NpyIter_New(op, NPY_ITER_READWRITE |
                            NPY_ITER_BUFFERED |
                            NPY_ITER_NBO,
                            NPY_ITER_ALIGNED,
                            NPY_KEEPORDER,
                            NPY_SAME_KIND_CASTING,
                            dtype, 0, NULL);
        Py_DECREF(dtype);

``NpyIter* NpyIter_MultiNew(npy_intp niter, PyArrayObject** op, npy_uint32 flags, NPY_ORDER order, NPY_CASTING casting, npy_uint32 *op_flags, PyArray_Descr** op_dtypes, npy_intp oa_ndim, npy_intp **op_axes, npy_intp buffersize)``

    Creates an iterator for broadcasting the ``niter`` array objects provided
    in ``op``.

    For normal usage, use 0 for ``oa_ndim`` and NULL for ``op_axes``.
    See below for a description of these parameters, which allow for
    custom manual broadcasting as well as reordering and leaving out axes.

    Any of the ``NPY_ORDER`` enum values may be passed to ``order``.  For
    efficient iteration, ``NPY_KEEPORDER`` is the best option, and the other
    orders enforce the particular iteration pattern.  When using
    ``NPY_KEEPORDER``, if you also want to ensure that the iteration is
    not reversed along an axis, you should pass the flag
    ``NPY_ITER_DONT_NEGATE_STRIDES``.

    Any of the ``NPY_CASTING`` enum values may be passed to ``casting``.
    The values include ``NPY_NO_CASTING``, ``NPY_EQUIV_CASTING``,
    ``NPY_SAFE_CASTING``, ``NPY_SAME_KIND_CASTING``, and
    ``NPY_UNSAFE_CASTING``.  To allow the casts to occur, copying or
    buffering must also be enabled.

    If ``op_dtypes`` isn't ``NULL``, it specifies a data type or ``NULL``
    for each ``op[i]``.

    The parameter ``oa_ndim``, when non-zero, specifies the number of
    dimensions that will be iterated with customized broadcasting.
    If it is provided, ``op_axes`` must also be provided.
    These two parameters let you control in detail how the
    axes of the operand arrays get matched together and iterated.
    In ``op_axes``, you must provide an array of ``niter`` pointers
    to ``oa_ndim``-sized arrays of type ``npy_intp``.  If an entry
    in ``op_axes`` is NULL, normal broadcasting rules will apply.
    In ``op_axes[j][i]`` is stored either a valid axis of ``op[j]``, or
    -1 which means ``newaxis``.  Within each ``op_axes[j]`` array, axes
    may not be repeated.  The following example is how normal broadcasting
    applies to a 3-D array, a 2-D array, a 1-D array and a scalar.

    .. code-block:: c

        npy_intp oa_ndim = 3;               /* # iteration axes */
        npy_intp op0_axes[] = {0, 1, 2};    /* 3-D operand */
        npy_intp op1_axes[] = {-1, 0, 1};   /* 2-D operand */
        npy_intp op2_axes[] = {-1, -1, 0};  /* 1-D operand */
        npy_intp op3_axes[] = {-1, -1, -1}  /* 0-D (scalar) operand */
        npy_intp *op_axes[] = {op0_axes, op1_axes, op2_axes, op3_axes};

    If ``buffersize`` is zero, a default buffer size is used,
    otherwise it specifies how big of a buffer to use.  Buffers
    which are powers of 2 such as 512 or 1024 are recommended.

    Returns NULL if there is an error, otherwise returns the allocated
    iterator.

    Flags that may be passed in ``flags``, applying to the whole
    iterator, are:

        ``NPY_ITER_C_INDEX``, ``NPY_ITER_F_INDEX``

            Causes the iterator to track an index matching C or
            Fortran order. These options are mutually exclusive.

        ``NPY_ITER_COORDS``

            Causes the iterator to track array coordinates.
            This prevents the iterator from coalescing axes to
            produce bigger inner loops.

        ``NPY_ITER_NO_INNER_ITERATION``

            Causes the iterator to skip iteration of the innermost
            loop, allowing the user of the iterator to handle it.

            This flag is incompatible with ``NPY_ITER_C_INDEX``,
            ``NPY_ITER_F_INDEX``, and ``NPY_ITER_COORDS``.

        ``NPY_ITER_DONT_NEGATE_STRIDES``

            This only affects the iterator when NPY_KEEPORDER is specified
            for the order parameter.  By default with NPY_KEEPORDER, the
            iterator reverses axes which have negative strides, so that
            memory is traversed in a forward direction.  This disables
            this step.  Use this flag if you want to use the underlying
            memory-ordering of the axes, but don't want an axis reversed.
            This is the behavior of ``numpy.ravel(a, order='K')``, for
            instance.

        ``NPY_ITER_COMMON_DTYPE``

            Causes the iterator to convert all the operands to a common
            data type, calculated based on the ufunc type promotion rules.
            The flags for each operand must be set so that the appropriate
            casting is permitted, and copying or buffering must be enabled.

            If the common data type is known ahead of time, don't use this
            flag.  Instead, set the requested dtype for all the operands.

        ``NPY_ITER_REFS_OK``

            Indicates that arrays with reference types (object
            arrays or structured arrays containing an object type)
            may be accepted and used in the iterator.  If this flag
            is enabled, the caller must be sure to check whether
            ``NpyIter_IterationNeedsAPI(iter)`` is true, in which case
            it may not release the GIL during iteration.

        ``NPY_ITER_ZEROSIZE_OK``

            Indicates that arrays with a size of zero should be permitted.
            Since the typical iteration loop does not naturally work with
            zero-sized arrays, you must check that the IterSize is non-zero
            before entering the iteration loop.

        ``NPY_ITER_REDUCE_OK``

            Permits writeable operands with a dimension with zero
            stride and size greater than one.  Note that such operands
            must be read/write.

            When buffering is enabled, this also switches to a special
            buffering mode which reduces the loop length as necessary to
            not trample on values being reduced.

            Note that if you want to do a reduction on an automatically
            allocated output, you must use ``NpyIter_GetOperandArray``
            to get its reference, then set every value to the reduction
            unit before doing the iteration loop.  In the case of a
            buffered reduction, this means you must also specify the
            flag ``NPY_ITER_DELAY_BUFALLOC``, then reset the iterator
            after initializing the allocated operand to prepare the
            buffers.

        ``NPY_ITER_RANGED``

            Enables support for iteration of sub-ranges of the full
            ``iterindex`` range ``[0, NpyIter_IterSize(iter))``.  Use
            the function ``NpyIter_ResetToIterIndexRange`` to specify
            a range for iteration.

            This flag can only be used with ``NPY_ITER_NO_INNER_ITERATION``
            when ``NPY_ITER_BUFFERED`` is enabled.  This is because
            without buffering, the inner loop is always the size of the
            innermost iteration dimension, and allowing it to get cut up
            would require special handling, effectively making it more
            like the buffered version.

        ``NPY_ITER_BUFFERED``

            Causes the iterator to store buffering data, and use buffering
            to satisfy data type, alignment, and byte-order requirements.
            To buffer an operand, do not specify the ``NPY_ITER_COPY``
            or ``NPY_ITER_UPDATEIFCOPY`` flags, because they will
            override buffering.  Buffering is especially useful for Python
            code using the iterator, allowing for larger chunks
            of data at once to amortize the Python interpreter overhead.

            If used with ``NPY_ITER_NO_INNER_ITERATION``, the inner loop
            for the caller may get larger chunks than would be possible
            without buffering, because of how the strides are laid out.

            Note that if an operand is given the flag ``NPY_ITER_COPY``
            or ``NPY_ITER_UPDATEIFCOPY``, a copy will be made in preference
            to buffering.  Buffering will still occur when the array was
            broadcast so elements need to be duplicated to get a constant
            stride.

            In normal buffering, the size of each inner loop is equal
            to the buffer size, or possibly larger if ``NPY_ITER_GROWINNER``
            is specified.  If ``NPY_ITER_REDUCE_OK`` is enabled and
            a reduction occurs, the inner loops may become smaller depending
            on the structure of the reduction.

        ``NPY_ITER_GROWINNER``

            When buffering is enabled, this allows the size of the inner
            loop to grow when buffering isn't necessary.  This option
            is best used if you're doing a straight pass through all the
            data, rather than anything with small cache-friendly arrays
            of temporary values for each inner loop.

        ``NPY_ITER_DELAY_BUFALLOC``

            When buffering is enabled, this delays allocation of the
            buffers until one of the ``NpyIter_Reset*`` functions is
            called.  This flag exists to avoid wasteful copying of
            buffer data when making multiple copies of a buffered
            iterator for multi-threaded iteration.

            Another use of this flag is for setting up reduction operations.
            After the iterator is created, and a reduction output
            is allocated automatically by the iterator (be sure to use
            READWRITE access), its value may be initialized to the reduction
            unit.  Use ``NpyIter_GetOperandArray`` to get the object.
            Then, call ``NpyIter_Reset`` to allocate and fill the buffers
            with their initial values.

    Flags that may be passed in ``op_flags[i]``, where ``0 <= i < niter``:

        ``NPY_ITER_READWRITE``, ``NPY_ITER_READONLY``, ``NPY_ITER_WRITEONLY``

            Indicate how the user of the iterator will read or write
            to ``op[i]``.  Exactly one of these flags must be specified
            per operand.

        ``NPY_ITER_COPY``

            Allow a copy of ``op[i]`` to be made if it does not
            meet the data type or alignment requirements as specified
            by the constructor flags and parameters.

        ``NPY_ITER_UPDATEIFCOPY``

            Triggers ``NPY_ITER_COPY``, and when an array operand
            is flagged for writing and is copied, causes the data
            in a copy to be copied back to ``op[i]`` when the iterator
            is destroyed.

            If the operand is flagged as write-only and a copy is needed,
            an uninitialized temporary array will be created and then copied
            to back to ``op[i]`` on destruction, instead of doing
            the unnecessary copy operation.

        ``NPY_ITER_NBO``, ``NPY_ITER_ALIGNED``, ``NPY_ITER_CONTIG``

            Causes the iterator to provide data for ``op[i]``
            that is in native byte order, aligned according to
            the dtype requirements, contiguous, or any combination.

            By default, the iterator produces pointers into the
            arrays provided, which may be aligned or unaligned, and
            with any byte order.  If copying or buffering is not
            enabled and the operand data doesn't satisfy the constraints,
            an error will be raised.

            The contiguous constraint applies only to the inner loop,
            successive inner loops may have arbitrary pointer changes.

            If the requested data type is in non-native byte order,
            the NBO flag overrides it and the requested data type is
            converted to be in native byte order.

        ``NPY_ITER_ALLOCATE``

            This is for output arrays, and requires that the flag
            ``NPY_ITER_WRITEONLY`` be set.  If ``op[i]`` is NULL,
            creates a new array with the final broadcast dimensions,
            and a layout matching the iteration order of the iterator.

            When ``op[i]`` is NULL, the requested data type
            ``op_dtypes[i]`` may be NULL as well, in which case it is
            automatically generated from the dtypes of the arrays which
            are flagged as readable.  The rules for generating the dtype
            are the same is for UFuncs.  Of special note is handling
            of byte order in the selected dtype.  If there is exactly
            one input, the input's dtype is used as is.  Otherwise,
            if more than one input dtypes are combined together, the
            output will be in native byte order.

            After being allocated with this flag, the caller may retrieve
            the new array by calling ``NpyIter_GetOperandArray`` and
            getting the i-th object in the returned C array.  The caller
            must call Py_INCREF on it to claim a reference to the array.

        ``NPY_ITER_NO_SUBTYPE``

            For use with ``NPY_ITER_ALLOCATE``, this flag disables
            allocating an array subtype for the output, forcing
            it to be a straight ndarray.

            TODO: Maybe it would be better to introduce a function
            ``NpyIter_GetWrappedOutput`` and remove this flag?

        ``NPY_ITER_NO_BROADCAST``

            Ensures that the input or output matches the iteration
            dimensions exactly.

        ``NPY_ITER_WRITEABLE_REFERENCES``

            By default, the iterator fails on creation if the iterator
            has a writeable operand where the data type involves Python
            references.  Adding this flag indicates that the code using
            the iterator is aware of this possibility and handles it
            correctly.

``NpyIter *NpyIter_Copy(NpyIter *iter)``

    Makes a copy of the given iterator.  This function is provided
    primarily to enable multi-threaded iteration of the data.

    *TODO*: Move this to a section about multithreaded iteration.

    The recommended approach to multithreaded iteration is to
    first create an iterator with the flags
    ``NPY_ITER_NO_INNER_ITERATION``, ``NPY_ITER_RANGED``,
    ``NPY_ITER_BUFFERED``, ``NPY_ITER_DELAY_BUFALLOC``, and
    possibly ``NPY_ITER_GROWINNER``.  Create a copy of this iterator
    for each thread (minus one for the first iterator).  Then, take
    the iteration index range ``[0, NpyIter_GetIterSize(iter))`` and
    split it up into tasks, for example using a TBB parallel_for loop.
    When a thread gets a task to execute, it then uses its copy of
    the iterator by calling ``NpyIter_ResetToIterIndexRange`` and
    iterating over the full range.

    When using the iterator in multi-threaded code or in code not
    holding the Python GIL, care must be taken to only call functions
    which are safe in that context.  ``NpyIter_Copy`` cannot be safely
    called without the Python GIL, because it increments Python
    references.  The ``Reset*`` and some other functions may be safely
    called by passing in the ``errmsg`` parameter as non-NULL, so that
    the functions will pass back errors through it instead of setting
    a Python exception.

``int NpyIter_UpdateIter(NpyIter *iter, npy_intp i, npy_uint32 op_flags, NPY_CASTING casting, PyArray_Descr *dtype)`` **UNIMPLEMENTED**

    Updates the i-th operand within the iterator to possibly have a new
    data type or more restrictive flag attributes.  A use-case for
    this is to allow the automatic allocation to determine an
    output data type based on the standard NumPy type promotion rules,
    then use this function to convert the inputs and possibly the
    automatic output to a different data type during processing.

    This operation can only be done if ``NPY_ITER_COORDS`` was passed
    as a flag to the iterator.  If coordinates are not needed,
    call the function ``NpyIter_RemoveCoords()`` once no more calls to
    ``NpyIter_UpdateIter`` are needed.

    If the i-th operand has already been copied, an error is thrown.  To
    avoid this, leave all the flags out except the read/write indicators
    for any operand that later has ``NpyIter_UpdateIter`` called on it.

    The flags that may be passed in ``op_flags`` are
    ``NPY_ITER_COPY``, ``NPY_ITER_UPDATEIFCOPY``,
    ``NPY_ITER_NBO``, ``NPY_ITER_ALIGNED``, ``NPY_ITER_CONTIG``.

``int NpyIter_RemoveAxis(NpyIter *iter, npy_intp axis)``

    Removes an axis from iteration.  This requires that
    ``NPY_ITER_COORDS`` was set for iterator creation, and does not work
    if buffering is enabled or an index is being tracked. This function
    also resets the iterator to its initial state.

    This is useful for setting up an accumulation loop, for example.
    The iterator can first be created with all the dimensions, including
    the accumulation axis, so that the output gets created correctly.
    Then, the accumulation axis can be removed, and the calculation
    done in a nested fashion.

    **WARNING**: This function may change the internal memory layout of
    the iterator.  Any cached functions or pointers from the iterator
    must be retrieved again!

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.


``int NpyIter_RemoveCoords(NpyIter *iter)``

    If the iterator has coordinates, this strips support for them, and
    does further iterator optimizations that are possible if coordinates
    are not needed.  This function also resets the iterator to its initial
    state.

    **WARNING**: This function may change the internal memory layout of
    the iterator.  Any cached functions or pointers from the iterator
    must be retrieved again!

    After calling this function, ``NpyIter_HasCoords(iter)`` will
    return false.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``int NpyIter_RemoveInnerLoop(NpyIter *iter)``

    If UpdateIter/RemoveCoords was used, you may want to specify the
    flag ``NPY_ITER_NO_INNER_ITERATION``.  This flag is not permitted
    together with ``NPY_ITER_COORDS``, so this function is provided
    to enable the feature after ``NpyIter_RemoveCoords`` is called.
    This function also resets the iterator to its initial state.

    **WARNING**: This function changes the internal logic of the iterator.
    Any cached functions or pointers from the iterator must be retrieved
    again!

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``int NpyIter_Deallocate(NpyIter *iter)``

    Deallocates the iterator object.  This additionally frees any
    copies made, triggering UPDATEIFCOPY behavior where necessary.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``int NpyIter_Reset(NpyIter *iter, char **errmsg)``

    Resets the iterator back to its initial state, at the beginning
    of the iteration range.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

``int NpyIter_ResetToIterIndexRange(NpyIter *iter, npy_intp istart, npy_intp iend, char **errmsg)``

    Resets the iterator and restricts it to the ``iterindex`` range
    ``[istart, iend)``.  See ``NpyIter_Copy`` for an explanation of
    how to use this for multi-threaded iteration.  This requires that
    the flag ``NPY_ITER_RANGED`` was passed to the iterator constructor.

    If you want to reset both the ``iterindex`` range and the base
    pointers at the same time, you can do the following to avoid
    extra buffer copying (be sure to add the return code error checks
    when you copy this code).

    .. code-block:: c

        /* Set to a trivial empty range */
        NpyIter_ResetToIterIndexRange(iter, 0, 0);
        /* Set the base pointers */
        NpyIter_ResetBasePointers(iter, baseptrs);
        /* Set to the desired range */
        NpyIter_ResetToIterIndexRange(iter, istart, iend);

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

``int NpyIter_ResetBasePointers(NpyIter *iter, char **baseptrs, char **errmsg)``

    Resets the iterator back to its initial state, but using the values
    in ``baseptrs`` for the data instead of the pointers from the arrays
    being iterated.  This functions is intended to be used, together with
    the ``op_axes`` parameter, by nested iteration code with two or more
    iterators.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

    *TODO*: Move the following into a special section on nested iterators.

    Creating iterators for nested iteration requires some care.  All
    the iterator operands must match exactly, or the calls to
    ``NpyIter_ResetBasePointers`` will be invalid.  This means that
    automatic copies and output allocation should not be used haphazardly.
    It is possible to still use the automatic data conversion and casting
    features of the iterator by creating one of the iterators with
    all the conversion parameters enabled, then grabbing the allocated
    operands with the ``NpyIter_GetOperandArray`` function and passing
    them into the constructors for the rest of the iterators.

    **WARNING**: When creating iterators for nested iteration,
    the code must not use a dimension more than once in the different
    iterators.  If this is done, nested iteration will produce
    out-of-bounds pointers during iteration.

    **WARNING**: When creating iterators for nested iteration, buffering
    can only be applied to the innermost iterator.  If a buffered iterator
    is used as the source for ``baseptrs``, it will point into a small buffer
    instead of the array and the inner iteration will be invalid.

    The pattern for using nested iterators is as follows:

    .. code-block:: c

        NpyIter *iter1, *iter1;
        NpyIter_IterNext_Fn iternext1, iternext2;
        char **dataptrs1;

        /*
         * With the exact same operands, no copies allowed, and
         * no axis in op_axes used both in iter1 and iter2.
         * Buffering may be enabled for iter2, but not for iter1.
         */
        iter1 = ...; iter2 = ...;

        iternext1 = NpyIter_GetIterNext(iter1);
        iternext2 = NpyIter_GetIterNext(iter2);
        dataptrs1 = NpyIter_GetDataPtrArray(iter1);

        do {
            NpyIter_ResetBasePointers(iter2, dataptrs1);
            do {
                /* Use the iter2 values */
            } while (iternext2(iter2));
        } while (iternext1(iter1));

``int NpyIter_GotoCoords(NpyIter *iter, npy_intp *coords)``

    Adjusts the iterator to point to the ``ndim`` coordinates
    pointed to by ``coords``.  Returns an error if coordinates
    are not being tracked, the coordinates are out of bounds,
    or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``int NpyIter_GotoIndex(NpyIter *iter, npy_intp index)``

    Adjusts the iterator to point to the ``index`` specified.
    If the iterator was constructed with the flag
    ``NPY_ITER_C_INDEX``, ``index`` is the C-order index,
    and if the iterator was constructed with the flag
    ``NPY_ITER_F_INDEX``, ``index`` is the Fortran-order
    index.  Returns an error if there is no index being tracked,
    the index is out of bounds, or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``npy_intp NpyIter_GetIterSize(NpyIter *iter)``

    Returns the number of elements being iterated.  This is the product
    of all the dimensions in the shape.

``npy_intp NpyIter_GetReduceBlockSizeFactor(NpyIter *iter)`` **UNIMPLEMENTED**

    This provides a factor that must divide into the blocksize used
    for ranged iteration to safely multithread a reduction.  If
    the iterator has no reduction, it returns 1.

    When using ranged iteration to multithread a reduction, there are
    two possible ways to do the reduction:

    If there is a big reduction to a small output, make a temporary
    array initialized to the reduction unit for each thread, then have
    each thread reduce into its temporary.  When that is complete,
    combine the temporaries together.  You can detect this case by
    observing that ``NpyIter_GetReduceBlockSizeFactor`` returns a
    large value, for instance half or a third of ``NpyIter_GetIterSize``.
    You should also check that the output is small just to be sure.

    If there are many small reductions to a big output, and the reduction
    dimensions are inner dimensions, ``NpyIter_GetReduceBlockSizeFactor``
    will return a small number, and as long as the block size you choose
    for multithreading is ``NpyIter_GetReduceBlockSizeFactor(iter)*n``
    for some ``n``, the operation will be safe.

    The bad case is when the a reduction dimension is the outermost
    loop in the iterator.  For example, if you have a C-order
    array with shape (3,1000,1000), and you reduce on dimension 0,
    ``NpyIter_GetReduceBlockSizeFactor`` will return a size equal to
    ``NpyIter_GetIterSize`` for ``NPY_KEEPORDER`` or ``NPY_CORDER``
    iteration orders.  While it is bad for the CPU cache, perhaps
    in the future another order possibility could be provided, maybe
    ``NPY_REDUCEORDER``, which pushes the reduction axes to the inner
    loop, but otherwise is the same as ``NPY_KEEPORDER``.

``npy_intp NpyIter_GetIterIndex(NpyIter *iter)``

    Gets the ``iterindex`` of the iterator, which is an index matching
    the iteration order of the iterator.

``void NpyIter_GetIterIndexRange(NpyIter *iter, npy_intp *istart, npy_intp *iend)``

    Gets the ``iterindex`` sub-range that is being iterated.  If
    ``NPY_ITER_RANGED`` was not specified, this always returns the
    range ``[0, NpyIter_IterSize(iter))``.

``int NpyIter_GotoIterIndex(NpyIter *iter, npy_intp iterindex)``

    Adjusts the iterator to point to the ``iterindex`` specified.
    The IterIndex is an index matching the iteration order of the iterator.
    Returns an error if the ``iterindex`` is out of bounds,
    buffering is enabled, or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``int NpyIter_HasInnerLoop(NpyIter *iter)``

    Returns 1 if the iterator handles the inner loop,
    or 0 if the caller needs to handle it.  This is controlled
    by the constructor flag ``NPY_ITER_NO_INNER_ITERATION``.

``int NpyIter_HasCoords(NpyIter *iter)``

    Returns 1 if the iterator was created with the
    ``NPY_ITER_COORDS`` flag, 0 otherwise.

``int NpyIter_HasIndex(NpyIter *iter)``

    Returns 1 if the iterator was created with the
    ``NPY_ITER_C_INDEX`` or ``NPY_ITER_F_INDEX``
    flag, 0 otherwise.

``int NpyIter_IsBuffered(NpyIter *iter)``

    Returns 1 if the iterator was created with the
    ``NPY_ITER_BUFFERED`` flag, 0 otherwise.

``int NpyIter_IsGrowInner(NpyIter *iter)``

    Returns 1 if the iterator was created with the
    ``NPY_ITER_GROWINNER`` flag, 0 otherwise.

``npy_intp NpyIter_GetBufferSize(NpyIter *iter)``

    If the iterator is buffered, returns the size of the buffer
    being used, otherwise returns 0.

``npy_intp NpyIter_GetNDim(NpyIter *iter)``

    Returns the number of dimensions being iterated.  If coordinates
    were not requested in the iterator constructor, this value
    may be smaller than the number of dimensions in the original
    objects.

``npy_intp NpyIter_GetNIter(NpyIter *iter)``

    Returns the number of objects being iterated.

``npy_intp *NpyIter_GetAxisStrideArray(NpyIter *iter, npy_intp axis)``

    Gets the array of strides for the specified axis. Requires that
    the iterator be tracking coordinates, and that buffering not
    be enabled.

    This may be used when you want to match up operand axes in
    some fashion, then remove them with ``NpyIter_RemoveAxis`` to
    handle their processing manually.  By calling this function
    before removing the axes, you can get the strides for the
    manual processing.

    Returns ``NULL`` on error.

``int NpyIter_GetShape(NpyIter *iter, npy_intp *outshape)``

    Returns the broadcast shape of the iterator in ``outshape``.
    This can only be called on an iterator which supports coordinates.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

``PyArray_Descr **NpyIter_GetDescrArray(NpyIter *iter)``

    This gives back a pointer to the ``niter`` data type Descrs for
    the objects being iterated.  The result points into ``iter``,
    so the caller does not gain any references to the Descrs.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it.

``PyObject **NpyIter_GetOperandArray(NpyIter *iter)``

    This gives back a pointer to the ``niter`` operand PyObjects
    that are being iterated.  The result points into ``iter``,
    so the caller does not gain any references to the PyObjects.

``PyObject *NpyIter_GetIterView(NpyIter *iter, npy_intp i)``

    This gives back a reference to a new ndarray view, which is a view
    into the i-th object in the array ``NpyIter_GetOperandArray()``,
    whose dimensions and strides match the internal optimized
    iteration pattern.  A C-order iteration of this view is equivalent
    to the iterator's iteration order.

    For example, if an iterator was created with a single array as its
    input, and it was possible to rearrange all its axes and then
    collapse it into a single strided iteration, this would return
    a view that is a one-dimensional array.

``void NpyIter_GetReadFlags(NpyIter *iter, char *outreadflags)``

    Fills ``niter`` flags. Sets ``outreadflags[i]`` to 1 if
    ``op[i]`` can be read from, and to 0 if not.

``void NpyIter_GetWriteFlags(NpyIter *iter, char *outwriteflags)``

    Fills ``niter`` flags. Sets ``outwriteflags[i]`` to 1 if
    ``op[i]`` can be written to, and to 0 if not.

Functions For Iteration
-----------------------

``NpyIter_IterNext_Fn NpyIter_GetIterNext(NpyIter *iter, char **errmsg)``

    Returns a function pointer for iteration.  A specialized version
    of the function pointer may be calculated by this function
    instead of being stored in the iterator structure. Thus, to
    get good performance, it is required that the function pointer
    be saved in a variable rather than retrieved for each loop iteration.

    Returns NULL if there is an error.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

    The typical looping construct is as follows:

    .. code-block:: c

        NpyIter_IterNext_Fn iternext = NpyIter_GetIterNext(iter, NULL);
        char **dataptr = NpyIter_GetDataPtrArray(iter);

        do {
            /* use the addresses dataptr[0], ... dataptr[niter-1] */
        } while(iternext(iter));

    When ``NPY_ITER_NO_INNER_ITERATION`` is specified, the typical
    inner loop construct is as follows:

    .. code-block:: c

        NpyIter_IterNext_Fn iternext = NpyIter_GetIterNext(iter, NULL);
        char **dataptr = NpyIter_GetDataPtrArray(iter);
        npy_intp *stride = NpyIter_GetInnerStrideArray(iter);
        npy_intp *size_ptr = NpyIter_GetInnerLoopSizePtr(iter), size;
        npy_intp iiter, niter = NpyIter_GetNIter(iter);

        do {
            size = *size_ptr;
            while (size--) {
                /* use the addresses dataptr[0], ... dataptr[niter-1] */
                for (iiter = 0; iiter < niter; ++iiter) {
                    dataptr[iiter] += stride[iiter];
                }
            }
        } while (iternext());

    Observe that we are using the dataptr array inside the iterator, not
    copying the values to a local temporary.  This is possible because
    when ``iternext()`` is called, these pointers will be overwritten
    with fresh values, not incrementally updated.

    If a compile-time fixed buffer is being used (both flags
    ``NPY_ITER_BUFFERED`` and ``NPY_ITER_NO_INNER_ITERATION``), the
    inner size may be used as a signal as well.  The size is guaranteed
    to become zero when ``iternext()`` returns false, enabling the
    following loop construct.  Note that if you use this construct,
    you should not pass ``NPY_ITER_GROWINNER`` as a flag, because it
    will cause larger sizes under some circumstances:

    .. code-block:: c

        /* The constructor should have buffersize passed as this value */
        #define FIXED_BUFFER_SIZE 1024

        NpyIter_IterNext_Fn iternext = NpyIter_GetIterNext(iter, NULL);
        char **dataptr = NpyIter_GetDataPtrArray(iter);
        npy_intp *stride = NpyIter_GetInnerStrideArray(iter);
        npy_intp *size_ptr = NpyIter_GetInnerLoopSizePtr(iter), size;
        npy_intp i, iiter, niter = NpyIter_GetNIter(iter);

        /* One loop with a fixed inner size */
        size = *size_ptr;
        while (size == FIXED_BUFFER_SIZE) {
            /*
             * This loop could be manually unrolled by a factor
             * which divides into FIXED_BUFFER_SIZE
             */
            for (i = 0; i < FIXED_BUFFER_SIZE; ++i) {
                /* use the addresses dataptr[0], ... dataptr[niter-1] */
                for (iiter = 0; iiter < niter; ++iiter) {
                    dataptr[iiter] += stride[iiter];
                }
            }
            iternext();
            size = *size_ptr;
        }

        /* Finish-up loop with variable inner size */
        if (size > 0) do {
            size = *size_ptr;
            while (size--) {
                /* use the addresses dataptr[0], ... dataptr[niter-1] */
                for (iiter = 0; iiter < niter; ++iiter) {
                    dataptr[iiter] += stride[iiter];
                }
            }
        } while (iternext());

``NpyIter_GetCoords_Fn NpyIter_GetGetCoords(NpyIter *iter, char **errmsg)``

    Returns a function pointer for getting the coordinates
    of the iterator.  Returns NULL if the iterator does not
    support coordinates.  It is recommended that this function
    pointer be cached in a local variable before the iteration
    loop.

    Returns NULL if there is an error.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

``char **NpyIter_GetDataPtrArray(NpyIter *iter)``

    This gives back a pointer to the ``niter`` data pointers.  If
    ``NPY_ITER_NO_INNER_ITERATION`` was not specified, each data
    pointer points to the current data item of the iterator.  If
    no inner iteration was specified, it points to the first data
    item of the inner loop.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it.  This function may be safely
    called without holding the Python GIL.

``npy_intp *NpyIter_GetIndexPtr(NpyIter *iter)``

    This gives back a pointer to the index being tracked, or NULL
    if no index is being tracked.  It is only usable if one of
    the flags ``NPY_ITER_C_INDEX`` or ``NPY_ITER_F_INDEX``
    were specified during construction.

When the flag ``NPY_ITER_NO_INNER_ITERATION`` is used, the code
needs to know the parameters for doing the inner loop.  These
functions provide that information.

``npy_intp *NpyIter_GetInnerStrideArray(NpyIter *iter)``

    Returns a pointer to an array of the ``niter`` strides,
    one for each iterated object, to be used by the inner loop.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it. This function may be safely
    called without holding the Python GIL.

``npy_intp* NpyIter_GetInnerLoopSizePtr(NpyIter *iter)``

    Returns a pointer to the number of iterations the
    inner loop should execute.

    This address may be cached before the iteration loop, calling
    ``iternext`` will not change it.  The value itself may change during
    iteration, in particular if buffering is enabled.  This function
    may be safely called without holding the Python GIL.

``void NpyIter_GetInnerFixedStrideArray(NpyIter *iter, npy_intp *out_strides)``

    Gets an array of strides which are fixed, or will not change during
    the entire iteration.  For strides that may change, the value
    NPY_MAX_INTP is placed in the stride.

    Once the iterator is prepared for iteration (after a reset if
    ``NPY_DELAY_BUFALLOC`` was used), call this to get the strides
    which may be used to select a fast inner loop function.  For example,
    if the stride is 0, that means the inner loop can always load its
    value into a variable once, then use the variable throughout the loop,
    or if the stride equals the itemsize, a contiguous version for that
    operand may be used.

    This function may be safely called without holding the Python GIL.

Examples
--------

A copy function using the iterator.  The ``order`` parameter
is used to control the memory layout of the allocated
result.

If the input is a reference type, this function will fail.
To fix this, the code must be changed to specially handle writeable
references, and add ``NPY_ITER_WRITEABLE_REFERENCES`` to the flags:

.. code-block:: c

    /* NOTE: This code has not been compiled/tested */
    PyObject *CopyArray(PyObject *arr, NPY_ORDER order)
    {
        NpyIter *iter;
        NpyIter_IterNext_Fn iternext;
        PyObject *op[2], *ret;
        npy_uint32 flags;
        npy_uint32 op_flags[2];
        npy_intp itemsize, *innersizeptr, innerstride;
        char **dataptrarray;

        /*
         * No inner iteration - inner loop is handled by CopyArray code
         */
        flags = NPY_ITER_NO_INNER_ITERATION;
        /*
         * Tell the constructor to automatically allocate the output.
         * The data type of the output will match that of the input.
         */
        op[0] = arr;
        op[1] = NULL;
        op_flags[0] = NPY_ITER_READONLY;
        op_flags[1] = NPY_ITER_WRITEONLY | NPY_ITER_ALLOCATE;

        /* Construct the iterator */
        iter = NpyIter_MultiNew(2, op, flags, order, NPY_NO_CASTING,
                                op_flags, NULL, 0, NULL);
        if (iter == NULL) {
            return NULL;
        }

        /*
         * Make a copy of the iternext function pointer and
         * a few other variables the inner loop needs.
         */
        iternext = NpyIter_GetIterNext(iter);
        innerstride = NpyIter_GetInnerStrideArray(iter)[0];
        itemsize = NpyIter_GetDescrArray(iter)[0]->elsize;
        /*
         * The inner loop size and data pointers may change during the
         * loop, so just cache the addresses.
         */
        innersizeptr = NpyIter_GetInnerLoopSizePtr(iter);
        dataptrarray = NpyIter_GetDataPtrArray(iter);

        /*
         * Note that because the iterator allocated the output,
         * it matches the iteration order and is packed tightly,
         * so we don't need to check it like the input.
         */
        if (innerstride == itemsize) {
            do {
                memcpy(dataptrarray[1], dataptrarray[0],
                                        itemsize * (*innersizeptr));
            } while (iternext(iter));
        } else {
            /* Should specialize this further based on item size... */
            npy_intp i;
            do {
                npy_intp size = *innersizeptr;
                char *src = dataaddr[0], *dst = dataaddr[1];
                for(i = 0; i < size; i++, src += innerstride, dst += itemsize) {
                    memcpy(dst, src, itemsize);
                }
            } while (iternext(iter));
        }

        /* Get the result from the iterator object array */
        ret = NpyIter_GetOperandArray(iter)[1];
        Py_INCREF(ret);

        if (NpyIter_Deallocate(iter) != NPY_SUCCEED) {
            Py_DECREF(ret);
            return NULL;
        }

        return ret;
    }

Python Lambda UFunc Example
---------------------------

To show how the new iterator allows the definition of efficient UFunc-like
functions in pure Python, we demonstrate the function ``luf``, which
makes a lambda-expression act like a UFunc.  This is very similar to the
``numexpr`` library, but only takes a few lines of code.

First, here is the definition of the ``luf`` function.::

    def luf(lamdaexpr, *args, **kwargs):
        """Lambda UFunc

            e.g.
            c = luf(lambda i,j:i+j, a, b, order='K',
                                casting='safe', buffersize=8192)

            c = np.empty(...)
            luf(lambda i,j:i+j, a, b, out=c, order='K',
                                casting='safe', buffersize=8192)
        """

        nargs = len(args)
        op = args + (kwargs.get('out',None),)
        it = np.nditer(op, ['buffered','no_inner_iteration'],
                [['readonly','nbo_aligned']]*nargs +
                                [['writeonly','allocate','no_broadcast']],
                order=kwargs.get('order','K'),
                casting=kwargs.get('casting','safe'),
                buffersize=kwargs.get('buffersize',0))
        while not it.finished:
            it[-1] = lamdaexpr(*it[:-1])
            it.iternext()

        return it.operands[-1]

Then, by using ``luf`` instead of straight Python expressions, we
can gain some performance from better cache behavior.::

    In [2]: a = np.random.random((50,50,50,10))
    In [3]: b = np.random.random((50,50,1,10))
    In [4]: c = np.random.random((50,50,50,1))

    In [5]: timeit 3*a+b-(a/c)
    1 loops, best of 3: 138 ms per loop

    In [6]: timeit luf(lambda a,b,c:3*a+b-(a/c), a, b, c)
    10 loops, best of 3: 60.9 ms per loop

    In [7]: np.all(3*a+b-(a/c) == luf(lambda a,b,c:3*a+b-(a/c), a, b, c))
    Out[7]: True


Python Addition Example
-----------------------

The iterator has been mostly written and exposed to Python.  To
see how it behaves, let's see what we can do with the np.add ufunc.
Even without changing the core of NumPy, we will be able to use
the iterator to make a faster add function.

The Python exposure supplies two iteration interfaces, one which
follows the Python iterator protocol, and another which mirrors the
C-style do-while pattern.  The native Python approach is better
in most cases, but if you need the iterator's coordinates or
index, use the C-style pattern.

Here is how we might write an ``iter_add`` function, using the
Python iterator protocol.::

    def iter_add_py(x, y, out=None):
        addop = np.add

        it = np.nditer([x,y,out], [],
                    [['readonly'],['readonly'],['writeonly','allocate']])

        for (a, b, c) in it:
            addop(a, b, c)

        return it.operands[2]

Here is the same function, but following the C-style pattern.::

    def iter_add(x, y, out=None):
        addop = np.add

        it = np.nditer([x,y,out], [],
                    [['readonly'],['readonly'],['writeonly','allocate']])

        while not it.finished:
            addop(it[0], it[1], it[2])
            it.iternext()

        return it.operands[2]

Some noteworthy points about this function:

* Cache np.add as a local variable to reduce namespace lookups
* Inputs are readonly, output is writeonly, and will be allocated
  automatically if it is None.
* Uses np.add's out parameter to avoid an extra copy.

Let's create some test variables, and time this function as well as the
built-in np.add.::

    In [1]: a = np.arange(1000000,dtype='f4').reshape(100,100,100)
    In [2]: b = np.arange(10000,dtype='f4').reshape(1,100,100)
    In [3]: c = np.arange(10000,dtype='f4').reshape(100,100,1)

    In [4]: timeit iter_add(a, b)
    1 loops, best of 3: 7.03 s per loop

    In [5]: timeit np.add(a, b)
    100 loops, best of 3: 6.73 ms per loop

At a thousand times slower, this is clearly not very good.  One feature
of the iterator, designed to help speed up the inner loops, is the flag
``no_inner_iteration``.  This is the same idea as the old iterator's
``PyArray_IterAllButAxis``, but slightly smarter.  Let's modify
``iter_add`` to use this feature.::

    def iter_add_noinner(x, y, out=None):
        addop = np.add

        it = np.nditer([x,y,out], ['no_inner_iteration'],
                    [['readonly'],['readonly'],['writeonly','allocate']])

        for (a, b, c) in it:
            addop(a, b, c)

        return it.operands[2]

The performance improves dramatically.::

    In[6]: timeit iter_add_noinner(a, b)
    100 loops, best of 3: 7.1 ms per loop

The performance is basically as good as the built-in function!  It
turns out this is because the iterator was able to coalesce the last two
dimensions, resulting in 100 adds of 10000 elements each.  If the
inner loop doesn't become as large, the performance doesn't improve
as dramatically.  Let's use ``c`` instead of ``b`` to see how this works.::

    In[7]: timeit iter_add_noinner(a, c)
    10 loops, best of 3: 76.4 ms per loop

It's still a lot better than seven seconds, but still over ten times worse
than the built-in function.  Here, the inner loop has 100 elements,
and it's iterating 10000 times.  If we were coding in C, our performance
would already be as good as the built-in performance, but in Python
there is too much overhead.

This leads us to another feature of the iterator, its ability to give
us views of the iterated memory.  The views it gives us are structured
so that processing them in C-order, like the built-in NumPy code does,
gives the same access order as the iterator itself.  Effectively, we
are using the iterator to solve for a good memory access pattern, then
using other NumPy machinery to efficiently execute it.  Let's
modify ``iter_add`` once again.::

    def iter_add_itview(x, y, out=None):
        it = np.nditer([x,y,out], [],
                    [['readonly'],['readonly'],['writeonly','allocate']])

        (a, b, c) = it.itviews
        np.add(a, b, c)

        return it.operands[2]

Now the performance pretty closely matches the built-in function's.::

    In [8]: timeit iter_add_itview(a, b)
    100 loops, best of 3: 6.18 ms per loop

    In [9]: timeit iter_add_itview(a, c)
    100 loops, best of 3: 6.69 ms per loop

Let us now step back to a case similar to the original motivation for the
new iterator.  Here are the same calculations in Fortran memory order instead
Of C memory order.::

    In [10]: a = np.arange(1000000,dtype='f4').reshape(100,100,100).T
    In [12]: b = np.arange(10000,dtype='f4').reshape(100,100,1).T
    In [11]: c = np.arange(10000,dtype='f4').reshape(1,100,100).T

    In [39]: timeit np.add(a, b)
    10 loops, best of 3: 34.3 ms per loop

    In [41]: timeit np.add(a, c)
    10 loops, best of 3: 31.6 ms per loop

    In [44]: timeit iter_add_itview(a, b)
    100 loops, best of 3: 6.58 ms per loop

    In [43]: timeit iter_add_itview(a, c)
    100 loops, best of 3: 6.33 ms per loop

As you can see, the performance of the built-in function dropped
significantly, but our newly-written add function maintained essentially
the same performance.  As one final test, let's try several adds chained
together.::

    In [4]: timeit np.add(np.add(np.add(a,b), c), a)
    1 loops, best of 3: 99.5 ms per loop

    In [9]: timeit iter_add_itview(iter_add_itview(iter_add_itview(a,b), c), a)
    10 loops, best of 3: 29.3 ms per loop

Also, just to check that it's doing the same thing,::

    In [22]: np.all(
       ....: iter_add_itview(iter_add_itview(iter_add_itview(a,b), c), a) ==
       ....: np.add(np.add(np.add(a,b), c), a)
       ....: )

    Out[22]: True

Image Compositing Example Revisited
-----------------------------------

For motivation, we had an example that did an 'over' composite operation
on two images.  Now let's see how we can write the function with
the new iterator.

Here is one of the original functions, for reference, and some
random image data.::

    In [5]: rand1 = np.random.random(1080*1920*4).astype(np.float32)
    In [6]: rand2 = np.random.random(1080*1920*4).astype(np.float32)
    In [7]: image1 = rand1.reshape(1080,1920,4).swapaxes(0,1)
    In [8]: image2 = rand2.reshape(1080,1920,4).swapaxes(0,1)

    In [3]: def composite_over(im1, im2):
      ....:     ret = (1-im1[:,:,-1])[:,:,np.newaxis]*im2
      ....:     ret += im1
      ....:     return ret

    In [4]: timeit composite_over(image1,image2)
    1 loops, best of 3: 1.39 s per loop

Here's the same function, rewritten to use a new iterator.  Note how
easy it was to add an optional output parameter.::

    In [5]: def composite_over_it(im1, im2, out=None, buffersize=4096):
      ....:     it = np.nditer([im1, im1[:,:,-1], im2, out],
      ....:                     ['buffered','no_inner_iteration'],
      ....:                     [['readonly']]*3+[['writeonly','allocate']],
      ....:                     op_axes=[None,[0,1,np.newaxis],None,None],
      ....:                     buffersize=buffersize)
      ....:     while not it.finished:
      ....:         np.multiply(1-it[1], it[2], it[3])
      ....:         it[3] += it[0]
      ....:         it.iternext()
      ....:     return it.operands[3]

    In [6]: timeit composite_over_it(image1, image2)
    1 loops, best of 3: 197 ms per loop

A big speed improvement, over even the best previous attempt using
straight NumPy and a C-order array!  By playing with the buffer size, we can
see how the speed improves until we hit the limits of the CPU cache
in the inner loop.::

    In [7]: timeit composite_over_it(image1, image2, buffersize=2**7)
    1 loops, best of 3: 1.23 s per loop

    In [8]: timeit composite_over_it(image1, image2, buffersize=2**8)
    1 loops, best of 3: 699 ms per loop

    In [9]: timeit composite_over_it(image1, image2, buffersize=2**9)
    1 loops, best of 3: 418 ms per loop

    In [10]: timeit composite_over_it(image1, image2, buffersize=2**10)
    1 loops, best of 3: 287 ms per loop

    In [11]: timeit composite_over_it(image1, image2, buffersize=2**11)
    1 loops, best of 3: 225 ms per loop

    In [12]: timeit composite_over_it(image1, image2, buffersize=2**12)
    1 loops, best of 3: 194 ms per loop

    In [13]: timeit composite_over_it(image1, image2, buffersize=2**13)
    1 loops, best of 3: 180 ms per loop

    In [14]: timeit composite_over_it(image1, image2, buffersize=2**14)
    1 loops, best of 3: 192 ms per loop

    In [15]: timeit composite_over_it(image1, image2, buffersize=2**15)
    1 loops, best of 3: 280 ms per loop

    In [16]: timeit composite_over_it(image1, image2, buffersize=2**16)
    1 loops, best of 3: 328 ms per loop

    In [17]: timeit composite_over_it(image1, image2, buffersize=2**17)
    1 loops, best of 3: 345 ms per loop

And finally, to double check that it's working, we can compare the two
functions.::

    In [18]: np.all(composite_over(image1, image2) ==
        ...:        composite_over_it(image1, image2))
    Out[18]: True

Image Compositing With NumExpr
------------------------------

As a test of the iterator, numexpr has been enhanced to allow use of
the iterator instead of its internal broadcasting code.  First, let's
implement the composite operation with numexpr.::

    In [22]: def composite_over_ne(im1, im2, out=None):
       ....:     ima = im1[:,:,-1][:,:,np.newaxis]
       ....:     return ne.evaluate("im1+(1-ima)*im2")

    In [23]: timeit composite_over_ne(image1,image2)
    1 loops, best of 3: 1.25 s per loop

This beats the straight NumPy operation, but isn't very good.  Switching
to the iterator version of numexpr, we get a big improvement over the
straight Python function using the iterator.  Note that this is on
a dual core machine.::

    In [29]: def composite_over_ne_it(im1, im2, out=None):
       ....:     ima = im1[:,:,-1][:,:,np.newaxis]
       ....:     return ne.evaluate_iter("im1+(1-ima)*im2")

    In [30]: timeit composite_over_ne_it(image1,image2)
    10 loops, best of 3: 67.2 ms per loop

    In [31]: ne.set_num_threads(1)
    In [32]: timeit composite_over_ne_it(image1,image2)
    10 loops, best of 3: 91.1 ms per loop
.. _NEP34:

===========================================================
NEP 34 — Disallow inferring ``dtype=object`` from sequences
===========================================================

:Author: Matti Picus
:Status: Accepted
:Type: Standards Track
:Created: 2019-10-10
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2019-October/080200.html

Abstract
--------

When users create arrays with sequences-of-sequences, they sometimes err in
matching the lengths of the nested sequences_, commonly called "ragged
arrays".  Here we will refer to them as ragged nested sequences. Creating such
arrays via ``np.array([<ragged_nested_sequence>])`` with no ``dtype`` keyword
argument will today default to an ``object``-dtype array. Change the behaviour to
raise a ``ValueError`` instead.

Motivation and Scope
--------------------

Users who specify lists-of-lists when creating a `numpy.ndarray` via
``np.array`` may mistakenly pass in lists of different lengths. Currently we
accept this input and automatically create an array with ``dtype=object``. This
can be confusing, since it is rarely what is desired. Changing the automatic
dtype detection to never return ``object`` for ragged nested sequences (defined as a
recursive sequence of sequences, where not all the sequences on the same
level have the same length) will force users who actually wish to create
``object`` arrays to specify that explicitly. Note that ``lists``, ``tuples``,
and ``nd.ndarrays`` are all sequences [0]_. See for instance `issue 5303`_.

Usage and Impact
----------------

After this change, array creation with ragged nested sequences must explicitly
define a dtype:

    >>> np.array([[1, 2], [1]])
    ValueError: cannot guess the desired dtype from the input

    >>> np.array([[1, 2], [1]], dtype=object)
    # succeeds, with no change from current behaviour

The deprecation will affect any call that internally calls ``np.asarray``.  For
instance, the ``assert_equal`` family of functions calls ``np.asarray``, so
users will have to change code like::

    np.assert_equal(a, [[1, 2], 3])

to::

    np.assert_equal(a, np.array([[1, 2], 3], dtype=object))

Detailed description
--------------------

To explicitly set the shape of the object array, since it is sometimes hard to
determine what shape is desired, one could use:

    >>> arr = np.empty(correct_shape, dtype=object)
    >>> arr[...] = values

We will also reject mixed sequences of non-sequence and sequence, for instance
all of these will be rejected:

    >>> arr = np.array([np.arange(10), [10]])
    >>> arr = np.array([[range(3), range(3), range(3)], [range(3), 0, 0]])

Related Work
------------

`PR 14341`_ tried to raise an error when ragged nested sequences were specified
with a numeric dtype ``np.array, [[1], [2, 3]], dtype=int)`` but failed due to
false-positives, for instance ``np.array([1, np.array([5])], dtype=int)``.

.. _`PR 14341`: https://github.com/numpy/numpy/pull/14341

Implementation
--------------

The code to be changed is inside ``PyArray_GetArrayParamsFromObject`` and the
internal ``discover_dimensions`` function. The first implementation in `PR
14794`_ caused a number of downstream library failures and was reverted before
the release of 1.18. Subsequently downstream libraries fixed the places they
were using ragged arrays. The reimplementation became `PR 15119`_ which was
merged for the 1.19 release.

Backward compatibility
----------------------

Anyone depending on creating object arrays from ragged nested sequences will
need to modify their code. There will be a deprecation period during which the
current behaviour will emit a ``DeprecationWarning``. 

Alternatives
------------

- We could continue with the current situation.

- It was also suggested to add a kwarg ``depth`` to array creation, or perhaps
  to add another array creation API function ``ragged_array_object``. The goal
  was to eliminate the ambiguity in creating an object array from ``array([[1,
  2], [1]], dtype=object)``: should the returned array have a shape of
  ``(1,)``, or ``(2,)``? This NEP does not deal with that issue, and only
  deprecates the use of ``array`` with no ``dtype=object`` for ragged nested
  sequences. Users of ragged nested sequences may face another deprecation
  cycle in the future. Rationale: we expect that there are very few users who
  intend to use ragged arrays like that, this was never intended as a use case
  of NumPy arrays. Users are likely better off with `another library`_ or just
  using list of lists.

- It was also suggested to deprecate all automatic creation of ``object``-dtype
  arrays, which would require adding an explicit ``dtype=object`` for something
  like ``np.array([Decimal(10), Decimal(10)])``. This too is out of scope for
  the current NEP. Rationale: it's harder to asses the impact of this larger
  change, we're not sure how many users this may impact.

Discussion
----------

Comments to `issue 5303`_ indicate this is unintended behaviour as far back as
2014. Suggestions to change it have been made in the ensuing years, but none
have stuck. The WIP implementation in `PR 14794`_ seems to point to the
viability of this approach.

References and Footnotes
------------------------

.. _`issue 5303`: https://github.com/numpy/numpy/issues/5303
.. _sequences: https://docs.python.org/3.7/glossary.html#term-sequence
.. _`PR 14794`: https://github.com/numpy/numpy/pull/14794
.. _`PR 15119`: https://github.com/numpy/numpy/pull/15119
.. _`another library`: https://github.com/scikit-hep/awkward-array

.. [0] ``np.ndarrays`` are not recursed into, rather their shape is used
   directly. This will not emit warnings::

      ragged = np.array([[1], [1, 2, 3]], dtype=object)
      np.array([ragged, ragged]) # no dtype needed

Copyright
---------

This document has been placed in the public domain.
.. _NEP19:

=======================================
NEP 19 — Random number generator policy
=======================================

:Author: Robert Kern <robert.kern@gmail.com>
:Status: Final
:Type: Standards Track
:Created: 2018-05-24
:Updated: 2019-05-21
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-July/078380.html

Abstract
--------

For the past decade, NumPy has had a strict backwards compatibility policy for
the number stream of all of its random number distributions.  Unlike other
numerical components in ``numpy``, which are usually allowed to return
different when results when they are modified if they remain correct, we have
obligated the random number distributions to always produce the exact same
numbers in every version.  The objective of our stream-compatibility guarantee
was to provide exact reproducibility for simulations across numpy versions in
order to promote reproducible research.  However, this policy has made it very
difficult to enhance any of the distributions with faster or more accurate
algorithms.  After a decade of experience and improvements in the surrounding
ecosystem of scientific software, we believe that there are now better ways to
achieve these objectives.  We propose relaxing our strict stream-compatibility
policy to remove the obstacles that are in the way of accepting contributions
to our random number generation capabilities.


The Status Quo
--------------

Our current policy, in full:

    A fixed seed and a fixed series of calls to ``RandomState`` methods using the
    same parameters will always produce the same results up to roundoff error
    except when the values were incorrect.  Incorrect values will be fixed and
    the NumPy version in which the fix was made will be noted in the relevant
    docstring.  Extension of existing parameter ranges and the addition of new
    parameters is allowed as long the previous behavior remains unchanged.

This policy was first instated in Nov 2008 (in essence; the full set of weasel
words grew over time) in response to a user wanting to be sure that the
simulations that formed the basis of their scientific publication could be
reproduced years later, exactly, with whatever version of ``numpy`` that was
current at the time.  We were keen to support reproducible research, and it was
still early in the life of ``numpy.random``.  We had not seen much cause to
change the distribution methods all that much.

We also had not thought very thoroughly about the limits of what we really
could promise (and by “we” in this section, we really mean Robert Kern, let’s
be honest).  Despite all of the weasel words, our policy overpromises
compatibility.  The same version of ``numpy`` built on different platforms, or
just in a different way could cause changes in the stream, with varying degrees
of rarity.  The biggest is that the ``.multivariate_normal()`` method relies on
``numpy.linalg`` functions.  Even on the same platform, if one links ``numpy``
with a different LAPACK, ``.multivariate_normal()`` may well return completely
different results.  More rarely, building on a different OS or CPU can cause
differences in the stream.  We use C ``long`` integers internally for integer
distribution (it seemed like a good idea at the time), and those can vary in
size depending on the platform.  Distribution methods can overflow their
internal C ``longs`` at different breakpoints depending on the platform and
cause all of the random variate draws that follow to be different.

And even if all of that is controlled, our policy still does not provide exact
guarantees across versions.  We still do apply bug fixes when correctness is at
stake.  And even if we didn’t do that, any nontrivial program does more than
just draw random numbers.  They do computations on those numbers, transform
those with numerical algorithms from the rest of ``numpy``, which is not
subject to so strict a policy.  Trying to maintain stream-compatibility for our
random number distributions does not help reproducible research for these
reasons.

The standard practice now for bit-for-bit reproducible research is to pin all
of the versions of code of your software stack, possibly down to the OS itself.
The landscape for accomplishing this is much easier today than it was in 2008.
We now have ``pip``.  We now have virtual machines.  Those who need to
reproduce simulations exactly now can (and ought to) do so by using the exact
same version of ``numpy``.  We do not need to maintain stream-compatibility
across ``numpy`` versions to help them.

Our stream-compatibility guarantee has hindered our ability to make
improvements to ``numpy.random``.  Several first-time contributors have
submitted PRs to improve the distributions, usually by implementing a faster,
or more accurate algorithm than the one that is currently there.
Unfortunately, most of them would have required breaking the stream to do so.
Blocked by our policy, and our inability to work around that policy, many of
those contributors simply walked away.


Implementation
--------------

Work on a proposed new Pseudo Random Number Generator (PRNG) subsystem is
already underway in the randomgen_
project.  The specifics of the new design are out of scope for this NEP and up
for much discussion, but we will discuss general policies that will guide the
evolution of whatever code is adopted.  We will also outline just a few of the
requirements that such a new system must have to support the policy proposed in
this NEP.

First, we will maintain API source compatibility just as we do with the rest of
``numpy``.  If we *must* make a breaking change, we will only do so with an
appropriate deprecation period and warnings.

Second, breaking stream-compatibility in order to introduce new features or
improve performance will be *allowed* with *caution*.  Such changes will be
considered features, and as such will be no faster than the standard release
cadence of features (i.e. on ``X.Y`` releases, never ``X.Y.Z``).  Slowness will
not be considered a bug for this purpose.  Correctness bug fixes that break
stream-compatibility can happen on bugfix releases, per usual, but developers
should consider if they can wait until the next feature release.  We encourage
developers to strongly weight user’s pain from the break in
stream-compatibility against the improvements.  One example of a worthwhile
improvement would be to change algorithms for a significant increase in
performance, for example, moving from the `Box-Muller transform
<https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform>`_ method of
Gaussian variate generation to the faster `Ziggurat algorithm
<https://en.wikipedia.org/wiki/Ziggurat_algorithm>`_.  An example of a
discouraged improvement would be tweaking the Ziggurat tables just a little bit
for a small performance improvement.

Any new design for the random subsystem will provide a choice of different core
uniform PRNG algorithms.  A promising design choice is to make these core
uniform PRNGs their own lightweight objects with a minimal set of methods
(randomgen_ calls them “BitGenerators”).  The broader set of non-uniform
distributions will be its own class that holds a reference to one of these core
uniform PRNG objects and simply delegates to the core uniform PRNG object when
it needs uniform random numbers (randomgen_ calls this the Generator).  To
borrow an example from randomgen_, the
class ``MT19937`` is a BitGenerator that implements the classic Mersenne Twister
algorithm.  The class ``Generator`` wraps around the BitGenerator to provide
all of the non-uniform distribution methods::

    # This is not the only way to instantiate this object.
    # This is just handy for demonstrating the delegation.
    >>> bg = MT19937(seed)
    >>> rg = Generator(bg)
    >>> x = rg.standard_normal(10)

We will be more strict about a select subset of methods on these BitGenerator
objects.  They MUST guarantee stream-compatibility for a specified set
of methods which are chosen to make it easier to compose them to build other
distributions and which are needed to abstract over the implementation details
of the variety of BitGenerator algorithms.  Namely,

    * ``.bytes()``
    * ``integers()`` (formerly ``.random_integers()``)
    * ``random()`` (formerly ``.random_sample()``)

The distributions class (``Generator``) SHOULD have all of the same
distribution methods as ``RandomState`` with close-enough function signatures
such that almost all code that currently works with ``RandomState`` instances
will work with ``Generator`` instances (ignoring the precise stream
values).  Some variance will be allowed for integer distributions: in order to
avoid some of the cross-platform problems described above, these SHOULD be
rewritten to work with ``uint64`` numbers on all platforms.

.. _randomgen: https://github.com/bashtage/randomgen


Supporting Unit Tests
:::::::::::::::::::::

Because we did make a strong stream-compatibility guarantee early in numpy’s
life, reliance on stream-compatibility has grown beyond reproducible
simulations.  One use case that remains for stream-compatibility across numpy
versions is to use pseudorandom streams to generate test data in unit tests.
With care, many of the cross-platform instabilities can be avoided in the
context of small unit tests.

The new PRNG subsystem MUST provide a second, legacy distributions class that
uses the same implementations of the distribution methods as the current
version of ``numpy.random.RandomState``.  The methods of this class will have
strict stream-compatibility guarantees, even stricter than the current policy.
It is intended that this class will no longer be modified, except to keep it
working when numpy internals change.  All new development should go into the
primary distributions class.  Bug fixes that change the stream SHALL NOT be
made to ``RandomState``; instead, buggy distributions should be made to warn
when they are buggy.  The purpose of ``RandomState`` will be documented as
providing certain fixed functionality for backwards compatibility and stable
numbers for the limited purpose of unit testing, and not making whole programs
reproducible across numpy versions.

This legacy distributions class MUST be accessible under the name
``numpy.random.RandomState`` for backwards compatibility.  All current ways of
instantiating ``numpy.random.RandomState`` with a given state should
instantiate the Mersenne Twister BitGenerator with the same state.  The legacy
distributions class MUST be capable of accepting other BitGenerators.  The
purpose
here is to ensure that one can write a program with a consistent BitGenerator
state with a mixture of libraries that may or may not have upgraded from
``RandomState``.  Instances of the legacy distributions class MUST respond
``True`` to ``isinstance(rg, numpy.random.RandomState)`` because there is
current utility code that relies on that check.  Similarly, old pickles of
``numpy.random.RandomState`` instances MUST unpickle correctly.


``numpy.random.*``
::::::::::::::::::

The preferred best practice for getting reproducible pseudorandom numbers is to
instantiate a generator object with a seed and pass it around.  The implicit
global ``RandomState`` behind the ``numpy.random.*`` convenience functions can
cause problems, especially when threads or other forms of concurrency are
involved.  Global state is always problematic.  We categorically recommend
avoiding using the convenience functions when reproducibility is involved.

That said, people do use them and use ``numpy.random.seed()`` to control the
state underneath them.  It can be hard to categorize and count API usages
consistently and usefully, but a very common usage is in unit tests where many
of the problems of global state are less likely.

This NEP does not propose removing these functions or changing them to use the
less-stable ``Generator`` distribution implementations.  Future NEPs
might.

Specifically, the initial release of the new PRNG subsystem SHALL leave these
convenience functions as aliases to the methods on a global ``RandomState``
that is initialized with a Mersenne Twister BitGenerator object.  A call to
``numpy.random.seed()`` will be forwarded to that BitGenerator object.  In
addition, the global ``RandomState`` instance MUST be accessible in this
initial release by the name ``numpy.random.mtrand._rand``: Robert Kern long ago
promised ``scikit-learn`` that this name would be stable.  Whoops.

In order to allow certain workarounds, it MUST be possible to replace the
BitGenerator underneath the global ``RandomState`` with any other BitGenerator
object (we leave the precise API details up to the new subsystem).  Calling
``numpy.random.seed()`` thereafter SHOULD just pass the given seed to the
current BitGenerator object and not attempt to reset the BitGenerator to the
Mersenne Twister.  The set of ``numpy.random.*`` convenience functions SHALL
remain the same as they currently are.  They SHALL be aliases to the
``RandomState`` methods and not the new less-stable distributions class
(``Generator``, in the examples above). Users who want to get the fastest, best
distributions can follow best practices and instantiate generator objects explicitly.

This NEP does not propose that these requirements remain in perpetuity.  After
we have experience with the new PRNG subsystem, we can and should revisit these
issues in future NEPs.


Alternatives
------------

Versioning
::::::::::

For a long time, we considered that the way to allow algorithmic improvements
while maintaining the stream was to apply some form of versioning.  That is,
every time we make a stream change in one of the distributions, we increment
some version number somewhere.  ``numpy.random`` would keep all past versions
of the code, and there would be a way to get the old versions.

We will not be doing this.  If one needs to get the exact bit-for-bit results
from a given version of ``numpy``, whether one uses random numbers or not, one
should use the exact version of ``numpy``.

Proposals of how to do RNG versioning varied widely, and we will not
exhaustively list them here.  We spent years going back and forth on these
designs and were not able to find one that sufficed.  Let that time lost, and
more importantly, the contributors that we lost while we dithered, serve as
evidence against the notion.

Concretely, adding in versioning makes maintenance of ``numpy.random``
difficult.  Necessarily, we would be keeping lots of versions of the same code
around.  Adding a new algorithm safely would still be quite hard.

But most importantly, versioning is fundamentally difficult to *use* correctly.
We want to make it easy and straightforward to get the latest, fastest, best
versions of the distribution algorithms; otherwise, what's the point?  The way
to make that easy is to make the latest the default.  But the default will
necessarily change from release to release, so the user’s code would need to be
altered anyway to specify the specific version that one wants to replicate.

Adding in versioning to maintain stream-compatibility would still only provide
the same level of stream-compatibility that we currently do, with all of the
limitations described earlier.  Given that the standard practice for such needs
is to pin the release of ``numpy`` as a whole, versioning ``RandomState`` alone
is superfluous.


``StableRandom``
::::::::::::::::

A previous version of this NEP proposed to leave ``RandomState`` completely
alone for a deprecation period and build the new subsystem alongside with new
names.  To satisfy the unit testing use case, it proposed introducing a small
distributions class nominally called ``StableRandom``. It would have provided
a small subset of distribution methods that were considered most useful in unit
testing, but not the full set such that it would be too likely to be used
outside of the testing context.

During discussion about this proposal, it became apparent that there was no
satisfactory subset.  At least some projects used a fairly broad selection of
the ``RandomState`` methods in unit tests.

Downstream project owners would have been forced to modify their code to
accommodate the new PRNG subsystem.  Some modifications might be simply
mechanical, but the bulk of the work would have been tedious churn for no
positive improvement to the downstream project, just avoiding being broken.

Furthermore, under this old proposal, we would have had a quite lengthy
deprecation period where ``RandomState`` existed alongside the new system of
BitGenerator and Generator classes. Leaving the implementation of
``RandomState`` fixed meant that it could not use the new BitGenerator state
objects.  Developing programs that use a mixture of libraries that have and
have not upgraded would require managing two sets of PRNG states.  This would
notionally have been time-limited, but we intended the deprecation to be very
long.

The current proposal solves all of these problems.  All current usages of
``RandomState`` will continue to work in perpetuity, though some may be
discouraged through documentation.  Unit tests can continue to use the full
complement of ``RandomState`` methods.  Mixed ``RandomState/Generator``
code can safely share the common BitGenerator state.  Unmodified ``RandomState``
code can make use of the new features of alternative BitGenerator-like settable
streams.


Discussion
----------

- `NEP discussion <https://mail.python.org/pipermail/numpy-discussion/2018-June/078126.html>`_
- `Earlier discussion <https://mail.python.org/pipermail/numpy-discussion/2018-January/077608.html>`_


Copyright
---------

This document has been placed in the public domain.
.. _NEP41:

=================================================
NEP 41 — First step towards a new datatype system
=================================================

:title: First step towards a new Datatype System
:Author: Sebastian Berg
:Author: Stéfan van der Walt
:Author: Matti Picus
:Status: Accepted
:Type: Standard Track
:Created: 2020-02-03
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2020-April/080573.html and https://mail.python.org/pipermail/numpy-discussion/2020-March/080495.html

.. note::

    This NEP is second in a series:

    - :ref:`NEP 40 <NEP40>` explains the shortcomings of NumPy's dtype implementation.

    - NEP 41 (this document) gives an overview of our proposed replacement.

    - :ref:`NEP 42 <NEP42>` describes the new design's datatype-related APIs.

    - :ref:`NEP 43 <NEP43>` describes the new design's API for universal functions.


Abstract
--------

:ref:`Datatypes <arrays.dtypes>` in NumPy describe how to interpret each
element in arrays. NumPy provides ``int``, ``float``, and ``complex`` numerical
types, as well as string, datetime, and structured datatype capabilities.
The growing Python community, however, has need for more diverse datatypes.
Examples are datatypes with unit information attached (such as meters) or
categorical datatypes (fixed set of possible values).
However, the current NumPy datatype API is too limited to allow the creation
of these.

This NEP is the first step to enable such growth; it will lead to
a simpler development path for new datatypes.
In the long run the new datatype system will also support the creation
of datatypes directly from Python rather than C.
Refactoring the datatype API will improve maintainability and facilitate
development of both user-defined external datatypes,
as well as new features for existing datatypes internal to NumPy.


Motivation and Scope
--------------------

.. seealso::

    The user impact section includes examples of what kind of new datatypes
    will be enabled by the proposed changes in the long run.
    It may thus help to read these section out of order.

Motivation
^^^^^^^^^^

One of the main issues with the current API is the definition of typical
functions such as addition and multiplication for parametric datatypes
(see also :ref:`NEP 40 <NEP40>`)
which require additional steps to determine the output type.
For example when adding two strings of length 4, the result is a string
of length 8, which is different from the input.
Similarly, a datatype which embeds a physical unit must calculate the new unit
information: dividing a distance by a time results in a speed.
A related difficulty is that the :ref:`current casting rules <ufuncs.casting>`
-- the conversion between different datatypes --
cannot describe casting for such parametric datatypes implemented outside of NumPy.

This additional functionality for supporting parametric datatypes introduces
increased complexity within NumPy itself,
and furthermore is not available to external user-defined datatypes.
In general the concerns of different datatypes are not well-encapsulated.
This burden is exacerbated by the exposure of internal C structures,
limiting the addition of new fields
(for example to support new sorting methods [new_sort]_).

Currently there are many factors which limit the creation of new user-defined
datatypes:

* Creating casting rules for parametric user-defined dtypes is either impossible
  or so complex that it has never been attempted.
* Type promotion, e.g. the operation deciding that adding float and integer
  values should return a float value, is very valuable for numeric datatypes
  but is limited in scope for user-defined and especially parametric datatypes.
* Much of the logic (e.g. promotion) is written in single functions
  instead of being split as methods on the datatype itself.
* In the current design datatypes cannot have methods that do not generalize
  to other datatypes. For example a unit datatype cannot have a ``.to_si()`` method to
  easily find the datatype which would represent the same values in SI units.

The large need to solve these issues has driven the scientific community
to create work-arounds in multiple projects implementing physical units as an
array-like class instead of a datatype, which would generalize better across
multiple array-likes (Dask, pandas, etc.).
Already, Pandas has made a push into the same direction with its
extension arrays [pandas_extension_arrays]_ and undoubtedly
the community would be best served if such new features could be common
between NumPy, Pandas, and other projects.

Scope
^^^^^

The proposed refactoring of the datatype system is a large undertaking and
thus is proposed to be split into various phases, roughly:

* Phase I: Restructure and extend the datatype infrastructure (This NEP 41)
* Phase II: Incrementally define or rework API (Detailed largely in NEPs 42/43)
* Phase III: Growth of NumPy and Scientific Python Ecosystem capabilities.

For a more detailed accounting of the various phases, see
"Plan to Approach the Full Refactor" in the Implementation section below.
This NEP proposes to move ahead with the necessary creation of new dtype
subclasses (Phase I),
and start working on implementing current functionality.
Within the context of this NEP all development will be fully private API or
use preliminary underscored names which must be changed in the future.
Most of the internal and public API choices are part of a second Phase
and will be discussed in more detail in the following NEPs 42 and 43.
The initial implementation of this NEP will have little or no effect on users,
but provides the necessary ground work for incrementally addressing the
full rework.

The implementation of this NEP and the following, implied large rework of how
datatypes are defined in NumPy is expected to create small incompatibilities
(see backward compatibility section).
However, a transition requiring large code adaption is not anticipated and not
within scope.

Specifically, this NEP makes the following design choices which are discussed
in more details in the detailed description section:

1. Each datatype will be an instance of a subclass of ``np.dtype``, with most of the
   datatype-specific logic being implemented
   as special methods on the class. In the C-API, these correspond to specific
   slots. In short, for ``f = np.dtype("f8")``, ``isinstance(f, np.dtype)`` will remain true,
   but ``type(f)`` will be a subclass of ``np.dtype`` rather than just ``np.dtype`` itself.
   The ``PyArray_ArrFuncs`` which are currently stored as a pointer on the instance (as ``PyArray_Descr->f``),
   should instead be stored on the class as typically done in Python.
   In the future these may correspond to python side dunder methods.
   Storage information such as itemsize and byteorder can differ between
   different dtype instances (e.g. "S3" vs. "S8") and will remain part of the instance.
   This means that in the long run the current lowlevel access to dtype methods
   will be removed (see ``PyArray_ArrFuncs`` in
   :ref:`NEP 40 <NEP40>`).

2. The current NumPy scalars will *not* change, they will not be instances of
   datatypes. This will also be true for new datatypes, scalars will not be
   instances of a dtype (although ``isinstance(scalar, dtype)`` may be made
   to return ``True`` when appropriate).

Detailed technical decisions to follow in NEP 42.

Further, the public API will be designed in a way that is extensible in the future:

3. All new C-API functions provided to the user will hide implementation details
   as much as possible. The public API should be an identical, but limited,
   version of the C-API used for the internal NumPy datatypes.

The datatype system may be targeted to work with NumPy arrays,
for example by providing strided-loops, but should avoid direct
interactions with the array-object (typically `np.ndarray` instances).
Instead, the design principle will be that the array-object is a consumer
of the datatype.
While only a guiding principle, this may allow splitting the datatype system
or even the NumPy datatypes into their own project which NumPy depends on.

The changes to the datatype system in Phase II must include a large refactor of the
UFunc machinery, which will be further defined in NEP 43:

4. To enable all of the desired functionality for new user-defined datatypes,
   the UFunc machinery will be changed to replace the current dispatching
   and type resolution system.
   The old system should be *mostly* supported as a legacy version for some time.

Additionally, as a general design principle, the addition of new user-defined
datatypes will *not* change the behaviour of programs.
For example ``common_dtype(a, b)`` must not be ``c`` unless ``a`` or ``b`` know
that ``c`` exists.


User Impact
-----------

The current ecosystem has very few user-defined datatypes using NumPy, the
two most prominent being: ``rational`` and ``quaternion``.
These represent fairly simple datatypes which are not strongly impacted
by the current limitations.
However, we have identified a need for datatypes such as:

* bfloat16, used in deep learning
* categorical types
* physical units (such as meters)
* datatypes for tracing/automatic differentiation
* high, fixed precision math
* specialized integer types such as int2, int24
* new, better datetime representations
* extending e.g. integer dtypes to have a sentinel NA value
* geometrical objects [pygeos]_

Some of these are partially solved; for example unit capability is provided
in ``astropy.units``, ``unyt``, or ``pint``, as `numpy.ndarray` subclasses.
Most of these datatypes, however, simply cannot be reasonably defined
right now.
An advantage of having such datatypes in NumPy is that they should integrate
seamlessly with other array or array-like packages such as Pandas,
``xarray`` [xarray_dtype_issue]_, or ``Dask``.

The long term user impact of implementing this NEP will be to allow both
the growth of the whole ecosystem by having such new datatypes, as well as
consolidating implementation of such datatypes within NumPy to achieve
better interoperability.


Examples
^^^^^^^^

The following examples represent future user-defined datatypes we wish to enable.
These datatypes are not part the NEP and choices (e.g. choice of casting rules)
are possibilities we wish to enable and do not represent recommendations.

Simple Numerical Types
""""""""""""""""""""""

Mainly used where memory is a consideration, lower-precision numeric types
such as `bfloat16 <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_
are common in other computational frameworks.
For these types the definitions of things such as ``np.common_type`` and
``np.can_cast`` are some of the most important interfaces. Once they
support ``np.common_type``, it is (for the most part) possible to find
the correct ufunc loop to call, since most ufuncs -- such as add -- effectively
only require ``np.result_type``::

    >>> np.add(arr1, arr2).dtype == np.result_type(arr1, arr2)

and `~numpy.result_type` is largely identical to `~numpy.common_type`.


Fixed, high precision math
""""""""""""""""""""""""""

Allowing arbitrary precision or higher precision math is important in
simulations. For instance ``mpmath`` defines a precision::

    >>> import mpmath as mp
    >>> print(mp.dps)  # the current (default) precision
    15

NumPy should be able to construct a native, memory-efficient array from
a list of ``mpmath.mpf`` floating point objects::

    >>> arr_15_dps = np.array(mp.arange(3))  # (mp.arange returns a list)
    >>> print(arr_15_dps)  # Must find the correct precision from the objects:
    array(['0.0', '1.0', '2.0'], dtype=mpf[dps=15])

We should also be able to specify the desired precision when
creating the datatype for an array. Here, we use ``np.dtype[mp.mpf]``
to find the DType class (the notation is not part of this NEP),
which is then instantiated with the desired parameter.
This could also be written as ``MpfDType`` class::

    >>> arr_100_dps = np.array([1, 2, 3], dtype=np.dtype[mp.mpf](dps=100))
    >>> print(arr_15_dps + arr_100_dps)
    array(['0.0', '2.0', '4.0'], dtype=mpf[dps=100])

The ``mpf`` datatype can decide that the result of the operation should be the
higher precision one of the two, so uses a precision of 100.
Furthermore, we should be able to define casting, for example as in::

    >>> np.can_cast(arr_15_dps.dtype, arr_100_dps.dtype, casting="safe")
    True
    >>> np.can_cast(arr_100_dps.dtype, arr_15_dps.dtype, casting="safe")
    False  # loses precision
    >>> np.can_cast(arr_100_dps.dtype, arr_100_dps.dtype, casting="same_kind")
    True

Casting from float is a probably always at least a ``same_kind`` cast, but
in general, it is not safe::

    >>> np.can_cast(np.float64, np.dtype[mp.mpf](dps=4), casting="safe")
    False

since a float64 has a higher precision than the ``mpf`` datatype with
``dps=4``.

Alternatively, we can say that::

    >>> np.common_type(np.dtype[mp.mpf](dps=5), np.dtype[mp.mpf](dps=10))
    np.dtype[mp.mpf](dps=10)

And possibly even::

    >>> np.common_type(np.dtype[mp.mpf](dps=5), np.float64)
    np.dtype[mp.mpf](dps=16)  # equivalent precision to float64 (I believe)

since ``np.float64`` can be cast to a ``np.dtype[mp.mpf](dps=16)`` safely.


Categoricals
""""""""""""

Categoricals are interesting in that they can have fixed, predefined values,
or can be dynamic with the ability to modify categories when necessary.
The fixed categories (defined ahead of time) is the most straight forward
categorical definition.
Categoricals are *hard*, since there are many strategies to implement them,
suggesting NumPy should only provide the scaffolding for user-defined
categorical types. For instance::

    >>> cat = Categorical(["eggs", "spam", "toast"])
    >>> breakfast = array(["eggs", "spam", "eggs", "toast"], dtype=cat)

could store the array very efficiently, since it knows that there are only 3
categories.
Since a categorical in this sense knows almost nothing about the data stored
in it, few operations makes, sense, although equality does:

    >>> breakfast2 = array(["eggs", "eggs", "eggs", "eggs"], dtype=cat)
    >>> breakfast == breakfast2
    array[True, False, True, False])

The categorical datatype could work like a dictionary: no two
items names can be equal (checked on dtype creation), so that the equality
operation above can be performed very efficiently.
If the values define an order, the category labels (internally integers) could
be ordered the same way to allow efficient sorting and comparison.

Whether or not casting is defined from one categorical with less to one with
strictly more values defined, is something that the Categorical datatype would
need to decide. Both options should be available.


Unit on the Datatype
""""""""""""""""""""

There are different ways to define Units, depending on how the internal
machinery would be organized, one way is to have a single Unit datatype
for every existing numerical type.
This will be written as ``Unit[float64]``, the unit itself is part of the
DType instance ``Unit[float64]("m")`` is a ``float64`` with meters attached::

    >>> from astropy import units
    >>> meters = np.array([1, 2, 3], dtype=np.float64) * units.m  # meters
    >>> print(meters)
    array([1.0, 2.0, 3.0], dtype=Unit[float64]("m"))

Note that units are a bit tricky. It is debatable, whether::

    >>> np.array([1.0, 2.0, 3.0], dtype=Unit[float64]("m"))

should be valid syntax (coercing the float scalars without a unit to meters).
Once the array is created, math will work without any issue::

    >>> meters / (2 * unit.seconds)
    array([0.5, 1.0, 1.5], dtype=Unit[float64]("m/s"))

Casting is not valid from one unit to the other, but can be valid between
different scales of the same dimensionality (although this may be "unsafe")::

    >>> meters.astype(Unit[float64]("s"))
    TypeError: Cannot cast meters to seconds.
    >>> meters.astype(Unit[float64]("km"))
    >>> # Convert to centimeter-gram-second (cgs) units:
    >>> meters.astype(meters.dtype.to_cgs())

The above notation is somewhat clumsy. Functions
could be used instead to convert between units.
There may be ways to make these more convenient, but those must be left
for future discussions::

    >>> units.convert(meters, "km")
    >>> units.to_cgs(meters)

There are some open questions. For example, whether additional methods
on the array object could exist to simplify some of the notions, and how these
would percolate from the datatype to the ``ndarray``.

The interaction with other scalars would likely be defined through::

    >>> np.common_type(np.float64, Unit)
    Unit[np.float64](dimensionless)

Ufunc output datatype determination can be more involved than for simple
numerical dtypes since there is no "universal" output type::

    >>> np.multiply(meters, seconds).dtype != np.result_type(meters, seconds)

In fact ``np.result_type(meters, seconds)`` must error without context
of the operation being done.
This example highlights how the specific ufunc loop
(loop with known, specific DTypes as inputs), has to be able to make
certain decisions before the actual calculation can start.



Implementation
--------------

Plan to Approach the Full Refactor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To address these issues in NumPy and enable new datatypes,
multiple development stages are required:

* Phase I: Restructure and extend the datatype infrastructure (This NEP)

  * Organize Datatypes like normal Python classes [`PR 15508`]_

* Phase II: Incrementally define or rework API

  * Incrementally define all necessary functionality through methods and
    properties on the DType (NEP 42):

    * The properties of the class hierarchy and DType class itself,
      including methods not covered by the following, most central, points.
    * The functionality that will support dtype casting using ``arr.astype()``
      and casting related operations such as ``np.common_type``.
    * The implementation of item access and storage, and the way shape and
      dtype are determined when creating an array with ``np.array()``
    * Create a public C-API to define new DTypes.

  * Restructure how universal functions work (NEP 43), to allow extending
    a `~numpy.ufunc` such as ``np.add`` for user-defined datatypes
    such as Units:

    * Refactor how the low-level C functions are organized to make it
      extensible and flexible enough for complicated DTypes such as Units.
    * Implement registration and efficient lookup for these low-level C
      functions as defined by the user.
    * Define how promotion will be used to implement behaviour when casting
      is required. For example ``np.float64(3) + np.int32(3)`` promotes the
      ``int32`` to a ``float64``.

* Phase III: Growth of NumPy and Scientific Python Ecosystem capabilities:

  * Cleanup of legacy behaviour where it is considered buggy or undesirable.
  * Provide a path to define new datatypes from Python.
  * Assist the community in creating types such as Units or Categoricals
  * Allow strings to be used in functions such as ``np.equal`` or ``np.add``.
  * Remove legacy code paths within NumPy to improve long term maintainability

This document serves as a basis for phase I and provides the vision and
motivation for the full project.
Phase I does not introduce any new user-facing features,
but is concerned with the necessary conceptual cleanup of the current datatype system.
It provides a more "pythonic" datatype Python type object, with a clear class hierarchy.

The second phase is the incremental creation of all APIs necessary to define
fully featured datatypes and reorganization of the NumPy datatype system.
This phase will thus be primarily concerned with defining an,
initially preliminary, stable public API.

Some of the benefits of a large refactor may only become evident after the full
deprecation of the current legacy implementation (i.e. larger code removals).
However, these steps are necessary for improvements to many parts of the
core NumPy API, and are expected to make the implementation generally
easier to understand.

The following figure illustrates the proposed design at a high level,
and roughly delineates the components of the overall design.
Note that this NEP only regards Phase I (shaded area),
the rest encompasses Phase II and the design choices are up for discussion,
however, it highlights that the DType datatype class is the central, necessary
concept:

.. image:: _static/nep-0041-mindmap.svg


First steps directly related to this NEP
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The required changes necessary to NumPy are large and touch many areas
of the code base
but many of these changes can be addressed incrementally.

To enable an incremental approach we will start by creating a C defined
``PyArray_DTypeMeta`` class with its instances being the ``DType`` classes,
subclasses of ``np.dtype``.
This is necessary to add the ability of storing custom slots on the DType in C.
This ``DTypeMeta`` will be implemented first to then enable incremental
restructuring of current code.

The addition of ``DType`` will then enable addressing other changes
incrementally, some of which may begin before the settling the full internal
API:

1. New machinery for array coercion, with the goal of enabling user DTypes
   with appropriate class methods.
2. The replacement or wrapping of the current casting machinery.
3. Incremental redefinition of the current ``PyArray_ArrFuncs`` slots into
   DType method slots.

At this point, no or only very limited new public API will be added and
the internal API is considered to be in flux.
Any new public API may be set up give warnings and will have leading underscores
to indicate that it is not finalized and can be changed without warning.


Backward compatibility
----------------------

While the actual backward compatibility impact of implementing Phase I and II
are not yet fully clear, we anticipate, and accept the following changes:

* **Python API**:

  * ``type(np.dtype("f8"))`` will be a subclass of ``np.dtype``, while right
    now ``type(np.dtype("f8")) is np.dtype``.
    Code should use ``isinstance`` checks, and in very rare cases may have to
    be adapted to use it.

* **C-API**:

  * In old versions of NumPy ``PyArray_DescrCheck`` is a macro which uses
    ``type(dtype) is np.dtype``. When compiling against an old NumPy version,
    the macro may have to be replaced with the corresponding
    ``PyObject_IsInstance`` call. (If this is a problem, we could backport
    fixing the macro)

  * The UFunc machinery changes will break *limited* parts of the current
    implementation. Replacing e.g. the default ``TypeResolver`` is expected
    to remain supported for a time, although optimized masked inner loop iteration
    (which is not even used *within* NumPy) will no longer be supported.

  * All functions currently defined on the dtypes, such as
    ``PyArray_Descr->f->nonzero``, will be defined and accessed differently.
    This means that in the long run lowlevel access code will
    have to be changed to use the new API. Such changes are expected to be
    necessary in very few project.

* **dtype implementors (C-API)**:

  * The array which is currently provided to some functions (such as cast functions),
    will no longer be provided.
    For example ``PyArray_Descr->f->nonzero`` or ``PyArray_Descr->f->copyswapn``,
    may instead receive a dummy array object with only some fields (mainly the
    dtype), being valid.
    At least in some code paths, a similar mechanism is already used.

  * The ``scalarkind`` slot and registration of scalar casting will be
    removed/ignored without replacement.
    It currently allows partial value-based casting.
    The ``PyArray_ScalarKind`` function will continue to work for builtin types,
    but will not be used internally and be deprecated.

  * Currently user dtypes are defined as instances of ``np.dtype``.
    The creation works by the user providing a prototype instance.
    NumPy will need to modify at least the type during registration.
    This has no effect for either ``rational`` or ``quaternion`` and mutation
    of the structure seems unlikely after registration.

Since there is a fairly large API surface concerning datatypes, further changes
or the limitation certain function to currently existing datatypes is
likely to occur.
For example functions which use the type number as input
should be replaced with functions taking DType classes instead.
Although public, large parts of this C-API seem to be used rarely,
possibly never, by downstream projects.



Detailed Description
--------------------

This section details the design decisions covered by this NEP.
The subsections correspond to the list of design choices presented
in the Scope section.

Datatypes as Python Classes (1)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The current NumPy datatypes are not full scale python classes.
They are instead (prototype) instances of a single ``np.dtype`` class.
Changing this means that any special handling, e.g. for ``datetime``
can be moved to the Datetime DType class instead, away from monolithic general
code (e.g. current ``PyArray_AdjustFlexibleDType``).

The main consequence of this change with respect to the API is that
special methods move from the dtype instances to methods on the new DType class.
This is the typical design pattern used in Python.
Organizing these methods and information in a more Pythonic way provides a
solid foundation for refining and extending the API in the future.
The current API cannot be extended due to how it is exposed publicly.
This means for example that the methods currently stored in ``PyArray_ArrFuncs``
on each datatype (see :ref:`NEP 40 <NEP40>`)
will be defined differently in the future and
deprecated in the long run.

The most prominent visible side effect of this will be that
``type(np.dtype(np.float64))`` will not be ``np.dtype`` anymore.
Instead it will be a subclass of ``np.dtype`` meaning that
``isinstance(np.dtype(np.float64), np.dtype)`` will remain true.
This will also add the ability to use ``isinstance(dtype, np.dtype[float64])``
thus removing the need to use ``dtype.kind``, ``dtype.char``, or ``dtype.type``
to do this check.

With the design decision of DTypes as full-scale Python classes,
the question of subclassing arises.
Inheritance, however, appears problematic and a complexity best avoided
(at least initially) for container datatypes.
Further, subclasses may be more interesting for interoperability for
example with GPU backends (CuPy) storing additional methods related to the
GPU rather than as a mechanism to define new datatypes.
A class hierarchy does provides value, and one can be achieved by
allowing the creation of *abstract* datatypes.
An example for an abstract datatype would be the datatype equivalent of
``np.floating``, representing any floating point number.
These can serve the same purpose as Python's abstract base classes.

This NEP chooses to duplicate the scalar hierarchy fully or in part.
The main reason is to uncouple the implementation of the DType and scalar.
To add a DType to NumPy, in theory the scalar will not need to be
modified or know about NumPy. Also note that the categorical DType as
currently implemented in pandas does not have a scalar correspondence
making it less straight forward to rely on scalars to implement behaviour.
While DType and Scalar describe the same concept/type (e.g. an `int64`),
it seems practical to split out the information and functionality necessary
for numpy into the DType class.

The dtype instances provide parameters and storage options
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

From a computer science point of view a type defines the *value space*
(all possible values its instances can take) and their *behaviour*.
As proposed in this NEP, the DType class defines value space and behaviour.
The ``dtype`` instance can be seen as part of the value, so that the typical
Python ``instance`` corresponds to ``dtype + element`` (where *element* is the
data stored in the array).
An alternative view would be to define value space and behaviour on the
``dtype`` instances directly.
These two options are presented in the following figure and compared to
similar Python implementation patterns:

.. image:: _static/nep-0041-type-sketch-no-fonts.svg

The difference is in how parameters, such as string length or the datetime
units (``ms``, ``ns``, ...), and storage options, such as byte-order, are handled.
When implementing a Python (scalar) ``type`` parameters, for example the datetimes
unit, will be stored in the instance.
This is the design NEP 42 tries to mimic, however, the parameters are now part
of the dtype instance, meaning that part of the data stored in the instance
is shared by all array elements.
As mentioned previously, this means that the Python ``instance`` corresponds
to the ``dtype + element`` stored in a NumPy array.

An more advanced approach in Python is to use a class factory and an abstract
base class (ABC).
This allows moving the parameter into the dynamically created ``type`` and
behaviour implementation may be specific to those parameters.
An alternative approach might use this model and implemented behaviour
directly on the ``dtype`` instance.

We believe that the version as proposed here is easier to work with and understand.
Python class factories are not commonly used and NumPy does not use code
specialized for dtype parameters or byte-orders.
Making such specialization easier to implement such specialization does not
seem to be a priority.
One result of this choice is that some DTypes may only have a singleton instance
if they have no parameters or storage variation.
However, all of the NumPy dtypes require dynamically created instances due
to allowing metadata to be attached.


Scalars should not be instances of the datatypes (2)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For simple datatypes such as ``float64`` (see also below), it seems
tempting that the instance of a ``np.dtype("float64")`` can be the scalar.
This idea may be even more appealing due to the fact that scalars,
rather than datatypes, currently define a useful type hierarchy.

However, we have specifically decided against this for a number of reasons.
First, the new datatypes described herein would be instances of DType classes.
Making these instances themselves classes, while possible, adds additional
complexity that users need to understand.
It would also mean that scalars must have storage information (such as byteorder)
which is generally unnecessary and currently is not used.
Second, while the simple NumPy scalars such as ``float64`` may be such instances,
it should be possible to create datatypes for Python objects without enforcing
NumPy as a dependency.
However, Python objects that do not depend on NumPy cannot be instances of a NumPy DType.
Third, there is a mismatch between the methods and attributes which are useful
for scalars and datatypes. For instance ``to_float()`` makes sense for a scalar
but not for a datatype and ``newbyteorder`` is not useful on a scalar (or has
a different meaning).

Overall, it seem rather than reducing the complexity, i.e. by merging
the two distinct type hierarchies, making scalars instances of DTypes would
increase the complexity of both the design and implementation.

A possible future path may be to instead simplify the current NumPy scalars to
be much simpler objects which largely derive their behaviour from the datatypes.

C-API for creating new Datatypes (3)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The current C-API with which users can create new datatypes
is limited in scope, and requires use of "private" structures. This means
the API is not extensible: no new members can be added to the structure
without losing binary compatibility.
This has already limited the inclusion of new sorting methods into
NumPy [new_sort]_.

The new version shall thus replace the current ``PyArray_ArrFuncs`` structure used
to define new datatypes.
Datatypes that currently exist and are defined using these slots will be
supported during a deprecation period.

The most likely solution is to hide the implementation from the user and thus make
it extensible in the future is to model the API after Python's stable
API [PEP-384]_:

.. code-block:: C

    static struct PyArrayMethodDef slots[] = {
        {NPY_dt_method, method_implementation},
        ...,
        {0, NULL}
    }

    typedef struct{
      PyTypeObject *typeobj;  /* type of python scalar */
      ...;
      PyType_Slot *slots;
    } PyArrayDTypeMeta_Spec;

    PyObject* PyArray_InitDTypeMetaFromSpec(
            PyArray_DTypeMeta *user_dtype, PyArrayDTypeMeta_Spec *dtype_spec);

The C-side slots should be designed to mirror Python side methods
such as ``dtype.__dtype_method__``, although the exposure to Python is
a later step in the implementation to reduce the complexity of the initial
implementation.


C-API Changes to the UFunc Machinery (4)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Proposed changes to the UFunc machinery will be part of NEP 43.
However, the following changes will be necessary
(see :ref:`NEP 40 <NEP40>`
for a detailed description of the current implementation and its issues):

* The current UFunc type resolution must be adapted to allow better control
  for user-defined dtypes as well as resolve current inconsistencies.
* The inner-loop used in UFuncs must be expanded to include a return value.
  Further, error reporting must be improved, and passing in dtype-specific
  information enabled.
  This requires the modification of the inner-loop function signature and
  addition of new hooks called before and after the inner-loop is used.

An important goal for any changes to the universal functions will be to
allow the reuse of existing loops.
It should be easy for a new units datatype to fall back to existing math
functions after handling the unit related computations.


Discussion
----------

See :ref:`NEP 40 <NEP40>`
for a list of previous meetings and discussions.

Additional discussion around this specific NEP has occurred on both
the mailing list and the pull request:

* `Mailing list discussion <https://mail.python.org/pipermail/numpy-discussion/2020-March/080481.html>`_
* `NEP 41 pull request <https://github.com/numpy/numpy/pull/15506>`_
* `Pull request thread on Dtype hierarchy and Scalars <https://github.com/numpy/numpy/pull/15506#discussion_r390016298>`_


References
----------

.. [pandas_extension_arrays] https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types

.. [xarray_dtype_issue] https://github.com/pydata/xarray/issues/1262

.. [pygeos] https://github.com/caspervdw/pygeos

.. [new_sort] https://github.com/numpy/numpy/pull/12945

.. [PEP-384] https://www.python.org/dev/peps/pep-0384/

.. [PR 15508] https://github.com/numpy/numpy/pull/15508


Copyright
---------

This document has been placed in the public domain.


Acknowledgments
---------------

The effort to create new datatypes for NumPy has been discussed for several
years in many different contexts and settings, making it impossible to list everyone involved.
We would like to thank especially Stephan Hoyer, Nathaniel Smith, and Eric Wieser
for repeated in-depth discussion about datatype design.
We are very grateful for the community input in reviewing and revising this
NEP and would like to thank especially Ross Barnowski and Ralf Gommers.
.. _NEP12:

============================================
NEP 12 — Missing data functionality in NumPy
============================================

:Author: Mark Wiebe <mwwiebe@gmail.com>
:Copyright: Copyright 2011 by Enthought, Inc
:License: CC By-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0/)
:Date: 2011-06-23
:Status: Deferred

*****************
Table of Contents
*****************

.. contents::

********
Abstract
********

Users interested in dealing with missing data within NumPy are generally
pointed to the masked array subclass of the ndarray, known
as 'numpy.ma'. This class has a number of users who depend strongly
on its capabilities, but people who are accustomed to the deep integration
of the missing data placeholder "NA" in the R project and others who
find the programming interface challenging or inconsistent tend not
to use it.

This NEP proposes to integrate a mask-based missing data solution
into NumPy, with an additional bitpattern-based missing data solution
that can be implemented  concurrently or later integrating seamlessly
with the mask-based solution.

The mask-based solution and the bitpattern-based solutions in this
proposal offer the exact same missing value abstraction, with several
differences in performance, memory overhead, and flexibility.

The mask-based solution is more flexible, supporting all behaviors of the
bitpattern-based solution, but leaving the hidden values untouched
whenever an element is masked.

The bitpattern-based solution requires less memory, is bit-level
compatible with the 64-bit floating point representation used in R, but
does not preserve the hidden values and in fact requires stealing at
least one bit pattern from the underlying dtype to represent the missing
value NA.

Both solutions are generic in the sense that they can be used with
custom data types very easily, with no effort in the case of the masked
solution, and with the requirement that a bit pattern to sacrifice be
chosen in the case of the bitpattern solution.

**************************
Definition of Missing Data
**************************

In order to be able to develop an intuition about what computation
will be done by various NumPy functions, a consistent conceptual
model of what a missing element means must be applied.
Ferreting out the behaviors people need or want when they are working
with "missing data" seems to be tricky, but I believe that it boils
down to two different ideas, each of which is internally self-consistent.

One of them, the "unknown yet existing data" interpretation, can be applied
rigorously to all computations, while the other makes sense for
some statistical operations like standard deviation but not for
linear algebra operations like matrix product.
Thus, making "unknown yet existing data" be the default interpretation
is superior, providing a consistent model across all computations,
and for those operations where the other interpretation makes sense,
an optional parameter "skipna=" can be added.

For people who want the other interpretation to be default, a mechanism
proposed elsewhere for customizing subclass ufunc behavior with a
_numpy_ufunc_ member function would allow a subclass with a different
default to be created.

Unknown Yet Existing Data (NA)
==============================

This is the approach taken in the R project, defining a missing element
as something which does have a valid value which isn't known, or is
NA (not available). This proposal adopts this behavior as the
default for all operations involving missing values.

In this interpretation, nearly any computation with a missing input produces
a missing output. For example, 'sum(a)' would produce a missing value
if 'a' contained just one missing element. When the output value does
not depend on one of the inputs, it is reasonable to output a value
that is not NA, such as logical_and(NA, False) == False.

Some more complex arithmetic operations, such as matrix products, are
well defined with this interpretation, and the result should be
the same as if the missing values were NaNs. Actually implementing
such things to the theoretical limit is probably not worth it,
and in many cases either raising an exception or returning all
missing values may be preferred to doing precise calculations.

Data That Doesn't Exist Or Is Being Skipped (IGNORE)
====================================================

Another useful interpretation is that the missing elements should be
treated as if they didn't exist in the array, and the operation should
do its best to interpret what that means according to the data
that's left. In this case, 'mean(a)' would compute the mean of just
the values that are available, adjusting both the sum and count it
uses based on which values are missing. To be consistent, the mean of
an array of all missing values must produce the same result as the
mean of a zero-sized array without missing value support.

This kind of data can arise when conforming sparsely sampled data
into a regular sampling pattern, and is a useful interpretation to
use when attempting to get best-guess answers for many statistical queries.

In R, many functions take a parameter "na.rm=T" which means to treat
the data as if the NA values are not part of the data set. This proposal
defines a standard parameter "skipna=True" for this same purpose.

********************************************
Implementation Techniques For Missing Values
********************************************

In addition to there being two different interpretations of missing values,
there are two different commonly used implementation techniques for
missing values. While there are some differing default behaviors between
existing implementations of the techniques, I believe that the design
choices made in a new implementation must be made based on their merits,
not by rote copying of previous designs.

Both masks and bitpatterns have different strong and weak points,
depending on the application context. This NEP thus proposes to implement
both. To enable the writing of generic "missing value" code which does
not have to worry about whether the arrays it is using have taken one
or the other approach, the missing value semantics will be identical
for the two implementations.

Bit Patterns Signalling Missing Values (bitpattern)
===================================================

One or more patterns of bits, for example a NaN with
a particular payload, are chosen to represent the missing value
placeholder NA.

A consequence of this approach is that assigning NA changes the bits
holding the value, so that value is gone.

Additionally, for some types such as integers, a good and proper value
must be sacrificed to enable this functionality.

Boolean Masks Signalling Missing Values (mask)
==============================================

A mask is a parallel array of booleans, either one byte per element or
one bit per element, allocated alongside the existing array data. In this
NEP, the convention is chosen that True means the element is valid
(unmasked), and False means the element is NA.

By taking care when writing any C algorithm that works with values
and masks together, it is possible to have the memory for a value
that is masked never be written to. This feature allows multiple
simultaneous views of the same data with different choices of what
is missing, a feature requested by many people on the mailing list.

This approach places no limitations on the values of the underlying
data type, it may take on any binary pattern without affecting the
NA behavior.

*****************
Glossary of Terms
*****************

Because the above discussions of the different concepts and their
relationships are tricky to understand, here are more succinct
definitions of the terms used in this NEP.

NA (Not Available/Propagate)
    A placeholder for a value which is unknown to computations. That
    value may be temporarily hidden with a mask, may have been lost
    due to hard drive corruption, or gone for any number of reasons.
    For sums and products this means to produce NA if any of the inputs
    are NA. This is the same as NA in the R project.

IGNORE (Ignore/Skip)
    A placeholder which should be treated by computations as if no value does
    or could exist there. For sums, this means act as if the value
    were zero, and for products, this means act as if the value were one.
    It's as if the array were compressed in some fashion to not include
    that element.

bitpattern
    A technique for implementing either NA or IGNORE, where a particular
    set of bit patterns are chosen from all the possible bit patterns of the
    value's data type to signal that the element is NA or IGNORE.

mask
    A technique for implementing either NA or IGNORE, where a
    boolean or enum array parallel to the data array is used to signal
    which elements are NA or IGNORE.

numpy.ma
    The existing implementation of a particular form of masked arrays,
    which is part of the NumPy codebase.

Python API
    All the interface mechanisms that are exposed to Python code
    for using missing values in NumPy. This API is designed to be
    Pythonic and fit into the way NumPy works as much as possible.

C API
    All the implementation mechanisms exposed for CPython extensions
    written in C that want to support NumPy missing value support.
    This API is designed to be as natural as possible in C, and
    is usually prioritizes flexibility and high performance.

********************************
Missing Values as Seen in Python
********************************

Working With Missing Values
===========================

NumPy will gain a global singleton called numpy.NA, similar to None,
but with semantics reflecting its status as a missing value. In particular,
trying to treat it as a boolean will raise an exception, and comparisons
with it will produce numpy.NA instead of True or False. These basics are
adopted from the behavior of the NA value in the R project. To dig
deeper into the ideas, https://en.wikipedia.org/wiki/Ternary_logic#Kleene_logic
provides a starting point.

For example,::

    >>> np.array([1.0, 2.0, np.NA, 7.0], maskna=True)
    array([1., 2., NA, 7.], maskna=True)
    >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA')
    array([1., 2., NA, 7.], dtype='NA[<f8]')
    >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f4]')
    array([1., 2., NA, 7.], dtype='NA[<f4]')

produce arrays with values [1.0, 2.0, <inaccessible>, 7.0] /
mask [Exposed, Exposed, Hidden, Exposed], and
values [1.0, 2.0, <NA bitpattern>, 7.0] for the masked and
NA dtype versions respectively.

The np.NA singleton may accept a dtype= keyword parameter, indicating
that it should be treated as an NA of a particular data type. This is also
a mechanism for preserving the dtype in a NumPy scalar-like fashion.
Here's what this looks like::

    >>> np.sum(np.array([1.0, 2.0, np.NA, 7.0], maskna=True))
    NA(dtype='<f8')
    >>> np.sum(np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f8]'))
    NA(dtype='NA[<f8]')

Assigning a value to an array always causes that element to not be NA,
transparently unmasking it if necessary. Assigning numpy.NA to the array
masks that element or assigns the NA bitpattern for the particular dtype.
In the mask-based implementation, the storage behind a missing value may never
be accessed in any way, other than to unmask it by assigning its value.

To test if a value is missing, the function "np.isna(arr[0])" will
be provided. One of the key reasons for the NumPy scalars is to allow
their values into dictionaries.

All operations which write to masked arrays will not affect the value
unless they also unmask that value. This allows the storage behind
masked elements to still be relied on if they are still accessible
from another view which doesn't have them masked. For example, the
following was run on the missingdata work-in-progress branch::

    >>> a = np.array([1,2])
    >>> b = a.view(maskna=True)
    >>> b
    array([1, 2], maskna=True)
    >>> b[0] = np.NA
    >>> b
    array([NA, 2], maskna=True)
    >>> a
    array([1, 2])
    >>> # The underlying number 1 value in 'a[0]' was untouched

Copying values between the mask-based implementation and the
bitpattern implementation will transparently do the correct thing,
turning the bitpattern into a masked value, or a masked value
into the bitpattern where appropriate. The one exception is
if a valid value in a masked array happens to have the NA bitpattern,
copying this value to the NA form of the dtype will cause it to
become NA as well.

When operations are done between arrays with NA dtypes and masked arrays,
the result will be masked arrays. This is because in some cases the
NA dtypes cannot represent all the values in the masked array, so
going to masked arrays is the only way to preserve all aspects of the data.

If np.NA or masked values are copied to an array without support for
missing values enabled, an exception will be raised. Adding a mask to
the target array would be problematic, because then having a mask
would be a "viral" property consuming extra memory and reducing
performance in unexpected ways.

By default, the string "NA" will be used to represent missing values
in str and repr outputs. A global configuration will allow
this to be changed, exactly extending the way nan and inf are treated.
The following works in the current draft implementation::

    >>> a = np.arange(6, maskna=True)
    >>> a[3] = np.NA
    >>> a
    array([0, 1, 2, NA, 4, 5], maskna=True)
    >>> np.set_printoptions(nastr='blah')
    >>> a
    array([0, 1, 2, blah, 4, 5], maskna=True)

For floating point numbers, Inf and NaN are separate concepts from
missing values. If a division by zero occurs in an array with default
missing value support, an unmasked Inf or NaN will be produced. To
mask those values, a further 'a[np.logical_not(a.isfinite(a))] = np.NA'
can achieve that. For the bitpattern approach, the parameterized
dtype('NA[f8,InfNan]') described in a later section can be used to get
these semantics without the extra manipulation.

A manual loop through a masked array like::

    >>> a = np.arange(5., maskna=True)
    >>> a[3] = np.NA
    >>> a
    array([ 0.,  1.,  2., NA,  4.], maskna=True)
    >>> for i in range(len(a)):
    ...     a[i] = np.log(a[i])
    ...
    __main__:2: RuntimeWarning: divide by zero encountered in log
    >>> a
    array([       -inf,  0.        ,  0.69314718, NA,  1.38629436], maskna=True)

works even with masked values, because 'a[i]' returns an NA object
with a data type associated, that can be treated properly by the ufuncs.

Accessing a Boolean Mask
========================

The mask used to implement missing data in the masked approach is not
accessible from Python directly. This is partially due to differing
opinions on whether True in the mask should mean "missing" or "not missing"
Additionally, exposing the mask directly would preclude a potential
space optimization, where a bit-level instead of a byte-level mask
is used to get a factor of eight memory usage improvement.

To access a mask directly, there are two functions provided. They
work equivalently for both arrays with masks and NA bit
patterns, so they are specified in terms of NA and available values
instead of masked and unmasked values. The functions are
'np.isna' and 'np.isavail', which test for NA or available values
respectively.

Creating NA-Masked Arrays
=========================

The usual way to create an array with an NA mask is to pass the keyword
parameter maskna=True to one of the constructors. Most functions that
create a new array take this parameter, and produce an NA-masked
array with all its elements exposed when the parameter is set to True.

There are also two flags which indicate and control the nature of the mask
used in masked arrays. These flags can be used to add a mask, or ensure
the mask isn't a view into another array's mask.

First is 'arr.flags.maskna', which is True for all masked arrays and
may be set to True to add a mask to an array which does not have one.

Second is 'arr.flags.ownmaskna', which is True if the array owns the
memory to the mask, and False if the array has no mask, or has a view
into the mask of another array. If this is set to True in a masked
array, the array will create a copy of the mask so that further modifications
to the mask will not affect the original mask from which the view was taken.

NA-Masks When Constructing From Lists
=====================================

The initial design of NA-mask construction was to make all construction
fully explicit. This turns out to be unwieldy when working interactively
with NA-masked arrays, and having an object array be created instead of
an NA-masked array can be very surprising.

Because of this, the design has been changed to enable an NA-mask whenever
creating an array from lists which have an NA object in them. There could
be some debate of whether one should create NA-masks or NA-bitpatterns
by default, but due to the time constraints it was only feasible to tackle
NA-masks, and extending the NA-mask support more fully throughout NumPy seems
much more reasonable than starting another system and ending up with two
incomplete systems.

Mask Implementation Details
===========================

The memory ordering of the mask will always match the ordering of
the array it is associated with. A Fortran-style array will have a
Fortran-style mask, etc.

When a view of an array with a mask is taken, the view will have
a mask which is also a view of the mask in the original
array. This means unmasking values in views will also unmask them
in the original array, and if a mask is added to an array, it will
not be possible to ever remove that mask except to create a new array
copying the data but not the mask.

It is still possible to temporarily treat an array with a mask without
giving it one, by first creating a view of the array and then adding a
mask to that view. A data set can be viewed with multiple different
masks simultaneously, by creating multiple views, and giving each view
a mask.

New ndarray Methods
===================

New functions added to the numpy namespace are::

    np.isna(arr) [IMPLEMENTED]
        Returns a boolean array with True wherever the array is masked
        or matches the NA bitpattern, and False elsewhere

    np.isavail(arr)
        Returns a boolean array with False wherever the array is masked
        or matches the NA bitpattern, and True elsewhere

New functions added to the ndarray are::

    arr.copy(..., replacena=np.NA)
        Modification to the copy function which replaces NA values,
        either masked or with the NA bitpattern, with the 'replacena='
        parameter suppled. When 'replacena' isn't NA, the copied
        array is unmasked and has the 'NA' part stripped from the
        parameterized dtype ('NA[f8]' becomes just 'f8').

        The default for replacena is chosen to be np.NA instead of None,
        because it may be desirable to replace NA with None in an
        NA-masked object array.

        For future multi-NA support, 'replacena' could accept a dictionary
        mapping the NA payload to the value to substitute for that
        particular NA. NAs with payloads not appearing in the dictionary
        would remain as NA unless a 'default' key was also supplied.

        Both the parameter to replacena and the values in the dictionaries
        can be either scalars or arrays which get broadcast onto 'arr'.

    arr.view(maskna=True) [IMPLEMENTED]
        This is a shortcut for
        >>> a = arr.view()
        >>> a.flags.maskna = True

    arr.view(ownmaskna=True) [IMPLEMENTED]
        This is a shortcut for
        >>> a = arr.view()
        >>> a.flags.maskna = True
        >>> a.flags.ownmaskna = True

Element-wise UFuncs With Missing Values
=======================================

As part of the implementation, ufuncs and other operations will
have to be extended to support masked computation. Because this
is a useful feature in general, even outside the context of
a masked array, in addition to working with masked arrays ufuncs
will take an optional 'where=' parameter which allows the use
of boolean arrays to choose where a computation should be done.::

    >>> np.add(a, b, out=b, where=(a > threshold))

A benefit of having this 'where=' parameter is that it provides a way
to temporarily treat an object with a mask without ever creating a
masked array object. In the example above, this would only do the
add for the array elements with True in the 'where' clause, and neither
'a' nor 'b' need to be masked arrays.

If the 'out' parameter isn't specified, use of the 'where=' parameter
will produce an array with a mask as the result, with missing values
for everywhere the 'where' clause had the value False.

For boolean operations, the R project special cases logical_and and
logical_or so that logical_and(NA, False) is False, and
logical_or(NA, True) is True. On the other hand, 0 * NA isn't 0, but
here the NA could represent Inf or NaN, in which case 0 * the backing
value wouldn't be 0 anyway.

For NumPy element-wise ufuncs, the design won't support this ability
for the mask of the output to depend simultaneously on the mask and
the value of the inputs. The NumPy 1.6 nditer, however, makes it
fairly easy to write standalone functions which look and feel just
like ufuncs, but deviate from their behavior. The functions logical_and
and logical_or can be moved into standalone function objects which are
backwards compatible with the current ufuncs.

Reduction UFuncs With Missing Values
====================================

Reduction operations like 'sum', 'prod', 'min', and 'max' will operate
consistently with the idea that a masked value exists, but its value
is unknown.

An optional parameter 'skipna=' will be added to those functions
which can interpret it appropriately to do the operation as if just
the unmasked values existed.

With 'skipna=True', when all the input values are masked,
'sum' and 'prod' will produce the additive and multiplicative identities
respectively, while 'min' and 'max' will produce masked values.
Statistics operations which require a count, like 'mean' and 'std'
will also use the unmasked value counts for their calculations if
'skipna=True', and produce masked values when all the inputs are masked.

Some examples::

    >>> a = np.array([1., 3., np.NA, 7.], maskna=True)
    >>> np.sum(a)
    array(NA, dtype='<f8', maskna=True)
    >>> np.sum(a, skipna=True)
    11.0
    >>> np.mean(a)
    NA(dtype='<f8')
    >>> np.mean(a, skipna=True)
    3.6666666666666665

    >>> a = np.array([np.NA, np.NA], dtype='f8', maskna=True)
    >>> np.sum(a, skipna=True)
    0.0
    >>> np.max(a, skipna=True)
    array(NA, dtype='<f8', maskna=True)
    >>> np.mean(a)
    NA(dtype='<f8')
    >>> np.mean(a, skipna=True)
    /home/mwiebe/virtualenvs/dev/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2374: RuntimeWarning: invalid value encountered in double_scalars
      return mean(axis, dtype, out)
    nan

The functions 'np.any' and 'np.all' require some special consideration,
just as logical_and and logical_or do. Maybe the best way to describe
their behavior is through a series of examples::

    >>> np.any(np.array([False, False, False], maskna=True))
    False
    >>> np.any(np.array([False, np.NA, False], maskna=True))
    NA
    >>> np.any(np.array([False, np.NA, True], maskna=True))
    True

    >>> np.all(np.array([True, True, True], maskna=True))
    True
    >>> np.all(np.array([True, np.NA, True], maskna=True))
    NA
    >>> np.all(np.array([False, np.NA, True], maskna=True))
    False

Since 'np.any' is the reduction for 'np.logical_or', and 'np.all'
is the reduction for 'np.logical_and', it makes sense for them to
have a 'skipna=' parameter like the other similar reduction functions.

Parameterized NA Data Types
===========================

A masked array isn't the only way to deal with missing data, and
some systems deal with the problem by defining a special "NA" value,
for data which is missing. This is distinct from NaN floating point
values, which are the result of bad floating point calculation values,
but many people use NaNs for this purpose.

In the case of IEEE floating point values, it is possible to use a
particular NaN value, of which there are many, for "NA", distinct
from NaN. For signed integers, a reasonable approach would be to use
the minimum storable value, which doesn't have a corresponding positive
value. For unsigned integers, the maximum storage value seems most
reasonable.

With the goal of providing a general mechanism, a parameterized type
mechanism for this is much more attractive than creating separate
nafloat32, nafloat64, naint64, nauint64, etc dtypes. If this is viewed
as an alternative way of treating the mask except without value preservation,
this parameterized type can work together with the mask in a special
way to produce a value + mask combination on the fly, and use the
exact same computational infrastructure as the masked array system.
This allows one to avoid the need to write special case code for each
ufunc and for each na* dtype, something that is hard to avoid when
building a separate independent dtype implementation for each na* dtype.

Reliable conversions with the NA bitpattern preserved across primitive
types requires consideration as well. Even in the simple case of
double -> float, where this is supported by hardware, the NA value
will get lost because the NaN payload is typically not preserved.
The ability to have different bit masks specified for the same underlying
type also needs to convert properly. With a well-defined interface
converting to/from a (value,flag) pair, this becomes straightforward
to support generically.

This approach also provides some opportunities for some subtle variations
with IEEE floats. By default, one exact bit-pattern, a silent NaN with
a payload that won't be generated by hardware floating point operations,
would be used. The choice R has made could be this default.

Additionally, it might be nice to sometimes treat all NaNs as missing values.
This requires a slightly more complex mapping to convert the floating point
values into mask/value combinations, and converting back would always
produce the default NaN used by NumPy. Finally, treating both NaNs
and Infs as missing values would be just a slight variation of the NaN
version.

Strings require a slightly different handling, because they
may be any size. One approach is to use a one-character signal consisting
of one of the first 32 ASCII/unicode values. There are many possible values
to use here, like 0x15 'Negative Acknowledgement' or 0x10 'Data Link Escape'.

The Object dtype has an obvious signal, the np.NA singleton itself. Any
dtype with object semantics won't be able to have this customized, since
specifying bit patterns applies only to plain binary data, not data
with object semantics of construction and destructions.

Struct dtypes are more of a core primitive dtype, in the same fashion that
this parameterized NA-capable dtype is. It won't be possible to put
these as the parameter for the parameterized NA-dtype.

The dtype names would be parameterized similar to how the datetime64
is parameterized by the metadata unit. What name to use may require some
debate, but "NA" seems like a reasonable choice. With the default
missing value bit-pattern, these dtypes would look like
np.dtype('NA[float32]'), np.dtype('NA[f8]'), or np.dtype('NA[i64]').

To override the bit pattern that signals a missing value, a raw
value in the format of a hexadecimal unsigned integer can be given,
and in the above special cases for floating point, special strings
can be provided. The defaults for some cases, written explicitly in this
form, are then::

    np.dtype('NA[?,0x02]')
    np.dtype('NA[i4,0x80000000]')
    np.dtype('NA[u4,0xffffffff]')
    np.dtype('NA[f4,0x7f8007a2')
    np.dtype('NA[f8,0x7ff00000000007a2') (R-compatible bitpattern)
    np.dtype('NA[S16,0x15]') (using the NAK character as the signal).

    np.dtype('NA[f8,NaN]') (for any NaN)
    np.dtype('NA[f8,InfNaN]') (for any NaN or Inf)

When no parameter is specified a flexible NA dtype is created, which itself
cannot hold values, but will conform to the input types in functions like
'np.astype'. The dtype 'f8' maps to 'NA[f8]', and [('a', 'f4'), ('b', 'i4')]
maps to [('a', 'NA[f4]'), ('b', 'NA[i4]')]. Thus, to view the memory
of an 'f8' array 'arr' with 'NA[f8]', you can say arr.view(dtype='NA').

Future Expansion to multi-NA Payloads
=====================================

The packages SAS and Stata both support multiple different "NA" values.
This allows one to specify different reasons for why a value, for
example homework that wasn't done because the dog ate it or the student
was sick. In these packages, the different NA values have a linear ordering
which specifies how different NA values combine together.

In the sections on C implementation details, the mask has been designed
so that a mask with a payload is a strict superset of the NumPy boolean
type, and the boolean type has a payload of just zero. Different payloads
combine with the 'min' operation.

The important part of future-proofing the design is making sure
the C ABI-level choices and the Python API-level choices have a natural
transition to multi-NA support. Here is one way multi-NA support could look::

    >>> a = np.array([np.NA(1), 3, np.NA(2)], maskna='multi')
    >>> np.sum(a)
    NA(1, dtype='<i4')
    >>> np.sum(a[1:])
    NA(2, dtype='<i4')
    >>> b = np.array([np.NA, 2, 5], maskna=True)
    >>> a + b
    array([NA(0), 5, NA(2)], maskna='multi')

The design of this NEP does not distinguish between NAs that come
from an NA mask or NAs that come from an NA dtype. Both of these get
treated equivalently in computations, with masks dominating over NA
dtypes.::

    >>> a = np.array([np.NA, 2, 5], maskna=True)
    >>> b = np.array([1, np.NA, 7], dtype='NA')
    >>> a + b
    array([NA, NA, 12], maskna=True)

The multi-NA approach allows one to distinguish between these NAs,
through assigning different payloads to the different types. If we
extend the 'skipna=' parameter to accept a list of payloads in addition
to True/False, one could do this::

    >>> a = np.array([np.NA(1), 2, 5], maskna='multi')
    >>> b = np.array([1, np.NA(0), 7], dtype='NA[f4,multi]')
    >>> a + b
    array([NA(1), NA(0), 12], maskna='multi')
    >>> np.sum(a, skipna=0)
    NA(1, dtype='<i4')
    >>> np.sum(a, skipna=1)
    7
    >>> np.sum(b, skipna=0)
    8
    >>> np.sum(b, skipna=1)
    NA(0, dtype='<f4')
    >>> np.sum(a+b, skipna=(0,1))
    12

Differences with numpy.ma
=========================

The computational model that numpy.ma uses does not strictly adhere to
either the NA or the IGNORE model. This section exhibits some examples
of how these differences affect simple computations. This information
will be very important for helping users navigate between the systems,
so a summary probably should be put in a table in the documentation.::

    >>> a = np.random.random((3, 2))
    >>> mask = [[False, True], [True, True], [False, False]]
    >>> b1 = np.ma.masked_array(a, mask=mask)
    >>> b2 = a.view(maskna=True)
    >>> b2[mask] = np.NA

    >>> b1
    masked_array(data =
     [[0.110804969841 --]
     [-- --]
     [0.955128477746 0.440430735546]],
                 mask =
     [[False  True]
     [ True  True]
     [False False]],
           fill_value = 1e+20)
    >>> b2
    array([[0.110804969841, NA],
           [NA, NA],
           [0.955128477746, 0.440430735546]],
           maskna=True)

    >>> b1.mean(axis=0)
    masked_array(data = [0.532966723794 0.440430735546],
                 mask = [False False],
           fill_value = 1e+20)

    >>> b2.mean(axis=0)
    array([NA, NA], dtype='<f8', maskna=True)
    >>> b2.mean(axis=0, skipna=True)
    array([0.532966723794 0.440430735546], maskna=True)

For functions like np.mean, when 'skipna=True', the behavior
for all NAs is consistent with an empty array::

    >>> b1.mean(axis=1)
    masked_array(data = [0.110804969841 -- 0.697779606646],
                 mask = [False  True False],
           fill_value = 1e+20)

    >>> b2.mean(axis=1)
    array([NA, NA, 0.697779606646], maskna=True)
    >>> b2.mean(axis=1, skipna=True)
    RuntimeWarning: invalid value encountered in double_scalars
    array([0.110804969841, nan, 0.697779606646], maskna=True)

    >>> np.mean([])
    RuntimeWarning: invalid value encountered in double_scalars
    nan

In particular, note that numpy.ma generally skips masked values,
except returns masked when all the values are masked, while
the 'skipna=' parameter returns zero when all the values are NA,
to be consistent with the result of np.sum([])::

    >>> b1[1]
    masked_array(data = [-- --],
                 mask = [ True  True],
           fill_value = 1e+20)
    >>> b2[1]
    array([NA, NA], dtype='<f8', maskna=True)
    >>> b1[1].sum()
    masked
    >>> b2[1].sum()
    NA(dtype='<f8')
    >>> b2[1].sum(skipna=True)
    0.0

    >>> np.sum([])
    0.0

Boolean Indexing
================

Indexing using a boolean array containing NAs does not have a consistent
interpretation according to the NA abstraction. For example::

    >>> a = np.array([1, 2])
    >>> mask = np.array([np.NA, True], maskna=True)
    >>> a[mask]
    What should happen here?

Since the NA represents a valid but unknown value, and it is a boolean,
it has two possible underlying values::

    >>> a[np.array([True, True])]
    array([1, 2])
    >>> a[np.array([False, True])]
    array([2])

The thing which changes is the length of the output array, nothing which
itself can be substituted for NA. For this reason, at least initially,
NumPy will raise an exception for this case.

Another possibility is to add an inconsistency, and follow the approach
R uses. That is, to produce the following::

    >>> a[mask]
    array([NA, 2], maskna=True)

If, in user testing, this is found necessary for pragmatic reasons,
the feature should be added even though it is inconsistent.

PEP 3118
========

PEP 3118 doesn't have any mask mechanism, so arrays with masks will
not be accessible through this interface. Similarly, it doesn't support
the specification of dtypes with NA or IGNORE bitpatterns, so the
parameterized NA dtypes will also not be accessible through this interface.

If NumPy did allow access through PEP 3118, this would circumvent the
missing value abstraction in a very damaging way. Other libraries would
try to use masked arrays, and silently get access to the data without
also getting access to the mask or being aware of the missing value
abstraction the mask and data together are following.

Cython
======

Cython uses PEP 3118 to work with NumPy arrays, so currently it will
simply refuse to work with them as described in the "PEP 3118" section.

In order to properly support NumPy missing values, Cython will need to
be modified in some fashion to add this support. Likely the best way
to do this will be to include it with supporting np.nditer, which
is most likely going to have an enhancement to make writing missing
value algorithms easier.

Hard Masks
==========

The numpy.ma implementation has a "hardmask" feature,
which prevents values from ever being unmasked by assigning a value.
This would be an internal array flag, named something like
'arr.flags.hardmask'.

If the hardmask feature is implemented, boolean indexing could
return a hardmasked array instead of a flattened array with the
arbitrary choice of C-ordering as it currently does. While this
improves the abstraction of the array significantly, it is not
a compatible change.

Shared Masks
============

One feature of numpy.ma is called 'shared masks'.

https://docs.scipy.org/doc/numpy/reference/maskedarray.baseclass.html#numpy.ma.MaskedArray.sharedmask

This feature cannot be supported by a masked implementation of
missing values without directly violating the missing value abstraction.
If the same mask memory is shared between two arrays 'a' and 'b', assigning
a value to a masked element in 'a' will simultaneously unmask the
element with matching index in 'b'. Because this isn't at the same time
assigning a valid value to that element in 'b', this has violated the
abstraction. For this reason, shared masks will not be supported
by the mask-based missing value implementation.

This is slightly different from what happens when taking a view
of an array with masked missing value support, where a view of
both the mask and the data are taken simultaneously. The result
is two views which share the same mask memory and the same data memory,
which still preserves the missing value abstraction.

Interaction With Pre-existing C API Usage
=========================================

Making sure existing code using the C API, whether it's written in C, C++,
or Cython, does something reasonable is an important goal of this implementation.
The general strategy is to make existing code which does not explicitly
tell numpy it supports NA masks fail with an exception saying so. There are
a few different access patterns people use to get ahold of the numpy array data,
here we examine a few of them to see what numpy can do. These examples are
found from doing google searches of numpy C API array access.

NumPy Documentation - How to extend NumPy
-----------------------------------------

https://docs.scipy.org/doc/numpy/user/c-info.how-to-extend.html#dealing-with-array-objects

This page has a section "Dealing with array objects" which has some advice for how
to access numpy arrays from C. When accepting arrays, the first step it suggests is
to use PyArray_FromAny or a macro built on that function, so code following this
advice will properly fail when given an NA-masked array it doesn't know how to handle.

The way this is handled is that PyArray_FromAny requires a special flag, NPY_ARRAY_ALLOWNA,
before it will allow NA-masked arrays to flow through.

https://docs.scipy.org/doc/numpy/reference/c-api.array.html#NPY_ARRAY_ALLOWNA

Code which does not follow this advice, and instead just calls PyArray_Check() to verify
it is an ndarray and checks some flags, will silently produce incorrect results. This style
of code does not provide any opportunity for numpy to say "hey, this array is special",
so also is not compatible with future ideas of lazy evaluation, derived dtypes, etc.

Tutorial From Cython Website
----------------------------

http://docs.cython.org/src/tutorial/numpy.html

This tutorial gives a convolution example, and all the examples fail with
Python exceptions when given inputs that contain NA values.

Before any Cython type annotation is introduced, the code functions just
as equivalent Python would in the interpreter.

When the type information is introduced, it is done via numpy.pxd which
defines a mapping between an ndarray declaration and PyArrayObject \*.
Under the hood, this maps to __Pyx_ArgTypeTest, which does a direct
comparison of Py_TYPE(obj) against the PyTypeObject for the ndarray.

Then the code does some dtype comparisons, and uses regular python indexing
to access the array elements. This python indexing still goes through the
Python API, so the NA handling and error checking in numpy still can work
like normal and fail if the inputs have NAs which cannot fit in the output
array. In this case it fails when trying to convert the NA into an integer
to set in the output.

The next version of the code introduces more efficient indexing. This
operates based on Python's buffer protocol. This causes Cython to call
__Pyx_GetBufferAndValidate, which calls __Pyx_GetBuffer, which calls
PyObject_GetBuffer. This call gives numpy the opportunity to raise an
exception if the inputs are arrays with NA-masks, something not supported
by the Python buffer protocol.

Numerical Python - JPL website
------------------------------

http://dsnra.jpl.nasa.gov/software/Python/numpydoc/numpy-13.html

This document is from 2001, so does not reflect recent numpy, but it is the
second hit when searching for "numpy c api example" on google.

There first example, heading "A simple example", is in fact already invalid for
recent numpy even without the NA support. In particular, if the data is misaligned
or in a different byteorder, it may crash or produce incorrect results.

The next thing the document does is introduce PyArray_ContiguousFromObject, which
gives numpy an opportunity to raise an exception when NA-masked arrays are used,
so the later code will raise exceptions as desired.

************************
C Implementation Details
************************

.. highlight:: c

The first version to implement is the array masks, because it is
the more general approach. The mask itself is an array, but since
it is intended to never be directly accessible from Python, it won't
be a full ndarray itself. The mask always has the same shape as
the array it is attached to, so it doesn't need its own shape. For
an array with a struct dtype, however, the mask will have a different
dtype than just a straight bool, so it does need its own dtype.
This gives us the following additions to the PyArrayObject::

    /*
     * Descriptor for the mask dtype.
     *   If no mask: NULL
     *   If mask   : bool/uint8/structured dtype of mask dtypes
     */
    PyArray_Descr *maskna_dtype;
    /*
     * Raw data buffer for mask. If the array has the flag
     * NPY_ARRAY_OWNMASKNA enabled, it owns this memory and
     * must call PyArray_free on it when destroyed.
     */
    npy_mask *maskna_data;
    /*
     * Just like dimensions and strides point into the same memory
     * buffer, we now just make the buffer 3x the nd instead of 2x
     * and use the same buffer.
     */
    npy_intp *maskna_strides;

These fields can be accessed through the inline functions::

    PyArray_Descr *
    PyArray_MASKNA_DTYPE(PyArrayObject *arr);

    npy_mask *
    PyArray_MASKNA_DATA(PyArrayObject *arr);

    npy_intp *
    PyArray_MASKNA_STRIDES(PyArrayObject *arr);

    npy_bool
    PyArray_HASMASKNA(PyArrayObject *arr);

There are 2 or 3 flags which must be added to the array flags, both
for requesting NA masks and for testing for them::

    NPY_ARRAY_MASKNA
    NPY_ARRAY_OWNMASKNA
    /* To possibly add in a later revision */
    NPY_ARRAY_HARDMASKNA

To allow the easy detection of NA support, and whether an array
has any missing values, we add the following functions:

PyDataType_HasNASupport(PyArray_Descr* dtype)
    Returns true if this is an NA dtype, or a struct
    dtype where every field has NA support.

PyArray_HasNASupport(PyArrayObject* obj)
    Returns true if the array dtype has NA support, or
    the array has an NA mask.

PyArray_ContainsNA(PyArrayObject* obj)
    Returns false if the array has no NA support. Returns
    true if the array has NA support AND there is an
    NA anywhere in the array.

int PyArray_AllocateMaskNA(PyArrayObject* arr, npy_bool ownmaskna, npy_bool multina)
    Allocates an NA mask for the array, ensuring ownership if requested
    and using NPY_MASK instead of NPY_BOOL for the dtype if multina is True.

Mask Binary Format
==================

The format of the mask itself is designed to indicate whether an
element is masked or not, as well as contain a payload so that multiple
different NAs with different payloads can be used in the future.
Initially, we will simply use the payload 0.

The mask has type npy_uint8, and bit 0 is used to indicate whether
a value is masked. If ((m&0x01) == 0), the element is masked, otherwise
it is unmasked. The rest of the bits are the payload, which is (m>>1).
The convention for combining masks with payloads is that smaller
payloads propagate. This design gives 128 payload values to masked elements,
and 128 payload values to unmasked elements.

The big benefit of this approach is that npy_bool also
works as a mask, because it takes on the values 0 for False and 1
for True. Additionally, the payload for npy_bool, which is always
zero, dominates over all the other possible payloads.

Since the design involves giving the mask its own dtype, we can
distinguish between masking with a single NA value (npy_bool mask),
and masking with multi-NA (npy_uint8 mask). Initial implementations
will just support the npy_bool mask.

An idea that was discarded is to allow the combination of masks + payloads
to be a simple 'min' operation. This can be done by putting the payload
in bits 0 through 6, so that the payload is (m&0x7f), and using bit 7
for the masking flag, so ((m&0x80) == 0) means the element is masked.
The fact that this makes masks completely different from booleans, instead
of a strict superset, is the primary reason this choice was discarded.

********************************************
C Iterator API Changes: Iteration With Masks
********************************************

For iteration and computation with masks, both in the context of missing
values and when the mask is used like the 'where=' parameter in ufuncs,
extending the nditer is the most natural way to expose this functionality.

Masked operations need to work with casting, alignment, and anything else
which causes values to be copied into a temporary buffer, something which
is handled nicely by the nditer but difficult to do outside that context.

First we describe iteration designed for use of masks outside the
context of missing values, then the features which include missing
value support.

Iterator Mask Features
======================

We add several new per-operand flags:

NPY_ITER_WRITEMASKED
    Indicates that any copies done from a buffer to the array are
    masked. This is necessary because READWRITE mode could destroy
    data if a float array was being treated like an int array, so
    copying to the buffer and back would truncate to integers. No
    similar flag is provided for reading, because it may not be possible
    to know the mask ahead of time, and copying everything into
    the buffer will never destroy data.

    The code using the iterator should only write to values which
    are not masked by the mask specified, otherwise the result will
    be different depending on whether buffering is enabled or not.

NPY_ITER_ARRAYMASK
    Indicates that this array is a boolean mask to use when copying
    any WRITEMASKED argument from a buffer back to the array. There
    can be only one such mask, and there cannot also be a virtual
    mask.

    As a special case, if the flag NPY_ITER_USE_MASKNA is specified
    at the same time, the mask for the operand is used instead
    of the operand itself. If the operand has no mask but is
    based on an NA dtype, that mask exposed by the iterator converts
    into the NA bitpattern when copying from the buffer to the
    array.

NPY_ITER_VIRTUAL
    Indicates that this operand is not an array, but rather created on
    the fly for the inner iteration code. This allocates enough buffer
    space for the code to read/write data, but does not have
    an actual array backing the data. When combined with NPY_ITER_ARRAYMASK,
    allows for creating a "virtual mask", specifying which values
    are unmasked without ever creating a full mask array.

Iterator NA-array Features
==========================

We add several new per-operand flags:

NPY_ITER_USE_MASKNA
    If the operand has an NA dtype, an NA mask, or both, this adds a new
    virtual operand to the end of the operand list which iterates
    over the mask for the particular operand.

NPY_ITER_IGNORE_MASKNA
    If an operand has an NA mask, by default the iterator will raise
    an exception unless NPY_ITER_USE_MASKNA is specified. This flag
    disables that check, and is intended for cases where one has first
    checked that all the elements in the array are not NA using the
    PyArray_ContainsNA function.

    If the dtype is an NA dtype, this also strips the NA-ness from the
    dtype, showing a dtype that does not support NA.

********************
Rejected Alternative
********************

Parameterized Data Type Which Adds Additional Memory for the NA Flag
====================================================================

Another alternative to having a separate mask added to the array is
to introduced a parameterized type, which takes a primitive dtype
as an argument. The dtype "i8" would turn into "maybe[i8]", and
a byte flag would be appended to the dtype to indicate whether the
value was NA or not.

This approach adds memory overhead greater or equal to keeping a separate
mask, but has better locality. To keep the dtype aligned, an 'i8' would
need to have 16 bytes to retain proper alignment, a 100% overhead compared
to 12.5% overhead for a separately kept mask.

***************
Acknowledgments
***************

In addition to feedback from Travis Oliphant and others at Enthought,
this NEP has been revised based on a great deal of feedback from
the NumPy-Discussion mailing list. The people participating in
the discussion are:

- Nathaniel Smith
- Robert Kern
- Charles Harris
- Gael Varoquaux
- Eric Firing
- Keith Goodman
- Pierre GM
- Christopher Barker
- Josef Perktold
- Ben Root
- Laurent Gautier
- Neal Becker
- Bruce Southey
- Matthew Brett
- Wes McKinney
- Lluís
- Olivier Delalleau
- Alan G Isaac
- E. Antero Tammi
- Jason Grout
- Dag Sverre Seljebotn
- Joe Harrington
- Gary Strangman
- Chris Jordan-Squire
- Peter

I apologize if I missed anyone.
.. _NEP29:

==================================================================================
NEP 29 — Recommend Python and NumPy version support as a community policy standard
==================================================================================


:Author: Thomas A Caswell <tcaswell@gmail.com>, Andreas Mueller, Brian Granger, Madicken Munk, Ralf Gommers, Matt Haberland <mhaberla@calpoly.edu>, Matthias Bussonnier <bussonniermatthias@gmail.com>, Stefan van der Walt <stefanv@berkeley.edu>
:Status: Final
:Type: Informational
:Created: 2019-07-13
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2019-October/080128.html


Abstract
--------

This NEP recommends that all projects across the Scientific
Python ecosystem adopt a common "time window-based" policy for
support of Python and NumPy versions. Standardizing a recommendation
for project support of minimum Python and NumPy versions will improve
downstream project planning.

This is an unusual NEP in that it offers recommendations for
community-wide policy and not for changes to NumPy itself.  Since a
common place for SPEEPs (Scientific Python Ecosystem Enhancement
Proposals) does not exist and given NumPy's central role in the
ecosystem, a NEP provides a visible place to document the proposed
policy.

This NEP is being put forward by maintainers of Matplotlib, scikit-learn,
IPython, Jupyter, yt, SciPy, NumPy, and scikit-image.



Detailed description
--------------------

For the purposes of this NEP we assume semantic versioning and define:

*major version*
   A release that changes the first number (e.g. X.0.0)

*minor version*
   A release that changes the second number (e.g 1.Y.0)

*patch version*
   A release that changes the third number (e.g. 1.1.Z)


When a project releases a new major or minor version, we recommend that
they support at least all minor versions of Python
introduced and released in the prior 42 months *from the
anticipated release date* with a minimum of 2 minor versions of
Python, and all minor versions of NumPy released in the prior 24
months *from the anticipated release date* with a minimum of 3
minor versions of NumPy.


Consider the following timeline::

       Jan 16      Jan 17      Jan 18      Jan 19      Jan 20
       |           |           |           |           |
  +++++|+++++++++++|+++++++++++|+++++++++++|+++++++++++|++++++++++++
   |              |                  |               |
   py 3.5.0       py 3.6.0           py 3.7.0        py 3.8.0
  |-----------------------------------------> Feb19
            |-----------------------------------------> Dec19
                      |-----------------------------------------> Nov20

It shows the 42 month support windows for Python.  A project with a
major or minor version release in February 2019 should support Python 3.5 and newer,
a project with a major or minor version released in December 2019 should
support Python 3.6 and newer, and a project with a major or minor version
release in November 2020 should support Python 3.7 and newer.

The current Python release cadence is 18 months so a 42 month window
ensures that there will always be at least two minor versions of Python
in the window.  The window is extended 6 months beyond the anticipated two-release
interval for Python to provide resilience against small fluctuations /
delays in its release schedule.

Because Python minor version support is based only on historical
release dates, a 42 month time window, and a planned project release
date, one can predict with high confidence when a project will be able
to drop any given minor version of Python.  This, in turn, could save
months of unnecessary maintenance burden.

If a project releases immediately after a minor version of Python
drops out of the support window, there will inevitably be some
mismatch in supported versions—but this situation should only last
until other projects in the ecosystem make releases.

Otherwise, once a project does a minor or major release, it is
guaranteed that there will be a stable release of all other projects
that, at the source level, support the same set of Python versions
supported by the new release.

If there is a Python 4 or a NumPy 2 this policy will have to be
reviewed in light of the community's and projects' best interests.


Support Table
~~~~~~~~~~~~~

============ ====== =====
Date         Python NumPy
------------ ------ -----
Jan 07, 2020 3.6+   1.15+
Jun 23, 2020 3.7+   1.15+
Jul 23, 2020 3.7+   1.16+
Jan 13, 2021 3.7+   1.17+
Jul 26, 2021 3.7+   1.18+
Dec 22, 2021 3.7+   1.19+
Dec 26, 2021 3.8+   1.19+
Jun 21, 2022 3.8+   1.20+
Apr 14, 2023 3.9+   1.20+
============ ====== =====


Drop Schedule
~~~~~~~~~~~~~

::

  On next release, drop support for Python 3.5 (initially released on Sep 13, 2015)
  On Jan 07, 2020 drop support for NumPy 1.14 (initially released on Jan 06, 2018)
  On Jun 23, 2020 drop support for Python 3.6 (initially released on Dec 23, 2016)
  On Jul 23, 2020 drop support for NumPy 1.15 (initially released on Jul 23, 2018)
  On Jan 13, 2021 drop support for NumPy 1.16 (initially released on Jan 13, 2019)
  On Jul 26, 2021 drop support for NumPy 1.17 (initially released on Jul 26, 2019)
  On Dec 22, 2021 drop support for NumPy 1.18 (initially released on Dec 22, 2019)
  On Dec 26, 2021 drop support for Python 3.7 (initially released on Jun 27, 2018)
  On Jun 21, 2022 drop support for NumPy 1.19 (initially released on Jun 20, 2020)
  On Apr 14, 2023 drop support for Python 3.8 (initially released on Oct 14, 2019)


Implementation
--------------

We suggest that all projects adopt the following language into their
development guidelines:

   This project supports:

   - All minor versions of Python released 42 months prior to the
     project, and at minimum the two latest minor versions.
   - All minor versions of ``numpy`` released in the 24 months prior
     to the project, and at minimum the last three minor versions.

   In ``setup.py``, the ``python_requires`` variable should be set to
   the minimum supported version of Python.  All supported minor
   versions of Python should be in the test matrix and have binary
   artifacts built for the release.

   Minimum Python and NumPy version support should be adjusted upward
   on every major and minor release, but never on a patch release.


Backward compatibility
----------------------

No backward compatibility issues.

Alternatives
------------

Ad-Hoc version support
~~~~~~~~~~~~~~~~~~~~~~

A project could, on every release, evaluate whether to increase
the minimum version of Python supported.
As a major downside, an ad-hoc approach makes it hard for downstream users to predict what
the future minimum versions will be.  As there is no objective threshold
to when the minimum version should be dropped, it is easy for these
version support discussions to devolve into `bike shedding <https://en.wikipedia.org/wiki/Wikipedia:Avoid_Parkinson%27s_bicycle-shed_effect>`_ and acrimony.


All CPython supported versions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The CPython supported versions of Python are listed in the Python
Developers Guide and the Python PEPs. Supporting these is a very clear
and conservative approach.  However, it means that there exists a four
year lag between when a new features is introduced into the language
and when a project is able to use it.  Additionally, for projects with
compiled extensions this requires building many binary artifacts for
each release.

For the case of NumPy, many projects carry workarounds to bugs that
are fixed in subsequent versions of NumPy.  Being proactive about
increasing the minimum version of NumPy allows downstream
packages to carry fewer version-specific patches.



Default version on Linux distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The policy could be to support the version of Python that ships by
default in the latest Ubuntu LTS or CentOS/RHEL release.  However, we
would still have to standardize across the community which
distribution to follow.

By following the versions supported by major Linux distributions, we
are giving up technical control of our projects to external
organizations that may have different motivations and concerns than we
do.


N minor versions of Python
~~~~~~~~~~~~~~~~~~~~~~~~~~

Given the current release cadence of the Python, the proposed time (42
months) is roughly equivalent to "the last two" Python minor versions.
However, if Python changes their release cadence substantially, any
rule based solely on the number of minor releases may need to be
changed to remain sensible.

A more fundamental problem with a policy based on number of Python
releases is that it is hard to predict when support for a given minor
version of Python will be dropped as that requires correctly
predicting the release schedule of Python for the next 3-4 years.  A
time-based rule, in contrast, only depends on past events
and the length of the support window.


Time window from the X.Y.1 Python release
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is equivalent to a few month longer support window from the X.Y.0
release.  This is because X.Y.1 bug-fix release is typically a few
months after the X.Y.0 release, thus a N month window from X.Y.1 is
roughly equivalent to a N+3 month from X.Y.0.

The X.Y.0 release is naturally a special release.  If we were to
anchor the window on X.Y.1 we would then have the discussion of why
not X.Y.M?


Discussion
----------


References and Footnotes
------------------------

Code to generate support and drop schedule tables ::

  from datetime import datetime, timedelta

  data = """Jan 15, 2017: NumPy 1.12
  Sep 13, 2015: Python 3.5
  Dec 23, 2016: Python 3.6
  Jun 27, 2018: Python 3.7
  Jun 07, 2017: NumPy 1.13
  Jan 06, 2018: NumPy 1.14
  Jul 23, 2018: NumPy 1.15
  Jan 13, 2019: NumPy 1.16
  Jul 26, 2019: NumPy 1.17
  Oct 14, 2019: Python 3.8
  Dec 22, 2019: NumPy 1.18
  Jun 20, 2020: NumPy 1.19
  """

  releases = []

  plus42 = timedelta(days=int(365*3.5 + 1))
  plus24 = timedelta(days=int(365*2 + 1))

  for line in data.splitlines():
      date, project_version = line.split(':')
      project, version = project_version.strip().split(' ')
      release = datetime.strptime(date, '%b %d, %Y')
      if project.lower() == 'numpy':
          drop = release + plus24
      else:
          drop = release + plus42
      releases.append((drop, project, version, release))

  releases = sorted(releases, key=lambda x: x[0])


  py_major,py_minor = sorted([int(x) for x in r[2].split('.')] for r in releases if r[1] == 'Python')[-1]
  minpy = f"{py_major}.{py_minor+1}+"

  num_major,num_minor = sorted([int(x) for x in r[2].split('.')] for r in releases if r[1] == 'NumPy')[-1]
  minnum = f"{num_major}.{num_minor+1}+"

  toprint_drop_dates = ['']
  toprint_support_table = []
  for d, p, v, r in releases[::-1]:
      df = d.strftime('%b %d, %Y')
      toprint_drop_dates.append(
          f'On {df} drop support for {p} {v} '
          f'(initially released on {r.strftime("%b %d, %Y")})')
      toprint_support_table.append(f'{df} {minpy:<6} {minnum:<5}')
      if p.lower() == 'numpy':
          minnum = v+'+'
      else:
          minpy = v+'+'
  print("On next release, drop support for Python 3.5 (initially released on Sep 13, 2015)")
  for e in toprint_drop_dates[-4::-1]:
      print(e)

  print('============ ====== =====')
  print('Date         Python NumPy')
  print('------------ ------ -----')
  for e in toprint_support_table[-4::-1]:
      print(e)
  print('============ ====== =====')


Copyright
---------

This document has been placed in the public domain.
=====================================
Roadmap & NumPy Enhancement Proposals
=====================================

This page provides an overview of development priorities for NumPy.
Specifically, it contains a roadmap with a higher-level overview, as
well as NumPy Enhancement Proposals (NEPs)—suggested changes
to the library—in various stages of discussion or completion (see `NEP
0 <nep-0000>`__).

Roadmap
-------
.. toctree::
   :maxdepth: 1

   Index <index>
   The Scope of NumPy <scope>
   Current roadmap <roadmap>
   Wishlist (opens new window) |wishlist_link|

.. |wishlist_link| raw:: html

   <a href="https://github.com/numpy/numpy/issues?q=is%3Aopen+is%3Aissue+label%3A%2223+-+Wish+List%22" target=" blank">WishList</a>


.. _NEP27:

=========================
NEP 27 — Zero rank arrays
=========================

:Author: Alexander Belopolsky (sasha), transcribed Matt Picus <matti.picus@gmail.com>
:Status: Final
:Type: Informational
:Created: 2006-06-10
:Resolution: https://mail.python.org/pipermail/numpy-discussion/2018-October/078824.html

.. note::

    NumPy has both zero rank arrays and scalars. This design document, adapted
    from a `2006 wiki entry`_, describes what zero rank arrays are and why they
    exist. It was transcribed 2018-10-13 into a NEP and links were updated.
    The pull request sparked `a lively discussion`_ about the continued need
    for zero rank arrays and scalars in NumPy.

    Some of the information here is dated, for instance indexing of 0-D arrays
    now is now implemented and does not error.

Zero-Rank Arrays
----------------

Zero-rank arrays are arrays with shape=().  For example:

    >>> x = array(1)
    >>> x.shape
    ()


Zero-Rank Arrays and Array Scalars
----------------------------------

Array scalars are similar to zero-rank arrays in many aspects::


    >>> int_(1).shape
    ()

They even print the same::


    >>> print int_(1)
    1
    >>> print array(1)
    1


However there are some important differences:

* Array scalars are immutable
* Array scalars have different python type for different data types

Motivation for Array Scalars
----------------------------

NumPy's design decision to provide 0-d arrays and array scalars in addition to
native python types goes against one of the fundamental python design
principles that there should be only one obvious way to do it.  In this section
we will try to explain why it is necessary to have three different ways to
represent a number.

There were several numpy-discussion threads:


* `rank-0 arrays`_ in a 2002 mailing list thread.
* Thoughts about zero dimensional arrays vs Python scalars in a `2005 mailing list thread`_]

It has been suggested several times that NumPy just use rank-0 arrays to
represent scalar quantities in all case.  Pros and cons of converting rank-0
arrays to scalars were summarized as follows:

- Pros:

  - Some cases when Python expects an integer (the most
    dramatic is when slicing and indexing a sequence:
    _PyEval_SliceIndex in ceval.c) it will not try to
    convert it to an integer first before raising an error.
    Therefore it is convenient to have 0-dim arrays that
    are integers converted for you by the array object.

  - No risk of user confusion by having two types that
    are nearly but not exactly the same and whose separate
    existence can only be explained by the history of
    Python and NumPy development.

  - No problems with code that does explicit typechecks
    ``(isinstance(x, float)`` or ``type(x) == types.FloatType)``. Although
    explicit typechecks are considered bad practice in general, there are a
    couple of valid reasons to use them.

  - No creation of a dependency on Numeric in pickle
    files (though this could also be done by a special case
    in the pickling code for arrays)

- Cons:

  - It is difficult to write generic code because scalars
    do not have the same methods and attributes as arrays.
    (such as ``.type``  or ``.shape``).  Also Python scalars have
    different numeric behavior as well.

  - This results in a special-case checking that is not
    pleasant.  Fundamentally it lets the user believe that
    somehow multidimensional homoegeneous arrays
    are something like Python lists (which except for
    Object arrays they are not).

NumPy implements a solution that is designed to have all the pros and none of the cons above.

    Create Python scalar types for all of the 21 types and also
    inherit from the three that already exist. Define equivalent
    methods and attributes for these Python scalar types.

The Need for Zero-Rank Arrays
-----------------------------

Once the idea to use zero-rank arrays to represent scalars was rejected, it was
natural to consider whether zero-rank arrays can be eliminated altogether.
However there are some important use cases where zero-rank arrays cannot be
replaced by array scalars.  See also `A case for rank-0 arrays`_ from February
2006.

* Output arguments::

    >>> y = int_(5)
    >>> add(5,5,x)
    array(10)
    >>> x
    array(10)
    >>> add(5,5,y)
    Traceback (most recent call last):
         File "<stdin>", line 1, in ?
    TypeError: return arrays must be of ArrayType

* Shared data::

    >>> x = array([1,2])
    >>> y = x[1:2]
    >>> y.shape = ()
    >>> y
    array(2)
    >>> x[1] = 20
    >>> y
    array(20)

Indexing of Zero-Rank Arrays
----------------------------

As of NumPy release 0.9.3, zero-rank arrays do not support any indexing::

    >>> x[...]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: 0-d arrays can't be indexed.

On the other hand there are several cases that make sense for rank-zero arrays.

Ellipsis and empty tuple
~~~~~~~~~~~~~~~~~~~~~~~~

Alexander started a `Jan 2006 discussion`_ on scipy-dev
with the following proposal:

    ... it may be reasonable to allow ``a[...]``.  This way
    ellipsis can be interpereted as any number of  ``:`` s including zero.
    Another subscript operation that makes sense for scalars would be
    ``a[...,newaxis]`` or even ``a[{newaxis, }* ..., {newaxis,}*]``, where
    ``{newaxis,}*`` stands for any number of comma-separated newaxis tokens.
    This will allow one to use ellipsis in generic code that would work on
    any numpy type.

Francesc Altet supported the idea of ``[...]`` on zero-rank arrays and
`suggested`_ that ``[()]`` be supported as well.

Francesc's proposal was::

    In [65]: type(numpy.array(0)[...])
    Out[65]: <type 'numpy.ndarray'>

    In [66]: type(numpy.array(0)[()])   # Indexing a la numarray
    Out[66]: <type 'int32_arrtype'>

    In [67]: type(numpy.array(0).item())  # already works
    Out[67]: <type 'int'>

There is a consensus that for a zero-rank array ``x``, both ``x[...]`` and ``x[()]`` should be valid, but the question
remains on what should be the type of the result - zero rank ndarray or ``x.dtype``?

(Alexander)
    First, whatever choice is made for ``x[...]`` and ``x[()]`` they should be
    the same because ``...`` is just syntactic sugar for "as many `:` as
    necessary", which in the case of zero rank leads to ``... = (:,)*0 = ()``.
    Second, rank zero arrays and numpy scalar types are interchangeable within
    numpy, but numpy scalars can be use in some python constructs where ndarrays
    can't.  For example::

        >>> (1,)[array(0)]
        Traceback (most recent call last):
          File "<stdin>", line 1, in ?
        TypeError: tuple indices must be integers
        >>> (1,)[int32(0)]
        1

Since most if not all numpy function automatically convert zero-rank arrays to scalars on return, there is no reason for
``[...]`` and ``[()]`` operations to be different.

See SVN changeset 1864 (which became git commit `9024ff0`_) for
implementation of ``x[...]`` and ``x[()]`` returning numpy scalars.

See SVN changeset 1866 (which became git commit `743d922`_) for
implementation of ``x[...] = v`` and ``x[()] = v``

Increasing rank with newaxis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Everyone who commented liked this feature, so as of SVN changeset 1871 (which became git commit `b32744e`_) any number of ellipses and
newaxis tokens can be placed as a subscript argument for a zero-rank array. For
example::

    >>> x = array(1)
    >>> x[newaxis,...,newaxis,...]
    array([[1]])

It is not clear why more than one ellipsis should be allowed, but this is the
behavior of higher rank arrays that we are trying to preserve.

Refactoring
~~~~~~~~~~~

Currently all indexing on zero-rank arrays is implemented in a special ``if (nd
== 0)`` branch of code that used to always raise an index error. This ensures
that the changes do not affect any existing usage (except, the usage that
relies on exceptions).  On the other hand part of motivation for these changes
was to make behavior of ndarrays more uniform and this should allow to
eliminate  ``if (nd == 0)`` checks altogether.

Copyright
---------

The original document appeared on the scipy.org wiki, with no Copyright notice, and its `history`_ attributes it to sasha.

.. _`2006 wiki entry`: https://web.archive.org/web/20100503065506/http://projects.scipy.org:80/numpy/wiki/ZeroRankArray
.. _`history`: https://web.archive.org/web/20100503065506/http://projects.scipy.org:80/numpy/wiki/ZeroRankArray?action=history
.. _`2005 mailing list thread`: https://sourceforge.net/p/numpy/mailman/message/11299166
.. _`suggested`: https://mail.python.org/pipermail/numpy-discussion/2006-January/005572.html
.. _`Jan 2006 discussion`: https://mail.python.org/pipermail/numpy-discussion/2006-January/005579.html
.. _`A case for rank-0 arrays`: https://mail.python.org/pipermail/numpy-discussion/2006-February/006384.html
.. _`rank-0 arrays`: https://mail.python.org/pipermail/numpy-discussion/2002-September/001600.html
.. _`9024ff0`: https://github.com/numpy/numpy/commit/9024ff0dc052888b5922dde0f3e615607a9e99d7
.. _`743d922`: https://github.com/numpy/numpy/commit/743d922bf5893acf00ac92e823fe12f460726f90
.. _`b32744e`: https://github.com/numpy/numpy/commit/b32744e3fc5b40bdfbd626dcc1f72907d77c01c4
.. _`a lively discussion`: https://github.com/numpy/numpy/pull/12166
.. _NEP24:

=============================================================
NEP 24 — Missing data functionality - Alternative 1 to NEP 12
=============================================================

:Author: Nathaniel J. Smith <njs@pobox.com>, Matthew Brett <matthew.brett@gmail.com>
:Status: Deferred
:Type: Standards Track
:Created: 2011-06-30


Abstract
--------

*Context: this NEP was written as an alternative to NEP 12, which at the time of writing
had an implementation that was merged into the NumPy main branch.*

The principle of this NEP is to separate the APIs for masking and for missing values, according to

* The current implementation of masked arrays (NEP 12)
* This proposal.

This discussion is only of the API, and not of the implementation.

Detailed description
--------------------


Rationale
^^^^^^^^^

The purpose of this NEP is to define two interfaces -- one for handling
'missing values', and one for handling 'masked arrays'.

An ordinary value is something like an integer or a floating point number. A
*missing* value is a placeholder for an ordinary value that is for some
reason unavailable. For example, in working with statistical data, we often
build tables in which each row represents one item, and each column
represents properties of that item. For instance, we might take a group of
people and for each one record height, age, education level, and income, and
then stick these values into a table. But then we discover that our research
assistant screwed up and forgot to record the age of one of our individuals.
We could throw out the rest of their data as well, but this would be
wasteful; even such an incomplete row is still perfectly usable for some
analyses (e.g., we can compute the correlation of height and income). The
traditional way to handle this would be to stick some particular meaningless
value in for the missing data, e.g., recording this person's age as 0. But
this is very error prone; we may later forget about these special values
while running other analyses, and discover to our surprise that babies have
higher incomes than teenagers. (In this case, the solution would be to just
leave out all the items where we have no age recorded, but this isn't a
general solution; many analyses require something more clever to handle
missing values.) So instead of using an ordinary value like 0, we define a
special "missing" value, written "NA" for "not available".

Therefore, missing values have the following properties: Like any other
value, they must be supported by your array's dtype -- you can't store a
floating point number in an array with dtype=int32, and you can't store an NA
in it either. You need an array with dtype=NAint32 or something (exact syntax
to be determined). Otherwise, they act exactly like any other values. In
particular, you can apply arithmetic functions and so forth to them. By
default, any function which takes an NA as an argument always returns an NA
as well, regardless of the values of the other arguments. This ensures that
if we try to compute the correlation of income with age, we will get "NA",
meaning "given that some of the entries could be anything, the answer could
be anything as well". This reminds us to spend a moment thinking about how we
should rephrase our question to be more meaningful. And as a convenience for
those times when you do decide that you just want the correlation between the
known ages and income, then you can enable this behavior by adding a single
argument to your function call.

For floating point computations, NAs and NaNs have (almost?) identical
behavior. But they represent different things -- NaN an invalid computation
like 0/0, NA a value that is not available -- and distinguishing between
these things is useful because in some situations they should be treated
differently. (For example, an imputation procedure should replace NAs with
imputed values, but probably should leave NaNs alone.) And anyway, we can't
use NaNs for integers, or strings, or booleans, so we need NA anyway, and
once we have NA support for all these types, we might as well support it for
floating point too for consistency.

A masked array is, conceptually, an ordinary rectangular numpy array, which
has had an arbitrarily-shaped mask placed over it. The result is,
essentially, a non-rectangular view of a rectangular array. In principle,
anything you can accomplish with a masked array could also be accomplished by
explicitly keeping a regular array and a boolean mask array and using numpy
indexing to combine them for each operation, but combining them into a single
structure is much more convenient when you need to perform complex operations
on the masked view of an array, while still being able to manipulate the mask
in the usual ways. Therefore, masks are preserved through indexing, and
functions generally treat masked-out values as if they were not even part of
the array in the first place. (Maybe this is a good heuristic: a length-4
array in which the last value has been masked out behaves just like an
ordinary length-3 array, so long as you don't change the mask.) Except, of
course, that you are free to manipulate the mask in arbitrary ways whenever
you like; it's just a standard numpy array.

There are some simple situations where one could use either of these tools to
get the job done -- or other tools entirely, like using designated surrogate
values (age=0), separate mask arrays, etc. But missing values are designed to
be particularly helpful in situations where the missingness is an intrinsic
feature of the data -- where there's a specific value that **should** exist,
if it did exist we'd it'd mean something specific, but it **doesn't**. Masked
arrays are designed to be particularly helpful in situations where we just
want to temporarily ignore some data that does exist, or generally when we
need to work with data that has a non-rectangular shape (e.g., if you make
some measurement at each point on a grid laid over a circular agar dish, then
the points that fall outside the dish aren't missing measurements, they're
just meaningless).

Initialization
^^^^^^^^^^^^^^

First, missing values can be set and be displayed as ``np.NA, NA``::

   >>> np.array([1.0, 2.0, np.NA, 7.0], dtype='NA[f8]')
   array([1., 2., NA, 7.], dtype='NA[<f8]')

As the initialization is not ambiguous, this can be written without the NA
dtype::

   >>> np.array([1.0, 2.0, np.NA, 7.0])
   array([1., 2., NA, 7.], dtype='NA[<f8]')

Masked values can be set and be displayed as ``np.IGNORE, IGNORE``::

   >>> np.array([1.0, 2.0, np.IGNORE, 7.0], masked=True)
   array([1., 2., IGNORE, 7.], masked=True)

As the initialization is not ambiguous, this can be written without
``masked=True``::

   >>> np.array([1.0, 2.0, np.IGNORE, 7.0])
   array([1., 2., IGNORE, 7.], masked=True)

Ufuncs
^^^^^^

By default, NA values propagate::

   >>> na_arr = np.array([1.0, 2.0, np.NA, 7.0])
   >>> np.sum(na_arr)
   NA('float64')

unless the ``skipna`` flag is set::

   >>> np.sum(na_arr, skipna=True)
   10.0

By default, masking does not propagate::

   >>> masked_arr = np.array([1.0, 2.0, np.IGNORE, 7.0])
   >>> np.sum(masked_arr)
   10.0

unless the ``propmask`` flag is set::

   >>> np.sum(masked_arr, propmask=True)
   IGNORE

An array can be masked, and contain NA values::

   >>> both_arr = np.array([1.0, 2.0, np.IGNORE, np.NA, 7.0])

In the default case, the behavior is obvious::

   >>> np.sum(both_arr)
   NA('float64')

It's also obvious what to do with ``skipna=True``::

   >>> np.sum(both_arr, skipna=True)
   10.0
   >>> np.sum(both_arr, skipna=True, propmask=True)
   IGNORE

To break the tie between NA and MSK, NAs propagate harder::

   >>> np.sum(both_arr, propmask=True)
   NA('float64')

Assignment
^^^^^^^^^^

is obvious in the NA case::

   >>> arr = np.array([1.0, 2.0, 7.0])
   >>> arr[2] = np.NA
   TypeError('dtype does not support NA')
   >>> na_arr = np.array([1.0, 2.0, 7.0], dtype='NA[f8]')
   >>> na_arr[2] = np.NA
   >>> na_arr
   array([1., 2., NA], dtype='NA[<f8]')

Direct assignnent in the masked case is magic and confusing, and so happens only
via the mask::

   >>> masked_array = np.array([1.0, 2.0, 7.0], masked=True)
   >>> masked_arr[2] = np.NA
   TypeError('dtype does not support NA')
   >>> masked_arr[2] = np.IGNORE
   TypeError('float() argument must be a string or a number')
   >>> masked_arr.visible[2] = False
   >>> masked_arr
   array([1., 2., IGNORE], masked=True)


Copyright
---------

This document has been placed in the public domain.
.. _NEP30:

======================================================
NEP 30 — Duck typing for NumPy arrays - Implementation
======================================================

:Author: Peter Andreas Entschev <pentschev@nvidia.com>
:Author: Stephan Hoyer <shoyer@google.com>
:Status: Draft
:Type: Standards Track
:Created: 2019-07-31
:Updated: 2019-07-31
:Resolution:

Abstract
--------

We propose the ``__duckarray__`` protocol, following the high-level overview
described in NEP 22, allowing downstream libraries to return arrays of their
defined types, in contrast to ``np.asarray``, that coerces those ``array_like``
objects to NumPy arrays.

Detailed description
--------------------

NumPy's API, including array definitions, is implemented and mimicked in
countless other projects. By definition, many of those arrays are fairly
similar in how they operate to the NumPy standard. The introduction of
``__array_function__`` allowed dispatching of functions implemented by several
of these projects directly via NumPy's API. This introduces a new requirement,
returning the NumPy-like array itself, rather than forcing a coercion into a
pure NumPy array.

For the purpose above, NEP 22 introduced the concept of duck typing to NumPy
arrays. The suggested solution described in the NEP allows libraries to avoid
coercion of a NumPy-like array to a pure NumPy array where necessary, while
still allowing that NumPy-like array libraries that do not wish to implement
the protocol to coerce arrays to a pure NumPy array via ``np.asarray``.

Usage Guidance
~~~~~~~~~~~~~~

Code that uses ``np.duckarray`` is meant for supporting other ndarray-like objects
that "follow the NumPy API". That is an ill-defined concept at the moment --
every known library implements the NumPy API only partly, and many deviate
intentionally in at least some minor ways. This cannot be easily remedied, so
for users of ``np.duckarray`` we recommend the following strategy: check if the
NumPy functionality used by the code that follows your use of ``np.duckarray``
is present in Dask, CuPy and Sparse. If so, it's reasonable to expect any duck
array to work here. If not, we suggest you indicate in your docstring what kinds
of duck arrays are accepted, or what properties they need to have.

To exemplify the usage of duck arrays, suppose one wants to take the ``mean()``
of an array-like object ``arr``. Using NumPy to achieve that, one could write
``np.asarray(arr).mean()`` to achieve the intended result. If ``arr`` is not
a NumPy array, this would create an actual NumPy array in order to call
``.mean()``. However, if the array is an object that is compliant with the NumPy
API (either in full or partially) such as a CuPy, Sparse or a Dask array, then
that copy would have been unnecessary. On the other hand, if one were to use the new
``__duckarray__`` protocol: ``np.duckarray(arr).mean()``, and ``arr`` is an object
compliant with the NumPy API, it would simply be returned rather than coerced
into a pure NumPy array, avoiding unnecessary copies and potential loss of
performance.

Implementation
--------------

The implementation idea is fairly straightforward, requiring a new function
``duckarray`` to be introduced in NumPy, and a new method ``__duckarray__`` in
NumPy-like array classes. The new ``__duckarray__`` method shall return the
downstream array-like object itself, such as the ``self`` object, while the
``__array__`` method raises ``TypeError``.  Alternatively, the ``__array__``
method could create an actual NumPy array and return that.

The new NumPy ``duckarray`` function can be implemented as follows:

.. code:: python

    def duckarray(array_like):
        if hasattr(array_like, '__duckarray__'):
            return array_like.__duckarray__()
        return np.asarray(array_like)

Example for a project implementing NumPy-like arrays
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now consider a library that implements a NumPy-compatible array class called
``NumPyLikeArray``, this class shall implement the methods described above, and
a complete implementation would look like the following:

.. code:: python

    class NumPyLikeArray:
        def __duckarray__(self):
            return self

        def __array__(self):
            raise TypeError("NumPyLikeArray can not be converted to a NumPy "
                             "array. You may want to use np.duckarray() instead.")

The implementation above exemplifies the simplest case, but the overall idea
is that libraries will implement a ``__duckarray__`` method that returns the
original object, and an ``__array__`` method that either creates and returns an
appropriate NumPy array, or raises a``TypeError`` to prevent unintentional use
as an object in a NumPy array (if ``np.asarray`` is called on an arbitrary
object that does not implement ``__array__``, it will create a NumPy array
scalar).

In case of existing libraries that don't already implement ``__array__`` but
would like to use duck array typing, it is advised that they introduce
both ``__array__`` and``__duckarray__`` methods.

Usage
-----

An example of how the ``__duckarray__`` protocol could be used to write a
``stack`` function based on ``concatenate``, and its produced outcome, can be
seen below. The example here was chosen not only to demonstrate the usage of
the ``duckarray`` function, but also to demonstrate its dependency on the NumPy
API, demonstrated by checks on the array's ``shape`` attribute. Note that the
example is merely a simplified version of NumPy's actual implementation of
``stack`` working on the first axis, and it is assumed that Dask has implemented
the ``__duckarray__`` method.

.. code:: python

    def duckarray_stack(arrays):
        arrays = [np.duckarray(arr) for arr in arrays]

        shapes = {arr.shape for arr in arrays}
        if len(shapes) != 1:
            raise ValueError('all input arrays must have the same shape')

        expanded_arrays = [arr[np.newaxis, ...] for arr in arrays]
        return np.concatenate(expanded_arrays, axis=0)

    dask_arr = dask.array.arange(10)
    np_arr = np.arange(10)
    np_like = list(range(10))

    duckarray_stack((dask_arr, dask_arr))   # Returns dask.array
    duckarray_stack((dask_arr, np_arr))     # Returns dask.array
    duckarray_stack((dask_arr, np_like))    # Returns dask.array

In contrast, using only ``np.asarray`` (at the time of writing of this NEP, this
is the usual method employed by library developers to ensure arrays are
NumPy-like) has a different outcome:

.. code:: python

    def asarray_stack(arrays):
        arrays = [np.asanyarray(arr) for arr in arrays]

        # The remaining implementation is the same as that of
        # ``duckarray_stack`` above

    asarray_stack((dask_arr, dask_arr))     # Returns np.ndarray
    asarray_stack((dask_arr, np_arr))       # Returns np.ndarray
    asarray_stack((dask_arr, np_like))      # Returns np.ndarray

Backward compatibility
----------------------

This proposal does not raise any backward compatibility issues within NumPy,
given that it only introduces a new function. However, downstream libraries
that opt to introduce the ``__duckarray__`` protocol may choose to remove the
ability of coercing arrays back to a NumPy array via ``np.array`` or
``np.asarray`` functions, preventing unintended effects of coercion of such
arrays back to a pure NumPy array (as some libraries already do, such as CuPy
and Sparse), but still leaving libraries not implementing the protocol with the
choice of utilizing ``np.duckarray`` to promote ``array_like`` objects to pure
NumPy arrays.

Previous proposals and discussion
---------------------------------

The duck typing protocol proposed here was described in a high level in
`NEP 22 <https://numpy.org/neps/nep-0022-ndarray-duck-typing-overview.html>`_.

Additionally, longer discussions about the protocol and related proposals
took place in
`numpy/numpy #13831 <https://github.com/numpy/numpy/issues/13831>`_

Copyright
---------

This document has been placed in the public domain.
.. _NEP21:

==================================================
NEP 21 — Simplified and explicit advanced indexing
==================================================

:Author: Sebastian Berg
:Author: Stephan Hoyer <shoyer@google.com>
:Status: Draft
:Type: Standards Track
:Created: 2015-08-27


Abstract
--------

NumPy's "advanced" indexing support for indexing array with other arrays is
one of its most powerful and popular features. Unfortunately, the existing
rules for advanced indexing with multiple array indices are typically confusing
to both new, and in many cases even old, users of NumPy. Here we propose an
overhaul and simplification of advanced indexing, including two new "indexer"
attributes ``oindex`` and ``vindex`` to facilitate explicit indexing.

Background
----------

Existing indexing operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NumPy arrays currently support a flexible range of indexing operations:

- "Basic" indexing involving only slices, integers, ``np.newaxis`` and ellipsis
  (``...``), e.g., ``x[0, :3, np.newaxis]`` for selecting the first element
  from the 0th axis, the first three elements from the 1st axis and inserting a
  new axis of size 1 at the end. Basic indexing always return a view of the
  indexed array's data.
- "Advanced" indexing, also called "fancy" indexing, includes all cases where
  arrays are indexed by other arrays. Advanced indexing always makes a copy:

  - "Boolean" indexing by boolean arrays, e.g., ``x[x > 0]`` for
    selecting positive elements.
  - "Vectorized" indexing by one or more integer arrays, e.g., ``x[[0, 1]]``
    for selecting the first two elements along the first axis. With multiple
    arrays, vectorized indexing uses broadcasting rules to combine indices along
    multiple dimensions. This allows for producing a result of arbitrary shape
    with arbitrary elements from the original arrays.
  - "Mixed" indexing involving any combinations of the other advancing types.
    This is no more powerful than vectorized indexing, but is sometimes more
    convenient.

For clarity, we will refer to these existing rules as "legacy indexing".
This is only a high-level summary; for more details, see NumPy's documentation
and `Examples` below.

Outer indexing
~~~~~~~~~~~~~~

One broadly useful class of indexing operations is not supported:

- "Outer" or orthogonal indexing treats one-dimensional arrays equivalently to
  slices for determining output shapes. The rule for outer indexing is that the
  result should be equivalent to independently indexing along each dimension
  with integer or boolean arrays as if both the indexed and indexing arrays
  were one-dimensional. This form of indexing is familiar to many users of other
  programming languages such as MATLAB, Fortran and R.

The reason why NumPy omits support for outer indexing is that the rules for
outer and vectorized conflict. Consider indexing a 2D array by two 1D integer
arrays, e.g., ``x[[0, 1], [0, 1]]``:

- Outer indexing is equivalent to combining multiple integer indices with
  ``itertools.product()``. The result in this case is another 2D array with
  all combinations of indexed elements, e.g.,
  ``np.array([[x[0, 0], x[0, 1]], [x[1, 0], x[1, 1]]])``
- Vectorized indexing is equivalent to combining multiple integer indices with
  ``zip()``. The result in this case is a 1D array containing the diagonal
  elements, e.g., ``np.array([x[0, 0], x[1, 1]])``.

This difference is a frequent stumbling block for new NumPy users. The outer
indexing model is easier to understand, and is a natural generalization of
slicing rules. But NumPy instead chose to support vectorized indexing, because
it is strictly more powerful.

It is always possible to emulate outer indexing by vectorized indexing with
the right indices. To make this easier, NumPy includes utility objects and
functions such as ``np.ogrid`` and ``np.ix_``, e.g.,
``x[np.ix_([0, 1], [0, 1])]``. However, there are no utilities for emulating
fully general/mixed outer indexing, which could unambiguously allow for slices,
integers, and 1D boolean and integer arrays.

Mixed indexing
~~~~~~~~~~~~~~

NumPy's existing rules for combining multiple types of indexing in the same
operation are quite complex, involving a number of edge cases.

One reason why mixed indexing is particularly confusing is that at first glance
the result works deceptively like outer indexing. Returning to our example of a
2D array, both ``x[:2, [0, 1]]`` and ``x[[0, 1], :2]`` return 2D arrays with
axes in the same order as the original array.

However, as soon as two or more non-slice objects (including integers) are
introduced, vectorized indexing rules apply. The axes introduced by the array
indices are at the front, unless all array indices are consecutive, in which
case NumPy deduces where the user "expects" them to be. Consider indexing a 3D
array ``arr`` with shape ``(X, Y, Z)``:

1. ``arr[:, [0, 1], 0]`` has shape ``(X, 2)``.
2. ``arr[[0, 1], 0, :]`` has shape ``(2, Z)``.
3. ``arr[0, :, [0, 1]]`` has shape ``(2, Y)``, not ``(Y, 2)``!

These first two cases are intuitive and consistent with outer indexing, but
this last case is quite surprising, even to many highly experienced NumPy users.

Mixed cases involving multiple array indices are also surprising, and only
less problematic because the current behavior is so useless that it is rarely
encountered in practice. When a boolean array index is mixed with another boolean or
integer array, boolean array is converted to integer array indices (equivalent
to ``np.nonzero()``) and then broadcast. For example, indexing a 2D array of
size ``(2, 2)`` like ``x[[True, False], [True, False]]`` produces a 1D vector
with shape ``(1,)``, not a 2D sub-matrix with shape ``(1, 1)``.

Mixed indexing seems so tricky that it is tempting to say that it never should
be used. However, it is not easy to avoid, because NumPy implicitly adds full
slices if there are fewer indices than the full dimensionality of the indexed
array. This means that indexing a 2D array like `x[[0, 1]]`` is equivalent to
``x[[0, 1], :]``. These cases are not surprising, but they constrain the
behavior of mixed indexing.

Indexing in other Python array libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Indexing is a useful and widely recognized mechanism for accessing
multi-dimensional array data, so it is no surprise that many other libraries in
the scientific Python ecosystem also support array indexing.

Unfortunately, the full complexity of NumPy's indexing rules mean that it is
both challenging and undesirable for other libraries to copy its behavior in all
of its nuance. The only full implementation of NumPy-style indexing is NumPy
itself. This includes projects like dask.array and h5py, which support *most*
types of array indexing in some form, and otherwise attempt to copy NumPy's API
exactly.

Vectorized indexing in particular can be challenging to implement with array
storage backends not based on NumPy. In contrast, indexing by 1D arrays along
at least one dimension in the style of outer indexing is much more acheivable.
This has led many libraries (including dask and h5py) to attempt to define a
safe subset of NumPy-style indexing that is equivalent to outer indexing, e.g.,
by only allowing indexing with an array along at most one dimension. However,
this is quite challenging to do correctly in a general enough way to be useful.
For example, the current versions of dask and h5py both handle mixed indexing
in case 3 above inconsistently with NumPy. This is quite likely to lead to
bugs.

These inconsistencies, in addition to the broader challenge of implementing
every type of indexing logic, make it challenging to write high-level array
libraries like xarray or dask.array that can interchangeably index many types of
array storage. In contrast, explicit APIs for outer and vectorized indexing in
NumPy would provide a model that external libraries could reliably emulate, even
if they don't support every type of indexing.

High level changes
------------------

Inspired by multiple "indexer" attributes for controlling different types
of indexing behavior in pandas, we propose to:

1. Introduce ``arr.oindex[indices]`` which allows array indices, but
   uses outer indexing logic.
2. Introduce ``arr.vindex[indices]`` which use the current
   "vectorized"/broadcasted logic but with two differences from
   legacy indexing:
       
   * Boolean indices are not supported. All indices must be integers,
     integer arrays or slices.
   * The integer index result dimensions are always the first axes
     of the result array. No transpose is done, even for a single
     integer array index.

3. Plain indexing on arrays will start to give warnings and eventually
   errors in cases where one of the explicit indexers should be preferred:

   * First, in all cases where legacy and outer indexing would give
     different results.
   * Later, potentially in all cases involving an integer array.

These constraints are sufficient for making indexing generally consistent
with expectations and providing a less surprising learning curve with
``oindex``.

Note that all things mentioned here apply both for assignment as well as
subscription.

Understanding these details is *not* easy. The `Examples` section in the
discussion gives code examples.
And the hopefully easier `Motivational Example` provides some
motivational use-cases for the general ideas and is likely a good start for
anyone not intimately familiar with advanced indexing.


Detailed Description
--------------------

Proposed rules
~~~~~~~~~~~~~~

From the three problems noted above some expectations for NumPy can
be deduced:

1. There should be a prominent outer/orthogonal indexing method such as
   ``arr.oindex[indices]``.

2. Considering how confusing vectorized/fancy indexing can be, it should
   be possible to be made more explicitly (e.g. ``arr.vindex[indices]``).

3. A new ``arr.vindex[indices]`` method, would not be tied to the
   confusing transpose rules of fancy indexing, which is for example
   needed for the simple case of a single advanced index. Thus,
   no transposing should be done. The axes created by the integer array
   indices are always inserted at the front, even for a single index.

4. Boolean indexing is conceptionally outer indexing. Broadcasting
   together with other advanced indices in the manner of legacy
   indexing is generally not helpful or well defined.
   A user who wishes the "``nonzero``" plus broadcast behaviour can thus
   be expected to do this manually. Thus, ``vindex`` does not need to
   support boolean index arrays.

5. An ``arr.legacy_index`` attribute should be implemented to support
   legacy indexing. This gives a simple way to update existing codebases
   using legacy indexing, which will make the deprecation of plain indexing
   behavior easier. The longer name ``legacy_index`` is intentionally chosen
   to be explicit and discourage its use in new code.

6. Plain indexing ``arr[...]`` should return an error for ambiguous cases.
   For the beginning, this probably means cases where ``arr[ind]`` and
   ``arr.oindex[ind]`` return different results give deprecation warnings.
   This includes every use of vectorized indexing with multiple integer arrays.
   Due to the transposing behaviour, this means that``arr[0, :, index_arr]``
   will be deprecated, but ``arr[:, 0, index_arr]`` will not for the time being.

7. To ensure that existing subclasses of `ndarray` that override indexing
   do not inadvertently revert to default behavior for indexing attributes,
   these attribute should have explicit checks that disable them if
   ``__getitem__`` or ``__setitem__`` has been overridden.

Unlike plain indexing, the new indexing attributes are explicitly aimed
at higher dimensional indexing, several additional changes should be implemented:

* The indexing attributes will enforce exact dimension and indexing match.
  This means that no implicit ellipsis (``...``) will be added. Unless
  an ellipsis is present the indexing expression will thus only work for
  an array with a specific number of dimensions.
  This makes the expression more explicit and safeguards against wrong
  dimensionality of arrays.
  There should be no implications for "duck typing" compatibility with
  builtin Python sequences, because Python sequences only support a limited
  form of "basic indexing" with integers and slices.

* The current plain indexing allows for the use of non-tuples for
  multi-dimensional indexing such as ``arr[[slice(None), 2]]``.
  This creates some inconsistencies and thus the indexing attributes
  should only allow plain python tuples for this purpose.
  (Whether or not this should be the case for plain indexing is a
  different issue.)

* The new attributes should not use getitem to implement setitem,
  since it is a cludge and not useful for vectorized
  indexing. (not implemented yet)


Open Questions
~~~~~~~~~~~~~~

* The names ``oindex``, ``vindex`` and ``legacy_index`` are just suggestions at
  the time of writing this, another name NumPy has used for something like
  ``oindex`` is ``np.ix_``. See also below.

* ``oindex`` and ``vindex`` could always return copies, even when no array
  operation occurs. One argument for allowing a view return is that this way
  ``oindex`` can be used as a general index replacement.
  However, there is one argument for returning copies. It is possible for
  ``arr.vindex[array_scalar, ...]``, where ``array_scalar`` should be
  a 0-D array but is not, since 0-D arrays tend to be converted.
  Copying always "fixes" this possible inconsistency.

* The final state to morph plain indexing in is not fixed in this PEP.
  It is for example possible that `arr[index]`` will be equivalent to
  ``arr.oindex`` at some point in the future.
  Since such a change will take years, it seems unnecessary to make
  specific decisions at this time.

* The proposed changes to plain indexing could be postponed indefinitely or
  not taken in order to not break or force major fixes to existing code bases.


Alternative Names
~~~~~~~~~~~~~~~~~

Possible names suggested (more suggestions will be added).

==============  ============ ========
**Orthogonal**  oindex       oix
**Vectorized**  vindex       vix
**Legacy**      legacy_index l/findex
==============  ============ ========


Subclasses
~~~~~~~~~~

Subclasses are a bit problematic in the light of these changes. There are
some possible solutions for this. For most subclasses (those which do not
provide ``__getitem__`` or ``__setitem__``) the special attributes should
just work. Subclasses that *do* provide it must be updated accordingly
and should preferably not subclass ``oindex`` and ``vindex``.

All subclasses will inherit the attributes, however, the implementation
of ``__getitem__`` on these attributes should test
``subclass.__getitem__ is ndarray.__getitem__``. If not, the
subclass has special handling for indexing and ``NotImplementedError``
should be raised, requiring that the indexing attributes is also explicitly
overwritten. Likewise, implementations of ``__setitem__`` should check to see
if ``__setitem__`` is overridden.

A further question is how to facilitate implementing the special attributes.
Also there is the weird functionality where ``__setitem__`` calls
``__getitem__`` for non-advanced indices. It might be good to avoid it for
the new attributes, but on the other hand, that may make it even more
confusing.

To facilitate implementations we could provide functions similar to
``operator.itemgetter`` and ``operator.setitem`` for the attributes.
Possibly a mixin could be provided to help implementation. These improvements
are not essential to the initial implementation, so they are saved for
future work.

Implementation
--------------

Implementation would start with writing special indexing objects available
through ``arr.oindex``, ``arr.vindex``, and ``arr.legacy_index`` to allow these
indexing operations. Also, we would need to start to deprecate those plain index
operations which are not ambiguous.
Furthermore, the NumPy code base will need to use the new attributes and
tests will have to be adapted.


Backward compatibility
----------------------

As a new feature, no backward compatibility issues with the new ``vindex``
and ``oindex`` attributes would arise.

To facilitate backwards compatibility as much as possible, we expect a long
deprecation cycle for legacy indexing behavior and propose the new
``legacy_index`` attribute.

Some forward compatibility issues with subclasses that do not specifically
implement the new methods may arise.


Alternatives
------------

NumPy may not choose to offer these different type of indexing methods, or
choose to only offer them through specific functions instead of the proposed
notation above.

We don't think that new functions are a good alternative, because indexing
notation ``[]`` offer some syntactic advantages in Python (i.e., direct
creation of slice objects) compared to functions.

A more reasonable alternative would be write new wrapper objects for alternative
indexing with functions rather than methods (e.g., ``np.oindex(arr)[indices]``
instead of ``arr.oindex[indices]``). Functionally, this would be equivalent,
but indexing is such a common operation that we think it is important to
minimize syntax and worth implementing it directly on `ndarray` objects
themselves. Indexing attributes also define a clear interface that is easier
for alternative array implementations to copy, nonwithstanding ongoing
efforts to make it easier to override NumPy functions [2]_.

Discussion
----------

The original discussion about vectorized vs outer/orthogonal indexing arose
on the NumPy mailing list:

 * https://mail.python.org/pipermail/numpy-discussion/2015-April/072550.html

Some discussion can be found on the original pull request for this NEP:

 * https://github.com/numpy/numpy/pull/6256

Python implementations of the indexing operations can be found at:

 * https://github.com/numpy/numpy/pull/5749
 * https://gist.github.com/shoyer/c700193625347eb68fee4d1f0dc8c0c8


Examples
~~~~~~~~

Since the various kinds of indexing is hard to grasp in many cases, these
examples hopefully give some more insights. Note that they are all in terms
of shape.
In the examples, all original dimensions have 5 or more elements,
advanced indexing inserts smaller dimensions.
These examples may be hard to grasp without working knowledge of advanced
indexing as of NumPy 1.9.

Example array::

    >>> arr = np.ones((5, 6, 7, 8))


Legacy fancy indexing
---------------------

Note that the same result can be achieved with ``arr.legacy_index``, but the
"future error" will still work in this case.

Single index is transposed (this is the same for all indexing types)::

    >>> arr[[0], ...].shape
    (1, 6, 7, 8)
    >>> arr[:, [0], ...].shape
    (5, 1, 7, 8)


Multiple indices are transposed *if* consecutive::

    >>> arr[:, [0], [0], :].shape  # future error
    (5, 1, 8)
    >>> arr[:, [0], :, [0]].shape  # future error
    (1, 5, 7)


It is important to note that a scalar *is* integer array index in this sense
(and gets broadcasted with the other advanced index)::

    >>> arr[:, [0], 0, :].shape
    (5, 1, 8)
    >>> arr[:, [0], :, 0].shape  # future error (scalar is "fancy")
    (1, 5, 7)


Single boolean index can act on multiple dimensions (especially the whole
array). It has to match (as of 1.10. a deprecation warning) the dimensions.
The boolean index is otherwise identical to (multiple consecutive) integer
array indices::

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr[:, 0, bindx].shape
    (5, 1)
    >>> arr[0, :, bindx].shape
    (1, 6)


The combination with anything that is not a scalar is confusing, e.g.::

    >>> arr[[0], :, bindx].shape  # bindx result broadcasts with [0]
    (1, 6)
    >>> arr[:, [0, 1], bindx].shape  # IndexError


Outer indexing
--------------

Multiple indices are "orthogonal" and their result axes are inserted 
at the same place (they are not broadcasted)::

    >>> arr.oindex[:, [0], [0, 1], :].shape
    (5, 1, 2, 8)
    >>> arr.oindex[:, [0], :, [0, 1]].shape
    (5, 1, 7, 2)
    >>> arr.oindex[:, [0], 0, :].shape
    (5, 1, 8)
    >>> arr.oindex[:, [0], :, 0].shape
    (5, 1, 7)


Boolean indices results are always inserted where the index is::

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr.oindex[:, 0, bindx].shape
    (5, 1)
    >>> arr.oindex[0, :, bindx].shape
    (6, 1)


Nothing changed in the presence of other advanced indices since::

    >>> arr.oindex[[0], :, bindx].shape
    (1, 6, 1)
    >>> arr.oindex[:, [0, 1], bindx].shape
    (5, 2, 1)


Vectorized/inner indexing
-------------------------

Multiple indices are broadcasted and iterated as one like fancy indexing,
but the new axes are always inserted at the front::

    >>> arr.vindex[:, [0], [0, 1], :].shape
    (2, 5, 8)
    >>> arr.vindex[:, [0], :, [0, 1]].shape
    (2, 5, 7)
    >>> arr.vindex[:, [0], 0, :].shape
    (1, 5, 8)
    >>> arr.vindex[:, [0], :, 0].shape
    (1, 5, 7)


Boolean indices results are always inserted where the index is, exactly
as in ``oindex`` given how specific they are to the axes they operate on::

    >>> # Create boolean index with one True value for the last two dimensions:
    >>> bindx = np.zeros((7, 8), dtype=np.bool_)
    >>> bindx[0, 0] = True
    >>> arr.vindex[:, 0, bindx].shape
    (5, 1)
    >>> arr.vindex[0, :, bindx].shape
    (6, 1)


But other advanced indices are again transposed to the front::

    >>> arr.vindex[[0], :, bindx].shape
    (1, 6, 1)
    >>> arr.vindex[:, [0, 1], bindx].shape
    (2, 5, 1)


Motivational Example
~~~~~~~~~~~~~~~~~~~~

Imagine having a data acquisition software storing ``D`` channels and
``N`` datapoints along the time. She stores this into an ``(N, D)`` shaped
array. During data analysis, we needs to fetch a pool of channels, for example
to calculate a mean over them.

This data can be faked using::

    >>> arr = np.random.random((100, 10))

Now one may remember indexing with an integer array and find the correct code::

    >>> group = arr[:, [2, 5]]
    >>> mean_value = arr.mean()

However, assume that there were some specific time points (first dimension
of the data) that need to be specially considered. These time points are
already known and given by::

    >>> interesting_times = np.array([1, 5, 8, 10], dtype=np.intp)

Now to fetch them, we may try to modify the previous code::

    >>> group_at_it = arr[interesting_times, [2, 5]]
    IndexError: Ambiguous index, use `.oindex` or `.vindex`

An error such as this will point to read up the indexing documentation.
This should make it clear, that ``oindex`` behaves more like slicing.
So, out of the different methods it is the obvious choice
(for now, this is a shape mismatch, but that could possibly also mention
``oindex``)::

    >>> group_at_it = arr.oindex[interesting_times, [2, 5]]

Now of course one could also have used ``vindex``, but it is much less
obvious how to achieve the right thing!::

    >>> reshaped_times = interesting_times[:, np.newaxis]
    >>> group_at_it = arr.vindex[reshaped_times, [2, 5]]


One may find, that for example our data is corrupt in some places.
So, we need to replace these values by zero (or anything else) for these
times. The first column may for example give the necessary information,
so that changing the values becomes easy remembering boolean indexing::

    >>> bad_data = arr[:, 0] > 0.5
    >>> arr[bad_data, :] = 0  # (corrupts further examples)

Again, however, the columns may need to be handled more individually (but in
groups), and the ``oindex`` attribute works well::

    >>> arr.oindex[bad_data, [2, 5]] = 0

Note that it would be very hard to do this using legacy fancy indexing.
The only way would be to create an integer array first::

    >>> bad_data_indx = np.nonzero(bad_data)[0]
    >>> bad_data_indx_reshaped = bad_data_indx[:, np.newaxis]
    >>> arr[bad_data_indx_reshaped, [2, 5]]

In any case we can use only ``oindex`` to do all of this without getting
into any trouble or confused by the whole complexity of advanced indexing.

But, some new features are added to the data acquisition. Different sensors
have to be used depending on the times. Let us assume we already have
created an array of indices::

    >>> correct_sensors = np.random.randint(10, size=(100, 2))

Which lists for each time the two correct sensors in an ``(N, 2)`` array.

A first try to achieve this may be ``arr[:, correct_sensors]`` and this does
not work. It should be clear quickly that slicing cannot achieve the desired
thing. But hopefully users will remember that there is ``vindex`` as a more
powerful and flexible approach to advanced indexing.
One may, if trying ``vindex`` randomly, be confused about::

    >>> new_arr = arr.vindex[:, correct_sensors]

which is neither the same, nor the correct result (see transposing rules)!
This is because slicing works still the same in ``vindex``. However, reading
the documentation and examples, one can hopefully quickly find the desired
solution::

    >>> rows = np.arange(len(arr))
    >>> rows = rows[:, np.newaxis]  # make shape fit with correct_sensors
    >>> new_arr = arr.vindex[rows, correct_sensors]
    
At this point we have left the straight forward world of ``oindex`` but can
do random picking of any element from the array. Note that in the last example
a method such as mentioned in the ``Related Questions`` section could be more
straight forward. But this approach is even more flexible, since ``rows``
does not have to be a simple ``arange``, but could be ``interesting_times``::

    >>> interesting_times = np.array([0, 4, 8, 9, 10])
    >>> correct_sensors_at_it = correct_sensors[interesting_times, :]
    >>> interesting_times_reshaped = interesting_times[:, np.newaxis]
    >>> new_arr_it = arr[interesting_times_reshaped, correct_sensors_at_it]

Truly complex situation would arise now if you would for example pool ``L``
experiments into an array shaped ``(L, N, D)``. But for ``oindex`` this should
not result into surprises. ``vindex``, being more powerful, will quite
certainly create some confusion in this case but also cover pretty much all
eventualities.


Copyright
---------

This document is placed under the CC0 1.0 Universell (CC0 1.0) Public Domain Dedication [1]_.


References and Footnotes
------------------------

.. [1] To the extent possible under law, the person who associated CC0 
   with this work has waived all copyright and related or neighboring
   rights to this work. The CC0 license may be found at
   https://creativecommons.org/publicdomain/zero/1.0/
.. [2] e.g., see NEP 18,
   http://www.numpy.org/neps/nep-0018-array-function-protocol.html
.. _NEP09:

===================================
NEP 9 — Structured array extensions
===================================

:Status: Deferred

1.  Create with-style context that makes "named-columns" available as names in the namespace.

   with np.columns(array):
        price = unit * quantityt


2. Allow structured arrays to be sliced by their column  (i.e. one additional indexing option for structured arrays) so that a[:4, 'foo':'bar']  would be allowed.

Contributors
============

A total of 12 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Brandon Carter
* Charles Harris
* Eric Wieser
* Iryna Shcherbina +
* James Bourbeau +
* Jonathan Helmus
* Julian Taylor
* Matti Picus
* Michael Lamparski +
* Michael Seifert
* Ralf Gommers

Pull requests merged
====================

A total of 21 pull requests were merged for this release.

* `#9390 <https://github.com/numpy/numpy/pull/9390>`__: BUG: Return the poly1d coefficients array directly
* `#9555 <https://github.com/numpy/numpy/pull/9555>`__: BUG: fix regression in 1.13.x in distutils.mingw32ccompiler.
* `#9556 <https://github.com/numpy/numpy/pull/9556>`__: BUG: Fix true_divide when dtype=np.float64 specified.
* `#9557 <https://github.com/numpy/numpy/pull/9557>`__: DOC: Fix some rst markup in numpy/doc/basics.py.
* `#9558 <https://github.com/numpy/numpy/pull/9558>`__: BLD: remove -xhost flag from IntelFCompiler.
* `#9559 <https://github.com/numpy/numpy/pull/9559>`__: DOC: removes broken docstring example (source code, png, pdf)...
* `#9580 <https://github.com/numpy/numpy/pull/9580>`__: BUG: Add hypot and cabs functions to WIN32 blacklist.
* `#9732 <https://github.com/numpy/numpy/pull/9732>`__: BUG: Make scalar function elision check if temp is writeable.
* `#9736 <https://github.com/numpy/numpy/pull/9736>`__: BUG: various fixes to np.gradient
* `#9742 <https://github.com/numpy/numpy/pull/9742>`__: BUG: Fix np.pad for CVE-2017-12852
* `#9744 <https://github.com/numpy/numpy/pull/9744>`__: BUG: Check for exception in sort functions, add tests
* `#9745 <https://github.com/numpy/numpy/pull/9745>`__: DOC: Add whitespace after "versionadded::" directive so it actually...
* `#9746 <https://github.com/numpy/numpy/pull/9746>`__: BUG: memory leak in np.dot of size 0
* `#9747 <https://github.com/numpy/numpy/pull/9747>`__: BUG: adjust gfortran version search regex
* `#9757 <https://github.com/numpy/numpy/pull/9757>`__: BUG: Cython 0.27 breaks NumPy on Python 3.
* `#9764 <https://github.com/numpy/numpy/pull/9764>`__: BUG: Ensure `_npy_scaled_cexp{,f,l}` is defined when needed.
* `#9765 <https://github.com/numpy/numpy/pull/9765>`__: BUG: PyArray_CountNonzero does not check for exceptions
* `#9766 <https://github.com/numpy/numpy/pull/9766>`__: BUG: Fixes histogram monotonicity check for unsigned bin values
* `#9767 <https://github.com/numpy/numpy/pull/9767>`__: BUG: ensure consistent result dtype of count_nonzero
* `#9771 <https://github.com/numpy/numpy/pull/9771>`__: MAINT,BUG: Fix mtrand for Cython 0.27.
* `#9772 <https://github.com/numpy/numpy/pull/9772>`__: DOC: Create the 1.13.2 release notes.

Contributors
============

A total of 10 people contributed to this release.

* CakeWithSteak
* Charles Harris
* Chris Burr
* Eric Wieser
* Fernando Saravia
* Lars Grueter
* Matti Picus
* Maxwell Aladago
* Qiming Sun
* Warren Weckesser

Pull requests merged
====================

A total of 14 pull requests were merged for this release.

* `#14211 <https://github.com/numpy/numpy/pull/14211>`__: BUG: Fix uint-overflow if padding with linear_ramp and negative...
* `#14275 <https://github.com/numpy/numpy/pull/14275>`__: BUG: fixing to allow unpickling of PY3 pickles from PY2
* `#14340 <https://github.com/numpy/numpy/pull/14340>`__: BUG: Fix misuse of .names and .fields in various places (backport...
* `#14423 <https://github.com/numpy/numpy/pull/14423>`__: BUG: test, fix regression in converting to ctypes.
* `#14434 <https://github.com/numpy/numpy/pull/14434>`__: BUG: Fixed maximum relative error reporting in assert_allclose
* `#14509 <https://github.com/numpy/numpy/pull/14509>`__: BUG: Fix regression in boolean matmul.
* `#14686 <https://github.com/numpy/numpy/pull/14686>`__: BUG: properly define PyArray_DescrCheck
* `#14853 <https://github.com/numpy/numpy/pull/14853>`__: BLD: add 'apt update' to shippable
* `#14854 <https://github.com/numpy/numpy/pull/14854>`__: BUG: Fix _ctypes class circular reference. (#13808)
* `#14856 <https://github.com/numpy/numpy/pull/14856>`__: BUG: Fix `np.einsum` errors on Power9 Linux and z/Linux
* `#14863 <https://github.com/numpy/numpy/pull/14863>`__: BLD: Prevent -flto from optimising long double representation...
* `#14864 <https://github.com/numpy/numpy/pull/14864>`__: BUG: lib: Fix histogram problem with signed integer arrays.
* `#15172 <https://github.com/numpy/numpy/pull/15172>`__: ENH: Backport improvements to testing functions.
* `#15191 <https://github.com/numpy/numpy/pull/15191>`__: REL: Prepare for 1.16.6 release.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Julian Taylor
* Matti Picus

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#11985 <https://github.com/numpy/numpy/pull/11985>`__: BUG: fix cached allocations without the GIL
* `#11986 <https://github.com/numpy/numpy/pull/11986>`__: BUG: Undo behavior change in ma.masked_values(shrink=True)
* `#11987 <https://github.com/numpy/numpy/pull/11987>`__: BUG: fix refcount leak in PyArray_AdaptFlexibleDType
* `#11995 <https://github.com/numpy/numpy/pull/11995>`__: TST: Add Python 3.7 testing to NumPy 1.14.

Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Ilhan Polat
* Matti Picus
* Michael Hudson-Doyle
* Ralf Gommers

Pull requests merged
====================

A total of 7 pull requests were merged for this release.

* `#14593 <https://github.com/numpy/numpy/pull/14593>`__: MAINT: backport Cython API cleanup to 1.17.x, remove docs
* `#14937 <https://github.com/numpy/numpy/pull/14937>`__: BUG: fix integer size confusion in handling array's ndmin argument
* `#14939 <https://github.com/numpy/numpy/pull/14939>`__: BUILD: remove SSE2 flag from numpy.random builds
* `#14993 <https://github.com/numpy/numpy/pull/14993>`__: MAINT: Added Python3.8 branch to dll lib discovery
* `#15038 <https://github.com/numpy/numpy/pull/15038>`__: BUG: Fix refcounting in ufunc object loops
* `#15067 <https://github.com/numpy/numpy/pull/15067>`__: BUG: Exceptions tracebacks are dropped
* `#15175 <https://github.com/numpy/numpy/pull/15175>`__: ENH: Backport improvements to testing functions.

Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Matti Picus
* Tyler Reddy
* Tony LaTorre +

Pull requests merged
====================

A total of 7 pull requests were merged for this release.

* `#12909 <https://github.com/numpy/numpy/pull/12909>`__: TST: fix vmImage dispatch in Azure
* `#12923 <https://github.com/numpy/numpy/pull/12923>`__: MAINT: remove complicated test of multiarray import failure mode
* `#13020 <https://github.com/numpy/numpy/pull/13020>`__: BUG: fix signed zero behavior in npy_divmod
* `#13026 <https://github.com/numpy/numpy/pull/13026>`__: MAINT: Add functions to parse shell-strings in the platform-native...
* `#13028 <https://github.com/numpy/numpy/pull/13028>`__: BUG: Fix regression in parsing of F90 and F77 environment variables
* `#13038 <https://github.com/numpy/numpy/pull/13038>`__: BUG: parse shell escaping in extra_compile_args and extra_link_args
* `#13041 <https://github.com/numpy/numpy/pull/13041>`__: BLD: Windows absolute path DLL loading

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Isuru Fernando
* Matthew Brett
* Sayed Adel
* Sebastian Berg
* 傅立业（Chris Fu） +

Pull requests merged
====================

A total of 9 pull requests were merged for this release.

* `#20278 <https://github.com/numpy/numpy/pull/20278>`__: BUG: Fix shadowed reference of ``dtype`` in type stub
* `#20293 <https://github.com/numpy/numpy/pull/20293>`__: BUG: Fix headers for universal2 builds
* `#20294 <https://github.com/numpy/numpy/pull/20294>`__: BUG: ``VOID_nonzero`` could sometimes mutate alignment flag
* `#20295 <https://github.com/numpy/numpy/pull/20295>`__: BUG: Do not use nonzero fastpath on unaligned arrays
* `#20296 <https://github.com/numpy/numpy/pull/20296>`__: BUG: Distutils patch to allow for 2 as a minor version (!)
* `#20297 <https://github.com/numpy/numpy/pull/20297>`__: BUG, SIMD: Fix 64-bit/8-bit integer division by a scalar
* `#20298 <https://github.com/numpy/numpy/pull/20298>`__: BUG, SIMD: Workaround broadcasting SIMD 64-bit integers on MSVC...
* `#20300 <https://github.com/numpy/numpy/pull/20300>`__: REL: Prepare for the NumPy 1.21.4 release.
* `#20302 <https://github.com/numpy/numpy/pull/20302>`__: TST: Fix a ``Arrayterator`` typing test failure

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aaron Meurer
* Bas van Beek
* Charles Harris
* Developer-Ecosystem-Engineering +
* Kevin Sheppard
* Sebastian Berg
* Warren Weckesser

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#19745 <https://github.com/numpy/numpy/pull/19745>`__: ENH: Add dtype-support to 3 `generic`/`ndarray` methods
* `#19955 <https://github.com/numpy/numpy/pull/19955>`__: BUG: Resolve Divide by Zero on Apple silicon + test failures...
* `#19958 <https://github.com/numpy/numpy/pull/19958>`__: MAINT: Mark type-check-only ufunc subclasses as ufunc aliases...
* `#19994 <https://github.com/numpy/numpy/pull/19994>`__: BUG: np.tan(np.inf) test failure
* `#20080 <https://github.com/numpy/numpy/pull/20080>`__: BUG: Correct incorrect advance in PCG with emulated int128
* `#20081 <https://github.com/numpy/numpy/pull/20081>`__: BUG: Fix NaT handling in the PyArray_CompareFunc for datetime...
* `#20082 <https://github.com/numpy/numpy/pull/20082>`__: DOC: Ensure that we add documentation also as to the dict for...
* `#20106 <https://github.com/numpy/numpy/pull/20106>`__: BUG: core: result_type(0, np.timedelta64(4)) would seg. fault.

Contributors
============

A total of 16 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Andreas Schwab
* Bharat Raghunathan +
* Bran +
* Charles Harris
* Eric Wieser
* Jakub Wilk
* Kevin Sheppard
* Marten van Kerkwijk
* Matti Picus
* Paul Ivanov
* Ralf Gommers
* Sebastian Berg
* Tyler Reddy
* Warren Weckesser
* Yu Feng
* adeak +

Pull requests merged
====================

A total of 26 pull requests were merged for this release.

* `#13072 <https://github.com/numpy/numpy/pull/13072>`__: BUG: Fixes to numpy.distutils.Configuration.get_version (#13056)
* `#13082 <https://github.com/numpy/numpy/pull/13082>`__: BUG: Fix errors in string formatting while producing an error
* `#13083 <https://github.com/numpy/numpy/pull/13083>`__: BUG: Convert fortran flags in environment variable
* `#13084 <https://github.com/numpy/numpy/pull/13084>`__: BUG: Remove error-prone borrowed reference handling
* `#13085 <https://github.com/numpy/numpy/pull/13085>`__: BUG: Add error checks when converting integers to datetime types
* `#13091 <https://github.com/numpy/numpy/pull/13091>`__: BUG: Remove our patched version of `distutils.split_quoted`
* `#13141 <https://github.com/numpy/numpy/pull/13141>`__: BUG: Fix testsuite failures on ppc and riscv
* `#13142 <https://github.com/numpy/numpy/pull/13142>`__: BUG: Fix parameter validity checks in ``random.choice``
* `#13143 <https://github.com/numpy/numpy/pull/13143>`__: BUG: Ensure linspace works on object input.
* `#13144 <https://github.com/numpy/numpy/pull/13144>`__: BLD: fix include list for sdist building.
* `#13145 <https://github.com/numpy/numpy/pull/13145>`__: BUG: __array_interface__ offset was always ignored
* `#13274 <https://github.com/numpy/numpy/pull/13274>`__: MAINT: f2py: Add a cast to avoid a compiler warning.
* `#13275 <https://github.com/numpy/numpy/pull/13275>`__: BUG, MAINT: fix reference count error on invalid input to ndarray.flat
* `#13276 <https://github.com/numpy/numpy/pull/13276>`__: ENH: Cast covariance to double in random mvnormal
* `#13278 <https://github.com/numpy/numpy/pull/13278>`__: BUG: Fix null pointer dereference in PyArray_DTypeFromObjectHelper
* `#13339 <https://github.com/numpy/numpy/pull/13339>`__: BUG: Use C call to sysctlbyname for AVX detection on MacOS.
* `#13340 <https://github.com/numpy/numpy/pull/13340>`__: BUG: Fix crash when calling savetxt on a padded array
* `#13341 <https://github.com/numpy/numpy/pull/13341>`__: BUG: ufunc.at iteration variable size fix
* `#13342 <https://github.com/numpy/numpy/pull/13342>`__: DOC: Add as_ctypes_type to the documentation
* `#13350 <https://github.com/numpy/numpy/pull/13350>`__: BUG: Return the coefficients array directly
* `#13351 <https://github.com/numpy/numpy/pull/13351>`__: BUG/MAINT: Tidy typeinfo.h and .c
* `#13359 <https://github.com/numpy/numpy/pull/13359>`__: BUG: Make allow_pickle=False the default for loading
* `#13360 <https://github.com/numpy/numpy/pull/13360>`__: DOC: fix some doctest failures
* `#13363 <https://github.com/numpy/numpy/pull/13363>`__: BUG/MAINT: Tidy typeinfo.h and .c
* `#13381 <https://github.com/numpy/numpy/pull/13381>`__: BLD: address mingw-w64 issue. Follow-up to gh-9977
* `#13382 <https://github.com/numpy/numpy/pull/13382>`__: REL: Prepare for the NumPy release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Matti Picus
* Rohit Goswami +
* Ross Barnowski
* Sayed Adel
* Sebastian Berg

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#20357 <https://github.com/numpy/numpy/pull/20357>`__: MAINT: Do not forward `__(deep)copy__` calls of `_GenericAlias`...
* `#20462 <https://github.com/numpy/numpy/pull/20462>`__: BUG: Fix float16 einsum fastpaths using wrong tempvar
* `#20463 <https://github.com/numpy/numpy/pull/20463>`__: BUG, DIST: Print os error message when the executable not exist
* `#20464 <https://github.com/numpy/numpy/pull/20464>`__: BLD: Verify the ability to compile C++ sources before initiating...
* `#20465 <https://github.com/numpy/numpy/pull/20465>`__: BUG: Force ``npymath` ` to respect ``npy_longdouble``
* `#20466 <https://github.com/numpy/numpy/pull/20466>`__: BUG: Fix failure to create aligned, empty structured dtype
* `#20467 <https://github.com/numpy/numpy/pull/20467>`__: ENH: provide a convenience function to replace npy_load_module
* `#20495 <https://github.com/numpy/numpy/pull/20495>`__: MAINT: update wheel to version that supports python3.10
* `#20497 <https://github.com/numpy/numpy/pull/20497>`__: BUG: Clear errors correctly in F2PY conversions
* `#20613 <https://github.com/numpy/numpy/pull/20613>`__: DEV: add a warningfilter to fix pytest workflow.
* `#20618 <https://github.com/numpy/numpy/pull/20618>`__: MAINT: Help boost::python libraries at least not crash

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Maxwell Aladago
* Pauli Virtanen
* Ralf Gommers
* Tyler Reddy
* Warren Weckesser

Pull requests merged
====================

A total of 13 pull requests were merged for this release.

* `#15158 <https://github.com/numpy/numpy/pull/15158>`__: MAINT: Update pavement.py for towncrier.
* `#15159 <https://github.com/numpy/numpy/pull/15159>`__: DOC: add moved modules to 1.18 release note
* `#15161 <https://github.com/numpy/numpy/pull/15161>`__: MAINT, DOC: Minor backports and updates for 1.18.x
* `#15176 <https://github.com/numpy/numpy/pull/15176>`__: TST: Add assert_array_equal test for big integer arrays
* `#15184 <https://github.com/numpy/numpy/pull/15184>`__: BUG: use tmp dir and check version for cython test (#15170)
* `#15220 <https://github.com/numpy/numpy/pull/15220>`__: BUG: distutils: fix msvc+gfortran openblas handling corner case
* `#15221 <https://github.com/numpy/numpy/pull/15221>`__: BUG: remove -std=c99 for c++ compilation (#15194)
* `#15222 <https://github.com/numpy/numpy/pull/15222>`__: MAINT: unskip test on win32
* `#15223 <https://github.com/numpy/numpy/pull/15223>`__: TST: add BLAS ILP64 run in Travis & Azure
* `#15245 <https://github.com/numpy/numpy/pull/15245>`__: MAINT: only add --std=c99 where needed
* `#15246 <https://github.com/numpy/numpy/pull/15246>`__: BUG: lib: Fix handling of integer arrays by gradient.
* `#15247 <https://github.com/numpy/numpy/pull/15247>`__: MAINT: Do not use private Python function in testing
* `#15250 <https://github.com/numpy/numpy/pull/15250>`__: REL: Prepare for the NumPy 1.18.1 release.

Contributors
============

A total of 133 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aaron Critchley +
* Aarthi +
* Aarthi Agurusa +
* Alex Thomas +
* Alexander Belopolsky
* Allan Haldane
* Anas Khan +
* Andras Deak
* Andrey Portnoy +
* Anna Chiara
* Aurelien Jarno +
* Baurzhan Muftakhidinov
* Berend Kapelle +
* Bernhard M. Wiedemann
* Bjoern Thiel +
* Bob Eldering
* Cenny Wenner +
* Charles Harris
* ChloeColeongco +
* Chris Billington +
* Christopher +
* Chun-Wei Yuan +
* Claudio Freire +
* Daniel Smith
* Darcy Meyer +
* David Abdurachmanov +
* David Freese
* Deepak Kumar Gouda +
* Dennis Weyland +
* Derrick Williams +
* Dmitriy Shalyga +
* Eric Cousineau +
* Eric Larson
* Eric Wieser
* Evgeni Burovski
* Frederick Lefebvre +
* Gaspar Karm +
* Geoffrey Irving
* Gerhard Hobler +
* Gerrit Holl
* Guo Ci +
* Hameer Abbasi +
* Han Shen
* Hiroyuki V. Yamazaki +
* Hong Xu
* Ihor Melnyk +
* Jaime Fernandez
* Jake VanderPlas +
* James Tocknell +
* Jarrod Millman
* Jeff VanOss +
* John Kirkham
* Jonas Rauber +
* Jonathan March +
* Joseph Fox-Rabinovitz
* Julian Taylor
* Junjie Bai +
* Juris Bogusevs +
* Jörg Döpfert
* Kenichi Maehashi +
* Kevin Sheppard
* Kimikazu Kato +
* Kirit Thadaka +
* Kritika Jalan +
* Kyle Sunden +
* Lakshay Garg +
* Lars G +
* Licht Takeuchi
* Louis Potok +
* Luke Zoltan Kelley
* MSeifert04 +
* Mads R. B. Kristensen +
* Malcolm Smith +
* Mark Harfouche +
* Marten H. van Kerkwijk +
* Marten van Kerkwijk
* Matheus Vieira Portela +
* Mathieu Lamarre
* Mathieu Sornay +
* Matthew Brett
* Matthew Rocklin +
* Matthias Bussonnier
* Matti Picus
* Michael Droettboom
* Miguel Sánchez de León Peque +
* Mike Toews +
* Milo +
* Nathaniel J. Smith
* Nelle Varoquaux
* Nicholas Nadeau, P.Eng., AVS +
* Nick Minkyu Lee +
* Nikita +
* Nikita Kartashov +
* Nils Becker +
* Oleg Zabluda
* Orestis Floros +
* Pat Gunn +
* Paul van Mulbregt +
* Pauli Virtanen
* Pierre Chanial +
* Ralf Gommers
* Raunak Shah +
* Robert Kern
* Russell Keith-Magee +
* Ryan Soklaski +
* Samuel Jackson +
* Sebastian Berg
* Siavash Eliasi +
* Simon Conseil
* Simon Gibbons
* Stefan Krah +
* Stefan van der Walt
* Stephan Hoyer
* Subhendu +
* Subhendu Ranjan Mishra +
* Tai-Lin Wu +
* Tobias Fischer +
* Toshiki Kataoka +
* Tyler Reddy +
* Unknown +
* Varun Nayyar
* Victor Rodriguez +
* Warren Weckesser
* William D. Irons +
* Zane Bradley +
* cclauss +
* fo40225 +
* lapack_lite code generator +
* lumbric +
* luzpaz +
* mamrehn +
* tynn +
* xoviat

Pull requests merged
====================

A total of 438 pull requests were merged for this release.

* `#8157 <https://github.com/numpy/numpy/pull/8157>`__: BUG: void .item() doesn't hold reference to original array
* `#8774 <https://github.com/numpy/numpy/pull/8774>`__: ENH: Add gcd and lcm ufuncs
* `#8819 <https://github.com/numpy/numpy/pull/8819>`__: ENH: Implement axes keyword argument for gufuncs.
* `#8952 <https://github.com/numpy/numpy/pull/8952>`__: MAINT: Removed duplicated code around `ufunc->identity`
* `#9686 <https://github.com/numpy/numpy/pull/9686>`__: DEP: Deprecate non-tuple nd-indices
* `#9980 <https://github.com/numpy/numpy/pull/9980>`__: MAINT: Implement `lstsq` as a `gufunc`
* `#9998 <https://github.com/numpy/numpy/pull/9998>`__: ENH: Nditer as context manager
* `#10073 <https://github.com/numpy/numpy/pull/10073>`__: ENH: Implement fft.fftshift/ifftshift with np.roll for improved...
* `#10078 <https://github.com/numpy/numpy/pull/10078>`__: DOC: document nested_iters
* `#10128 <https://github.com/numpy/numpy/pull/10128>`__: BUG: Prefix library names with `lib` on windows.
* `#10142 <https://github.com/numpy/numpy/pull/10142>`__: DEP: Pending deprecation warning for matrix
* `#10154 <https://github.com/numpy/numpy/pull/10154>`__: MAINT: Use a StructSequence in place of the typeinfo tuples
* `#10158 <https://github.com/numpy/numpy/pull/10158>`__: BUG: Fix a few smaller valgrind errors
* `#10178 <https://github.com/numpy/numpy/pull/10178>`__: MAINT: Prepare master for 1.15 development.
* `#10186 <https://github.com/numpy/numpy/pull/10186>`__: MAINT: Move histogram and histogramdd into their own module
* `#10187 <https://github.com/numpy/numpy/pull/10187>`__: BUG: Extra space is inserted on first line for long elements
* `#10192 <https://github.com/numpy/numpy/pull/10192>`__: DEP: Deprecate the pickle aliases
* `#10193 <https://github.com/numpy/numpy/pull/10193>`__: BUG: Fix bugs found by testing in release mode.
* `#10194 <https://github.com/numpy/numpy/pull/10194>`__: BUG, MAINT: Ufunc reduce reference leak
* `#10195 <https://github.com/numpy/numpy/pull/10195>`__: DOC: Fixup percentile docstring, from review in gh-9213
* `#10196 <https://github.com/numpy/numpy/pull/10196>`__: BUG: Fix regression in np.ma.load in gh-10055
* `#10199 <https://github.com/numpy/numpy/pull/10199>`__: ENH: Quantile
* `#10203 <https://github.com/numpy/numpy/pull/10203>`__: MAINT: Update development branch version to 1.15.0.
* `#10205 <https://github.com/numpy/numpy/pull/10205>`__: BUG: Handle NaNs correctly in arange
* `#10207 <https://github.com/numpy/numpy/pull/10207>`__: ENH: Allow `np.r_` to accept 0d arrays
* `#10208 <https://github.com/numpy/numpy/pull/10208>`__: MAINT: Improve error message for void(-1)
* `#10210 <https://github.com/numpy/numpy/pull/10210>`__: DOC: change 'a'->'prototype' in empty_like docs (addresses #10209)
* `#10211 <https://github.com/numpy/numpy/pull/10211>`__: MAINT,ENH: remove MaskedArray.astype, as the base type does everything.
* `#10212 <https://github.com/numpy/numpy/pull/10212>`__: DOC: fix minor typos
* `#10213 <https://github.com/numpy/numpy/pull/10213>`__: ENH: Set up proposed NEP process
* `#10214 <https://github.com/numpy/numpy/pull/10214>`__: DOC: add warning to isclose function
* `#10216 <https://github.com/numpy/numpy/pull/10216>`__: BUG: Fix broken format string picked up by LGTM.com
* `#10220 <https://github.com/numpy/numpy/pull/10220>`__: DOC: clarify that np.absolute == np.abs
* `#10223 <https://github.com/numpy/numpy/pull/10223>`__: ENH: added masked version of 'numpy.stack' with tests.
* `#10225 <https://github.com/numpy/numpy/pull/10225>`__: ENH: distutils: parallelize builds by default
* `#10226 <https://github.com/numpy/numpy/pull/10226>`__: BUG: distutils: use correct top-level package name
* `#10229 <https://github.com/numpy/numpy/pull/10229>`__: BUG: distutils: fix extra DLL loading in certain scenarios
* `#10231 <https://github.com/numpy/numpy/pull/10231>`__: BUG: Fix sign-compare warnings in datetime.c and datetime_strings.c.
* `#10232 <https://github.com/numpy/numpy/pull/10232>`__: BUG: Don't reimplement isclose in np.ma
* `#10237 <https://github.com/numpy/numpy/pull/10237>`__: DOC: give correct version of np.nansum change
* `#10241 <https://github.com/numpy/numpy/pull/10241>`__: MAINT: Avoid repeated validation of percentiles in nanpercentile
* `#10247 <https://github.com/numpy/numpy/pull/10247>`__: MAINT: fix typo
* `#10248 <https://github.com/numpy/numpy/pull/10248>`__: DOC: Add installation notes for Linux users
* `#10249 <https://github.com/numpy/numpy/pull/10249>`__: MAINT: Fix tests failures on travis CI merge.
* `#10250 <https://github.com/numpy/numpy/pull/10250>`__: MAINT: Check for `__array_ufunc__` before doing anything else.
* `#10251 <https://github.com/numpy/numpy/pull/10251>`__: ENH: Enable AVX2/AVX512 support to numpy
* `#10252 <https://github.com/numpy/numpy/pull/10252>`__: MAINT: Workaround for new travis sdist failures.
* `#10255 <https://github.com/numpy/numpy/pull/10255>`__: MAINT: Fix loop and simd sign-compare warnings.
* `#10257 <https://github.com/numpy/numpy/pull/10257>`__: BUG: duplicate message print if warning raises an exception
* `#10259 <https://github.com/numpy/numpy/pull/10259>`__: BUG: Make sure einsum default value of `optimize` is True.
* `#10260 <https://github.com/numpy/numpy/pull/10260>`__: ENH: Add pytest support
* `#10261 <https://github.com/numpy/numpy/pull/10261>`__: MAINT: Extract helper functions from histogram
* `#10262 <https://github.com/numpy/numpy/pull/10262>`__: DOC: Add missing release note for #10207
* `#10263 <https://github.com/numpy/numpy/pull/10263>`__: BUG: Fix strange behavior of infinite-step-size/underflow-case...
* `#10264 <https://github.com/numpy/numpy/pull/10264>`__: MAINT: Fix (some) yield warnings
* `#10266 <https://github.com/numpy/numpy/pull/10266>`__: BUG: distutils: fix locale decoding errors
* `#10268 <https://github.com/numpy/numpy/pull/10268>`__: BUG: Fix misleading error when coercing to array
* `#10269 <https://github.com/numpy/numpy/pull/10269>`__: MAINT: extract private helper function to compute histogram bin...
* `#10271 <https://github.com/numpy/numpy/pull/10271>`__: BUG: Allow nan values in the data when the bins are explicit
* `#10278 <https://github.com/numpy/numpy/pull/10278>`__: ENH: Add support for datetimes to histograms
* `#10282 <https://github.com/numpy/numpy/pull/10282>`__: MAINT: Extract helper function for last-bound-inclusive search_sorted
* `#10283 <https://github.com/numpy/numpy/pull/10283>`__: MAINT: Fallback on the default sequence multiplication behavior
* `#10284 <https://github.com/numpy/numpy/pull/10284>`__: MAINT/BUG: Tidy gen_umath
* `#10286 <https://github.com/numpy/numpy/pull/10286>`__: BUG: Fix memory leak (#10157).
* `#10287 <https://github.com/numpy/numpy/pull/10287>`__: ENH: Allow ptp to take an axis tuple and keepdims
* `#10292 <https://github.com/numpy/numpy/pull/10292>`__: BUG: Masked singleton can be reshaped to be non-scalar
* `#10293 <https://github.com/numpy/numpy/pull/10293>`__: MAINT: Fix sign-compare warnings in mem_overlap.c.
* `#10294 <https://github.com/numpy/numpy/pull/10294>`__: MAINT: pytest cleanups
* `#10298 <https://github.com/numpy/numpy/pull/10298>`__: DOC: Explain np.digitize and np.searchsorted more clearly
* `#10300 <https://github.com/numpy/numpy/pull/10300>`__: MAINT, DOC: Documentation and misc. typos
* `#10303 <https://github.com/numpy/numpy/pull/10303>`__: MAINT: Array wrap/prepare identification cleanup
* `#10309 <https://github.com/numpy/numpy/pull/10309>`__: MAINT: deduplicate check_nonreorderable_axes
* `#10314 <https://github.com/numpy/numpy/pull/10314>`__: BUG: Ensure `__array_finalize__` cannot back-mangle shape
* `#10316 <https://github.com/numpy/numpy/pull/10316>`__: DOC: add documentation about how to handle new array printing
* `#10320 <https://github.com/numpy/numpy/pull/10320>`__: BUG: skip the extra-dll directory when there are no DLLS
* `#10323 <https://github.com/numpy/numpy/pull/10323>`__: MAINT: Remove duplicated code for promoting dtype and array types.
* `#10324 <https://github.com/numpy/numpy/pull/10324>`__: BUG: Fix crashes when using float32 values in uniform histograms
* `#10325 <https://github.com/numpy/numpy/pull/10325>`__: MAINT: Replace manual expansion of PyArray_MinScalarType with...
* `#10327 <https://github.com/numpy/numpy/pull/10327>`__: MAINT: Fix misc. typos
* `#10333 <https://github.com/numpy/numpy/pull/10333>`__: DOC: typo fix in numpy.linalg.det docstring
* `#10334 <https://github.com/numpy/numpy/pull/10334>`__: DOC: Fix typos in docs for partition method
* `#10336 <https://github.com/numpy/numpy/pull/10336>`__: DOC: Post 1.14.0 release updates.
* `#10337 <https://github.com/numpy/numpy/pull/10337>`__: ENH: Show the silenced error and traceback in warning `__cause__`
* `#10341 <https://github.com/numpy/numpy/pull/10341>`__: BUG: fix config where PATH isn't set on win32
* `#10342 <https://github.com/numpy/numpy/pull/10342>`__: BUG: arrays not being flattened in `union1d`
* `#10346 <https://github.com/numpy/numpy/pull/10346>`__: ENH: Check matching inputs/outputs in umath generation
* `#10352 <https://github.com/numpy/numpy/pull/10352>`__: BUG: Fix einsum optimize logic for singleton dimensions
* `#10354 <https://github.com/numpy/numpy/pull/10354>`__: BUG: fix error message not formatted in einsum
* `#10359 <https://github.com/numpy/numpy/pull/10359>`__: BUG: do not optimize einsum with only 2 arguments.
* `#10361 <https://github.com/numpy/numpy/pull/10361>`__: BUG: complex repr has extra spaces, missing +
* `#10362 <https://github.com/numpy/numpy/pull/10362>`__: MAINT: Update download URL in setup.py.
* `#10367 <https://github.com/numpy/numpy/pull/10367>`__: BUG: add missing paren and remove quotes from repr of fieldless...
* `#10371 <https://github.com/numpy/numpy/pull/10371>`__: BUG: fix einsum issue with unicode input and py2
* `#10381 <https://github.com/numpy/numpy/pull/10381>`__: BUG/ENH: Improve output for structured non-void types
* `#10388 <https://github.com/numpy/numpy/pull/10388>`__: ENH: Add types for int and uint of explicit sizes to swig.
* `#10390 <https://github.com/numpy/numpy/pull/10390>`__: MAINT: Adjust type promotion in linalg.norm
* `#10391 <https://github.com/numpy/numpy/pull/10391>`__: BUG: Make dtype.descr error for out-of-order fields
* `#10392 <https://github.com/numpy/numpy/pull/10392>`__: DOC: Document behaviour of `np.concatenate` with `axis=None`
* `#10401 <https://github.com/numpy/numpy/pull/10401>`__: BUG: Resize bytes_ columns in genfromtxt
* `#10402 <https://github.com/numpy/numpy/pull/10402>`__: DOC: added "steals a reference" to PyArray_FromAny
* `#10406 <https://github.com/numpy/numpy/pull/10406>`__: ENH: add `np.printoptions`, a context manager
* `#10411 <https://github.com/numpy/numpy/pull/10411>`__: BUG: Revert multifield-indexing adds padding bytes for NumPy...
* `#10412 <https://github.com/numpy/numpy/pull/10412>`__: ENH: Fix repr of np.record objects to match np.void types
* `#10414 <https://github.com/numpy/numpy/pull/10414>`__: MAINT: Fix sign-compare warnings in umath_linalg.
* `#10415 <https://github.com/numpy/numpy/pull/10415>`__: MAINT: Fix sign-compare warnings in npy_binsearch, npy_partition.
* `#10416 <https://github.com/numpy/numpy/pull/10416>`__: MAINT: Fix sign-compare warnings in dragon4.c.
* `#10418 <https://github.com/numpy/numpy/pull/10418>`__: MAINT: Remove repeated #ifdefs implementing `isinstance(x, basestring)`...
* `#10420 <https://github.com/numpy/numpy/pull/10420>`__: DOC: Fix version added labels in numpy.unique docs
* `#10421 <https://github.com/numpy/numpy/pull/10421>`__: DOC: Fix type of axis in nanfunctions
* `#10423 <https://github.com/numpy/numpy/pull/10423>`__: MAINT: Update zesty to artful for i386 testing
* `#10426 <https://github.com/numpy/numpy/pull/10426>`__: DOC: Add version when linalg.norm accepted axis
* `#10427 <https://github.com/numpy/numpy/pull/10427>`__: DOC: Fix typo in docs for argpartition
* `#10430 <https://github.com/numpy/numpy/pull/10430>`__: MAINT: Use ValueError for duplicate field names in lookup
* `#10433 <https://github.com/numpy/numpy/pull/10433>`__: DOC: Add 1.14.1 release notes template (forward port)
* `#10434 <https://github.com/numpy/numpy/pull/10434>`__: MAINT: Move `tools/announce.py` to `tools/changelog.py`.
* `#10441 <https://github.com/numpy/numpy/pull/10441>`__: BUG: Fix nan_to_num return with integer input
* `#10443 <https://github.com/numpy/numpy/pull/10443>`__: BUG: Fix various Big-Endian test failures (ppc64)
* `#10444 <https://github.com/numpy/numpy/pull/10444>`__: MAINT: Implement float128 dragon4 for IBM double-double (ppc64)
* `#10451 <https://github.com/numpy/numpy/pull/10451>`__: BUG: prevent the MSVC 14.1 compiler (Visual Studio 2017) from...
* `#10453 <https://github.com/numpy/numpy/pull/10453>`__: Revert "BUG: prevent the MSVC 14.1 compiler (Visual Studio 2017)...
* `#10458 <https://github.com/numpy/numpy/pull/10458>`__: BLD: Use zip_safe=False in setup() call
* `#10459 <https://github.com/numpy/numpy/pull/10459>`__: MAINT: Remove duplicated logic between array_wrap and array_prepare
* `#10463 <https://github.com/numpy/numpy/pull/10463>`__: ENH: Add entry_points for f2py, conv_template, and from_template.
* `#10465 <https://github.com/numpy/numpy/pull/10465>`__: MAINT: Fix miscellaneous sign-compare warnings.
* `#10472 <https://github.com/numpy/numpy/pull/10472>`__: DOC: Document A@B in Matlab/NumPy summary table
* `#10473 <https://github.com/numpy/numpy/pull/10473>`__: BUG: Fixed polydiv for Complex Numbers
* `#10475 <https://github.com/numpy/numpy/pull/10475>`__: DOC: Add CircleCI builder for devdocs
* `#10476 <https://github.com/numpy/numpy/pull/10476>`__: DOC: fix formatting in interp example
* `#10477 <https://github.com/numpy/numpy/pull/10477>`__: BUG: Align type definition with generated lapack
* `#10478 <https://github.com/numpy/numpy/pull/10478>`__: DOC: Minor punctuation cleanups and improved explanation.
* `#10479 <https://github.com/numpy/numpy/pull/10479>`__: BUG: Fix calling ufuncs with a positional output argument.
* `#10482 <https://github.com/numpy/numpy/pull/10482>`__: BUG: Add missing DECREF in Py2 int() cast
* `#10484 <https://github.com/numpy/numpy/pull/10484>`__: MAINT: Remove unused code path for applying maskedarray domains...
* `#10497 <https://github.com/numpy/numpy/pull/10497>`__: DOC: Tell matlab users about np.block
* `#10498 <https://github.com/numpy/numpy/pull/10498>`__: MAINT: Remove special cases in np.unique
* `#10501 <https://github.com/numpy/numpy/pull/10501>`__: BUG: fromregex: asbytes called on regexp objects
* `#10502 <https://github.com/numpy/numpy/pull/10502>`__: MAINT: Use AxisError in swapaxes, unique, and diagonal
* `#10503 <https://github.com/numpy/numpy/pull/10503>`__: BUG: Fix unused-result warning.
* `#10506 <https://github.com/numpy/numpy/pull/10506>`__: MAINT: Delete unused `_build_utils/common.py`
* `#10508 <https://github.com/numpy/numpy/pull/10508>`__: BUG: Add missing `#define _MULTIARRAYMODULE` to vdot.c
* `#10509 <https://github.com/numpy/numpy/pull/10509>`__: MAINT: Use new-style format strings for clarity
* `#10516 <https://github.com/numpy/numpy/pull/10516>`__: MAINT: Allow errors to escape from InitOperators
* `#10518 <https://github.com/numpy/numpy/pull/10518>`__: ENH: Add a repr to np._NoValue
* `#10522 <https://github.com/numpy/numpy/pull/10522>`__: MAINT: Remove the unmaintained umath ``__version__`` constant.
* `#10524 <https://github.com/numpy/numpy/pull/10524>`__: BUG: fix np.save issue with python 2.7.5
* `#10529 <https://github.com/numpy/numpy/pull/10529>`__: BUG: Provide a better error message for out-of-order fields
* `#10543 <https://github.com/numpy/numpy/pull/10543>`__: DEP: Issue FutureWarning when malformed records detected.
* `#10544 <https://github.com/numpy/numpy/pull/10544>`__: BUG: infinite recursion in str of 0d subclasses
* `#10546 <https://github.com/numpy/numpy/pull/10546>`__: BUG: In numpy.i, clear CARRAY flag if wrapped buffer is not C_CONTIGUOUS.
* `#10547 <https://github.com/numpy/numpy/pull/10547>`__: DOC: Fix incorrect formula in gradient docstring.
* `#10548 <https://github.com/numpy/numpy/pull/10548>`__: BUG: Set missing exception after malloc
* `#10549 <https://github.com/numpy/numpy/pull/10549>`__: ENH: Make NpzFile conform to the Mapping protocol
* `#10553 <https://github.com/numpy/numpy/pull/10553>`__: MAINT: Cleanups to promote_types and result_types
* `#10554 <https://github.com/numpy/numpy/pull/10554>`__: DOC: promote_types is not associative by design,
* `#10555 <https://github.com/numpy/numpy/pull/10555>`__: BUG: Add missing PyErr_NoMemory() after malloc
* `#10564 <https://github.com/numpy/numpy/pull/10564>`__: BUG: Provide correct format in Py_buffer for scalars
* `#10566 <https://github.com/numpy/numpy/pull/10566>`__: BUG: Fix travis failure in previous commit
* `#10571 <https://github.com/numpy/numpy/pull/10571>`__: BUG: Fix corner-case behavior of cond() and use SVD when possible
* `#10576 <https://github.com/numpy/numpy/pull/10576>`__: MAINT: Fix misc. documentation typos
* `#10583 <https://github.com/numpy/numpy/pull/10583>`__: MAINT: Fix typos in DISTUTILS.rst.txt.
* `#10588 <https://github.com/numpy/numpy/pull/10588>`__: BUG: Revert sort optimization in np.unique.
* `#10589 <https://github.com/numpy/numpy/pull/10589>`__: BUG: fix entry_points typo for from-template
* `#10591 <https://github.com/numpy/numpy/pull/10591>`__: ENH: Add histogram_bin_edges function and test
* `#10592 <https://github.com/numpy/numpy/pull/10592>`__: DOC: Corrected url for Guide to NumPy book; see part of #8520,...
* `#10596 <https://github.com/numpy/numpy/pull/10596>`__: MAINT: Update sphinxext submodule hash.
* `#10599 <https://github.com/numpy/numpy/pull/10599>`__: ENH: Make flatnonzero call asanyarray before ravel()
* `#10603 <https://github.com/numpy/numpy/pull/10603>`__: MAINT: Improve error message in histogram.
* `#10604 <https://github.com/numpy/numpy/pull/10604>`__: MAINT: Fix Misc. typos
* `#10606 <https://github.com/numpy/numpy/pull/10606>`__: MAINT: Do not use random roots when testing roots.
* `#10618 <https://github.com/numpy/numpy/pull/10618>`__: MAINT: Stop using non-tuple indices internally
* `#10619 <https://github.com/numpy/numpy/pull/10619>`__: BUG: np.ma.flatnotmasked_contiguous behaves differently on mask=nomask...
* `#10621 <https://github.com/numpy/numpy/pull/10621>`__: BUG: deallocate recursive closure in arrayprint.py
* `#10623 <https://github.com/numpy/numpy/pull/10623>`__: BUG: Correctly identify comma separated dtype strings
* `#10625 <https://github.com/numpy/numpy/pull/10625>`__: BUG: Improve the accuracy of the FFT implementation
* `#10635 <https://github.com/numpy/numpy/pull/10635>`__: ENH: Implement initial kwarg for ufunc.add.reduce
* `#10641 <https://github.com/numpy/numpy/pull/10641>`__: MAINT: Post 1.14.1 release updates for master branch
* `#10650 <https://github.com/numpy/numpy/pull/10650>`__: BUG: Fix missing NPY_VISIBILITY_HIDDEN on npy_longdouble_to_PyLong
* `#10653 <https://github.com/numpy/numpy/pull/10653>`__: MAINT: Remove duplicate implementation for aliased functions.
* `#10657 <https://github.com/numpy/numpy/pull/10657>`__: BUG: f2py: fix f2py generated code to work on Pypy
* `#10658 <https://github.com/numpy/numpy/pull/10658>`__: BUG: Make np.partition and np.sort work on np.matrix when axis=None
* `#10660 <https://github.com/numpy/numpy/pull/10660>`__: BUG/MAINT: Remove special cases for 0d arrays in interp
* `#10661 <https://github.com/numpy/numpy/pull/10661>`__: MAINT: Unify reductions in fromnumeric.py
* `#10665 <https://github.com/numpy/numpy/pull/10665>`__: ENH: umath: don't make temporary copies for in-place accumulation
* `#10666 <https://github.com/numpy/numpy/pull/10666>`__: BUG: fix complex casting error in cov with aweights
* `#10669 <https://github.com/numpy/numpy/pull/10669>`__: MAINT: Covariance must be symmetric as well as positive-semidefinite.
* `#10670 <https://github.com/numpy/numpy/pull/10670>`__: DEP: Deprecate np.sum(generator)
* `#10671 <https://github.com/numpy/numpy/pull/10671>`__: DOC/MAINT: More misc. typos
* `#10672 <https://github.com/numpy/numpy/pull/10672>`__: ENH: Allow dtype field names to be ascii encoded unicode in Python2
* `#10676 <https://github.com/numpy/numpy/pull/10676>`__: BUG: F2py mishandles quoted control characters
* `#10677 <https://github.com/numpy/numpy/pull/10677>`__: STY: Minor stylistic cleanup of numeric.py
* `#10679 <https://github.com/numpy/numpy/pull/10679>`__: DOC: zeros, empty, and ones now have consistent docstrings
* `#10684 <https://github.com/numpy/numpy/pull/10684>`__: ENH: Modify intersect1d to return common indices
* `#10689 <https://github.com/numpy/numpy/pull/10689>`__: BLD: Add configuration changes to allow cross platform builds...
* `#10691 <https://github.com/numpy/numpy/pull/10691>`__: DOC: add versionadded for NDArrayOperatorsMixin.
* `#10694 <https://github.com/numpy/numpy/pull/10694>`__: DOC: Improve docstring of memmap
* `#10698 <https://github.com/numpy/numpy/pull/10698>`__: BUG: Further back-compat fix for subclassed array repr (forward...
* `#10699 <https://github.com/numpy/numpy/pull/10699>`__: DOC: Grammar of np.gradient docstring
* `#10702 <https://github.com/numpy/numpy/pull/10702>`__: TST, DOC: Upload devdocs and neps after circleci build
* `#10703 <https://github.com/numpy/numpy/pull/10703>`__: MAINT: NEP process updates
* `#10708 <https://github.com/numpy/numpy/pull/10708>`__: BUG: fix problem with modifying pyf lines containing ';' in f2py
* `#10710 <https://github.com/numpy/numpy/pull/10710>`__: BUG: fix error message in numpy.select
* `#10711 <https://github.com/numpy/numpy/pull/10711>`__: MAINT: Hard tab and whitespace cleanup.
* `#10715 <https://github.com/numpy/numpy/pull/10715>`__: MAINT: Fixed C++ guard in f2py test.
* `#10716 <https://github.com/numpy/numpy/pull/10716>`__: BUG: dragon4 fractional output mode adds too many trailing zeros
* `#10718 <https://github.com/numpy/numpy/pull/10718>`__: BUG: Fix bug in asserting near equality of float16 arrays.
* `#10719 <https://github.com/numpy/numpy/pull/10719>`__: DOC: add documentation for constants
* `#10720 <https://github.com/numpy/numpy/pull/10720>`__: BUG: distutils: Remove named templates from the processed output...
* `#10722 <https://github.com/numpy/numpy/pull/10722>`__: MAINT: Misc small fixes.
* `#10730 <https://github.com/numpy/numpy/pull/10730>`__: DOC: Fix minor typo in how-to-document.
* `#10732 <https://github.com/numpy/numpy/pull/10732>`__: BUG: Fix `setup.py build install egg_info`, which did not previously...
* `#10734 <https://github.com/numpy/numpy/pull/10734>`__: DOC: Post 1.14.2 release update.
* `#10737 <https://github.com/numpy/numpy/pull/10737>`__: MAINT: Fix low-hanging PyPy compatibility issues
* `#10739 <https://github.com/numpy/numpy/pull/10739>`__: BUG: Fix histogram bins="auto" for data with little variance
* `#10740 <https://github.com/numpy/numpy/pull/10740>`__: MAINT, TST: Fixes for Python 3.7
* `#10743 <https://github.com/numpy/numpy/pull/10743>`__: MAINT: Import abstract classes from collections.abc
* `#10745 <https://github.com/numpy/numpy/pull/10745>`__: ENH: Add object loops to the comparison ufuncs
* `#10746 <https://github.com/numpy/numpy/pull/10746>`__: MAINT: Fix typo in warning message
* `#10748 <https://github.com/numpy/numpy/pull/10748>`__: DOC: a.size and np.prod(a.shape) are not equivalent
* `#10750 <https://github.com/numpy/numpy/pull/10750>`__: DOC: Add graph showing different behaviors of np.percentile
* `#10755 <https://github.com/numpy/numpy/pull/10755>`__: DOC: Move bin estimator documentation from `histogram` to `histogram_bin_edges`
* `#10758 <https://github.com/numpy/numpy/pull/10758>`__: TST: Change most travisci tests to Python3.6.
* `#10763 <https://github.com/numpy/numpy/pull/10763>`__: BUG: floating types should override tp_print
* `#10766 <https://github.com/numpy/numpy/pull/10766>`__: MAINT: Remove the unused scalarmath getters for fmod and sqrt
* `#10773 <https://github.com/numpy/numpy/pull/10773>`__: BUG: Use dummy_threading on platforms that don't support threading
* `#10774 <https://github.com/numpy/numpy/pull/10774>`__: BUG: Fix SQRT_MIN for platforms with 8-byte long double
* `#10775 <https://github.com/numpy/numpy/pull/10775>`__: BUG: Return NULL from PyInit_* when exception is raised
* `#10777 <https://github.com/numpy/numpy/pull/10777>`__: MAINT: Remove use of unittest in NumPy tests.
* `#10778 <https://github.com/numpy/numpy/pull/10778>`__: BUG: test, fix for missing flags['WRITEBACKIFCOPY'] key
* `#10781 <https://github.com/numpy/numpy/pull/10781>`__: ENH: NEP index builder
* `#10785 <https://github.com/numpy/numpy/pull/10785>`__: DOC: Fixed author name in reference to book
* `#10786 <https://github.com/numpy/numpy/pull/10786>`__: ENH: Add "stable" option to np.sort as an alias for "mergesort".
* `#10790 <https://github.com/numpy/numpy/pull/10790>`__: TST: Various fixes prior to switching to pytest
* `#10795 <https://github.com/numpy/numpy/pull/10795>`__: BUG: Allow spaces in output string of einsum
* `#10796 <https://github.com/numpy/numpy/pull/10796>`__: BUG: fix wrong inplace vectorization on overlapping arguments
* `#10798 <https://github.com/numpy/numpy/pull/10798>`__: BUG: error checking before mapping of einsum axes.
* `#10800 <https://github.com/numpy/numpy/pull/10800>`__: DOC: Add remarks about array vs scalar output to every ufunc
* `#10802 <https://github.com/numpy/numpy/pull/10802>`__: BUG/DOC/MAINT: Tidy up histogramdd
* `#10807 <https://github.com/numpy/numpy/pull/10807>`__: DOC: Update link to tox in development docs (#10806)
* `#10812 <https://github.com/numpy/numpy/pull/10812>`__: MAINT: Rearrange `numpy/testing` files
* `#10814 <https://github.com/numpy/numpy/pull/10814>`__: BUG: verify the OS supports avx instruction
* `#10822 <https://github.com/numpy/numpy/pull/10822>`__: BUG: fixes exception in numpy.genfromtxt, see #10780
* `#10824 <https://github.com/numpy/numpy/pull/10824>`__: BUG: test, fix PyArray_DiscardWritebackIfCopy refcount issue...
* `#10826 <https://github.com/numpy/numpy/pull/10826>`__: BUG: np.squeeze() now respects older API axis expectation
* `#10827 <https://github.com/numpy/numpy/pull/10827>`__: ENH: Add tester for pytest.
* `#10828 <https://github.com/numpy/numpy/pull/10828>`__: BUG: fix obvious mistake in testing/decorators warning.
* `#10829 <https://github.com/numpy/numpy/pull/10829>`__: BLD: use Python 3.6 instead of 2.7 as default for doc build.
* `#10830 <https://github.com/numpy/numpy/pull/10830>`__: BUG: Fix obvious warning bugs.
* `#10831 <https://github.com/numpy/numpy/pull/10831>`__: DOC: Fix minor typos
* `#10832 <https://github.com/numpy/numpy/pull/10832>`__: ENH: datetime64: support AC dates starting with '+'
* `#10833 <https://github.com/numpy/numpy/pull/10833>`__: ENH: Add support for the 64-bit RISC-V architecture
* `#10834 <https://github.com/numpy/numpy/pull/10834>`__: DOC: note that NDEBUG should be set when OPT should increase...
* `#10836 <https://github.com/numpy/numpy/pull/10836>`__: MAINT: Fix script name for pushing NEP docs to repo
* `#10840 <https://github.com/numpy/numpy/pull/10840>`__: MAINT: Fix typo in code example.
* `#10842 <https://github.com/numpy/numpy/pull/10842>`__: TST: Switch to pytest
* `#10849 <https://github.com/numpy/numpy/pull/10849>`__: DOC: fix examples in docstring for np.flip
* `#10850 <https://github.com/numpy/numpy/pull/10850>`__: DEP: Issue deprecation warnings for some imports.
* `#10858 <https://github.com/numpy/numpy/pull/10858>`__: MAINT: Post pytest switch cleanup
* `#10859 <https://github.com/numpy/numpy/pull/10859>`__: MAINT: Remove yield tests
* `#10860 <https://github.com/numpy/numpy/pull/10860>`__: BUG: core: fix NPY_TITLE_KEY macro on pypy
* `#10863 <https://github.com/numpy/numpy/pull/10863>`__: MAINT: More Histogramdd cleanup
* `#10867 <https://github.com/numpy/numpy/pull/10867>`__: DOC: Cross Link full/full_like in a few see-also sections.
* `#10869 <https://github.com/numpy/numpy/pull/10869>`__: BUG: Fix encoding regression in ma/bench.py (Issue #10868)
* `#10871 <https://github.com/numpy/numpy/pull/10871>`__: MAINT: Remove unnecessary special case in np.histogramdd for...
* `#10872 <https://github.com/numpy/numpy/pull/10872>`__: ENH: Extend np.flip to work over multiple axes
* `#10874 <https://github.com/numpy/numpy/pull/10874>`__: DOC: State in docstring that lexsort is stable (#10873).
* `#10875 <https://github.com/numpy/numpy/pull/10875>`__: BUG: fix savetxt, loadtxt for '+-' in complex
* `#10878 <https://github.com/numpy/numpy/pull/10878>`__: DOC: rework documents and silence warnings during sphinx build
* `#10882 <https://github.com/numpy/numpy/pull/10882>`__: BUG: have `_array_from_buffer_3118` correctly handle errors
* `#10883 <https://github.com/numpy/numpy/pull/10883>`__: DOC: Fix negative binomial documentation.
* `#10885 <https://github.com/numpy/numpy/pull/10885>`__: TST: Re-enable test display on appveyor
* `#10890 <https://github.com/numpy/numpy/pull/10890>`__: MAINT: lstsq: compute residuals inside the ufunc
* `#10891 <https://github.com/numpy/numpy/pull/10891>`__: TST: Extract a helper function to test for reference cycles
* `#10898 <https://github.com/numpy/numpy/pull/10898>`__: ENH: Have dtype transfer for equivalent user dtypes prefer user-defined...
* `#10901 <https://github.com/numpy/numpy/pull/10901>`__: DOC, BUG : Bad link to `np.random.randint`
* `#10903 <https://github.com/numpy/numpy/pull/10903>`__: DOC: Fix link in `See Also` section of `randn` docstring.
* `#10907 <https://github.com/numpy/numpy/pull/10907>`__: TST: reactivate module docstring tests, fix float formatting
* `#10911 <https://github.com/numpy/numpy/pull/10911>`__: BUG: Fix casting between npy_half and float in einsum
* `#10916 <https://github.com/numpy/numpy/pull/10916>`__: BUG: Add missing underscore to prototype in check_embedded_lapack
* `#10919 <https://github.com/numpy/numpy/pull/10919>`__: BUG: Pass non-None outputs to `__array_prepare__` and `__array_wrap__`
* `#10921 <https://github.com/numpy/numpy/pull/10921>`__: DOC: clear up warnings, fix matplotlib plot
* `#10923 <https://github.com/numpy/numpy/pull/10923>`__: BUG: fixed dtype alignment for array of structs in case of converting...
* `#10925 <https://github.com/numpy/numpy/pull/10925>`__: DOC: Fix typos in 1.15.0 changelog
* `#10936 <https://github.com/numpy/numpy/pull/10936>`__: DOC: Fix NumpyVersion example (closes gh-10935)
* `#10938 <https://github.com/numpy/numpy/pull/10938>`__: MAINT: One step closer to vectorizing lstsq
* `#10940 <https://github.com/numpy/numpy/pull/10940>`__: DOC: fix broken links for developer documentation
* `#10943 <https://github.com/numpy/numpy/pull/10943>`__: ENH: Add a search box to the sidebar in the docs
* `#10945 <https://github.com/numpy/numpy/pull/10945>`__: MAINT: Remove references to the 2008 documentation marathon
* `#10946 <https://github.com/numpy/numpy/pull/10946>`__: BUG: 'style' arg to array2string broken in legacy mode
* `#10949 <https://github.com/numpy/numpy/pull/10949>`__: DOC: cleanup documentation, continuation of nditer PR #9998
* `#10951 <https://github.com/numpy/numpy/pull/10951>`__: BUG: it.close() disallows access to iterator, fixes #10950
* `#10953 <https://github.com/numpy/numpy/pull/10953>`__: MAINT: address extraneous shape tuple checks in descriptor.c
* `#10958 <https://github.com/numpy/numpy/pull/10958>`__: MAINT, DOC: Fix typos
* `#10967 <https://github.com/numpy/numpy/pull/10967>`__: DOC: add quantile, nanquantile to toc
* `#10970 <https://github.com/numpy/numpy/pull/10970>`__: WIP: Remove fragile use of `__array_interface__` in ctypeslib.as_array
* `#10971 <https://github.com/numpy/numpy/pull/10971>`__: MAINT: Remove workaround for gh-10891
* `#10973 <https://github.com/numpy/numpy/pull/10973>`__: DOC: advise against use of matrix.
* `#10975 <https://github.com/numpy/numpy/pull/10975>`__: MAINT: move linalg tests using matrix to matrixlib
* `#10980 <https://github.com/numpy/numpy/pull/10980>`__: DOC: link to governance, convert external link to internal
* `#10984 <https://github.com/numpy/numpy/pull/10984>`__: MAINT: Added pytest cache folder to .gitignore
* `#10985 <https://github.com/numpy/numpy/pull/10985>`__: MAINT, ENH: Move matrix_power to linalg and allow higher dimensions.
* `#10986 <https://github.com/numpy/numpy/pull/10986>`__: MAINT: move all masked array matrix tests to matrixlib.
* `#10987 <https://github.com/numpy/numpy/pull/10987>`__: DOC: Correction to docstring example (result was correct)
* `#10988 <https://github.com/numpy/numpy/pull/10988>`__: MAINT: Small tidy-ups to ufunc_object.c
* `#10991 <https://github.com/numpy/numpy/pull/10991>`__: DOC: Update genfromtxt docs to use StringIO and u-strings
* `#10996 <https://github.com/numpy/numpy/pull/10996>`__: DOC: Make doc examples using StringIO python2-3 compatible
* `#11003 <https://github.com/numpy/numpy/pull/11003>`__: DOC: work around GH isaacs/github#316 to show SVG image
* `#11005 <https://github.com/numpy/numpy/pull/11005>`__: MAINT: Misc. typos
* `#11006 <https://github.com/numpy/numpy/pull/11006>`__: TST, BUILD: add latex to circleci doc build
* `#11008 <https://github.com/numpy/numpy/pull/11008>`__: REL: Fwd port 1.14.3 changelog
* `#11009 <https://github.com/numpy/numpy/pull/11009>`__: DOC: release walkthrough updates from 1.14.3
* `#11010 <https://github.com/numpy/numpy/pull/11010>`__: Move remaining Matrix tests to matrixlib
* `#11011 <https://github.com/numpy/numpy/pull/11011>`__: MAINT: Simplify dimension-juggling in np.pad
* `#11012 <https://github.com/numpy/numpy/pull/11012>`__: MAINT: np.pad: Add helper functions for producing slices along...
* `#11018 <https://github.com/numpy/numpy/pull/11018>`__: ENH: Implement axis for generalized ufuncs.
* `#11023 <https://github.com/numpy/numpy/pull/11023>`__: BUG: np.histogramdd loses precision on its inputs, leading to...
* `#11026 <https://github.com/numpy/numpy/pull/11026>`__: MAINT: reduce code duplication in ufunc_frompyfunc
* `#11033 <https://github.com/numpy/numpy/pull/11033>`__: BUG: Fix padding with large integers
* `#11036 <https://github.com/numpy/numpy/pull/11036>`__: BUG: optimizing compilers can reorder call to npy_get_floatstatus
* `#11037 <https://github.com/numpy/numpy/pull/11037>`__: BUG: initialize value before use
* `#11038 <https://github.com/numpy/numpy/pull/11038>`__: ENH: Add `__deepcopy__` to MaskedConstant
* `#11043 <https://github.com/numpy/numpy/pull/11043>`__: BUG: reduce using SSE only warns if inside SSE loop
* `#11050 <https://github.com/numpy/numpy/pull/11050>`__: BUG: remove fast scalar power for arrays with object dtype
* `#11053 <https://github.com/numpy/numpy/pull/11053>`__: DOC: bump scipy-sphinx-theme to current version
* `#11055 <https://github.com/numpy/numpy/pull/11055>`__: DOC: Add explanation for comments=None in loadtxt.
* `#11056 <https://github.com/numpy/numpy/pull/11056>`__: MAINT: Improve performance of random permutation
* `#11057 <https://github.com/numpy/numpy/pull/11057>`__: BUG: use absolute imports in test files
* `#11066 <https://github.com/numpy/numpy/pull/11066>`__: MAINT: `distutils.system_info`: handle Accelerate like any other...
* `#11073 <https://github.com/numpy/numpy/pull/11073>`__: DOC: expand reasoning behind npy_*floatstatus_barrer()
* `#11076 <https://github.com/numpy/numpy/pull/11076>`__: BUG: Ensure `PyArray_AssignRawScalar` respects `NPY_NEEDS_INIT`
* `#11082 <https://github.com/numpy/numpy/pull/11082>`__: DOC: link to updated module docstring, not NEP
* `#11083 <https://github.com/numpy/numpy/pull/11083>`__: ENH: remove nose from travis tests
* `#11085 <https://github.com/numpy/numpy/pull/11085>`__: DOC: create label and ref, fixes broken link
* `#11086 <https://github.com/numpy/numpy/pull/11086>`__: DOC: Mention we can return unitinitialized values
* `#11089 <https://github.com/numpy/numpy/pull/11089>`__: BLD: cleanup `_configtest.o.d` during build
* `#11090 <https://github.com/numpy/numpy/pull/11090>`__: BUG: Added support for index values 27-52 in C einsum
* `#11091 <https://github.com/numpy/numpy/pull/11091>`__: BUG: Python2 doubles don't print correctly in interactive shell
* `#11094 <https://github.com/numpy/numpy/pull/11094>`__: DOC: add numpy.lib.format to docs and link to it
* `#11095 <https://github.com/numpy/numpy/pull/11095>`__: MAINT: Einsum argument parsing cleanup
* `#11097 <https://github.com/numpy/numpy/pull/11097>`__: BUG: fix datetime.timedelta->timedelta64 unit detection logic
* `#11098 <https://github.com/numpy/numpy/pull/11098>`__: ENH: Add keepdims argument for generalized ufuncs.
* `#11105 <https://github.com/numpy/numpy/pull/11105>`__: ENH: Add (put|take)_along_axis
* `#11111 <https://github.com/numpy/numpy/pull/11111>`__: BUG: fix case of ISA selector in ufunc selection
* `#11116 <https://github.com/numpy/numpy/pull/11116>`__: BUG: Typo in variable name in binary_repr
* `#11120 <https://github.com/numpy/numpy/pull/11120>`__: MAINT: remove redundant code in `MaskedArray.__new__`
* `#11122 <https://github.com/numpy/numpy/pull/11122>`__: BUG,MAINT: Ensure masked elements can be tested against nan and...
* `#11124 <https://github.com/numpy/numpy/pull/11124>`__: BUG: Ensure that fully masked arrays pass assert_array_equal.
* `#11134 <https://github.com/numpy/numpy/pull/11134>`__: DOC: Clarify tofile requirements
* `#11137 <https://github.com/numpy/numpy/pull/11137>`__: MAINT: move remaining MaskedArray matrix tests to matrixlib.
* `#11139 <https://github.com/numpy/numpy/pull/11139>`__: TST: turn some build warnings into errors
* `#11140 <https://github.com/numpy/numpy/pull/11140>`__: MAINT: Update artful to bionic for i386 testing
* `#11141 <https://github.com/numpy/numpy/pull/11141>`__: MAINT: Extract a helper function for prepending and appending
* `#11145 <https://github.com/numpy/numpy/pull/11145>`__: DOC: cleanup NEP creation
* `#11146 <https://github.com/numpy/numpy/pull/11146>`__: DOC: add a NEP to split MaskedArray into a separate package
* `#11148 <https://github.com/numpy/numpy/pull/11148>`__: TST: make build warning into an error in runtest.py
* `#11149 <https://github.com/numpy/numpy/pull/11149>`__: BUG: guessing datetime, time precedence
* `#11152 <https://github.com/numpy/numpy/pull/11152>`__: BENCH: Add basic benchmarks for numpy.pad
* `#11155 <https://github.com/numpy/numpy/pull/11155>`__: BUG: Prevent stackoverflow in conversion to datetime types
* `#11158 <https://github.com/numpy/numpy/pull/11158>`__: TST: disable gc in refcount test
* `#11159 <https://github.com/numpy/numpy/pull/11159>`__: TST: Skip ctypes dependent test that fails on Python < 2.7.7.
* `#11160 <https://github.com/numpy/numpy/pull/11160>`__: TST: windows builds now properly support floating error states
* `#11163 <https://github.com/numpy/numpy/pull/11163>`__: MAINT: Work around non-deterministic Python readdir order in...
* `#11167 <https://github.com/numpy/numpy/pull/11167>`__: MAINT: Cleanup dragon4 code in various ways
* `#11168 <https://github.com/numpy/numpy/pull/11168>`__: TST: linalg: add regression test for gh-8577
* `#11169 <https://github.com/numpy/numpy/pull/11169>`__: MAINT: add sanity-checks to be run at import time
* `#11173 <https://github.com/numpy/numpy/pull/11173>`__: MAINT: Ensure that parsing errors are passed on even in tests.
* `#11176 <https://github.com/numpy/numpy/pull/11176>`__: MAINT: avoid setting non-existing gufunc strides for keepdims=True.
* `#11177 <https://github.com/numpy/numpy/pull/11177>`__: DOC: improvement of the documentation for gufunc.
* `#11178 <https://github.com/numpy/numpy/pull/11178>`__: TST: Test dimensions/indices found from parsed gufunc signatures.
* `#11180 <https://github.com/numpy/numpy/pull/11180>`__: BUG: void dtype setup checked offset not actual pointer for alignment
* `#11182 <https://github.com/numpy/numpy/pull/11182>`__: BUG: Avoid deprecated non-tuple indexing
* `#11184 <https://github.com/numpy/numpy/pull/11184>`__: MAINT: Add bitmask helper functions
* `#11185 <https://github.com/numpy/numpy/pull/11185>`__: MAINT: Add comments to long_double detection code
* `#11186 <https://github.com/numpy/numpy/pull/11186>`__: TST: Add np.core._multiarray_tests.format_float_OSprintf_g
* `#11187 <https://github.com/numpy/numpy/pull/11187>`__: MAINT: Use the more common -1 / 0 to indicate error / success
* `#11189 <https://github.com/numpy/numpy/pull/11189>`__: NEP: Array function protocol
* `#11190 <https://github.com/numpy/numpy/pull/11190>`__: DOC: Update NEP0 to clarify that discussion should happen on...
* `#11191 <https://github.com/numpy/numpy/pull/11191>`__: MAINT: remove darwin hardcoded LDOUBLE detection
* `#11193 <https://github.com/numpy/numpy/pull/11193>`__: BUG: Fix reference count/memory leak exposed by better testing
* `#11200 <https://github.com/numpy/numpy/pull/11200>`__: BUG: Bytes delimiter/comments in genfromtxt should be decoded
* `#11209 <https://github.com/numpy/numpy/pull/11209>`__: DOC: Fix doctest formatting in `rot90()` examples
* `#11218 <https://github.com/numpy/numpy/pull/11218>`__: BUG: Fixes einsum broadcasting bug when optimize=True
* `#11222 <https://github.com/numpy/numpy/pull/11222>`__: DOC: Make reference doc nditer examples python3 friendly
* `#11223 <https://github.com/numpy/numpy/pull/11223>`__: BUG: Forcibly promote shape to uint64 in numpy.memmap.
* `#11225 <https://github.com/numpy/numpy/pull/11225>`__: DOC: add existing recfunctions documentation to output
* `#11226 <https://github.com/numpy/numpy/pull/11226>`__: MAINT: add 'rst' to nep filename, fixup urls
* `#11229 <https://github.com/numpy/numpy/pull/11229>`__: NEP: New RNG policy
* `#11231 <https://github.com/numpy/numpy/pull/11231>`__: MAINT: ensure we do not create unnecessary tuples for outputs
* `#11238 <https://github.com/numpy/numpy/pull/11238>`__: MAINT: Don't update the flags a second time
* `#11239 <https://github.com/numpy/numpy/pull/11239>`__: MAINT: Use PyArray_NewFromDescr where possible, remove unused...
* `#11240 <https://github.com/numpy/numpy/pull/11240>`__: MAINT: Remove dead code backporting py2.6 warnings
* `#11246 <https://github.com/numpy/numpy/pull/11246>`__: BUG: Set ndarray.base before `__array_finalize__`
* `#11247 <https://github.com/numpy/numpy/pull/11247>`__: MAINT/BUG: Remove out-of-band reference count in PyArray_Newshape,...
* `#11248 <https://github.com/numpy/numpy/pull/11248>`__: MAINT: Don't update the flags a second time
* `#11249 <https://github.com/numpy/numpy/pull/11249>`__: BUG: Remove errant flag meddling in .real and .imag
* `#11252 <https://github.com/numpy/numpy/pull/11252>`__: DOC: show how to generate release notes in release walkthrough
* `#11257 <https://github.com/numpy/numpy/pull/11257>`__: BUG: ensure extobj and axes have their own references.
* `#11260 <https://github.com/numpy/numpy/pull/11260>`__: MAINT: Do proper cleanup in get_ufunc_arguments.
* `#11263 <https://github.com/numpy/numpy/pull/11263>`__: DOC: Update master after NumPy 1.14.4 release.
* `#11269 <https://github.com/numpy/numpy/pull/11269>`__: BUG: Correct use of NPY_UNUSED.
* `#11273 <https://github.com/numpy/numpy/pull/11273>`__: BUG: Remove invalid read in searchsorted if needle is empty
* `#11275 <https://github.com/numpy/numpy/pull/11275>`__: TST: Do not use empty arrays in tests (unless they are not read)
* `#11277 <https://github.com/numpy/numpy/pull/11277>`__: BUG: Work around past and present PEP3118 issues in ctypes
* `#11280 <https://github.com/numpy/numpy/pull/11280>`__: DOC: make docstring of np.interp clearer
* `#11286 <https://github.com/numpy/numpy/pull/11286>`__: BUG: einsum needs to check overlap on an out argument
* `#11287 <https://github.com/numpy/numpy/pull/11287>`__: DOC: Minor documentation improvements
* `#11291 <https://github.com/numpy/numpy/pull/11291>`__: BUG: Remove extra trailing parentheses.
* `#11293 <https://github.com/numpy/numpy/pull/11293>`__: DOC: fix hierarchy of numericaltype
* `#11296 <https://github.com/numpy/numpy/pull/11296>`__: BUG: Fix segfault on failing `__array_wrap__`
* `#11298 <https://github.com/numpy/numpy/pull/11298>`__: BUG: Undo behavior change in ma.masked_values(shrink=True)
* `#11307 <https://github.com/numpy/numpy/pull/11307>`__: BUG: Fix memmap regression when shape=None
* `#11314 <https://github.com/numpy/numpy/pull/11314>`__: MAINT: remove unused "npy_import"
* `#11315 <https://github.com/numpy/numpy/pull/11315>`__: MAINT: Package `tools/allocation_tracking`
* `#11319 <https://github.com/numpy/numpy/pull/11319>`__: REL, REV: Revert f2py fixes that exposed SciPy bug.
* `#11327 <https://github.com/numpy/numpy/pull/11327>`__: DOC: Update release notes for 1.15.0.
* `#11339 <https://github.com/numpy/numpy/pull/11339>`__: BUG: decref in failure path; replace PyObject_Type by Py_TYPE
* `#11352 <https://github.com/numpy/numpy/pull/11352>`__: DEP: Actually deprecate the normed argument to histogram
* `#11359 <https://github.com/numpy/numpy/pull/11359>`__: DOC: document new functions
* `#11367 <https://github.com/numpy/numpy/pull/11367>`__: BUG: add missing NpyIter_Close in einsum
* `#11368 <https://github.com/numpy/numpy/pull/11368>`__: BUG/TST: String indexing should just fail, not emit a futurewarning
* `#11389 <https://github.com/numpy/numpy/pull/11389>`__: ENH: Remove NpyIter_Close
* `#11392 <https://github.com/numpy/numpy/pull/11392>`__: BUG: Make scalar.squeeze accept axis arg
* `#11393 <https://github.com/numpy/numpy/pull/11393>`__: REL,MAINT: Update numpyconfig.h for 1.15.
* `#11394 <https://github.com/numpy/numpy/pull/11394>`__: MAINT: Update mailmap
* `#11403 <https://github.com/numpy/numpy/pull/11403>`__: DOC: Remove npyiter close from notes
* `#11427 <https://github.com/numpy/numpy/pull/11427>`__: BUG: Fix incorrect deprecation logic for histogram(normed=...)...
* `#11489 <https://github.com/numpy/numpy/pull/11489>`__: BUG: Ensure out is returned in einsum.
* `#11491 <https://github.com/numpy/numpy/pull/11491>`__: BUG/ENH: Einsum optimization path updates and bug fixes.
* `#11493 <https://github.com/numpy/numpy/pull/11493>`__: BUG: Revert #10229 to fix DLL loads on Windows.
* `#11494 <https://github.com/numpy/numpy/pull/11494>`__: MAINT: add PyPI classifier for Python 3.7
* `#11495 <https://github.com/numpy/numpy/pull/11495>`__: BENCH: belated addition of lcm, gcd to ufunc benchmark.
* `#11496 <https://github.com/numpy/numpy/pull/11496>`__: BUG: Advanced indexing assignment incorrectly took 1-D fastpath
* `#11511 <https://github.com/numpy/numpy/pull/11511>`__: BUG: Fix #define for ppc64 and ppc64le
* `#11529 <https://github.com/numpy/numpy/pull/11529>`__: ENH: Add density argument to histogramdd.
* `#11532 <https://github.com/numpy/numpy/pull/11532>`__: BUG: Decref of field title caused segfault
* `#11540 <https://github.com/numpy/numpy/pull/11540>`__: DOC: Update the 1.15.0 release notes.
* `#11577 <https://github.com/numpy/numpy/pull/11577>`__: BLD: Modify cpu detection and printing to get working aarch64...
* `#11578 <https://github.com/numpy/numpy/pull/11578>`__: DOC: link to TESTS.rst.txt testing guidelines, tweak testing...
* `#11602 <https://github.com/numpy/numpy/pull/11602>`__: TST: Add Python 3.7 to CI testing

Contributors
============

A total of 113 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alan Fontenot +
* Allan Haldane
* Alon Hershenhorn +
* Alyssa Quek +
* Andreas Nussbaumer +
* Anner +
* Anthony Sottile +
* Antony Lee
* Ayappan P +
* Bas van Schaik +
* C.A.M. Gerlach +
* Charles Harris
* Chris Billington
* Christian Clauss
* Christoph Gohlke
* Christopher Pezley +
* Daniel B Allan +
* Daniel Smith
* Dawid Zych +
* Derek Kim +
* Dima Pasechnik +
* Edgar Giovanni Lepe +
* Elena Mokeeva +
* Elliott Sales de Andrade +
* Emil Hessman +
* Eric Larson
* Eric Schles +
* Eric Wieser
* Giulio Benetti +
* Guillaume Gautier +
* Guo Ci
* Heath Henley +
* Isuru Fernando +
* J. Lewis Muir +
* Jack Vreeken +
* Jaime Fernandez
* James Bourbeau
* Jeff VanOss
* Jeffrey Yancey +
* Jeremy Chen +
* Jeremy Manning +
* Jeroen Demeyer
* John Darbyshire +
* John Kirkham
* John Zwinck
* Jonas Jensen +
* Joscha Reimer +
* Juan Azcarreta +
* Julian Taylor
* Kevin Sheppard
* Krzysztof Chomski +
* Kyle Sunden
* Lars Grüter
* Lilian Besson +
* MSeifert04
* Mark Harfouche
* Marten van Kerkwijk
* Martin Thoma
* Matt Harrigan +
* Matthew Bowden +
* Matthew Brett
* Matthias Bussonnier
* Matti Picus
* Max Aifer +
* Michael Hirsch, Ph.D +
* Michael James Jamie  Schnaitter +
* MichaelSaah +
* Mike Toews
* Minkyu Lee +
* Mircea Akos Bruma +
* Mircea-Akos Brumă +
* Moshe Looks +
* Muhammad Kasim +
* Nathaniel J. Smith
* Nikita Titov +
* Paul Müller +
* Paul van Mulbregt
* Pauli Virtanen
* Pierre Glaser +
* Pim de Haan
* Ralf Gommers
* Robert Kern
* Robin Aggleton +
* Rohit Pandey +
* Roman Yurchak +
* Ryan Soklaski
* Sebastian Berg
* Sho Nakamura +
* Simon Gibbons
* Stan Seibert +
* Stefan Otte
* Stefan van der Walt
* Stephan Hoyer
* Stuart Archibald
* Taylor Smith +
* Tim Felgentreff +
* Tim Swast +
* Tim Teichmann +
* Toshiki Kataoka
* Travis Oliphant
* Tyler Reddy
* Uddeshya Singh +
* Warren Weckesser
* Weitang Li +
* Wenjamin Petrenko +
* William D. Irons
* Yannick Jadoul +
* Yaroslav Halchenko
* Yug Khanna +
* Yuji Kanagawa +
* Yukun Guo +
* @ankokumoyashi +
* @lerbuke +

Pull requests merged
====================

A total of 490 pull requests were merged for this release.

* `#6256 <https://github.com/numpy/numpy/pull/6256>`__: NEP: Add proposal for oindex and vindex.
* `#6377 <https://github.com/numpy/numpy/pull/6377>`__: BUG: define "uint-alignment", fixes complex64 alignment
* `#8206 <https://github.com/numpy/numpy/pull/8206>`__: ENH: add padding options to diff
* `#8923 <https://github.com/numpy/numpy/pull/8923>`__: ENH: Add 'stone' estimator to np.histogram
* `#8955 <https://github.com/numpy/numpy/pull/8955>`__: ENH: Allow ufunc.identity to be any python object
* `#9022 <https://github.com/numpy/numpy/pull/9022>`__: BUG: don't silence `__array_wrap__` errors in `ufunc.reduce`
* `#10551 <https://github.com/numpy/numpy/pull/10551>`__: BUG: memmap close files when it shouldn't, load leaves them open...
* `#10602 <https://github.com/numpy/numpy/pull/10602>`__: MAINT: Move dtype string functions to python
* `#10704 <https://github.com/numpy/numpy/pull/10704>`__: NEP 15: Merging multiarray and umath
* `#10797 <https://github.com/numpy/numpy/pull/10797>`__: DEP: Updated `unravel_index()` to support `shape` kwarg
* `#10915 <https://github.com/numpy/numpy/pull/10915>`__: ENH: implement nep 0015: merge multiarray and umath
* `#10998 <https://github.com/numpy/numpy/pull/10998>`__: DOC: removed spurious FIXME comment in number.c
* `#11002 <https://github.com/numpy/numpy/pull/11002>`__: MAINT: add clearer message to assist users with failed builds.
* `#11016 <https://github.com/numpy/numpy/pull/11016>`__: ENH: Add AARCH32 support.
* `#11084 <https://github.com/numpy/numpy/pull/11084>`__: DOC: link to TESTS.rst.txt testing guidelines, tweak testing...
* `#11119 <https://github.com/numpy/numpy/pull/11119>`__: ENH: Chain exceptions to give better error messages for invalid...
* `#11175 <https://github.com/numpy/numpy/pull/11175>`__: ENH: Generalized ufunc signature expansion for frozen and flexible...
* `#11197 <https://github.com/numpy/numpy/pull/11197>`__: BUG/ENH: Removed non-standard scaling of the covariance matrix...
* `#11234 <https://github.com/numpy/numpy/pull/11234>`__: DOC: Update einsum docs
* `#11282 <https://github.com/numpy/numpy/pull/11282>`__: MAINT: move comparison operator special-handling out of ufunc...
* `#11297 <https://github.com/numpy/numpy/pull/11297>`__: NEP: Expansion of gufunc signatures.
* `#11299 <https://github.com/numpy/numpy/pull/11299>`__: BUG: Prevent crashes on 0-length structured void scalars
* `#11303 <https://github.com/numpy/numpy/pull/11303>`__: DOC: revision of NEP-18 (`__array_function__`)
* `#11312 <https://github.com/numpy/numpy/pull/11312>`__: WIP: DOC: slightly tweak the directions to create a release
* `#11318 <https://github.com/numpy/numpy/pull/11318>`__: REL: Setup master for 1.16 development.
* `#11323 <https://github.com/numpy/numpy/pull/11323>`__: DEP: Actually deprecate the normed argument to histogram
* `#11324 <https://github.com/numpy/numpy/pull/11324>`__: MAINT: Don't use dtype strings when the dtypes themselves can...
* `#11326 <https://github.com/numpy/numpy/pull/11326>`__: DOC: Update master after NumPy 1.14.5 release.
* `#11328 <https://github.com/numpy/numpy/pull/11328>`__: MAINT: Misc numeric cleanup
* `#11335 <https://github.com/numpy/numpy/pull/11335>`__: DOC: Change array lengths/entries in `broadcast_arrays` example...
* `#11336 <https://github.com/numpy/numpy/pull/11336>`__: BUG: decref in failure path; replace `PyObject_Type` by `Py_TYPE`
* `#11338 <https://github.com/numpy/numpy/pull/11338>`__: MAINT: Ensure ufunc override call each class only once, plus...
* `#11340 <https://github.com/numpy/numpy/pull/11340>`__: BUG: sctypeDict['f8'] randomly points to double or longdouble...
* `#11345 <https://github.com/numpy/numpy/pull/11345>`__: BUG/ENH: Einsum optimization path updates and bug fixes.
* `#11347 <https://github.com/numpy/numpy/pull/11347>`__: DOC: Silence many sphinx warnings
* `#11348 <https://github.com/numpy/numpy/pull/11348>`__: ENH: Improve support for pathlib.Path objects in load functions
* `#11349 <https://github.com/numpy/numpy/pull/11349>`__: DOC: document new functions
* `#11351 <https://github.com/numpy/numpy/pull/11351>`__: MAINT: Improve speed of ufunc kwargs parsing
* `#11353 <https://github.com/numpy/numpy/pull/11353>`__: DOC, MAINT: HTTP -> HTTPS, and other linkrot fixes
* `#11356 <https://github.com/numpy/numpy/pull/11356>`__: NEP: Update NEP 19: RNG Policy
* `#11357 <https://github.com/numpy/numpy/pull/11357>`__: MAINT: Add new `_test.c` files and `benchmarks/html` to `gitignore`
* `#11365 <https://github.com/numpy/numpy/pull/11365>`__: BUG: add missing NpyIter_Close in einsum
* `#11366 <https://github.com/numpy/numpy/pull/11366>`__: BUG/TST: String indexing should just fail, not emit a futurewarning
* `#11371 <https://github.com/numpy/numpy/pull/11371>`__: DOC: Clarify requirement that histogram bins are monotonic.
* `#11373 <https://github.com/numpy/numpy/pull/11373>`__: TST: Show that histogramdd's normed argument is histogram's density
* `#11374 <https://github.com/numpy/numpy/pull/11374>`__: WIP: additional revision for NEP-18 (`__array_function__`)
* `#11376 <https://github.com/numpy/numpy/pull/11376>`__: ENH: Remove NpyIter_Close
* `#11379 <https://github.com/numpy/numpy/pull/11379>`__: BUG: changed hardcoded axis to 0 for checking indices
* `#11382 <https://github.com/numpy/numpy/pull/11382>`__: DEP: deprecate undocumented, unused dtype type dicts
* `#11383 <https://github.com/numpy/numpy/pull/11383>`__: ENH: Allow size=0 in numpy.random.choice
* `#11385 <https://github.com/numpy/numpy/pull/11385>`__: BUG: Make scalar.squeeze accept axis arg
* `#11390 <https://github.com/numpy/numpy/pull/11390>`__: REL,MAINT: Update numpyconfig.h for 1.15.
* `#11391 <https://github.com/numpy/numpy/pull/11391>`__: MAINT: Update mailmap
* `#11396 <https://github.com/numpy/numpy/pull/11396>`__: TST: Added regression test for #11395
* `#11405 <https://github.com/numpy/numpy/pull/11405>`__: BUG: Ensure comparisons on scalar strings pass without warning.
* `#11406 <https://github.com/numpy/numpy/pull/11406>`__: BUG: Ensure out is returned in einsum.
* `#11409 <https://github.com/numpy/numpy/pull/11409>`__: DOC: Update testing section of README.
* `#11414 <https://github.com/numpy/numpy/pull/11414>`__: DOC: major revision of NEP 21, advanced indexing
* `#11422 <https://github.com/numpy/numpy/pull/11422>`__: BENCH: Add benchmarks for np.loadtxt reading from CSV format...
* `#11424 <https://github.com/numpy/numpy/pull/11424>`__: ENH: Allow use of svd on empty arrays
* `#11425 <https://github.com/numpy/numpy/pull/11425>`__: DOC: Clear up confusion between np.where(cond) and np.where(cond,...
* `#11428 <https://github.com/numpy/numpy/pull/11428>`__: BUG: Fix incorrect deprecation logic for histogram(normed=...)...
* `#11429 <https://github.com/numpy/numpy/pull/11429>`__: NEP: accept NEP 20 partially (frozen, flexible, but not broadcastable...
* `#11432 <https://github.com/numpy/numpy/pull/11432>`__: MAINT: Refactor differences between cblas_matrixproduct and PyArray_MatrixProduct2
* `#11434 <https://github.com/numpy/numpy/pull/11434>`__: MAINT: add PyPI classifier for Python 3.7
* `#11436 <https://github.com/numpy/numpy/pull/11436>`__: DOC: Document average return type
* `#11440 <https://github.com/numpy/numpy/pull/11440>`__: BUG: fix interpolation with inf and NaN present
* `#11444 <https://github.com/numpy/numpy/pull/11444>`__: DOC: Fix documentation for fromfunction
* `#11449 <https://github.com/numpy/numpy/pull/11449>`__: BUG: Revert #10229 to fix DLL loads on Windows.
* `#11450 <https://github.com/numpy/numpy/pull/11450>`__: MAINT/DEP: properly implement `ndarray.__pos__`
* `#11453 <https://github.com/numpy/numpy/pull/11453>`__: BENCH: add ufunc argument parsing benchmarks.
* `#11455 <https://github.com/numpy/numpy/pull/11455>`__: BENCH: belated addition of lcm, gcd to ufunc benchmark.
* `#11459 <https://github.com/numpy/numpy/pull/11459>`__: NEP: Add some text to NEP 0 to clarify how a NEP is accepted
* `#11461 <https://github.com/numpy/numpy/pull/11461>`__: MAINT: Add discussion link to NEP 15
* `#11462 <https://github.com/numpy/numpy/pull/11462>`__: Add NEP 22, a high level overview for the duck array work
* `#11463 <https://github.com/numpy/numpy/pull/11463>`__: MAINT: Produce a more readable repr of argument packs in benchmark
* `#11464 <https://github.com/numpy/numpy/pull/11464>`__: BUG: Don't convert inputs to `np.float64` in digitize
* `#11468 <https://github.com/numpy/numpy/pull/11468>`__: BUG: Advanced indexing assignment incorrectly took 1-D fastpath
* `#11470 <https://github.com/numpy/numpy/pull/11470>`__: BLD: Don't leave the build task running if runtests.py is interrupted
* `#11471 <https://github.com/numpy/numpy/pull/11471>`__: MAINT: Remove python-side docstrings from add_newdocs.
* `#11472 <https://github.com/numpy/numpy/pull/11472>`__: DOC: include NEP number on each NEP page
* `#11473 <https://github.com/numpy/numpy/pull/11473>`__: MAINT: Move pytesttester outside of np.testing, to avoid creating...
* `#11474 <https://github.com/numpy/numpy/pull/11474>`__: MAINT: Move add_newdocs into core, since it only adds docs to...
* `#11479 <https://github.com/numpy/numpy/pull/11479>`__: BUG: Fix #define for ppc64 and ppc64le
* `#11480 <https://github.com/numpy/numpy/pull/11480>`__: MAINT: move ufunc override code to umath and multiarray as much...
* `#11482 <https://github.com/numpy/numpy/pull/11482>`__: DOC: Include warning in np.resize() docs
* `#11484 <https://github.com/numpy/numpy/pull/11484>`__: BUG: Increase required cython version on python 3.7
* `#11487 <https://github.com/numpy/numpy/pull/11487>`__: DOC: extend sanity check message
* `#11488 <https://github.com/numpy/numpy/pull/11488>`__: NEP: clarify bugfix policy for legacy RandomState.
* `#11501 <https://github.com/numpy/numpy/pull/11501>`__: MAINT: Tidy cython invocation
* `#11503 <https://github.com/numpy/numpy/pull/11503>`__: MAINT: improve error message for isposinf and isneginf on complex...
* `#11512 <https://github.com/numpy/numpy/pull/11512>`__: DOC: Add templates for issues and PRs
* `#11514 <https://github.com/numpy/numpy/pull/11514>`__: Prefer the same-python cython to the on-PATH cython
* `#11515 <https://github.com/numpy/numpy/pull/11515>`__: BUG: decref of field title caused segfault
* `#11518 <https://github.com/numpy/numpy/pull/11518>`__: MAINT: Speed up normalize_axis_tuple by about 30%
* `#11522 <https://github.com/numpy/numpy/pull/11522>`__: BUG: fix np.load() of empty .npz file
* `#11525 <https://github.com/numpy/numpy/pull/11525>`__: MAINT: Append `*FLAGS` instead of overriding
* `#11526 <https://github.com/numpy/numpy/pull/11526>`__: ENH: add multi-field assignment helpers in np.lib.recfunctions
* `#11527 <https://github.com/numpy/numpy/pull/11527>`__: DOC: Note that method is the polar form of Box-Muller.
* `#11528 <https://github.com/numpy/numpy/pull/11528>`__: ENH: Add support for ipython latex printing to polynomial
* `#11531 <https://github.com/numpy/numpy/pull/11531>`__: ENH: Add density argument to histogramdd.
* `#11533 <https://github.com/numpy/numpy/pull/11533>`__: DOC: Fixed example code for cheb2poly and poly2cheb (see #11519)
* `#11534 <https://github.com/numpy/numpy/pull/11534>`__: DOC: Minor improvements to np.concatenate docstring
* `#11535 <https://github.com/numpy/numpy/pull/11535>`__: MAINT: Improve memory usage in PEP3118 format parsing
* `#11553 <https://github.com/numpy/numpy/pull/11553>`__: DOC: Tiny typo on numpy/reference/arrays.dtypes.html
* `#11556 <https://github.com/numpy/numpy/pull/11556>`__: BUG: Make assert_string_equal check str equality simply without...
* `#11559 <https://github.com/numpy/numpy/pull/11559>`__: NEP: accept nep 0015
* `#11560 <https://github.com/numpy/numpy/pull/11560>`__: NEP: accept nep 0019
* `#11562 <https://github.com/numpy/numpy/pull/11562>`__: DOC: update release notes for LDFLAGS append behavior (gh-11525).
* `#11565 <https://github.com/numpy/numpy/pull/11565>`__: MAINT: convert the doctests for polynomial to regular tests
* `#11566 <https://github.com/numpy/numpy/pull/11566>`__: BLD: Do not use gcc warnings flags when 'gcc' is actually clang.
* `#11567 <https://github.com/numpy/numpy/pull/11567>`__: TST: Integrate codecov testing
* `#11568 <https://github.com/numpy/numpy/pull/11568>`__: BLD: Modify cpu detection and printing to get working aarch64...
* `#11571 <https://github.com/numpy/numpy/pull/11571>`__: DOC: Updated array2string description
* `#11572 <https://github.com/numpy/numpy/pull/11572>`__: DOC: Updated Slice Description
* `#11573 <https://github.com/numpy/numpy/pull/11573>`__: TST: add broadcast_arrays() kwarg unit test for TypeError
* `#11580 <https://github.com/numpy/numpy/pull/11580>`__: MAINT: refactor ufunc iter operand flags handling
* `#11591 <https://github.com/numpy/numpy/pull/11591>`__: MAINT: update runtests.py node id example for pytest usage
* `#11592 <https://github.com/numpy/numpy/pull/11592>`__: DOC: add Stefan van der Walt to Steering Council
* `#11593 <https://github.com/numpy/numpy/pull/11593>`__: ENH: handle empty matrices in qr decomposition
* `#11594 <https://github.com/numpy/numpy/pull/11594>`__: ENH: support for empty matrices in linalg.lstsq
* `#11595 <https://github.com/numpy/numpy/pull/11595>`__: BUG:warn on Nan in minimum,maximum for scalars, float16
* `#11596 <https://github.com/numpy/numpy/pull/11596>`__: NEP: backwards compatibility and deprecation policy
* `#11598 <https://github.com/numpy/numpy/pull/11598>`__: TST: Add Python 3.7 to CI testing
* `#11601 <https://github.com/numpy/numpy/pull/11601>`__: BUG: Make np.array([[1], 2]) and np.array([1, [2]]) behave in...
* `#11606 <https://github.com/numpy/numpy/pull/11606>`__: DOC: Post 1.15.0 release updates for master.
* `#11607 <https://github.com/numpy/numpy/pull/11607>`__: DOC: minor clarification and typo fix to NEP 21 (outer indexing).
* `#11610 <https://github.com/numpy/numpy/pull/11610>`__: TST: including C source line coverage for CI / codecov
* `#11611 <https://github.com/numpy/numpy/pull/11611>`__: NEP: Add roadmap section and subdocuments to NEPs
* `#11613 <https://github.com/numpy/numpy/pull/11613>`__: BUG: have geometric() raise ValueError on p=0
* `#11615 <https://github.com/numpy/numpy/pull/11615>`__: BUG: Clip uses wrong memory order in output
* `#11616 <https://github.com/numpy/numpy/pull/11616>`__: DOC: add a brief note on "Protocols for methods" to NEP 18
* `#11621 <https://github.com/numpy/numpy/pull/11621>`__: DOC: Use "real symmetric" rather than "symmetric" in ``eigh``...
* `#11626 <https://github.com/numpy/numpy/pull/11626>`__: DOC: Show plot in meshgrid example.
* `#11630 <https://github.com/numpy/numpy/pull/11630>`__: DOC: Include the versionadded to the isnat documentation.
* `#11634 <https://github.com/numpy/numpy/pull/11634>`__: MAINT: Filter Cython warnings in `__init__.py`
* `#11637 <https://github.com/numpy/numpy/pull/11637>`__: ENH: np.angle: Remove unnecessary multiplication, and allow subclasses...
* `#11638 <https://github.com/numpy/numpy/pull/11638>`__: ENH: Make expand_dims work on subclasses
* `#11642 <https://github.com/numpy/numpy/pull/11642>`__: BUG: Fixes for unicode field names in Python 2
* `#11643 <https://github.com/numpy/numpy/pull/11643>`__: DOC: Insert up to date link to Spyder website in Dev Env doc...
* `#11644 <https://github.com/numpy/numpy/pull/11644>`__: BUG: Fix doc source links to unwrap decorators
* `#11652 <https://github.com/numpy/numpy/pull/11652>`__: BUG: Ensure singleton dimensions are not dropped when converting...
* `#11660 <https://github.com/numpy/numpy/pull/11660>`__: ENH: Add Nan warnings for maximum, minimum on more dtypes
* `#11669 <https://github.com/numpy/numpy/pull/11669>`__: BUG: Fix regression in `void_getitem`
* `#11670 <https://github.com/numpy/numpy/pull/11670>`__: MAINT: trivially refactor mapped indexing
* `#11673 <https://github.com/numpy/numpy/pull/11673>`__: DOC: Add geomspace to "See also" of linspace
* `#11679 <https://github.com/numpy/numpy/pull/11679>`__: TST: ignore setup.py files for codecov reports
* `#11688 <https://github.com/numpy/numpy/pull/11688>`__: DOC: Update broadcasting doc with current exception details
* `#11691 <https://github.com/numpy/numpy/pull/11691>`__: BUG: Make matrix_power again work for object arrays.
* `#11692 <https://github.com/numpy/numpy/pull/11692>`__: MAINT: Remove duplicate code.
* `#11693 <https://github.com/numpy/numpy/pull/11693>`__: NEP: Mark NEP 18 as accepted
* `#11694 <https://github.com/numpy/numpy/pull/11694>`__: BUG: Fix pickle and memoryview for datetime64, timedelta64 scalars
* `#11695 <https://github.com/numpy/numpy/pull/11695>`__: BUG: Add missing PyErr_NoMemory after failing malloc
* `#11703 <https://github.com/numpy/numpy/pull/11703>`__: MAINT: Remove np.pkgload, which seems to be unusable anyway
* `#11708 <https://github.com/numpy/numpy/pull/11708>`__: BUG: Fix regression in np.loadtxt for bz2 text files in Python...
* `#11710 <https://github.com/numpy/numpy/pull/11710>`__: BUG: Check for compiler used in env['CC'], then config_vars['CC']
* `#11711 <https://github.com/numpy/numpy/pull/11711>`__: BUG: Fix undefined functions on big-endian systems.
* `#11715 <https://github.com/numpy/numpy/pull/11715>`__: TST: Fix urlopen stubbing.
* `#11717 <https://github.com/numpy/numpy/pull/11717>`__: MAINT: Make einsum optimize default to False.
* `#11718 <https://github.com/numpy/numpy/pull/11718>`__: BUG: Revert use of `console_scripts`.
* `#11722 <https://github.com/numpy/numpy/pull/11722>`__: MAINT: Remove duplicate docstring and correct location of `__all__`...
* `#11725 <https://github.com/numpy/numpy/pull/11725>`__: BUG: Fix Fortran kind detection for aarch64 & s390x.
* `#11727 <https://github.com/numpy/numpy/pull/11727>`__: BUG: Fix printing of longdouble on ppc64le.
* `#11729 <https://github.com/numpy/numpy/pull/11729>`__: DOC: fix capitalization of kilojoules
* `#11731 <https://github.com/numpy/numpy/pull/11731>`__: DOC: fix typo in vectorize docstring
* `#11733 <https://github.com/numpy/numpy/pull/11733>`__: DOC: recommend polynomial.Polynomial over np.polyfit
* `#11735 <https://github.com/numpy/numpy/pull/11735>`__: BUG: Fix test sensitive to platform byte order.
* `#11738 <https://github.com/numpy/numpy/pull/11738>`__: TST, MAINT: add lgtm.yml to tweak LGTM.com analysis
* `#11739 <https://github.com/numpy/numpy/pull/11739>`__: BUG: disallow setting flag to writeable after fromstring, frombuffer
* `#11740 <https://github.com/numpy/numpy/pull/11740>`__: BUG: Deprecation triggers segfault
* `#11742 <https://github.com/numpy/numpy/pull/11742>`__: DOC: Reduce warnings and cleanup redundant c-api documentation
* `#11745 <https://github.com/numpy/numpy/pull/11745>`__: DOC: Small docstring fixes for old polyfit.
* `#11754 <https://github.com/numpy/numpy/pull/11754>`__: BUG: check return value of `_buffer_format_string`
* `#11755 <https://github.com/numpy/numpy/pull/11755>`__: MAINT: Fix typos in random.hypergeometric's notes
* `#11756 <https://github.com/numpy/numpy/pull/11756>`__: MAINT: Make assert_array_compare more generic.
* `#11765 <https://github.com/numpy/numpy/pull/11765>`__: DOC: Move documentation from `help(ndarray.ctypes)` to `help(some_array.ctypes)`
* `#11771 <https://github.com/numpy/numpy/pull/11771>`__: BUG: Make `random.shuffle` work on 1-D instances of `ndarray`...
* `#11774 <https://github.com/numpy/numpy/pull/11774>`__: BUG: Fix regression in intersect1d.
* `#11778 <https://github.com/numpy/numpy/pull/11778>`__: BUG: Avoid signed overflow in histogram
* `#11783 <https://github.com/numpy/numpy/pull/11783>`__: MAINT: check `_append_char` return value
* `#11784 <https://github.com/numpy/numpy/pull/11784>`__: MAINT: reformat line spacing before test methods
* `#11797 <https://github.com/numpy/numpy/pull/11797>`__: DOC: Update docs after 1.15.1 release.
* `#11800 <https://github.com/numpy/numpy/pull/11800>`__: DOC: document use when f2py is not in the PATH
* `#11802 <https://github.com/numpy/numpy/pull/11802>`__: ENH: Use entry_points to install the f2py scripts.
* `#11805 <https://github.com/numpy/numpy/pull/11805>`__: BUG: add type cast check for ediff1d
* `#11806 <https://github.com/numpy/numpy/pull/11806>`__: DOC: Polybase augmented assignment notes
* `#11812 <https://github.com/numpy/numpy/pull/11812>`__: DOC: edit setup.py docstring that is displayed on PyPI.
* `#11813 <https://github.com/numpy/numpy/pull/11813>`__: BUG: fix array_split incorrect behavior with array size bigger...
* `#11814 <https://github.com/numpy/numpy/pull/11814>`__: DOC, MAINT: Fixes for errstate() and README.md documentation.
* `#11817 <https://github.com/numpy/numpy/pull/11817>`__: DOC: add examples and extend existing dos for polynomial subclasses
* `#11818 <https://github.com/numpy/numpy/pull/11818>`__: TST: add missing tests for all polynomial subclass pow fns.
* `#11823 <https://github.com/numpy/numpy/pull/11823>`__: TST: add test for array2string unexpected kwarg
* `#11830 <https://github.com/numpy/numpy/pull/11830>`__: MAINT: reduce void type repr code duplication
* `#11834 <https://github.com/numpy/numpy/pull/11834>`__: MAINT, DOC: Replace 'an' by 'a' in some docstrings.
* `#11837 <https://github.com/numpy/numpy/pull/11837>`__: DOC: Make clear the connection between numpy types and C types
* `#11840 <https://github.com/numpy/numpy/pull/11840>`__: BUG: Let 0-D arrays of Python timedelta convert to np.timedelta64.
* `#11843 <https://github.com/numpy/numpy/pull/11843>`__: MAINT: remove surviving, unused, list comprehension
* `#11849 <https://github.com/numpy/numpy/pull/11849>`__: TST: reorder duplicate mem_overlap.c compile
* `#11850 <https://github.com/numpy/numpy/pull/11850>`__: DOC: add comment to remove fn after python 2 support is dropped
* `#11852 <https://github.com/numpy/numpy/pull/11852>`__: BUG: timedelta64 now accepts NumPy ints
* `#11858 <https://github.com/numpy/numpy/pull/11858>`__: DOC: add docstrings for numeric types
* `#11862 <https://github.com/numpy/numpy/pull/11862>`__: BUG: Re-add `_ones_like` to numpy.core.umath.
* `#11864 <https://github.com/numpy/numpy/pull/11864>`__: TST: Update travis testing to use latest virtualenv.
* `#11865 <https://github.com/numpy/numpy/pull/11865>`__: DOC: add a Code of Conduct document.
* `#11866 <https://github.com/numpy/numpy/pull/11866>`__: TST: Drop Python 3.4 testing
* `#11868 <https://github.com/numpy/numpy/pull/11868>`__: MAINT: include benchmarks, complete docs, dev tool files in sdist.
* `#11870 <https://github.com/numpy/numpy/pull/11870>`__: MAINT: dtype(unicode) should raise TypeError on failure
* `#11874 <https://github.com/numpy/numpy/pull/11874>`__: BENCH: split out slow setup method in bench_shape_base.Block
* `#11877 <https://github.com/numpy/numpy/pull/11877>`__: BUG: Fix memory leak in pyfragments.swg
* `#11880 <https://github.com/numpy/numpy/pull/11880>`__: BUG: The multiarray/ufunc merge broke old wheels.
* `#11882 <https://github.com/numpy/numpy/pull/11882>`__: DOC: Recommend the use of `np.ndim` over `np.isscalar`, and explain...
* `#11889 <https://github.com/numpy/numpy/pull/11889>`__: BENCH: Split bench_function_base.Sort into Sort and SortWorst.
* `#11891 <https://github.com/numpy/numpy/pull/11891>`__: MAINT: remove exec_command() from build_ext
* `#11892 <https://github.com/numpy/numpy/pull/11892>`__: TST: Parametrize PEP3118 scalar tests.
* `#11893 <https://github.com/numpy/numpy/pull/11893>`__: TST: Fix duplicated test name.
* `#11894 <https://github.com/numpy/numpy/pull/11894>`__: TST: Parametrize f2py tests.
* `#11895 <https://github.com/numpy/numpy/pull/11895>`__: TST: Parametrize some linalg tests over types.
* `#11896 <https://github.com/numpy/numpy/pull/11896>`__: BUG: Fix matrix PendingDeprecationWarning suppression for pytest...
* `#11898 <https://github.com/numpy/numpy/pull/11898>`__: MAINT: remove exec_command usage from ccompiler.py
* `#11899 <https://github.com/numpy/numpy/pull/11899>`__: MAINT: remove exec_command from system_info.py
* `#11900 <https://github.com/numpy/numpy/pull/11900>`__: MAINT: remove exec_command from gnu.py
* `#11901 <https://github.com/numpy/numpy/pull/11901>`__: MAINT: remove exec_command usage in ibm.py
* `#11904 <https://github.com/numpy/numpy/pull/11904>`__: Use pytest for some already-parametrized core tests
* `#11905 <https://github.com/numpy/numpy/pull/11905>`__: TST: Start testing with "-std=c99" on travisCI.
* `#11906 <https://github.com/numpy/numpy/pull/11906>`__: TST: add shippable ARMv8 to CI
* `#11907 <https://github.com/numpy/numpy/pull/11907>`__: Link HOWTO_DOCUMENT to specific section on docstrings
* `#11909 <https://github.com/numpy/numpy/pull/11909>`__: MAINT: flake8 cleanups
* `#11910 <https://github.com/numpy/numpy/pull/11910>`__: MAINT: test, refactor design of recursive closures
* `#11912 <https://github.com/numpy/numpy/pull/11912>`__: DOC: dtype offset and itemsize is limited by range of C int
* `#11914 <https://github.com/numpy/numpy/pull/11914>`__: DOC: Clarify difference between PySequence_GETITEM, PyArray_GETITEM
* `#11916 <https://github.com/numpy/numpy/pull/11916>`__: DEP: deprecate np.set_numeric_ops and friends
* `#11920 <https://github.com/numpy/numpy/pull/11920>`__: TST: Fix 'def' test_numerictypes.py::TestSctypeDict to 'class'...
* `#11921 <https://github.com/numpy/numpy/pull/11921>`__: MAINT: Don't rely on `__name__` in bitname - use the information...
* `#11922 <https://github.com/numpy/numpy/pull/11922>`__: TST: Add tests for maximum_sctype
* `#11929 <https://github.com/numpy/numpy/pull/11929>`__: DOC: #defining -> #define / Added a short explanation for Numeric
* `#11930 <https://github.com/numpy/numpy/pull/11930>`__: DOC: fix scipy-sphinx-theme license path
* `#11932 <https://github.com/numpy/numpy/pull/11932>`__: MAINT: Move `np.dtype.name.__get__` to python
* `#11933 <https://github.com/numpy/numpy/pull/11933>`__: TST: Fix unit tests that used to call unittest.TestCase.fail
* `#11934 <https://github.com/numpy/numpy/pull/11934>`__: NEP: Revert "NEP: Mark NEP 18 as accepted"
* `#11935 <https://github.com/numpy/numpy/pull/11935>`__: MAINT: remove usage of exec_command in config.py
* `#11937 <https://github.com/numpy/numpy/pull/11937>`__: MAINT: remove exec_command() from f2py init
* `#11941 <https://github.com/numpy/numpy/pull/11941>`__: BUG: Ensure einsum(optimize=True) dispatches tensordot deterministically
* `#11943 <https://github.com/numpy/numpy/pull/11943>`__: DOC: Add warning/clarification about backwards compat in NEP-18
* `#11948 <https://github.com/numpy/numpy/pull/11948>`__: DEP: finish making all comparisons to NaT false
* `#11949 <https://github.com/numpy/numpy/pull/11949>`__: MAINT: Small tidy-ups to `np.core._dtype`
* `#11950 <https://github.com/numpy/numpy/pull/11950>`__: MAINT: Extract tangential improvements made in #11175
* `#11952 <https://github.com/numpy/numpy/pull/11952>`__: MAINT: test NPY_INTERNAL_BUILD only if defined
* `#11953 <https://github.com/numpy/numpy/pull/11953>`__: TST: codecov.yml improvements
* `#11957 <https://github.com/numpy/numpy/pull/11957>`__: ENH: mark that large allocations can use huge pages
* `#11958 <https://github.com/numpy/numpy/pull/11958>`__: TST: Add a test for np.pad where constant_values is an object
* `#11959 <https://github.com/numpy/numpy/pull/11959>`__: MAINT: Explicitely cause pagefaults to happen before starting...
* `#11961 <https://github.com/numpy/numpy/pull/11961>`__: TST: Add more tests for np.pad
* `#11962 <https://github.com/numpy/numpy/pull/11962>`__: ENH: maximum lines of content to be read from numpy.loadtxt
* `#11965 <https://github.com/numpy/numpy/pull/11965>`__: BENCH: Add a benchmark comparing block to copy in the 3D case
* `#11966 <https://github.com/numpy/numpy/pull/11966>`__: MAINT: Rewrite shape normalization in pad function
* `#11967 <https://github.com/numpy/numpy/pull/11967>`__: BUG: fix refcount leak in PyArray_AdaptFlexibleDType
* `#11971 <https://github.com/numpy/numpy/pull/11971>`__: MAINT: Block algorithm with a single copy per call to `block`
* `#11973 <https://github.com/numpy/numpy/pull/11973>`__: BUG: fix cached allocations without the GIL
* `#11976 <https://github.com/numpy/numpy/pull/11976>`__: MAINT/DOC: Show the location of an empty list in np.block
* `#11979 <https://github.com/numpy/numpy/pull/11979>`__: MAINT: Ensure that a copy of the array is returned when calling...
* `#11989 <https://github.com/numpy/numpy/pull/11989>`__: BUG: Ensure boolean indexing of subclasses sets base correctly.
* `#11991 <https://github.com/numpy/numpy/pull/11991>`__: MAINT: speed up `_block` by avoiding a recursive closure
* `#11996 <https://github.com/numpy/numpy/pull/11996>`__: TST: Parametrize and break apart dtype tests
* `#11997 <https://github.com/numpy/numpy/pull/11997>`__: MAINT: Extract string helpers to a new private file
* `#12002 <https://github.com/numpy/numpy/pull/12002>`__: Revert "NEP: Revert "NEP: Mark NEP 18 as accepted""
* `#12004 <https://github.com/numpy/numpy/pull/12004>`__: BUG: Fix f2py compile function testing.
* `#12005 <https://github.com/numpy/numpy/pull/12005>`__: ENH: initial implementation of core `__array_function__` machinery
* `#12008 <https://github.com/numpy/numpy/pull/12008>`__: MAINT: Reassociate `np.cast` with the comment describing it
* `#12009 <https://github.com/numpy/numpy/pull/12009>`__: MAINT: Eliminate the private `numerictypes._typestr`
* `#12011 <https://github.com/numpy/numpy/pull/12011>`__: ENH: implementation of array_reduce_ex
* `#12012 <https://github.com/numpy/numpy/pull/12012>`__: MAINT: Extract the crazy number of type aliases to their own...
* `#12014 <https://github.com/numpy/numpy/pull/12014>`__: TST: prefer pytest.skip() over SkipTest
* `#12015 <https://github.com/numpy/numpy/pull/12015>`__: TST: improve warnings parallel test safety
* `#12017 <https://github.com/numpy/numpy/pull/12017>`__: NEP: add 3 missing data NEPs rescued from 2011-2012
* `#12018 <https://github.com/numpy/numpy/pull/12018>`__: MAINT: Simplify parts of `_type_aliases`
* `#12019 <https://github.com/numpy/numpy/pull/12019>`__: DOC: MAINT: address comments @eric-wieser on NEP 24-26 PR.
* `#12020 <https://github.com/numpy/numpy/pull/12020>`__: TST: Add tests for np.sctype2char
* `#12021 <https://github.com/numpy/numpy/pull/12021>`__: DOC: Post NumPy 1.15.2 release updates.[ci skip]
* `#12024 <https://github.com/numpy/numpy/pull/12024>`__: MAINT: Normalize axes the normal way in fftpack.py
* `#12027 <https://github.com/numpy/numpy/pull/12027>`__: DOC: Add docstrings for abstract types in scalar type hierarchy
* `#12030 <https://github.com/numpy/numpy/pull/12030>`__: DOC: use "import numpy as np" style
* `#12032 <https://github.com/numpy/numpy/pull/12032>`__: BUG: check return value from PyArray_PromoteTypes
* `#12033 <https://github.com/numpy/numpy/pull/12033>`__: TST: Mark check for f2py script xfail.
* `#12034 <https://github.com/numpy/numpy/pull/12034>`__: MAINT: Add version deprecated to some deprecation messages.
* `#12035 <https://github.com/numpy/numpy/pull/12035>`__: BUG: Fix memory leak in PY3K buffer code.
* `#12041 <https://github.com/numpy/numpy/pull/12041>`__: MAINT: remove duplicate imports
* `#12042 <https://github.com/numpy/numpy/pull/12042>`__: MAINT: cleanup and better document core/overrides.py
* `#12045 <https://github.com/numpy/numpy/pull/12045>`__: BUG: fix memory leak of buffer format string
* `#12048 <https://github.com/numpy/numpy/pull/12048>`__: BLD: pin sphinx to 1.7.9
* `#12051 <https://github.com/numpy/numpy/pull/12051>`__: TST: add macos azure testing to CI
* `#12054 <https://github.com/numpy/numpy/pull/12054>`__: MAINT: avoid modifying mutable default values
* `#12056 <https://github.com/numpy/numpy/pull/12056>`__: MAINT: The crackfortran function is called with an extra argument
* `#12057 <https://github.com/numpy/numpy/pull/12057>`__: MAINT: remove unused imports
* `#12058 <https://github.com/numpy/numpy/pull/12058>`__: MAINT: remove redundant assignment
* `#12060 <https://github.com/numpy/numpy/pull/12060>`__: MAINT: remove unused stdlib imports
* `#12061 <https://github.com/numpy/numpy/pull/12061>`__: MAINT: remove redundant imports
* `#12062 <https://github.com/numpy/numpy/pull/12062>`__: BUG: `OBJECT_to_*` should check for errors
* `#12064 <https://github.com/numpy/numpy/pull/12064>`__: MAINT: delay initialization of getlimits (circular imports)
* `#12072 <https://github.com/numpy/numpy/pull/12072>`__: BUG: test_path() now uses Path.resolve()
* `#12073 <https://github.com/numpy/numpy/pull/12073>`__: MAINT Avoid some memory copies in numpy.polynomial.hermite
* `#12079 <https://github.com/numpy/numpy/pull/12079>`__: MAINT: Blacklist some MSVC complex functions.
* `#12081 <https://github.com/numpy/numpy/pull/12081>`__: TST: add Windows test matrix to Azure CI
* `#12082 <https://github.com/numpy/numpy/pull/12082>`__: TST: Add Python 3.5 to Azure windows CI.
* `#12088 <https://github.com/numpy/numpy/pull/12088>`__: BUG: limit default for get_num_build_jobs() to 8
* `#12089 <https://github.com/numpy/numpy/pull/12089>`__: BUG: Fix in-place permutation
* `#12090 <https://github.com/numpy/numpy/pull/12090>`__: TST, MAINT: Update pickling tests by making them loop over all...
* `#12091 <https://github.com/numpy/numpy/pull/12091>`__: TST: Install pickle5 for CI testing with python 3.6/7
* `#12093 <https://github.com/numpy/numpy/pull/12093>`__: Provide information about what kind is actually not integer kind
* `#12099 <https://github.com/numpy/numpy/pull/12099>`__: ENH: Validate dispatcher functions in array_function_dispatch
* `#12102 <https://github.com/numpy/numpy/pull/12102>`__: TST: improve coverage of nd_grid
* `#12103 <https://github.com/numpy/numpy/pull/12103>`__: MAINT: Add azure-pipeline status badge to README.md
* `#12106 <https://github.com/numpy/numpy/pull/12106>`__: TST, MAINT: Skip some f2py tests on Mac.
* `#12108 <https://github.com/numpy/numpy/pull/12108>`__: BUG: Allow boolean subtract in histogram
* `#12109 <https://github.com/numpy/numpy/pull/12109>`__: TST: add unit test for issctype
* `#12112 <https://github.com/numpy/numpy/pull/12112>`__: ENH: check getfield arguments to prevent invalid memory access
* `#12115 <https://github.com/numpy/numpy/pull/12115>`__: ENH: `__array_function__` support for most of `numpy.core`
* `#12116 <https://github.com/numpy/numpy/pull/12116>`__: ENH: `__array_function__` support for `np.lib`, part 1/2
* `#12117 <https://github.com/numpy/numpy/pull/12117>`__: ENH: `__array_function__` support for `np.fft` and `np.linalg`
* `#12119 <https://github.com/numpy/numpy/pull/12119>`__: ENH: `__array_function__` support for `np.lib`, part 2/2
* `#12120 <https://github.com/numpy/numpy/pull/12120>`__: ENH: add timedelta modulus operator support (mm)
* `#12121 <https://github.com/numpy/numpy/pull/12121>`__: MAINT: Clarify the error message for resize failure
* `#12123 <https://github.com/numpy/numpy/pull/12123>`__: DEP: deprecate asscalar
* `#12124 <https://github.com/numpy/numpy/pull/12124>`__: BUG: refactor float error status to support Alpine linux
* `#12125 <https://github.com/numpy/numpy/pull/12125>`__: TST: expand cases in test_issctype()
* `#12127 <https://github.com/numpy/numpy/pull/12127>`__: BUG: Fix memory leak in mapping.c
* `#12131 <https://github.com/numpy/numpy/pull/12131>`__: BUG: fix PyDataType_ISBOOL
* `#12133 <https://github.com/numpy/numpy/pull/12133>`__: MAINT, TST refactor pickle imports and tests
* `#12134 <https://github.com/numpy/numpy/pull/12134>`__: DOC: Remove duplicated sentence in numpy.multiply
* `#12137 <https://github.com/numpy/numpy/pull/12137>`__: TST: error tests for fill_diagonal()
* `#12138 <https://github.com/numpy/numpy/pull/12138>`__: TST: error tests for diag_indices_from()
* `#12140 <https://github.com/numpy/numpy/pull/12140>`__: DOC: fixups for NEP-18 based on the implementation
* `#12141 <https://github.com/numpy/numpy/pull/12141>`__: DOC: minor tweak to CoC (update NumFOCUS contact address).
* `#12145 <https://github.com/numpy/numpy/pull/12145>`__: MAINT: Update ndarrayobject.h `__cplusplus` block.
* `#12146 <https://github.com/numpy/numpy/pull/12146>`__: MAINT: Fix typo in comment
* `#12147 <https://github.com/numpy/numpy/pull/12147>`__: MAINT: Move duplicated type_reso_error code into a helper function
* `#12148 <https://github.com/numpy/numpy/pull/12148>`__: DOC: document NEP-18 overrides in release notes
* `#12151 <https://github.com/numpy/numpy/pull/12151>`__: TST: byte_bounds contiguity handling
* `#12153 <https://github.com/numpy/numpy/pull/12153>`__: DOC, TST: cover setdiff1d assume_unique
* `#12154 <https://github.com/numpy/numpy/pull/12154>`__: ENH: `__array_function__` for `np.core.defchararray`
* `#12155 <https://github.com/numpy/numpy/pull/12155>`__: MAINT: Define Py_SETREF for pre-3.5.2 python and use in code
* `#12157 <https://github.com/numpy/numpy/pull/12157>`__: ENH: Add support for third-party path-like objects by backporting...
* `#12159 <https://github.com/numpy/numpy/pull/12159>`__: MAINT: remove unused nd_grid `__len__`.
* `#12163 <https://github.com/numpy/numpy/pull/12163>`__: ENH: `__array_function__` for `np.einsum` and `np.block`
* `#12165 <https://github.com/numpy/numpy/pull/12165>`__: Mark NEP 22 as accepted, and add "Informational" NEPs to NEP...
* `#12166 <https://github.com/numpy/numpy/pull/12166>`__: NEP: Add zero-rank arrays historical info NEP
* `#12173 <https://github.com/numpy/numpy/pull/12173>`__: NEP: add notes about updates to NEP-18
* `#12174 <https://github.com/numpy/numpy/pull/12174>`__: NEP 16 abstract arrays: rebased and marked as "Withdrawn"
* `#12175 <https://github.com/numpy/numpy/pull/12175>`__: ENH: `__array_function__` for multiarray functions
* `#12176 <https://github.com/numpy/numpy/pull/12176>`__: TST: add test for weighted histogram mismatch
* `#12177 <https://github.com/numpy/numpy/pull/12177>`__: MAINT: remove unused `_assertSquareness()`
* `#12179 <https://github.com/numpy/numpy/pull/12179>`__: MAINT: Move `_kind_to_stem` to `np.core._dtype`, so that it can...
* `#12180 <https://github.com/numpy/numpy/pull/12180>`__: NEP: change toc titles, cross reference, mark 16 superseded
* `#12181 <https://github.com/numpy/numpy/pull/12181>`__: MAINT: fix depreciation message typo for np.sum
* `#12185 <https://github.com/numpy/numpy/pull/12185>`__: TST: test multi_dot with 2 arrays
* `#12199 <https://github.com/numpy/numpy/pull/12199>`__: TST: add Azure CI triggers
* `#12209 <https://github.com/numpy/numpy/pull/12209>`__: Delay import of distutils.msvccompiler to avoid warning on non-Windows.
* `#12211 <https://github.com/numpy/numpy/pull/12211>`__: DOC: Clarify the examples for argmax and argmin
* `#12212 <https://github.com/numpy/numpy/pull/12212>`__: MAINT: `ndarray.__repr__` should not rely on `__array_function__`
* `#12214 <https://github.com/numpy/numpy/pull/12214>`__: TST: add test for tensorinv()
* `#12215 <https://github.com/numpy/numpy/pull/12215>`__: TST: test dims match on lstsq()
* `#12216 <https://github.com/numpy/numpy/pull/12216>`__: TST: test invalid histogram range
* `#12217 <https://github.com/numpy/numpy/pull/12217>`__: TST: test histogram bins dims
* `#12219 <https://github.com/numpy/numpy/pull/12219>`__: ENH: make matmul into a ufunc
* `#12222 <https://github.com/numpy/numpy/pull/12222>`__: TST: unit tests for column_stack.
* `#12224 <https://github.com/numpy/numpy/pull/12224>`__: BUG: Fix MaskedArray fill_value type conversion.
* `#12229 <https://github.com/numpy/numpy/pull/12229>`__: MAINT: Fix typo in comment
* `#12236 <https://github.com/numpy/numpy/pull/12236>`__: BUG: maximum, minimum no longer emit warnings on NAN
* `#12240 <https://github.com/numpy/numpy/pull/12240>`__: BUG: Fix crash in repr of void subclasses
* `#12241 <https://github.com/numpy/numpy/pull/12241>`__: TST: arg handling tests in histogramdd
* `#12243 <https://github.com/numpy/numpy/pull/12243>`__: BUG: Fix misleading assert message in assert_almost_equal #12200
* `#12245 <https://github.com/numpy/numpy/pull/12245>`__: TST: tests for sort_complex()
* `#12246 <https://github.com/numpy/numpy/pull/12246>`__: DOC: Update docs after NumPy 1.15.3 release.
* `#12249 <https://github.com/numpy/numpy/pull/12249>`__: BUG: Dealloc cached buffer info
* `#12250 <https://github.com/numpy/numpy/pull/12250>`__: DOC: add missing docs
* `#12251 <https://github.com/numpy/numpy/pull/12251>`__: MAINT: improved error message when no `__array_function__` implementation...
* `#12254 <https://github.com/numpy/numpy/pull/12254>`__: MAINT: Move ctype -> dtype conversion to python
* `#12257 <https://github.com/numpy/numpy/pull/12257>`__: BUG: Fix fill value in masked array '==' and '!=' ops.
* `#12259 <https://github.com/numpy/numpy/pull/12259>`__: TST: simplify how the different code paths for block are tested.
* `#12265 <https://github.com/numpy/numpy/pull/12265>`__: BUG: Revert linspace import for concatenation funcs
* `#12266 <https://github.com/numpy/numpy/pull/12266>`__: BUG: Avoid SystemErrors by checking the return value of PyPrint
* `#12268 <https://github.com/numpy/numpy/pull/12268>`__: DOC: add broadcasting article from scipy old-wiki
* `#12270 <https://github.com/numpy/numpy/pull/12270>`__: MAINT: set `__module__` for more `array_function_dispatch` uses
* `#12276 <https://github.com/numpy/numpy/pull/12276>`__: MAINT: remove unused parse_index()
* `#12279 <https://github.com/numpy/numpy/pull/12279>`__: NEP: tweak and mark NEP 0027 as final
* `#12280 <https://github.com/numpy/numpy/pull/12280>`__: DEP: deprecate passing a generator to stack functions
* `#12281 <https://github.com/numpy/numpy/pull/12281>`__: NEP: revise note for NEP 27
* `#12285 <https://github.com/numpy/numpy/pull/12285>`__: ENH: array does not need to be writable to use as input to take
* `#12286 <https://github.com/numpy/numpy/pull/12286>`__: ENH: Do not emit compiler warning if forcing old API
* `#12288 <https://github.com/numpy/numpy/pull/12288>`__: BUILD: force LGTM to use cython>=0.29
* `#12291 <https://github.com/numpy/numpy/pull/12291>`__: MAINT: `_set_out_array()` syntax fix
* `#12292 <https://github.com/numpy/numpy/pull/12292>`__: MAINT: removed unused vars in f2py test code
* `#12299 <https://github.com/numpy/numpy/pull/12299>`__: BUILD: use system python3 in the chroot
* `#12302 <https://github.com/numpy/numpy/pull/12302>`__: DOC: Update the docstring of asfortranarray and ascontiguousarray
* `#12306 <https://github.com/numpy/numpy/pull/12306>`__: TST: add 32-bit linux Azure CI job
* `#12312 <https://github.com/numpy/numpy/pull/12312>`__: MAINT, TST: unreachable Python code paths
* `#12321 <https://github.com/numpy/numpy/pull/12321>`__: MAINT: Simple speed-ups for getting overloaded types
* `#12326 <https://github.com/numpy/numpy/pull/12326>`__: DOC: NumPy 1.15.4 post release documentation update.
* `#12328 <https://github.com/numpy/numpy/pull/12328>`__: MAINT: Allow subclasses in `ndarray.__array_function__`.
* `#12330 <https://github.com/numpy/numpy/pull/12330>`__: TST: test_tofile_fromfile now uses initialized memory
* `#12331 <https://github.com/numpy/numpy/pull/12331>`__: DEV: change ASV benchmarks to run on Python 3.6 by default
* `#12338 <https://github.com/numpy/numpy/pull/12338>`__: DOC: add a docstring for the function 'compare_chararrays' (See...
* `#12342 <https://github.com/numpy/numpy/pull/12342>`__: BUG: Fix for np.dtype(ctypes.Structure) does not respect _pack_...
* `#12347 <https://github.com/numpy/numpy/pull/12347>`__: DOC: typo in docstring numpy.random.beta, shape parameters must...
* `#12349 <https://github.com/numpy/numpy/pull/12349>`__: TST, DOC: store circleci doc artifacts
* `#12353 <https://github.com/numpy/numpy/pull/12353>`__: BUG: test, fix for threshold='nan'
* `#12354 <https://github.com/numpy/numpy/pull/12354>`__: BUG: Fix segfault when an error occurs in np.fromfile
* `#12355 <https://github.com/numpy/numpy/pull/12355>`__: BUG: fix a bug in npy_PyFile_Dup2 where it didn't return immediately...
* `#12357 <https://github.com/numpy/numpy/pull/12357>`__: MAINT: Cleanup pavement file
* `#12358 <https://github.com/numpy/numpy/pull/12358>`__: BUG: test, fix loading structured dtypes with padding
* `#12362 <https://github.com/numpy/numpy/pull/12362>`__: MAINT: disable `__array_function__` dispatch unless environment...
* `#12363 <https://github.com/numpy/numpy/pull/12363>`__: MAINT: update gfortran RPATH for AIX/Windows non-support.
* `#12364 <https://github.com/numpy/numpy/pull/12364>`__: NEP: clarify the purpose of "types" in `__array_function__`.
* `#12366 <https://github.com/numpy/numpy/pull/12366>`__: MAINT: Refactor sorting header file
* `#12372 <https://github.com/numpy/numpy/pull/12372>`__: BUG: random: Fix handling of a=0 for numpy.random.weibull.
* `#12373 <https://github.com/numpy/numpy/pull/12373>`__: MAINT: Improve error message for legal but unsupported PEP3118...
* `#12376 <https://github.com/numpy/numpy/pull/12376>`__: BUG: do not override exception on import failure
* `#12377 <https://github.com/numpy/numpy/pull/12377>`__: NEP: move nep 15 from accepted to final
* `#12378 <https://github.com/numpy/numpy/pull/12378>`__: TST: Update complex long double precision tests.
* `#12380 <https://github.com/numpy/numpy/pull/12380>`__: BUG: Fix for #10533 np.dtype(ctype) does not respect endianness
* `#12381 <https://github.com/numpy/numpy/pull/12381>`__: BUG: graceful DataSource __del__ when __init__ fails
* `#12382 <https://github.com/numpy/numpy/pull/12382>`__: ENH: set correct __module__ for objects in numpy's public API
* `#12388 <https://github.com/numpy/numpy/pull/12388>`__: ENH: allow arrays for start and stop in {lin,log,geom}space
* `#12390 <https://github.com/numpy/numpy/pull/12390>`__: DEV: remove shim added in 1.4
* `#12391 <https://github.com/numpy/numpy/pull/12391>`__: DEP: raise on a call to deprecated numpy.lib.function_base.unique
* `#12392 <https://github.com/numpy/numpy/pull/12392>`__: DOC: Add release notes for ctypes improvements
* `#12398 <https://github.com/numpy/numpy/pull/12398>`__: BUG: fix possible overlap issues with avx enabled
* `#12399 <https://github.com/numpy/numpy/pull/12399>`__: DOC: Fix typo in polyint. Fixes #12386.
* `#12405 <https://github.com/numpy/numpy/pull/12405>`__: ENH: Add support for `np.dtype(ctypes.Union)`
* `#12407 <https://github.com/numpy/numpy/pull/12407>`__: BUG: Fall back to 'ascii' locale in build (if needed)
* `#12408 <https://github.com/numpy/numpy/pull/12408>`__: BUG: multifield-view of MaskedArray gets bad fill_value
* `#12409 <https://github.com/numpy/numpy/pull/12409>`__: MAINT: correct the dtype.descr docstring
* `#12413 <https://github.com/numpy/numpy/pull/12413>`__: BUG: Do not double-quote arguments to the command line
* `#12414 <https://github.com/numpy/numpy/pull/12414>`__: MAINT: Update cversion hash.
* `#12417 <https://github.com/numpy/numpy/pull/12417>`__: BUG: Fix regression on np.dtype(ctypes.c_void_p)
* `#12419 <https://github.com/numpy/numpy/pull/12419>`__: Fix PyArray_FillFunc function definitions
* `#12420 <https://github.com/numpy/numpy/pull/12420>`__: gfortran needs -lpthread & -maix64(64 build) in AIX
* `#12422 <https://github.com/numpy/numpy/pull/12422>`__: MNT: Reword error message about loading pickled data.
* `#12424 <https://github.com/numpy/numpy/pull/12424>`__: BUG: Fix inconsistent cache keying in ndpointer
* `#12429 <https://github.com/numpy/numpy/pull/12429>`__: MAINT: Update mailmap for 1.16.0 release.
* `#12431 <https://github.com/numpy/numpy/pull/12431>`__: BUG/ENH: Fix use of ndpointer in return values
* `#12437 <https://github.com/numpy/numpy/pull/12437>`__: MAINT: refactor datetime.c_metadata creation
* `#12439 <https://github.com/numpy/numpy/pull/12439>`__: BUG: test, fix NPY_VISIBILITY_HIDDEN on gcc, which becomes NPY_NO_EXPORT
* `#12440 <https://github.com/numpy/numpy/pull/12440>`__: BUG: don't override original errors when casting inside np.dot()...
* `#12443 <https://github.com/numpy/numpy/pull/12443>`__: MAINT Use set litterals
* `#12445 <https://github.com/numpy/numpy/pull/12445>`__: MAINT: Use list and dict comprehension when possible
* `#12446 <https://github.com/numpy/numpy/pull/12446>`__: MAINT: Fixups to new functions in np.lib.recfunctions
* `#12447 <https://github.com/numpy/numpy/pull/12447>`__: ENH: add back the multifield copy->view change
* `#12448 <https://github.com/numpy/numpy/pull/12448>`__: MAINT: Review F401,F841,F842 flake8 errors (unused variables...
* `#12455 <https://github.com/numpy/numpy/pull/12455>`__: TST: use condition directive for Azure 2.7 check
* `#12458 <https://github.com/numpy/numpy/pull/12458>`__: MAINT, DOC: fix Azure README badge
* `#12464 <https://github.com/numpy/numpy/pull/12464>`__: BUG: IndexError for empty list on structured MaskedArray.
* `#12466 <https://github.com/numpy/numpy/pull/12466>`__: TST: use openblas for Windows CI
* `#12470 <https://github.com/numpy/numpy/pull/12470>`__: MAINT: remove wrapper functions from numpy.core.multiarray
* `#12471 <https://github.com/numpy/numpy/pull/12471>`__: ENH: override support for np.linspace and friends
* `#12474 <https://github.com/numpy/numpy/pull/12474>`__: TST: enable dispatcher test coverage
* `#12477 <https://github.com/numpy/numpy/pull/12477>`__: DOC: fix example for __call__. See #12451
* `#12486 <https://github.com/numpy/numpy/pull/12486>`__: DOC: Update copyright year in the license
* `#12488 <https://github.com/numpy/numpy/pull/12488>`__: ENH: implement matmul on NDArrayOperatorsMixin
* `#12493 <https://github.com/numpy/numpy/pull/12493>`__: BUG: fix records.fromfile fails to read data >4 GB
* `#12494 <https://github.com/numpy/numpy/pull/12494>`__: BUG: test, fix matmul, dot for vector array with stride[i]=0
* `#12498 <https://github.com/numpy/numpy/pull/12498>`__: TST: sync Azure Win openblas
* `#12501 <https://github.com/numpy/numpy/pull/12501>`__: MAINT: removed word/typo from comment in site.cfg.example
* `#12556 <https://github.com/numpy/numpy/pull/12556>`__: BUG: only override vector size for avx code for 1.16
* `#12562 <https://github.com/numpy/numpy/pull/12562>`__: DOC, MAINT: Make `PYVER = 3` in doc/Makefile.
* `#12563 <https://github.com/numpy/numpy/pull/12563>`__: DOC: more doc updates for structured arrays
* `#12564 <https://github.com/numpy/numpy/pull/12564>`__: BUG: fix an unsafe PyTuple_GET_ITEM call
* `#12565 <https://github.com/numpy/numpy/pull/12565>`__: Fix lgtm.com C/C++ build
* `#12567 <https://github.com/numpy/numpy/pull/12567>`__: BUG: reorder operations for VS2015
* `#12568 <https://github.com/numpy/numpy/pull/12568>`__: BUG: fix improper use of C-API
* `#12569 <https://github.com/numpy/numpy/pull/12569>`__: BUG: Make new-lines in compiler error messages print to the console
* `#12570 <https://github.com/numpy/numpy/pull/12570>`__: MAINT: don't check alignment size=0 arrays (RELAXED_STRIDES)
* `#12573 <https://github.com/numpy/numpy/pull/12573>`__: BUG: fix refcount issue caused by #12524
* `#12580 <https://github.com/numpy/numpy/pull/12580>`__: BUG: fix segfault in ctypeslib with obj being collected
* `#12581 <https://github.com/numpy/numpy/pull/12581>`__: TST: activate shippable maintenance branches
* `#12582 <https://github.com/numpy/numpy/pull/12582>`__: BUG: fix f2py pep338 execution method
* `#12587 <https://github.com/numpy/numpy/pull/12587>`__: BUG: Make `arr.ctypes.data` hold a reference to the underlying...
* `#12588 <https://github.com/numpy/numpy/pull/12588>`__: BUG: check for errors after PyArray_DESCR_REPLACE
* `#12590 <https://github.com/numpy/numpy/pull/12590>`__: DOC, MAINT: Prepare for 1.16.0rc1 release.
* `#12603 <https://github.com/numpy/numpy/pull/12603>`__: DOC: Fix markup in 1.16.0 release notes.
* `#12621 <https://github.com/numpy/numpy/pull/12621>`__: BUG: longdouble with elsize 12 is never uint alignable.
* `#12622 <https://github.com/numpy/numpy/pull/12622>`__: BUG: Add missing free in ufunc dealloc
* `#12623 <https://github.com/numpy/numpy/pull/12623>`__: MAINT: add test for 12-byte alignment
* `#12655 <https://github.com/numpy/numpy/pull/12655>`__: BUG: fix uint alignment asserts in lowlevel loops
* `#12656 <https://github.com/numpy/numpy/pull/12656>`__: BENCH: don't fail at import time with old Numpy
* `#12657 <https://github.com/numpy/numpy/pull/12657>`__: DOC: update 2018 -> 2019
* `#12705 <https://github.com/numpy/numpy/pull/12705>`__: ENH: Better links in documentation
* `#12706 <https://github.com/numpy/numpy/pull/12706>`__: MAINT: Further fixups to uint alignment checks
* `#12707 <https://github.com/numpy/numpy/pull/12707>`__: BUG: Add 'sparc' to platforms implementing 16 byte reals.
* `#12708 <https://github.com/numpy/numpy/pull/12708>`__: TST: Fix endianness in unstuctured_to_structured test
* `#12710 <https://github.com/numpy/numpy/pull/12710>`__: TST: pin Azure brew version for stability.

Contributors
============

A total of 10 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Carl Johnsen +
* Charles Harris
* Gwyn Ciesla +
* Matthieu Dartiailh
* Matti Picus
* Niyas Sait +
* Ralf Gommers
* Sayed Adel
* Sebastian Berg

Pull requests merged
====================

A total of 18 pull requests were merged for this release.

* `#19497 <https://github.com/numpy/numpy/pull/19497>`__: MAINT: set Python version for 1.21.x to ``<3.11``
* `#19533 <https://github.com/numpy/numpy/pull/19533>`__: BUG: Fix an issue wherein importing ``numpy.typing`` could raise
* `#19646 <https://github.com/numpy/numpy/pull/19646>`__: MAINT: Update Cython version for Python 3.10.
* `#19648 <https://github.com/numpy/numpy/pull/19648>`__: TST: Bump the python 3.10 test version from beta4 to rc1
* `#19651 <https://github.com/numpy/numpy/pull/19651>`__: TST: avoid distutils.sysconfig in runtests.py
* `#19652 <https://github.com/numpy/numpy/pull/19652>`__: MAINT: add missing dunder method to nditer type hints
* `#19656 <https://github.com/numpy/numpy/pull/19656>`__: BLD, SIMD: Fix testing extra checks when ``-Werror`` isn't applicable...
* `#19657 <https://github.com/numpy/numpy/pull/19657>`__: BUG: Remove logical object ufuncs with bool output
* `#19658 <https://github.com/numpy/numpy/pull/19658>`__: MAINT: Include .coveragerc in source distributions to support...
* `#19659 <https://github.com/numpy/numpy/pull/19659>`__: BUG: Fix bad write in masked iterator output copy paths
* `#19660 <https://github.com/numpy/numpy/pull/19660>`__: ENH: Add support for windows on arm targets
* `#19661 <https://github.com/numpy/numpy/pull/19661>`__: BUG: add base to templated arguments for platlib
* `#19662 <https://github.com/numpy/numpy/pull/19662>`__: BUG,DEP: Non-default UFunc signature/dtype usage should be deprecated
* `#19666 <https://github.com/numpy/numpy/pull/19666>`__: MAINT: Add Python 3.10 to supported versions.
* `#19668 <https://github.com/numpy/numpy/pull/19668>`__: TST,BUG: Sanitize path-separators when running ``runtest.py``
* `#19671 <https://github.com/numpy/numpy/pull/19671>`__: BLD: load extra flags when checking for libflame
* `#19676 <https://github.com/numpy/numpy/pull/19676>`__: BLD: update circleCI docker image
* `#19677 <https://github.com/numpy/numpy/pull/19677>`__: REL: Prepare for 1.21.2 release.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Eric Wieser
* Pauli Virtanen

Pull requests merged
====================

A total of 5 pull requests were merged for this release.

* `#10674 <https://github.com/numpy/numpy/pull/10674>`__: BUG: Further back-compat fix for subclassed array repr
* `#10725 <https://github.com/numpy/numpy/pull/10725>`__: BUG: dragon4 fractional output mode adds too many trailing zeros
* `#10726 <https://github.com/numpy/numpy/pull/10726>`__: BUG: Fix f2py generated code to work on PyPy
* `#10727 <https://github.com/numpy/numpy/pull/10727>`__: BUG: Fix missing NPY_VISIBILITY_HIDDEN on npy_longdouble_to_PyLong
* `#10729 <https://github.com/numpy/numpy/pull/10729>`__: DOC: Create 1.14.2 notes and changelog.

Contributors
============

A total of 1 person contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#11274 <https://github.com/numpy/numpy/pull/11274>`__: BUG: Correct use of NPY_UNUSED.
* `#11294 <https://github.com/numpy/numpy/pull/11294>`__: BUG: Remove extra trailing parentheses.

Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Jonathan March +
* Malcolm Smith +
* Matti Picus
* Pauli Virtanen

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#10862 <https://github.com/numpy/numpy/pull/10862>`__: BUG: floating types should override tp_print (1.14 backport)
* `#10905 <https://github.com/numpy/numpy/pull/10905>`__: BUG: for 1.14 back-compat, accept list-of-lists in fromrecords
* `#10947 <https://github.com/numpy/numpy/pull/10947>`__: BUG: 'style' arg to array2string broken in legacy mode (1.14...
* `#10959 <https://github.com/numpy/numpy/pull/10959>`__: BUG: test, fix for missing flags['WRITEBACKIFCOPY'] key
* `#10960 <https://github.com/numpy/numpy/pull/10960>`__: BUG: Add missing underscore to prototype in check_embedded_lapack
* `#10961 <https://github.com/numpy/numpy/pull/10961>`__: BUG: Fix encoding regression in ma/bench.py (Issue #10868)
* `#10962 <https://github.com/numpy/numpy/pull/10962>`__: BUG: core: fix NPY_TITLE_KEY macro on pypy
* `#10974 <https://github.com/numpy/numpy/pull/10974>`__: BUG: test, fix PyArray_DiscardWritebackIfCopy...
=========
Changelog
=========

Contributors
============

A total of 10 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Greg Young
* Joerg Behrmann +
* John Kirkham
* Julian Taylor
* Marten van Kerkwijk
* Matthew Brett
* Shota Kawabuchi
* Jean Utke +

Pull requests merged
====================

* `#8483 <https://github.com/numpy/numpy/pull/8483>`__: BUG: Fix wrong future nat warning and equiv type logic error...
* `#8489 <https://github.com/numpy/numpy/pull/8489>`__: BUG: Fix wrong masked median for some special cases
* `#8490 <https://github.com/numpy/numpy/pull/8490>`__: DOC: Place np.average in inline code
* `#8491 <https://github.com/numpy/numpy/pull/8491>`__: TST: Work around isfinite inconsistency on i386
* `#8494 <https://github.com/numpy/numpy/pull/8494>`__: BUG: Guard against replacing constants without '_' spec in f2py.
* `#8524 <https://github.com/numpy/numpy/pull/8524>`__: BUG: Fix mean for float 16 non-array inputs for 1.12
* `#8571 <https://github.com/numpy/numpy/pull/8571>`__: BUG: Fix calling python api with error set and minor leaks for...
* `#8602 <https://github.com/numpy/numpy/pull/8602>`__: BUG: Make iscomplexobj compatible with custom dtypes again
* `#8618 <https://github.com/numpy/numpy/pull/8618>`__: BUG: Fix undefined behaviour induced by bad __array_wrap__
* `#8648 <https://github.com/numpy/numpy/pull/8648>`__: BUG: Fix MaskedArray.__setitem__
* `#8659 <https://github.com/numpy/numpy/pull/8659>`__: BUG: PPC64el machines are POWER for Fortran in f2py
* `#8665 <https://github.com/numpy/numpy/pull/8665>`__: BUG: Look up methods on MaskedArray in `_frommethod`
* `#8674 <https://github.com/numpy/numpy/pull/8674>`__: BUG: Remove extra digit in binary_repr at limit
* `#8704 <https://github.com/numpy/numpy/pull/8704>`__: BUG: Fix deepcopy regression for empty arrays.
* `#8707 <https://github.com/numpy/numpy/pull/8707>`__: BUG: Fix ma.median for empty ndarrays

Contributors
============

A total of 153 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* @DWesl
* @Illviljan
* @h-vetinari
* @yan-wyb +
* Aaron Meurer
* Abel Aoun +
* Adrian Gao +
* Ahmet Can Solak +
* Ajay DS +
* Alban Colley +
* Alberto Rubiales +
* Alessia Marcolini +
* Amit Kumar +
* Andrei Batomunkuev +
* Andrew Watson +
* Anirudh Dagar +
* Ankit Dwivedi +
* Antony Lee
* Arfy Slowy +
* Arryan Singh +
* Arun Palaniappen +
* Arushi Sharma +
* Bas van Beek
* Brent Brewington +
* Carl Johnsen +
* Carl Michal +
* Charles Harris
* Chiara Marmo
* Chris Fu (傅立业) +
* Christoph Buchner +
* Christoph Reiter +
* Chunlin Fang
* Clément Robert +
* Constanza Fierro
* Damien Caliste
* Daniel Ching
* David Badnar +
* David Cortes +
* David Okpare +
* Derek Huang +
* Developer-Ecosystem-Engineering +
* Dima Pasechnik
* Dimitri Papadopoulos +
* Dmitriy Fishman +
* Eero Vaher +
* Elias Koromilas +
* Eliaz Bobadilla +
* Elisha Hollander +
* Eric Wieser
* Eskild Eriksen +
* Evan Miller +
* Fayas Noushad +
* Gagandeep Singh +
* Ganesh Kathiresan
* Ghiles Meddour +
* Greg Lucas
* Gregory R. Lee
* Guo Shuai +
* Gwyn Ciesla +
* Hameer Abbasi
* Hector Martin +
* Henry Schreiner +
* Himanshu +
* Hood Chatham +
* Hugo Defois +
* Hugo van Kemenade
* I-Shen Leong +
* Imen Rajhi +
* Irina Maria Mocan +
* Irit Katriel +
* Isuru Fernando
* Jakob Jakobson
* Jerry Morrison +
* Jessi J Zhao +
* Joe Marshall +
* Johan von Forstner +
* Jonas I. Liechti +
* Jonathan Reichelt Gjertsen +
* Joshua Himmens +
* Jérome Eertmans
* Jérôme Kieffer +
* KIU Shueng Chuan +
* Kazuki Sakamoto +
* Kenichi Maehashi
* Kenny Huynh +
* Kent R. Spillner +
* Kevin Granados +
* Kevin Modzelewski +
* Kevin Sheppard
* Lalit Musmade +
* Malik Idrees Hasan Khan +
* Marco Aurelio da Costa +
* Margret Pax +
* Mars Lee +
* Marten van Kerkwijk
* Matthew Barber +
* Matthew Brett
* Matthias Bussonnier
* Matthieu Dartiailh
* Matti Picus
* Melissa Weber Mendonça
* Michael McCann +
* Mike Jarvis +
* Mike McCann +
* Mike Toews
* Mukulika Pahari
* Nick Pope +
* Nick Wogan +
* Niels Dunnewind +
* Niko Savola +
* Nikola Forró
* Niyas Sait +
* Pamphile ROY
* Paul Ganssle +
* Pauli Virtanen
* Pearu Peterson
* Peter Hawkins +
* Peter Tillema +
* Prathmesh Shirsat +
* Raghuveer Devulapalli
* Ralf Gommers
* Robert Kern
* Rohit Goswami +
* Ronan Lamy
* Ross Barnowski
* Roy Jacobson +
* Samyak S Sarnayak +
* Sayantika Banik +
* Sayed Adel
* Sebastian Berg
* Sebastian Schleehauf +
* Serge Guelton
* Shriraj Hegde +
* Shubham Gupta +
* Sista Seetaram +
* Stefan van der Walt
* Stephannie Jimenez Gacha +
* Tania Allard
* Theodoros Nikolaou +
* Thomas Green +
* Thomas J. Fan
* Thomas Li +
* Tim Hoffmann
* Tom Tan +
* Tyler Reddy
* Vijay Arora +
* Vinith Kishore +
* Warren Weckesser
* Yang Hau
* Yashasvi Misra
* Yuval Ofek +
* Zac Hatfield-Dodds
* Zhang Na +

Pull requests merged
====================

A total of 609 pull requests were merged for this release.

* `#15847 <https://github.com/numpy/numpy/pull/15847>`__: BUG: avoid infinite recurrence on dependencies in crackfortran
* `#16740 <https://github.com/numpy/numpy/pull/16740>`__: ENH: Add broadcast support to Generator.multinomial
* `#16796 <https://github.com/numpy/numpy/pull/16796>`__: DOC: Added a warning about fractional steps in np.arange
* `#17530 <https://github.com/numpy/numpy/pull/17530>`__: ENH: Allow ``ctypeslib.load_library`` to take any path-like object
* `#17582 <https://github.com/numpy/numpy/pull/17582>`__: ENH: Configurable allocator
* `#18203 <https://github.com/numpy/numpy/pull/18203>`__: MAINT: Speedup np.quantile.
* `#18330 <https://github.com/numpy/numpy/pull/18330>`__: TST: Add cygwin build to CI
* `#18421 <https://github.com/numpy/numpy/pull/18421>`__: DOC: Adjust polyfit doc to clarify the meaning of w
* `#18536 <https://github.com/numpy/numpy/pull/18536>`__: ENH: Add smallest_normal and smallest_subnormal attributes to...
* `#18585 <https://github.com/numpy/numpy/pull/18585>`__: ENH: Implementation of the NEP 47 (adopting the array API standard)
* `#18759 <https://github.com/numpy/numpy/pull/18759>`__: BUG: revise string_from_pyobj/try_pyarr_from_string with respect...
* `#18762 <https://github.com/numpy/numpy/pull/18762>`__: MAINT: Remove unused imports and unreachable code
* `#18775 <https://github.com/numpy/numpy/pull/18775>`__: DOC: Ensure that we add documentation also as to the dict for...
* `#18884 <https://github.com/numpy/numpy/pull/18884>`__: DOC: Add support for documenting C/C++ via Doxygen & Breathe
* `#18905 <https://github.com/numpy/numpy/pull/18905>`__: MAINT: Refactor reductions to use NEP 43 style dispatching/promotion
* `#18964 <https://github.com/numpy/numpy/pull/18964>`__: DOC: replace np.ma functions' return types with ``MaskedArray``
* `#18984 <https://github.com/numpy/numpy/pull/18984>`__: DOC: add example showing how to convert POSIX timestamps to datetime64
* `#19003 <https://github.com/numpy/numpy/pull/19003>`__: DOC: Remove misleading info about Fortran compiler in Building...
* `#19016 <https://github.com/numpy/numpy/pull/19016>`__: BUG: Update coordinates on PyArray_ITER_GOTO1D
* `#19022 <https://github.com/numpy/numpy/pull/19022>`__: SIMD: Add new universal intrinsic for ceil
* `#19023 <https://github.com/numpy/numpy/pull/19023>`__: BUG: fix np.ma.MaskedArray.anom when input is masked
* `#19036 <https://github.com/numpy/numpy/pull/19036>`__: MAINT: replace imgmath with mathjax for docs
* `#19058 <https://github.com/numpy/numpy/pull/19058>`__: BUG: Fixes to getter signatures
* `#19060 <https://github.com/numpy/numpy/pull/19060>`__: ENH: Add initial annotations to ``np.core.multiarray``
* `#19062 <https://github.com/numpy/numpy/pull/19062>`__: ENH: Add a mypy plugin for inferring the precision of ``np.ctypeslib.c_intp``
* `#19070 <https://github.com/numpy/numpy/pull/19070>`__: REL: Prepare for NumPy 1.22.0 development
* `#19071 <https://github.com/numpy/numpy/pull/19071>`__: BUG: Fix compile-time test of POPCNT
* `#19072 <https://github.com/numpy/numpy/pull/19072>`__: BUG, TST: Fix test_numpy_version.
* `#19082 <https://github.com/numpy/numpy/pull/19082>`__: MAINT: Bump hypothesis from 6.12.0 to 6.13.4
* `#19083 <https://github.com/numpy/numpy/pull/19083>`__: ENH: Implement the DLPack Array API protocols for ndarray.
* `#19086 <https://github.com/numpy/numpy/pull/19086>`__: BUG: Linter should only run on pull requests.
* `#19087 <https://github.com/numpy/numpy/pull/19087>`__: DOC: Add note to savez about naming variables with keyword ``file``.
* `#19089 <https://github.com/numpy/numpy/pull/19089>`__: DOC: Add example to histogram2d docstring
* `#19090 <https://github.com/numpy/numpy/pull/19090>`__: MAINT: removed unused imports listed in LGTM
* `#19092 <https://github.com/numpy/numpy/pull/19092>`__: BUG: Fixed an issue wherein ``_GenericAlias.__getitem__`` would...
* `#19093 <https://github.com/numpy/numpy/pull/19093>`__: DOC: add a "Returns" section for ``np.frombuffer``
* `#19096 <https://github.com/numpy/numpy/pull/19096>`__: BUG: Fix setup.py to work in maintenance branches.
* `#19098 <https://github.com/numpy/numpy/pull/19098>`__: BUG, SIMD: Fix detect host/native CPU features on ICC during...
* `#19099 <https://github.com/numpy/numpy/pull/19099>`__: DOC: fixed unsigned integer alias links.
* `#19102 <https://github.com/numpy/numpy/pull/19102>`__: MAINT: Removed suitable unused variables shown in LGTM
* `#19110 <https://github.com/numpy/numpy/pull/19110>`__: DOC: Fix the documented default value of the ``order`` parameter...
* `#19115 <https://github.com/numpy/numpy/pull/19115>`__: DOC: Misc fixes to ``absolute_beginners.html``
* `#19118 <https://github.com/numpy/numpy/pull/19118>`__: MAINT: Misc cleaning of ``numpy.typing``
* `#19119 <https://github.com/numpy/numpy/pull/19119>`__: BUG: Adjust shallow clone in the gitpod container
* `#19121 <https://github.com/numpy/numpy/pull/19121>`__: DOC: Fix missing files and deprecated commands.
* `#19124 <https://github.com/numpy/numpy/pull/19124>`__: BUG: Fixed an issue wherein ``poly1d.__getitem__`` could return...
* `#19128 <https://github.com/numpy/numpy/pull/19128>`__: DOC:``Building the NumPy API and reference docs`` rewrite
* `#19130 <https://github.com/numpy/numpy/pull/19130>`__: ENH: SIMD architectures to show_config
* `#19131 <https://github.com/numpy/numpy/pull/19131>`__: DOC: added explanation about tril/triu n-dimensional functionality.
* `#19132 <https://github.com/numpy/numpy/pull/19132>`__: BUG: Use larger fetch depth in gitpod.yml
* `#19135 <https://github.com/numpy/numpy/pull/19135>`__: BUG: Remove complex floor divide
* `#19139 <https://github.com/numpy/numpy/pull/19139>`__: MAINT: Bump hypothesis from 6.13.4 to 6.13.10
* `#19140 <https://github.com/numpy/numpy/pull/19140>`__: ENH: Add dtype-support to 3 ``generic``/``ndarray`` methods
* `#19142 <https://github.com/numpy/numpy/pull/19142>`__: BUG: expose ``short_version`` as previously in version.py
* `#19151 <https://github.com/numpy/numpy/pull/19151>`__: ENH: Vectorising np.linalg.qr
* `#19165 <https://github.com/numpy/numpy/pull/19165>`__: DOC: Explicitly mention that ``searchsorted`` returns an integer...
* `#19167 <https://github.com/numpy/numpy/pull/19167>`__: ENH: Improve readibility of error message in terminal.
* `#19170 <https://github.com/numpy/numpy/pull/19170>`__: API: Delay string and number promotion deprecation/future warning
* `#19172 <https://github.com/numpy/numpy/pull/19172>`__: BUG: Fixed an issue wherein ``_GenericAlias`` could raise for non-iterable...
* `#19173 <https://github.com/numpy/numpy/pull/19173>`__: ENH: Add support for copy modes to NumPy
* `#19174 <https://github.com/numpy/numpy/pull/19174>`__: MAINT, BUG: Adapt ``castingimpl.casting`` to denote a minimal level
* `#19176 <https://github.com/numpy/numpy/pull/19176>`__: REV,BUG: Replace ``NotImplemented`` with ``typing.Any``
* `#19177 <https://github.com/numpy/numpy/pull/19177>`__: BUG: Add ``-std=c99`` to intel icc compiler flags on linux
* `#19179 <https://github.com/numpy/numpy/pull/19179>`__: ENH: Add annotations for ``np.testing``
* `#19181 <https://github.com/numpy/numpy/pull/19181>`__: MAINT: Bump pytest-cov from 2.12.0 to 2.12.1
* `#19182 <https://github.com/numpy/numpy/pull/19182>`__: MAINT: Bump hypothesis from 6.13.10 to 6.13.14
* `#19185 <https://github.com/numpy/numpy/pull/19185>`__: DOC: Crosslinking to Gitpod guide
* `#19186 <https://github.com/numpy/numpy/pull/19186>`__: DOC: ndindex class docstrings fix
* `#19188 <https://github.com/numpy/numpy/pull/19188>`__: NEP: Accept NEP 35 (``like=`` keyword for array creation) as final
* `#19195 <https://github.com/numpy/numpy/pull/19195>`__: DOC: Link issue label
* `#19196 <https://github.com/numpy/numpy/pull/19196>`__: DOC: update references to other repos head branch to 'main'
* `#19200 <https://github.com/numpy/numpy/pull/19200>`__: DOC: NeighborhoodIterator position on creation
* `#19202 <https://github.com/numpy/numpy/pull/19202>`__: BUG: Fix out-of-bounds access in convert_datetime_divisor_to_multiple
* `#19209 <https://github.com/numpy/numpy/pull/19209>`__: TST: Ignore exp FP exceptions test for glibc ver < 2.17
* `#19211 <https://github.com/numpy/numpy/pull/19211>`__: ENH: Adding keepdims to np.argmin,np.argmax
* `#19212 <https://github.com/numpy/numpy/pull/19212>`__: MAINT: Add annotations for the missing ``period`` parameter to...
* `#19214 <https://github.com/numpy/numpy/pull/19214>`__: ENH: Support major version larger than 9 in ``NumpyVersion``
* `#19218 <https://github.com/numpy/numpy/pull/19218>`__: MAINT: Add ``complex`` as allowed type for the ``np.complexfloating``...
* `#19223 <https://github.com/numpy/numpy/pull/19223>`__: ENH: Add annotations for ``np.pad``
* `#19224 <https://github.com/numpy/numpy/pull/19224>`__: MAINT: Remove python 2 specific string comparison code
* `#19225 <https://github.com/numpy/numpy/pull/19225>`__: DOC: Fix some inconsistencies in the docstring of matrix_rank
* `#19227 <https://github.com/numpy/numpy/pull/19227>`__: ENH: Add annotations to ``np.core.multiarray`` part 2/4
* `#19228 <https://github.com/numpy/numpy/pull/19228>`__: BUG: Invalid dtypes comparison should not raise ``TypeError``
* `#19235 <https://github.com/numpy/numpy/pull/19235>`__: Revert "BUG: revise string_from_pyobj/try_pyarr_from_string with...
* `#19237 <https://github.com/numpy/numpy/pull/19237>`__: ENH: Add annotations to ``np.core.multiarray`` part 3/4
* `#19241 <https://github.com/numpy/numpy/pull/19241>`__: MAINT: Bump hypothesis from 6.13.14 to 6.14.0
* `#19242 <https://github.com/numpy/numpy/pull/19242>`__: MAINT: Bump mypy from 0.812 to 0.902
* `#19244 <https://github.com/numpy/numpy/pull/19244>`__: BUG: Fix an issue wherein assigment to ``np.ma.masked_array`` ignores...
* `#19245 <https://github.com/numpy/numpy/pull/19245>`__: ENH: Add dtype-support to the ``np.core.shape_base`` annotations
* `#19251 <https://github.com/numpy/numpy/pull/19251>`__: BUG: revise string_from_pyobj/try_pyarr_from_string with respect...
* `#19254 <https://github.com/numpy/numpy/pull/19254>`__: MAINT: Refactor output ufunc wrapping logic
* `#19256 <https://github.com/numpy/numpy/pull/19256>`__: DOC: Fix formatting in rot90() docstring
* `#19257 <https://github.com/numpy/numpy/pull/19257>`__: MAINT: Move array-prep and type resolution to earlier
* `#19258 <https://github.com/numpy/numpy/pull/19258>`__: MAINT: Refactor and simplify the main ufunc iterator loop code
* `#19259 <https://github.com/numpy/numpy/pull/19259>`__: MAINT: Align masked with normal ufunc loops
* `#19261 <https://github.com/numpy/numpy/pull/19261>`__: ENH: Add annotations for ``np.lib.twodim_base``
* `#19262 <https://github.com/numpy/numpy/pull/19262>`__: MAINT: Some tiny fixes and style changes in ``ufunc_object.c``
* `#19263 <https://github.com/numpy/numpy/pull/19263>`__: STY: Small changes to the ``PyUFunc_ReduceWrapper``
* `#19264 <https://github.com/numpy/numpy/pull/19264>`__: DOC: fix duplicate navbar in development documentation index
* `#19275 <https://github.com/numpy/numpy/pull/19275>`__: MAINT: Misc typing maintenance for ``np.dtype``
* `#19276 <https://github.com/numpy/numpy/pull/19276>`__: BUG: Fix ``arr.flat.index`` for large arrays and big-endian machines
* `#19277 <https://github.com/numpy/numpy/pull/19277>`__: BUG: Add missing DECREF in new path
* `#19278 <https://github.com/numpy/numpy/pull/19278>`__: MAINT: Remove accidentally created directory.
* `#19281 <https://github.com/numpy/numpy/pull/19281>`__: ENH: add ``numpy.f2py.get_include`` function
* `#19284 <https://github.com/numpy/numpy/pull/19284>`__: NEP: Fixes from NEP36 feedback
* `#19285 <https://github.com/numpy/numpy/pull/19285>`__: MAINT: Use Ubuntu focal for travis-ci builds.
* `#19286 <https://github.com/numpy/numpy/pull/19286>`__: ENH: Add annotations for ``np.lib.type_check``
* `#19289 <https://github.com/numpy/numpy/pull/19289>`__: BUG: Fix reference count leak in ufunc dtype handling
* `#19290 <https://github.com/numpy/numpy/pull/19290>`__: DOC: Unpin pydata sphinx theme and update config to avoid long...
* `#19292 <https://github.com/numpy/numpy/pull/19292>`__: MAINT: Add lightweight identity-hash map
* `#19293 <https://github.com/numpy/numpy/pull/19293>`__: MAINT: Add simple tuple creation helper and use it
* `#19295 <https://github.com/numpy/numpy/pull/19295>`__: DOC: Add ``versionadded`` directives to ``numpy.typing``
* `#19298 <https://github.com/numpy/numpy/pull/19298>`__: DOC: Add documentation for ``np.ctypeslib.c_intp``
* `#19301 <https://github.com/numpy/numpy/pull/19301>`__: BUG: Do not raise deprecation warning for all nans in unique
* `#19306 <https://github.com/numpy/numpy/pull/19306>`__: DOC: Fix some docstrings that crash pdf generation.
* `#19314 <https://github.com/numpy/numpy/pull/19314>`__: MAINT: bump scipy-mathjax
* `#19316 <https://github.com/numpy/numpy/pull/19316>`__: BUG: Fix warning problems of the mod operator
* `#19317 <https://github.com/numpy/numpy/pull/19317>`__: MAINT: Clean up multiarray interned strings
* `#19320 <https://github.com/numpy/numpy/pull/19320>`__: REL: Update main after 1.21.0 release.
* `#19322 <https://github.com/numpy/numpy/pull/19322>`__: BUG: Fix cast safety and comparisons for zero sized voids
* `#19323 <https://github.com/numpy/numpy/pull/19323>`__: BUG: Correct Cython declaration in random
* `#19326 <https://github.com/numpy/numpy/pull/19326>`__: BUG: protect against accessing base attribute of a NULL subarray
* `#19328 <https://github.com/numpy/numpy/pull/19328>`__: MAINT: Replace ``"dtype[Any]"`` with ``dtype`` in the definiton of...
* `#19329 <https://github.com/numpy/numpy/pull/19329>`__: ENH Add a conda-based CI job on azure.
* `#19338 <https://github.com/numpy/numpy/pull/19338>`__: DOC: Removed duplicate instructions for building docs from ``dev/index``...
* `#19344 <https://github.com/numpy/numpy/pull/19344>`__: MAINT: Annotate missing attributes of ``np.number`` subclasses
* `#19355 <https://github.com/numpy/numpy/pull/19355>`__: ENH: Adding ``bit_count`` (popcount)
* `#19356 <https://github.com/numpy/numpy/pull/19356>`__: API: Ensure np.vectorize outputs can be subclasses.
* `#19359 <https://github.com/numpy/numpy/pull/19359>`__: ENH: Add annotations for ``np.f2py``
* `#19360 <https://github.com/numpy/numpy/pull/19360>`__: MAINT: remove ``print()``'s in distutils template handling
* `#19361 <https://github.com/numpy/numpy/pull/19361>`__: ENH: Use literals for annotating ``int``- & ``str``-based constants
* `#19362 <https://github.com/numpy/numpy/pull/19362>`__: BUG, SIMD: Fix detecting AVX512 features on Darwin
* `#19368 <https://github.com/numpy/numpy/pull/19368>`__: MAINT: Bump mypy from 0.902 to 0.910
* `#19369 <https://github.com/numpy/numpy/pull/19369>`__: DOC: Moved VQ example & target from duplicate array broadcasting...
* `#19370 <https://github.com/numpy/numpy/pull/19370>`__: MAINT: Move masked strided/inner-loop code to its "final" place
* `#19371 <https://github.com/numpy/numpy/pull/19371>`__: MAINT: Use cast-is-view flag for the ufunc trivial-loop check
* `#19378 <https://github.com/numpy/numpy/pull/19378>`__: DOC: fix remaining np.min/np.max usages
* `#19380 <https://github.com/numpy/numpy/pull/19380>`__: BUG: Fix NULL special case in object-to-any cast code
* `#19381 <https://github.com/numpy/numpy/pull/19381>`__: MAINT: Modify initialization order during multiarray import
* `#19393 <https://github.com/numpy/numpy/pull/19393>`__: MAINT: fix overly broad exception handling listed in LGTM
* `#19394 <https://github.com/numpy/numpy/pull/19394>`__: BUG, SIMD: Fix infinite loop during count non-zero on GCC-11
* `#19396 <https://github.com/numpy/numpy/pull/19396>`__: BUG: fix a numpy.npiter leak in npyiter_multi_index_set
* `#19402 <https://github.com/numpy/numpy/pull/19402>`__: DOC: typo fix
* `#19403 <https://github.com/numpy/numpy/pull/19403>`__: BUG: Fix memory leak in function npyiter_multi_index_set
* `#19404 <https://github.com/numpy/numpy/pull/19404>`__: NEP: update NEP with the PyDataMem_Handler struct as implemented...
* `#19407 <https://github.com/numpy/numpy/pull/19407>`__: DOC: Rearranged parts of the Indexing docs to consolidate content
* `#19408 <https://github.com/numpy/numpy/pull/19408>`__: ENH: Add annotations for misc python-based functions
* `#19409 <https://github.com/numpy/numpy/pull/19409>`__: BUG: fix some memory leaks in ufunc_object
* `#19412 <https://github.com/numpy/numpy/pull/19412>`__: MAINT: Bump sphinx from 4.0.1 to 4.0.3
* `#19413 <https://github.com/numpy/numpy/pull/19413>`__: MAINT: Bump hypothesis from 6.14.0 to 6.14.1
* `#19416 <https://github.com/numpy/numpy/pull/19416>`__: DOC: Remove duplicate information about governance
* `#19418 <https://github.com/numpy/numpy/pull/19418>`__: DOC: Removing tutorials from sphinx documentation
* `#19419 <https://github.com/numpy/numpy/pull/19419>`__: BUG: fix f2py markinnerspace for multiple quotations
* `#19421 <https://github.com/numpy/numpy/pull/19421>`__: ENH: Add annotations for ``np.core.getlimits``
* `#19422 <https://github.com/numpy/numpy/pull/19422>`__: DOC: Additional ideas related to numpy-tutorials integration
* `#19423 <https://github.com/numpy/numpy/pull/19423>`__: Skip finite recursion and refcounting tests for pyston
* `#19426 <https://github.com/numpy/numpy/pull/19426>`__: MAINT: Use arm64-graviton2 for testing on travis
* `#19429 <https://github.com/numpy/numpy/pull/19429>`__: BUG: Fix some multiarray leaks
* `#19431 <https://github.com/numpy/numpy/pull/19431>`__: MAINT: Delete old SSE2 ``absolute`` implementation
* `#19434 <https://github.com/numpy/numpy/pull/19434>`__: MAINT: Fix the module of ``flagsobj``
* `#19436 <https://github.com/numpy/numpy/pull/19436>`__: ENH: Improve the annotations of ``flagsobj``
* `#19440 <https://github.com/numpy/numpy/pull/19440>`__: MAINT: factored out _PyArray_ArgMinMaxCommon
* `#19442 <https://github.com/numpy/numpy/pull/19442>`__: MAINT: Use "with open(...)"
* `#19444 <https://github.com/numpy/numpy/pull/19444>`__: ENH: Add annotations for ``np.lib.shape_base``
* `#19445 <https://github.com/numpy/numpy/pull/19445>`__: DOC: broadcast_to() supports int as shape parameter
* `#19446 <https://github.com/numpy/numpy/pull/19446>`__: MAINT: Start testing with Python 3.10.0b3.
* `#19447 <https://github.com/numpy/numpy/pull/19447>`__: DOC: BLAS/LAPACK linking rules
* `#19450 <https://github.com/numpy/numpy/pull/19450>`__: TST: Simplify property-based test
* `#19451 <https://github.com/numpy/numpy/pull/19451>`__: BUG: Make openblas_support support ILP64 on Windows.
* `#19456 <https://github.com/numpy/numpy/pull/19456>`__: TST: Fix a ``GenericAlias`` test failure for python 3.9.0
* `#19458 <https://github.com/numpy/numpy/pull/19458>`__: MAINT: Avoid unicode characters in division SIMD code comments
* `#19459 <https://github.com/numpy/numpy/pull/19459>`__: ENH: Add the ``axis`` and ``ndim`` attributes to ``np.AxisError``
* `#19460 <https://github.com/numpy/numpy/pull/19460>`__: MAINT: Bump sphinx from 4.0.3 to 4.1.0
* `#19461 <https://github.com/numpy/numpy/pull/19461>`__: MAINT: Bump hypothesis from 6.14.1 to 6.14.2
* `#19462 <https://github.com/numpy/numpy/pull/19462>`__: BUILD: move to OpenBLAS 0.3.16
* `#19463 <https://github.com/numpy/numpy/pull/19463>`__: MAINT: Use straight arm64 in TravisCI.
* `#19468 <https://github.com/numpy/numpy/pull/19468>`__: MAINT: Add missing ``dtype`` overloads for object- and ctypes-based...
* `#19475 <https://github.com/numpy/numpy/pull/19475>`__: DOC: Fix see also references in ``numpy.resize``
* `#19478 <https://github.com/numpy/numpy/pull/19478>`__: ENH: Vectorizing umath module using AVX-512 (open sourced from...
* `#19479 <https://github.com/numpy/numpy/pull/19479>`__: BLD: Add clang ``-ftrapping-math`` also for ``compiler_so``
* `#19483 <https://github.com/numpy/numpy/pull/19483>`__: MAINT: Update for using ``openblas64_``.
* `#19485 <https://github.com/numpy/numpy/pull/19485>`__: TST/BENCH: Adding test coverage and benchmarks for floating point...
* `#19486 <https://github.com/numpy/numpy/pull/19486>`__: DOC: Add link to NumPy PDF docs
* `#19491 <https://github.com/numpy/numpy/pull/19491>`__: MAINT: Disable test_blas64_dot.
* `#19492 <https://github.com/numpy/numpy/pull/19492>`__: BUILD: update OpenBLAS to v0.3.17
* `#19493 <https://github.com/numpy/numpy/pull/19493>`__: TST: generalise ``clip`` test
* `#19498 <https://github.com/numpy/numpy/pull/19498>`__: MAINT: Update manylinux ci test to manylinux2014
* `#19506 <https://github.com/numpy/numpy/pull/19506>`__: DOC: Fix typos
* `#19512 <https://github.com/numpy/numpy/pull/19512>`__: REL: Update main after 1.21.1 release.
* `#19513 <https://github.com/numpy/numpy/pull/19513>`__: ENH: Add support for windows on arm targets
* `#19516 <https://github.com/numpy/numpy/pull/19516>`__: DOC: Created fundamentals doc for explanations in ``ufunc`` reference...
* `#19517 <https://github.com/numpy/numpy/pull/19517>`__: MAINT: Bump sphinx from 4.1.0 to 4.1.1
* `#19518 <https://github.com/numpy/numpy/pull/19518>`__: MAINT: Bump hypothesis from 6.14.2 to 6.14.3
* `#19519 <https://github.com/numpy/numpy/pull/19519>`__: MAINT: Bump cython from 0.29.23 to 0.29.24
* `#19525 <https://github.com/numpy/numpy/pull/19525>`__: TST: Test that ``numpy.typing`` can be imported in the absence...
* `#19526 <https://github.com/numpy/numpy/pull/19526>`__: MAINT: bump Sphinx in environment.yml file
* `#19527 <https://github.com/numpy/numpy/pull/19527>`__: BLD: Add LoongArch support
* `#19529 <https://github.com/numpy/numpy/pull/19529>`__: SIMD: Force inlining all functions that accept AVX registers
* `#19534 <https://github.com/numpy/numpy/pull/19534>`__: BLD: Tell fortran compiler Cygwin doesn't support rpath.
* `#19535 <https://github.com/numpy/numpy/pull/19535>`__: TST: Add Cygwin to the x86 feature tests.
* `#19538 <https://github.com/numpy/numpy/pull/19538>`__: DOC: Fix typo in PCG64
* `#19539 <https://github.com/numpy/numpy/pull/19539>`__: DEP: Remove deprecated numeric style dtype strings
* `#19540 <https://github.com/numpy/numpy/pull/19540>`__: MAINT: Update the ``np.finfo`` annotations
* `#19542 <https://github.com/numpy/numpy/pull/19542>`__: TST: Parametrize a few more tests.
* `#19543 <https://github.com/numpy/numpy/pull/19543>`__: MAINT: Improve the ``np.core.numerictypes`` stubs
* `#19545 <https://github.com/numpy/numpy/pull/19545>`__: DOC: Add clarification
* `#19546 <https://github.com/numpy/numpy/pull/19546>`__: DOC: Add link and explanation of ``_add_newdocs`` to developer...
* `#19547 <https://github.com/numpy/numpy/pull/19547>`__: BLD: Use cygpath utility for path conversion in cyg2win32
* `#19554 <https://github.com/numpy/numpy/pull/19554>`__: MAINT: add missing dunder method to nditer type hints
* `#19557 <https://github.com/numpy/numpy/pull/19557>`__: DOC: clarify doc re: unsupported keys in savez.
* `#19559 <https://github.com/numpy/numpy/pull/19559>`__: ENH: Add annotations for ``__path__`` and ``PytestTester``
* `#19560 <https://github.com/numpy/numpy/pull/19560>`__: TST: Bump the GitHub actions python 3.10 version
* `#19561 <https://github.com/numpy/numpy/pull/19561>`__: DOC: Remove explicit parameter sparse=False in meshgrid() indexing...
* `#19563 <https://github.com/numpy/numpy/pull/19563>`__: MAINT: Bump hypothesis from 6.14.3 to 6.14.4
* `#19564 <https://github.com/numpy/numpy/pull/19564>`__: TST: Add "Scaled float" custom DType for testng
* `#19565 <https://github.com/numpy/numpy/pull/19565>`__: DOC: Fix sphinx warnings in c-info.beyond-basics.rst
* `#19566 <https://github.com/numpy/numpy/pull/19566>`__: DOC: Remove ``dot`` docstring in numpy/core/_add_newdocs.py
* `#19567 <https://github.com/numpy/numpy/pull/19567>`__: DOC: Fix Unknown section warning when building docs
* `#19568 <https://github.com/numpy/numpy/pull/19568>`__: BUG: Seed random state in test_vonmises_large_kappa_range.
* `#19571 <https://github.com/numpy/numpy/pull/19571>`__: MAINT: Refactor UFunc core to use NEP 43 style dispatching
* `#19572 <https://github.com/numpy/numpy/pull/19572>`__: MAINT: Cleanup unused function _move_axis_to_0
* `#19576 <https://github.com/numpy/numpy/pull/19576>`__: MAINT: Make Python3.8 the default for CI testing.
* `#19578 <https://github.com/numpy/numpy/pull/19578>`__: TST: Add basic tests for custom DType (scaled float) ufuncs
* `#19580 <https://github.com/numpy/numpy/pull/19580>`__: ENH: Add basic promoter capability to ufunc dispatching
* `#19582 <https://github.com/numpy/numpy/pull/19582>`__: BLD: load extra flags when checking for libflame
* `#19587 <https://github.com/numpy/numpy/pull/19587>`__: MAINT: Refactor DType slots into an opaque, allocated struct
* `#19590 <https://github.com/numpy/numpy/pull/19590>`__: DOC Fix sphinx warnings related to scope of c:macro.
* `#19593 <https://github.com/numpy/numpy/pull/19593>`__: DOC,MAINT: Update wording surrounding ``fname`` parameter for loadtxt/genfromtxt
* `#19595 <https://github.com/numpy/numpy/pull/19595>`__: MAINT: Bump sphinx from 4.1.1 to 4.1.2
* `#19596 <https://github.com/numpy/numpy/pull/19596>`__: MAINT: Bump hypothesis from 6.14.4 to 6.14.5
* `#19598 <https://github.com/numpy/numpy/pull/19598>`__: PERF: Speed-up common case of loadtxt()ing non-hex floats.
* `#19599 <https://github.com/numpy/numpy/pull/19599>`__: PERF: Avoid using ``@recursive``.
* `#19600 <https://github.com/numpy/numpy/pull/19600>`__: BUG: Fix bad write in masked iterator output copy paths
* `#19601 <https://github.com/numpy/numpy/pull/19601>`__: PERF: Speedup comments handling in loadtxt.
* `#19605 <https://github.com/numpy/numpy/pull/19605>`__: DEV: Update default Python in benchmark config.
* `#19607 <https://github.com/numpy/numpy/pull/19607>`__: BUG: Fix NaT handling in the PyArray_CompareFunc for datetime...
* `#19608 <https://github.com/numpy/numpy/pull/19608>`__: PERF: Specialize loadtxt packer for uniform-dtype data.
* `#19609 <https://github.com/numpy/numpy/pull/19609>`__: PERF: In loadtxt, decide once and for all whether decoding is...
* `#19610 <https://github.com/numpy/numpy/pull/19610>`__: PERF: Special-case single-converter in loadtxt.
* `#19612 <https://github.com/numpy/numpy/pull/19612>`__: TST: Bump the python 3.10 test version from beta4 to rc1
* `#19613 <https://github.com/numpy/numpy/pull/19613>`__: DOC: isclose accepts boolean input
* `#19615 <https://github.com/numpy/numpy/pull/19615>`__: MAINT: Proposal to expire three deprecated functions in numpy.lib.npyio
* `#19616 <https://github.com/numpy/numpy/pull/19616>`__: MAINT: In loadtxt, refactor detection of the number of columns.
* `#19618 <https://github.com/numpy/numpy/pull/19618>`__: MAINT: Optimize loadtxt usecols.
* `#19619 <https://github.com/numpy/numpy/pull/19619>`__: MAINT: Include .coveragerc in source distributions to support...
* `#19620 <https://github.com/numpy/numpy/pull/19620>`__: PERF: Simplify some of loadtxt's standard converters.
* `#19621 <https://github.com/numpy/numpy/pull/19621>`__: BUG: The normal cast-safety for ufunc loops is "no" casting
* `#19622 <https://github.com/numpy/numpy/pull/19622>`__: MAINT: Skip a type check in loadtxt when using user converters.
* `#19627 <https://github.com/numpy/numpy/pull/19627>`__: BUG: Ignore whitespaces while parsing gufunc signatures
* `#19628 <https://github.com/numpy/numpy/pull/19628>`__: TST: avoid distutils.sysconfig in runtests.py
* `#19632 <https://github.com/numpy/numpy/pull/19632>`__: BUG,DEP: Non-default UFunc signature/dtype usage should be deprecated
* `#19633 <https://github.com/numpy/numpy/pull/19633>`__: MAINT: Bump hypothesis from 6.14.5 to 6.14.6
* `#19638 <https://github.com/numpy/numpy/pull/19638>`__: MAINT: Remove import time compile
* `#19639 <https://github.com/numpy/numpy/pull/19639>`__: MAINT: Update Cython version for Python 3.10.
* `#19640 <https://github.com/numpy/numpy/pull/19640>`__: BUG: Remove logical object ufuncs with bool output
* `#19642 <https://github.com/numpy/numpy/pull/19642>`__: BLD, SIMD: Fix testing extra checks when ``-Werror`` isn't applicable...
* `#19645 <https://github.com/numpy/numpy/pull/19645>`__: DOC: Reorganized the documentation contribution docs
* `#19654 <https://github.com/numpy/numpy/pull/19654>`__: BUG: add base to templated arguments for platlib
* `#19663 <https://github.com/numpy/numpy/pull/19663>`__: NEP: add qualifier for free(), mention ContextVar
* `#19665 <https://github.com/numpy/numpy/pull/19665>`__: MAINT: Drop Python3.7 from supported versions.
* `#19667 <https://github.com/numpy/numpy/pull/19667>`__: ENH: Add annotations for ``np.lib.npyio``
* `#19672 <https://github.com/numpy/numpy/pull/19672>`__: BLD: update circleCI docker image
* `#19678 <https://github.com/numpy/numpy/pull/19678>`__: REL: Update main after 1.21.2 release.
* `#19680 <https://github.com/numpy/numpy/pull/19680>`__: ENH: Allow ``np.fromregex`` to accept ``os.PathLike`` implementations
* `#19681 <https://github.com/numpy/numpy/pull/19681>`__: MAINT: Update wheel requirement from <0.36.3 to <0.37.1
* `#19682 <https://github.com/numpy/numpy/pull/19682>`__: MAINT: Bump hypothesis from 6.14.6 to 6.14.7
* `#19683 <https://github.com/numpy/numpy/pull/19683>`__: ENH: Add annotations for ``np.lib.stride_tricks``
* `#19686 <https://github.com/numpy/numpy/pull/19686>`__: ENH: Add spaces after punctuation in dtype repr/str.
* `#19692 <https://github.com/numpy/numpy/pull/19692>`__: DOC: Fix trivial doc typo.
* `#19693 <https://github.com/numpy/numpy/pull/19693>`__: MAINT: In loadtxt, inline read_data.
* `#19695 <https://github.com/numpy/numpy/pull/19695>`__: DOC: Fix typo in ``unwrap`` docstring.
* `#19698 <https://github.com/numpy/numpy/pull/19698>`__: DOC: fix typo in example
* `#19702 <https://github.com/numpy/numpy/pull/19702>`__: MAINT: Replace deprecated unittest aliases
* `#19713 <https://github.com/numpy/numpy/pull/19713>`__: MAINT: Replace numpy custom generation engine by raw C++
* `#19714 <https://github.com/numpy/numpy/pull/19714>`__: MAINT: Remove redundant Python2 float/int conversions
* `#19715 <https://github.com/numpy/numpy/pull/19715>`__: BUG: Casting ``bool_`` to float16
* `#19725 <https://github.com/numpy/numpy/pull/19725>`__: MAINT: Use a contextmanager to ensure loadtxt closes the input...
* `#19727 <https://github.com/numpy/numpy/pull/19727>`__: DOC: fix basics.creation.rst to address issue 19726
* `#19730 <https://github.com/numpy/numpy/pull/19730>`__: BUG: Fix reference leak of capi_tmp in f2py/cb_rules.py
* `#19731 <https://github.com/numpy/numpy/pull/19731>`__: BUG: fix time cast-safety for ``factor*unit`` e.g. in ``10**6*ms``...
* `#19732 <https://github.com/numpy/numpy/pull/19732>`__: MAINT: Spelling fixes in documentation
* `#19733 <https://github.com/numpy/numpy/pull/19733>`__: DOC: add citation file for GitHub support
* `#19736 <https://github.com/numpy/numpy/pull/19736>`__: BUG: Fix passing a MaskedArray instance to ``MaskedArray.__setitem__``
* `#19738 <https://github.com/numpy/numpy/pull/19738>`__: MAINT: Bump hypothesis from 6.14.7 to 6.15.0
* `#19739 <https://github.com/numpy/numpy/pull/19739>`__: NEP: Update NEP 47: Adopting the array API standard
* `#19742 <https://github.com/numpy/numpy/pull/19742>`__: MAINT: Remove redundant test.
* `#19743 <https://github.com/numpy/numpy/pull/19743>`__: MAINT: Avoid use of confusing compat aliases.
* `#19747 <https://github.com/numpy/numpy/pull/19747>`__: MAINT: Update README.md with badges
* `#19754 <https://github.com/numpy/numpy/pull/19754>`__: ENH: Add clang-format file
* `#19758 <https://github.com/numpy/numpy/pull/19758>`__: MAINT: Remove redundant semicolon
* `#19764 <https://github.com/numpy/numpy/pull/19764>`__: BUG: np.around fails when using doctest
* `#19766 <https://github.com/numpy/numpy/pull/19766>`__: BUG: Remove np.around's footnote [2]
* `#19775 <https://github.com/numpy/numpy/pull/19775>`__: MAINT,DOC: Readability improvements and cleanup for f2py
* `#19776 <https://github.com/numpy/numpy/pull/19776>`__: DOC: Add explanation of a sparse mesh grid
* `#19781 <https://github.com/numpy/numpy/pull/19781>`__: MAINT: refactor "for ... in range(len(" statements
* `#19784 <https://github.com/numpy/numpy/pull/19784>`__: MAINT: Remove typing code-paths specific to Python 3.7
* `#19789 <https://github.com/numpy/numpy/pull/19789>`__: MAINT: Bump hypothesis from 6.15.0 to 6.17.3
* `#19791 <https://github.com/numpy/numpy/pull/19791>`__: DOC: Created an explanation document for copies and views
* `#19799 <https://github.com/numpy/numpy/pull/19799>`__: TST: Drop typing-extensions from test_requirements.txt
* `#19800 <https://github.com/numpy/numpy/pull/19800>`__: ENH: Add entry point for Array API implementation
* `#19802 <https://github.com/numpy/numpy/pull/19802>`__: STY: Use the new PEP 457 positional-only syntax for typing
* `#19803 <https://github.com/numpy/numpy/pull/19803>`__: ENH: Add ``is_integer`` to ``np.floating`` & ``np.integer``
* `#19805 <https://github.com/numpy/numpy/pull/19805>`__: ENH: Symbolic solver for dimension specifications.
* `#19809 <https://github.com/numpy/numpy/pull/19809>`__: MAINT: Fix compiler warnings generated by convert_datatype.h.
* `#19810 <https://github.com/numpy/numpy/pull/19810>`__: MAINT: Minor include rationalizations.
* `#19811 <https://github.com/numpy/numpy/pull/19811>`__: DEP: Deprecate quote_args (from numpy.distutils.misc_util)
* `#19813 <https://github.com/numpy/numpy/pull/19813>`__: DOC: Fix import of default_rng
* `#19814 <https://github.com/numpy/numpy/pull/19814>`__: ENH: Replaced markdown issue templates with issue forms
* `#19815 <https://github.com/numpy/numpy/pull/19815>`__: MAINT: revise OSError aliases (IOError, EnvironmentError)
* `#19817 <https://github.com/numpy/numpy/pull/19817>`__: ENH: Use custom file-like protocols instead of ``typing.IO``
* `#19818 <https://github.com/numpy/numpy/pull/19818>`__: MAINT: fix unhashable instance and potential exception identified...
* `#19819 <https://github.com/numpy/numpy/pull/19819>`__: MAINT: mark _version.py as generated
* `#19821 <https://github.com/numpy/numpy/pull/19821>`__: BUG: Fixed an issue wherein certain ``nan<x>`` functions could...
* `#19824 <https://github.com/numpy/numpy/pull/19824>`__: MAINT: Small cleanups of includes in *.c files.
* `#19826 <https://github.com/numpy/numpy/pull/19826>`__: MAINT: Standardize guards in numpy/core/include
* `#19827 <https://github.com/numpy/numpy/pull/19827>`__: MAINT: Standardize guards in numpy/core/src/common.
* `#19829 <https://github.com/numpy/numpy/pull/19829>`__: MAINT: Standardize guards in numpy/core/src/multiarray.
* `#19837 <https://github.com/numpy/numpy/pull/19837>`__: MAINT: Bump hypothesis from 6.17.3 to 6.18.0
* `#19838 <https://github.com/numpy/numpy/pull/19838>`__: MAINT: Bump pytest from 6.2.4 to 6.2.5
* `#19843 <https://github.com/numpy/numpy/pull/19843>`__: TST: Fix/Improve cast nonstandard bool to numeric test
* `#19844 <https://github.com/numpy/numpy/pull/19844>`__: DOC: Added missing C-API functions
* `#19845 <https://github.com/numpy/numpy/pull/19845>`__: TST: Make nanfunc test ignore overflow instead of xfailing test
* `#19846 <https://github.com/numpy/numpy/pull/19846>`__: MAINT: Update testing to 3.10rc2
* `#19849 <https://github.com/numpy/numpy/pull/19849>`__: DOC: Fix sentence casing in page titles
* `#19850 <https://github.com/numpy/numpy/pull/19850>`__: Replace posix specific ssize_t with py_ssize_t to compile on...
* `#19854 <https://github.com/numpy/numpy/pull/19854>`__: BUG: Fixed an issue wherein ``var`` would raise for 0d object arrays
* `#19856 <https://github.com/numpy/numpy/pull/19856>`__: MAINT: Mark type-check-only ufunc subclasses as ufunc aliases...
* `#19857 <https://github.com/numpy/numpy/pull/19857>`__: MAINT, ENH: Refactor percentile and quantile methods
* `#19862 <https://github.com/numpy/numpy/pull/19862>`__: DOC: Add BRANCH_WALKTHROUGH
* `#19863 <https://github.com/numpy/numpy/pull/19863>`__: BUG: Fix ``nanpercentile`` ignoring the dtype of all-nan arrays
* `#19864 <https://github.com/numpy/numpy/pull/19864>`__: DOC: Update RELEASE_WALKTHROUGH
* `#19865 <https://github.com/numpy/numpy/pull/19865>`__: DOC: Moved NumPy Internals to Under-the-hood documentation for...
* `#19867 <https://github.com/numpy/numpy/pull/19867>`__: MAINT: Bump hypothesis from 6.18.0 to 6.21.1
* `#19868 <https://github.com/numpy/numpy/pull/19868>`__: MAINT: Bump sphinx from 4.1.2 to 4.2.0
* `#19869 <https://github.com/numpy/numpy/pull/19869>`__: BUG: ensure np.median does not drop subclass for NaN result.
* `#19870 <https://github.com/numpy/numpy/pull/19870>`__: DOC: Small fixups for the release walkthrough
* `#19874 <https://github.com/numpy/numpy/pull/19874>`__: DOC: Fix typo in upcoming changes filename
* `#19879 <https://github.com/numpy/numpy/pull/19879>`__: ENH: Add ``__class_getitem__`` to ``ndarray``, ``dtype`` and ``number``
* `#19882 <https://github.com/numpy/numpy/pull/19882>`__: MAINT: Use SHA-256 instead of SHA-1
* `#19883 <https://github.com/numpy/numpy/pull/19883>`__: DOC: Fix the reported module names of objects in the ``numpy.typing``...
* `#19884 <https://github.com/numpy/numpy/pull/19884>`__: TST: Make this sysconfig handling a bit more portable
* `#19887 <https://github.com/numpy/numpy/pull/19887>`__: ENH: Add annotations for ``np.linalg``
* `#19888 <https://github.com/numpy/numpy/pull/19888>`__: BUG: core: Fix *_like strides for str and bytes dtype.
* `#19890 <https://github.com/numpy/numpy/pull/19890>`__: DOC: Added hyperlink on numpy logo in README.md
* `#19893 <https://github.com/numpy/numpy/pull/19893>`__: MAINT,DOC: f2py restructring
* `#19894 <https://github.com/numpy/numpy/pull/19894>`__: ENH: Add a typing protocol for representing nested sequences
* `#19899 <https://github.com/numpy/numpy/pull/19899>`__: DOC: replace return type in np.ma.* docstring
* `#19900 <https://github.com/numpy/numpy/pull/19900>`__: DOC:Fixed refguide errors for basics.creation.rst
* `#19902 <https://github.com/numpy/numpy/pull/19902>`__: BUG,DOC: Ignore upcoming_changes from refguide
* `#19903 <https://github.com/numpy/numpy/pull/19903>`__: DOC: Fixed refguide errors for basics.broadcasting.rst
* `#19905 <https://github.com/numpy/numpy/pull/19905>`__: DOC: fix docstring formatting of polynomial fit method return...
* `#19907 <https://github.com/numpy/numpy/pull/19907>`__: MAINT: Bump hypothesis from 6.21.1 to 6.21.6
* `#19908 <https://github.com/numpy/numpy/pull/19908>`__: BUG: Check whether an error is already set for invalid casting
* `#19909 <https://github.com/numpy/numpy/pull/19909>`__: MAINT: Re-export ``LinAlgError`` to the ``np.linalg.linalg`` stubs
* `#19911 <https://github.com/numpy/numpy/pull/19911>`__: DOC: Typos found by codespell
* `#19913 <https://github.com/numpy/numpy/pull/19913>`__: MAINT: Fix LGTM.com error: Unmatchable caret in regular expression
* `#19914 <https://github.com/numpy/numpy/pull/19914>`__: MAINT: Fix LGTM.com warning in nditer_imp.h
* `#19915 <https://github.com/numpy/numpy/pull/19915>`__: ENH: Add annotations for ``np.char``
* `#19916 <https://github.com/numpy/numpy/pull/19916>`__: MAINT: Repair ``make_lite.py``
* `#19917 <https://github.com/numpy/numpy/pull/19917>`__: ENH: Add annotations for ``np.lib.arraysetops``
* `#19918 <https://github.com/numpy/numpy/pull/19918>`__: MAINT: Override the modules of ``np.char`` and ``np.rec`` functions
* `#19919 <https://github.com/numpy/numpy/pull/19919>`__: ENH: Create an experimental export of the new DType API
* `#19920 <https://github.com/numpy/numpy/pull/19920>`__: DOC: Fix typos in NEPs, found by codespell
* `#19921 <https://github.com/numpy/numpy/pull/19921>`__: DEP: Use ``delimiter`` rather than ``delimitor`` as kwarg in mrecords
* `#19925 <https://github.com/numpy/numpy/pull/19925>`__: BUG: ufunc: Fix potential memory leak.
* `#19926 <https://github.com/numpy/numpy/pull/19926>`__: BUG: Resolve Divide by Zero on Apple silicon + test failures
* `#19927 <https://github.com/numpy/numpy/pull/19927>`__: BUG: Only call the get_versions() function once on import
* `#19928 <https://github.com/numpy/numpy/pull/19928>`__: MAINT: lib: Check that the dtype given to fromregex is structured.
* `#19929 <https://github.com/numpy/numpy/pull/19929>`__: duplicate item in see also.
* `#19933 <https://github.com/numpy/numpy/pull/19933>`__: MAINT: random: Use expm1 where appropriate.
* `#19934 <https://github.com/numpy/numpy/pull/19934>`__: BUG: core: Fix memory leak in the C function boundarraymethod_repr.
* `#19936 <https://github.com/numpy/numpy/pull/19936>`__: BUG: Make sure __version__ is defined in setup mode
* `#19937 <https://github.com/numpy/numpy/pull/19937>`__: ENH: Updates to numpy.array_api
* `#19939 <https://github.com/numpy/numpy/pull/19939>`__: MAINT: Fix LGTM.com warning: Constant in conditional expression...
* `#19940 <https://github.com/numpy/numpy/pull/19940>`__: MAINT: Fix LGTM.com warning: Variable ``isrec`` defined multiple...
* `#19942 <https://github.com/numpy/numpy/pull/19942>`__: MAINT: Fix LGTM.com warning: Unreachable code
* `#19943 <https://github.com/numpy/numpy/pull/19943>`__: MAINT: Fix LGTM.com warning: Variable ``f`` defined multiple times
* `#19944 <https://github.com/numpy/numpy/pull/19944>`__: MAINT: Fix LGTM.com warning: Comparison result is always the...
* `#19946 <https://github.com/numpy/numpy/pull/19946>`__: MAINT: Fix LGTM.com warning: Comparison result is always the...
* `#19948 <https://github.com/numpy/numpy/pull/19948>`__: MAINT: Add annotations for three missing ``ndarray`` methods
* `#19949 <https://github.com/numpy/numpy/pull/19949>`__: ENH: Add annotations for ``np.rec``
* `#19951 <https://github.com/numpy/numpy/pull/19951>`__: MAINT: Fix LGTM.com warning: Comparison is always false because...
* `#19953 <https://github.com/numpy/numpy/pull/19953>`__: ENH: Add annotations to ``np.core.multiarray`` part 4/4
* `#19957 <https://github.com/numpy/numpy/pull/19957>`__: DOC: Add syntax highlighting, update pronouns
* `#19960 <https://github.com/numpy/numpy/pull/19960>`__: DOC: Minor syntax fix for numpydoc warnings
* `#19961 <https://github.com/numpy/numpy/pull/19961>`__: MAINT: Minor cleanups after merging gh-19805
* `#19962 <https://github.com/numpy/numpy/pull/19962>`__: DOC: Remove overstated TDD evangelism.
* `#19963 <https://github.com/numpy/numpy/pull/19963>`__: DOC: rename ``np.lib.scimath`` to ``np.emath``
* `#19965 <https://github.com/numpy/numpy/pull/19965>`__: MAINT: Update funding link in FUNDING.yml
* `#19967 <https://github.com/numpy/numpy/pull/19967>`__: DOC: Update basics.io.genfromtxt.rst
* `#19968 <https://github.com/numpy/numpy/pull/19968>`__: ENH: nagfor from NAG is available on Darwin
* `#19969 <https://github.com/numpy/numpy/pull/19969>`__: MAINT: Misc ``np.array_api`` annotation fixes
* `#19972 <https://github.com/numpy/numpy/pull/19972>`__: MAINT: Bump hypothesis from 6.21.6 to 6.23.0
* `#19974 <https://github.com/numpy/numpy/pull/19974>`__: BUG: np.tan(np.inf) test failure in Apple silicon
* `#19976 <https://github.com/numpy/numpy/pull/19976>`__: DOC Remove reference to ``PyArray_MultiIter_SIZE``
* `#19977 <https://github.com/numpy/numpy/pull/19977>`__: MAINT: clang-format for f2py
* `#19978 <https://github.com/numpy/numpy/pull/19978>`__: MAINT: Reduce DepreciationWarnings, use more data API types for...
* `#19979 <https://github.com/numpy/numpy/pull/19979>`__: ENH: Add annotations for ``np.memmap``
* `#19980 <https://github.com/numpy/numpy/pull/19980>`__: ENH: Add the linalg extension to the array_api submodule
* `#19981 <https://github.com/numpy/numpy/pull/19981>`__: DOC: Deindent some sphinx declarations to avoid warnings.
* `#19983 <https://github.com/numpy/numpy/pull/19983>`__: DOC: Specifically mention the C99 requirement in 'Building from...
* `#19984 <https://github.com/numpy/numpy/pull/19984>`__: MAINT: Configure pytest to ignore array_api warnings.
* `#19986 <https://github.com/numpy/numpy/pull/19986>`__: MAINT: Fix LGTM.com warning: Comparison result is always the...
* `#19987 <https://github.com/numpy/numpy/pull/19987>`__: BUG: Remove double cast to char in favor of PyArray_BYTES
* `#19988 <https://github.com/numpy/numpy/pull/19988>`__: DOC: Update links to online copy of Abramowitz and Stegun.
* `#19992 <https://github.com/numpy/numpy/pull/19992>`__: ENH: nagfor - get_flags_linker_so() on darwin
* `#19995 <https://github.com/numpy/numpy/pull/19995>`__: DOC: for new_order parameter, add alias for 'native' order
* `#19997 <https://github.com/numpy/numpy/pull/19997>`__: STY: Harmonize rules with cb_rules for f2py
* `#19999 <https://github.com/numpy/numpy/pull/19999>`__: DOC: Copy-edit and fix typos.
* `#20000 <https://github.com/numpy/numpy/pull/20000>`__: BUG,DEP: Allow (arg-)partition to accept ``uint64`` indices
* `#20002 <https://github.com/numpy/numpy/pull/20002>`__: MAINT: Introduce various linting and misc fixes to ``numpy.typing``
* `#20003 <https://github.com/numpy/numpy/pull/20003>`__: BLD: updated mypy version from 0.902 to 0.910
* `#20004 <https://github.com/numpy/numpy/pull/20004>`__: DOC: Fix typos in the random and f2py documentation.
* `#20006 <https://github.com/numpy/numpy/pull/20006>`__: ENH: Add annotations for ``np.lib.function_base`` part 1
* `#20007 <https://github.com/numpy/numpy/pull/20007>`__: MAINT: Removed the ``cdoc`` directory
* `#20008 <https://github.com/numpy/numpy/pull/20008>`__: BUG: Fix the ``lib.function_base`` window functions ignoring extended...
* `#20010 <https://github.com/numpy/numpy/pull/20010>`__: MAINT: correct linker flags for NAG Fortran compiler
* `#20015 <https://github.com/numpy/numpy/pull/20015>`__: DOC: np.select: use an example that also shows default value
* `#20016 <https://github.com/numpy/numpy/pull/20016>`__: BUG: Add a warning for user dtypes modifying casts after use
* `#20018 <https://github.com/numpy/numpy/pull/20018>`__: ENH: core: More informative error message for broadcast(*args)
* `#20019 <https://github.com/numpy/numpy/pull/20019>`__: MAINT:redundant 'else' statement with 'for' loop#19077
* `#20026 <https://github.com/numpy/numpy/pull/20026>`__: MAINT: Test PyPy3.8
* `#20027 <https://github.com/numpy/numpy/pull/20027>`__: ENH: Add missing parameters to the ``nan<x>`` functions
* `#20029 <https://github.com/numpy/numpy/pull/20029>`__: MAINT: Bump pytz from 2021.1 to 2021.3
* `#20031 <https://github.com/numpy/numpy/pull/20031>`__: MAINT: Bump hypothesis from 6.23.0 to 6.23.1
* `#20032 <https://github.com/numpy/numpy/pull/20032>`__: MAINT: Bump pytest-cov from 2.12.1 to 3.0.0
* `#20034 <https://github.com/numpy/numpy/pull/20034>`__: ENH: Add annotations for ``np.lib.function_base`` part 2/3
* `#20036 <https://github.com/numpy/numpy/pull/20036>`__: ENH: Add annotations for ``np.lib.function_base`` part 3/3
* `#20037 <https://github.com/numpy/numpy/pull/20037>`__: MAINT: Fixed an issue wherein ``npt._NestedSequence`` was not a...
* `#20040 <https://github.com/numpy/numpy/pull/20040>`__: TST: Add python 3.10 to the CI
* `#20047 <https://github.com/numpy/numpy/pull/20047>`__: DOC:add an example to show flag writeable cleared upon copy related...
* `#20049 <https://github.com/numpy/numpy/pull/20049>`__: BUG: Correct advance in PCG with emulated int128
* `#20051 <https://github.com/numpy/numpy/pull/20051>`__: DOC:add-html-reference-to-some-ma-methods
* `#20057 <https://github.com/numpy/numpy/pull/20057>`__: MAINT: LGTM.com warnings
* `#20058 <https://github.com/numpy/numpy/pull/20058>`__: MAINT: update OpenBLAS to 0.3.18
* `#20059 <https://github.com/numpy/numpy/pull/20059>`__: MAINT: LGTM.com recommendations
* `#20060 <https://github.com/numpy/numpy/pull/20060>`__: MAINT: Remove encoding declarations: ``# -*- coding: utf-8 -*-``
* `#20061 <https://github.com/numpy/numpy/pull/20061>`__: DOC: Remove references to Python 2
* `#20063 <https://github.com/numpy/numpy/pull/20063>`__: ENH: Add annotations for ``np.lib.histograms``
* `#20065 <https://github.com/numpy/numpy/pull/20065>`__: ENH: Add annotations for ``np.lib.polynomial``
* `#20066 <https://github.com/numpy/numpy/pull/20066>`__: MAINT: A few updates to the array_api
* `#20067 <https://github.com/numpy/numpy/pull/20067>`__: MAINT: Use ``Py_SET_TYPE`` macro instead of assigning to ``Py_TYPE``
* `#20069 <https://github.com/numpy/numpy/pull/20069>`__: BUG: Add workaround for missing ufunc error propagation
* `#20071 <https://github.com/numpy/numpy/pull/20071>`__: MAINT: Remove unused imports and remove duplicated tests
* `#20076 <https://github.com/numpy/numpy/pull/20076>`__: DOC: Document the dtype comparison operations
* `#20084 <https://github.com/numpy/numpy/pull/20084>`__: MAINT: move "git submodule update" earlier in docker creation
* `#20087 <https://github.com/numpy/numpy/pull/20087>`__: BLD: fix submodule update in gitpod.Dockerfile
* `#20088 <https://github.com/numpy/numpy/pull/20088>`__: BUG: core: result_type(0, np.timedelta64(4)) would seg. fault.
* `#20091 <https://github.com/numpy/numpy/pull/20091>`__: DOC: fix typo in docstring of bitwise_or
* `#20094 <https://github.com/numpy/numpy/pull/20094>`__: BUG: AVX-512F log() overflows
* `#20096 <https://github.com/numpy/numpy/pull/20096>`__: MAINT: Bump hypothesis from 6.23.1 to 6.23.2
* `#20097 <https://github.com/numpy/numpy/pull/20097>`__: MAINT: Bump pycodestyle from 2.7.0 to 2.8.0
* `#20102 <https://github.com/numpy/numpy/pull/20102>`__: BLD Uses cibuildwheel for linux + osx wheels [cd build]
* `#20104 <https://github.com/numpy/numpy/pull/20104>`__: MAINT: Update F2PY documentation URL
* `#20105 <https://github.com/numpy/numpy/pull/20105>`__: ENH: Add annotations for ``np.matrix``
* `#20111 <https://github.com/numpy/numpy/pull/20111>`__: DOC: fix minor typo in comment
* `#20115 <https://github.com/numpy/numpy/pull/20115>`__: DOC: Modify code in absolute beginners tutorial to match image
* `#20116 <https://github.com/numpy/numpy/pull/20116>`__: MAINT: Fix issue with C compiler args containing spaces
* `#20119 <https://github.com/numpy/numpy/pull/20119>`__: DOC: Remove double property ctypes from ndarray
* `#20123 <https://github.com/numpy/numpy/pull/20123>`__: DOC: Add note to iterable docstring about 0d arrays.
* `#20129 <https://github.com/numpy/numpy/pull/20129>`__: ENH: Misc typing improvements to ``np.array_api``
* `#20130 <https://github.com/numpy/numpy/pull/20130>`__: MAINT: Bump hypothesis from 6.23.2 to 6.23.3
* `#20134 <https://github.com/numpy/numpy/pull/20134>`__: BUG: fix win32 np.clip slowness
* `#20136 <https://github.com/numpy/numpy/pull/20136>`__: BUG: core: Fix incorrect check of NpyIter_Deallocate return value.
* `#20137 <https://github.com/numpy/numpy/pull/20137>`__: DOC:Reword array has one axis
* `#20139 <https://github.com/numpy/numpy/pull/20139>`__: MAINT,BUG: Fix ``ufunc.at`` to use new ufunc API
* `#20142 <https://github.com/numpy/numpy/pull/20142>`__: MAINT: core: Update the comment about _parse_signature with more...
* `#20146 <https://github.com/numpy/numpy/pull/20146>`__: DOC: Updated docstring for floating point rounding
* `#20149 <https://github.com/numpy/numpy/pull/20149>`__: REL: Update main after 1.21.3 release.
* `#20150 <https://github.com/numpy/numpy/pull/20150>`__: BUG: lib: Fix error raised by insert.
* `#20153 <https://github.com/numpy/numpy/pull/20153>`__: BUG, SIMD: Fix 64-bit/8-bit integer division by a scalar
* `#20154 <https://github.com/numpy/numpy/pull/20154>`__: MAINT: Add breathe to environment.yml
* `#20155 <https://github.com/numpy/numpy/pull/20155>`__: BUG: Distutils patch to allow for 2 as a minor version (!)
* `#20156 <https://github.com/numpy/numpy/pull/20156>`__: DOC: Fixed docstring for parameters 2 and -2 on linalg.cond
* `#20159 <https://github.com/numpy/numpy/pull/20159>`__: BUG: Relax homogeneous signature fallback in type resolution
* `#20162 <https://github.com/numpy/numpy/pull/20162>`__: BUG: fixes for MSVC version checks
* `#20163 <https://github.com/numpy/numpy/pull/20163>`__: ENH: Expose promoters and Common-DType API experimentally
* `#20164 <https://github.com/numpy/numpy/pull/20164>`__: MAINT: Remove useless custom tp_alloc and tp_free on ndarray
* `#20165 <https://github.com/numpy/numpy/pull/20165>`__: ENH: Add annotations for ``np.chararray``
* `#20166 <https://github.com/numpy/numpy/pull/20166>`__: MAINT, STY: Run clang-format on cpp files and headers.
* `#20170 <https://github.com/numpy/numpy/pull/20170>`__: More informative error for unparsable version
* `#20172 <https://github.com/numpy/numpy/pull/20172>`__: Allow clib callable build flags
* `#20173 <https://github.com/numpy/numpy/pull/20173>`__: TST: Disable test_partial_iteration_cleanup on 32 bit Windows.
* `#20174 <https://github.com/numpy/numpy/pull/20174>`__: TST: xfail ``test_overrides`` when numpy is built with MKL support
* `#20179 <https://github.com/numpy/numpy/pull/20179>`__: BUG: Do not use nonzero fastpath on unaligned arrays
* `#20182 <https://github.com/numpy/numpy/pull/20182>`__: DOC, MAINT: Update build systems for f2py
* `#20183 <https://github.com/numpy/numpy/pull/20183>`__: Thin compatibility layer for C/C++ math header
* `#20184 <https://github.com/numpy/numpy/pull/20184>`__: MAINT: Miscellaneous typing cleanups
* `#20187 <https://github.com/numpy/numpy/pull/20187>`__: BUG,DOC: Resolve a refguide failure for ``ndarray.__class_getitem__``
* `#20188 <https://github.com/numpy/numpy/pull/20188>`__: MAINT: Bump hypothesis from 6.23.3 to 6.24.0
* `#20190 <https://github.com/numpy/numpy/pull/20190>`__: BUG: Don't pass /arch:SSE2 to MSVC when targeting x64
* `#20194 <https://github.com/numpy/numpy/pull/20194>`__: DOC: add release note and move NEP 49 to Final
* `#20195 <https://github.com/numpy/numpy/pull/20195>`__: DOC: Two small changes in array.rst:
* `#20196 <https://github.com/numpy/numpy/pull/20196>`__: Fix minor grammar issues in docs
* `#20197 <https://github.com/numpy/numpy/pull/20197>`__: DOC, MAINT : fixing typo in numpy doc
* `#20199 <https://github.com/numpy/numpy/pull/20199>`__: ENH: Add dtype typing support to ``np.core.numeric``
* `#20200 <https://github.com/numpy/numpy/pull/20200>`__: MAINT: Only warn for transferred ownership if env variable is...
* `#20201 <https://github.com/numpy/numpy/pull/20201>`__: DEP: Deprecate ``np.MachAr``
* `#20205 <https://github.com/numpy/numpy/pull/20205>`__: BUG,DOC: Fix ``random.power``'s error description
* `#20206 <https://github.com/numpy/numpy/pull/20206>`__: CI: Add new workflow/action for testing universal intrinsics...
* `#20207 <https://github.com/numpy/numpy/pull/20207>`__: ENH: Add prompt for title in issue forms
* `#20213 <https://github.com/numpy/numpy/pull/20213>`__: DOC: Mention ``nan`` results in ``power`` and ``float_power``.
* `#20214 <https://github.com/numpy/numpy/pull/20214>`__: BUG: fix test c-extension compilation inside a venv
* `#20217 <https://github.com/numpy/numpy/pull/20217>`__: DOC: Add a release note for fully annotating the main numpy namespace
* `#20219 <https://github.com/numpy/numpy/pull/20219>`__: BUG, SIMD: Workaround broadcasting SIMD 64-bit integers on MSVC...
* `#20222 <https://github.com/numpy/numpy/pull/20222>`__: Run rebase on Cygwin CI
* `#20224 <https://github.com/numpy/numpy/pull/20224>`__: BUG: Fix shadowed reference of ``dtype`` in type stubs
* `#20228 <https://github.com/numpy/numpy/pull/20228>`__: MAINT: Better error message from histogram2d
* `#20230 <https://github.com/numpy/numpy/pull/20230>`__: ENH: Add annotations for ``np.ctypeslib``
* `#20232 <https://github.com/numpy/numpy/pull/20232>`__: CI: Add new workflow for Intel SDE
* `#20234 <https://github.com/numpy/numpy/pull/20234>`__: MAINT: Update vs2017 to vs2019.
* `#20235 <https://github.com/numpy/numpy/pull/20235>`__: DOC: fix typo in example, put the return statement inside the...
* `#20237 <https://github.com/numpy/numpy/pull/20237>`__: BUG: ``VOID_nonzero`` could sometimes mutate alignment flag
* `#20238 <https://github.com/numpy/numpy/pull/20238>`__: BUG: Fix environment checking logic for ``NUMPY_WARN_IF_NO_MEM_POLICY``
* `#20242 <https://github.com/numpy/numpy/pull/20242>`__: DOC: centralized min-max documentation
* `#20243 <https://github.com/numpy/numpy/pull/20243>`__: DOC: Fixes wording for fmod and remainder functions.
* `#20255 <https://github.com/numpy/numpy/pull/20255>`__: DOC: fix missing link in "What is NumPy?" to broadcasting
* `#20256 <https://github.com/numpy/numpy/pull/20256>`__: The module name in the reshape section of the absolute_beginners.html...
* `#20261 <https://github.com/numpy/numpy/pull/20261>`__: [DOC] Fix math block in hermmulx, lagmulx
* `#20267 <https://github.com/numpy/numpy/pull/20267>`__: Adding Examples to numpy.roll()
* `#20268 <https://github.com/numpy/numpy/pull/20268>`__: MAINT: remove Dependabot
* `#20269 <https://github.com/numpy/numpy/pull/20269>`__: MAINT: Bump hypothesis from 6.24.0 to 6.24.1
* `#20270 <https://github.com/numpy/numpy/pull/20270>`__: BUG: Fix headers for universal2 builds
* `#20271 <https://github.com/numpy/numpy/pull/20271>`__: TST: Add a test for device property
* `#20274 <https://github.com/numpy/numpy/pull/20274>`__: TST: Some fixes & refactoring around glibc-dependent skips in...
* `#20279 <https://github.com/numpy/numpy/pull/20279>`__: ENH: Add annotations for ``np.fft``
* `#20281 <https://github.com/numpy/numpy/pull/20281>`__: DOC: Correct grammar in isfinite docstring
* `#20282 <https://github.com/numpy/numpy/pull/20282>`__: MAINT: Fix runtests.py overriding $PYTHONPATH environment variable
* `#20283 <https://github.com/numpy/numpy/pull/20283>`__: MAINT Fix typo for event name in wheels.yml
* `#20284 <https://github.com/numpy/numpy/pull/20284>`__: BUG: Fix duplication of names in 'numpy.__all__'.
* `#20287 <https://github.com/numpy/numpy/pull/20287>`__: TST, MAINT: module name excluded in typing tests
* `#20290 <https://github.com/numpy/numpy/pull/20290>`__: DOC: Do not leave space between directive name and double colon.
* `#20292 <https://github.com/numpy/numpy/pull/20292>`__: SIMD: replace raw SIMD of ceil with universal intrinsics
* `#20299 <https://github.com/numpy/numpy/pull/20299>`__: BLD: in conda, pin setuptools to a known working version
* `#20303 <https://github.com/numpy/numpy/pull/20303>`__: BUG: Fix requirement that user DTypes had to be heaptypes
* `#20307 <https://github.com/numpy/numpy/pull/20307>`__: REL: Update main after 1.21.4 release.
* `#20308 <https://github.com/numpy/numpy/pull/20308>`__: MAINT: Add ``IS_PYSTON`` to ``np.testing.__all__``
* `#20309 <https://github.com/numpy/numpy/pull/20309>`__: MAINT: Add annotations for a missing ``percentile`` interpolation...
* `#20310 <https://github.com/numpy/numpy/pull/20310>`__: BUG: Fix float16 einsum fastpaths using wrong tempvar
* `#20314 <https://github.com/numpy/numpy/pull/20314>`__: BUG: Get full precision for 32 bit floating point random values.
* `#20315 <https://github.com/numpy/numpy/pull/20315>`__: MAINT: Remove Python <3.8 support from C
* `#20318 <https://github.com/numpy/numpy/pull/20318>`__: MAINT: Remove codeql-analysis.yml.
* `#20325 <https://github.com/numpy/numpy/pull/20325>`__: DOC: Remove non-existent quantile ``interpolation="inclusive"``...
* `#20327 <https://github.com/numpy/numpy/pull/20327>`__: BUG,DEP: Fixup quantile/percentile and rename interpolation->method
* `#20331 <https://github.com/numpy/numpy/pull/20331>`__: MAINT: Update quantile default lerp method
* `#20333 <https://github.com/numpy/numpy/pull/20333>`__: DEP: remove code for supporting GCC <4 in Mingw32CCompiler
* `#20334 <https://github.com/numpy/numpy/pull/20334>`__: MAINT: Rename commit trigger to "wheel build" for building wheels
* `#20342 <https://github.com/numpy/numpy/pull/20342>`__: CI: Bump azure MacOS version to macOS-1015
* `#20343 <https://github.com/numpy/numpy/pull/20343>`__: ENH: add a 'version' field to PyDataMem_Handler
* `#20344 <https://github.com/numpy/numpy/pull/20344>`__: BLD: do not position 'cxx=-std=c++11' as a default compiler flag
* `#20345 <https://github.com/numpy/numpy/pull/20345>`__: ENH: Avoid re-encapsulation of the default handler
* `#20347 <https://github.com/numpy/numpy/pull/20347>`__: MAINT: Do not forward ``__(deep)copy__`` calls of ``_GenericAlias``...
* `#20350 <https://github.com/numpy/numpy/pull/20350>`__: DOC: random: Fix a mistake in the zipf example.
* `#20352 <https://github.com/numpy/numpy/pull/20352>`__: ENH: Prefix log messages with their levels
* `#20353 <https://github.com/numpy/numpy/pull/20353>`__: BUG, DIST: Print os error message when the executable not exist
* `#20354 <https://github.com/numpy/numpy/pull/20354>`__: BLD: Verify the ability to compile C++ sources before initiating...
* `#20360 <https://github.com/numpy/numpy/pull/20360>`__: BUG: Revert from ``long double`` changes, and force ``npymath`` to...
* `#20361 <https://github.com/numpy/numpy/pull/20361>`__: MAINT: Update SVML sources to prevent an executable stack
* `#20364 <https://github.com/numpy/numpy/pull/20364>`__: BUG: Relax unary ufunc (sqrt, etc.) stride assert
* `#20365 <https://github.com/numpy/numpy/pull/20365>`__: BUG: Fix failure to create aligned, empty structured dtype
* `#20366 <https://github.com/numpy/numpy/pull/20366>`__: MAINT,TST: Avoid small positive integers in refcount test
* `#20367 <https://github.com/numpy/numpy/pull/20367>`__: ENH, SIMD: add new universal intrinsics for trunc
* `#20369 <https://github.com/numpy/numpy/pull/20369>`__: MAINT: Fix newlines in diagnostics output of numpy.f2py.
* `#20373 <https://github.com/numpy/numpy/pull/20373>`__: MAINT: Prepare for branching maintenance/1.22.x
* `#20379 <https://github.com/numpy/numpy/pull/20379>`__: DOC: Fix formatting of a code example in ``numpy.random.Generator.multivariate_normal()``...
* `#20386 <https://github.com/numpy/numpy/pull/20386>`__: REV: Add MaskedArray creation from non nd-array back in
* `#20402 <https://github.com/numpy/numpy/pull/20402>`__: BLD: Fix Macos Builds [wheel build]
* `#20410 <https://github.com/numpy/numpy/pull/20410>`__: BUG, SIMD: Fix ``exp`` FP stack overflow when ``AVX512_SKX`` is enabled
* `#20411 <https://github.com/numpy/numpy/pull/20411>`__: ENH: provide a convenience function to replace npy_load_module...
* `#20415 <https://github.com/numpy/numpy/pull/20415>`__: CI: CircleCI: Install numpy after processing doc_requirements.txt
* `#20419 <https://github.com/numpy/numpy/pull/20419>`__: MAINT: import setuptools before distutils in one np.random test
* `#20420 <https://github.com/numpy/numpy/pull/20420>`__: BUG: Clear errors correctly in F2PY conversions
* `#20429 <https://github.com/numpy/numpy/pull/20429>`__: MAINT: Fix longdouble precision check in test_umath.py
* `#20430 <https://github.com/numpy/numpy/pull/20430>`__: MAINT: Fix METH_NOARGS function signatures
* `#20434 <https://github.com/numpy/numpy/pull/20434>`__: REL: Prepare for the NumPy 1.22.0r1 release.
* `#20436 <https://github.com/numpy/numpy/pull/20436>`__: BUG: Fix an incorrect protocol used in ``np.lib.shape_base``
* `#20473 <https://github.com/numpy/numpy/pull/20473>`__: BUG: Fix two overload-related problems
* `#20474 <https://github.com/numpy/numpy/pull/20474>`__: TST: remove obsolete TestF77Mismatch
* `#20475 <https://github.com/numpy/numpy/pull/20475>`__: MAINT: Update the required setuptools version.
* `#20476 <https://github.com/numpy/numpy/pull/20476>`__: BUG: Restore support for i386 and PowerPC (OS X)
* `#20487 <https://github.com/numpy/numpy/pull/20487>`__: MAINT: update wheel to version that supports python3.10
* `#20496 <https://github.com/numpy/numpy/pull/20496>`__: TST: use pypy3.8-v7.3.7 final versions
* `#20502 <https://github.com/numpy/numpy/pull/20502>`__: BUG: Fix the .T attribute in the array_api namespace
* `#20503 <https://github.com/numpy/numpy/pull/20503>`__: BUG: Protect divide by 0 in multinomial distribution.
* `#20535 <https://github.com/numpy/numpy/pull/20535>`__: BUG: Fix reduce promotion with out argument
* `#20538 <https://github.com/numpy/numpy/pull/20538>`__: BUG: Fix handling of the dtype parameter to numpy.array_api.prod()
* `#20539 <https://github.com/numpy/numpy/pull/20539>`__: PERF: Fix performance bug in dispatching cache
* `#20541 <https://github.com/numpy/numpy/pull/20541>`__: REL: Prepare for NumPy 1.22.0rc2 release.
* `#20548 <https://github.com/numpy/numpy/pull/20548>`__: REV: Revert adding a default ufunc promoter
* `#20576 <https://github.com/numpy/numpy/pull/20576>`__: BUG: Fix small issues found using valgrind
* `#20577 <https://github.com/numpy/numpy/pull/20577>`__: BUG: Fix sorting of int8/int16
* `#20578 <https://github.com/numpy/numpy/pull/20578>`__: ENH: Add ``__array__`` to the array_api Array object
* `#20579 <https://github.com/numpy/numpy/pull/20579>`__: MAINT: make sure CI stays on VS2019 unless changed explicitly
* `#20585 <https://github.com/numpy/numpy/pull/20585>`__: DOC: Update front page of documentation with Sphinx-Panels
* `#20598 <https://github.com/numpy/numpy/pull/20598>`__: BUG: Fix issues (mainly) found using pytest-leaks
* `#20599 <https://github.com/numpy/numpy/pull/20599>`__: MAINT: Fix two minor typing-related problems
* `#20600 <https://github.com/numpy/numpy/pull/20600>`__: BUG: Fix leaks found using pytest-leaks
* `#20601 <https://github.com/numpy/numpy/pull/20601>`__: MAINT: Check for buffer interface support rather than try/except
* `#20602 <https://github.com/numpy/numpy/pull/20602>`__: BUG: Fix PyInit__umath_linalg type
* `#20605 <https://github.com/numpy/numpy/pull/20605>`__: DEV: add a warningfilter to fix pytest workflow.
* `#20614 <https://github.com/numpy/numpy/pull/20614>`__: TST: Bump mypy: 0.910 -> 0.920
* `#20617 <https://github.com/numpy/numpy/pull/20617>`__: MAINT: Help boost::python libraries at least not crash
* `#20632 <https://github.com/numpy/numpy/pull/20632>`__: DOC: Document implementation of NEP 43 and experimental new DType...
* `#20649 <https://github.com/numpy/numpy/pull/20649>`__: DOC: Modify SVG to be visible on Chrome
* `#20650 <https://github.com/numpy/numpy/pull/20650>`__: BUG: Support env argument in CCompiler.spawn
* `#20651 <https://github.com/numpy/numpy/pull/20651>`__: BUG: f2py: Simplify creation of an exception message.
* `#20680 <https://github.com/numpy/numpy/pull/20680>`__: TYP,TST: Bump mypy to 0.930
* `#20681 <https://github.com/numpy/numpy/pull/20681>`__: BUG: Fix setstate logic for empty arrays
* `#20682 <https://github.com/numpy/numpy/pull/20682>`__: ENH: Add ARM Compiler with ARM Performance Library support

Contributors
============

A total of 11 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Ganesh Kathiresan
* Gregory R. Lee
* Hugo Defois +
* Kevin Sheppard
* Matti Picus
* Ralf Gommers
* Sayed Adel
* Sebastian Berg
* Thomas J. Fan

Pull requests merged
====================

A total of 26 pull requests were merged for this release.

* `#19311 <https://github.com/numpy/numpy/pull/19311>`__: REV,BUG: Replace ``NotImplemented`` with ``typing.Any``
* `#19324 <https://github.com/numpy/numpy/pull/19324>`__: MAINT: Fixed the return-dtype of ``ndarray.real`` and ``imag``
* `#19330 <https://github.com/numpy/numpy/pull/19330>`__: MAINT: Replace ``"dtype[Any]"`` with ``dtype`` in the definiton of...
* `#19342 <https://github.com/numpy/numpy/pull/19342>`__: DOC: Fix some docstrings that crash pdf generation.
* `#19343 <https://github.com/numpy/numpy/pull/19343>`__: MAINT: bump scipy-mathjax
* `#19347 <https://github.com/numpy/numpy/pull/19347>`__: BUG: Fix arr.flat.index for large arrays and big-endian machines
* `#19348 <https://github.com/numpy/numpy/pull/19348>`__: ENH: add ``numpy.f2py.get_include`` function
* `#19349 <https://github.com/numpy/numpy/pull/19349>`__: BUG: Fix reference count leak in ufunc dtype handling
* `#19350 <https://github.com/numpy/numpy/pull/19350>`__: MAINT: Annotate missing attributes of ``np.number`` subclasses
* `#19351 <https://github.com/numpy/numpy/pull/19351>`__: BUG: Fix cast safety and comparisons for zero sized voids
* `#19352 <https://github.com/numpy/numpy/pull/19352>`__: BUG: Correct Cython declaration in random
* `#19353 <https://github.com/numpy/numpy/pull/19353>`__: BUG: protect against accessing base attribute of a NULL subarray
* `#19365 <https://github.com/numpy/numpy/pull/19365>`__: BUG, SIMD: Fix detecting AVX512 features on Darwin
* `#19366 <https://github.com/numpy/numpy/pull/19366>`__: MAINT: remove ``print()``'s in distutils template handling
* `#19390 <https://github.com/numpy/numpy/pull/19390>`__: ENH: SIMD architectures to show_config
* `#19391 <https://github.com/numpy/numpy/pull/19391>`__: BUG: Do not raise deprecation warning for all nans in unique...
* `#19392 <https://github.com/numpy/numpy/pull/19392>`__: BUG: Fix NULL special case in object-to-any cast code
* `#19430 <https://github.com/numpy/numpy/pull/19430>`__: MAINT: Use arm64-graviton2 for testing on travis
* `#19495 <https://github.com/numpy/numpy/pull/19495>`__: BUILD: update OpenBLAS to v0.3.17
* `#19496 <https://github.com/numpy/numpy/pull/19496>`__: MAINT: Avoid unicode characters in division SIMD code comments
* `#19499 <https://github.com/numpy/numpy/pull/19499>`__: BUG, SIMD: Fix infinite loop during count non-zero on GCC-11
* `#19500 <https://github.com/numpy/numpy/pull/19500>`__: BUG: fix a numpy.npiter leak in npyiter_multi_index_set
* `#19501 <https://github.com/numpy/numpy/pull/19501>`__: TST: Fix a ``GenericAlias`` test failure for python 3.9.0
* `#19502 <https://github.com/numpy/numpy/pull/19502>`__: MAINT: Start testing with Python 3.10.0b3.
* `#19503 <https://github.com/numpy/numpy/pull/19503>`__: MAINT: Add missing dtype overloads for object- and ctypes-based...
* `#19510 <https://github.com/numpy/numpy/pull/19510>`__: REL: Prepare for NumPy 1.21.1 release.


Contributors
============

A total of 114 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Abhinav Sagar
* Alex Henrie +
* Alexander Jung +
* Allan Haldane
* Andrea Pattori
* Andrew Liu +
* Anis Ladram +
* Anne Bonner +
* Antoine Dechaume +
* Aryan Naraghi +
* Bastian Eichenberger +
* Brian Wignall +
* Brigitta Sipocz
* CakeWithSteak +
* Charles Harris
* Chris Barker
* Chris Burr +
* Chris Markiewicz +
* Christoph Gohlke
* Christopher Whelan
* Colin Snyder
* Dan Allan
* Daniel Ching
* David Stansby +
* David Zwicker +
* Dieter Werthmüller
* Disconnect3d +
* Dmytro +
* Doug Davis +
* Eric Larson
* Eric Wieser
* Esben Haabendal +
* Eugene Prilepin +
* Felix Divo +
* Gary Gurlaskie
* Gina +
* Giuseppe Cuccu +
* Grzegorz Bokota +
* Guanqun Lu +
* Guilherme Leobas +
* Guillaume Horel
* Géraud Le Falher +
* Hameer Abbasi
* Harmon
* Hiroyuki V. Yamazaki
* Huang, Guangtai +
* Hugo +
* Hyeonguk Ryu +
* Ilhan Polat +
* Isaac Virshup
* Jack J. Woehr +
* Jack Woehr +
* Jackie Leng
* Jaime Fernandez
* Jeff Hale +
* Johann Faouzi +
* Jon Dufresne +
* Joseph Fox-Rabinovitz
* Joseph R. Fox-Rabinovitz +
* João Marcos Gris +
* Justus Magin +
* Jérémie du Boisberranger
* Kai Striega
* Kevin Sheppard
* Kexuan Sun
* Kmol Yuan +
* Kriti Singh
* Larry Bradley +
* Lars Grueter
* Luis Pedro Coelho
* MSeifert04
* Magdalena Proszewska +
* Manny +
* Mark Harfouche
* Martin Reinecke
* Martin Thoma
* Matt Haberland +
* Matt McCormick +
* Matthias Bussonnier
* Matti Picus
* Max Bolingbroke +
* Maxwell Aladago +
* Michael Hudson-Doyle +
* Oleksandr Pavlyk
* Omar Merghany +
* Pauli Virtanen
* Peter Andreas Entschev
* Peter Bell
* Peter Cock +
* Pradeep Reddy Raamana +
* Qiming Sun +
* Raghuveer Devulapalli
* Ralf Gommers
* Samesh +
* Samesh Lakhotia +
* Sebastian Berg
* Sergei Lebedev
* Seth Troisi +
* Siddhesh Poyarekar +
* Simon +
* Simon Notley +
* Stefan van der Walt
* Stephan Hoyer
* Steve Stagg
* Thomas A Caswell
* Thomas Kluyver
* Tim Hoffmann +
* Tirth Patel +
* Tyler Reddy
* Vladimir Pershin +
* Warren Weckesser
* Yadong Zhang +
* Zieji Pohz +
* Zolisa Bleki +

Pull requests merged
====================

A total of 413 pull requests were merged for this release.

* `#9301 <https://github.com/numpy/numpy/pull/9301>`__: DOC: added note to docstring of numpy.savez
* `#10151 <https://github.com/numpy/numpy/pull/10151>`__: BUG: Numpy scalar types sometimes have the same name
* `#12129 <https://github.com/numpy/numpy/pull/12129>`__: DOC: Improve axes shift description and example in np.tensordot
* `#12205 <https://github.com/numpy/numpy/pull/12205>`__: MAINT: avoid relying on ``np.generic.__name__`` in ``np.dtype.name``
* `#12284 <https://github.com/numpy/numpy/pull/12284>`__: ENH: supply our version of numpy.pxd, requires cython>=0.29
* `#12633 <https://github.com/numpy/numpy/pull/12633>`__: BUG: General fixes to f2py reference counts (dereferencing)
* `#12658 <https://github.com/numpy/numpy/pull/12658>`__: BUG: NaT now sorts to ends of arrays
* `#12828 <https://github.com/numpy/numpy/pull/12828>`__: DOC: Updates to nditer usage instructions
* `#13003 <https://github.com/numpy/numpy/pull/13003>`__: BUG: Do not crash on recursive ``.dtype`` attribute lookup.
* `#13368 <https://github.com/numpy/numpy/pull/13368>`__: ENH: Use AVX for float32 implementation of np.sin & np.cos
* `#13605 <https://github.com/numpy/numpy/pull/13605>`__: DEP: Deprecate silent ignoring of bad data in fromfile/fromstring
* `#13610 <https://github.com/numpy/numpy/pull/13610>`__: ENH: Always produce a consistent shape in the result of ``argwhere``
* `#13673 <https://github.com/numpy/numpy/pull/13673>`__: DOC: array(obj, dtype=dt) can downcast
* `#13698 <https://github.com/numpy/numpy/pull/13698>`__: DOC: Document ma.filled behavior with non-scalar fill_value
* `#13710 <https://github.com/numpy/numpy/pull/13710>`__: DOC: Add note to irfft-like functions about the default sizes
* `#13739 <https://github.com/numpy/numpy/pull/13739>`__: BUG: Don't produce undefined behavior for a << b if b >= bitsof(a)
* `#13766 <https://github.com/numpy/numpy/pull/13766>`__: MAINT: Update NEP template.
* `#13794 <https://github.com/numpy/numpy/pull/13794>`__: ENH: random: Add the multivariate hypergeometric distribution.
* `#13799 <https://github.com/numpy/numpy/pull/13799>`__: DOC: Fix unrendered links
* `#13802 <https://github.com/numpy/numpy/pull/13802>`__: BUG: Fixed maximum relative error reporting in assert_allclose
* `#13812 <https://github.com/numpy/numpy/pull/13812>`__: MAINT: Rewrite Floyd algorithm
* `#13825 <https://github.com/numpy/numpy/pull/13825>`__: DOC: Add missing macros to C-API documentation
* `#13829 <https://github.com/numpy/numpy/pull/13829>`__: ENH: Add axis argument to random.permutation and random.shuffle
* `#13847 <https://github.com/numpy/numpy/pull/13847>`__: DOC: Adds documentation of functions exposed in numpy namespace
* `#13860 <https://github.com/numpy/numpy/pull/13860>`__: BUG: Refcount fixes
* `#13871 <https://github.com/numpy/numpy/pull/13871>`__: MAINT: Ensure array_dealloc does not modify refcount of self
* `#13874 <https://github.com/numpy/numpy/pull/13874>`__: MAINT: Prepare master for 1.18.0 development.
* `#13876 <https://github.com/numpy/numpy/pull/13876>`__: MAINT,BUG,DOC: Fix errors in _add_newdocs
* `#13880 <https://github.com/numpy/numpy/pull/13880>`__: MAINT: Remove an unnessary backslash between two string literals
* `#13881 <https://github.com/numpy/numpy/pull/13881>`__: MAINT: Update pavement to use python3 in shell commands.
* `#13882 <https://github.com/numpy/numpy/pull/13882>`__: MAINT: Remove unnecessary backslashes (and replace others by...
* `#13883 <https://github.com/numpy/numpy/pull/13883>`__: MAINT: Replace integers in places where booleans are expected
* `#13884 <https://github.com/numpy/numpy/pull/13884>`__: DOC: Add missing parameter description for keepdims in MaskedArray
* `#13885 <https://github.com/numpy/numpy/pull/13885>`__: ENH: use AVX for float32 and float64 implementation of sqrt,...
* `#13886 <https://github.com/numpy/numpy/pull/13886>`__: DOC: reformat top-level release index
* `#13892 <https://github.com/numpy/numpy/pull/13892>`__: DOC : Refactor Array API documentation -- Array Structure and...
* `#13895 <https://github.com/numpy/numpy/pull/13895>`__: DOC: Fix typo in "make_mask" documentation
* `#13896 <https://github.com/numpy/numpy/pull/13896>`__: MAINT: Delete unused _aliased_types.py
* `#13899 <https://github.com/numpy/numpy/pull/13899>`__: MAINT: Change the type of error raised in set_printoptions
* `#13901 <https://github.com/numpy/numpy/pull/13901>`__: BLD: Remove Trusty dist in Travis CI build
* `#13907 <https://github.com/numpy/numpy/pull/13907>`__: BUG: Handle weird bytestrings in dtype()
* `#13908 <https://github.com/numpy/numpy/pull/13908>`__: ENH: use towncrier to build the release note
* `#13913 <https://github.com/numpy/numpy/pull/13913>`__: ENH: improve error message for ragged-array creation failure
* `#13914 <https://github.com/numpy/numpy/pull/13914>`__: DOC: Update the description of byteswap
* `#13916 <https://github.com/numpy/numpy/pull/13916>`__: BUG: i0 Bessel function regression on array-likes supporting...
* `#13920 <https://github.com/numpy/numpy/pull/13920>`__: ENH, BUILD: refactor all OpenBLAS downloads into a single, testable...
* `#13922 <https://github.com/numpy/numpy/pull/13922>`__: MAINT: Remove unnecessary parenthesis in numpy.ma.core
* `#13925 <https://github.com/numpy/numpy/pull/13925>`__: MAINT: Fix wrong spelling of ufunc
* `#13926 <https://github.com/numpy/numpy/pull/13926>`__: DOC: Remove explicit .next method calls with built-in next function...
* `#13928 <https://github.com/numpy/numpy/pull/13928>`__: DOC: Don't override MaskedArray.view documentation with the one...
* `#13930 <https://github.com/numpy/numpy/pull/13930>`__: BUG: Fix incorrect GIL release in array.nonzero
* `#13935 <https://github.com/numpy/numpy/pull/13935>`__: MAINT: Warn if ``_add_newdocs.py`` is used to add docstrings to...
* `#13943 <https://github.com/numpy/numpy/pull/13943>`__: MAINT: Revert #13876, "MAINT,BUG,DOC: Fix errors in _add_newdocs"
* `#13944 <https://github.com/numpy/numpy/pull/13944>`__: MAINT,BUG,DOC: Fix errors in _add_newdocs
* `#13945 <https://github.com/numpy/numpy/pull/13945>`__: DOC, MAINT: emphasize random API changes, remove Generator.randint
* `#13946 <https://github.com/numpy/numpy/pull/13946>`__: DOC: Add a numpy-doc docstring to add_newdoc
* `#13947 <https://github.com/numpy/numpy/pull/13947>`__: DOC: Fix rst rendering in data types
* `#13948 <https://github.com/numpy/numpy/pull/13948>`__: DOC:Update the description of set_printoptions in quickstart...
* `#13950 <https://github.com/numpy/numpy/pull/13950>`__: Fixing failure on Python 2.7 on Windows 7
* `#13952 <https://github.com/numpy/numpy/pull/13952>`__: Fix a typo related to the range of indices
* `#13959 <https://github.com/numpy/numpy/pull/13959>`__: DOC: add space between words across lines
* `#13964 <https://github.com/numpy/numpy/pull/13964>`__: BUG, DOC: add new recfunctions to ``__all__``
* `#13967 <https://github.com/numpy/numpy/pull/13967>`__: DOC: Change (old) range() to np.arange()
* `#13968 <https://github.com/numpy/numpy/pull/13968>`__: DOC: improve np.sort docstring
* `#13970 <https://github.com/numpy/numpy/pull/13970>`__: DOC: spellcheck numpy/doc/broadcasting.py
* `#13976 <https://github.com/numpy/numpy/pull/13976>`__: MAINT, TST: remove test-installed-numpy.py
* `#13979 <https://github.com/numpy/numpy/pull/13979>`__: DOC: Document array_function at a higher level.
* `#13985 <https://github.com/numpy/numpy/pull/13985>`__: DOC: show workaround for backward compatibility
* `#13988 <https://github.com/numpy/numpy/pull/13988>`__: DOC: Add a call for contribution paragraph to the readme
* `#13989 <https://github.com/numpy/numpy/pull/13989>`__: BUG: Missing warnings import in polyutils
* `#13990 <https://github.com/numpy/numpy/pull/13990>`__: BUILD: adapt "make version-check" to "make dist"
* `#13991 <https://github.com/numpy/numpy/pull/13991>`__: DOC: emphasize need for matching numpy, git versions
* `#14002 <https://github.com/numpy/numpy/pull/14002>`__: TST, MAINT, BUG: expand OpenBLAS version checking
* `#14004 <https://github.com/numpy/numpy/pull/14004>`__: ENH: Chain exception for typed item assignment
* `#14005 <https://github.com/numpy/numpy/pull/14005>`__: MAINT: Fix spelling error in npy_tempita kwarg
* `#14010 <https://github.com/numpy/numpy/pull/14010>`__: DOC: Array API : Directory restructure and code cleanup
* `#14011 <https://github.com/numpy/numpy/pull/14011>`__: [DOC] Remove unused/deprecated functions
* `#14022 <https://github.com/numpy/numpy/pull/14022>`__: Update system_info.py
* `#14025 <https://github.com/numpy/numpy/pull/14025>`__: DOC:Link between the two indexing documentation pages
* `#14026 <https://github.com/numpy/numpy/pull/14026>`__: DOC: Update NumFOCUS subcommittee replacing Nathaniel with Sebastian
* `#14027 <https://github.com/numpy/numpy/pull/14027>`__: DOC: update "Contributing to NumPy" with more activities/roles
* `#14028 <https://github.com/numpy/numpy/pull/14028>`__: DOC: Improve quickstart documentation of new random Generator
* `#14030 <https://github.com/numpy/numpy/pull/14030>`__: DEP: Speed up WarnOnWrite deprecation in buffer interface
* `#14032 <https://github.com/numpy/numpy/pull/14032>`__: NEP: numpy.org website redesign
* `#14035 <https://github.com/numpy/numpy/pull/14035>`__: DOC: Fix docstring of numpy.allclose regarding NaNs
* `#14036 <https://github.com/numpy/numpy/pull/14036>`__: DEP: Raise warnings for deprecated functions PyArray_As1D, PyArray_As2D
* `#14039 <https://github.com/numpy/numpy/pull/14039>`__: DEP: Remove np.rank which has been deprecated for more than 5...
* `#14048 <https://github.com/numpy/numpy/pull/14048>`__: BUG, TEST: Adding validation test suite to validate float32 exp
* `#14051 <https://github.com/numpy/numpy/pull/14051>`__: ENH,DEP: Allow multiple axes in expand_dims
* `#14053 <https://github.com/numpy/numpy/pull/14053>`__: ENH: add pyproject.toml
* `#14060 <https://github.com/numpy/numpy/pull/14060>`__: DOC: Update cversions.py links and wording
* `#14062 <https://github.com/numpy/numpy/pull/14062>`__: DOC, BUILD: cleanups and fix (again) 'make dist'
* `#14063 <https://github.com/numpy/numpy/pull/14063>`__: BUG: Fix file-like object check when saving arrays
* `#14064 <https://github.com/numpy/numpy/pull/14064>`__: DOC: Resolve bad references in Sphinx warnings
* `#14068 <https://github.com/numpy/numpy/pull/14068>`__: MAINT: bump ARMv8 / POWER8 OpenBLAS in CI
* `#14069 <https://github.com/numpy/numpy/pull/14069>`__: DOC: Emphasize the need to run tests when building from source
* `#14070 <https://github.com/numpy/numpy/pull/14070>`__: DOC:Add example to clarify "numpy.save" behavior on already open...
* `#14072 <https://github.com/numpy/numpy/pull/14072>`__: DEP: Deprecate full and economic modes for linalg.qr
* `#14073 <https://github.com/numpy/numpy/pull/14073>`__: DOC: Doc release
* `#14074 <https://github.com/numpy/numpy/pull/14074>`__: BUG: fix build issue on icc 2016
* `#14076 <https://github.com/numpy/numpy/pull/14076>`__: TST: Add 3.8-dev to travisCI testing.
* `#14085 <https://github.com/numpy/numpy/pull/14085>`__: DOC: Add blank line above doctest for intersect1d
* `#14086 <https://github.com/numpy/numpy/pull/14086>`__: ENH: Propose standard policy for dropping support of old Python...
* `#14089 <https://github.com/numpy/numpy/pull/14089>`__: DOC: Use ``pip install .`` where possible instead of calling setup.py
* `#14091 <https://github.com/numpy/numpy/pull/14091>`__: MAINT: adjustments to test_ufunc_noncontigous
* `#14092 <https://github.com/numpy/numpy/pull/14092>`__: MAINT: Improve NEP template
* `#14096 <https://github.com/numpy/numpy/pull/14096>`__: DOC: fix documentation of i and j for tri.
* `#14097 <https://github.com/numpy/numpy/pull/14097>`__: MAINT: Lazy import testing on python >=3.7
* `#14100 <https://github.com/numpy/numpy/pull/14100>`__: DEP: Deprecate PyArray_FromDimsAndDataAndDescr, PyArray_FromDims
* `#14101 <https://github.com/numpy/numpy/pull/14101>`__: MAINT: Clearer error message while padding with stat_length=0
* `#14106 <https://github.com/numpy/numpy/pull/14106>`__: MAINT: remove duplicate variable assignments
* `#14108 <https://github.com/numpy/numpy/pull/14108>`__: BUG: initialize variable that is passed by pointer
* `#14110 <https://github.com/numpy/numpy/pull/14110>`__: DOC: fix typo in c-api/array.rst doc
* `#14115 <https://github.com/numpy/numpy/pull/14115>`__: DOC: fix markup of news fragment readme
* `#14121 <https://github.com/numpy/numpy/pull/14121>`__: BUG: Add gcd/lcm definitions to npy_math.h
* `#14122 <https://github.com/numpy/numpy/pull/14122>`__: MAINT: Mark umath accuracy test xfail.
* `#14124 <https://github.com/numpy/numpy/pull/14124>`__: MAINT: Use equality instead of identity check with literal
* `#14130 <https://github.com/numpy/numpy/pull/14130>`__: MAINT: Fix small typo in quickstart docs
* `#14134 <https://github.com/numpy/numpy/pull/14134>`__: DOC, MAINT: Update master after 1.17.0 release.
* `#14141 <https://github.com/numpy/numpy/pull/14141>`__: ENH: add c-imported modules for freeze analysis in np.random
* `#14143 <https://github.com/numpy/numpy/pull/14143>`__: BUG: Fix DeprecationWarning in python 3.8
* `#14144 <https://github.com/numpy/numpy/pull/14144>`__: BUG: Remove stray print that causes a SystemError on python 3.7...
* `#14145 <https://github.com/numpy/numpy/pull/14145>`__: BUG: Remove the broken clip wrapper
* `#14152 <https://github.com/numpy/numpy/pull/14152>`__: BUG: avx2_scalef_ps must be static
* `#14153 <https://github.com/numpy/numpy/pull/14153>`__: TST: Allow fuss in testing strided/non-strided exp/log loops
* `#14170 <https://github.com/numpy/numpy/pull/14170>`__: NEP: Proposal for __duckarray__ protocol
* `#14171 <https://github.com/numpy/numpy/pull/14171>`__: BUG: Make advanced indexing result on read-only subclass writeable
* `#14174 <https://github.com/numpy/numpy/pull/14174>`__: BUG: Check for existence of ``fromstr`` which used in ``fromstr_next_element``
* `#14178 <https://github.com/numpy/numpy/pull/14178>`__: TST: Clean up of test_pocketfft.py
* `#14181 <https://github.com/numpy/numpy/pull/14181>`__: DEP: Deprecate np.alen
* `#14183 <https://github.com/numpy/numpy/pull/14183>`__: DOC: Fix misleading ``allclose`` docstring for ``equal_nan``
* `#14185 <https://github.com/numpy/numpy/pull/14185>`__: MAINT: Workaround for Intel compiler bug leading to failing test
* `#14190 <https://github.com/numpy/numpy/pull/14190>`__: DOC: Fix hermitian argument docs in ``svd``
* `#14195 <https://github.com/numpy/numpy/pull/14195>`__: MAINT: Fix a docstring typo.
* `#14196 <https://github.com/numpy/numpy/pull/14196>`__: DOC: Fix links in ``/.github/CONTRIBUTING.md``.
* `#14197 <https://github.com/numpy/numpy/pull/14197>`__: ENH: Multivariate normal speedups
* `#14203 <https://github.com/numpy/numpy/pull/14203>`__: MAINT: Improve mismatch message of np.testing.assert_array_equal
* `#14204 <https://github.com/numpy/numpy/pull/14204>`__: DOC,MAINT: Move towncrier files and fixup categories
* `#14207 <https://github.com/numpy/numpy/pull/14207>`__: BUG: Fixed default BitGenerator name
* `#14209 <https://github.com/numpy/numpy/pull/14209>`__: BUG: Fix uint-overflow if padding with linear_ramp and negative...
* `#14216 <https://github.com/numpy/numpy/pull/14216>`__: ENH: Enable huge pages in all Linux builds
* `#14217 <https://github.com/numpy/numpy/pull/14217>`__: BUG: Fix leak in the f2py-generated module init and ``PyMem_Del``...
* `#14219 <https://github.com/numpy/numpy/pull/14219>`__: DOC: new nan_to_num keywords are from 1.17 onwards
* `#14223 <https://github.com/numpy/numpy/pull/14223>`__: TST: Add tests for deprecated C functions (PyArray_As1D, PyArray_As1D)
* `#14224 <https://github.com/numpy/numpy/pull/14224>`__: DOC: mention ``take_along_axis`` in ``choose``
* `#14227 <https://github.com/numpy/numpy/pull/14227>`__: ENH: Parse complex number from string
* `#14231 <https://github.com/numpy/numpy/pull/14231>`__: DOC: update or remove outdated sourceforge links
* `#14234 <https://github.com/numpy/numpy/pull/14234>`__: MAINT: Better error message for norm
* `#14235 <https://github.com/numpy/numpy/pull/14235>`__: DOC: add backlinks to numpy.org
* `#14240 <https://github.com/numpy/numpy/pull/14240>`__: BUG: Don't fail when lexsorting some empty arrays.
* `#14241 <https://github.com/numpy/numpy/pull/14241>`__: BUG: Fix segfault in ``random.permutation(x)`` when x is a string.
* `#14245 <https://github.com/numpy/numpy/pull/14245>`__: Doc: fix a typo in NEP21
* `#14249 <https://github.com/numpy/numpy/pull/14249>`__: DOC: set status of NEP 28 (website redesign) to Accepted
* `#14250 <https://github.com/numpy/numpy/pull/14250>`__: BLD: MAINT: change default behavior of build flag appending.
* `#14252 <https://github.com/numpy/numpy/pull/14252>`__: BUG: Fixes StopIteration error from 'np.genfromtext' for empty...
* `#14255 <https://github.com/numpy/numpy/pull/14255>`__: BUG: fix inconsistent axes ordering for axis in function ``unique``
* `#14256 <https://github.com/numpy/numpy/pull/14256>`__: DEP: Deprecate load/dump functions in favour of pickle methods
* `#14257 <https://github.com/numpy/numpy/pull/14257>`__: MAINT: Update NEP-30
* `#14259 <https://github.com/numpy/numpy/pull/14259>`__: DEP: Deprecate arrayprint formatting functions
* `#14263 <https://github.com/numpy/numpy/pull/14263>`__: DOC: Make Py3K docs C code snippets RST literal blocks
* `#14266 <https://github.com/numpy/numpy/pull/14266>`__: DOC: remove scipy.org from the breadcrumb formattiong
* `#14270 <https://github.com/numpy/numpy/pull/14270>`__: BUG: Fix formatting error in exception message
* `#14272 <https://github.com/numpy/numpy/pull/14272>`__: DOC: Address typos in dispatch docs
* `#14279 <https://github.com/numpy/numpy/pull/14279>`__: BUG: Fix ZeroDivisionError for zero length arrays in pocketfft.
* `#14290 <https://github.com/numpy/numpy/pull/14290>`__: BUG: Fix misuse of .names and .fields in various places
* `#14291 <https://github.com/numpy/numpy/pull/14291>`__: TST, BUG: Use python3.6-dbg.
* `#14295 <https://github.com/numpy/numpy/pull/14295>`__: BUG: core: Handle large negative np.int64 args in binary_repr.
* `#14298 <https://github.com/numpy/numpy/pull/14298>`__: BUG: Fix numpy.random bug in platform detection
* `#14303 <https://github.com/numpy/numpy/pull/14303>`__: MAINT: random: Match type of SeedSequence.pool_size to DEFAULT_POOL_SIZE.
* `#14310 <https://github.com/numpy/numpy/pull/14310>`__: Bug: Fix behavior of structured_to_unstructured on non-trivial...
* `#14311 <https://github.com/numpy/numpy/pull/14311>`__: DOC: add two commas, move one word
* `#14313 <https://github.com/numpy/numpy/pull/14313>`__: DOC: Clarify rules about broadcasting when empty arrays are involved.
* `#14321 <https://github.com/numpy/numpy/pull/14321>`__: TST, MAINT: bump to OpenBLAS 0.3.7 stable
* `#14325 <https://github.com/numpy/numpy/pull/14325>`__: DEP: numpy.testing.rand
* `#14335 <https://github.com/numpy/numpy/pull/14335>`__: DEP: Deprecate class ``SafeEval``
* `#14341 <https://github.com/numpy/numpy/pull/14341>`__: BUG: revert detecting and raising error on ragged arrays
* `#14342 <https://github.com/numpy/numpy/pull/14342>`__: DOC: Improve documentation of ``isscalar``.
* `#14349 <https://github.com/numpy/numpy/pull/14349>`__: MAINT: Fix bloated mismatch error percentage in array comparisons.
* `#14351 <https://github.com/numpy/numpy/pull/14351>`__: DOC: Fix a minor typo in dispatch documentation.
* `#14352 <https://github.com/numpy/numpy/pull/14352>`__: MAINT: Remove redundant deprecation checks
* `#14353 <https://github.com/numpy/numpy/pull/14353>`__: MAINT: polynomial: Add an N-d vander implementation used under...
* `#14355 <https://github.com/numpy/numpy/pull/14355>`__: DOC: clarify that PytestTester is non-public
* `#14356 <https://github.com/numpy/numpy/pull/14356>`__: DOC: support and require sphinx>=2.2
* `#14360 <https://github.com/numpy/numpy/pull/14360>`__: DOC: random: fix doc linking, was referencing private submodules.
* `#14364 <https://github.com/numpy/numpy/pull/14364>`__: MAINT: Fixes for prospective Python 3.10 and 4.0
* `#14365 <https://github.com/numpy/numpy/pull/14365>`__: DOC: lib: Add more explanation of the weighted average calculation.
* `#14368 <https://github.com/numpy/numpy/pull/14368>`__: MAINT: Avoid BytesWarning in PyArray_DescrConverter()
* `#14369 <https://github.com/numpy/numpy/pull/14369>`__: MAINT: Post NumPy 1.17.1 update.
* `#14370 <https://github.com/numpy/numpy/pull/14370>`__: DOC: Fixed dtype docs for var, nanvar.
* `#14372 <https://github.com/numpy/numpy/pull/14372>`__: DOC: Document project as Python 3 only with a trove classifier
* `#14378 <https://github.com/numpy/numpy/pull/14378>`__: BUILD: move all test dependencies to ./test_requirements.txt
* `#14381 <https://github.com/numpy/numpy/pull/14381>`__: BUG: lib: Fix histogram problem with signed integer arrays.
* `#14385 <https://github.com/numpy/numpy/pull/14385>`__: REL: Update master after NumPy 1.16.5 release.
* `#14387 <https://github.com/numpy/numpy/pull/14387>`__: BUG: test, fix regression in converting to ctypes
* `#14389 <https://github.com/numpy/numpy/pull/14389>`__: NEP: Add initial draft of NEP-31: Context-local and global overrides...
* `#14390 <https://github.com/numpy/numpy/pull/14390>`__: DOC: document numpy/doc update process
* `#14392 <https://github.com/numpy/numpy/pull/14392>`__: DOC: update np.around docstring with note about floating-point...
* `#14393 <https://github.com/numpy/numpy/pull/14393>`__: BUG: view with fieldless dtype should raise if itemsize != 0
* `#14395 <https://github.com/numpy/numpy/pull/14395>`__: DOC: fix issue with __new__ usage in subclassing doc.
* `#14398 <https://github.com/numpy/numpy/pull/14398>`__: DOC: Fix release notes table of contents
* `#14399 <https://github.com/numpy/numpy/pull/14399>`__: NEP 32: Remove the financial functions from NumPy
* `#14404 <https://github.com/numpy/numpy/pull/14404>`__: BLD: Update RELEASE_WALKTHROUGH and cythonize.
* `#14407 <https://github.com/numpy/numpy/pull/14407>`__: Bump pytest from 5.1.1 to 5.1.2
* `#14408 <https://github.com/numpy/numpy/pull/14408>`__: TST: Remove build job since we now use Dependabot
* `#14410 <https://github.com/numpy/numpy/pull/14410>`__: BLD: Only allow using Cython module when cythonizing.
* `#14411 <https://github.com/numpy/numpy/pull/14411>`__: TST: Add dependabot config file.
* `#14416 <https://github.com/numpy/numpy/pull/14416>`__: BUG: Fix format statement associated with AttributeError.
* `#14417 <https://github.com/numpy/numpy/pull/14417>`__: BUG: Fix aradixsort indirect indexing.
* `#14426 <https://github.com/numpy/numpy/pull/14426>`__: DOC: add the reference to 'printoptions'
* `#14429 <https://github.com/numpy/numpy/pull/14429>`__: BUG: Do not show Override module in private error classes.
* `#14444 <https://github.com/numpy/numpy/pull/14444>`__: DOC: Make implementation bullet points consistent in NEP 29
* `#14447 <https://github.com/numpy/numpy/pull/14447>`__: MAINT: Clarify policy language in NEP-29.
* `#14448 <https://github.com/numpy/numpy/pull/14448>`__: REL: Update master after 1.17.2 release.
* `#14452 <https://github.com/numpy/numpy/pull/14452>`__: MAINT: clean up pocketfft modules inside numpy.fft namespace
* `#14453 <https://github.com/numpy/numpy/pull/14453>`__: BLD: remove generated Cython files from sdist
* `#14454 <https://github.com/numpy/numpy/pull/14454>`__: MAINT: add test to prevent new public-looking modules being added
* `#14458 <https://github.com/numpy/numpy/pull/14458>`__: BUG: random.hypergeometic assumes npy_long is npy_int64, hangs...
* `#14459 <https://github.com/numpy/numpy/pull/14459>`__: ENH: Print the amount of memory that would be used by a failed...
* `#14460 <https://github.com/numpy/numpy/pull/14460>`__: MAINT: use test_requirements.txt in tox and shippable, ship it...
* `#14464 <https://github.com/numpy/numpy/pull/14464>`__: BUG: add a specialized loop for boolean matmul
* `#14469 <https://github.com/numpy/numpy/pull/14469>`__: BUG: Fix _ctypes class circular reference. (#13808)
* `#14472 <https://github.com/numpy/numpy/pull/14472>`__: BUG: core: Fix the str function of the rational dtype.
* `#14475 <https://github.com/numpy/numpy/pull/14475>`__: DOC: add timedelta64 signature
* `#14477 <https://github.com/numpy/numpy/pull/14477>`__: MAINT: Extract raising of MemoryError to a helper function
* `#14483 <https://github.com/numpy/numpy/pull/14483>`__: BUG,MAINT: Some fixes and minor cleanup based on clang analysis
* `#14484 <https://github.com/numpy/numpy/pull/14484>`__: MAINT: Add ``NPY_UNUSED`` and ``const`` qualified suggested by clang
* `#14485 <https://github.com/numpy/numpy/pull/14485>`__: MAINT: Silence integer comparison build warnings in assert statements
* `#14486 <https://github.com/numpy/numpy/pull/14486>`__: MAINT: distutils: Add newline at the end of printed warnings.
* `#14490 <https://github.com/numpy/numpy/pull/14490>`__: BUG: random: Revert gh-14458 and refix gh-14557.
* `#14493 <https://github.com/numpy/numpy/pull/14493>`__: DOC: Fix reference NPY_ARRAY_OWNDATA instead of NPY_OWNDATA.
* `#14495 <https://github.com/numpy/numpy/pull/14495>`__: ENH: Allow NPY_PKG_CONFIG_PATH environment variable override
* `#14498 <https://github.com/numpy/numpy/pull/14498>`__: MAINT: remove the entropy c-extension module
* `#14499 <https://github.com/numpy/numpy/pull/14499>`__: DOC: Add backslashes so PyUFunc_FromFuncAndDataAndSignatureAndIdentity...
* `#14500 <https://github.com/numpy/numpy/pull/14500>`__: DOC: Fix a minor typo in changelog readme
* `#14501 <https://github.com/numpy/numpy/pull/14501>`__: BUG: Fix randint when range is 2**32
* `#14503 <https://github.com/numpy/numpy/pull/14503>`__: DOC: tweak np.round docstring to clarify floating-point error
* `#14508 <https://github.com/numpy/numpy/pull/14508>`__: DOC: Add warning to NPV function
* `#14510 <https://github.com/numpy/numpy/pull/14510>`__: API: Do not return None from recfunctions.drop_fields
* `#14511 <https://github.com/numpy/numpy/pull/14511>`__: BUG: Fix flatten_dtype so that nested 0-field structs are flattened...
* `#14514 <https://github.com/numpy/numpy/pull/14514>`__: DOC: Build release notes during CircleCI step
* `#14518 <https://github.com/numpy/numpy/pull/14518>`__: BUILD: Hide platform configuration probe behind --debug-configure
* `#14520 <https://github.com/numpy/numpy/pull/14520>`__: Mention that split() returns views into the original array
* `#14521 <https://github.com/numpy/numpy/pull/14521>`__: MAINT: Simplify lookfor function
* `#14523 <https://github.com/numpy/numpy/pull/14523>`__: MAINT: random: Remove a few duplicated C function prototypes.
* `#14525 <https://github.com/numpy/numpy/pull/14525>`__: BUILD, MAINT: run tests with verbose for PyPY, also do not leak...
* `#14526 <https://github.com/numpy/numpy/pull/14526>`__: BUG: fix release snippet failures caught only after merging
* `#14527 <https://github.com/numpy/numpy/pull/14527>`__: BLD: add warn-error option, adds -Werror to compiler
* `#14531 <https://github.com/numpy/numpy/pull/14531>`__: BUG: random: Create a legacy implementation of random.binomial.
* `#14534 <https://github.com/numpy/numpy/pull/14534>`__: MAINT: remove unused functions, rearrange headers (from CC=clang)
* `#14535 <https://github.com/numpy/numpy/pull/14535>`__: DOC: Fix a bit of code in 'Beyond the Basics' C API user guide.
* `#14536 <https://github.com/numpy/numpy/pull/14536>`__: MAINT: Cleanup old_defines in DOC
* `#14540 <https://github.com/numpy/numpy/pull/14540>`__: DOC: Added missing versionadded to diff(prepend)
* `#14543 <https://github.com/numpy/numpy/pull/14543>`__: BUG: Avoid ctypes in Generators
* `#14545 <https://github.com/numpy/numpy/pull/14545>`__: Changing ImportWarning to DeprecationWarning
* `#14548 <https://github.com/numpy/numpy/pull/14548>`__: MAINT: handle case where GIT_VERSION is empty string
* `#14554 <https://github.com/numpy/numpy/pull/14554>`__: MAINT: core: Remove duplicated inner loop ee->e from log, exp,...
* `#14555 <https://github.com/numpy/numpy/pull/14555>`__: DOC: clarify input types in basics.io.genfromtxt.rst
* `#14557 <https://github.com/numpy/numpy/pull/14557>`__: DOC: remove note about Pocketfft license file (non-existing here).
* `#14558 <https://github.com/numpy/numpy/pull/14558>`__: DOC: Fix code that generates the table in the 'Casting Rules'...
* `#14562 <https://github.com/numpy/numpy/pull/14562>`__: MAINT: don't install partial numpy.random C/Cython API.
* `#14564 <https://github.com/numpy/numpy/pull/14564>`__: TST: ensure coercion tables aren't printed on failing public...
* `#14567 <https://github.com/numpy/numpy/pull/14567>`__: DEP: remove deprecated (and private) numpy.testing submodules.
* `#14568 <https://github.com/numpy/numpy/pull/14568>`__: BLD, DOC: fix gh-14518, add release note
* `#14570 <https://github.com/numpy/numpy/pull/14570>`__: BUG: importing build_src breaks setuptools monkeypatch for msvc14
* `#14572 <https://github.com/numpy/numpy/pull/14572>`__: DOC: Note runtests.py ``-- -s`` method to use pytests ``-s``
* `#14573 <https://github.com/numpy/numpy/pull/14573>`__: DOC: update submodule docstrings, remove info.py files
* `#14576 <https://github.com/numpy/numpy/pull/14576>`__: DOC: Document the NPY_SCALARKIND values as C variables.
* `#14582 <https://github.com/numpy/numpy/pull/14582>`__: MAINT: Bump pytest from 5.1.2 to 5.1.3
* `#14583 <https://github.com/numpy/numpy/pull/14583>`__: DEP: remove deprecated select behaviour
* `#14585 <https://github.com/numpy/numpy/pull/14585>`__: BUG: Add missing check for 0-sized array in ravel_multi_index
* `#14586 <https://github.com/numpy/numpy/pull/14586>`__: BUG: dtype refcount cleanups
* `#14587 <https://github.com/numpy/numpy/pull/14587>`__: DOC: Fix a minor typo in changelog entry
* `#14592 <https://github.com/numpy/numpy/pull/14592>`__: MAINT: Fix typo: remoge → remove
* `#14595 <https://github.com/numpy/numpy/pull/14595>`__: DOC: Change the promotion table checkmark to 'Y'.
* `#14596 <https://github.com/numpy/numpy/pull/14596>`__: DEP: Complete deprecation of invalid array/memory order
* `#14598 <https://github.com/numpy/numpy/pull/14598>`__: DOC: Add to doc that interp cannot contain NaN
* `#14600 <https://github.com/numpy/numpy/pull/14600>`__: NEP: Accept NEP 32.
* `#14601 <https://github.com/numpy/numpy/pull/14601>`__: NEP: Fix discrepancies in NEPs
* `#14603 <https://github.com/numpy/numpy/pull/14603>`__: NEP: Only list "Active" NEPs under "Meta-NEPs"
* `#14604 <https://github.com/numpy/numpy/pull/14604>`__: API: restructure and document numpy.random C-API
* `#14605 <https://github.com/numpy/numpy/pull/14605>`__: BUG: properly define PyArray_DescrCheck{,Exact}
* `#14607 <https://github.com/numpy/numpy/pull/14607>`__: MAINT: Remove duplicate files from .gitignore
* `#14608 <https://github.com/numpy/numpy/pull/14608>`__: API: rearrange the cython files in numpy.random
* `#14614 <https://github.com/numpy/numpy/pull/14614>`__: MAINT: Bump pytest from 5.1.3 to 5.2.0
* `#14615 <https://github.com/numpy/numpy/pull/14615>`__: MAINT: Add "MAINT" tag to dependabot commit msg
* `#14616 <https://github.com/numpy/numpy/pull/14616>`__: DOC: Updated sphinx directive formatting
* `#14620 <https://github.com/numpy/numpy/pull/14620>`__: DEP: Finish deprecation of non-integer ``num`` in linspace
* `#14621 <https://github.com/numpy/numpy/pull/14621>`__: DOC: s/OR/AND/ in np.logical_and docstring
* `#14623 <https://github.com/numpy/numpy/pull/14623>`__: DOC: misleading np.sinc() documentation
* `#14629 <https://github.com/numpy/numpy/pull/14629>`__: DOC: clarify residual in np.polyfit
* `#14630 <https://github.com/numpy/numpy/pull/14630>`__: BUILD: change to build_src --verbose-cfg, runtests.py --debug-info
* `#14631 <https://github.com/numpy/numpy/pull/14631>`__: BUG: always free clean_sep
* `#14634 <https://github.com/numpy/numpy/pull/14634>`__: DOC: Create ``class Extension`` docstring and add it to documentation.
* `#14636 <https://github.com/numpy/numpy/pull/14636>`__: DOC: add ``printoptions`` as a context manager to ``set_printoptions``
* `#14639 <https://github.com/numpy/numpy/pull/14639>`__: DOC: Fix typo in NEP 29
* `#14643 <https://github.com/numpy/numpy/pull/14643>`__: MAINT: Use scalar math power function directly
* `#14649 <https://github.com/numpy/numpy/pull/14649>`__: DOC: Add IPython to dependencies needed to build docs.
* `#14652 <https://github.com/numpy/numpy/pull/14652>`__: MAINT: Bump pytest-cov from 2.7.1 to 2.8.1
* `#14653 <https://github.com/numpy/numpy/pull/14653>`__: MAINT: Bump pytest from 5.2.0 to 5.2.1
* `#14654 <https://github.com/numpy/numpy/pull/14654>`__: MAINT: Bump pytz from 2019.2 to 2019.3
* `#14656 <https://github.com/numpy/numpy/pull/14656>`__: MAINT: Use ``extract_unit`` throughout datetime
* `#14657 <https://github.com/numpy/numpy/pull/14657>`__: BUG: fix fromfile behavior when reading sub-array dtypes
* `#14662 <https://github.com/numpy/numpy/pull/14662>`__: BUG: random: Use correct length when axis is given to shuffle.
* `#14669 <https://github.com/numpy/numpy/pull/14669>`__: BUG: Do not rely on undefined behaviour to cast from float to...
* `#14674 <https://github.com/numpy/numpy/pull/14674>`__: NEP: add default-dtype-object-deprecation nep 34
* `#14681 <https://github.com/numpy/numpy/pull/14681>`__: MAINT: Remove unused boolean negative/subtract loops
* `#14682 <https://github.com/numpy/numpy/pull/14682>`__: DEP: ufunc ``out`` argument must be a tuple for multiple outputs
* `#14693 <https://github.com/numpy/numpy/pull/14693>`__: BUG: Fix ``np.einsum`` errors on Power9 Linux and z/Linux
* `#14696 <https://github.com/numpy/numpy/pull/14696>`__: DOC: Note release notes process changes on devdocs start page
* `#14699 <https://github.com/numpy/numpy/pull/14699>`__: Doc warnings
* `#14703 <https://github.com/numpy/numpy/pull/14703>`__: TST: Adding CI stages, with one initial job to the Travis CI
* `#14705 <https://github.com/numpy/numpy/pull/14705>`__: DOC: Switch Markdown link to RST in NEP 29
* `#14709 <https://github.com/numpy/numpy/pull/14709>`__: TST: Divide Azure CI Pipelines into stages.
* `#14710 <https://github.com/numpy/numpy/pull/14710>`__: DEP: Finish the out kwarg deprecation for ufunc calls
* `#14711 <https://github.com/numpy/numpy/pull/14711>`__: DOC: Removing mentions of appveyor
* `#14714 <https://github.com/numpy/numpy/pull/14714>`__: BUG: Default start to 0 for timedelta arange
* `#14717 <https://github.com/numpy/numpy/pull/14717>`__: API: NaT (arg)min/max behavior
* `#14718 <https://github.com/numpy/numpy/pull/14718>`__: API: Forbid Q<->m safe casting
* `#14720 <https://github.com/numpy/numpy/pull/14720>`__: DEP: deprecate financial functions.
* `#14721 <https://github.com/numpy/numpy/pull/14721>`__: DOC: Move newsfragment to correct folder
* `#14723 <https://github.com/numpy/numpy/pull/14723>`__: DOC: cleaning up examples in maskedarray.generic
* `#14725 <https://github.com/numpy/numpy/pull/14725>`__: MAINT: umath: Change error message for unsupported bool subtraction.
* `#14730 <https://github.com/numpy/numpy/pull/14730>`__: ENH: Add complex number support for fromfile
* `#14732 <https://github.com/numpy/numpy/pull/14732>`__: TST: run refguide-check on rst files in doc/*
* `#14734 <https://github.com/numpy/numpy/pull/14734>`__: DOC: Edit NEP procedure for better discussion
* `#14736 <https://github.com/numpy/numpy/pull/14736>`__: DOC: Post 1.17.3 release update.
* `#14737 <https://github.com/numpy/numpy/pull/14737>`__: NEP: Accept NEP 29 as final
* `#14738 <https://github.com/numpy/numpy/pull/14738>`__: BUG: Don't narrow intp to int when producing error messages
* `#14742 <https://github.com/numpy/numpy/pull/14742>`__: DOC: lib: Fix deprecation markup in financial function docstrings.
* `#14743 <https://github.com/numpy/numpy/pull/14743>`__: DOC: Change from HTTP to HTTPS
* `#14745 <https://github.com/numpy/numpy/pull/14745>`__: BUG: clear only attribute errors in get_attr_string.h::maybe_get_attr
* `#14762 <https://github.com/numpy/numpy/pull/14762>`__: MAINT: doc: Remove doc/newdtype_example/
* `#14763 <https://github.com/numpy/numpy/pull/14763>`__: Reword cautionary note about dtype.descr
* `#14769 <https://github.com/numpy/numpy/pull/14769>`__: BUG: fix integer size confusion in handling array's ndmin argument
* `#14771 <https://github.com/numpy/numpy/pull/14771>`__: TST, BUILD: add a gcc 4.8 run on ubuntu 18.04
* `#14775 <https://github.com/numpy/numpy/pull/14775>`__: Update CLASSIFIERS with python 3.8 support
* `#14777 <https://github.com/numpy/numpy/pull/14777>`__: BUG: random: biased samples from integers() with 8 or 16 bit...
* `#14782 <https://github.com/numpy/numpy/pull/14782>`__: DOC: Add release note about changed random variate stream from...
* `#14786 <https://github.com/numpy/numpy/pull/14786>`__: DOC: Make changes to NEP procedure
* `#14790 <https://github.com/numpy/numpy/pull/14790>`__: DOC: random: Remove redundant 'See Also' entry in 'uniform' docstring.
* `#14791 <https://github.com/numpy/numpy/pull/14791>`__: MAINT: Minor typo fix
* `#14792 <https://github.com/numpy/numpy/pull/14792>`__: MAINT: Bump pytest from 5.2.1 to 5.2.2
* `#14793 <https://github.com/numpy/numpy/pull/14793>`__: DOC: Adjust NEP-31 to new template.
* `#14794 <https://github.com/numpy/numpy/pull/14794>`__: DEP: issue deprecation warning when creating ragged array (NEP...
* `#14798 <https://github.com/numpy/numpy/pull/14798>`__: NEP: move 'NEP 29 random' from Accepted to Final
* `#14799 <https://github.com/numpy/numpy/pull/14799>`__: DOC: Add take_along_axis to the see also section in argmin, argmax...
* `#14800 <https://github.com/numpy/numpy/pull/14800>`__: ENH: change object-array comparisons to prefer OO->O unfuncs
* `#14805 <https://github.com/numpy/numpy/pull/14805>`__: TST: Don't construct Fraction instances from numpy scalars
* `#14814 <https://github.com/numpy/numpy/pull/14814>`__: Rename helper functions to not use the word rank
* `#14820 <https://github.com/numpy/numpy/pull/14820>`__: MAINT: Use templating to merge float loops
* `#14826 <https://github.com/numpy/numpy/pull/14826>`__: BUILD: ignore more build.log warnings
* `#14827 <https://github.com/numpy/numpy/pull/14827>`__: BLD: Prevent -flto from optimising long double representation...
* `#14829 <https://github.com/numpy/numpy/pull/14829>`__: BUG: raise ValueError for empty arrays passed to _pyarray_correlate
* `#14830 <https://github.com/numpy/numpy/pull/14830>`__: MAINT: move buffer.h -> npy_buffer.h to avoid conflicts
* `#14836 <https://github.com/numpy/numpy/pull/14836>`__: MAINT: Bump cython from 0.29.13 to 0.29.14
* `#14841 <https://github.com/numpy/numpy/pull/14841>`__: ENH: add isinf, isnan, fmin, fmax loops for datetime64, timedelta64
* `#14842 <https://github.com/numpy/numpy/pull/14842>`__: BLD: add 'apt update' to shippable
* `#14845 <https://github.com/numpy/numpy/pull/14845>`__: MAINT: revert gh-14800, which gave precedence to OO->O over OO->?
* `#14874 <https://github.com/numpy/numpy/pull/14874>`__: REL: Update master after 1.17.4 release.
* `#14878 <https://github.com/numpy/numpy/pull/14878>`__: BUILD: remove SSE2 flag from numpy.random builds
* `#14879 <https://github.com/numpy/numpy/pull/14879>`__: DOC: Update NEP29 with Python3.8 informations.
* `#14881 <https://github.com/numpy/numpy/pull/14881>`__: BUG: Remove builtins from __all__
* `#14898 <https://github.com/numpy/numpy/pull/14898>`__: MAINT: Delete and ignore generated files
* `#14899 <https://github.com/numpy/numpy/pull/14899>`__: Update FUNDING.yml
* `#14901 <https://github.com/numpy/numpy/pull/14901>`__: MAINT: Remove uses of scalar aliases
* `#14903 <https://github.com/numpy/numpy/pull/14903>`__: NEP: move nep 34 to accepted
* `#14907 <https://github.com/numpy/numpy/pull/14907>`__: TST: Add s390x to the TravisCI test matrix.
* `#14912 <https://github.com/numpy/numpy/pull/14912>`__: DOC: Note FFT type promotion
* `#14914 <https://github.com/numpy/numpy/pull/14914>`__: TST: Test with Python3.8 on Windows.
* `#14915 <https://github.com/numpy/numpy/pull/14915>`__: TST: Update travis.yml
* `#14921 <https://github.com/numpy/numpy/pull/14921>`__: TST: add no_tracing decorator to refcount-sensitive codepath...
* `#14926 <https://github.com/numpy/numpy/pull/14926>`__: MAINT: Bump pytest from 5.2.2 to 5.2.4
* `#14929 <https://github.com/numpy/numpy/pull/14929>`__: BUG: Fix step returned by linspace when num=1 and endpoint=False
* `#14932 <https://github.com/numpy/numpy/pull/14932>`__: DOC: Compare 'tolist' function to 'list' in example
* `#14935 <https://github.com/numpy/numpy/pull/14935>`__: DOC: Clarify return type for default_rng
* `#14944 <https://github.com/numpy/numpy/pull/14944>`__: MAINT: move numpy/random/examples -> numpy/random/_examples
* `#14947 <https://github.com/numpy/numpy/pull/14947>`__: DOC: testing: Note handling of scalars in assert_array_equal...
* `#14948 <https://github.com/numpy/numpy/pull/14948>`__: DOC, API: add random.__init__.pxd and document random.* functions
* `#14951 <https://github.com/numpy/numpy/pull/14951>`__: DOC: Clean up examples of low-level random access
* `#14954 <https://github.com/numpy/numpy/pull/14954>`__: TST. API: test using distributions.h via cffi
* `#14962 <https://github.com/numpy/numpy/pull/14962>`__: TST: skip if cython is not available
* `#14967 <https://github.com/numpy/numpy/pull/14967>`__: MAINT: Cleaned up mintypecode for Py3
* `#14973 <https://github.com/numpy/numpy/pull/14973>`__: DOC: fix docstring of np.linalg.norm
* `#14974 <https://github.com/numpy/numpy/pull/14974>`__: MAINT: Added Python3.8 branch to dll lib discovery on Windows
* `#14976 <https://github.com/numpy/numpy/pull/14976>`__: DEV: update asv.conf.json
* `#14978 <https://github.com/numpy/numpy/pull/14978>`__: MAINT: Bump pytest from 5.2.4 to 5.3.0
* `#14982 <https://github.com/numpy/numpy/pull/14982>`__: MAINT: Fix typos
* `#14983 <https://github.com/numpy/numpy/pull/14983>`__: REV: "ENH: Improved performance of PyArray_FromAny for sequences...
* `#14994 <https://github.com/numpy/numpy/pull/14994>`__: BUG: warn when saving dtype with metadata
* `#14996 <https://github.com/numpy/numpy/pull/14996>`__: DEP: Deprecate the axis argument to masked_rows and masked_cols
* `#15004 <https://github.com/numpy/numpy/pull/15004>`__: MAINT: Fix long name of PCG64
* `#15007 <https://github.com/numpy/numpy/pull/15007>`__: DOC, API: improve the C-API/Cython documentation and interfaces...
* `#15009 <https://github.com/numpy/numpy/pull/15009>`__: DOC: Fix typo in numpy.loadtxt and numpy.genfromtxt documentation
* `#15012 <https://github.com/numpy/numpy/pull/15012>`__: ENH: allow using symbol-suffixed 64-bit BLAS/LAPACK for numpy.dot...
* `#15014 <https://github.com/numpy/numpy/pull/15014>`__: DOC: add a more useful comment to compat.py3k.py
* `#15019 <https://github.com/numpy/numpy/pull/15019>`__: DOC: lib: Use a clearer example of ddof in the notes of the cov...
* `#15021 <https://github.com/numpy/numpy/pull/15021>`__: TST: machinery for tests requiring large memory + lapack64 smoketest
* `#15023 <https://github.com/numpy/numpy/pull/15023>`__: MAINT: Only copy input array in _replace_nan() if there are nans...
* `#15025 <https://github.com/numpy/numpy/pull/15025>`__: MAINT: Bump pytest from 5.3.0 to 5.3.1
* `#15027 <https://github.com/numpy/numpy/pull/15027>`__: REV: "ENH: Improved performance of PyArray_FromAny for sequences...
* `#15031 <https://github.com/numpy/numpy/pull/15031>`__: REL: Prepare for 1.18 branch
* `#15032 <https://github.com/numpy/numpy/pull/15032>`__: MAINT: Cleaned up mintypecode for Py3 (pt. 2)
* `#15036 <https://github.com/numpy/numpy/pull/15036>`__: BUG: Fix refcounting in ufunc object loops
* `#15039 <https://github.com/numpy/numpy/pull/15039>`__: BUG: Exceptions tracebacks are dropped
* `#15053 <https://github.com/numpy/numpy/pull/15053>`__: REV: Revert "Merge pull request #14794 from mattip/nep-0034-impl"
* `#15058 <https://github.com/numpy/numpy/pull/15058>`__: API, DOC: change names to multivariate_hypergeometric, improve docs
* `#15059 <https://github.com/numpy/numpy/pull/15059>`__: REL: Prepare for NumPy 1.18.0 release.
* `#15109 <https://github.com/numpy/numpy/pull/15109>`__: TST: Check requires_memory immediately before the test
* `#15111 <https://github.com/numpy/numpy/pull/15111>`__: ENH: Add support to sort timedelta64 ``NaT`` to end of the array
* `#15112 <https://github.com/numpy/numpy/pull/15112>`__: MAINT: follow-up cleanup for blas64 PR
* `#15113 <https://github.com/numpy/numpy/pull/15113>`__: ENH: f2py: add --f2cmap option for specifying the name of .f2py_f2cmap
* `#15114 <https://github.com/numpy/numpy/pull/15114>`__: ENH: add support for ILP64 OpenBLAS (without symbol suffix)
* `#15146 <https://github.com/numpy/numpy/pull/15146>`__: REL: Prepare for 1.18.0 release.
=========
Changelog
=========

Contributors
============

A total of 139 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aditya Panchal +
* Ales Erjavec +
* Alex Griffing
* Alexandr Shadchin +
* Alistair Muldal
* Allan Haldane
* Amit Aronovitch +
* Andrei Kucharavy +
* Antony Lee
* Antti Kaihola +
* Arne de Laat +
* Auke Wiggers +
* AustereCuriosity +
* Badhri Narayanan Krishnakumar +
* Ben North +
* Ben Rowland +
* Bertrand Lefebvre
* Boxiang Sun
* CJ Carey
* Charles Harris
* Christoph Gohlke
* Daniel Ching +
* Daniel Rasmussen +
* Daniel Smith +
* David Schaich +
* Denis Alevi +
* Devin Jeanpierre +
* Dmitry Odzerikho
* Dongjoon Hyun +
* Edward Richards +
* Ekaterina Tuzova +
* Emilien Kofman +
* Endolith
* Eren Sezener +
* Eric Moore
* Eric Quintero +
* Eric Wieser +
* Erik M. Bray
* Frederic Bastien
* Friedrich Dunne +
* Gerrit Holl
* Golnaz Irannejad +
* Graham Markall +
* Greg Knoll +
* Greg Young
* Gustavo Serra Scalet +
* Ines Wichert +
* Irvin Probst +
* Jaime Fernandez
* James Sanders +
* Jan David Mol +
* Jan Schlüter
* Jeremy Tuloup +
* John Kirkham
* John Zwinck +
* Jonathan Helmus
* Joseph Fox-Rabinovitz
* Josh Wilson +
* Joshua Warner +
* Julian Taylor
* Ka Wo Chen +
* Kamil Rytarowski +
* Kelsey Jordahl +
* Kevin Deldycke +
* Khaled Ben Abdallah Okuda +
* Lion Krischer +
* Loïc Estève +
* Luca Mussi +
* Mads Ohm Larsen +
* Manoj Kumar +
* Mario Emmenlauer +
* Marshall Bockrath-Vandegrift +
* Marshall Ward +
* Marten van Kerkwijk
* Mathieu Lamarre +
* Matthew Brett
* Matthew Harrigan +
* Matthias Geier
* Matti Picus +
* Meet Udeshi +
* Michael Felt +
* Michael Goerz +
* Michael Martin +
* Michael Seifert +
* Mike Nolta +
* Nathaniel Beaver +
* Nathaniel J. Smith
* Naveen Arunachalam +
* Nick Papior
* Nikola Forró +
* Oleksandr Pavlyk +
* Olivier Grisel
* Oren Amsalem +
* Pauli Virtanen
* Pavel Potocek +
* Pedro Lacerda +
* Peter Creasey +
* Phil Elson +
* Philip Gura +
* Phillip J. Wolfram +
* Pierre de Buyl +
* Raghav RV +
* Ralf Gommers
* Ray Donnelly +
* Rehas Sachdeva
* Rob Malouf +
* Robert Kern
* Samuel St-Jean
* Sanchez Gonzalez Alvaro +
* Saurabh Mehta +
* Scott Sanderson +
* Sebastian Berg
* Shayan Pooya +
* Shota Kawabuchi +
* Simon Conseil
* Simon Gibbons
* Sorin Sbarnea +
* Stefan van der Walt
* Stephan Hoyer
* Steven J Kern +
* Stuart Archibald
* Tadeu Manoel +
* Takuya Akiba +
* Thomas A Caswell
* Tom Bird +
* Tony Kelman +
* Toshihiro Kamishima +
* Valentin Valls +
* Varun Nayyar
* Victor Stinner +
* Warren Weckesser
* Wendell Smith
* Wojtek Ruszczewski +
* Xavier Abellan Ecija +
* Yaroslav Halchenko
* Yash Shah +
* Yinon Ehrlich +
* Yu Feng +
* nevimov +

Pull requests merged
====================

A total of 418 pull requests were merged for this release.

* `#4073 <https://github.com/numpy/numpy/pull/4073>`__: BUG: change real output checking to test if all imaginary parts...
* `#4619 <https://github.com/numpy/numpy/pull/4619>`__: BUG : np.sum silently drops keepdims for sub-classes of ndarray
* `#5488 <https://github.com/numpy/numpy/pull/5488>`__: ENH: add `contract`: optimizing numpy's einsum expression
* `#5706 <https://github.com/numpy/numpy/pull/5706>`__: ENH: make some masked array methods behave more like ndarray...
* `#5822 <https://github.com/numpy/numpy/pull/5822>`__: Allow many distributions to have a scale of 0.
* `#6054 <https://github.com/numpy/numpy/pull/6054>`__: WIP: MAINT: Add deprecation warning to views of multi-field indexes
* `#6298 <https://github.com/numpy/numpy/pull/6298>`__: Check lower base limit in base_repr.
* `#6430 <https://github.com/numpy/numpy/pull/6430>`__: Fix issues with zero-width string fields
* `#6656 <https://github.com/numpy/numpy/pull/6656>`__: ENH: usecols now accepts an int when only one column has to be...
* `#6660 <https://github.com/numpy/numpy/pull/6660>`__: Added pathlib support for several functions
* `#6872 <https://github.com/numpy/numpy/pull/6872>`__: ENH: linear interpolation of complex values in lib.interp
* `#6997 <https://github.com/numpy/numpy/pull/6997>`__: MAINT: Simplify mtrand.pyx helpers
* `#7003 <https://github.com/numpy/numpy/pull/7003>`__: BUG: Fix string copying for np.place
* `#7026 <https://github.com/numpy/numpy/pull/7026>`__: DOC: Clarify behavior in np.random.uniform
* `#7055 <https://github.com/numpy/numpy/pull/7055>`__: BUG: One Element Array Inputs Return Scalars in np.random
* `#7063 <https://github.com/numpy/numpy/pull/7063>`__: REL: Update master branch after 1.11.x branch has been made.
* `#7073 <https://github.com/numpy/numpy/pull/7073>`__: DOC: Update the 1.11.0 release notes.
* `#7076 <https://github.com/numpy/numpy/pull/7076>`__: MAINT: Update the git .mailmap file.
* `#7082 <https://github.com/numpy/numpy/pull/7082>`__: TST, DOC: Added Broadcasting Tests in test_random.py
* `#7087 <https://github.com/numpy/numpy/pull/7087>`__: BLD: fix compilation on non glibc-Linuxes
* `#7088 <https://github.com/numpy/numpy/pull/7088>`__: BUG: Have `norm` cast non-floating point arrays to 64-bit float...
* `#7090 <https://github.com/numpy/numpy/pull/7090>`__: ENH: Added 'doane' and 'sqrt' estimators to np.histogram in numpy.function_base
* `#7091 <https://github.com/numpy/numpy/pull/7091>`__: Revert "BLD: fix compilation on non glibc-Linuxes"
* `#7092 <https://github.com/numpy/numpy/pull/7092>`__: BLD: fix compilation on non glibc-Linuxes
* `#7099 <https://github.com/numpy/numpy/pull/7099>`__: TST: Suppressed warnings
* `#7102 <https://github.com/numpy/numpy/pull/7102>`__: MAINT: Removed conditionals that are always false in datetime_strings.c
* `#7105 <https://github.com/numpy/numpy/pull/7105>`__: DEP: Deprecate as_strided returning a writable array as default
* `#7109 <https://github.com/numpy/numpy/pull/7109>`__: DOC: update Python versions requirements in the install docs
* `#7114 <https://github.com/numpy/numpy/pull/7114>`__: MAINT: Fix typos in docs
* `#7116 <https://github.com/numpy/numpy/pull/7116>`__: TST: Fixed f2py test for win32 virtualenv
* `#7118 <https://github.com/numpy/numpy/pull/7118>`__: TST: Fixed f2py test for non-versioned python executables
* `#7119 <https://github.com/numpy/numpy/pull/7119>`__: BUG: Fixed mingw.lib error
* `#7125 <https://github.com/numpy/numpy/pull/7125>`__: DOC: Updated documentation wording and examples for np.percentile.
* `#7129 <https://github.com/numpy/numpy/pull/7129>`__: BUG: Fixed 'midpoint' interpolation of np.percentile in odd cases.
* `#7131 <https://github.com/numpy/numpy/pull/7131>`__: Fix setuptools sdist
* `#7133 <https://github.com/numpy/numpy/pull/7133>`__: ENH: savez: temporary file alongside with target file and improve...
* `#7134 <https://github.com/numpy/numpy/pull/7134>`__: MAINT: Fix some typos in a code string and comments
* `#7141 <https://github.com/numpy/numpy/pull/7141>`__: BUG: Unpickled void scalars should be contiguous
* `#7144 <https://github.com/numpy/numpy/pull/7144>`__: MAINT: Change `call_fortran` into `callfortran` in comments.
* `#7145 <https://github.com/numpy/numpy/pull/7145>`__: BUG: Fixed regressions in np.piecewise in ref to #5737 and #5729.
* `#7147 <https://github.com/numpy/numpy/pull/7147>`__: Temporarily disable __numpy_ufunc__
* `#7148 <https://github.com/numpy/numpy/pull/7148>`__: ENH,TST: Bump stacklevel and add tests for warnings
* `#7149 <https://github.com/numpy/numpy/pull/7149>`__: TST: Add missing suffix to temppath manager
* `#7152 <https://github.com/numpy/numpy/pull/7152>`__: BUG: mode kwargs passed as unicode to np.pad raises an exception
* `#7156 <https://github.com/numpy/numpy/pull/7156>`__: BUG: Reascertain that linspace respects ndarray subclasses in...
* `#7167 <https://github.com/numpy/numpy/pull/7167>`__: DOC: Update Wikipedia references for mtrand.pyx
* `#7171 <https://github.com/numpy/numpy/pull/7171>`__: TST: Fixed f2py test for Anaconda non-win32
* `#7174 <https://github.com/numpy/numpy/pull/7174>`__: DOC: Fix broken pandas link in release notes
* `#7177 <https://github.com/numpy/numpy/pull/7177>`__: ENH: added axis param for np.count_nonzero
* `#7178 <https://github.com/numpy/numpy/pull/7178>`__: BUG: Fix binary_repr for negative numbers
* `#7180 <https://github.com/numpy/numpy/pull/7180>`__: BUG: Fixed previous attempt to fix dimension mismatch in nanpercentile
* `#7181 <https://github.com/numpy/numpy/pull/7181>`__: DOC: Updated minor typos in function_base.py and test_function_base.py
* `#7191 <https://github.com/numpy/numpy/pull/7191>`__: DOC: add vstack, hstack, dstack reference to stack documentation.
* `#7193 <https://github.com/numpy/numpy/pull/7193>`__: MAINT: Removed supurious assert in histogram estimators
* `#7194 <https://github.com/numpy/numpy/pull/7194>`__: BUG: Raise a quieter `MaskedArrayFutureWarning` for mask changes.
* `#7195 <https://github.com/numpy/numpy/pull/7195>`__: STY: Drop some trailing spaces in `numpy.ma.core`.
* `#7196 <https://github.com/numpy/numpy/pull/7196>`__: Revert "DOC: add vstack, hstack, dstack reference to stack documentation."
* `#7197 <https://github.com/numpy/numpy/pull/7197>`__: TST: Pin virtualenv used on Travis CI.
* `#7198 <https://github.com/numpy/numpy/pull/7198>`__: ENH: Unlock the GIL for gufuncs
* `#7199 <https://github.com/numpy/numpy/pull/7199>`__: MAINT: Cleanup for histogram bin estimator selection
* `#7201 <https://github.com/numpy/numpy/pull/7201>`__: Raise IOError on not a file in python2
* `#7202 <https://github.com/numpy/numpy/pull/7202>`__: MAINT: Made `iterable` return a boolean
* `#7209 <https://github.com/numpy/numpy/pull/7209>`__: TST: Bump `virtualenv` to 14.0.6
* `#7211 <https://github.com/numpy/numpy/pull/7211>`__: DOC: Fix fmin examples
* `#7215 <https://github.com/numpy/numpy/pull/7215>`__: MAINT: Use PySlice_GetIndicesEx instead of custom reimplementation
* `#7229 <https://github.com/numpy/numpy/pull/7229>`__: ENH: implement __complex__
* `#7231 <https://github.com/numpy/numpy/pull/7231>`__: MRG: allow distributors to run custom init
* `#7232 <https://github.com/numpy/numpy/pull/7232>`__: BLD: Switch order of test for lapack_mkl and openblas_lapack
* `#7239 <https://github.com/numpy/numpy/pull/7239>`__: DOC: Removed residual merge markup from previous commit
* `#7240 <https://github.com/numpy/numpy/pull/7240>`__: Change 'pubic' to 'public'.
* `#7241 <https://github.com/numpy/numpy/pull/7241>`__: MAINT: update doc/sphinxext to numpydoc 0.6.0, and fix up some...
* `#7243 <https://github.com/numpy/numpy/pull/7243>`__: ENH: Adding support to the range keyword for estimation of the...
* `#7246 <https://github.com/numpy/numpy/pull/7246>`__: DOC: mention writeable keyword in as_strided in release notes
* `#7247 <https://github.com/numpy/numpy/pull/7247>`__: TST: Fail quickly on AppVeyor for superseded PR builds
* `#7248 <https://github.com/numpy/numpy/pull/7248>`__: DOC: remove link to documentation wiki editor from HOWTO_DOCUMENT.
* `#7250 <https://github.com/numpy/numpy/pull/7250>`__: DOC,REL: Update 1.11.0 notes.
* `#7251 <https://github.com/numpy/numpy/pull/7251>`__: BUG: only benchmark complex256 if it exists
* `#7252 <https://github.com/numpy/numpy/pull/7252>`__: Forward port a fix and enhancement from 1.11.x
* `#7253 <https://github.com/numpy/numpy/pull/7253>`__: DOC: note in h/v/dstack points users to stack/concatenate
* `#7254 <https://github.com/numpy/numpy/pull/7254>`__: BUG: Enforce dtype for randint singletons
* `#7256 <https://github.com/numpy/numpy/pull/7256>`__: MAINT: Use `is None` or `is not None` instead of `== None` or...
* `#7257 <https://github.com/numpy/numpy/pull/7257>`__: DOC: Fix mismatched variable names in docstrings.
* `#7258 <https://github.com/numpy/numpy/pull/7258>`__: ENH: Make numpy floor_divide and remainder agree with Python...
* `#7260 <https://github.com/numpy/numpy/pull/7260>`__: BUG/TST: Fix #7259, do not "force scalar" for already scalar...
* `#7261 <https://github.com/numpy/numpy/pull/7261>`__: Added self to mailmap
* `#7266 <https://github.com/numpy/numpy/pull/7266>`__: BUG: Segfault for classes with deceptive __len__
* `#7268 <https://github.com/numpy/numpy/pull/7268>`__: ENH: add geomspace function
* `#7274 <https://github.com/numpy/numpy/pull/7274>`__: BUG: Preserve array order in np.delete
* `#7275 <https://github.com/numpy/numpy/pull/7275>`__: DEP: Warn about assigning 'data' attribute of ndarray
* `#7276 <https://github.com/numpy/numpy/pull/7276>`__: DOC: apply_along_axis missing whitespace inserted (before colon)
* `#7278 <https://github.com/numpy/numpy/pull/7278>`__: BUG: Make returned unravel_index arrays writeable
* `#7279 <https://github.com/numpy/numpy/pull/7279>`__: TST: Fixed elements being shuffled
* `#7280 <https://github.com/numpy/numpy/pull/7280>`__: MAINT: Remove redundant trailing semicolons.
* `#7285 <https://github.com/numpy/numpy/pull/7285>`__: BUG: Make Randint Backwards Compatible with Pandas
* `#7286 <https://github.com/numpy/numpy/pull/7286>`__: MAINT: Fix typos in docs/comments of `ma` and `polynomial` modules.
* `#7292 <https://github.com/numpy/numpy/pull/7292>`__: Clarify error on repr failure in assert_equal.
* `#7294 <https://github.com/numpy/numpy/pull/7294>`__: ENH: add support for BLIS to numpy.distutils
* `#7295 <https://github.com/numpy/numpy/pull/7295>`__: DOC: understanding code and getting started section to dev doc
* `#7296 <https://github.com/numpy/numpy/pull/7296>`__: Revert part of #3907 which incorrectly propagated MaskedArray...
* `#7299 <https://github.com/numpy/numpy/pull/7299>`__: DOC: Fix mismatched variable names in docstrings.
* `#7300 <https://github.com/numpy/numpy/pull/7300>`__: DOC: dev: stop recommending keeping local master updated with...
* `#7301 <https://github.com/numpy/numpy/pull/7301>`__: DOC: Update release notes
* `#7305 <https://github.com/numpy/numpy/pull/7305>`__: BUG: Remove data race in mtrand: two threads could mutate the...
* `#7307 <https://github.com/numpy/numpy/pull/7307>`__: DOC: Missing some characters in link.
* `#7308 <https://github.com/numpy/numpy/pull/7308>`__: BUG: Incrementing the wrong reference on return
* `#7310 <https://github.com/numpy/numpy/pull/7310>`__: STY: Fix GitHub rendering of ordered lists >9
* `#7311 <https://github.com/numpy/numpy/pull/7311>`__: ENH: Make _pointer_type_cache functional
* `#7313 <https://github.com/numpy/numpy/pull/7313>`__: DOC: corrected grammatical error in quickstart doc
* `#7325 <https://github.com/numpy/numpy/pull/7325>`__: BUG, MAINT: Improve fromnumeric.py interface for downstream compatibility
* `#7328 <https://github.com/numpy/numpy/pull/7328>`__: DEP: Deprecated using a float index in linspace
* `#7331 <https://github.com/numpy/numpy/pull/7331>`__: Add comment, TST: fix MemoryError on win32
* `#7332 <https://github.com/numpy/numpy/pull/7332>`__: Check for no solution in np.irr Fixes #6744
* `#7338 <https://github.com/numpy/numpy/pull/7338>`__: TST: Install `pytz` in the CI.
* `#7340 <https://github.com/numpy/numpy/pull/7340>`__: DOC: Fixed math rendering in tensordot docs.
* `#7341 <https://github.com/numpy/numpy/pull/7341>`__: TST: Add test for #6469
* `#7344 <https://github.com/numpy/numpy/pull/7344>`__: DOC: Fix more typos in docs and comments.
* `#7346 <https://github.com/numpy/numpy/pull/7346>`__: Generalized flip
* `#7347 <https://github.com/numpy/numpy/pull/7347>`__: ENH Generalized rot90
* `#7348 <https://github.com/numpy/numpy/pull/7348>`__: Maint: Removed extra space from `ureduce`
* `#7349 <https://github.com/numpy/numpy/pull/7349>`__: MAINT: Hide nan warnings for masked internal MA computations
* `#7350 <https://github.com/numpy/numpy/pull/7350>`__: BUG: MA ufuncs should set mask to False, not array([False])
* `#7351 <https://github.com/numpy/numpy/pull/7351>`__: TST: Fix some MA tests to avoid looking at the .data attribute
* `#7358 <https://github.com/numpy/numpy/pull/7358>`__: BUG: pull request related to the issue #7353
* `#7359 <https://github.com/numpy/numpy/pull/7359>`__: Update 7314, DOC: Clarify valid integer range for random.seed...
* `#7361 <https://github.com/numpy/numpy/pull/7361>`__: MAINT: Fix copy and paste oversight.
* `#7363 <https://github.com/numpy/numpy/pull/7363>`__: ENH: Make no unshare mask future warnings less noisy
* `#7366 <https://github.com/numpy/numpy/pull/7366>`__: TST: fix #6542, add tests to check non-iterable argument raises...
* `#7373 <https://github.com/numpy/numpy/pull/7373>`__: ENH: Add bitwise_and identity
* `#7378 <https://github.com/numpy/numpy/pull/7378>`__: added NumPy logo and separator
* `#7382 <https://github.com/numpy/numpy/pull/7382>`__: MAINT: cleanup np.average
* `#7385 <https://github.com/numpy/numpy/pull/7385>`__: DOC: note about wheels / windows wheels for PyPI
* `#7386 <https://github.com/numpy/numpy/pull/7386>`__: Added label icon to Travis status
* `#7397 <https://github.com/numpy/numpy/pull/7397>`__: BUG: incorrect type for objects whose __len__ fails
* `#7398 <https://github.com/numpy/numpy/pull/7398>`__: DOC: fix typo
* `#7404 <https://github.com/numpy/numpy/pull/7404>`__: Use PyMem_RawMalloc on Python 3.4 and newer
* `#7406 <https://github.com/numpy/numpy/pull/7406>`__: ENH ufunc called on memmap return a ndarray
* `#7407 <https://github.com/numpy/numpy/pull/7407>`__: BUG: Fix decref before incref for in-place accumulate
* `#7410 <https://github.com/numpy/numpy/pull/7410>`__: DOC: add nanprod to the list of math routines
* `#7414 <https://github.com/numpy/numpy/pull/7414>`__: Tweak corrcoef
* `#7415 <https://github.com/numpy/numpy/pull/7415>`__: DOC: Documentation fixes
* `#7416 <https://github.com/numpy/numpy/pull/7416>`__: BUG: Incorrect handling of range in `histogram` with automatic...
* `#7418 <https://github.com/numpy/numpy/pull/7418>`__: DOC: Minor typo fix, hermefik -> hermefit.
* `#7421 <https://github.com/numpy/numpy/pull/7421>`__: ENH: adds np.nancumsum and np.nancumprod
* `#7423 <https://github.com/numpy/numpy/pull/7423>`__: BUG: Ongoing fixes to PR#7416
* `#7430 <https://github.com/numpy/numpy/pull/7430>`__: DOC: Update 1.11.0-notes.
* `#7433 <https://github.com/numpy/numpy/pull/7433>`__: MAINT: FutureWarning for changes to np.average subclass handling
* `#7437 <https://github.com/numpy/numpy/pull/7437>`__: np.full now defaults to the filling value's dtype.
* `#7438 <https://github.com/numpy/numpy/pull/7438>`__: Allow rolling multiple axes at the same time.
* `#7439 <https://github.com/numpy/numpy/pull/7439>`__: BUG: Do not try sequence repeat unless necessary
* `#7442 <https://github.com/numpy/numpy/pull/7442>`__: MANT: Simplify diagonal length calculation logic
* `#7445 <https://github.com/numpy/numpy/pull/7445>`__: BUG: reference count leak in bincount, fixes #6805
* `#7446 <https://github.com/numpy/numpy/pull/7446>`__: DOC: ndarray typo fix
* `#7447 <https://github.com/numpy/numpy/pull/7447>`__: BUG: scalar integer negative powers gave wrong results.
* `#7448 <https://github.com/numpy/numpy/pull/7448>`__: DOC: array "See also" link to full and full_like instead of fill
* `#7456 <https://github.com/numpy/numpy/pull/7456>`__: BUG: int overflow in reshape, fixes #7455, fixes #7293
* `#7463 <https://github.com/numpy/numpy/pull/7463>`__: BUG: fix array too big error for wide dtypes.
* `#7466 <https://github.com/numpy/numpy/pull/7466>`__: BUG: segfault inplace object reduceat, fixes #7465
* `#7468 <https://github.com/numpy/numpy/pull/7468>`__: BUG: more on inplace reductions, fixes #615
* `#7469 <https://github.com/numpy/numpy/pull/7469>`__: MAINT: Update git .mailmap
* `#7472 <https://github.com/numpy/numpy/pull/7472>`__: MAINT: Update .mailmap.
* `#7477 <https://github.com/numpy/numpy/pull/7477>`__: MAINT: Yet more .mailmap updates for recent contributors.
* `#7481 <https://github.com/numpy/numpy/pull/7481>`__: BUG: Fix segfault in PyArray_OrderConverter
* `#7482 <https://github.com/numpy/numpy/pull/7482>`__: BUG: Memory Leak in _GenericBinaryOutFunction
* `#7489 <https://github.com/numpy/numpy/pull/7489>`__: Faster real_if_close.
* `#7491 <https://github.com/numpy/numpy/pull/7491>`__: DOC: Update subclassing doc regarding downstream compatibility
* `#7496 <https://github.com/numpy/numpy/pull/7496>`__: BUG: don't use pow for integer power ufunc loops.
* `#7504 <https://github.com/numpy/numpy/pull/7504>`__: DOC: remove "arr" from keepdims docstrings
* `#7505 <https://github.com/numpy/numpy/pull/7505>`__: MAIN: fix to #7382, make scl in np.average writeable
* `#7507 <https://github.com/numpy/numpy/pull/7507>`__: MAINT: Remove nose.SkipTest import.
* `#7508 <https://github.com/numpy/numpy/pull/7508>`__: DOC: link frompyfunc and vectorize
* `#7511 <https://github.com/numpy/numpy/pull/7511>`__: numpy.power(0, 0) should return 1
* `#7515 <https://github.com/numpy/numpy/pull/7515>`__: BUG: MaskedArray.count treats negative axes incorrectly
* `#7518 <https://github.com/numpy/numpy/pull/7518>`__: BUG: Extend glibc complex trig functions blacklist to glibc <...
* `#7521 <https://github.com/numpy/numpy/pull/7521>`__: DOC: rephrase writeup of memmap changes
* `#7522 <https://github.com/numpy/numpy/pull/7522>`__: BUG: Fixed iteration over additional bad commands
* `#7526 <https://github.com/numpy/numpy/pull/7526>`__: DOC: Removed an extra `:const:`
* `#7529 <https://github.com/numpy/numpy/pull/7529>`__: BUG: Floating exception with invalid axis in np.lexsort
* `#7534 <https://github.com/numpy/numpy/pull/7534>`__: MAINT: Update setup.py to reflect supported python versions.
* `#7536 <https://github.com/numpy/numpy/pull/7536>`__: MAINT: Always use PyCapsule instead of PyCObject in mtrand.pyx
* `#7539 <https://github.com/numpy/numpy/pull/7539>`__: MAINT: Cleanup of random stuff
* `#7549 <https://github.com/numpy/numpy/pull/7549>`__: BUG: allow graceful recovery for no Linux compiler
* `#7562 <https://github.com/numpy/numpy/pull/7562>`__: BUG: Fix test_from_object_array_unicode (test_defchararray.TestBasic)…
* `#7565 <https://github.com/numpy/numpy/pull/7565>`__: BUG: Fix test_ctypeslib and test_indexing for debug interpreter
* `#7566 <https://github.com/numpy/numpy/pull/7566>`__: MAINT: use manylinux1 wheel for cython
* `#7568 <https://github.com/numpy/numpy/pull/7568>`__: Fix a false positive OverflowError in Python 3.x when value above...
* `#7579 <https://github.com/numpy/numpy/pull/7579>`__: DOC: clarify purpose of Attributes section
* `#7584 <https://github.com/numpy/numpy/pull/7584>`__: BUG: fixes #7572, percent in path
* `#7586 <https://github.com/numpy/numpy/pull/7586>`__: Make np.ma.take works on scalars
* `#7587 <https://github.com/numpy/numpy/pull/7587>`__: BUG: linalg.norm(): Don't convert object arrays to float
* `#7598 <https://github.com/numpy/numpy/pull/7598>`__: Cast array size to int64 when loading from archive
* `#7602 <https://github.com/numpy/numpy/pull/7602>`__: DOC: Remove isreal and iscomplex from ufunc list
* `#7605 <https://github.com/numpy/numpy/pull/7605>`__: DOC: fix incorrect Gamma distribution parameterization comments
* `#7609 <https://github.com/numpy/numpy/pull/7609>`__: BUG: Fix TypeError when raising TypeError
* `#7611 <https://github.com/numpy/numpy/pull/7611>`__: ENH: expose test runner raise_warnings option
* `#7614 <https://github.com/numpy/numpy/pull/7614>`__: BLD: Avoid using os.spawnve in favor of os.spawnv in exec_command
* `#7618 <https://github.com/numpy/numpy/pull/7618>`__: BUG: distance arg of np.gradient must be scalar, fix docstring
* `#7626 <https://github.com/numpy/numpy/pull/7626>`__: DOC: RST definition list fixes
* `#7627 <https://github.com/numpy/numpy/pull/7627>`__: MAINT: unify tup processing, move tup use to after all PyTuple_SetItem...
* `#7630 <https://github.com/numpy/numpy/pull/7630>`__: MAINT: add ifdef around PyDictProxy_Check macro
* `#7631 <https://github.com/numpy/numpy/pull/7631>`__: MAINT: linalg: fix comment, simplify math
* `#7634 <https://github.com/numpy/numpy/pull/7634>`__: BLD: correct C compiler customization in system_info.py Closes...
* `#7635 <https://github.com/numpy/numpy/pull/7635>`__: BUG: ma.median alternate fix for #7592
* `#7636 <https://github.com/numpy/numpy/pull/7636>`__: MAINT: clean up testing.assert_raises_regexp, 2.6-specific code...
* `#7637 <https://github.com/numpy/numpy/pull/7637>`__: MAINT: clearer exception message when importing multiarray fails.
* `#7639 <https://github.com/numpy/numpy/pull/7639>`__: TST: fix a set of test errors in master.
* `#7643 <https://github.com/numpy/numpy/pull/7643>`__: DOC : minor changes to linspace docstring
* `#7651 <https://github.com/numpy/numpy/pull/7651>`__: BUG: one to any power is still 1. Broken edgecase for int arrays
* `#7655 <https://github.com/numpy/numpy/pull/7655>`__: BLD: Remove Intel compiler flag -xSSE4.2
* `#7658 <https://github.com/numpy/numpy/pull/7658>`__: BUG: fix incorrect printing of 1D masked arrays
* `#7659 <https://github.com/numpy/numpy/pull/7659>`__: BUG: Temporary fix for str(mvoid) for object field types
* `#7664 <https://github.com/numpy/numpy/pull/7664>`__: BUG: Fix unicode with byte swap transfer and copyswap
* `#7667 <https://github.com/numpy/numpy/pull/7667>`__: Restore histogram consistency
* `#7668 <https://github.com/numpy/numpy/pull/7668>`__: ENH: Do not check the type of module.__dict__ explicit in test.
* `#7669 <https://github.com/numpy/numpy/pull/7669>`__: BUG: boolean assignment no GIL release when transfer needs API
* `#7673 <https://github.com/numpy/numpy/pull/7673>`__: DOC: Create Numpy 1.11.1 release notes.
* `#7675 <https://github.com/numpy/numpy/pull/7675>`__: BUG: fix handling of right edge of final bin.
* `#7678 <https://github.com/numpy/numpy/pull/7678>`__: BUG: Fix np.clip bug NaN handling for Visual Studio 2015
* `#7679 <https://github.com/numpy/numpy/pull/7679>`__: MAINT: Fix up C++ comment in arraytypes.c.src.
* `#7681 <https://github.com/numpy/numpy/pull/7681>`__: DOC: Update 1.11.1 release notes.
* `#7686 <https://github.com/numpy/numpy/pull/7686>`__: ENH: Changing FFT cache to a bounded LRU cache
* `#7688 <https://github.com/numpy/numpy/pull/7688>`__: DOC: fix broken genfromtxt examples in user guide. Closes gh-7662.
* `#7689 <https://github.com/numpy/numpy/pull/7689>`__: BENCH: add correlate/convolve benchmarks.
* `#7696 <https://github.com/numpy/numpy/pull/7696>`__: DOC: update wheel build / upload instructions
* `#7699 <https://github.com/numpy/numpy/pull/7699>`__: BLD: preserve library order
* `#7704 <https://github.com/numpy/numpy/pull/7704>`__: ENH: Add bits attribute to np.finfo
* `#7712 <https://github.com/numpy/numpy/pull/7712>`__: BUG: Fix race condition with new FFT cache
* `#7715 <https://github.com/numpy/numpy/pull/7715>`__: BUG: Remove memory leak in np.place
* `#7719 <https://github.com/numpy/numpy/pull/7719>`__: BUG: Fix segfault in np.random.shuffle for arrays of different...
* `#7723 <https://github.com/numpy/numpy/pull/7723>`__: Change mkl_info.dir_env_var from MKL to MKLROOT
* `#7727 <https://github.com/numpy/numpy/pull/7727>`__: DOC: Corrections in Datetime Units-arrays.datetime.rst
* `#7729 <https://github.com/numpy/numpy/pull/7729>`__: DOC: fix typo in savetxt docstring (closes #7620)
* `#7733 <https://github.com/numpy/numpy/pull/7733>`__: Update 7525, DOC: Fix order='A' docs of np.array.
* `#7734 <https://github.com/numpy/numpy/pull/7734>`__: Update 7542, ENH: Add `polyrootval` to numpy.polynomial
* `#7735 <https://github.com/numpy/numpy/pull/7735>`__: BUG: fix issue on OS X with Python 3.x where npymath.ini was...
* `#7739 <https://github.com/numpy/numpy/pull/7739>`__: DOC: Mention the changes of #6430 in the release notes.
* `#7740 <https://github.com/numpy/numpy/pull/7740>`__: DOC: add reference to poisson rng
* `#7743 <https://github.com/numpy/numpy/pull/7743>`__: Update 7476, DEP: deprecate Numeric-style typecodes, closes #2148
* `#7744 <https://github.com/numpy/numpy/pull/7744>`__: DOC: Remove "ones_like" from ufuncs list (it is not)
* `#7746 <https://github.com/numpy/numpy/pull/7746>`__: DOC: Clarify the effect of rcond in numpy.linalg.lstsq.
* `#7747 <https://github.com/numpy/numpy/pull/7747>`__: Update 7672, BUG: Make sure we don't divide by zero
* `#7748 <https://github.com/numpy/numpy/pull/7748>`__: DOC: Update float32 mean example in docstring
* `#7754 <https://github.com/numpy/numpy/pull/7754>`__: Update 7612, ENH: Add broadcast.ndim to match code elsewhere.
* `#7757 <https://github.com/numpy/numpy/pull/7757>`__: Update 7175, BUG: Invalid read of size 4 in PyArray_FromFile
* `#7759 <https://github.com/numpy/numpy/pull/7759>`__: BUG: Fix numpy.i support for numpy API < 1.7.
* `#7760 <https://github.com/numpy/numpy/pull/7760>`__: ENH: Make assert_almost_equal & assert_array_almost_equal consistent.
* `#7766 <https://github.com/numpy/numpy/pull/7766>`__: fix an English typo
* `#7771 <https://github.com/numpy/numpy/pull/7771>`__: DOC: link geomspace from logspace
* `#7773 <https://github.com/numpy/numpy/pull/7773>`__: DOC: Remove a redundant the
* `#7777 <https://github.com/numpy/numpy/pull/7777>`__: DOC: Update Numpy 1.11.1 release notes.
* `#7785 <https://github.com/numpy/numpy/pull/7785>`__: DOC: update wheel building procedure for release
* `#7789 <https://github.com/numpy/numpy/pull/7789>`__: MRG: add note of 64-bit wheels on Windows
* `#7791 <https://github.com/numpy/numpy/pull/7791>`__: f2py.compile issues (#7683)
* `#7799 <https://github.com/numpy/numpy/pull/7799>`__: "lambda" is not allowed to use as keyword arguments in a sample...
* `#7803 <https://github.com/numpy/numpy/pull/7803>`__: BUG: interpret 'c' PEP3118/struct type as 'S1'.
* `#7807 <https://github.com/numpy/numpy/pull/7807>`__: DOC: Misplaced parens in formula
* `#7817 <https://github.com/numpy/numpy/pull/7817>`__: BUG: Make sure npy_mul_with_overflow_<type> detects overflow.
* `#7818 <https://github.com/numpy/numpy/pull/7818>`__: numpy/distutils/misc_util.py fix for #7809: check that _tmpdirs...
* `#7820 <https://github.com/numpy/numpy/pull/7820>`__: MAINT: Allocate fewer bytes for empty arrays.
* `#7823 <https://github.com/numpy/numpy/pull/7823>`__: BUG: Fixed masked array behavior for scalar inputs to np.ma.atleast_*d
* `#7834 <https://github.com/numpy/numpy/pull/7834>`__: DOC: Added an example
* `#7839 <https://github.com/numpy/numpy/pull/7839>`__: Pypy fixes
* `#7840 <https://github.com/numpy/numpy/pull/7840>`__: Fix ATLAS version detection
* `#7842 <https://github.com/numpy/numpy/pull/7842>`__: Fix versionadded tags
* `#7848 <https://github.com/numpy/numpy/pull/7848>`__: MAINT: Fix remaining uses of deprecated Python imp module.
* `#7853 <https://github.com/numpy/numpy/pull/7853>`__: BUG: Make sure numpy globals keep identity after reload.
* `#7863 <https://github.com/numpy/numpy/pull/7863>`__: ENH: turn quicksort into introsort
* `#7866 <https://github.com/numpy/numpy/pull/7866>`__: Document runtests extra argv
* `#7871 <https://github.com/numpy/numpy/pull/7871>`__: BUG: handle introsort depth limit properly
* `#7879 <https://github.com/numpy/numpy/pull/7879>`__: DOC: fix typo in documentation of loadtxt (closes #7878)
* `#7885 <https://github.com/numpy/numpy/pull/7885>`__: Handle NetBSD specific <sys/endian.h>
* `#7889 <https://github.com/numpy/numpy/pull/7889>`__: DOC: #7881. Fix link to record arrays
* `#7894 <https://github.com/numpy/numpy/pull/7894>`__: fixup-7790, BUG: construct ma.array from np.array which contains...
* `#7898 <https://github.com/numpy/numpy/pull/7898>`__: Spelling and grammar fix.
* `#7903 <https://github.com/numpy/numpy/pull/7903>`__: BUG: fix float16 type not being called due to wrong ordering
* `#7908 <https://github.com/numpy/numpy/pull/7908>`__: BLD: Fixed detection for recent MKL versions
* `#7911 <https://github.com/numpy/numpy/pull/7911>`__: BUG: fix for issue#7835 (ma.median of 1d)
* `#7912 <https://github.com/numpy/numpy/pull/7912>`__: ENH: skip or avoid gc/objectmodel differences btwn pypy and cpython
* `#7918 <https://github.com/numpy/numpy/pull/7918>`__: ENH: allow numpy.apply_along_axis() to work with ndarray subclasses
* `#7922 <https://github.com/numpy/numpy/pull/7922>`__: ENH: Add ma.convolve and ma.correlate for #6458
* `#7925 <https://github.com/numpy/numpy/pull/7925>`__: Monkey-patch _msvccompile.gen_lib_option like any other compilators
* `#7931 <https://github.com/numpy/numpy/pull/7931>`__: BUG: Check for HAVE_LDOUBLE_DOUBLE_DOUBLE_LE in npy_math_complex.
* `#7936 <https://github.com/numpy/numpy/pull/7936>`__: ENH: improve duck typing inside iscomplexobj
* `#7937 <https://github.com/numpy/numpy/pull/7937>`__: BUG: Guard against buggy comparisons in generic quicksort.
* `#7938 <https://github.com/numpy/numpy/pull/7938>`__: DOC: add cbrt to math summary page
* `#7941 <https://github.com/numpy/numpy/pull/7941>`__: BUG: Make sure numpy globals keep identity after reload.
* `#7943 <https://github.com/numpy/numpy/pull/7943>`__: DOC: #7927. Remove deprecated note for memmap relevant for Python...
* `#7952 <https://github.com/numpy/numpy/pull/7952>`__: BUG: Use keyword arguments to initialize Extension base class.
* `#7956 <https://github.com/numpy/numpy/pull/7956>`__: BLD: remove __NUMPY_SETUP__ from builtins at end of setup.py
* `#7963 <https://github.com/numpy/numpy/pull/7963>`__: BUG: MSVCCompiler grows 'lib' & 'include' env strings exponentially.
* `#7965 <https://github.com/numpy/numpy/pull/7965>`__: BUG: cannot modify tuple after use
* `#7976 <https://github.com/numpy/numpy/pull/7976>`__: DOC: Fixed documented dimension of return value
* `#7977 <https://github.com/numpy/numpy/pull/7977>`__: DOC: Create 1.11.2 release notes.
* `#7979 <https://github.com/numpy/numpy/pull/7979>`__: DOC: Corrected allowed keywords in ``add_installed_library``
* `#7980 <https://github.com/numpy/numpy/pull/7980>`__: ENH: Add ability to runtime select ufunc loops, add AVX2 integer...
* `#7985 <https://github.com/numpy/numpy/pull/7985>`__: Rebase 7763, ENH: Add new warning suppression/filtering context
* `#7987 <https://github.com/numpy/numpy/pull/7987>`__: DOC: See also np.load and np.memmap in np.lib.format.open_memmap
* `#7988 <https://github.com/numpy/numpy/pull/7988>`__: DOC: Include docstring for cbrt, spacing and fabs in documentation
* `#7999 <https://github.com/numpy/numpy/pull/7999>`__: ENH: add inplace cases to fast ufunc loop macros
* `#8006 <https://github.com/numpy/numpy/pull/8006>`__: DOC: Update 1.11.2 release notes.
* `#8008 <https://github.com/numpy/numpy/pull/8008>`__: MAINT: Remove leftover imp module imports.
* `#8009 <https://github.com/numpy/numpy/pull/8009>`__: DOC: Fixed three typos in the c-info.ufunc-tutorial
* `#8011 <https://github.com/numpy/numpy/pull/8011>`__: DOC: Update 1.11.2 release notes.
* `#8014 <https://github.com/numpy/numpy/pull/8014>`__: BUG: Fix fid.close() to use os.close(fid)
* `#8016 <https://github.com/numpy/numpy/pull/8016>`__: BUG: Fix numpy.ma.median.
* `#8018 <https://github.com/numpy/numpy/pull/8018>`__: BUG: Fixes return for np.ma.count if keepdims is True and axis...
* `#8021 <https://github.com/numpy/numpy/pull/8021>`__: DOC: change all non-code instances of Numpy to NumPy
* `#8027 <https://github.com/numpy/numpy/pull/8027>`__: ENH: Add platform independent lib dir to PYTHONPATH
* `#8028 <https://github.com/numpy/numpy/pull/8028>`__: DOC: Update 1.11.2 release notes.
* `#8030 <https://github.com/numpy/numpy/pull/8030>`__: BUG: fix np.ma.median with only one non-masked value and an axis...
* `#8038 <https://github.com/numpy/numpy/pull/8038>`__: MAINT: Update error message in rollaxis.
* `#8040 <https://github.com/numpy/numpy/pull/8040>`__: Update add_newdocs.py
* `#8042 <https://github.com/numpy/numpy/pull/8042>`__: BUG: core: fix bug in NpyIter buffering with discontinuous arrays
* `#8045 <https://github.com/numpy/numpy/pull/8045>`__: DOC: Update 1.11.2 release notes.
* `#8050 <https://github.com/numpy/numpy/pull/8050>`__: remove refcount semantics, now a.resize() almost always requires...
* `#8051 <https://github.com/numpy/numpy/pull/8051>`__: Clear signaling NaN exceptions
* `#8054 <https://github.com/numpy/numpy/pull/8054>`__: ENH: add signature argument to vectorize for vectorizing like...
* `#8057 <https://github.com/numpy/numpy/pull/8057>`__: BUG: lib: Simplify (and fix) pad's handling of the pad_width
* `#8061 <https://github.com/numpy/numpy/pull/8061>`__: BUG : financial.pmt modifies input (issue #8055)
* `#8064 <https://github.com/numpy/numpy/pull/8064>`__: MAINT: Add PMIP files to .gitignore
* `#8065 <https://github.com/numpy/numpy/pull/8065>`__: BUG: Assert fromfile ending earlier in pyx_processing
* `#8066 <https://github.com/numpy/numpy/pull/8066>`__: BUG, TST: Fix python3-dbg bug in Travis script
* `#8071 <https://github.com/numpy/numpy/pull/8071>`__: MAINT: Add Tempita to randint helpers
* `#8075 <https://github.com/numpy/numpy/pull/8075>`__: DOC: Fix description of isinf in nan_to_num
* `#8080 <https://github.com/numpy/numpy/pull/8080>`__: BUG: non-integers can end up in dtype offsets
* `#8081 <https://github.com/numpy/numpy/pull/8081>`__: Update outdated Nose URL to nose.readthedocs.io
* `#8083 <https://github.com/numpy/numpy/pull/8083>`__: ENH: Deprecation warnings for `/` integer division when running...
* `#8084 <https://github.com/numpy/numpy/pull/8084>`__: DOC: Fix erroneous return type description for np.roots.
* `#8087 <https://github.com/numpy/numpy/pull/8087>`__: BUG: financial.pmt modifies input #8055
* `#8088 <https://github.com/numpy/numpy/pull/8088>`__: MAINT: Remove duplicate randint helpers code.
* `#8093 <https://github.com/numpy/numpy/pull/8093>`__: MAINT: fix assert_raises_regex when used as a context manager
* `#8096 <https://github.com/numpy/numpy/pull/8096>`__: ENH: Vendorize tempita.
* `#8098 <https://github.com/numpy/numpy/pull/8098>`__: DOC: Enhance description/usage for np.linalg.eig*h
* `#8103 <https://github.com/numpy/numpy/pull/8103>`__: Pypy fixes
* `#8104 <https://github.com/numpy/numpy/pull/8104>`__: Fix test code on cpuinfo's main function
* `#8107 <https://github.com/numpy/numpy/pull/8107>`__: BUG: Fix array printing with precision=0.
* `#8109 <https://github.com/numpy/numpy/pull/8109>`__: Fix bug in ravel_multi_index for big indices (Issue #7546)
* `#8110 <https://github.com/numpy/numpy/pull/8110>`__: BUG: distutils: fix issue with rpath in fcompiler/gnu.py
* `#8111 <https://github.com/numpy/numpy/pull/8111>`__: ENH: Add a tool for release authors and PRs.
* `#8112 <https://github.com/numpy/numpy/pull/8112>`__: DOC: Fix "See also" links in linalg.
* `#8114 <https://github.com/numpy/numpy/pull/8114>`__: BUG: core: add missing error check after PyLong_AsSsize_t
* `#8121 <https://github.com/numpy/numpy/pull/8121>`__: DOC: Improve histogram2d() example.
* `#8122 <https://github.com/numpy/numpy/pull/8122>`__: BUG: Fix broken pickle in MaskedArray when dtype is object (Return...
* `#8124 <https://github.com/numpy/numpy/pull/8124>`__: BUG: Fixed build break
* `#8125 <https://github.com/numpy/numpy/pull/8125>`__: Rebase, BUG: Fixed deepcopy of F-order object arrays.
* `#8127 <https://github.com/numpy/numpy/pull/8127>`__: BUG: integers to a negative integer powers should error.
* `#8141 <https://github.com/numpy/numpy/pull/8141>`__: improve configure checks for broken systems
* `#8142 <https://github.com/numpy/numpy/pull/8142>`__: BUG: np.ma.mean and var should return scalar if no mask
* `#8148 <https://github.com/numpy/numpy/pull/8148>`__: BUG: import full module path in npy_load_module
* `#8153 <https://github.com/numpy/numpy/pull/8153>`__: MAINT: Expose void-scalar "base" attribute in python
* `#8156 <https://github.com/numpy/numpy/pull/8156>`__: DOC: added example with empty indices for a scalar, #8138
* `#8160 <https://github.com/numpy/numpy/pull/8160>`__: BUG: fix _array2string for structured array (issue #5692)
* `#8164 <https://github.com/numpy/numpy/pull/8164>`__: MAINT: Update mailmap for NumPy 1.12.0
* `#8165 <https://github.com/numpy/numpy/pull/8165>`__: Fixup 8152, BUG: assert_allclose(..., equal_nan=False) doesn't...
* `#8167 <https://github.com/numpy/numpy/pull/8167>`__: Fixup 8146, DOC: Clarify when PyArray_{Max, Min, Ptp} return...
* `#8168 <https://github.com/numpy/numpy/pull/8168>`__: DOC: Minor spelling fix in genfromtxt() docstring.
* `#8173 <https://github.com/numpy/numpy/pull/8173>`__: BLD: Enable build on AIX
* `#8174 <https://github.com/numpy/numpy/pull/8174>`__: DOC: warn that dtype.descr is only for use in PEP3118
* `#8177 <https://github.com/numpy/numpy/pull/8177>`__: MAINT: Add python 3.6 support to suppress_warnings
* `#8178 <https://github.com/numpy/numpy/pull/8178>`__: MAINT: Fix ResourceWarning new in Python 3.6.
* `#8180 <https://github.com/numpy/numpy/pull/8180>`__: FIX: protect stolen ref by PyArray_NewFromDescr in array_empty
* `#8181 <https://github.com/numpy/numpy/pull/8181>`__: ENH: Improve announce to find github squash-merge commits.
* `#8182 <https://github.com/numpy/numpy/pull/8182>`__: MAINT: Update .mailmap
* `#8183 <https://github.com/numpy/numpy/pull/8183>`__: MAINT: Ediff1d performance
* `#8184 <https://github.com/numpy/numpy/pull/8184>`__: MAINT: make `assert_allclose` behavior on nans match pre 1.12
* `#8188 <https://github.com/numpy/numpy/pull/8188>`__: DOC: 'highest' is exclusive for randint()
* `#8189 <https://github.com/numpy/numpy/pull/8189>`__: BUG: setfield should raise if arr is not writeable
* `#8190 <https://github.com/numpy/numpy/pull/8190>`__: ENH: Add a float_power function with at least float64 precision.
* `#8197 <https://github.com/numpy/numpy/pull/8197>`__: DOC: Add missing arguments to np.ufunc.outer
* `#8198 <https://github.com/numpy/numpy/pull/8198>`__: DEP: Deprecate the keepdims argument to accumulate
* `#8199 <https://github.com/numpy/numpy/pull/8199>`__: MAINT: change path to env in distutils.system_info. Closes gh-8195.
* `#8200 <https://github.com/numpy/numpy/pull/8200>`__: BUG: Fix structured array format functions
* `#8202 <https://github.com/numpy/numpy/pull/8202>`__: ENH: specialize name of dev package by interpreter
* `#8205 <https://github.com/numpy/numpy/pull/8205>`__: DOC: change development instructions from SSH to HTTPS access.
* `#8216 <https://github.com/numpy/numpy/pull/8216>`__: DOC: Patch doc errors for atleast_nd and frombuffer
* `#8218 <https://github.com/numpy/numpy/pull/8218>`__: BUG: ediff1d should return subclasses
* `#8219 <https://github.com/numpy/numpy/pull/8219>`__: DOC: Turn SciPy references into links.
* `#8222 <https://github.com/numpy/numpy/pull/8222>`__: ENH: Make numpy.mean() do more precise computation
* `#8227 <https://github.com/numpy/numpy/pull/8227>`__: BUG: Better check for invalid bounds in np.random.uniform.
* `#8231 <https://github.com/numpy/numpy/pull/8231>`__: ENH: Refactor numpy ** operators for numpy scalar integer powers
* `#8234 <https://github.com/numpy/numpy/pull/8234>`__: DOC: Clarified when a copy is made in numpy.asarray
* `#8236 <https://github.com/numpy/numpy/pull/8236>`__: DOC: Fix documentation pull requests.
* `#8238 <https://github.com/numpy/numpy/pull/8238>`__: MAINT: Update pavement.py
* `#8239 <https://github.com/numpy/numpy/pull/8239>`__: ENH: Improve announce tool.
* `#8240 <https://github.com/numpy/numpy/pull/8240>`__: REL: Prepare for 1.12.x branch
* `#8243 <https://github.com/numpy/numpy/pull/8243>`__: BUG: Update operator `**` tests for new behavior.
* `#8246 <https://github.com/numpy/numpy/pull/8246>`__: REL: Reset strides for RELAXED_STRIDE_CHECKING for 1.12 releases.
* `#8265 <https://github.com/numpy/numpy/pull/8265>`__: BUG: np.piecewise not working for scalars
* `#8272 <https://github.com/numpy/numpy/pull/8272>`__: TST: Path test should resolve symlinks when comparing
* `#8282 <https://github.com/numpy/numpy/pull/8282>`__: DOC: Update 1.12.0 release notes.
* `#8286 <https://github.com/numpy/numpy/pull/8286>`__: BUG: Fix pavement.py write_release_task.
* `#8296 <https://github.com/numpy/numpy/pull/8296>`__: BUG: Fix iteration over reversed subspaces in mapiter_@name@.
* `#8304 <https://github.com/numpy/numpy/pull/8304>`__: BUG: Fix PyPy crash in PyUFunc_GenericReduction.
* `#8319 <https://github.com/numpy/numpy/pull/8319>`__: BLD: blacklist powl (longdouble power function) on OS X.
* `#8320 <https://github.com/numpy/numpy/pull/8320>`__: BUG: do not link to Accelerate if OpenBLAS, MKL or BLIS are found.
* `#8322 <https://github.com/numpy/numpy/pull/8322>`__: BUG: fixed kind specifications for parameters
* `#8336 <https://github.com/numpy/numpy/pull/8336>`__: BUG: fix packbits and unpackbits to correctly handle empty arrays
* `#8338 <https://github.com/numpy/numpy/pull/8338>`__: BUG: fix test_api test that fails intermittently in python 3
* `#8339 <https://github.com/numpy/numpy/pull/8339>`__: BUG: Fix ndarray.tofile large file corruption in append mode.
* `#8359 <https://github.com/numpy/numpy/pull/8359>`__: BUG: Fix suppress_warnings (again) for Python 3.6.
* `#8372 <https://github.com/numpy/numpy/pull/8372>`__: BUG: Fixes for ma.median and nanpercentile.
* `#8373 <https://github.com/numpy/numpy/pull/8373>`__: BUG: correct letter case
* `#8379 <https://github.com/numpy/numpy/pull/8379>`__: DOC: Update 1.12.0-notes.rst.
* `#8390 <https://github.com/numpy/numpy/pull/8390>`__: ENH: retune apply_along_axis nanmedian cutoff in 1.12
* `#8391 <https://github.com/numpy/numpy/pull/8391>`__: DEP: Fix escaped string characters deprecated in Python 3.6.
* `#8394 <https://github.com/numpy/numpy/pull/8394>`__: DOC: create 1.11.3 release notes.
* `#8399 <https://github.com/numpy/numpy/pull/8399>`__: BUG: Fix author search in announce.py
* `#8402 <https://github.com/numpy/numpy/pull/8402>`__: DOC, MAINT: Update 1.12.0 notes and mailmap.
* `#8418 <https://github.com/numpy/numpy/pull/8418>`__: BUG: Fix ma.median even elements for 1.12
* `#8424 <https://github.com/numpy/numpy/pull/8424>`__: DOC: Fix tools and release notes to be more markdown compatible.
* `#8427 <https://github.com/numpy/numpy/pull/8427>`__: BUG: Add a lock to assert_equal and other testing functions
* `#8431 <https://github.com/numpy/numpy/pull/8431>`__: BUG: Fix apply_along_axis() for when func1d() returns a non-ndarray.
* `#8432 <https://github.com/numpy/numpy/pull/8432>`__: BUG: Let linspace accept input that has an array_interface.
* `#8437 <https://github.com/numpy/numpy/pull/8437>`__: TST: Update 3.6-dev tests to 3.6 after Python final release.
* `#8439 <https://github.com/numpy/numpy/pull/8439>`__: DOC: Update 1.12.0 release notes.
* `#8466 <https://github.com/numpy/numpy/pull/8466>`__: MAINT: Update mailmap entries.
* `#8467 <https://github.com/numpy/numpy/pull/8467>`__: DOC: Back-port the missing part of gh-8464.
* `#8476 <https://github.com/numpy/numpy/pull/8476>`__: DOC: Update 1.12.0 release notes.
* `#8477 <https://github.com/numpy/numpy/pull/8477>`__: DOC: Update 1.12.0 release notes.

Contributors
============

A total of 14 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Daniel Smith
* Dennis Weyland +
* Eric Larson
* Eric Wieser
* Jarrod Millman
* Kenichi Maehashi +
* Marten van Kerkwijk
* Mathieu Lamarre
* Sebastian Berg
* Simon Conseil
* Simon Gibbons
* xoviat

Pull requests merged
====================

A total of 36 pull requests were merged for this release.

* `#10339 <https://github.com/numpy/numpy/pull/10339>`__: BUG: restrict the __config__ modifications to win32
* `#10368 <https://github.com/numpy/numpy/pull/10368>`__: MAINT: Adjust type promotion in linalg.norm
* `#10375 <https://github.com/numpy/numpy/pull/10375>`__: BUG: add missing paren and remove quotes from repr of fieldless...
* `#10395 <https://github.com/numpy/numpy/pull/10395>`__: MAINT: Update download URL in setup.py.
* `#10396 <https://github.com/numpy/numpy/pull/10396>`__: BUG: fix einsum issue with unicode input and py2
* `#10397 <https://github.com/numpy/numpy/pull/10397>`__: BUG: fix error message not formatted in einsum
* `#10398 <https://github.com/numpy/numpy/pull/10398>`__: DOC: add documentation about how to handle new array printing
* `#10403 <https://github.com/numpy/numpy/pull/10403>`__: BUG: Set einsum optimize parameter default to `False`.
* `#10424 <https://github.com/numpy/numpy/pull/10424>`__: ENH: Fix repr of np.record objects to match np.void types #10412
* `#10425 <https://github.com/numpy/numpy/pull/10425>`__: MAINT: Update zesty to artful for i386 testing
* `#10431 <https://github.com/numpy/numpy/pull/10431>`__: REL: Add 1.14.1 release notes template
* `#10435 <https://github.com/numpy/numpy/pull/10435>`__: MAINT: Use ValueError for duplicate field names in lookup (backport)
* `#10534 <https://github.com/numpy/numpy/pull/10534>`__: BUG: Provide a better error message for out-of-order fields
* `#10536 <https://github.com/numpy/numpy/pull/10536>`__: BUG: Resize bytes_ columns in genfromtxt (backport of #10401)
* `#10537 <https://github.com/numpy/numpy/pull/10537>`__: BUG: multifield-indexing adds padding bytes: revert for 1.14.1
* `#10539 <https://github.com/numpy/numpy/pull/10539>`__: BUG: fix np.save issue with python 2.7.5
* `#10540 <https://github.com/numpy/numpy/pull/10540>`__: BUG: Add missing DECREF in Py2 int() cast
* `#10541 <https://github.com/numpy/numpy/pull/10541>`__: TST: Add circleci document testing to maintenance/1.14.x
* `#10542 <https://github.com/numpy/numpy/pull/10542>`__: BUG: complex repr has extra spaces, missing + (1.14 backport)
* `#10550 <https://github.com/numpy/numpy/pull/10550>`__: BUG: Set missing exception after malloc
* `#10557 <https://github.com/numpy/numpy/pull/10557>`__: BUG: In numpy.i, clear CARRAY flag if wrapped buffer is not C_CONTIGUOUS.
* `#10558 <https://github.com/numpy/numpy/pull/10558>`__: DEP: Issue FutureWarning when malformed records detected.
* `#10559 <https://github.com/numpy/numpy/pull/10559>`__: BUG: Fix einsum optimize logic for singleton dimensions
* `#10560 <https://github.com/numpy/numpy/pull/10560>`__: BUG: Fix calling ufuncs with a positional output argument.
* `#10561 <https://github.com/numpy/numpy/pull/10561>`__: BUG: Fix various Big-Endian test failures (ppc64)
* `#10562 <https://github.com/numpy/numpy/pull/10562>`__: BUG: Make dtype.descr error for out-of-order fields.
* `#10563 <https://github.com/numpy/numpy/pull/10563>`__: BUG: arrays not being flattened in `union1d`
* `#10607 <https://github.com/numpy/numpy/pull/10607>`__: MAINT: Update sphinxext submodule hash.
* `#10608 <https://github.com/numpy/numpy/pull/10608>`__: BUG: Revert sort optimization in np.unique.
* `#10609 <https://github.com/numpy/numpy/pull/10609>`__: BUG: infinite recursion in str of 0d subclasses
* `#10610 <https://github.com/numpy/numpy/pull/10610>`__: BUG: Align type definition with generated lapack
* `#10612 <https://github.com/numpy/numpy/pull/10612>`__: BUG/ENH: Improve output for structured non-void types
* `#10622 <https://github.com/numpy/numpy/pull/10622>`__: BUG: deallocate recursive closure in arrayprint.py (1.14 backport)
* `#10624 <https://github.com/numpy/numpy/pull/10624>`__: BUG: Correctly identify comma separated dtype strings
* `#10629 <https://github.com/numpy/numpy/pull/10629>`__: BUG: deallocate recursive closure in arrayprint.py (backport...
* `#10630 <https://github.com/numpy/numpy/pull/10630>`__: REL: Prepare for 1.14.1 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Billington
* Elliott Sales de Andrade +
* Eric Wieser
* Jeremy Manning +
* Matti Picus
* Ralf Gommers

Pull requests merged
====================

A total of 24 pull requests were merged for this release.

* `#11647 <https://github.com/numpy/numpy/pull/11647>`__: MAINT: Filter Cython warnings in ``__init__.py``
* `#11648 <https://github.com/numpy/numpy/pull/11648>`__: BUG: Fix doc source links to unwrap decorators
* `#11657 <https://github.com/numpy/numpy/pull/11657>`__: BUG: Ensure singleton dimensions are not dropped when converting...
* `#11661 <https://github.com/numpy/numpy/pull/11661>`__: BUG: Warn on Nan in minimum,maximum for scalars
* `#11665 <https://github.com/numpy/numpy/pull/11665>`__: BUG: cython sometimes emits invalid gcc attribute
* `#11682 <https://github.com/numpy/numpy/pull/11682>`__: BUG: Fix regression in void_getitem
* `#11698 <https://github.com/numpy/numpy/pull/11698>`__: BUG: Make matrix_power again work for object arrays.
* `#11700 <https://github.com/numpy/numpy/pull/11700>`__: BUG: Add missing PyErr_NoMemory after failing malloc
* `#11719 <https://github.com/numpy/numpy/pull/11719>`__: BUG: Fix undefined functions on big-endian systems.
* `#11720 <https://github.com/numpy/numpy/pull/11720>`__: MAINT: Make einsum optimize default to False.
* `#11746 <https://github.com/numpy/numpy/pull/11746>`__: BUG: Fix regression in loadtxt for bz2 text files in Python 2.
* `#11757 <https://github.com/numpy/numpy/pull/11757>`__: BUG: Revert use of `console_scripts`.
* `#11758 <https://github.com/numpy/numpy/pull/11758>`__: BUG: Fix Fortran kind detection for aarch64 & s390x.
* `#11759 <https://github.com/numpy/numpy/pull/11759>`__: BUG: Fix printing of longdouble on ppc64le.
* `#11760 <https://github.com/numpy/numpy/pull/11760>`__: BUG: Fixes for unicode field names in Python 2
* `#11761 <https://github.com/numpy/numpy/pull/11761>`__: BUG: Increase required cython version on python 3.7
* `#11763 <https://github.com/numpy/numpy/pull/11763>`__: BUG: check return value of _buffer_format_string
* `#11775 <https://github.com/numpy/numpy/pull/11775>`__: MAINT: Make assert_array_compare more generic.
* `#11776 <https://github.com/numpy/numpy/pull/11776>`__: TST: Fix urlopen stubbing.
* `#11777 <https://github.com/numpy/numpy/pull/11777>`__: BUG: Fix regression in intersect1d.
* `#11779 <https://github.com/numpy/numpy/pull/11779>`__: BUG: Fix test sensitive to platform byte order.
* `#11781 <https://github.com/numpy/numpy/pull/11781>`__: BUG: Avoid signed overflow in histogram
* `#11785 <https://github.com/numpy/numpy/pull/11785>`__: BUG: Fix pickle and memoryview for datetime64, timedelta64 scalars
* `#11786 <https://github.com/numpy/numpy/pull/11786>`__: BUG: Deprecation triggers segfault

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Julian Taylor
* Marten van Kerkwijk
* Matti Picus

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#11902 <https://github.com/numpy/numpy/pull/11902>`__: BUG: Fix matrix PendingDeprecationWarning suppression for pytest...
* `#11981 <https://github.com/numpy/numpy/pull/11981>`__: BUG: fix cached allocations without the GIL for 1.15.x
* `#11982 <https://github.com/numpy/numpy/pull/11982>`__: BUG: fix refcount leak in PyArray_AdaptFlexibleDType
* `#11992 <https://github.com/numpy/numpy/pull/11992>`__: BUG: Ensure boolean indexing of subclasses sets base correctly.

Contributors
============

A total of 3 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Siyuan +

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#16439 <https://github.com/numpy/numpy/pull/16439>`__: ENH: enable pickle protocol 5 support for python3.5
* `#16441 <https://github.com/numpy/numpy/pull/16441>`__: BUG: relpath fails for different drives on windows

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Pauli Virtanen
* Philippe Ombredanne +
* Sebastian Berg
* Stefan Behnel +
* Stephan Loyd +
* Zac Hatfield-Dodds

Pull requests merged
====================

A total of 9 pull requests were merged for this release.

* `#16959 <https://github.com/numpy/numpy/pull/16959>`__: TST: Change aarch64 to arm64 in travis.yml.
* `#16998 <https://github.com/numpy/numpy/pull/16998>`__: MAINT: Configure hypothesis in ``np.test()`` for determinism,...
* `#17000 <https://github.com/numpy/numpy/pull/17000>`__: BLD: pin setuptools < 49.2.0
* `#17015 <https://github.com/numpy/numpy/pull/17015>`__: ENH: Add NumPy declarations to be used by Cython 3.0+
* `#17125 <https://github.com/numpy/numpy/pull/17125>`__: BUG: Remove non-threadsafe sigint handling from fft calculation
* `#17243 <https://github.com/numpy/numpy/pull/17243>`__: BUG: core: fix ilp64 blas dot/vdot/... for strides > int32 max
* `#17244 <https://github.com/numpy/numpy/pull/17244>`__: DOC: Use SPDX license expressions with correct license
* `#17245 <https://github.com/numpy/numpy/pull/17245>`__: DOC: Fix the link to the quick-start in the old API functions
* `#17272 <https://github.com/numpy/numpy/pull/17272>`__: BUG: fix pickling of arrays larger than 2GiB

Contributors
============

A total of 15 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Abhinav Reddy +
* Anirudh Subramanian
* Antonio Larrosa +
* Charles Harris
* Chunlin Fang
* Eric Wieser
* Etienne Guesnet +
* Kevin Sheppard
* Matti Picus
* Raghuveer Devulapalli
* Roman Yurchak
* Ross Barnowski
* Sayed Adel
* Sebastian Berg
* Tyler Reddy

Pull requests merged
====================

A total of 25 pull requests were merged for this release.

* `#16649 <https://github.com/numpy/numpy/pull/16649>`__: MAINT, CI: disable Shippable cache
* `#16652 <https://github.com/numpy/numpy/pull/16652>`__: MAINT: Replace `PyUString_GET_SIZE` with `PyUnicode_GetLength`.
* `#16654 <https://github.com/numpy/numpy/pull/16654>`__: REL: Fix outdated docs link
* `#16656 <https://github.com/numpy/numpy/pull/16656>`__: BUG: raise IEEE exception on AIX
* `#16672 <https://github.com/numpy/numpy/pull/16672>`__: BUG: Fix bug in AVX complex absolute while processing array of...
* `#16693 <https://github.com/numpy/numpy/pull/16693>`__: TST: Add extra debugging information to CPU features detection
* `#16703 <https://github.com/numpy/numpy/pull/16703>`__: BLD: Add CPU entry for Emscripten / WebAssembly
* `#16705 <https://github.com/numpy/numpy/pull/16705>`__: TST: Disable Python 3.9-dev testing.
* `#16714 <https://github.com/numpy/numpy/pull/16714>`__: MAINT: Disable use_hugepages in case of ValueError
* `#16724 <https://github.com/numpy/numpy/pull/16724>`__: BUG: Fix PyArray_SearchSorted signature.
* `#16768 <https://github.com/numpy/numpy/pull/16768>`__: MAINT: Fixes for deprecated functions in scalartypes.c.src
* `#16772 <https://github.com/numpy/numpy/pull/16772>`__: MAINT: Remove unneeded call to PyUnicode_READY
* `#16776 <https://github.com/numpy/numpy/pull/16776>`__: MAINT: Fix deprecated functions in scalarapi.c
* `#16779 <https://github.com/numpy/numpy/pull/16779>`__: BLD, ENH: Add RPATH support for AIX
* `#16780 <https://github.com/numpy/numpy/pull/16780>`__: BUG: Fix default fallback in genfromtxt
* `#16784 <https://github.com/numpy/numpy/pull/16784>`__: BUG: Added missing return after raising error in methods.c
* `#16795 <https://github.com/numpy/numpy/pull/16795>`__: BLD: update cython to 0.29.21
* `#16832 <https://github.com/numpy/numpy/pull/16832>`__: MAINT: setuptools 49.2.0 emits a warning, avoid it
* `#16872 <https://github.com/numpy/numpy/pull/16872>`__: BUG: Validate output size in bin- and multinomial
* `#16875 <https://github.com/numpy/numpy/pull/16875>`__: BLD, MAINT: Pin setuptools
* `#16904 <https://github.com/numpy/numpy/pull/16904>`__: DOC: Reconstruct Testing Guideline.
* `#16905 <https://github.com/numpy/numpy/pull/16905>`__: TST, BUG: Re-raise MemoryError exception in test_large_zip's...
* `#16906 <https://github.com/numpy/numpy/pull/16906>`__: BUG,DOC: Fix bad MPL kwarg.
* `#16916 <https://github.com/numpy/numpy/pull/16916>`__: BUG: Fix string/bytes to complex assignment
* `#16922 <https://github.com/numpy/numpy/pull/16922>`__: REL: Prepare for NumPy 1.19.1 release

Contributors
============

A total of 150 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aaron Voelker +
* Abdur Rehman +
* Abdur-Rahmaan Janhangeer +
* Abhinav Sagar +
* Adam J. Stewart +
* Adam Orr +
* Albert Thomas +
* Alex Watt +
* Alexander Blinne +
* Alexander Shadchin
* Allan Haldane
* Ander Ustarroz +
* Andras Deak
* Andrea Pattori +
* Andreas Schwab
* Andrew Naguib +
* Andy Scholand +
* Ankit Shukla +
* Anthony Sottile
* Antoine Pitrou
* Antony Lee
* Arcesio Castaneda Medina +
* Assem +
* Bernardt Duvenhage +
* Bharat Raghunathan +
* Bharat123rox +
* Bran +
* Bruce Merry +
* Charles Harris
* Chirag Nighut +
* Christoph Gohlke
* Christopher Whelan +
* Chuanzhu Xu +
* Colin Snyder +
* Dan Allan +
* Daniel Hrisca
* Daniel Lawrence +
* Debsankha Manik +
* Dennis Zollo +
* Dieter Werthmüller +
* Dominic Jack +
* EelcoPeacs +
* Eric Larson
* Eric Wieser
* Fabrice Fontaine +
* Gary Gurlaskie +
* Gregory Lee +
* Gregory R. Lee
* Guillaume Horel +
* Hameer Abbasi
* Haoyu Sun +
* Harmon +
* He Jia +
* Hunter Damron +
* Ian Sanders +
* Ilja +
* Isaac Virshup +
* Isaiah Norton +
* Jackie Leng +
* Jaime Fernandez
* Jakub Wilk
* Jan S. (Milania1) +
* Jarrod Millman
* Javier Dehesa +
* Jeremy Lay +
* Jim Turner +
* Jingbei Li +
* Joachim Hereth +
* Johannes Hampp +
* John Belmonte +
* John Kirkham
* John Law +
* Jonas Jensen
* Joseph Fox-Rabinovitz
* Joseph Martinot-Lagarde
* Josh Wilson
* Juan Luis Cano Rodríguez
* Julian Taylor
* Jérémie du Boisberranger +
* Kai Striega +
* Katharine Hyatt +
* Kevin Sheppard
* Kexuan Sun
* Kiko Correoso +
* Kriti Singh +
* Lars Grueter +
* Luis Pedro Coelho
* Maksim Shabunin +
* Manvi07 +
* Mark Harfouche
* Marten van Kerkwijk
* Martin Reinecke +
* Matthew Brett
* Matthias Bussonnier
* Matti Picus
* Michel Fruchart +
* Mike Lui +
* Mike Taves +
* Min ho Kim +
* Mircea Akos Bruma
* Nick Minkyu Lee
* Nick Papior
* Nick R. Papior +
* Nicola Soranzo +
* Nimish Telang +
* OBATA Akio +
* Oleksandr Pavlyk
* Ori Broda +
* Paul Ivanov
* Pauli Virtanen
* Peter Andreas Entschev +
* Peter Bell +
* Pierre de Buyl
* Piyush Jaipuriayar +
* Prithvi MK +
* Raghuveer Devulapalli +
* Ralf Gommers
* Richard Harris +
* Rishabh Chakrabarti +
* Riya Sharma +
* Robert Kern
* Roman Yurchak
* Ryan Levy +
* Sebastian Berg
* Sergei Lebedev +
* Shekhar Prasad Rajak +
* Stefan van der Walt
* Stephan Hoyer
* Steve Stagg +
* SuryaChand P +
* Søren Rasmussen +
* Thibault Hallouin +
* Thomas A Caswell
* Tobias Uelwer +
* Tony LaTorre +
* Toshiki Kataoka
* Tyler Moncur +
* Tyler Reddy
* Valentin Haenel
* Vrinda Narayan +
* Warren Weckesser
* Weitang Li
* Wojtek Ruszczewski
* Yu Feng
* Yu Kobayashi +
* Yury Kirienko +
* aashuli +
* luzpaz
* parul +
* spacescientist +

Pull requests merged
====================

A total of 532 pull requests were merged for this release.

* `#4808 <https://github.com/numpy/numpy/pull/4808>`__: ENH: Make the `mode` parameter of np.pad default to 'constant'
* `#8131 <https://github.com/numpy/numpy/pull/8131>`__: BUG: Fix help() formatting for deprecated functions.
* `#8159 <https://github.com/numpy/numpy/pull/8159>`__: ENH: Add import time benchmarks.
* `#8641 <https://github.com/numpy/numpy/pull/8641>`__: BUG: Preserve types of empty arrays in ix_ when known
* `#8662 <https://github.com/numpy/numpy/pull/8662>`__: ENH: preserve subclasses in ufunc.outer
* `#9330 <https://github.com/numpy/numpy/pull/9330>`__: ENH: Make errstate a ContextDecorator in Python3
* `#10308 <https://github.com/numpy/numpy/pull/10308>`__: API: Make MaskedArray.mask return a view, rather than the underlying...
* `#10417 <https://github.com/numpy/numpy/pull/10417>`__: ENH: Allow dtype objects to be indexed with multiple fields at...
* `#10723 <https://github.com/numpy/numpy/pull/10723>`__: BUG: longdouble(int) does not work
* `#10741 <https://github.com/numpy/numpy/pull/10741>`__: ENH: Implement `np.floating.as_integer_ratio`
* `#10855 <https://github.com/numpy/numpy/pull/10855>`__: ENH: Adding a count parameter to np.unpackbits
* `#11230 <https://github.com/numpy/numpy/pull/11230>`__: MAINT: More cleanup of einsum
* `#11233 <https://github.com/numpy/numpy/pull/11233>`__: BUG: ensure i0 does not change the shape.
* `#11358 <https://github.com/numpy/numpy/pull/11358>`__: MAINT: Rewrite numpy.pad without concatenate
* `#11684 <https://github.com/numpy/numpy/pull/11684>`__: BUG: Raise when unravel_index, ravel_multi_index are given empty...
* `#11689 <https://github.com/numpy/numpy/pull/11689>`__: DOC: Add ref docs for C generic types.
* `#11721 <https://github.com/numpy/numpy/pull/11721>`__: BUG: Make `arr.ctypes.data` hold onto a reference to the underlying...
* `#11829 <https://github.com/numpy/numpy/pull/11829>`__: MAINT: Use textwrap.dedent in f2py tests
* `#11859 <https://github.com/numpy/numpy/pull/11859>`__: BUG: test and fix np.dtype('i,L') #5645
* `#11888 <https://github.com/numpy/numpy/pull/11888>`__: ENH: Add pocketfft sources to numpy for testing, benchmarks,...
* `#11977 <https://github.com/numpy/numpy/pull/11977>`__: BUG: reference cycle in np.vectorize
* `#12025 <https://github.com/numpy/numpy/pull/12025>`__: DOC: add detail for 'where' argument in ufunc
* `#12152 <https://github.com/numpy/numpy/pull/12152>`__: TST: Added tests for np.tensordot()
* `#12201 <https://github.com/numpy/numpy/pull/12201>`__: TST: coverage for _commonType()
* `#12234 <https://github.com/numpy/numpy/pull/12234>`__: MAINT: refactor PyArray_AdaptFlexibleDType to return a meaningful...
* `#12239 <https://github.com/numpy/numpy/pull/12239>`__: BUG: polyval returned non-masked arrays for masked input.
* `#12253 <https://github.com/numpy/numpy/pull/12253>`__: DOC, TST: enable doctests
* `#12308 <https://github.com/numpy/numpy/pull/12308>`__: ENH: add mm->q floordiv
* `#12317 <https://github.com/numpy/numpy/pull/12317>`__: ENH: port np.core.overrides to C for speed
* `#12333 <https://github.com/numpy/numpy/pull/12333>`__: DOC: update description of the Dirichlet distribution
* `#12418 <https://github.com/numpy/numpy/pull/12418>`__: ENH: Add timsort to npysort
* `#12428 <https://github.com/numpy/numpy/pull/12428>`__: ENH: always use zip64, upgrade pickle protocol to 3
* `#12456 <https://github.com/numpy/numpy/pull/12456>`__: ENH: Add np.ctypeslib.as_ctypes_type(dtype), improve `np.ctypeslib.as_ctypes`
* `#12457 <https://github.com/numpy/numpy/pull/12457>`__: TST: openblas for Azure MacOS
* `#12463 <https://github.com/numpy/numpy/pull/12463>`__: DOC: fix docstrings for broadcastable inputs in ufunc
* `#12502 <https://github.com/numpy/numpy/pull/12502>`__: TST: Azure Python version fix
* `#12506 <https://github.com/numpy/numpy/pull/12506>`__: MAINT: Prepare master for 1.17.0 development.
* `#12508 <https://github.com/numpy/numpy/pull/12508>`__: DOC, MAINT: Make `PYVER = 3` in doc/Makefile.
* `#12511 <https://github.com/numpy/numpy/pull/12511>`__: BUG: don't check alignment of size=0 arrays (RELAXED_STRIDES)
* `#12512 <https://github.com/numpy/numpy/pull/12512>`__: added template-generated files to .gitignore
* `#12519 <https://github.com/numpy/numpy/pull/12519>`__: ENH/DEP: Use a ufunc under the hood for ndarray.clip
* `#12522 <https://github.com/numpy/numpy/pull/12522>`__: BUG: Make new-lines in compiler error messages print to the console
* `#12524 <https://github.com/numpy/numpy/pull/12524>`__: BUG: fix improper use of C-API
* `#12526 <https://github.com/numpy/numpy/pull/12526>`__: BUG: reorder operations for VS2015
* `#12527 <https://github.com/numpy/numpy/pull/12527>`__: DEV: Fix lgtm.com C/C++ build
* `#12528 <https://github.com/numpy/numpy/pull/12528>`__: BUG: fix an unsafe PyTuple_GET_ITEM call
* `#12532 <https://github.com/numpy/numpy/pull/12532>`__: DEV: add ctags option file
* `#12534 <https://github.com/numpy/numpy/pull/12534>`__: DOC: Fix desc. of Ellipsis behavior in reference
* `#12537 <https://github.com/numpy/numpy/pull/12537>`__: DOC: Change 'num' to 'np'
* `#12538 <https://github.com/numpy/numpy/pull/12538>`__: MAINT: remove VC 9.0 from CI
* `#12539 <https://github.com/numpy/numpy/pull/12539>`__: DEV: remove travis 32 bit job since it is running on azure
* `#12543 <https://github.com/numpy/numpy/pull/12543>`__: TST: wheel-match Linux openblas in CI
* `#12544 <https://github.com/numpy/numpy/pull/12544>`__: BUG: fix refcount issue caused by #12524
* `#12545 <https://github.com/numpy/numpy/pull/12545>`__: BUG: Ensure probabilities are not NaN in choice
* `#12546 <https://github.com/numpy/numpy/pull/12546>`__: BUG: check for errors after PyArray_DESCR_REPLACE
* `#12547 <https://github.com/numpy/numpy/pull/12547>`__: ENH: Cast covariance to double in random mvnormal
* `#12549 <https://github.com/numpy/numpy/pull/12549>`__: TST: relax codecov project threshold
* `#12551 <https://github.com/numpy/numpy/pull/12551>`__: MAINT: add warning to numpy.distutils for LDFLAGS append behavior.
* `#12552 <https://github.com/numpy/numpy/pull/12552>`__: BENCH: Improve benchmarks for numpy.pad
* `#12554 <https://github.com/numpy/numpy/pull/12554>`__: DOC: more doc updates for structured arrays
* `#12555 <https://github.com/numpy/numpy/pull/12555>`__: BUG: only override vector size for avx code
* `#12560 <https://github.com/numpy/numpy/pull/12560>`__: DOC: fix some doctest failures
* `#12566 <https://github.com/numpy/numpy/pull/12566>`__: BUG: fix segfault in ctypeslib with obj being collected
* `#12571 <https://github.com/numpy/numpy/pull/12571>`__: Revert "Merge pull request #11721 from eric-wieser/fix-9647"
* `#12572 <https://github.com/numpy/numpy/pull/12572>`__: BUG: Make `arr.ctypes.data` hold a reference to the underlying...
* `#12575 <https://github.com/numpy/numpy/pull/12575>`__: ENH: improve performance for numpy.core.records.find_duplicate
* `#12577 <https://github.com/numpy/numpy/pull/12577>`__: BUG: fix f2py pep338 execution method
* `#12578 <https://github.com/numpy/numpy/pull/12578>`__: TST: activate shippable maintenance branches
* `#12583 <https://github.com/numpy/numpy/pull/12583>`__: TST: add test for 'python -mnumpy.f2py'
* `#12584 <https://github.com/numpy/numpy/pull/12584>`__: Clarify skiprows in loadtxt
* `#12586 <https://github.com/numpy/numpy/pull/12586>`__: ENH: Implement radix sort
* `#12589 <https://github.com/numpy/numpy/pull/12589>`__: MAINT: Update changelog.py for Python 3.
* `#12591 <https://github.com/numpy/numpy/pull/12591>`__: ENH: add "max difference" messages to np.testing.assert_array_equal
* `#12592 <https://github.com/numpy/numpy/pull/12592>`__: BUG,TST: Remove the misguided `run_command` that wraps subprocess
* `#12593 <https://github.com/numpy/numpy/pull/12593>`__: ENH,WIP: Use richer exception types for ufunc type resolution...
* `#12594 <https://github.com/numpy/numpy/pull/12594>`__: DEV, BUILD: add pypy3 to azure CI
* `#12596 <https://github.com/numpy/numpy/pull/12596>`__: ENH: improve performance of numpy.core.records.fromarrays
* `#12601 <https://github.com/numpy/numpy/pull/12601>`__: DOC: Correct documentation of `numpy.delete` obj parameter.
* `#12602 <https://github.com/numpy/numpy/pull/12602>`__: DOC: Update RELEASE_WALKTHROUGH.rst.txt.
* `#12604 <https://github.com/numpy/numpy/pull/12604>`__: BUG: Check that dtype and formats arguments for None.
* `#12606 <https://github.com/numpy/numpy/pull/12606>`__: DOC: Document NPY_SORTKIND parameter in PyArray_Sort
* `#12608 <https://github.com/numpy/numpy/pull/12608>`__: MAINT: Use `*.format` for some strings.
* `#12609 <https://github.com/numpy/numpy/pull/12609>`__: ENH: Deprecate writeable broadcast_array
* `#12610 <https://github.com/numpy/numpy/pull/12610>`__: TST: Update runtests.py to specify C99 for gcc.
* `#12611 <https://github.com/numpy/numpy/pull/12611>`__: BUG: longdouble with elsize 12 is never uint alignable
* `#12612 <https://github.com/numpy/numpy/pull/12612>`__: TST: Update `travis-test.sh` for C99
* `#12616 <https://github.com/numpy/numpy/pull/12616>`__: BLD: Fix minimum Python version in setup.py
* `#12617 <https://github.com/numpy/numpy/pull/12617>`__: BUG: Add missing free in ufunc dealloc
* `#12618 <https://github.com/numpy/numpy/pull/12618>`__: MAINT: add test for 12-byte alignment
* `#12620 <https://github.com/numpy/numpy/pull/12620>`__: BLD: move -std=c99 addition to CFLAGS to Azure config
* `#12624 <https://github.com/numpy/numpy/pull/12624>`__: BUG: Fix incorrect/missing reference cleanups found using valgrind
* `#12626 <https://github.com/numpy/numpy/pull/12626>`__: BUG: fix uint alignment asserts in lowlevel loops
* `#12631 <https://github.com/numpy/numpy/pull/12631>`__: BUG: fix f2py problem to build wrappers using PGI's Fortran
* `#12634 <https://github.com/numpy/numpy/pull/12634>`__: DOC, TST: remove "agg" setting from docs
* `#12639 <https://github.com/numpy/numpy/pull/12639>`__: BENCH: don't fail at import time with old Numpy
* `#12641 <https://github.com/numpy/numpy/pull/12641>`__: DOC: update 2018 -> 2019
* `#12644 <https://github.com/numpy/numpy/pull/12644>`__: ENH: where for ufunc reductions
* `#12645 <https://github.com/numpy/numpy/pull/12645>`__: DOC: Minor fix to pocketfft release note
* `#12650 <https://github.com/numpy/numpy/pull/12650>`__: BUG: Fix reference counting for subarrays containing objects
* `#12651 <https://github.com/numpy/numpy/pull/12651>`__: DOC: SimpleNewFromDescr cannot be given NULL for descr
* `#12666 <https://github.com/numpy/numpy/pull/12666>`__: BENCH: add asv nanfunction benchmarks
* `#12668 <https://github.com/numpy/numpy/pull/12668>`__: ENH: Improve error messages for non-matching shapes in concatenate.
* `#12671 <https://github.com/numpy/numpy/pull/12671>`__: TST: Fix endianness in unstuctured_to_structured test
* `#12672 <https://github.com/numpy/numpy/pull/12672>`__: BUG: Add 'sparc' to platforms implementing 16 byte reals.
* `#12677 <https://github.com/numpy/numpy/pull/12677>`__: MAINT: Further fixups to uint alignment checks
* `#12679 <https://github.com/numpy/numpy/pull/12679>`__: ENH: remove "Invalid value" warnings from median, percentile
* `#12680 <https://github.com/numpy/numpy/pull/12680>`__: BUG: Ensure failing memory allocations are reported
* `#12683 <https://github.com/numpy/numpy/pull/12683>`__: ENH: add mm->qm divmod
* `#12684 <https://github.com/numpy/numpy/pull/12684>`__: DEV: remove _arg from public API, add matmul to benchmark ufuncs
* `#12685 <https://github.com/numpy/numpy/pull/12685>`__: BUG: Make pocketfft handle long doubles.
* `#12687 <https://github.com/numpy/numpy/pull/12687>`__: ENH: Better links in documentation
* `#12690 <https://github.com/numpy/numpy/pull/12690>`__: WIP, ENH: add _nan_mask function
* `#12693 <https://github.com/numpy/numpy/pull/12693>`__: ENH: Add a hermitian argument to `pinv` and `svd`, matching `matrix_rank`
* `#12696 <https://github.com/numpy/numpy/pull/12696>`__: BUG: Fix leak of void scalar buffer info
* `#12698 <https://github.com/numpy/numpy/pull/12698>`__: DOC: improve comments in copycast_isaligned
* `#12700 <https://github.com/numpy/numpy/pull/12700>`__: ENH: chain additional exception on ufunc method lookup error
* `#12702 <https://github.com/numpy/numpy/pull/12702>`__: TST: Check FFT results for C/Fortran ordered and non contiguous...
* `#12704 <https://github.com/numpy/numpy/pull/12704>`__: TST: pin Azure brew version for stability
* `#12709 <https://github.com/numpy/numpy/pull/12709>`__: TST: add ppc64le to Travis CI matrix
* `#12713 <https://github.com/numpy/numpy/pull/12713>`__: BUG: loosen kwargs requirements in ediff1d
* `#12722 <https://github.com/numpy/numpy/pull/12722>`__: BUG: Fix rounding of denormals in double and float to half casts...
* `#12723 <https://github.com/numpy/numpy/pull/12723>`__: BENCH: Include other sort benchmarks
* `#12724 <https://github.com/numpy/numpy/pull/12724>`__: BENCH: quiet DeprecationWarning
* `#12727 <https://github.com/numpy/numpy/pull/12727>`__: DOC: fix and doctest tutorial
* `#12728 <https://github.com/numpy/numpy/pull/12728>`__: DOC: clarify the suffix of single/extended precision math constants
* `#12729 <https://github.com/numpy/numpy/pull/12729>`__: DOC: Extend documentation of `ndarray.tolist`
* `#12731 <https://github.com/numpy/numpy/pull/12731>`__: DOC: Update release notes and changelog after 1.16.0 release.
* `#12733 <https://github.com/numpy/numpy/pull/12733>`__: DOC: clarify the extend of __array_function__ support in NumPy...
* `#12741 <https://github.com/numpy/numpy/pull/12741>`__: DOC: fix generalized eigenproblem reference in "NumPy for MATLAB...
* `#12743 <https://github.com/numpy/numpy/pull/12743>`__: BUG: Fix crash in error message formatting introduced by gh-11230
* `#12748 <https://github.com/numpy/numpy/pull/12748>`__: BUG: Fix SystemError when pickling datetime64 array with pickle5
* `#12757 <https://github.com/numpy/numpy/pull/12757>`__: BUG: Added parens to macro argument expansions
* `#12758 <https://github.com/numpy/numpy/pull/12758>`__: DOC: Update docstring of diff() to use 'i' not 'n'
* `#12762 <https://github.com/numpy/numpy/pull/12762>`__: MAINT: Change the order of checking for locale file and import...
* `#12783 <https://github.com/numpy/numpy/pull/12783>`__: DOC: document C99 requirement in dev guide
* `#12787 <https://github.com/numpy/numpy/pull/12787>`__: DOC: remove recommendation to add main for testing
* `#12805 <https://github.com/numpy/numpy/pull/12805>`__: BUG: double decref of dtype in failure codepath. Test and fix
* `#12807 <https://github.com/numpy/numpy/pull/12807>`__: BUG, DOC: test, fix that f2py.compile accepts str and bytes,...
* `#12814 <https://github.com/numpy/numpy/pull/12814>`__: BUG: resolve writeback in arr_insert failure paths
* `#12815 <https://github.com/numpy/numpy/pull/12815>`__: BUG: Fix testing of f2py.compile from strings.
* `#12818 <https://github.com/numpy/numpy/pull/12818>`__: DOC: remove python2-only methods, small cleanups
* `#12824 <https://github.com/numpy/numpy/pull/12824>`__: BUG: fix to check before apply `shlex.split`
* `#12830 <https://github.com/numpy/numpy/pull/12830>`__: ENH: __array_function__ updates for NumPy 1.17.0
* `#12831 <https://github.com/numpy/numpy/pull/12831>`__: BUG: Catch stderr when checking compiler version
* `#12842 <https://github.com/numpy/numpy/pull/12842>`__: BUG: ndarrays pickled by 1.16 cannot be loaded by 1.15.4 and...
* `#12846 <https://github.com/numpy/numpy/pull/12846>`__: BUG: fix signed zero behavior in npy_divmod
* `#12850 <https://github.com/numpy/numpy/pull/12850>`__: BUG: fail if old multiarray module detected
* `#12851 <https://github.com/numpy/numpy/pull/12851>`__: TEST: use xenial by default for travis
* `#12854 <https://github.com/numpy/numpy/pull/12854>`__: BUG: do not Py_DECREF NULL pointer
* `#12857 <https://github.com/numpy/numpy/pull/12857>`__: STY: simplify code
* `#12863 <https://github.com/numpy/numpy/pull/12863>`__: TEST: pin mingw version
* `#12866 <https://github.com/numpy/numpy/pull/12866>`__: DOC: link to benchmarking info
* `#12867 <https://github.com/numpy/numpy/pull/12867>`__: TST: Use same OpenBLAS build for testing as for current wheels.
* `#12871 <https://github.com/numpy/numpy/pull/12871>`__: ENH: add c-imported modules to namespace for freeze analysis
* `#12877 <https://github.com/numpy/numpy/pull/12877>`__: Remove deprecated ``sudo: false`` from .travis.yml
* `#12879 <https://github.com/numpy/numpy/pull/12879>`__: DEP: deprecate exec_command
* `#12885 <https://github.com/numpy/numpy/pull/12885>`__: DOC: fix math formatting of np.linalg.lstsq docs
* `#12886 <https://github.com/numpy/numpy/pull/12886>`__: DOC: add missing character routines, fix #8578
* `#12887 <https://github.com/numpy/numpy/pull/12887>`__: BUG: Fix np.rec.fromarrays on arrays which are already structured
* `#12889 <https://github.com/numpy/numpy/pull/12889>`__: BUG: Make allow_pickle=False the default for loading
* `#12892 <https://github.com/numpy/numpy/pull/12892>`__: BUG: Do not double-quote arguments passed on to the linker
* `#12894 <https://github.com/numpy/numpy/pull/12894>`__: MAINT: Removed unused and confusingly indirect imports from mingw32ccompiler
* `#12895 <https://github.com/numpy/numpy/pull/12895>`__: BUG: Do not insert extra double quote into preprocessor macros
* `#12903 <https://github.com/numpy/numpy/pull/12903>`__: TST: fix vmImage dispatch in Azure
* `#12905 <https://github.com/numpy/numpy/pull/12905>`__: BUG: fix byte order reversal for datetime64[ns]
* `#12908 <https://github.com/numpy/numpy/pull/12908>`__: DOC: Update master following 1.16.1 release.
* `#12911 <https://github.com/numpy/numpy/pull/12911>`__: BLD: fix doc build for distribution.
* `#12915 <https://github.com/numpy/numpy/pull/12915>`__: ENH: pathlib support for fromfile(), .tofile() and .dump()
* `#12920 <https://github.com/numpy/numpy/pull/12920>`__: MAINT: remove complicated test of multiarray import failure mode
* `#12922 <https://github.com/numpy/numpy/pull/12922>`__: DOC: Add note about arbitrary code execution to numpy.load
* `#12925 <https://github.com/numpy/numpy/pull/12925>`__: BUG: parse shell escaping in extra_compile_args and extra_link_args
* `#12928 <https://github.com/numpy/numpy/pull/12928>`__: MAINT: Merge together the unary and binary type resolvers
* `#12929 <https://github.com/numpy/numpy/pull/12929>`__: DOC: fix documentation bug in np.argsort and extend examples
* `#12931 <https://github.com/numpy/numpy/pull/12931>`__: MAINT: Remove recurring check
* `#12932 <https://github.com/numpy/numpy/pull/12932>`__: BUG: do not dereference NULL pointer
* `#12937 <https://github.com/numpy/numpy/pull/12937>`__: DOC: Correct negative_binomial docstring
* `#12944 <https://github.com/numpy/numpy/pull/12944>`__: BUG: Make timsort deal with zero length elements.
* `#12945 <https://github.com/numpy/numpy/pull/12945>`__: BUG: Add timsort without breaking the API.
* `#12949 <https://github.com/numpy/numpy/pull/12949>`__: DOC: ndarray.max is missing
* `#12962 <https://github.com/numpy/numpy/pull/12962>`__: ENH: Add 'bitorder' keyword to packbits, unpackbits
* `#12963 <https://github.com/numpy/numpy/pull/12963>`__: DOC: Grammatical fix in numpy doc
* `#12964 <https://github.com/numpy/numpy/pull/12964>`__: DOC: Document that ``scale==0`` is now allowed in many distributions.
* `#12965 <https://github.com/numpy/numpy/pull/12965>`__: DOC: Properly format Return section of ogrid Docstring,
* `#12968 <https://github.com/numpy/numpy/pull/12968>`__: BENCH: Re-write sorting benchmarks
* `#12971 <https://github.com/numpy/numpy/pull/12971>`__: ENH: Add 'offset' keyword to 'numpy.fromfile()'
* `#12973 <https://github.com/numpy/numpy/pull/12973>`__: DOC: Recommend adding dimension to switch between row and column...
* `#12983 <https://github.com/numpy/numpy/pull/12983>`__: DOC: Randomstate docstring fixes
* `#12984 <https://github.com/numpy/numpy/pull/12984>`__: DOC: Add examples of negative shifts in np.roll
* `#12986 <https://github.com/numpy/numpy/pull/12986>`__: BENCH: set ones in any/all benchmarks to 1 instead of 0
* `#12988 <https://github.com/numpy/numpy/pull/12988>`__: ENH: Create boolean and integer ufuncs for isnan, isinf, and...
* `#12989 <https://github.com/numpy/numpy/pull/12989>`__: ENH: Correct handling of infinities in np.interp (option B)
* `#12995 <https://github.com/numpy/numpy/pull/12995>`__: BUG: Add missing PyErr_NoMemory() for reporting a failed malloc
* `#12996 <https://github.com/numpy/numpy/pull/12996>`__: MAINT: Use the same multiplication order in interp for cached...
* `#13002 <https://github.com/numpy/numpy/pull/13002>`__: DOC: reduce warnings when building, and rephrase slightly
* `#13004 <https://github.com/numpy/numpy/pull/13004>`__: MAINT: minor changes for consistency to site.cfg.example
* `#13008 <https://github.com/numpy/numpy/pull/13008>`__: MAINT: Move pickle import to numpy.compat
* `#13019 <https://github.com/numpy/numpy/pull/13019>`__: BLD: Windows absolute path DLL loading
* `#13023 <https://github.com/numpy/numpy/pull/13023>`__: BUG: Changes to string-to-shell parsing behavior broke paths...
* `#13027 <https://github.com/numpy/numpy/pull/13027>`__: BUG: Fix regression in parsing of F90 and F77 environment variables
* `#13031 <https://github.com/numpy/numpy/pull/13031>`__: MAINT: Replace if statement with a dictionary lookup for ease...
* `#13032 <https://github.com/numpy/numpy/pull/13032>`__: MAINT: Extract the loop macros into their own header
* `#13033 <https://github.com/numpy/numpy/pull/13033>`__: MAINT: Convert property to @property
* `#13035 <https://github.com/numpy/numpy/pull/13035>`__: DOC: Draw more attention to which functions in random are convenience...
* `#13036 <https://github.com/numpy/numpy/pull/13036>`__: BUG: __array_interface__ offset was always ignored
* `#13039 <https://github.com/numpy/numpy/pull/13039>`__: BUG: Remove error-prone borrowed reference handling
* `#13044 <https://github.com/numpy/numpy/pull/13044>`__: DOC: link to devdocs in README
* `#13046 <https://github.com/numpy/numpy/pull/13046>`__: ENH: Add shape to *_like() array creation
* `#13049 <https://github.com/numpy/numpy/pull/13049>`__: MAINT: remove undocumented __buffer__ attribute lookup
* `#13050 <https://github.com/numpy/numpy/pull/13050>`__: BLD: make doc build work more robustly.
* `#13054 <https://github.com/numpy/numpy/pull/13054>`__: DOC: Added maximum_sctype to documentation
* `#13055 <https://github.com/numpy/numpy/pull/13055>`__: DOC: Post NumPy 1.16.2 release update.
* `#13056 <https://github.com/numpy/numpy/pull/13056>`__: BUG: Fixes to numpy.distutils.Configuration.get_version
* `#13058 <https://github.com/numpy/numpy/pull/13058>`__: DOC: update docstring in numpy.interp docstring
* `#13060 <https://github.com/numpy/numpy/pull/13060>`__: BUG: Use C call to sysctlbyname for AVX detection on MacOS
* `#13063 <https://github.com/numpy/numpy/pull/13063>`__: DOC: revert PR #13058 and fixup Makefile
* `#13067 <https://github.com/numpy/numpy/pull/13067>`__: MAINT: Use with statements for opening files in distutils
* `#13068 <https://github.com/numpy/numpy/pull/13068>`__: BUG: Add error checks when converting integers to datetime types
* `#13071 <https://github.com/numpy/numpy/pull/13071>`__: DOC: Removed incorrect claim regarding shape constraints for...
* `#13073 <https://github.com/numpy/numpy/pull/13073>`__: MAINT: Fix ABCPolyBase in various ways
* `#13075 <https://github.com/numpy/numpy/pull/13075>`__: BUG: Convert fortran flags in environment variable
* `#13076 <https://github.com/numpy/numpy/pull/13076>`__: BUG: Remove our patched version of `distutils.split_quoted`
* `#13077 <https://github.com/numpy/numpy/pull/13077>`__: BUG: Fix errors in string formatting while producing an error
* `#13078 <https://github.com/numpy/numpy/pull/13078>`__: MAINT: deduplicate fromroots in np.polynomial
* `#13079 <https://github.com/numpy/numpy/pull/13079>`__: MAINT: Merge duplicate implementations of `*vander2d` and `*vander3d`...
* `#13086 <https://github.com/numpy/numpy/pull/13086>`__: BLD: fix include list for sdist building
* `#13090 <https://github.com/numpy/numpy/pull/13090>`__: BUILD: sphinx 1.8.3 can be used with our outdated templates
* `#13092 <https://github.com/numpy/numpy/pull/13092>`__: BUG: ensure linspace works on object input.
* `#13093 <https://github.com/numpy/numpy/pull/13093>`__: BUG: Fix parameter validity checks in ``random.choice``.
* `#13095 <https://github.com/numpy/numpy/pull/13095>`__: BUG: Fix testsuite failures on ppc and riscv
* `#13096 <https://github.com/numpy/numpy/pull/13096>`__: TEST: allow refcheck result to vary, increase discoverability...
* `#13097 <https://github.com/numpy/numpy/pull/13097>`__: DOC: update doc of `ndarray.T`
* `#13099 <https://github.com/numpy/numpy/pull/13099>`__: DOC: Add note about "copy and slicing"
* `#13104 <https://github.com/numpy/numpy/pull/13104>`__: DOC: fix references in docs
* `#13107 <https://github.com/numpy/numpy/pull/13107>`__: MAINT: Unify polynomial valnd functions
* `#13108 <https://github.com/numpy/numpy/pull/13108>`__: MAINT: Merge duplicate implementations of `hermvander2d` and...
* `#13109 <https://github.com/numpy/numpy/pull/13109>`__: Prevent traceback chaining in _wrapfunc.
* `#13111 <https://github.com/numpy/numpy/pull/13111>`__: MAINT: Unify polydiv
* `#13115 <https://github.com/numpy/numpy/pull/13115>`__: DOC: Fix #12050 by updating numpy.random.hypergeometric docs
* `#13116 <https://github.com/numpy/numpy/pull/13116>`__: DOC: Add backticks in linalg docstrings.
* `#13117 <https://github.com/numpy/numpy/pull/13117>`__: DOC: Fix arg type for np.pad, fix #9489
* `#13118 <https://github.com/numpy/numpy/pull/13118>`__: DOC: update scipy-sphinx-theme, fixes search
* `#13119 <https://github.com/numpy/numpy/pull/13119>`__: DOC: Fix c-api function documentation duplication.
* `#13125 <https://github.com/numpy/numpy/pull/13125>`__: BUG: Fix unhandled exception in CBLAS detection
* `#13126 <https://github.com/numpy/numpy/pull/13126>`__: DEP: polynomial: Be stricter about integral arguments
* `#13127 <https://github.com/numpy/numpy/pull/13127>`__: DOC: Tidy 1.17.0 release note newlines
* `#13128 <https://github.com/numpy/numpy/pull/13128>`__: MAINT: Unify polynomial addition and subtraction functions
* `#13130 <https://github.com/numpy/numpy/pull/13130>`__: MAINT: Unify polynomial fitting functions
* `#13131 <https://github.com/numpy/numpy/pull/13131>`__: BUILD: use 'quiet' when building docs
* `#13132 <https://github.com/numpy/numpy/pull/13132>`__: BLD: Allow users to specify BLAS and LAPACK library link order
* `#13134 <https://github.com/numpy/numpy/pull/13134>`__: ENH: Use AVX for float32 implementation of np.exp & np.log
* `#13137 <https://github.com/numpy/numpy/pull/13137>`__: BUG: Fix build for glibc on ARC and uclibc.
* `#13140 <https://github.com/numpy/numpy/pull/13140>`__: DEV: cleanup imports and some assignments (from LGTM)
* `#13146 <https://github.com/numpy/numpy/pull/13146>`__: MAINT: Unify polynomial power functions
* `#13147 <https://github.com/numpy/numpy/pull/13147>`__: DOC: Add description of overflow errors
* `#13149 <https://github.com/numpy/numpy/pull/13149>`__: DOC: correction to numpy.pad docstring
* `#13157 <https://github.com/numpy/numpy/pull/13157>`__: BLD: streamlined library names in site.cfg sections
* `#13158 <https://github.com/numpy/numpy/pull/13158>`__: BLD: Add libflame as a LAPACK back-end
* `#13161 <https://github.com/numpy/numpy/pull/13161>`__: BLD: streamlined CBLAS linkage tries, default to try libraries...
* `#13162 <https://github.com/numpy/numpy/pull/13162>`__: BUILD: update numpydoc to latest version
* `#13163 <https://github.com/numpy/numpy/pull/13163>`__: ENH: randomgen
* `#13169 <https://github.com/numpy/numpy/pull/13169>`__: STY: Fix weird indents to be multiples of 4 spaces
* `#13170 <https://github.com/numpy/numpy/pull/13170>`__: DOC, BUILD: fail the devdoc build if there are warnings
* `#13174 <https://github.com/numpy/numpy/pull/13174>`__: DOC: Removed some c-api duplication
* `#13176 <https://github.com/numpy/numpy/pull/13176>`__: BUG: fix reference count error on invalid input to ndarray.flat
* `#13181 <https://github.com/numpy/numpy/pull/13181>`__: BENCH, BUG: fix Savez suite, previously was actually calling...
* `#13182 <https://github.com/numpy/numpy/pull/13182>`__: MAINT: add overlap checks to choose, take, put, putmask
* `#13188 <https://github.com/numpy/numpy/pull/13188>`__: MAINT: Simplify logic in convert_datetime_to_datetimestruct
* `#13202 <https://github.com/numpy/numpy/pull/13202>`__: ENH: use rotated companion matrix to reduce error
* `#13203 <https://github.com/numpy/numpy/pull/13203>`__: DOC: Use std docstring for multivariate normal
* `#13205 <https://github.com/numpy/numpy/pull/13205>`__: DOC : Fix C-API documentation references to items that don't...
* `#13206 <https://github.com/numpy/numpy/pull/13206>`__: BUILD: pin sphinx to 1.8.5
* `#13208 <https://github.com/numpy/numpy/pull/13208>`__: MAINT: cleanup of fast_loop_macros.h
* `#13216 <https://github.com/numpy/numpy/pull/13216>`__: Adding an example of successful execution of numpy.test() to...
* `#13217 <https://github.com/numpy/numpy/pull/13217>`__: TST: always publish Azure tests
* `#13218 <https://github.com/numpy/numpy/pull/13218>`__: ENH: `isfinite` support for `datetime64` and `timedelta64`
* `#13219 <https://github.com/numpy/numpy/pull/13219>`__: ENH: nan_to_num keyword addition (was #9355)
* `#13222 <https://github.com/numpy/numpy/pull/13222>`__: DOC: Document/ Deprecate functions exposed in "numpy" namespace
* `#13224 <https://github.com/numpy/numpy/pull/13224>`__: Improve error message for negative valued argument
* `#13226 <https://github.com/numpy/numpy/pull/13226>`__: DOC: Fix small issues in mtrand doc strings
* `#13231 <https://github.com/numpy/numpy/pull/13231>`__: DOC: Change the required Sphinx version to build documentation
* `#13234 <https://github.com/numpy/numpy/pull/13234>`__: DOC : PyArray_Descr.names undocumented
* `#13239 <https://github.com/numpy/numpy/pull/13239>`__: DOC: Minor grammatical fixes in NumPy docs
* `#13242 <https://github.com/numpy/numpy/pull/13242>`__: DOC: fix docstring for floor_divide
* `#13243 <https://github.com/numpy/numpy/pull/13243>`__: MAINT: replace SETREF with assignment to ret array in ndarray.flat
* `#13244 <https://github.com/numpy/numpy/pull/13244>`__: DOC: Improve mtrand docstrings
* `#13250 <https://github.com/numpy/numpy/pull/13250>`__: MAINT: Improve efficiency of pad by avoiding use of apply_along_axis
* `#13253 <https://github.com/numpy/numpy/pull/13253>`__: TST: fail Azure CI if test failures
* `#13259 <https://github.com/numpy/numpy/pull/13259>`__: DOC: Small readability improvement
* `#13262 <https://github.com/numpy/numpy/pull/13262>`__: DOC : Correcting bug on Documentation Page (Byteswapping)
* `#13264 <https://github.com/numpy/numpy/pull/13264>`__: TST: use OpenBLAS v0.3.5 for POWER8 CI runs
* `#13269 <https://github.com/numpy/numpy/pull/13269>`__: BUG, MAINT: f2py: Add a cast to avoid a compiler warning.
* `#13270 <https://github.com/numpy/numpy/pull/13270>`__: TST: use OpenBLAS v0.3.5 for ARMv8 CI
* `#13271 <https://github.com/numpy/numpy/pull/13271>`__: ENH: vectorize np.abs for unsigned ints and half, improving performance...
* `#13273 <https://github.com/numpy/numpy/pull/13273>`__: BUG: Fix null pointer dereference in PyArray_DTypeFromObject
* `#13277 <https://github.com/numpy/numpy/pull/13277>`__: DOC: Document caveat in random.uniform
* `#13287 <https://github.com/numpy/numpy/pull/13287>`__: Add benchmark for sorting random array.
* `#13289 <https://github.com/numpy/numpy/pull/13289>`__: DOC: add Quansight Labs as an Institutional Partner
* `#13291 <https://github.com/numpy/numpy/pull/13291>`__: MAINT: fix unused variable warning in npy_math_complex.c.src
* `#13292 <https://github.com/numpy/numpy/pull/13292>`__: DOC: update numpydoc to latest master
* `#13293 <https://github.com/numpy/numpy/pull/13293>`__: DOC: add more info to failure message
* `#13298 <https://github.com/numpy/numpy/pull/13298>`__: ENH: Added clearer exception for np.diff on 0-dimensional ndarray
* `#13301 <https://github.com/numpy/numpy/pull/13301>`__: BUG: Fix crash when calling savetxt on a padded array
* `#13305 <https://github.com/numpy/numpy/pull/13305>`__: NEP: Update NEP-18 to include the ``__skip_array_function__``...
* `#13306 <https://github.com/numpy/numpy/pull/13306>`__: MAINT: better MemoryError message (#13225)
* `#13309 <https://github.com/numpy/numpy/pull/13309>`__: DOC: list Quansight rather than Quansight Labs as Institutional...
* `#13310 <https://github.com/numpy/numpy/pull/13310>`__: ENH: Add project_urls to setup
* `#13311 <https://github.com/numpy/numpy/pull/13311>`__: BUG: Fix bad error message in np.memmap
* `#13312 <https://github.com/numpy/numpy/pull/13312>`__: BUG: Close files if an error occurs in genfromtxt
* `#13313 <https://github.com/numpy/numpy/pull/13313>`__: MAINT: fix typo in 'self'
* `#13314 <https://github.com/numpy/numpy/pull/13314>`__: DOC: remove misplaced section at bottom of governance people...
* `#13316 <https://github.com/numpy/numpy/pull/13316>`__: DOC: Added anti-diagonal examples to np.diagonal and np.fill_diagonal
* `#13320 <https://github.com/numpy/numpy/pull/13320>`__: MAINT: remove unused file
* `#13321 <https://github.com/numpy/numpy/pull/13321>`__: MAINT: Move exceptions from core._internal to core._exceptions
* `#13322 <https://github.com/numpy/numpy/pull/13322>`__: MAINT: Move umath error helpers into their own module
* `#13323 <https://github.com/numpy/numpy/pull/13323>`__: BUG: ufunc.at iteration variable size fix
* `#13324 <https://github.com/numpy/numpy/pull/13324>`__: MAINT: Move asarray helpers into their own module
* `#13326 <https://github.com/numpy/numpy/pull/13326>`__: DEP: Deprecate collapsing shape-1 dtype fields to scalars.
* `#13328 <https://github.com/numpy/numpy/pull/13328>`__: MAINT: Tidy up error message for accumulate and reduceat
* `#13331 <https://github.com/numpy/numpy/pull/13331>`__: DOC, BLD: fix doc build issues in preparation for the next numpydoc...
* `#13332 <https://github.com/numpy/numpy/pull/13332>`__: BUG: Always return views from structured_to_unstructured when...
* `#13334 <https://github.com/numpy/numpy/pull/13334>`__: BUG: Fix structured_to_unstructured on single-field types
* `#13335 <https://github.com/numpy/numpy/pull/13335>`__: DOC: Add as_ctypes_type to the documentation
* `#13336 <https://github.com/numpy/numpy/pull/13336>`__: BUILD: fail documentation build if numpy version does not match
* `#13337 <https://github.com/numpy/numpy/pull/13337>`__: DOC: Add docstrings for consistency in aliases
* `#13346 <https://github.com/numpy/numpy/pull/13346>`__: BUG/MAINT: Tidy typeinfo.h and .c
* `#13348 <https://github.com/numpy/numpy/pull/13348>`__: BUG: Return the coefficients array directly
* `#13354 <https://github.com/numpy/numpy/pull/13354>`__: TST: Added test_fftpocket.py::test_axes
* `#13367 <https://github.com/numpy/numpy/pull/13367>`__: DOC: reorganize developer docs, use scikit-image as a base for...
* `#13371 <https://github.com/numpy/numpy/pull/13371>`__: BUG/ENH: Make floor, ceil, and trunc call the matching special...
* `#13374 <https://github.com/numpy/numpy/pull/13374>`__: DOC: Specify range for numpy.angle
* `#13377 <https://github.com/numpy/numpy/pull/13377>`__: DOC: Add missing macros to C API documentation
* `#13379 <https://github.com/numpy/numpy/pull/13379>`__: BLD: address mingw-w64 issue. Follow-up to gh-9977
* `#13383 <https://github.com/numpy/numpy/pull/13383>`__: MAINT, DOC: Post 1.16.3 release updates
* `#13388 <https://github.com/numpy/numpy/pull/13388>`__: BUG: Some PyPy versions lack PyStructSequence_InitType2.
* `#13389 <https://github.com/numpy/numpy/pull/13389>`__: ENH: implement ``__skip_array_function__`` attribute for NEP-18
* `#13390 <https://github.com/numpy/numpy/pull/13390>`__: ENH: Add support for Fraction to percentile and quantile
* `#13391 <https://github.com/numpy/numpy/pull/13391>`__: MAINT, DEP: Fix deprecated ``assertEquals()``
* `#13395 <https://github.com/numpy/numpy/pull/13395>`__: DOC: note re defaults allclose to assert_allclose
* `#13397 <https://github.com/numpy/numpy/pull/13397>`__: DOC: Resolve confusion regarding hashtag in header line of csv
* `#13399 <https://github.com/numpy/numpy/pull/13399>`__: ENH: Improved performance of PyArray_FromAny for sequences of...
* `#13402 <https://github.com/numpy/numpy/pull/13402>`__: DOC: Show the default value of deletechars in the signature of...
* `#13403 <https://github.com/numpy/numpy/pull/13403>`__: DOC: fix typos in dev/index
* `#13404 <https://github.com/numpy/numpy/pull/13404>`__: DOC: Add Sebastian Berg as sponsored by BIDS
* `#13406 <https://github.com/numpy/numpy/pull/13406>`__: DOC: clarify array_{2string,str,repr} defaults
* `#13409 <https://github.com/numpy/numpy/pull/13409>`__: BUG: (py2 only) fix unicode support for savetxt fmt string
* `#13413 <https://github.com/numpy/numpy/pull/13413>`__: DOC: document existence of linalg backends
* `#13415 <https://github.com/numpy/numpy/pull/13415>`__: BUG: fixing bugs in AVX exp/log while handling special value...
* `#13416 <https://github.com/numpy/numpy/pull/13416>`__: BUG: Protect generators from log(0.0)
* `#13417 <https://github.com/numpy/numpy/pull/13417>`__: DOC: dimension sizes are non-negative, not positive
* `#13425 <https://github.com/numpy/numpy/pull/13425>`__: MAINT: fixed typo 'Mismacth' from numpy/core/setup_common.py
* `#13433 <https://github.com/numpy/numpy/pull/13433>`__: BUG: Handle subarrays in descr_to_dtype
* `#13435 <https://github.com/numpy/numpy/pull/13435>`__: BUG: Add TypeError to accepted exceptions in crackfortran.
* `#13436 <https://github.com/numpy/numpy/pull/13436>`__: TST: Add file-not-closed check to LGTM analysis.
* `#13440 <https://github.com/numpy/numpy/pull/13440>`__: MAINT: fixed typo 'wtihout' from numpy/core/shape_base.py
* `#13443 <https://github.com/numpy/numpy/pull/13443>`__: BLD, TST: implicit func errors
* `#13445 <https://github.com/numpy/numpy/pull/13445>`__: MAINT: refactor PyArrayMultiIterObject constructors
* `#13446 <https://github.com/numpy/numpy/pull/13446>`__: MANT: refactor unravel_index for code repetition
* `#13449 <https://github.com/numpy/numpy/pull/13449>`__: BUG: missing git raises an OSError
* `#13456 <https://github.com/numpy/numpy/pull/13456>`__: TST: refine Azure fail reports
* `#13463 <https://github.com/numpy/numpy/pull/13463>`__: BUG,DEP: Fix writeable flag setting for arrays without base
* `#13467 <https://github.com/numpy/numpy/pull/13467>`__: ENH: err msg for too large sequences. See #13450
* `#13469 <https://github.com/numpy/numpy/pull/13469>`__: DOC: correct "version added" in npymath docs
* `#13471 <https://github.com/numpy/numpy/pull/13471>`__: LICENSE: split license file in standard BSD 3-clause and bundled.
* `#13477 <https://github.com/numpy/numpy/pull/13477>`__: DOC: have notes in histogram_bin_edges match parameter style
* `#13479 <https://github.com/numpy/numpy/pull/13479>`__: DOC: Mention the handling of nan in the assert_equal docstring.
* `#13482 <https://github.com/numpy/numpy/pull/13482>`__: TEST: add duration report to tests, speed up two outliers
* `#13483 <https://github.com/numpy/numpy/pull/13483>`__: DOC: update mailmap for Bill Spotz
* `#13485 <https://github.com/numpy/numpy/pull/13485>`__: DOC: add security vulnerability reporting and doc links to README
* `#13491 <https://github.com/numpy/numpy/pull/13491>`__: BUG/ENH: Create npy format 3.0 to support extended unicode characters...
* `#13495 <https://github.com/numpy/numpy/pull/13495>`__: BUG: test all ufunc.types for return type, fix for exp, log
* `#13496 <https://github.com/numpy/numpy/pull/13496>`__: BUG: ma.tostring should respect the order parameter
* `#13498 <https://github.com/numpy/numpy/pull/13498>`__: DOC: Clarify rcond normalization in linalg.pinv
* `#13499 <https://github.com/numpy/numpy/pull/13499>`__: MAINT: Use with statement to open/close files to fix LGTM alerts
* `#13503 <https://github.com/numpy/numpy/pull/13503>`__: ENH: Support object arrays in matmul
* `#13504 <https://github.com/numpy/numpy/pull/13504>`__: DOC: Update links in PULL_REQUEST_TEMPLATE.md
* `#13506 <https://github.com/numpy/numpy/pull/13506>`__: ENH: Add sparse option to np.core.numeric.indices
* `#13507 <https://github.com/numpy/numpy/pull/13507>`__: BUG: np.array cleared errors occurred in PyMemoryView_FromObject
* `#13508 <https://github.com/numpy/numpy/pull/13508>`__: BUG: Removes ValueError for empty kwargs in arraymultiter_new
* `#13518 <https://github.com/numpy/numpy/pull/13518>`__: MAINT: implement assert_array_compare without converting array...
* `#13520 <https://github.com/numpy/numpy/pull/13520>`__: BUG: exp, log AVX loops do not use steps
* `#13523 <https://github.com/numpy/numpy/pull/13523>`__: BUG: distutils/system_info.py fix missing subprocess import
* `#13529 <https://github.com/numpy/numpy/pull/13529>`__: MAINT: Use exec() instead array_function_dispatch to improve...
* `#13530 <https://github.com/numpy/numpy/pull/13530>`__: BENCH: Modify benchmarks for radix sort.
* `#13534 <https://github.com/numpy/numpy/pull/13534>`__: BLD: Make CI pass again with pytest 4.5
* `#13541 <https://github.com/numpy/numpy/pull/13541>`__: ENH: restore unpack bit lookup table
* `#13544 <https://github.com/numpy/numpy/pull/13544>`__: ENH: Allow broadcast to be called with zero arguments
* `#13550 <https://github.com/numpy/numpy/pull/13550>`__: TST: Register markers in conftest.py.
* `#13551 <https://github.com/numpy/numpy/pull/13551>`__: DOC: Add note to ``nonzero`` docstring.
* `#13558 <https://github.com/numpy/numpy/pull/13558>`__: MAINT: Fix errors seen on new python 3.8
* `#13570 <https://github.com/numpy/numpy/pull/13570>`__: DOC: Remove duplicate documentation of the PyArray_SimpleNew...
* `#13571 <https://github.com/numpy/numpy/pull/13571>`__: DOC: Mention that expand_dims returns a view
* `#13574 <https://github.com/numpy/numpy/pull/13574>`__: DOC: remove performance claim from searchsorted()
* `#13575 <https://github.com/numpy/numpy/pull/13575>`__: TST: Apply ufunc signature and type test fixmes.
* `#13581 <https://github.com/numpy/numpy/pull/13581>`__: ENH: AVX support for exp/log for strided float32 arrays
* `#13584 <https://github.com/numpy/numpy/pull/13584>`__: DOC: roadmap update
* `#13589 <https://github.com/numpy/numpy/pull/13589>`__: MAINT: Increment stacklevel for warnings to account for NEP-18...
* `#13590 <https://github.com/numpy/numpy/pull/13590>`__: BUG: Fixes for Undefined Behavior Sanitizer (UBSan) errors.
* `#13595 <https://github.com/numpy/numpy/pull/13595>`__: NEP: update NEP 19 with API terminology
* `#13599 <https://github.com/numpy/numpy/pull/13599>`__: DOC: Fixed minor doc error in take_along_axis
* `#13603 <https://github.com/numpy/numpy/pull/13603>`__: TST: bump / verify OpenBLAS in CI
* `#13619 <https://github.com/numpy/numpy/pull/13619>`__: DOC: Add missing return value documentation in ndarray.require
* `#13621 <https://github.com/numpy/numpy/pull/13621>`__: DOC: Update boolean indices in index arrays with slices example
* `#13623 <https://github.com/numpy/numpy/pull/13623>`__: BUG: Workaround for bug in clang7.0
* `#13624 <https://github.com/numpy/numpy/pull/13624>`__: DOC: revert __skip_array_function__ from NEP-18
* `#13626 <https://github.com/numpy/numpy/pull/13626>`__: DOC: update isfortran docs with return value
* `#13627 <https://github.com/numpy/numpy/pull/13627>`__: MAINT: revert __skip_array_function__ from NEP-18
* `#13629 <https://github.com/numpy/numpy/pull/13629>`__: BUG: setup.py install --skip-build fails
* `#13632 <https://github.com/numpy/numpy/pull/13632>`__: MAINT: Collect together the special-casing of 0d nonzero into...
* `#13633 <https://github.com/numpy/numpy/pull/13633>`__: DOC: caution against relying upon NumPy's implementation in subclasses
* `#13634 <https://github.com/numpy/numpy/pull/13634>`__: MAINT: avoid nested dispatch in numpy.core.shape_base
* `#13636 <https://github.com/numpy/numpy/pull/13636>`__: DOC: Add return section to linalg.matrix_rank & tensordot
* `#13639 <https://github.com/numpy/numpy/pull/13639>`__: MAINT: Update mailmap for 1.17.0
* `#13642 <https://github.com/numpy/numpy/pull/13642>`__: BUG: special case object arrays when printing rel-, abs-error...
* `#13648 <https://github.com/numpy/numpy/pull/13648>`__: BUG: ensure that casting to/from structured is properly checked.
* `#13649 <https://github.com/numpy/numpy/pull/13649>`__: DOC: Mention PyArray_GetField steals a reference
* `#13652 <https://github.com/numpy/numpy/pull/13652>`__: MAINT: remove superfluous setting in can_cast_safely_table.
* `#13655 <https://github.com/numpy/numpy/pull/13655>`__: BUG/MAINT: Non-native byteorder in random ints
* `#13656 <https://github.com/numpy/numpy/pull/13656>`__: PERF: Use intrinsic rotr on Windows
* `#13657 <https://github.com/numpy/numpy/pull/13657>`__: BUG: Avoid leading underscores in C function names.
* `#13660 <https://github.com/numpy/numpy/pull/13660>`__: DOC: Updates following NumPy 1.16.4 release.
* `#13663 <https://github.com/numpy/numpy/pull/13663>`__: BUG: regression for array([pandas.DataFrame()])
* `#13664 <https://github.com/numpy/numpy/pull/13664>`__: MAINT: Misc. typo fixes
* `#13665 <https://github.com/numpy/numpy/pull/13665>`__: MAINT: Use intrinsics in Win64-PCG64
* `#13670 <https://github.com/numpy/numpy/pull/13670>`__: BUG: Fix RandomState argument name
* `#13672 <https://github.com/numpy/numpy/pull/13672>`__: DOC: Fix rst markup in RELEASE_WALKTHROUGH.
* `#13678 <https://github.com/numpy/numpy/pull/13678>`__: BUG: fix benchmark suite importability on Numpy<1.17
* `#13682 <https://github.com/numpy/numpy/pull/13682>`__: ENH: Support __length_hint__ in PyArray_FromIter
* `#13684 <https://github.com/numpy/numpy/pull/13684>`__: BUG: Move ndarray.dump to python and make it close the file it...
* `#13687 <https://github.com/numpy/numpy/pull/13687>`__: DOC: Remove misleading statement
* `#13688 <https://github.com/numpy/numpy/pull/13688>`__: MAINT: Correct masked aliases
* `#13690 <https://github.com/numpy/numpy/pull/13690>`__: MAINT: Remove version added from Generator
* `#13691 <https://github.com/numpy/numpy/pull/13691>`__: BUG: Prevent passing of size 0 to array alloc C functions
* `#13692 <https://github.com/numpy/numpy/pull/13692>`__: DOC: Update C-API documentation of scanfunc, fromstr
* `#13693 <https://github.com/numpy/numpy/pull/13693>`__: ENH: Pass input strides and dimensions by pointer to const
* `#13695 <https://github.com/numpy/numpy/pull/13695>`__: BUG: Ensure Windows choice returns int32
* `#13696 <https://github.com/numpy/numpy/pull/13696>`__: DOC: Put the useful constants first
* `#13697 <https://github.com/numpy/numpy/pull/13697>`__: MAINT: speed up hstack and vstack by eliminating list comprehension.
* `#13700 <https://github.com/numpy/numpy/pull/13700>`__: Add links for GitHub Sponsors button.
* `#13703 <https://github.com/numpy/numpy/pull/13703>`__: DOC: Adds documentation for numpy.dtype.base
* `#13704 <https://github.com/numpy/numpy/pull/13704>`__: DOC: Mention PyArray_DIMS can be NULL
* `#13708 <https://github.com/numpy/numpy/pull/13708>`__: DEP: Deprecate nonzero(0d) in favor of calling atleast_1d explicitly
* `#13715 <https://github.com/numpy/numpy/pull/13715>`__: BUG: Fix use-after-free in boolean indexing
* `#13716 <https://github.com/numpy/numpy/pull/13716>`__: BUG: Fix random.choice when probability is not C contiguous
* `#13720 <https://github.com/numpy/numpy/pull/13720>`__: MAINT/BUG: Manage more files with with statements
* `#13721 <https://github.com/numpy/numpy/pull/13721>`__: MAINT,BUG: More ufunc exception cleanup
* `#13724 <https://github.com/numpy/numpy/pull/13724>`__: MAINT: fix use of cache_dim
* `#13725 <https://github.com/numpy/numpy/pull/13725>`__: BUG: fix compilation of 3rd party modules with Py_LIMITED_API...
* `#13726 <https://github.com/numpy/numpy/pull/13726>`__: MAINT: Update PCG jump sizes
* `#13729 <https://github.com/numpy/numpy/pull/13729>`__: DOC: Merge together DISTUTILS.rst.txt#template-files" and distutils.r…
* `#13730 <https://github.com/numpy/numpy/pull/13730>`__: MAINT: Change keyword from reserved word
* `#13737 <https://github.com/numpy/numpy/pull/13737>`__: DOC: Mention and try to explain pairwise summation in sum
* `#13741 <https://github.com/numpy/numpy/pull/13741>`__: MAINT: random: Remove unused empty file binomial.h.
* `#13743 <https://github.com/numpy/numpy/pull/13743>`__: MAINT: random: Rename legacy distributions file.
* `#13744 <https://github.com/numpy/numpy/pull/13744>`__: DOC: Update the C style guide for C99.
* `#13745 <https://github.com/numpy/numpy/pull/13745>`__: BUG: fix segfault on side-effect in __bool__ function in array.nonzero()
* `#13746 <https://github.com/numpy/numpy/pull/13746>`__: [WIP] DOC : Refactor C-API -- Python Types and C structures
* `#13757 <https://github.com/numpy/numpy/pull/13757>`__: MAINT: fix histogram*d dispatchers
* `#13760 <https://github.com/numpy/numpy/pull/13760>`__: DOC: update test guidelines document to use pytest for skipif
* `#13761 <https://github.com/numpy/numpy/pull/13761>`__: MAINT: random: Rewrite the hypergeometric distribution.
* `#13762 <https://github.com/numpy/numpy/pull/13762>`__: MAINT: Use textwrap.dedent for multiline strings
* `#13763 <https://github.com/numpy/numpy/pull/13763>`__: MAINT: Use with statements and dedent in core/setup.py
* `#13767 <https://github.com/numpy/numpy/pull/13767>`__: DOC: Adds examples for dtype attributes
* `#13770 <https://github.com/numpy/numpy/pull/13770>`__: MAINT: random: Combine ziggurat.h and ziggurat_constants.h
* `#13771 <https://github.com/numpy/numpy/pull/13771>`__: DOC: Change random to uninitialized and unpredictable in empty...
* `#13772 <https://github.com/numpy/numpy/pull/13772>`__: BUILD: use numpy-wheels/openblas_support.py to create _distributor_init.py
* `#13773 <https://github.com/numpy/numpy/pull/13773>`__: DOC: Update of reference to paper for Lemire's method
* `#13774 <https://github.com/numpy/numpy/pull/13774>`__: BUG: Make ``Generator._masked`` flag default to ``False``.
* `#13777 <https://github.com/numpy/numpy/pull/13777>`__: MAINT: Remove duplication of should_use_min_scalar_type function
* `#13780 <https://github.com/numpy/numpy/pull/13780>`__: ENH: use SeedSequence instead of seed()
* `#13781 <https://github.com/numpy/numpy/pull/13781>`__: DOC: Update TESTS.rst.txt for pytest
* `#13786 <https://github.com/numpy/numpy/pull/13786>`__: MAINT: random: Fix a few compiler warnings.
* `#13787 <https://github.com/numpy/numpy/pull/13787>`__: DOC: Fixed the problem of "versionadded"
* `#13788 <https://github.com/numpy/numpy/pull/13788>`__: MAINT: fix 'in' -> 'is' typo
* `#13789 <https://github.com/numpy/numpy/pull/13789>`__: MAINT: Fix warnings in radixsort.c.src: comparing integers of...
* `#13791 <https://github.com/numpy/numpy/pull/13791>`__: MAINT: remove dSFMT
* `#13792 <https://github.com/numpy/numpy/pull/13792>`__: LICENSE: update dragon4 license to MIT
* `#13793 <https://github.com/numpy/numpy/pull/13793>`__: MAINT: remove xoshiro* BitGenerators
* `#13795 <https://github.com/numpy/numpy/pull/13795>`__: DOC: Update description of sep in fromstring
* `#13803 <https://github.com/numpy/numpy/pull/13803>`__: DOC: Improve documentation for ``defchararray``
* `#13813 <https://github.com/numpy/numpy/pull/13813>`__: BUG: further fixup to histogram2d dispatcher.
* `#13815 <https://github.com/numpy/numpy/pull/13815>`__: MAINT: Correct intrinsic use on Windows
* `#13818 <https://github.com/numpy/numpy/pull/13818>`__: TST: Add tests for ComplexWarning in astype
* `#13819 <https://github.com/numpy/numpy/pull/13819>`__: DOC: Fix documented default value of ``__array_priority__`` for...
* `#13820 <https://github.com/numpy/numpy/pull/13820>`__: MAINT, DOC: Fix misspelled words in documentation.
* `#13821 <https://github.com/numpy/numpy/pull/13821>`__: MAINT: core: Fix a compiler warning.
* `#13830 <https://github.com/numpy/numpy/pull/13830>`__: MAINT: Update tox for supported Python versions
* `#13832 <https://github.com/numpy/numpy/pull/13832>`__: MAINT: remove pcg32 BitGenerator
* `#13833 <https://github.com/numpy/numpy/pull/13833>`__: MAINT: remove ThreeFry BitGenerator
* `#13837 <https://github.com/numpy/numpy/pull/13837>`__: MAINT, BUG: fixes from seedsequence
* `#13838 <https://github.com/numpy/numpy/pull/13838>`__: ENH: SFC64 BitGenerator
* `#13839 <https://github.com/numpy/numpy/pull/13839>`__: MAINT: Ignore some generated files.
* `#13840 <https://github.com/numpy/numpy/pull/13840>`__: ENH: np.random.default_gen()
* `#13843 <https://github.com/numpy/numpy/pull/13843>`__: DOC: remove note about `__array_ufunc__` being provisional for...
* `#13849 <https://github.com/numpy/numpy/pull/13849>`__: DOC: np.random documentation cleanup and expansion.
* `#13850 <https://github.com/numpy/numpy/pull/13850>`__: DOC: Update performance numbers
* `#13851 <https://github.com/numpy/numpy/pull/13851>`__: MAINT: Update shippable.yml to remove Python 2 dependency
* `#13855 <https://github.com/numpy/numpy/pull/13855>`__: BUG: Fix memory leak in dtype from dict constructor
* `#13856 <https://github.com/numpy/numpy/pull/13856>`__: MAINT: move location of bitgen.h
* `#13858 <https://github.com/numpy/numpy/pull/13858>`__: BUG: do not force emulation of 128-bit arithmetic.
* `#13859 <https://github.com/numpy/numpy/pull/13859>`__: DOC: Update performance numbers for PCG64
* `#13861 <https://github.com/numpy/numpy/pull/13861>`__: BUG: Ensure consistent interpretation of uint64 states.
* `#13863 <https://github.com/numpy/numpy/pull/13863>`__: DOC: Document the precise PCG variant.
* `#13864 <https://github.com/numpy/numpy/pull/13864>`__: TST: Ignore DeprecationWarning during nose imports
* `#13869 <https://github.com/numpy/numpy/pull/13869>`__: DOC: Prepare for 1.17.0rc1 release
* `#13870 <https://github.com/numpy/numpy/pull/13870>`__: MAINT,BUG: Use nbytes to also catch empty descr during allocation
* `#13873 <https://github.com/numpy/numpy/pull/13873>`__: ENH: Rename default_gen -> default_rng
* `#13893 <https://github.com/numpy/numpy/pull/13893>`__: DOC: fix links in 1.17 release note
* `#13897 <https://github.com/numpy/numpy/pull/13897>`__: DOC: Use Cython >= 0.29.11 for Python 3.8 support.
* `#13932 <https://github.com/numpy/numpy/pull/13932>`__: MAINT,BUG,DOC: Fix errors in _add_newdocs
* `#13963 <https://github.com/numpy/numpy/pull/13963>`__: ENH, BUILD: refactor all OpenBLAS downloads into a single, testable...
* `#13971 <https://github.com/numpy/numpy/pull/13971>`__: DOC: emphasize random API changes
* `#13972 <https://github.com/numpy/numpy/pull/13972>`__: MAINT: Rewrite Floyd algorithm
* `#13992 <https://github.com/numpy/numpy/pull/13992>`__: BUG: Do not crash on recursive `.dtype` attribute lookup.
* `#13993 <https://github.com/numpy/numpy/pull/13993>`__: DEP: Speed up WarnOnWrite deprecation in buffer interface
* `#13995 <https://github.com/numpy/numpy/pull/13995>`__: BLD: Remove Trusty dist in Travis CI build
* `#13996 <https://github.com/numpy/numpy/pull/13996>`__: BUG: Handle weird bytestrings in dtype()
* `#13997 <https://github.com/numpy/numpy/pull/13997>`__: BUG: i0 Bessel function regression on array-likes supporting...
* `#13998 <https://github.com/numpy/numpy/pull/13998>`__: BUG: Missing warnings import in polyutils.
* `#13999 <https://github.com/numpy/numpy/pull/13999>`__: DOC: Document array_function at a higher level.
* `#14001 <https://github.com/numpy/numpy/pull/14001>`__: DOC: Show workaround for Generator.integers backward compatibility
* `#14021 <https://github.com/numpy/numpy/pull/14021>`__: DOC: Prepare 1.17.0rc2 release.
* `#14040 <https://github.com/numpy/numpy/pull/14040>`__: DOC: Improve quickstart documentation of new random Generator.
* `#14041 <https://github.com/numpy/numpy/pull/14041>`__: TST, MAINT: expand OpenBLAS version checking
* `#14080 <https://github.com/numpy/numpy/pull/14080>`__: BUG, DOC: add new recfunctions to `__all__`
* `#14081 <https://github.com/numpy/numpy/pull/14081>`__: BUG: fix build issue on icc 2016
* `#14082 <https://github.com/numpy/numpy/pull/14082>`__: BUG: Fix file-like object check when saving arrays
* `#14109 <https://github.com/numpy/numpy/pull/14109>`__: REV: "ENH: Improved performance of PyArray_FromAny for sequences...
* `#14126 <https://github.com/numpy/numpy/pull/14126>`__: BUG, TEST: Adding validation test suite to validate float32 exp
* `#14127 <https://github.com/numpy/numpy/pull/14127>`__: DOC: Add blank line above doctest for intersect1d
* `#14128 <https://github.com/numpy/numpy/pull/14128>`__: MAINT: adjustments to test_ufunc_noncontigous
* `#14129 <https://github.com/numpy/numpy/pull/14129>`__: MAINT: Use equality instead of identity check with literal
* `#14133 <https://github.com/numpy/numpy/pull/14133>`__: MAINT: Update mailmap and changelog for 1.17.0

Contributors
============

A total of 19 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Andras Deak +
* Bob Eldering +
* Brandon Carter
* Charles Harris
* Daniel Hrisca +
* Eric Wieser
* Iryna Shcherbina +
* James Bourbeau +
* Jonathan Helmus
* Joshua Leahy +
* Julian Taylor
* Matti Picus
* Michael Lamparski +
* Michael Seifert
* Pauli Virtanen
* Ralf Gommers
* Roland Kaufmann
* Warren Weckesser

Pull requests merged
====================

A total of 41 pull requests were merged for this release.

* `#9240 <https://github.com/numpy/numpy/pull/9240>`__: DOC: BLD: fix lots of Sphinx warnings/errors.
* `#9255 <https://github.com/numpy/numpy/pull/9255>`__: Revert "DEP: Raise TypeError for subtract(bool_, bool_)."
* `#9261 <https://github.com/numpy/numpy/pull/9261>`__: BUG: don't elide into readonly and updateifcopy temporaries for...
* `#9262 <https://github.com/numpy/numpy/pull/9262>`__: BUG: fix missing keyword rename for common block in numpy.f2py
* `#9263 <https://github.com/numpy/numpy/pull/9263>`__: BUG: handle resize of 0d array
* `#9267 <https://github.com/numpy/numpy/pull/9267>`__: DOC: update f2py front page and some doc build metadata.
* `#9299 <https://github.com/numpy/numpy/pull/9299>`__: BUG: Fix Intel compilation on Unix.
* `#9317 <https://github.com/numpy/numpy/pull/9317>`__: BUG: fix wrong ndim used in empty where check
* `#9319 <https://github.com/numpy/numpy/pull/9319>`__: BUG: Make extensions compilable with MinGW on Py2.7
* `#9339 <https://github.com/numpy/numpy/pull/9339>`__: BUG: Prevent crash if ufunc doc string is null
* `#9340 <https://github.com/numpy/numpy/pull/9340>`__: BUG: umath: un-break ufunc where= when no out= is given
* `#9371 <https://github.com/numpy/numpy/pull/9371>`__: DOC: Add isnat/positive ufunc to documentation
* `#9372 <https://github.com/numpy/numpy/pull/9372>`__: BUG: Fix error in fromstring function from numpy.core.records...
* `#9373 <https://github.com/numpy/numpy/pull/9373>`__: BUG: ')' is printed at the end pointer of the buffer in numpy.f2py.
* `#9374 <https://github.com/numpy/numpy/pull/9374>`__: DOC: Create NumPy 1.13.1 release notes.
* `#9376 <https://github.com/numpy/numpy/pull/9376>`__: BUG: Prevent hang traversing ufunc userloop linked list
* `#9377 <https://github.com/numpy/numpy/pull/9377>`__: DOC: Use x1 and x2 in the heaviside docstring.
* `#9378 <https://github.com/numpy/numpy/pull/9378>`__: DOC: Add $PARAMS to the isnat docstring
* `#9379 <https://github.com/numpy/numpy/pull/9379>`__: DOC: Update the 1.13.1 release notes
* `#9390 <https://github.com/numpy/numpy/pull/9390>`__: BUG: Return the poly1d coefficients array directly
* `#9555 <https://github.com/numpy/numpy/pull/9555>`__: BUG: fix regression in 1.13.x in distutils.mingw32ccompiler.
* `#9556 <https://github.com/numpy/numpy/pull/9556>`__: BUG: Fix true_divide when dtype=np.float64 specified.
* `#9557 <https://github.com/numpy/numpy/pull/9557>`__: DOC: Fix some rst markup in numpy/doc/basics.py.
* `#9558 <https://github.com/numpy/numpy/pull/9558>`__: BLD: remove -xhost flag from IntelFCompiler.
* `#9559 <https://github.com/numpy/numpy/pull/9559>`__: DOC: removes broken docstring example (source code, png, pdf)...
* `#9580 <https://github.com/numpy/numpy/pull/9580>`__: BUG: Add hypot and cabs functions to WIN32 blacklist.
* `#9732 <https://github.com/numpy/numpy/pull/9732>`__: BUG: Make scalar function elision check if temp is writeable.
* `#9736 <https://github.com/numpy/numpy/pull/9736>`__: BUG: various fixes to np.gradient
* `#9742 <https://github.com/numpy/numpy/pull/9742>`__: BUG: Fix np.pad for CVE-2017-12852
* `#9744 <https://github.com/numpy/numpy/pull/9744>`__: BUG: Check for exception in sort functions, add tests
* `#9745 <https://github.com/numpy/numpy/pull/9745>`__: DOC: Add whitespace after "versionadded::" directive so it actually...
* `#9746 <https://github.com/numpy/numpy/pull/9746>`__: BUG: memory leak in np.dot of size 0
* `#9747 <https://github.com/numpy/numpy/pull/9747>`__: BUG: adjust gfortran version search regex
* `#9757 <https://github.com/numpy/numpy/pull/9757>`__: BUG: Cython 0.27 breaks NumPy on Python 3.
* `#9764 <https://github.com/numpy/numpy/pull/9764>`__: BUG: Ensure `_npy_scaled_cexp{,f,l}` is defined when needed.
* `#9765 <https://github.com/numpy/numpy/pull/9765>`__: BUG: PyArray_CountNonzero does not check for exceptions
* `#9766 <https://github.com/numpy/numpy/pull/9766>`__: BUG: Fixes histogram monotonicity check for unsigned bin values
* `#9767 <https://github.com/numpy/numpy/pull/9767>`__: BUG: ensure consistent result dtype of count_nonzero
* `#9771 <https://github.com/numpy/numpy/pull/9771>`__: MAINT,BUG: Fix mtrand for Cython 0.27.
* `#9772 <https://github.com/numpy/numpy/pull/9772>`__: DOC: Create the 1.13.2 release notes.
* `#9794 <https://github.com/numpy/numpy/pull/9794>`__: DOC: Create 1.13.3 release notes.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Kevin Sheppard
* Matti Picus
* Ralf Gommers
* Sebastian Berg
* Warren Weckesser

Pull requests merged
====================

A total of 12 pull requests were merged for this release.

* `#14456 <https://github.com/numpy/numpy/pull/14456>`__: MAINT: clean up pocketfft modules inside numpy.fft namespace.
* `#14463 <https://github.com/numpy/numpy/pull/14463>`__: BUG: random.hypergeometic assumes npy_long is npy_int64, hung...
* `#14502 <https://github.com/numpy/numpy/pull/14502>`__: BUG: random: Revert gh-14458 and refix gh-14557.
* `#14504 <https://github.com/numpy/numpy/pull/14504>`__: BUG: add a specialized loop for boolean matmul.
* `#14506 <https://github.com/numpy/numpy/pull/14506>`__: MAINT: Update pytest version for Python 3.8
* `#14512 <https://github.com/numpy/numpy/pull/14512>`__: DOC: random: fix doc linking, was referencing private submodules.
* `#14513 <https://github.com/numpy/numpy/pull/14513>`__: BUG,MAINT: Some fixes and minor cleanup based on clang analysis
* `#14515 <https://github.com/numpy/numpy/pull/14515>`__: BUG: Fix randint when range is 2**32
* `#14519 <https://github.com/numpy/numpy/pull/14519>`__: MAINT: remove the entropy c-extension module
* `#14563 <https://github.com/numpy/numpy/pull/14563>`__: DOC: remove note about Pocketfft license file (non-existing here).
* `#14578 <https://github.com/numpy/numpy/pull/14578>`__: BUG: random: Create a legacy implementation of random.binomial.
* `#14687 <https://github.com/numpy/numpy/pull/14687>`__: BUG: properly define PyArray_DescrCheck

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Brown +
* Daniel Vanzo +
* E. Madison Bray +
* Hugo van Kemenade +
* Ralf Gommers
* Sebastian Berg
* @danbeibei +

Pull requests merged
====================

A total of 10 pull requests were merged for this release.

* `#17298 <https://github.com/numpy/numpy/pull/17298>`__: BLD: set upper versions for build dependencies
* `#17336 <https://github.com/numpy/numpy/pull/17336>`__: BUG: Set deprecated fields to null in PyArray_InitArrFuncs
* `#17446 <https://github.com/numpy/numpy/pull/17446>`__: ENH: Warn on unsupported Python 3.10+
* `#17450 <https://github.com/numpy/numpy/pull/17450>`__: MAINT: Update test_requirements.txt.
* `#17522 <https://github.com/numpy/numpy/pull/17522>`__: ENH: Support for the NVIDIA HPC SDK nvfortran compiler
* `#17568 <https://github.com/numpy/numpy/pull/17568>`__: BUG: Cygwin Workaround for #14787 on affected platforms
* `#17647 <https://github.com/numpy/numpy/pull/17647>`__: BUG: Fix memory leak of buffer-info cache due to relaxed strides
* `#17652 <https://github.com/numpy/numpy/pull/17652>`__: MAINT: Backport openblas_support from master.
* `#17653 <https://github.com/numpy/numpy/pull/17653>`__: TST: Add Python 3.9 to the CI testing on Windows, Mac.
* `#17660 <https://github.com/numpy/numpy/pull/17660>`__: TST: Simplify source path names in test_extending.

Contributors
============

A total of 16 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Antoine Pitrou
* Arcesio Castaneda Medina +
* Charles Harris
* Chris Markiewicz +
* Christoph Gohlke
* Christopher J. Markiewicz +
* Daniel Hrisca +
* EelcoPeacs +
* Eric Wieser
* Kevin Sheppard
* Matti Picus
* OBATA Akio +
* Ralf Gommers
* Sebastian Berg
* Stephan Hoyer
* Tyler Reddy

Pull requests merged
====================

A total of 33 pull requests were merged for this release.

* `#12754 <https://github.com/numpy/numpy/pull/12754>`__: BUG: Check paths are unicode, bytes or path-like
* `#12767 <https://github.com/numpy/numpy/pull/12767>`__: ENH: add mm->q floordiv
* `#12768 <https://github.com/numpy/numpy/pull/12768>`__: ENH: port np.core.overrides to C for speed
* `#12769 <https://github.com/numpy/numpy/pull/12769>`__: ENH: Add np.ctypeslib.as_ctypes_type(dtype), improve `np.ctypeslib.as_ctypes`
* `#12771 <https://github.com/numpy/numpy/pull/12771>`__: BUG: Ensure probabilities are not NaN in choice
* `#12772 <https://github.com/numpy/numpy/pull/12772>`__: MAINT: add warning to numpy.distutils for LDFLAGS append behavior.
* `#12773 <https://github.com/numpy/numpy/pull/12773>`__: ENH: add "max difference" messages to np.testing.assert_array_equal...
* `#12774 <https://github.com/numpy/numpy/pull/12774>`__: BUG: Fix incorrect/missing reference cleanups found using valgrind
* `#12776 <https://github.com/numpy/numpy/pull/12776>`__: BUG,TST: Remove the misguided `run_command` that wraps subprocess
* `#12777 <https://github.com/numpy/numpy/pull/12777>`__: DOC, TST: Clean up matplotlib imports
* `#12781 <https://github.com/numpy/numpy/pull/12781>`__: BUG: Fix reference counting for subarrays containing objects
* `#12782 <https://github.com/numpy/numpy/pull/12782>`__: BUG: Ensure failing memory allocations are reported
* `#12784 <https://github.com/numpy/numpy/pull/12784>`__: BUG: Fix leak of void scalar buffer info
* `#12788 <https://github.com/numpy/numpy/pull/12788>`__: MAINT: Change the order of checking for local file.
* `#12808 <https://github.com/numpy/numpy/pull/12808>`__: BUG: loosen kwargs requirements in ediff1d
* `#12809 <https://github.com/numpy/numpy/pull/12809>`__: DOC: clarify the extend of __array_function__ support in NumPy...
* `#12810 <https://github.com/numpy/numpy/pull/12810>`__: BUG: Check that dtype or formats arguments are not None.
* `#12811 <https://github.com/numpy/numpy/pull/12811>`__: BUG: fix f2py problem to build wrappers using PGI's Fortran
* `#12812 <https://github.com/numpy/numpy/pull/12812>`__: BUG: double decref of dtype in failure codepath. Test and fix
* `#12813 <https://github.com/numpy/numpy/pull/12813>`__: BUG, DOC: test, fix that f2py.compile accepts str and bytes,...
* `#12816 <https://github.com/numpy/numpy/pull/12816>`__: BUG: resolve writeback in arr_insert failure paths
* `#12820 <https://github.com/numpy/numpy/pull/12820>`__: ENH: Add mm->qm divmod
* `#12843 <https://github.com/numpy/numpy/pull/12843>`__: BUG: fix to check before apply `shlex.split`
* `#12844 <https://github.com/numpy/numpy/pull/12844>`__: BUG: Fix SystemError when pickling datetime64 array with pickle5
* `#12845 <https://github.com/numpy/numpy/pull/12845>`__: BUG: Fix rounding of denormals in double and float to half casts.
* `#12868 <https://github.com/numpy/numpy/pull/12868>`__: TEST: pin mingw version
* `#12869 <https://github.com/numpy/numpy/pull/12869>`__: BUG: ndarrays pickled by 1.16 cannot be loaded by 1.15.4 and...
* `#12870 <https://github.com/numpy/numpy/pull/12870>`__: BUG: do not Py_DECREF NULL pointer
* `#12890 <https://github.com/numpy/numpy/pull/12890>`__: ENH: add _dtype_ctype to namespace for freeze analysis
* `#12891 <https://github.com/numpy/numpy/pull/12891>`__: BUG: fail if old multiarray module detected
* `#12898 <https://github.com/numpy/numpy/pull/12898>`__: BUG: Do not double-quote arguments passed on to the linker
* `#12899 <https://github.com/numpy/numpy/pull/12899>`__: BUG: Do not insert extra double quote into preprocessor macros
* `#12902 <https://github.com/numpy/numpy/pull/12902>`__: DOC: Prepare for 1.16.1 release.

Contributors
============

A total of 1 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#17679 <https://github.com/numpy/numpy/pull/17679>`__: MAINT: Add check for Windows 10 version 2004 bug.
* `#17680 <https://github.com/numpy/numpy/pull/17680>`__: REV: Revert OpenBLAS to 1.19.2 version for 1.19.4

Contributors
============

A total of 12 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Andras Deak +
* Bob Eldering +
* Charles Harris
* Daniel Hrisca +
* Eric Wieser
* Joshua Leahy +
* Julian Taylor
* Michael Seifert
* Pauli Virtanen
* Ralf Gommers
* Roland Kaufmann
* Warren Weckesser

Pull requests merged
====================

A total of 19 pull requests were merged for this release.

* `#9240 <https://github.com/numpy/numpy/pull/9240>`__: DOC: BLD: fix lots of Sphinx warnings/errors.
* `#9255 <https://github.com/numpy/numpy/pull/9255>`__: Revert "DEP: Raise TypeError for subtract(bool_, bool_)."
* `#9261 <https://github.com/numpy/numpy/pull/9261>`__: BUG: don't elide into readonly and updateifcopy temporaries for...
* `#9262 <https://github.com/numpy/numpy/pull/9262>`__: BUG: fix missing keyword rename for common block in numpy.f2py
* `#9263 <https://github.com/numpy/numpy/pull/9263>`__: BUG: handle resize of 0d array
* `#9267 <https://github.com/numpy/numpy/pull/9267>`__: DOC: update f2py front page and some doc build metadata.
* `#9299 <https://github.com/numpy/numpy/pull/9299>`__: BUG: Fix Intel compilation on Unix.
* `#9317 <https://github.com/numpy/numpy/pull/9317>`__: BUG: fix wrong ndim used in empty where check
* `#9319 <https://github.com/numpy/numpy/pull/9319>`__: BUG: Make extensions compilable with MinGW on Py2.7
* `#9339 <https://github.com/numpy/numpy/pull/9339>`__: BUG: Prevent crash if ufunc doc string is null
* `#9340 <https://github.com/numpy/numpy/pull/9340>`__: BUG: umath: un-break ufunc where= when no out= is given
* `#9371 <https://github.com/numpy/numpy/pull/9371>`__: DOC: Add isnat/positive ufunc to documentation
* `#9372 <https://github.com/numpy/numpy/pull/9372>`__: BUG: Fix error in fromstring function from numpy.core.records...
* `#9373 <https://github.com/numpy/numpy/pull/9373>`__: BUG: ')' is printed at the end pointer of the buffer in numpy.f2py.
* `#9374 <https://github.com/numpy/numpy/pull/9374>`__: DOC: Create NumPy 1.13.1 release notes.
* `#9376 <https://github.com/numpy/numpy/pull/9376>`__: BUG: Prevent hang traversing ufunc userloop linked list
* `#9377 <https://github.com/numpy/numpy/pull/9377>`__: DOC: Use x1 and x2 in the heaviside docstring.
* `#9378 <https://github.com/numpy/numpy/pull/9378>`__: DOC: Add $PARAMS to the isnat docstring
* `#9379 <https://github.com/numpy/numpy/pull/9379>`__: DOC: Update the 1.13.1 release notes

Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Max Balandat +
* @Mibu287 +
* Pan Jan +
* Sebastian Berg
* @panpiort8 +

Pull requests merged
====================

A total of 5 pull requests were merged for this release.

* `#15916 <https://github.com/numpy/numpy/pull/15916>`__: BUG: Fix eigh and cholesky methods of numpy.random.multivariate_normal
* `#15929 <https://github.com/numpy/numpy/pull/15929>`__: BUG,MAINT: Remove incorrect special case in string to number...
* `#15930 <https://github.com/numpy/numpy/pull/15930>`__: BUG: Guarantee array is in valid state after memory error occurs...
* `#15954 <https://github.com/numpy/numpy/pull/15954>`__: BUG: Check that `pvals` is 1D in `_generator.multinomial`.
* `#16017 <https://github.com/numpy/numpy/pull/16017>`__: BUG: Alpha parameter must be 1D in `generator.dirichlet`
=========
Changelog
=========

Contributors
============

A total of 102 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* A. Jesse Jiryu Davis +
* Alessandro Pietro Bardelli +
* Alex Rothberg +
* Alexander Shadchin
* Allan Haldane
* Andres Guzman-Ballen +
* Antoine Pitrou
* Antony Lee
* B R S Recht +
* Baurzhan Muftakhidinov +
* Ben Rowland
* Benda Xu +
* Blake Griffith
* Bradley Wogsland +
* Brandon Carter +
* CJ Carey
* Charles Harris
* Christoph Gohlke
* Danny Hermes +
* David Hagen +
* Duke Vijitbenjaronk +
* Egor Klenin +
* Elliott Forney +
* Elliott M Forney +
* Endolith
* Eric Wieser
* Erik M. Bray
* Eugene +
* Evan Limanto +
* Felix Berkenkamp +
* François Bissey +
* Frederic Bastien
* Greg Young
* Gregory R. Lee
* Importance of Being Ernest +
* Jaime Fernandez
* Jakub Wilk +
* James Cowgill +
* James Sanders
* Jean Utke +
* Jesse Thoren +
* Jim Crist +
* Joerg Behrmann +
* John Kirkham
* Jonathan Helmus
* Jonathan L Long
* Jonathan Tammo Siebert +
* Joseph Fox-Rabinovitz
* Joshua Loyal +
* Juan Nunez-Iglesias +
* Julian Taylor
* Kirill Balunov +
* Likhith Chitneni +
* Loïc Estève
* Mads Ohm Larsen
* Marein Könings +
* Marten van Kerkwijk
* Martin Thoma
* Martino Sorbaro +
* Marvin Schmidt +
* Matthew Brett
* Matthias Bussonnier +
* Matthias C. M. Troffaes +
* Matti Picus
* Michael Seifert
* Mikhail Pak +
* Mortada Mehyar
* Nathaniel J. Smith
* Nick Papior
* Oscar Villellas +
* Pauli Virtanen
* Pavel Potocek
* Pete Peeradej Tanruangporn +
* Philipp A +
* Ralf Gommers
* Robert Kern
* Roland Kaufmann +
* Ronan Lamy
* Sami Salonen +
* Sanchez Gonzalez Alvaro
* Sebastian Berg
* Shota Kawabuchi
* Simon Gibbons
* Stefan Otte
* Stefan Peterson +
* Stephan Hoyer
* Søren Fuglede Jørgensen +
* Takuya Akiba
* Tom Boyd +
* Ville Skyttä +
* Warren Weckesser
* Wendell Smith
* Yu Feng
* Zixu Zhao +
* Zè Vinícius +
* aha66 +
* davidjn +
* drabach +
* drlvk +
* jsh9 +
* solarjoe +
* zengi +

Pull requests merged
====================

A total of 309 pull requests were merged for this release.

* `#3861 <https://github.com/numpy/numpy/pull/3861>`__: ENH: Make it possible to NpyIter_RemoveAxis an empty dimension
* `#5302 <https://github.com/numpy/numpy/pull/5302>`__: Fixed meshgrid to return arrays with same dtype as arguments.
* `#5726 <https://github.com/numpy/numpy/pull/5726>`__: BUG, API: np.random.multivariate_normal behavior with bad covariance...
* `#6632 <https://github.com/numpy/numpy/pull/6632>`__: TST/BUG: fromfile - fix test and expose bug with io class argument
* `#6659 <https://github.com/numpy/numpy/pull/6659>`__: BUG: Let linspace accept input that has an array_interface.
* `#7742 <https://github.com/numpy/numpy/pull/7742>`__: Add `axis` argument to numpy.unique
* `#7862 <https://github.com/numpy/numpy/pull/7862>`__: BLD: rewrite np.distutils.exec_command.exec_command()
* `#7997 <https://github.com/numpy/numpy/pull/7997>`__: ENH: avoid temporary arrays in expressions (again)
* `#8043 <https://github.com/numpy/numpy/pull/8043>`__: ENH: umath: ensure ufuncs are well-defined with memory overlapping...
* `#8106 <https://github.com/numpy/numpy/pull/8106>`__: DOC: Document release procedure with a walkthrough.
* `#8194 <https://github.com/numpy/numpy/pull/8194>`__: BUG: np.piecewise not working for scalars
* `#8235 <https://github.com/numpy/numpy/pull/8235>`__: BUG: add checks for some invalid structured dtypes. Fixes #2865.
* `#8241 <https://github.com/numpy/numpy/pull/8241>`__: MAINT: Prepare for 1.13.0 after 1.12.x branch
* `#8242 <https://github.com/numpy/numpy/pull/8242>`__: BUG: Update operator `**` tests for new behavior.
* `#8244 <https://github.com/numpy/numpy/pull/8244>`__: DOC: fix typos in arrayprint docstrings.
* `#8247 <https://github.com/numpy/numpy/pull/8247>`__: ENH: Add `__array_ufunc__`
* `#8251 <https://github.com/numpy/numpy/pull/8251>`__: MAINT: Cleaned up mailmap
* `#8267 <https://github.com/numpy/numpy/pull/8267>`__: DOC: Changed shape assignment example to reshape. Elaborated...
* `#8271 <https://github.com/numpy/numpy/pull/8271>`__: TST: Path test should resolve symlinks when comparing
* `#8277 <https://github.com/numpy/numpy/pull/8277>`__: DOC: improve comment in prepare_index
* `#8279 <https://github.com/numpy/numpy/pull/8279>`__: BUG: bool(dtype) is True
* `#8281 <https://github.com/numpy/numpy/pull/8281>`__: DOC: Update 1.12.0 release notes.
* `#8284 <https://github.com/numpy/numpy/pull/8284>`__: BUG: Fix iteration over reversed subspaces in mapiter_@name@
* `#8285 <https://github.com/numpy/numpy/pull/8285>`__: BUG: Fix pavement.py write_release_task.
* `#8287 <https://github.com/numpy/numpy/pull/8287>`__: DOC: Update 1.13.0 release notes.
* `#8290 <https://github.com/numpy/numpy/pull/8290>`__: MAINT: let average preserve subclass information.
* `#8297 <https://github.com/numpy/numpy/pull/8297>`__: DEP: Handle expired deprecations.
* `#8299 <https://github.com/numpy/numpy/pull/8299>`__: BUG: Make f2py respect kind specifications for real parameters
* `#8302 <https://github.com/numpy/numpy/pull/8302>`__: BUG: Fix PyPy crash in PyUFunc_GenericReduction.
* `#8308 <https://github.com/numpy/numpy/pull/8308>`__: BUG: do not link to Accelerate if OpenBLAS, MKL or BLIS are found.
* `#8312 <https://github.com/numpy/numpy/pull/8312>`__: DEP: Drop deprecated boolean indexing behavior and update to...
* `#8318 <https://github.com/numpy/numpy/pull/8318>`__: BLD: blacklist powl (longdouble power function) on OS X.
* `#8326 <https://github.com/numpy/numpy/pull/8326>`__: ENH: Vectorize packbits with SSE2
* `#8327 <https://github.com/numpy/numpy/pull/8327>`__: BUG: Fix packbits to correctly handle empty arrays
* `#8335 <https://github.com/numpy/numpy/pull/8335>`__: BUG: Fix ndarray.tofile large file corruption in append mode
* `#8337 <https://github.com/numpy/numpy/pull/8337>`__: BUG: fix test_api test that fails intermittently in python 3
* `#8343 <https://github.com/numpy/numpy/pull/8343>`__: TST: Ellipsis indexing creates a view
* `#8348 <https://github.com/numpy/numpy/pull/8348>`__: ENH: Allow bincount(..., minlength=0).
* `#8349 <https://github.com/numpy/numpy/pull/8349>`__: BUG: Apply more robust string converts in loadtxt
* `#8351 <https://github.com/numpy/numpy/pull/8351>`__: BUG: correct letter case
* `#8354 <https://github.com/numpy/numpy/pull/8354>`__: BUG: Fix suppress_warnings (again) for Python 3.6.
* `#8355 <https://github.com/numpy/numpy/pull/8355>`__: Fix building extensions with MinGW for Python 3.5
* `#8356 <https://github.com/numpy/numpy/pull/8356>`__: Allow extensions to be built with MinGW in a virtualenv
* `#8360 <https://github.com/numpy/numpy/pull/8360>`__: MAINT: Drop special case code for python2 < 2.7 and python3 <...
* `#8364 <https://github.com/numpy/numpy/pull/8364>`__: BUG: handle unmasked NaN in ma.median like normal median
* `#8366 <https://github.com/numpy/numpy/pull/8366>`__: BUG: fix nanpercentile not returning scalar with axis argument
* `#8367 <https://github.com/numpy/numpy/pull/8367>`__: xlocale.h is not available in newlib / Cygwin
* `#8368 <https://github.com/numpy/numpy/pull/8368>`__: ENH: Implement most linalg operations for 0x0 matrices
* `#8369 <https://github.com/numpy/numpy/pull/8369>`__: TST: Fix various incorrect linalg tests
* `#8374 <https://github.com/numpy/numpy/pull/8374>`__: DOC: Fixed minor typo in William Gosset's name
* `#8377 <https://github.com/numpy/numpy/pull/8377>`__: Switch to the PyPI version of plex to generate lapack_lite
* `#8380 <https://github.com/numpy/numpy/pull/8380>`__: DOC: Update 1.12.0-notes.rst.
* `#8381 <https://github.com/numpy/numpy/pull/8381>`__: MAINT: Rebuild lapack lite
* `#8382 <https://github.com/numpy/numpy/pull/8382>`__: DEP: Fix escaped string characters deprecated in Python 3.6.
* `#8384 <https://github.com/numpy/numpy/pull/8384>`__: ENH: Add tool to check for deprecated escaped characters.
* `#8388 <https://github.com/numpy/numpy/pull/8388>`__: API: Return scalars for scalar inputs to np.real/imag
* `#8389 <https://github.com/numpy/numpy/pull/8389>`__: ENH: retune apply_along_axis nanmedian cutoff
* `#8395 <https://github.com/numpy/numpy/pull/8395>`__: DOC: create 1.11.3 release notes.
* `#8398 <https://github.com/numpy/numpy/pull/8398>`__: BUG: Fix author search in announce.py
* `#8400 <https://github.com/numpy/numpy/pull/8400>`__: Fix `corrcoef` and `cov` rowvar param handling
* `#8401 <https://github.com/numpy/numpy/pull/8401>`__: DOC, MAINT: Update 1.12.0 notes and mailmap.
* `#8410 <https://github.com/numpy/numpy/pull/8410>`__: BUG: Fixed behavior of assert_array_less for +/-inf
* `#8414 <https://github.com/numpy/numpy/pull/8414>`__: BUG: fixed failure of np.ma.median for 1-D even arrays.
* `#8416 <https://github.com/numpy/numpy/pull/8416>`__: BUG operations involving MaskedArray with output given do not...
* `#8421 <https://github.com/numpy/numpy/pull/8421>`__: ENH: Add isnat function and make comparison tests NAT specific
* `#8423 <https://github.com/numpy/numpy/pull/8423>`__: Adding isin function for multidimensional arrays
* `#8426 <https://github.com/numpy/numpy/pull/8426>`__: BUG: Fix apply_along_axis() for when func1d() returns a non-ndarray
* `#8434 <https://github.com/numpy/numpy/pull/8434>`__: TST: Update 3.6-dev tests to 3.6 after Python final release.
* `#8441 <https://github.com/numpy/numpy/pull/8441>`__: BUG: Fix crash on 0d return value in apply_along_axis
* `#8443 <https://github.com/numpy/numpy/pull/8443>`__: BUG: fix set memmap offset attribute correctly when offset is...
* `#8445 <https://github.com/numpy/numpy/pull/8445>`__: BUG: correct norm='ortho' scaling for rfft when n != None
* `#8446 <https://github.com/numpy/numpy/pull/8446>`__: ENH: gradient support for unevenly spaced data
* `#8448 <https://github.com/numpy/numpy/pull/8448>`__: TST: remove a duplicate test. Closes gh-8447.
* `#8452 <https://github.com/numpy/numpy/pull/8452>`__: BUG: assert_almost_equal fails on subclasses that cannot handle...
* `#8454 <https://github.com/numpy/numpy/pull/8454>`__: MAINT: Fix building extensions with MinGW in WinPython 3.4
* `#8464 <https://github.com/numpy/numpy/pull/8464>`__: [DOC]Small release doc fix
* `#8468 <https://github.com/numpy/numpy/pull/8468>`__: BUG: Ensure inf/nan removal in assert_array_compare is matrix-safe.
* `#8470 <https://github.com/numpy/numpy/pull/8470>`__: DOC: Add example to np.savez_compressed
* `#8474 <https://github.com/numpy/numpy/pull/8474>`__: MAINT: use env in shebang instead of absolute path to python
* `#8475 <https://github.com/numpy/numpy/pull/8475>`__: DOC: improve clip docstring
* `#8478 <https://github.com/numpy/numpy/pull/8478>`__: MAINT: Forward port accumulated changes from the 1.12.0 release.
* `#8482 <https://github.com/numpy/numpy/pull/8482>`__: TST: switch to ubuntu yakkety for i386 testing
* `#8483 <https://github.com/numpy/numpy/pull/8483>`__: BUG: fix wrong future nat warning and equiv type logic error
* `#8486 <https://github.com/numpy/numpy/pull/8486>`__: BUG: Prevent crash for length-0 input to fromrecords
* `#8488 <https://github.com/numpy/numpy/pull/8488>`__: ENH: Improve the alignment of `recarray.__repr__`
* `#8489 <https://github.com/numpy/numpy/pull/8489>`__: BUG: fix wrong masked median for some special cases
* `#8490 <https://github.com/numpy/numpy/pull/8490>`__: DOC: Place np.average in inline code
* `#8491 <https://github.com/numpy/numpy/pull/8491>`__: TST: work around isfinite inconsistency on i386
* `#8494 <https://github.com/numpy/numpy/pull/8494>`__: BUG: guard against replacing constants without `'_'` spec
* `#8496 <https://github.com/numpy/numpy/pull/8496>`__: Update LICENSE.txt to 2017
* `#8497 <https://github.com/numpy/numpy/pull/8497>`__: BUG: Fix creating a np.matrix from string syntax involving booleans
* `#8501 <https://github.com/numpy/numpy/pull/8501>`__: Changing spurious Legendre reference to Chebyshev in chebfit...
* `#8504 <https://github.com/numpy/numpy/pull/8504>`__: ENH: hard-code finfo parameters for known types
* `#8508 <https://github.com/numpy/numpy/pull/8508>`__: BUG: Fix loss of dimensionality of np.ma.masked in ufunc
* `#8524 <https://github.com/numpy/numpy/pull/8524>`__: BUG: fix mean for float 16 non-array inputs
* `#8527 <https://github.com/numpy/numpy/pull/8527>`__: DOC: fix return value for PyArray_Resize
* `#8539 <https://github.com/numpy/numpy/pull/8539>`__: BUG: core: in dot(), make copies if out has memory overlap with...
* `#8540 <https://github.com/numpy/numpy/pull/8540>`__: DOC: Update arrays.ndarray.rst
* `#8541 <https://github.com/numpy/numpy/pull/8541>`__: DOC: Revert 8540 patch 1
* `#8542 <https://github.com/numpy/numpy/pull/8542>`__: MAINT: typo in histogram docstring
* `#8551 <https://github.com/numpy/numpy/pull/8551>`__: DOC: Missing backticks
* `#8555 <https://github.com/numpy/numpy/pull/8555>`__: Fixing docstring error in polyvander2d
* `#8558 <https://github.com/numpy/numpy/pull/8558>`__: DOC: Improve documentation of None as interval bounds in clip.
* `#8567 <https://github.com/numpy/numpy/pull/8567>`__: TST: core: use aligned memory for dot() out= arrays
* `#8568 <https://github.com/numpy/numpy/pull/8568>`__: TST: re-enable PPC longdouble spacing tests
* `#8569 <https://github.com/numpy/numpy/pull/8569>`__: ENH: Add missing `__tracebackhide__` to testing functions.
* `#8570 <https://github.com/numpy/numpy/pull/8570>`__: BUG: fix issue #8250 when np.array gets called on an invalid...
* `#8571 <https://github.com/numpy/numpy/pull/8571>`__: BUG: fix calling python api with error set and minor leaks
* `#8572 <https://github.com/numpy/numpy/pull/8572>`__: MAINT: remove ma out= workaround
* `#8575 <https://github.com/numpy/numpy/pull/8575>`__: DOC: fix several typos #8537.
* `#8584 <https://github.com/numpy/numpy/pull/8584>`__: MAINT: Use the same exception for all bad axis requests
* `#8586 <https://github.com/numpy/numpy/pull/8586>`__: MAINT: PyPy3 compatibility: sys.getsizeof()
* `#8590 <https://github.com/numpy/numpy/pull/8590>`__: BUG MaskedArray `__eq__` wrong for masked scalar, multi-d recarray
* `#8591 <https://github.com/numpy/numpy/pull/8591>`__: BUG: make np.squeeze always return an array, never a scalar
* `#8592 <https://github.com/numpy/numpy/pull/8592>`__: MAINT: Remove `__setslice__` and `__getslice__`
* `#8594 <https://github.com/numpy/numpy/pull/8594>`__: BUG: Fix `MaskedArray.__setitem__`
* `#8596 <https://github.com/numpy/numpy/pull/8596>`__: BUG: match hard-coded finfo to calculated MachAr
* `#8602 <https://github.com/numpy/numpy/pull/8602>`__: BUG: Make iscomplexobj compatible with custom dtypes again
* `#8605 <https://github.com/numpy/numpy/pull/8605>`__: DOC: gradient uses 1st order central difference in the interior
* `#8606 <https://github.com/numpy/numpy/pull/8606>`__: Revert "DOC: gradient uses 1st order central difference in the...
* `#8610 <https://github.com/numpy/numpy/pull/8610>`__: Revert "BUG: make np.squeeze always return an array, never a...
* `#8611 <https://github.com/numpy/numpy/pull/8611>`__: DOC: The axis argument of average can be a tuple of ints
* `#8612 <https://github.com/numpy/numpy/pull/8612>`__: MAINT: Decrease merge conflicts in release notes
* `#8614 <https://github.com/numpy/numpy/pull/8614>`__: BUG: Don't leak internal exceptions when given an empty array
* `#8617 <https://github.com/numpy/numpy/pull/8617>`__: BUG: Copy meshgrid after broadcasting
* `#8618 <https://github.com/numpy/numpy/pull/8618>`__: BUG: Fix undefined behaviour induced by bad `__array_wrap__`
* `#8619 <https://github.com/numpy/numpy/pull/8619>`__: BUG: blas_info should record include_dirs
* `#8625 <https://github.com/numpy/numpy/pull/8625>`__: DOC: Create 1.12.1 release notes.
* `#8629 <https://github.com/numpy/numpy/pull/8629>`__: ENH: Improve the efficiency of indices
* `#8631 <https://github.com/numpy/numpy/pull/8631>`__: Fix typo in fill_diagonal docstring.
* `#8633 <https://github.com/numpy/numpy/pull/8633>`__: DOC: Mention boolean arrays in the ix_ documentation.
* `#8636 <https://github.com/numpy/numpy/pull/8636>`__: MAINT: ensure benchmark suite is importable on old numpy versions
* `#8638 <https://github.com/numpy/numpy/pull/8638>`__: BUG: fix wrong odd determination in packbits
* `#8643 <https://github.com/numpy/numpy/pull/8643>`__: BUG: Fix double-wrapping of object scalars
* `#8645 <https://github.com/numpy/numpy/pull/8645>`__: MAINT: Use getmask where possible
* `#8646 <https://github.com/numpy/numpy/pull/8646>`__: ENH: Allow for an in-place nan_to_num conversion
* `#8647 <https://github.com/numpy/numpy/pull/8647>`__: Fix various bugs in np.ma.where
* `#8649 <https://github.com/numpy/numpy/pull/8649>`__: Upgrade to Lapack lite 3.2.2
* `#8650 <https://github.com/numpy/numpy/pull/8650>`__: DOC: Fix obsolete data in readme
* `#8651 <https://github.com/numpy/numpy/pull/8651>`__: MAINT: Split lapack_lite more logically across files
* `#8652 <https://github.com/numpy/numpy/pull/8652>`__: TST: Improve testing of read-only mmaps
* `#8655 <https://github.com/numpy/numpy/pull/8655>`__: MAINT: Squelch parenthesis warnings from GCC
* `#8656 <https://github.com/numpy/numpy/pull/8656>`__: BUG: allow for precision > 17 in longdouble repr test
* `#8658 <https://github.com/numpy/numpy/pull/8658>`__: BUG: fix denormal linspace test for longdouble
* `#8659 <https://github.com/numpy/numpy/pull/8659>`__: BUG: PPC64el machines are POWER for Fortran
* `#8663 <https://github.com/numpy/numpy/pull/8663>`__: ENH: Fix alignment of repr for array subclasses
* `#8665 <https://github.com/numpy/numpy/pull/8665>`__: BUG: Look up methods on MaskedArray in _frommethod
* `#8667 <https://github.com/numpy/numpy/pull/8667>`__: BUG: Preserve identity of dtypes in make_mask_descr
* `#8668 <https://github.com/numpy/numpy/pull/8668>`__: DOC: Add more examples for `np.c_`
* `#8669 <https://github.com/numpy/numpy/pull/8669>`__: MAINT: Warn users when calling np.ma.MaskedArray.partition function.
* `#8672 <https://github.com/numpy/numpy/pull/8672>`__: BUG: Use int for axes, not intp
* `#8674 <https://github.com/numpy/numpy/pull/8674>`__: BUG: Remove extra digit in binary_repr at limit
* `#8675 <https://github.com/numpy/numpy/pull/8675>`__: BUG: Fix problems detecting runtime for MSYS2 compiler on Windows
* `#8677 <https://github.com/numpy/numpy/pull/8677>`__: MAINT: We can now rely on itertools.izip_longest existing
* `#8678 <https://github.com/numpy/numpy/pull/8678>`__: BUG: Fix argsort vs sort in Masked arrays
* `#8680 <https://github.com/numpy/numpy/pull/8680>`__: DOC: Removed broken link
* `#8682 <https://github.com/numpy/numpy/pull/8682>`__: ENH: allow argument to matrix_rank to be stacked
* `#8685 <https://github.com/numpy/numpy/pull/8685>`__: ENH: add dtype.ndim
* `#8688 <https://github.com/numpy/numpy/pull/8688>`__: DOC: Added note to np.diff
* `#8692 <https://github.com/numpy/numpy/pull/8692>`__: MAINT: Fix deprecated escape sequences
* `#8694 <https://github.com/numpy/numpy/pull/8694>`__: BUG: missing comma disabled some header checks
* `#8695 <https://github.com/numpy/numpy/pull/8695>`__: MAINT: Remove numpy-macosx-installer and win32build directories.
* `#8698 <https://github.com/numpy/numpy/pull/8698>`__: DOC: fix incorrect mask value when value was changed
* `#8702 <https://github.com/numpy/numpy/pull/8702>`__: DOC: Fixed small mistakes in numpy.copy documentation.
* `#8704 <https://github.com/numpy/numpy/pull/8704>`__: BUG: Fix deepcopy regression for empty arrays.
* `#8705 <https://github.com/numpy/numpy/pull/8705>`__: BUG: fix ma.median for empty ndarrays
* `#8709 <https://github.com/numpy/numpy/pull/8709>`__: DOC: Fixed minor typos in temp_elide.c
* `#8713 <https://github.com/numpy/numpy/pull/8713>`__: BUG: Don't signal FP exceptions in np.absolute
* `#8716 <https://github.com/numpy/numpy/pull/8716>`__: MAINT: Mark some tests with slow decorator
* `#8718 <https://github.com/numpy/numpy/pull/8718>`__: BUG: Fix assert statements in random.choice tests
* `#8729 <https://github.com/numpy/numpy/pull/8729>`__: DOC: Add float_power to routines.math documentation autosummary
* `#8731 <https://github.com/numpy/numpy/pull/8731>`__: DOC: added linalg.multi_dot to doc
* `#8737 <https://github.com/numpy/numpy/pull/8737>`__: DOC: Mention that expand_dims and squeeze are inverses
* `#8744 <https://github.com/numpy/numpy/pull/8744>`__: MAINT: Remove files and constants that were only needed for Bento.
* `#8745 <https://github.com/numpy/numpy/pull/8745>`__: TST: Remove unused env from tox
* `#8746 <https://github.com/numpy/numpy/pull/8746>`__: DOC: Update 1.12.1 release notes.
* `#8749 <https://github.com/numpy/numpy/pull/8749>`__: DOC: Add 1.12.1 release notes to documentation.
* `#8750 <https://github.com/numpy/numpy/pull/8750>`__: BUG: Fix np.average for object arrays
* `#8754 <https://github.com/numpy/numpy/pull/8754>`__: ENH: Allows building npy_math with static inlining
* `#8756 <https://github.com/numpy/numpy/pull/8756>`__: BUG: Correct lapack ld* args
* `#8759 <https://github.com/numpy/numpy/pull/8759>`__: BUG: Add HOME to the git environment.
* `#8761 <https://github.com/numpy/numpy/pull/8761>`__: MAINT: better warning message when running build_src from sdist
* `#8762 <https://github.com/numpy/numpy/pull/8762>`__: BUG: Prevent crash in `poly1d.__eq__`
* `#8781 <https://github.com/numpy/numpy/pull/8781>`__: BUG: Revert gh-8570.
* `#8788 <https://github.com/numpy/numpy/pull/8788>`__: BUG: Fix scipy incompatibility with cleanup to poly1d
* `#8792 <https://github.com/numpy/numpy/pull/8792>`__: DOC: Fix typos
* `#8793 <https://github.com/numpy/numpy/pull/8793>`__: DOC: fix minor docstring typos
* `#8795 <https://github.com/numpy/numpy/pull/8795>`__: ENH: Add the 'heaviside' ufunc.
* `#8796 <https://github.com/numpy/numpy/pull/8796>`__: BUG: fix regex of determineexprtype_re_3 in numpy/f2py/crackfortran.py
* `#8799 <https://github.com/numpy/numpy/pull/8799>`__: DOC: Include np. prefix in meshgrid examples
* `#8801 <https://github.com/numpy/numpy/pull/8801>`__: BUG: fix the error msg of empty hstack input
* `#8806 <https://github.com/numpy/numpy/pull/8806>`__: BUG: Raise TypeError on ternary power
* `#8807 <https://github.com/numpy/numpy/pull/8807>`__: TST: Prove that poly1d coeffs are immutable
* `#8813 <https://github.com/numpy/numpy/pull/8813>`__: MAINT: tidy up some of npyio
* `#8816 <https://github.com/numpy/numpy/pull/8816>`__: BUG: `np.lib.index_tricks.r_` mutates its own state
* `#8820 <https://github.com/numpy/numpy/pull/8820>`__: DOC: Add 'heaviside' to the ufunc documentation.
* `#8822 <https://github.com/numpy/numpy/pull/8822>`__: DOC: Use gray and hsv colormaps in examples
* `#8824 <https://github.com/numpy/numpy/pull/8824>`__: MAINT: a couple distutils cleanups
* `#8825 <https://github.com/numpy/numpy/pull/8825>`__: STY: Fix bad style in umath_linalg
* `#8828 <https://github.com/numpy/numpy/pull/8828>`__: DOC: Add missing release note for #8584
* `#8830 <https://github.com/numpy/numpy/pull/8830>`__: DOC: added a whitespace so that sphinx directive displays correctly
* `#8832 <https://github.com/numpy/numpy/pull/8832>`__: MAINT: Remove python <2.7,<3.3 string/unicode workarounds
* `#8834 <https://github.com/numpy/numpy/pull/8834>`__: BENCH: use initialized memory for count_nonzero benchmark
* `#8835 <https://github.com/numpy/numpy/pull/8835>`__: DOC: Include nextafter and spacing function in documentation.
* `#8836 <https://github.com/numpy/numpy/pull/8836>`__: DOC: Several documentation fixes (broken links, incorrect sphinx...
* `#8837 <https://github.com/numpy/numpy/pull/8837>`__: DOC: Spell out note for `hstack`
* `#8840 <https://github.com/numpy/numpy/pull/8840>`__: DOC: update docs and comments for move of mailing list to python.org
* `#8843 <https://github.com/numpy/numpy/pull/8843>`__: MAINT: Use AxisError in more places
* `#8844 <https://github.com/numpy/numpy/pull/8844>`__: DOC: Spell out note for `dstack`
* `#8845 <https://github.com/numpy/numpy/pull/8845>`__: DOC: Add release note about np.real and np.conj
* `#8846 <https://github.com/numpy/numpy/pull/8846>`__: BUG: Buttress handling of extreme values in randint
* `#8847 <https://github.com/numpy/numpy/pull/8847>`__: DOC: Preliminary edit of 1.13.0 release notes.
* `#8850 <https://github.com/numpy/numpy/pull/8850>`__: DOC: Updated doc of nonzero()
* `#8852 <https://github.com/numpy/numpy/pull/8852>`__: MAINT: restore auto-vectorization of inplace operations
* `#8854 <https://github.com/numpy/numpy/pull/8854>`__: MAINT: Remove manual expansion of template loop for some ufuncs
* `#8857 <https://github.com/numpy/numpy/pull/8857>`__: DOC: remove empty jargon reference in glossary
* `#8859 <https://github.com/numpy/numpy/pull/8859>`__: DOC: Fixed README formatting
* `#8861 <https://github.com/numpy/numpy/pull/8861>`__: MAINT: Include the function name in all argument error messages
* `#8862 <https://github.com/numpy/numpy/pull/8862>`__: BUG: do not memcpy ptr to freed object
* `#8870 <https://github.com/numpy/numpy/pull/8870>`__: TST: Respect compiler customizations
* `#8871 <https://github.com/numpy/numpy/pull/8871>`__: DOC: Replace line that was errantly removed in #8850
* `#8873 <https://github.com/numpy/numpy/pull/8873>`__: BUG: Make runtests.py --shell behave better on windows
* `#8874 <https://github.com/numpy/numpy/pull/8874>`__: TST: Use explicit NaT in test_structure_format
* `#8876 <https://github.com/numpy/numpy/pull/8876>`__: MAINT: Minor ufunc cleanup
* `#8883 <https://github.com/numpy/numpy/pull/8883>`__: BUG: Ensure Errors are correctly checked when PyFloat_AsDouble...
* `#8884 <https://github.com/numpy/numpy/pull/8884>`__: BUG: Check for errors when PyInt_AsLong is called in np.random
* `#8885 <https://github.com/numpy/numpy/pull/8885>`__: ENH: add support for python3.6 memory tracing
* `#8886 <https://github.com/numpy/numpy/pull/8886>`__: ENH: add np.block to improve upon np.bmat
* `#8888 <https://github.com/numpy/numpy/pull/8888>`__: BUG: Don't modify types after PyType_Ready
* `#8890 <https://github.com/numpy/numpy/pull/8890>`__: DOC: proposed fixes for issues #7622 and #7914
* `#8894 <https://github.com/numpy/numpy/pull/8894>`__: MAINT: Use PyArray_FROM_* macros
* `#8895 <https://github.com/numpy/numpy/pull/8895>`__: BUG: return values of exec_command were swapped
* `#8896 <https://github.com/numpy/numpy/pull/8896>`__: ENH: do integer**2. inplace
* `#8897 <https://github.com/numpy/numpy/pull/8897>`__: ENH: don't rebuild unchanged files
* `#8898 <https://github.com/numpy/numpy/pull/8898>`__: BUG: Move ctypes ImportError catching to appropriate place
* `#8900 <https://github.com/numpy/numpy/pull/8900>`__: Fix typos.
* `#8903 <https://github.com/numpy/numpy/pull/8903>`__: BUG: Fix setitem on UNICODE, STRING, and LONGDOUBLE
* `#8905 <https://github.com/numpy/numpy/pull/8905>`__: BUG: Correctly distinguish between 0d arrays and scalars in `MaskedArray.__getitem__`
* `#8907 <https://github.com/numpy/numpy/pull/8907>`__: COMPAT: notify garbage collector when memory is allocated
* `#8911 <https://github.com/numpy/numpy/pull/8911>`__: BUG: check_api_dict does not correctly handle tuple values
* `#8914 <https://github.com/numpy/numpy/pull/8914>`__: DOC: Replace reference to np.swapaxis with np.swapaxes
* `#8918 <https://github.com/numpy/numpy/pull/8918>`__: DEP: deprecate calling ma.argsort without an axis
* `#8919 <https://github.com/numpy/numpy/pull/8919>`__: MAINT, TST: Remove duplicated code for testing the two types...
* `#8921 <https://github.com/numpy/numpy/pull/8921>`__: MAINT: avoid memcpy when i == j
* `#8925 <https://github.com/numpy/numpy/pull/8925>`__: DOC: Fix incorrect call to set_printoptions
* `#8928 <https://github.com/numpy/numpy/pull/8928>`__: BUG: runtests --bench fails on windows
* `#8929 <https://github.com/numpy/numpy/pull/8929>`__: BENCH: Masked array benchmarks
* `#8939 <https://github.com/numpy/numpy/pull/8939>`__: DEP: Deprecate `np.ma.MaskedArray.mini`
* `#8942 <https://github.com/numpy/numpy/pull/8942>`__: DOC: stop referring to 'S' dtype as string
* `#8948 <https://github.com/numpy/numpy/pull/8948>`__: DEP: Deprecate NPY_CHAR
* `#8949 <https://github.com/numpy/numpy/pull/8949>`__: REL: add `python_requires` to setup.py
* `#8951 <https://github.com/numpy/numpy/pull/8951>`__: ENH: Add ufunc.identity for hypot and logical_xor
* `#8953 <https://github.com/numpy/numpy/pull/8953>`__: DEP: Add back `ndarray.__[sg]etslice__`, but deprecate it
* `#8959 <https://github.com/numpy/numpy/pull/8959>`__: DEP: Remove alter/restore dot methods
* `#8961 <https://github.com/numpy/numpy/pull/8961>`__: MAINT: Update Intel compiler options.
* `#8962 <https://github.com/numpy/numpy/pull/8962>`__: DOC: Wrong return type of np.random.choice and wrong variable...
* `#8963 <https://github.com/numpy/numpy/pull/8963>`__: BUG: Prevent crash on repr of recursive array
* `#8964 <https://github.com/numpy/numpy/pull/8964>`__: BUG: don't create array with invalid memory in where
* `#8967 <https://github.com/numpy/numpy/pull/8967>`__: ENH: add np.positive ufunc
* `#8971 <https://github.com/numpy/numpy/pull/8971>`__: BUG: do not change size 0 description when viewing data
* `#8976 <https://github.com/numpy/numpy/pull/8976>`__: BUG: Prevent VOID_copyswapn ignoring strides
* `#8978 <https://github.com/numpy/numpy/pull/8978>`__: TST: enable shadowed test
* `#8980 <https://github.com/numpy/numpy/pull/8980>`__: DOC: Correct shape of edges in np.histogram2d
* `#8988 <https://github.com/numpy/numpy/pull/8988>`__: DOC: Explain the behavior of diff on unsigned types
* `#8989 <https://github.com/numpy/numpy/pull/8989>`__: ENH: Print object arrays containing lists unambiguously
* `#8996 <https://github.com/numpy/numpy/pull/8996>`__: BUG/DEP: Make ufunclike functions more ufunc-like
* `#8997 <https://github.com/numpy/numpy/pull/8997>`__: TST: fix io test that doesn't close file
* `#8998 <https://github.com/numpy/numpy/pull/8998>`__: DOC: Use ` instead of * to refer to a function parameter.
* `#8999 <https://github.com/numpy/numpy/pull/8999>`__: TST: Enable NPY_RELAXED_STRIDES_DEBUG environment variable.
* `#9002 <https://github.com/numpy/numpy/pull/9002>`__: MAINT: Document ufunc(where=...) as defaulting to True
* `#9012 <https://github.com/numpy/numpy/pull/9012>`__: MAINT: Set the `__name__` of generated methods
* `#9013 <https://github.com/numpy/numpy/pull/9013>`__: BUG: Fix np.lib.nanfunctions on object arrays
* `#9014 <https://github.com/numpy/numpy/pull/9014>`__: BUG: `__array_ufunc__= None` -> TypeError
* `#9015 <https://github.com/numpy/numpy/pull/9015>`__: ENH: Use `__array_ufunc__ = None` in polynomial convenience classes.
* `#9021 <https://github.com/numpy/numpy/pull/9021>`__: BUG: Make ndarray inplace operators forward calls when needed.
* `#9024 <https://github.com/numpy/numpy/pull/9024>`__: DOC: Correct default stop index value for negative stepping.
* `#9026 <https://github.com/numpy/numpy/pull/9026>`__: ENH: Show full PEP 457 argument lists for ufuncs
* `#9027 <https://github.com/numpy/numpy/pull/9027>`__: DOC: update binary-op / ufunc interactions and recommendations...
* `#9038 <https://github.com/numpy/numpy/pull/9038>`__: BUG: check compiler flags to determine the need for a rebuild
* `#9039 <https://github.com/numpy/numpy/pull/9039>`__: DOC: actually produce docs for as_strided
* `#9050 <https://github.com/numpy/numpy/pull/9050>`__: BUG: distutils, add compatibility python parallelization
* `#9054 <https://github.com/numpy/numpy/pull/9054>`__: BUG: Various fixes to _dtype_from_pep3118
* `#9058 <https://github.com/numpy/numpy/pull/9058>`__: MAINT: Update FutureWarning message.
* `#9060 <https://github.com/numpy/numpy/pull/9060>`__: DEP: deprecate ndarray.conjugate's no-op fall through for non-numeric...
* `#9061 <https://github.com/numpy/numpy/pull/9061>`__: BUG: ndarray.conjugate broken for custom dtypes (unlike np.conjugate)
* `#9062 <https://github.com/numpy/numpy/pull/9062>`__: STY: two blank lines between classes per PEP8
* `#9063 <https://github.com/numpy/numpy/pull/9063>`__: ENH: add np.divmod ufunc
* `#9070 <https://github.com/numpy/numpy/pull/9070>`__: BUG: Preserve field order in join_by, avoids FutureWarning
* `#9072 <https://github.com/numpy/numpy/pull/9072>`__: BUG: if importing multiarray fails, don't discard the error message
* `#9074 <https://github.com/numpy/numpy/pull/9074>`__: MAINT: Python 3.6 invalid escape sequence deprecation fixes
* `#9075 <https://github.com/numpy/numpy/pull/9075>`__: ENH: Spelling fixes
* `#9077 <https://github.com/numpy/numpy/pull/9077>`__: BUG: Prevent stackoverflow on self-containing arrays
* `#9080 <https://github.com/numpy/numpy/pull/9080>`__: MAINT, DOC: Update 1.13.0 release notes and .mailmap
* `#9087 <https://github.com/numpy/numpy/pull/9087>`__: BUG: `__array_ufunc__` should always be looked up on the type,...
* `#9091 <https://github.com/numpy/numpy/pull/9091>`__: MAINT: refine error message for `__array_ufunc__` not implemented
* `#9093 <https://github.com/numpy/numpy/pull/9093>`__: BUG remove memory leak in array ufunc override.
* `#9097 <https://github.com/numpy/numpy/pull/9097>`__: TST: fix test_basic failure on Windows
* `#9111 <https://github.com/numpy/numpy/pull/9111>`__: BUG: Array ufunc reduce out tuple
* `#9123 <https://github.com/numpy/numpy/pull/9123>`__: DOC: update 1.13 release note for MaskedArray, masked constants...
* `#9124 <https://github.com/numpy/numpy/pull/9124>`__: BUG: Do not elide complex abs() for 1.13
* `#9129 <https://github.com/numpy/numpy/pull/9129>`__: BUG: `ndarray.__pow__` does not check result of fast_scalar_power
* `#9133 <https://github.com/numpy/numpy/pull/9133>`__: DEP: Deprecate incorrect behavior of expand_dims.
* `#9135 <https://github.com/numpy/numpy/pull/9135>`__: BUG: delay calls of array repr in getlimits
* `#9136 <https://github.com/numpy/numpy/pull/9136>`__: BUG: Compilation crashes in MSVC when LIB or INCLUDE is not set
* `#9173 <https://github.com/numpy/numpy/pull/9173>`__: BUG: have as_strided() keep custom dtypes
* `#9175 <https://github.com/numpy/numpy/pull/9175>`__: BUG: ensure structured `ndarray.__eq__,__ne__` defer when appropriate.
* `#9196 <https://github.com/numpy/numpy/pull/9196>`__: BUG: pull request 9087 modifies a tuple after use
* `#9199 <https://github.com/numpy/numpy/pull/9199>`__: DOC: Update bincount docs to reflect gh-8348 (backport)

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Christoph Gohlke
* Matti Picus
* Raghuveer Devulapalli
* Sebastian Berg
* Simon Graham +
* Veniamin Petrenko +
* Bernie Gray +

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#17756 <https://github.com/numpy/numpy/pull/17756>`__: BUG: Fix segfault due to out of bound pointer in floatstatus...
* `#17774 <https://github.com/numpy/numpy/pull/17774>`__: BUG: fix np.timedelta64('nat').__format__ throwing an exception
* `#17775 <https://github.com/numpy/numpy/pull/17775>`__: BUG: Fixed file handle leak in array_tofile.
* `#17786 <https://github.com/numpy/numpy/pull/17786>`__: BUG: Raise recursion error during dimension discovery
* `#17917 <https://github.com/numpy/numpy/pull/17917>`__: BUG: Fix subarray dtype used with too large count in fromfile
* `#17918 <https://github.com/numpy/numpy/pull/17918>`__: BUG: 'bool' object has no attribute 'ndim'
* `#17919 <https://github.com/numpy/numpy/pull/17919>`__: BUG: ensure _UFuncNoLoopError can be pickled
* `#17924 <https://github.com/numpy/numpy/pull/17924>`__: BLD: use BUFFERSIZE=20 in OpenBLAS
* `#18026 <https://github.com/numpy/numpy/pull/18026>`__: BLD: update to OpenBLAS 0.3.13
* `#18036 <https://github.com/numpy/numpy/pull/18036>`__: BUG: make a variable volatile to work around clang compiler bug
* `#18114 <https://github.com/numpy/numpy/pull/18114>`__: REL: Prepare for the NumPy 1.19.5 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Bas van Beek
* Charles Harris
* Christoph Gohlke
* Mateusz Sokół +
* Michael Lamparski
* Sebastian Berg

Pull requests merged
====================

A total of 20 pull requests were merged for this release.

* `#18382 <https://github.com/numpy/numpy/pull/18382>`__: MAINT: Update f2py from master.
* `#18459 <https://github.com/numpy/numpy/pull/18459>`__: BUG: ``diagflat`` could overflow on windows or 32-bit platforms
* `#18460 <https://github.com/numpy/numpy/pull/18460>`__: BUG: Fix refcount leak in f2py ``complex_double_from_pyobj``.
* `#18461 <https://github.com/numpy/numpy/pull/18461>`__: BUG: Fix tiny memory leaks when ``like=`` overrides are used
* `#18462 <https://github.com/numpy/numpy/pull/18462>`__: BUG: Remove temporary change of descr/flags in VOID functions
* `#18469 <https://github.com/numpy/numpy/pull/18469>`__: BUG: Segfault in nditer buffer dealloc for Object arrays
* `#18485 <https://github.com/numpy/numpy/pull/18485>`__: BUG: Remove suspicious type casting
* `#18486 <https://github.com/numpy/numpy/pull/18486>`__: BUG: remove nonsensical comparison of pointer < 0
* `#18487 <https://github.com/numpy/numpy/pull/18487>`__: BUG: verify pointer against NULL before using it
* `#18488 <https://github.com/numpy/numpy/pull/18488>`__: BUG: check if PyArray_malloc succeeded
* `#18546 <https://github.com/numpy/numpy/pull/18546>`__: BUG: incorrect error fallthrough in nditer
* `#18559 <https://github.com/numpy/numpy/pull/18559>`__: CI: Backport CI fixes from main.
* `#18599 <https://github.com/numpy/numpy/pull/18599>`__: MAINT: Add annotations for ``dtype.__getitem__``, ``__mul__`` and...
* `#18611 <https://github.com/numpy/numpy/pull/18611>`__: BUG: NameError in numpy.distutils.fcompiler.compaq
* `#18612 <https://github.com/numpy/numpy/pull/18612>`__: BUG: Fixed ``where`` keyword for ``np.mean`` & ``np.var`` methods
* `#18617 <https://github.com/numpy/numpy/pull/18617>`__: CI: Update apt package list before Python install
* `#18636 <https://github.com/numpy/numpy/pull/18636>`__: MAINT: Ensure that re-exported sub-modules are properly annotated
* `#18638 <https://github.com/numpy/numpy/pull/18638>`__: BUG: Fix ma coercion list-of-ma-arrays if they do not cast to...
* `#18661 <https://github.com/numpy/numpy/pull/18661>`__: BUG: Fix small valgrind-found issues
* `#18671 <https://github.com/numpy/numpy/pull/18671>`__: BUG: Fix small issues found with pytest-leaks

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Marten van Kerkwijk
* Matti Picus
* Pauli Virtanen
* Ryan Soklaski +
* Sebastian Berg

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#11104 <https://github.com/numpy/numpy/pull/11104>`__: BUG: str of DOUBLE_DOUBLE format wrong on ppc64
* `#11170 <https://github.com/numpy/numpy/pull/11170>`__: TST: linalg: add regression test for gh-8577
* `#11174 <https://github.com/numpy/numpy/pull/11174>`__: MAINT: add sanity-checks to be run at import time
* `#11181 <https://github.com/numpy/numpy/pull/11181>`__: BUG: void dtype setup checked offset not actual pointer for alignment
* `#11194 <https://github.com/numpy/numpy/pull/11194>`__: BUG: Python2 doubles don't print correctly in interactive shell.
* `#11198 <https://github.com/numpy/numpy/pull/11198>`__: BUG: optimizing compilers can reorder call to npy_get_floatstatus
* `#11199 <https://github.com/numpy/numpy/pull/11199>`__: BUG: reduce using SSE only warns if inside SSE loop
* `#11203 <https://github.com/numpy/numpy/pull/11203>`__: BUG: Bytes delimiter/comments in genfromtxt should be decoded
* `#11211 <https://github.com/numpy/numpy/pull/11211>`__: BUG: Fix reference count/memory leak exposed by better testing
* `#11219 <https://github.com/numpy/numpy/pull/11219>`__: BUG: Fixes einsum broadcasting bug when optimize=True
* `#11251 <https://github.com/numpy/numpy/pull/11251>`__: DOC: Document 1.14.4 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Jeroen Demeyer
* Kevin Sheppard
* Matthew Bowden +
* Matti Picus
* Tyler Reddy

Pull requests merged
====================

A total of 12 pull requests were merged for this release.

* `#12080 <https://github.com/numpy/numpy/pull/12080>`__: MAINT: Blacklist some MSVC complex functions.
* `#12083 <https://github.com/numpy/numpy/pull/12083>`__: TST: Add azure CI testing to 1.15.x branch.
* `#12084 <https://github.com/numpy/numpy/pull/12084>`__: BUG: test_path() now uses Path.resolve()
* `#12085 <https://github.com/numpy/numpy/pull/12085>`__: TST, MAINT: Fix some failing tests on azure-pipelines mac and...
* `#12187 <https://github.com/numpy/numpy/pull/12187>`__: BUG: Fix memory leak in mapping.c
* `#12188 <https://github.com/numpy/numpy/pull/12188>`__: BUG: Allow boolean subtract in histogram
* `#12189 <https://github.com/numpy/numpy/pull/12189>`__: BUG: Fix in-place permutation
* `#12190 <https://github.com/numpy/numpy/pull/12190>`__: BUG: limit default for get_num_build_jobs() to 8
* `#12191 <https://github.com/numpy/numpy/pull/12191>`__: BUG: OBJECT_to_* should check for errors
* `#12192 <https://github.com/numpy/numpy/pull/12192>`__: DOC: Prepare for NumPy 1.15.3 release.
* `#12237 <https://github.com/numpy/numpy/pull/12237>`__: BUG: Fix MaskedArray fill_value type conversion.
* `#12238 <https://github.com/numpy/numpy/pull/12238>`__: TST: Backport azure-pipeline testing fixes for Mac

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Nicholas McKibben +
* Pearu Peterson
* Ralf Gommers
* Sebastian Berg
* Tyler Reddy
* @Aerysv +

Pull requests merged
====================

A total of 15 pull requests were merged for this release.

* `#18306 <https://github.com/numpy/numpy/pull/18306>`__: MAINT: Add missing placeholder annotations
* `#18310 <https://github.com/numpy/numpy/pull/18310>`__: BUG: Fix typo in ``numpy.__init__.py``
* `#18326 <https://github.com/numpy/numpy/pull/18326>`__: BUG: don't mutate list of fake libraries while iterating over...
* `#18327 <https://github.com/numpy/numpy/pull/18327>`__: MAINT: gracefully shuffle memoryviews
* `#18328 <https://github.com/numpy/numpy/pull/18328>`__: BUG: Use C linkage for random distributions
* `#18336 <https://github.com/numpy/numpy/pull/18336>`__: CI: fix when GitHub Actions builds trigger, and allow ci skips
* `#18337 <https://github.com/numpy/numpy/pull/18337>`__: BUG: Allow unmodified use of isclose, allclose, etc. with timedelta
* `#18345 <https://github.com/numpy/numpy/pull/18345>`__: BUG: Allow pickling all relevant DType types/classes
* `#18351 <https://github.com/numpy/numpy/pull/18351>`__: BUG: Fix missing signed_char dependency. Closes #18335.
* `#18352 <https://github.com/numpy/numpy/pull/18352>`__: DOC: Change license date 2020 -> 2021
* `#18353 <https://github.com/numpy/numpy/pull/18353>`__: CI: CircleCI seems to occasionally time out, increase the limit
* `#18354 <https://github.com/numpy/numpy/pull/18354>`__: BUG: Fix f2py bugs when wrapping F90 subroutines.
* `#18356 <https://github.com/numpy/numpy/pull/18356>`__: MAINT: crackfortran regex simplify
* `#18357 <https://github.com/numpy/numpy/pull/18357>`__: BUG: threads.h existence test requires GLIBC > 2.12.
* `#18359 <https://github.com/numpy/numpy/pull/18359>`__: REL: Prepare for the NumPy 1.20.1 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* CakeWithSteak +
* Charles Harris
* Dan Allan
* Hameer Abbasi
* Lars Grueter
* Matti Picus
* Sebastian Berg

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#14418 <https://github.com/numpy/numpy/pull/14418>`__: BUG: Fix aradixsort indirect indexing.
* `#14420 <https://github.com/numpy/numpy/pull/14420>`__: DOC: Fix a minor typo in dispatch documentation.
* `#14421 <https://github.com/numpy/numpy/pull/14421>`__: BUG: test, fix regression in converting to ctypes
* `#14430 <https://github.com/numpy/numpy/pull/14430>`__: BUG: Do not show Override module in private error classes.
* `#14432 <https://github.com/numpy/numpy/pull/14432>`__: BUG: Fixed maximum relative error reporting in assert_allclose.
* `#14433 <https://github.com/numpy/numpy/pull/14433>`__: BUG: Fix uint-overflow if padding with linear_ramp and negative...
* `#14436 <https://github.com/numpy/numpy/pull/14436>`__: BUG: Update 1.17.x with 1.18.0-dev pocketfft.py.
* `#14446 <https://github.com/numpy/numpy/pull/14446>`__: REL: Prepare for NumPy 1.17.2 release.

Contributors
============

A total of 14 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Arryan Singh
* Bas van Beek
* Charles Harris
* Denis Laxalde
* Isuru Fernando
* Kevin Sheppard
* Matthew Barber
* Matti Picus
* Melissa Weber Mendonça
* Mukulika Pahari
* Omid Rajaei +
* Pearu Peterson
* Ralf Gommers
* Sebastian Berg

Pull requests merged
====================

A total of 20 pull requests were merged for this release.

* `#20702 <https://github.com/numpy/numpy/pull/20702>`__: MAINT, DOC: Post 1.22.0 release fixes.
* `#20703 <https://github.com/numpy/numpy/pull/20703>`__: DOC, BUG: Use pngs instead of svgs.
* `#20704 <https://github.com/numpy/numpy/pull/20704>`__: DOC:Fixed the link on user-guide landing page
* `#20714 <https://github.com/numpy/numpy/pull/20714>`__: BUG: Restore vc141 support
* `#20724 <https://github.com/numpy/numpy/pull/20724>`__: BUG: Fix array dimensions solver for multidimensional arguments...
* `#20725 <https://github.com/numpy/numpy/pull/20725>`__: TYP: change type annotation for `__array_namespace__` to ModuleType
* `#20726 <https://github.com/numpy/numpy/pull/20726>`__: TYP, MAINT: Allow `ndindex` to accept integer tuples
* `#20757 <https://github.com/numpy/numpy/pull/20757>`__: BUG: Relax dtype identity check in reductions
* `#20763 <https://github.com/numpy/numpy/pull/20763>`__: TYP: Allow time manipulation functions to accept `date` and `timedelta`...
* `#20768 <https://github.com/numpy/numpy/pull/20768>`__: TYP: Relax the type of `ndarray.__array_finalize__`
* `#20795 <https://github.com/numpy/numpy/pull/20795>`__: MAINT: Raise RuntimeError if setuptools version is too recent.
* `#20796 <https://github.com/numpy/numpy/pull/20796>`__: BUG, DOC: Fixes SciPy docs build warnings
* `#20797 <https://github.com/numpy/numpy/pull/20797>`__: DOC: fix OpenBLAS version in release note
* `#20798 <https://github.com/numpy/numpy/pull/20798>`__: PERF: Optimize array check for bounded 0,1 values
* `#20805 <https://github.com/numpy/numpy/pull/20805>`__: BUG: Fix that reduce-likes honor out always (and live in the...
* `#20806 <https://github.com/numpy/numpy/pull/20806>`__: BUG: ``array_api.argsort(descending=True)`` respects relative...
* `#20807 <https://github.com/numpy/numpy/pull/20807>`__: BUG: Allow integer inputs for pow-related functions in `array_api`
* `#20814 <https://github.com/numpy/numpy/pull/20814>`__: DOC: Refer to NumPy, not pandas, in main page
* `#20815 <https://github.com/numpy/numpy/pull/20815>`__: DOC: Update Copyright to 2022 [License]
* `#20819 <https://github.com/numpy/numpy/pull/20819>`__: BUG: Return correctly shaped inverse indices in array_api set...

Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Ganesh Kathiresan +
* Matti Picus
* Sebastian Berg
* przemb +

Pull requests merged
====================

A total of 7 pull requests were merged for this release.

* `#15675 <https://github.com/numpy/numpy/pull/15675>`__: TST: move _no_tracing to testing._private
* `#15676 <https://github.com/numpy/numpy/pull/15676>`__: MAINT: Large overhead in some random functions
* `#15677 <https://github.com/numpy/numpy/pull/15677>`__: TST: Do not create gfortran link in azure Mac testing.
* `#15679 <https://github.com/numpy/numpy/pull/15679>`__: BUG: Added missing error check in `ndarray.__contains__`
* `#15722 <https://github.com/numpy/numpy/pull/15722>`__: MAINT: use list-based APIs to call subprocesses
* `#15729 <https://github.com/numpy/numpy/pull/15729>`__: REL: Prepare for 1.18.2 release.
* `#15734 <https://github.com/numpy/numpy/pull/15734>`__: BUG: fix logic error when nm fails on 32-bit

Contributors
============

A total of 17 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alexander Jung +
* Allan Haldane
* Charles Harris
* Eric Wieser
* Giuseppe Cuccu +
* Hiroyuki V. Yamazaki
* Jérémie du Boisberranger
* Kmol Yuan +
* Matti Picus
* Max Bolingbroke +
* Maxwell Aladago +
* Oleksandr Pavlyk
* Peter Andreas Entschev
* Sergei Lebedev
* Seth Troisi +
* Vladimir Pershin +
* Warren Weckesser

Pull requests merged
====================

A total of 24 pull requests were merged for this release.

* `#14156 <https://github.com/numpy/numpy/pull/14156>`__: TST: Allow fuss in testing strided/non-strided exp/log loops
* `#14157 <https://github.com/numpy/numpy/pull/14157>`__: BUG: avx2_scalef_ps must be static
* `#14158 <https://github.com/numpy/numpy/pull/14158>`__: BUG: Remove stray print that causes a SystemError on python 3.7.
* `#14159 <https://github.com/numpy/numpy/pull/14159>`__: BUG: Fix DeprecationWarning in python 3.8.
* `#14160 <https://github.com/numpy/numpy/pull/14160>`__: BLD: Add missing gcd/lcm definitions to npy_math.h
* `#14161 <https://github.com/numpy/numpy/pull/14161>`__: DOC, BUILD: cleanups and fix (again) 'build dist'
* `#14166 <https://github.com/numpy/numpy/pull/14166>`__: TST: Add 3.8-dev to travisCI testing.
* `#14194 <https://github.com/numpy/numpy/pull/14194>`__: BUG: Remove the broken clip wrapper (Backport)
* `#14198 <https://github.com/numpy/numpy/pull/14198>`__: DOC: Fix hermitian argument docs in svd.
* `#14199 <https://github.com/numpy/numpy/pull/14199>`__: MAINT: Workaround for Intel compiler bug leading to failing test
* `#14200 <https://github.com/numpy/numpy/pull/14200>`__: TST: Clean up of test_pocketfft.py
* `#14201 <https://github.com/numpy/numpy/pull/14201>`__: BUG: Make advanced indexing result on read-only subclass writeable...
* `#14236 <https://github.com/numpy/numpy/pull/14236>`__: BUG: Fixed default BitGenerator name
* `#14237 <https://github.com/numpy/numpy/pull/14237>`__: ENH: add c-imported modules for freeze analysis in np.random
* `#14296 <https://github.com/numpy/numpy/pull/14296>`__: TST: Pin pytest version to 5.0.1
* `#14301 <https://github.com/numpy/numpy/pull/14301>`__: BUG: Fix leak in the f2py-generated module init and `PyMem_Del`...
* `#14302 <https://github.com/numpy/numpy/pull/14302>`__: BUG: Fix formatting error in exception message
* `#14307 <https://github.com/numpy/numpy/pull/14307>`__: MAINT: random: Match type of SeedSequence.pool_size to DEFAULT_POOL_SIZE.
* `#14308 <https://github.com/numpy/numpy/pull/14308>`__: BUG: Fix numpy.random bug in platform detection
* `#14309 <https://github.com/numpy/numpy/pull/14309>`__: ENH: Enable huge pages in all Linux builds
* `#14330 <https://github.com/numpy/numpy/pull/14330>`__: BUG: Fix segfault in `random.permutation(x)` when x is a string.
* `#14338 <https://github.com/numpy/numpy/pull/14338>`__: BUG: don't fail when lexsorting some empty arrays (#14228)
* `#14339 <https://github.com/numpy/numpy/pull/14339>`__: BUG: Fix misuse of .names and .fields in various places (backport...
* `#14345 <https://github.com/numpy/numpy/pull/14345>`__: BUG: fix behavior of structured_to_unstructured on non-trivial...
* `#14350 <https://github.com/numpy/numpy/pull/14350>`__: REL: Prepare 1.17.1 release

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Anne Archibald
* Bas van Beek
* Charles Harris
* Dong Keun Oh +
* Kamil Choudhury +
* Sayed Adel
* Sebastian Berg

Pull requests merged
====================

A total of 15 pull requests were merged for this release.

* `#18763 <https://github.com/numpy/numpy/pull/18763>`__: BUG: Correct ``datetime64`` missing type overload for ``datetime.date``...
* `#18764 <https://github.com/numpy/numpy/pull/18764>`__: MAINT: Remove ``__all__`` in favor of explicit re-exports
* `#18768 <https://github.com/numpy/numpy/pull/18768>`__: BLD: Strip extra newline when dumping gfortran version on MacOS
* `#18769 <https://github.com/numpy/numpy/pull/18769>`__: BUG: fix segfault in object/longdouble operations
* `#18794 <https://github.com/numpy/numpy/pull/18794>`__: MAINT: Use towncrier build explicitly
* `#18887 <https://github.com/numpy/numpy/pull/18887>`__: MAINT: Relax certain integer-type constraints
* `#18915 <https://github.com/numpy/numpy/pull/18915>`__: MAINT: Remove unsafe unions and ABCs from return-annotations
* `#18921 <https://github.com/numpy/numpy/pull/18921>`__: MAINT: Allow more recursion depth for scalar tests.
* `#18922 <https://github.com/numpy/numpy/pull/18922>`__: BUG: Initialize the full nditer buffer in case of error
* `#18923 <https://github.com/numpy/numpy/pull/18923>`__: BLD: remove unnecessary flag ``-faltivec`` on macOS
* `#18924 <https://github.com/numpy/numpy/pull/18924>`__: MAINT, CI: treats _SIMD module build warnings as errors through...
* `#18925 <https://github.com/numpy/numpy/pull/18925>`__: BUG: for MINGW, threads.h existence test requires GLIBC > 2.12
* `#18941 <https://github.com/numpy/numpy/pull/18941>`__: BUG: Make changelog recognize gh- as a PR number prefix.
* `#18948 <https://github.com/numpy/numpy/pull/18948>`__: REL, DOC: Prepare for the NumPy 1.20.3 release.
* `#18953 <https://github.com/numpy/numpy/pull/18953>`__: BUG: Fix failing mypy test in 1.20.x.

Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Burr +
* Matti Picus
* Qiming Sun +
* Warren Weckesser

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#14758 <https://github.com/numpy/numpy/pull/14758>`__: BLD: declare support for python 3.8
* `#14781 <https://github.com/numpy/numpy/pull/14781>`__: BUG: random: biased samples from integers() with 8 or 16 bit...
* `#14851 <https://github.com/numpy/numpy/pull/14851>`__: BUG: Fix _ctypes class circular reference. (#13808)
* `#14852 <https://github.com/numpy/numpy/pull/14852>`__: BLD: add 'apt update' to shippable
* `#14855 <https://github.com/numpy/numpy/pull/14855>`__: BUG: Fix `np.einsum` errors on Power9 Linux and z/Linux
* `#14857 <https://github.com/numpy/numpy/pull/14857>`__: BUG: lib: Fix histogram problem with signed integer arrays.
* `#14858 <https://github.com/numpy/numpy/pull/14858>`__: BLD: Prevent -flto from optimising long double representation...
* `#14866 <https://github.com/numpy/numpy/pull/14866>`__: MAINT: move buffer.h -> npy_buffer.h to avoid conflicts

Contributors
============

A total of 184 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aaron Meurer +
* Abhilash Barigidad +
* Abhinav Reddy +
* Abhishek Singh +
* Al-Baraa El-Hag +
* Albert Villanova del Moral +
* Alex Leontiev +
* Alex Rockhill +
* Alex Rogozhnikov
* Alexander Belopolsky
* Alexander Kuhn-Regnier +
* Allen Downey +
* Andras Deak
* Andrea Olivo +
* Andrew Eckart +
* Anirudh Subramanian
* Anthony Byuraev +
* Antonio Larrosa +
* Ashutosh Singh +
* Bangcheng Yang +
* Bas van Beek +
* Ben Derrett +
* Ben Elliston +
* Ben Nathanson +
* Bernie Gray +
* Bharat Medasani +
* Bharat Raghunathan
* Bijesh Mohan +
* Bradley Dice +
* Brandon David +
* Brandt Bucher
* Brian Soto +
* Brigitta Sipocz
* Cameron Blocker +
* Carl Leake +
* Charles Harris
* Chris Brown +
* Chris Vavaliaris +
* Christoph Gohlke
* Chunlin Fang
* CloseChoice +
* Daniel G. A. Smith +
* Daniel Hrisca
* Daniel Vanzo +
* David Pitchford +
* Davide Dal Bosco +
* Derek Homeier
* Dima Kogan +
* Dmitry Kutlenkov +
* Douglas Fenstermacher +
* Dustin Spicuzza +
* E. Madison Bray +
* Elia Franzella +
* Enrique Matías Sánchez +
* Erfan Nariman | Veneficus +
* Eric Larson
* Eric Moore
* Eric Wieser
* Erik M. Bray
* EthanCJ-git +
* Etienne Guesnet +
* FX Coudert +
* Felix Divo
* Frankie Robertson +
* Ganesh Kathiresan
* Gengxin Xie
* Gerry Manoim +
* Guilherme Leobas
* Hassan Kibirige
* Hugo Mendes +
* Hugo van Kemenade
* Ian Thomas +
* InessaPawson +
* Isabela Presedo-Floyd +
* Isuru Fernando
* Jakob Jakobson +
* Jakub Wilk
* James Myatt +
* Jesse Li +
* John Hagen +
* John Zwinck
* Joseph Fox-Rabinovitz
* Josh Wilson
* Jovial Joe Jayarson +
* Julia Signell +
* Jun Kudo +
* Karan Dhir +
* Kaspar Thommen +
* Kerem Hallaç
* Kevin Moore +
* Kevin Sheppard
* Klaus Zimmermann +
* LSchroefl +
* Laurie +
* Laurie Stephey +
* Levi Stovall +
* Lisa Schwetlick +
* Lukas Geiger +
* Madhulika Jain Chambers +
* Matthias Bussonnier
* Matti Picus
* Melissa Weber Mendonça
* Michael Hirsch
* Nick R. Papior
* Nikola Forró
* Noman Arshad +
* Paul YS Lee +
* Pauli Virtanen
* Paweł Redzyński +
* Peter Andreas Entschev
* Peter Bell
* Philippe Ombredanne +
* Phoenix Meadowlark +
* Piotr Gaiński
* Raghav Khanna +
* Raghuveer Devulapalli
* Rajas Rade +
* Rakesh Vasudevan
* Ralf Gommers
* Raphael Kruse +
* Rashmi K A +
* Robert Kern
* Rohit Sanjay +
* Roman Yurchak
* Ross Barnowski
* Royston E Tauro +
* Ryan C Cooper +
* Ryan Soklaski
* Safouane Chergui +
* Sahil Siddiq +
* Sarthak Vineet Kumar +
* Sayed Adel
* Sebastian Berg
* Sergei Vorfolomeev +
* Seth Troisi
* Sidhant Bansal +
* Simon Gasse
* Simon Graham +
* Stefan Appelhoff +
* Stefan Behnel +
* Stefan van der Walt
* Steve Dower
* Steve Joachim +
* Steven Pitman +
* Stuart Archibald
* Sturla Molden
* Susan Chang +
* Takanori H +
* Tapajyoti Bose +
* Thomas A Caswell
* Tina Oberoi
* Tirth Patel
* Tobias Pitters +
* Tomoki, Karatsu +
* Tyler Reddy
* Veniamin Petrenko +
* Wansoo Kim +
* Warren Weckesser
* Wei Yang +
* Wojciech Rzadkowski
* Yang Hau +
* Yogesh Raisinghani +
* Yu Feng
* Yuya Unno +
* Zac Hatfield-Dodds
* Zuhair Ali-Khan +
* @abhilash42 +
* @danbeibei +
* @dojafrat
* @dpitch40 +
* @forfun +
* @iamsoto +
* @jbrockmendel +
* @leeyspaul +
* @mitch +
* @prateek arora +
* @serge-sans-paille +
* @skywalker +
* @stphnlyd +
* @xoviat
* @谭九鼎 +
* @JMFT +
* @Jack +
* @Neal C +

Pull requests merged
====================

A total of 716 pull requests were merged for this release.

* `#13516 <https://github.com/numpy/numpy/pull/13516>`__: ENH: enable multi-platform SIMD compiler optimizations
* `#14779 <https://github.com/numpy/numpy/pull/14779>`__: NEP 36 (fair play)
* `#14882 <https://github.com/numpy/numpy/pull/14882>`__: DEP: Deprecate aliases of builtin types in python 3.7+
* `#15037 <https://github.com/numpy/numpy/pull/15037>`__: BUG: ``np.resize`` negative shape and subclasses edge case fixes
* `#15121 <https://github.com/numpy/numpy/pull/15121>`__: ENH: random: Add the method ``permuted`` to Generator.
* `#15162 <https://github.com/numpy/numpy/pull/15162>`__: BUG,MAINT: Fix issues with non-reduce broadcasting axes
* `#15471 <https://github.com/numpy/numpy/pull/15471>`__: BUG: Ensure PyArray_FromScalar always returns the requested dtype
* `#15507 <https://github.com/numpy/numpy/pull/15507>`__: NEP 42: Technical decisions for new DTypes
* `#15508 <https://github.com/numpy/numpy/pull/15508>`__: API: Create Preliminary DTypeMeta class and np.dtype subclasses
* `#15551 <https://github.com/numpy/numpy/pull/15551>`__: DOC: Simd optimization documentation
* `#15604 <https://github.com/numpy/numpy/pull/15604>`__: MAINT: Avoid exception in NpzFile destructor if constructor raises...
* `#15666 <https://github.com/numpy/numpy/pull/15666>`__: ENH: Improved ``__str__`` for polynomials
* `#15759 <https://github.com/numpy/numpy/pull/15759>`__: BUILD: Remove Accelerate support
* `#15791 <https://github.com/numpy/numpy/pull/15791>`__: [DOC] Added tutorial about the numpy.ma module.
* `#15852 <https://github.com/numpy/numpy/pull/15852>`__: ENH: Add where argument to np.mean
* `#15886 <https://github.com/numpy/numpy/pull/15886>`__: DEP: Deprecate passing shape=None to mean shape=()
* `#15900 <https://github.com/numpy/numpy/pull/15900>`__: DEP: Ensure indexing errors will be raised even on empty results
* `#15997 <https://github.com/numpy/numpy/pull/15997>`__: ENH: improve printing of arrays with multi-line reprs
* `#16056 <https://github.com/numpy/numpy/pull/16056>`__: DEP: Deprecate inexact matches for mode, searchside
* `#16130 <https://github.com/numpy/numpy/pull/16130>`__: DOC: Correct documentation of ``__array__`` when used as output...
* `#16134 <https://github.com/numpy/numpy/pull/16134>`__: ENH: Implement concatenate dtype and casting keyword arguments
* `#16156 <https://github.com/numpy/numpy/pull/16156>`__: DEP: Deprecate ``numpy.dual``.
* `#16161 <https://github.com/numpy/numpy/pull/16161>`__: BUG: Potential fix for divmod(1.0, 0.0) to raise divbyzero and...
* `#16167 <https://github.com/numpy/numpy/pull/16167>`__: DOC: Increase guidance and detail of np.polynomial docstring
* `#16174 <https://github.com/numpy/numpy/pull/16174>`__: DOC: Add transition note to all lib/poly functions
* `#16200 <https://github.com/numpy/numpy/pull/16200>`__: ENH: Rewrite of array-coercion to support new dtypes
* `#16205 <https://github.com/numpy/numpy/pull/16205>`__: ENH: Add ``full_output`` argument to ``f2py.compile``.
* `#16207 <https://github.com/numpy/numpy/pull/16207>`__: DOC: Add PyArray_ContiguousFromObject C docs
* `#16232 <https://github.com/numpy/numpy/pull/16232>`__: DEP: Deprecate ufunc.outer with matrix inputs
* `#16237 <https://github.com/numpy/numpy/pull/16237>`__: MAINT: precompute ``log(2.0 * M_PI)`` in ``random_loggam``
* `#16238 <https://github.com/numpy/numpy/pull/16238>`__: MAINT: Unify cached (C-level static) imports
* `#16239 <https://github.com/numpy/numpy/pull/16239>`__: BUG,DOC: Allow attach docs twice but error if wrong
* `#16242 <https://github.com/numpy/numpy/pull/16242>`__: BUG: Fix default fallback in genfromtxt
* `#16247 <https://github.com/numpy/numpy/pull/16247>`__: ENH:Umath Replace raw SIMD of unary float point(32-64) with NPYV...
* `#16248 <https://github.com/numpy/numpy/pull/16248>`__: MRG, ENH: added edge keyword argument to digitize
* `#16253 <https://github.com/numpy/numpy/pull/16253>`__: DOC: Clarify tiny/xmin in finfo and machar
* `#16254 <https://github.com/numpy/numpy/pull/16254>`__: MAINT: Chain exceptions in generate_umath.py
* `#16257 <https://github.com/numpy/numpy/pull/16257>`__: DOC: Update the f2py section of the "Using Python as Glue" page.
* `#16260 <https://github.com/numpy/numpy/pull/16260>`__: DOC: Improve ``rec.array`` function documentation
* `#16266 <https://github.com/numpy/numpy/pull/16266>`__: ENH: include dt64/td64 isinstance checks in ``__init__.pxd``
* `#16267 <https://github.com/numpy/numpy/pull/16267>`__: DOC: Clarifications for np.std
* `#16273 <https://github.com/numpy/numpy/pull/16273>`__: BUG: Order percentile monotonically
* `#16274 <https://github.com/numpy/numpy/pull/16274>`__: MAINT: cleanups to quantile
* `#16275 <https://github.com/numpy/numpy/pull/16275>`__: REL: Update master after 1.19.x branch.
* `#16276 <https://github.com/numpy/numpy/pull/16276>`__: BUG: Ensure out argument is returned by identity for 0d arrays
* `#16278 <https://github.com/numpy/numpy/pull/16278>`__: DOC: Clarifications for ``np.var``.
* `#16283 <https://github.com/numpy/numpy/pull/16283>`__: DOC: Add a note about performance of isclose compared to math.isclose
* `#16284 <https://github.com/numpy/numpy/pull/16284>`__: MAINT: Clean up the implementation of quantile
* `#16285 <https://github.com/numpy/numpy/pull/16285>`__: MAINT: Bump hypothesis from 5.12.0 to 5.14.0
* `#16288 <https://github.com/numpy/numpy/pull/16288>`__: BLD: Avoid "visibility attribute not supported" warning
* `#16291 <https://github.com/numpy/numpy/pull/16291>`__: DOC: Improve "tobytes" docstring.
* `#16292 <https://github.com/numpy/numpy/pull/16292>`__: BUG: Fix tools/download-wheels.py.
* `#16295 <https://github.com/numpy/numpy/pull/16295>`__: BUG: Require Python >= 3.6 in setup.py
* `#16296 <https://github.com/numpy/numpy/pull/16296>`__: DOC: Fix malformed docstrings in ma.
* `#16297 <https://github.com/numpy/numpy/pull/16297>`__: ENH: Optimize Cpu feature detect in X86, fix for GCC on macOS
* `#16298 <https://github.com/numpy/numpy/pull/16298>`__: BUG: np.info does not show keyword-only arguments
* `#16300 <https://github.com/numpy/numpy/pull/16300>`__: DOC: Fix bad reference in ``numpy.ma``
* `#16304 <https://github.com/numpy/numpy/pull/16304>`__: TST, MAINT: Fix detecting and testing armhf features
* `#16305 <https://github.com/numpy/numpy/pull/16305>`__: DOC: Fix packbits documentation rendering,
* `#16306 <https://github.com/numpy/numpy/pull/16306>`__: DOC: Fix troubleshooting code snippet when env vars are empty
* `#16308 <https://github.com/numpy/numpy/pull/16308>`__: BUG: relpath fails for different drives on windows
* `#16311 <https://github.com/numpy/numpy/pull/16311>`__: DOC: Fix ``np.ma.core.doc_note``
* `#16316 <https://github.com/numpy/numpy/pull/16316>`__: MAINT: Bump numpydoc version
* `#16318 <https://github.com/numpy/numpy/pull/16318>`__: MAINT: Stop Using PyEval_Call* and simplify some uses
* `#16321 <https://github.com/numpy/numpy/pull/16321>`__: ENH: Improve the ARM cpu feature detection by parsing /proc/cpuinfo
* `#16323 <https://github.com/numpy/numpy/pull/16323>`__: DOC: Reconstruct Testing Guideline.
* `#16327 <https://github.com/numpy/numpy/pull/16327>`__: BUG: Don't segfault on bad __len__ when assigning.
* `#16329 <https://github.com/numpy/numpy/pull/16329>`__: MAINT: Cleanup 'tools/download-wheels.py'
* `#16332 <https://github.com/numpy/numpy/pull/16332>`__: DOC: link np.interp to SciPy's interpolation functions (closes...
* `#16333 <https://github.com/numpy/numpy/pull/16333>`__: DOC: Fix spelling typo - homogenous to homogeneous. (#16324)
* `#16334 <https://github.com/numpy/numpy/pull/16334>`__: ENH: Use AVX-512 for np.isnan, np.infinite, np.isinf and np.signbit
* `#16336 <https://github.com/numpy/numpy/pull/16336>`__: BUG: Fix refcounting in add_newdoc
* `#16337 <https://github.com/numpy/numpy/pull/16337>`__: CI: Create a link for the circleCI artifact
* `#16346 <https://github.com/numpy/numpy/pull/16346>`__: MAINT: Remove f-strings in setup.py.
* `#16348 <https://github.com/numpy/numpy/pull/16348>`__: BUG: Fix dtype leak in ``PyArray_FromAny`` error path
* `#16349 <https://github.com/numpy/numpy/pull/16349>`__: BUG: Indentation for docstrings
* `#16350 <https://github.com/numpy/numpy/pull/16350>`__: BUG: Set readonly flag in array interface
* `#16351 <https://github.com/numpy/numpy/pull/16351>`__: BUG: Fix small leaks in error path and ``empty_like`` with shape
* `#16362 <https://github.com/numpy/numpy/pull/16362>`__: MAINT: Streamline download-wheels.
* `#16365 <https://github.com/numpy/numpy/pull/16365>`__: DOC: Fix an obvious mistake in a message printed in doc/Makefile.
* `#16367 <https://github.com/numpy/numpy/pull/16367>`__: MAINT: Bump cython from 0.29.17 to 0.29.19
* `#16368 <https://github.com/numpy/numpy/pull/16368>`__: MAINT: Bump hypothesis from 5.14.0 to 5.15.1
* `#16369 <https://github.com/numpy/numpy/pull/16369>`__: MAINT: Bump pytest-cov from 2.8.1 to 2.9.0
* `#16371 <https://github.com/numpy/numpy/pull/16371>`__: ENH: Use AVX-512 for np.frexp and np.ldexp
* `#16373 <https://github.com/numpy/numpy/pull/16373>`__: MAINT, DOC: add index for user docs.
* `#16375 <https://github.com/numpy/numpy/pull/16375>`__: ENH: ARM Neon implementation with intrinsic for np.argmax.
* `#16385 <https://github.com/numpy/numpy/pull/16385>`__: DOC: Tighten howto-docs guide #16259
* `#16387 <https://github.com/numpy/numpy/pull/16387>`__: MAINT: Make ctypes optional on Windows
* `#16389 <https://github.com/numpy/numpy/pull/16389>`__: ENH: Hardcode buffer handling for simple scalars
* `#16392 <https://github.com/numpy/numpy/pull/16392>`__: MAINT: Stop uploading wheels to Rackspace.
* `#16393 <https://github.com/numpy/numpy/pull/16393>`__: MAINT: Use a raw string for the fromstring docstring.
* `#16395 <https://github.com/numpy/numpy/pull/16395>`__: ENH: Validate and disable CPU features in runtime
* `#16397 <https://github.com/numpy/numpy/pull/16397>`__: ENH: Implement the NumPy C SIMD vectorization interface
* `#16404 <https://github.com/numpy/numpy/pull/16404>`__: DOC,BLD: Update make dist html target.
* `#16408 <https://github.com/numpy/numpy/pull/16408>`__: DOC,BLD: Update sphinx conf to use xelatex.
* `#16409 <https://github.com/numpy/numpy/pull/16409>`__: TST, CI: turn on codecov patch diffs
* `#16411 <https://github.com/numpy/numpy/pull/16411>`__: BUG: endpoints of array returned by geomspace() should match...
* `#16417 <https://github.com/numpy/numpy/pull/16417>`__: MAINT: support python 3.10
* `#16418 <https://github.com/numpy/numpy/pull/16418>`__: MAINT: Chain some exceptions.
* `#16420 <https://github.com/numpy/numpy/pull/16420>`__: DOC: Improve intersect1d docstring
* `#16422 <https://github.com/numpy/numpy/pull/16422>`__: DOC: Update assert_warns parameter list
* `#16423 <https://github.com/numpy/numpy/pull/16423>`__: TST: Simplify assert_warns in test_io.py
* `#16427 <https://github.com/numpy/numpy/pull/16427>`__: DOC: make NEP 18 status Final
* `#16428 <https://github.com/numpy/numpy/pull/16428>`__: DOC: Add style guide to howto_document
* `#16430 <https://github.com/numpy/numpy/pull/16430>`__: DOC: NEP for C style guide
* `#16433 <https://github.com/numpy/numpy/pull/16433>`__: DOC: Fix description of dtype default in linspace
* `#16435 <https://github.com/numpy/numpy/pull/16435>`__: BUG: Add extern to PyArrayDTypeMeta_Type declaration
* `#16436 <https://github.com/numpy/numpy/pull/16436>`__: DOC: Add a reference into NEP 29,
* `#16438 <https://github.com/numpy/numpy/pull/16438>`__: MAINT: Catch remaining cases of Py_SIZE and Py_TYPE as lvalues
* `#16442 <https://github.com/numpy/numpy/pull/16442>`__: ENH: Fix deprecated warn for Intel/Apple/Clang Compiler
* `#16444 <https://github.com/numpy/numpy/pull/16444>`__: DOC: make clearer that sinc is normalized by a factor pi
* `#16445 <https://github.com/numpy/numpy/pull/16445>`__: DOC: update roadmap
* `#16446 <https://github.com/numpy/numpy/pull/16446>`__: BUG: fixes einsum output order with optimization (#14615)
* `#16447 <https://github.com/numpy/numpy/pull/16447>`__: DOC: add a "make show" command to doc/Makefile
* `#16450 <https://github.com/numpy/numpy/pull/16450>`__: DOC: Add a NEP link to all neps.
* `#16452 <https://github.com/numpy/numpy/pull/16452>`__: DOC,ENH: extend error message when Accelerate is detected
* `#16454 <https://github.com/numpy/numpy/pull/16454>`__: TST: Add tests for PyArray_IntpConverter
* `#16463 <https://github.com/numpy/numpy/pull/16463>`__: DOC: Improve assert_warns docstring with example
* `#16464 <https://github.com/numpy/numpy/pull/16464>`__: MAINT: Bump hypothesis from 5.15.1 to 5.16.0
* `#16465 <https://github.com/numpy/numpy/pull/16465>`__: DOC: Fix development_workflow links
* `#16468 <https://github.com/numpy/numpy/pull/16468>`__: BUG: fix GCC 10 major version comparison
* `#16471 <https://github.com/numpy/numpy/pull/16471>`__: BLD: install mingw32 v7.3.0 for win32
* `#16472 <https://github.com/numpy/numpy/pull/16472>`__: DOC: Fixes for 18 broken links
* `#16474 <https://github.com/numpy/numpy/pull/16474>`__: MAINT: use zip instead of range in piecewise
* `#16476 <https://github.com/numpy/numpy/pull/16476>`__: ENH: add ``norm=forward,backward`` to numpy.fft functions
* `#16482 <https://github.com/numpy/numpy/pull/16482>`__: SIMD: Optimize the performace of np.packbits in ARM-based machine.
* `#16485 <https://github.com/numpy/numpy/pull/16485>`__: BUG: Fix result when a gufunc output broadcasts the inputs.
* `#16500 <https://github.com/numpy/numpy/pull/16500>`__: DOC: Point Contributing page to new NEP 45
* `#16501 <https://github.com/numpy/numpy/pull/16501>`__: MAINT: make Py_SET_SIZE and Py_SET_TYPE macros a bit safer
* `#16503 <https://github.com/numpy/numpy/pull/16503>`__: BUG:random: Error when ``size`` is smaller than broadcast input...
* `#16504 <https://github.com/numpy/numpy/pull/16504>`__: DOC: Correct MV Normal sig
* `#16505 <https://github.com/numpy/numpy/pull/16505>`__: BUG: raise IEEE exception on AIX
* `#16506 <https://github.com/numpy/numpy/pull/16506>`__: DOC: only single-polynomial fitting in np.polynomial.Polynomial.fit()
* `#16510 <https://github.com/numpy/numpy/pull/16510>`__: DOC: Minor rounding correction in Generator.binomial
* `#16514 <https://github.com/numpy/numpy/pull/16514>`__: STY: trivial doc style fix in NEP 45.
* `#16515 <https://github.com/numpy/numpy/pull/16515>`__: ENH: add type stubs from numpy-stubs
* `#16519 <https://github.com/numpy/numpy/pull/16519>`__: BUG: f2py: make callbacks threadsafe
* `#16520 <https://github.com/numpy/numpy/pull/16520>`__: STY: f2py: replace \t by whitespace for readability
* `#16522 <https://github.com/numpy/numpy/pull/16522>`__: MAINT:ARMHF Fix detecting feature groups NEON_HALF and NEON_VFPV4
* `#16523 <https://github.com/numpy/numpy/pull/16523>`__: MAINT: Improve buffer speed
* `#16524 <https://github.com/numpy/numpy/pull/16524>`__: MAINT: f2py: move thread-local declaration definition to common...
* `#16529 <https://github.com/numpy/numpy/pull/16529>`__: BUG: Fix cython warning in random/_common.pyx.
* `#16530 <https://github.com/numpy/numpy/pull/16530>`__: MAINT: Bump pytest from 5.4.2 to 5.4.3
* `#16532 <https://github.com/numpy/numpy/pull/16532>`__: BUG: Remove non-threadsafe sigint handling from fft calculation
* `#16540 <https://github.com/numpy/numpy/pull/16540>`__: SIMD: SSE2 intrinsic implementation for float64 input of np.enisum
* `#16551 <https://github.com/numpy/numpy/pull/16551>`__: BUG: Ensure SeedSequence 0-padding does not collide with spawn...
* `#16554 <https://github.com/numpy/numpy/pull/16554>`__: DEP: Remove deprecated numeric types and deprecate remaining
* `#16555 <https://github.com/numpy/numpy/pull/16555>`__: CI: drop win32 3.7, 3.6 builds
* `#16556 <https://github.com/numpy/numpy/pull/16556>`__: MAINT: simplifying annotations for np.core.from_numeric
* `#16558 <https://github.com/numpy/numpy/pull/16558>`__: ENH: make typing module available at runtime
* `#16570 <https://github.com/numpy/numpy/pull/16570>`__: ENH: Throw TypeError on operator concat on Numpy Arrays
* `#16571 <https://github.com/numpy/numpy/pull/16571>`__: TST: Add new tests for array coercion
* `#16572 <https://github.com/numpy/numpy/pull/16572>`__: BUG: fix sin/cos bug when input is strided array
* `#16574 <https://github.com/numpy/numpy/pull/16574>`__: MAINT: fix name of first parameter to dtype constructor in type...
* `#16581 <https://github.com/numpy/numpy/pull/16581>`__: DOC: Added an example for np.transpose(4d_array)
* `#16583 <https://github.com/numpy/numpy/pull/16583>`__: MAINT: changed np.generic arguments to positional-only
* `#16589 <https://github.com/numpy/numpy/pull/16589>`__: MAINT: Remove nickname from polynomial classes.
* `#16590 <https://github.com/numpy/numpy/pull/16590>`__: DOC: Clarify dtype default for logspace and geomspace
* `#16591 <https://github.com/numpy/numpy/pull/16591>`__: DOC: Disallow complex args in arange
* `#16592 <https://github.com/numpy/numpy/pull/16592>`__: BUG: Raise TypeError for float->timedelta promotion
* `#16594 <https://github.com/numpy/numpy/pull/16594>`__: ENH: Add ``__f2py_numpy_version__`` attribute to Fortran modules.
* `#16596 <https://github.com/numpy/numpy/pull/16596>`__: BUG: Fix reference count leak in mapping.c
* `#16601 <https://github.com/numpy/numpy/pull/16601>`__: MAINT: Move and improve ``test_ignore_nan_ulperror``.
* `#16603 <https://github.com/numpy/numpy/pull/16603>`__: DOC: make addition of types a "new feature" in release notes
* `#16605 <https://github.com/numpy/numpy/pull/16605>`__: MAINT: Avx512 intrinsics implementation for float64 input np.log
* `#16606 <https://github.com/numpy/numpy/pull/16606>`__: MAINT: Bump pytest-cov from 2.9.0 to 2.10.0
* `#16607 <https://github.com/numpy/numpy/pull/16607>`__: MAINT: Bump hypothesis from 5.16.0 to 5.16.1
* `#16613 <https://github.com/numpy/numpy/pull/16613>`__: MAINT: bump mypy version to 0.780
* `#16617 <https://github.com/numpy/numpy/pull/16617>`__: BLD: Openblas 0.3.10
* `#16618 <https://github.com/numpy/numpy/pull/16618>`__: ENH: add annotation for abs
* `#16619 <https://github.com/numpy/numpy/pull/16619>`__: BLD: check if std=c99 is really required
* `#16620 <https://github.com/numpy/numpy/pull/16620>`__: MAINT, CI: disable Shippable cache
* `#16621 <https://github.com/numpy/numpy/pull/16621>`__: BENCH: Expand array-creation benchmarks
* `#16622 <https://github.com/numpy/numpy/pull/16622>`__: MAINT: Implemented two dtype-related TODO's
* `#16623 <https://github.com/numpy/numpy/pull/16623>`__: BUG: Initialize stop-reading in array_from_text
* `#16627 <https://github.com/numpy/numpy/pull/16627>`__: DOC: Updated documentation for numpy.squeeze
* `#16629 <https://github.com/numpy/numpy/pull/16629>`__: ENH: add tool to find functions missing types
* `#16630 <https://github.com/numpy/numpy/pull/16630>`__: ENH,BUG:distutils Remove the origins from the implied features
* `#16633 <https://github.com/numpy/numpy/pull/16633>`__: MAINT: lib: Some code clean up in loadtxt
* `#16635 <https://github.com/numpy/numpy/pull/16635>`__: BENCH: remove obsolete goal_time param
* `#16639 <https://github.com/numpy/numpy/pull/16639>`__: BUG: Fix uint->timedelta promotion to raise TypeError
* `#16642 <https://github.com/numpy/numpy/pull/16642>`__: MAINT: Replace ``PyUString_GET_SIZE`` with ``PyUnicode_GetLength``.
* `#16643 <https://github.com/numpy/numpy/pull/16643>`__: REL: Fix outdated docs link
* `#16644 <https://github.com/numpy/numpy/pull/16644>`__: MAINT: Improve performance of np.full
* `#16646 <https://github.com/numpy/numpy/pull/16646>`__: TST: add a static typing test for memoryviews as ArrayLikes
* `#16647 <https://github.com/numpy/numpy/pull/16647>`__: ENH: Added annotations to 8 functions from np.core.fromnumeric
* `#16648 <https://github.com/numpy/numpy/pull/16648>`__: REL: Update master after 1.19.0 release.
* `#16650 <https://github.com/numpy/numpy/pull/16650>`__: ENH: Allow genfromtxt to unpack structured arrays
* `#16651 <https://github.com/numpy/numpy/pull/16651>`__: MAINT: Prefer generator expressions over list comprehensions...
* `#16653 <https://github.com/numpy/numpy/pull/16653>`__: DOC: cross-reference numpy.dot and numpy.linalg.multi_dot
* `#16658 <https://github.com/numpy/numpy/pull/16658>`__: MAINT: Bump hypothesis from 5.16.1 to 5.16.3
* `#16659 <https://github.com/numpy/numpy/pull/16659>`__: MAINT: Bump mypy from 0.780 to 0.781
* `#16664 <https://github.com/numpy/numpy/pull/16664>`__: DOC: Add lib.format.open_memmap to autosummary.
* `#16666 <https://github.com/numpy/numpy/pull/16666>`__: BUG: Fix bug in AVX complex absolute while processing array of...
* `#16669 <https://github.com/numpy/numpy/pull/16669>`__: MAINT: remove blacklist/whitelist terms
* `#16671 <https://github.com/numpy/numpy/pull/16671>`__: DOC: Simplify and update git setup page
* `#16674 <https://github.com/numpy/numpy/pull/16674>`__: TST: Add extra debugging information to CPU features detection
* `#16675 <https://github.com/numpy/numpy/pull/16675>`__: ENH: Add support for file like objects to np.core.records.fromfile
* `#16683 <https://github.com/numpy/numpy/pull/16683>`__: DOC: updated gcc minimum recommend version to build from source
* `#16684 <https://github.com/numpy/numpy/pull/16684>`__: MAINT: Allow None to be passed to certain generic subclasses
* `#16690 <https://github.com/numpy/numpy/pull/16690>`__: DOC: fixed docstring for descr_to_dtype
* `#16691 <https://github.com/numpy/numpy/pull/16691>`__: DOC: Remove "matrix" from ``triu`` docstring.
* `#16696 <https://github.com/numpy/numpy/pull/16696>`__: MAINT: add py.typed sentinel to package manifest
* `#16699 <https://github.com/numpy/numpy/pull/16699>`__: MAINT: Fixup quantile tests to not use ``np.float``
* `#16702 <https://github.com/numpy/numpy/pull/16702>`__: BLD: Add CPU entry for Emscripten / WebAssembly
* `#16704 <https://github.com/numpy/numpy/pull/16704>`__: TST: Disable Python 3.9-dev testing.
* `#16706 <https://github.com/numpy/numpy/pull/16706>`__: DOC: Add instruction about stable symlink
* `#16708 <https://github.com/numpy/numpy/pull/16708>`__: MAINT: Disable use_hugepages in case of ValueError
* `#16709 <https://github.com/numpy/numpy/pull/16709>`__: DOC: Add dep directive to alen docstring.
* `#16710 <https://github.com/numpy/numpy/pull/16710>`__: ENH, BLD: Add RPATH support for AIX
* `#16718 <https://github.com/numpy/numpy/pull/16718>`__: DOC: fix typo
* `#16720 <https://github.com/numpy/numpy/pull/16720>`__: BUG: Fix PyArray_SearchSorted signature.
* `#16723 <https://github.com/numpy/numpy/pull/16723>`__: NEP: Initial draft for NEP 43 for extensible ufuncs
* `#16729 <https://github.com/numpy/numpy/pull/16729>`__: ENH: Add annotations to the last 8 functions in numpy.core.fromnumeric
* `#16730 <https://github.com/numpy/numpy/pull/16730>`__: ENH: Use f90 compiler specified in f2py command line args for...
* `#16731 <https://github.com/numpy/numpy/pull/16731>`__: DOC: reword random c-api introduction, cython is documented in...
* `#16735 <https://github.com/numpy/numpy/pull/16735>`__: DOC: Tweak a sentence about broadcasting.
* `#16736 <https://github.com/numpy/numpy/pull/16736>`__: DOC: Prepend ``ma.`` to references in ``numpy.ma``
* `#16738 <https://github.com/numpy/numpy/pull/16738>`__: DOC: Remove redundant word
* `#16742 <https://github.com/numpy/numpy/pull/16742>`__: DOC: add unique() to See Also of repeat()
* `#16743 <https://github.com/numpy/numpy/pull/16743>`__: DOC: add example to unique() and make connection to repeat()
* `#16747 <https://github.com/numpy/numpy/pull/16747>`__: MAINT: Chaining exceptions in numpy/core/_internal.py
* `#16752 <https://github.com/numpy/numpy/pull/16752>`__: BLD: add manylinux1 OpenBlAS 0.3.10 hashes and test for them
* `#16757 <https://github.com/numpy/numpy/pull/16757>`__: DOC: Add Matti Picus to steering council page
* `#16759 <https://github.com/numpy/numpy/pull/16759>`__: ENH: make dtype generic over scalar type
* `#16760 <https://github.com/numpy/numpy/pull/16760>`__: DOC: Added a section in the 'Iterating over arrays' doc page...
* `#16761 <https://github.com/numpy/numpy/pull/16761>`__: MAINT: Tidy exception chaining in _datasource.py
* `#16762 <https://github.com/numpy/numpy/pull/16762>`__: MAINT: Fixes for deprecated functions in scalartypes.c.src
* `#16764 <https://github.com/numpy/numpy/pull/16764>`__: MAINT: Bump mypy from 0.781 to 0.782
* `#16765 <https://github.com/numpy/numpy/pull/16765>`__: MAINT: Bump hypothesis from 5.16.3 to 5.19.0
* `#16767 <https://github.com/numpy/numpy/pull/16767>`__: ENH: Update NumPy logos
* `#16770 <https://github.com/numpy/numpy/pull/16770>`__: MAINT: Remove unneeded call to PyUnicode_READY
* `#16771 <https://github.com/numpy/numpy/pull/16771>`__: MAINT: Fix deprecated functions in scalarapi.c
* `#16775 <https://github.com/numpy/numpy/pull/16775>`__: DOC: switch to logo with text
* `#16777 <https://github.com/numpy/numpy/pull/16777>`__: BUG: Added missing return after raising error in methods.c
* `#16778 <https://github.com/numpy/numpy/pull/16778>`__: NEP: Update NEP 42 to note the issue of circular references
* `#16782 <https://github.com/numpy/numpy/pull/16782>`__: ENH, TST: Bring the NumPy C SIMD vectorization interface "NPYV"...
* `#16786 <https://github.com/numpy/numpy/pull/16786>`__: BENCH: Add basic benchmarks for scalar indexing and assignment
* `#16789 <https://github.com/numpy/numpy/pull/16789>`__: BUG: fix decode error when building and get rid of warn
* `#16792 <https://github.com/numpy/numpy/pull/16792>`__: DOC: Minor RST formatting.
* `#16793 <https://github.com/numpy/numpy/pull/16793>`__: BLD, MAINT: update cython to 0.29.21
* `#16794 <https://github.com/numpy/numpy/pull/16794>`__: TST: Upgrade to Python 3.8 for DEBUG testing.
* `#16798 <https://github.com/numpy/numpy/pull/16798>`__: DOC: Fix RST/numpydoc standard.
* `#16800 <https://github.com/numpy/numpy/pull/16800>`__: MAINT: Move typing tests
* `#16802 <https://github.com/numpy/numpy/pull/16802>`__: MAINT: Explicitly disallow object user dtypes
* `#16805 <https://github.com/numpy/numpy/pull/16805>`__: DOC: add example to corrcoef function
* `#16806 <https://github.com/numpy/numpy/pull/16806>`__: DOC: adding docs on passing dimensions as tuple to ndindex
* `#16807 <https://github.com/numpy/numpy/pull/16807>`__: BUG, MAINT: Remove overzealous automatic RST link
* `#16811 <https://github.com/numpy/numpy/pull/16811>`__: DOC: Add explanation of 'K' and 'A' layout options to 'asarray*'...
* `#16814 <https://github.com/numpy/numpy/pull/16814>`__: DOC: Add a reST label to /user/building.rst
* `#16815 <https://github.com/numpy/numpy/pull/16815>`__: BUG: fix mgrid output for lower precision float inputs
* `#16816 <https://github.com/numpy/numpy/pull/16816>`__: BLD: temporarily disable OpenBLAS hash checks
* `#16817 <https://github.com/numpy/numpy/pull/16817>`__: BUG: Do not inherit flags from the structured part of a union...
* `#16819 <https://github.com/numpy/numpy/pull/16819>`__: DOC: replace dec.slow with pytest.mark.slow
* `#16820 <https://github.com/numpy/numpy/pull/16820>`__: MAINT: Make void scalar to array creation copy when dtype is...
* `#16821 <https://github.com/numpy/numpy/pull/16821>`__: DOC: fix inconsistent parameter name in np.ndindex docstring
* `#16822 <https://github.com/numpy/numpy/pull/16822>`__: MAINT: setuptools 49.2.0 emits a warning, avoid it
* `#16824 <https://github.com/numpy/numpy/pull/16824>`__: DOC: add examples to random number generator pages
* `#16826 <https://github.com/numpy/numpy/pull/16826>`__: DOC: describe ufunc copy behavior when input and output overlap
* `#16827 <https://github.com/numpy/numpy/pull/16827>`__: MAINT: Fix ``runtest.py`` warning.
* `#16829 <https://github.com/numpy/numpy/pull/16829>`__: DOC,BLD: Add pandas to doc_requirements.txt
* `#16831 <https://github.com/numpy/numpy/pull/16831>`__: MAINT: fix sphinx deprecation
* `#16834 <https://github.com/numpy/numpy/pull/16834>`__: Avoid using uninitialized bytes in getlimits.py.
* `#16835 <https://github.com/numpy/numpy/pull/16835>`__: DOC: Explaining why datetime64 doesn't work for allclose + isclose
* `#16836 <https://github.com/numpy/numpy/pull/16836>`__: DOC: improve SIMD features tables
* `#16837 <https://github.com/numpy/numpy/pull/16837>`__: BLD: update openblas hashes, re-enable check
* `#16838 <https://github.com/numpy/numpy/pull/16838>`__: MAINT: Remove code that will never run
* `#16840 <https://github.com/numpy/numpy/pull/16840>`__: MAINT: Bump hypothesis from 5.19.0 to 5.19.1
* `#16841 <https://github.com/numpy/numpy/pull/16841>`__: BUG: linspace should round towards -infinity
* `#16845 <https://github.com/numpy/numpy/pull/16845>`__: TST: Disable shippable until we can fix it.
* `#16847 <https://github.com/numpy/numpy/pull/16847>`__: MAINT: Remove Duplicated Code (function extract rmap)
* `#16848 <https://github.com/numpy/numpy/pull/16848>`__: MAINT: Remove Duplicated Code
* `#16849 <https://github.com/numpy/numpy/pull/16849>`__: MAINT: Change for loop (range -> for each)
* `#16850 <https://github.com/numpy/numpy/pull/16850>`__: DEP: Deprecate NumPy object scalars
* `#16854 <https://github.com/numpy/numpy/pull/16854>`__: DOC: clarify whats required for new features see #13924
* `#16857 <https://github.com/numpy/numpy/pull/16857>`__: MAINT: fix new compiler warnings on clang
* `#16858 <https://github.com/numpy/numpy/pull/16858>`__: BUG: fix the search dir of dispatch-able sources
* `#16860 <https://github.com/numpy/numpy/pull/16860>`__: MAINT: Remove deprecated python function 'file()'
* `#16868 <https://github.com/numpy/numpy/pull/16868>`__: BUG: Validate output size in bin- and multinomial
* `#16870 <https://github.com/numpy/numpy/pull/16870>`__: BLD, MAINT: Pin setuptools
* `#16871 <https://github.com/numpy/numpy/pull/16871>`__: BUG: Update compiler check for AVX-512F
* `#16874 <https://github.com/numpy/numpy/pull/16874>`__: TST, MAINT: fix the test for ``np.ones``
* `#16878 <https://github.com/numpy/numpy/pull/16878>`__: DOC: edit to the documentation of lib/polynomial.py/polyfit
* `#16879 <https://github.com/numpy/numpy/pull/16879>`__: MAINT: Configure hypothesis in ``np.test()`` for determinism,...
* `#16882 <https://github.com/numpy/numpy/pull/16882>`__: BLD: Remove unused pip install
* `#16883 <https://github.com/numpy/numpy/pull/16883>`__: BUG,DOC: Fix bad MPL kwarg in docs
* `#16886 <https://github.com/numpy/numpy/pull/16886>`__: DOC: Fix types including curly braces
* `#16887 <https://github.com/numpy/numpy/pull/16887>`__: DOC: Remove the links for ``True`` and ``False``
* `#16888 <https://github.com/numpy/numpy/pull/16888>`__: ENH: Integrate the new CPU dispatcher with umath generator
* `#16890 <https://github.com/numpy/numpy/pull/16890>`__: TST, BUG: Re-raise MemoryError exception in test_large_zip's...
* `#16894 <https://github.com/numpy/numpy/pull/16894>`__: DOC: Fix wrong markups in ``arrays.dtypes``
* `#16896 <https://github.com/numpy/numpy/pull/16896>`__: DOC: Remove links for C codes
* `#16897 <https://github.com/numpy/numpy/pull/16897>`__: DOC: Fix the declarations of C fuctions
* `#16899 <https://github.com/numpy/numpy/pull/16899>`__: MNT: also use Py_SET_REFCNT instead of Py_REFCNT
* `#16900 <https://github.com/numpy/numpy/pull/16900>`__: MAINT: Chaining exceptions in numpy/__init__.py
* `#16907 <https://github.com/numpy/numpy/pull/16907>`__: DOC: update val to be scalar or array like optional closes #16901
* `#16910 <https://github.com/numpy/numpy/pull/16910>`__: MAINT: Bump hypothesis from 5.19.1 to 5.20.2
* `#16911 <https://github.com/numpy/numpy/pull/16911>`__: ENH: Speed up trim_zeros
* `#16914 <https://github.com/numpy/numpy/pull/16914>`__: BUG: Fix string/bytes to complex assignment
* `#16917 <https://github.com/numpy/numpy/pull/16917>`__: DOC: Add correctness vs strictness consideration for np.dtype
* `#16919 <https://github.com/numpy/numpy/pull/16919>`__: DOC: Add ufunc docstring to generated docs.
* `#16925 <https://github.com/numpy/numpy/pull/16925>`__: REL: Update master after 1.19.1 release.
* `#16931 <https://github.com/numpy/numpy/pull/16931>`__: Revert "Merge pull request #16248 from alexrockhill/edge"
* `#16935 <https://github.com/numpy/numpy/pull/16935>`__: ENH: implement NEP-35's ``like=`` argument
* `#16936 <https://github.com/numpy/numpy/pull/16936>`__: BUG: Fix memory leak of buffer-info cache due to relaxed strides
* `#16938 <https://github.com/numpy/numpy/pull/16938>`__: ENH,API: Store exported buffer info on the array
* `#16940 <https://github.com/numpy/numpy/pull/16940>`__: BLD: update OpenBLAS build
* `#16941 <https://github.com/numpy/numpy/pull/16941>`__: BUG: Allow array-like types to be coerced as object array elements
* `#16943 <https://github.com/numpy/numpy/pull/16943>`__: DEP: Deprecate size-one ragged array coercion
* `#16944 <https://github.com/numpy/numpy/pull/16944>`__: Change the name of the folder "icons" to "logo".
* `#16949 <https://github.com/numpy/numpy/pull/16949>`__: ENH: enable colors for ``runtests.py --ipython``
* `#16950 <https://github.com/numpy/numpy/pull/16950>`__: DOC: Clarify input to irfft/irfft2/irfftn
* `#16952 <https://github.com/numpy/numpy/pull/16952>`__: MAINT: Bump hypothesis from 5.20.2 to 5.23.2
* `#16953 <https://github.com/numpy/numpy/pull/16953>`__: update numpy/lib/arraypad.py with appropriate chain exception
* `#16957 <https://github.com/numpy/numpy/pull/16957>`__: MAINT: Use arm64 instead of aarch64 on travisCI.
* `#16962 <https://github.com/numpy/numpy/pull/16962>`__: MAINT: Chain exception in ``distutils/fcompiler/environment.py``.
* `#16966 <https://github.com/numpy/numpy/pull/16966>`__: MAINT: Added the ``order`` parameter to ``np.array()``
* `#16969 <https://github.com/numpy/numpy/pull/16969>`__: ENH: Add Neon SIMD implementations for add, sub, mul, and div
* `#16973 <https://github.com/numpy/numpy/pull/16973>`__: DOC: Fixed typo in lib/recfunctions.py
* `#16974 <https://github.com/numpy/numpy/pull/16974>`__: TST: Add pypy win32 CI testing.
* `#16982 <https://github.com/numpy/numpy/pull/16982>`__: ENH: Increase the use of ``Literal`` types
* `#16986 <https://github.com/numpy/numpy/pull/16986>`__: ENH: Add NumPy declarations to be used by Cython 3.0+
* `#16988 <https://github.com/numpy/numpy/pull/16988>`__: DOC: Add the new NumPy logo to Sphinx pages
* `#16991 <https://github.com/numpy/numpy/pull/16991>`__: MAINT: Bump hypothesis from 5.23.2 to 5.23.9
* `#16992 <https://github.com/numpy/numpy/pull/16992>`__: MAINT: Bump pytest from 5.4.3 to 6.0.1
* `#16993 <https://github.com/numpy/numpy/pull/16993>`__: BLD: pin setuptools < 49.2.0
* `#16996 <https://github.com/numpy/numpy/pull/16996>`__: DOC: Revise glossary page
* `#17002 <https://github.com/numpy/numpy/pull/17002>`__: DOC: clip() allows arguments.
* `#17009 <https://github.com/numpy/numpy/pull/17009>`__: NEP: Updated NEP-35 with keyword-only instruction
* `#17010 <https://github.com/numpy/numpy/pull/17010>`__: BUG: Raise correct errors in boolean indexing fast path
* `#17013 <https://github.com/numpy/numpy/pull/17013>`__: MAINT: Simplify scalar power
* `#17014 <https://github.com/numpy/numpy/pull/17014>`__: MAINT: Improve error handling in umathmodule setup
* `#17022 <https://github.com/numpy/numpy/pull/17022>`__: DOC: Fix non-matching pronoun.
* `#17028 <https://github.com/numpy/numpy/pull/17028>`__: DOC: Disclaimer for FFT library
* `#17029 <https://github.com/numpy/numpy/pull/17029>`__: MAINT: Add error return to all casting functionality and NpyIter
* `#17033 <https://github.com/numpy/numpy/pull/17033>`__: BUG: fix a compile and a test warning
* `#17036 <https://github.com/numpy/numpy/pull/17036>`__: DOC: Clarify that ``np.char`` comparison functions always return...
* `#17039 <https://github.com/numpy/numpy/pull/17039>`__: DOC: Use a less ambiguous example for array_split
* `#17041 <https://github.com/numpy/numpy/pull/17041>`__: MAINT: Bump hypothesis from 5.23.9 to 5.23.12
* `#17048 <https://github.com/numpy/numpy/pull/17048>`__: STY: core._internal style fixups
* `#17050 <https://github.com/numpy/numpy/pull/17050>`__: MAINT: Remove _EXTRAFLAGS variable
* `#17050 <https://github.com/numpy/numpy/pull/17051>`__: MAINT: change ``for line in open()`` to ``with open() as f``
* `#17052 <https://github.com/numpy/numpy/pull/17052>`__: MAINT: Delete obsolete conversion to list
* `#17053 <https://github.com/numpy/numpy/pull/17053>`__: BUG: fix typo in polydiv that prevented promotion to poly1d
* `#17055 <https://github.com/numpy/numpy/pull/17055>`__: MAINT: Replace lambda function by list comprehension
* `#17058 <https://github.com/numpy/numpy/pull/17058>`__: MAINT: Revert boolean casting back to elementwise comparisons...
* `#17059 <https://github.com/numpy/numpy/pull/17059>`__: BUG: fix pickling of arrays larger than 2GiB
* `#17062 <https://github.com/numpy/numpy/pull/17062>`__: API, BUG: Raise error on complex input to i0
* `#17063 <https://github.com/numpy/numpy/pull/17063>`__: MAINT: Remove obsolete conversion to set
* `#17067 <https://github.com/numpy/numpy/pull/17067>`__: DEP: lib: Remove the deprecated financial functions.
* `#17068 <https://github.com/numpy/numpy/pull/17068>`__: MAINT, BUG: Remove uses of PyString_FromString.
* `#17074 <https://github.com/numpy/numpy/pull/17074>`__: DOC: use the pydata_sphinx_theme
* `#17078 <https://github.com/numpy/numpy/pull/17078>`__: DOC: Fixes duplication of toctree content (Closes #17077)
* `#17091 <https://github.com/numpy/numpy/pull/17091>`__: MAINT: Bump pytest-cov from 2.10.0 to 2.10.1
* `#17092 <https://github.com/numpy/numpy/pull/17092>`__: MAINT: Bump hypothesis from 5.23.12 to 5.26.0
* `#17093 <https://github.com/numpy/numpy/pull/17093>`__: NEP: Adjust NEP-35 to make it more user-accessible
* `#17104 <https://github.com/numpy/numpy/pull/17104>`__: ENH: Add placeholder stubs for all sub-modules
* `#17109 <https://github.com/numpy/numpy/pull/17109>`__: MAINT: Split einsum into multiple files
* `#17112 <https://github.com/numpy/numpy/pull/17112>`__: BUG: Handle errors from the PyCapsule API
* `#17115 <https://github.com/numpy/numpy/pull/17115>`__: DOC: Fix spacing in vectorize doc
* `#17116 <https://github.com/numpy/numpy/pull/17116>`__: API: Remove ``np.ctypeslib.ctypes_load_library``
* `#17119 <https://github.com/numpy/numpy/pull/17119>`__: DOC: make spacing consistent in NEP 41 bullet points
* `#17121 <https://github.com/numpy/numpy/pull/17121>`__: BUG: core: fix ilp64 blas dot/vdot/... for strides > int32 max
* `#17123 <https://github.com/numpy/numpy/pull/17123>`__: ENH: allow running mypy through runtests.py
* `#17127 <https://github.com/numpy/numpy/pull/17127>`__: MAINT: Remove duplicated symbols from link step
* `#17129 <https://github.com/numpy/numpy/pull/17129>`__: BLD: Check for reduce intrinsics and AVX512BW mask operations
* `#17132 <https://github.com/numpy/numpy/pull/17132>`__: MAINT: Chain some exceptions in arraysetops.
* `#17133 <https://github.com/numpy/numpy/pull/17133>`__: MAINT: Chain ValueError in ma.timer_comparison
* `#17137 <https://github.com/numpy/numpy/pull/17137>`__: API,MAINT: Rewrite promotion using common DType and common instance
* `#17141 <https://github.com/numpy/numpy/pull/17141>`__: MAINT: Make arrayprint str and repr the ndarray defaults.
* `#17142 <https://github.com/numpy/numpy/pull/17142>`__: DOC: NEP-42: Fix a few typos.
* `#17143 <https://github.com/numpy/numpy/pull/17143>`__: MAINT: Change handling of the expired financial functions.
* `#17144 <https://github.com/numpy/numpy/pull/17144>`__: ENH: Add annotations to 3 functions in ``np.core.function_base``
* `#17145 <https://github.com/numpy/numpy/pull/17145>`__: MAINT, BUG: Replace uses of PyString_AsString.
* `#17146 <https://github.com/numpy/numpy/pull/17146>`__: MAINT: ``Replace PyUString_*`` by ``PyUnicode_*`` equivalents.
* `#17149 <https://github.com/numpy/numpy/pull/17149>`__: MAINT: Replace PyInt macros with their PyLong replacement
* `#17150 <https://github.com/numpy/numpy/pull/17150>`__: ENH: Add support for the abstract scalars to cython code
* `#17151 <https://github.com/numpy/numpy/pull/17151>`__: BUG: Fix incorrect cython definition of npy_cfloat
* `#17152 <https://github.com/numpy/numpy/pull/17152>`__: MAINT: Clean up some ``Npy_`` vs ``Py_`` macro usage
* `#17154 <https://github.com/numpy/numpy/pull/17154>`__: DOC: Remove references to PyCObject
* `#17159 <https://github.com/numpy/numpy/pull/17159>`__: DOC: Update numpy4matlab
* `#17160 <https://github.com/numpy/numpy/pull/17160>`__: Clean up some more bytes vs unicode handling
* `#17161 <https://github.com/numpy/numpy/pull/17161>`__: BUG: Remove Void special case for "safe casting"
* `#17163 <https://github.com/numpy/numpy/pull/17163>`__: MAINT: Remove redundant headers
* `#17164 <https://github.com/numpy/numpy/pull/17164>`__: MAINT: Remove NPY_COPY_PYOBJECT_PTR
* `#17167 <https://github.com/numpy/numpy/pull/17167>`__: BLD: Merge the npysort library into multiarray
* `#17168 <https://github.com/numpy/numpy/pull/17168>`__: TST: Add tests mapping out the rules for metadata in promotion
* `#17171 <https://github.com/numpy/numpy/pull/17171>`__: BUG: revert trim_zeros changes from gh-16911
* `#17172 <https://github.com/numpy/numpy/pull/17172>`__: ENH: Make ``np.complexfloating`` generic w.r.t. ``np.floating``
* `#17176 <https://github.com/numpy/numpy/pull/17176>`__: MAINT/ENH: datetime: remove calls to PyUnicode_AsASCIIString,...
* `#17180 <https://github.com/numpy/numpy/pull/17180>`__: ENH: Added missing methods to ``np.flatiter``
* `#17181 <https://github.com/numpy/numpy/pull/17181>`__: DOC: Correct error in description of ndarray.base
* `#17182 <https://github.com/numpy/numpy/pull/17182>`__: DOC: Document ``dtype.metadata``
* `#17186 <https://github.com/numpy/numpy/pull/17186>`__: MAINT: Use utf8 strings in more of datetime
* `#17188 <https://github.com/numpy/numpy/pull/17188>`__: MAINT: Add placeholder stubs for ``ndarray`` and ``generic``
* `#17191 <https://github.com/numpy/numpy/pull/17191>`__: MAINT: Bump hypothesis from 5.26.0 to 5.30.0
* `#17193 <https://github.com/numpy/numpy/pull/17193>`__: MAINT: Remove some callers of functions in numpy.compat
* `#17195 <https://github.com/numpy/numpy/pull/17195>`__: ENH: Make the window functions exactly symmetric
* `#17197 <https://github.com/numpy/numpy/pull/17197>`__: MAINT: Improve error handling in npy_cpu_init
* `#17199 <https://github.com/numpy/numpy/pull/17199>`__: DOC: Fix the documented signatures of four ``ufunc`` methods
* `#17201 <https://github.com/numpy/numpy/pull/17201>`__: MAINT: Make the ``NPY_CPU_DISPATCH_CALL`` macros expressions not...
* `#17204 <https://github.com/numpy/numpy/pull/17204>`__: DOC: Fixed headings for tutorials so they appear at new theme...
* `#17210 <https://github.com/numpy/numpy/pull/17210>`__: DOC: Canonical_urls
* `#17214 <https://github.com/numpy/numpy/pull/17214>`__: MAINT: Fix various issues with the ``np.generic`` annotations
* `#17215 <https://github.com/numpy/numpy/pull/17215>`__: DOC: Use official MATLAB spelling in numpy-for-matlab-users.rst
* `#17219 <https://github.com/numpy/numpy/pull/17219>`__: BLD: enabled negation of library choices in NPY_*_ORDER
* `#17220 <https://github.com/numpy/numpy/pull/17220>`__: BUG, DOC: comment out metadata added via javascript
* `#17222 <https://github.com/numpy/numpy/pull/17222>`__: MAINT, DOC: move informational files from numpy.doc.*.py to their...
* `#17223 <https://github.com/numpy/numpy/pull/17223>`__: MAINT: use sysconfig not distutils.sysconfig where possible
* `#17225 <https://github.com/numpy/numpy/pull/17225>`__: BUG: Fix dimension discovery of within array ragged cases
* `#17227 <https://github.com/numpy/numpy/pull/17227>`__: DOC: Added templates for different types of issues.
* `#17233 <https://github.com/numpy/numpy/pull/17233>`__: DEP: Deprecated ndindex.ndincr
* `#17235 <https://github.com/numpy/numpy/pull/17235>`__: MAINT: Remove old PY_VERSION_HEX and sys.version_info code
* `#17237 <https://github.com/numpy/numpy/pull/17237>`__: BUG: Avoid using ``np.random`` in typing tests.
* `#17238 <https://github.com/numpy/numpy/pull/17238>`__: DOC: Use SPDX license expressions with correct license
* `#17239 <https://github.com/numpy/numpy/pull/17239>`__: DOC: Fix link quick-start in old random API functions
* `#17240 <https://github.com/numpy/numpy/pull/17240>`__: MAINT: added exception chaining in shape_base.py
* `#17241 <https://github.com/numpy/numpy/pull/17241>`__: MAINT: ``__array_interface__`` data address cannot be bytes
* `#17242 <https://github.com/numpy/numpy/pull/17242>`__: MAINT: Run slow CI jobs earlier so builds finishes sooner
* `#17247 <https://github.com/numpy/numpy/pull/17247>`__: ENH: Add tool to help speed up Travis CI
* `#17250 <https://github.com/numpy/numpy/pull/17250>`__: DOC: Fix docstring cross-referencing
* `#17252 <https://github.com/numpy/numpy/pull/17252>`__: DOC: Added a PR "Reviewer guidelines" document.
* `#17257 <https://github.com/numpy/numpy/pull/17257>`__: DOC: work around a bug in the new theme
* `#17258 <https://github.com/numpy/numpy/pull/17258>`__: SIMD: add fused multiply subtract/add intrinics for all supported...
* `#17259 <https://github.com/numpy/numpy/pull/17259>`__: MAINT: Bump hypothesis from 5.30.0 to 5.33.0
* `#17260 <https://github.com/numpy/numpy/pull/17260>`__: MAINT: Bump pydata-sphinx-theme from 0.3.2 to 0.4.0
* `#17263 <https://github.com/numpy/numpy/pull/17263>`__: DOC: add new glossary terms
* `#17264 <https://github.com/numpy/numpy/pull/17264>`__: DOC: remove some glosssary terms
* `#17267 <https://github.com/numpy/numpy/pull/17267>`__: TST: Fix the path to ``mypy.ini`` in ``runtests.py``
* `#17268 <https://github.com/numpy/numpy/pull/17268>`__: BUG: sysconfig attributes/distutils issue
* `#17273 <https://github.com/numpy/numpy/pull/17273>`__: ENH: Annotate the arithmetic operations of ``ndarray`` and ``generic``
* `#17278 <https://github.com/numpy/numpy/pull/17278>`__: MAINT: Merge together index page content into a single file
* `#17279 <https://github.com/numpy/numpy/pull/17279>`__: DOC: Fix a typo in shape_base.
* `#17284 <https://github.com/numpy/numpy/pull/17284>`__: ENH: Pass optimizations arguments to asv build
* `#17285 <https://github.com/numpy/numpy/pull/17285>`__: DEP: Change the financial name access warning to DeprecationWarning
* `#17288 <https://github.com/numpy/numpy/pull/17288>`__: REL: Update master after 1.19.2 release.
* `#17289 <https://github.com/numpy/numpy/pull/17289>`__: MAINT: Simplify ufunc pickling
* `#17290 <https://github.com/numpy/numpy/pull/17290>`__: MAINT: Cleanup some pystring macros
* `#17292 <https://github.com/numpy/numpy/pull/17292>`__: MAINT: Replace remaining PyString macros.
* `#17293 <https://github.com/numpy/numpy/pull/17293>`__: MAINT: Replace PyUString_Check by PyUnicode_Check.
* `#17295 <https://github.com/numpy/numpy/pull/17295>`__: BUG,ENH: fix pickling user-scalars by allowing non-format buffer...
* `#17296 <https://github.com/numpy/numpy/pull/17296>`__: MAINT: Replace some ``pyint_*`` macros defined in ``npy_3kcompat``.
* `#17297 <https://github.com/numpy/numpy/pull/17297>`__: BLD: set upper versions for build dependencies
* `#17299 <https://github.com/numpy/numpy/pull/17299>`__: MAINT: (dtype-transfer) make copyswapn and legacy cast wrapper...
* `#17300 <https://github.com/numpy/numpy/pull/17300>`__: MAINT: Replace PyBaseString_Check by PyUnicode_Check
* `#17302 <https://github.com/numpy/numpy/pull/17302>`__: MAINT: Replace a couple of missed npy_3kcompat macros
* `#17304 <https://github.com/numpy/numpy/pull/17304>`__: BUILD: pin pygments to 2.6.1, 2.7.0 breaks custom NumPyC lexer
* `#17307 <https://github.com/numpy/numpy/pull/17307>`__: MAINT: Bump hypothesis from 5.33.0 to 5.35.1
* `#17308 <https://github.com/numpy/numpy/pull/17308>`__: MAINT: Bump pytest from 6.0.1 to 6.0.2
* `#17309 <https://github.com/numpy/numpy/pull/17309>`__: MAINT: Move the ``fromnumeric`` annotations to their own stub file
* `#17312 <https://github.com/numpy/numpy/pull/17312>`__: MAINT: Syntax-highlight .src files on github
* `#17313 <https://github.com/numpy/numpy/pull/17313>`__: MAINT: Mark vendored/generated files in .gitattributes
* `#17315 <https://github.com/numpy/numpy/pull/17315>`__: MAINT: Cleanup f2py/cfuncs.py
* `#17319 <https://github.com/numpy/numpy/pull/17319>`__: BUG: Set deprecated fields to null in PyArray_InitArrFuncs
* `#17320 <https://github.com/numpy/numpy/pull/17320>`__: BUG: allow registration of hard-coded structured dtypes
* `#17326 <https://github.com/numpy/numpy/pull/17326>`__: ENH: Add annotations for five array construction functions
* `#17329 <https://github.com/numpy/numpy/pull/17329>`__: DOC: Fix incorrect ``.. deprecated::`` syntax that led to this...
* `#17330 <https://github.com/numpy/numpy/pull/17330>`__: DOC: improve ``issubdtype`` and scalar type docs
* `#17331 <https://github.com/numpy/numpy/pull/17331>`__: DOC: Remove the tables of scalar types, and use ``..autoclass``...
* `#17332 <https://github.com/numpy/numpy/pull/17332>`__: DOC, BLD: update lexer highlighting and make numpydocs a regular...
* `#17334 <https://github.com/numpy/numpy/pull/17334>`__: MAINT: Chaining exceptions in npyio.py
* `#17337 <https://github.com/numpy/numpy/pull/17337>`__: NEP: Regenerate table in NEP 29 (add numpy 1.18 and 1.19 to list)
* `#17338 <https://github.com/numpy/numpy/pull/17338>`__: DOC: Fix syntax errors in docstrings for versionchanged, versionadded
* `#17340 <https://github.com/numpy/numpy/pull/17340>`__: SIMD: Add partial/non-contig load and store intrinsics for 32/64-bit
* `#17344 <https://github.com/numpy/numpy/pull/17344>`__: ENH, BLD: Support for the NVIDIA HPC SDK nvfortran compiler
* `#17346 <https://github.com/numpy/numpy/pull/17346>`__: BLD,BUG: Fix a macOS build failure when ``NPY_BLAS_ORDER=""``
* `#17350 <https://github.com/numpy/numpy/pull/17350>`__: DEV: Add PR prefix labeler and numpy prefix mapping
* `#17352 <https://github.com/numpy/numpy/pull/17352>`__: DOC: Guide to writing how-tos
* `#17353 <https://github.com/numpy/numpy/pull/17353>`__: DOC: How-to guide for I/O
* `#17354 <https://github.com/numpy/numpy/pull/17354>`__: DOC: clarify residuals return param
* `#17356 <https://github.com/numpy/numpy/pull/17356>`__: ENH: Add Npy__PyLong_AsInt function.
* `#17357 <https://github.com/numpy/numpy/pull/17357>`__: MAINT: Bump hypothesis from 5.35.1 to 5.35.3
* `#17364 <https://github.com/numpy/numpy/pull/17364>`__: MAINT: Finish replacing PyInt_Check
* `#17369 <https://github.com/numpy/numpy/pull/17369>`__: DOC: distutils: Remove an obsolete paragraph.
* `#17370 <https://github.com/numpy/numpy/pull/17370>`__: NEP: Edit nep-0042 for more clarity
* `#17372 <https://github.com/numpy/numpy/pull/17372>`__: ENH: Add annotations for remaining ``ndarray`` / ``generic`` non-magic...
* `#17373 <https://github.com/numpy/numpy/pull/17373>`__: BUG: Fixes module data docstrings.
* `#17375 <https://github.com/numpy/numpy/pull/17375>`__: DOC: Fix default_rng docstring
* `#17377 <https://github.com/numpy/numpy/pull/17377>`__: BUG: ensure _UFuncNoLoopError can be pickled
* `#17380 <https://github.com/numpy/numpy/pull/17380>`__: Minor grammatical correction in quickstart doc.
* `#17382 <https://github.com/numpy/numpy/pull/17382>`__: DOC: NumPy restyling for pydata theme
* `#17383 <https://github.com/numpy/numpy/pull/17383>`__: MAINT: Fix docstring for np.matmul
* `#17386 <https://github.com/numpy/numpy/pull/17386>`__: MAINT: Bump hypothesis from 5.35.3 to 5.36.1
* `#17388 <https://github.com/numpy/numpy/pull/17388>`__: MAINT: Remove old debug print statement.
* `#17391 <https://github.com/numpy/numpy/pull/17391>`__: DOC: Replace "About NumPy" with "Document conventions"
* `#17392 <https://github.com/numpy/numpy/pull/17392>`__: DOC: Update info on doc style rules
* `#17393 <https://github.com/numpy/numpy/pull/17393>`__: BUG: Fix default void, datetime, and timedelta in array coercion
* `#17394 <https://github.com/numpy/numpy/pull/17394>`__: ENH: Implement sliding window
* `#17396 <https://github.com/numpy/numpy/pull/17396>`__: MAINT: Replace append_metastr_to_string function.
* `#17399 <https://github.com/numpy/numpy/pull/17399>`__: BLD: Fixed ARGOUTVIEWM memory deallocation. Closes #17398.
* `#17400 <https://github.com/numpy/numpy/pull/17400>`__: DOC: rm incorrect alias from recarray user article.
* `#17401 <https://github.com/numpy/numpy/pull/17401>`__: MAINT: Rewrite can-cast logic in terms of NEP 42
* `#17402 <https://github.com/numpy/numpy/pull/17402>`__: DOC: Add arraysetops to an autosummary
* `#17404 <https://github.com/numpy/numpy/pull/17404>`__: MAINT: Replace PyUString_ConcatAndDel in nditer_constr.c.
* `#17405 <https://github.com/numpy/numpy/pull/17405>`__: MAINT: Replace PyUString_ConcatAndDel in mapping.c.
* `#17406 <https://github.com/numpy/numpy/pull/17406>`__: ENH: Replace the module-level ``__getattr__`` with explicit type...
* `#17407 <https://github.com/numpy/numpy/pull/17407>`__: DOC: in PR template, set expectations for PR review timeline
* `#17409 <https://github.com/numpy/numpy/pull/17409>`__: MAINT: Cleanup remaining PyUString_ConcatAndDel use.
* `#17410 <https://github.com/numpy/numpy/pull/17410>`__: API: Special case how numpy scalars are coerced to signed integer
* `#17411 <https://github.com/numpy/numpy/pull/17411>`__: TST: Mark the typing tests as slow
* `#17412 <https://github.com/numpy/numpy/pull/17412>`__: DOC: Fix a parameter type in the ``putmask`` docs
* `#17418 <https://github.com/numpy/numpy/pull/17418>`__: DOC: adding operational form documentation for array ops
* `#17419 <https://github.com/numpy/numpy/pull/17419>`__: DEP: Deprecate coercion to subarray dtypes
* `#17421 <https://github.com/numpy/numpy/pull/17421>`__: BUG: Fix memory leak in array-coercion error paths
* `#17422 <https://github.com/numpy/numpy/pull/17422>`__: MAINT: chains nested try-except in numpy/ma/core.py
* `#17423 <https://github.com/numpy/numpy/pull/17423>`__: DOC: Remove bogus reference to _a_
* `#17424 <https://github.com/numpy/numpy/pull/17424>`__: DOC: Fix formatting issues in description of .c.src files
* `#17427 <https://github.com/numpy/numpy/pull/17427>`__: NEP: nep-0029 typo correction
* `#17429 <https://github.com/numpy/numpy/pull/17429>`__: MAINT: Move aliases for common scalar unions to ``numpy.typing``
* `#17430 <https://github.com/numpy/numpy/pull/17430>`__: BUG: Fix memoryleaks related to NEP 37 function overrides
* `#17431 <https://github.com/numpy/numpy/pull/17431>`__: DOC: Fix the links for ``Ellipsis``
* `#17432 <https://github.com/numpy/numpy/pull/17432>`__: DOC: add references to einops and opt_einsum
* `#17433 <https://github.com/numpy/numpy/pull/17433>`__: MAINT : Disable 32 bit PyPy CI testing on Windows.
* `#17435 <https://github.com/numpy/numpy/pull/17435>`__: DOC: Security warning for issues template
* `#17436 <https://github.com/numpy/numpy/pull/17436>`__: DOC: Fix "Feature request" spelling in issue templates
* `#17438 <https://github.com/numpy/numpy/pull/17438>`__: MAINT: Chaining exception in numpy\numpy\ma\mrecords.py
* `#17440 <https://github.com/numpy/numpy/pull/17440>`__: DOC: Cleaner template for PRs
* `#17442 <https://github.com/numpy/numpy/pull/17442>`__: MAINT: fix exception chaining in format.py
* `#17443 <https://github.com/numpy/numpy/pull/17443>`__: ENH: Warn on unsupported Python 3.10+
* `#17444 <https://github.com/numpy/numpy/pull/17444>`__: ENH: Add ``Typing :: Typed`` to the PyPI classifier
* `#17445 <https://github.com/numpy/numpy/pull/17445>`__: DOC: Fix the references for macros
* `#17447 <https://github.com/numpy/numpy/pull/17447>`__: NEP: update NEP 42 with discussion of type hinting applications
* `#17448 <https://github.com/numpy/numpy/pull/17448>`__: DOC: Remove CoC pages from Sphinx
* `#17453 <https://github.com/numpy/numpy/pull/17453>`__: MAINT: Chain exceptions in "_polybase.py"
* `#17455 <https://github.com/numpy/numpy/pull/17455>`__: MAINT: Bump hypothesis from 5.36.1 to 5.37.0
* `#17456 <https://github.com/numpy/numpy/pull/17456>`__: ENH: add dtype option to numpy.lib.function_base.cov and corrcoef
* `#17457 <https://github.com/numpy/numpy/pull/17457>`__: BUG: Fixes incorrect error message in numpy.ediff1d
* `#17459 <https://github.com/numpy/numpy/pull/17459>`__: DOC: update code of conduct URL
* `#17464 <https://github.com/numpy/numpy/pull/17464>`__: DOC: Add some entries for C types and macros
* `#17465 <https://github.com/numpy/numpy/pull/17465>`__: ENH: Add annotations for bitwise operations
* `#17468 <https://github.com/numpy/numpy/pull/17468>`__: DOC: add some missing scalar aliases
* `#17472 <https://github.com/numpy/numpy/pull/17472>`__: TST: Fix doctest for full_like
* `#17473 <https://github.com/numpy/numpy/pull/17473>`__: MAINT: py3k: remove os.fspath and os.PathLike backports
* `#17474 <https://github.com/numpy/numpy/pull/17474>`__: MAINT: Move the ``np.core.numeric`` annotations to their own stub...
* `#17479 <https://github.com/numpy/numpy/pull/17479>`__: ENH: type ``np.unicode_`` as ``np.str_``
* `#17481 <https://github.com/numpy/numpy/pull/17481>`__: DOC: Fix the entries for members of structures
* `#17483 <https://github.com/numpy/numpy/pull/17483>`__: DOC: Fix the references for ``random.*``
* `#17485 <https://github.com/numpy/numpy/pull/17485>`__: BLD: circleCI- merge before build, add -n to sphinx
* `#17487 <https://github.com/numpy/numpy/pull/17487>`__: MAINT: Remove duplicate placeholder annotations
* `#17493 <https://github.com/numpy/numpy/pull/17493>`__: DOC: New round of NEP 42 edits
* `#17497 <https://github.com/numpy/numpy/pull/17497>`__: DOC: Use consistent lowercase on docs landing page
* `#17498 <https://github.com/numpy/numpy/pull/17498>`__: MAINT: fix incompatible type comparison in numpy.lib.utils.info
* `#17501 <https://github.com/numpy/numpy/pull/17501>`__: BUG: Fix failures in master related to userdtype registeration
* `#17502 <https://github.com/numpy/numpy/pull/17502>`__: BUG: remove ``sys`` from the type stubs
* `#17503 <https://github.com/numpy/numpy/pull/17503>`__: DOC: Fix empty 'C style guide' page
* `#17504 <https://github.com/numpy/numpy/pull/17504>`__: DOC: Rename 'Quickstart tutorial'
* `#17508 <https://github.com/numpy/numpy/pull/17508>`__: ENH: Added the Final feature for all constants
* `#17510 <https://github.com/numpy/numpy/pull/17510>`__: DOC: Fewer blank lines in PR template
* `#17520 <https://github.com/numpy/numpy/pull/17520>`__: DOC: Display real license on license page
* `#17521 <https://github.com/numpy/numpy/pull/17521>`__: DOC: Add docstrings for some scalar types
* `#17523 <https://github.com/numpy/numpy/pull/17523>`__: DOC: Update top links in landing page
* `#17525 <https://github.com/numpy/numpy/pull/17525>`__: CI: Make merge ref grabbing conditional on the PR being active
* `#17527 <https://github.com/numpy/numpy/pull/17527>`__: DOC: Fix Bool types in C functions
* `#17528 <https://github.com/numpy/numpy/pull/17528>`__: Doc: Fix some links and typos
* `#17529 <https://github.com/numpy/numpy/pull/17529>`__: MAINT: Cleanup compatibility code for pathlib
* `#17534 <https://github.com/numpy/numpy/pull/17534>`__: DOC: Fix a typo
* `#17535 <https://github.com/numpy/numpy/pull/17535>`__: ENH: add function to get broadcast shape from a given set of...
* `#17536 <https://github.com/numpy/numpy/pull/17536>`__: BUG: Fixed crash on self-referential dtypes
* `#17537 <https://github.com/numpy/numpy/pull/17537>`__: MAINT: Bump hypothesis from 5.37.0 to 5.37.1
* `#17538 <https://github.com/numpy/numpy/pull/17538>`__: MAINT: Bump pydata-sphinx-theme from 0.4.0 to 0.4.1
* `#17539 <https://github.com/numpy/numpy/pull/17539>`__: MAINT: Bump mypy from 0.782 to 0.790
* `#17540 <https://github.com/numpy/numpy/pull/17540>`__: ENH: Make ``np.number`` generic with respect to its precision
* `#17541 <https://github.com/numpy/numpy/pull/17541>`__: CI: fix conditional for PR merge command
* `#17546 <https://github.com/numpy/numpy/pull/17546>`__: MAINT: explicit disabling ``CCompilerOpt`` in F2PY
* `#17548 <https://github.com/numpy/numpy/pull/17548>`__: BUG: Cygwin Workaround for #14787 on affected platforms
* `#17549 <https://github.com/numpy/numpy/pull/17549>`__: DOC: Fix the entries of C functions
* `#17555 <https://github.com/numpy/numpy/pull/17555>`__: DOC: Fix wrong blockquotes
* `#17558 <https://github.com/numpy/numpy/pull/17558>`__: DOC: MAINT: Add NEP 43 links to NEP 42
* `#17559 <https://github.com/numpy/numpy/pull/17559>`__: DOC: Remove directives for some constants
* `#17564 <https://github.com/numpy/numpy/pull/17564>`__: MAINT: Update the annotations in ``np.core.numeric``
* `#17570 <https://github.com/numpy/numpy/pull/17570>`__: DOC: Add the entry for ``NPY_FEATURE_VERSION``
* `#17571 <https://github.com/numpy/numpy/pull/17571>`__: DOC: Fix typos
* `#17572 <https://github.com/numpy/numpy/pull/17572>`__: ENH: Add annotations for three new constants
* `#17576 <https://github.com/numpy/numpy/pull/17576>`__: DOC: Fix Boolean array indexing typo
* `#17577 <https://github.com/numpy/numpy/pull/17577>`__: BUG: Respect dtype of all-zero argument to poly1d
* `#17578 <https://github.com/numpy/numpy/pull/17578>`__: NEP36: include additional feedback
* `#17580 <https://github.com/numpy/numpy/pull/17580>`__: MAINT: Cleanup swig for Python 3.
* `#17581 <https://github.com/numpy/numpy/pull/17581>`__: MAINT: Move the ``np.core.numerictypes`` annotations to their own...
* `#17583 <https://github.com/numpy/numpy/pull/17583>`__: MAINT: Bump hypothesis from 5.37.1 to 5.37.3
* `#17584 <https://github.com/numpy/numpy/pull/17584>`__: ENH: Add annotations for ``np.core._type_aliases``
* `#17594 <https://github.com/numpy/numpy/pull/17594>`__: DOC: Typo in lexsort docstring
* `#17596 <https://github.com/numpy/numpy/pull/17596>`__: DEP,BUG: Coercion/cast of array to a subarray dtype will be fixed
* `#17597 <https://github.com/numpy/numpy/pull/17597>`__: TST: Clean up the errors of the typing tests
* `#17598 <https://github.com/numpy/numpy/pull/17598>`__: BUG: Fixed file handle leak in array_tofile.
* `#17601 <https://github.com/numpy/numpy/pull/17601>`__: TST: Fix a broken ``np.core.numeric`` test
* `#17603 <https://github.com/numpy/numpy/pull/17603>`__: MAINT: Mark dead code as intentional for clang.
* `#17607 <https://github.com/numpy/numpy/pull/17607>`__: DOC: removed old references to submodule licenses
* `#17608 <https://github.com/numpy/numpy/pull/17608>`__: DOC: Fix typos (general documentation)
* `#17610 <https://github.com/numpy/numpy/pull/17610>`__: Fully qualify license trove classifier
* `#17611 <https://github.com/numpy/numpy/pull/17611>`__: BUG: mac dylib treated as part of extra objects by f2py
* `#17613 <https://github.com/numpy/numpy/pull/17613>`__: ENH: Add annotations for 9 ``ndarray``/``generic`` magic methods
* `#17614 <https://github.com/numpy/numpy/pull/17614>`__: DOC: Fix the document for arrays interface
* `#17618 <https://github.com/numpy/numpy/pull/17618>`__: MAINT: Conversion of some strings to f-strings
* `#17619 <https://github.com/numpy/numpy/pull/17619>`__: DOC: Fix some references
* `#17621 <https://github.com/numpy/numpy/pull/17621>`__: TST: Valid docstring for config_py function show()
* `#17622 <https://github.com/numpy/numpy/pull/17622>`__: MAINT: Conversion of some strings to fstrings, part II
* `#17623 <https://github.com/numpy/numpy/pull/17623>`__: MAINT: Conversion of some strings to fstrings, part III
* `#17624 <https://github.com/numpy/numpy/pull/17624>`__: DOC: Tidy up references to ``str_`` / ``bytes_``
* `#17625 <https://github.com/numpy/numpy/pull/17625>`__: MAINT: Conversion of some strings to fstrings, part iv
* `#17627 <https://github.com/numpy/numpy/pull/17627>`__: DOC: Fix the references for ``__array_*__``
* `#17628 <https://github.com/numpy/numpy/pull/17628>`__: DOC: Add entries for macros
* `#17629 <https://github.com/numpy/numpy/pull/17629>`__: DOC: Add ``identity_value`` to ``PyUFuncObject``
* `#17630 <https://github.com/numpy/numpy/pull/17630>`__: DOC: Replace ``PyCObject`` with ``PyCapsule``
* `#17633 <https://github.com/numpy/numpy/pull/17633>`__: DOC: Don't use Python highlighting for non-python code
* `#17638 <https://github.com/numpy/numpy/pull/17638>`__: DOC: Fix some references
* `#17639 <https://github.com/numpy/numpy/pull/17639>`__: MAINT: Bump hypothesis from 5.37.3 to 5.38.0
* `#17641 <https://github.com/numpy/numpy/pull/17641>`__: MAINT, BLD: update to OpenBLAS v0.3.12
* `#17642 <https://github.com/numpy/numpy/pull/17642>`__: DOC: Fix reference to atleast_1d
* `#17643 <https://github.com/numpy/numpy/pull/17643>`__: ENH: Add annotations for ``np.core._ufunc_config``
* `#17644 <https://github.com/numpy/numpy/pull/17644>`__: ENH: Add annotations for ``np.core.shape_base``
* `#17645 <https://github.com/numpy/numpy/pull/17645>`__: BUG: fix np.timedelta64('nat').__format__ throwing an exception
* `#17654 <https://github.com/numpy/numpy/pull/17654>`__: BUG: f2py incorrectly translates dimension declarations.
* `#17655 <https://github.com/numpy/numpy/pull/17655>`__: BLD: Fix installing Numpy on z/OS
* `#17657 <https://github.com/numpy/numpy/pull/17657>`__: NEP: Ensure inner loop signature is complete everywhere
* `#17658 <https://github.com/numpy/numpy/pull/17658>`__: TST: simplify source path names in compilation test
* `#17662 <https://github.com/numpy/numpy/pull/17662>`__: TST: f2py: Add a doctest for ``getlincoef``
* `#17666 <https://github.com/numpy/numpy/pull/17666>`__: REL: Update master after 1.19.3 release.
* `#17668 <https://github.com/numpy/numpy/pull/17668>`__: TST: Make test suite work in FIPS (140-2) Mode
* `#17670 <https://github.com/numpy/numpy/pull/17670>`__: DOC: f2py: Add a docstring for getarrlen
* `#17672 <https://github.com/numpy/numpy/pull/17672>`__: DOC: Update README badge for travis-ci.com
* `#17673 <https://github.com/numpy/numpy/pull/17673>`__: MAINT: Refine a number of ``np.generic`` annotations
* `#17675 <https://github.com/numpy/numpy/pull/17675>`__: MAINT: Update release documentation and software
* `#17681 <https://github.com/numpy/numpy/pull/17681>`__: SIMD: Add sum intrinsics for float/double.
* `#17682 <https://github.com/numpy/numpy/pull/17682>`__: BUG: (nditer_impl.h) Use ``intp`` instead of ``char *`` for offset...
* `#17689 <https://github.com/numpy/numpy/pull/17689>`__: BUG: Fix small bug in ``make_lite.py``.
* `#17691 <https://github.com/numpy/numpy/pull/17691>`__: DOC: Modify Templates
* `#17692 <https://github.com/numpy/numpy/pull/17692>`__: MAINT: Bump hypothesis from 5.38.0 to 5.41.0
* `#17693 <https://github.com/numpy/numpy/pull/17693>`__: MAINT: Bump pytz from 2020.1 to 2020.4
* `#17695 <https://github.com/numpy/numpy/pull/17695>`__: TST: use a more standard workflow for PyPy
* `#17696 <https://github.com/numpy/numpy/pull/17696>`__: REL: Update master after 1.19.4 release.
* `#17699 <https://github.com/numpy/numpy/pull/17699>`__: MAINT: Rename ``DtypeLike`` to ``DTypeLike``
* `#17700 <https://github.com/numpy/numpy/pull/17700>`__: Fix small typos.
* `#17701 <https://github.com/numpy/numpy/pull/17701>`__: BUG: Fixed an issue where ``.pyi`` files were ignored by numpy...
* `#17703 <https://github.com/numpy/numpy/pull/17703>`__: Fix Doc Typos & Added Example
* `#17706 <https://github.com/numpy/numpy/pull/17706>`__: BUG: Raise promotion error if a DType was provided in array coercion
* `#17708 <https://github.com/numpy/numpy/pull/17708>`__: Improve the einsum bench by adding new bench cases and variable...
* `#17711 <https://github.com/numpy/numpy/pull/17711>`__: ENH: adds type hints to numpy.version
* `#17715 <https://github.com/numpy/numpy/pull/17715>`__: REV: Revert gh-17654 - f2py incorrectly translates dimension...
* `#17717 <https://github.com/numpy/numpy/pull/17717>`__: MAINT: Add more files to ``.gitgnore``
* `#17720 <https://github.com/numpy/numpy/pull/17720>`__: API: Do not import sliding_window_view to main namespace
* `#17723 <https://github.com/numpy/numpy/pull/17723>`__: MAINT: Do not override ``sliding_window_view`` module to ``numpy``
* `#17725 <https://github.com/numpy/numpy/pull/17725>`__: NEP: Add NEP-35 instructions on reading like= downstream
* `#17729 <https://github.com/numpy/numpy/pull/17729>`__: BLD: Use importlib to find numpy root directory in distutils
* `#17733 <https://github.com/numpy/numpy/pull/17733>`__: MAINT: ma: Remove unused ``**options`` from MaskedArray ``__new__``...
* `#17735 <https://github.com/numpy/numpy/pull/17735>`__: TST: Remove Python 3.6 CI testing.
* `#17738 <https://github.com/numpy/numpy/pull/17738>`__: BLD, TST: move linux jobs to github actions
* `#17740 <https://github.com/numpy/numpy/pull/17740>`__: MAINT: Bump hypothesis from 5.41.0 to 5.41.2
* `#17743 <https://github.com/numpy/numpy/pull/17743>`__: BLD, BUG: Fix cblas detection on windows
* `#17745 <https://github.com/numpy/numpy/pull/17745>`__: TST: add pypy3.7
* `#17748 <https://github.com/numpy/numpy/pull/17748>`__: BLD: compare platform.architecture() correctly
* `#17749 <https://github.com/numpy/numpy/pull/17749>`__: DOC: Add "performance" category to the release notes
* `#17751 <https://github.com/numpy/numpy/pull/17751>`__: BUG: Fix segfault due to out of bound pointer in floatstatus...
* `#17753 <https://github.com/numpy/numpy/pull/17753>`__: BUG: Fix buffer export dtype references
* `#17755 <https://github.com/numpy/numpy/pull/17755>`__: BUG: Fix memory leaks found using valgrind
* `#17758 <https://github.com/numpy/numpy/pull/17758>`__: BLD: Lazy load f2py test utilities
* `#17759 <https://github.com/numpy/numpy/pull/17759>`__: BLD: use BUFFERSIZE=20 in OpenBLAS
* `#17763 <https://github.com/numpy/numpy/pull/17763>`__: SIMD, BUG: fix reuses the previous values during the fallback...
* `#17768 <https://github.com/numpy/numpy/pull/17768>`__: MAINT: update link to website in FUNDING.yml
* `#17773 <https://github.com/numpy/numpy/pull/17773>`__: MAINT: Add BLD and STY to labeler prefixes.
* `#17776 <https://github.com/numpy/numpy/pull/17776>`__: MAINT: Simplify Hypothesis configuration
* `#17787 <https://github.com/numpy/numpy/pull/17787>`__: NEP: Make like= argument added in NEP-35 strict
* `#17788 <https://github.com/numpy/numpy/pull/17788>`__: DOC: Fix up links, code blocks of release note fragments
* `#17796 <https://github.com/numpy/numpy/pull/17796>`__: MAINT: Minor touchups in npyio
* `#17802 <https://github.com/numpy/numpy/pull/17802>`__: MAINT: Update mailmap.
* `#17805 <https://github.com/numpy/numpy/pull/17805>`__: MAINT: Set the ufunc and ndarray ops return type to ``Any``
* `#17812 <https://github.com/numpy/numpy/pull/17812>`__: Update linalg.py
* `#17815 <https://github.com/numpy/numpy/pull/17815>`__: DOC: Fix empty_like docstring
* `#17823 <https://github.com/numpy/numpy/pull/17823>`__: DOC: Add missing release fragments to ``upcoming_changes``.
* `#17828 <https://github.com/numpy/numpy/pull/17828>`__: BUG: Fix incorrectly passed size in masked processing
* `#17829 <https://github.com/numpy/numpy/pull/17829>`__: MAINT: Bump hypothesis from 5.41.2 to 5.41.3
* `#17830 <https://github.com/numpy/numpy/pull/17830>`__: TST: Add back durations flag for DEBUG builds.
* `#17832 <https://github.com/numpy/numpy/pull/17832>`__: BUG: Fix subarray dtype used with too large count in fromfile
* `#17833 <https://github.com/numpy/numpy/pull/17833>`__: BUG: Fix pickling of scalars with NPY_LISTPICKLE
* `#17838 <https://github.com/numpy/numpy/pull/17838>`__: DOC: Update the ``numpy.typing`` documentation
* `#17841 <https://github.com/numpy/numpy/pull/17841>`__: DOC: Fixing boilerplate code example
* `#17844 <https://github.com/numpy/numpy/pull/17844>`__: MAINT: Add ``__all__`` to ``numpy.typing``
* `#17848 <https://github.com/numpy/numpy/pull/17848>`__: DOC: Add release note for gh-16161.
* `#17855 <https://github.com/numpy/numpy/pull/17855>`__: BUG: Fix incorrect C function prototypes/declarations.
* `#17857 <https://github.com/numpy/numpy/pull/17857>`__: MAINT: Prepare for the NumPy 1.20.x branch.
* `#17869 <https://github.com/numpy/numpy/pull/17869>`__: BUG, TST: use python-version not PYTHON_VERSION
* `#17879 <https://github.com/numpy/numpy/pull/17879>`__: BUG: Fix buffer readflag errors and small leaks
* `#17893 <https://github.com/numpy/numpy/pull/17893>`__: DOC: Prepare for 1.20.0 release
* `#17898 <https://github.com/numpy/numpy/pull/17898>`__: MAINT: Remove remaining uses of Python 3.6.
* `#17899 <https://github.com/numpy/numpy/pull/17899>`__: TST: use latest pypy37 not pypy36
* `#17901 <https://github.com/numpy/numpy/pull/17901>`__: MAINT: clean up a spurious warning in numpy/typing/setup.py
* `#17904 <https://github.com/numpy/numpy/pull/17904>`__: ENH: Speed up default ``where`` in the reduce-like method
* `#17915 <https://github.com/numpy/numpy/pull/17915>`__: TST: remove stray '+' from f-string upgrade
* `#17916 <https://github.com/numpy/numpy/pull/17916>`__: ENH: add support for fujitsu compiler to numpy.
* `#17922 <https://github.com/numpy/numpy/pull/17922>`__: BUG: 'bool' object has no attribute 'ndim'
* `#17931 <https://github.com/numpy/numpy/pull/17931>`__: DOC: Update release notes to mention ``type(dtype) is not np.dtype``
* `#17990 <https://github.com/numpy/numpy/pull/17990>`__: BUG: Replace f-string in setup.py
* `#18015 <https://github.com/numpy/numpy/pull/18015>`__: BUG: Ignore fewer errors during array-coercion
* `#18016 <https://github.com/numpy/numpy/pull/18016>`__: BUG: Fix a MacOS build failure
* `#18017 <https://github.com/numpy/numpy/pull/18017>`__: TST: Fix crosstalk issues with polynomial str tests.
* `#18018 <https://github.com/numpy/numpy/pull/18018>`__: TST: Ensure tests are not sensitive to execution order
* `#18019 <https://github.com/numpy/numpy/pull/18019>`__: BLD: update to OpenBLAS 0.3.13
* `#18024 <https://github.com/numpy/numpy/pull/18024>`__: DEP: Futurewarn on requiring __len__ on array-likes
* `#18035 <https://github.com/numpy/numpy/pull/18035>`__: BUG: make a variable volatile to work around clang compiler bug
* `#18049 <https://github.com/numpy/numpy/pull/18049>`__: TST: add back sdist test run
* `#18063 <https://github.com/numpy/numpy/pull/18063>`__: BUG: Fix concatenation when the output is "S" or "U"
* `#18064 <https://github.com/numpy/numpy/pull/18064>`__: BLD, BUG: Fix detecting aarch64 on macOS
* `#18068 <https://github.com/numpy/numpy/pull/18068>`__: REL: Prepare for 1.20.0rc2 release.
* `#18108 <https://github.com/numpy/numpy/pull/18108>`__: BUG, BLD: Generate the main dispatcher config header into the...
* `#18120 <https://github.com/numpy/numpy/pull/18120>`__: BUG, SIMD: Fix _simd module build for 64bit ARM/NEON clang
* `#18127 <https://github.com/numpy/numpy/pull/18127>`__: REL: Update 1.20.x after 1.19.5 release.
* `#18130 <https://github.com/numpy/numpy/pull/18130>`__: BUG: Fix promotion of half and string
* `#18146 <https://github.com/numpy/numpy/pull/18146>`__: BUG, MAINT: improve avx512 mask logical operations
* `#18154 <https://github.com/numpy/numpy/pull/18154>`__: BUG: Promotion between strings and objects was assymetric
* `#18192 <https://github.com/numpy/numpy/pull/18192>`__: MAINT: Use explicit reexports for numpy.typing objects
* `#18201 <https://github.com/numpy/numpy/pull/18201>`__: BUG: Keep ignoring most errors during array-protocol lookup
* `#18219 <https://github.com/numpy/numpy/pull/18219>`__: MAINT: random shuffle: warn on unrecognized objects, fix empty...
* `#18231 <https://github.com/numpy/numpy/pull/18231>`__: BLD: update OpenBLAS to af2b0d02
* `#18237 <https://github.com/numpy/numpy/pull/18237>`__: DOC: Clarify the type alias deprecation message
* `#18257 <https://github.com/numpy/numpy/pull/18257>`__: BUG: Ensure too many advanced indices raises an exception
* `#18258 <https://github.com/numpy/numpy/pull/18258>`__: MAINT: add an 'apt update'
* `#18259 <https://github.com/numpy/numpy/pull/18259>`__: DOC: Prepare for the NumPy 1.20.0 release.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Sebastian Berg
* Warren Weckesser

Pull requests merged
====================

A total of 6 pull requests were merged for this release.

* `#16055 <https://github.com/numpy/numpy/pull/16055>`__: BLD: add i686 for 1.18 builds
* `#16090 <https://github.com/numpy/numpy/pull/16090>`__: BUG: random: ``Generator.integers(2**32)`` always returned 0.
* `#16091 <https://github.com/numpy/numpy/pull/16091>`__: BLD: fix path to libgfortran on macOS
* `#16109 <https://github.com/numpy/numpy/pull/16109>`__: REV: Reverts side-effect changes to casting
* `#16114 <https://github.com/numpy/numpy/pull/16114>`__: BLD: put openblas library in local directory on windows
* `#16132 <https://github.com/numpy/numpy/pull/16132>`__: DOC: Change import error "howto" to link to new troubleshooting...

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Sebastian Berg
* bbbbbbbbba +

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#12296 <https://github.com/numpy/numpy/pull/12296>`__: BUG: Dealloc cached buffer info (#12249)
* `#12297 <https://github.com/numpy/numpy/pull/12297>`__: BUG: Fix fill value in masked array '==' and '!=' ops.
* `#12307 <https://github.com/numpy/numpy/pull/12307>`__: DOC: Correct the default value of `optimize` in `numpy.einsum`
* `#12320 <https://github.com/numpy/numpy/pull/12320>`__: REL: Prepare for the NumPy 1.15.4 release

Contributors
============

A total of 10 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Dennis Zollo +
* Hunter Damron +
* Jingbei Li +
* Kevin Sheppard
* Matti Picus
* Nicola Soranzo +
* Sebastian Berg
* Tyler Reddy

Pull requests merged
====================

A total of 16 pull requests were merged for this release.

* `#13392 <https://github.com/numpy/numpy/pull/13392>`__: BUG: Some PyPy versions lack PyStructSequence_InitType2.
* `#13394 <https://github.com/numpy/numpy/pull/13394>`__: MAINT, DEP: Fix deprecated ``assertEquals()``
* `#13396 <https://github.com/numpy/numpy/pull/13396>`__: BUG: Fix structured_to_unstructured on single-field types (backport)
* `#13549 <https://github.com/numpy/numpy/pull/13549>`__: BLD: Make CI pass again with pytest 4.5
* `#13552 <https://github.com/numpy/numpy/pull/13552>`__: TST: Register markers in conftest.py.
* `#13559 <https://github.com/numpy/numpy/pull/13559>`__: BUG: Removes ValueError for empty kwargs in arraymultiter_new
* `#13560 <https://github.com/numpy/numpy/pull/13560>`__: BUG: Add TypeError to accepted exceptions in crackfortran.
* `#13561 <https://github.com/numpy/numpy/pull/13561>`__: BUG: Handle subarrays in descr_to_dtype
* `#13562 <https://github.com/numpy/numpy/pull/13562>`__: BUG: Protect generators from log(0.0)
* `#13563 <https://github.com/numpy/numpy/pull/13563>`__: BUG: Always return views from structured_to_unstructured when...
* `#13564 <https://github.com/numpy/numpy/pull/13564>`__: BUG: Catch stderr when checking compiler version
* `#13565 <https://github.com/numpy/numpy/pull/13565>`__: BUG: longdouble(int) does not work
* `#13587 <https://github.com/numpy/numpy/pull/13587>`__: BUG: distutils/system_info.py fix missing subprocess import (#13523)
* `#13620 <https://github.com/numpy/numpy/pull/13620>`__: BUG,DEP: Fix writeable flag setting for arrays without base
* `#13641 <https://github.com/numpy/numpy/pull/13641>`__: MAINT: Prepare for the 1.16.4 release.
* `#13644 <https://github.com/numpy/numpy/pull/13644>`__: BUG: special case object arrays when printing rel-, abs-error

Contributors
============

A total of 18 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alexander Shadchin
* Allan Haldane
* Bruce Merry +
* Charles Harris
* Colin Snyder +
* Dan Allan +
* Emile +
* Eric Wieser
* Grey Baker +
* Maksim Shabunin +
* Marten van Kerkwijk
* Matti Picus
* Peter Andreas Entschev +
* Ralf Gommers
* Richard Harris +
* Sebastian Berg
* Sergei Lebedev +
* Stephan Hoyer

Pull requests merged
====================

A total of 23 pull requests were merged for this release.

* `#13742 <https://github.com/numpy/numpy/pull/13742>`__: ENH: Add project URLs to setup.py
* `#13823 <https://github.com/numpy/numpy/pull/13823>`__: TEST, ENH: fix tests and ctypes code for PyPy
* `#13845 <https://github.com/numpy/numpy/pull/13845>`__: BUG: use npy_intp instead of int for indexing array
* `#13867 <https://github.com/numpy/numpy/pull/13867>`__: TST: Ignore DeprecationWarning during nose imports
* `#13905 <https://github.com/numpy/numpy/pull/13905>`__: BUG: Fix use-after-free in boolean indexing
* `#13933 <https://github.com/numpy/numpy/pull/13933>`__: MAINT/BUG/DOC: Fix errors in _add_newdocs
* `#13984 <https://github.com/numpy/numpy/pull/13984>`__: BUG: fix byte order reversal for datetime64[ns]
* `#13994 <https://github.com/numpy/numpy/pull/13994>`__: MAINT,BUG: Use nbytes to also catch empty descr during allocation
* `#14042 <https://github.com/numpy/numpy/pull/14042>`__: BUG: np.array cleared errors occurred in PyMemoryView_FromObject
* `#14043 <https://github.com/numpy/numpy/pull/14043>`__: BUG: Fixes for Undefined Behavior Sanitizer (UBSan) errors.
* `#14044 <https://github.com/numpy/numpy/pull/14044>`__: BUG: ensure that casting to/from structured is properly checked.
* `#14045 <https://github.com/numpy/numpy/pull/14045>`__: MAINT: fix histogram*d dispatchers
* `#14046 <https://github.com/numpy/numpy/pull/14046>`__: BUG: further fixup to histogram2d dispatcher.
* `#14052 <https://github.com/numpy/numpy/pull/14052>`__: BUG: Replace contextlib.suppress for Python 2.7
* `#14056 <https://github.com/numpy/numpy/pull/14056>`__: BUG: fix compilation of 3rd party modules with Py_LIMITED_API...
* `#14057 <https://github.com/numpy/numpy/pull/14057>`__: BUG: Fix memory leak in dtype from dict constructor
* `#14058 <https://github.com/numpy/numpy/pull/14058>`__: DOC: Document array_function at a higher level.
* `#14084 <https://github.com/numpy/numpy/pull/14084>`__: BUG, DOC: add new recfunctions to `__all__`
* `#14162 <https://github.com/numpy/numpy/pull/14162>`__: BUG: Remove stray print that causes a SystemError on python 3.7
* `#14297 <https://github.com/numpy/numpy/pull/14297>`__: TST: Pin pytest version to 5.0.1.
* `#14322 <https://github.com/numpy/numpy/pull/14322>`__: ENH: Enable huge pages in all Linux builds
* `#14346 <https://github.com/numpy/numpy/pull/14346>`__: BUG: fix behavior of structured_to_unstructured on non-trivial...
* `#14382 <https://github.com/numpy/numpy/pull/14382>`__: REL: Prepare for the NumPy 1.16.5 release.

Contributors
============

A total of 126 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alex Henrie
* Alexandre de Siqueira +
* Andras Deak
* Andrea Sangalli +
* Andreas Klöckner +
* Andrei Shirobokov +
* Anirudh Subramanian +
* Anne Bonner
* Anton Ritter-Gogerly +
* Benjamin Trendelkamp-Schroer +
* Bharat Raghunathan
* Brandt Bucher +
* Brian Wignall
* Bui Duc Minh +
* Changqing Li +
* Charles Harris
* Chris Barker
* Chris Holland +
* Christian Kastner +
* Chunlin +
* Chunlin Fang +
* Damien Caliste +
* Dan Allan
* Daniel Hrisca
* Daniel Povey +
* Dustan Levenstein +
* Emmanuelle Gouillart +
* Eric Larson
* Eric M. Bray
* Eric Mariasis +
* Eric Wieser
* Erik Welch +
* Fabio Zeiser +
* Gabriel Gerlero +
* Ganesh Kathiresan +
* Gengxin Xie +
* Guilherme Leobas
* Guillaume Peillex +
* Hameer Abbasi
* Hao Jin +
* Harshal Prakash Patankar +
* Heshy Roskes +
* Himanshu Garg +
* Huon Wilson +
* John Han +
* John Kirkham
* Jon Dufresne
* Jon Morris +
* Josh Wilson
* Justus Magin
* Kai Striega
* Kerem Hallaç +
* Kevin Sheppard
* Kirill Zinovjev +
* Marcin Podhajski +
* Mark Harfouche
* Marten van Kerkwijk
* Martin Michlmayr +
* Masashi Kishimoto +
* Mathieu Lamarre
* Matt Hancock +
* MatteoRaso +
* Matthew Harrigan
* Matthias Bussonnier
* Matti Picus
* Max Balandat +
* Maximilian Konrad +
* Maxwell Aladago
* Maxwell Bileschi +
* Melissa Weber Mendonça +
* Michael Felt
* Michael Hirsch +
* Mike Taves
* Nico Schlömer
* Pan Jan +
* Paul Rougieux +
* Pauli Virtanen
* Peter Andreas Entschev
* Petre-Flaviu Gostin +
* Pierre de Buyl
* Piotr Gaiński +
* Przemyslaw Bartosik +
* Raghuveer Devulapalli
* Rakesh Vasudevan +
* Ralf Gommers
* RenaRuirui +
* Robert Kern
* Roman Yurchak
* Ross Barnowski +
* Ryan +
* Ryan Soklaski
* Sanjeev Kumar +
* SanthoshBala18 +
* Sayed Adel +
* Sebastian Berg
* Seth Troisi
* Sha Liu +
* Siba Smarak Panigrahi +
* Simon Gasse +
* Stephan Hoyer
* Steve Dower +
* Thomas A Caswell
* Till Hoffmann +
* Tim Hoffmann
* Tina Oberoi +
* Tirth Patel
* Tyler Reddy
* Warren Weckesser
* Wojciech Rzadkowski +
* Xavier Thomas +
* Yilin LI +
* Zac Hatfield-Dodds +
* Zé Vinícius +
* @Adam +
* @Anthony +
* @Jim +
* @bartosz-grabowski +
* @dojafrat +
* @gamboon +
* @jfbu +
* @keremh +
* @mayeut +
* @ndunnewind +
* @nglinh +
* @shreepads +
* @sslivkoff +


Pull requests merged
====================

A total of 488 pull requests were merged for this release.

* `#8255 <https://github.com/numpy/numpy/pull/8255>`__: ENH: add identity kwarg to frompyfunc
* `#10600 <https://github.com/numpy/numpy/pull/10600>`__: DOC: Do not complain about contiguity when mutating ``ndarray.shape``
* `#12646 <https://github.com/numpy/numpy/pull/12646>`__: TST: check exception details in refguide_check.py
* `#13421 <https://github.com/numpy/numpy/pull/13421>`__: ENH: improve runtime detection of CPU features
* `#14326 <https://github.com/numpy/numpy/pull/14326>`__: TST: Add assert_array_equal test for big integer arrays.
* `#14376 <https://github.com/numpy/numpy/pull/14376>`__: MAINT: Remove unnecessary 'from __future__ import ...' statements
* `#14530 <https://github.com/numpy/numpy/pull/14530>`__: MAINT: Fix typos and copy edit NEP-0030.
* `#14546 <https://github.com/numpy/numpy/pull/14546>`__: DOC: NumPy for absolute beginners tutorial
* `#14715 <https://github.com/numpy/numpy/pull/14715>`__: NEP: Proposal for array creation dispatching with ``__array_function__``
* `#14867 <https://github.com/numpy/numpy/pull/14867>`__: ENH: Use AVX-512F for np.maximum and np.minimum
* `#14924 <https://github.com/numpy/numpy/pull/14924>`__: BUG: Fix numpy.random.dirichlet returns NaN for small 'alpha'...
* `#14933 <https://github.com/numpy/numpy/pull/14933>`__: API: Use ``ResultType`` in ``PyArray_ConvertToCommonType``
* `#14940 <https://github.com/numpy/numpy/pull/14940>`__: BUG: pickle the content of a scalar containing objects, not the...
* `#14942 <https://github.com/numpy/numpy/pull/14942>`__: MAINT,API: ignore and NULL fasttake/fastputmask ArrFuncs slots
* `#14981 <https://github.com/numpy/numpy/pull/14981>`__: BUG: Make ``ediff1d`` kwarg casting consistent
* `#14988 <https://github.com/numpy/numpy/pull/14988>`__: DOC: linalg: Include information about scipy.linalg.
* `#14995 <https://github.com/numpy/numpy/pull/14995>`__: BUG: Use ``__array__`` during dimension discovery
* `#15011 <https://github.com/numpy/numpy/pull/15011>`__: MAINT: cleanup compat.py3k.py
* `#15022 <https://github.com/numpy/numpy/pull/15022>`__: ENH: f2py: improve error messages
* `#15024 <https://github.com/numpy/numpy/pull/15024>`__: DOC: clarify documentation for transpose()
* `#15028 <https://github.com/numpy/numpy/pull/15028>`__: [DOC] LaTeX: fix preamble (closes #15026)
* `#15035 <https://github.com/numpy/numpy/pull/15035>`__: BUG: add endfunction, endsubroutine to valid fortran end words
* `#15040 <https://github.com/numpy/numpy/pull/15040>`__: TST: Add test for object method (and general unary) loops
* `#15042 <https://github.com/numpy/numpy/pull/15042>`__: REL: Update master after 1.18.x branch.
* `#15043 <https://github.com/numpy/numpy/pull/15043>`__: DOC: Update HOWTO_RELEASE.rst.txt
* `#15046 <https://github.com/numpy/numpy/pull/15046>`__: API, DOC: change names to multivariate_hypergeometric, improve...
* `#15050 <https://github.com/numpy/numpy/pull/15050>`__: DOC: Fix statement about norms
* `#15052 <https://github.com/numpy/numpy/pull/15052>`__: MAINT: follow-up cleanup for blas64 PR
* `#15054 <https://github.com/numpy/numpy/pull/15054>`__: DOC: add docstrings to refguide-check
* `#15066 <https://github.com/numpy/numpy/pull/15066>`__: Revert "DEP: issue deprecation warning when creating ragged array...
* `#15068 <https://github.com/numpy/numpy/pull/15068>`__: ENH: Add support to sort timedelta64 ``NaT`` to end of the array
* `#15069 <https://github.com/numpy/numpy/pull/15069>`__: ENH: add support for ILP64 OpenBLAS (without symbol suffix)
* `#15070 <https://github.com/numpy/numpy/pull/15070>`__: DOC: correct version for NaT sort
* `#15072 <https://github.com/numpy/numpy/pull/15072>`__: TST: Check requires_memory immediately before the test
* `#15073 <https://github.com/numpy/numpy/pull/15073>`__: MAINT: core: Fix a very long line in the ufunc docstrings.
* `#15076 <https://github.com/numpy/numpy/pull/15076>`__: BUG: test, fix flexible dtype conversion on class with __array__
* `#15082 <https://github.com/numpy/numpy/pull/15082>`__: TST: add value to pytest.ini for pytest6 compatibility
* `#15085 <https://github.com/numpy/numpy/pull/15085>`__: MAINT: Ragged cleanup
* `#15097 <https://github.com/numpy/numpy/pull/15097>`__: DOC: bring the out parameter docstring into line with ufuncs
* `#15106 <https://github.com/numpy/numpy/pull/15106>`__: ENH: f2py: add --f2cmap option for specifying the name of .f2py_f2cmap
* `#15107 <https://github.com/numpy/numpy/pull/15107>`__: TST: add BLAS ILP64 run in Travis & Azure
* `#15110 <https://github.com/numpy/numpy/pull/15110>`__: MAINT: Fix expm1 instability for small complex numbers.
* `#15115 <https://github.com/numpy/numpy/pull/15115>`__: MAINT: random: Remove a few unused imports from test files.
* `#15116 <https://github.com/numpy/numpy/pull/15116>`__: MAINT: Bump pytest from 5.3.1 to 5.3.2
* `#15118 <https://github.com/numpy/numpy/pull/15118>`__: API: remove undocumented use of __array__(dtype, context)
* `#15120 <https://github.com/numpy/numpy/pull/15120>`__: MAINT,CI: fix signed-unsigned comparison warning
* `#15124 <https://github.com/numpy/numpy/pull/15124>`__: DOC: Update documentation of np.clip
* `#15125 <https://github.com/numpy/numpy/pull/15125>`__: DOC: Remove reference to basic RNG
* `#15126 <https://github.com/numpy/numpy/pull/15126>`__: MAINT: Fix randint 0d limits and other 0d cleanups
* `#15129 <https://github.com/numpy/numpy/pull/15129>`__: DOC: Fix typos, via a Levenshtein-style corrector
* `#15133 <https://github.com/numpy/numpy/pull/15133>`__: MAINT: CI: Clean up .travis.yml
* `#15136 <https://github.com/numpy/numpy/pull/15136>`__: DOC: Correct choice signature
* `#15138 <https://github.com/numpy/numpy/pull/15138>`__: DOC: Correct documentation in choice
* `#15143 <https://github.com/numpy/numpy/pull/15143>`__: TST: shippable build efficiency
* `#15144 <https://github.com/numpy/numpy/pull/15144>`__: BUG: ensure reduction output matches input along non-reduction...
* `#15149 <https://github.com/numpy/numpy/pull/15149>`__: REL: Update master after NumPy 1.18.0 release.
* `#15150 <https://github.com/numpy/numpy/pull/15150>`__: MAINT: Update pavement.py for towncrier.
* `#15153 <https://github.com/numpy/numpy/pull/15153>`__: DOC: update cholesky docstring regarding input checking
* `#15154 <https://github.com/numpy/numpy/pull/15154>`__: DOC: update documentation on how to build NumPy
* `#15156 <https://github.com/numpy/numpy/pull/15156>`__: DOC: add moved modules to 1.18 release note
* `#15160 <https://github.com/numpy/numpy/pull/15160>`__: MAINT: Update required cython version to 0.29.14.
* `#15164 <https://github.com/numpy/numpy/pull/15164>`__: BUG: searchsorted: passing the keys as a keyword argument
* `#15170 <https://github.com/numpy/numpy/pull/15170>`__: BUG: use tmp dir and check version for cython test
* `#15178 <https://github.com/numpy/numpy/pull/15178>`__: TST: improve assert message of assert_array_max_ulp
* `#15187 <https://github.com/numpy/numpy/pull/15187>`__: MAINT: unskip test on win32
* `#15189 <https://github.com/numpy/numpy/pull/15189>`__: ENH: Add property-based tests using Hypothesis
* `#15194 <https://github.com/numpy/numpy/pull/15194>`__: BUG: test, fix for c++ compilation
* `#15195 <https://github.com/numpy/numpy/pull/15195>`__: MAINT: refactoring in np.core.records
* `#15196 <https://github.com/numpy/numpy/pull/15196>`__: DOC: Adding instructions for building documentation to developer...
* `#15197 <https://github.com/numpy/numpy/pull/15197>`__: DOC: NEP 37: A dispatch protocol for NumPy-like modules
* `#15203 <https://github.com/numpy/numpy/pull/15203>`__: MAINT: Do not use private Python function in testing
* `#15205 <https://github.com/numpy/numpy/pull/15205>`__: DOC: Improvements to Quickstart Tutorial.
* `#15211 <https://github.com/numpy/numpy/pull/15211>`__: BUG: distutils: fix msvc+gfortran openblas handling corner case
* `#15212 <https://github.com/numpy/numpy/pull/15212>`__: BUG: lib: Fix handling of integer arrays by gradient.
* `#15215 <https://github.com/numpy/numpy/pull/15215>`__: MAINT: lib: A little bit of clean up for the new year.
* `#15216 <https://github.com/numpy/numpy/pull/15216>`__: REL: Update master after NumPy 1.16.6 and 1.17.5 releases.
* `#15217 <https://github.com/numpy/numpy/pull/15217>`__: DEP: records: Deprecate treating shape=0 as shape=None
* `#15218 <https://github.com/numpy/numpy/pull/15218>`__: ENH: build fallback lapack_lite with 64-bit integers on 64-bit...
* `#15224 <https://github.com/numpy/numpy/pull/15224>`__: MAINT: linalg: use symbol suffix in fallback lapack_lite
* `#15227 <https://github.com/numpy/numpy/pull/15227>`__: DOC: typo in release.rst
* `#15228 <https://github.com/numpy/numpy/pull/15228>`__: NEP: universal SIMD NEP 38
* `#15229 <https://github.com/numpy/numpy/pull/15229>`__: MAINT: Remove unused int_asbuffer
* `#15230 <https://github.com/numpy/numpy/pull/15230>`__: BUG: do not emit warnings for np.sign, np.equal when using nan
* `#15231 <https://github.com/numpy/numpy/pull/15231>`__: MAINT: Remove Python2 specific C module setup [part2]
* `#15232 <https://github.com/numpy/numpy/pull/15232>`__: MAINT: Cleaning up PY_MAJOR_VERSION/PY_VERSION_HEX
* `#15233 <https://github.com/numpy/numpy/pull/15233>`__: MAINT: Clean up more PY_VERSION_HEX
* `#15236 <https://github.com/numpy/numpy/pull/15236>`__: MAINT: Remove implicit inheritance from object class
* `#15238 <https://github.com/numpy/numpy/pull/15238>`__: MAINT: only add --std=c99 where needed
* `#15239 <https://github.com/numpy/numpy/pull/15239>`__: MAINT: Remove Python2 newbuffer getbuffer
* `#15240 <https://github.com/numpy/numpy/pull/15240>`__: MAINT: Py3K array_as_buffer and gentype_as_buffer
* `#15241 <https://github.com/numpy/numpy/pull/15241>`__: MAINT: Remove references to non-existent sys.exc_clear()
* `#15242 <https://github.com/numpy/numpy/pull/15242>`__: DOC: Update HOWTO_RELEASE.rst
* `#15248 <https://github.com/numpy/numpy/pull/15248>`__: MAINT: cleanup use of sys.exc_info
* `#15249 <https://github.com/numpy/numpy/pull/15249>`__: MAINT: Eliminate some calls to ``eval``
* `#15251 <https://github.com/numpy/numpy/pull/15251>`__: MAINT: Improve const-correctness of shapes and strides
* `#15253 <https://github.com/numpy/numpy/pull/15253>`__: DOC: clarify the effect of None parameters passed to ndarray.view
* `#15254 <https://github.com/numpy/numpy/pull/15254>`__: MAINT: Improve const-correctness of string arguments
* `#15255 <https://github.com/numpy/numpy/pull/15255>`__: MAINT: Delete numpy.distutils.compat
* `#15256 <https://github.com/numpy/numpy/pull/15256>`__: MAINT: Implement keyword-only arguments as syntax
* `#15260 <https://github.com/numpy/numpy/pull/15260>`__: MAINT: Remove FIXME comments introduced in the previous commit
* `#15261 <https://github.com/numpy/numpy/pull/15261>`__: MAINT: Work with unicode strings in ``dtype('i8,i8')``
* `#15262 <https://github.com/numpy/numpy/pull/15262>`__: BUG: Use PyDict_GetItemWithError() instead of PyDict_GetItem()
* `#15263 <https://github.com/numpy/numpy/pull/15263>`__: MAINT: Remove python2 array_{get,set}slice
* `#15264 <https://github.com/numpy/numpy/pull/15264>`__: DOC: Add some missing functions in the list of available ufuncs.
* `#15265 <https://github.com/numpy/numpy/pull/15265>`__: MAINT: Tidy PyArray_DescrConverter
* `#15266 <https://github.com/numpy/numpy/pull/15266>`__: MAINT: remove duplicated if statements between DescrConverters
* `#15267 <https://github.com/numpy/numpy/pull/15267>`__: BUG: Fix PyArray_DescrAlignConverter2 on tuples
* `#15268 <https://github.com/numpy/numpy/pull/15268>`__: MAINT: Remove Python2 ndarray.__unicode__
* `#15272 <https://github.com/numpy/numpy/pull/15272>`__: MAINT: Remove Python 2 divide
* `#15273 <https://github.com/numpy/numpy/pull/15273>`__: MAINT: minor formatting fixups for NEP-37
* `#15274 <https://github.com/numpy/numpy/pull/15274>`__: MAINT: Post NumPy 1.18.1 update.
* `#15275 <https://github.com/numpy/numpy/pull/15275>`__: MAINT: travis-ci: Update CI scripts.
* `#15278 <https://github.com/numpy/numpy/pull/15278>`__: BENCH: Add benchmark for small array coercions
* `#15279 <https://github.com/numpy/numpy/pull/15279>`__: BUILD: use standard build of OpenBLAS for aarch64, ppc64le, s390x
* `#15280 <https://github.com/numpy/numpy/pull/15280>`__: BENCH: Add basic benchmarks for take and putmask
* `#15281 <https://github.com/numpy/numpy/pull/15281>`__: MAINT: Cleanup most PY3K #ifdef guards
* `#15282 <https://github.com/numpy/numpy/pull/15282>`__: DOC: BLD: add empty release notes for 1.19.0 to fix doc build...
* `#15283 <https://github.com/numpy/numpy/pull/15283>`__: MAINT: Cleanup more NPY_PY3K
* `#15284 <https://github.com/numpy/numpy/pull/15284>`__: MAINT: Use a simpler return convention for internal functions
* `#15285 <https://github.com/numpy/numpy/pull/15285>`__: MAINT: Simplify ``np.int_`` inheritance
* `#15286 <https://github.com/numpy/numpy/pull/15286>`__: DOC" Update np.full docstring.
* `#15287 <https://github.com/numpy/numpy/pull/15287>`__: MAINT: Express PyArray_DescrAlignConverter in terms of _convert_from_any
* `#15288 <https://github.com/numpy/numpy/pull/15288>`__: MAINT: Push down declarations in _convert_from_*
* `#15289 <https://github.com/numpy/numpy/pull/15289>`__: MAINT: C code simplifications
* `#15291 <https://github.com/numpy/numpy/pull/15291>`__: BUG: Add missing error handling to _convert_from_list
* `#15295 <https://github.com/numpy/numpy/pull/15295>`__: DOC: Added tutorial about linear algebra on multidimensional...
* `#15300 <https://github.com/numpy/numpy/pull/15300>`__: MAINT: Refactor dtype conversion functions to be more similar
* `#15303 <https://github.com/numpy/numpy/pull/15303>`__: DOC: Updating f2py docs to python 3 and fixing some typos
* `#15304 <https://github.com/numpy/numpy/pull/15304>`__: MAINT: Remove NPY_PY3K constant
* `#15305 <https://github.com/numpy/numpy/pull/15305>`__: MAINT: Remove sys.version checks in tests
* `#15307 <https://github.com/numpy/numpy/pull/15307>`__: MAINT: cleanup sys.version dependant code
* `#15310 <https://github.com/numpy/numpy/pull/15310>`__: MAINT: Ensure ``_convert_from_*`` functions set errors
* `#15312 <https://github.com/numpy/numpy/pull/15312>`__: MAINT: Avoid escaping unicode in error messages
* `#15315 <https://github.com/numpy/numpy/pull/15315>`__: MAINT: Change file extension of ma README to rst.
* `#15319 <https://github.com/numpy/numpy/pull/15319>`__: BUG: fix NameError in clip nan propagation tests
* `#15323 <https://github.com/numpy/numpy/pull/15323>`__: NEP: document reimplementation of NEP 34
* `#15324 <https://github.com/numpy/numpy/pull/15324>`__: MAINT: fix typos
* `#15328 <https://github.com/numpy/numpy/pull/15328>`__: TST: move pypy CI to ubuntu 18.04
* `#15329 <https://github.com/numpy/numpy/pull/15329>`__: TST: move _no_tracing to testing._private, remove testing.support
* `#15333 <https://github.com/numpy/numpy/pull/15333>`__: BUG: Add some missing C error handling
* `#15335 <https://github.com/numpy/numpy/pull/15335>`__: MAINT: Remove sys.version checks
* `#15336 <https://github.com/numpy/numpy/pull/15336>`__: DEP: Deprecate ``->f->fastclip`` at registration time
* `#15338 <https://github.com/numpy/numpy/pull/15338>`__: DOC: document site.cfg.example
* `#15350 <https://github.com/numpy/numpy/pull/15350>`__: MAINT: Fix mistype in histogramdd docstring
* `#15351 <https://github.com/numpy/numpy/pull/15351>`__: DOC, BLD: reword release note, upgrade sphinx version
* `#15353 <https://github.com/numpy/numpy/pull/15353>`__: MAINT: Remove unnecessary calls to PyArray_DATA from binomial...
* `#15354 <https://github.com/numpy/numpy/pull/15354>`__: MAINT: Bump pytest from 5.3.2 to 5.3.3
* `#15355 <https://github.com/numpy/numpy/pull/15355>`__: MAINT: Const qualify UFunc inner loops
* `#15358 <https://github.com/numpy/numpy/pull/15358>`__: MAINT: Remove six
* `#15361 <https://github.com/numpy/numpy/pull/15361>`__: MAINT: Revise imports from collections.abc module
* `#15362 <https://github.com/numpy/numpy/pull/15362>`__: MAINT: remove internal functions required to handle Python2/3...
* `#15364 <https://github.com/numpy/numpy/pull/15364>`__: MAINT: Remove other uses of six module
* `#15366 <https://github.com/numpy/numpy/pull/15366>`__: MAINT: resolve pyflake F403 'from module import *' used
* `#15367 <https://github.com/numpy/numpy/pull/15367>`__: DOC: Fix Multithreaded Generation example docs
* `#15368 <https://github.com/numpy/numpy/pull/15368>`__: MAINT: Update tox for supported Python versions
* `#15369 <https://github.com/numpy/numpy/pull/15369>`__: MAINT: simd: Avoid signed comparison warning
* `#15370 <https://github.com/numpy/numpy/pull/15370>`__: DOC: Updating Chararry Buffer datatypes
* `#15373 <https://github.com/numpy/numpy/pull/15373>`__: MAINT: Remove sys.version checks
* `#15374 <https://github.com/numpy/numpy/pull/15374>`__: TST: Simplify unicode test
* `#15375 <https://github.com/numpy/numpy/pull/15375>`__: MAINT: Use ``with open`` when possible
* `#15377 <https://github.com/numpy/numpy/pull/15377>`__: MAINT: Cleanup python2 references
* `#15379 <https://github.com/numpy/numpy/pull/15379>`__: MAINT: Python2 Cleanups
* `#15381 <https://github.com/numpy/numpy/pull/15381>`__: DEP: add PendingDeprecation to matlib.py funky namespace
* `#15385 <https://github.com/numpy/numpy/pull/15385>`__: BUG, MAINT: Stop using the error-prone deprecated Py_UNICODE...
* `#15386 <https://github.com/numpy/numpy/pull/15386>`__: MAINT: clean up some macros in scalarapi.c
* `#15393 <https://github.com/numpy/numpy/pull/15393>`__: MAINT/BUG: Fixups to scalar base classes
* `#15397 <https://github.com/numpy/numpy/pull/15397>`__: BUG: np.load does not handle empty array with an empty descr
* `#15398 <https://github.com/numpy/numpy/pull/15398>`__: MAINT: Revise imports from urllib modules
* `#15399 <https://github.com/numpy/numpy/pull/15399>`__: MAINT: Remove Python3 DeprecationWarning from pytest.ini
* `#15400 <https://github.com/numpy/numpy/pull/15400>`__: MAINT: cleanup _pytesttester.py
* `#15401 <https://github.com/numpy/numpy/pull/15401>`__: BUG: Flags should not contain spaces
* `#15403 <https://github.com/numpy/numpy/pull/15403>`__: MAINT: Clean up, mostly unused imports.
* `#15405 <https://github.com/numpy/numpy/pull/15405>`__: BUG/TEST: core: Fix an undefined name in a test.
* `#15407 <https://github.com/numpy/numpy/pull/15407>`__: MAINT: Replace basestring with str.
* `#15408 <https://github.com/numpy/numpy/pull/15408>`__: ENH: Use AVX-512F for complex number arithmetic, absolute, square...
* `#15414 <https://github.com/numpy/numpy/pull/15414>`__: MAINT: Remove Python2 workarounds
* `#15415 <https://github.com/numpy/numpy/pull/15415>`__: MAINT: Revert f2py Python 2.6 workaround
* `#15417 <https://github.com/numpy/numpy/pull/15417>`__: MAINT: Cleanup references to python2
* `#15418 <https://github.com/numpy/numpy/pull/15418>`__: MAINT, DOC: Remove use of old Python __builtin__, now known as...
* `#15421 <https://github.com/numpy/numpy/pull/15421>`__: ENH: Make use of ExitStack in npyio.py
* `#15422 <https://github.com/numpy/numpy/pull/15422>`__: MAINT: Inline gentype_getreadbuf
* `#15423 <https://github.com/numpy/numpy/pull/15423>`__: MAINT: Use f-strings for clarity.
* `#15425 <https://github.com/numpy/numpy/pull/15425>`__: MAINT: dir(numpy) returns duplicate "testing"
* `#15426 <https://github.com/numpy/numpy/pull/15426>`__: MAINT: Use the PyArrayScalar_VAL macro where possible
* `#15427 <https://github.com/numpy/numpy/pull/15427>`__: DEP: Schedule unused C-API functions for removal/disabling
* `#15428 <https://github.com/numpy/numpy/pull/15428>`__: DOC: Improve ndarray.ctypes example
* `#15429 <https://github.com/numpy/numpy/pull/15429>`__: DOC: distutils: Add a docstring to show_config().
* `#15430 <https://github.com/numpy/numpy/pull/15430>`__: MAINT: Use contextmanager in _run_doctests
* `#15434 <https://github.com/numpy/numpy/pull/15434>`__: MAINT: Updated polynomial to use fstrings
* `#15435 <https://github.com/numpy/numpy/pull/15435>`__: DOC: Fix Incorrect document in Beginner Docs
* `#15436 <https://github.com/numpy/numpy/pull/15436>`__: MAINT: Update core.py with fstrings (issue #15420)
* `#15439 <https://github.com/numpy/numpy/pull/15439>`__: DOC: fix docstrings so ``python tools/refguide-check --rst <file>``...
* `#15441 <https://github.com/numpy/numpy/pull/15441>`__: MAINT: Tidy macros in scalar_new
* `#15444 <https://github.com/numpy/numpy/pull/15444>`__: MAINT: use 'yield from <expr>' for simple cases
* `#15445 <https://github.com/numpy/numpy/pull/15445>`__: MAINT: Bump pytest from 5.3.3 to 5.3.4
* `#15446 <https://github.com/numpy/numpy/pull/15446>`__: BUG: Reject nonsense arguments to scalar constructors
* `#15449 <https://github.com/numpy/numpy/pull/15449>`__: DOC: Update refguide_check note on how to skip code
* `#15451 <https://github.com/numpy/numpy/pull/15451>`__: MAINT: Simplify ``np.object_.__new__``
* `#15452 <https://github.com/numpy/numpy/pull/15452>`__: STY,MAINT: avoid 'multiple imports on one line'
* `#15463 <https://github.com/numpy/numpy/pull/15463>`__: ENH: expose ``bit_generator`` and random C-API to cython
* `#15464 <https://github.com/numpy/numpy/pull/15464>`__: MAINT: Cleanup duplicate line in refguide_check
* `#15465 <https://github.com/numpy/numpy/pull/15465>`__: MAINT: cleanup unused imports; avoid redefinition of imports
* `#15468 <https://github.com/numpy/numpy/pull/15468>`__: BUG: Fix for SVD not always sorted with hermitian=True
* `#15469 <https://github.com/numpy/numpy/pull/15469>`__: MAINT: Simplify scalar __new__ some more
* `#15474 <https://github.com/numpy/numpy/pull/15474>`__: MAINT: Eliminate messy _WORK macro
* `#15476 <https://github.com/numpy/numpy/pull/15476>`__: update result of rng.random(3) to current rng output
* `#15480 <https://github.com/numpy/numpy/pull/15480>`__: DOC: Correct get_state doc
* `#15482 <https://github.com/numpy/numpy/pull/15482>`__: MAINT: Use ``.identifier = val`` to fill type structs
* `#15483 <https://github.com/numpy/numpy/pull/15483>`__: [DOC] Mention behaviour of np.squeeze with one element
* `#15484 <https://github.com/numpy/numpy/pull/15484>`__: ENH: fixing generic error messages to be more specific in multiarray/descriptor.c
* `#15487 <https://github.com/numpy/numpy/pull/15487>`__: BUG: Fixing result of np quantile edge case
* `#15491 <https://github.com/numpy/numpy/pull/15491>`__: TST: mark the top 3 slowest tests to save ~10 seconds
* `#15493 <https://github.com/numpy/numpy/pull/15493>`__: MAINT: Bump pytest from 5.3.4 to 5.3.5
* `#15500 <https://github.com/numpy/numpy/pull/15500>`__: MAINT: Use True/False instead of 1/0 in np.dtype.__reduce__
* `#15503 <https://github.com/numpy/numpy/pull/15503>`__: MAINT: Do not allow ``copyswap`` and friends to fail silently
* `#15504 <https://github.com/numpy/numpy/pull/15504>`__: DOC: Remove duplicated code in true_divide docstring
* `#15505 <https://github.com/numpy/numpy/pull/15505>`__: NEP 40: Informational NEP about current DTypes
* `#15506 <https://github.com/numpy/numpy/pull/15506>`__: NEP 41: First steps towards improved Datatype Support
* `#15510 <https://github.com/numpy/numpy/pull/15510>`__: DOC: Update unique docstring example
* `#15511 <https://github.com/numpy/numpy/pull/15511>`__: MAINT: Large overhead in some random functions
* `#15516 <https://github.com/numpy/numpy/pull/15516>`__: TST: Fix missing output in refguide-check
* `#15521 <https://github.com/numpy/numpy/pull/15521>`__: MAINT: Simplify arraydescr_richcompare
* `#15522 <https://github.com/numpy/numpy/pull/15522>`__: MAINT: Fix internal misuses of ``NPY_TITLE_KEY``
* `#15524 <https://github.com/numpy/numpy/pull/15524>`__: DOC: Update instructions for building/archiving docs.
* `#15526 <https://github.com/numpy/numpy/pull/15526>`__: BUG: Fix inline assembly that detects cpu features on x86(32bit)
* `#15532 <https://github.com/numpy/numpy/pull/15532>`__: update doctests, small bugs and changes of repr
* `#15534 <https://github.com/numpy/numpy/pull/15534>`__: DEP: Do not allow "abstract" dtype conversion/creation
* `#15536 <https://github.com/numpy/numpy/pull/15536>`__: DOC: Minor copyediting on NEP 37.
* `#15538 <https://github.com/numpy/numpy/pull/15538>`__: MAINT: Extract repeated code to a helper function
* `#15543 <https://github.com/numpy/numpy/pull/15543>`__: NEP: edit and move NEP 38 to accepted status
* `#15547 <https://github.com/numpy/numpy/pull/15547>`__: MAINT: Refresh Doxyfile and modernize numpyfilter.py
* `#15549 <https://github.com/numpy/numpy/pull/15549>`__: TST: Accuracy test float32 sin/cos/exp/log for AVX platforms
* `#15550 <https://github.com/numpy/numpy/pull/15550>`__: DOC: Improve the ``numpy.linalg.eig`` docstring.
* `#15553 <https://github.com/numpy/numpy/pull/15553>`__: BUG: Added missing error check in ``ndarray.__contains__``
* `#15554 <https://github.com/numpy/numpy/pull/15554>`__: NEP 44 - Restructuring the NumPy Documentation
* `#15556 <https://github.com/numpy/numpy/pull/15556>`__: TST: (Travis CI) Use full python3-dbg path for virtual env creation
* `#15560 <https://github.com/numpy/numpy/pull/15560>`__: BUG, DOC: restore missing import
* `#15566 <https://github.com/numpy/numpy/pull/15566>`__: DOC: Removing bad practices from quick start + some PEP8
* `#15574 <https://github.com/numpy/numpy/pull/15574>`__: TST: Do not create symbolic link named gfortran.
* `#15575 <https://github.com/numpy/numpy/pull/15575>`__: DOC: Document caveat in random.uniform
* `#15577 <https://github.com/numpy/numpy/pull/15577>`__: TST: Test division by zero both with scalar and with array
* `#15579 <https://github.com/numpy/numpy/pull/15579>`__: DOC: numpy.clip is equivalent to minimum(..., maximum(...))
* `#15582 <https://github.com/numpy/numpy/pull/15582>`__: MAINT: Bump cython from 0.29.14 to 0.29.15
* `#15583 <https://github.com/numpy/numpy/pull/15583>`__: MAINT: Bump hypothesis from 5.3.0 to 5.5.4
* `#15585 <https://github.com/numpy/numpy/pull/15585>`__: BLD: manylinux2010 docker reports machine=i686
* `#15598 <https://github.com/numpy/numpy/pull/15598>`__: BUG: Ignore differences in NAN for computing ULP differences
* `#15600 <https://github.com/numpy/numpy/pull/15600>`__: TST: use manylinux2010 docker instead of ubuntu
* `#15610 <https://github.com/numpy/numpy/pull/15610>`__: TST: mask DeprecationWarning in xfailed test
* `#15612 <https://github.com/numpy/numpy/pull/15612>`__: BUG: Fix bug in AVX-512F np.maximum and np.minimum
* `#15614 <https://github.com/numpy/numpy/pull/15614>`__: DOC: Reword docstring for assert_equal
* `#15615 <https://github.com/numpy/numpy/pull/15615>`__: BUG: Remove check requiring natural alignment of float/double...
* `#15616 <https://github.com/numpy/numpy/pull/15616>`__: DOC: Add missing imports, definitions and dummy file
* `#15619 <https://github.com/numpy/numpy/pull/15619>`__: DOC: Fix documentation for apply_along_axis
* `#15624 <https://github.com/numpy/numpy/pull/15624>`__: DOC: fix printing, np., deprecation for refguide
* `#15631 <https://github.com/numpy/numpy/pull/15631>`__: MAINT: Pull identical line out of conditional.
* `#15633 <https://github.com/numpy/numpy/pull/15633>`__: DOC: remove broken link in f2py tutorial
* `#15639 <https://github.com/numpy/numpy/pull/15639>`__: BLD: update openblas download to new location, use manylinux2010-base
* `#15644 <https://github.com/numpy/numpy/pull/15644>`__: DOC: Update to clarify actual behavior real_if_(all elements)_close
* `#15648 <https://github.com/numpy/numpy/pull/15648>`__: MAINT: AVX512 implementation with intrinsic for float64 input...
* `#15653 <https://github.com/numpy/numpy/pull/15653>`__: BLD: update OpenBLAS to pre-0.3.9 version
* `#15662 <https://github.com/numpy/numpy/pull/15662>`__: DOC: Refactor ``np.polynomial`` docs using ``automodule``
* `#15665 <https://github.com/numpy/numpy/pull/15665>`__: BUG: fix doctest exception messages
* `#15672 <https://github.com/numpy/numpy/pull/15672>`__: MAINT: Added comment pointing FIXME to relevant PR.
* `#15673 <https://github.com/numpy/numpy/pull/15673>`__: DOC: Make extension module wording more clear
* `#15678 <https://github.com/numpy/numpy/pull/15678>`__: DOC: Improve np.finfo docs
* `#15680 <https://github.com/numpy/numpy/pull/15680>`__: DOC: Improve Benchmark README with environment setup and more...
* `#15682 <https://github.com/numpy/numpy/pull/15682>`__: MAINT: Bump hypothesis from 5.5.4 to 5.6.0
* `#15683 <https://github.com/numpy/numpy/pull/15683>`__: NEP: move NEP 44 to accepted status
* `#15685 <https://github.com/numpy/numpy/pull/15685>`__: ENH: Add ``subok`` parameter to np.copy function (cf. #6509)
* `#15694 <https://github.com/numpy/numpy/pull/15694>`__: DOC: Fix indexing docs to pass refguide
* `#15695 <https://github.com/numpy/numpy/pull/15695>`__: MAINT: Test during import to detect bugs with Accelerate(MacOS)...
* `#15696 <https://github.com/numpy/numpy/pull/15696>`__: MAINT: Add a fast path to var for complex input
* `#15701 <https://github.com/numpy/numpy/pull/15701>`__: MAINT: Convert shebang from python to python3 (#15687)
* `#15702 <https://github.com/numpy/numpy/pull/15702>`__: MAINT: replace optparse with argparse for 'doc' and 'tools' scripts
* `#15703 <https://github.com/numpy/numpy/pull/15703>`__: DOC: Fix quickstart doc to pass refguide
* `#15705 <https://github.com/numpy/numpy/pull/15705>`__: DOC: Change list to tuple in example description.
* `#15706 <https://github.com/numpy/numpy/pull/15706>`__: MAINT: Fixing typos in f2py comments and code.
* `#15710 <https://github.com/numpy/numpy/pull/15710>`__: DOC: fix SVD tutorial to pass refguide
* `#15714 <https://github.com/numpy/numpy/pull/15714>`__: MAINT: use list-based APIs to call subprocesses
* `#15715 <https://github.com/numpy/numpy/pull/15715>`__: ENH: update numpy.linalg.multi_dot to accept an ``out`` argument
* `#15716 <https://github.com/numpy/numpy/pull/15716>`__: TST: always use 'python -mpip' not 'pip'
* `#15717 <https://github.com/numpy/numpy/pull/15717>`__: DOC: update datetime reference to pass refguide
* `#15718 <https://github.com/numpy/numpy/pull/15718>`__: DOC: Fix coremath.rst to fix refguide_check
* `#15720 <https://github.com/numpy/numpy/pull/15720>`__: DOC: fix remaining doc files for refguide_check
* `#15723 <https://github.com/numpy/numpy/pull/15723>`__: BUG: fix logic error when nm fails on 32-bit
* `#15724 <https://github.com/numpy/numpy/pull/15724>`__: TST: Remove nose from the test_requirements.txt file.
* `#15733 <https://github.com/numpy/numpy/pull/15733>`__: DOC: Allow NEPs to link to python, numpy, scipy, and matplotlib...
* `#15735 <https://github.com/numpy/numpy/pull/15735>`__: DOC: LICENSE 2019 -> 2020
* `#15736 <https://github.com/numpy/numpy/pull/15736>`__: BUG: Guarantee array is in valid state after memory error occurs...
* `#15738 <https://github.com/numpy/numpy/pull/15738>`__: MAINT: Remove non-native byte order from _var check.
* `#15740 <https://github.com/numpy/numpy/pull/15740>`__: MAINT: Add better error handling in linalg.norm for vectors and...
* `#15745 <https://github.com/numpy/numpy/pull/15745>`__: MAINT: doc: Remove doc/summarize.py
* `#15747 <https://github.com/numpy/numpy/pull/15747>`__: BUG: lib: Handle axes with length 0 in np.unique.
* `#15749 <https://github.com/numpy/numpy/pull/15749>`__: DOC: document inconsistency between the shape of data and mask...
* `#15750 <https://github.com/numpy/numpy/pull/15750>`__: BUG, TST: fix f2py for PyPy, skip one test for PyPy
* `#15752 <https://github.com/numpy/numpy/pull/15752>`__: MAINT: Fix swig tests issue #15743
* `#15757 <https://github.com/numpy/numpy/pull/15757>`__: MAINT: CI: Add an explicit 'pr' section to azure-pipelines.yml
* `#15762 <https://github.com/numpy/numpy/pull/15762>`__: MAINT: Bump pytest from 5.3.5 to 5.4.1
* `#15766 <https://github.com/numpy/numpy/pull/15766>`__: BUG,MAINT: Remove incorrect special case in string to number...
* `#15768 <https://github.com/numpy/numpy/pull/15768>`__: REL: Update master after 1.18.2 release.
* `#15769 <https://github.com/numpy/numpy/pull/15769>`__: ENH: Allow toggling madvise hugepage and fix default
* `#15771 <https://github.com/numpy/numpy/pull/15771>`__: DOC: Fix runtests example in developer docs
* `#15773 <https://github.com/numpy/numpy/pull/15773>`__: DEP: Make issubdtype consistent for types and dtypes
* `#15774 <https://github.com/numpy/numpy/pull/15774>`__: MAINT: remove useless ``global`` statements
* `#15778 <https://github.com/numpy/numpy/pull/15778>`__: BLD: Add requirements.txt file for building docs
* `#15781 <https://github.com/numpy/numpy/pull/15781>`__: BUG: don't add 'public' or 'private' if the other one exists
* `#15784 <https://github.com/numpy/numpy/pull/15784>`__: ENH: Use TypeError in ``np.array`` for python consistency
* `#15794 <https://github.com/numpy/numpy/pull/15794>`__: BUG: Add basic __format__ for masked element to fix incorrect...
* `#15797 <https://github.com/numpy/numpy/pull/15797>`__: TST: Add unit test for out=None of np.einsum
* `#15799 <https://github.com/numpy/numpy/pull/15799>`__: MAINT: Cleanups to np.insert and np.delete
* `#15800 <https://github.com/numpy/numpy/pull/15800>`__: BUG: Add error-checking versions of strided casts.
* `#15802 <https://github.com/numpy/numpy/pull/15802>`__: DEP: Make ``np.insert`` and ``np.delete`` on 0d arrays with an axis...
* `#15803 <https://github.com/numpy/numpy/pull/15803>`__: DOC: correct possible list lengths for ``extobj`` in ufunc calls
* `#15804 <https://github.com/numpy/numpy/pull/15804>`__: DEP: Make np.delete on out-of-bounds indices an error
* `#15805 <https://github.com/numpy/numpy/pull/15805>`__: DEP: Forbid passing non-integral index arrays to ``insert`` and...
* `#15806 <https://github.com/numpy/numpy/pull/15806>`__: TST: Parametrize sort test
* `#15809 <https://github.com/numpy/numpy/pull/15809>`__: TST: switch PyPy job with CPython
* `#15812 <https://github.com/numpy/numpy/pull/15812>`__: TST: Remove code that is not supposed to warn out of warning...
* `#15815 <https://github.com/numpy/numpy/pull/15815>`__: DEP: Do not cast boolean indices to integers in np.delete
* `#15816 <https://github.com/numpy/numpy/pull/15816>`__: MAINT: simplify code that assumes str/unicode and int/long are...
* `#15827 <https://github.com/numpy/numpy/pull/15827>`__: BUG: Break on all errors when performing strided casts.
* `#15830 <https://github.com/numpy/numpy/pull/15830>`__: MAINT: pathlib and hashlib are in stdlib in Python 3.5+
* `#15832 <https://github.com/numpy/numpy/pull/15832>`__: ENH: improved error message ``IndexError: too many indices for``...
* `#15834 <https://github.com/numpy/numpy/pull/15834>`__: NEP: Add paragraph to NEP 41 about no array-object use and fix...
* `#15836 <https://github.com/numpy/numpy/pull/15836>`__: BUG: Fix IndexError for illegal axis in np.mean
* `#15839 <https://github.com/numpy/numpy/pull/15839>`__: DOC: Minor fix to _hist_bin_fd documentation
* `#15840 <https://github.com/numpy/numpy/pull/15840>`__: BUG,DEP: Make ``scalar.__round__()`` behave like pythons round
* `#15843 <https://github.com/numpy/numpy/pull/15843>`__: DOC: First steps towards docs restructuring (NEP 44)
* `#15848 <https://github.com/numpy/numpy/pull/15848>`__: DOC, TST: enable refguide_check in circleci
* `#15850 <https://github.com/numpy/numpy/pull/15850>`__: DOC: fix typo in C-API reference
* `#15854 <https://github.com/numpy/numpy/pull/15854>`__: DOC: Fix docstring for _hist_bin_auto.
* `#15866 <https://github.com/numpy/numpy/pull/15866>`__: MAINT: Bump cython from 0.29.15 to 0.29.16
* `#15867 <https://github.com/numpy/numpy/pull/15867>`__: DEP: Deprecate ndarray.tostring()
* `#15868 <https://github.com/numpy/numpy/pull/15868>`__: TST: use draft OpenBLAS build
* `#15870 <https://github.com/numpy/numpy/pull/15870>`__: ENH: Add keepdims argument to count_nonzero
* `#15872 <https://github.com/numpy/numpy/pull/15872>`__: BUG: Fix eigh and cholesky methods of numpy.random.multivariate_normal
* `#15876 <https://github.com/numpy/numpy/pull/15876>`__: BUG: Check that ``pvals`` is 1D in ``_generator.multinomial``.
* `#15877 <https://github.com/numpy/numpy/pull/15877>`__: DOC: Add missing signature from nditer docstring
* `#15881 <https://github.com/numpy/numpy/pull/15881>`__: BUG: Fix empty_like to respect shape=()
* `#15882 <https://github.com/numpy/numpy/pull/15882>`__: BUG: Do not ignore empty tuple of strides in ndarray.__new__
* `#15883 <https://github.com/numpy/numpy/pull/15883>`__: MAINT: Remove duplicated code in iotools.py
* `#15884 <https://github.com/numpy/numpy/pull/15884>`__: BUG: Setting a 0d array's strides to themselves should be legal
* `#15885 <https://github.com/numpy/numpy/pull/15885>`__: BUG: Respect itershape=() in nditer
* `#15887 <https://github.com/numpy/numpy/pull/15887>`__: MAINT: Clean-up 'next = __next__' used for Python 2 compatibility
* `#15891 <https://github.com/numpy/numpy/pull/15891>`__: DOC: Clarify docs on mixed advanced indexing and slicing
* `#15893 <https://github.com/numpy/numpy/pull/15893>`__: TST: Run test_large_zip in a child process
* `#15894 <https://github.com/numpy/numpy/pull/15894>`__: DOC: Add missing doc of numpy.ma.apply_over_axes in API list.
* `#15899 <https://github.com/numpy/numpy/pull/15899>`__: DOC: Improve record module documentation
* `#15901 <https://github.com/numpy/numpy/pull/15901>`__: DOC: Fixed order of items and link to mailing list in dev docs...
* `#15903 <https://github.com/numpy/numpy/pull/15903>`__: BLD: report clang version on macOS
* `#15904 <https://github.com/numpy/numpy/pull/15904>`__: MAINT: records: Remove private ``format_parser._descr`` attribute
* `#15907 <https://github.com/numpy/numpy/pull/15907>`__: DOC: Update documentation w.r.t. NPY_RELAXED_STRIDES_CHECKING
* `#15914 <https://github.com/numpy/numpy/pull/15914>`__: BUG: random: Disallow p=0 in negative_binomial
* `#15920 <https://github.com/numpy/numpy/pull/15920>`__: DOC: Improve docstring for numpy.linalg.lstsq
* `#15921 <https://github.com/numpy/numpy/pull/15921>`__: ENH: Use sysconfig instead of probing Makefile
* `#15928 <https://github.com/numpy/numpy/pull/15928>`__: DOC: Update np.copy docstring to include ragged case
* `#15931 <https://github.com/numpy/numpy/pull/15931>`__: DOC: Correct private function name to PyArray_AdaptFlexibleDType
* `#15936 <https://github.com/numpy/numpy/pull/15936>`__: MAINT: Fix capitalization in error message in ``mtrand.pyx``
* `#15938 <https://github.com/numpy/numpy/pull/15938>`__: BUG: Add _LARGE_FILES to def_macros[] when platform is AIX.
* `#15939 <https://github.com/numpy/numpy/pull/15939>`__: DOC: Update np.rollaxis docstring
* `#15949 <https://github.com/numpy/numpy/pull/15949>`__: BUG: fix AttributeError on accessing object in nested MaskedArray.
* `#15951 <https://github.com/numpy/numpy/pull/15951>`__: BUG: Alpha parameter must be 1D in ``generator.dirichlet``
* `#15953 <https://github.com/numpy/numpy/pull/15953>`__: NEP: minor maintenance, update filename and fix a cross-reference
* `#15964 <https://github.com/numpy/numpy/pull/15964>`__: MAINT: Bump hypothesis from 5.8.0 to 5.8.3
* `#15967 <https://github.com/numpy/numpy/pull/15967>`__: TST: Add slow_pypy support
* `#15968 <https://github.com/numpy/numpy/pull/15968>`__: DOC: Added note to angle function docstring about angle(0) being...
* `#15982 <https://github.com/numpy/numpy/pull/15982>`__: MAINT/BUG: Cleanup and minor fixes to conform_reduce_result
* `#15985 <https://github.com/numpy/numpy/pull/15985>`__: BUG: Avoid duplication in stack trace of ``linspace(a, b, num=1.5)``
* `#15988 <https://github.com/numpy/numpy/pull/15988>`__: BUG: Fix inf and NaN-warnings in half float ``nextafter``
* `#15989 <https://github.com/numpy/numpy/pull/15989>`__: MAINT: Remove 0d check for PyArray_ISONESEGMENT
* `#15990 <https://github.com/numpy/numpy/pull/15990>`__: DEV: Pass additional runtests.py args to ASV
* `#15991 <https://github.com/numpy/numpy/pull/15991>`__: BUG: max/min of a masked array dtype fix
* `#15993 <https://github.com/numpy/numpy/pull/15993>`__: DOC: Fix method documentation of function sort in MaskedArray
* `#16000 <https://github.com/numpy/numpy/pull/16000>`__: NEP: Improve Value Based Casting paragraph in NEP 40
* `#16001 <https://github.com/numpy/numpy/pull/16001>`__: DOC: add note on flatten ordering in matlab page
* `#16007 <https://github.com/numpy/numpy/pull/16007>`__: TST: Add tests for the conversion utilities
* `#16008 <https://github.com/numpy/numpy/pull/16008>`__: BUG: Unify handling of string enum converters
* `#16009 <https://github.com/numpy/numpy/pull/16009>`__: MAINT: Replace npyiter_order_converter with PyArray_OrderConverter
* `#16010 <https://github.com/numpy/numpy/pull/16010>`__: BUG: Fix lexsort axis check
* `#16011 <https://github.com/numpy/numpy/pull/16011>`__: DOC: Clarify single-segment arrays in np reference
* `#16014 <https://github.com/numpy/numpy/pull/16014>`__: DOC: Change import error "howto" to link to new troubleshooting...
* `#16015 <https://github.com/numpy/numpy/pull/16015>`__: DOC: update first section of NEP 37 (``__array_function__`` downsides)
* `#16021 <https://github.com/numpy/numpy/pull/16021>`__: REL: Update master after 1.18.3 release.
* `#16024 <https://github.com/numpy/numpy/pull/16024>`__: MAINT: Bump hypothesis from 5.8.3 to 5.10.1
* `#16025 <https://github.com/numpy/numpy/pull/16025>`__: DOC: initialise random number generator before first use in quickstart
* `#16032 <https://github.com/numpy/numpy/pull/16032>`__: ENH: Fix exception causes in build_clib.py
* `#16038 <https://github.com/numpy/numpy/pull/16038>`__: MAINT,TST: Move _repr_latex tests to test_printing.
* `#16041 <https://github.com/numpy/numpy/pull/16041>`__: BUG: missing 'f' prefix for fstring
* `#16042 <https://github.com/numpy/numpy/pull/16042>`__: ENH: Fix exception causes in build_ext.py
* `#16043 <https://github.com/numpy/numpy/pull/16043>`__: DOC: Add converters example to the loadtxt docstring
* `#16051 <https://github.com/numpy/numpy/pull/16051>`__: DOC: Add missing bracket
* `#16053 <https://github.com/numpy/numpy/pull/16053>`__: DOC: Small typo fixes to NEP 40.
* `#16054 <https://github.com/numpy/numpy/pull/16054>`__: DOC, BLD: update release howto and walkthrough for ananconda.org...
* `#16061 <https://github.com/numpy/numpy/pull/16061>`__: ENH: Chained exceptions in linalg.py and polyutils.py
* `#16064 <https://github.com/numpy/numpy/pull/16064>`__: MAINT: Chain exceptions in several places.
* `#16067 <https://github.com/numpy/numpy/pull/16067>`__: MAINT: Chain exceptions in memmap.py and core.py
* `#16068 <https://github.com/numpy/numpy/pull/16068>`__: BUG: Fix string to bool cast regression
* `#16069 <https://github.com/numpy/numpy/pull/16069>`__: DOC: Added page describing how to contribute to the docs team
* `#16075 <https://github.com/numpy/numpy/pull/16075>`__: DOC: add a note on sampling 2-D arrays to random.choice docstring
* `#16076 <https://github.com/numpy/numpy/pull/16076>`__: BUG: random: Generator.integers(2**32) always returned 0.
* `#16077 <https://github.com/numpy/numpy/pull/16077>`__: BLD: fix path to libgfortran on macOS
* `#16078 <https://github.com/numpy/numpy/pull/16078>`__: DOC: Add axis to random module "new or different" docs
* `#16079 <https://github.com/numpy/numpy/pull/16079>`__: DOC,BLD: Limit timeit iterations in random docs.
* `#16080 <https://github.com/numpy/numpy/pull/16080>`__: BUG: numpy.einsum indexing arrays now accept numpy int type
* `#16081 <https://github.com/numpy/numpy/pull/16081>`__: DOC: add note on type casting to numpy.left_shift().
* `#16083 <https://github.com/numpy/numpy/pull/16083>`__: DOC: improve development debugging doc
* `#16084 <https://github.com/numpy/numpy/pull/16084>`__: DOC: tweak neps/scope.rst
* `#16085 <https://github.com/numpy/numpy/pull/16085>`__: MAINT: Bump cython from 0.29.16 to 0.29.17
* `#16086 <https://github.com/numpy/numpy/pull/16086>`__: MAINT: Bump hypothesis from 5.10.1 to 5.10.4
* `#16094 <https://github.com/numpy/numpy/pull/16094>`__: TST: use latest released PyPy instead of nightly builds
* `#16097 <https://github.com/numpy/numpy/pull/16097>`__: MAINT, DOC: Improve grammar on a comment in the quickstart
* `#16100 <https://github.com/numpy/numpy/pull/16100>`__: NEP 41: Accept NEP 41 and add DType<->scalar duplication paragraph
* `#16101 <https://github.com/numpy/numpy/pull/16101>`__: BLD: put openblas library in local directory on windows
* `#16102 <https://github.com/numpy/numpy/pull/16102>`__: ENH: correct identity for logaddexp2 ufunc: -inf
* `#16113 <https://github.com/numpy/numpy/pull/16113>`__: MAINT: Fix random.PCG64 signature
* `#16119 <https://github.com/numpy/numpy/pull/16119>`__: DOC: Move misplaced news fragment for gh-13421
* `#16122 <https://github.com/numpy/numpy/pull/16122>`__: DOC: Fix links for NEP 40 in NEP 41
* `#16125 <https://github.com/numpy/numpy/pull/16125>`__: BUG: lib: Fix a problem with vectorize with default parameters.
* `#16128 <https://github.com/numpy/numpy/pull/16128>`__: ENH: Add equal_nan keyword argument to array_equal
* `#16129 <https://github.com/numpy/numpy/pull/16129>`__: ENH: Better error message when ``bins`` has float value in ``histogramdd``.
* `#16133 <https://github.com/numpy/numpy/pull/16133>`__: MAINT: Unify casting error creation (outside the iterator)
* `#16141 <https://github.com/numpy/numpy/pull/16141>`__: BENCH: Default to building HEAD instead of master
* `#16144 <https://github.com/numpy/numpy/pull/16144>`__: REL: Update master after NumPy 1.18.4 release
* `#16145 <https://github.com/numpy/numpy/pull/16145>`__: DOC: Add VSCode help link to importerror troubleshooting
* `#16147 <https://github.com/numpy/numpy/pull/16147>`__: CI: pin 32-bit manylinux2010 image tag
* `#16151 <https://github.com/numpy/numpy/pull/16151>`__: MAINT: Bump pytz from 2019.3 to 2020.1
* `#16153 <https://github.com/numpy/numpy/pull/16153>`__: BUG: Correct loop order in MT19937 jump
* `#16155 <https://github.com/numpy/numpy/pull/16155>`__: CI: unpin 32-bit manylinux2010 image tag
* `#16162 <https://github.com/numpy/numpy/pull/16162>`__: BUG: add missing numpy/__init__.pxd to the wheel
* `#16168 <https://github.com/numpy/numpy/pull/16168>`__: BUG:Umath remove unnecessary include of simd.inc in fast_loop_macro.h
* `#16169 <https://github.com/numpy/numpy/pull/16169>`__: DOC,BLD: Add :doc: to whitelisted roles in refguide_check.
* `#16170 <https://github.com/numpy/numpy/pull/16170>`__: ENH: resync numpy/__init__.pxd with upstream
* `#16171 <https://github.com/numpy/numpy/pull/16171>`__: ENH: allow choosing which manylinux artifact to download
* `#16173 <https://github.com/numpy/numpy/pull/16173>`__: MAINT: Mark tests as a subpackage rather than data.
* `#16182 <https://github.com/numpy/numpy/pull/16182>`__: Update Docs : point users of np.outer to np.multiply.outer
* `#16183 <https://github.com/numpy/numpy/pull/16183>`__: DOC: Fix link to numpy docs in README.
* `#16185 <https://github.com/numpy/numpy/pull/16185>`__: ENH: Allow pickle with protocol 5 when higher is requested
* `#16188 <https://github.com/numpy/numpy/pull/16188>`__: MAINT: cleanups to _iotools.StringConverter
* `#16197 <https://github.com/numpy/numpy/pull/16197>`__: DOC: Unify cross-references between array joining methods
* `#16199 <https://github.com/numpy/numpy/pull/16199>`__: DOC: Improve docstring of ``numpy.core.records``
* `#16201 <https://github.com/numpy/numpy/pull/16201>`__: DOC: update Code of Conduct committee
* `#16203 <https://github.com/numpy/numpy/pull/16203>`__: MAINT: Bump hypothesis from 5.10.4 to 5.12.0
* `#16204 <https://github.com/numpy/numpy/pull/16204>`__: MAINT: Bump pytest from 5.4.1 to 5.4.2
* `#16210 <https://github.com/numpy/numpy/pull/16210>`__: DOC: warn about runtime of shares_memory
* `#16213 <https://github.com/numpy/numpy/pull/16213>`__: ENH: backport scipy changes to openblas download script
* `#16214 <https://github.com/numpy/numpy/pull/16214>`__: BUG: skip complex256 arcsinh precision test on glibc2.17
* `#16215 <https://github.com/numpy/numpy/pull/16215>`__: MAINT: Chain exceptions and use NameError in np.bmat
* `#16216 <https://github.com/numpy/numpy/pull/16216>`__: DOC,BLD: pin sphinx to <3.0 in doc_requirements.txt
* `#16223 <https://github.com/numpy/numpy/pull/16223>`__: BUG: fix signature of PyArray_SearchSorted in __init__.pxd
* `#16224 <https://github.com/numpy/numpy/pull/16224>`__: ENH: add manylinux1 openblas hashes
* `#16226 <https://github.com/numpy/numpy/pull/16226>`__: DOC: Fix Generator.choice docstring
* `#16227 <https://github.com/numpy/numpy/pull/16227>`__: DOC: Add PyDev instructions to troubleshooting doc
* `#16228 <https://github.com/numpy/numpy/pull/16228>`__: DOC: Add Clang and MSVC to supported compilers list
* `#16240 <https://github.com/numpy/numpy/pull/16240>`__: DOC: Warn about behavior of ptp with signed integers.
* `#16258 <https://github.com/numpy/numpy/pull/16258>`__: DOC: Update the f2py section of the "Using Python as Glue" page.
* `#16263 <https://github.com/numpy/numpy/pull/16263>`__: BUG: Add missing decref in fromarray error path
* `#16265 <https://github.com/numpy/numpy/pull/16265>`__: ENH: Add tool for downloading release wheels from Anaconda.
* `#16269 <https://github.com/numpy/numpy/pull/16269>`__: DOC: Fix typos and cosmetic issues
* `#16280 <https://github.com/numpy/numpy/pull/16280>`__: REL: Prepare for the 1.19.0 release
* `#16293 <https://github.com/numpy/numpy/pull/16293>`__: BUG: Fix tools/download-wheels.py.
* `#16301 <https://github.com/numpy/numpy/pull/16301>`__: BUG: Require Python >= 3.6 in setup.py
* `#16312 <https://github.com/numpy/numpy/pull/16312>`__: BUG: relpath fails for different drives on windows
* `#16314 <https://github.com/numpy/numpy/pull/16314>`__: DOC: Fix documentation rendering,
* `#16341 <https://github.com/numpy/numpy/pull/16341>`__: BUG: Don't segfault on bad __len__ when assigning. (gh-16327)
* `#16342 <https://github.com/numpy/numpy/pull/16342>`__: MAINT: Stop Using PyEval_Call* and simplify some uses
* `#16343 <https://github.com/numpy/numpy/pull/16343>`__: BLD: Avoid "visibility attribute not supported" warning.
* `#16344 <https://github.com/numpy/numpy/pull/16344>`__: BUG: Allow attaching documentation twice in add_docstring
* `#16355 <https://github.com/numpy/numpy/pull/16355>`__: MAINT: Remove f-strings in setup.py. (gh-16346)
* `#16356 <https://github.com/numpy/numpy/pull/16356>`__: BUG: Indentation for docstrings
* `#16358 <https://github.com/numpy/numpy/pull/16358>`__: BUG: Fix dtype leak in ``PyArray_FromAny`` error path
* `#16383 <https://github.com/numpy/numpy/pull/16383>`__: ENH: Optimize Cpu feature detect in X86, fix for GCC on macOS...
* `#16398 <https://github.com/numpy/numpy/pull/16398>`__: MAINT: core: Use a raw string for the fromstring docstring.
* `#16399 <https://github.com/numpy/numpy/pull/16399>`__: MAINT: Make ctypes optional on Windows
* `#16400 <https://github.com/numpy/numpy/pull/16400>`__: BUG: Fix small leaks in error path and ``empty_like`` with shape
* `#16402 <https://github.com/numpy/numpy/pull/16402>`__: TST, MAINT: Fix detecting and testing armhf features
* `#16412 <https://github.com/numpy/numpy/pull/16412>`__: DOC,BLD: Update sphinx conf to use xelatex.
* `#16413 <https://github.com/numpy/numpy/pull/16413>`__: DOC,BLD: Update make dist html target.
* `#16414 <https://github.com/numpy/numpy/pull/16414>`__: MAINT, DOC: add index for user docs.
* `#16437 <https://github.com/numpy/numpy/pull/16437>`__: MAINT: support python 3.10
* `#16456 <https://github.com/numpy/numpy/pull/16456>`__: DOC: Fix troubleshooting code snippet when env vars are empty
* `#16457 <https://github.com/numpy/numpy/pull/16457>`__: REL: Prepare for the NumPy 1.19.0rc2 release.
* `#16526 <https://github.com/numpy/numpy/pull/16526>`__: MAINT:ARMHF Fix detecting feature groups NEON_HALF and NEON_VFPV4
* `#16527 <https://github.com/numpy/numpy/pull/16527>`__: BUG:random: Error when ``size`` is smaller than broadcast input...
* `#16528 <https://github.com/numpy/numpy/pull/16528>`__: BUG: fix GCC 10 major version comparison
* `#16563 <https://github.com/numpy/numpy/pull/16563>`__: BUG: Ensure SeedSequence 0-padding does not collide with spawn...
* `#16586 <https://github.com/numpy/numpy/pull/16586>`__: BUG: fix sin/cos bug when input is strided array
* `#16602 <https://github.com/numpy/numpy/pull/16602>`__: MAINT: Move and improve ``test_ignore_nan_ulperror``.
* `#16645 <https://github.com/numpy/numpy/pull/16645>`__: REL: Update 1.19.0-changelog.rst for 1.19.0 release.

Contributors
============

A total of 100 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alexey Brodkin +
* Allan Haldane
* Andras Deak +
* Andrew Lawson +
* Anna Chiara +
* Antoine Pitrou
* Bernhard M. Wiedemann +
* Bob Eldering +
* Brandon Carter
* CJ Carey
* Charles Harris
* Chris Lamb
* Christoph Boeddeker +
* Christoph Gohlke
* Daniel Hrisca +
* Daniel Smith
* Danny Hermes
* David Freese
* David Hagen
* David Linke +
* David Schaefer +
* Dillon Niederhut +
* Egor Panfilov +
* Emilien Kofman
* Eric Wieser
* Erik Bray +
* Erik Quaeghebeur +
* Garry Polley +
* Gunjan +
* Han Shen +
* Henke Adolfsson +
* Hidehiro NAGAOKA +
* Hemil Desai +
* Hong Xu +
* Iryna Shcherbina +
* Jaime Fernandez
* James Bourbeau +
* Jamie Townsend +
* Jarrod Millman
* Jean Helie +
* Jeroen Demeyer +
* John Goetz +
* John Kirkham
* John Zwinck
* Jonathan Helmus
* Joseph Fox-Rabinovitz
* Joseph Paul Cohen +
* Joshua Leahy +
* Julian Taylor
* Jörg Döpfert +
* Keno Goertz +
* Kevin Sheppard +
* Kexuan Sun +
* Konrad Kapp +
* Kristofor Maynard +
* Licht Takeuchi +
* Loïc Estève
* Lukas Mericle +
* Marten van Kerkwijk
* Matheus Portela +
* Matthew Brett
* Matti Picus
* Michael Lamparski +
* Michael Odintsov +
* Michael Schnaitter +
* Michael Seifert
* Mike Nolta
* Nathaniel J. Smith
* Nelle Varoquaux +
* Nicholas Del Grosso +
* Nico Schlömer +
* Oleg Zabluda +
* Oleksandr Pavlyk
* Pauli Virtanen
* Pim de Haan +
* Ralf Gommers
* Robert T. McGibbon +
* Roland Kaufmann
* Sebastian Berg
* Serhiy Storchaka +
* Shitian Ni +
* Spencer Hill +
* Srinivas Reddy Thatiparthy +
* Stefan Winkler +
* Stephan Hoyer
* Steven Maude +
* SuperBo +
* Thomas Köppe +
* Toon Verstraelen
* Vedant Misra +
* Warren Weckesser
* Wirawan Purwanto +
* Yang Li +
* Ziyan Zhou +
* chaoyu3 +
* orbit-stabilizer +
* solarjoe
* wufangjie +
* xoviat +
* Élie Gouzien +

Pull requests merged
====================

A total of 381 pull requests were merged for this release.

* `#5580 <https://github.com/numpy/numpy/pull/5580>`__: BUG, DEP: Fix masked arrays to properly edit views. ( #5558 )
* `#6053 <https://github.com/numpy/numpy/pull/6053>`__: MAINT: struct assignment "by field position", multi-field indices...
* `#7994 <https://github.com/numpy/numpy/pull/7994>`__: BUG: Allow 'shape': () in __array_interface__ regardless of the...
* `#8187 <https://github.com/numpy/numpy/pull/8187>`__: MAINT: Remove the unused keepdim argument from np.ufunc.accumulate
* `#8278 <https://github.com/numpy/numpy/pull/8278>`__: MAINT: Make the refactor suggested in prepare_index
* `#8557 <https://github.com/numpy/numpy/pull/8557>`__: ENH: add hermitian=False kwarg to np.linalg.matrix_rank
* `#8722 <https://github.com/numpy/numpy/pull/8722>`__: DOC: Clarifying the meaning of small values for `suppress` print...
* `#8827 <https://github.com/numpy/numpy/pull/8827>`__: BUG: Fix pinv for stacked matrices
* `#8920 <https://github.com/numpy/numpy/pull/8920>`__: ENH: use caching memory allocator in more places
* `#8934 <https://github.com/numpy/numpy/pull/8934>`__: MAINT: Use np.concatenate instead of np.vstack
* `#8977 <https://github.com/numpy/numpy/pull/8977>`__: BUG: Fix all kinds of problems when itemsize == 0
* `#8981 <https://github.com/numpy/numpy/pull/8981>`__: ENH: implement voidtype_repr and voidtype_str
* `#8983 <https://github.com/numpy/numpy/pull/8983>`__: ENH: fix str/repr for 0d-arrays and int* scalars
* `#9020 <https://github.com/numpy/numpy/pull/9020>`__: BUG: don't silence warnings in ufunc.reduce
* `#9025 <https://github.com/numpy/numpy/pull/9025>`__: ENH: np.save() to align data at 64 bytes
* `#9056 <https://github.com/numpy/numpy/pull/9056>`__: DOC: update structured array docs to reflect #6053
* `#9065 <https://github.com/numpy/numpy/pull/9065>`__: DEP: 0 should be passed to bincount, not None
* `#9083 <https://github.com/numpy/numpy/pull/9083>`__: MAINT: Improve error message from sorting with duplicate key
* `#9089 <https://github.com/numpy/numpy/pull/9089>`__: MAINT: refine error message for __array_ufunc__ not implemented
* `#9090 <https://github.com/numpy/numpy/pull/9090>`__: MAINT: Update master branch for 1.14.0 development.
* `#9092 <https://github.com/numpy/numpy/pull/9092>`__: BUG remove memory leak in array ufunc override.
* `#9096 <https://github.com/numpy/numpy/pull/9096>`__: ENH: Allow inplace also as keyword parameter for ndarray.byteswap
* `#9099 <https://github.com/numpy/numpy/pull/9099>`__: TST: fix test_basic failure on Windows
* `#9106 <https://github.com/numpy/numpy/pull/9106>`__: BUG: Array ufunc reduce out tuple
* `#9110 <https://github.com/numpy/numpy/pull/9110>`__: BUG: Do not elide complex abs()
* `#9112 <https://github.com/numpy/numpy/pull/9112>`__: BUG: ndarray.__pow__ does not check result of fast_scalar_power
* `#9113 <https://github.com/numpy/numpy/pull/9113>`__: BUG: delay calls of array repr in getlimits
* `#9115 <https://github.com/numpy/numpy/pull/9115>`__: BUG: Compilation crashes in MSVC when LIB or INCLUDE is not set
* `#9116 <https://github.com/numpy/numpy/pull/9116>`__: DOC: link to stack from column_stack
* `#9118 <https://github.com/numpy/numpy/pull/9118>`__: BUG: Fix reference count error of types when init multiarraymodule
* `#9119 <https://github.com/numpy/numpy/pull/9119>`__: BUG: Fix error handling on PyCapsule when initializing multiarraymodule
* `#9122 <https://github.com/numpy/numpy/pull/9122>`__: DOC: update 1.13 release note for MaskedArray, masked constants...
* `#9132 <https://github.com/numpy/numpy/pull/9132>`__: DEP: Deprecate incorrect behavior of expand_dims.
* `#9138 <https://github.com/numpy/numpy/pull/9138>`__: MAINT: Update .mailmap
* `#9139 <https://github.com/numpy/numpy/pull/9139>`__: ENH: remove unneeded spaces in float/bool reprs, fixes 0d str
* `#9141 <https://github.com/numpy/numpy/pull/9141>`__: DOC: Update ufunc documentation
* `#9142 <https://github.com/numpy/numpy/pull/9142>`__: BUG: set default type for empty index array to `numpy.intp`
* `#9149 <https://github.com/numpy/numpy/pull/9149>`__: DOC: Fix incorrect function signature in UFunc documentation.
* `#9151 <https://github.com/numpy/numpy/pull/9151>`__: DOC: better link display text for Developer Zone.
* `#9152 <https://github.com/numpy/numpy/pull/9152>`__: DOC: Fix some very minor spelling/grammar mistakes in docs
* `#9155 <https://github.com/numpy/numpy/pull/9155>`__: MAINT: Take out code that will never be executed
* `#9157 <https://github.com/numpy/numpy/pull/9157>`__: DOC: Fixed broken link to scipy developer zone
* `#9164 <https://github.com/numpy/numpy/pull/9164>`__: BUG: have as_strided() keep custom dtypes
* `#9167 <https://github.com/numpy/numpy/pull/9167>`__: BUG: ensure structured ndarray.__eq__,__ne__ defer when appropriate.
* `#9168 <https://github.com/numpy/numpy/pull/9168>`__: MAINT: Simplify if statement
* `#9174 <https://github.com/numpy/numpy/pull/9174>`__: BUG: allow pickling generic datetime
* `#9176 <https://github.com/numpy/numpy/pull/9176>`__: DOC: Update protocols in git development document.
* `#9181 <https://github.com/numpy/numpy/pull/9181>`__: COMPAT: PyPy calls clongdouble_int which raises a warning
* `#9195 <https://github.com/numpy/numpy/pull/9195>`__: BUG: pull request 9087 modifies a tuple after use
* `#9200 <https://github.com/numpy/numpy/pull/9200>`__: DOC: Update bincount docs to reflect gh-8348
* `#9201 <https://github.com/numpy/numpy/pull/9201>`__: BUG: Fix unicode(unicode_array_0d) on python 2.7
* `#9202 <https://github.com/numpy/numpy/pull/9202>`__: MAINT: Move ndarray.__str__ and ndarray.__repr__ to their own...
* `#9205 <https://github.com/numpy/numpy/pull/9205>`__: DOC: Remove all references to bigndarray in documentation.
* `#9209 <https://github.com/numpy/numpy/pull/9209>`__: ENH: Add an out argument to concatenate
* `#9212 <https://github.com/numpy/numpy/pull/9212>`__: MAINT: Combine similar branches
* `#9214 <https://github.com/numpy/numpy/pull/9214>`__: MAINT: Don't internally use the one-argument where
* `#9215 <https://github.com/numpy/numpy/pull/9215>`__: BUG: Avoid bare except clauses
* `#9217 <https://github.com/numpy/numpy/pull/9217>`__: BUG: handle resize of 0d array
* `#9218 <https://github.com/numpy/numpy/pull/9218>`__: BUG: Only propagate TypeError from where we throw it
* `#9219 <https://github.com/numpy/numpy/pull/9219>`__: DOC: Link to ufunc.outer from np.outer
* `#9220 <https://github.com/numpy/numpy/pull/9220>`__: MAINT: Factor out code duplicated by nanmedian and nanpercentile
* `#9226 <https://github.com/numpy/numpy/pull/9226>`__: DOC, ENH: Add 1.13.0-changelog.rst
* `#9238 <https://github.com/numpy/numpy/pull/9238>`__: DOC: BLD: fix lots of Sphinx warnings/errors.
* `#9241 <https://github.com/numpy/numpy/pull/9241>`__: MAINT: Fixup release notes, changelogs after #9238 merge.
* `#9242 <https://github.com/numpy/numpy/pull/9242>`__: BUG: Make 0-length dim handling of tensordot consistent with...
* `#9246 <https://github.com/numpy/numpy/pull/9246>`__: ENH: Release the GIL in einsum() special-cased loops
* `#9247 <https://github.com/numpy/numpy/pull/9247>`__: BUG: fix missing keyword rename for common block in numpy.f2py
* `#9253 <https://github.com/numpy/numpy/pull/9253>`__: DOC: Add isnat/positive ufunc to documentation.
* `#9259 <https://github.com/numpy/numpy/pull/9259>`__: MAINT: Use XOR for bool arrays in `np.diff`
* `#9260 <https://github.com/numpy/numpy/pull/9260>`__: BUG: don't elide into readonly and updateifcopy temporaries
* `#9264 <https://github.com/numpy/numpy/pull/9264>`__: DOC: some doc build maintenance and f2py doc updates
* `#9266 <https://github.com/numpy/numpy/pull/9266>`__: BUG: Fix unused variable in ufunc_object.c,
* `#9268 <https://github.com/numpy/numpy/pull/9268>`__: ENH: testing: load available nose plugins that are external to...
* `#9271 <https://github.com/numpy/numpy/pull/9271>`__: BUG: fix issue when using ``python setup.py somecommand --force``.
* `#9280 <https://github.com/numpy/numpy/pull/9280>`__: BUG: Make extensions compilable with MinGW on Py2.7
* `#9281 <https://github.com/numpy/numpy/pull/9281>`__: DOC: add @ operator in array vs. matrix comparison doc
* `#9285 <https://github.com/numpy/numpy/pull/9285>`__: BUG: Fix Intel compilation on Unix.
* `#9292 <https://github.com/numpy/numpy/pull/9292>`__: MAINT: Fix lgtm alerts
* `#9294 <https://github.com/numpy/numpy/pull/9294>`__: BUG: Fixes histogram monotonicity check for unsigned bin values
* `#9300 <https://github.com/numpy/numpy/pull/9300>`__: BUG: PyArray_CountNonzero does not check for exceptions
* `#9302 <https://github.com/numpy/numpy/pull/9302>`__: BUG: Fix fillvalue
* `#9306 <https://github.com/numpy/numpy/pull/9306>`__: BUG: f2py: Convert some error messages printed to stderr to exceptions.
* `#9310 <https://github.com/numpy/numpy/pull/9310>`__: BUG: fix wrong ndim used in empty where check
* `#9316 <https://github.com/numpy/numpy/pull/9316>`__: BUG: `runtest -t` should recognize development mode
* `#9320 <https://github.com/numpy/numpy/pull/9320>`__: DOC: Use x1 and x2 in the heaviside docstring.
* `#9322 <https://github.com/numpy/numpy/pull/9322>`__: BUG: np.ma.astype fails on structured types
* `#9323 <https://github.com/numpy/numpy/pull/9323>`__: DOC: Add $PARAMS to the isnat docstring
* `#9324 <https://github.com/numpy/numpy/pull/9324>`__: DOC: Fix missing asterisks in git development_setup doc page
* `#9325 <https://github.com/numpy/numpy/pull/9325>`__: DOC: add a NumFOCUS badge to README.md
* `#9332 <https://github.com/numpy/numpy/pull/9332>`__: ENH: fix 0d array printing using `str` or `formatter`.
* `#9335 <https://github.com/numpy/numpy/pull/9335>`__: BUG: umath: un-break ufunc where= when no out= is given
* `#9336 <https://github.com/numpy/numpy/pull/9336>`__: BUG: Fix various problems with the np.ma.masked constant
* `#9337 <https://github.com/numpy/numpy/pull/9337>`__: BUG: Prevent crash if ufunc doc string is null
* `#9341 <https://github.com/numpy/numpy/pull/9341>`__: BUG: np.resize discards empty shapes
* `#9343 <https://github.com/numpy/numpy/pull/9343>`__: BUG: recfunctions fail in a bunch of ways due to using .descr
* `#9344 <https://github.com/numpy/numpy/pull/9344>`__: DOC: fixes issue #9326, by removing the statement.
* `#9346 <https://github.com/numpy/numpy/pull/9346>`__: BUG: void masked fillvalue cannot be cast to void in python 3
* `#9354 <https://github.com/numpy/numpy/pull/9354>`__: BUG: Prevent hang traversing ufunc userloop linked list
* `#9357 <https://github.com/numpy/numpy/pull/9357>`__: DOC: Add examples for complex dtypes
* `#9361 <https://github.com/numpy/numpy/pull/9361>`__: DOC: isscalar add example for str
* `#9362 <https://github.com/numpy/numpy/pull/9362>`__: ENH: Rearrange testing module to isolate nose dependency.
* `#9364 <https://github.com/numpy/numpy/pull/9364>`__: BUG: ')' is printed at the end pointer of the buffer in numpy.f2py.
* `#9369 <https://github.com/numpy/numpy/pull/9369>`__: BUG: fix error in fromstring function from numpy.core.records
* `#9375 <https://github.com/numpy/numpy/pull/9375>`__: DOC: Document the internal workings of PY_ARRAY_UNIQUE_SYMBOL
* `#9380 <https://github.com/numpy/numpy/pull/9380>`__: DOC: Forward port 1.13.1 notes and changelog.
* `#9381 <https://github.com/numpy/numpy/pull/9381>`__: TST: test doc string of COMMON block arrays for numpy.f2py.
* `#9387 <https://github.com/numpy/numpy/pull/9387>`__: MAINT: Simplify code using PyArray_ISBYTESWAPPED macro.
* `#9388 <https://github.com/numpy/numpy/pull/9388>`__: MAINT: Use PyArray_ISBYTESWAPPED instead of !PyArray_ISNOTSWAPPED.
* `#9389 <https://github.com/numpy/numpy/pull/9389>`__: DOC: Fix reference, PyArray_DescrNew -> PyArray_NewFromDescr
* `#9392 <https://github.com/numpy/numpy/pull/9392>`__: DOC: UPDATEIFCOPY raises an error if not an array.
* `#9399 <https://github.com/numpy/numpy/pull/9399>`__: DOC: document how to free memory from PyArray_IntpConverter.
* `#9400 <https://github.com/numpy/numpy/pull/9400>`__: MAINT: Further unify handling of unnamed ufuncs
* `#9403 <https://github.com/numpy/numpy/pull/9403>`__: MAINT: Replace tab escapes with four spaces
* `#9407 <https://github.com/numpy/numpy/pull/9407>`__: DOC: add ``suppress_warnings`` to the testing routine listing.
* `#9408 <https://github.com/numpy/numpy/pull/9408>`__: BUG: various fixes to np.gradient
* `#9411 <https://github.com/numpy/numpy/pull/9411>`__: MAINT/BUG: improve gradient dtype handling
* `#9412 <https://github.com/numpy/numpy/pull/9412>`__: BUG: Check for exception in sort functions
* `#9422 <https://github.com/numpy/numpy/pull/9422>`__: DOC: correct formatting of basic.types.html
* `#9423 <https://github.com/numpy/numpy/pull/9423>`__: MAINT: change http to https for numfocus.org link in README
* `#9425 <https://github.com/numpy/numpy/pull/9425>`__: ENH: Einsum calls BLAS if it advantageous to do so
* `#9426 <https://github.com/numpy/numpy/pull/9426>`__: DOC: Add a link to einsum_path
* `#9431 <https://github.com/numpy/numpy/pull/9431>`__: ENH: distutils: make msvc + mingw-gfortran work
* `#9432 <https://github.com/numpy/numpy/pull/9432>`__: BUG: Fix loss of masks in masked 0d methods
* `#9433 <https://github.com/numpy/numpy/pull/9433>`__: BUG: make np.transpose return a view of the mask
* `#9434 <https://github.com/numpy/numpy/pull/9434>`__: MAINT: Remove unittest dependencies
* `#9437 <https://github.com/numpy/numpy/pull/9437>`__: DOC: Update 1.14.0 release notes.
* `#9446 <https://github.com/numpy/numpy/pull/9446>`__: BUG: Inlined functions must be defined somewhere.
* `#9447 <https://github.com/numpy/numpy/pull/9447>`__: API: Make ``a.flat.__array__`` return a copy when ``a`` non-contiguous.
* `#9452 <https://github.com/numpy/numpy/pull/9452>`__: MAINT: Use new-style classes on 2.7
* `#9454 <https://github.com/numpy/numpy/pull/9454>`__: MAINT: Remove branch in __array__ where if and else were the...
* `#9457 <https://github.com/numpy/numpy/pull/9457>`__: MAINT: Add a common subclass to all the masked ufunc wrappers
* `#9458 <https://github.com/numpy/numpy/pull/9458>`__: MAINT: Improve performance of np.copyto(where=scalar)
* `#9469 <https://github.com/numpy/numpy/pull/9469>`__: BUG: Fix true_divide when dtype=np.float64 specified.
* `#9470 <https://github.com/numpy/numpy/pull/9470>`__: MAINT: Make `setxor1d` a bit clearer and speed it up
* `#9471 <https://github.com/numpy/numpy/pull/9471>`__: BLD: remove -xhost flag from IntelFCompiler.
* `#9475 <https://github.com/numpy/numpy/pull/9475>`__: DEP: deprecate rollaxis
* `#9482 <https://github.com/numpy/numpy/pull/9482>`__: MAINT: Make diff iterative instead of recursive
* `#9487 <https://github.com/numpy/numpy/pull/9487>`__: DEP: Letting fromstring pretend to be frombuffer is a bad idea
* `#9490 <https://github.com/numpy/numpy/pull/9490>`__: DOC: Replace xrange by range in quickstart docs
* `#9491 <https://github.com/numpy/numpy/pull/9491>`__: TST: Add filter for new Py3K warning in python 2
* `#9492 <https://github.com/numpy/numpy/pull/9492>`__: ENH: Add np.polynomial.chebyshev.chebinterpolate function.
* `#9498 <https://github.com/numpy/numpy/pull/9498>`__: DOC: fix versionadded in docstring for moveaxis
* `#9499 <https://github.com/numpy/numpy/pull/9499>`__: MAINT/BUG: Improve error messages for dtype reassigment, fix...
* `#9503 <https://github.com/numpy/numpy/pull/9503>`__: MAINT: Move variables into deepest relevant scope, for clarity
* `#9505 <https://github.com/numpy/numpy/pull/9505>`__: BUG: issubdtype is inconsistent on types and dtypes
* `#9517 <https://github.com/numpy/numpy/pull/9517>`__: MAINT/DOC: Use builtin when np.{x} is builtins.{x}.
* `#9519 <https://github.com/numpy/numpy/pull/9519>`__: MAINT: Remove `level=` keyword from test arguments.
* `#9520 <https://github.com/numpy/numpy/pull/9520>`__: MAINT: types.TypeType does not ever need to be used
* `#9521 <https://github.com/numpy/numpy/pull/9521>`__: BUG: Make issubclass(np.number, numbers.Number) return true
* `#9522 <https://github.com/numpy/numpy/pull/9522>`__: BUG: Fix problems with obj2sctype
* `#9524 <https://github.com/numpy/numpy/pull/9524>`__: TST, MAINT: Add `__init__.py` files to tests directories.
* `#9527 <https://github.com/numpy/numpy/pull/9527>`__: BUG: Fix scalar methods to receive keyword arguments
* `#9529 <https://github.com/numpy/numpy/pull/9529>`__: BUG: The NAT deprecation warning should not be given for every...
* `#9536 <https://github.com/numpy/numpy/pull/9536>`__: ENH: Show domain and window as kwargs in repr
* `#9540 <https://github.com/numpy/numpy/pull/9540>`__: BUG: MaskedArray _optinfo dictionary is not updated when calling...
* `#9543 <https://github.com/numpy/numpy/pull/9543>`__: DOC: Adding backslash between double-backtick and s.
* `#9544 <https://github.com/numpy/numpy/pull/9544>`__: MAINT: Use the error_converting macro where possible
* `#9545 <https://github.com/numpy/numpy/pull/9545>`__: DEP: Deprecate the event argument to datetime types, which is...
* `#9550 <https://github.com/numpy/numpy/pull/9550>`__: DOC: removes broken docstring example (source code, png, pdf)...
* `#9552 <https://github.com/numpy/numpy/pull/9552>`__: DOC, BUG: Fix Python 3.6 invalid escape sequence.
* `#9554 <https://github.com/numpy/numpy/pull/9554>`__: BUG: fix regression in 1.13.x in distutils.mingw32ccompiler.
* `#9564 <https://github.com/numpy/numpy/pull/9564>`__: BUG: fix distutils/cpuinfo.py:getoutput()
* `#9574 <https://github.com/numpy/numpy/pull/9574>`__: BUG: deal with broken hypot() for MSVC on win32
* `#9575 <https://github.com/numpy/numpy/pull/9575>`__: BUG: deal with broken cabs*() for MSVC on win32
* `#9577 <https://github.com/numpy/numpy/pull/9577>`__: BUG: Missing dirichlet input validation
* `#9581 <https://github.com/numpy/numpy/pull/9581>`__: DOC: Fix link in numpy.ndarray.copy method (missing backticks)
* `#9582 <https://github.com/numpy/numpy/pull/9582>`__: ENH: Warn to change lstsq default for rcond
* `#9586 <https://github.com/numpy/numpy/pull/9586>`__: DOC: update example in np.nonzero docstring
* `#9588 <https://github.com/numpy/numpy/pull/9588>`__: MAINT: Remove direct access to flatiter attributes
* `#9590 <https://github.com/numpy/numpy/pull/9590>`__: ENH: Remove unnecessary restriction in noncen-f
* `#9591 <https://github.com/numpy/numpy/pull/9591>`__: MAINT: Remove unnecessary imports
* `#9599 <https://github.com/numpy/numpy/pull/9599>`__: BUG: fix infinite loop when creating np.pad on an empty array
* `#9601 <https://github.com/numpy/numpy/pull/9601>`__: DOC: rot90 wrongly positioned versionadded directive.
* `#9604 <https://github.com/numpy/numpy/pull/9604>`__: MAINT: Refactor the code used to compute sha256, md5 hashes
* `#9606 <https://github.com/numpy/numpy/pull/9606>`__: MAINT: Remove global statement in linalg.py
* `#9609 <https://github.com/numpy/numpy/pull/9609>`__: BUG: Add `__ne__` method to dummy_ctype class.
* `#9610 <https://github.com/numpy/numpy/pull/9610>`__: BUG: core: fix wrong method flags for scalartypes.c.src:gentype_copy
* `#9611 <https://github.com/numpy/numpy/pull/9611>`__: MAINT: remove try..except clause.
* `#9613 <https://github.com/numpy/numpy/pull/9613>`__: DOC: Update release notes for noncentral_f changes.
* `#9614 <https://github.com/numpy/numpy/pull/9614>`__: MAINT: Fix a comment regarding the formula for arange length
* `#9618 <https://github.com/numpy/numpy/pull/9618>`__: DOC: Fix type definitions in mtrand
* `#9619 <https://github.com/numpy/numpy/pull/9619>`__: ENH: Allow Fortran arrays of dimension 0
* `#9624 <https://github.com/numpy/numpy/pull/9624>`__: BUG: memory leak in np.dot of size 0
* `#9626 <https://github.com/numpy/numpy/pull/9626>`__: BUG: Fix broken runtests '-t' option.
* `#9629 <https://github.com/numpy/numpy/pull/9629>`__: BUG: test, fix issue #9620 __radd__ in char scalars
* `#9630 <https://github.com/numpy/numpy/pull/9630>`__: DOC: Updates order of parameters in save docstring
* `#9636 <https://github.com/numpy/numpy/pull/9636>`__: MAINT: Fix compiler warnings and update travis jobs
* `#9638 <https://github.com/numpy/numpy/pull/9638>`__: BUG: ensure consistent result dtype of count_nonzero
* `#9639 <https://github.com/numpy/numpy/pull/9639>`__: MAINT: Refactor updateifcopy
* `#9640 <https://github.com/numpy/numpy/pull/9640>`__: BUG: fix padding an empty array in reflect mode.
* `#9643 <https://github.com/numpy/numpy/pull/9643>`__: DOC: add new steering council members.
* `#9645 <https://github.com/numpy/numpy/pull/9645>`__: ENH: enable OpenBLAS on windows.
* `#9648 <https://github.com/numpy/numpy/pull/9648>`__: DOC: Correct the signature in pad doc for callable mode.
* `#9649 <https://github.com/numpy/numpy/pull/9649>`__: DOC: Fixed doc example of apply along axis with 3D return
* `#9652 <https://github.com/numpy/numpy/pull/9652>`__: BUG: Make system_info output reproducible
* `#9658 <https://github.com/numpy/numpy/pull/9658>`__: BUG: Fix usage of keyword "from" as argument name for "can_cast".
* `#9667 <https://github.com/numpy/numpy/pull/9667>`__: MAINT: Simplify block implementation
* `#9668 <https://github.com/numpy/numpy/pull/9668>`__: DOC: clarify wording in tutorial
* `#9672 <https://github.com/numpy/numpy/pull/9672>`__: BUG: dot/matmul 'out' arg should accept any ndarray subclass
* `#9681 <https://github.com/numpy/numpy/pull/9681>`__: MAINT: Add block benchmarks
* `#9682 <https://github.com/numpy/numpy/pull/9682>`__: DOC: Add whitespace after "versionadded::" directive so it actually...
* `#9683 <https://github.com/numpy/numpy/pull/9683>`__: DOC: Add polyutils subpackage to reference documentation
* `#9685 <https://github.com/numpy/numpy/pull/9685>`__: BUG: Fixes #7395, operator.index now fails on numpy.bool_
* `#9688 <https://github.com/numpy/numpy/pull/9688>`__: MAINT: rework recursive guard to keep array2string signature
* `#9691 <https://github.com/numpy/numpy/pull/9691>`__: PEP 3141 numbers should be considered scalars
* `#9692 <https://github.com/numpy/numpy/pull/9692>`__: ENH: Add support of ARC architecture
* `#9695 <https://github.com/numpy/numpy/pull/9695>`__: DOC: `start` is not needed even when `step` is given.
* `#9700 <https://github.com/numpy/numpy/pull/9700>`__: DOC: Add mandatory memo argument to __deepcopy__ method documentation
* `#9701 <https://github.com/numpy/numpy/pull/9701>`__: DOC: Add keepdims argument for ndarray.max documentation
* `#9702 <https://github.com/numpy/numpy/pull/9702>`__: DOC: Warn about the difference between np.remainder and math.remainder
* `#9703 <https://github.com/numpy/numpy/pull/9703>`__: DOC: Fix mistaken word in nanprod docstring
* `#9707 <https://github.com/numpy/numpy/pull/9707>`__: MAINT: When linspace's step is a NumPy scalar, do multiplication in-place
* `#9709 <https://github.com/numpy/numpy/pull/9709>`__: DOC: allclose doesn't require matching shapes
* `#9711 <https://github.com/numpy/numpy/pull/9711>`__: BUG: Make scalar function elision check if writeable.
* `#9715 <https://github.com/numpy/numpy/pull/9715>`__: MAINT: Fix typo "Porland" -> "Portland" in `building` doc.
* `#9718 <https://github.com/numpy/numpy/pull/9718>`__: DEP: Deprecate truth testing on empty arrays
* `#9720 <https://github.com/numpy/numpy/pull/9720>`__: MAINT: Remove unnecessary special-casing of scalars in isclose
* `#9724 <https://github.com/numpy/numpy/pull/9724>`__: BUG: adjust gfortran version search regex
* `#9725 <https://github.com/numpy/numpy/pull/9725>`__: MAINT: cleanup circular import b/w arrayprint.py,numeric.py
* `#9726 <https://github.com/numpy/numpy/pull/9726>`__: ENH: Better error message for savetxt when X.ndim > 2 or X.ndim...
* `#9737 <https://github.com/numpy/numpy/pull/9737>`__: MAINT: Use zip, not enumerate
* `#9740 <https://github.com/numpy/numpy/pull/9740>`__: BUG: Ensure `_npy_scaled_cexp{,f,l}` is defined when needed.
* `#9741 <https://github.com/numpy/numpy/pull/9741>`__: BUG: core: use npy_cabs for abs() for np.complex* scalar types
* `#9743 <https://github.com/numpy/numpy/pull/9743>`__: MAINT: Use PyArray_CHKFLAGS in more places.
* `#9749 <https://github.com/numpy/numpy/pull/9749>`__: BUG: Fix loss of precision for large values in long double divmod
* `#9752 <https://github.com/numpy/numpy/pull/9752>`__: BUG: Errors thrown by 0d arrays in setitem are silenced and replaced
* `#9753 <https://github.com/numpy/numpy/pull/9753>`__: DOC: Fix ndarray.__setstate__ documentation, it only takes one...
* `#9755 <https://github.com/numpy/numpy/pull/9755>`__: BUG: Cython 0.27 breaks NumPy on Python 3.
* `#9756 <https://github.com/numpy/numpy/pull/9756>`__: BUG/TST: Check if precision is lost in longcomplex
* `#9762 <https://github.com/numpy/numpy/pull/9762>`__: MAINT: Use the PyArray_(GET|SET)_ITEM functions where possible
* `#9768 <https://github.com/numpy/numpy/pull/9768>`__: MAINT: Cleanup `ma.array.__str__`
* `#9770 <https://github.com/numpy/numpy/pull/9770>`__: MAINT,BUG: Fix mtrand for Cython 0.27.
* `#9773 <https://github.com/numpy/numpy/pull/9773>`__: BUG: Fixes optimal einsum path for multi-term intermediates
* `#9778 <https://github.com/numpy/numpy/pull/9778>`__: BUG: can_cast(127, np.int8) is False
* `#9779 <https://github.com/numpy/numpy/pull/9779>`__: BUG: np.ma.trace gives the wrong result on ND arrays
* `#9780 <https://github.com/numpy/numpy/pull/9780>`__: MAINT: Make f2py generated file not contain the (local) date.
* `#9782 <https://github.com/numpy/numpy/pull/9782>`__: DOC: Update after NumPy 1.13.2 release.
* `#9784 <https://github.com/numpy/numpy/pull/9784>`__: BUG: remove voidtype-repr recursion in scalartypes.c/arrayprint.py
* `#9785 <https://github.com/numpy/numpy/pull/9785>`__: BUG: Fix size-checking in masked_where, and structured shrink_mask
* `#9792 <https://github.com/numpy/numpy/pull/9792>`__: ENH: Various improvements to Maskedarray repr
* `#9796 <https://github.com/numpy/numpy/pull/9796>`__: TST: linalg: add basic smoketest for cholesky
* `#9800 <https://github.com/numpy/numpy/pull/9800>`__: DOC: Clean up README
* `#9803 <https://github.com/numpy/numpy/pull/9803>`__: DOC: add missing underscore in set_printoptions
* `#9805 <https://github.com/numpy/numpy/pull/9805>`__: CI: set correct test mode for appveyor
* `#9806 <https://github.com/numpy/numpy/pull/9806>`__: MAINT: Add appveyor badge to README
* `#9807 <https://github.com/numpy/numpy/pull/9807>`__: MAINT: Make appveyor config a dot-file
* `#9810 <https://github.com/numpy/numpy/pull/9810>`__: DOC: Improve ndarray.shape documentation.
* `#9812 <https://github.com/numpy/numpy/pull/9812>`__: DOC: update scipy.integrate recommendation
* `#9814 <https://github.com/numpy/numpy/pull/9814>`__: BUG: Fix datetime->string conversion
* `#9815 <https://github.com/numpy/numpy/pull/9815>`__: BUG: fix stray comma in _array2string
* `#9817 <https://github.com/numpy/numpy/pull/9817>`__: BUG: Added exception for casting numpy.ma.masked to long
* `#9822 <https://github.com/numpy/numpy/pull/9822>`__: BUG: Allow subclasses of MaskedConstant to behave as unique singletons
* `#9824 <https://github.com/numpy/numpy/pull/9824>`__: BUG: Fixes for np.random.zipf
* `#9826 <https://github.com/numpy/numpy/pull/9826>`__: DOC: Add unravel_index examples to np.arg(min|max|sort)
* `#9828 <https://github.com/numpy/numpy/pull/9828>`__: DOC: Improve documentation of axis parameter in numpy.unpackbits()
* `#9835 <https://github.com/numpy/numpy/pull/9835>`__: BENCH: Added missing ufunc benchmarks
* `#9840 <https://github.com/numpy/numpy/pull/9840>`__: DOC: ndarray.__copy__ takes no arguments
* `#9842 <https://github.com/numpy/numpy/pull/9842>`__: BUG: Prevent invalid array shapes in seed
* `#9845 <https://github.com/numpy/numpy/pull/9845>`__: DOC: Refine SVD documentation
* `#9849 <https://github.com/numpy/numpy/pull/9849>`__: MAINT: Fix all special-casing of dtypes in `count_nonzero`
* `#9854 <https://github.com/numpy/numpy/pull/9854>`__: BLD: distutils: auto-find vcpkg include and library directories
* `#9856 <https://github.com/numpy/numpy/pull/9856>`__: BUG: Make bool(void_scalar) and void_scalar.astype(bool) consistent
* `#9858 <https://github.com/numpy/numpy/pull/9858>`__: DOC: Some minor fixes regarding import_array
* `#9862 <https://github.com/numpy/numpy/pull/9862>`__: BUG: Restore the environment variables when import multiarray...
* `#9863 <https://github.com/numpy/numpy/pull/9863>`__: ENH: Save to ZIP files without using temporary files.
* `#9865 <https://github.com/numpy/numpy/pull/9865>`__: DOC: Replace PyFITS reference with Astropy and PyTables with...
* `#9866 <https://github.com/numpy/numpy/pull/9866>`__: BUG: Fix runtests --benchmark-compare in python 3
* `#9868 <https://github.com/numpy/numpy/pull/9868>`__: DOC: Update arraypad to use np.pad in examples
* `#9869 <https://github.com/numpy/numpy/pull/9869>`__: DOC: Make qr options render correctly as list.
* `#9881 <https://github.com/numpy/numpy/pull/9881>`__: BUG: count_nonzero treats empty axis tuples strangely
* `#9883 <https://github.com/numpy/numpy/pull/9883>`__: ENH: Implement ndarray.__format__ for 0d arrays
* `#9884 <https://github.com/numpy/numpy/pull/9884>`__: BUG: Allow `unravel_index(0, ())` to return ()
* `#9887 <https://github.com/numpy/numpy/pull/9887>`__: BUG: add.reduce gives wrong results for arrays with funny strides
* `#9888 <https://github.com/numpy/numpy/pull/9888>`__: MAINT: Remove workarounds for gh-9527
* `#9889 <https://github.com/numpy/numpy/pull/9889>`__: MAINT: Tidy np.histogram, and improve error messages
* `#9893 <https://github.com/numpy/numpy/pull/9893>`__: ENH: Added compatibility for the NAG Fortran compiler, nagfor
* `#9896 <https://github.com/numpy/numpy/pull/9896>`__: DOC: Unindent enumeration in savetxt docstring
* `#9899 <https://github.com/numpy/numpy/pull/9899>`__: Remove unused isscalar imports, and incorrect documentation using...
* `#9900 <https://github.com/numpy/numpy/pull/9900>`__: MAINT/BUG: Remove special-casing for 0d arrays, now that indexing...
* `#9904 <https://github.com/numpy/numpy/pull/9904>`__: MAINT: Make warnings for nanmin and nanmax consistent
* `#9911 <https://github.com/numpy/numpy/pull/9911>`__: CI: travis: switch to container
* `#9912 <https://github.com/numpy/numpy/pull/9912>`__: BENCH: histogramming benchmarks
* `#9913 <https://github.com/numpy/numpy/pull/9913>`__: MAINT: Tidy up Maskedarray repr
* `#9916 <https://github.com/numpy/numpy/pull/9916>`__: DOC: Clarify behavior of genfromtxt names field
* `#9920 <https://github.com/numpy/numpy/pull/9920>`__: DOC: dot: Add explanation in case `b` has only 1 dimension.
* `#9925 <https://github.com/numpy/numpy/pull/9925>`__: DOC: ndarray.reshape allows shape as int arguments or tuple
* `#9930 <https://github.com/numpy/numpy/pull/9930>`__: MAINT: Add parameter checks to polynomial integration functions.
* `#9936 <https://github.com/numpy/numpy/pull/9936>`__: DOC: Clarify docstring for numpy.array_split
* `#9941 <https://github.com/numpy/numpy/pull/9941>`__: ENH: Use Dragon4 algorithm to print floating values
* `#9942 <https://github.com/numpy/numpy/pull/9942>`__: ENH: Add PGI flang compiler support for Windows
* `#9944 <https://github.com/numpy/numpy/pull/9944>`__: MAINT/BUG: Don't squash useful error messages in favor of generic...
* `#9945 <https://github.com/numpy/numpy/pull/9945>`__: DOC: fix operation plural in along axis glossary
* `#9946 <https://github.com/numpy/numpy/pull/9946>`__: DOC: describe the expansion of take and apply_along_axis in detail
* `#9947 <https://github.com/numpy/numpy/pull/9947>`__: MAINT/TST: Tidy dtype indexing
* `#9950 <https://github.com/numpy/numpy/pull/9950>`__: BUG: Passing an incorrect type to dtype.__getitem__ should raise...
* `#9952 <https://github.com/numpy/numpy/pull/9952>`__: ENH: add Decimal support to numpy.lib.financial
* `#9953 <https://github.com/numpy/numpy/pull/9953>`__: MAINT: Add a PyDataType_ISUNSIZED macro
* `#9957 <https://github.com/numpy/numpy/pull/9957>`__: DOC: update asv url
* `#9961 <https://github.com/numpy/numpy/pull/9961>`__: BUG: Allow float64('1e10000') to overflow
* `#9962 <https://github.com/numpy/numpy/pull/9962>`__: MAINT: Rename formatters to match scalar type names
* `#9965 <https://github.com/numpy/numpy/pull/9965>`__: BLD: Disable npymath whole program opt (LTCG) on win32
* `#9966 <https://github.com/numpy/numpy/pull/9966>`__: BUG: str(np.float) should print with the same number of digits...
* `#9967 <https://github.com/numpy/numpy/pull/9967>`__: MAINT: Separate correct `longdouble.__float__` from incorrect...
* `#9971 <https://github.com/numpy/numpy/pull/9971>`__: BUG: Fix casting from longdouble to long
* `#9973 <https://github.com/numpy/numpy/pull/9973>`__: TST: Fix error in test on PyPy, add comment explaining known...
* `#9976 <https://github.com/numpy/numpy/pull/9976>`__: BUG: Ensure lstsq can handle RHS with all sizes.
* `#9977 <https://github.com/numpy/numpy/pull/9977>`__: MAINT: distutils: trivial cleanups
* `#9978 <https://github.com/numpy/numpy/pull/9978>`__: BUG: cast to str_ should not convert to pure-python intermediate
* `#9983 <https://github.com/numpy/numpy/pull/9983>`__: ENH: let f2py discover location of libgfortran
* `#9985 <https://github.com/numpy/numpy/pull/9985>`__: ENH: skip NPY_ALLOW_C_API for UFUNC_ERR_IGNORE
* `#9986 <https://github.com/numpy/numpy/pull/9986>`__: MAINT: Remove similar branches from linalg.lstsq
* `#9991 <https://github.com/numpy/numpy/pull/9991>`__: MAINT: small robustness change for mingw support on Windows.
* `#9994 <https://github.com/numpy/numpy/pull/9994>`__: BUG: test was not using 'mode'
* `#9996 <https://github.com/numpy/numpy/pull/9996>`__: ENH: Adding `order=` keyword to `np.eye()`.
* `#9997 <https://github.com/numpy/numpy/pull/9997>`__: BUG: prototypes for [cz]dot[uc] are incorrect
* `#9999 <https://github.com/numpy/numpy/pull/9999>`__: ENH: Make `np.in1d()` work for unorderable object arrays
* `#10000 <https://github.com/numpy/numpy/pull/10000>`__: MAINT: Fix test_int_from_huge_longdouble on Darwin.
* `#10005 <https://github.com/numpy/numpy/pull/10005>`__: DOC: reword PyArray_DiscardWritebackIfCopy description
* `#10006 <https://github.com/numpy/numpy/pull/10006>`__: NEP: Drop Python2 support.
* `#10007 <https://github.com/numpy/numpy/pull/10007>`__: MAINT: simplify logic from #9983
* `#10008 <https://github.com/numpy/numpy/pull/10008>`__: MAINT: Backcompat fixes for dragon4 changes
* `#10011 <https://github.com/numpy/numpy/pull/10011>`__: TST: Group together all the nested_iter tests
* `#10017 <https://github.com/numpy/numpy/pull/10017>`__: REV: Undo bad rebase in 7fdfdd6a52fc0761c0d45931247c5ed2480224eb...
* `#10021 <https://github.com/numpy/numpy/pull/10021>`__: ENH: Don't show the boolean dtype in array_repr
* `#10022 <https://github.com/numpy/numpy/pull/10022>`__: MAINT: Update c-api version and hash for NumPy 1.14.
* `#10030 <https://github.com/numpy/numpy/pull/10030>`__: MAINT: Legacy mode specified as string, fix all-zeros legacy...
* `#10031 <https://github.com/numpy/numpy/pull/10031>`__: BUG: Fix f2py string variables in callbacks.
* `#10032 <https://github.com/numpy/numpy/pull/10032>`__: MAINT: Remove newline before dtype in repr of arrays
* `#10034 <https://github.com/numpy/numpy/pull/10034>`__: MAINT: legacy-printing-mode preserves 1.13 float & complex str
* `#10042 <https://github.com/numpy/numpy/pull/10042>`__: BUG: Allow `int` to be called on nested object arrays, fix `np.str_.__int__`
* `#10044 <https://github.com/numpy/numpy/pull/10044>`__: DEP: FutureWarning for void.item(): Will return bytes
* `#10049 <https://github.com/numpy/numpy/pull/10049>`__: DOC: Add copy of deprecated defindex.html template.
* `#10052 <https://github.com/numpy/numpy/pull/10052>`__: BUG: Fix legacy printing mode check.
* `#10053 <https://github.com/numpy/numpy/pull/10053>`__: STY: C style whitespace fixups
* `#10054 <https://github.com/numpy/numpy/pull/10054>`__: ENH: Add encoding option to numpy text IO.
* `#10055 <https://github.com/numpy/numpy/pull/10055>`__: BUG: Changed dump(a, F) so it would close file
* `#10057 <https://github.com/numpy/numpy/pull/10057>`__: DOC: v/h/dstack docstr shouldn't imply deprecation
* `#10065 <https://github.com/numpy/numpy/pull/10065>`__: DOC, BLD: Update site.cfg.example on the MKL part.
* `#10067 <https://github.com/numpy/numpy/pull/10067>`__: MAINT: Replace sphinx extension sphinx.ext.pngmath by sphinx.ext.imgmath.
* `#10068 <https://github.com/numpy/numpy/pull/10068>`__: BUG: Fix memory leak for subclass slicing
* `#10072 <https://github.com/numpy/numpy/pull/10072>`__: MAINT: Fix minor typos in numpy/core/fromnumeric.py
* `#10079 <https://github.com/numpy/numpy/pull/10079>`__: DOC: mention generalized ufuncs, document signature attribute
* `#10096 <https://github.com/numpy/numpy/pull/10096>`__: BUG: Fix assert_equal on time-like objects
* `#10097 <https://github.com/numpy/numpy/pull/10097>`__: BUG: Fix crash for 0d timedelta repr
* `#10101 <https://github.com/numpy/numpy/pull/10101>`__: BUG: Fix out-of-bounds access when handling rank-zero ndarrays.
* `#10105 <https://github.com/numpy/numpy/pull/10105>`__: DOC: Update license documentation.
* `#10108 <https://github.com/numpy/numpy/pull/10108>`__: DOC: Add documentation for datetime_data
* `#10109 <https://github.com/numpy/numpy/pull/10109>`__: DOC: fix the lack of np.
* `#10111 <https://github.com/numpy/numpy/pull/10111>`__: ENH: Improve alignment of datetime64 arrays containing NaT
* `#10112 <https://github.com/numpy/numpy/pull/10112>`__: MAINT: Simplify IntegerFormatter
* `#10113 <https://github.com/numpy/numpy/pull/10113>`__: BUG: Fix further out-of-bounds accesses when handling 0d ndarrays
* `#10114 <https://github.com/numpy/numpy/pull/10114>`__: MAINT: Remove duplicate cond check from assert_array_compare
* `#10116 <https://github.com/numpy/numpy/pull/10116>`__: BLD: [ipo] compilation error with intel compiler
* `#10120 <https://github.com/numpy/numpy/pull/10120>`__: BUG: stray comma should be preserved for legacy printing
* `#10121 <https://github.com/numpy/numpy/pull/10121>`__: DOC: Summarize printing changes in release notes
* `#10125 <https://github.com/numpy/numpy/pull/10125>`__: BLD: Add license file to NumPy wheels.
* `#10129 <https://github.com/numpy/numpy/pull/10129>`__: ENH: Strip trailing spaces from continuation in multiline arrayprint
* `#10130 <https://github.com/numpy/numpy/pull/10130>`__: MAINT: Simplify _leading_trailing
* `#10131 <https://github.com/numpy/numpy/pull/10131>`__: BUG: Fix downcasting in _array2string
* `#10136 <https://github.com/numpy/numpy/pull/10136>`__: BUG: edgeitems kwarg is ignored
* `#10143 <https://github.com/numpy/numpy/pull/10143>`__: MAINT: Combine legacy sections of _formatArray
* `#10159 <https://github.com/numpy/numpy/pull/10159>`__: DOC: Update 1.14 notes
* `#10160 <https://github.com/numpy/numpy/pull/10160>`__: BUG: test, fix problems from PR #9639
* `#10164 <https://github.com/numpy/numpy/pull/10164>`__: MAINT/BUG: Simplify _formatArray, fixing array_repr(matrix) in...
* `#10166 <https://github.com/numpy/numpy/pull/10166>`__: DOC: document PyArray_ResolveWritebackIfCopy
* `#10168 <https://github.com/numpy/numpy/pull/10168>`__: DOC: continuation of PyArray_ResolveIfCopy fixes
* `#10172 <https://github.com/numpy/numpy/pull/10172>`__: BUG: The last line of formatArray is not always wrapped correctly
* `#10175 <https://github.com/numpy/numpy/pull/10175>`__: BUG: linewidth was not respected for arrays other than 1d
* `#10176 <https://github.com/numpy/numpy/pull/10176>`__: ENH: add suffix option to array2str, wraps properly
* `#10177 <https://github.com/numpy/numpy/pull/10177>`__: MAINT, BUG: Final 1.14 formatting fixes
* `#10182 <https://github.com/numpy/numpy/pull/10182>`__: BUG: Extra space is inserted on first line for long elements
* `#10190 <https://github.com/numpy/numpy/pull/10190>`__: BUG: Fix regression in np.ma.load in gh-10055
* `#10200 <https://github.com/numpy/numpy/pull/10200>`__: BUG: Ufunc reduce reference leak (backport)
* `#10202 <https://github.com/numpy/numpy/pull/10202>`__: BUG: Fix bugs found by testing in release mode.
* `#10272 <https://github.com/numpy/numpy/pull/10272>`__: BUG: Align extra-dll folder name with auditwheel
* `#10275 <https://github.com/numpy/numpy/pull/10275>`__: BUG: fix duplicate message print
* `#10276 <https://github.com/numpy/numpy/pull/10276>`__: MAINT: Workaround for new travis sdist failures.
* `#10311 <https://github.com/numpy/numpy/pull/10311>`__: BUG: Make sure einsum default value of `optimize` is True.
* `#10312 <https://github.com/numpy/numpy/pull/10312>`__: BUG: Handle NaNs correctly in arange
* `#10313 <https://github.com/numpy/numpy/pull/10313>`__: BUG: Don't reimplement isclose in np.ma
* `#10315 <https://github.com/numpy/numpy/pull/10315>`__: DOC: NumPy 1.14.0 release prep.

Contributors
============

A total of 175 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* @8bitmp3 +
* @DWesl +
* @Endolith
* @Illviljan +
* @Lbogula +
* @Lisa +
* @Patrick +
* @Scian +
* @h-vetinari +
* @h6197627 +
* @jbCodeHub +
* @legoffant +
* @sfolje0 +
* @tautaus +
* @yetanothercheer +
* Abhay Raghuvanshi +
* Adrian Price-Whelan +
* Aerik Pawson +
* Agbonze Osazuwa +
* Aitik Gupta +
* Al-Baraa El-Hag
* Alex Henrie
* Alexander Hunt +
* Alizé Papp +
* Allan Haldane
* Amarnath1904 +
* Amrit Krishnan +
* Andras Deak
* AngelGris +
* Anne Archibald
* Anthony Vo +
* Antony Lee
* Atharva-Vidwans +
* Ayush Verma +
* Bas van Beek
* Bharat Raghunathan
* Bhargav V +
* Brian Soto
* Carl Michal +
* Charles Harris
* Charles Stern +
* Chiara Marmo +
* Chris Barnes +
* Chris Vavaliaris
* Christina Hedges +
* Christoph Gohlke
* Christopher Dahlin +
* Christos Efstathiou +
* Chunlin Fang
* Constanza Fierro +
* Daniel Evans +
* Daniel Montes +
* Dario Mory +
* David Carlier +
* David Stansby
* Deepyaman Datta +
* Derek Homeier
* Dong Keun Oh +
* Dylan Cutler +
* Eric Larson
* Eric Wieser
* Eva Jau +
* Evgeni Burovski
* FX Coudert +
* Faris A Chugthai +
* Filip Ter +
* Filip Trojan +
* François Le Lay +
* Ganesh Kathiresan
* Giannis Zapantis +
* Giulio Procopio +
* Greg Lucas +
* Hollow Man +
* Holly Corbett +
* I-Shen Leong +
* Inessa Pawson
* Isabela Presedo-Floyd
* Ismael Jimenez +
* Isuru Fernando
* Jakob Jakobson
* James Gerity +
* Jamie Macey +
* Jasmin Classen +
* Jody Klymak +
* Joseph Fox-Rabinovitz
* Jérome Eertmans +
* Jérôme Kieffer +
* Kamil Choudhury +
* Kasia Leszek +
* Keller Meier +
* Kenichi Maehashi
* Kevin Sheppard
* Kulin Seth +
* Kumud Lakara +
* Laura Kopf +
* Laura Martens +
* Leo Singer +
* Leonardus Chen +
* Lima Tango +
* Lumir Balhar +
* Maia Kaplan +
* Mainak Debnath +
* Marco Aurélio da Costa +
* Marta Lemanczyk +
* Marten van Kerkwijk
* Mary Conley +
* Marysia Winkels +
* Mateusz Sokół +
* Matt Haberland
* Matt Hall +
* Matt Ord +
* Matthew Badin +
* Matthias Bussonnier
* Matthias Geier
* Matti Picus
* Matías Ríos +
* Maxim Belkin +
* Melissa Weber Mendonça
* Meltem Eren Copur +
* Michael Dubravski +
* Michael Lamparski
* Michal W. Tarnowski +
* Michał Górny +
* Mike Boyle +
* Mike Toews
* Misal Raj +
* Mitchell Faas +
* Mukulikaa Parhari +
* Neil Girdhar +
* Nicholas McKibben +
* Nico Schlömer
* Nicolas Hug +
* Nilo Kruchelski +
* Nirjas Jakilim +
* Ohad Ravid +
* Olivier Grisel
* Pamphile ROY +
* Panos Mavrogiorgos +
* Patrick T. Komiske III +
* Pearu Peterson
* Peter Hawkins +
* Raghuveer Devulapalli
* Ralf Gommers
* Raúl Montón Pinillos +
* Rin Arakaki +
* Robert Kern
* Rohit Sanjay
* Roman Yurchak
* Ronan Lamy
* Ross Barnowski
* Ryan C Cooper
* Ryan Polley +
* Ryan Soklaski
* Sabrina Simao +
* Sayed Adel
* Sebastian Berg
* Shen Zhou +
* Stefan van der Walt
* Sylwester Arabas +
* Takanori Hirano
* Tania Allard +
* Thomas J. Fan +
* Thomas Orgis +
* Tim Hoffmann
* Tomoki, Karatsu +
* Tong Zou +
* Touqir Sajed +
* Tyler Reddy
* Wansoo Kim
* Warren Weckesser
* Weh Andreas +
* Yang Hau
* Yashasvi Misra +
* Zolboo Erdenebaatar +
* Zolisa Bleki

Pull requests merged
====================

A total of 581 pull requests were merged for this release.

* `#13578 <https://github.com/numpy/numpy/pull/13578>`__: DEP: Deprecate `data_type.dtype` if attribute is not already...
* `#15269 <https://github.com/numpy/numpy/pull/15269>`__: ENH: Implement faster keyword argument parsing capable of ``METH_FASTCALL``
* `#15271 <https://github.com/numpy/numpy/pull/15271>`__: ENH: Optimize and cleanup ufunc calls and ufunc CheckOverrides
* `#15392 <https://github.com/numpy/numpy/pull/15392>`__: BUG: Remove temporary change of descr/flags in VOID functions
* `#16164 <https://github.com/numpy/numpy/pull/16164>`__: DOC: Add more information about poly1d -> polynomial to reference...
* `#16241 <https://github.com/numpy/numpy/pull/16241>`__: ENH: Warn when reloading numpy or using numpy in sub-interpreter
* `#16370 <https://github.com/numpy/numpy/pull/16370>`__: DOC: Fix for building with sphinx 3
* `#16588 <https://github.com/numpy/numpy/pull/16588>`__: DOC: unify the docs for np.transpose and ndarray.transpose
* `#16818 <https://github.com/numpy/numpy/pull/16818>`__: DOC: added examples section for rfft2 and irfft2 docstring
* `#16855 <https://github.com/numpy/numpy/pull/16855>`__: DOC: Fix Typo (Wrong argument name)
* `#16987 <https://github.com/numpy/numpy/pull/16987>`__: ENH: Phase unwrapping generalized to arbitrary interval size
* `#17102 <https://github.com/numpy/numpy/pull/17102>`__: SIMD: Optimize the performance of np.packbits in AVX2/AVX512F/VSX.
* `#17122 <https://github.com/numpy/numpy/pull/17122>`__: MAINT: Use numpy version for f2py version.
* `#17492 <https://github.com/numpy/numpy/pull/17492>`__: DEP: Shift correlate mode parsing to C and deprecate inexact...
* `#17586 <https://github.com/numpy/numpy/pull/17586>`__: DEP: Formally deprecate `np.typeDict`
* `#17587 <https://github.com/numpy/numpy/pull/17587>`__: SIMD: Replace raw SIMD of sin/cos with NPYV(universal intrinsics)
* `#17636 <https://github.com/numpy/numpy/pull/17636>`__: MAINT: Bump pydata-sphinx-theme and set logo link to index
* `#17637 <https://github.com/numpy/numpy/pull/17637>`__: DOC: Add module template
* `#17719 <https://github.com/numpy/numpy/pull/17719>`__: ENH: Make `ndarray` generic w.r.t. its shape and dtype
* `#17727 <https://github.com/numpy/numpy/pull/17727>`__: ENH: Added libdivide for floor divide
* `#17736 <https://github.com/numpy/numpy/pull/17736>`__: BUG, Benchmark: fix passing optimization build options to asv
* `#17737 <https://github.com/numpy/numpy/pull/17737>`__: MAINT, Benchmark: print the supported CPU features during the...
* `#17778 <https://github.com/numpy/numpy/pull/17778>`__: ENH: Add annotations for comparison operations
* `#17782 <https://github.com/numpy/numpy/pull/17782>`__: SIMD: Optimize the performance of einsum's submodule multiply...
* `#17789 <https://github.com/numpy/numpy/pull/17789>`__: ENH, SIMD: Add new NPYV intrinsics pack(0)
* `#17790 <https://github.com/numpy/numpy/pull/17790>`__: ENH, SIMD: Add new NPYV intrinsics pack(1)
* `#17791 <https://github.com/numpy/numpy/pull/17791>`__: BLD: Enable Werror=undef in travis
* `#17792 <https://github.com/numpy/numpy/pull/17792>`__: ENH: add support for fujitsu compiler to numpy.
* `#17795 <https://github.com/numpy/numpy/pull/17795>`__: ENH: Add two new `_<X>Like` unions
* `#17817 <https://github.com/numpy/numpy/pull/17817>`__: BUG: Ignore fewer errors during array-coercion
* `#17836 <https://github.com/numpy/numpy/pull/17836>`__: MAINT: Add git rules to ignore all SIMD generated files
* `#17843 <https://github.com/numpy/numpy/pull/17843>`__: ENH: Add a mypy plugin for inferring platform-specific `np.number`...
* `#17847 <https://github.com/numpy/numpy/pull/17847>`__: TST: use latest pypy37 not pypy36
* `#17852 <https://github.com/numpy/numpy/pull/17852>`__: DOC: Doc for deprecate_with_doc
* `#17853 <https://github.com/numpy/numpy/pull/17853>`__: DOC: Clarify docs of np.resize().
* `#17861 <https://github.com/numpy/numpy/pull/17861>`__: MAINT: Update master after 1.20.x branch.
* `#17862 <https://github.com/numpy/numpy/pull/17862>`__: Make it clearer that np.interp input must be monotonically increasing
* `#17863 <https://github.com/numpy/numpy/pull/17863>`__: MAINT: Implement new casting loops based on NEP 42 and 43
* `#17866 <https://github.com/numpy/numpy/pull/17866>`__: DOC: fix typo in glossary.rst
* `#17868 <https://github.com/numpy/numpy/pull/17868>`__: BUG, TST: use python-version not PYTHON_VERSION
* `#17872 <https://github.com/numpy/numpy/pull/17872>`__: DOC: update the release howto for oldest-supported-numpy
* `#17874 <https://github.com/numpy/numpy/pull/17874>`__: MAINT: clean up a spurious warning in numpy/typing/setup.py
* `#17875 <https://github.com/numpy/numpy/pull/17875>`__: DOC: Prepare for 1.20.0 release
* `#17876 <https://github.com/numpy/numpy/pull/17876>`__: DOC: fixed typo in np-indexing.png explaining [-2:] slice in...
* `#17877 <https://github.com/numpy/numpy/pull/17877>`__: BUG: Fix buffer readflag errors and small leaks
* `#17878 <https://github.com/numpy/numpy/pull/17878>`__: BUG: np.arange: Allow `stop` not `start` as sole kwargs.
* `#17881 <https://github.com/numpy/numpy/pull/17881>`__: MAINT: Bump hypothesis from 5.41.3 to 5.41.4
* `#17883 <https://github.com/numpy/numpy/pull/17883>`__: MAINT: Remove duplicate dictionary entry
* `#17884 <https://github.com/numpy/numpy/pull/17884>`__: BUG: numpy.putmask not respecting writeable flag
* `#17886 <https://github.com/numpy/numpy/pull/17886>`__: ENH: Timestamp development versions.
* `#17887 <https://github.com/numpy/numpy/pull/17887>`__: DOC: Update arraycreation
* `#17888 <https://github.com/numpy/numpy/pull/17888>`__: DOC: Correct sentence/statement composition
* `#17889 <https://github.com/numpy/numpy/pull/17889>`__: DOC: Rename basics to fundamentals + added description
* `#17895 <https://github.com/numpy/numpy/pull/17895>`__: MAINT: Remove remaining uses of Python 3.6.
* `#17896 <https://github.com/numpy/numpy/pull/17896>`__: ENH: Speed up default `where` in the reduce-like method
* `#17897 <https://github.com/numpy/numpy/pull/17897>`__: BUG: merging PR to use -Werror=undef broke another PR
* `#17900 <https://github.com/numpy/numpy/pull/17900>`__: DEP: Finalize unravel_index `dims` alias for `shape` keyword
* `#17906 <https://github.com/numpy/numpy/pull/17906>`__: BUG: Fix a MacOS build failure
* `#17907 <https://github.com/numpy/numpy/pull/17907>`__: BUG: 'bool' object has no attribute 'ndim'
* `#17912 <https://github.com/numpy/numpy/pull/17912>`__: BUG: remove stray '+' from f-string upgrade in numba/extending.py
* `#17914 <https://github.com/numpy/numpy/pull/17914>`__: DOC: Update release notes to mention `type(dtype) is not np.dtype`
* `#17920 <https://github.com/numpy/numpy/pull/17920>`__: NEP: Update NEP 42 and 43 according to the current implementation
* `#17921 <https://github.com/numpy/numpy/pull/17921>`__: BUG: Enforce high >= low on uniform number generators
* `#17929 <https://github.com/numpy/numpy/pull/17929>`__: MAINT: Replace `contextlib_nullcontext` with `contextlib.nullcontext`
* `#17934 <https://github.com/numpy/numpy/pull/17934>`__: DOC: Add information about leak checking and valgrind
* `#17936 <https://github.com/numpy/numpy/pull/17936>`__: TST: Fixed an issue where the typing tests would fail for comparison...
* `#17942 <https://github.com/numpy/numpy/pull/17942>`__: DOC: Clarify savez documentation of naming arrays in output file
* `#17943 <https://github.com/numpy/numpy/pull/17943>`__: [DOC]: Wrong length for underline in docstring.
* `#17945 <https://github.com/numpy/numpy/pull/17945>`__: MAINT: Bump hypothesis from 5.41.4 to 5.41.5
* `#17950 <https://github.com/numpy/numpy/pull/17950>`__: BUG: Removed empty String from Nag Compiler's Flags
* `#17953 <https://github.com/numpy/numpy/pull/17953>`__: NEP: Accept NEP 42 -- New and extensible DTypes
* `#17955 <https://github.com/numpy/numpy/pull/17955>`__: DOC: Replace {var} in docstrings type annotation with `scalar...
* `#17956 <https://github.com/numpy/numpy/pull/17956>`__: ENH: Use versioneer to manage numpy versions.
* `#17957 <https://github.com/numpy/numpy/pull/17957>`__: TST: Fix crosstalk issues with polynomial str tests.
* `#17958 <https://github.com/numpy/numpy/pull/17958>`__: MAINT: Optimize the performance of count_nonzero by using universal...
* `#17960 <https://github.com/numpy/numpy/pull/17960>`__: TST, BUILD: Add a native x86 baseline build running on ubuntu-20.04
* `#17962 <https://github.com/numpy/numpy/pull/17962>`__: TST: Ensure tests are not sensitive to execution order
* `#17966 <https://github.com/numpy/numpy/pull/17966>`__: BUG: Add missing decref to arange
* `#17968 <https://github.com/numpy/numpy/pull/17968>`__: ENH: Use more typevars in `np.dtype`
* `#17971 <https://github.com/numpy/numpy/pull/17971>`__: BUG, SIMD: Fix direactive check for AVX512BW of intrinsics npyv_tobits_*
* `#17973 <https://github.com/numpy/numpy/pull/17973>`__: DEP: Futurewarn on requiring __len__ on array-likes
* `#17974 <https://github.com/numpy/numpy/pull/17974>`__: BLD: Fixes for versioneer and setup.py sdist.
* `#17976 <https://github.com/numpy/numpy/pull/17976>`__: DOC: Add/remove spaces in snippets and re-format here and there
* `#17978 <https://github.com/numpy/numpy/pull/17978>`__: MAINT: Update test_requirements and release_requirements.
* `#17981 <https://github.com/numpy/numpy/pull/17981>`__: ENH: Add proper dtype-support to `np.flatiter`
* `#17985 <https://github.com/numpy/numpy/pull/17985>`__: ENH, SIMD: Ditching the old CPU dispatcher(Arithmetic)
* `#17992 <https://github.com/numpy/numpy/pull/17992>`__: DOC: Replace verbatim with reference to local parameter
* `#17993 <https://github.com/numpy/numpy/pull/17993>`__: [DOC] np.kron use double backticks for non-references
* `#17994 <https://github.com/numpy/numpy/pull/17994>`__: SIMD: Optimize the performance of einsum's submodule dot .
* `#17995 <https://github.com/numpy/numpy/pull/17995>`__: MAINT: Bump pytest from 6.0.2 to 6.2.0
* `#17996 <https://github.com/numpy/numpy/pull/17996>`__: MAINT: Update wheel requirement from <=0.35.1 to <0.36.3
* `#17997 <https://github.com/numpy/numpy/pull/17997>`__: MAINT: Bump hypothesis from 5.41.5 to 5.43.3
* `#17998 <https://github.com/numpy/numpy/pull/17998>`__: TST: ignore pytest warning
* `#17999 <https://github.com/numpy/numpy/pull/17999>`__: Replace Numpy with NumPy
* `#18001 <https://github.com/numpy/numpy/pull/18001>`__: BLD, BUG: Fix detecting aarch64 on macOS
* `#18002 <https://github.com/numpy/numpy/pull/18002>`__: DOC: Fix and extend the docstring for np.inner
* `#18007 <https://github.com/numpy/numpy/pull/18007>`__: DOC: Add a brief explanation of float printing
* `#18008 <https://github.com/numpy/numpy/pull/18008>`__: DOC: fix for doctests
* `#18011 <https://github.com/numpy/numpy/pull/18011>`__: BLD: update to OpenBLAS 0.3.13
* `#18012 <https://github.com/numpy/numpy/pull/18012>`__: SIMD: Optimize the performance of einsum's submodule sum.
* `#18014 <https://github.com/numpy/numpy/pull/18014>`__: DOC: random: add some examples for SeedSequence
* `#18027 <https://github.com/numpy/numpy/pull/18027>`__: DOC, MAINT: Minor fixes to refguide_check.py documentation.
* `#18030 <https://github.com/numpy/numpy/pull/18030>`__: BUG: make a variable volatile to work around clang compiler bug
* `#18031 <https://github.com/numpy/numpy/pull/18031>`__: DOC: Parameter name typo axes -> axis in numpy.fft._pocketfft.
* `#18032 <https://github.com/numpy/numpy/pull/18032>`__: ENH: Add annotations for `np.core.arrayprint`
* `#18034 <https://github.com/numpy/numpy/pull/18034>`__: DOC: Fix a couple of reference to verbatim and vice versa
* `#18042 <https://github.com/numpy/numpy/pull/18042>`__: MAINT: Add dist_info to "other" setup.py commands.
* `#18045 <https://github.com/numpy/numpy/pull/18045>`__: MAINT: Bump pytest from 6.2.0 to 6.2.1
* `#18046 <https://github.com/numpy/numpy/pull/18046>`__: TST: add back sdist test run
* `#18047 <https://github.com/numpy/numpy/pull/18047>`__: BLD,DOC: pin sphinx to 3.3.1
* `#18048 <https://github.com/numpy/numpy/pull/18048>`__: DOC: Update TESTS.rst.txt
* `#18050 <https://github.com/numpy/numpy/pull/18050>`__: MAINT: Add aliases for commonly used `ArrayLike` objects
* `#18051 <https://github.com/numpy/numpy/pull/18051>`__: DEP: deprecate np.testing.dec
* `#18052 <https://github.com/numpy/numpy/pull/18052>`__: BUG: Fix concatenation when the output is "S" or "U"
* `#18054 <https://github.com/numpy/numpy/pull/18054>`__: DOC: Update stack docstrings
* `#18057 <https://github.com/numpy/numpy/pull/18057>`__: BLD: ensure we give the right error message for old Python versions
* `#18062 <https://github.com/numpy/numpy/pull/18062>`__: DOC: add missing details to linalg.lstsq docstring
* `#18065 <https://github.com/numpy/numpy/pull/18065>`__: MAINT: CPUs that support unaligned access.
* `#18066 <https://github.com/numpy/numpy/pull/18066>`__: TST: Allow mypy output types to be specified via aliases
* `#18067 <https://github.com/numpy/numpy/pull/18067>`__: MAINT: Remove obsolete workaround to set ndarray.__hash__ = None
* `#18070 <https://github.com/numpy/numpy/pull/18070>`__: BUG: Fix unique handling of nan entries.
* `#18072 <https://github.com/numpy/numpy/pull/18072>`__: MAINT: crackfortran regex simplify
* `#18074 <https://github.com/numpy/numpy/pull/18074>`__: MAINT: exprtype regex simplify
* `#18075 <https://github.com/numpy/numpy/pull/18075>`__: ENH, SIMD: Dispatch for unsigned floor division
* `#18077 <https://github.com/numpy/numpy/pull/18077>`__: NEP: mark NEP 28 on website redesign as final
* `#18078 <https://github.com/numpy/numpy/pull/18078>`__: Fix build warnings in NEPs
* `#18079 <https://github.com/numpy/numpy/pull/18079>`__: MAINT: Bump sphinx from 3.3.1 to 3.4.1
* `#18080 <https://github.com/numpy/numpy/pull/18080>`__: MAINT: Bump pytz from 2020.4 to 2020.5
* `#18081 <https://github.com/numpy/numpy/pull/18081>`__: MAINT: Bump hypothesis from 5.43.3 to 5.43.4
* `#18082 <https://github.com/numpy/numpy/pull/18082>`__: DOC: roadmap update
* `#18083 <https://github.com/numpy/numpy/pull/18083>`__: MAINT: regex char class improve
* `#18084 <https://github.com/numpy/numpy/pull/18084>`__: NEP: NumPy sponsorship guidelines (NEP 46)
* `#18085 <https://github.com/numpy/numpy/pull/18085>`__: DOC: replace 'this platform' with the actual platform in the...
* `#18086 <https://github.com/numpy/numpy/pull/18086>`__: BUG, SIMD: Fix _simd module build for 64bit Arm/neon clang
* `#18088 <https://github.com/numpy/numpy/pull/18088>`__: DOC: Update reference to verbatim in a few location.
* `#18090 <https://github.com/numpy/numpy/pull/18090>`__: MAINT: multiline regex class simplify
* `#18091 <https://github.com/numpy/numpy/pull/18091>`__: DOC: Avoid using "set of" when talking about an ordered list.
* `#18097 <https://github.com/numpy/numpy/pull/18097>`__: NEP: update backwards compatibility and deprecation policy NEP
* `#18100 <https://github.com/numpy/numpy/pull/18100>`__: BUG, BLD: Generate the main dispatcher config header into the...
* `#18101 <https://github.com/numpy/numpy/pull/18101>`__: ENH: move exp, log, frexp, ldexp to SIMD dispatching
* `#18103 <https://github.com/numpy/numpy/pull/18103>`__: TST: Avoid changing odd tempfile names in tests' site.cfg
* `#18104 <https://github.com/numpy/numpy/pull/18104>`__: TST: Turn some tests with loops into parametrized tests.
* `#18109 <https://github.com/numpy/numpy/pull/18109>`__: MAINT: Fix exception cause in mingw32ccompiler.py
* `#18110 <https://github.com/numpy/numpy/pull/18110>`__: API: make piecewise subclass safe using use zeros_like.
* `#18111 <https://github.com/numpy/numpy/pull/18111>`__: MAINT: Bump hypothesis from 5.43.4 to 5.46.0
* `#18115 <https://github.com/numpy/numpy/pull/18115>`__: BUG: Fix promotion of half and string
* `#18116 <https://github.com/numpy/numpy/pull/18116>`__: DEP: Deprecate promotion of numbers and bool to string
* `#18118 <https://github.com/numpy/numpy/pull/18118>`__: BUG, MAINT: improve avx512 mask logical operations
* `#18126 <https://github.com/numpy/numpy/pull/18126>`__: REL: Update master after 1.19.5 release.
* `#18128 <https://github.com/numpy/numpy/pull/18128>`__: ENH: Add dtype support to the array comparison ops
* `#18136 <https://github.com/numpy/numpy/pull/18136>`__: ENH: Adding keyboard interrupt support for array creation
* `#18144 <https://github.com/numpy/numpy/pull/18144>`__: BLD: add found Cython version to check in cythonize.py
* `#18148 <https://github.com/numpy/numpy/pull/18148>`__: MAINT: Bump sphinx from 3.4.1 to 3.4.3
* `#18149 <https://github.com/numpy/numpy/pull/18149>`__: MAINT: Bump hypothesis from 5.46.0 to 6.0.0
* `#18150 <https://github.com/numpy/numpy/pull/18150>`__: BUG: Ensure too many advanced indices raises an exception
* `#18152 <https://github.com/numpy/numpy/pull/18152>`__: BUG: Promotion between strings and objects was assymetric
* `#18156 <https://github.com/numpy/numpy/pull/18156>`__: MAINT: Remove redundant null check before free
* `#18157 <https://github.com/numpy/numpy/pull/18157>`__: BUG: Initialize value of no_castable_output used in ufunc_loop_matches
* `#18161 <https://github.com/numpy/numpy/pull/18161>`__: MAINT: Make keyword arrays static
* `#18164 <https://github.com/numpy/numpy/pull/18164>`__: TST: add a pypy37 windows 64-bit build
* `#18166 <https://github.com/numpy/numpy/pull/18166>`__: Use sinus based formula for ``chebpts1``
* `#18169 <https://github.com/numpy/numpy/pull/18169>`__: ENH: cpu features detection implementation on FreeBSD ARM
* `#18173 <https://github.com/numpy/numpy/pull/18173>`__: TST: Clear the mypy cache before running any typing tests
* `#18174 <https://github.com/numpy/numpy/pull/18174>`__: MAINT: Changed the `NBitBase` variancy in `number` from co- to...
* `#18176 <https://github.com/numpy/numpy/pull/18176>`__: ENH: Improve performance of tril_indices and triu_indices
* `#18178 <https://github.com/numpy/numpy/pull/18178>`__: SIMD: add fast integer division intrinsics for all supported...
* `#18180 <https://github.com/numpy/numpy/pull/18180>`__: BUG: threads.h existence test requires GLIBC > 2.12.
* `#18181 <https://github.com/numpy/numpy/pull/18181>`__: ENH: [f2py] Add external attribute support.
* `#18182 <https://github.com/numpy/numpy/pull/18182>`__: MAINT: Bump hypothesis from 6.0.0 to 6.0.2
* `#18183 <https://github.com/numpy/numpy/pull/18183>`__: MAINT: Optimize numpy.count_nonzero for int types using SIMD...
* `#18184 <https://github.com/numpy/numpy/pull/18184>`__: BUG: Fix f2py bugs when wrapping F90 subroutines.
* `#18185 <https://github.com/numpy/numpy/pull/18185>`__: MAINT: Give the `_<X>Like` and `_ArrayLike<X>` type aliases a...
* `#18187 <https://github.com/numpy/numpy/pull/18187>`__: STY: unify imports in __init__.py
* `#18191 <https://github.com/numpy/numpy/pull/18191>`__: STY: Use explicit reexports for numpy.typing objects
* `#18193 <https://github.com/numpy/numpy/pull/18193>`__: MAINT: Fix typo in docstring example
* `#18194 <https://github.com/numpy/numpy/pull/18194>`__: MAINT: einsum: Optimize the sub function two-operands by using...
* `#18196 <https://github.com/numpy/numpy/pull/18196>`__: BLD: update OpenBLAS to af2b0d02
* `#18197 <https://github.com/numpy/numpy/pull/18197>`__: BUG: Keep ignoring most errors during array-protocol lookup
* `#18200 <https://github.com/numpy/numpy/pull/18200>`__: ENH: Add new intrinsics sum_u8/u16/u64.
* `#18204 <https://github.com/numpy/numpy/pull/18204>`__: TST: Speed up the typing tests
* `#18205 <https://github.com/numpy/numpy/pull/18205>`__: MAINT: Update pavement.py to work with versioneer.
* `#18208 <https://github.com/numpy/numpy/pull/18208>`__: TST: raise memory limit for test
* `#18210 <https://github.com/numpy/numpy/pull/18210>`__: DOC: typo in post-loop return
* `#18211 <https://github.com/numpy/numpy/pull/18211>`__: MAINT: random shuffle: warn on unrecognized objects, fix empty...
* `#18213 <https://github.com/numpy/numpy/pull/18213>`__: DOC: Formatting consistency.
* `#18214 <https://github.com/numpy/numpy/pull/18214>`__: DOC: Double backticks for inline code example.
* `#18217 <https://github.com/numpy/numpy/pull/18217>`__: MAINT: Ignore ComplexWarning in ``test_iter_copy_casts``.
* `#18221 <https://github.com/numpy/numpy/pull/18221>`__: DOC: Misc single to double backticks fixes.
* `#18223 <https://github.com/numpy/numpy/pull/18223>`__: DOC: Improve doc for numpy.random.Generator.choice
* `#18224 <https://github.com/numpy/numpy/pull/18224>`__: MAINT: Bump pydata-sphinx-theme from 0.4.1 to 0.4.2
* `#18225 <https://github.com/numpy/numpy/pull/18225>`__: MAINT: Bump mypy from 0.790 to 0.800
* `#18226 <https://github.com/numpy/numpy/pull/18226>`__: MAINT: Bump hypothesis from 6.0.2 to 6.0.3
* `#18227 <https://github.com/numpy/numpy/pull/18227>`__: MAINT: Bump pytest-cov from 2.10.1 to 2.11.1
* `#18228 <https://github.com/numpy/numpy/pull/18228>`__: ENH: Add dtype-support to the ufunc-based `ndarray` magic methods...
* `#18229 <https://github.com/numpy/numpy/pull/18229>`__: MAINT: Clean up all module-level dunders
* `#18230 <https://github.com/numpy/numpy/pull/18230>`__: DOC: Clarify the type alias deprecation message
* `#18232 <https://github.com/numpy/numpy/pull/18232>`__: DOC: lib/shape_base numpydoc formatting.
* `#18233 <https://github.com/numpy/numpy/pull/18233>`__: NEP: accept NEP 23 (backwards compatibility policy)
* `#18234 <https://github.com/numpy/numpy/pull/18234>`__: NEP: accept NEP 46 (sponsorship guidelines)
* `#18235 <https://github.com/numpy/numpy/pull/18235>`__: DOC: Fix command in "Writing custom array containers" guide
* `#18236 <https://github.com/numpy/numpy/pull/18236>`__: ENH: Add aliases for commonly used dtype-like objects
* `#18238 <https://github.com/numpy/numpy/pull/18238>`__: DOC: __array__ accepts a dtype argument
* `#18245 <https://github.com/numpy/numpy/pull/18245>`__: BLD: fix issue with `bdist_egg`, which made `make dist` in doc/...
* `#18247 <https://github.com/numpy/numpy/pull/18247>`__: DOC: Misc numpydoc format fixes
* `#18248 <https://github.com/numpy/numpy/pull/18248>`__: DOC: See also -> See Also (casing)
* `#18251 <https://github.com/numpy/numpy/pull/18251>`__: DOC: more misc fixes of syntax
* `#18252 <https://github.com/numpy/numpy/pull/18252>`__: DOC: cleanup of numpy/polynomial.
* `#18253 <https://github.com/numpy/numpy/pull/18253>`__: DOC: improve description of `_NoValue`
* `#18255 <https://github.com/numpy/numpy/pull/18255>`__: MAINT: add an 'apt update'
* `#18262 <https://github.com/numpy/numpy/pull/18262>`__: REL: Update master after 1.20.0 release.
* `#18263 <https://github.com/numpy/numpy/pull/18263>`__: ENH: Added sanity check to printoptions
* `#18264 <https://github.com/numpy/numpy/pull/18264>`__: BUG: Use C linkage for random distributions
* `#18269 <https://github.com/numpy/numpy/pull/18269>`__: DOC: Numpydoc format space before `:` in Parameters
* `#18272 <https://github.com/numpy/numpy/pull/18272>`__: DOC: Numpydoc warning incorrect underline length.
* `#18274 <https://github.com/numpy/numpy/pull/18274>`__: MAINT: Chain exceptions in linalg
* `#18275 <https://github.com/numpy/numpy/pull/18275>`__: MAINT: Bump hypothesis from 6.0.3 to 6.1.1
* `#18276 <https://github.com/numpy/numpy/pull/18276>`__: MAINT: Bump pytest from 6.2.1 to 6.2.2
* `#18277 <https://github.com/numpy/numpy/pull/18277>`__: MAINT: Bump pydata-sphinx-theme from 0.4.2 to 0.4.3
* `#18278 <https://github.com/numpy/numpy/pull/18278>`__: MAINT: defer the import of shutil
* `#18282 <https://github.com/numpy/numpy/pull/18282>`__: MAINT: gracefully shuffle memoryviews
* `#18284 <https://github.com/numpy/numpy/pull/18284>`__: ENH: Add annotations for the remaining `np.generic` aliases
* `#18285 <https://github.com/numpy/numpy/pull/18285>`__: TST: Pin `typing_extensions` to the latest version
* `#18289 <https://github.com/numpy/numpy/pull/18289>`__: MAINT: Move transferdata into buffer-wise struct
* `#18293 <https://github.com/numpy/numpy/pull/18293>`__: BUG: Fix typo in ``numpy.__init__.py``
* `#18295 <https://github.com/numpy/numpy/pull/18295>`__: BUG: don't mutate list of fake libraries while iterating over...
* `#18301 <https://github.com/numpy/numpy/pull/18301>`__: MAINT: avoid chaining exceptions in conv_template.py
* `#18302 <https://github.com/numpy/numpy/pull/18302>`__: MAINT: Add missing placeholder annotations
* `#18303 <https://github.com/numpy/numpy/pull/18303>`__: MAINT: Fix typo in PyArray_RegisterDataType error
* `#18307 <https://github.com/numpy/numpy/pull/18307>`__: DOC: Corrected numpy.power example.
* `#18313 <https://github.com/numpy/numpy/pull/18313>`__: Numpy logo fix on README
* `#18315 <https://github.com/numpy/numpy/pull/18315>`__: CI: rearrange Azure build jobs
* `#18317 <https://github.com/numpy/numpy/pull/18317>`__: MAINT: Fixed chain exception for array_split func
* `#18320 <https://github.com/numpy/numpy/pull/18320>`__: DOC: add links to polynomial function/class listing
* `#18322 <https://github.com/numpy/numpy/pull/18322>`__: ENH: Add a mypy plugin for exposing platform-specific extended-precision...
* `#18323 <https://github.com/numpy/numpy/pull/18323>`__: ENH: Add dtype-support to the ufunc-based `ndarray` magic methods...
* `#18324 <https://github.com/numpy/numpy/pull/18324>`__: MAINT: Avoid moveaxis overhead in median.
* `#18329 <https://github.com/numpy/numpy/pull/18329>`__: BUG: Allow unmodified use of isclose, allclose, etc. with timedelta
* `#18331 <https://github.com/numpy/numpy/pull/18331>`__: MAINT: Update openblas_support for macosx-arm64
* `#18332 <https://github.com/numpy/numpy/pull/18332>`__: BUG: Allow pickling all relevant DType types/classes
* `#18333 <https://github.com/numpy/numpy/pull/18333>`__: CI: fix when GitHub Actions builds trigger, and allow ci skips
* `#18334 <https://github.com/numpy/numpy/pull/18334>`__: TST: use setup-python action for pypy, disable win64 pypy
* `#18338 <https://github.com/numpy/numpy/pull/18338>`__: DOC: Fix whitespace before "last updated" on overview page
* `#18339 <https://github.com/numpy/numpy/pull/18339>`__: DOC: Discussion on the @ operator and the matrix class
* `#18340 <https://github.com/numpy/numpy/pull/18340>`__: DOC: remove pygments_style from conf.py
* `#18342 <https://github.com/numpy/numpy/pull/18342>`__: DOC: Specified all possible return types for trapz function #18140
* `#18344 <https://github.com/numpy/numpy/pull/18344>`__: DOC: Added sentence to docstring of histogram_bin_edges to explain...
* `#18346 <https://github.com/numpy/numpy/pull/18346>`__: DOC: Change license date 2020 -> 2021
* `#18347 <https://github.com/numpy/numpy/pull/18347>`__: MAINT: Delete unused "dst" clearing functions
* `#18348 <https://github.com/numpy/numpy/pull/18348>`__: DEP: doc-deprecate BLAS_SRC/LAPACK_SRC
* `#18349 <https://github.com/numpy/numpy/pull/18349>`__: CI: CircleCI seems to occasionally time out, increase the limit
* `#18350 <https://github.com/numpy/numpy/pull/18350>`__: BUG: Fix missing signed_char dependency.
* `#18361 <https://github.com/numpy/numpy/pull/18361>`__: ENH: Share memory of read-only intent(in) arrays.
* `#18362 <https://github.com/numpy/numpy/pull/18362>`__: REL: Update master after 1.20.1 release.
* `#18364 <https://github.com/numpy/numpy/pull/18364>`__: DOC: Update landing page to match table of contents
* `#18366 <https://github.com/numpy/numpy/pull/18366>`__: MAINT: Disable TravisCI git clone depth.
* `#18367 <https://github.com/numpy/numpy/pull/18367>`__: MAINT: Bump pytz from 2020.5 to 2021.1
* `#18369 <https://github.com/numpy/numpy/pull/18369>`__: BUG: np.in1d bug on the object array (issue 17923)
* `#18372 <https://github.com/numpy/numpy/pull/18372>`__: DOC: improve standard_t example in numpy.random.
* `#18374 <https://github.com/numpy/numpy/pull/18374>`__: TST: Add a test for nditer write masked with references
* `#18375 <https://github.com/numpy/numpy/pull/18375>`__: BUG: fix regression in a hidden callback use case in f2py.
* `#18377 <https://github.com/numpy/numpy/pull/18377>`__: ENH: Add annotations for `np.lib.ufunclike`
* `#18379 <https://github.com/numpy/numpy/pull/18379>`__: DOC: Fix docstring of _median_nancheck.
* `#18384 <https://github.com/numpy/numpy/pull/18384>`__: BUG: improve the interface of `tofile` method
* `#18389 <https://github.com/numpy/numpy/pull/18389>`__: MAINT: Fix version of wheel to support Python 3.10
* `#18390 <https://github.com/numpy/numpy/pull/18390>`__: ENH: Add annotations for `np.core.einsumfunc`
* `#18392 <https://github.com/numpy/numpy/pull/18392>`__: BUG: Remove check in shuffle for non-ndarrays
* `#18394 <https://github.com/numpy/numpy/pull/18394>`__: MAINT: Added Chain exceptions where appropriate
* `#18395 <https://github.com/numpy/numpy/pull/18395>`__: ENH: Initial typing of random
* `#18396 <https://github.com/numpy/numpy/pull/18396>`__: MAINT: Threading and Unicode strings
* `#18397 <https://github.com/numpy/numpy/pull/18397>`__: ENH: Add annotations for `np.lib.index_tricks`
* `#18398 <https://github.com/numpy/numpy/pull/18398>`__: MAINT: Fix casting signatures to align with NEP 43 signature
* `#18400 <https://github.com/numpy/numpy/pull/18400>`__: MAINT: Added Chain exceptions where appropriate
* `#18402 <https://github.com/numpy/numpy/pull/18402>`__: BUG: Fix typo in char_codes
* `#18404 <https://github.com/numpy/numpy/pull/18404>`__: BUG: Fix iterator shape in advanced index assignment broadcast...
* `#18405 <https://github.com/numpy/numpy/pull/18405>`__: DOC: Mention `scipy.signal.correlate` and FFT method in `np.correlate`closes...
* `#18413 <https://github.com/numpy/numpy/pull/18413>`__: MAINT: Bump sphinx from 3.4.3 to 3.5.0
* `#18414 <https://github.com/numpy/numpy/pull/18414>`__: MAINT: Bump hypothesis from 6.1.1 to 6.2.0
* `#18415 <https://github.com/numpy/numpy/pull/18415>`__: MAINT: Update END statements parsing for recent Fortran standards.
* `#18416 <https://github.com/numpy/numpy/pull/18416>`__: BUG: Fix f2py parsing continued lines that follow comment lines.
* `#18417 <https://github.com/numpy/numpy/pull/18417>`__: ENH: Add dtype-support to the ufunc-based `ndarray` magic methods...
* `#18418 <https://github.com/numpy/numpy/pull/18418>`__: DOC: remove layout overrides for headers
* `#18420 <https://github.com/numpy/numpy/pull/18420>`__: BUG: Fix tiny memory leaks when ``like=`` overrides are used
* `#18423 <https://github.com/numpy/numpy/pull/18423>`__: ENH: Lint checks for PR diffs
* `#18428 <https://github.com/numpy/numpy/pull/18428>`__: DOC: remove explanations.rst
* `#18429 <https://github.com/numpy/numpy/pull/18429>`__: DOC: point intersphinx to matplotlib/stable...
* `#18432 <https://github.com/numpy/numpy/pull/18432>`__: MAINT: Correct code producing warnings
* `#18433 <https://github.com/numpy/numpy/pull/18433>`__: ENH: Add typing for RandomState
* `#18436 <https://github.com/numpy/numpy/pull/18436>`__: BUG: Fix refcount leak in f2py `complex_double_from_pyobj`
* `#18437 <https://github.com/numpy/numpy/pull/18437>`__: TST: Fix some uninitialized memory in the tests
* `#18438 <https://github.com/numpy/numpy/pull/18438>`__: BUG: Correct shuffling of objects in 1-d array likes
* `#18439 <https://github.com/numpy/numpy/pull/18439>`__: MAINT: random: Use 'from exc' when raising a ValueError in choice.
* `#18443 <https://github.com/numpy/numpy/pull/18443>`__: BUG: fix stacklevel in warning within random.shuffle
* `#18448 <https://github.com/numpy/numpy/pull/18448>`__: DOC: Remove unfinished Linear Algebra section from Quickstart...
* `#18450 <https://github.com/numpy/numpy/pull/18450>`__: BUG: Segfault in nditer buffer dealloc for Object arrays
* `#18454 <https://github.com/numpy/numpy/pull/18454>`__: NEP: add Spending NumPy Project Funds (NEP 48)
* `#18455 <https://github.com/numpy/numpy/pull/18455>`__: BUG: ``diagflat`` could overflow on windows or 32-bit platforms
* `#18456 <https://github.com/numpy/numpy/pull/18456>`__: NEP: array API standard adoption (NEP 47)
* `#18458 <https://github.com/numpy/numpy/pull/18458>`__: DOC: update NEP status for accepted/finished NEPs
* `#18463 <https://github.com/numpy/numpy/pull/18463>`__: MAINT: Bump mypy from 0.800 to 0.812
* `#18464 <https://github.com/numpy/numpy/pull/18464>`__: MAINT: Bump sphinx from 3.5.0 to 3.5.1
* `#18465 <https://github.com/numpy/numpy/pull/18465>`__: MAINT: Bump cython from 0.29.21 to 0.29.22
* `#18466 <https://github.com/numpy/numpy/pull/18466>`__: MAINT: Bump hypothesis from 6.2.0 to 6.3.0
* `#18475 <https://github.com/numpy/numpy/pull/18475>`__: ENH: Added type annotations to eye() function
* `#18476 <https://github.com/numpy/numpy/pull/18476>`__: BUG: Remove suspicious type casting
* `#18477 <https://github.com/numpy/numpy/pull/18477>`__: BUG: remove nonsensical comparison of pointer < 0
* `#18478 <https://github.com/numpy/numpy/pull/18478>`__: BUG: verify pointer against NULL before using it
* `#18479 <https://github.com/numpy/numpy/pull/18479>`__: BUG: check if PyArray_malloc succeeded
* `#18481 <https://github.com/numpy/numpy/pull/18481>`__: DOC: Generator and RandomState doc improvements
* `#18482 <https://github.com/numpy/numpy/pull/18482>`__: ENH: Improve error message in multinomial
* `#18489 <https://github.com/numpy/numpy/pull/18489>`__: DOC: Rename "Ones and zeros" section in array-creation documentation.
* `#18493 <https://github.com/numpy/numpy/pull/18493>`__: BUG: Fix non-versioneer uses of numpy.distutils
* `#18497 <https://github.com/numpy/numpy/pull/18497>`__: TST: Remove the `einsum` typing tests reliance on issuing a `ComplexWarning`
* `#18498 <https://github.com/numpy/numpy/pull/18498>`__: BUG: Fixed Von Mises distribution for big values of kappa
* `#18499 <https://github.com/numpy/numpy/pull/18499>`__: TST: Branch coverage improvement for `np.polynomial`
* `#18502 <https://github.com/numpy/numpy/pull/18502>`__: DOC: Fix links to landing page
* `#18505 <https://github.com/numpy/numpy/pull/18505>`__: DOC: add guide for downstream package authors
* `#18509 <https://github.com/numpy/numpy/pull/18509>`__: DOC: trunc, floor, ceil, rint, fix should all link to each other
* `#18513 <https://github.com/numpy/numpy/pull/18513>`__: BLD: add _2_24 to valid manylinux names
* `#18515 <https://github.com/numpy/numpy/pull/18515>`__: MAINT: Improve error message when common type not found.
* `#18517 <https://github.com/numpy/numpy/pull/18517>`__: MAINT: Bump hypothesis from 6.3.0 to 6.3.4
* `#18518 <https://github.com/numpy/numpy/pull/18518>`__: DOC Improve formatting in the depending_on_numpy documentation
* `#18522 <https://github.com/numpy/numpy/pull/18522>`__: BUG: remove extraneous ARGOUTVIEWM dim. 4 typemaps
* `#18526 <https://github.com/numpy/numpy/pull/18526>`__: MAINT: Specify color in RGB in the docs about the new NumPy logo
* `#18530 <https://github.com/numpy/numpy/pull/18530>`__: BUG: incorrect error fallthrough in nditer
* `#18531 <https://github.com/numpy/numpy/pull/18531>`__: CI: Use Ubuntu 18.04 to run "full" test.
* `#18537 <https://github.com/numpy/numpy/pull/18537>`__: [BLD] use the new openblas lib
* `#18538 <https://github.com/numpy/numpy/pull/18538>`__: Fix the numpy Apple M1 build
* `#18539 <https://github.com/numpy/numpy/pull/18539>`__: BUG: NameError in numpy.distutils.fcompiler.compaq
* `#18544 <https://github.com/numpy/numpy/pull/18544>`__: MAINT: Update master to main after branch rename
* `#18545 <https://github.com/numpy/numpy/pull/18545>`__: ENH: Add annotations for `np.lib.arrayterator`
* `#18554 <https://github.com/numpy/numpy/pull/18554>`__: CI: Pin docker image for Linux_Python_38_32bit_full_with_asserts...
* `#18560 <https://github.com/numpy/numpy/pull/18560>`__: BUG: Fixed ``where`` keyword for ``np.mean`` & ``np.var`` methods
* `#18566 <https://github.com/numpy/numpy/pull/18566>`__: CI: another master -> main fix
* `#18567 <https://github.com/numpy/numpy/pull/18567>`__: CI: skip lint check on merges with main
* `#18569 <https://github.com/numpy/numpy/pull/18569>`__: CI: Ensure that doc-build uses "main" as branch name
* `#18570 <https://github.com/numpy/numpy/pull/18570>`__: CI: Use `git branch -m` instead of `--initial-branch=main`
* `#18571 <https://github.com/numpy/numpy/pull/18571>`__: BUG: Fix overflow warning on apple silicon
* `#18572 <https://github.com/numpy/numpy/pull/18572>`__: CI: Set git default branch to "main" in CircleCI.
* `#18574 <https://github.com/numpy/numpy/pull/18574>`__: MAINT: Update the Call for Contributions section
* `#18575 <https://github.com/numpy/numpy/pull/18575>`__: MAINT: Bump sphinx from 3.5.1 to 3.5.2
* `#18576 <https://github.com/numpy/numpy/pull/18576>`__: MAINT: Bump hypothesis from 6.3.4 to 6.6.0
* `#18578 <https://github.com/numpy/numpy/pull/18578>`__: MAINT: Bump pycodestyle from 2.5.0 to 2.6.0
* `#18579 <https://github.com/numpy/numpy/pull/18579>`__: MAINT: OrderedDict is no longer necessary from Python 3.7
* `#18582 <https://github.com/numpy/numpy/pull/18582>`__: BLD, TST: use pypy nightly to work around bug
* `#18583 <https://github.com/numpy/numpy/pull/18583>`__: DOC: Clarify docs for fliplr() / flipud()
* `#18584 <https://github.com/numpy/numpy/pull/18584>`__: DOC: Added documentation for linter (#18423)
* `#18593 <https://github.com/numpy/numpy/pull/18593>`__: MAINT: Do not claim input to binops is `self` (array object)
* `#18594 <https://github.com/numpy/numpy/pull/18594>`__: MAINT: Remove strange `op == NULL` check
* `#18596 <https://github.com/numpy/numpy/pull/18596>`__: MAINT: Chain exceptions in index_tricks.py and mrecords.py
* `#18598 <https://github.com/numpy/numpy/pull/18598>`__: MAINT: Add annotations for `dtype.__getitem__`, `__mul__` and...
* `#18602 <https://github.com/numpy/numpy/pull/18602>`__: CI: Do not fail CI on lint error
* `#18605 <https://github.com/numpy/numpy/pull/18605>`__: BUG: Fix ma coercion list-of-ma-arrays if they do not cast to...
* `#18614 <https://github.com/numpy/numpy/pull/18614>`__: MAINT: Bump pycodestyle from 2.6.0 to 2.7.0
* `#18615 <https://github.com/numpy/numpy/pull/18615>`__: MAINT: Bump hypothesis from 6.6.0 to 6.8.1
* `#18616 <https://github.com/numpy/numpy/pull/18616>`__: CI: Update apt package list before Python install
* `#18618 <https://github.com/numpy/numpy/pull/18618>`__: MAINT: Ensure that re-exported sub-modules are properly annotated
* `#18622 <https://github.com/numpy/numpy/pull/18622>`__: DOC: Consistently use rng as variable name for random generators
* `#18629 <https://github.com/numpy/numpy/pull/18629>`__: BUG, ENH: fix array2string rounding bug by adding min_digits...
* `#18630 <https://github.com/numpy/numpy/pull/18630>`__: DOC: add note to numpy.rint() docstrings
* `#18634 <https://github.com/numpy/numpy/pull/18634>`__: BUG: Use npy_log1p where appropriate in random generation
* `#18635 <https://github.com/numpy/numpy/pull/18635>`__: ENH: Improve the exception for default low in Generator.integers
* `#18641 <https://github.com/numpy/numpy/pull/18641>`__: MAINT: Remove useless declarations in `bad_commands`
* `#18642 <https://github.com/numpy/numpy/pull/18642>`__: ENH: Use new argument parsing for array creation functions
* `#18643 <https://github.com/numpy/numpy/pull/18643>`__: DOC: Remove mention of nose from README
* `#18645 <https://github.com/numpy/numpy/pull/18645>`__: DOC: Minor fix in inline code example of ufunc reference
* `#18648 <https://github.com/numpy/numpy/pull/18648>`__: MAINT: use super() as described by PEP 3135
* `#18649 <https://github.com/numpy/numpy/pull/18649>`__: MAINT: Add missing type to cdef statement
* `#18651 <https://github.com/numpy/numpy/pull/18651>`__: BUG: Fix small valgrind-found issues
* `#18652 <https://github.com/numpy/numpy/pull/18652>`__: DOC: Update some plotting code to current Matplotlib idioms
* `#18657 <https://github.com/numpy/numpy/pull/18657>`__: ENH: Improve performance of `np.save` for small arrays
* `#18658 <https://github.com/numpy/numpy/pull/18658>`__: BLD: remove /usr/include from default include dirs
* `#18659 <https://github.com/numpy/numpy/pull/18659>`__: DEV: add a conda environment.yml with all development dependencies
* `#18660 <https://github.com/numpy/numpy/pull/18660>`__: DOC: add release note for removal of /usr/include from include...
* `#18664 <https://github.com/numpy/numpy/pull/18664>`__: MAINT: Bump sphinx from 3.5.2 to 3.5.3
* `#18666 <https://github.com/numpy/numpy/pull/18666>`__: ENH: Use exponentials in place of inversion in Rayleigh and geometric
* `#18670 <https://github.com/numpy/numpy/pull/18670>`__: BUG: Fix small issues found with pytest-leaks
* `#18676 <https://github.com/numpy/numpy/pull/18676>`__: MAINT: Implement new style promotion for `np.result_type`, etc.
* `#18679 <https://github.com/numpy/numpy/pull/18679>`__: BUG: Changed METH_VARARGS to METH_NOARGS
* `#18680 <https://github.com/numpy/numpy/pull/18680>`__: Docs: simd-optimizations.rst: fix typo (basline ~> baseline)
* `#18685 <https://github.com/numpy/numpy/pull/18685>`__: REL: Update main after 1.20.2 release.
* `#18686 <https://github.com/numpy/numpy/pull/18686>`__: BUG: Fix test_ccompiler_opt when path contains dots
* `#18689 <https://github.com/numpy/numpy/pull/18689>`__: DOC: Change matrix size in absolute beginners doc.
* `#18690 <https://github.com/numpy/numpy/pull/18690>`__: BUG: Correct datetime64 missing type overload for datetime.date...
* `#18691 <https://github.com/numpy/numpy/pull/18691>`__: BUG: fix segfault in object/longdouble operations
* `#18692 <https://github.com/numpy/numpy/pull/18692>`__: MAINT: Bump pydata-sphinx-theme from 0.5.0 to 0.5.2
* `#18693 <https://github.com/numpy/numpy/pull/18693>`__: MAINT: Bump hypothesis from 6.8.1 to 6.8.3
* `#18694 <https://github.com/numpy/numpy/pull/18694>`__: TST: pin pypy version to 7.3.4rc1
* `#18695 <https://github.com/numpy/numpy/pull/18695>`__: ENH: Support parsing Fortran abstract interface blocks.
* `#18697 <https://github.com/numpy/numpy/pull/18697>`__: DEP: Disable PyUFunc_GenericFunction and PyUFunc_SetUsesArraysAsData
* `#18698 <https://github.com/numpy/numpy/pull/18698>`__: MAINT: Specify the color space in all new NumPy logo files
* `#18701 <https://github.com/numpy/numpy/pull/18701>`__: BLD: Strip extra newline when dumping gfortran version on MacOS
* `#18705 <https://github.com/numpy/numpy/pull/18705>`__: DOC: update Steering Council membership and people on governance...
* `#18706 <https://github.com/numpy/numpy/pull/18706>`__: DOC: Add release notes to upcoming_changes
* `#18708 <https://github.com/numpy/numpy/pull/18708>`__: TST: add tests for using np.meshgrid for higher dimensional grids.
* `#18712 <https://github.com/numpy/numpy/pull/18712>`__: DOC: Simplifies Mandelbrot set plot in Quickstart guide
* `#18718 <https://github.com/numpy/numpy/pull/18718>`__: API, DEP: Move ufunc signature parsing to the start
* `#18722 <https://github.com/numpy/numpy/pull/18722>`__: DOC: deduplicate dtype basic types (2)
* `#18725 <https://github.com/numpy/numpy/pull/18725>`__: MAINT: Bump pytest from 6.2.2 to 6.2.3
* `#18726 <https://github.com/numpy/numpy/pull/18726>`__: MAINT: Bump hypothesis from 6.8.3 to 6.8.4
* `#18728 <https://github.com/numpy/numpy/pull/18728>`__: MAINT: Add exception chaining where appropriate
* `#18731 <https://github.com/numpy/numpy/pull/18731>`__: BUG: Check out requirements and raise when not satisfied
* `#18733 <https://github.com/numpy/numpy/pull/18733>`__: DEV: Adds gitpod to numpy
* `#18737 <https://github.com/numpy/numpy/pull/18737>`__: BLD: introduce use of BLAS_LIBS and LAPACK_LIBS in distutils/system_info
* `#18739 <https://github.com/numpy/numpy/pull/18739>`__: MAINT: Add exception chaining where appropriate
* `#18741 <https://github.com/numpy/numpy/pull/18741>`__: DOC: Emphasize distinctions between np.copy and ndarray.copy
* `#18745 <https://github.com/numpy/numpy/pull/18745>`__: CI: remove shippable CI
* `#18750 <https://github.com/numpy/numpy/pull/18750>`__: MAINT: Allow more recursion depth for scalar tests.
* `#18751 <https://github.com/numpy/numpy/pull/18751>`__: BUG: Regression #18075 | Fixing Ufunc TD generation order
* `#18753 <https://github.com/numpy/numpy/pull/18753>`__: BLD: Negative zero handling with ifort
* `#18755 <https://github.com/numpy/numpy/pull/18755>`__: MAINT: Bump sphinx from 3.5.3 to 3.5.4
* `#18757 <https://github.com/numpy/numpy/pull/18757>`__: MAINT: Bump hypothesis from 6.8.4 to 6.9.1
* `#18758 <https://github.com/numpy/numpy/pull/18758>`__: DOC: Update howto-docs with link to NumPy tutorials.
* `#18761 <https://github.com/numpy/numpy/pull/18761>`__: DOC: Small fixes (including formatting) for NEP 43
* `#18765 <https://github.com/numpy/numpy/pull/18765>`__: ENH: Improve the placeholder annotations for the main numpy namespace
* `#18766 <https://github.com/numpy/numpy/pull/18766>`__: ENH, SIMD: Replace libdivide functions of signed integer division...
* `#18770 <https://github.com/numpy/numpy/pull/18770>`__: DOC: More concise "How to import NumPy" description
* `#18771 <https://github.com/numpy/numpy/pull/18771>`__: DOC: Use: from numpy.testing import ...
* `#18772 <https://github.com/numpy/numpy/pull/18772>`__: CI: Use informational mode for codecov
* `#18773 <https://github.com/numpy/numpy/pull/18773>`__: CI: Fixing typo in Azure job run
* `#18777 <https://github.com/numpy/numpy/pull/18777>`__: DOC: update random and asserts in test guidelines
* `#18778 <https://github.com/numpy/numpy/pull/18778>`__: MAINT: Relax the integer-type-constraint of `npt._ShapeLike`
* `#18779 <https://github.com/numpy/numpy/pull/18779>`__: DOC: fix spelling of "reccomended" ("recommended")
* `#18780 <https://github.com/numpy/numpy/pull/18780>`__: ENH: Improve the placeholder annotations for the main numpy namespace...
* `#18781 <https://github.com/numpy/numpy/pull/18781>`__: ENH: Add `__all__` to a number of public modules
* `#18785 <https://github.com/numpy/numpy/pull/18785>`__: DOC: change `dec.parametrize` to `pytest.mark.parametrize`
* `#18786 <https://github.com/numpy/numpy/pull/18786>`__: DOC: add note for clip() special case a_min > a_max See #18782
* `#18787 <https://github.com/numpy/numpy/pull/18787>`__: DOC: Document newer pytest conventions
* `#18789 <https://github.com/numpy/numpy/pull/18789>`__: DEV: Pin pydata-sphinx-theme to 0.5.2.
* `#18790 <https://github.com/numpy/numpy/pull/18790>`__: CI: Use `towncrier build` explicitly
* `#18791 <https://github.com/numpy/numpy/pull/18791>`__: DOC: Fixes small things in the genfromtext docstring
* `#18792 <https://github.com/numpy/numpy/pull/18792>`__: MAINT: Use recent towncrier releases on PyPI.
* `#18795 <https://github.com/numpy/numpy/pull/18795>`__: SIMD, TEST: Workaround for misaligned stack GCC BUG ABI on WIN64
* `#18796 <https://github.com/numpy/numpy/pull/18796>`__: DOC: Misc Numpydoc and formatting for proper parsing.
* `#18797 <https://github.com/numpy/numpy/pull/18797>`__: DOC: Update random c-api documentation
* `#18799 <https://github.com/numpy/numpy/pull/18799>`__: MAINT: Improve the placeholder annotations for the main numpy...
* `#18800 <https://github.com/numpy/numpy/pull/18800>`__: MAINT: Relax miscellaneous integer-type constraints
* `#18801 <https://github.com/numpy/numpy/pull/18801>`__: DOC: fix typo in frexp docstring
* `#18802 <https://github.com/numpy/numpy/pull/18802>`__: DOC: Improve random.choice() documentation
* `#18805 <https://github.com/numpy/numpy/pull/18805>`__: NEP: propose new nep for allocator policies
* `#18806 <https://github.com/numpy/numpy/pull/18806>`__: MAINT: Bump hypothesis from 6.9.1 to 6.10.0
* `#18807 <https://github.com/numpy/numpy/pull/18807>`__: MAINT: Bump cython from 0.29.22 to 0.29.23
* `#18809 <https://github.com/numpy/numpy/pull/18809>`__: MAINT: runtests help text cleanup
* `#18812 <https://github.com/numpy/numpy/pull/18812>`__: DOC: Document howto build documentation in a virtual environment
* `#18813 <https://github.com/numpy/numpy/pull/18813>`__: BUG: Initialize the full nditer buffer in case of error
* `#18818 <https://github.com/numpy/numpy/pull/18818>`__: ENH: Add annotations for 4 objects in `np.core.numerictypes`
* `#18820 <https://github.com/numpy/numpy/pull/18820>`__: MAINT: Remove incorrect inline
* `#18822 <https://github.com/numpy/numpy/pull/18822>`__: DEV: general Gitpod enhancements
* `#18823 <https://github.com/numpy/numpy/pull/18823>`__: MAINT: Minor fix to add reference link to numpy.fill_diagonal...
* `#18825 <https://github.com/numpy/numpy/pull/18825>`__: MAINT: Update README.md
* `#18831 <https://github.com/numpy/numpy/pull/18831>`__: BUG: Prevent nan being used in percentile
* `#18834 <https://github.com/numpy/numpy/pull/18834>`__: DOC: Fix typo in random docs
* `#18836 <https://github.com/numpy/numpy/pull/18836>`__: MAINT: Generalize and shorten the ufunc "trivially iterable"...
* `#18837 <https://github.com/numpy/numpy/pull/18837>`__: ENH, SIMD: Add support for dispatching C++ sources
* `#18839 <https://github.com/numpy/numpy/pull/18839>`__: DOC: Add Gitpod development documentation
* `#18841 <https://github.com/numpy/numpy/pull/18841>`__: DOC: Add favicon
* `#18842 <https://github.com/numpy/numpy/pull/18842>`__: ENH: Improve the placeholder annotations within sub-modules
* `#18843 <https://github.com/numpy/numpy/pull/18843>`__: DOC: Clarify isreal docstring
* `#18845 <https://github.com/numpy/numpy/pull/18845>`__: DOC: Move Sphinx numpy target in reference index.
* `#18851 <https://github.com/numpy/numpy/pull/18851>`__: MAINT: Disable pip version check for azure lint check.
* `#18853 <https://github.com/numpy/numpy/pull/18853>`__: ENH: Improve the placeholder annotations within sub-modules (part...
* `#18855 <https://github.com/numpy/numpy/pull/18855>`__: STY: change CRLF line terminators to Unix
* `#18856 <https://github.com/numpy/numpy/pull/18856>`__: MAINT: Fix the typo "implment"
* `#18862 <https://github.com/numpy/numpy/pull/18862>`__: TST: Skip f2py TestSharedMemory for LONGDOUBLE on macos/arm64
* `#18863 <https://github.com/numpy/numpy/pull/18863>`__: ENH: Add max values comparison for floating point
* `#18864 <https://github.com/numpy/numpy/pull/18864>`__: MAINT: Remove dead codepath in generalized ufuncs
* `#18868 <https://github.com/numpy/numpy/pull/18868>`__: Upgrade to GitHub-native Dependabot
* `#18869 <https://github.com/numpy/numpy/pull/18869>`__: MAINT: Fix azure linter problems with pip 21.1
* `#18871 <https://github.com/numpy/numpy/pull/18871>`__: MAINT: Bump hypothesis from 6.10.0 to 6.10.1
* `#18874 <https://github.com/numpy/numpy/pull/18874>`__: BLD, ENH: Enable Accelerate Framework
* `#18877 <https://github.com/numpy/numpy/pull/18877>`__: MAINT: Update PyPy version used by CI
* `#18880 <https://github.com/numpy/numpy/pull/18880>`__: API: Ensure that casting does not affect ufunc loop
* `#18882 <https://github.com/numpy/numpy/pull/18882>`__: ENH: Add min values comparison for floating point
* `#18885 <https://github.com/numpy/numpy/pull/18885>`__: MAINT: Remove unsafe unions and ABCs from return-annotations
* `#18889 <https://github.com/numpy/numpy/pull/18889>`__: ENH: Add SIMD operations for min and max value comparision
* `#18890 <https://github.com/numpy/numpy/pull/18890>`__: MAINT: ssize_t -> Py_ssize_t and other fixes for Python v3.10.0
* `#18891 <https://github.com/numpy/numpy/pull/18891>`__: MAINT: Bump typing-extensions from 3.7.4.3 to 3.10.0.0
* `#18893 <https://github.com/numpy/numpy/pull/18893>`__: DOC: Add a set of standard replies.
* `#18895 <https://github.com/numpy/numpy/pull/18895>`__: DOC: Improve cumsum documentation
* `#18896 <https://github.com/numpy/numpy/pull/18896>`__: MAINT: Explicitly mark text files in .gitattributes.
* `#18897 <https://github.com/numpy/numpy/pull/18897>`__: MAINT: Add ".csv" some data file names.
* `#18899 <https://github.com/numpy/numpy/pull/18899>`__: BLD, BUG: Fix compiler optimization log AttributeError
* `#18900 <https://github.com/numpy/numpy/pull/18900>`__: BLD: remove unnecessary flag `-faltivec` on macOS
* `#18903 <https://github.com/numpy/numpy/pull/18903>`__: MAINT, CI: treats _SIMD module build warnings as errors through...
* `#18906 <https://github.com/numpy/numpy/pull/18906>`__: ENH: Add PCG64DXSM BitGenerator
* `#18908 <https://github.com/numpy/numpy/pull/18908>`__: MAINT: Adjust NumPy float hashing to Python's slightly changed...
* `#18909 <https://github.com/numpy/numpy/pull/18909>`__: ENH: Improve the placeholder annotations within sub-modules (part...
* `#18910 <https://github.com/numpy/numpy/pull/18910>`__: BUG : for MINGW, threads.h existence test requires GLIBC > 2.12
* `#18911 <https://github.com/numpy/numpy/pull/18911>`__: BLD, BUG: Fix bdist_wheel duplicate building
* `#18912 <https://github.com/numpy/numpy/pull/18912>`__: CI: fix the GitHub Actions trigger in docker.yml
* `#18918 <https://github.com/numpy/numpy/pull/18918>`__: DOC: fix documentation of cloning over ssh
* `#18919 <https://github.com/numpy/numpy/pull/18919>`__: ENH: Add placeholder annotations for two missing `np.testing`...
* `#18920 <https://github.com/numpy/numpy/pull/18920>`__: BUG: Report underflow condition in AVX implementation of np.exp
* `#18927 <https://github.com/numpy/numpy/pull/18927>`__: NEP: add mailing list thread, fixes from review
* `#18930 <https://github.com/numpy/numpy/pull/18930>`__: BUG: Make changelog recognize ``gh-`` as a PR number prefix.
* `#18931 <https://github.com/numpy/numpy/pull/18931>`__: BUG: Fix refcounting in string-promotion deprecation code path
* `#18933 <https://github.com/numpy/numpy/pull/18933>`__: BUG: Fix underflow error in AVX512 implementation of ufunc exp/f64
* `#18934 <https://github.com/numpy/numpy/pull/18934>`__: DOC: Add a release note for the improved placeholder annotations
* `#18935 <https://github.com/numpy/numpy/pull/18935>`__: API: Add `npt.NDArray`, a runtime-subscriptable alias for `np.ndarray`
* `#18936 <https://github.com/numpy/numpy/pull/18936>`__: DOC: Update performance for new PRNG
* `#18940 <https://github.com/numpy/numpy/pull/18940>`__: ENH: manually inline PCG64DXSM code for performance.
* `#18943 <https://github.com/numpy/numpy/pull/18943>`__: TST: xfail `TestCond.test_nan` unconditionally
* `#18944 <https://github.com/numpy/numpy/pull/18944>`__: ENH: Add annotations for `np.lib.utils`
* `#18954 <https://github.com/numpy/numpy/pull/18954>`__: DOC: Update beginners docu for sum function with axis
* `#18955 <https://github.com/numpy/numpy/pull/18955>`__: DOC: add an extra example in runtests.py help test
* `#18956 <https://github.com/numpy/numpy/pull/18956>`__: DOC: change copyright SciPy to NumPy
* `#18957 <https://github.com/numpy/numpy/pull/18957>`__: DOC: Improve datetime64 docs.
* `#18958 <https://github.com/numpy/numpy/pull/18958>`__: MAINT: Do not use deprecated ``mktemp()``
* `#18959 <https://github.com/numpy/numpy/pull/18959>`__: DOC: improve numpy.histogram2d() documentation
* `#18960 <https://github.com/numpy/numpy/pull/18960>`__: BUG: fixed ma.average ignoring masked weights
* `#18961 <https://github.com/numpy/numpy/pull/18961>`__: DOC: add note and examples to `isrealobj` docstring
* `#18962 <https://github.com/numpy/numpy/pull/18962>`__: DOC: Update a page title with proper case
* `#18963 <https://github.com/numpy/numpy/pull/18963>`__: DEP: remove PolyBase from np.polynomial.polyutils
* `#18965 <https://github.com/numpy/numpy/pull/18965>`__: DOC: Improve description of array scalar in glossary
* `#18967 <https://github.com/numpy/numpy/pull/18967>`__: BUG: fix np.ma.masked_where(copy=False) when input has no mask
* `#18970 <https://github.com/numpy/numpy/pull/18970>`__: MAINT, SIMD: Hardened the AVX compile-time tests
* `#18972 <https://github.com/numpy/numpy/pull/18972>`__: ENH: Include co-authors in changelog.
* `#18973 <https://github.com/numpy/numpy/pull/18973>`__: MAINT: Bump sphinx from 3.5.4 to 4.0.0
* `#18974 <https://github.com/numpy/numpy/pull/18974>`__: MAINT: Bump hypothesis from 6.10.1 to 6.12.0
* `#18976 <https://github.com/numpy/numpy/pull/18976>`__: MAINT: Bump pytest from 6.2.3 to 6.2.4
* `#18980 <https://github.com/numpy/numpy/pull/18980>`__: DOC: Gitpod documentation enhancements
* `#18982 <https://github.com/numpy/numpy/pull/18982>`__: MAINT: Cleanup tools/changelog.py
* `#18983 <https://github.com/numpy/numpy/pull/18983>`__: REL: Update main after 1.20.3 release.
* `#18985 <https://github.com/numpy/numpy/pull/18985>`__: MAINT: Remove usage of the PEP 604 pipe operator
* `#18987 <https://github.com/numpy/numpy/pull/18987>`__: BUG: Update coordinates in PyArray_ITER_GOTO1D
* `#18989 <https://github.com/numpy/numpy/pull/18989>`__: BUG: fix potential buffer overflow(#18939)
* `#18990 <https://github.com/numpy/numpy/pull/18990>`__: ENH: Add annotations for `np.lib.NumpyVersion`
* `#18996 <https://github.com/numpy/numpy/pull/18996>`__: MAINT: Remove warning when checking AVX512f on MSVC
* `#18998 <https://github.com/numpy/numpy/pull/18998>`__: ENH: Improve annotations of the `item`, `tolist`, `take` and...
* `#18999 <https://github.com/numpy/numpy/pull/18999>`__: DEP: Ensure the string promotion FutureWarning is raised
* `#19001 <https://github.com/numpy/numpy/pull/19001>`__: DEP: Deprecate error clearing for special method in array-coercion
* `#19002 <https://github.com/numpy/numpy/pull/19002>`__: ENH: Add annotations for `np.broadcast` and `np.DataSource`
* `#19005 <https://github.com/numpy/numpy/pull/19005>`__: ENH: Add dtype-support to 11 `ndarray` / `generic` methods
* `#19007 <https://github.com/numpy/numpy/pull/19007>`__: BUG: fix potential use of null pointer in nditer buffers
* `#19008 <https://github.com/numpy/numpy/pull/19008>`__: BUG: fix variable misprint in multiarray test code
* `#19009 <https://github.com/numpy/numpy/pull/19009>`__: BUG: fix variable misprint checking wrong variable in umath tests
* `#19011 <https://github.com/numpy/numpy/pull/19011>`__: BUG: fix ValueError in PyArray_Std on win_amd64
* `#19012 <https://github.com/numpy/numpy/pull/19012>`__: MAINT: Small cleanups in `PyArray_NewFromDescr_int`
* `#19014 <https://github.com/numpy/numpy/pull/19014>`__: Revert "BUG: Update coordinates in PyArray_ITER_GOTO1D"
* `#19018 <https://github.com/numpy/numpy/pull/19018>`__: DOC: "NumPy" <- "numpy" in NumPy Fundamentals - Indexing
* `#19021 <https://github.com/numpy/numpy/pull/19021>`__: DOC: Add comment for ifdef macro guard
* `#19024 <https://github.com/numpy/numpy/pull/19024>`__: MAINT: Bump pytest-cov from 2.11.1 to 2.12.0
* `#19025 <https://github.com/numpy/numpy/pull/19025>`__: MAINT: Bump sphinx from 4.0.0 to 4.0.1
* `#19026 <https://github.com/numpy/numpy/pull/19026>`__: DOC: Clarify minimum numpy version needed to use random c-api
* `#19029 <https://github.com/numpy/numpy/pull/19029>`__: ENH: Improve the annotations of `np.core._internal`
* `#19031 <https://github.com/numpy/numpy/pull/19031>`__: DEP: Deprecate 4 `ndarray.ctypes` methods
* `#19035 <https://github.com/numpy/numpy/pull/19035>`__: MAINT: Python3 classes do not need to inherit from object
* `#19037 <https://github.com/numpy/numpy/pull/19037>`__: BUG: do not use PyLong_FromLong for intp
* `#19041 <https://github.com/numpy/numpy/pull/19041>`__: DOC: Improve trapz docstring
* `#19043 <https://github.com/numpy/numpy/pull/19043>`__: DOC: Fix typo in release notes for v1.21
* `#19046 <https://github.com/numpy/numpy/pull/19046>`__: BUG, SIMD: Fix unexpected result of uint8 division on X86
* `#19047 <https://github.com/numpy/numpy/pull/19047>`__: BUG, SIMD: Fix NumPy build on ppc64le(IBM/Power) for old versions...
* `#19048 <https://github.com/numpy/numpy/pull/19048>`__: BUG: Fix duplicate variable names in compiler check for AVX512_SKX
* `#19049 <https://github.com/numpy/numpy/pull/19049>`__: BLD,API: (distutils) Force strict floating point error model...
* `#19052 <https://github.com/numpy/numpy/pull/19052>`__: ENH: Improve the `np.ufunc` annotations
* `#19055 <https://github.com/numpy/numpy/pull/19055>`__: DOC: Forward port missing 1.18.5 release note.
* `#19063 <https://github.com/numpy/numpy/pull/19063>`__: ENH: Stubs for array_equal appear out of date.
* `#19066 <https://github.com/numpy/numpy/pull/19066>`__: BUG: Fixed an issue wherein `nanmedian` could return an array...
* `#19068 <https://github.com/numpy/numpy/pull/19068>`__: MAINT: Update mailmap
* `#19073 <https://github.com/numpy/numpy/pull/19073>`__: REL: Prepare 1.21.0 release
* `#19074 <https://github.com/numpy/numpy/pull/19074>`__: BUG: Fix compile-time test of POPCNT
* `#19075 <https://github.com/numpy/numpy/pull/19075>`__: BUG: Fix test_numpy_version.
* `#19094 <https://github.com/numpy/numpy/pull/19094>`__: BUG: Fixed an issue wherein `_GenericAlias.__getitem__` would...
* `#19100 <https://github.com/numpy/numpy/pull/19100>`__: BUG: Linter should only run on pull requests.
* `#19120 <https://github.com/numpy/numpy/pull/19120>`__: BUG: Fix setup.py to work in maintenance branches.
* `#19144 <https://github.com/numpy/numpy/pull/19144>`__: BUG: expose short_version as previously in version.py
* `#19175 <https://github.com/numpy/numpy/pull/19175>`__: API: Delay string and number promotion deprecation/future warning
* `#19178 <https://github.com/numpy/numpy/pull/19178>`__: BUG, SIMD: Fix detect host/native CPU features on ICC at compile-time
* `#19180 <https://github.com/numpy/numpy/pull/19180>`__: BUG: Add -std=c99 to intel icc compiler flags on linux
* `#19193 <https://github.com/numpy/numpy/pull/19193>`__: NEP: Accept NEP 35 as final
* `#19194 <https://github.com/numpy/numpy/pull/19194>`__: MAINT, BUG: Adapt `castingimpl.casting` to denote a minimal level
* `#19197 <https://github.com/numpy/numpy/pull/19197>`__: REL: Prepare for NumPy 1.20.0rc2 release.
* `#19213 <https://github.com/numpy/numpy/pull/19213>`__: MAINT: Add annotations for the missing `period` parameter to...
* `#19219 <https://github.com/numpy/numpy/pull/19219>`__: MAINT: Add `complex` as allowed type for the `np.complexfloating`...
* `#19233 <https://github.com/numpy/numpy/pull/19233>`__: TST: Ignore exp FP exceptions test for glibc ver < 2.17
* `#19238 <https://github.com/numpy/numpy/pull/19238>`__: MAINT: replace imgmath with mathjax for docs
* `#19239 <https://github.com/numpy/numpy/pull/19239>`__: BUG: Fix out-of-bounds access in convert_datetime_divisor_to_multiple
* `#19240 <https://github.com/numpy/numpy/pull/19240>`__: ENH: Support major version larger than 9 in NumpyVersion
* `#19268 <https://github.com/numpy/numpy/pull/19268>`__: DOC: fix duplicate navbar in development documentation index
* `#19269 <https://github.com/numpy/numpy/pull/19269>`__: BUG: Invalid dtypes comparison should not raise TypeError
* `#19280 <https://github.com/numpy/numpy/pull/19280>`__: BUG: Add missing DECREF in new path
* `#19283 <https://github.com/numpy/numpy/pull/19283>`__: REL: Prepare for 1.21.0 release
.. include:: ../../benchmarks/README.rst
********
Glossary
********

.. glossary::


   (`n`,)
       A parenthesized number followed by a comma denotes a tuple with one
       element. The trailing comma distinguishes a one-element tuple from a
       parenthesized ``n``.


   -1
       - **In a dimension entry**, instructs NumPy to choose the length
         that will keep the total number of array elements the same.

           >>> np.arange(12).reshape(4, -1).shape
           (4, 3)

       - **In an index**, any negative value
         `denotes <https://docs.python.org/dev/faq/programming.html#what-s-a-negative-index>`_
         indexing from the right.

   . . .
       An :py:data:`Ellipsis`.

       - **When indexing an array**, shorthand that the missing axes, if they
         exist, are full slices.

           >>> a = np.arange(24).reshape(2,3,4)

           >>> a[...].shape
           (2, 3, 4)

           >>> a[...,0].shape
           (2, 3)

           >>> a[0,...].shape
           (3, 4)

           >>> a[0,...,0].shape
           (3,)

         It can be used at most once; ``a[...,0,...]`` raises an :exc:`IndexError`.

       - **In printouts**, NumPy substitutes ``...`` for the middle elements of
         large arrays. To see the entire array, use `numpy.printoptions`


   :
       The Python :term:`python:slice`
       operator. In ndarrays, slicing can be applied to every
       axis:

           >>> a = np.arange(24).reshape(2,3,4)
           >>> a
           array([[[ 0,  1,  2,  3],
                   [ 4,  5,  6,  7],
                   [ 8,  9, 10, 11]],
           <BLANKLINE>
                  [[12, 13, 14, 15],
                   [16, 17, 18, 19],
                   [20, 21, 22, 23]]])
           <BLANKLINE>
           >>> a[1:,-2:,:-1]
           array([[[16, 17, 18],
                   [20, 21, 22]]])

       Trailing slices can be omitted: ::

           >>> a[1] == a[1,:,:]
           array([[ True,  True,  True,  True],
                  [ True,  True,  True,  True],
                  [ True,  True,  True,  True]])

       In contrast to Python, where slicing creates a copy, in NumPy slicing
       creates a :term:`view`.

       For details, see :ref:`combining-advanced-and-basic-indexing`.


   <
       In a dtype declaration, indicates that the data is
       :term:`little-endian` (the bracket is big on the right). ::

           >>> dt = np.dtype('<f')  # little-endian single-precision float


   >
       In a dtype declaration, indicates that the data is
       :term:`big-endian` (the bracket is big on the left). ::

           >>> dt = np.dtype('>H')  # big-endian unsigned short


   advanced indexing
       Rather than using a :doc:`scalar <reference/arrays.scalars>` or slice as
       an index, an axis can be indexed with an array, providing fine-grained
       selection. This is known as :ref:`advanced indexing<advanced-indexing>`
       or "fancy indexing".


   along an axis
       An operation `along axis n` of array ``a`` behaves as if its argument
       were an array of slices of ``a`` where each slice has a successive
       index of axis `n`.

       For example, if ``a`` is a 3 x `N` array, an operation along axis 0
       behaves as if its argument were an array containing slices of each row:

           >>> np.array((a[0,:], a[1,:], a[2,:])) #doctest: +SKIP

       To make it concrete, we can pick the operation to be the array-reversal
       function :func:`numpy.flip`, which accepts an ``axis`` argument. We
       construct a 3 x 4 array ``a``:

           >>> a = np.arange(12).reshape(3,4)
           >>> a
           array([[ 0,  1,  2,  3],
                  [ 4,  5,  6,  7],
                  [ 8,  9, 10, 11]])

       Reversing along axis 0 (the row axis) yields

           >>> np.flip(a,axis=0)
           array([[ 8,  9, 10, 11],
                  [ 4,  5,  6,  7],
                  [ 0,  1,  2,  3]])

       Recalling the definition of `along an axis`,  ``flip`` along axis 0 is
       treating its argument as if it were

           >>> np.array((a[0,:], a[1,:], a[2,:]))
           array([[ 0,  1,  2,  3],
                  [ 4,  5,  6,  7],
                  [ 8,  9, 10, 11]])

       and the result of ``np.flip(a,axis=0)`` is to reverse the slices:

           >>> np.array((a[2,:],a[1,:],a[0,:]))
           array([[ 8,  9, 10, 11],
                  [ 4,  5,  6,  7],
                  [ 0,  1,  2,  3]])


   array
       Used synonymously in the NumPy docs with :term:`ndarray`.


   array_like
       Any :doc:`scalar <reference/arrays.scalars>` or
       :term:`python:sequence`
       that can be interpreted as an ndarray.  In addition to ndarrays
       and scalars this category includes lists (possibly nested and with
       different element types) and tuples. Any argument accepted by
       :doc:`numpy.array <reference/generated/numpy.array>`
       is array_like. ::

           >>> a = np.array([[1, 2.0], [0, 0], (1+1j, 3.)])

           >>> a
           array([[1.+0.j, 2.+0.j],
                  [0.+0.j, 0.+0.j],
                  [1.+1.j, 3.+0.j]])


   array scalar
       An :doc:`array scalar <reference/arrays.scalars>` is an instance of the types/classes float32, float64, 
       etc.. For uniformity in handling operands, NumPy treats a scalar as 
       an array of zero dimension. In contrast, a 0-dimensional array is an :doc:`ndarray <reference/arrays.ndarray>` instance 
       containing precisely one value. 


   axis
       Another term for an array dimension. Axes are numbered left to right;
       axis 0 is the first element in the shape tuple.

       In a two-dimensional vector, the elements of axis 0 are rows and the
       elements of axis 1 are columns.

       In higher dimensions, the picture changes. NumPy prints
       higher-dimensional vectors as replications of row-by-column building
       blocks, as in this three-dimensional vector:

           >>> a = np.arange(12).reshape(2,2,3)
           >>> a
           array([[[ 0,  1,  2],
                   [ 3,  4,  5]],
                  [[ 6,  7,  8],
                   [ 9, 10, 11]]])

       ``a`` is depicted as a two-element array whose elements are 2x3 vectors.
       From this point of view, rows and columns are the final two axes,
       respectively, in any shape.

       This rule helps you anticipate how a vector will be printed, and
       conversely how to find the index of any of the printed elements. For
       instance, in the example, the last two values of 8's index must be 0 and
       2. Since 8 appears in the second of the two 2x3's, the first index must
       be 1:

           >>> a[1,0,2]
           8

       A convenient way to count dimensions in a printed vector is to
       count ``[`` symbols after the open-parenthesis. This is
       useful in distinguishing, say, a (1,2,3) shape from a (2,3) shape:

           >>> a = np.arange(6).reshape(2,3)
           >>> a.ndim
           2
           >>> a
           array([[0, 1, 2],
                  [3, 4, 5]])

           >>> a = np.arange(6).reshape(1,2,3)
           >>> a.ndim
           3
           >>> a
           array([[[0, 1, 2],
                   [3, 4, 5]]])


   .base

       If an array does not own its memory, then its
       :doc:`base <reference/generated/numpy.ndarray.base>` attribute returns
       the object whose memory the array is referencing. That object may be
       referencing the memory from still another object, so the owning object
       may be ``a.base.base.base...``. Some writers erroneously claim that
       testing ``base`` determines if arrays are :term:`view`\ s. For the
       correct way, see :func:`numpy.shares_memory`.


   big-endian
       See `Endianness <https://en.wikipedia.org/wiki/Endianness>`_.


   BLAS
       `Basic Linear Algebra Subprograms <https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>`_


   broadcast
       *broadcasting* is NumPy's ability to process ndarrays of
       different sizes as if all were the same size.

       It permits an elegant do-what-I-mean behavior where, for instance,
       adding a scalar to a vector adds the scalar value to every element.

           >>> a = np.arange(3)
           >>> a
           array([0, 1, 2])

           >>> a + [3, 3, 3]
           array([3, 4, 5])

           >>> a + 3
           array([3, 4, 5])

       Ordinarly, vector operands must all be the same size, because NumPy
       works element by element -- for instance, ``c = a * b`` is ::

           c[0,0,0] = a[0,0,0] * b[0,0,0]
           c[0,0,1] = a[0,0,1] * b[0,0,1]
          ...

       But in certain useful cases, NumPy can duplicate data along "missing"
       axes or "too-short" dimensions so shapes will match. The duplication
       costs no memory or time. For details, see
       :doc:`Broadcasting. <user/basics.broadcasting>`


   C order
       Same as :term:`row-major`.


   column-major
       See `Row- and column-major order <https://en.wikipedia.org/wiki/Row-_and_column-major_order>`_.


   contiguous
       An array is contiguous if
           * it occupies an unbroken block of memory, and
           * array elements with higher indexes occupy higher addresses (that
             is, no :term:`stride` is negative).


   copy
       See :term:`view`.


   dimension
       See :term:`axis`.


   dtype
       The datatype describing the (identically typed) elements in an ndarray.
       It can be changed to reinterpret the array contents. For details, see
       :doc:`Data type objects (dtype). <reference/arrays.dtypes>`


   fancy indexing
       Another term for :term:`advanced indexing`.


   field
       In a :term:`structured data type`, each subtype is called a `field`.
       The `field` has a name (a string), a type (any valid dtype), and
       an optional `title`. See :ref:`arrays.dtypes`.


   Fortran order
       Same as :term:`column-major`.


   flattened
       See :term:`ravel`.


   homogeneous
       All elements of a homogeneous array have the same type. ndarrays, in
       contrast to Python lists, are homogeneous. The type can be complicated,
       as in a :term:`structured array`, but all elements have that type.

       NumPy `object arrays <#term-object-array>`_, which contain references to
       Python objects, fill the role of heterogeneous arrays.


   itemsize
       The size of the dtype element in bytes.


   little-endian
       See `Endianness <https://en.wikipedia.org/wiki/Endianness>`_.


   mask
       A boolean array used to select only certain elements for an operation:

           >>> x = np.arange(5)
           >>> x
           array([0, 1, 2, 3, 4])

           >>> mask = (x > 2)
           >>> mask
           array([False, False, False, True,  True])

           >>> x[mask] = -1
           >>> x
           array([ 0,  1,  2,  -1, -1])


   masked array
       Bad or missing data can be cleanly ignored by putting it in a masked
       array, which has an internal boolean array indicating invalid
       entries. Operations with masked arrays ignore these entries. ::

         >>> a = np.ma.masked_array([np.nan, 2, np.nan], [True, False, True])
         >>> a
         masked_array(data=[--, 2.0, --],
                      mask=[ True, False,  True],
                fill_value=1e+20)

         >>> a + [1, 2, 3]
         masked_array(data=[--, 4.0, --],
                      mask=[ True, False,  True],
                fill_value=1e+20)

       For details, see :doc:`Masked arrays. <reference/maskedarray>`


   matrix
       NumPy's two-dimensional
       :doc:`matrix class <reference/generated/numpy.matrix>`
       should no longer be used; use regular ndarrays.


   ndarray
      :doc:`NumPy's basic structure <reference/arrays>`.


   object array
       An array whose dtype is ``object``; that is, it contains references to
       Python objects. Indexing the array dereferences the Python objects, so
       unlike other ndarrays, an object array has the ability to hold
       heterogeneous objects.


   ravel
       :doc:`numpy.ravel \
       <reference/generated/numpy.ravel>`
       and :doc:`numpy.flatten \
       <reference/generated/numpy.ndarray.flatten>`
       both flatten an ndarray. ``ravel`` will return a view if possible;
       ``flatten`` always returns a copy.

       Flattening collapses a multimdimensional array to a single dimension;
       details of how this is done (for instance, whether ``a[n+1]`` should be
       the next row or next column) are parameters.


   record array
       A :term:`structured array` with allowing access in an attribute style
       (``a.field``) in addition to ``a['field']``. For details, see
       :doc:`numpy.recarray. <reference/generated/numpy.recarray>`


   row-major
       See `Row- and column-major order <https://en.wikipedia.org/wiki/Row-_and_column-major_order>`_.
       NumPy creates arrays in row-major order by default.


   scalar
       In NumPy, usually a synonym for :term:`array scalar`.


   shape
       A tuple showing the length of each dimension of an ndarray. The
       length of the tuple itself is the number of dimensions
       (:doc:`numpy.ndim <reference/generated/numpy.ndarray.ndim>`).
       The product of the tuple elements is the number of elements in the
       array. For details, see
       :doc:`numpy.ndarray.shape <reference/generated/numpy.ndarray.shape>`.


   stride
       Physical memory is one-dimensional;  strides provide a mechanism to map
       a given index to an address in memory. For an N-dimensional array, its
       ``strides`` attribute is an N-element tuple; advancing from index
       ``i`` to index ``i+1`` on axis ``n`` means adding ``a.strides[n]`` bytes
       to the address.

       Strides are computed automatically from an array's dtype and
       shape, but can be directly specified using
       :doc:`as_strided. <reference/generated/numpy.lib.stride_tricks.as_strided>`

       For details, see
       :doc:`numpy.ndarray.strides <reference/generated/numpy.ndarray.strides>`.

       To see how striding underlies the power of NumPy views, see
       `The NumPy array: a structure for efficient numerical computation. \
       <https://arxiv.org/pdf/1102.1523.pdf>`_


   structured array
       Array whose :term:`dtype` is a :term:`structured data type`.


   structured data type
       Users can create arbitrarily complex :term:`dtypes <dtype>`
       that can include other arrays and dtypes. These composite dtypes are called
       :doc:`structured data types. <user/basics.rec>`


   subarray
      An array nested in a :term:`structured data type`, as ``b`` is here:

        >>> dt = np.dtype([('a', np.int32), ('b', np.float32, (3,))])
        >>> np.zeros(3, dtype=dt)
        array([(0, [0., 0., 0.]), (0, [0., 0., 0.]), (0, [0., 0., 0.])],
              dtype=[('a', '<i4'), ('b', '<f4', (3,))])


   subarray data type
       An element of a structured datatype that behaves like an ndarray.


   title
       An alias for a field name in a structured datatype.


   type
       In NumPy, usually a synonym for :term:`dtype`. For the more general
       Python meaning, :term:`see here. <python:type>`


   ufunc
       NumPy's fast element-by-element computation (:term:`vectorization`)
       gives a choice which function gets applied. The general term for the
       function is ``ufunc``, short for ``universal function``. NumPy routines
       have built-in ufuncs, but users can also
       :doc:`write their own. <reference/ufuncs>`


   vectorization
       NumPy hands off array processing to C, where looping and computation are
       much faster than in Python. To exploit this, programmers using NumPy
       eliminate Python loops in favor of array-to-array operations.
       :term:`vectorization` can refer both to the C offloading and to
       structuring NumPy code to leverage it.

   view
       Without touching underlying data, NumPy can make one array appear
       to change its datatype and shape.

       An array created this way is a `view`, and NumPy often exploits the
       performance gain of using a view versus making a new array.

       A potential drawback is that writing to a view can alter the original
       as well. If this is a problem, NumPy instead needs to create a
       physically distinct array -- a `copy`.

       Some NumPy routines always return views, some always return copies, some
       may return one or the other, and for some the choice can be specified.
       Responsibility for managing views and copies falls to the programmer.
       :func:`numpy.shares_memory` will check whether ``b`` is a view of
       ``a``, but an exact answer isn't always feasible, as the documentation
       page explains.

         >>> x = np.arange(5)
         >>> x
         array([0, 1, 2, 3, 4])

         >>> y = x[::2]
         >>> y
         array([0, 2, 4])

         >>> x[0] = 3 # changing x changes y as well, since y is a view on x
         >>> y
         array([3, 2, 4])

:orphan:

Getting started
===============**************
Reporting bugs
**************

File bug reports or feature requests, and make contributions
(e.g. code patches), by opening a "new issue" on GitHub:

- NumPy Issues: https://github.com/numpy/numpy/issues

Please give as much information as you can in the ticket. It is extremely
useful if you can supply a small self-contained code snippet that reproduces
the problem. Also specify the component, the version you are referring to and
the milestone.

Report bugs to the appropriate GitHub project (there is one for NumPy
and a different one for SciPy).

More information can be found on the
https://www.scipy.org/scipylib/dev-zone.html website.
.. _numpy_docs_mainpage:

###################
NumPy documentation
###################

.. toctree::
   :maxdepth: 1
   :hidden:

   User Guide <user/index>
   API reference <reference/index>
   Development <dev/index>


**Version**: |version|

**Download documentation**:
`PDF Version <https://numpy.org/doc/stable/numpy-user.pdf>`_ |
`Historical versions of documentation <https://numpy.org/doc/>`_
   
**Useful links**:
`Installation <https://numpy.org/install/>`_ |
`Source Repository <https://github.com/numpy/numpy>`_ |
`Issue Tracker <https://github.com/numpy/numpy/issues>`_ |
`Q&A Support <https://numpy.org/gethelp/>`_ |
`Mailing List <https://mail.python.org/mailman/listinfo/numpy-discussion>`_

NumPy is the fundamental package for scientific computing in Python. It is a
Python library that provides a multidimensional array object, various derived
objects (such as masked arrays and matrices), and an assortment of routines for
fast operations on arrays, including mathematical, logical, shape manipulation,
sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra,
basic statistical operations, random simulation and much more.

.. panels::
    :card: + intro-card text-center
    :column: col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2

    ---
    :img-top: ../source/_static/index-images/getting_started.svg

    Getting Started
    ^^^^^^^^^^^^^^^

    New to NumPy? Check out the Absolute Beginner's Guide. It contains an
    introduction to NumPy's main concepts and links to additional tutorials.

    .. link-button:: user/absolute_beginners
        :type: ref
        :text: 
        :classes: stretched-link
    
    ---
    :img-top: ../source/_static/index-images/user_guide.svg

    User Guide
    ^^^^^^^^^^

    The user guide provides in-depth information on the
    key concepts of NumPy with useful background information and explanation.

    .. link-button:: user
        :type: ref
        :text: 
        :classes: stretched-link

    ---
    :img-top: ../source/_static/index-images/api.svg

    API Reference
    ^^^^^^^^^^^^^

    The reference guide contains a detailed description of the functions,
    modules, and objects included in NumPy. The reference describes how the
    methods work and which parameters can be used. It assumes that you have an
    understanding of the key concepts.

    .. link-button:: reference
        :type: ref
        :text: 
        :classes: stretched-link 

    ---
    :img-top: ../source/_static/index-images/contributor.svg

    Contributor's Guide
    ^^^^^^^^^^^^^^^

    Want to add to the codebase? Can help add translation or a flowchart to the
    documentation? The contributing guidelines will guide you through the
    process of improving NumPy.

    .. link-button:: devindex
        :type: ref
        :text: 
        :classes: stretched-link

.. This is not really the index page, that is found in
   _templates/indexcontent.html The toctree content here will be added to the
   top of the template header
*************
Release notes
*************

.. toctree::
    :maxdepth: 3

    1.23.0 <release/1.23.0-notes>
    1.22.1 <release/1.22.1-notes>
    1.22.0 <release/1.22.0-notes>
    1.21.5 <release/1.21.5-notes>
    1.21.4 <release/1.21.4-notes>
    1.21.3 <release/1.21.3-notes>
    1.21.2 <release/1.21.2-notes>
    1.21.1 <release/1.21.1-notes>
    1.21.0 <release/1.21.0-notes>
    1.20.3 <release/1.20.3-notes>
    1.20.2 <release/1.20.2-notes>
    1.20.1 <release/1.20.1-notes>
    1.20.0 <release/1.20.0-notes>
    1.19.5 <release/1.19.5-notes>
    1.19.4 <release/1.19.4-notes>
    1.19.3 <release/1.19.3-notes>
    1.19.2 <release/1.19.2-notes>
    1.19.1 <release/1.19.1-notes>
    1.19.0 <release/1.19.0-notes>
    1.18.5 <release/1.18.5-notes>
    1.18.4 <release/1.18.4-notes>
    1.18.3 <release/1.18.3-notes>
    1.18.2 <release/1.18.2-notes>
    1.18.1 <release/1.18.1-notes>
    1.18.0 <release/1.18.0-notes>
    1.17.5 <release/1.17.5-notes>
    1.17.4 <release/1.17.4-notes>
    1.17.3 <release/1.17.3-notes>
    1.17.2 <release/1.17.2-notes>
    1.17.1 <release/1.17.1-notes>
    1.17.0 <release/1.17.0-notes>
    1.16.6 <release/1.16.6-notes>
    1.16.5 <release/1.16.5-notes>
    1.16.4 <release/1.16.4-notes>
    1.16.3 <release/1.16.3-notes>
    1.16.2 <release/1.16.2-notes>
    1.16.1 <release/1.16.1-notes>
    1.16.0 <release/1.16.0-notes>
    1.15.4 <release/1.15.4-notes>
    1.15.3 <release/1.15.3-notes>
    1.15.2 <release/1.15.2-notes>
    1.15.1 <release/1.15.1-notes>
    1.15.0 <release/1.15.0-notes>
    1.14.6 <release/1.14.6-notes>
    1.14.5 <release/1.14.5-notes>
    1.14.4 <release/1.14.4-notes>
    1.14.3 <release/1.14.3-notes>
    1.14.2 <release/1.14.2-notes>
    1.14.1 <release/1.14.1-notes>
    1.14.0 <release/1.14.0-notes>
    1.13.3 <release/1.13.3-notes>
    1.13.2 <release/1.13.2-notes>
    1.13.1 <release/1.13.1-notes>
    1.13.0 <release/1.13.0-notes>
    1.12.1 <release/1.12.1-notes>
    1.12.0 <release/1.12.0-notes>
    1.11.3 <release/1.11.3-notes>
    1.11.2 <release/1.11.2-notes>
    1.11.1 <release/1.11.1-notes>
    1.11.0 <release/1.11.0-notes>
    1.10.4 <release/1.10.4-notes>
    1.10.3 <release/1.10.3-notes>
    1.10.2 <release/1.10.2-notes>
    1.10.1 <release/1.10.1-notes>
    1.10.0 <release/1.10.0-notes>
    1.9.2 <release/1.9.2-notes>
    1.9.1 <release/1.9.1-notes>
    1.9.0 <release/1.9.0-notes>
    1.8.2 <release/1.8.2-notes>
    1.8.1 <release/1.8.1-notes>
    1.8.0 <release/1.8.0-notes>
    1.7.2 <release/1.7.2-notes>
    1.7.1 <release/1.7.1-notes>
    1.7.0 <release/1.7.0-notes>
    1.6.2 <release/1.6.2-notes>
    1.6.1 <release/1.6.1-notes>
    1.6.0 <release/1.6.0-notes>
    1.5.0 <release/1.5.0-notes>
    1.4.0 <release/1.4.0-notes>
    1.3.0 <release/1.3.0-notes>
{{ fullname | escape | underline}}

.. automodule:: {{ fullname }}

   {% block docstring %}
   {% endblock %}


{% extends "!autosummary/module.rst" %}

{# This file is almost the same as the default, but adds :toctree: to the autosummary directives.
   The original can be found at `sphinx/ext/autosummary/templates/autosummary/module.rst`. #}

{% block attributes %}
{% if attributes %}
   .. rubric:: Module Attributes

   .. autosummary::
      :toctree:
   {% for item in attributes %}
      {{ item }}
   {%- endfor %}
{% endif %}
{% endblock %}

{% block functions %}
{% if functions %}
   .. rubric:: Functions

   .. autosummary::
      :toctree:
   {% for item in functions %}
      {{ item }}
   {%- endfor %}
{% endif %}
{% endblock %}

{% block classes %}
{% if classes %}
   .. rubric:: Classes

   .. autosummary::
      :toctree:
   {% for item in classes %}
      {{ item }}
   {%- endfor %}
{% endif %}
{% endblock %}
:orphan:

{{ fullname | escape | underline}}

.. currentmodule:: {{ module }}

method

.. auto{{ objtype }}:: {{ fullname | replace("numpy.", "numpy::") }}

{# In the fullname (e.g. `numpy.ma.MaskedArray.methodname`), the module name
is ambiguous. Using a `::` separator (e.g. `numpy::ma.MaskedArray.methodname`)
specifies `numpy` as the module name. #}
:orphan:

{{ fullname | escape | underline}}

.. currentmodule:: {{ module }}

member

.. auto{{ objtype }}:: {{ fullname | replace("numpy.", "numpy::") }}

{# In the fullname (e.g. `numpy.ma.MaskedArray.methodname`), the module name
is ambiguous. Using a `::` separator (e.g. `numpy::ma.MaskedArray.methodname`)
specifies `numpy` as the module name. #}
:orphan:

{{ fullname | escape | underline}}

.. currentmodule:: {{ module }}

attribute

.. auto{{ objtype }}:: {{ fullname | replace("numpy.", "numpy::") }}

{# In the fullname (e.g. `numpy.ma.MaskedArray.methodname`), the module name
is ambiguous. Using a `::` separator (e.g. `numpy::ma.MaskedArray.methodname`)
specifies `numpy` as the module name. #}
{% if objtype == 'property' %}
:orphan:
{% endif %}

{{ fullname | escape | underline}}

.. currentmodule:: {{ module }}

{% if objtype == 'property' %}
property
{% endif %}

.. auto{{ objtype }}:: {{ fullname | replace("numpy.", "numpy::") }}

{# In the fullname (e.g. `numpy.ma.MaskedArray.methodname`), the module name
is ambiguous. Using a `::` separator (e.g. `numpy::ma.MaskedArray.methodname`)
specifies `numpy` as the module name. #}
{% extends "!autosummary/class.rst" %}

{% block methods %}
{% if methods %}
   .. HACK -- the point here is that we don't want this to appear in the output, but the autosummary should still generate the pages.
      .. autosummary::
         :toctree:
      {% for item in all_methods %}
         {%- if not item.startswith('_') or item in ['__call__'] %}
         {{ name }}.{{ item }}
         {%- endif -%}
      {%- endfor %}
{% endif %}
{% endblock %}

{% block attributes %}
{% if attributes %}
   .. HACK -- the point here is that we don't want this to appear in the output, but the autosummary should still generate the pages.
      .. autosummary::
         :toctree:
      {% for item in all_attributes %}
         {%- if not item.startswith('_') %}
         {{ name }}.{{ item }}
         {%- endif -%}
      {%- endfor %}
{% endif %}
{% endblock %}
.. currentmodule:: numpy

==========================
NumPy 1.16.6 Release Notes
==========================

The NumPy 1.16.6 release fixes bugs reported against the 1.16.5 release, and
also backports several enhancements from master that seem appropriate for a
release series that is the last to support Python 2.7. The wheels on PyPI are
linked with OpenBLAS v0.3.7, which should fix errors on Skylake series
cpus.

Downstream developers building this release should use Cython >= 0.29.2 and, if
using OpenBLAS, OpenBLAS >= v0.3.7. The supported Python versions are 2.7 and
3.5-3.7.

Highlights
==========

- The ``np.testing.utils`` functions have been updated from 1.19.0-dev0.
  This improves the function documentation and error messages as well
  extending the ``assert_array_compare`` function to additional types.


New functions
=============

Allow matmul (`@` operator) to work with object arrays.
-------------------------------------------------------
This is an enhancement that was added in NumPy 1.17 and seems reasonable to
include in the LTS 1.16 release series.


Compatibility notes
===================

Fix regression in matmul (`@` operator) for boolean types
---------------------------------------------------------
Booleans were being treated as integers rather than booleans,
which was a regression from previous behavior.


Improvements
============

Array comparison assertions include maximum differences
-------------------------------------------------------
Error messages from array comparison tests such as ``testing.assert_allclose``
now include "max absolute difference" and "max relative difference," in
addition to the previous "mismatch" percentage.  This information makes it
easier to update absolute and relative error tolerances.

Contributors
============

A total of 10 people contributed to this release.

* CakeWithSteak
* Charles Harris
* Chris Burr
* Eric Wieser
* Fernando Saravia
* Lars Grueter
* Matti Picus
* Maxwell Aladago
* Qiming Sun
* Warren Weckesser

Pull requests merged
====================

A total of 14 pull requests were merged for this release.

* `#14211 <https://github.com/numpy/numpy/pull/14211>`__: BUG: Fix uint-overflow if padding with linear_ramp and negative...
* `#14275 <https://github.com/numpy/numpy/pull/14275>`__: BUG: fixing to allow unpickling of PY3 pickles from PY2
* `#14340 <https://github.com/numpy/numpy/pull/14340>`__: BUG: Fix misuse of .names and .fields in various places (backport...
* `#14423 <https://github.com/numpy/numpy/pull/14423>`__: BUG: test, fix regression in converting to ctypes.
* `#14434 <https://github.com/numpy/numpy/pull/14434>`__: BUG: Fixed maximum relative error reporting in assert_allclose
* `#14509 <https://github.com/numpy/numpy/pull/14509>`__: BUG: Fix regression in boolean matmul.
* `#14686 <https://github.com/numpy/numpy/pull/14686>`__: BUG: properly define PyArray_DescrCheck
* `#14853 <https://github.com/numpy/numpy/pull/14853>`__: BLD: add 'apt update' to shippable
* `#14854 <https://github.com/numpy/numpy/pull/14854>`__: BUG: Fix _ctypes class circular reference. (#13808)
* `#14856 <https://github.com/numpy/numpy/pull/14856>`__: BUG: Fix `np.einsum` errors on Power9 Linux and z/Linux
* `#14863 <https://github.com/numpy/numpy/pull/14863>`__: BLD: Prevent -flto from optimising long double representation...
* `#14864 <https://github.com/numpy/numpy/pull/14864>`__: BUG: lib: Fix histogram problem with signed integer arrays.
* `#15172 <https://github.com/numpy/numpy/pull/15172>`__: ENH: Backport improvements to testing functions.
* `#15191 <https://github.com/numpy/numpy/pull/15191>`__: REL: Prepare for 1.16.6 release.
==========================
NumPy 1.16.3 Release Notes
==========================

The NumPy 1.16.3 release fixes bugs reported against the 1.16.2 release, and
also backports several enhancements from master that seem appropriate for a
release series that is the last to support Python 2.7. The wheels on PyPI are
linked with OpenBLAS v0.3.4+,  which should fix the known threading issues
found in previous OpenBLAS versions.

Downstream developers building this release should use Cython >= 0.29.2 and,
if using OpenBLAS, OpenBLAS > v0.3.4.

The most noticeable change in this release is that unpickling object arrays
when loading ``*.npy`` or ``*.npz`` files now requires an explicit opt-in.
This backwards incompatible change was made in response to
`CVE-2019-6446 <https://nvd.nist.gov/vuln/detail/CVE-2019-6446>`_.


Compatibility notes
===================

Unpickling while loading requires explicit opt-in
-------------------------------------------------
The functions ``np.load``, and ``np.lib.format.read_array`` take an
`allow_pickle` keyword which now defaults to ``False`` in response to
`CVE-2019-6446 <https://nvd.nist.gov/vuln/detail/CVE-2019-6446>`_.


Improvements
============

Covariance in `random.mvnormal` cast to double
----------------------------------------------
This should make the tolerance used when checking the singular values of the
covariance matrix more meaningful.


Changes
=======

``__array_interface__`` offset now works as documented
------------------------------------------------------
The interface may use an ``offset`` value that was previously mistakenly
ignored.

==========================
NumPy 1.16.5 Release Notes
==========================

The NumPy 1.16.5 release fixes bugs reported against the 1.16.4 release, and
also backports several enhancements from master that seem appropriate for a
release series that is the last to support Python 2.7. The wheels on PyPI are
linked with OpenBLAS v0.3.7-dev, which should fix errors on Skylake series
cpus.

Downstream developers building this release should use Cython >= 0.29.2 and, if
using OpenBLAS, OpenBLAS >= v0.3.7. The supported Python versions are 2.7 and
3.5-3.7.


Contributors
============

A total of 18 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alexander Shadchin
* Allan Haldane
* Bruce Merry +
* Charles Harris
* Colin Snyder +
* Dan Allan +
* Emile +
* Eric Wieser
* Grey Baker +
* Maksim Shabunin +
* Marten van Kerkwijk
* Matti Picus
* Peter Andreas Entschev +
* Ralf Gommers
* Richard Harris +
* Sebastian Berg
* Sergei Lebedev +
* Stephan Hoyer

Pull requests merged
====================

A total of 23 pull requests were merged for this release.

* `#13742 <https://github.com/numpy/numpy/pull/13742>`__: ENH: Add project URLs to setup.py
* `#13823 <https://github.com/numpy/numpy/pull/13823>`__: TEST, ENH: fix tests and ctypes code for PyPy
* `#13845 <https://github.com/numpy/numpy/pull/13845>`__: BUG: use npy_intp instead of int for indexing array
* `#13867 <https://github.com/numpy/numpy/pull/13867>`__: TST: Ignore DeprecationWarning during nose imports
* `#13905 <https://github.com/numpy/numpy/pull/13905>`__: BUG: Fix use-after-free in boolean indexing
* `#13933 <https://github.com/numpy/numpy/pull/13933>`__: MAINT/BUG/DOC: Fix errors in _add_newdocs
* `#13984 <https://github.com/numpy/numpy/pull/13984>`__: BUG: fix byte order reversal for datetime64[ns]
* `#13994 <https://github.com/numpy/numpy/pull/13994>`__: MAINT,BUG: Use nbytes to also catch empty descr during allocation
* `#14042 <https://github.com/numpy/numpy/pull/14042>`__: BUG: np.array cleared errors occurred in PyMemoryView_FromObject
* `#14043 <https://github.com/numpy/numpy/pull/14043>`__: BUG: Fixes for Undefined Behavior Sanitizer (UBSan) errors.
* `#14044 <https://github.com/numpy/numpy/pull/14044>`__: BUG: ensure that casting to/from structured is properly checked.
* `#14045 <https://github.com/numpy/numpy/pull/14045>`__: MAINT: fix histogram*d dispatchers
* `#14046 <https://github.com/numpy/numpy/pull/14046>`__: BUG: further fixup to histogram2d dispatcher.
* `#14052 <https://github.com/numpy/numpy/pull/14052>`__: BUG: Replace contextlib.suppress for Python 2.7
* `#14056 <https://github.com/numpy/numpy/pull/14056>`__: BUG: fix compilation of 3rd party modules with Py_LIMITED_API...
* `#14057 <https://github.com/numpy/numpy/pull/14057>`__: BUG: Fix memory leak in dtype from dict constructor
* `#14058 <https://github.com/numpy/numpy/pull/14058>`__: DOC: Document array_function at a higher level.
* `#14084 <https://github.com/numpy/numpy/pull/14084>`__: BUG, DOC: add new recfunctions to `__all__`
* `#14162 <https://github.com/numpy/numpy/pull/14162>`__: BUG: Remove stray print that causes a SystemError on python 3.7
* `#14297 <https://github.com/numpy/numpy/pull/14297>`__: TST: Pin pytest version to 5.0.1.
* `#14322 <https://github.com/numpy/numpy/pull/14322>`__: ENH: Enable huge pages in all Linux builds
* `#14346 <https://github.com/numpy/numpy/pull/14346>`__: BUG: fix behavior of structured_to_unstructured on non-trivial...
* `#14382 <https://github.com/numpy/numpy/pull/14382>`__: REL: Prepare for the NumPy 1.16.5 release.
.. currentmodule:: numpy

==========================
NumPy 1.21.1 Release Notes
==========================
The NumPy 1.21.1 is maintenance release that fixes bugs discovered after the
1.21.0 release and updates OpenBLAS to v0.3.17 to deal with problems on arm64.

The Python versions supported for this release are 3.7-3.9. The 1.21.x series
is compatible with development Python 3.10. Python 3.10 will be officially
supported after it is released.

.. warning::
   There are unresolved problems compiling NumPy 1.20.0 with gcc-11.1.

   * Optimization level `-O3` results in many incorrect warnings when
     running the tests.
   * On some hardware NumPY will hang in an infinite loop.

Contributors
============

A total of 11 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Ganesh Kathiresan
* Gregory R. Lee
* Hugo Defois +
* Kevin Sheppard
* Matti Picus
* Ralf Gommers
* Sayed Adel
* Sebastian Berg
* Thomas J. Fan

Pull requests merged
====================

A total of 26 pull requests were merged for this release.

* `#19311 <https://github.com/numpy/numpy/pull/19311>`__: REV,BUG: Replace ``NotImplemented`` with ``typing.Any``
* `#19324 <https://github.com/numpy/numpy/pull/19324>`__: MAINT: Fixed the return-dtype of ``ndarray.real`` and ``imag``
* `#19330 <https://github.com/numpy/numpy/pull/19330>`__: MAINT: Replace ``"dtype[Any]"`` with ``dtype`` in the definiton of...
* `#19342 <https://github.com/numpy/numpy/pull/19342>`__: DOC: Fix some docstrings that crash pdf generation.
* `#19343 <https://github.com/numpy/numpy/pull/19343>`__: MAINT: bump scipy-mathjax
* `#19347 <https://github.com/numpy/numpy/pull/19347>`__: BUG: Fix arr.flat.index for large arrays and big-endian machines
* `#19348 <https://github.com/numpy/numpy/pull/19348>`__: ENH: add ``numpy.f2py.get_include`` function
* `#19349 <https://github.com/numpy/numpy/pull/19349>`__: BUG: Fix reference count leak in ufunc dtype handling
* `#19350 <https://github.com/numpy/numpy/pull/19350>`__: MAINT: Annotate missing attributes of ``np.number`` subclasses
* `#19351 <https://github.com/numpy/numpy/pull/19351>`__: BUG: Fix cast safety and comparisons for zero sized voids
* `#19352 <https://github.com/numpy/numpy/pull/19352>`__: BUG: Correct Cython declaration in random
* `#19353 <https://github.com/numpy/numpy/pull/19353>`__: BUG: protect against accessing base attribute of a NULL subarray
* `#19365 <https://github.com/numpy/numpy/pull/19365>`__: BUG, SIMD: Fix detecting AVX512 features on Darwin
* `#19366 <https://github.com/numpy/numpy/pull/19366>`__: MAINT: remove ``print()``'s in distutils template handling
* `#19390 <https://github.com/numpy/numpy/pull/19390>`__: ENH: SIMD architectures to show_config
* `#19391 <https://github.com/numpy/numpy/pull/19391>`__: BUG: Do not raise deprecation warning for all nans in unique...
* `#19392 <https://github.com/numpy/numpy/pull/19392>`__: BUG: Fix NULL special case in object-to-any cast code
* `#19430 <https://github.com/numpy/numpy/pull/19430>`__: MAINT: Use arm64-graviton2 for testing on travis
* `#19495 <https://github.com/numpy/numpy/pull/19495>`__: BUILD: update OpenBLAS to v0.3.17
* `#19496 <https://github.com/numpy/numpy/pull/19496>`__: MAINT: Avoid unicode characters in division SIMD code comments
* `#19499 <https://github.com/numpy/numpy/pull/19499>`__: BUG, SIMD: Fix infinite loop during count non-zero on GCC-11
* `#19500 <https://github.com/numpy/numpy/pull/19500>`__: BUG: fix a numpy.npiter leak in npyiter_multi_index_set
* `#19501 <https://github.com/numpy/numpy/pull/19501>`__: TST: Fix a ``GenericAlias`` test failure for python 3.9.0
* `#19502 <https://github.com/numpy/numpy/pull/19502>`__: MAINT: Start testing with Python 3.10.0b3.
* `#19503 <https://github.com/numpy/numpy/pull/19503>`__: MAINT: Add missing dtype overloads for object- and ctypes-based...
* `#19510 <https://github.com/numpy/numpy/pull/19510>`__: REL: Prepare for NumPy 1.21.1 release.

.. currentmodule:: numpy

==========================
NumPy 1.23.0 Release Notes
==========================


Highlights
==========


New functions
=============


Deprecations
============


Future Changes
==============


Expired deprecations
====================


Compatibility notes
===================


C API changes
=============


New Features
============


Improvements
============


Changes
=======
==========================
NumPy 1.12.0 Release Notes
==========================

This release supports Python 2.7 and 3.4 - 3.6.

Highlights
==========
The NumPy 1.12.0 release contains a large number of fixes and improvements, but
few that stand out above all others. That makes picking out the highlights
somewhat arbitrary but the following may be of particular interest or indicate
areas likely to have future consequences.

* Order of operations in ``np.einsum`` can now be optimized for large speed improvements.
* New ``signature`` argument to ``np.vectorize`` for vectorizing with core dimensions.
* The ``keepdims`` argument was added to many functions.
* New context manager for testing warnings
* Support for BLIS in numpy.distutils
* Much improved support for PyPy (not yet finished)

Dropped Support
===============

* Support for Python 2.6, 3.2, and 3.3 has been dropped.


Added Support
=============

* Support for PyPy 2.7 v5.6.0 has been added. While not complete (nditer
  ``updateifcopy`` is not supported yet), this is a milestone for PyPy's
  C-API compatibility layer.


Build System Changes
====================

* Library order is preserved, instead of being reordered to match that of
  the directories.


Deprecations
============

Assignment of ndarray object's ``data`` attribute
-------------------------------------------------
Assigning the 'data' attribute is an inherently unsafe operation as pointed
out in gh-7083. Such a capability will be removed in the future.

Unsafe int casting of the num attribute in ``linspace``
-------------------------------------------------------
``np.linspace`` now raises DeprecationWarning when num cannot be safely
interpreted as an integer.

Insufficient bit width parameter to ``binary_repr``
---------------------------------------------------
If a 'width' parameter is passed into ``binary_repr`` that is insufficient to
represent the number in base 2 (positive) or 2's complement (negative) form,
the function used to silently ignore the parameter and return a representation
using the minimal number of bits needed for the form in question. Such behavior
is now considered unsafe from a user perspective and will raise an error in the
future.


Future Changes
==============

* In 1.13 NAT will always compare False except for ``NAT != NAT``,
  which will be True.  In short, NAT will behave like NaN
* In 1.13 ``np.average`` will preserve subclasses, to match the behavior of most
  other numpy functions such as np.mean. In particular, this means calls which
  returned a scalar may return a 0-d subclass object instead.

Multiple-field manipulation of structured arrays
------------------------------------------------
In 1.13 the behavior of structured arrays involving multiple fields will change
in two ways:

First, indexing a structured array with multiple fields (eg,
``arr[['f1', 'f3']]``) will return a view into the original array in 1.13,
instead of a copy. Note the returned view will have extra padding bytes
corresponding to intervening fields in the original array, unlike the copy in
1.12, which will affect code such as ``arr[['f1', 'f3']].view(newdtype)``.

Second, for numpy versions 1.6 to 1.12 assignment between structured arrays
occurs "by field name": Fields in the destination array are set to the
identically-named field in the source array or to 0 if the source does not have
a field::

    >>> a = np.array([(1,2),(3,4)], dtype=[('x', 'i4'), ('y', 'i4')])
    >>> b = np.ones(2, dtype=[('z', 'i4'), ('y', 'i4'), ('x', 'i4')])
    >>> b[:] = a
    >>> b
    array([(0, 2, 1), (0, 4, 3)],
          dtype=[('z', '<i4'), ('y', '<i4'), ('x', '<i4')])

In 1.13 assignment will instead occur "by position": The Nth field of the
destination will be set to the Nth field of the source regardless of field
name. The old behavior can be obtained by using indexing to reorder the fields
before
assignment, e.g., ``b[['x', 'y']] = a[['y', 'x']]``.


Compatibility notes
===================

DeprecationWarning to error
---------------------------

* Indexing with floats raises ``IndexError``,
  e.g., a[0, 0.0].
* Indexing with non-integer array_like raises ``IndexError``,
  e.g., ``a['1', '2']``
* Indexing with multiple ellipsis raises ``IndexError``,
  e.g., ``a[..., ...]``.
* Non-integers used as index values raise ``TypeError``,
  e.g., in ``reshape``, ``take``, and specifying reduce axis.

FutureWarning to changed behavior
---------------------------------

* ``np.full`` now returns an array of the fill-value's dtype if no dtype is
  given, instead of defaulting to float.
* ``np.average`` will emit a warning if the argument is a subclass of ndarray,
  as the subclass will be preserved starting in 1.13. (see Future Changes)

``power`` and ``**`` raise errors for integer to negative integer powers
------------------------------------------------------------------------
The previous behavior depended on whether numpy scalar integers or numpy
integer arrays were involved.

For arrays

* Zero to negative integer powers returned least integral value.
* Both 1, -1 to negative integer powers returned correct values.
* The remaining integers returned zero when raised to negative integer powers.

For scalars

* Zero to negative integer powers returned least integral value.
* Both 1, -1 to negative integer powers returned correct values.
* The remaining integers sometimes returned zero, sometimes the
  correct float depending on the integer type combination.

All of these cases now raise a ``ValueError`` except for those integer
combinations whose common type is float, for instance uint64 and int8. It was
felt that a simple rule was the best way to go rather than have special
exceptions for the integer units. If you need negative powers, use an inexact
type.

Relaxed stride checking is the default
--------------------------------------
This will have some impact on code that assumed that ``F_CONTIGUOUS`` and
``C_CONTIGUOUS`` were mutually exclusive and could be set to determine the
default order for arrays that are now both.

The ``np.percentile`` 'midpoint' interpolation method fixed for exact indices
-----------------------------------------------------------------------------
The 'midpoint' interpolator now gives the same result as 'lower' and 'higher' when
the two coincide. Previous behavior of 'lower' + 0.5 is fixed.

``keepdims`` kwarg is passed through to user-class methods
----------------------------------------------------------
numpy functions that take a ``keepdims`` kwarg now pass the value
through to the corresponding methods on ndarray sub-classes.  Previously the
``keepdims`` keyword would be silently dropped.  These functions now have
the following behavior:

1. If user does not provide ``keepdims``, no keyword is passed to the underlying
   method.
2. Any user-provided value of ``keepdims`` is passed through as a keyword
   argument to the method.

This will raise in the case where the method does not support a
``keepdims`` kwarg and the user explicitly passes in ``keepdims``.

The following functions are changed: ``sum``, ``product``,
``sometrue``, ``alltrue``, ``any``, ``all``, ``amax``, ``amin``,
``prod``, ``mean``, ``std``, ``var``, ``nanmin``, ``nanmax``,
``nansum``, ``nanprod``, ``nanmean``, ``nanmedian``, ``nanvar``,
``nanstd``

``bitwise_and`` identity changed
--------------------------------
The previous identity was 1, it is now -1. See entry in Improvements for
more explanation.

ma.median warns and returns nan when unmasked invalid values are encountered
----------------------------------------------------------------------------
Similar to unmasked median the masked median `ma.median` now emits a Runtime
warning and returns `NaN` in slices where an unmasked `NaN` is present.

Greater consistency in ``assert_almost_equal``
----------------------------------------------
The precision check for scalars has been changed to match that for arrays. It
is now::

    abs(actual - desired) < 1.5 * 10**(-decimal)

Note that this is looser than previously documented, but agrees with the
previous implementation used in ``assert_array_almost_equal``. Due to the
change in implementation some very delicate tests may fail that did not
fail before.

``NoseTester`` behaviour of warnings during testing
---------------------------------------------------
When ``raise_warnings="develop"`` is given, all uncaught warnings will now
be considered a test failure. Previously only selected ones were raised.
Warnings which are not caught or raised (mostly when in release mode)
will be shown once during the test cycle similar to the default python
settings.

``assert_warns`` and ``deprecated`` decorator more specific
-----------------------------------------------------------
The ``assert_warns`` function and context manager are now more specific
to the given warning category. This increased specificity leads to them
being handled according to the outer warning settings. This means that
no warning may be raised in cases where a wrong category warning is given
and ignored outside the context. Alternatively the increased specificity
may mean that warnings that were incorrectly ignored will now be shown
or raised. See also the new ``suppress_warnings`` context manager.
The same is true for the ``deprecated`` decorator.

C API
-----
No changes.


New Features
============

Writeable keyword argument for ``as_strided``
---------------------------------------------
``np.lib.stride_tricks.as_strided`` now has a ``writeable``
keyword argument. It can be set to False when no write operation
to the returned array is expected to avoid accidental
unpredictable writes.

``axes`` keyword argument for ``rot90``
---------------------------------------
The ``axes`` keyword argument in ``rot90`` determines the plane in which the
array is rotated. It defaults to ``axes=(0,1)`` as in the original function.

Generalized ``flip``
--------------------
``flipud`` and ``fliplr`` reverse the elements of an array along axis=0 and
axis=1 respectively. The newly added ``flip`` function reverses the elements of
an array along any given axis.

* ``np.count_nonzero`` now has an ``axis`` parameter, allowing
  non-zero counts to be generated on more than just a flattened
  array object.

BLIS support in ``numpy.distutils``
-----------------------------------
Building against the BLAS implementation provided by the BLIS library is now
supported.  See the ``[blis]`` section in ``site.cfg.example`` (in the root of
the numpy repo or source distribution).

Hook in ``numpy/__init__.py`` to run distribution-specific checks
-----------------------------------------------------------------
Binary distributions of numpy may need to run specific hardware checks or load
specific libraries during numpy initialization.  For example, if we are
distributing numpy with a BLAS library that requires SSE2 instructions, we
would like to check the machine on which numpy is running does have SSE2 in
order to give an informative error.

Add a hook in ``numpy/__init__.py`` to import a ``numpy/_distributor_init.py``
file that will remain empty (bar a docstring) in the standard numpy source,
but that can be overwritten by people making binary distributions of numpy.

New nanfunctions ``nancumsum`` and ``nancumprod`` added
-------------------------------------------------------
Nan-functions ``nancumsum`` and ``nancumprod`` have been added to
compute ``cumsum`` and ``cumprod`` by ignoring nans.

``np.interp`` can now interpolate complex values
------------------------------------------------
``np.lib.interp(x, xp, fp)`` now allows the interpolated array ``fp``
to be complex and will interpolate at ``complex128`` precision.

New polynomial evaluation function ``polyvalfromroots`` added
-------------------------------------------------------------
The new function ``polyvalfromroots`` evaluates a polynomial at given points
from the roots of the polynomial. This is useful for higher order polynomials,
where expansion into polynomial coefficients is inaccurate at machine
precision.

New array creation function ``geomspace`` added
-----------------------------------------------
The new function ``geomspace`` generates a geometric sequence.  It is similar
to ``logspace``, but with start and stop specified directly:
``geomspace(start, stop)`` behaves the same as
``logspace(log10(start), log10(stop))``.

New context manager for testing warnings
----------------------------------------
A new context manager ``suppress_warnings`` has been added to the testing
utils. This context manager is designed to help reliably test warnings.
Specifically to reliably filter/ignore warnings. Ignoring warnings
by using an "ignore" filter in Python versions before 3.4.x can quickly
result in these (or similar) warnings not being tested reliably.

The context manager allows to filter (as well as record) warnings similar
to the ``catch_warnings`` context, but allows for easier specificity.
Also printing warnings that have not been filtered or nesting the
context manager will work as expected. Additionally, it is possible
to use the context manager as a decorator which can be useful when
multiple tests give need to hide the same warning.

New masked array functions ``ma.convolve`` and ``ma.correlate`` added
---------------------------------------------------------------------
These functions wrapped the non-masked versions, but propagate through masked
values. There are two different propagation modes. The default causes masked
values to contaminate the result with masks, but the other mode only outputs
masks if there is no alternative.

New ``float_power`` ufunc
-------------------------
The new ``float_power`` ufunc is like the ``power`` function except all
computation is done in a minimum precision of float64. There was a long
discussion on the numpy mailing list of how to treat integers to negative
integer powers and a popular proposal was that the ``__pow__`` operator should
always return results of at least float64 precision. The ``float_power``
function implements that option. Note that it does not support object arrays.

``np.loadtxt`` now supports a single integer as ``usecol`` argument
-------------------------------------------------------------------
Instead of using ``usecol=(n,)`` to read the nth column of a file
it is now allowed to use ``usecol=n``. Also the error message is
more user friendly when a non-integer is passed as a column index.

Improved automated bin estimators for ``histogram``
---------------------------------------------------
Added 'doane' and 'sqrt' estimators to ``histogram`` via the ``bins``
argument. Added support for range-restricted histograms with automated
bin estimation.

``np.roll`` can now roll multiple axes at the same time
-------------------------------------------------------
The ``shift`` and ``axis`` arguments to ``roll`` are now broadcast against each
other, and each specified axis is shifted accordingly.

The ``__complex__`` method has been implemented for the ndarrays
----------------------------------------------------------------
Calling ``complex()`` on a size 1 array will now cast to a python
complex.

``pathlib.Path`` objects now supported
--------------------------------------
The standard ``np.load``, ``np.save``, ``np.loadtxt``, ``np.savez``, and similar
functions can now take ``pathlib.Path`` objects as an argument instead of a
filename or open file object.

New ``bits`` attribute for ``np.finfo``
---------------------------------------
This makes ``np.finfo`` consistent with ``np.iinfo`` which already has that
attribute.

New ``signature`` argument to ``np.vectorize``
----------------------------------------------
This argument allows for vectorizing user defined functions with core
dimensions, in the style of NumPy's
:ref:`generalized universal functions<c-api.generalized-ufuncs>`. This allows
for vectorizing a much broader class of functions. For example, an arbitrary
distance metric that combines two vectors to produce a scalar could be
vectorized with ``signature='(n),(n)->()'``. See ``np.vectorize`` for full
details.

Emit py3kwarnings for division of integer arrays
------------------------------------------------
To help people migrate their code bases from Python 2 to Python 3, the
python interpreter has a handy option -3, which issues warnings at runtime.
One of its warnings is for integer division::

    $ python -3 -c "2/3"

    -c:1: DeprecationWarning: classic int division

In Python 3, the new integer division semantics also apply to numpy arrays.
With this version, numpy will emit a similar warning::

    $ python -3 -c "import numpy as np; np.array(2)/np.array(3)"

    -c:1: DeprecationWarning: numpy: classic int division

numpy.sctypes now includes bytes on Python3 too
-----------------------------------------------
Previously, it included str (bytes) and unicode on Python2, but only str
(unicode) on Python3.


Improvements
============

``bitwise_and`` identity changed
--------------------------------
The previous identity was 1 with the result that all bits except the LSB were
masked out when the reduce method was used.  The new identity is -1, which
should work properly on twos complement machines as all bits will be set to
one.

Generalized Ufuncs will now unlock the GIL
------------------------------------------
Generalized Ufuncs, including most of the linalg module, will now unlock
the Python global interpreter lock.

Caches in `np.fft` are now bounded in total size and item count
---------------------------------------------------------------
The caches in `np.fft` that speed up successive FFTs of the same length can no
longer grow without bounds. They have been replaced with LRU (least recently
used) caches that automatically evict no longer needed items if either the
memory size or item count limit has been reached.

Improved handling of zero-width string/unicode dtypes
-----------------------------------------------------
Fixed several interfaces that explicitly disallowed arrays with zero-width
string dtypes (i.e. ``dtype('S0')`` or ``dtype('U0')``, and fixed several
bugs where such dtypes were not handled properly.  In particular, changed
``ndarray.__new__`` to not implicitly convert ``dtype('S0')`` to
``dtype('S1')`` (and likewise for unicode) when creating new arrays.

Integer ufuncs vectorized with AVX2
-----------------------------------
If the cpu supports it at runtime the basic integer ufuncs now use AVX2
instructions. This feature is currently only available when compiled with GCC.

Order of operations optimization in ``np.einsum``
--------------------------------------------------
``np.einsum`` now supports the ``optimize`` argument which will optimize the
order of contraction. For example, ``np.einsum`` would complete the chain dot
example ``np.einsum(‘ij,jk,kl->il’, a, b, c)`` in a single pass which would
scale like ``N^4``; however, when ``optimize=True`` ``np.einsum`` will create
an intermediate array to reduce this scaling to ``N^3`` or effectively
``np.dot(a, b).dot(c)``. Usage of intermediate tensors to reduce scaling has
been applied to the general einsum summation notation. See ``np.einsum_path``
for more details.

quicksort has been changed to an introsort
------------------------------------------
The quicksort kind of ``np.sort`` and ``np.argsort`` is now an introsort which
is regular quicksort but changing to a heapsort when not enough progress is
made. This retains the good quicksort performance while changing the worst case
runtime from ``O(N^2)`` to ``O(N*log(N))``.

``ediff1d`` improved performance and subclass handling
------------------------------------------------------
The ediff1d function uses an array instead on a flat iterator for the
subtraction.  When to_begin or to_end is not None, the subtraction is performed
in place to eliminate a copy operation.  A side effect is that certain
subclasses are handled better, namely astropy.Quantity, since the complete
array is created, wrapped, and then begin and end values are set, instead of
using concatenate.

Improved precision of ``ndarray.mean`` for float16 arrays
---------------------------------------------------------
The computation of the mean of float16 arrays is now carried out in float32 for
improved precision. This should be useful in packages such as Theano
where the precision of float16 is adequate and its smaller footprint is
desirable.


Changes
=======

All array-like methods are now called with keyword arguments in fromnumeric.py
------------------------------------------------------------------------------
Internally, many array-like methods in fromnumeric.py were being called with
positional arguments instead of keyword arguments as their external signatures
were doing. This caused a complication in the downstream 'pandas' library
that encountered an issue with 'numpy' compatibility. Now, all array-like
methods in this module are called with keyword arguments instead.

Operations on np.memmap objects return numpy arrays in most cases
-----------------------------------------------------------------
Previously operations on a memmap object would misleadingly return a memmap
instance even if the result was actually not memmapped.  For example,
``arr + 1`` or ``arr + arr`` would return memmap instances, although no memory
from the output array is memmapped. Version 1.12 returns ordinary numpy arrays
from these operations.

Also, reduction of a memmap (e.g.  ``.sum(axis=None``) now returns a numpy
scalar instead of a 0d memmap.

stacklevel of warnings increased
--------------------------------
The stacklevel for python based warnings was increased so that most warnings
will report the offending line of the user code instead of the line the
warning itself is given. Passing of stacklevel is now tested to ensure that
new warnings will receive the ``stacklevel`` argument.

This causes warnings with the "default" or "module" filter to be shown once
for every offending user code line or user module instead of only once. On
python versions before 3.4, this can cause warnings to appear that were falsely
ignored before, which may be surprising especially in test suits.
.. currentmodule:: numpy

==========================
NumPy 1.20.0 Release Notes
==========================
This NumPy release is the largest so made to date, some 684 PRs contributed by
184 people have been merged. See the list of highlights below for more details.
The Python versions supported for this release are 3.7-3.9, support for Python
3.6 has been dropped. Highlights are

- Annotations for NumPy functions. This work is ongoing and improvements can
  be expected pending feedback from users.

- Wider use of SIMD to increase execution speed of ufuncs. Much work has been
  done in introducing universal functions that will ease use of modern
  features across different hardware platforms. This work is ongoing.

- Preliminary work in changing the dtype and casting implementations in order to
  provide an easier path to extending dtypes. This work is ongoing but enough
  has been done to allow experimentation and feedback.

- Extensive documentation improvements comprising some 185 PR merges. This work
  is ongoing and part of the larger project to improve NumPy's online presence
  and usefulness to new users.

- Further cleanups related to removing Python 2.7. This improves code
  readability and removes technical debt.

- Preliminary support for the upcoming Cython 3.0.


New functions
=============

The random.Generator class has a new ``permuted`` function.
-----------------------------------------------------------
The new function differs from ``shuffle`` and ``permutation`` in that the
subarrays indexed by an axis are permuted rather than the axis being treated as
a separate 1-D array for every combination of the other indexes. For example,
it is now possible to permute the rows or columns of a 2-D array.

(`gh-15121 <https://github.com/numpy/numpy/pull/15121>`__)

``sliding_window_view`` provides a sliding window view for numpy arrays
-----------------------------------------------------------------------
`numpy.lib.stride_tricks.sliding_window_view` constructs views on numpy
arrays that offer a sliding or moving window access to the array. This allows
for the simple implementation of certain algorithms, such as running means.

(`gh-17394 <https://github.com/numpy/numpy/pull/17394>`__)

`numpy.broadcast_shapes` is a new user-facing function
------------------------------------------------------
`~numpy.broadcast_shapes` gets the resulting shape from
broadcasting the given shape tuples against each other.

.. code:: python

    >>> np.broadcast_shapes((1, 2), (3, 1))
    (3, 2)

    >>> np.broadcast_shapes(2, (3, 1))
    (3, 2)

    >>> np.broadcast_shapes((6, 7), (5, 6, 1), (7,), (5, 1, 7))
    (5, 6, 7)

(`gh-17535 <https://github.com/numpy/numpy/pull/17535>`__)


Deprecations
============

Using the aliases of builtin types like ``np.int`` is deprecated
----------------------------------------------------------------

For a long time, ``np.int`` has been an alias of the builtin ``int``. This is
repeatedly a cause of confusion for newcomers, and existed mainly for historic
reasons.

These aliases have been deprecated. The table below shows the full list of
deprecated aliases, along with their exact meaning. Replacing uses of items in
the first column with the contents of the second column will work identically
and silence the deprecation warning.

The third column lists alternative NumPy names which may occasionally be
preferential. See also :ref:`basics.types` for additional details.

=================  ============  ==================================================================
Deprecated name    Identical to  NumPy scalar type names
=================  ============  ==================================================================
``numpy.bool``     ``bool``      `numpy.bool_`
``numpy.int``      ``int``       `numpy.int_` (default), ``numpy.int64``, or ``numpy.int32``
``numpy.float``    ``float``     `numpy.float64`, `numpy.float_`, `numpy.double` (equivalent)
``numpy.complex``  ``complex``   `numpy.complex128`, `numpy.complex_`, `numpy.cdouble` (equivalent)
``numpy.object``   ``object``    `numpy.object_`
``numpy.str``      ``str``       `numpy.str_`
``numpy.long``     ``int``       `numpy.int_` (C ``long``), `numpy.longlong` (largest integer type)
``numpy.unicode``  ``str``       `numpy.unicode_`
=================  ============  ==================================================================

To give a clear guideline for the vast majority of cases, for the types
``bool``, ``object``, ``str`` (and ``unicode``) using the plain version
is shorter and clear, and generally a good replacement.
For ``float`` and ``complex`` you can use ``float64`` and ``complex128``
if you wish to be more explicit about the precision.

For ``np.int`` a direct replacement with ``np.int_`` or ``int`` is also
good and will not change behavior, but the precision will continue to depend
on the computer and operating system.
If you want to be more explicit and review the current use, you have the
following alternatives:

* ``np.int64`` or ``np.int32`` to specify the precision exactly.
  This ensures that results cannot depend on the computer or operating system.
* ``np.int_`` or ``int`` (the default), but be aware that it depends on
  the computer and operating system.
* The C types: ``np.cint`` (int), ``np.int_`` (long), ``np.longlong``.
* ``np.intp`` which is 32bit on 32bit machines 64bit on 64bit machines.
  This can be the best type to use for indexing.

When used with ``np.dtype(...)`` or ``dtype=...`` changing it to the
NumPy name as mentioned above will have no effect on the output.
If used as a scalar with::

    np.float(123)

changing it can subtly change the result.  In this case, the Python version
``float(123)`` or ``int(12.)`` is normally preferable, although the NumPy
version may be useful for consistency with NumPy arrays (for example,
NumPy behaves differently for things like division by zero).

(`gh-14882 <https://github.com/numpy/numpy/pull/14882>`__)

Passing ``shape=None`` to functions with a non-optional shape argument is deprecated
------------------------------------------------------------------------------------
Previously, this was an alias for passing ``shape=()``.
This deprecation is emitted by `PyArray_IntpConverter` in the C API. If your
API is intended to support passing ``None``, then you should check for ``None``
prior to invoking the converter, so as to be able to distinguish ``None`` and
``()``.

(`gh-15886 <https://github.com/numpy/numpy/pull/15886>`__)

Indexing errors will be reported even when index result is empty
----------------------------------------------------------------
In the future, NumPy will raise an IndexError when an
integer array index contains out of bound values even if a non-indexed
dimension is of length 0. This will now emit a DeprecationWarning.
This can happen when the array is previously empty, or an empty
slice is involved::

    arr1 = np.zeros((5, 0))
    arr1[[20]]
    arr2 = np.zeros((5, 5))
    arr2[[20], :0]

Previously the non-empty index ``[20]`` was not checked for correctness.
It will now be checked causing a deprecation warning which will be turned
into an error. This also applies to assignments.

(`gh-15900 <https://github.com/numpy/numpy/pull/15900>`__)

Inexact matches for ``mode`` and ``searchside`` are deprecated
--------------------------------------------------------------
Inexact and case insensitive matches for ``mode`` and ``searchside`` were valid
inputs earlier and will give a DeprecationWarning now.  For example, below are
some example usages which are now deprecated and will give a
DeprecationWarning::

    import numpy as np
    arr = np.array([[3, 6, 6], [4, 5, 1]])
    # mode: inexact match
    np.ravel_multi_index(arr, (7, 6), mode="clap")  # should be "clip"
    # searchside: inexact match
    np.searchsorted(arr[0], 4, side='random')  # should be "right"

(`gh-16056 <https://github.com/numpy/numpy/pull/16056>`__)

Deprecation of `numpy.dual`
---------------------------
The module `numpy.dual` is deprecated.  Instead of importing functions
from `numpy.dual`, the functions should be imported directly from NumPy
or SciPy.

(`gh-16156 <https://github.com/numpy/numpy/pull/16156>`__)

``outer`` and ``ufunc.outer`` deprecated for matrix
---------------------------------------------------
``np.matrix`` use with `~numpy.outer` or generic ufunc outer
calls such as ``numpy.add.outer``. Previously, matrix was
converted to an array here. This will not be done in the future
requiring a manual conversion to arrays.

(`gh-16232 <https://github.com/numpy/numpy/pull/16232>`__)

Further Numeric Style types Deprecated
--------------------------------------

The remaining numeric-style type codes ``Bytes0``, ``Str0``,
``Uint32``, ``Uint64``, and ``Datetime64``
have been deprecated.  The lower-case variants should be used
instead.  For bytes and string ``"S"`` and ``"U"``
are further alternatives.

(`gh-16554 <https://github.com/numpy/numpy/pull/16554>`__)

The ``ndincr`` method of ``ndindex`` is deprecated
--------------------------------------------------
The documentation has warned against using this function since NumPy 1.8.
Use ``next(it)`` instead of ``it.ndincr()``.

(`gh-17233 <https://github.com/numpy/numpy/pull/17233>`__)

ArrayLike objects which do not define ``__len__`` and ``__getitem__``
---------------------------------------------------------------------
Objects which define one of the protocols ``__array__``,
``__array_interface__``, or ``__array_struct__`` but are not sequences
(usually defined by having a ``__len__`` and ``__getitem__``) will behave
differently during array-coercion in the future.

When nested inside sequences, such as ``np.array([array_like])``, these
were handled as a single Python object rather than an array.
In the future they will behave identically to::

    np.array([np.array(array_like)])

This change should only have an effect if ``np.array(array_like)`` is not 0-D.
The solution to this warning may depend on the object:

* Some array-likes may expect the new behaviour, and users can ignore the
  warning.  The object can choose to expose the sequence protocol to opt-in
  to the new behaviour.
* For example, ``shapely`` will allow conversion to an array-like using
  ``line.coords`` rather than ``np.asarray(line)``. Users may work around
  the warning, or use the new convention when it becomes available.

Unfortunately, using the new behaviour can only be achieved by
calling ``np.array(array_like)``.

If you wish to ensure that the old behaviour remains unchanged, please create
an object array and then fill it explicitly, for example::

    arr = np.empty(3, dtype=object)
    arr[:] = [array_like1, array_like2, array_like3]

This will ensure NumPy knows to not enter the array-like and use it as
a object instead.

(`gh-17973 <https://github.com/numpy/numpy/pull/17973>`__)


Future Changes
==============

Arrays cannot be using subarray dtypes
--------------------------------------
Array creation and casting using ``np.array(arr, dtype)``
and ``arr.astype(dtype)`` will use different logic when ``dtype``
is a subarray dtype such as ``np.dtype("(2)i,")``.

For such a ``dtype`` the following behaviour is true::

    res = np.array(arr, dtype)

    res.dtype is not dtype
    res.dtype is dtype.base
    res.shape == arr.shape + dtype.shape

But ``res`` is filled using the logic::

    res = np.empty(arr.shape + dtype.shape, dtype=dtype.base)
    res[...] = arr

which uses incorrect broadcasting (and often leads to an error).
In the future, this will instead cast each element individually,
leading to the same result as::

    res = np.array(arr, dtype=np.dtype(["f", dtype]))["f"]

Which can normally be used to opt-in to the new behaviour.

This change does not affect ``np.array(list, dtype="(2)i,")`` unless the
``list`` itself includes at least one array.  In particular, the behaviour
is unchanged for a list of tuples.

(`gh-17596 <https://github.com/numpy/numpy/pull/17596>`__)


Expired deprecations
====================

* The deprecation of numeric style type-codes ``np.dtype("Complex64")``
  (with upper case spelling), is expired.  ``"Complex64"`` corresponded to
  ``"complex128"`` and ``"Complex32"`` corresponded to ``"complex64"``.
* The deprecation of ``np.sctypeNA`` and ``np.typeNA`` is expired. Both
  have been removed from the public API. Use ``np.typeDict`` instead.

  (`gh-16554 <https://github.com/numpy/numpy/pull/16554>`__)

* The 14-year deprecation of ``np.ctypeslib.ctypes_load_library`` is expired.
  Use :func:`~numpy.ctypeslib.load_library` instead, which is identical.

  (`gh-17116 <https://github.com/numpy/numpy/pull/17116>`__)

Financial functions removed
---------------------------
In accordance with NEP 32, the financial functions are removed
from NumPy 1.20. The functions that have been removed are ``fv``,
``ipmt``, ``irr``, ``mirr``, ``nper``, ``npv``, ``pmt``, ``ppmt``,
``pv``, and ``rate``.  These functions are available in the
`numpy_financial <https://pypi.org/project/numpy-financial>`_
library.

(`gh-17067 <https://github.com/numpy/numpy/pull/17067>`__)


Compatibility notes
===================

``isinstance(dtype, np.dtype)`` and not ``type(dtype) is not np.dtype``
-----------------------------------------------------------------------
NumPy dtypes are not direct instances of ``np.dtype`` anymore.  Code that
may have used ``type(dtype) is np.dtype`` will always return ``False`` and
must be updated to use the correct version ``isinstance(dtype, np.dtype)``.

This change also affects the C-side macro ``PyArray_DescrCheck`` if compiled
against a NumPy older than 1.16.6. If code uses this macro and wishes to
compile against an older version of NumPy, it must replace the macro
(see also `C API changes`_ section).


Same kind casting in concatenate with ``axis=None``
---------------------------------------------------
When `~numpy.concatenate` is called with ``axis=None``,
the flattened arrays were cast with ``unsafe``. Any other axis
choice uses "same kind". That different default
has been deprecated and "same kind" casting will be used
instead. The new ``casting`` keyword argument
can be used to retain the old behaviour.

(`gh-16134 <https://github.com/numpy/numpy/pull/16134>`__)

NumPy Scalars are cast when assigned to arrays
----------------------------------------------

When creating or assigning to arrays, in all relevant cases NumPy
scalars will now be cast identically to NumPy arrays.  In particular
this changes the behaviour in some cases which previously raised an
error::

    np.array([np.float64(np.nan)], dtype=np.int64)

will succeed and return an undefined result (usually the smallest possible
integer).  This also affects assignments::

    arr[0] = np.float64(np.nan)

At this time, NumPy retains the behaviour for::

    np.array(np.float64(np.nan), dtype=np.int64)

The above changes do not affect Python scalars::

    np.array([float("NaN")], dtype=np.int64)

remains unaffected (``np.nan`` is a Python ``float``, not a NumPy one).
Unlike signed integers, unsigned integers do not retain this special case,
since they always behaved more like casting.
The following code stops raising an error::

    np.array([np.float64(np.nan)], dtype=np.uint64)

To avoid backward compatibility issues, at this time assignment from
``datetime64`` scalar to strings of too short length remains supported.
This means that ``np.asarray(np.datetime64("2020-10-10"), dtype="S5")``
succeeds now, when it failed before.  In the long term this may be
deprecated or the unsafe cast may be allowed generally to make assignment
of arrays and scalars behave consistently.


Array coercion changes when Strings and other types are mixed
-------------------------------------------------------------

When strings and other types are mixed, such as::

    np.array(["string", np.float64(3.)], dtype="S")

The results will change, which may lead to string dtypes with longer strings
in some cases.  In particularly, if ``dtype="S"`` is not provided any numerical
value will lead to a string results long enough to hold all possible numerical
values. (e.g. "S32" for floats).  Note that you should always provide
``dtype="S"`` when converting non-strings to strings.

If ``dtype="S"`` is provided the results will be largely identical to before,
but NumPy scalars (not a Python float like ``1.0``), will still enforce
a uniform string length::

    np.array([np.float64(3.)], dtype="S")  # gives "S32"
    np.array([3.0], dtype="S")  # gives "S3"

Previously the first version gave the same result as the second.


Array coercion restructure
--------------------------

Array coercion has been restructured.  In general, this should not affect
users.  In extremely rare corner cases where array-likes are nested::

    np.array([array_like1])

Things will now be more consistent with::

    np.array([np.array(array_like1)])

This can subtly change output for some badly defined array-likes.
One example for this are array-like objects which are not also sequences
of matching shape.
In NumPy 1.20, a warning will be given when an array-like is not also a
sequence (but behaviour remains identical, see deprecations).
If an array like is also a sequence (defines ``__getitem__`` and ``__len__``)
NumPy will now only use the result given by ``__array__``,
``__array_interface__``, or ``__array_struct__``. This will result in
differences when the (nested) sequence describes a different shape.

(`gh-16200 <https://github.com/numpy/numpy/pull/16200>`__)

Writing to the result of `numpy.broadcast_arrays` will export readonly buffers
------------------------------------------------------------------------------

In NumPy 1.17 `numpy.broadcast_arrays` started warning when the resulting array
was written to. This warning was skipped when the array was used through the
buffer interface (e.g. ``memoryview(arr)``). The same thing will now occur for the
two protocols ``__array_interface__``, and ``__array_struct__`` returning read-only
buffers instead of giving a warning.

(`gh-16350 <https://github.com/numpy/numpy/pull/16350>`__)

Numeric-style type names have been removed from type dictionaries
-----------------------------------------------------------------

To stay in sync with the deprecation for ``np.dtype("Complex64")``
and other numeric-style (capital case) types.  These were removed
from ``np.sctypeDict`` and ``np.typeDict``.  You should use
the lower case versions instead.  Note that ``"Complex64"``
corresponds to ``"complex128"`` and ``"Complex32"`` corresponds
to ``"complex64"``.  The numpy style (new) versions, denote the full
size and not the size of the real/imaginary part.

(`gh-16554 <https://github.com/numpy/numpy/pull/16554>`__)

The ``operator.concat`` function now raises TypeError for array arguments
-------------------------------------------------------------------------
The previous behavior was to fall back to addition and add the two arrays,
which was thought to be unexpected behavior for a concatenation function.

(`gh-16570 <https://github.com/numpy/numpy/pull/16570>`__)

``nickname`` attribute removed from ABCPolyBase
-----------------------------------------------

An abstract property ``nickname`` has been removed from  ``ABCPolyBase`` as it
was no longer used in the derived convenience classes.
This may affect users who have derived classes from ``ABCPolyBase`` and
overridden the methods for representation and display, e.g. ``__str__``,
``__repr__``, ``_repr_latex``, etc.

(`gh-16589 <https://github.com/numpy/numpy/pull/16589>`__)

``float->timedelta`` and ``uint64->timedelta`` promotion will raise a TypeError
-------------------------------------------------------------------------------
Float and timedelta promotion consistently raises a TypeError.
``np.promote_types("float32", "m8")`` aligns with
``np.promote_types("m8", "float32")`` now and both raise a TypeError.
Previously, ``np.promote_types("float32", "m8")`` returned ``"m8"`` which
was considered a bug.

Uint64 and timedelta promotion consistently raises a TypeError.
``np.promote_types("uint64", "m8")`` aligns with
``np.promote_types("m8", "uint64")`` now and both raise a TypeError.
Previously, ``np.promote_types("uint64", "m8")`` returned ``"m8"`` which
was considered a bug.

(`gh-16592 <https://github.com/numpy/numpy/pull/16592>`__)

``numpy.genfromtxt`` now correctly unpacks structured arrays
------------------------------------------------------------
Previously, `numpy.genfromtxt` failed to unpack if it was called with
``unpack=True`` and a structured datatype was passed to the ``dtype`` argument
(or ``dtype=None`` was passed and a structured datatype was inferred).
For example::

    >>> data = StringIO("21 58.0\n35 72.0")
    >>> np.genfromtxt(data, dtype=None, unpack=True)
    array([(21, 58.), (35, 72.)], dtype=[('f0', '<i8'), ('f1', '<f8')])

Structured arrays will now correctly unpack into a list of arrays,
one for each column::

    >>> np.genfromtxt(data, dtype=None, unpack=True)
    [array([21, 35]), array([58., 72.])]

(`gh-16650 <https://github.com/numpy/numpy/pull/16650>`__)

``mgrid``, ``r_``, etc. consistently return correct outputs for non-default precision input
-------------------------------------------------------------------------------------------
Previously, ``np.mgrid[np.float32(0.1):np.float32(0.35):np.float32(0.1),]``
and ``np.r_[0:10:np.complex64(3j)]`` failed to return meaningful output.
This bug potentially affects `~numpy.mgrid`, `~numpy.ogrid`, `~numpy.r_`,
and `~numpy.c_` when an input with dtype other than the default
``float64`` and ``complex128`` and equivalent Python types were used.
The methods have been fixed to handle varying precision correctly.

(`gh-16815 <https://github.com/numpy/numpy/pull/16815>`__)

Boolean array indices with mismatching shapes now properly give ``IndexError``
------------------------------------------------------------------------------

Previously, if a boolean array index matched the size of the indexed array but
not the shape, it was incorrectly allowed in some cases. In other cases, it
gave an error, but the error was incorrectly a ``ValueError`` with a message
about broadcasting instead of the correct ``IndexError``.

For example, the following used to incorrectly give ``ValueError: operands
could not be broadcast together with shapes (2,2) (1,4)``:

.. code:: python

   np.empty((2, 2))[np.array([[True, False, False, False]])]

And the following used to incorrectly return ``array([], dtype=float64)``:

.. code:: python

   np.empty((2, 2))[np.array([[False, False, False, False]])]

Both now correctly give ``IndexError: boolean index did not match indexed
array along dimension 0; dimension is 2 but corresponding boolean dimension is
1``.

(`gh-17010 <https://github.com/numpy/numpy/pull/17010>`__)

Casting errors interrupt Iteration
----------------------------------
When iterating while casting values, an error may stop the iteration
earlier than before. In any case, a failed casting operation always
returned undefined, partial results. Those may now be even more
undefined and partial.
For users of the ``NpyIter`` C-API such cast errors will now
cause the `iternext()` function to return 0 and thus abort
iteration.
Currently, there is no API to detect such an error directly.
It is necessary to check ``PyErr_Occurred()``, which
may be problematic in combination with ``NpyIter_Reset``.
These issues always existed, but new API could be added
if required by users.

(`gh-17029 <https://github.com/numpy/numpy/pull/17029>`__)

f2py generated code may return unicode instead of byte strings
--------------------------------------------------------------
Some byte strings previously returned by f2py generated code may now be unicode
strings. This results from the ongoing Python2 -> Python3 cleanup.

(`gh-17068 <https://github.com/numpy/numpy/pull/17068>`__)

The first element of the ``__array_interface__["data"]`` tuple  must be an integer
----------------------------------------------------------------------------------
This has been the documented interface for many years, but there was still
code that would accept a byte string representation of the pointer address.
That code has been removed, passing the address as a byte string will now
raise an error.

(`gh-17241 <https://github.com/numpy/numpy/pull/17241>`__)

poly1d respects the dtype of all-zero argument
----------------------------------------------
Previously, constructing an instance of ``poly1d`` with all-zero
coefficients would cast the coefficients to ``np.float64``.
This affected the output dtype of methods which construct
``poly1d`` instances internally, such as ``np.polymul``.

(`gh-17577 <https://github.com/numpy/numpy/pull/17577>`__)

The numpy.i file for swig is Python 3 only.
-------------------------------------------
Uses of Python 2.7 C-API functions have been updated to Python 3 only. Users
who need the old version should take it from an older version of NumPy.

(`gh-17580 <https://github.com/numpy/numpy/pull/17580>`__)

Void dtype discovery in ``np.array``
------------------------------------
In calls using ``np.array(..., dtype="V")``, ``arr.astype("V")``,
and similar a TypeError will now be correctly raised unless all
elements have the identical void length. An example for this is::

     np.array([b"1", b"12"], dtype="V")

Which previously returned an array with dtype ``"V2"`` which
cannot represent ``b"1"`` faithfully.

(`gh-17706 <https://github.com/numpy/numpy/pull/17706>`__)


C API changes
=============

The ``PyArray_DescrCheck`` macro is modified
--------------------------------------------
The ``PyArray_DescrCheck`` macro has been updated since NumPy 1.16.6 to be::

    #define PyArray_DescrCheck(op) PyObject_TypeCheck(op, &PyArrayDescr_Type)

Starting with NumPy 1.20 code that is compiled against an earlier version
will be API incompatible with NumPy 1.20.
The fix is to either compile against 1.16.6 (if the NumPy 1.16 release is
the oldest release you wish to support), or manually inline the macro by
replacing it with the new definition::

    PyObject_TypeCheck(op, &PyArrayDescr_Type)

which is compatible with all NumPy versions.


Size of ``np.ndarray`` and ``np.void_`` changed
-----------------------------------------------
The size of the ``PyArrayObject`` and ``PyVoidScalarObject``
structures have changed.  The following header definition has been
removed::

    #define NPY_SIZEOF_PYARRAYOBJECT (sizeof(PyArrayObject_fields))

since the size must not be considered a compile time constant: it will
change for different runtime versions of NumPy.

The most likely relevant use are potential subclasses written in C which
will have to be recompiled and should be updated.  Please see the
documentation for :c:type:`PyArrayObject` for more details and contact
the NumPy developers if you are affected by this change.

NumPy will attempt to give a graceful error but a program expecting a
fixed structure size may have undefined behaviour and likely crash.

(`gh-16938 <https://github.com/numpy/numpy/pull/16938>`__)


New Features
============

``where`` keyword argument for ``numpy.all`` and ``numpy.any`` functions
------------------------------------------------------------------------
The keyword argument ``where`` is added and allows to only consider specified
elements or subaxes from an array in the Boolean evaluation of ``all`` and
``any``. This new keyword is available to the functions ``all`` and ``any``
both via ``numpy`` directly or in the methods of ``numpy.ndarray``.

Any broadcastable Boolean array or a scalar can be set as ``where``. It
defaults to ``True`` to evaluate the functions for all elements in an array if
``where`` is not set by the user. Examples are given in the documentation of
the functions.


``where`` keyword argument for ``numpy`` functions ``mean``, ``std``, ``var``
-----------------------------------------------------------------------------
The keyword argument ``where`` is added and allows to limit the scope in the
calculation of ``mean``, ``std`` and ``var`` to only a subset of elements. It
is available both via ``numpy`` directly or in the methods of
``numpy.ndarray``.

Any broadcastable Boolean array or a scalar can be set as ``where``. It
defaults to ``True`` to evaluate the functions for all elements in an array if
``where`` is not set by the user. Examples are given in the documentation of
the functions.

(`gh-15852 <https://github.com/numpy/numpy/pull/15852>`__)

``norm=backward``, ``forward`` keyword options for ``numpy.fft`` functions
--------------------------------------------------------------------------
The keyword argument option ``norm=backward`` is added as an alias for ``None``
and acts as the default option; using it has the direct transforms unscaled
and the inverse transforms scaled by ``1/n``.

Using the new keyword argument option ``norm=forward`` has the direct
transforms scaled by ``1/n`` and the inverse transforms unscaled (i.e. exactly
opposite to the default option ``norm=backward``).

(`gh-16476 <https://github.com/numpy/numpy/pull/16476>`__)

NumPy is now typed
------------------
Type annotations have been added for large parts of NumPy. There is
also a new `numpy.typing` module that contains useful types for
end-users. The currently available types are

- ``ArrayLike``: for objects that can be coerced to an array
- ``DtypeLike``: for objects that can be coerced to a dtype

(`gh-16515 <https://github.com/numpy/numpy/pull/16515>`__)

``numpy.typing`` is accessible at runtime
-----------------------------------------
The types in ``numpy.typing`` can now be imported at runtime. Code
like the following will now work:

.. code:: python

    from numpy.typing import ArrayLike
    x: ArrayLike = [1, 2, 3, 4]

(`gh-16558 <https://github.com/numpy/numpy/pull/16558>`__)

New ``__f2py_numpy_version__`` attribute for f2py generated modules.
--------------------------------------------------------------------
Because f2py is released together with NumPy, ``__f2py_numpy_version__``
provides a way to track the version f2py used to generate the module.

(`gh-16594 <https://github.com/numpy/numpy/pull/16594>`__)

``mypy`` tests can be run via runtests.py
-----------------------------------------
Currently running mypy with the NumPy stubs configured requires
either:

* Installing NumPy
* Adding the source directory to MYPYPATH and linking to the ``mypy.ini``

Both options are somewhat inconvenient, so add a ``--mypy`` option to runtests
that handles setting things up for you. This will also be useful in the future
for any typing codegen since it will ensure the project is built before type
checking.

(`gh-17123 <https://github.com/numpy/numpy/pull/17123>`__)

Negation of user defined BLAS/LAPACK detection order
----------------------------------------------------
`~numpy.distutils` allows negation of libraries when determining BLAS/LAPACK
libraries.
This may be used to remove an item from the library resolution phase, i.e.
to disallow NetLIB libraries one could do:

.. code:: bash

    NPY_BLAS_ORDER='^blas' NPY_LAPACK_ORDER='^lapack' python setup.py build

That will use any of the accelerated libraries instead.

(`gh-17219 <https://github.com/numpy/numpy/pull/17219>`__)

Allow passing optimizations arguments to asv build
--------------------------------------------------
It is now possible to pass  ``-j``, ``--cpu-baseline``, ``--cpu-dispatch`` and
``--disable-optimization`` flags to ASV build when the ``--bench-compare``
argument is used.

(`gh-17284 <https://github.com/numpy/numpy/pull/17284>`__)

The NVIDIA HPC SDK nvfortran compiler is now supported
------------------------------------------------------
Support for the nvfortran compiler, a version of pgfortran, has been added.

(`gh-17344 <https://github.com/numpy/numpy/pull/17344>`__)

``dtype`` option for ``cov`` and ``corrcoef``
---------------------------------------------
The ``dtype`` option is now available for `numpy.cov` and `numpy.corrcoef`.
It specifies which data-type the returned result should have.
By default the functions still return a `numpy.float64` result.

(`gh-17456 <https://github.com/numpy/numpy/pull/17456>`__)


Improvements
============

Improved string representation for polynomials (``__str__``)
------------------------------------------------------------

The string representation (``__str__``) of all six polynomial types in
`numpy.polynomial` has been updated to give the polynomial as a mathematical
expression instead of an array of coefficients. Two package-wide formats for
the polynomial expressions are available - one using Unicode characters for
superscripts and subscripts, and another using only ASCII characters.

(`gh-15666 <https://github.com/numpy/numpy/pull/15666>`__)

Remove the Accelerate library as a candidate LAPACK library
-----------------------------------------------------------
Apple no longer supports Accelerate. Remove it.

(`gh-15759 <https://github.com/numpy/numpy/pull/15759>`__)

Object arrays containing multi-line objects have a more readable ``repr``
-------------------------------------------------------------------------
If elements of an object array have a ``repr`` containing new lines, then the
wrapped lines will be aligned by column. Notably, this improves the ``repr`` of
nested arrays::

    >>> np.array([np.eye(2), np.eye(3)], dtype=object)
    array([array([[1., 0.],
                  [0., 1.]]),
           array([[1., 0., 0.],
                  [0., 1., 0.],
                  [0., 0., 1.]])], dtype=object)

(`gh-15997 <https://github.com/numpy/numpy/pull/15997>`__)

Concatenate supports providing an output dtype
----------------------------------------------
Support was added to `~numpy.concatenate` to provide
an output ``dtype`` and ``casting`` using keyword
arguments. The ``dtype`` argument cannot be provided
in conjunction with the ``out`` one.

(`gh-16134 <https://github.com/numpy/numpy/pull/16134>`__)

Thread safe f2py callback functions
-----------------------------------

Callback functions in f2py are now thread safe.

(`gh-16519 <https://github.com/numpy/numpy/pull/16519>`__)

`numpy.core.records.fromfile` now supports file-like objects
------------------------------------------------------------
`numpy.rec.fromfile` can now use file-like objects, for instance
:py:class:`io.BytesIO`

(`gh-16675 <https://github.com/numpy/numpy/pull/16675>`__)

RPATH support on AIX added to distutils
---------------------------------------
This allows SciPy to be built on AIX.

(`gh-16710 <https://github.com/numpy/numpy/pull/16710>`__)

Use f90 compiler specified by the command line args
---------------------------------------------------

The compiler command selection for Fortran Portland Group Compiler is changed
in `numpy.distutils.fcompiler`.  This only affects the linking command.  This
forces the use of the executable provided by the command line option (if
provided) instead of the pgfortran executable.  If no executable is provided to
the command line option it defaults to the pgf90 executable, which is an alias
for pgfortran according to the PGI documentation.

(`gh-16730 <https://github.com/numpy/numpy/pull/16730>`__)

Add NumPy declarations for Cython 3.0 and later
-----------------------------------------------

The pxd declarations for Cython 3.0 were improved to avoid using deprecated
NumPy C-API features.  Extension modules built with Cython 3.0+ that use NumPy
can now set the C macro ``NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION`` to avoid
C compiler warnings about deprecated API usage.

(`gh-16986 <https://github.com/numpy/numpy/pull/16986>`__)

Make the window functions exactly symmetric
-------------------------------------------
Make sure the window functions provided by NumPy are symmetric. There were
previously small deviations from symmetry due to numerical precision that are
now avoided by better arrangement of the computation.

(`gh-17195 <https://github.com/numpy/numpy/pull/17195>`__)


Performance improvements and changes
====================================

Enable multi-platform SIMD compiler optimizations
-------------------------------------------------

A series of improvements for NumPy infrastructure to pave the way to
**NEP-38**, that can be summarized as follow:

-  **New Build Arguments**

   -  ``--cpu-baseline`` to specify the minimal set of required
      optimizations, default value is ``min`` which provides the minimum
      CPU features that can safely run on a wide range of users
      platforms.

   -  ``--cpu-dispatch`` to specify the dispatched set of additional
      optimizations, default value is ``max -xop -fma4`` which enables
      all CPU features, except for AMD legacy features.

   -  ``--disable-optimization`` to explicitly disable the whole new
      improvements, It also adds a new **C** compiler #definition
      called ``NPY_DISABLE_OPTIMIZATION`` which it can be used as
      guard for any SIMD code.

-  **Advanced CPU dispatcher**

   A flexible cross-architecture CPU dispatcher built on the top of
   Python/Numpy distutils, support all common compilers with a wide range of
   CPU features.

   The new dispatcher requires a special file extension ``*.dispatch.c`` to
   mark the dispatch-able **C** sources. These sources have the ability to be
   compiled multiple times so that each compilation process represents certain
   CPU features and provides different #definitions and flags that affect the
   code paths.

-  **New auto-generated C header ``core/src/common/_cpu_dispatch.h``**

   This header is generated by the distutils module ``ccompiler_opt``, and
   contains all the #definitions and headers of instruction sets, that had been
   configured through command arguments '--cpu-baseline' and '--cpu-dispatch'.

-  **New C header ``core/src/common/npy_cpu_dispatch.h``**

   This header contains all utilities that required for the whole CPU
   dispatching process, it also can be considered as a bridge linking the new
   infrastructure work with NumPy CPU runtime detection.

-  **Add new attributes to NumPy umath module(Python level)**

   - ``__cpu_baseline__`` a list contains the minimal set of required
     optimizations that supported by the compiler and platform according to the
     specified values to command argument '--cpu-baseline'.

   - ``__cpu_dispatch__`` a list contains the dispatched set of additional
     optimizations that supported by the compiler and platform according to the
     specified values to command argument '--cpu-dispatch'.

-  **Print the supported CPU features during the run of PytestTester**

(`gh-13516 <https://github.com/numpy/numpy/pull/13516>`__)


Changes
=======

Changed behavior of ``divmod(1., 0.)`` and related functions
------------------------------------------------------------
The changes also assure that different compiler versions have the same behavior
for nan or inf usages in these operations. This was previously compiler
dependent, we now force the invalid and divide by zero flags, making the
results the same across compilers. For example, gcc-5, gcc-8, or gcc-9 now
result in the same behavior. The changes are tabulated below:

.. list-table:: Summary of New Behavior
   :widths: auto
   :header-rows: 1

   * - Operator
     - Old Warning
     - New Warning
     - Old Result
     - New Result
     - Works on MacOS
   * - np.divmod(1.0, 0.0)
     - Invalid
     - Invalid and Dividebyzero
     - nan, nan
     - inf, nan
     - Yes
   * - np.fmod(1.0, 0.0)
     - Invalid
     - Invalid
     - nan
     - nan
     - No? Yes
   * - np.floor_divide(1.0, 0.0)
     - Invalid
     - Dividebyzero
     - nan
     - inf
     - Yes
   * - np.remainder(1.0, 0.0)
     - Invalid
     - Invalid
     - nan
     - nan
     - Yes

(`gh-16161 <https://github.com/numpy/numpy/pull/16161>`__)

``np.linspace`` on integers now uses floor
------------------------------------------
When using a ``int`` dtype in `numpy.linspace`, previously float values would
be rounded towards zero. Now `numpy.floor` is used instead, which rounds toward
``-inf``. This changes the results for negative values. For example, the
following would previously give::

    >>> np.linspace(-3, 1, 8, dtype=int)
    array([-3, -2, -1, -1,  0,  0,  0,  1])

and now results in::

    >>> np.linspace(-3, 1, 8, dtype=int)
    array([-3, -3, -2, -2, -1, -1,  0,  1])

The former result can still be obtained with::

    >>> np.linspace(-3, 1, 8).astype(int)
    array([-3, -2, -1, -1,  0,  0,  0,  1])

(`gh-16841 <https://github.com/numpy/numpy/pull/16841>`__)

==========================
NumPy 1.15.4 Release Notes
==========================

This is a bugfix release for bugs and regressions reported following the 1.15.3
release.  The Python versions supported by this release are 2.7, 3.4-3.7. The
wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg
problems reported for NumPy 1.14.

Compatibility Note
==================

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit
binaries.  That will also be the case in future releases. See
`#11625 <https://github.com/numpy/numpy/issues/11625>`__ for the related
discussion.  Those needing 32-bit support should look elsewhere or build
from source.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Sebastian Berg
* bbbbbbbbba +

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#12296 <https://github.com/numpy/numpy/pull/12296>`__: BUG: Dealloc cached buffer info
* `#12297 <https://github.com/numpy/numpy/pull/12297>`__: BUG: Fix fill value in masked array '==' and '!=' ops.
* `#12307 <https://github.com/numpy/numpy/pull/12307>`__: DOC: Correct the default value of `optimize` in `numpy.einsum`
* `#12320 <https://github.com/numpy/numpy/pull/12320>`__: REL: Prepare for the NumPy 1.15.4 release
.. currentmodule:: numpy

==========================
NumPy 1.17.1 Release Notes
==========================

This release contains a number of fixes for bugs reported against NumPy 1.17.0
along with a few documentation and build improvements.  The Python versions
supported are 3.5-3.7, note that Python 2.7 has been dropped.  Python 3.8b3
should work with the released source packages, but there are no future
guarantees.

Downstream developers should use Cython >= 0.29.13 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid problems on the Skylake architecture. The NumPy wheels
on PyPI are built from the OpenBLAS development branch in order to avoid those
problems.


Contributors
============

A total of 17 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Alexander Jung +
* Allan Haldane
* Charles Harris
* Eric Wieser
* Giuseppe Cuccu +
* Hiroyuki V. Yamazaki
* Jérémie du Boisberranger
* Kmol Yuan +
* Matti Picus
* Max Bolingbroke +
* Maxwell Aladago +
* Oleksandr Pavlyk
* Peter Andreas Entschev
* Sergei Lebedev
* Seth Troisi +
* Vladimir Pershin +
* Warren Weckesser


Pull requests merged
====================

A total of 24 pull requests were merged for this release.

* `#14156 <https://github.com/numpy/numpy/pull/14156>`__: TST: Allow fuss in testing strided/non-strided exp/log loops
* `#14157 <https://github.com/numpy/numpy/pull/14157>`__: BUG: avx2_scalef_ps must be static
* `#14158 <https://github.com/numpy/numpy/pull/14158>`__: BUG: Remove stray print that causes a SystemError on python 3.7.
* `#14159 <https://github.com/numpy/numpy/pull/14159>`__: BUG: Fix DeprecationWarning in python 3.8.
* `#14160 <https://github.com/numpy/numpy/pull/14160>`__: BLD: Add missing gcd/lcm definitions to npy_math.h
* `#14161 <https://github.com/numpy/numpy/pull/14161>`__: DOC, BUILD: cleanups and fix (again) 'build dist'
* `#14166 <https://github.com/numpy/numpy/pull/14166>`__: TST: Add 3.8-dev to travisCI testing.
* `#14194 <https://github.com/numpy/numpy/pull/14194>`__: BUG: Remove the broken clip wrapper (Backport)
* `#14198 <https://github.com/numpy/numpy/pull/14198>`__: DOC: Fix hermitian argument docs in svd.
* `#14199 <https://github.com/numpy/numpy/pull/14199>`__: MAINT: Workaround for Intel compiler bug leading to failing test
* `#14200 <https://github.com/numpy/numpy/pull/14200>`__: TST: Clean up of test_pocketfft.py
* `#14201 <https://github.com/numpy/numpy/pull/14201>`__: BUG: Make advanced indexing result on read-only subclass writeable...
* `#14236 <https://github.com/numpy/numpy/pull/14236>`__: BUG: Fixed default BitGenerator name
* `#14237 <https://github.com/numpy/numpy/pull/14237>`__: ENH: add c-imported modules for freeze analysis in np.random
* `#14296 <https://github.com/numpy/numpy/pull/14296>`__: TST: Pin pytest version to 5.0.1
* `#14301 <https://github.com/numpy/numpy/pull/14301>`__: BUG: Fix leak in the f2py-generated module init and `PyMem_Del`...
* `#14302 <https://github.com/numpy/numpy/pull/14302>`__: BUG: Fix formatting error in exception message
* `#14307 <https://github.com/numpy/numpy/pull/14307>`__: MAINT: random: Match type of SeedSequence.pool_size to DEFAULT_POOL_SIZE.
* `#14308 <https://github.com/numpy/numpy/pull/14308>`__: BUG: Fix numpy.random bug in platform detection
* `#14309 <https://github.com/numpy/numpy/pull/14309>`__: ENH: Enable huge pages in all Linux builds
* `#14330 <https://github.com/numpy/numpy/pull/14330>`__: BUG: Fix segfault in `random.permutation(x)` when x is a string.
* `#14338 <https://github.com/numpy/numpy/pull/14338>`__: BUG: don't fail when lexsorting some empty arrays (#14228)
* `#14339 <https://github.com/numpy/numpy/pull/14339>`__: BUG: Fix misuse of .names and .fields in various places (backport...
* `#14345 <https://github.com/numpy/numpy/pull/14345>`__: BUG: fix behavior of structured_to_unstructured on non-trivial...
* `#14350 <https://github.com/numpy/numpy/pull/14350>`__: REL: Prepare 1.17.1 release
==========================
NumPy 1.13.0 Release Notes
==========================

This release supports Python 2.7 and 3.4 - 3.6.


Highlights
==========

 * Operations like ``a + b + c`` will reuse temporaries on some platforms,
   resulting in less memory use and faster execution.
 * Inplace operations check if inputs overlap outputs and create temporaries
   to avoid problems.
 * New ``__array_ufunc__`` attribute provides improved ability for classes to
   override default ufunc behavior.
 * New ``np.block`` function for creating blocked arrays.


New functions
=============

* New ``np.positive`` ufunc.
* New ``np.divmod`` ufunc provides more efficient divmod.
* New ``np.isnat`` ufunc tests for NaT special values.
* New ``np.heaviside`` ufunc computes the Heaviside function.
* New ``np.isin`` function, improves on ``in1d``.
* New ``np.block`` function for creating blocked arrays.
* New ``PyArray_MapIterArrayCopyIfOverlap`` added to NumPy C-API.

See below for details.


Deprecations
============

* Calling ``np.fix``, ``np.isposinf``, and ``np.isneginf`` with ``f(x, y=out)``
  is deprecated - the argument should be passed as ``f(x, out=out)``, which
  matches other ufunc-like interfaces.
* Use of the C-API ``NPY_CHAR`` type number deprecated since version 1.7 will
  now raise deprecation warnings at runtime. Extensions built with older f2py
  versions need to be recompiled to remove the warning.
* ``np.ma.argsort``, ``np.ma.minimum.reduce``, and ``np.ma.maximum.reduce``
  should be called with an explicit `axis` argument when applied to arrays with
  more than 2 dimensions, as the default value of this argument (``None``) is
  inconsistent with the rest of numpy (``-1``, ``0``, and ``0``, respectively).
* ``np.ma.MaskedArray.mini`` is deprecated, as it almost duplicates the
  functionality of ``np.MaskedArray.min``. Exactly equivalent behaviour
  can be obtained with ``np.ma.minimum.reduce``.
* The single-argument form of ``np.ma.minimum`` and ``np.ma.maximum`` is
  deprecated. ``np.maximum``. ``np.ma.minimum(x)`` should now be spelt
  ``np.ma.minimum.reduce(x)``, which is consistent with how this would be done
  with ``np.minimum``.
* Calling ``ndarray.conjugate`` on non-numeric dtypes is deprecated (it
  should match the behavior of ``np.conjugate``, which throws an error).
* Calling ``expand_dims`` when the ``axis`` keyword does not satisfy
  ``-a.ndim - 1 <= axis <= a.ndim``, where ``a`` is the array being reshaped,
  is deprecated.


Future Changes
==============

* Assignment between structured arrays with different field names will change
  in NumPy 1.14. Previously, fields in the dst would be set to the value of the
  identically-named field in the src. In numpy 1.14 fields will instead be
  assigned 'by position': The n-th field of the dst will be set to the n-th
  field of the src array. Note that the ``FutureWarning`` raised in NumPy 1.12
  incorrectly reported this change as scheduled for NumPy 1.13 rather than
  NumPy 1.14.


Build System Changes
====================

* ``numpy.distutils`` now automatically determines C-file dependencies with
  GCC compatible compilers.


Compatibility notes
===================

Error type changes
------------------

* ``numpy.hstack()`` now throws ``ValueError`` instead of ``IndexError`` when
  input is empty.
* Functions taking an axis argument, when that argument is out of range, now
  throw ``np.AxisError`` instead of a mixture of ``IndexError`` and
  ``ValueError``. For backwards compatibility, ``AxisError`` subclasses both of
  these.

Tuple object dtypes
-------------------

Support has been removed for certain obscure dtypes that were unintentionally
allowed, of the form ``(old_dtype, new_dtype)``, where either of the dtypes
is or contains the ``object`` dtype. As an exception, dtypes of the form
``(object, [('name', object)])`` are still supported due to evidence of
existing use.

DeprecationWarning to error
---------------------------
See Changes section for more detail.

* ``partition``, TypeError when non-integer partition index is used.
* ``NpyIter_AdvancedNew``, ValueError when ``oa_ndim == 0`` and ``op_axes`` is NULL
* ``negative(bool_)``, TypeError when negative applied to booleans.
* ``subtract(bool_, bool_)``, TypeError when subtracting boolean from boolean.
* ``np.equal, np.not_equal``, object identity doesn't override failed comparison.
* ``np.equal, np.not_equal``, object identity doesn't override non-boolean comparison.
* Deprecated boolean indexing behavior dropped. See Changes below for details.
* Deprecated ``np.alterdot()`` and ``np.restoredot()`` removed.

FutureWarning to changed behavior
---------------------------------
See Changes section for more detail.

* ``numpy.average`` preserves subclasses
* ``array == None`` and ``array != None`` do element-wise comparison.
* ``np.equal, np.not_equal``, object identity doesn't override comparison result.

dtypes are now always true
--------------------------

Previously ``bool(dtype)`` would fall back to the default python
implementation, which checked if ``len(dtype) > 0``. Since ``dtype`` objects
implement ``__len__`` as the number of record fields, ``bool`` of scalar dtypes
would evaluate to ``False``, which was unintuitive. Now ``bool(dtype) == True``
for all dtypes.

``__getslice__`` and ``__setslice__`` are no longer needed in ``ndarray`` subclasses
------------------------------------------------------------------------------------
When subclassing np.ndarray in Python 2.7, it is no longer _necessary_ to
implement ``__*slice__`` on the derived class, as ``__*item__`` will intercept
these calls correctly.

Any code that did implement these will work exactly as before. Code that
invokes``ndarray.__getslice__`` (e.g. through ``super(...).__getslice__``) will
now issue a DeprecationWarning - ``.__getitem__(slice(start, end))`` should be
used instead.

Indexing MaskedArrays/Constants with ``...`` (ellipsis) now returns MaskedArray
-------------------------------------------------------------------------------
This behavior mirrors that of np.ndarray, and accounts for nested arrays in
MaskedArrays of object dtype, and ellipsis combined with other forms of
indexing.

C API changes
=============

GUfuncs on empty arrays and NpyIter axis removal
------------------------------------------------
It is now allowed to remove a zero-sized axis from NpyIter. Which may mean
that code removing axes from NpyIter has to add an additional check when
accessing the removed dimensions later on.

The largest followup change is that gufuncs are now allowed to have zero-sized
inner dimensions. This means that a gufunc now has to anticipate an empty inner
dimension, while this was never possible and an error raised instead.

For most gufuncs no change should be necessary. However, it is now possible
for gufuncs with a signature such as ``(..., N, M) -> (..., M)`` to return
a valid result if ``N=0`` without further wrapping code.

``PyArray_MapIterArrayCopyIfOverlap`` added to NumPy C-API
----------------------------------------------------------
Similar to ``PyArray_MapIterArray`` but with an additional ``copy_if_overlap``
argument. If ``copy_if_overlap != 0``,  checks if input has memory overlap with
any of the other arrays and make copies as appropriate to avoid problems if the
input is modified during the iteration. See the documentation for more complete
documentation.


New Features
============

``__array_ufunc__`` added
-------------------------
This is the renamed and redesigned ``__numpy_ufunc__``. Any class, ndarray
subclass or not, can define this method or set it to ``None`` in order to
override the behavior of NumPy's ufuncs. This works quite similarly to Python's
``__mul__`` and other binary operation routines. See the documentation for a
more detailed description of the implementation and behavior of this new
option. The API is provisional, we do not yet guarantee backward compatibility
as modifications may be made pending feedback. See `NEP 13`_  and
documentation_ for more details.

.. _`NEP 13`: http://www.numpy.org/neps/nep-0013-ufunc-overrides.html
.. _documentation: https://github.com/numpy/numpy/blob/master/doc/source/reference/arrays.classes.rst

New ``positive`` ufunc
----------------------
This ufunc corresponds to unary `+`, but unlike `+` on an ndarray it will raise
an error if array values do not support numeric operations.

New ``divmod`` ufunc
--------------------
This ufunc corresponds to the Python builtin `divmod`, and is used to implement
`divmod` when called on numpy arrays. ``np.divmod(x, y)`` calculates a result
equivalent to ``(np.floor_divide(x, y), np.remainder(x, y))`` but is
approximately twice as fast as calling the functions separately.

``np.isnat`` ufunc tests for NaT special datetime and timedelta values
----------------------------------------------------------------------
The new ufunc ``np.isnat`` finds the positions of special NaT values
within datetime and timedelta arrays. This is analogous to ``np.isnan``.

``np.heaviside`` ufunc computes the Heaviside function
------------------------------------------------------
The new function ``np.heaviside(x, h0)`` (a ufunc) computes the Heaviside
function:

.. code::

                       { 0   if x < 0,
    heaviside(x, h0) = { h0  if x == 0,
                       { 1   if x > 0.

``np.block`` function for creating blocked arrays
-------------------------------------------------
Add a new ``block`` function to the current stacking functions ``vstack``,
``hstack``, and ``stack``. This allows concatenation across multiple axes
simultaneously, with a similar syntax to array creation, but where elements
can themselves be arrays. For instance::

    >>> A = np.eye(2) * 2
    >>> B = np.eye(3) * 3
    >>> np.block([
    ...     [A,               np.zeros((2, 3))],
    ...     [np.ones((3, 2)), B               ]
    ... ])
    array([[ 2.,  0.,  0.,  0.,  0.],
           [ 0.,  2.,  0.,  0.,  0.],
           [ 1.,  1.,  3.,  0.,  0.],
           [ 1.,  1.,  0.,  3.,  0.],
           [ 1.,  1.,  0.,  0.,  3.]])

While primarily useful for block matrices, this works for arbitrary dimensions
of arrays.

It is similar to Matlab's square bracket notation for creating block matrices.

``isin`` function, improving on ``in1d``
----------------------------------------
The new function ``isin`` tests whether each element of an N-dimensional
array is present anywhere within a second array. It is an enhancement
of ``in1d`` that preserves the shape of the first array.

Temporary elision
-----------------
On platforms providing the ``backtrace`` function NumPy will try to avoid
creating temporaries in expression involving basic numeric types.
For example ``d = a + b + c`` is transformed to ``d = a + b; d += c`` which can
improve performance for large arrays as less memory bandwidth is required to
perform the operation.

``axes`` argument for ``unique``
--------------------------------
In an N-dimensional array, the user can now choose the axis along which to look
for duplicate N-1-dimensional elements using ``numpy.unique``. The original
behaviour is recovered if ``axis=None`` (default).

``np.gradient`` now supports unevenly spaced data
-------------------------------------------------
Users can now specify a not-constant spacing for data.
In particular ``np.gradient`` can now take:

1. A single scalar to specify a sample distance for all dimensions.
2. N scalars to specify a constant sample distance for each dimension.
   i.e. ``dx``, ``dy``, ``dz``, ...
3. N arrays to specify the coordinates of the values along each dimension of F.
   The length of the array must match the size of the corresponding dimension
4. Any combination of N scalars/arrays with the meaning of 2. and 3.

This means that, e.g., it is now possible to do the following::

    >>> f = np.array([[1, 2, 6], [3, 4, 5]], dtype=np.float_)
    >>> dx = 2.
    >>> y = [1., 1.5, 3.5]
    >>> np.gradient(f, dx, y)
    [array([[ 1. ,  1. , -0.5], [ 1. ,  1. , -0.5]]),
     array([[ 2. ,  2. ,  2. ], [ 2. ,  1.7,  0.5]])]

Support for returning arrays of arbitrary dimensions in ``apply_along_axis``
----------------------------------------------------------------------------
Previously, only scalars or 1D arrays could be returned by the function passed
to ``apply_along_axis``. Now, it can return an array of any dimensionality
(including 0D), and the shape of this array replaces the axis of the array
being iterated over.

``.ndim`` property added to ``dtype`` to complement ``.shape``
--------------------------------------------------------------
For consistency with ``ndarray`` and ``broadcast``, ``d.ndim`` is a shorthand
for ``len(d.shape)``.

Support for tracemalloc in Python 3.6
-------------------------------------
NumPy now supports memory tracing with tracemalloc_ module of Python 3.6 or
newer. Memory allocations from NumPy are placed into the domain defined by
``numpy.lib.tracemalloc_domain``.
Note that NumPy allocation will not show up in tracemalloc_ of earlier Python
versions.

.. _tracemalloc: https://docs.python.org/3/library/tracemalloc.html

NumPy may be built with relaxed stride checking debugging
---------------------------------------------------------
Setting NPY_RELAXED_STRIDES_DEBUG=1 in the environment when relaxed stride
checking is enabled will cause NumPy to be compiled with the affected strides
set to the maximum value of npy_intp in order to help detect invalid usage of
the strides in downstream projects. When enabled, invalid usage often results
in an error being raised, but the exact type of error depends on the details of
the code. TypeError and OverflowError have been observed in the wild.

It was previously the case that this option was disabled for releases and
enabled in master and changing between the two required editing the code. It is
now disabled by default but can be enabled for test builds.


Improvements
============

Ufunc behavior for overlapping inputs
-------------------------------------

Operations where ufunc input and output operands have memory overlap
produced undefined results in previous NumPy versions, due to data
dependency issues. In NumPy 1.13.0, results from such operations are
now defined to be the same as for equivalent operations where there is
no memory overlap.

Operations affected now make temporary copies, as needed to eliminate
data dependency. As detecting these cases is computationally
expensive, a heuristic is used, which may in rare cases result to
needless temporary copies.  For operations where the data dependency
is simple enough for the heuristic to analyze, temporary copies will
not be made even if the arrays overlap, if it can be deduced copies
are not necessary.  As an example,``np.add(a, b, out=a)`` will not
involve copies.

To illustrate a previously undefined operation::

    >>> x = np.arange(16).astype(float)
    >>> np.add(x[1:], x[:-1], out=x[1:])

In NumPy 1.13.0 the last line is guaranteed to be equivalent to::

    >>> np.add(x[1:].copy(), x[:-1].copy(), out=x[1:])

A similar operation with simple non-problematic data dependence is::

    >>> x = np.arange(16).astype(float)
    >>> np.add(x[1:], x[:-1], out=x[:-1])

It will continue to produce the same results as in previous NumPy
versions, and will not involve unnecessary temporary copies.

The change applies also to in-place binary operations, for example::

    >>> x = np.random.rand(500, 500)
    >>> x += x.T

This statement is now guaranteed to be equivalent to ``x[...] = x + x.T``,
whereas in previous NumPy versions the results were undefined.

Partial support for 64-bit f2py extensions with MinGW
-----------------------------------------------------
Extensions that incorporate Fortran libraries can now be built using the free
MinGW_ toolset, also under Python 3.5. This works best for extensions that only
do calculations and uses the runtime modestly (reading and writing from files,
for instance). Note that this does not remove the need for Mingwpy; if you make
extensive use of the runtime, you will most likely run into issues_. Instead,
it should be regarded as a band-aid until Mingwpy is fully functional.

Extensions can also be compiled using the MinGW toolset using the runtime
library from the (moveable) WinPython 3.4 distribution, which can be useful for
programs with a PySide1/Qt4 front-end.

.. _MinGW: https://sf.net/projects/mingw-w64/files/Toolchains%20targetting%20Win64/Personal%20Builds/mingw-builds/6.2.0/threads-win32/seh/

.. _issues: https://mingwpy.github.io/issues.html

Performance improvements for ``packbits`` and ``unpackbits``
------------------------------------------------------------
The functions ``numpy.packbits`` with boolean input and ``numpy.unpackbits`` have
been optimized to be a significantly faster for contiguous data.

Fix for PPC long double floating point information
--------------------------------------------------
In previous versions of NumPy, the ``finfo`` function returned invalid
information about the `double double`_ format of the ``longdouble`` float type
on Power PC (PPC).  The invalid values resulted from the failure of the NumPy
algorithm to deal with the variable number of digits in the significand
that are a feature of `PPC long doubles`.  This release by-passes the failing
algorithm by using heuristics to detect the presence of the PPC double double
format.  A side-effect of using these heuristics is that the ``finfo``
function is faster than previous releases.

.. _PPC long doubles: https://www.ibm.com/support/knowledgecenter/en/ssw_aix_71/com.ibm.aix.genprogc/128bit_long_double_floating-point_datatype.htm

.. _double double: https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Double-double_arithmetic

Better default repr for ``ndarray`` subclasses
----------------------------------------------
Subclasses of ndarray with no ``repr`` specialization now correctly indent
their data and type lines.

More reliable comparisons of masked arrays
------------------------------------------
Comparisons of masked arrays were buggy for masked scalars and failed for
structured arrays with dimension higher than one. Both problems are now
solved. In the process, it was ensured that in getting the result for a
structured array, masked fields are properly ignored, i.e., the result is equal
if all fields that are non-masked in both are equal, thus making the behaviour
identical to what one gets by comparing an unstructured masked array and then
doing ``.all()`` over some axis.

np.matrix with booleans elements can now be created using the string syntax
---------------------------------------------------------------------------
``np.matrix`` failed whenever one attempts to use it with booleans, e.g.,
``np.matrix('True')``. Now, this works as expected.

More ``linalg`` operations now accept empty vectors and matrices
----------------------------------------------------------------
All of the following functions in ``np.linalg`` now work when given input
arrays with a 0 in the last two dimensions: ``det``, ``slogdet``, ``pinv``,
``eigvals``, ``eigvalsh``, ``eig``, ``eigh``.

Bundled version of LAPACK is now 3.2.2
--------------------------------------
NumPy comes bundled with a minimal implementation of lapack for systems without
a lapack library installed, under the name of ``lapack_lite``. This has been
upgraded from LAPACK 3.0.0 (June 30, 1999) to LAPACK 3.2.2 (June 30, 2010). See
the `LAPACK changelogs`_ for details on the all the changes this entails.

While no new features are exposed through ``numpy``, this fixes some bugs
regarding "workspace" sizes, and in some places may use faster algorithms.

.. _`LAPACK changelogs`: http://www.netlib.org/lapack/release_notes.html#_4_history_of_lapack_releases

``reduce`` of ``np.hypot.reduce`` and ``np.logical_xor`` allowed in more cases
------------------------------------------------------------------------------
This now works on empty arrays, returning 0, and can reduce over multiple axes.
Previously, a ``ValueError`` was thrown in these cases.

Better ``repr`` of object arrays
--------------------------------
Object arrays that contain themselves no longer cause a recursion error.

Object arrays that contain ``list`` objects are now printed in a way that makes
clear the difference between a 2d object array, and a 1d object array of lists.

Changes
=======

``argsort`` on masked arrays takes the same default arguments as ``sort``
-------------------------------------------------------------------------
By default, ``argsort`` now places the masked values at the end of the sorted
array, in the same way that ``sort`` already did. Additionally, the
``end_with`` argument is added to ``argsort``, for consistency with ``sort``.
Note that this argument is not added at the end, so breaks any code that
passed ``fill_value`` as a positional argument.

``average`` now preserves subclasses
------------------------------------
For ndarray subclasses, ``numpy.average`` will now return an instance of the
subclass, matching the behavior of most other NumPy functions such as ``mean``.
As a consequence, also calls that returned a scalar may now return a subclass
array scalar.

``array == None`` and ``array != None`` do element-wise comparison
------------------------------------------------------------------
Previously these operations returned scalars ``False`` and ``True`` respectively.

``np.equal, np.not_equal`` for object arrays ignores object identity
--------------------------------------------------------------------
Previously, these functions always treated identical objects as equal. This had
the effect of overriding comparison failures, comparison of objects that did
not return booleans, such as np.arrays, and comparison of objects where the
results differed from object identity, such as NaNs.

Boolean indexing changes
------------------------
* Boolean array-likes (such as lists of python bools) are always treated as
  boolean indexes.

* Boolean scalars (including python ``True``) are legal boolean indexes and
  never treated as integers.

* Boolean indexes must match the dimension of the axis that they index.

* Boolean indexes used on the lhs of an assignment must match the dimensions of
  the rhs.

* Boolean indexing into scalar arrays return a new 1-d array.  This means that
  ``array(1)[array(True)]`` gives ``array([1])`` and not the original array.

``np.random.multivariate_normal`` behavior with bad covariance matrix
---------------------------------------------------------------------

It is now possible to adjust the behavior the function will have when dealing
with the covariance matrix by using two new keyword arguments:

* ``tol`` can be used to specify a tolerance to use when checking that
  the covariance matrix is positive semidefinite.

* ``check_valid`` can be used to configure what the function will do in the
  presence of a matrix that is not positive semidefinite. Valid options are
  ``ignore``, ``warn`` and ``raise``. The default value, ``warn`` keeps the
  the behavior used on previous releases.

``assert_array_less`` compares ``np.inf`` and ``-np.inf`` now
-------------------------------------------------------------
Previously, ``np.testing.assert_array_less`` ignored all infinite values. This
is not the expected behavior both according to documentation and intuitively.
Now, -inf < x < inf is considered ``True`` for any real number x and all
other cases fail.

``assert_array_`` and masked arrays ``assert_equal`` hide less warnings
-----------------------------------------------------------------------
Some warnings that were previously hidden by the ``assert_array_``
functions are not hidden anymore. In most cases the warnings should be
correct and, should they occur, will require changes to the tests using
these functions.
For the masked array ``assert_equal`` version, warnings may occur when
comparing NaT. The function presently does not handle NaT or NaN
specifically and it may be best to avoid it at this time should a warning
show up due to this change.

``offset`` attribute value in ``memmap`` objects
------------------------------------------------
The ``offset`` attribute in a ``memmap`` object is now set to the
offset into the file. This is a behaviour change only for offsets
greater than ``mmap.ALLOCATIONGRANULARITY``.

``np.real`` and ``np.imag`` return scalars for scalar inputs
------------------------------------------------------------
Previously, ``np.real`` and ``np.imag`` used to return array objects when
provided a scalar input, which was inconsistent with other functions like
``np.angle`` and ``np.conj``.

The polynomial convenience classes cannot be passed to ufuncs
-------------------------------------------------------------
The ABCPolyBase class, from which the convenience classes are derived, sets
``__array_ufun__ = None`` in order of opt out of ufuncs. If a polynomial
convenience class instance is passed as an argument to a ufunc, a ``TypeError``
will now be raised.

Output arguments to ufuncs can be tuples also for ufunc methods
---------------------------------------------------------------
For calls to ufuncs, it was already possible, and recommended, to use an
``out`` argument with a tuple for ufuncs with multiple outputs. This has now
been extended to output arguments in the ``reduce``, ``accumulate``, and
``reduceat`` methods. This is mostly for compatibility with ``__array_ufunc``;
there are no ufuncs yet that have more than one output.
.. currentmodule:: numpy

==========================
NumPy 1.22.1 Release Notes
==========================

The NumPy 1.22.1 is a maintenance release that fixes bugs discovered after the
1.22.0 release. Notable fixes are:

- Fix f2PY docstring problems (SciPy)
- Fix reduction type problems (AstroPy)
- Fix various typing bugs.

The Python versions supported for this release are 3.8-3.10.


Contributors
============

A total of 14 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Arryan Singh
* Bas van Beek
* Charles Harris
* Denis Laxalde
* Isuru Fernando
* Kevin Sheppard
* Matthew Barber
* Matti Picus
* Melissa Weber Mendonça
* Mukulika Pahari
* Omid Rajaei +
* Pearu Peterson
* Ralf Gommers
* Sebastian Berg


Pull requests merged
====================

A total of 20 pull requests were merged for this release.

* `#20702 <https://github.com/numpy/numpy/pull/20702>`__: MAINT, DOC: Post 1.22.0 release fixes.
* `#20703 <https://github.com/numpy/numpy/pull/20703>`__: DOC, BUG: Use pngs instead of svgs.
* `#20704 <https://github.com/numpy/numpy/pull/20704>`__: DOC: Fixed the link on user-guide landing page
* `#20714 <https://github.com/numpy/numpy/pull/20714>`__: BUG: Restore vc141 support
* `#20724 <https://github.com/numpy/numpy/pull/20724>`__: BUG: Fix array dimensions solver for multidimensional arguments...
* `#20725 <https://github.com/numpy/numpy/pull/20725>`__: TYP: change type annotation for ``__array_namespace__`` to ModuleType
* `#20726 <https://github.com/numpy/numpy/pull/20726>`__: TYP, MAINT: Allow ``ndindex`` to accept integer tuples
* `#20757 <https://github.com/numpy/numpy/pull/20757>`__: BUG: Relax dtype identity check in reductions
* `#20763 <https://github.com/numpy/numpy/pull/20763>`__: TYP: Allow time manipulation functions to accept ``date`` and ``timedelta``...
* `#20768 <https://github.com/numpy/numpy/pull/20768>`__: TYP: Relax the type of ``ndarray.__array_finalize__``
* `#20795 <https://github.com/numpy/numpy/pull/20795>`__: MAINT: Raise RuntimeError if setuptools version is too recent.
* `#20796 <https://github.com/numpy/numpy/pull/20796>`__: BUG, DOC: Fixes SciPy docs build warnings
* `#20797 <https://github.com/numpy/numpy/pull/20797>`__: DOC: fix OpenBLAS version in release note
* `#20798 <https://github.com/numpy/numpy/pull/20798>`__: PERF: Optimize array check for bounded 0,1 values
* `#20805 <https://github.com/numpy/numpy/pull/20805>`__: BUG: Fix that reduce-likes honor out always (and live in the...
* `#20806 <https://github.com/numpy/numpy/pull/20806>`__: BUG: ``array_api.argsort(descending=True)`` respects relative...
* `#20807 <https://github.com/numpy/numpy/pull/20807>`__: BUG: Allow integer inputs for pow-related functions in ``array_api``
* `#20814 <https://github.com/numpy/numpy/pull/20814>`__: DOC: Refer to NumPy, not pandas, in main page
* `#20815 <https://github.com/numpy/numpy/pull/20815>`__: DOC: Update Copyright to 2022 [License]
* `#20819 <https://github.com/numpy/numpy/pull/20819>`__: BUG: Return correctly shaped inverse indices in array_api set...
.. currentmodule:: numpy

==========================
NumPy 1.21.0 Release Notes
==========================
The NumPy 1.21.0 release highlights are

* continued SIMD work covering more functions and platforms,
* initial work on the new dtype infrastructure and casting,
* universal2 wheels for Python 3.8 and Python 3.9 on Mac,
* improved documentation,
* improved annotations,
* new ``PCG64DXSM`` bitgenerator for random numbers.

In addition there are the usual large number of bug fixes and other improvements.

The Python versions supported for this release are 3.7-3.9. Official support
for Python 3.10 will be added when it is released.

.. warning::
   There are unresolved problems compiling NumPy 1.20.0 with gcc-11.1.

   * Optimization level `-O3` results in many incorrect warnings when
     running the tests.
   * On some hardware NumPY will hang in an infinite loop.





New functions
=============

.. currentmodule:: numpy.random

Add `PCG64DXSM` `BitGenerator`
------------------------------

Uses of the ``PCG64`` ``BitGenerator`` in a massively-parallel context have been
shown to have statistical weaknesses that were not apparent at the first
release in numpy 1.17. Most users will never observe this weakness and are
safe to continue to use ``PCG64``. We have introduced a new ``PCG64DXSM``
``BitGenerator`` that will eventually become the new default ``BitGenerator``
implementation used by ``default_rng`` in future releases. ``PCG64DXSM`` solves
the statistical weakness while preserving the performance and the features of
``PCG64``.

See :ref:`upgrading-pcg64` for more details.

.. currentmodule:: numpy

(`gh-18906 <https://github.com/numpy/numpy/pull/18906>`__)


Expired deprecations
====================

* The ``shape`` argument `~numpy.unravel_index` cannot be passed
  as ``dims`` keyword argument anymore. (Was deprecated in NumPy 1.16.)

  (`gh-17900 <https://github.com/numpy/numpy/pull/17900>`__)

* The function ``PyUFunc_GenericFunction`` has been disabled.
  It was deprecated in NumPy 1.19.  Users should call the ufunc
  directly using the Python API.

  (`gh-18697 <https://github.com/numpy/numpy/pull/18697>`__)

* The function ``PyUFunc_SetUsesArraysAsData`` has been disabled.
  It was deprecated in NumPy 1.19.

  (`gh-18697 <https://github.com/numpy/numpy/pull/18697>`__)

* The class ``PolyBase`` has been removed (deprecated in numpy 1.9.0). Please
  use the abstract ``ABCPolyBase`` class instead.

  (`gh-18963 <https://github.com/numpy/numpy/pull/18963>`__)

* The unused ``PolyError`` and ``PolyDomainError`` exceptions are
  removed.

  (`gh-18963 <https://github.com/numpy/numpy/pull/18963>`__)


Deprecations
============

The ``.dtype`` attribute must return a ``dtype``
------------------------------------------------

A ``DeprecationWarning`` is now given if the ``.dtype`` attribute
of an object passed into ``np.dtype`` or as a ``dtype=obj`` argument
is not a dtype. NumPy will stop attempting to recursively coerce the
result of ``.dtype``.

(`gh-13578 <https://github.com/numpy/numpy/pull/13578>`__)

Inexact matches for ``numpy.convolve`` and ``numpy.correlate`` are deprecated
-----------------------------------------------------------------------------

`~numpy.convolve` and `~numpy.correlate` now emit a warning when there are case
insensitive and/or inexact matches found for ``mode`` argument in the functions.
Pass full ``"same"``, ``"valid"``, ``"full"`` strings instead of
``"s"``, ``"v"``, ``"f"`` for the ``mode`` argument.

(`gh-17492 <https://github.com/numpy/numpy/pull/17492>`__)

``np.typeDict`` has been formally deprecated
--------------------------------------------
``np.typeDict`` is a deprecated alias for ``np.sctypeDict`` and
has been so for over 14 years (6689502_).
A deprecation warning will now be issued whenever getting ``np.typeDict``.

.. _6689502: https://github.com/numpy/numpy/commit/668950285c407593a368336ff2e737c5da84af7d

(`gh-17586 <https://github.com/numpy/numpy/pull/17586>`__)

Exceptions will be raised during array-like creation
----------------------------------------------------
When an object raised an exception during access of the special
attributes ``__array__`` or ``__array_interface__``, this exception
was usually ignored.
A warning is now given when the exception is anything but AttributeError.
To silence the warning, the type raising the exception has to be adapted
to raise an ``AttributeError``.

(`gh-19001 <https://github.com/numpy/numpy/pull/19001>`__)

Four ``ndarray.ctypes`` methods have been deprecated
----------------------------------------------------
Four methods of the `ndarray.ctypes` object have been deprecated,
as they are (undocumentated) implementation artifacts of their respective
properties.

The methods in question are:

* ``_ctypes.get_data`` (use ``_ctypes.data`` instead)
* ``_ctypes.get_shape`` (use ``_ctypes.shape`` instead)
* ``_ctypes.get_strides`` (use ``_ctypes.strides`` instead)
* ``_ctypes.get_as_parameter`` (use ``_ctypes._as_parameter_`` instead)

(`gh-19031 <https://github.com/numpy/numpy/pull/19031>`__)


Expired deprecations
====================

* The ``shape`` argument `numpy.unravel_index` cannot be passed
  as ``dims`` keyword argument anymore. (Was deprecated in NumPy 1.16.)

  (`gh-17900 <https://github.com/numpy/numpy/pull/17900>`__)

* The function ``PyUFunc_GenericFunction`` has been disabled.
  It was deprecated in NumPy 1.19.  Users should call the ufunc
  directly using the Python API.

  (`gh-18697 <https://github.com/numpy/numpy/pull/18697>`__)

* The function ``PyUFunc_SetUsesArraysAsData`` has been disabled.
  It was deprecated in NumPy 1.19.

  (`gh-18697 <https://github.com/numpy/numpy/pull/18697>`__)

Remove deprecated ``PolyBase`` and unused ``PolyError`` and ``PolyDomainError``
-------------------------------------------------------------------------------

The class ``PolyBase`` has been removed (deprecated in numpy 1.9.0). Please use
the abstract ``ABCPolyBase`` class instead.

Furthermore, the unused ``PolyError`` and ``PolyDomainError`` exceptions are
removed from the `numpy.polynomial`.

(`gh-18963 <https://github.com/numpy/numpy/pull/18963>`__)


Compatibility notes
===================

Error type changes in universal functions
-----------------------------------------
The universal functions may now raise different errors on invalid input in some
cases.  The main changes should be that a ``RuntimeError`` was replaced with a
more fitting ``TypeError``.  When multiple errors were present in the same
call, NumPy may now raise a different one.

(`gh-15271 <https://github.com/numpy/numpy/pull/15271>`__)

``__array_ufunc__`` argument validation
---------------------------------------
NumPy will now partially validate arguments before calling ``__array_ufunc__``.
Previously, it was possible to pass on invalid arguments (such as a
non-existing keyword argument) when dispatch was known to occur.

(`gh-15271 <https://github.com/numpy/numpy/pull/15271>`__)

``__array_ufunc__`` and additional positional arguments
-------------------------------------------------------
Previously, all positionally passed arguments were checked for
``__array_ufunc__`` support.  In the case of ``reduce``, ``accumulate``, and
``reduceat`` all arguments may be passed by position.  This means that when
they were passed by position, they could previously have been asked to handle
the ufunc call via ``__array_ufunc__``.  Since this depended on the way the
arguments were passed (by position or by keyword), NumPy will now only dispatch
on the input and output array.  For example, NumPy will never dispatch on the
``where`` array in a reduction such as ``np.add.reduce``.

(`gh-15271 <https://github.com/numpy/numpy/pull/15271>`__)

Validate input values in ``Generator.uniform``
----------------------------------------------
Checked that ``high - low >= 0`` in ``np.random.Generator.uniform``. Raises
``ValueError`` if ``low > high``. Previously out-of-order inputs were accepted
and silently swapped, so that if ``low > high``, the value generated was
``high + (low - high) * random()``.

(`gh-17921 <https://github.com/numpy/numpy/pull/17921>`__)

``/usr/include`` removed from default include paths
---------------------------------------------------
The default include paths when building a package with ``numpy.distutils`` no
longer include ``/usr/include``. This path is normally added by the compiler,
and hardcoding it can be problematic. In case this causes a problem, please
open an issue. A workaround is documented in PR 18658.

(`gh-18658 <https://github.com/numpy/numpy/pull/18658>`__)

Changes to comparisons with ``dtype=...``
-----------------------------------------
When the ``dtype=`` (or ``signature``) arguments to comparison
ufuncs (``equal``, ``less``, etc.) is used, this will denote
the desired output dtype in the future.
This means that:

    np.equal(2, 3, dtype=object)

will give a ``FutureWarning`` that it will return an ``object``
array in the future, which currently happens for:

    np.equal(None, None, dtype=object)

due to the fact that ``np.array(None)`` is already an object
array. (This also happens for some other dtypes.)

Since comparisons normally only return boolean arrays, providing
any other dtype will always raise an error in the future and
give a ``DeprecationWarning`` now.

(`gh-18718 <https://github.com/numpy/numpy/pull/18718>`__)

Changes to ``dtype`` and ``signature`` arguments in ufuncs
----------------------------------------------------------
The universal function arguments ``dtype`` and ``signature``
which are also valid for reduction such as ``np.add.reduce``
(which is the implementation for ``np.sum``) will now issue
a warning when the ``dtype`` provided is not a "basic" dtype.

NumPy almost always ignored metadata, byteorder or time units
on these inputs.  NumPy will now always ignore it and raise an
error if byteorder or time unit changed.
The following are the most important examples of changes which
will give the error.  In some cases previously the information
stored was not ignored, in all of these an error is now raised::

    # Previously ignored the byte-order (affect if non-native)
    np.add(3, 5, dtype=">i32")

    # The biggest impact is for timedelta or datetimes:
    arr = np.arange(10, dtype="m8[s]")
    # The examples always ignored the time unit "ns":
    np.add(arr, arr, dtype="m8[ns]")
    np.maximum.reduce(arr, dtype="m8[ns]")

    # The following previously did use "ns" (as opposed to `arr.dtype`)
    np.add(3, 5, dtype="m8[ns]")  # Now return generic time units
    np.maximum(arr, arr, dtype="m8[ns]")  # Now returns "s" (from `arr`)

The same applies for functions like ``np.sum`` which use these internally.
This change is necessary to achieve consistent handling within NumPy.

If you run into these, in most cases pass for example ``dtype=np.timedelta64``
which clearly denotes a general ``timedelta64`` without any unit or byte-order
defined.  If you need to specify the output dtype precisely, you may do so
by either casting the inputs or providing an output array using `out=`.

NumPy may choose to allow providing an exact output ``dtype`` here in the
future, which would be preceded by a ``FutureWarning``.

(`gh-18718 <https://github.com/numpy/numpy/pull/18718>`__)

Ufunc ``signature=...`` and ``dtype=`` generalization and ``casting``
---------------------------------------------------------------------
The behaviour for ``np.ufunc(1.0, 1.0, signature=...)`` or
``np.ufunc(1.0, 1.0, dtype=...)`` can now yield different loops in 1.21
compared to 1.20 because of changes in promotion.
When ``signature`` was previously used, the casting check on inputs
was relaxed, which could lead to downcasting inputs unsafely especially
if combined with ``casting="unsafe"``.

Casting is now guaranteed to be safe.  If a signature is only
partially provided, for example using ``signature=("float64", None, None)``,
this could lead to no loop being found (an error).
In that case, it is necessary to provide the complete signature
to enforce casting the inputs.
If ``dtype="float64"`` is used or only outputs are set (e.g.
``signature=(None, None, "float64")`` the is unchanged.
We expect that very few users are affected by this change.

Further, the meaning of ``dtype="float64"`` has been slightly modified and
now strictly enforces only the correct output (and not input) DTypes.
This means it is now always equivalent to::

    signature=(None, None, "float64")

(If the ufunc has two inputs and one output).  Since this could lead
to no loop being found in some cases, NumPy will normally also search
for the loop::

    signature=("float64", "float64", "float64")

if the first search failed.
In the future, this behaviour may be customized to achieve the expected
results for more complex ufuncs.  (For some universal functions such as
``np.ldexp`` inputs can have different DTypes.)

(`gh-18880 <https://github.com/numpy/numpy/pull/18880>`__)

Distutils forces strict floating point model on clang
-----------------------------------------------------
NumPy distutils will now always add the ``-ffp-exception-behavior=strict``
compiler flag when compiling with clang.  Clang defaults to a non-strict
version, which allows the compiler to generate code that does not set
floating point warnings/errors correctly.

(`gh-19049 <https://github.com/numpy/numpy/pull/19049>`__)


C API changes
=============

Use of ``ufunc->type_resolver`` and "type tuple"
------------------------------------------------
NumPy now normalizes the "type tuple" argument to the type resolver functions
before calling it.  Note that in the use of this type resolver is legacy
behaviour and NumPy will not do so when possible.  Calling
``ufunc->type_resolver`` or ``PyUFunc_DefaultTypeResolver`` is strongly
discouraged and will now enforce a normalized type tuple if done.  Note that
this does not affect providing a type resolver, which is expected to keep
working in most circumstances.  If you have an unexpected use-case for calling
the type resolver, please inform the NumPy developers so that a solution can be
found.

(`gh-18718 <https://github.com/numpy/numpy/pull/18718>`__)


New Features
============

Added a mypy plugin for handling platform-specific ``numpy.number`` precisions
------------------------------------------------------------------------------
A mypy_ plugin is now available for automatically assigning the (platform-dependent)
precisions of certain `~numpy.number` subclasses, including the likes of
`~numpy.int_`, `~numpy.intp` and `~numpy.longlong`. See the documentation on
:ref:`scalar types <arrays.scalars.built-in>` for a comprehensive overview
of the affected classes.

Note that while usage of the plugin is completely optional, without it the
precision of above-mentioned classes will be inferred as `~typing.Any`.

To enable the plugin, one must add it to their mypy `configuration file`_:

.. code-block:: ini

    [mypy]
    plugins = numpy.typing.mypy_plugin


.. _mypy: http://mypy-lang.org/
.. _configuration file: https://mypy.readthedocs.io/en/stable/config_file.html

(`gh-17843 <https://github.com/numpy/numpy/pull/17843>`__)

Let the mypy plugin manage extended-precision ``numpy.number`` subclasses
-------------------------------------------------------------------------
The mypy_ plugin, introduced in `numpy/numpy#17843`_, has been expanded:
the plugin now removes annotations for platform-specific extended-precision
types that are not available to the platform in question.
For example, it will remove `~numpy.float128` when not available.

Without the plugin *all* extended-precision types will, as far as mypy is concerned,
be available on all platforms.

To enable the plugin, one must add it to their mypy `configuration file`_:

.. code-block:: ini

    [mypy]
    plugins = numpy.typing.mypy_plugin


.. _mypy: http://mypy-lang.org/
.. _configuration file: https://mypy.readthedocs.io/en/stable/config_file.html
.. _`numpy/numpy#17843`: https://github.com/numpy/numpy/pull/17843

(`gh-18322 <https://github.com/numpy/numpy/pull/18322>`__)

New ``min_digits`` argument for printing float values
-----------------------------------------------------
A new ``min_digits`` argument has been added to the dragon4 float printing
functions `~numpy.format_float_positional` and `~numpy.format_float_scientific`
. This kwd guarantees that at least the given number of digits will be printed
when printing in unique=True mode, even if the extra digits are unnecessary to
uniquely specify the value. It is the counterpart to the precision argument
which sets the maximum number of digits to be printed. When unique=False in
fixed precision mode, it has no effect and the precision argument fixes the
number of digits.

(`gh-18629 <https://github.com/numpy/numpy/pull/18629>`__)

f2py now recognizes Fortran abstract interface blocks
-----------------------------------------------------
`~numpy.f2py` can now parse abstract interface blocks.

(`gh-18695 <https://github.com/numpy/numpy/pull/18695>`__)

BLAS and LAPACK configuration via environment variables
-------------------------------------------------------
Autodetection of installed BLAS and LAPACK libraries can be bypassed by using
the ``NPY_BLAS_LIBS`` and ``NPY_LAPACK_LIBS`` environment variables. Instead,
the link flags in these environment variables will be used directly, and the
language is assumed to be F77.  This is especially useful in automated builds
where the BLAS and LAPACK that are installed are known exactly.  A use case is
replacing the actual implementation at runtime via stub library links.

If ``NPY_CBLAS_LIBS`` is set (optional in addition to ``NPY_BLAS_LIBS``), this
will be used as well, by defining ``HAVE_CBLAS`` and appending the environment
variable content to the link flags.

(`gh-18737 <https://github.com/numpy/numpy/pull/18737>`__)

A runtime-subcriptable alias has been added for ``ndarray``
-----------------------------------------------------------
``numpy.typing.NDArray`` has been added, a runtime-subscriptable alias for
``np.ndarray[Any, np.dtype[~Scalar]]``. The new type alias can be used
for annotating arrays with a given dtype and unspecified shape. :sup:`1`

:sup:`1` NumPy does not support the annotating of array shapes as of 1.21,
this is expected to change in the future though (see :pep:`646`).

Examples
~~~~~~~~

.. code-block:: python

    >>> import numpy as np
    >>> import numpy.typing as npt

    >>> print(npt.NDArray)
    numpy.ndarray[typing.Any, numpy.dtype[~ScalarType]]

    >>> print(npt.NDArray[np.float64])
    numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]

    >>> NDArrayInt = npt.NDArray[np.int_]
    >>> a: NDArrayInt = np.arange(10)

    >>> def func(a: npt.ArrayLike) -> npt.NDArray[Any]:
    ...     return np.array(a)

(`gh-18935 <https://github.com/numpy/numpy/pull/18935>`__)


Improvements
============

Arbitrary ``period`` option for ``numpy.unwrap``
------------------------------------------------
The size of the interval over which phases are unwrapped is no longer restricted to ``2 * pi``.
This is especially useful for unwrapping degrees, but can also be used for other intervals.

.. code:: python

    >>> phase_deg = np.mod(np.linspace(0,720,19), 360) - 180
    >>> phase_deg
    array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
           -180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
           -180.])

    >>> unwrap(phase_deg, period=360)
    array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
            180.,  220.,  260.,  300.,  340.,  380.,  420.,  460.,  500.,
            540.])

(`gh-16987 <https://github.com/numpy/numpy/pull/16987>`__)

``np.unique`` now returns single ``NaN``
----------------------------------------
When ``np.unique`` operated on an array with multiple ``NaN`` entries,
its return included a ``NaN`` for each entry that was ``NaN`` in the original array.
This is now improved such that the returned array contains just one ``NaN`` as the
last element.

Also for complex arrays all ``NaN`` values are considered equivalent
(no matter whether the ``NaN`` is in the real or imaginary part). As the
representant for the returned array the smallest one in the
lexicographical order is chosen - see ``np.sort`` for how the lexicographical
order is defined for complex arrays.

(`gh-18070 <https://github.com/numpy/numpy/pull/18070>`__)

``Generator.rayleigh`` and ``Generator.geometric`` performance improved
-----------------------------------------------------------------------
The performance of Rayleigh and geometric random variate generation
in ``Generator`` has improved. These are both transformation of exponential
random variables and the slow log-based inverse cdf transformation has
been replaced with the Ziggurat-based exponential variate generator.

This change breaks the stream of variates generated  when variates from
either of these distributions are produced.

(`gh-18666 <https://github.com/numpy/numpy/pull/18666>`__)

Placeholder annotations have been improved
------------------------------------------
All placeholder annotations, that were previously annotated as ``typing.Any``,
have been improved. Where appropriate they have been replaced with explicit
function definitions, classes or other miscellaneous objects.

(`gh-18934 <https://github.com/numpy/numpy/pull/18934>`__)


Performance improvements
========================

Improved performance in integer division of NumPy arrays
--------------------------------------------------------
Integer division of NumPy arrays now uses
`libdivide <https://libdivide.com/>`__ when the divisor is a constant. With the
usage of libdivide and other minor optimizations, there is a large speedup.
The ``//`` operator and ``np.floor_divide`` makes use of the new changes.

(`gh-17727 <https://github.com/numpy/numpy/pull/17727>`__)

Improve performance of ``np.save`` and ``np.load`` for small arrays
-------------------------------------------------------------------
``np.save`` is now a lot faster for small arrays.

``np.load`` is also faster for small arrays,
but only when serializing with a version >= ``(3, 0)``.

Both are done by removing checks that are only relevant for Python 2,
while still maintaining compatibility with arrays
which might have been created by Python 2.

(`gh-18657 <https://github.com/numpy/numpy/pull/18657>`__)


Changes
=======

`numpy.piecewise` output class now matches the input class
----------------------------------------------------------
When `~numpy.ndarray` subclasses are used on input to `~numpy.piecewise`,
they are passed on to the functions. The output will now be of the
same subclass as well.

(`gh-18110 <https://github.com/numpy/numpy/pull/18110>`__)

Enable Accelerate Framework
----------------------------
With the release of macOS 11.3, several different issues that numpy was
encountering when using Accelerate Framework's implementation of BLAS and
LAPACK should be resolved.  This change enables the Accelerate Framework as an
option on macOS.  If additional issues are found, please file a bug report
against Accelerate using the developer feedback assistant tool
(https://developer.apple.com/bug-reporting/). We intend to address issues
promptly and plan to continue supporting and updating our BLAS and LAPACK
libraries.

(`gh-18874 <https://github.com/numpy/numpy/pull/18874>`__)
=========================
NumPy 1.6.2 Release Notes
=========================

This is a bugfix release in the 1.6.x series.  Due to the delay of the NumPy
1.7.0 release, this release contains far more fixes than a regular NumPy bugfix
release.  It also includes a number of documentation and build improvements.

Issues fixed
============

``numpy.core``
--------------

* #2063: make unique() return consistent index
* #1138: allow creating arrays from empty buffers or empty slices
* #1446: correct note about correspondence vstack and concatenate
* #1149: make argmin() work for datetime
* #1672: fix allclose() to work for scalar inf
* #1747: make np.median() work for 0-D arrays
* #1776: make complex division by zero to yield inf properly
* #1675: add scalar support for the format() function
* #1905: explicitly check for NaNs in allclose()
* #1952: allow floating ddof in std() and var()
* #1948: fix regression for indexing chararrays with empty list
* #2017: fix type hashing
* #2046: deleting array attributes causes segfault
* #2033: a**2.0 has incorrect type
* #2045: make attribute/iterator_element deletions not segfault
* #2021: fix segfault in searchsorted()
* #2073: fix float16 __array_interface__ bug


``numpy.lib``
-------------

* #2048: break reference cycle in NpzFile
* #1573: savetxt() now handles complex arrays
* #1387: allow bincount() to accept empty arrays
* #1899: fixed histogramdd() bug with empty inputs
* #1793: fix failing npyio test under py3k
* #1936: fix extra nesting for subarray dtypes
* #1848: make tril/triu return the same dtype as the original array
* #1918: use Py_TYPE to access ob_type, so it works also on Py3


``numpy.distutils``
-------------------

* #1261: change compile flag on AIX from -O5 to -O3
* #1377: update HP compiler flags
* #1383: provide better support for C++ code on HPUX
* #1857: fix build for py3k + pip
* BLD: raise a clearer warning in case of building without cleaning up first
* BLD: follow build_ext coding convention in build_clib
* BLD: fix up detection of Intel CPU on OS X in system_info.py
* BLD: add support for the new X11 directory structure on Ubuntu & co.
* BLD: add ufsparse to the libraries search path.
* BLD: add 'pgfortran' as a valid compiler in the Portland Group
* BLD: update version match regexp for IBM AIX Fortran compilers.


``numpy.random``
----------------

* BUG: Use npy_intp instead of long in mtrand

Changes
=======

``numpy.f2py``
--------------

* ENH: Introduce new options extra_f77_compiler_args and extra_f90_compiler_args
* BLD: Improve reporting of fcompiler value
* BUG: Fix f2py test_kind.py test


``numpy.poly``
--------------

* ENH: Add some tests for polynomial printing
* ENH: Add companion matrix functions
* DOC: Rearrange the polynomial documents
* BUG: Fix up links to classes
* DOC: Add version added to some of the polynomial package modules
* DOC: Document xxxfit functions in the polynomial package modules
* BUG: The polynomial convenience classes let different types interact
* DOC: Document the use of the polynomial convenience classes
* DOC: Improve numpy reference documentation of polynomial classes
* ENH: Improve the computation of polynomials from roots
* STY: Code cleanup in polynomial [*]fromroots functions
* DOC: Remove references to cast and NA, which were added in 1.7
==========================
NumPy 1.13.1 Release Notes
==========================

This is a bugfix release for problems found in 1.13.0. The major changes are
fixes for the new memory overlap detection and temporary elision as well as
reversion of the removal of the boolean binary ``-`` operator. Users of 1.13.0
should upgrade.

Thr Python versions supported are 2.7 and 3.4 - 3.6. Note that the Python 3.6
wheels available from PIP are built against 3.6.1, hence will not work when
used with 3.6.0 due to Python bug 29943_. NumPy 1.13.2 will be released shortly
after Python 3.6.2 is out to fix that problem. If you are using 3.6.0 the
workaround is to upgrade to 3.6.1 or use an earlier Python version.

.. _29943: https://bugs.python.org/issue29943


Pull requests merged
====================
A total of 19 pull requests were merged for this release.

* #9240 DOC: BLD: fix lots of Sphinx warnings/errors.
* #9255 Revert "DEP: Raise TypeError for subtract(bool, bool)."
* #9261 BUG: don't elide into readonly and updateifcopy temporaries for...
* #9262 BUG: fix missing keyword rename for common block in numpy.f2py
* #9263 BUG: handle resize of 0d array
* #9267 DOC: update f2py front page and some doc build metadata.
* #9299 BUG: Fix Intel compilation on Unix.
* #9317 BUG: fix wrong ndim used in empty where check
* #9319 BUG: Make extensions compilable with MinGW on Py2.7
* #9339 BUG: Prevent crash if ufunc doc string is null
* #9340 BUG: umath: un-break ufunc where= when no out= is given
* #9371 DOC: Add isnat/positive ufunc to documentation
* #9372 BUG: Fix error in fromstring function from numpy.core.records...
* #9373 BUG: ')' is printed at the end pointer of the buffer in numpy.f2py.
* #9374 DOC: Create NumPy 1.13.1 release notes.
* #9376 BUG: Prevent hang traversing ufunc userloop linked list
* #9377 DOC: Use x1 and x2 in the heaviside docstring.
* #9378 DOC: Add $PARAMS to the isnat docstring
* #9379 DOC: Update the 1.13.1 release notes


Contributors
============
A total of 12 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Andras Deak +
* Bob Eldering +
* Charles Harris
* Daniel Hrisca +
* Eric Wieser
* Joshua Leahy +
* Julian Taylor
* Michael Seifert
* Pauli Virtanen
* Ralf Gommers
* Roland Kaufmann
* Warren Weckesser
=========================
NumPy 1.9.1 Release Notes
=========================

This is a bugfix only release in the 1.9.x series.

Issues fixed
============

* gh-5184: restore linear edge behaviour of gradient to as it was in < 1.9.
  The second order behaviour is available via the `edge_order` keyword
* gh-4007: workaround Accelerate sgemv crash on OSX 10.9
* gh-5100: restore object dtype inference from iterable objects without `len()`
* gh-5163: avoid gcc-4.1.2 (red hat 5) miscompilation causing a crash
* gh-5138: fix nanmedian on arrays containing inf
* gh-5240: fix not returning out array from ufuncs with subok=False set
* gh-5203: copy inherited masks in MaskedArray.__array_finalize__
* gh-2317: genfromtxt did not handle filling_values=0 correctly
* gh-5067: restore api of npy_PyFile_DupClose in python2
* gh-5063: cannot convert invalid sequence index to tuple
* gh-5082: Segmentation fault with argmin() on unicode arrays
* gh-5095: don't propagate subtypes from np.where
* gh-5104: np.inner segfaults with SciPy's sparse matrices
* gh-5251: Issue with fromarrays not using correct format for unicode arrays
* gh-5136: Import dummy_threading if importing threading fails
* gh-5148: Make numpy import when run with Python flag '-OO'
* gh-5147: Einsum double contraction in particular order causes ValueError
* gh-479: Make f2py work with intent(in out)
* gh-5170: Make python2 .npy files readable in python3
* gh-5027: Use 'll' as the default length specifier for long long
* gh-4896: fix build error with MSVC 2013 caused by C99 complex support
* gh-4465: Make PyArray_PutTo respect writeable flag
* gh-5225: fix crash when using arange on datetime without dtype set
* gh-5231: fix build in c99 mode
=========================
NumPy 1.4.0 Release Notes
=========================

This minor includes numerous bug fixes, as well as a few new features. It
is backward compatible with 1.3.0 release.

Highlights
==========

* New datetime dtype support to deal with dates in arrays

* Faster import time

* Extended array wrapping mechanism for ufuncs

* New Neighborhood iterator (C-level only)

* C99-like complex functions in npymath

New features
============

Extended array wrapping mechanism for ufuncs
--------------------------------------------

An __array_prepare__ method has been added to ndarray to provide subclasses
greater flexibility to interact with ufuncs and ufunc-like functions. ndarray
already provided __array_wrap__, which allowed subclasses to set the array type
for the result and populate metadata on the way out of the ufunc (as seen in
the implementation of MaskedArray). For some applications it is necessary to
provide checks and populate metadata *on the way in*. __array_prepare__ is
therefore called just after the ufunc has initialized the output array but
before computing the results and populating it. This way, checks can be made
and errors raised before operations which may modify data in place.

Automatic detection of forward incompatibilities
------------------------------------------------

Previously, if an extension was built against a version N of NumPy, and used on
a system with NumPy M < N, the import_array was successful, which could cause
crashes because the version M does not have a function in N. Starting from
NumPy 1.4.0, this will cause a failure in import_array, so the error will be
caught early on.

New iterators
-------------

A new neighborhood iterator has been added to the C API. It can be used to
iterate over the items in a neighborhood of an array, and can handle boundaries
conditions automatically. Zero and one padding are available, as well as
arbitrary constant value, mirror and circular padding.

New polynomial support
----------------------

New modules chebyshev and polynomial have been added. The new polynomial module
is not compatible with the current polynomial support in numpy, but is much
like the new chebyshev module. The most noticeable difference to most will
be that coefficients are specified from low to high power, that the low
level functions do *not* work with the Chebyshev and Polynomial classes as
arguments, and that the Chebyshev and Polynomial classes include a domain.
Mapping between domains is a linear substitution and the two classes can be
converted one to the other, allowing, for instance, a Chebyshev series in
one domain to be expanded as a polynomial in another domain. The new classes
should generally be used instead of the low level functions, the latter are
provided for those who wish to build their own classes.

The new modules are not automatically imported into the numpy namespace,
they must be explicitly brought in with an "import numpy.polynomial"
statement.

New C API
---------

The following C functions have been added to the C API:

    #. PyArray_GetNDArrayCFeatureVersion: return the *API* version of the
       loaded numpy.
    #. PyArray_Correlate2 - like PyArray_Correlate, but implements the usual
       definition of correlation. Inputs are not swapped, and conjugate is
       taken for complex arrays.
    #. PyArray_NeighborhoodIterNew - a new iterator to iterate over a
       neighborhood of a point, with automatic boundaries handling. It is
       documented in the iterators section of the C-API reference, and you can
       find some examples in  the multiarray_test.c.src file in numpy.core.

New ufuncs
----------

The following ufuncs have been added to the C API:

    #. copysign - return the value of the first argument with the sign copied
       from the second argument.
    #. nextafter - return the next representable floating point value of the
       first argument toward the second argument.

New defines
-----------

The alpha processor is now defined and available in numpy/npy_cpu.h. The
failed detection of the PARISC processor has been fixed. The defines are:

    #. NPY_CPU_HPPA: PARISC
    #. NPY_CPU_ALPHA: Alpha

Testing
-------

    #. deprecated decorator: this decorator may be used to avoid cluttering
       testing output while testing DeprecationWarning is effectively raised by
       the decorated test.
    #. assert_array_almost_equal_nulps: new method to compare two arrays of
       floating point values. With this function, two values are considered
       close if there are not many representable floating point values in
       between, thus being more robust than assert_array_almost_equal when the
       values fluctuate a lot.
    #. assert_array_max_ulp: raise an assertion if there are more than N
       representable numbers between two floating point values.
    #. assert_warns: raise an AssertionError if a callable does not generate a
       warning of the appropriate class, without altering the warning state.

Reusing npymath
---------------

In 1.3.0, we started putting portable C math routines in npymath library, so
that people can use those to write portable extensions. Unfortunately, it was
not possible to easily link against this library: in 1.4.0, support has been
added to numpy.distutils so that 3rd party can reuse this library. See coremath
documentation for more information.

Improved set operations
-----------------------

In previous versions of NumPy some set functions (intersect1d,
setxor1d, setdiff1d and setmember1d) could return incorrect results if
the input arrays contained duplicate items. These now work correctly
for input arrays with duplicates. setmember1d has been renamed to
in1d, as with the change to accept arrays with duplicates it is
no longer a set operation, and is conceptually similar to an
elementwise version of the Python operator 'in'.  All of these
functions now accept the boolean keyword assume_unique. This is False
by default, but can be set True if the input arrays are known not
to contain duplicates, which can increase the functions' execution
speed.

Improvements
============

    #. numpy import is noticeably faster (from 20 to 30 % depending on the
       platform and computer)

    #. The sort functions now sort nans to the end.

        * Real sort order is [R, nan]
        * Complex sort order is [R + Rj, R + nanj, nan + Rj, nan + nanj]

       Complex numbers with the same nan placements are sorted according to
       the non-nan part if it exists.
    #. The type comparison functions have been made consistent with the new
       sort order of nans. Searchsorted now works with sorted arrays
       containing nan values.
    #. Complex division has been made more resistant to overflow.
    #. Complex floor division has been made more resistant to overflow.

Deprecations
============

The following functions are deprecated:

    #. correlate: it takes a new keyword argument old_behavior. When True (the
       default), it returns the same result as before. When False, compute the
       conventional correlation, and take the conjugate for complex arrays. The
       old behavior will be removed in NumPy 1.5, and raises a
       DeprecationWarning in 1.4.

    #. unique1d: use unique instead. unique1d raises a deprecation
       warning in 1.4, and will be removed in 1.5.

    #. intersect1d_nu: use intersect1d instead. intersect1d_nu raises
       a deprecation warning in 1.4, and will be removed in 1.5.

    #. setmember1d: use in1d instead. setmember1d raises a deprecation
       warning in 1.4, and will be removed in 1.5.

The following raise errors:

    #. When operating on 0-d arrays, ``numpy.max`` and other functions accept
       only ``axis=0``, ``axis=-1`` and ``axis=None``. Using an out-of-bounds
       axes is an indication of a bug, so Numpy raises an error for these cases
       now.

    #. Specifying ``axis > MAX_DIMS`` is no longer allowed; Numpy raises now an
       error instead of behaving similarly as for ``axis=None``.

Internal changes
================

Use C99 complex functions when available
----------------------------------------

The numpy complex types are now guaranteed to be ABI compatible with C99
complex type, if available on the platform. Moreover, the complex ufunc now use
the platform C99 functions instead of our own.

split multiarray and umath source code
--------------------------------------

The source code of multiarray and umath has been split into separate logic
compilation units. This should make the source code more amenable for
newcomers.

Separate compilation
--------------------

By default, every file of multiarray (and umath) is merged into one for
compilation as was the case before, but if NPY_SEPARATE_COMPILATION env
variable is set to a non-negative value, experimental individual compilation of
each file is enabled. This makes the compile/debug cycle much faster when
working on core numpy.

Separate core math library
--------------------------

New functions which have been added:

	* npy_copysign
        * npy_nextafter
        * npy_cpack
        * npy_creal
        * npy_cimag
        * npy_cabs
        * npy_cexp
        * npy_clog
        * npy_cpow
        * npy_csqr
        * npy_ccos
        * npy_csin
=========================
NumPy 1.7.1 Release Notes
=========================

This is a bugfix only release in the 1.7.x series.
It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last series that
supports Python 2.4 - 2.5.


Issues fixed
============

* gh-2973: Fix `1` is printed during numpy.test()
* gh-2983: BUG: gh-2969: Backport memory leak fix 80b3a34.
* gh-3007: Backport gh-3006
* gh-2984: Backport fix complex polynomial fit
* gh-2982: BUG: Make nansum work with booleans.
* gh-2985: Backport large sort fixes
* gh-3039: Backport object take
* gh-3105: Backport nditer fix op axes initialization
* gh-3108: BUG: npy-pkg-config ini files were missing after Bento build.
* gh-3124: BUG: PyArray_LexSort allocates too much temporary memory.
* gh-3131: BUG: Exported f2py_size symbol prevents linking multiple f2py modules.
* gh-3117: Backport gh-2992
* gh-3135: DOC: Add mention of PyArray_SetBaseObject stealing a reference
* gh-3134: DOC: Fix typo in fft docs (the indexing variable is 'm', not 'n').
* gh-3136: Backport #3128
==========================
NumPy 1.11.2 Release Notes
==========================

Numpy 1.11.2 supports Python 2.6 - 2.7 and 3.2 - 3.5. It fixes bugs and
regressions found in Numpy 1.11.1 and includes several build related
improvements. Wheels for Linux, Windows, and OS X can be found on PyPI.

Pull Requests Merged
====================

Fixes overridden by later merges and release notes updates are omitted.

- #7736 BUG: Many functions silently drop 'keepdims' kwarg.
- #7738 ENH: Add extra kwargs and update doc of many MA methods.
- #7778 DOC: Update Numpy 1.11.1 release notes.
- #7793 BUG: MaskedArray.count treats negative axes incorrectly.
- #7816 BUG: Fix array too big error for wide dtypes.
- #7821 BUG: Make sure npy_mul_with_overflow_<type> detects overflow.
- #7824 MAINT: Allocate fewer bytes for empty arrays.
- #7847 MAINT,DOC: Fix some imp module uses and update f2py.compile docstring.
- #7849 MAINT: Fix remaining uses of deprecated Python imp module.
- #7851 BLD: Fix ATLAS version detection.
- #7896 BUG: Construct ma.array from np.array which contains padding.
- #7904 BUG: Fix float16 type not being called due to wrong ordering.
- #7917 BUG: Production install of numpy should not require nose.
- #7919 BLD: Fixed MKL detection for recent versions of this library.
- #7920 BUG: Fix for issue #7835 (ma.median of 1d).
- #7932 BUG: Monkey-patch _msvccompile.gen_lib_option like other compilers.
- #7939 BUG: Check for HAVE_LDOUBLE_DOUBLE_DOUBLE_LE in npy_math_complex.
- #7953 BUG: Guard against buggy comparisons in generic quicksort.
- #7954 BUG: Use keyword arguments to initialize Extension base class.
- #7955 BUG: Make sure numpy globals keep identity after reload.
- #7972 BUG: MSVCCompiler grows 'lib' & 'include' env strings exponentially.
- #8005 BLD: Remove __NUMPY_SETUP__ from builtins at end of setup.py.
- #8010 MAINT: Remove leftover imp module imports.
- #8020 BUG: Fix return of np.ma.count if keepdims is True and axis is None.
- #8024 BUG: Fix numpy.ma.median.
- #8031 BUG: Fix np.ma.median with only one non-masked value.
- #8044 BUG: Fix bug in NpyIter buffering with discontinuous arrays.
=========================
NumPy 1.9.0 Release Notes
=========================

This release supports Python 2.6 - 2.7 and 3.2 - 3.4.


Highlights
==========
* Numerous performance improvements in various areas, most notably indexing and
  operations on small arrays are significantly faster.
  Indexing operations now also release the GIL.
* Addition of `nanmedian` and `nanpercentile` rounds out the nanfunction set.


Dropped Support
===============

* The oldnumeric and numarray modules have been removed.
* The doc/pyrex and doc/cython directories have been removed.
* The doc/numpybook directory has been removed.
* The numpy/testing/numpytest.py file has been removed together with
  the importall function it contained.


Future Changes
==============

* The numpy/polynomial/polytemplate.py file will be removed in NumPy 1.10.0.
* Default casting for inplace operations will change to 'same_kind' in
  Numpy 1.10.0. This will certainly break some code that is currently
  ignoring the warning.
* Relaxed stride checking will be the default in 1.10.0
* String version checks will break because, e.g., '1.9' > '1.10' is True. A
  NumpyVersion class has been added that can be used for such comparisons.
* The diagonal and diag functions will return writeable views in 1.10.0
* The `S` and/or `a` dtypes may be changed to represent Python strings
  instead of bytes, in Python 3 these two types are very different.


Compatibility notes
===================

The diagonal and diag functions return readonly views.
------------------------------------------------------
In NumPy 1.8, the diagonal and diag functions returned readonly copies, in
NumPy 1.9 they return readonly views, and in 1.10 they will return writeable
views.

Special scalar float values don't cause upcast to double anymore
----------------------------------------------------------------
In previous numpy versions operations involving floating point scalars
containing special values ``NaN``, ``Inf`` and ``-Inf`` caused the result
type to be at least ``float64``.  As the special values can be represented
in the smallest available floating point type, the upcast is not performed
anymore.

For example the dtype of:

    ``np.array([1.], dtype=np.float32) * float('nan')``

now remains ``float32`` instead of being cast to ``float64``.
Operations involving non-special values have not been changed.

Percentile output changes
-------------------------
If given more than one percentile to compute numpy.percentile returns an
array instead of a list. A single percentile still returns a scalar.  The
array is equivalent to converting the list returned in older versions
to an array via ``np.array``.

If the ``overwrite_input`` option is used the input is only partially
instead of fully sorted.

ndarray.tofile exception type
-----------------------------
All ``tofile`` exceptions are now ``IOError``, some were previously
``ValueError``.

Invalid fill value exceptions
-----------------------------
Two changes to numpy.ma.core._check_fill_value:

* When the fill value is a string and the array type is not one of
  'OSUV', TypeError is raised instead of the default fill value being used.

* When the fill value overflows the array type, TypeError is raised instead
  of OverflowError.

Polynomial Classes no longer derived from PolyBase
--------------------------------------------------
This may cause problems with folks who depended on the polynomial classes
being derived from PolyBase. They are now all derived from the abstract
base class ABCPolyBase. Strictly speaking, there should be a deprecation
involved, but no external code making use of the old baseclass could be
found.

Using numpy.random.binomial may change the RNG state vs. numpy < 1.9
--------------------------------------------------------------------
A bug in one of the algorithms to generate a binomial random variate has
been fixed. This change will likely alter the number of random draws
performed, and hence the sequence location will be different after a
call to distribution.c::rk_binomial_btpe. Any tests which rely on the RNG
being in a known state should be checked and/or updated as a result.

Random seed enforced to be a 32 bit unsigned integer
----------------------------------------------------
``np.random.seed`` and ``np.random.RandomState`` now throw a ``ValueError``
if the seed cannot safely be converted to 32 bit unsigned integers.
Applications that now fail can be fixed by masking the higher 32 bit values to
zero: ``seed = seed & 0xFFFFFFFF``. This is what is done silently in older
versions so the random stream remains the same.

Argmin and argmax out argument
------------------------------
The ``out`` argument to ``np.argmin`` and ``np.argmax`` and their
equivalent C-API functions is now checked to match the desired output shape
exactly.  If the check fails a ``ValueError`` instead of ``TypeError`` is
raised.

Einsum
------
Remove unnecessary broadcasting notation restrictions.
``np.einsum('ijk,j->ijk', A, B)`` can also be written as
``np.einsum('ij...,j->ij...', A, B)`` (ellipsis is no longer required on 'j')

Indexing
--------

The NumPy indexing has seen a complete rewrite in this version. This makes
most advanced integer indexing operations much faster and should have no
other implications.  However some subtle changes and deprecations were
introduced in advanced indexing operations:

* Boolean indexing into scalar arrays will always return a new 1-d array.
  This means that ``array(1)[array(True)]`` gives ``array([1])`` and
  not the original array.

* Advanced indexing into one dimensional arrays used to have
  (undocumented) special handling regarding repeating the value array in
  assignments when the shape of the value array was too small or did not
  match.  Code using this will raise an error. For compatibility you can
  use ``arr.flat[index] = values``, which uses the old code branch.  (for
  example ``a = np.ones(10); a[np.arange(10)] = [1, 2, 3]``)

* The iteration order over advanced indexes used to be always C-order.
  In NumPy 1.9. the iteration order adapts to the inputs and is not
  guaranteed (with the exception of a *single* advanced index which is
  never reversed for compatibility reasons). This means that the result
  is undefined if multiple values are assigned to the same element.  An
  example for this is ``arr[[0, 0], [1, 1]] = [1, 2]``, which may set
  ``arr[0, 1]`` to either 1 or 2.

* Equivalent to the iteration order, the memory layout of the advanced
  indexing result is adapted for faster indexing and cannot be predicted.

* All indexing operations return a view or a copy. No indexing operation
  will return the original array object. (For example ``arr[...]``)

* In the future Boolean array-likes (such as lists of python bools) will
  always be treated as Boolean indexes and Boolean scalars (including
  python ``True``) will be a legal *boolean* index. At this time, this is
  already the case for scalar arrays to allow the general
  ``positive = a[a > 0]`` to work when ``a`` is zero dimensional.

* In NumPy 1.8 it was possible to use ``array(True)`` and
  ``array(False)`` equivalent to 1 and 0 if the result of the operation
  was a scalar.  This will raise an error in NumPy 1.9 and, as noted
  above, treated as a boolean index in the future.

* All non-integer array-likes are deprecated, object arrays of custom
  integer like objects may have to be cast explicitly.

* The error reporting for advanced indexing is more informative, however
  the error type has changed in some cases. (Broadcasting errors of
  indexing arrays are reported as ``IndexError``)

* Indexing with more then one ellipsis (``...``) is deprecated.

Non-integer reduction axis indexes are deprecated
-------------------------------------------------
Non-integer axis indexes to reduction ufuncs like `add.reduce` or `sum` are
deprecated.

``promote_types`` and string dtype
----------------------------------
``promote_types`` function now returns a valid string length when given an
integer or float dtype as one argument and a string dtype as another
argument.  Previously it always returned the input string dtype, even if it
wasn't long enough to store the max integer/float value converted to a
string.

``can_cast`` and string dtype
-----------------------------
``can_cast`` function now returns False in "safe" casting mode for
integer/float dtype and string dtype if the string dtype length is not long
enough to store the max integer/float value converted to a string.
Previously ``can_cast`` in "safe" mode returned True for integer/float
dtype and a string dtype of any length.

astype and string dtype
-----------------------
The ``astype`` method now returns an error if the string dtype to cast to
is not long enough in "safe" casting mode to hold the max value of
integer/float array that is being casted. Previously the casting was
allowed even if the result was truncated.

`npyio.recfromcsv` keyword arguments change
-------------------------------------------
`npyio.recfromcsv` no longer accepts the undocumented `update` keyword,
which used to override the `dtype` keyword.

The ``doc/swig`` directory moved
--------------------------------
The ``doc/swig`` directory has been moved to ``tools/swig``.

The ``npy_3kcompat.h`` header changed
-------------------------------------
The unused ``simple_capsule_dtor`` function has been removed from
``npy_3kcompat.h``.  Note that this header is not meant to be used outside
of numpy; other projects should be using their own copy of this file when
needed.

Negative indices in C-Api ``sq_item`` and ``sq_ass_item`` sequence methods
--------------------------------------------------------------------------
When directly accessing the ``sq_item`` or ``sq_ass_item`` PyObject slots
for item getting, negative indices will not be supported anymore.
``PySequence_GetItem`` and ``PySequence_SetItem`` however fix negative
indices so that they can be used there.

NDIter
------
When ``NpyIter_RemoveAxis`` is now called, the iterator range will be reset.

When a multi index is being tracked and an iterator is not buffered, it is
possible to use ``NpyIter_RemoveAxis``. In this case an iterator can shrink
in size. Because the total size of an iterator is limited, the iterator
may be too large before these calls. In this case its size will be set to ``-1``
and an error issued not at construction time but when removing the multi
index, setting the iterator range, or getting the next function.

This has no effect on currently working code, but highlights the necessity
of checking for an error return if these conditions can occur. In most
cases the arrays being iterated are as large as the iterator so that such
a problem cannot occur.

This change was already applied to the 1.8.1 release.

``zeros_like`` for string dtypes now returns empty strings
----------------------------------------------------------
To match the `zeros` function `zeros_like` now returns an array initialized
with empty strings instead of an array filled with `'0'`.


New Features
============

Percentile supports more interpolation options
----------------------------------------------
``np.percentile`` now has the interpolation keyword argument to specify in
which way points should be interpolated if the percentiles fall between two
values.  See the documentation for the available options.

Generalized axis support for median and percentile
--------------------------------------------------
``np.median`` and ``np.percentile`` now support generalized axis arguments like
ufunc reductions do since 1.7. One can now say axis=(index, index) to pick a
list of axes for the reduction. The ``keepdims`` keyword argument was also
added to allow convenient broadcasting to arrays of the original shape.

Dtype parameter added to ``np.linspace`` and ``np.logspace``
------------------------------------------------------------
The returned data type from the ``linspace`` and ``logspace`` functions can
now be specified using the dtype parameter.

More general ``np.triu`` and ``np.tril`` broadcasting
-----------------------------------------------------
For arrays with ``ndim`` exceeding 2, these functions will now apply to the
final two axes instead of raising an exception.

``tobytes`` alias for ``tostring`` method
-----------------------------------------
``ndarray.tobytes`` and ``MaskedArray.tobytes`` have been added as aliases
for ``tostring`` which exports arrays as ``bytes``. This is more consistent
in Python 3 where ``str`` and ``bytes`` are not the same.

Build system
------------
Added experimental support for the ppc64le and OpenRISC architecture.

Compatibility to python ``numbers`` module
------------------------------------------
All numerical numpy types are now registered with the type hierarchy in
the python ``numbers`` module.

``increasing`` parameter added to ``np.vander``
-----------------------------------------------
The ordering of the columns of the Vandermonde matrix can be specified with
this new boolean argument.

``unique_counts`` parameter added to ``np.unique``
--------------------------------------------------
The number of times each unique item comes up in the input can now be
obtained as an optional return value.

Support for median and percentile in nanfunctions
-------------------------------------------------
The ``np.nanmedian`` and ``np.nanpercentile`` functions behave like
the median and percentile functions except that NaNs are ignored.

NumpyVersion class added
------------------------
The class may be imported from numpy.lib and can be used for version
comparison when the numpy version goes to 1.10.devel. For example::

    >>> from numpy.lib import NumpyVersion
    >>> if NumpyVersion(np.__version__) < '1.10.0'):
    ...     print('Wow, that is an old NumPy version!')

Allow saving arrays with large number of named columns
------------------------------------------------------
The numpy storage format 1.0 only allowed the array header to have a total size
of 65535 bytes. This can be exceeded by structured arrays with a large number
of columns. A new format 2.0 has been added which extends the header size to 4
GiB. `np.save` will automatically save in 2.0 format if the data requires it,
else it will always use the more compatible 1.0 format.

Full broadcasting support for ``np.cross``
------------------------------------------
``np.cross`` now properly broadcasts its two input arrays, even if they
have different number of dimensions. In earlier versions this would result
in either an error being raised, or wrong results computed.


Improvements
============

Better numerical stability for sum in some cases
------------------------------------------------
Pairwise summation is now used in the sum method, but only along the fast
axis and for groups of the values <= 8192 in length. This should also
improve the accuracy of var and std in some common cases.

Percentile implemented in terms of ``np.partition``
---------------------------------------------------
``np.percentile`` has been implemented in terms of ``np.partition`` which
only partially sorts the data via a selection algorithm. This improves the
time complexity from ``O(nlog(n))`` to ``O(n)``.

Performance improvement for ``np.array``
----------------------------------------
The performance of converting lists containing arrays to arrays using
``np.array`` has been improved. It is now equivalent in speed to
``np.vstack(list)``.

Performance improvement for ``np.searchsorted``
-----------------------------------------------
For the built-in numeric types, ``np.searchsorted`` no longer relies on the
data type's ``compare`` function to perform the search, but is now
implemented by type specific functions. Depending on the size of the
inputs, this can result in performance improvements over 2x.

Optional reduced verbosity for np.distutils
-------------------------------------------
Set ``numpy.distutils.system_info.system_info.verbosity = 0`` and then
calls to ``numpy.distutils.system_info.get_info('blas_opt')`` will not
print anything on the output. This is mostly for other packages using
numpy.distutils.

Covariance check in ``np.random.multivariate_normal``
-----------------------------------------------------
A ``RuntimeWarning`` warning is raised when the covariance matrix is not
positive-semidefinite.

Polynomial Classes no longer template based
-------------------------------------------
The polynomial classes have been refactored to use an abstract base class
rather than a template in order to implement a common interface. This makes
importing the polynomial package faster as the classes do not need to be
compiled on import.

More GIL releases
-----------------
Several more functions now release the Global Interpreter Lock allowing more
efficient parallelization using the ``threading`` module. Most notably the GIL is
now released for fancy indexing, ``np.where`` and the ``random`` module now
uses a per-state lock instead of the GIL.

MaskedArray support for more complicated base classes
-----------------------------------------------------
Built-in assumptions that the baseclass behaved like a plain array are being
removed. In particular, ``repr`` and ``str`` should now work more reliably.


C-API
-----


Deprecations
============

Non-integer scalars for sequence repetition
-------------------------------------------
Using non-integer numpy scalars to repeat python sequences is deprecated.
For example ``np.float_(2) * [1]`` will be an error in the future.

``select`` input deprecations
-----------------------------
The integer and empty input to ``select`` is deprecated. In the future only
boolean arrays will be valid conditions and an empty ``condlist`` will be
considered an input error instead of returning the default.

``rank`` function
-----------------
The ``rank`` function has been deprecated to avoid confusion with
``numpy.linalg.matrix_rank``.

Object array equality comparisons
---------------------------------
In the future object array comparisons both `==` and `np.equal` will not
make use of identity checks anymore. For example:

>>> a = np.array([np.array([1, 2, 3]), 1])
>>> b = np.array([np.array([1, 2, 3]), 1])
>>> a == b

will consistently return False (and in the future an error) even if the array
in `a` and `b` was the same object.

The equality operator `==` will in the future raise errors like `np.equal`
if broadcasting or element comparisons, etc. fails.

Comparison with `arr == None` will in the future do an elementwise comparison
instead of just returning False. Code should be using `arr is None`.

All of these changes will give Deprecation- or FutureWarnings at this time.

C-API
-----

The utility function npy_PyFile_Dup and npy_PyFile_DupClose are broken by the
internal buffering python 3 applies to its file objects.
To fix this two new functions npy_PyFile_Dup2 and npy_PyFile_DupClose2 are
declared in npy_3kcompat.h and the old functions are deprecated.
Due to the fragile nature of these functions it is recommended to instead use
the python API when possible.

This change was already applied to the 1.8.1 release.
==========================
NumPy 1.15.0 Release Notes
==========================

NumPy 1.15.0 is a release with an unusual number of cleanups, many deprecations
of old functions, and improvements to many existing functions. Please read the
detailed descriptions below to see if you are affected.

For testing, we have switched to pytest as a replacement for the no longer
maintained nose framework. The old nose based interface remains for downstream
projects who may still be using it.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are
linked with OpenBLAS v0.3.0, which should fix some of the linalg problems
reported for NumPy 1.14.


Highlights
==========

* NumPy has switched to pytest for testing.
* A new  `numpy.printoptions` context manager.
* Many improvements to the histogram functions.
* Support for unicode field names in python 2.7.
* Improved support for PyPy.
* Fixes and improvements to `numpy.einsum`.


New functions
=============

* `numpy.gcd` and `numpy.lcm`, to compute the greatest common divisor and least
  common multiple.

* `numpy.ma.stack`, the `numpy.stack` array-joining function generalized to
  masked arrays.

* `numpy.quantile` function, an interface to ``percentile`` without factors of
  100

* `numpy.nanquantile` function, an interface to ``nanpercentile`` without
  factors of 100

* `numpy.printoptions`, a context manager that sets print options temporarily
  for the scope of the ``with`` block::

    >>> with np.printoptions(precision=2):
    ...     print(np.array([2.0]) / 3)
    [0.67]

* `numpy.histogram_bin_edges`, a function to get the edges of the bins used by a
  histogram without needing to calculate the histogram.

* C functions `npy_get_floatstatus_barrier` and `npy_clear_floatstatus_barrier`
  have been added to deal with compiler optimization changing the order of
  operations.  See below for details.


Deprecations
============

* Aliases of builtin `pickle` functions are deprecated, in favor of their
  unaliased ``pickle.<func>`` names:

  * `numpy.loads`
  * `numpy.core.numeric.load`
  * `numpy.core.numeric.loads`
  * `numpy.ma.loads`, `numpy.ma.dumps`
  * `numpy.ma.load`, `numpy.ma.dump` - these functions already failed on
    python 3 when called with a string.

* Multidimensional indexing with anything but a tuple is deprecated. This means
  that the index list in ``ind = [slice(None), 0]; arr[ind]`` should be changed
  to a tuple, e.g., ``ind = [slice(None), 0]; arr[tuple(ind)]`` or
  ``arr[(slice(None), 0)]``. That change is necessary to avoid ambiguity in
  expressions such as ``arr[[[0, 1], [0, 1]]]``, currently interpreted as
  ``arr[array([0, 1]), array([0, 1])]``, that will be interpreted
  as ``arr[array([[0, 1], [0, 1]])]`` in the future.

* Imports from the following sub-modules are deprecated, they will be removed
  at some future date.

  * `numpy.testing.utils`
  * `numpy.testing.decorators`
  * `numpy.testing.nosetester`
  * `numpy.testing.noseclasses`
  * `numpy.core.umath_tests`

* Giving a generator to `numpy.sum` is now deprecated. This was undocumented
  behavior, but worked. Previously, it would calculate the sum of the generator
  expression.  In the future, it might return a different result. Use
  ``np.sum(np.from_iter(generator))`` or the built-in Python ``sum`` instead.

* Users of the C-API should call ``PyArrayResolveWriteBackIfCopy`` or
  ``PyArray_DiscardWritbackIfCopy`` on any array with the ``WRITEBACKIFCOPY``
  flag set, before deallocating the array. A deprecation warning will be
  emitted if those calls are not used when needed.

* Users of ``nditer`` should use the nditer object as a context manager
  anytime one of the iterator operands is writeable, so that numpy can
  manage writeback semantics, or should call ``it.close()``. A
  `RuntimeWarning` may be emitted otherwise in these cases.

* The ``normed`` argument of ``np.histogram``, deprecated long ago in 1.6.0,
  now emits a ``DeprecationWarning``.


Future Changes
==============

* NumPy 1.16 will drop support for Python 3.4.
* NumPy 1.17 will drop support for Python 2.7.


Compatibility notes
===================

Compiled testing modules renamed and made private
-------------------------------------------------
The following compiled modules have been renamed and made private:

* ``umath_tests`` -> ``_umath_tests``
* ``test_rational`` -> ``_rational_tests``
* ``multiarray_tests`` -> ``_multiarray_tests``
* ``struct_ufunc_test`` -> ``_struct_ufunc_tests``
* ``operand_flag_tests`` -> ``_operand_flag_tests``

The ``umath_tests`` module is still available for backwards compatibility, but
will be removed in the future.

The ``NpzFile`` returned by ``np.savez`` is now a ``collections.abc.Mapping``
-----------------------------------------------------------------------------
This means it behaves like a readonly dictionary, and has a new ``.values()``
method and ``len()`` implementation.

For python 3, this means that ``.iteritems()``, ``.iterkeys()`` have been
deprecated, and ``.keys()`` and ``.items()`` now return views and not lists.
This is consistent with how the builtin ``dict`` type changed between python 2
and python 3.

Under certain conditions, ``nditer`` must be used in a context manager
----------------------------------------------------------------------
When using an `numpy.nditer` with the ``"writeonly"`` or ``"readwrite"`` flags, there
are some circumstances where nditer doesn't actually give you a view of the
writable array. Instead, it gives you a copy, and if you make changes to the
copy, nditer later writes those changes back into your actual array. Currently,
this writeback occurs when the array objects are garbage collected, which makes
this API error-prone on CPython and entirely broken on PyPy. Therefore,
``nditer`` should now be used as a context manager whenever it is used
with writeable arrays, e.g., ``with np.nditer(...) as it: ...``. You may also
explicitly call ``it.close()`` for cases where a context manager is unusable,
for instance in generator expressions.

Numpy has switched to using pytest instead of nose for testing
--------------------------------------------------------------
The last nose release was 1.3.7 in June, 2015, and development of that tool has
ended, consequently NumPy has now switched to using pytest. The old decorators
and nose tools that were previously used by some downstream projects remain
available, but will not be maintained. The standard testing utilities,
``assert_almost_equal`` and such, are not be affected by this change except for
the nose specific functions ``import_nose`` and ``raises``. Those functions are
not used in numpy, but are kept for downstream compatibility.

Numpy no longer monkey-patches ``ctypes`` with ``__array_interface__``
----------------------------------------------------------------------
Previously numpy added ``__array_interface__`` attributes to all the integer
types from ``ctypes``.

``np.ma.notmasked_contiguous`` and ``np.ma.flatnotmasked_contiguous`` always return lists
-----------------------------------------------------------------------------------------
This is the documented behavior, but previously the result could be any of
slice, None, or list.

All downstream users seem to check for the ``None`` result from
``flatnotmasked_contiguous`` and replace it with ``[]``.  Those callers will
continue to work as before.

``np.squeeze`` restores old behavior of objects that cannot handle an ``axis`` argument
---------------------------------------------------------------------------------------
Prior to version ``1.7.0``, `numpy.squeeze` did not have an ``axis`` argument and
all empty axes were removed by default. The incorporation of an ``axis``
argument made it possible to selectively squeeze single or multiple empty axes,
but the old API expectation was not respected because axes could still be
selectively removed (silent success) from an object expecting all empty axes to
be removed. That silent, selective removal of empty axes for objects expecting
the old behavior has been fixed and the old behavior restored.

unstructured void array's ``.item`` method now returns a bytes object
---------------------------------------------------------------------
``.item`` now returns a ``bytes`` object instead of a buffer or byte array.
This may affect code which assumed the return value was mutable, which is no
longer the case.

``copy.copy`` and ``copy.deepcopy`` no longer turn ``masked`` into an array
---------------------------------------------------------------------------
Since ``np.ma.masked`` is a readonly scalar, copying should be a no-op. These
functions now behave consistently with ``np.copy()``.

Multifield Indexing of Structured Arrays will still return a copy
-----------------------------------------------------------------
The change that multi-field indexing of structured arrays returns a view
instead of a copy is pushed back to 1.16. A new method
``numpy.lib.recfunctions.repack_fields`` has been introduced to help mitigate
the effects of this change, which can be used to write code compatible with
both numpy 1.15 and 1.16. For more information on how to update code to account
for this future change see the "accessing multiple fields" section of the
`user guide <https://docs.scipy.org/doc/numpy/user/basics.rec.html>`__.


C API changes
=============

New functions ``npy_get_floatstatus_barrier`` and ``npy_clear_floatstatus_barrier``
-----------------------------------------------------------------------------------
Functions ``npy_get_floatstatus_barrier`` and ``npy_clear_floatstatus_barrier``
have been added and should be used in place of the ``npy_get_floatstatus``and
``npy_clear_status`` functions. Optimizing compilers like GCC 8.1 and Clang
were rearranging the order of operations when the previous functions were used
in the ufunc SIMD functions, resulting in the floatstatus flags being checked
before the operation whose status we wanted to check was run.  See `#10339
<https://github.com/numpy/numpy/issues/10370>`__.

Changes to ``PyArray_GetDTypeTransferFunction``
-----------------------------------------------
``PyArray_GetDTypeTransferFunction`` now defaults to using user-defined
``copyswapn`` / ``copyswap`` for user-defined dtypes. If this causes a
significant performance hit, consider implementing ``copyswapn`` to reflect the
implementation of ``PyArray_GetStridedCopyFn``.  See `#10898
<https://github.com/numpy/numpy/pull/10898>`__.


New Features
============

``np.gcd`` and ``np.lcm`` ufuncs added for integer and objects types
--------------------------------------------------------------------
These compute the greatest common divisor, and lowest common multiple,
respectively. These work on all the numpy integer types, as well as the
builtin arbitrary-precision ``Decimal`` and ``long`` types.

Support for cross-platform builds for iOS
-----------------------------------------
The build system has been modified to add support for the
``_PYTHON_HOST_PLATFORM`` environment variable, used by ``distutils`` when
compiling on one platform for another platform. This makes it possible to
compile NumPy for iOS targets.

This only enables you to compile NumPy for one specific platform at a time.
Creating a full iOS-compatible NumPy package requires building for the 5
architectures supported by iOS (i386, x86_64, armv7, armv7s and arm64), and
combining these 5 compiled builds products into a single "fat" binary.

``return_indices`` keyword added for ``np.intersect1d``
-------------------------------------------------------
New keyword ``return_indices`` returns the indices of the two input arrays
that correspond to the common elements.

``np.quantile`` and ``np.nanquantile``
--------------------------------------
Like ``np.percentile`` and ``np.nanpercentile``, but takes quantiles in [0, 1]
rather than percentiles in [0, 100]. ``np.percentile`` is now a thin wrapper
around ``np.quantile`` with the extra step of dividing by 100.


Build system
------------
Added experimental support for the 64-bit RISC-V architecture.


Improvements
============

``np.einsum`` updates
---------------------
Syncs einsum path optimization tech between `numpy` and `opt_einsum`. In
particular, the `greedy` path has received many enhancements by @jcmgray. A
full list of issues fixed are:

* Arbitrary memory can be passed into the `greedy` path. Fixes gh-11210.
* The greedy path has been updated to contain more dynamic programming ideas
  preventing a large number of duplicate (and expensive) calls that figure out
  the actual pair contraction that takes place. Now takes a few seconds on
  several hundred input tensors. Useful for matrix product state theories.
* Reworks the broadcasting dot error catching found in gh-11218 gh-10352 to be
  a bit earlier in the process.
* Enhances the `can_dot` functionality that previous missed an edge case (part
  of gh-11308).

``np.ufunc.reduce`` and related functions now accept an initial value
---------------------------------------------------------------------
``np.ufunc.reduce``, ``np.sum``, ``np.prod``, ``np.min`` and ``np.max`` all
now accept an ``initial`` keyword argument that specifies the value to start
the reduction with.

``np.flip`` can operate over multiple axes
------------------------------------------
``np.flip`` now accepts None, or tuples of int, in its ``axis`` argument. If
axis is None, it will flip over all the axes.

``histogram`` and ``histogramdd`` functions have moved to ``np.lib.histograms``
-------------------------------------------------------------------------------
These were originally found in ``np.lib.function_base``. They are still
available under their un-scoped ``np.histogram(dd)`` names, and
to maintain compatibility, aliased at ``np.lib.function_base.histogram(dd)``.

Code that does ``from np.lib.function_base import *`` will need to be updated
with the new location, and should consider not using ``import *`` in future.

``histogram`` will accept NaN values when explicit bins are given
-----------------------------------------------------------------
Previously it would fail when trying to compute a finite range for the data.
Since the range is ignored anyway when the bins are given explicitly, this error
was needless.

Note that calling ``histogram`` on NaN values continues to raise the
``RuntimeWarning`` s typical of working with nan values, which can be silenced
as usual with ``errstate``.

``histogram`` works on datetime types, when explicit bin edges are given
------------------------------------------------------------------------
Dates, times, and timedeltas can now be histogrammed. The bin edges must be
passed explicitly, and are not yet computed automatically.

``histogram`` "auto" estimator handles limited variance better
--------------------------------------------------------------
No longer does an IQR of 0 result in ``n_bins=1``, rather the number of bins
chosen is related to the data size in this situation.

The edges returned by `histogram`` and ``histogramdd`` now match the data float type
------------------------------------------------------------------------------------
When passed ``np.float16``, ``np.float32``, or ``np.longdouble`` data, the
returned edges are now of the same dtype. Previously, ``histogram`` would only
return the same type if explicit bins were given, and ``histogram`` would
produce ``float64`` bins no matter what the inputs.

``histogramdd`` allows explicit ranges to be given in a subset of axes
----------------------------------------------------------------------
The ``range`` argument of `numpy.histogramdd` can now contain ``None`` values to
indicate that the range for the corresponding axis should be computed from the
data. Previously, this could not be specified on a per-axis basis.

The normed arguments of ``histogramdd`` and ``histogram2d`` have been renamed
-----------------------------------------------------------------------------
These arguments are now called ``density``, which is consistent with
``histogram``. The old argument continues to work, but the new name should be
preferred.

``np.r_`` works with 0d arrays, and ``np.ma.mr_`` works with ``np.ma.masked``
-----------------------------------------------------------------------------
0d arrays passed to the `r_` and `mr_` concatenation helpers are now treated as
though they are arrays of length 1. Previously, passing these was an error.
As a result, `numpy.ma.mr_` now works correctly on the ``masked`` constant.

``np.ptp`` accepts a ``keepdims`` argument, and extended axis tuples
--------------------------------------------------------------------
``np.ptp`` (peak-to-peak) can now work over multiple axes, just like ``np.max``
and ``np.min``.

``MaskedArray.astype`` now is identical to ``ndarray.astype``
-------------------------------------------------------------
This means it takes all the same arguments, making more code written for
ndarray work for masked array too.

Enable AVX2/AVX512 at compile time
----------------------------------
Change to simd.inc.src to allow use of AVX2 or AVX512 at compile time. Previously
compilation for avx2 (or 512) with -march=native would still use the SSE
code for the simd functions even when the rest of the code got AVX2.

``nan_to_num`` always returns scalars when receiving scalar or 0d inputs
------------------------------------------------------------------------
Previously an array was returned for integer scalar inputs, which is
inconsistent with the behavior for float inputs, and that of ufuncs in general.
For all types of scalar or 0d input, the result is now a scalar.

``np.flatnonzero`` works on numpy-convertible types
---------------------------------------------------
``np.flatnonzero`` now uses ``np.ravel(a)`` instead of ``a.ravel()``, so it
works for lists, tuples, etc.

``np.interp`` returns numpy scalars rather than builtin scalars
---------------------------------------------------------------
Previously ``np.interp(0.5, [0, 1], [10, 20])`` would return a ``float``, but
now it returns a ``np.float64`` object, which more closely matches the behavior
of other functions.

Additionally, the special case of ``np.interp(object_array_0d, ...)`` is no
longer supported, as ``np.interp(object_array_nd)`` was never supported anyway.

As a result of this change, the ``period`` argument can now be used on 0d
arrays.

Allow dtype field names to be unicode in Python 2
-------------------------------------------------
Previously ``np.dtype([(u'name', float)])`` would raise a ``TypeError`` in
Python 2, as only bytestrings were allowed in field names. Now any unicode
string field names will be encoded with the ``ascii`` codec, raising a
``UnicodeEncodeError`` upon failure.

This change makes it easier to write Python 2/3 compatible code using
``from __future__ import unicode_literals``, which previously would cause
string literal field names to raise a TypeError in Python 2.

Comparison ufuncs accept ``dtype=object``, overriding the default ``bool``
--------------------------------------------------------------------------
This allows object arrays of symbolic types, which override ``==`` and other
operators to return expressions, to be compared elementwise with
``np.equal(a, b, dtype=object)``.

``sort`` functions accept ``kind='stable'``
-------------------------------------------
Up until now, to perform a stable sort on the data, the user must do:

    >>> np.sort([5, 2, 6, 2, 1], kind='mergesort')
    [1, 2, 2, 5, 6]

because merge sort is the only stable sorting algorithm available in
NumPy. However, having kind='mergesort' does not make it explicit that
the user wants to perform a stable sort thus harming the readability.

This change allows the user to specify kind='stable' thus clarifying
the intent.

Do not make temporary copies for in-place accumulation
------------------------------------------------------
When ufuncs perform accumulation they no longer make temporary copies because
of the overlap between input an output, that is, the next element accumulated
is added before the accumulated result is stored in its place, hence the
overlap is safe. Avoiding the copy results in faster execution.

``linalg.matrix_power`` can now handle stacks of matrices
---------------------------------------------------------
Like other functions in ``linalg``, ``matrix_power`` can now deal with arrays
of dimension larger than 2, which are treated as stacks of matrices. As part
of the change, to further improve consistency, the name of the first argument
has been changed to ``a`` (from ``M``), and the exceptions for non-square
matrices have been changed to ``LinAlgError`` (from ``ValueError``).

Increased performance in ``random.permutation`` for multidimensional arrays
---------------------------------------------------------------------------
``permutation`` uses the fast path in ``random.shuffle`` for all input
array dimensions.  Previously the fast path was only used for 1-d arrays.

Generalized ufuncs now accept ``axes``, ``axis`` and ``keepdims`` arguments
---------------------------------------------------------------------------
One can control over which axes a generalized ufunc operates by passing in an
``axes`` argument, a list of tuples with indices of particular axes.  For
instance, for a signature of ``(i,j),(j,k)->(i,k)`` appropriate for matrix
multiplication, the base elements are two-dimensional matrices and these are
taken to be stored in the two last axes of each argument.  The corresponding
axes keyword would be ``[(-2, -1), (-2, -1), (-2, -1)]``. If one wanted to
use leading dimensions instead, one would pass in ``[(0, 1), (0, 1), (0, 1)]``.

For simplicity, for generalized ufuncs that operate on 1-dimensional arrays
(vectors), a single integer is accepted instead of a single-element tuple, and
for generalized ufuncs for which all outputs are scalars, the (empty) output
tuples can be omitted.  Hence, for a signature of ``(i),(i)->()`` appropriate
for an inner product, one could pass in ``axes=[0, 0]`` to indicate that the
vectors are stored in the first dimensions of the two inputs arguments.

As a short-cut for generalized ufuncs that are similar to reductions, i.e.,
that act on a single, shared core dimension such as the inner product example
above, one can pass an ``axis`` argument. This is equivalent to passing in
``axes`` with identical entries for all arguments with that core dimension
(e.g., for the example above, ``axes=[(axis,), (axis,)]``).

Furthermore, like for reductions, for generalized ufuncs that have inputs that
all have the same number of core dimensions and outputs with no core dimension,
one can pass in ``keepdims`` to leave a dimension with size 1 in the outputs,
thus allowing proper broadcasting against the original inputs. The location of
the extra dimension can be controlled with ``axes``. For instance, for the
inner-product example, ``keepdims=True, axes=[-2, -2, -2]`` would act on the
inner-product example, ``keepdims=True, axis=-2`` would act on the
one-but-last dimension of the input arguments, and leave a size 1 dimension in
that place in the output.

float128 values now print correctly on ppc systems
--------------------------------------------------
Previously printing float128 values was buggy on ppc, since the special
double-double floating-point-format on these systems was not accounted for.
float128s now print with correct rounding and uniqueness.

Warning to ppc users: You should upgrade glibc if it is version <=2.23,
especially if using float128. On ppc, glibc's malloc in these version often
misaligns allocated memory which can crash numpy when using float128 values.

New ``np.take_along_axis`` and ``np.put_along_axis`` functions
--------------------------------------------------------------
When used on multidimensional arrays, ``argsort``, ``argmin``, ``argmax``, and
``argpartition`` return arrays that are difficult to use as indices.
``take_along_axis`` provides an easy way to use these indices to lookup values
within an array, so that::

    np.take_along_axis(a, np.argsort(a, axis=axis), axis=axis)

is the same as::

    np.sort(a, axis=axis)

``np.put_along_axis`` acts as the dual operation for writing to these indices
within an array.

.. currentmodule:: numpy

==========================
NumPy 1.19.2 Release Notes
==========================

NumPy 1.19.2 fixes several bugs, prepares for the upcoming Cython 3.x release.
and pins setuptools to keep distutils working while upstream modifications are
ongoing. The aarch64 wheels are built with the latest manylinux2014 release
that fixes the problem of differing page sizes used by different linux distros.

This release supports Python 3.6-3.8. Cython >= 0.29.21 needs to be used when
building with Python 3.9 for testing purposes.

There is a known problem with Windows 10 version=2004 and OpenBLAS svd that we
are trying to debug. If you are running that Windows version you should use a
NumPy version that links to the MKL library, earlier Windows versions are fine.

Improvements
============

Add NumPy declarations for Cython 3.0 and later
-----------------------------------------------
The pxd declarations for Cython 3.0 were improved to avoid using deprecated
NumPy C-API features.  Extension modules built with Cython 3.0+ that use NumPy
can now set the C macro ``NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION`` to avoid
C compiler warnings about deprecated API usage.

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Pauli Virtanen
* Philippe Ombredanne +
* Sebastian Berg
* Stefan Behnel +
* Stephan Loyd +
* Zac Hatfield-Dodds

Pull requests merged
====================

A total of 9 pull requests were merged for this release.

* `#16959 <https://github.com/numpy/numpy/pull/16959>`__: TST: Change aarch64 to arm64 in travis.yml.
* `#16998 <https://github.com/numpy/numpy/pull/16998>`__: MAINT: Configure hypothesis in ``np.test()`` for determinism,...
* `#17000 <https://github.com/numpy/numpy/pull/17000>`__: BLD: pin setuptools < 49.2.0
* `#17015 <https://github.com/numpy/numpy/pull/17015>`__: ENH: Add NumPy declarations to be used by Cython 3.0+
* `#17125 <https://github.com/numpy/numpy/pull/17125>`__: BUG: Remove non-threadsafe sigint handling from fft calculation
* `#17243 <https://github.com/numpy/numpy/pull/17243>`__: BUG: core: fix ilp64 blas dot/vdot/... for strides > int32 max
* `#17244 <https://github.com/numpy/numpy/pull/17244>`__: DOC: Use SPDX license expressions with correct license
* `#17245 <https://github.com/numpy/numpy/pull/17245>`__: DOC: Fix the link to the quick-start in the old API functions
* `#17272 <https://github.com/numpy/numpy/pull/17272>`__: BUG: fix pickling of arrays larger than 2GiB
.. currentmodule:: numpy

==========================
NumPy 1.21.4 Release Notes
==========================

The NumPy 1.21.4 is a maintenance release that fixes a few bugs discovered
after 1.21.3. The most important fix here is a fix for the NumPy header files
to make them work for both x86_64 and M1 hardware when included in the Mac
universal2 wheels. Previously, the header files only worked for M1 and this
caused problems for folks building x86_64 extensions. This problem was not seen
before Python 3.10 because there were thin wheels for x86_64 that had
precedence. This release also provides thin x86_64 Mac wheels for Python 3.10.

The Python versions supported in this release are 3.7-3.10. If you want to
compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid
problems.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Isuru Fernando
* Matthew Brett
* Sayed Adel
* Sebastian Berg
* 傅立业（Chris Fu） +

Pull requests merged
====================

A total of 9 pull requests were merged for this release.

* `#20278 <https://github.com/numpy/numpy/pull/20278>`__: BUG: Fix shadowed reference of ``dtype`` in type stub
* `#20293 <https://github.com/numpy/numpy/pull/20293>`__: BUG: Fix headers for universal2 builds
* `#20294 <https://github.com/numpy/numpy/pull/20294>`__: BUG: ``VOID_nonzero`` could sometimes mutate alignment flag
* `#20295 <https://github.com/numpy/numpy/pull/20295>`__: BUG: Do not use nonzero fastpath on unaligned arrays
* `#20296 <https://github.com/numpy/numpy/pull/20296>`__: BUG: Distutils patch to allow for 2 as a minor version (!)
* `#20297 <https://github.com/numpy/numpy/pull/20297>`__: BUG, SIMD: Fix 64-bit/8-bit integer division by a scalar
* `#20298 <https://github.com/numpy/numpy/pull/20298>`__: BUG, SIMD: Workaround broadcasting SIMD 64-bit integers on MSVC...
* `#20300 <https://github.com/numpy/numpy/pull/20300>`__: REL: Prepare for the NumPy 1.21.4 release.
* `#20302 <https://github.com/numpy/numpy/pull/20302>`__: TST: Fix a ``Arrayterator`` typing test failure
==========================
NumPy 1.16.4 Release Notes
==========================

The NumPy 1.16.4 release fixes bugs reported against the 1.16.3 release, and
also backports several enhancements from master that seem appropriate for a
release series that is the last to support Python 2.7. The wheels on PyPI are
linked with OpenBLAS v0.3.7-dev, which should fix issues on Skylake series
cpus.

Downstream developers building this release should use Cython >= 0.29.2 and,
if using OpenBLAS, OpenBLAS > v0.3.7. The supported Python versions are 2.7 and
3.5-3.7.


New deprecations
================
Writeable flag of C-API wrapped arrays
--------------------------------------
When an array is created from the C-API to wrap a pointer to data, the only
indication we have of the read-write nature of the data is the ``writeable``
flag set during creation. It is dangerous to force the flag to writeable.  In
the future it will not be possible to switch the writeable flag to ``True``
from python.  This deprecation should not affect many users since arrays
created in such a manner are very rare in practice and only available through
the NumPy C-API.


Compatibility notes
===================

Potential changes to the random stream
--------------------------------------
Due to bugs in the application of log to random floating point numbers,
the stream may change when sampling from ``np.random.beta``, ``np.random.binomial``,
``np.random.laplace``, ``np.random.logistic``, ``np.random.logseries`` or
``np.random.multinomial`` if a 0 is generated in the underlying MT19937 random stream.
There is a 1 in :math:`10^{53}` chance of this occurring, and so the probability that
the stream changes for any given seed is extremely small. If a 0 is encountered in the
underlying generator, then the incorrect value produced (either ``np.inf``
or ``np.nan``) is now dropped.


Changes
=======

`numpy.lib.recfunctions.structured_to_unstructured` does not squeeze single-field views
---------------------------------------------------------------------------------------
Previously ``structured_to_unstructured(arr[['a']])`` would produce a squeezed
result inconsistent with ``structured_to_unstructured(arr[['a', b']])``. This
was accidental. The old behavior can be retained with
``structured_to_unstructured(arr[['a']]).squeeze(axis=-1)`` or far more simply,
``arr['a']``.


Contributors
============

A total of 10 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Dennis Zollo +
* Hunter Damron +
* Jingbei Li +
* Kevin Sheppard
* Matti Picus
* Nicola Soranzo +
* Sebastian Berg
* Tyler Reddy


Pull requests merged
====================

A total of 16 pull requests were merged for this release.

* `#13392 <https://github.com/numpy/numpy/pull/13392>`__: BUG: Some PyPy versions lack PyStructSequence_InitType2.
* `#13394 <https://github.com/numpy/numpy/pull/13394>`__: MAINT, DEP: Fix deprecated ``assertEquals()``
* `#13396 <https://github.com/numpy/numpy/pull/13396>`__: BUG: Fix structured_to_unstructured on single-field types (backport)
* `#13549 <https://github.com/numpy/numpy/pull/13549>`__: BLD: Make CI pass again with pytest 4.5
* `#13552 <https://github.com/numpy/numpy/pull/13552>`__: TST: Register markers in conftest.py.
* `#13559 <https://github.com/numpy/numpy/pull/13559>`__: BUG: Removes ValueError for empty kwargs in arraymultiter_new
* `#13560 <https://github.com/numpy/numpy/pull/13560>`__: BUG: Add TypeError to accepted exceptions in crackfortran.
* `#13561 <https://github.com/numpy/numpy/pull/13561>`__: BUG: Handle subarrays in descr_to_dtype
* `#13562 <https://github.com/numpy/numpy/pull/13562>`__: BUG: Protect generators from log(0.0)
* `#13563 <https://github.com/numpy/numpy/pull/13563>`__: BUG: Always return views from structured_to_unstructured when...
* `#13564 <https://github.com/numpy/numpy/pull/13564>`__: BUG: Catch stderr when checking compiler version
* `#13565 <https://github.com/numpy/numpy/pull/13565>`__: BUG: longdouble(int) does not work
* `#13587 <https://github.com/numpy/numpy/pull/13587>`__: BUG: distutils/system_info.py fix missing subprocess import (#13523)
* `#13620 <https://github.com/numpy/numpy/pull/13620>`__: BUG,DEP: Fix writeable flag setting for arrays without base
* `#13641 <https://github.com/numpy/numpy/pull/13641>`__: MAINT: Prepare for the 1.16.4 release.
* `#13644 <https://github.com/numpy/numpy/pull/13644>`__: BUG: special case object arrays when printing rel-, abs-error
==========================
NumPy 1.10.2 Release Notes
==========================

This release deals with a number of bugs that turned up in 1.10.1 and
adds various build and release improvements.

Numpy 1.10.1 supports Python 2.6 - 2.7 and 3.2 - 3.5.


Compatibility notes
===================

Relaxed stride checking is no longer the default
------------------------------------------------
There were back compatibility problems involving views changing the dtype of
multidimensional Fortran arrays that need to be dealt with over a longer
timeframe.

Fix swig bug in ``numpy.i``
---------------------------
Relaxed stride checking revealed a bug in ``array_is_fortran(a)``, that was
using PyArray_ISFORTRAN to check for Fortran contiguity instead of
PyArray_IS_F_CONTIGUOUS. You may want to regenerate swigged files using the
updated numpy.i

Deprecate views changing dimensions in fortran order
----------------------------------------------------
This deprecates assignment of a new descriptor to the dtype attribute of
a non-C-contiguous array if it result in changing the shape. This
effectively bars viewing a multidimensional Fortran array using a dtype
that changes the element size along the first axis.

The reason for the deprecation is that, when relaxed strides checking is
enabled, arrays that are both C and Fortran contiguous are always treated
as C contiguous which breaks some code that depended the two being mutually
exclusive for non-scalar arrays of ndim > 1. This deprecation prepares the
way to always enable relaxed stride checking.


Issues Fixed
============

* gh-6019 Masked array repr fails for structured array with multi-dimensional column.
* gh-6462 Median of empty array produces IndexError.
* gh-6467 Performance regression for record array access.
* gh-6468 numpy.interp uses 'left' value even when x[0]==xp[0].
* gh-6475 np.allclose returns a memmap when one of its arguments is a memmap.
* gh-6491 Error in broadcasting stride_tricks array.
* gh-6495 Unrecognized command line option '-ffpe-summary' in gfortran.
* gh-6497 Failure of reduce operation on recarrays.
* gh-6498 Mention change in default casting rule in 1.10 release notes.
* gh-6530 The partition function errors out on empty input.
* gh-6532 numpy.inner return wrong inaccurate value sometimes.
* gh-6563 Intent(out) broken in recent versions of f2py.
* gh-6569 Cannot run tests after 'python setup.py build_ext -i'
* gh-6572 Error in broadcasting stride_tricks array component.
* gh-6575 BUG: Split produces empty arrays with wrong number of dimensions
* gh-6590 Fortran Array problem in numpy 1.10.
* gh-6602 Random __all__ missing choice and dirichlet.
* gh-6611 ma.dot no longer always returns a masked array in 1.10.
* gh-6618 NPY_FORTRANORDER in make_fortran() in numpy.i
* gh-6636 Memory leak in nested dtypes in numpy.recarray
* gh-6641 Subsetting recarray by fields yields a structured array.
* gh-6667 ma.make_mask handles ma.nomask input incorrectly.
* gh-6675 Optimized blas detection broken in master and 1.10.
* gh-6678 Getting unexpected error from: X.dtype = complex (or Y = X.view(complex))
* gh-6718 f2py test fail in pip installed numpy-1.10.1 in virtualenv.
* gh-6719 Error compiling Cython file: Pythonic division not allowed without gil.
* gh-6771 Numpy.rec.fromarrays losing dtype metadata between versions 1.9.2 and 1.10.1
* gh-6781 The travis-ci script in maintenance/1.10.x needs fixing.
* gh-6807 Windows testing errors for 1.10.2


Merged PRs
==========

The following PRs have been merged into 1.10.2. When the PR is a backport,
the PR number for the original PR against master is listed.

* gh-5773 MAINT: Hide testing helper tracebacks when using them with pytest.
* gh-6094 BUG: Fixed a bug with string representation of masked structured arrays.
* gh-6208 MAINT: Speedup field access by removing unneeded safety checks.
* gh-6460 BUG: Replacing the os.environ.clear by less invasive procedure.
* gh-6470 BUG: Fix AttributeError in numpy distutils.
* gh-6472 MAINT: Use Python 3.5 instead of 3.5-dev for travis 3.5 testing.
* gh-6474 REL: Update Paver script for sdist and auto-switch test warnings.
* gh-6478 BUG: Fix Intel compiler flags for OS X build.
* gh-6481 MAINT: LIBPATH with spaces is now supported Python 2.7+ and Win32.
* gh-6487 BUG: Allow nested use of parameters in definition of arrays in f2py.
* gh-6488 BUG: Extend common blocks rather than overwriting in f2py.
* gh-6499 DOC: Mention that default casting for inplace operations has changed.
* gh-6500 BUG: Recarrays viewed as subarrays don't convert to np.record type.
* gh-6501 REL: Add "make upload" command for built docs, update "make dist".
* gh-6526 BUG: Fix use of __doc__ in setup.py for -OO mode.
* gh-6527 BUG: Fix the IndexError when taking the median of an empty array.
* gh-6537 BUG: Make ma.atleast_* with scalar argument return arrays.
* gh-6538 BUG: Fix ma.masked_values does not shrink mask if requested.
* gh-6546 BUG: Fix inner product regression for non-contiguous arrays.
* gh-6553 BUG: Fix partition and argpartition error for empty input.
* gh-6556 BUG: Error in broadcast_arrays with as_strided array.
* gh-6558 MAINT: Minor update to "make upload" doc build command.
* gh-6562 BUG: Disable view safety checks in recarray.
* gh-6567 BUG: Revert some import * fixes in f2py.
* gh-6574 DOC: Release notes for Numpy 1.10.2.
* gh-6577 BUG: Fix for #6569, allowing build_ext --inplace
* gh-6579 MAINT: Fix mistake in doc upload rule.
* gh-6596 BUG: Fix swig for relaxed stride checking.
* gh-6606 DOC: Update 1.10.2 release notes.
* gh-6614 BUG: Add choice and dirichlet to numpy.random.__all__.
* gh-6621 BUG: Fix swig make_fortran function.
* gh-6628 BUG: Make allclose return python bool.
* gh-6642 BUG: Fix memleak in _convert_from_dict.
* gh-6643 ENH: make recarray.getitem return a recarray.
* gh-6653 BUG: Fix ma dot to always return masked array.
* gh-6668 BUG: ma.make_mask should always return nomask for nomask argument.
* gh-6686 BUG: Fix a bug in assert_string_equal.
* gh-6695 BUG: Fix removing tempdirs created during build.
* gh-6697 MAINT: Fix spurious semicolon in macro definition of PyArray_FROM_OT.
* gh-6698 TST: test np.rint bug for large integers.
* gh-6717 BUG: Readd fallback CBLAS detection on linux.
* gh-6721 BUG: Fix for #6719.
* gh-6726 BUG: Fix bugs exposed by relaxed stride rollback.
* gh-6757 BUG: link cblas library if cblas is detected.
* gh-6756 TST: only test f2py, not f2py2.7 etc, fixes #6718.
* gh-6747 DEP: Deprecate changing shape of non-C-contiguous array via descr.
* gh-6775 MAINT: Include from __future__ boilerplate in some files missing it.
* gh-6780 BUG: metadata is not copied to base_dtype.
* gh-6783 BUG: Fix travis ci testing for new google infrastructure.
* gh-6785 BUG: Quick and dirty fix for interp.
* gh-6813 TST,BUG: Make test_mvoid_multidim_print work for 32 bit systems.
* gh-6817 BUG: Disable 32-bit msvc9 compiler optimizations for npy_rint.
* gh-6819 TST: Fix test_mvoid_multidim_print failures on Python 2.x for Windows.

Initial support for mingwpy was reverted as it was causing problems for
non-windows builds.

* gh-6536 BUG: Revert gh-5614 to fix non-windows build problems

A fix for np.lib.split was reverted because it resulted in "fixing"
behavior that will be present in the Numpy 1.11 and that was already
present in Numpy 1.9. See the discussion of the issue at gh-6575 for
clarification.

* gh-6576 BUG: Revert gh-6376 to fix split behavior for empty arrays.

Relaxed stride checking was reverted. There were back compatibility
problems involving views changing the dtype of multidimensional Fortran
arrays that need to be dealt with over a longer timeframe.

* gh-6735 MAINT: Make no relaxed stride checking the default for 1.10.


Notes
=====
A bug in the Numpy 1.10.1 release resulted in exceptions being raised for
``RuntimeWarning`` and ``DeprecationWarning`` in projects depending on Numpy.
That has been fixed.
==========================
NumPy 1.14.4 Release Notes
==========================

This is a bugfix release for bugs reported following the 1.14.3 release. The
most significant fixes are:

* fixes for compiler instruction reordering that resulted in NaN's not being
  properly propagated in `np.max` and `np.min`,

* fixes for bus faults on SPARC and older ARM due to incorrect alignment
  checks.

There are also improvements to printing of long doubles on PPC platforms. All
is not yet perfect on that platform, the whitespace padding is still incorrect
and is to be fixed in numpy 1.15, consequently NumPy still fails some
printing-related (and other) unit tests on ppc systems. However, the printed
values are now correct.

Note that NumPy will error on import if it detects incorrect float32 `dot`
results. This problem has been seen on the Mac when working in the Anaconda
environment and is due to a subtle interaction between MKL and PyQt5.  It is not
strictly a NumPy problem, but it is best that users be aware of it.  See the
gh-8577 NumPy issue for more information.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python
3.6 wheels available from PIP are built with Python 3.6.2 and should be
compatible with all previous versions of Python 3.6. The source releases were
cythonized with Cython 0.28.2 and should work for the upcoming Python 3.7.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Marten van Kerkwijk
* Matti Picus
* Pauli Virtanen
* Ryan Soklaski +
* Sebastian Berg

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#11104 <https://github.com/numpy/numpy/pull/11104>`__: BUG: str of DOUBLE_DOUBLE format wrong on ppc64
* `#11170 <https://github.com/numpy/numpy/pull/11170>`__: TST: linalg: add regression test for gh-8577
* `#11174 <https://github.com/numpy/numpy/pull/11174>`__: MAINT: add sanity-checks to be run at import time
* `#11181 <https://github.com/numpy/numpy/pull/11181>`__: BUG: void dtype setup checked offset not actual pointer for alignment
* `#11194 <https://github.com/numpy/numpy/pull/11194>`__: BUG: Python2 doubles don't print correctly in interactive shell.
* `#11198 <https://github.com/numpy/numpy/pull/11198>`__: BUG: optimizing compilers can reorder call to npy_get_floatstatus
* `#11199 <https://github.com/numpy/numpy/pull/11199>`__: BUG: reduce using SSE only warns if inside SSE loop
* `#11203 <https://github.com/numpy/numpy/pull/11203>`__: BUG: Bytes delimiter/comments in genfromtxt should be decoded
* `#11211 <https://github.com/numpy/numpy/pull/11211>`__: BUG: Fix reference count/memory leak exposed by better testing
* `#11219 <https://github.com/numpy/numpy/pull/11219>`__: BUG: Fixes einsum broadcasting bug when optimize=True
* `#11251 <https://github.com/numpy/numpy/pull/11251>`__: DOC: Document 1.14.4 release.
==========================
NumPy 1.11.0 Release Notes
==========================

This release supports Python 2.6 - 2.7 and 3.2 - 3.5 and contains a number
of enhancements and improvements. Note also the build system changes listed
below as they may have subtle effects.

No Windows (TM) binaries are provided for this release due to a broken
toolchain. One of the providers of Python packages for Windows (TM) is your
best bet.


Highlights
==========

Details of these improvements can be found below.

* The datetime64 type is now timezone naive.
* A dtype parameter has been added to ``randint``.
* Improved detection of two arrays possibly sharing memory.
* Automatic bin size estimation for ``np.histogram``.
* Speed optimization of A @ A.T and dot(A, A.T).
* New function ``np.moveaxis`` for reordering array axes.


Build System Changes
====================

* Numpy now uses ``setuptools`` for its builds instead of plain distutils.
  This fixes usage of ``install_requires='numpy'`` in the ``setup.py`` files of
  projects that depend on Numpy (see gh-6551).  It potentially affects the way
  that build/install methods for Numpy itself behave though.  Please report any
  unexpected behavior on the Numpy issue tracker.
* Bento build support and related files have been removed.
* Single file build support and related files have been removed.


Future Changes
==============

The following changes are scheduled for Numpy 1.12.0.

* Support for Python 2.6, 3.2, and 3.3 will be dropped.
* Relaxed stride checking will become the default. See the 1.8.0 release
  notes for a more extended discussion of what this change implies.
* The behavior of the datetime64 "not a time" (NaT) value will be changed
  to match that of floating point "not a number" (NaN) values: all
  comparisons involving NaT will return False, except for NaT != NaT which
  will return True.
* Indexing with floats will raise IndexError,
  e.g., a[0, 0.0].
* Indexing with non-integer array_like will raise ``IndexError``,
  e.g., ``a['1', '2']``
* Indexing with multiple ellipsis will raise ``IndexError``,
  e.g., ``a[..., ...]``.
* Non-integers used as index values will raise ``TypeError``,
  e.g., in ``reshape``, ``take``, and specifying reduce axis.


In a future release the following changes will be made.

* The ``rand`` function exposed in ``numpy.testing`` will be removed. That
  function is left over from early Numpy and was implemented using the
  Python random module.  The random number generators from ``numpy.random``
  should be used instead.
* The ``ndarray.view`` method will only allow c_contiguous arrays to be
  viewed using a dtype of different size causing the last dimension to
  change.  That differs from the current behavior where arrays that are
  f_contiguous but not c_contiguous can be viewed as a dtype type of
  different size causing the first dimension to change.
* Slicing a ``MaskedArray`` will return views of both data **and** mask.
  Currently the mask is copy-on-write and changes to the mask in the slice do
  not propagate to the original mask. See the FutureWarnings section below for
  details.


Compatibility notes
===================

datetime64 changes
------------------
In prior versions of NumPy the experimental datetime64 type always stored
times in UTC. By default, creating a datetime64 object from a string or
printing it would convert from or to local time::

    # old behavior
    >>> np.datetime64('2000-01-01T00:00:00')
    numpy.datetime64('2000-01-01T00:00:00-0800')  # note the timezone offset -08:00


A consensus of datetime64 users agreed that this behavior is undesirable
and at odds with how datetime64 is usually used (e.g., by `pandas
<http://pandas.pydata.org>`__). For most use cases, a timezone naive datetime
type is preferred, similar to the ``datetime.datetime`` type in the Python
standard library. Accordingly, datetime64 no longer assumes that input is in
local time, nor does it print local times::

    >>> np.datetime64('2000-01-01T00:00:00')
    numpy.datetime64('2000-01-01T00:00:00')

For backwards compatibility, datetime64 still parses timezone offsets, which
it handles by converting to UTC. However, the resulting datetime is timezone
naive::

    >>> np.datetime64('2000-01-01T00:00:00-08')
    DeprecationWarning: parsing timezone aware datetimes is deprecated;
    this will raise an error in the future
    numpy.datetime64('2000-01-01T08:00:00')

As a corollary to this change, we no longer prohibit casting between datetimes
with date units and datetimes with time units. With timezone naive datetimes,
the rule for casting from dates to times is no longer ambiguous.

``linalg.norm`` return type changes
-----------------------------------
The return type of the ``linalg.norm`` function is now floating point without
exception.  Some of the norm types previously returned integers.

polynomial fit changes
----------------------
The various fit functions in the numpy polynomial package no longer accept
non-integers for degree specification.

*np.dot* now raises ``TypeError`` instead of ``ValueError``
-----------------------------------------------------------
This behaviour mimics that of other functions such as ``np.inner``. If the two
arguments cannot be cast to a common type, it could have raised a ``TypeError``
or ``ValueError`` depending on their order. Now, ``np.dot`` will now always
raise a ``TypeError``.

FutureWarning to changed behavior
---------------------------------

* In ``np.lib.split`` an empty array in the result always had dimension
  ``(0,)`` no matter the dimensions of the array being split. This
  has been changed so that the dimensions will be preserved. A
  ``FutureWarning`` for this change has been in place since Numpy 1.9 but,
  due to a bug, sometimes no warning was raised and the dimensions were
  already preserved.

``%`` and ``//`` operators
--------------------------
These operators are implemented with the ``remainder`` and ``floor_divide``
functions respectively. Those functions are now based around ``fmod`` and are
computed together so as to be compatible with each other and with the Python
versions for float types.  The results should be marginally more accurate or
outright bug fixes compared to the previous results, but they may
differ significantly in cases where roundoff makes a difference in the integer
returned by ``floor_divide``. Some corner cases also change, for instance, NaN
is always returned for both functions when the divisor is zero,
``divmod(1.0, inf)`` returns ``(0.0, 1.0)`` except on MSVC 2008, and
``divmod(-1.0, inf)`` returns ``(-1.0, inf)``.

C API
-----

Removed the ``check_return`` and ``inner_loop_selector`` members of
the ``PyUFuncObject`` struct (replacing them with ``reserved`` slots
to preserve struct layout). These were never used for anything, so
it's unlikely that any third-party code is using them either, but we
mention it here for completeness.


object dtype detection for old-style classes
--------------------------------------------

In python 2, objects which are instances of old-style user-defined classes no
longer automatically count as 'object' type in the dtype-detection handler.
Instead, as in python 3, they may potentially count as sequences, but only if
they define both a `__len__` and a `__getitem__` method. This fixes a segfault
and inconsistency between python 2 and 3.

New Features
============

* ``np.histogram`` now provides plugin estimators for automatically
  estimating the optimal number of bins. Passing one of ['auto', 'fd',
  'scott', 'rice', 'sturges'] as the argument to 'bins' results in the
  corresponding estimator being used.

* A benchmark suite using `Airspeed Velocity
  <https://asv.readthedocs.io/>`__ has been added, converting the
  previous vbench-based one. You can run the suite locally via ``python
  runtests.py --bench``. For more details, see ``benchmarks/README.rst``.

* A new function ``np.shares_memory`` that can check exactly whether two
  arrays have memory overlap is added. ``np.may_share_memory`` also now has
  an option to spend more effort to reduce false positives.

* ``SkipTest`` and ``KnownFailureException`` exception classes are exposed
  in the ``numpy.testing`` namespace. Raise them in a test function to mark
  the test to be skipped or mark it as a known failure, respectively.

* ``f2py.compile`` has a new ``extension`` keyword parameter that allows the
  fortran extension to be specified for generated temp files. For instance,
  the files can be specifies to be ``*.f90``. The ``verbose`` argument is
  also activated, it was previously ignored.

* A ``dtype`` parameter has been added to ``np.random.randint``
  Random ndarrays of the following types can now be generated:

  - ``np.bool_``,
  - ``np.int8``, ``np.uint8``,
  - ``np.int16``, ``np.uint16``,
  - ``np.int32``, ``np.uint32``,
  - ``np.int64``, ``np.uint64``,
  - ``np.int_ ``, ``np.intp``

  The specification is by precision rather than by C type. Hence, on some
  platforms ``np.int64`` may be a ``long`` instead of ``long long`` even if
  the specified dtype is ``long long`` because the two may have the same
  precision. The resulting type depends on which C type numpy uses for the
  given precision. The byteorder specification is also ignored, the
  generated arrays are always in native byte order.

* A new ``np.moveaxis`` function allows for moving one or more array axes
  to a new position by explicitly providing source and destination axes.
  This function should be easier to use than the current ``rollaxis``
  function as well as providing more functionality.

* The ``deg`` parameter of the various ``numpy.polynomial`` fits has been
  extended to accept a list of the degrees of the terms to be included in
  the fit, the coefficients of all other terms being constrained to zero.
  The change is backward compatible, passing a scalar ``deg`` will behave
  as before.

* A divmod function for float types modeled after the Python version has
  been added to the npy_math library.


Improvements
============

``np.gradient`` now supports an ``axis`` argument
-------------------------------------------------
The ``axis`` parameter was added to ``np.gradient`` for consistency.  It
allows to specify over which axes the gradient is calculated.

``np.lexsort`` now supports arrays with object data-type
--------------------------------------------------------
The function now internally calls the generic ``npy_amergesort`` when the
type does not implement a merge-sort kind of ``argsort`` method.

``np.ma.core.MaskedArray`` now supports an ``order`` argument
-------------------------------------------------------------
When constructing a new ``MaskedArray`` instance, it can be configured with
an ``order`` argument analogous to the one when calling ``np.ndarray``. The
addition of this argument allows for the proper processing of an ``order``
argument in several MaskedArray-related utility functions such as
``np.ma.core.array`` and ``np.ma.core.asarray``.

Memory and speed improvements for masked arrays
-----------------------------------------------
Creating a masked array with ``mask=True`` (resp. ``mask=False``) now uses
``np.ones`` (resp. ``np.zeros``) to create the mask, which is faster and
avoid a big memory peak. Another optimization was done to avoid a memory
peak and useless computations when printing a masked array.

``ndarray.tofile`` now uses fallocate on linux
----------------------------------------------
The function now uses the fallocate system call to reserve sufficient
disk space on file systems that support it.

Optimizations for operations of the form ``A.T @ A`` and ``A @ A.T``
--------------------------------------------------------------------
Previously, ``gemm`` BLAS operations were used for all matrix products. Now,
if the matrix product is between a matrix and its transpose, it will use
``syrk`` BLAS operations for a performance boost. This optimization has been
extended to ``@``, ``numpy.dot``, ``numpy.inner``, and ``numpy.matmul``.

**Note:** Requires the transposed and non-transposed matrices to share data.

``np.testing.assert_warns`` can now be used as a context manager
----------------------------------------------------------------
This matches the behavior of ``assert_raises``.

Speed improvement for np.random.shuffle
---------------------------------------
``np.random.shuffle`` is now much faster for 1d ndarrays.


Changes
=======

Pyrex support was removed from ``numpy.distutils``
--------------------------------------------------
The method ``build_src.generate_a_pyrex_source`` will remain available; it
has been monkeypatched by users to support Cython instead of Pyrex.  It's
recommended to switch to a better supported method of build Cython
extensions though.

``np.broadcast`` can now be called with a single argument
---------------------------------------------------------
The resulting object in that case will simply mimic iteration over
a single array. This change obsoletes distinctions like

    if len(x) == 1:
        shape = x[0].shape
    else:
        shape = np.broadcast(\*x).shape

Instead, ``np.broadcast`` can be used in all cases.

``np.trace`` now respects array subclasses
------------------------------------------
This behaviour mimics that of other functions such as ``np.diagonal`` and
ensures, e.g., that for masked arrays ``np.trace(ma)`` and ``ma.trace()`` give
the same result.

``np.dot`` now raises ``TypeError`` instead of ``ValueError``
-------------------------------------------------------------
This behaviour mimics that of other functions such as ``np.inner``. If the two
arguments cannot be cast to a common type, it could have raised a ``TypeError``
or ``ValueError`` depending on their order. Now, ``np.dot`` will now always
raise a ``TypeError``.

``linalg.norm`` return type changes
-----------------------------------
The ``linalg.norm`` function now does all its computations in floating point
and returns floating results. This change fixes bugs due to integer overflow
and the failure of abs with signed integers of minimum value, e.g., int8(-128).
For consistency, floats are used even where an integer might work.


Deprecations
============

Views of arrays in Fortran order
--------------------------------
The F_CONTIGUOUS flag was used to signal that views using a dtype that
changed the element size would change the first index. This was always
problematical for arrays that were both F_CONTIGUOUS and C_CONTIGUOUS
because C_CONTIGUOUS took precedence. Relaxed stride checking results in
more such dual contiguous arrays and breaks some existing code as a result.
Note that this also affects changing the dtype by assigning to the dtype
attribute of an array. The aim of this deprecation is to restrict views to
C_CONTIGUOUS arrays at some future time. A work around that is backward
compatible is to use ``a.T.view(...).T`` instead. A parameter may also be
added to the view method to explicitly ask for Fortran order views, but
that will not be backward compatible.

Invalid arguments for array ordering
------------------------------------
It is currently possible to pass in arguments for the ``order``
parameter in methods like ``array.flatten`` or ``array.ravel``
that were not one of the following: 'C', 'F', 'A', 'K' (note that
all of these possible values are both unicode and case insensitive).
Such behavior will not be allowed in future releases.

Random number generator in the ``testing`` namespace
----------------------------------------------------
The Python standard library random number generator was previously exposed
in the ``testing`` namespace as ``testing.rand``. Using this generator is
not recommended and it will be removed in a future release. Use generators
from ``numpy.random`` namespace instead.

Random integer generation on a closed interval
----------------------------------------------
In accordance with the Python C API, which gives preference to the half-open
interval over the closed one, ``np.random.random_integers`` is being
deprecated in favor of calling ``np.random.randint``, which has been
enhanced with the ``dtype`` parameter as described under "New Features".
However, ``np.random.random_integers`` will not be removed anytime soon.


FutureWarnings
==============

Assigning to slices/views of ``MaskedArray``
--------------------------------------------
Currently a slice of a masked array contains a view of the original data and a
copy-on-write view of the mask. Consequently, any changes to the slice's mask
will result in a copy of the original mask being made and that new mask being
changed rather than the original. For example, if we make a slice of the
original like so, ``view = original[:]``, then modifications to the data in one
array will affect the data of the other but, because the mask will be copied
during assignment operations, changes to the mask will remain local. A similar
situation occurs when explicitly constructing a masked array using
``MaskedArray(data, mask)``, the returned array will contain a view of ``data``
but the mask will be a copy-on-write view of ``mask``.

In the future, these cases will be normalized so that the data and mask arrays
are treated the same way and modifications to either will propagate between
views. In 1.11, numpy will issue a ``MaskedArrayFutureWarning`` warning
whenever user code modifies the mask of a view that in the future may cause
values to propagate back to the original.  To silence these warnings and make
your code robust against the upcoming changes, you have two options: if you
want to keep the current behavior, call ``masked_view.unshare_mask()`` before
modifying the mask.  If you want to get the future behavior early, use
``masked_view._sharedmask = False``. However, note that setting the
``_sharedmask`` attribute will break following explicit calls to
``masked_view.unshare_mask()``.
.. currentmodule:: numpy

==========================
NumPy 1.18.3 Release Notes
==========================

This release contains various bug/regression fixes.

The Python versions supported in this release are 3.5-3.8. Downstream
developers should use Cython >= 0.29.15 for Python 3.8 support and OpenBLAS >=
3.7 to avoid errors on the Skylake architecture.


Highlights
==========

* Fix for the `method='eigh'` and `method='cholesky'` methods in
  `numpy.random.multivariate_normal`. Those were producing samples from the
  wrong distribution.


Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Max Balandat +
* @Mibu287 +
* Pan Jan +
* Sebastian Berg
* @panpiort8 +


Pull requests merged
====================

A total of 5 pull requests were merged for this release.

* `#15916 <https://github.com/numpy/numpy/pull/15916>`__: BUG: Fix eigh and cholesky methods of numpy.random.multivariate_normal
* `#15929 <https://github.com/numpy/numpy/pull/15929>`__: BUG,MAINT: Remove incorrect special case in string to number...
* `#15930 <https://github.com/numpy/numpy/pull/15930>`__: BUG: Guarantee array is in valid state after memory error occurs...
* `#15954 <https://github.com/numpy/numpy/pull/15954>`__: BUG: Check that `pvals` is 1D in `_generator.multinomial`.
* `#16017 <https://github.com/numpy/numpy/pull/16017>`__: BUG: Alpha parameter must be 1D in `generator.dirichlet`
==========================
NumPy 1.15.2 Release Notes
==========================

This is a bugfix release for bugs and regressions reported following the 1.15.1
release.

* The matrix PendingDeprecationWarning is now suppressed in pytest 3.8.
* The new cached allocations machinery has been fixed to be thread safe.
* The boolean indexing of subclasses now works correctly.
* A small memory leak in PyArray_AdaptFlexibleDType has been fixed.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are
linked with OpenBLAS v0.3.0, which should fix some of the linalg problems
reported for NumPy 1.14.

Compatibility Note
==================

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit
binaries.  That will also be the case in future releases. See
`#11625 <https://github.com/numpy/numpy/issues/11625>`__ for the related
discussion.  Those needing 32-bit support should look elsewhere or build
from source.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Julian Taylor
* Marten van Kerkwijk
* Matti Picus

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#11902 <https://github.com/numpy/numpy/pull/11902>`__: BUG: Fix matrix PendingDeprecationWarning suppression for pytest...
* `#11981 <https://github.com/numpy/numpy/pull/11981>`__: BUG: fix cached allocations without the GIL for 1.15.x
* `#11982 <https://github.com/numpy/numpy/pull/11982>`__: BUG: fix refcount leak in PyArray_AdaptFlexibleDType
* `#11992 <https://github.com/numpy/numpy/pull/11992>`__: BUG: Ensure boolean indexing of subclasses sets base correctly.
.. currentmodule:: numpy

==========================
NumPy 1.17.5 Release Notes
==========================

This release contains fixes for bugs reported against NumPy 1.17.4 along with
some build improvements. The Python versions supported in this release
are 3.5-3.8.

Downstream developers should use Cython >= 0.29.14 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture.

It is recommended that developers interested in the new random bit generators
upgrade to the NumPy 1.18.x series, as it has updated documentation and
many small improvements.


Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Ilhan Polat
* Matti Picus
* Michael Hudson-Doyle
* Ralf Gommers


Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#14593 <https://github.com/numpy/numpy/pull/14593>`__: MAINT: backport Cython API cleanup to 1.17.x, remove docs
* `#14937 <https://github.com/numpy/numpy/pull/14937>`__: BUG: fix integer size confusion in handling array's ndmin argument
* `#14939 <https://github.com/numpy/numpy/pull/14939>`__: BUILD: remove SSE2 flag from numpy.random builds
* `#14993 <https://github.com/numpy/numpy/pull/14993>`__: MAINT: Added Python3.8 branch to dll lib discovery
* `#15038 <https://github.com/numpy/numpy/pull/15038>`__: BUG: Fix refcounting in ufunc object loops
* `#15067 <https://github.com/numpy/numpy/pull/15067>`__: BUG: Exceptions tracebacks are dropped
* `#15175 <https://github.com/numpy/numpy/pull/15175>`__: ENH: Backport improvements to testing functions.
* `#15213 <https://github.com/numpy/numpy/pull/15213>`__: REL: Prepare for the NumPy 1.17.5 release.
==========================
NumPy 1.10.0 Release Notes
==========================

This release supports Python 2.6 - 2.7 and 3.2 - 3.5.


Highlights
==========
* numpy.distutils now supports parallel compilation via the --parallel/-j
  argument passed to setup.py build
* numpy.distutils now supports additional customization via site.cfg to
  control compilation parameters, i.e. runtime libraries, extra
  linking/compilation flags.
* Addition of *np.linalg.multi_dot*: compute the dot product of two or more
  arrays in a single function call, while automatically selecting the fastest
  evaluation order.
* The new function `np.stack` provides a general interface for joining a
  sequence of arrays along a new axis, complementing `np.concatenate` for
  joining along an existing axis.
* Addition of `nanprod` to the set of nanfunctions.
* Support for the '@' operator in Python 3.5.

Dropped Support
===============

* The _dotblas module has been removed. CBLAS Support is now in
  Multiarray.
* The testcalcs.py file has been removed.
* The polytemplate.py file has been removed.
* npy_PyFile_Dup and npy_PyFile_DupClose have been removed from
  npy_3kcompat.h.
* splitcmdline has been removed from numpy/distutils/exec_command.py.
* try_run and get_output have been removed from
  numpy/distutils/command/config.py
* The a._format attribute is no longer supported for array printing.
* Keywords ``skiprows`` and ``missing`` removed from np.genfromtxt.
* Keyword ``old_behavior`` removed from np.correlate.

Future Changes
==============

* In array comparisons like ``arr1 == arr2``, many corner cases
  involving strings or structured dtypes that used to return scalars
  now issue ``FutureWarning`` or ``DeprecationWarning``, and in the
  future will be change to either perform elementwise comparisons or
  raise an error.
* In ``np.lib.split`` an empty array in the result always had dimension
  ``(0,)`` no matter the dimensions of the array being split. In Numpy 1.11
  that behavior will be changed so that the dimensions will be preserved. A
  ``FutureWarning`` for this change has been in place since Numpy 1.9 but,
  due to a bug, sometimes no warning was raised and the dimensions were
  already preserved.
* The SafeEval class will be removed in Numpy 1.11.
* The alterdot and restoredot functions will be removed in Numpy 1.11.

See below for more details on these changes.

Compatibility notes
===================

Default casting rule change
---------------------------
Default casting for inplace operations has changed to ``'same_kind'``. For
instance, if n is an array of integers, and f is an array of floats, then
``n += f`` will result in a ``TypeError``, whereas in previous Numpy
versions the floats would be silently cast to ints. In the unlikely case
that the example code is not an actual bug, it can be updated in a backward
compatible way by rewriting it as ``np.add(n, f, out=n, casting='unsafe')``.
The old ``'unsafe'`` default has been deprecated since Numpy 1.7.

numpy version string
--------------------
The numpy version string for development builds has been changed from
``x.y.z.dev-githash`` to ``x.y.z.dev0+githash`` (note the +) in order to comply
with PEP 440.

relaxed stride checking
-----------------------
NPY_RELAXED_STRIDE_CHECKING is now true by default.

UPDATE: In 1.10.2 the default value of  NPY_RELAXED_STRIDE_CHECKING was
changed to false for back compatibility reasons. More time is needed before
it can be made the default. As part of the roadmap a deprecation of
dimension changing views of f_contiguous not c_contiguous arrays was also
added.

Concatenation of 1d arrays along any but ``axis=0`` raises ``IndexError``
-------------------------------------------------------------------------
Using axis != 0 has raised a DeprecationWarning since NumPy 1.7, it now
raises an error.

*np.ravel*, *np.diagonal* and *np.diag* now preserve subtypes
-------------------------------------------------------------
There was inconsistent behavior between *x.ravel()* and *np.ravel(x)*, as
well as between *x.diagonal()* and *np.diagonal(x)*, with the methods
preserving subtypes while the functions did not. This has been fixed and
the functions now behave like the methods, preserving subtypes except in
the case of matrices.  Matrices are special cased for backward
compatibility and still return 1-D arrays as before. If you need to
preserve the matrix subtype, use the methods instead of the functions.

*rollaxis* and *swapaxes* always return a view
----------------------------------------------
Previously, a view was returned except when no change was made in the order
of the axes, in which case the input array was returned.  A view is now
returned in all cases.

*nonzero* now returns base ndarrays
-----------------------------------
Previously, an inconsistency existed between 1-D inputs (returning a
base ndarray) and higher dimensional ones (which preserved subclasses).
Behavior has been unified, and the return will now be a base ndarray.
Subclasses can still override this behavior by providing their own
*nonzero* method.

C API
-----
The changes to *swapaxes* also apply to the *PyArray_SwapAxes* C function,
which now returns a view in all cases.

The changes to *nonzero* also apply to the *PyArray_Nonzero* C function,
which now returns a base ndarray in all cases.

The dtype structure (PyArray_Descr) has a new member at the end to cache
its hash value.  This shouldn't affect any well-written applications.

The change to the concatenation function DeprecationWarning also affects
PyArray_ConcatenateArrays,

recarray field return types
---------------------------
Previously the returned types for recarray fields accessed by attribute and by
index were inconsistent, and fields of string type were returned as chararrays.
Now, fields accessed by either attribute or indexing will return an ndarray for
fields of non-structured type, and a recarray for fields of structured type.
Notably, this affect recarrays containing strings with whitespace, as trailing
whitespace is trimmed from chararrays but kept in ndarrays of string type.
Also, the dtype.type of nested structured fields is now inherited.

recarray views
--------------
Viewing an ndarray as a recarray now automatically converts the dtype to
np.record. See new record array documentation. Additionally, viewing a recarray
with a non-structured dtype no longer converts the result's type to ndarray -
the result will remain a recarray.

'out' keyword argument of ufuncs now accepts tuples of arrays
-------------------------------------------------------------
When using the 'out' keyword argument of a ufunc, a tuple of arrays, one per
ufunc output, can be provided. For ufuncs with a single output a single array
is also a valid 'out' keyword argument. Previously a single array could be
provided in the 'out' keyword argument, and it would be used as the first
output for ufuncs with multiple outputs, is deprecated, and will result in a
`DeprecationWarning` now and an error in the future.

byte-array indices now raises an IndexError
-------------------------------------------
Indexing an ndarray using a byte-string in Python 3 now raises an IndexError
instead of a ValueError.

Masked arrays containing objects with arrays
--------------------------------------------
For such (rare) masked arrays, getting a single masked item no longer returns a
corrupted masked array, but a fully masked version of the item.

Median warns and returns nan when invalid values are encountered
----------------------------------------------------------------
Similar to mean, median and percentile now emits a Runtime warning and
returns `NaN` in slices where a `NaN` is present.
To compute the median or percentile while ignoring invalid values use the
new `nanmedian` or `nanpercentile` functions.

Functions available from numpy.ma.testutils have changed
--------------------------------------------------------
All functions from numpy.testing were once available from
numpy.ma.testutils but not all of them were redefined to work with masked
arrays. Most of those functions have now been removed from
numpy.ma.testutils with a small subset retained in order to preserve
backward compatibility. In the long run this should help avoid mistaken use
of the wrong functions, but it may cause import problems for some.


New Features
============

Reading extra flags from site.cfg
---------------------------------
Previously customization of compilation of dependency libraries and numpy
itself was only accomblishable via code changes in the distutils package.
Now numpy.distutils reads in the following extra flags from each group of the
*site.cfg*:

* ``runtime_library_dirs/rpath``, sets runtime library directories to override
    ``LD_LIBRARY_PATH``
* ``extra_compile_args``, add extra flags to the compilation of sources
* ``extra_link_args``, add extra flags when linking libraries

This should, at least partially, complete user customization.

*np.cbrt* to compute cube root for real floats
----------------------------------------------
*np.cbrt* wraps the C99 cube root function *cbrt*.
Compared to *np.power(x, 1./3.)* it is well defined for negative real floats
and a bit faster.

numpy.distutils now allows parallel compilation
-----------------------------------------------
By passing *--parallel=n* or *-j n* to *setup.py build* the compilation of
extensions is now performed in *n* parallel processes.
The parallelization is limited to files within one extension so projects using
Cython will not profit because it builds extensions from single files.

*genfromtxt* has a new ``max_rows`` argument
--------------------------------------------
A ``max_rows`` argument has been added to *genfromtxt* to limit the
number of rows read in a single call. Using this functionality, it is
possible to read in multiple arrays stored in a single file by making
repeated calls to the function.

New function *np.broadcast_to* for invoking array broadcasting
--------------------------------------------------------------
*np.broadcast_to* manually broadcasts an array to a given shape according to
numpy's broadcasting rules. The functionality is similar to broadcast_arrays,
which in fact has been rewritten to use broadcast_to internally, but only a
single array is necessary.

New context manager *clear_and_catch_warnings* for testing warnings
-------------------------------------------------------------------
When Python emits a warning, it records that this warning has been emitted in
the module that caused the warning, in a module attribute
``__warningregistry__``.  Once this has happened, it is not possible to emit
the warning again, unless you clear the relevant entry in
``__warningregistry__``.  This makes is hard and fragile to test warnings,
because if your test comes after another that has already caused the warning,
you will not be able to emit the warning or test it. The context manager
``clear_and_catch_warnings`` clears warnings from the module registry on entry
and resets them on exit, meaning that warnings can be re-raised.

*cov* has new ``fweights`` and ``aweights`` arguments
-----------------------------------------------------
The ``fweights`` and ``aweights`` arguments add new functionality to
covariance calculations by applying two types of weighting to observation
vectors. An array of ``fweights`` indicates the number of repeats of each
observation vector, and an array of ``aweights`` provides their relative
importance or probability.

Support for the '@' operator in Python 3.5+
-------------------------------------------
Python 3.5 adds support for a matrix multiplication operator '@' proposed
in PEP465. Preliminary support for that has been implemented, and an
equivalent function ``matmul`` has also been added for testing purposes and
use in earlier Python versions. The function is preliminary and the order
and number of its optional arguments can be expected to change.

New argument ``norm`` to fft functions
--------------------------------------
The default normalization has the direct transforms unscaled and the inverse
transforms are scaled by :math:`1/n`. It is possible to obtain unitary
transforms by setting the keyword argument ``norm`` to ``"ortho"`` (default is
`None`) so that both direct and inverse transforms will be scaled by
:math:`1/\\sqrt{n}`.


Improvements
============

*np.digitize* using binary search
---------------------------------
*np.digitize* is now implemented in terms of *np.searchsorted*. This means
that a binary search is used to bin the values, which scales much better
for larger number of bins than the previous linear search. It also removes
the requirement for the input array to be 1-dimensional.

*np.poly* now casts integer inputs to float
-------------------------------------------
*np.poly* will now cast 1-dimensional input arrays of integer type to double
precision floating point, to prevent integer overflow when computing the monic
polynomial. It is still possible to obtain higher precision results by
passing in an array of object type, filled e.g. with Python ints.

*np.interp* can now be used with periodic functions
---------------------------------------------------
*np.interp* now has a new parameter *period* that supplies the period of the
input data *xp*. In such case, the input data is properly normalized to the
given period and one end point is added to each extremity of *xp* in order to
close the previous and the next period cycles, resulting in the correct
interpolation behavior.

*np.pad* supports more input types for ``pad_width`` and ``constant_values``
----------------------------------------------------------------------------
``constant_values`` parameters now accepts NumPy arrays and float values.
NumPy arrays are supported as input for ``pad_width``, and an exception is
raised if its values are not of integral type.

*np.argmax* and *np.argmin* now support an ``out`` argument
-----------------------------------------------------------
The ``out`` parameter was added to *np.argmax* and *np.argmin* for consistency
with *ndarray.argmax* and *ndarray.argmin*. The new parameter behaves exactly
as it does in those methods.

More system C99 complex functions detected and used
---------------------------------------------------
All of the functions ``in complex.h`` are now detected. There are new
fallback implementations of the following functions.

* npy_ctan,
* npy_cacos, npy_casin, npy_catan
* npy_ccosh, npy_csinh, npy_ctanh,
* npy_cacosh, npy_casinh, npy_catanh

As a result of these improvements, there will be some small changes in
returned values, especially for corner cases.

*np.loadtxt* support for the strings produced by the ``float.hex`` method
-------------------------------------------------------------------------
The strings produced by ``float.hex`` look like ``0x1.921fb54442d18p+1``,
so this is not the hex used to represent unsigned integer types.

*np.isclose* properly handles minimal values of integer dtypes
--------------------------------------------------------------
In order to properly handle minimal values of integer types, *np.isclose* will
now cast to the float dtype during comparisons. This aligns its behavior with
what was provided by *np.allclose*.

*np.allclose* uses *np.isclose* internally.
-------------------------------------------
*np.allclose* now uses *np.isclose* internally and inherits the ability to
compare NaNs as equal by setting ``equal_nan=True``. Subclasses, such as
*np.ma.MaskedArray*, are also preserved now.

*np.genfromtxt* now handles large integers correctly
----------------------------------------------------
*np.genfromtxt* now correctly handles integers larger than ``2**31-1`` on
32-bit systems and larger than ``2**63-1`` on 64-bit systems (it previously
crashed with an ``OverflowError`` in these cases). Integers larger than
``2**63-1`` are converted to floating-point values.

*np.load*, *np.save* have pickle backward compatibility flags
-------------------------------------------------------------

The functions *np.load* and *np.save* have additional keyword
arguments for controlling backward compatibility of pickled Python
objects. This enables Numpy on Python 3 to load npy files containing
object arrays that were generated on Python 2.

MaskedArray support for more complicated base classes
-----------------------------------------------------
Built-in assumptions that the baseclass behaved like a plain array are being
removed. In particular, setting and getting elements and ranges will respect
baseclass overrides of ``__setitem__`` and ``__getitem__``, and arithmetic
will respect overrides of ``__add__``, ``__sub__``, etc.

Changes
=======

dotblas functionality moved to multiarray
-----------------------------------------
The cblas versions of dot, inner, and vdot have been integrated into
the multiarray module. In particular, vdot is now a multiarray function,
which it was not before.

stricter check of gufunc signature compliance
---------------------------------------------
Inputs to generalized universal functions are now more strictly checked
against the function's signature: all core dimensions are now required to
be present in input arrays; core dimensions with the same label must have
the exact same size; and output core dimension's must be specified, either
by a same label input core dimension or by a passed-in output array.

views returned from *np.einsum* are writeable
---------------------------------------------
Views returned by *np.einsum* will now be writeable whenever the input
array is writeable.

*np.argmin* skips NaT values
----------------------------

*np.argmin* now skips NaT values in datetime64 and timedelta64 arrays,
making it consistent with *np.min*, *np.argmax* and *np.max*.


Deprecations
============

Array comparisons involving strings or structured dtypes
--------------------------------------------------------

Normally, comparison operations on arrays perform elementwise
comparisons and return arrays of booleans. But in some corner cases,
especially involving strings are structured dtypes, NumPy has
historically returned a scalar instead. For example::

  ### Current behaviour

  np.arange(2) == "foo"
  # -> False

  np.arange(2) < "foo"
  # -> True on Python 2, error on Python 3

  np.ones(2, dtype="i4,i4") == np.ones(2, dtype="i4,i4,i4")
  # -> False

Continuing work started in 1.9, in 1.10 these comparisons will now
raise ``FutureWarning`` or ``DeprecationWarning``, and in the future
they will be modified to behave more consistently with other
comparison operations, e.g.::

  ### Future behaviour

  np.arange(2) == "foo"
  # -> array([False, False])

  np.arange(2) < "foo"
  # -> error, strings and numbers are not orderable

  np.ones(2, dtype="i4,i4") == np.ones(2, dtype="i4,i4,i4")
  # -> [False, False]

SafeEval
--------
The SafeEval class in numpy/lib/utils.py is deprecated and will be removed
in the next release.

alterdot, restoredot
--------------------
The alterdot and restoredot functions no longer do anything, and are
deprecated.

pkgload, PackageLoader
----------------------
These ways of loading packages are now deprecated.

bias, ddof arguments to corrcoef
--------------------------------

The values for the ``bias`` and ``ddof`` arguments to the ``corrcoef``
function canceled in the division implied by the correlation coefficient and
so had no effect on the returned values.

We now deprecate these arguments to ``corrcoef`` and the masked array version
``ma.corrcoef``.

Because we are deprecating the ``bias`` argument to ``ma.corrcoef``, we also
deprecate the use of the ``allow_masked`` argument as a positional argument,
as its position will change with the removal of ``bias``.  ``allow_masked``
will in due course become a keyword-only argument.

dtype string representation changes
-----------------------------------
Since 1.6, creating a dtype object from its string representation, e.g.
``'f4'``, would issue a deprecation warning if the size did not correspond
to an existing type, and default to creating a dtype of the default size
for the type. Starting with this release, this will now raise a ``TypeError``.

The only exception is object dtypes, where both ``'O4'`` and ``'O8'`` will
still issue a deprecation warning. This platform-dependent representation
will raise an error in the next release.

In preparation for this upcoming change, the string representation of an
object dtype, i.e. ``np.dtype(object).str``, no longer includes the item
size, i.e. will return ``'|O'`` instead of ``'|O4'`` or ``'|O8'`` as
before.
=========================
NumPy 1.6.1 Release Notes
=========================

This is a bugfix only release in the 1.6.x series.


Issues Fixed
============

* #1834: einsum fails for specific shapes
* #1837: einsum throws nan or freezes python for specific array shapes
* #1838: object <-> structured type arrays regression
* #1851: regression for SWIG based code in 1.6.0
* #1863: Buggy results when operating on array copied with astype()
* #1870: Fix corner case of object array assignment
* #1843: Py3k: fix error with recarray
* #1885: nditer: Error in detecting double reduction loop
* #1874: f2py: fix --include_paths bug
* #1749: Fix ctypes.load_library()
* #1895/1896:  iter: writeonly operands weren't always being buffered correctly
.. currentmodule:: numpy

==========================
NumPy 1.18.5 Release Notes
==========================

This is a short release to allow pickle ``protocol=5`` to be used in
Python3.5. It is motivated by the recent backport of pickle5 to Python3.5.

The Python versions supported in this release are 3.5-3.8. Downstream
developers should use Cython >= 0.29.15 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture.

Contributors
============

A total of 3 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Siyuan Zhuang +

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#16439 <https://github.com/numpy/numpy/pull/16439>`__: ENH: enable pickle protocol 5 support for python3.5
* `#16441 <https://github.com/numpy/numpy/pull/16441>`__: BUG: relpath fails for different drives on windows

==========================
NumPy 1.15.3 Release Notes
==========================

This is a bugfix release for bugs and regressions reported following the 1.15.2
release.  The Python versions supported by this release are 2.7, 3.4-3.7. The
wheels are linked with OpenBLAS v0.3.0, which should fix some of the linalg
problems reported for NumPy 1.14.

Compatibility Note
==================

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit
binaries.  That will also be the case in future releases. See
`#11625 <https://github.com/numpy/numpy/issues/11625>`__ for the related
discussion.  Those needing 32-bit support should look elsewhere or build
from source.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Jeroen Demeyer
* Kevin Sheppard
* Matthew Bowden +
* Matti Picus
* Tyler Reddy

Pull requests merged
====================

A total of 12 pull requests were merged for this release.

* `#12080 <https://github.com/numpy/numpy/pull/12080>`__: MAINT: Blacklist some MSVC complex functions.
* `#12083 <https://github.com/numpy/numpy/pull/12083>`__: TST: Add azure CI testing to 1.15.x branch.
* `#12084 <https://github.com/numpy/numpy/pull/12084>`__: BUG: test_path() now uses Path.resolve()
* `#12085 <https://github.com/numpy/numpy/pull/12085>`__: TST, MAINT: Fix some failing tests on azure-pipelines mac and...
* `#12187 <https://github.com/numpy/numpy/pull/12187>`__: BUG: Fix memory leak in mapping.c
* `#12188 <https://github.com/numpy/numpy/pull/12188>`__: BUG: Allow boolean subtract in histogram
* `#12189 <https://github.com/numpy/numpy/pull/12189>`__: BUG: Fix in-place permutation
* `#12190 <https://github.com/numpy/numpy/pull/12190>`__: BUG: limit default for get_num_build_jobs() to 8
* `#12191 <https://github.com/numpy/numpy/pull/12191>`__: BUG: OBJECT_to_* should check for errors
* `#12192 <https://github.com/numpy/numpy/pull/12192>`__: DOC: Prepare for NumPy 1.15.3 release.
* `#12237 <https://github.com/numpy/numpy/pull/12237>`__: BUG: Fix MaskedArray fill_value type conversion.
* `#12238 <https://github.com/numpy/numpy/pull/12238>`__: TST: Backport azure-pipeline testing fixes for Mac
.. currentmodule:: numpy

==========================
NumPy 1.19.3 Release Notes
==========================

NumPy 1.19.3 is a small maintenance release with two major improvements:

- Python 3.9 binary wheels on all supported platforms.
- OpenBLAS fixes for Windows 10 version 2004 fmod bug.

This release supports Python 3.6-3.9 and is linked with OpenBLAS 0.3.12 to avoid
some of the fmod problems on Windows 10 version 2004. Microsoft is aware of the
problem and users should upgrade when the fix becomes available, the fix here
is limited in scope.

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Brown +
* Daniel Vanzo +
* E. Madison Bray +
* Hugo van Kemenade +
* Ralf Gommers
* Sebastian Berg
* @danbeibei +

Pull requests merged
====================

A total of 10 pull requests were merged for this release.

* `#17298 <https://github.com/numpy/numpy/pull/17298>`__: BLD: set upper versions for build dependencies
* `#17336 <https://github.com/numpy/numpy/pull/17336>`__: BUG: Set deprecated fields to null in PyArray_InitArrFuncs
* `#17446 <https://github.com/numpy/numpy/pull/17446>`__: ENH: Warn on unsupported Python 3.10+
* `#17450 <https://github.com/numpy/numpy/pull/17450>`__: MAINT: Update test_requirements.txt.
* `#17522 <https://github.com/numpy/numpy/pull/17522>`__: ENH: Support for the NVIDIA HPC SDK nvfortran compiler
* `#17568 <https://github.com/numpy/numpy/pull/17568>`__: BUG: Cygwin Workaround for #14787 on affected platforms
* `#17647 <https://github.com/numpy/numpy/pull/17647>`__: BUG: Fix memory leak of buffer-info cache due to relaxed strides
* `#17652 <https://github.com/numpy/numpy/pull/17652>`__: MAINT: Backport openblas_support from master.
* `#17653 <https://github.com/numpy/numpy/pull/17653>`__: TST: Add Python 3.9 to the CI testing on Windows, Mac.
* `#17660 <https://github.com/numpy/numpy/pull/17660>`__: TST: Simplify source path names in test_extending.
==========================
NumPy 1.13.2 Release Notes
==========================

This is a bugfix release for some problems found since 1.13.1. The most
important fixes are for CVE-2017-12852 and temporary elision. Users of earlier
versions of 1.13 should upgrade.

The Python versions supported are 2.7 and 3.4 - 3.6. The Python 3.6 wheels
available from PIP are built with Python 3.6.2 and should be compatible with
all previous versions of Python 3.6. The Windows wheels are now built
with OpenBlas instead ATLAS, which should improve the performance of the linear
algebra functions.

Contributors
============

A total of 12 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Brandon Carter
* Charles Harris
* Eric Wieser
* Iryna Shcherbina +
* James Bourbeau +
* Jonathan Helmus
* Julian Taylor
* Matti Picus
* Michael Lamparski +
* Michael Seifert
* Ralf Gommers

Pull requests merged
====================

A total of 20 pull requests were merged for this release.

* #9390 BUG: Return the poly1d coefficients array directly
* #9555 BUG: Fix regression in 1.13.x in distutils.mingw32ccompiler.
* #9556 BUG: Fix true_divide when dtype=np.float64 specified.
* #9557 DOC: Fix some rst markup in numpy/doc/basics.py.
* #9558 BLD: Remove -xhost flag from IntelFCompiler.
* #9559 DOC: Removes broken docstring example (source code, png, pdf)...
* #9580 BUG: Add hypot and cabs functions to WIN32 blacklist.
* #9732 BUG: Make scalar function elision check if temp is writeable.
* #9736 BUG: Various fixes to np.gradient
* #9742 BUG: Fix np.pad for CVE-2017-12852
* #9744 BUG: Check for exception in sort functions, add tests
* #9745 DOC: Add whitespace after "versionadded::" directive so it actually...
* #9746 BUG: Memory leak in np.dot of size 0
* #9747 BUG: Adjust gfortran version search regex
* #9757 BUG: Cython 0.27 breaks NumPy on Python 3.
* #9764 BUG: Ensure `_npy_scaled_cexp{,f,l}` is defined when needed.
* #9765 BUG: PyArray_CountNonzero does not check for exceptions
* #9766 BUG: Fixes histogram monotonicity check for unsigned bin values
* #9767 BUG: Ensure consistent result dtype of count_nonzero
* #9771 BUG, MAINT: Fix mtrand for Cython 0.27.
==========================
NumPy 1.16.2 Release Notes
==========================

NumPy 1.16.2 is a quick release fixing several problems encountered on Windows.
The Python versions supported are 2.7 and 3.5-3.7. The Windows problems
addressed are:

- DLL load problems for NumPy wheels on Windows,
- distutils command line parsing on Windows.

There is also a regression fix correcting signed zeros produced by divmod, see
below for details.

Downstream developers building this release should use Cython >= 0.29.2 and, if
using OpenBLAS, OpenBLAS > v0.3.4.

If you are installing using pip, you may encounter a problem with older
installed versions of NumPy that pip did not delete becoming mixed with the
current version, resulting in an ``ImportError``. That problem is particularly
common on Debian derived distributions due to a modified pip.  The fix is to
make sure all previous NumPy versions installed by pip have been removed. See
`#12736 <https://github.com/numpy/numpy/issues/12736>`__ for discussion of the
issue.


Compatibility notes
===================

Signed zero when using divmod
-----------------------------
Starting in version 1.12.0, numpy incorrectly returned a negatively signed zero
when using the ``divmod`` and ``floor_divide`` functions when the result was
zero. For example::

   >>> np.zeros(10)//1
   array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])

With this release, the result is correctly returned as a positively signed
zero::

   >>> np.zeros(10)//1
   array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])


Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Matti Picus
* Tyler Reddy
* Tony LaTorre +


Pull requests merged
====================

A total of 7 pull requests were merged for this release.

* `#12909 <https://github.com/numpy/numpy/pull/12909>`__: TST: fix vmImage dispatch in Azure
* `#12923 <https://github.com/numpy/numpy/pull/12923>`__: MAINT: remove complicated test of multiarray import failure mode
* `#13020 <https://github.com/numpy/numpy/pull/13020>`__: BUG: fix signed zero behavior in npy_divmod
* `#13026 <https://github.com/numpy/numpy/pull/13026>`__: MAINT: Add functions to parse shell-strings in the platform-native...
* `#13028 <https://github.com/numpy/numpy/pull/13028>`__: BUG: Fix regression in parsing of F90 and F77 environment variables
* `#13038 <https://github.com/numpy/numpy/pull/13038>`__: BUG: parse shell escaping in extra_compile_args and extra_link_args
* `#13041 <https://github.com/numpy/numpy/pull/13041>`__: BLD: Windows absolute path DLL loading
.. currentmodule:: numpy

==========================
NumPy 1.17.2 Release Notes
==========================

This release contains fixes for bugs reported against NumPy 1.17.1 along with a
some documentation improvements. The most important fix is for lexsort when the
keys are of type (u)int8 or (u)int16. If you are currently using 1.17 you
should upgrade.

The Python versions supported in this release are 3.5-3.7, Python 2.7 has been
dropped.  Python 3.8b4 should work with the released source packages, but there
are no future guarantees.

Downstream developers should use Cython >= 0.29.13 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture. The NumPy wheels
on PyPI are built from the OpenBLAS development branch in order to avoid those
errors.


Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* CakeWithSteak +
* Charles Harris
* Dan Allan
* Hameer Abbasi
* Lars Grueter
* Matti Picus
* Sebastian Berg


Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#14418 <https://github.com/numpy/numpy/pull/14418>`__: BUG: Fix aradixsort indirect indexing.
* `#14420 <https://github.com/numpy/numpy/pull/14420>`__: DOC: Fix a minor typo in dispatch documentation.
* `#14421 <https://github.com/numpy/numpy/pull/14421>`__: BUG: test, fix regression in converting to ctypes
* `#14430 <https://github.com/numpy/numpy/pull/14430>`__: BUG: Do not show Override module in private error classes.
* `#14432 <https://github.com/numpy/numpy/pull/14432>`__: BUG: Fixed maximum relative error reporting in assert_allclose.
* `#14433 <https://github.com/numpy/numpy/pull/14433>`__: BUG: Fix uint-overflow if padding with linear_ramp and negative...
* `#14436 <https://github.com/numpy/numpy/pull/14436>`__: BUG: Update 1.17.x with 1.18.0-dev pocketfft.py.
* `#14446 <https://github.com/numpy/numpy/pull/14446>`__: REL: Prepare for NumPy 1.17.2 release.
.. currentmodule:: numpy

==========================
NumPy 1.17.4 Release Notes
==========================

This release contains fixes for bugs reported against NumPy 1.17.3 along with
some build improvements. The Python versions supported in this release
are 3.5-3.8.

Downstream developers should use Cython >= 0.29.13 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture.


Highlights
==========

- Fixed `random.random_integers` biased generation of 8 and 16 bit integers.
- Fixed `np.einsum` regression on Power9 and z/Linux.
- Fixed histogram problem with signed integer arrays.


Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Burr +
* Matti Picus
* Qiming Sun +
* Warren Weckesser


Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#14758 <https://github.com/numpy/numpy/pull/14758>`__: BLD: declare support for python 3.8
* `#14781 <https://github.com/numpy/numpy/pull/14781>`__: BUG: random: biased samples from integers() with 8 or 16 bit...
* `#14851 <https://github.com/numpy/numpy/pull/14851>`__: BUG: Fix _ctypes class circular reference. (#13808)
* `#14852 <https://github.com/numpy/numpy/pull/14852>`__: BLD: add 'apt update' to shippable
* `#14855 <https://github.com/numpy/numpy/pull/14855>`__: BUG: Fix `np.einsum` errors on Power9 Linux and z/Linux
* `#14857 <https://github.com/numpy/numpy/pull/14857>`__: BUG: lib: Fix histogram problem with signed integer arrays.
* `#14858 <https://github.com/numpy/numpy/pull/14858>`__: BLD: Prevent -flto from optimising long double representation...
* `#14866 <https://github.com/numpy/numpy/pull/14866>`__: MAINT: move buffer.h -> npy_buffer.h to avoid conflicts

==========================
NumPy 1.11.3 Release Notes
==========================

Numpy 1.11.3 fixes a bug that leads to file corruption when very large files
opened in append mode are used in ``ndarray.tofile``. It supports Python
versions 2.6 - 2.7 and 3.2 - 3.5. Wheels for Linux, Windows, and OS X can be
found on PyPI.


Contributors to maintenance/1.11.3
==================================

A total of 2 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

- Charles Harris
- Pavel Potocek +

Pull Requests Merged
====================

- `#8341 <https://github.com/numpy/numpy/pull/8341>`__: BUG: Fix ndarray.tofile large file corruption in append mode.
- `#8346 <https://github.com/numpy/numpy/pull/8346>`__: TST: Fix tests in PR #8341 for NumPy 1.11.x

.. currentmodule:: numpy

==========================
NumPy 1.20.2 Release Notes
==========================

NumPy 1.20.2 is a bugfix release containing several fixes merged to the main
branch after the NumPy 1.20.1 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Bas van Beek
* Charles Harris
* Christoph Gohlke
* Mateusz Sokół +
* Michael Lamparski
* Sebastian Berg

Pull requests merged
====================

A total of 20 pull requests were merged for this release.

* `#18382 <https://github.com/numpy/numpy/pull/18382>`__: MAINT: Update f2py from master.
* `#18459 <https://github.com/numpy/numpy/pull/18459>`__: BUG: ``diagflat`` could overflow on windows or 32-bit platforms
* `#18460 <https://github.com/numpy/numpy/pull/18460>`__: BUG: Fix refcount leak in f2py ``complex_double_from_pyobj``.
* `#18461 <https://github.com/numpy/numpy/pull/18461>`__: BUG: Fix tiny memory leaks when ``like=`` overrides are used
* `#18462 <https://github.com/numpy/numpy/pull/18462>`__: BUG: Remove temporary change of descr/flags in VOID functions
* `#18469 <https://github.com/numpy/numpy/pull/18469>`__: BUG: Segfault in nditer buffer dealloc for Object arrays
* `#18485 <https://github.com/numpy/numpy/pull/18485>`__: BUG: Remove suspicious type casting
* `#18486 <https://github.com/numpy/numpy/pull/18486>`__: BUG: remove nonsensical comparison of pointer < 0
* `#18487 <https://github.com/numpy/numpy/pull/18487>`__: BUG: verify pointer against NULL before using it
* `#18488 <https://github.com/numpy/numpy/pull/18488>`__: BUG: check if PyArray_malloc succeeded
* `#18546 <https://github.com/numpy/numpy/pull/18546>`__: BUG: incorrect error fallthrough in nditer
* `#18559 <https://github.com/numpy/numpy/pull/18559>`__: CI: Backport CI fixes from main.
* `#18599 <https://github.com/numpy/numpy/pull/18599>`__: MAINT: Add annotations for `dtype.__getitem__`, `__mul__` and...
* `#18611 <https://github.com/numpy/numpy/pull/18611>`__: BUG: NameError in numpy.distutils.fcompiler.compaq
* `#18612 <https://github.com/numpy/numpy/pull/18612>`__: BUG: Fixed ``where`` keyword for ``np.mean`` & ``np.var`` methods
* `#18617 <https://github.com/numpy/numpy/pull/18617>`__: CI: Update apt package list before Python install
* `#18636 <https://github.com/numpy/numpy/pull/18636>`__: MAINT: Ensure that re-exported sub-modules are properly annotated
* `#18638 <https://github.com/numpy/numpy/pull/18638>`__: BUG: Fix ma coercion list-of-ma-arrays if they do not cast to...
* `#18661 <https://github.com/numpy/numpy/pull/18661>`__: BUG: Fix small valgrind-found issues
* `#18671 <https://github.com/numpy/numpy/pull/18671>`__: BUG: Fix small issues found with pytest-leaks
.. currentmodule:: numpy

==========================
NumPy 1.20.3 Release Notes
==========================

NumPy 1.20.3 is a bugfix release containing several fixes merged to the main
branch after the NumPy 1.20.2 release.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Anne Archibald
* Bas van Beek
* Charles Harris
* Dong Keun Oh +
* Kamil Choudhury +
* Sayed Adel
* Sebastian Berg

Pull requests merged
====================

A total of 15 pull requests were merged for this release.

* `#18763 <https://github.com/numpy/numpy/pull/18763>`__: BUG: Correct ``datetime64`` missing type overload for ``datetime.date``...
* `#18764 <https://github.com/numpy/numpy/pull/18764>`__: MAINT: Remove ``__all__`` in favor of explicit re-exports
* `#18768 <https://github.com/numpy/numpy/pull/18768>`__: BLD: Strip extra newline when dumping gfortran version on MacOS
* `#18769 <https://github.com/numpy/numpy/pull/18769>`__: BUG: fix segfault in object/longdouble operations
* `#18794 <https://github.com/numpy/numpy/pull/18794>`__: MAINT: Use towncrier build explicitly
* `#18887 <https://github.com/numpy/numpy/pull/18887>`__: MAINT: Relax certain integer-type constraints
* `#18915 <https://github.com/numpy/numpy/pull/18915>`__: MAINT: Remove unsafe unions and ABCs from return-annotations
* `#18921 <https://github.com/numpy/numpy/pull/18921>`__: MAINT: Allow more recursion depth for scalar tests.
* `#18922 <https://github.com/numpy/numpy/pull/18922>`__: BUG: Initialize the full nditer buffer in case of error
* `#18923 <https://github.com/numpy/numpy/pull/18923>`__: BLD: remove unnecessary flag ``-faltivec`` on macOS
* `#18924 <https://github.com/numpy/numpy/pull/18924>`__: MAINT, CI: treats _SIMD module build warnings as errors through...
* `#18925 <https://github.com/numpy/numpy/pull/18925>`__: BUG: for MINGW, threads.h existence test requires GLIBC > 2.12
* `#18941 <https://github.com/numpy/numpy/pull/18941>`__: BUG: Make changelog recognize gh- as a PR number prefix.
* `#18948 <https://github.com/numpy/numpy/pull/18948>`__: REL, DOC: Prepare for the NumPy 1.20.3 release.
* `#18953 <https://github.com/numpy/numpy/pull/18953>`__: BUG: Fix failing mypy test in 1.20.x.
==========================
NumPy 1.11.1 Release Notes
==========================

Numpy 1.11.1 supports Python 2.6 - 2.7 and 3.2 - 3.5. It fixes bugs and
regressions found in Numpy 1.11.0 and includes several build related
improvements. Wheels for Linux, Windows, and OSX can be found on PyPI.

Fixes Merged
============

- #7506 BUG: Make sure numpy imports on python 2.6 when nose is unavailable.
- #7530 BUG: Floating exception with invalid axis in np.lexsort.
- #7535 BUG: Extend glibc complex trig functions blacklist to glibc < 2.18.
- #7551 BUG: Allow graceful recovery for no compiler.
- #7558 BUG: Constant padding expected wrong type in constant_values.
- #7578 BUG: Fix OverflowError in Python 3.x. in swig interface.
- #7590 BLD: Fix configparser.InterpolationSyntaxError.
- #7597 BUG: Make np.ma.take work on scalars.
- #7608 BUG: linalg.norm(): Don't convert object arrays to float.
- #7638 BLD: Correct C compiler customization in system_info.py.
- #7654 BUG: ma.median of 1d array should return a scalar.
- #7656 BLD: Remove hardcoded Intel compiler flag -xSSE4.2.
- #7660 BUG: Temporary fix for str(mvoid) for object field types.
- #7665 BUG: Fix incorrect printing of 1D masked arrays.
- #7670 BUG: Correct initial index estimate in histogram.
- #7671 BUG: Boolean assignment no GIL release when transfer needs API.
- #7676 BUG: Fix handling of right edge of final histogram bin.
- #7680 BUG: Fix np.clip bug NaN handling for Visual Studio 2015.
- #7724 BUG: Fix segfaults in np.random.shuffle.
- #7731 MAINT: Change mkl_info.dir_env_var from MKL to MKLROOT.
- #7737 BUG: Fix issue on OS X with Python 3.x, npymath.ini not installed.
.. currentmodule:: numpy

==========================
NumPy 1.21.5 Release Notes
==========================

NumPy 1.21.5 is a maintenance release that fixes a few bugs discovered after
the 1.21.4 release and does some maintenance to extend the 1.21.x lifetime.
The Python versions supported in this release are 3.7-3.10. If you want to
compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid
problems.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Matti Picus
* Rohit Goswami
* Ross Barnowski
* Sayed Adel
* Sebastian Berg

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#20357 <https://github.com/numpy/numpy/pull/20357>`__: MAINT: Do not forward ``__(deep)copy__`` calls of ``_GenericAlias``...
* `#20462 <https://github.com/numpy/numpy/pull/20462>`__: BUG: Fix float16 einsum fastpaths using wrong tempvar
* `#20463 <https://github.com/numpy/numpy/pull/20463>`__: BUG, DIST: Print os error message when the executable not exist
* `#20464 <https://github.com/numpy/numpy/pull/20464>`__: BLD: Verify the ability to compile C++ sources before initiating...
* `#20465 <https://github.com/numpy/numpy/pull/20465>`__: BUG: Force ``npymath` ` to respect ``npy_longdouble``
* `#20466 <https://github.com/numpy/numpy/pull/20466>`__: BUG: Fix failure to create aligned, empty structured dtype
* `#20467 <https://github.com/numpy/numpy/pull/20467>`__: ENH: provide a convenience function to replace npy_load_module
* `#20495 <https://github.com/numpy/numpy/pull/20495>`__: MAINT: update wheel to version that supports python3.10
* `#20497 <https://github.com/numpy/numpy/pull/20497>`__: BUG: Clear errors correctly in F2PY conversions
* `#20613 <https://github.com/numpy/numpy/pull/20613>`__: DEV: add a warningfilter to fix pytest workflow.
* `#20618 <https://github.com/numpy/numpy/pull/20618>`__: MAINT: Help boost::python libraries at least not crash
=========================
NumPy 1.8.2 Release Notes
=========================

This is a bugfix only release in the 1.8.x series.

Issues fixed
============

* gh-4836: partition produces wrong results for multiple selections in equal ranges
* gh-4656: Make fftpack._raw_fft threadsafe
* gh-4628: incorrect argument order to _copyto in in np.nanmax, np.nanmin
* gh-4642: Hold GIL for converting dtypes types with fields
* gh-4733: fix np.linalg.svd(b, compute_uv=False)
* gh-4853: avoid unaligned simd load on reductions on i386
* gh-4722: Fix seg fault converting empty string to object
* gh-4613: Fix lack of NULL check in array_richcompare
* gh-4774: avoid unaligned access for strided byteswap
* gh-650: Prevent division by zero when creating arrays from some buffers
* gh-4602: ifort has issues with optimization flag O2, use O1
.. currentmodule:: numpy

==========================
NumPy 1.18.0 Release Notes
==========================

In addition to the usual bug fixes, this NumPy release cleans up and documents
the new random C-API, expires a large number of old deprecations, and improves
the appearance of the documentation. The Python versions supported are 3.5-3.8.
This is the last NumPy release series that will support Python 3.5.

Downstream developers should use Cython >= 0.29.14 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid problems on the Skylake
architecture.


Highlights
==========

* The C-API for ``numpy.random`` has been defined and documented.
* Basic infrastructure for linking with 64 bit BLAS and LAPACK libraries.
* Many documentation improvements.


New functions
=============

Multivariate hypergeometric distribution added to ``numpy.random``
------------------------------------------------------------------
The method ``multivariate_hypergeometric`` has been added to the class
`numpy.random.Generator`.  This method generates random variates from
the multivariate hypergeometric probability distribution.
(`gh-13794 <https://github.com/numpy/numpy/pull/13794>`__)


Deprecations
============

``np.fromfile`` and ``np.fromstring`` will error on bad data
------------------------------------------------------------

In future numpy releases, the functions ``np.fromfile`` and ``np.fromstring``
will throw an error when parsing bad data.
This will now give a ``DeprecationWarning`` where previously partial or
even invalid data was silently returned. This deprecation also affects
the C defined functions ``PyArray_FromString`` and ``PyArray_FromFile``
(`gh-13605 <https://github.com/numpy/numpy/pull/13605>`__)

Deprecate non-scalar arrays as fill values in ``ma.fill_value``
---------------------------------------------------------------
Setting a ``MaskedArray.fill_value`` to a non-scalar array is deprecated
since the logic to broadcast the fill value to the array is fragile,
especially when slicing.
(`gh-13698 <https://github.com/numpy/numpy/pull/13698>`__)

Deprecate ``PyArray_As1D``, ``PyArray_As2D``
--------------------------------------------
``PyArray_As1D``, ``PyArray_As2D`` are deprecated, use
``PyArray_AsCArray`` instead
(`gh-14036 <https://github.com/numpy/numpy/pull/14036>`__)

Deprecate ``np.alen``
---------------------
``np.alen`` was deprecated. Use ``len`` instead.
(`gh-14181 <https://github.com/numpy/numpy/pull/14181>`__)

Deprecate the financial functions
---------------------------------
In accordance with
`NEP-32 <https://numpy.org/neps/nep-0032-remove-financial-functions.html>`_,
the financial functions ``fv`` ``ipmt``, ``irr``, ``mirr``, ``nper``,
``npv``, ``pmt``, ``ppmt``, ``pv`` and ``rate`` are deprecated, and will be
removed from NumPy 1.20.The replacement for these functions is the Python package
`numpy-financial <https://pypi.org/project/numpy-financial>`_.
(`gh-14720 <https://github.com/numpy/numpy/pull/14720>`__)

The ``axis`` argument to ``numpy.ma.mask_cols`` and ``numpy.ma.mask_row`` is deprecated
---------------------------------------------------------------------------------------
This argument was always ignored.
(`gh-14996 <https://github.com/numpy/numpy/pull/14996>`__)


Expired deprecations
====================

* ``PyArray_As1D`` and ``PyArray_As2D`` have been removed in favor of
  ``PyArray_AsCArray``
  (`gh-14036 <https://github.com/numpy/numpy/pull/14036>`__)

* ``np.rank`` has been removed. This was deprecated in NumPy 1.10
  and has been replaced by ``np.ndim``.
  (`gh-14039 <https://github.com/numpy/numpy/pull/14039>`__)

* The deprecation of ``expand_dims`` out-of-range axes in 1.13.0 has
  expired.
  (`gh-14051 <https://github.com/numpy/numpy/pull/14051>`__)

* ``PyArray_FromDimsAndDataAndDescr`` and ``PyArray_FromDims`` have been
  removed (they will always raise an error). Use ``PyArray_NewFromDescr``
  and ``PyArray_SimpleNew`` instead.
  (`gh-14100 <https://github.com/numpy/numpy/pull/14100>`__)

* ``numeric.loads``, ``numeric.load``, ``np.ma.dump``,
  ``np.ma.dumps``, ``np.ma.load``, ``np.ma.loads`` are removed,
  use ``pickle`` methods instead
  (`gh-14256 <https://github.com/numpy/numpy/pull/14256>`__)

* ``arrayprint.FloatFormat``, ``arrayprint.LongFloatFormat`` has been removed,
  use ``FloatingFormat`` instead

* ``arrayprint.ComplexFormat``, ``arrayprint.LongComplexFormat`` has been
  removed, use ``ComplexFloatingFormat`` instead

* ``arrayprint.StructureFormat`` has been removed, use ``StructureVoidFormat``
  instead
  (`gh-14259 <https://github.com/numpy/numpy/pull/14259>`__)

* ``np.testing.rand`` has been removed. This was deprecated in NumPy 1.11
  and has been replaced by ``np.random.rand``.
  (`gh-14325 <https://github.com/numpy/numpy/pull/14325>`__)

* Class ``SafeEval`` in ``numpy/lib/utils.py`` has been removed.
  This was deprecated in NumPy 1.10. Use ``np.safe_eval`` instead.
  (`gh-14335 <https://github.com/numpy/numpy/pull/14335>`__)

* Remove deprecated support for boolean and empty condition lists in
  ``np.select``
  (`gh-14583 <https://github.com/numpy/numpy/pull/14583>`__)

* Array order only accepts 'C', 'F', 'A', and 'K'. More permissive options
  were deprecated in NumPy 1.11.
  (`gh-14596 <https://github.com/numpy/numpy/pull/14596>`__)

* np.linspace parameter ``num`` must be an integer. Deprecated in NumPy 1.12.
  (`gh-14620 <https://github.com/numpy/numpy/pull/14620>`__)

* UFuncs with multiple outputs must use a tuple for the ``out`` kwarg. This
  finishes a deprecation started in NumPy 1.10.
  (`gh-14682 <https://github.com/numpy/numpy/pull/14682>`__)

The files ``numpy/testing/decorators.py``, ``numpy/testing/noseclasses.py``
and ``numpy/testing/nosetester.py`` have been removed.  They were never
meant to be public (all relevant objects are present in the
``numpy.testing`` namespace), and importing them has given a deprecation
warning since NumPy 1.15.0
(`gh-14567 <https://github.com/numpy/numpy/pull/14567>`__)


Compatibility notes
===================

`numpy.lib.recfunctions.drop_fields` can no longer return None
--------------------------------------------------------------
If ``drop_fields`` is used to drop all fields, previously the array would
be completely discarded and None returned. Now it returns an array of the
same shape as the input, but with no fields. The old behavior can be retained
with::

    dropped_arr = drop_fields(arr, ['a', 'b'])
    if dropped_arr.dtype.names == ():
        dropped_arr = None

converting the empty recarray to None
(`gh-14510 <https://github.com/numpy/numpy/pull/14510>`__)

``numpy.argmin/argmax/min/max`` returns ``NaT`` if it exists in array
---------------------------------------------------------------------
``numpy.argmin``, ``numpy.argmax``, ``numpy.min``, and ``numpy.max`` will return
``NaT`` if it exists in the array.
(`gh-14717 <https://github.com/numpy/numpy/pull/14717>`__)

``np.can_cast(np.uint64, np.timedelta64, casting='safe')`` is now ``False``
---------------------------------------------------------------------------
Previously this was ``True`` - however, this was inconsistent with ``uint64``
not being safely castable to ``int64``, and resulting in strange type
resolution.

If this impacts your code, cast ``uint64`` to ``int64`` first.
(`gh-14718 <https://github.com/numpy/numpy/pull/14718>`__)

Changed random variate stream from ``numpy.random.Generator.integers``
----------------------------------------------------------------------
There was a bug in ``numpy.random.Generator.integers`` that caused biased
sampling of 8 and 16 bit integer types. Fixing that bug has changed the
output stream from what it was in previous releases.
(`gh-14777 <https://github.com/numpy/numpy/pull/14777>`__)

Add more ufunc loops for ``datetime64``, ``timedelta64``
--------------------------------------------------------
``np.datetime('NaT')`` should behave more like ``float('Nan')``. Add needed
infrastructure so ``np.isinf(a)`` and ``np.isnan(a)`` will run on
``datetime64`` and ``timedelta64`` dtypes. Also added specific loops for
``numpy.fmin`` and ``numpy.fmax`` that mask ``NaT``. This may require
adjustment to user- facing code. Specifically, code that either disallowed the
calls to ``numpy.isinf`` or ``numpy.isnan`` or checked that they raised an
exception will require adaptation, and code that mistakenly called
``numpy.fmax`` and ``numpy.fmin`` instead of ``numpy.maximum`` or
``numpy.minimum`` respectively will require adjustment. This also affects
``numpy.nanmax`` and ``numpy.nanmin``.
(`gh-14841 <https://github.com/numpy/numpy/pull/14841>`__)

Moved modules in ``numpy.random``
---------------------------------
As part of the API cleanup, the submodules in ``numpy.random``
``bit_generator``, ``philox``, ``pcg64``, ``sfc64, ``common``, ``generator``,
and ``bounded_integers`` were moved to ``_bit_generator``, ``_philox``,
``_pcg64``, ``_sfc64, ``_common``, ``_generator``, and ``_bounded_integers``
respectively to indicate that they are not part of the public interface.
(`gh-14608 <https://github.com/numpy/numpy/pull/14608>`__)


C API changes
=============

``PyDataType_ISUNSIZED(descr)`` now returns False for structured datatypes
--------------------------------------------------------------------------
Previously this returned True for any datatype of itemsize 0, but now this
returns false for the non-flexible datatype with itemsize 0, ``np.dtype([])``.
(`gh-14393 <https://github.com/numpy/numpy/pull/14393>`__)


New Features
============

Add our own ``*.pxd`` cython import file
----------------------------------------
Added a ``numpy/__init__.pxd`` file. It will be used for ``cimport numpy``
(`gh-12284 <https://github.com/numpy/numpy/pull/12284>`__)

A tuple of axes can now be input to ``expand_dims``
---------------------------------------------------
The ``numpy.expand_dims`` ``axis`` keyword can now accept a tuple of
axes.  Previously, ``axis`` was required to be an integer.
(`gh-14051 <https://github.com/numpy/numpy/pull/14051>`__)

Support for 64-bit OpenBLAS
---------------------------
Added support for 64-bit (ILP64) OpenBLAS. See ``site.cfg.example``
for details.
(`gh-15012 <https://github.com/numpy/numpy/pull/15012>`__)

Add ``--f2cmap`` option to F2PY
-------------------------------
Allow specifying a file to load Fortran-to-C type map
customizations from.
(`gh-15113 <https://github.com/numpy/numpy/pull/15113>`__)


Improvements
============

Different C numeric types of the same size have unique names
------------------------------------------------------------
On any given platform, two of ``np.intc``, ``np.int_``, and ``np.longlong``
would previously appear indistinguishable through their ``repr``, despite
their corresponding ``dtype`` having different properties.
A similar problem existed for the unsigned counterparts to these types, and on
some platforms for ``np.double`` and ``np.longdouble``

These types now always print with a unique ``__name__``.
(`gh-10151 <https://github.com/numpy/numpy/pull/10151>`__)

``argwhere`` now produces a consistent result on 0d arrays
----------------------------------------------------------
On N-d arrays, ``numpy.argwhere`` now always produces an array of shape
``(n_non_zero, arr.ndim)``, even when ``arr.ndim == 0``. Previously, the
last axis would have a dimension of 1 in this case.
(`gh-13610 <https://github.com/numpy/numpy/pull/13610>`__)

Add ``axis`` argument for ``random.permutation`` and ``random.shuffle``
-----------------------------------------------------------------------

Previously the ``random.permutation`` and ``random.shuffle`` functions
can only shuffle an array along the first axis; they now have a
new argument ``axis`` which allows shuffle along a specified axis.
(`gh-13829 <https://github.com/numpy/numpy/pull/13829>`__)

``method`` keyword argument for ``np.random.multivariate_normal``
-----------------------------------------------------------------
A ``method`` keyword argument is now available for
``np.random.multivariate_normal`` with possible values
``{'svd', 'eigh', 'cholesky'}``. To use it, write
``np.random.multivariate_normal(..., method=<method>)``.
(`gh-14197 <https://github.com/numpy/numpy/pull/14197>`__)

Add complex number support for ``numpy.fromstring``
---------------------------------------------------
Now ``numpy.fromstring`` can read complex numbers.
(`gh-14227 <https://github.com/numpy/numpy/pull/14227>`__)

``numpy.unique`` has consistent axes order when ``axis`` is not None
--------------------------------------------------------------------
Using ``moveaxis`` instead of ``swapaxes`` in ``numpy.unique``, so that the ordering of axes
except the axis in arguments will not be broken.
(`gh-14255 <https://github.com/numpy/numpy/pull/14255>`__)

``numpy.matmul`` with boolean output now converts to boolean values
-------------------------------------------------------------------
Calling ``numpy.matmul`` where the output is a boolean array would fill the array
with uint8 equivalents of the result, rather than 0/1. Now it forces the output
to 0 or 1 (``NPY_TRUE`` or ``NPY_FALSE``).
(`gh-14464 <https://github.com/numpy/numpy/pull/14464>`__)

``numpy.random.randint`` produced incorrect value when the range was ``2**32``
------------------------------------------------------------------------------
The implementation introduced in 1.17.0 had an incorrect check when
determining whether to use the 32-bit path or the full 64-bit
path that incorrectly redirected random integer generation with a high - low
range of ``2**32`` to the 64-bit generator.
(`gh-14501 <https://github.com/numpy/numpy/pull/14501>`__)

Add complex number support for ``numpy.fromfile``
-------------------------------------------------
Now ``numpy.fromfile`` can read complex numbers.
(`gh-14730 <https://github.com/numpy/numpy/pull/14730>`__)

``std=c99`` added if compiler is named ``gcc``
----------------------------------------------
GCC before version 5 requires the ``-std=c99`` command line argument. Newer
compilers automatically turn on C99 mode. The compiler setup code will
automatically add the code if the compiler name has ``gcc`` in it.
(`gh-14771 <https://github.com/numpy/numpy/pull/14771>`__)


Changes
=======


``NaT`` now sorts to the end of arrays
--------------------------------------
``NaT`` is now effectively treated as the largest integer for sorting
purposes, so that it sorts to the end of arrays. This change is for consistency
with ``NaN`` sorting behavior.
(`gh-12658 <https://github.com/numpy/numpy/pull/12658>`__)
(`gh-15068 <https://github.com/numpy/numpy/pull/15068>`__)

Incorrect ``threshold`` in ``np.set_printoptions`` raises ``TypeError`` or ``ValueError``
-----------------------------------------------------------------------------------------
Previously an incorrect ``threshold`` raised ``ValueError``; it now raises ``TypeError``
for non-numeric types and ``ValueError`` for ``nan`` values.
(`gh-13899 <https://github.com/numpy/numpy/pull/13899>`__)

Warn when saving a dtype with metadata
--------------------------------------
A ``UserWarning`` will be emitted when saving an array via ``numpy.save`` with
``metadata``. Saving such an array may not preserve metadata, and if metadata
is preserved, loading it will cause a ``ValueError``. This shortcoming in save
and load will be addressed in a future release.
(`gh-14142 <https://github.com/numpy/numpy/pull/14142>`__)

``numpy.distutils`` append behavior changed for LDFLAGS and similar
-------------------------------------------------------------------
`numpy.distutils` has always overridden rather than appended to ``LDFLAGS`` and
other similar such environment variables for compiling Fortran extensions. Now
the default behavior has changed to appending - which is the expected behavior
in most situations.  To preserve the old (overwriting) behavior, set the
``NPY_DISTUTILS_APPEND_FLAGS`` environment variable to 0.  This applies to:
``LDFLAGS``, ``F77FLAGS``, ``F90FLAGS``, ``FREEFLAGS``, ``FOPT``, ``FDEBUG``,
and ``FFLAGS``. NumPy 1.16 and 1.17 gave build warnings in situations where this
change in behavior would have affected the compile flags used.
(`gh-14248 <https://github.com/numpy/numpy/pull/14248>`__)

Remove ``numpy.random.entropy`` without a deprecation
-----------------------------------------------------

``numpy.random.entropy`` was added to the ``numpy.random`` namespace in 1.17.0.
It was meant to be a private c-extension module, but was exposed as public.
It has been replaced by ``numpy.random.SeedSequence`` so the module was
completely removed.
(`gh-14498 <https://github.com/numpy/numpy/pull/14498>`__)

Add options to quiet build configuration and build with ``-Werror``
-------------------------------------------------------------------
Added two new configuration options. During the ``build_src`` subcommand, as
part of configuring NumPy, the files ``_numpyconfig.h`` and ``config.h`` are
created by probing support for various runtime functions and routines.
Previously, the very verbose compiler output during this stage clouded more
important information. By default the output is silenced. Running
``runtests.py --debug-info`` will add ``--verbose-cfg`` to the ``build_src``
subcommand,which will restore the previous behaviour.

Adding ``CFLAGS=-Werror`` to turn warnings into errors would trigger errors
during the configuration. Now ``runtests.py --warn-error`` will add
``--warn-error`` to the ``build`` subcommand, which will percolate to the
``build_ext`` and ``build_lib`` subcommands. This will add the compiler flag
to those stages and turn compiler warnings into errors while actually building
NumPy itself, avoiding the ``build_src`` subcommand compiler calls.

(`gh-14527 <https://github.com/numpy/numpy/pull/14527>`__)
(`gh-14518 <https://github.com/numpy/numpy/pull/14518>`__)
=========================
NumPy 1.8.0 Release Notes
=========================

This release supports  Python 2.6 -2.7 and 3.2 - 3.3.


Highlights
==========


* New, no 2to3, Python 2 and Python 3 are supported by a common code base.
* New, gufuncs for linear algebra, enabling operations on stacked arrays.
* New, inplace fancy indexing for ufuncs with the ``.at`` method.
* New, ``partition`` function, partial sorting via selection for fast median.
* New, ``nanmean``, ``nanvar``, and ``nanstd`` functions skipping NaNs.
* New, ``full`` and ``full_like`` functions to create value initialized arrays.
* New, ``PyUFunc_RegisterLoopForDescr``, better ufunc support for user dtypes.
* Numerous performance improvements in many areas.


Dropped Support
===============


Support for Python versions 2.4 and 2.5 has been dropped,

Support for SCons has been removed.


Future Changes
==============


The Datetime64 type remains experimental in this release. In 1.9 there will
probably be some changes to make it more usable.

The diagonal method currently returns a new array and raises a
FutureWarning. In 1.9 it will return a readonly view.

Multiple field selection from an array of structured type currently
returns a new array and raises a FutureWarning. In 1.9 it will return a
readonly view.

The numpy/oldnumeric and numpy/numarray compatibility modules will be
removed in 1.9.


Compatibility notes
===================


The doc/sphinxext content has been moved into its own github repository,
and is included in numpy as a submodule. See the instructions in
doc/HOWTO_BUILD_DOCS.rst.txt for how to access the content.

.. _numpydoc: https://github.com/numpy/numpydoc

The hash function of numpy.void scalars has been changed.  Previously the
pointer to the data was hashed as an integer.  Now, the hash function uses
the tuple-hash algorithm to combine the hash functions of the elements of
the scalar, but only if the scalar is read-only.

Numpy has switched its build system to using 'separate compilation' by
default.  In previous releases this was supported, but not default. This
should produce the same results as the old system, but if you're trying to
do something complicated like link numpy statically or using an unusual
compiler, then it's possible you will encounter problems. If so, please
file a bug and as a temporary workaround you can re-enable the old build
system by exporting the shell variable NPY_SEPARATE_COMPILATION=0.

For the AdvancedNew iterator the ``oa_ndim`` flag should now be -1 to indicate
that no ``op_axes`` and ``itershape`` are passed in. The ``oa_ndim == 0``
case, now indicates a 0-D iteration and ``op_axes`` being NULL and the old
usage is deprecated. This does not effect the ``NpyIter_New`` or
``NpyIter_MultiNew`` functions.

The functions nanargmin and nanargmax now return np.iinfo['intp'].min for
the index in all-NaN slices. Previously the functions would raise a ValueError
for array returns and NaN for scalar returns.

NPY_RELAXED_STRIDES_CHECKING
----------------------------
There is a new compile time environment variable
``NPY_RELAXED_STRIDES_CHECKING``. If this variable is set to 1, then
numpy will consider more arrays to be C- or F-contiguous -- for
example, it becomes possible to have a column vector which is
considered both C- and F-contiguous simultaneously. The new definition
is more accurate, allows for faster code that makes fewer unnecessary
copies, and simplifies numpy's code internally. However, it may also
break third-party libraries that make too-strong assumptions about the
stride values of C- and F-contiguous arrays. (It is also currently
known that this breaks Cython code using memoryviews, which will be
fixed in Cython.) THIS WILL BECOME THE DEFAULT IN A FUTURE RELEASE, SO
PLEASE TEST YOUR CODE NOW AGAINST NUMPY BUILT WITH::

  NPY_RELAXED_STRIDES_CHECKING=1 python setup.py install

You can check whether NPY_RELAXED_STRIDES_CHECKING is in effect by
running::

  np.ones((10, 1), order="C").flags.f_contiguous

This will be ``True`` if relaxed strides checking is enabled, and
``False`` otherwise. The typical problem we've seen so far is C code
that works with C-contiguous arrays, and assumes that the itemsize can
be accessed by looking at the last element in the ``PyArray_STRIDES(arr)``
array. When relaxed strides are in effect, this is not true (and in
fact, it never was true in some corner cases). Instead, use
``PyArray_ITEMSIZE(arr)``.

For more information check the "Internal memory layout of an ndarray"
section in the documentation.

Binary operations with non-arrays as second argument
----------------------------------------------------
Binary operations of the form ``<array-or-subclass> * <non-array-subclass>``
where ``<non-array-subclass>`` declares an ``__array_priority__`` higher than
that of ``<array-or-subclass>`` will now unconditionally return
*NotImplemented*, giving ``<non-array-subclass>`` a chance to handle the
operation.  Previously, `NotImplemented` would only be returned if
``<non-array-subclass>`` actually implemented the reversed operation, and after
a (potentially expensive) array conversion of ``<non-array-subclass>`` had been
attempted. (`bug <https://github.com/numpy/numpy/issues/3375>`_, `pull request
<https://github.com/numpy/numpy/pull/3501>`_)

Function `median` used with `overwrite_input` only partially sorts array
------------------------------------------------------------------------
If `median` is used with `overwrite_input` option the input array will now only
be partially sorted instead of fully sorted.

Fix to financial.npv
--------------------
The npv function had a bug. Contrary to what the documentation stated, it
summed from indexes ``1`` to ``M`` instead of from ``0`` to ``M - 1``. The
fix changes the returned value. The mirr function called the npv function,
but worked around the problem, so that was also fixed and the return value
of the mirr function remains unchanged.

Runtime warnings when comparing NaN numbers
-------------------------------------------
Comparing ``NaN`` floating point numbers now raises the ``invalid`` runtime
warning. If a ``NaN`` is expected the warning can be ignored using np.errstate.
E.g.::

  with np.errstate(invalid='ignore'):
      operation()


New Features
============


Support for linear algebra on stacked arrays
--------------------------------------------
The gufunc machinery is now used for np.linalg, allowing operations on
stacked arrays and vectors. For example::

    >>> a
    array([[[ 1.,  1.],
            [ 0.,  1.]],

           [[ 1.,  1.],
            [ 0.,  1.]]])

    >>> np.linalg.inv(a)
    array([[[ 1., -1.],
            [ 0.,  1.]],

           [[ 1., -1.],
            [ 0.,  1.]]])

In place fancy indexing for ufuncs
----------------------------------
The function ``at`` has been added to ufunc objects to allow in place
ufuncs with no buffering when fancy indexing is used. For example, the
following will increment the first and second items in the array, and will
increment the third item twice: ``numpy.add.at(arr, [0, 1, 2, 2], 1)``

This is what many have mistakenly thought ``arr[[0, 1, 2, 2]] += 1`` would do,
but that does not work as the incremented value of ``arr[2]`` is simply copied
into the third slot in ``arr`` twice, not incremented twice.

New functions `partition` and `argpartition`
--------------------------------------------
New functions to partially sort arrays via a selection algorithm.

A ``partition`` by index ``k`` moves the ``k`` smallest element to the front of
an array.  All elements before ``k`` are then smaller or equal than the value
in position ``k`` and all elements following ``k`` are then greater or equal
than the value in position ``k``. The ordering of the values within these
bounds is undefined.
A sequence of indices can be provided to sort all of them into their sorted
position at once iterative partitioning.
This can be used to efficiently obtain order statistics like median or
percentiles of samples.
``partition`` has a linear time complexity of ``O(n)`` while a full sort has
``O(n log(n))``.

New functions `nanmean`, `nanvar` and `nanstd`
----------------------------------------------
New nan aware statistical functions are added. In these functions the
results are what would be obtained if nan values were omitted from all
computations.

New functions `full` and `full_like`
------------------------------------
New convenience functions to create arrays filled with a specific value;
complementary to the existing `zeros` and `zeros_like` functions.

IO compatibility with large files
---------------------------------
Large NPZ files >2GB can be loaded on 64-bit systems.

Building against OpenBLAS
-------------------------
It is now possible to build numpy against OpenBLAS by editing site.cfg.

New constant
------------
Euler's constant is now exposed in numpy as euler_gamma.

New modes for qr
----------------
New modes 'complete', 'reduced', and 'raw' have been added to the qr
factorization and the old 'full' and 'economic' modes are deprecated.
The 'reduced' mode replaces the old 'full' mode and is the default as was
the 'full' mode, so backward compatibility can be maintained by not
specifying the mode.

The 'complete' mode returns a full dimensional factorization, which can be
useful for obtaining a basis for the orthogonal complement of the range
space. The 'raw' mode returns arrays that contain the Householder
reflectors and scaling factors that can be used in the future to apply q
without needing to convert to a matrix. The 'economic' mode is simply
deprecated, there isn't much use for it and it isn't any more efficient
than the 'raw' mode.

New `invert` argument to `in1d`
-------------------------------
The function `in1d` now accepts a `invert` argument which, when `True`,
causes the returned array to be inverted.

Advanced indexing using `np.newaxis`
------------------------------------
It is now possible to use `np.newaxis`/`None` together with index
arrays instead of only in simple indices. This means that
``array[np.newaxis, [0, 1]]`` will now work as expected and select the first
two rows while prepending a new axis to the array.


C-API
-----
New ufuncs can now be registered with builtin input types and a custom
output type. Before this change, NumPy wouldn't be able to find the right
ufunc loop function when the ufunc was called from Python, because the ufunc
loop signature matching logic wasn't looking at the output operand type.
Now the correct ufunc loop is found, as long as the user provides an output
argument with the correct output type.

runtests.py
-----------
A simple test runner script ``runtests.py`` was added. It also builds Numpy via
``setup.py build`` and can be used to run tests easily during development.


Improvements
============

IO performance improvements
---------------------------
Performance in reading large files was improved by chunking (see also IO compatibility).

Performance improvements to `pad`
---------------------------------
The `pad` function has a new implementation, greatly improving performance for
all inputs except `mode=<function>` (retained for backwards compatibility).
Scaling with dimensionality is dramatically improved for rank >= 4.

Performance improvements to `isnan`, `isinf`, `isfinite` and `byteswap`
-----------------------------------------------------------------------
`isnan`, `isinf`, `isfinite` and `byteswap` have been improved to take
advantage of compiler builtins to avoid expensive calls to libc.
This improves performance of these operations by about a factor of two on gnu
libc systems.

Performance improvements via SSE2 vectorization
-----------------------------------------------
Several functions have been optimized to make use of SSE2 CPU SIMD instructions.

* Float32 and float64:
    * base math (`add`, `subtract`, `divide`, `multiply`)
    * `sqrt`
    * `minimum/maximum`
    * `absolute`
* Bool:
    * `logical_or`
    * `logical_and`
    * `logical_not`

This improves performance of these operations up to 4x/2x for float32/float64
and up to 10x for bool depending on the location of the data in the CPU caches.
The performance gain is greatest for in-place operations.

In order to use the improved functions the SSE2 instruction set must be enabled
at compile time. It is enabled by default on x86_64 systems. On x86_32 with a
capable CPU it must be enabled by passing the appropriate flag to the CFLAGS
build variable (-msse2 with gcc).

Performance improvements to `median`
------------------------------------
`median` is now implemented in terms of `partition` instead of `sort` which
reduces its time complexity from O(n log(n)) to O(n).
If used with the `overwrite_input` option the array will now only be partially
sorted instead of fully sorted.


Overridable operand flags in ufunc C-API
----------------------------------------
When creating a ufunc, the default ufunc operand flags can be overridden
via the new op_flags attribute of the ufunc object. For example, to set
the operand flag for the first input to read/write:

PyObject \*ufunc = PyUFunc_FromFuncAndData(...);
ufunc->op_flags[0] = NPY_ITER_READWRITE;

This allows a ufunc to perform an operation in place. Also, global nditer flags
can be overridden via the new iter_flags attribute of the ufunc object.
For example, to set the reduce flag for a ufunc:

ufunc->iter_flags = NPY_ITER_REDUCE_OK;


Changes
=======


General
-------
The function np.take now allows 0-d arrays as indices.

The separate compilation mode is now enabled by default.

Several changes to np.insert and np.delete:

* Previously, negative indices and indices that pointed past the end of
  the array were simply ignored. Now, this will raise a Future or Deprecation
  Warning. In the future they will be treated like normal indexing treats
  them -- negative indices will wrap around, and out-of-bound indices will
  generate an error.
* Previously, boolean indices were treated as if they were integers (always
  referring to either the 0th or 1st item in the array). In the future, they
  will be treated as masks. In this release, they raise a FutureWarning
  warning of this coming change.
* In Numpy 1.7. np.insert already allowed the syntax
  `np.insert(arr, 3, [1,2,3])` to insert multiple items at a single position.
  In Numpy 1.8. this is also possible for `np.insert(arr, [3], [1, 2, 3])`.

Padded regions from np.pad are now correctly rounded, not truncated.

C-API Array Additions
---------------------
Four new functions have been added to the array C-API.

* PyArray_Partition
* PyArray_ArgPartition
* PyArray_SelectkindConverter
* PyDataMem_NEW_ZEROED

C-API Ufunc Additions
---------------------
One new function has been added to the ufunc C-API that allows to register
an inner loop for user types using the descr.

* PyUFunc_RegisterLoopForDescr

C-API Developer Improvements
----------------------------
The ``PyArray_Type`` instance creation function ``tp_new`` now
uses ``tp_basicsize`` to determine how much memory to allocate.
In previous releases only ``sizeof(PyArrayObject)`` bytes of
memory were allocated, often requiring C-API subtypes to
reimplement ``tp_new``.

Deprecations
============

The 'full' and 'economic' modes of qr factorization are deprecated.

General
-------
The use of non-integer for indices and most integer arguments has been
deprecated. Previously float indices and function arguments such as axes or
shapes were truncated to integers without warning. For example
`arr.reshape(3., -1)` or `arr[0.]` will trigger a deprecation warning in
NumPy 1.8., and in some future version of NumPy they will raise an error.


Authors
=======

This release contains work by the following people who contributed at least
one patch to this release. The names are in alphabetical order by first name:

* 87
* Adam Ginsburg +
* Adam Griffiths +
* Alexander Belopolsky +
* Alex Barth +
* Alex Ford +
* Andreas Hilboll +
* Andreas Kloeckner +
* Andreas Schwab +
* Andrew Horton +
* argriffing +
* Arink Verma +
* Bago Amirbekian +
* Bartosz Telenczuk +
* bebert218 +
* Benjamin Root +
* Bill Spotz +
* Bradley M. Froehle
* Carwyn Pelley +
* Charles Harris
* Chris
* Christian Brueffer +
* Christoph Dann +
* Christoph Gohlke
* Dan Hipschman +
* Daniel +
* Dan Miller +
* daveydave400 +
* David Cournapeau
* David Warde-Farley
* Denis Laxalde
* dmuellner +
* Edward Catmur +
* Egor Zindy +
* endolith
* Eric Firing
* Eric Fode
* Eric Moore +
* Eric Price +
* Fazlul Shahriar +
* Félix Hartmann +
* Fernando Perez
* Frank B +
* Frank Breitling +
* Frederic
* Gabriel
* GaelVaroquaux
* Guillaume Gay +
* Han Genuit
* HaroldMills +
* hklemm +
* jamestwebber +
* Jason Madden +
* Jay Bourque
* jeromekelleher +
* Jesús Gómez +
* jmozmoz +
* jnothman +
* Johannes Schönberger +
* John Benediktsson +
* John Salvatier +
* John Stechschulte +
* Jonathan Waltman +
* Joon Ro +
* Jos de Kloe +
* Joseph Martinot-Lagarde +
* Josh Warner (Mac) +
* Jostein Bø Fløystad +
* Juan Luis Cano Rodríguez +
* Julian Taylor +
* Julien Phalip +
* K.-Michael Aye +
* Kumar Appaiah +
* Lars Buitinck
* Leon Weber +
* Luis Pedro Coelho
* Marcin Juszkiewicz
* Mark Wiebe
* Marten van Kerkwijk +
* Martin Baeuml +
* Martin Spacek
* Martin Teichmann +
* Matt Davis +
* Matthew Brett
* Maximilian Albert +
* m-d-w +
* Michael Droettboom
* mwtoews +
* Nathaniel J. Smith
* Nicolas Scheffer +
* Nils Werner +
* ochoadavid +
* Ondřej Čertík
* ovillellas +
* Paul Ivanov
* Pauli Virtanen
* peterjc
* Ralf Gommers
* Raul Cota +
* Richard Hattersley +
* Robert Costa +
* Robert Kern
* Rob Ruana +
* Ronan Lamy
* Sandro Tosi
* Sascha Peilicke +
* Sebastian Berg
* Skipper Seabold
* Stefan van der Walt
* Steve +
* Takafumi Arakaki +
* Thomas Robitaille +
* Tomas Tomecek +
* Travis E. Oliphant
* Valentin Haenel
* Vladimir Rutsky +
* Warren Weckesser
* Yaroslav Halchenko
* Yury V. Zaytsev +

A total of 119 people contributed to this release.
People with a "+" by their names contributed a patch for the first time.
.. currentmodule:: numpy

==========================
NumPy 1.19.1 Release Notes
==========================

NumPy 1.19.1 fixes several bugs found in the 1.19.0 release, replaces several
functions deprecated in the upcoming Python-3.9 release, has improved support
for AIX, and has a number of development related updates to keep CI working
with recent upstream changes.

This release supports Python 3.6-3.8. Cython >= 0.29.21 needs to be used when
building with Python 3.9 for testing purposes.


Contributors
============

A total of 15 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Abhinav Reddy +
* Anirudh Subramanian
* Antonio Larrosa +
* Charles Harris
* Chunlin Fang
* Eric Wieser
* Etienne Guesnet +
* Kevin Sheppard
* Matti Picus
* Raghuveer Devulapalli
* Roman Yurchak
* Ross Barnowski
* Sayed Adel
* Sebastian Berg
* Tyler Reddy


Pull requests merged
====================

A total of 25 pull requests were merged for this release.

* `#16649 <https://github.com/numpy/numpy/pull/16649>`__: MAINT, CI: disable Shippable cache
* `#16652 <https://github.com/numpy/numpy/pull/16652>`__: MAINT: Replace `PyUString_GET_SIZE` with `PyUnicode_GetLength`.
* `#16654 <https://github.com/numpy/numpy/pull/16654>`__: REL: Fix outdated docs link
* `#16656 <https://github.com/numpy/numpy/pull/16656>`__: BUG: raise IEEE exception on AIX
* `#16672 <https://github.com/numpy/numpy/pull/16672>`__: BUG: Fix bug in AVX complex absolute while processing array of...
* `#16693 <https://github.com/numpy/numpy/pull/16693>`__: TST: Add extra debugging information to CPU features detection
* `#16703 <https://github.com/numpy/numpy/pull/16703>`__: BLD: Add CPU entry for Emscripten / WebAssembly
* `#16705 <https://github.com/numpy/numpy/pull/16705>`__: TST: Disable Python 3.9-dev testing.
* `#16714 <https://github.com/numpy/numpy/pull/16714>`__: MAINT: Disable use_hugepages in case of ValueError
* `#16724 <https://github.com/numpy/numpy/pull/16724>`__: BUG: Fix PyArray_SearchSorted signature.
* `#16768 <https://github.com/numpy/numpy/pull/16768>`__: MAINT: Fixes for deprecated functions in scalartypes.c.src
* `#16772 <https://github.com/numpy/numpy/pull/16772>`__: MAINT: Remove unneeded call to PyUnicode_READY
* `#16776 <https://github.com/numpy/numpy/pull/16776>`__: MAINT: Fix deprecated functions in scalarapi.c
* `#16779 <https://github.com/numpy/numpy/pull/16779>`__: BLD, ENH: Add RPATH support for AIX
* `#16780 <https://github.com/numpy/numpy/pull/16780>`__: BUG: Fix default fallback in genfromtxt
* `#16784 <https://github.com/numpy/numpy/pull/16784>`__: BUG: Added missing return after raising error in methods.c
* `#16795 <https://github.com/numpy/numpy/pull/16795>`__: BLD: update cython to 0.29.21
* `#16832 <https://github.com/numpy/numpy/pull/16832>`__: MAINT: setuptools 49.2.0 emits a warning, avoid it
* `#16872 <https://github.com/numpy/numpy/pull/16872>`__: BUG: Validate output size in bin- and multinomial
* `#16875 <https://github.com/numpy/numpy/pull/16875>`__: BLD, MAINT: Pin setuptools
* `#16904 <https://github.com/numpy/numpy/pull/16904>`__: DOC: Reconstruct Testing Guideline.
* `#16905 <https://github.com/numpy/numpy/pull/16905>`__: TST, BUG: Re-raise MemoryError exception in test_large_zip's...
* `#16906 <https://github.com/numpy/numpy/pull/16906>`__: BUG,DOC: Fix bad MPL kwarg.
* `#16916 <https://github.com/numpy/numpy/pull/16916>`__: BUG: Fix string/bytes to complex assignment
* `#16922 <https://github.com/numpy/numpy/pull/16922>`__: REL: Prepare for NumPy 1.19.1 release
==========================
NumPy 1.14.3 Release Notes
==========================

This is a bugfix release for a few bugs reported following the 1.14.2 release:

* np.lib.recfunctions.fromrecords accepts a list-of-lists, until 1.15
* In python2, float types use the new print style when printing to a file
* style arg in "legacy" print mode now works for 0d arrays

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python
3.6 wheels available from PIP are built with Python 3.6.2 and should be
compatible with all previous versions of Python 3.6. The source releases were
cythonized with Cython 0.28.2.

Contributors
============

A total of 6 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Jonathan March +
* Malcolm Smith +
* Matti Picus
* Pauli Virtanen

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#10862 <https://github.com/numpy/numpy/pull/10862>`__: BUG: floating types should override tp_print (1.14 backport)
* `#10905 <https://github.com/numpy/numpy/pull/10905>`__: BUG: for 1.14 back-compat, accept list-of-lists in fromrecords
* `#10947 <https://github.com/numpy/numpy/pull/10947>`__: BUG: 'style' arg to array2string broken in legacy mode (1.14...
* `#10959 <https://github.com/numpy/numpy/pull/10959>`__: BUG: test, fix for missing flags['WRITEBACKIFCOPY'] key
* `#10960 <https://github.com/numpy/numpy/pull/10960>`__: BUG: Add missing underscore to prototype in check_embedded_lapack
* `#10961 <https://github.com/numpy/numpy/pull/10961>`__: BUG: Fix encoding regression in ma/bench.py (Issue #10868)
* `#10962 <https://github.com/numpy/numpy/pull/10962>`__: BUG: core: fix NPY_TITLE_KEY macro on pypy
* `#10974 <https://github.com/numpy/numpy/pull/10974>`__: BUG: test, fix PyArray_DiscardWritebackIfCopy...
.. currentmodule:: numpy

==========================
NumPy 1.17.0 Release Notes
==========================

This NumPy release contains a number of new features that should substantially
improve its performance and usefulness, see Highlights below for a summary. The
Python versions supported are 3.5-3.7, note that Python 2.7 has been dropped.
Python 3.8b2 should work with the released source packages, but there are no
future guarantees.

Downstream developers should use Cython >= 0.29.11 for Python 3.8 support and
OpenBLAS >= 3.7 (not currently out) to avoid problems on the Skylake
architecture. The NumPy wheels on PyPI are built from the OpenBLAS development
branch in order to avoid those problems.


Highlights
==========

* A new extensible `random` module along with four selectable `random number
  generators <random.BitGenerators>` and improved seeding designed for use in parallel
  processes has been added. The currently available bit generators are `MT19937
  <random.mt19937.MT19937>`, `PCG64 <random.pcg64.PCG64>`, `Philox
  <random.philox.Philox>`, and `SFC64 <random.sfc64.SFC64>`. See below under
  New Features.

* NumPy's `FFT <fft>` implementation was changed from fftpack to pocketfft,
  resulting in faster, more accurate transforms and better handling of datasets
  of prime length. See below under Improvements.

* New radix sort and timsort sorting methods. It is currently not possible to
  choose which will be used. They are hardwired to the datatype and used
  when either ``stable`` or ``mergesort`` is passed as the method. See below
  under Improvements.

* Overriding numpy functions is now possible by default,
  see ``__array_function__`` below.


New functions
=============

* `numpy.errstate` is now also a function decorator


Deprecations
============

`numpy.polynomial` functions warn when passed ``float`` in place of ``int``
---------------------------------------------------------------------------
Previously functions in this module would accept ``float`` values provided they
were integral (``1.0``, ``2.0``, etc). For consistency with the rest of numpy,
doing so is now deprecated, and in future will raise a ``TypeError``.

Similarly, passing a float like ``0.5`` in place of an integer will now raise a
``TypeError`` instead of the previous ``ValueError``.

Deprecate `numpy.distutils.exec_command` and ``temp_file_name``
---------------------------------------------------------------
The internal use of these functions has been refactored and there are better
alternatives. Replace ``exec_command`` with `subprocess.Popen` and
`temp_file_name <numpy.distutils.exec_command>` with `tempfile.mkstemp`.

Writeable flag of C-API wrapped arrays
--------------------------------------
When an array is created from the C-API to wrap a pointer to data, the only
indication we have of the read-write nature of the data is the ``writeable``
flag set during creation. It is dangerous to force the flag to writeable.
In the future it will not be possible to switch the writeable flag to ``True``
from python.
This deprecation should not affect many users since arrays created in such
a manner are very rare in practice and only available through the NumPy C-API.

`numpy.nonzero` should no longer be called on 0d arrays
-------------------------------------------------------
The behavior of `numpy.nonzero` on 0d arrays was surprising, making uses of it
almost always incorrect. If the old behavior was intended, it can be preserved
without a warning by using ``nonzero(atleast_1d(arr))`` instead of
``nonzero(arr)``.  In a future release, it is most likely this will raise a
``ValueError``.

Writing to the result of `numpy.broadcast_arrays` will warn
-----------------------------------------------------------

Commonly `numpy.broadcast_arrays` returns a writeable array with internal
overlap, making it unsafe to write to. A future version will set the
``writeable`` flag to ``False``, and require users to manually set it to
``True`` if they are sure that is what they want to do. Now writing to it will
emit a deprecation warning with instructions to set the ``writeable`` flag
``True``.  Note that if one were to inspect the flag before setting it, one
would find it would already be ``True``.  Explicitly setting it, though, as one
will need to do in future versions, clears an internal flag that is used to
produce the deprecation warning. To help alleviate confusion, an additional
`FutureWarning` will be emitted when accessing the ``writeable`` flag state to
clarify the contradiction.

Note that for the C-side buffer protocol such an array will return a
readonly buffer immediately unless a writable buffer is requested. If
a writeable buffer is requested a warning will be given. When using
cython, the ``const`` qualifier should be used with such arrays to avoid
the warning (e.g. ``cdef const double[::1] view``).


Future Changes
==============

Shape-1 fields in dtypes won't be collapsed to scalars in a future version
--------------------------------------------------------------------------

Currently, a field specified as ``[(name, dtype, 1)]`` or ``"1type"`` is
interpreted as a scalar field (i.e., the same as ``[(name, dtype)]`` or
``[(name, dtype, ()]``). This now raises a FutureWarning; in a future version,
it will be interpreted as a shape-(1,) field, i.e. the same as ``[(name,
dtype, (1,))]`` or ``"(1,)type"`` (consistently with ``[(name, dtype, n)]``
/ ``"ntype"`` with ``n>1``, which is already equivalent to ``[(name, dtype,
(n,)]`` / ``"(n,)type"``).


Compatibility notes
===================

``float16`` subnormal rounding
------------------------------
Casting from a different floating point precision to ``float16`` used incorrect
rounding in some edge cases. This means in rare cases, subnormal results will
now be rounded up instead of down, changing the last bit (ULP) of the result.

Signed zero when using divmod
-----------------------------
Starting in version `1.12.0`, numpy incorrectly returned a negatively signed zero
when using the ``divmod`` and ``floor_divide`` functions when the result was
zero. For example::

   >>> np.zeros(10)//1
   array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])

With this release, the result is correctly returned as a positively signed
zero::

   >>> np.zeros(10)//1
   array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

``MaskedArray.mask`` now returns a view of the mask, not the mask itself
------------------------------------------------------------------------
Returning the mask itself was unsafe, as it could be reshaped in place which
would violate expectations of the masked array code. The behavior of `mask
<ma.MaskedArray.mask>` is now consistent with `data <ma.MaskedArray.data>`,
which also returns a view.

The underlying mask can still be accessed with ``._mask`` if it is needed.
Tests that contain ``assert x.mask is not y.mask`` or similar will need to be
updated.

Do not lookup ``__buffer__`` attribute in `numpy.frombuffer`
------------------------------------------------------------
Looking up ``__buffer__`` attribute in `numpy.frombuffer` was undocumented and
non-functional. This code was removed. If needed, use
``frombuffer(memoryview(obj), ...)`` instead.

``out`` is buffered for memory overlaps in `take`, `choose`, `put`
------------------------------------------------------------------
If the out argument to these functions is provided and has memory overlap with
the other arguments, it is now buffered to avoid order-dependent behavior.

Unpickling while loading requires explicit opt-in
-------------------------------------------------
The functions `load`, and ``lib.format.read_array`` take an
``allow_pickle`` keyword which now defaults to ``False`` in response to
`CVE-2019-6446 <https://nvd.nist.gov/vuln/detail/CVE-2019-6446>`_.


.. currentmodule:: numpy.random

Potential changes to the random stream in old random module
-----------------------------------------------------------
Due to bugs in the application of ``log`` to random floating point numbers,
the stream may change when sampling from `~RandomState.beta`, `~RandomState.binomial`,
`~RandomState.laplace`, `~RandomState.logistic`, `~RandomState.logseries` or
`~RandomState.multinomial` if a ``0`` is generated in the underlying `MT19937`
random stream.  There is a ``1`` in
:math:`10^{53}` chance of this occurring, so the probability that the stream
changes for any given seed is extremely small. If a ``0`` is encountered in the
underlying generator, then the incorrect value produced (either `numpy.inf` or
`numpy.nan`) is now dropped.

.. currentmodule:: numpy

`i0` now always returns a result with the same shape as the input
-----------------------------------------------------------------
Previously, the output was squeezed, such that, e.g., input with just a single
element would lead to an array scalar being returned, and inputs with shapes
such as ``(10, 1)`` would yield results that would not broadcast against the
input.

Note that we generally recommend the SciPy implementation over the numpy one:
it is a proper ufunc written in C, and more than an order of magnitude faster.

`can_cast` no longer assumes all unsafe casting is allowed
----------------------------------------------------------
Previously, `can_cast` returned `True` for almost all inputs for
``casting='unsafe'``, even for cases where casting was not possible, such as
from a structured dtype to a regular one.  This has been fixed, making it
more consistent with actual casting using, e.g., the `.astype <ndarray.astype>`
method.

``ndarray.flags.writeable`` can be switched to true slightly more often
-----------------------------------------------------------------------

In rare cases, it was not possible to switch an array from not writeable
to writeable, although a base array is writeable. This can happen if an
intermediate `ndarray.base` object is writeable. Previously, only the deepest
base object was considered for this decision. However, in rare cases this
object does not have the necessary information. In that case switching to
writeable was never allowed. This has now been fixed.


C API changes
=============

dimension or stride input arguments are now passed by ``npy_intp const*``
-------------------------------------------------------------------------
Previously these function arguments were declared as the more strict
``npy_intp*``, which prevented the caller passing constant data.
This change is backwards compatible, but now allows code like::

    npy_intp const fixed_dims[] = {1, 2, 3};
    // no longer complains that the const-qualifier is discarded
    npy_intp size = PyArray_MultiplyList(fixed_dims, 3);


New Features
============

.. currentmodule:: numpy.random

New extensible `numpy.random` module with selectable random number generators
-----------------------------------------------------------------------------
A new extensible `numpy.random` module along with four selectable random number
generators and improved seeding designed for use in parallel processes has been
added. The currently available `Bit Generators` are
`~mt19937.MT19937`, `~pcg64.PCG64`, `~philox.Philox`, and `~sfc64.SFC64`.
``PCG64`` is the new default while ``MT19937`` is retained for backwards
compatibility. Note that the legacy random module is unchanged and is now
frozen, your current results will not change. More information is available in
the :ref:`API change description <new-or-different>` and in the `top-level view
<numpy.random>` documentation.

.. currentmodule:: numpy

libFLAME
--------
Support for building NumPy with the libFLAME linear algebra package as the LAPACK,
implementation, see
`libFLAME <https://www.cs.utexas.edu/~flame/web/libFLAME.html>`_ for details.

User-defined BLAS detection order
---------------------------------
`distutils` now uses an environment variable, comma-separated and case
insensitive, to determine the detection order for BLAS libraries.
By default ``NPY_BLAS_ORDER=mkl,blis,openblas,atlas,accelerate,blas``.
However, to force the use of OpenBLAS simply do::

   NPY_BLAS_ORDER=openblas python setup.py build

which forces the use of OpenBLAS.
This may be helpful for users which have a MKL installation but wishes to try
out different implementations.

User-defined LAPACK detection order
-----------------------------------
``numpy.distutils`` now uses an environment variable, comma-separated and case
insensitive, to determine the detection order for LAPACK libraries.
By default ``NPY_LAPACK_ORDER=mkl,openblas,flame,atlas,accelerate,lapack``.
However, to force the use of OpenBLAS simply do::

   NPY_LAPACK_ORDER=openblas python setup.py build

which forces the use of OpenBLAS.
This may be helpful for users which have a MKL installation but wishes to try
out different implementations.

`ufunc.reduce` and related functions now accept a ``where`` mask
----------------------------------------------------------------
`ufunc.reduce`, `sum`, `prod`, `min`, `max` all
now accept a ``where`` keyword argument, which can be used to tell which
elements to include in the reduction.  For reductions that do not have an
identity, it is necessary to also pass in an initial value (e.g.,
``initial=np.inf`` for `min`).  For instance, the equivalent of
`nansum` would be ``np.sum(a, where=~np.isnan(a))``.

Timsort and radix sort have replaced mergesort for stable sorting
-----------------------------------------------------------------
Both radix sort and timsort have been implemented and are now used in place of
mergesort. Due to the need to maintain backward compatibility, the sorting
``kind`` options ``"stable"`` and ``"mergesort"`` have been made aliases of
each other with the actual sort implementation depending on the array type.
Radix sort is used for small integer types of 16 bits or less and timsort for
the remaining types.  Timsort features improved performance on data containing
already or nearly sorted data and performs like mergesort on random data and
requires :math:`O(n/2)` working space.  Details of the timsort algorithm can be
found at `CPython listsort.txt
<https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.

`packbits` and `unpackbits` accept an ``order`` keyword
-------------------------------------------------------
The ``order`` keyword defaults to ``big``, and will order the **bits**
accordingly. For ``'order=big'`` 3 will become ``[0, 0, 0, 0, 0, 0, 1, 1]``,
and ``[1, 1, 0, 0, 0, 0, 0, 0]`` for ``order=little``

`unpackbits` now accepts a ``count`` parameter
----------------------------------------------
``count`` allows subsetting the number of bits that will be unpacked up-front,
rather than reshaping and subsetting later, making the `packbits` operation
invertible, and the unpacking less wasteful. Counts larger than the number of
available bits add zero padding. Negative counts trim bits off the end instead
of counting from the beginning. None counts implement the existing behavior of
unpacking everything.

`linalg.svd` and `linalg.pinv` can be faster on hermitian inputs
----------------------------------------------------------------
These functions now accept a ``hermitian`` argument, matching the one added
to `linalg.matrix_rank` in 1.14.0.

divmod operation is now supported for two ``timedelta64`` operands
------------------------------------------------------------------
The divmod operator now handles two ``timedelta64`` operands, with
type signature ``mm->qm``.

`fromfile` now takes an ``offset`` argument
-------------------------------------------
This function now takes an ``offset`` keyword argument for binary files,
which specifics the offset (in bytes) from the file's current position.
Defaults to ``0``.

New mode "empty" for `pad`
--------------------------
This mode pads an array to a desired shape without initializing the new
entries.

`empty_like` and related functions now accept a ``shape`` argument
------------------------------------------------------------------
`empty_like`, `full_like`, `ones_like` and `zeros_like` now accept a ``shape``
keyword argument, which can be used to create a new array
as the prototype, overriding its shape as well. This is particularly useful
when combined with the ``__array_function__`` protocol, allowing the creation
of new arbitrary-shape arrays from NumPy-like libraries when such an array
is used as the prototype.

Floating point scalars implement ``as_integer_ratio`` to match the builtin float
--------------------------------------------------------------------------------
This returns a (numerator, denominator) pair, which can be used to construct a
`fractions.Fraction`.

Structured ``dtype`` objects can be indexed with multiple fields names
----------------------------------------------------------------------
``arr.dtype[['a', 'b']]`` now returns a dtype that is equivalent to
``arr[['a', 'b']].dtype``, for consistency with
``arr.dtype['a'] == arr['a'].dtype``.

Like the dtype of structured arrays indexed with a list of fields, this dtype
has the same ``itemsize`` as the original, but only keeps a subset of the fields.

This means that ``arr[['a', 'b']]`` and ``arr.view(arr.dtype[['a', 'b']])`` are
equivalent.

``.npy`` files support unicode field names
------------------------------------------
A new format version of 3.0 has been introduced, which enables structured types
with non-latin1 field names. This is used automatically when needed.


Improvements
============

Array comparison assertions include maximum differences
-------------------------------------------------------
Error messages from array comparison tests such as
`testing.assert_allclose` now include "max absolute difference" and
"max relative difference," in addition to the previous "mismatch" percentage.
This information makes it easier to update absolute and relative error
tolerances.

Replacement of the fftpack based `fft` module by the pocketfft library
----------------------------------------------------------------------
Both implementations have the same ancestor (Fortran77 FFTPACK by Paul N.
Swarztrauber), but pocketfft contains additional modifications which improve
both accuracy and performance in some circumstances. For FFT lengths containing
large prime factors, pocketfft uses Bluestein's algorithm, which maintains
:math:`O(N log N)` run time complexity instead of deteriorating towards
:math:`O(N*N)` for prime lengths. Also, accuracy for real valued FFTs with near
prime lengths has improved and is on par with complex valued FFTs.

Further improvements to ``ctypes`` support in `numpy.ctypeslib`
---------------------------------------------------------------
A new `numpy.ctypeslib.as_ctypes_type` function has been added, which can be
used to converts a `dtype` into a best-guess `ctypes` type. Thanks to this
new function, `numpy.ctypeslib.as_ctypes` now supports a much wider range of
array types, including structures, booleans, and integers of non-native
endianness.

`numpy.errstate` is now also a function decorator
-------------------------------------------------
Currently, if you have a function like::

    def foo():
        pass

and you want to wrap the whole thing in `errstate`, you have to rewrite it
like so::

    def foo():
        with np.errstate(...):
            pass

but with this change, you can do::

    @np.errstate(...)
    def foo():
        pass

thereby saving a level of indentation

`numpy.exp` and `numpy.log` speed up for float32 implementation
---------------------------------------------------------------
float32 implementation of `exp` and `log` now benefit from AVX2/AVX512
instruction set which are detected during runtime. `exp` has a max ulp
error of 2.52 and `log` has a max ulp error or 3.83.

Improve performance of `numpy.pad`
----------------------------------
The performance of the function has been improved for most cases by filling in
a preallocated array with the desired padded shape instead of using
concatenation.

`numpy.interp` handles infinities more robustly
-----------------------------------------------
In some cases where `interp` would previously return `nan`, it now
returns an appropriate infinity.

Pathlib support for `fromfile`, `tofile` and `ndarray.dump`
-----------------------------------------------------------
`fromfile`, `ndarray.ndarray.tofile` and `ndarray.dump` now support
the `pathlib.Path` type for the ``file``/``fid`` parameter.

Specialized `isnan`, `isinf`, and `isfinite` ufuncs for bool and int types
--------------------------------------------------------------------------
The boolean and integer types are incapable of storing `nan` and `inf` values,
which allows us to provide specialized ufuncs that are up to 250x faster than
the previous approach.

`isfinite` supports ``datetime64`` and ``timedelta64`` types
-----------------------------------------------------------------
Previously, `isfinite` used to raise a `TypeError` on being used on these
two types.

New keywords added to `nan_to_num`
----------------------------------
`nan_to_num` now accepts keywords ``nan``, ``posinf`` and ``neginf``
allowing the user to define the value to replace the ``nan``, positive and
negative ``np.inf`` values respectively.

MemoryErrors caused by allocated overly large arrays are more descriptive
-------------------------------------------------------------------------
Often the cause of a MemoryError is incorrect broadcasting, which results in a
very large and incorrect shape. The message of the error now includes this
shape to help diagnose the cause of failure.

`floor`, `ceil`, and `trunc` now respect builtin magic methods
--------------------------------------------------------------
These ufuncs now call the ``__floor__``, ``__ceil__``, and ``__trunc__``
methods when called on object arrays, making them compatible with
`decimal.Decimal` and `fractions.Fraction` objects.

`quantile` now works on `fraction.Fraction` and `decimal.Decimal` objects
-------------------------------------------------------------------------
In general, this handles object arrays more gracefully, and avoids floating-
point operations if exact arithmetic types are used.

Support of object arrays in `matmul`
------------------------------------
It is now possible to use `matmul` (or the ``@`` operator) with object arrays.
For instance, it is now possible to do::

    from fractions import Fraction
    a = np.array([[Fraction(1, 2), Fraction(1, 3)], [Fraction(1, 3), Fraction(1, 2)]])
    b = a @ a


Changes
=======

`median` and `percentile` family of functions no longer warn about ``nan``
--------------------------------------------------------------------------
`numpy.median`, `numpy.percentile`, and `numpy.quantile` used to emit a
``RuntimeWarning`` when encountering an `nan`. Since they return the
``nan`` value, the warning is redundant and has been removed.

``timedelta64 % 0`` behavior adjusted to return ``NaT``
-------------------------------------------------------
The modulus operation with two ``np.timedelta64`` operands now returns
``NaT`` in the case of division by zero, rather than returning zero

NumPy functions now always support overrides with ``__array_function__``
------------------------------------------------------------------------
NumPy now always checks the ``__array_function__`` method to implement overrides
of NumPy functions on non-NumPy arrays, as described in `NEP 18`_. The feature
was available for testing with NumPy 1.16 if appropriate environment variables
are set, but is now always enabled.

.. _`NEP 18` : http://www.numpy.org/neps/nep-0018-array-function-protocol.html

``lib.recfunctions.structured_to_unstructured`` does not squeeze single-field views
-----------------------------------------------------------------------------------
Previously ``structured_to_unstructured(arr[['a']])`` would produce a squeezed
result inconsistent with ``structured_to_unstructured(arr[['a', b']])``. This
was accidental. The old behavior can be retained with
``structured_to_unstructured(arr[['a']]).squeeze(axis=-1)`` or far more simply,
``arr['a']``.

`clip` now uses a ufunc under the hood
--------------------------------------
This means that registering clip functions for custom dtypes in C via
``descr->f->fastclip`` is deprecated - they should use the ufunc registration
mechanism instead, attaching to the ``np.core.umath.clip`` ufunc.

It also means that ``clip`` accepts ``where`` and ``casting`` arguments,
and can be override with ``__array_ufunc__``.

A consequence of this change is that some behaviors of the old ``clip`` have
been deprecated:

* Passing ``nan`` to mean "do not clip" as one or both bounds. This didn't work
  in all cases anyway, and can be better handled by passing infinities of the
  appropriate sign.
* Using "unsafe" casting by default when an ``out`` argument is passed. Using
  ``casting="unsafe"`` explicitly will silence this warning.

Additionally, there are some corner cases with behavior changes:

* Padding ``max < min`` has changed to be more consistent across dtypes, but
  should not be relied upon.
* Scalar ``min`` and ``max`` take part in promotion rules like they do in all
  other ufuncs.

``__array_interface__`` offset now works as documented
------------------------------------------------------
The interface may use an ``offset`` value that was mistakenly ignored.

Pickle protocol in `savez` set to 3 for ``force zip64`` flag
-----------------------------------------------------------------
`savez` was not using the ``force_zip64`` flag, which limited the size of
the archive to 2GB. But using the flag requires us to use pickle protocol 3 to
write ``object`` arrays. The protocol used was bumped to 3, meaning the archive
will be unreadable by Python2.

Structured arrays indexed with non-existent fields raise ``KeyError`` not ``ValueError``
----------------------------------------------------------------------------------------
``arr['bad_field']`` on a structured type raises ``KeyError``, for consistency
with ``dict['bad_field']``.
=========================
NumPy 1.6.0 Release Notes
=========================

This release includes several new features as well as numerous bug fixes and
improved documentation.  It is backward compatible with the 1.5.0 release, and
supports Python 2.4 - 2.7 and 3.1 - 3.2.


Highlights
==========

* Re-introduction of datetime dtype support to deal with dates in arrays.

* A new 16-bit floating point type.

* A new iterator, which improves performance of many functions.


New features
============

New 16-bit floating point type
------------------------------

This release adds support for the IEEE 754-2008 binary16 format, available as
the data type ``numpy.half``.  Within Python, the type behaves similarly to
`float` or `double`, and C extensions can add support for it with the exposed
half-float API.


New iterator
------------

A new iterator has been added, replacing the functionality of the
existing iterator and multi-iterator with a single object and API.
This iterator works well with general memory layouts different from
C or Fortran contiguous, and handles both standard NumPy and
customized broadcasting. The buffering, automatic data type
conversion, and optional output parameters, offered by
ufuncs but difficult to replicate elsewhere, are now exposed by this
iterator.


Legendre, Laguerre, Hermite, HermiteE polynomials in ``numpy.polynomial``
-------------------------------------------------------------------------

Extend the number of polynomials available in the polynomial package. In
addition, a new ``window`` attribute has been added to the classes in
order to specify the range the ``domain`` maps to. This is mostly useful
for the Laguerre, Hermite, and HermiteE polynomials whose natural domains
are infinite and provides a more intuitive way to get the correct mapping
of values without playing unnatural tricks with the domain.


Fortran assumed shape array and size function support in ``numpy.f2py``
-----------------------------------------------------------------------

F2py now supports wrapping Fortran 90 routines that use assumed shape
arrays.  Before such routines could be called from Python but the
corresponding Fortran routines received assumed shape arrays as zero
length arrays which caused unpredicted results. Thanks to Lorenz
Hüdepohl for pointing out the correct way to interface routines with
assumed shape arrays.

In addition, f2py supports now automatic wrapping of Fortran routines
that use two argument ``size`` function in dimension specifications.


Other new functions
-------------------

``numpy.ravel_multi_index`` : Converts a multi-index tuple into
an array of flat indices, applying boundary modes to the indices.

``numpy.einsum`` : Evaluate the Einstein summation convention.  Using the
Einstein summation convention, many common multi-dimensional array operations
can be represented in a simple fashion.  This function provides a way compute
such summations.

``numpy.count_nonzero`` : Counts the number of non-zero elements in an array.

``numpy.result_type`` and ``numpy.min_scalar_type`` : These functions expose
the underlying type promotion used by the ufuncs and other operations to
determine the types of outputs. These improve upon the ``numpy.common_type``
and ``numpy.mintypecode`` which provide similar functionality but do
not match the ufunc implementation.


Changes
=======

``default error handling``
--------------------------

The default error handling has been change from ``print`` to ``warn`` for
all except for ``underflow``, which remains as ``ignore``.


``numpy.distutils``
-------------------

Several new compilers are supported for building Numpy: the Portland Group
Fortran compiler on OS X, the PathScale compiler suite and the 64-bit Intel C
compiler on Linux.


``numpy.testing``
-----------------

The testing framework gained ``numpy.testing.assert_allclose``, which provides
a more convenient way to compare floating point arrays than
`assert_almost_equal`, `assert_approx_equal` and `assert_array_almost_equal`.


``C API``
---------

In addition to the APIs for the new iterator and half data type, a number
of other additions have been made to the C API. The type promotion
mechanism used by ufuncs is exposed via ``PyArray_PromoteTypes``,
``PyArray_ResultType``, and ``PyArray_MinScalarType``. A new enumeration
``NPY_CASTING`` has been added which controls what types of casts are
permitted. This is used by the new functions ``PyArray_CanCastArrayTo``
and ``PyArray_CanCastTypeTo``.  A more flexible way to handle
conversion of arbitrary python objects into arrays is exposed by
``PyArray_GetArrayParamsFromObject``.


Deprecated features
===================

The "normed" keyword in ``numpy.histogram`` is deprecated. Its functionality
will be replaced by the new "density" keyword.


Removed features
================

``numpy.fft``
-------------

The functions `refft`, `refft2`, `refftn`, `irefft`, `irefft2`, `irefftn`,
which were aliases for the same functions without the 'e' in the name, were
removed.


``numpy.memmap``
----------------

The `sync()` and `close()` methods of memmap were removed.  Use `flush()` and
"del memmap" instead.


``numpy.lib``
-------------

The deprecated functions ``numpy.unique1d``, ``numpy.setmember1d``,
``numpy.intersect1d_nu`` and ``numpy.lib.ufunclike.log2`` were removed.


``numpy.ma``
------------

Several deprecated items were removed from the ``numpy.ma`` module::

  * ``numpy.ma.MaskedArray`` "raw_data" method
  * ``numpy.ma.MaskedArray`` constructor "flag" keyword
  * ``numpy.ma.make_mask`` "flag" keyword
  * ``numpy.ma.allclose`` "fill_value" keyword


``numpy.distutils``
-------------------

The ``numpy.get_numpy_include`` function was removed, use ``numpy.get_include``
instead.
==========================
NumPy 1.10.3 Release Notes
==========================

N/A this release did not happen due to various screwups involving PyPI.
==========================
NumPy 1.15.1 Release Notes
==========================

This is a bugfix release for bugs and regressions reported following the 1.15.0
release.

* The annoying but harmless RuntimeWarning that "numpy.dtype size changed" has
  been suppressed. The long standing suppression was lost in the transition to
  pytest.
* The update to Cython 0.28.3 exposed a problematic use of a gcc attribute used
  to prefer code size over speed in module initialization, possibly resulting in
  incorrect compiled code. This has been fixed in latest Cython but has been
  disabled here for safety.
* Support for big-endian and ARMv8 architectures has been improved.

The Python versions supported by this release are 2.7, 3.4-3.7. The wheels are
linked with OpenBLAS v0.3.0, which should fix some of the linalg problems
reported for NumPy 1.14.


Compatibility Note
==================

The NumPy 1.15.x OS X wheels released on PyPI no longer contain 32-bit
binaries.  That will also be the case in future releases. See
`#11625 <https://github.com/numpy/numpy/issues/11625>`__ for the related
discussion.  Those needing 32-bit support should look elsewhere or build
from source.


Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Chris Billington
* Elliott Sales de Andrade +
* Eric Wieser
* Jeremy Manning +
* Matti Picus
* Ralf Gommers

Pull requests merged
====================

A total of 24 pull requests were merged for this release.

* `#11647 <https://github.com/numpy/numpy/pull/11647>`__: MAINT: Filter Cython warnings in ``__init__.py``
* `#11648 <https://github.com/numpy/numpy/pull/11648>`__: BUG: Fix doc source links to unwrap decorators
* `#11657 <https://github.com/numpy/numpy/pull/11657>`__: BUG: Ensure singleton dimensions are not dropped when converting...
* `#11661 <https://github.com/numpy/numpy/pull/11661>`__: BUG: Warn on Nan in minimum,maximum for scalars
* `#11665 <https://github.com/numpy/numpy/pull/11665>`__: BUG: cython sometimes emits invalid gcc attribute
* `#11682 <https://github.com/numpy/numpy/pull/11682>`__: BUG: Fix regression in void_getitem
* `#11698 <https://github.com/numpy/numpy/pull/11698>`__: BUG: Make matrix_power again work for object arrays.
* `#11700 <https://github.com/numpy/numpy/pull/11700>`__: BUG: Add missing PyErr_NoMemory after failing malloc
* `#11719 <https://github.com/numpy/numpy/pull/11719>`__: BUG: Fix undefined functions on big-endian systems.
* `#11720 <https://github.com/numpy/numpy/pull/11720>`__: MAINT: Make einsum optimize default to False.
* `#11746 <https://github.com/numpy/numpy/pull/11746>`__: BUG: Fix regression in loadtxt for bz2 text files in Python 2.
* `#11757 <https://github.com/numpy/numpy/pull/11757>`__: BUG: Revert use of `console_scripts`.
* `#11758 <https://github.com/numpy/numpy/pull/11758>`__: BUG: Fix Fortran kind detection for aarch64 & s390x.
* `#11759 <https://github.com/numpy/numpy/pull/11759>`__: BUG: Fix printing of longdouble on ppc64le.
* `#11760 <https://github.com/numpy/numpy/pull/11760>`__: BUG: Fixes for unicode field names in Python 2
* `#11761 <https://github.com/numpy/numpy/pull/11761>`__: BUG: Increase required cython version on python 3.7
* `#11763 <https://github.com/numpy/numpy/pull/11763>`__: BUG: check return value of _buffer_format_string
* `#11775 <https://github.com/numpy/numpy/pull/11775>`__: MAINT: Make assert_array_compare more generic.
* `#11776 <https://github.com/numpy/numpy/pull/11776>`__: TST: Fix urlopen stubbing.
* `#11777 <https://github.com/numpy/numpy/pull/11777>`__: BUG: Fix regression in intersect1d.
* `#11779 <https://github.com/numpy/numpy/pull/11779>`__: BUG: Fix test sensitive to platform byte order.
* `#11781 <https://github.com/numpy/numpy/pull/11781>`__: BUG: Avoid signed overflow in histogram
* `#11785 <https://github.com/numpy/numpy/pull/11785>`__: BUG: Fix pickle and memoryview for datetime64, timedelta64 scalars
* `#11786 <https://github.com/numpy/numpy/pull/11786>`__: BUG: Deprecation triggers segfault
==========================
NumPy 1.16.0 Release Notes
==========================

This NumPy release is the last one to support Python 2.7 and will be maintained
as a long term release with bug fixes until 2020.  Support for Python 3.4 been
dropped, the supported Python versions are 2.7 and 3.5-3.7. The wheels on PyPI
are linked with OpenBLAS v0.3.4+,  which should fix the known threading issues
found in previous OpenBLAS versions.

Downstream developers building this release should use Cython >= 0.29 and, if
using OpenBLAS, OpenBLAS > v0.3.4.

This release has seen a lot of refactoring and features many bug fixes, improved
code organization, and better cross platform compatibility. Not all of these
improvements will be visible to users, but they should help make maintenance
easier going forward.


Highlights
==========

* Experimental (opt-in only) support for overriding numpy functions,
  see ``__array_function__`` below.

* The ``matmul`` function is now a ufunc. This provides better
  performance and allows overriding with ``__array_ufunc__``.

* Improved support for the ARM and POWER architectures.

* Improved support for AIX and PyPy.

* Improved interop with ctypes.

* Improved support for PEP 3118.



New functions
=============

* New functions added to the `numpy.lib.recfuntions` module to ease the
  structured assignment changes:

    * ``assign_fields_by_name``
    * ``structured_to_unstructured``
    * ``unstructured_to_structured``
    * ``apply_along_fields``
    * ``require_fields``

  See the user guide at <https://docs.scipy.org/doc/numpy/user/basics.rec.html>
  for more info.


New deprecations
================

* The type dictionaries `numpy.core.typeNA` and `numpy.core.sctypeNA` are
  deprecated. They were buggy and not documented and will be removed in the
  1.18 release. Use`numpy.sctypeDict` instead.

* The `numpy.asscalar` function is deprecated. It is an alias to the more
  powerful `numpy.ndarray.item`, not tested, and fails for scalars.

* The `numpy.set_array_ops` and `numpy.get_array_ops` functions are deprecated.
  As part of `NEP 15`, they have been deprecated along with the C-API functions
  :c:func:`PyArray_SetNumericOps` and :c:func:`PyArray_GetNumericOps`. Users
  who wish to override the inner loop functions in built-in ufuncs should use
  :c:func:`PyUFunc_ReplaceLoopBySignature`.

* The `numpy.unravel_index` keyword argument ``dims`` is deprecated, use
  ``shape`` instead.

* The `numpy.histogram` ``normed`` argument is deprecated.  It was deprecated
  previously, but no warning was issued.

* The ``positive`` operator (``+``) applied to non-numerical arrays is
  deprecated. See below for details.

* Passing an iterator to the stack functions is deprecated


Expired deprecations
====================

* NaT comparisons now return ``False`` without a warning, finishing a
  deprecation cycle begun in NumPy 1.11.

* ``np.lib.function_base.unique`` was removed, finishing a deprecation cycle
  begun in NumPy 1.4. Use `numpy.unique` instead.

* multi-field indexing now returns views instead of copies, finishing a
  deprecation cycle begun in NumPy 1.7. The change was previously attempted in
  NumPy 1.14 but reverted until now.

* ``np.PackageLoader`` and ``np.pkgload`` have been removed. These were
  deprecated in 1.10, had no tests, and seem to no longer work in 1.15.


Future changes
==============

* NumPy 1.17 will drop support for Python 2.7.


Compatibility notes
===================

f2py script on Windows
----------------------
On Windows, the installed script for running f2py is now an ``.exe`` file
rather than a ``*.py`` file and should be run from the command line as ``f2py``
whenever the ``Scripts`` directory is in the path. Running ``f2py`` as a module
``python -m numpy.f2py [...]`` will work without path modification in any
version of NumPy.

NaT comparisons
---------------
Consistent with the behavior of NaN, all comparisons other than inequality
checks with datetime64 or timedelta64 NaT ("not-a-time") values now always
return ``False``, and inequality checks with NaT now always return ``True``.
This includes comparisons between NaT values. For compatibility with the
old behavior, use ``np.isnat`` to explicitly check for NaT or convert
datetime64/timedelta64 arrays with ``.astype(np.int64)`` before making
comparisons.

complex64/128 alignment has changed
-----------------------------------
The memory alignment of complex types is now the same as a C-struct composed of
two floating point values, while before it was equal to the size of the type.
For many users (for instance on x64/unix/gcc) this means that complex64 is now
4-byte aligned instead of 8-byte aligned. An important consequence is that
aligned structured dtypes may now have a different size. For instance,
``np.dtype('c8,u1', align=True)`` used to have an itemsize of 16 (on x64/gcc)
but now it is 12.

More in detail, the complex64 type now has the same alignment as a C-struct
``struct {float r, i;}``, according to the compiler used to compile numpy, and
similarly for the complex128 and complex256 types.

nd_grid __len__ removal
-----------------------
``len(np.mgrid)`` and ``len(np.ogrid)`` are now considered nonsensical
and raise a ``TypeError``.

``np.unravel_index`` now accepts ``shape`` keyword argument
-----------------------------------------------------------
Previously, only the ``dims`` keyword argument was accepted
for specification of the shape of the array to be used
for unraveling. ``dims`` remains supported, but is now deprecated.

multi-field views return a view instead of a copy
-------------------------------------------------
Indexing a structured array with multiple fields, e.g., ``arr[['f1', 'f3']]``,
returns a view into the original array instead of a copy. The returned view
will often have extra padding bytes corresponding to intervening fields in the
original array, unlike before, which will affect code such as
``arr[['f1', 'f3']].view('float64')``. This change has been planned since numpy
1.7. Operations hitting this path have emitted ``FutureWarnings`` since then.
Additional ``FutureWarnings`` about this change were added in 1.12.

To help users update their code to account for these changes, a number of
functions have been added to the ``numpy.lib.recfunctions`` module which
safely allow such operations. For instance, the code above can be replaced
with ``structured_to_unstructured(arr[['f1', 'f3']], dtype='float64')``.
See the "accessing multiple fields" section of the
`user guide <https://docs.scipy.org/doc/numpy/user/basics.rec.html#accessing-multiple-fields>`__.


C API changes
=============

The :c:data:`NPY_FEATURE_VERSION` was incremented to 0x0000D, due to
the addition of:

* :c:member:`PyUFuncObject.core_dim_flags`
* :c:member:`PyUFuncObject.core_dim_sizes`
* :c:member:`PyUFuncObject.identity_value`
* :c:func:`PyUFunc_FromFuncAndDataAndSignatureAndIdentity`


New Features
============

Integrated squared error (ISE) estimator added to ``histogram``
---------------------------------------------------------------
This method (``bins='stone'``) for optimizing the bin number is a
generalization of the Scott's rule. The Scott's rule assumes the distribution
is approximately Normal, while the ISE_ is a non-parametric method based on
cross-validation.

.. _ISE: https://en.wikipedia.org/wiki/Histogram#Minimizing_cross-validation_estimated_squared_error

``max_rows`` keyword added for ``np.loadtxt``
---------------------------------------------
New keyword ``max_rows`` in `numpy.loadtxt` sets the maximum rows of the
content to be read after ``skiprows``, as in `numpy.genfromtxt`.

modulus operator support added for ``np.timedelta64`` operands
--------------------------------------------------------------
The modulus (remainder) operator is now supported for two operands
of type ``np.timedelta64``. The operands may have different units
and the return value will match the type of the operands.


Improvements
============

no-copy pickling of numpy arrays
--------------------------------
Up to protocol 4, numpy array pickling created 2 spurious copies of the data
being serialized.  With pickle protocol 5, and the ``PickleBuffer`` API, a
large variety of numpy arrays can now be serialized without any copy using
out-of-band buffers, and with one less copy using in-band buffers. This
results, for large arrays, in an up to 66% drop in peak memory usage.

build shell independence
------------------------
NumPy builds should no longer interact with the host machine
shell directly. ``exec_command`` has been replaced with
``subprocess.check_output`` where appropriate.

`np.polynomial.Polynomial` classes render in LaTeX in Jupyter notebooks
-----------------------------------------------------------------------
When used in a front-end that supports it, `Polynomial` instances are now
rendered through LaTeX. The current format is experimental, and is subject to
change.

``randint`` and ``choice`` now work on empty distributions
----------------------------------------------------------
Even when no elements needed to be drawn, ``np.random.randint`` and
``np.random.choice`` raised an error when the arguments described an empty
distribution. This has been fixed so that e.g.
``np.random.choice([], 0) == np.array([], dtype=float64)``.

``linalg.lstsq``, ``linalg.qr``, and ``linalg.svd`` now work with empty arrays
------------------------------------------------------------------------------
Previously, a ``LinAlgError`` would be raised when an empty matrix/empty
matrices (with zero rows and/or columns) is/are passed in. Now outputs of
appropriate shapes are returned.

Chain exceptions to give better error messages for invalid PEP3118 format strings
---------------------------------------------------------------------------------
This should help track down problems.

Einsum optimization path updates and efficiency improvements
------------------------------------------------------------
Einsum was synchronized with the current upstream work.

`numpy.angle` and `numpy.expand_dims` now work on ``ndarray`` subclasses
------------------------------------------------------------------------
In particular, they now work for masked arrays.

``NPY_NO_DEPRECATED_API`` compiler warning suppression
------------------------------------------------------
Setting ``NPY_NO_DEPRECATED_API`` to a value of 0 will suppress the current compiler
warnings when the deprecated numpy API is used.

``np.diff`` Added kwargs prepend and append
-------------------------------------------
New kwargs ``prepend`` and ``append``, allow for values to be inserted on
either end of the differences.  Similar to options for `ediff1d`. Now the
inverse of `cumsum` can be obtained easily via ``prepend=0``.

ARM support updated
-------------------
Support for ARM CPUs has been updated to accommodate 32 and 64 bit targets,
and also big and little endian byte ordering. AARCH32 memory alignment issues
have been addressed. CI testing has been expanded to include AARCH64 targets
via the services of shippable.com.

Appending to build flags
------------------------
`numpy.distutils` has always overridden rather than appended to `LDFLAGS` and
other similar such environment variables for compiling Fortran extensions.
Now, if the `NPY_DISTUTILS_APPEND_FLAGS` environment variable is set to 1, the
behavior will be appending.  This applied to: `LDFLAGS`, `F77FLAGS`,
`F90FLAGS`, `FREEFLAGS`, `FOPT`, `FDEBUG`, and `FFLAGS`.  See gh-11525 for more
details.

Generalized ufunc signatures now allow fixed-size dimensions
------------------------------------------------------------
By using a numerical value in the signature of a generalized ufunc, one can
indicate that the given function requires input or output to have dimensions
with the given size. E.g., the signature of a function that converts a polar
angle to a two-dimensional cartesian unit vector would be ``()->(2)``; that
for one that converts two spherical angles to a three-dimensional unit vector
would be ``(),()->(3)``; and that for the cross product of two
three-dimensional vectors would be ``(3),(3)->(3)``.

Note that to the elementary function these dimensions are not treated any
differently from variable ones indicated with a name starting with a letter;
the loop still is passed the corresponding size, but it can now count on that
size being equal to the fixed one given in the signature.

Generalized ufunc signatures now allow flexible dimensions
----------------------------------------------------------
Some functions, in particular numpy's implementation of ``@`` as ``matmul``,
are very similar to generalized ufuncs in that they operate over core
dimensions, but one could not present them as such because they were able to
deal with inputs in which a dimension is missing. To support this, it is now
allowed to postfix a dimension name with a question mark to indicate that the
dimension does not necessarily have to be present.

With this addition, the signature for ``matmul`` can be expressed as
``(m?,n),(n,p?)->(m?,p?)``.  This indicates that if, e.g., the second operand
has only one dimension, for the purposes of the elementary function it will be
treated as if that input has core shape ``(n, 1)``, and the output has the
corresponding core shape of ``(m, 1)``. The actual output array, however, has
the flexible dimension removed, i.e., it will have shape ``(..., m)``.
Similarly, if both arguments have only a single dimension, the inputs will be
presented as having shapes ``(1, n)`` and ``(n, 1)`` to the elementary
function, and the output as ``(1, 1)``, while the actual output array returned
will have shape ``()``. In this way, the signature allows one to use a
single elementary function for four related but different signatures,
``(m,n),(n,p)->(m,p)``, ``(n),(n,p)->(p)``, ``(m,n),(n)->(m)`` and
``(n),(n)->()``.

``np.clip`` and the ``clip`` method check for memory overlap
------------------------------------------------------------
The ``out`` argument to these functions is now always tested for memory overlap
to avoid corrupted results when memory overlap occurs.

New value ``unscaled`` for option ``cov`` in ``np.polyfit``
-----------------------------------------------------------
A further possible value has been added to the ``cov`` parameter of the
``np.polyfit`` function. With ``cov='unscaled'`` the scaling of the covariance
matrix is disabled completely (similar to setting ``absolute_sigma=True`` in
``scipy.optimize.curve_fit``). This would be useful in occasions, where the
weights are given by 1/sigma with sigma being the (known) standard errors of
(Gaussian distributed) data points, in which case the unscaled matrix is
already a correct estimate for the covariance matrix.

Detailed docstrings for scalar numeric types
--------------------------------------------
The ``help`` function, when applied to numeric types such as `numpy.intc`,
`numpy.int_`, and `numpy.longlong`, now lists all of the aliased names for that
type, distinguishing between platform -dependent and -independent aliases.

``__module__`` attribute now points to public modules
-----------------------------------------------------
The ``__module__`` attribute on most NumPy functions has been updated to refer
to the preferred public module from which to access a function, rather than
the module in which the function happens to be defined. This produces more
informative displays for functions in tools such as IPython, e.g., instead of
``<function 'numpy.core.fromnumeric.sum'>`` you now see
``<function 'numpy.sum'>``.

Large allocations marked as suitable for transparent hugepages
--------------------------------------------------------------
On systems that support transparent hugepages over the madvise system call
numpy now marks that large memory allocations can be backed by hugepages which
reduces page fault overhead and can in some fault heavy cases improve
performance significantly. On Linux the setting for huge pages to be used,
`/sys/kernel/mm/transparent_hugepage/enabled`, must be at least `madvise`.
Systems which already have it set to `always` will not see much difference as
the kernel will automatically use huge pages where appropriate.

Users of very old Linux kernels (~3.x and older) should make sure that
`/sys/kernel/mm/transparent_hugepage/defrag` is not set to `always` to avoid
performance problems due concurrency issues in the memory defragmentation.

Alpine Linux (and other musl c library distros) support
-------------------------------------------------------
We now default to use `fenv.h` for floating point status error reporting.
Previously we had a broken default that sometimes would not report underflow,
overflow, and invalid floating point operations. Now we can support non-glibc
distributions like Alpine Linux as long as they ship `fenv.h`.

Speedup ``np.block`` for large arrays
-------------------------------------
Large arrays (greater than ``512 * 512``) now use a blocking algorithm based on
copying the data directly into the appropriate slice of the resulting array.
This results in significant speedups for these large arrays, particularly for
arrays being blocked along more than 2 dimensions.

``arr.ctypes.data_as(...)`` holds a reference to arr
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Previously the caller was responsible for keeping the array alive for the
lifetime of the pointer.

Speedup ``np.take`` for read-only arrays
----------------------------------------
The implementation of ``np.take`` no longer makes an unnecessary copy of the
source array when its ``writeable`` flag is set to ``False``.

Support path-like objects for more functions
--------------------------------------------
The ``np.core.records.fromfile`` function now supports ``pathlib.Path``
and other path-like objects in addition to a file object. Furthermore, the
``np.load`` function now also supports path-like objects when using memory
mapping (``mmap_mode`` keyword argument).

Better behaviour of ufunc identities during reductions
------------------------------------------------------
Universal functions have an ``.identity`` which is used when ``.reduce`` is
called on an empty axis.

As of this release, the logical binary ufuncs, `logical_and`, `logical_or`,
and `logical_xor`, now have ``identity`` s of type `bool`, where previously they
were of type `int`. This restores the 1.14 behavior of getting ``bool`` s when
reducing empty object arrays with these ufuncs, while also keeping the 1.15
behavior of getting ``int`` s when reducing empty object arrays with arithmetic
ufuncs like ``add`` and ``multiply``.

Additionally, `logaddexp` now has an identity of ``-inf``, allowing it to be
called on empty sequences, where previously it could not be.

This is possible thanks to the new
:c:func:`PyUFunc_FromFuncAndDataAndSignatureAndIdentity`, which allows
arbitrary values to be used as identities now.

Improved conversion from ctypes objects
---------------------------------------
Numpy has always supported taking a value or type from ``ctypes`` and
converting it into an array or dtype, but only behaved correctly for simpler
types. As of this release, this caveat is lifted - now:

* The ``_pack_`` attribute of ``ctypes.Structure``, used to emulate C's
  ``__attribute__((packed))``, is respected.
* Endianness of all ctypes objects is preserved
* ``ctypes.Union`` is supported
* Non-representable constructs raise exceptions, rather than producing
  dangerously incorrect results:

  * Bitfields are no longer interpreted as sub-arrays
  * Pointers are no longer replaced with the type that they point to

A new ``ndpointer.contents`` member
-----------------------------------
This matches the ``.contents`` member of normal ctypes arrays, and can be used
to construct an ``np.array`` around the pointers contents.  This replaces
``np.array(some_nd_pointer)``, which stopped working in 1.15.  As a side effect
of this change, ``ndpointer`` now supports dtypes with overlapping fields and
padding.

``matmul`` is now a ``ufunc``
-----------------------------
`numpy.matmul` is now a ufunc which means that both the function and the
``__matmul__`` operator can now be overridden by ``__array_ufunc__``. Its
implementation has also changed. It uses the same BLAS routines as
`numpy.dot`, ensuring its performance is similar for large matrices.

Start and stop arrays for ``linspace``, ``logspace`` and ``geomspace``
----------------------------------------------------------------------
These functions used to be limited to scalar stop and start values, but can
now take arrays, which will be properly broadcast and result in an output
which has one axis prepended.  This can be used, e.g., to obtain linearly
interpolated points between sets of points.

CI extended with additional services
------------------------------------
We now use additional free CI services, thanks to the companies that provide:

* Codecoverage testing via codecov.io
* Arm testing via shippable.com
* Additional test runs on azure pipelines

These are in addition to our continued use of travis, appveyor (for wheels) and
LGTM


Changes
=======

Comparison ufuncs will now error rather than return NotImplemented
------------------------------------------------------------------
Previously, comparison ufuncs such as ``np.equal`` would return
`NotImplemented` if their arguments had structured dtypes, to help comparison
operators such as ``__eq__`` deal with those.  This is no longer needed, as the
relevant logic has moved to the comparison operators proper (which thus do
continue to return `NotImplemented` as needed). Hence, like all other ufuncs,
the comparison ufuncs will now error on structured dtypes.

Positive will now raise a deprecation warning for non-numerical arrays
----------------------------------------------------------------------
Previously, ``+array`` unconditionally returned a copy. Now, it will
raise a ``DeprecationWarning`` if the array is not numerical (i.e.,
if ``np.positive(array)`` raises a ``TypeError``. For ``ndarray``
subclasses that override the default ``__array_ufunc__`` implementation,
the ``TypeError`` is passed on.

``NDArrayOperatorsMixin`` now implements matrix multiplication
--------------------------------------------------------------
Previously, ``np.lib.mixins.NDArrayOperatorsMixin`` did not implement the
special methods for Python's matrix multiplication operator (``@``). This has
changed now that ``matmul`` is a ufunc and can be overridden using
``__array_ufunc__``.

The scaling of the covariance matrix in ``np.polyfit`` is different
-------------------------------------------------------------------
So far, ``np.polyfit`` used a non-standard factor in the scaling of the the
covariance matrix. Namely, rather than using the standard ``chisq/(M-N)``, it
scaled it with ``chisq/(M-N-2)`` where M is the number of data points and N is the
number of parameters.  This scaling is inconsistent with other fitting programs
such as e.g. ``scipy.optimize.curve_fit`` and was changed to ``chisq/(M-N)``.

``maximum`` and ``minimum`` no longer emit warnings
---------------------------------------------------
As part of code introduced in 1.10,  ``float32`` and ``float64`` set invalid
float status when a Nan is encountered in `numpy.maximum` and `numpy.minimum`,
when using SSE2 semantics. This caused a `RuntimeWarning` to sometimes be
emitted. In 1.15 we fixed the inconsistencies which caused the warnings to
become more conspicuous. Now no warnings will be emitted.

Umath and multiarray c-extension modules merged into a single module
--------------------------------------------------------------------
The two modules were merged, according to `NEP 15`_. Previously `np.core.umath`
and `np.core.multiarray` were separate c-extension modules. They are now python
wrappers to the single `np.core/_multiarray_math` c-extension module.

.. _`NEP 15` : http://www.numpy.org/neps/nep-0015-merge-multiarray-umath.html

``getfield`` validity checks extended
-------------------------------------
`numpy.ndarray.getfield` now checks the dtype and offset arguments to prevent
accessing invalid memory locations.

NumPy functions now support overrides with ``__array_function__``
-----------------------------------------------------------------
NumPy has a new experimental mechanism for overriding the implementation of
almost all NumPy functions on non-NumPy arrays by defining an
``__array_function__`` method, as described in `NEP 18`_.

This feature is not yet been enabled by default, but has been released to
facilitate experimentation by potential users. See the NEP for details on
setting the appropriate environment variable. We expect the NumPy 1.17 release
will enable overrides by default, which will also be more performant due to a
new implementation written in C.

.. _`NEP 18` : http://www.numpy.org/neps/nep-0018-array-function-protocol.html

Arrays based off readonly buffers cannot be set ``writeable``
-------------------------------------------------------------
We now disallow setting the ``writeable`` flag True on arrays created
from ``fromstring(readonly-buffer)``.
.. currentmodule:: numpy

==========================
NumPy 1.21.3 Release Notes
==========================

NumPy 1.21.3 is a maintenance release that fixes a few bugs discovered after
1.21.2. It also provides 64 bit Python 3.10.0 wheels. Note a few oddities about
Python 3.10:

* There are no 32 bit wheels for Windows, Mac, or Linux.
* The Mac Intel builds are only available in universal2 wheels.

The Python versions supported in this release are 3.7-3.10. If you want to
compile your own version using gcc-11, you will need to use gcc-11.2+ to avoid
problems.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Aaron Meurer
* Bas van Beek
* Charles Harris
* Developer-Ecosystem-Engineering +
* Kevin Sheppard
* Sebastian Berg
* Warren Weckesser

Pull requests merged
====================

A total of 8 pull requests were merged for this release.

* `#19745 <https://github.com/numpy/numpy/pull/19745>`__: ENH: Add dtype-support to 3 ```generic``/``ndarray`` methods
* `#19955 <https://github.com/numpy/numpy/pull/19955>`__: BUG: Resolve Divide by Zero on Apple silicon + test failures...
* `#19958 <https://github.com/numpy/numpy/pull/19958>`__: MAINT: Mark type-check-only ufunc subclasses as ufunc aliases...
* `#19994 <https://github.com/numpy/numpy/pull/19994>`__: BUG: np.tan(np.inf) test failure
* `#20080 <https://github.com/numpy/numpy/pull/20080>`__: BUG: Correct incorrect advance in PCG with emulated int128
* `#20081 <https://github.com/numpy/numpy/pull/20081>`__: BUG: Fix NaT handling in the PyArray_CompareFunc for datetime...
* `#20082 <https://github.com/numpy/numpy/pull/20082>`__: DOC: Ensure that we add documentation also as to the dict for...
* `#20106 <https://github.com/numpy/numpy/pull/20106>`__: BUG: core: result_type(0, np.timedelta64(4)) would seg. fault.
=========================
NumPy 1.7.0 Release Notes
=========================

This release includes several new features as well as numerous bug fixes and
refactorings. It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last
release that supports Python 2.4 - 2.5.

Highlights
==========

* ``where=`` parameter to ufuncs (allows the use of boolean arrays to choose
  where a computation should be done)
* ``vectorize`` improvements (added 'excluded' and 'cache' keyword, general
  cleanup and bug fixes)
* ``numpy.random.choice`` (random sample generating function)


Compatibility notes
===================

In a future version of numpy, the functions np.diag, np.diagonal, and the
diagonal method of ndarrays will return a view onto the original array,
instead of producing a copy as they do now. This makes a difference if you
write to the array returned by any of these functions. To facilitate this
transition, numpy 1.7 produces a FutureWarning if it detects that you may
be attempting to write to such an array. See the documentation for
np.diagonal for details.

Similar to np.diagonal above, in a future version of numpy, indexing a
record array by a list of field names will return a view onto the original
array, instead of producing a copy as they do now. As with np.diagonal,
numpy 1.7 produces a FutureWarning if it detects that you may be attempting
to write to such an array. See the documentation for array indexing for
details.

In a future version of numpy, the default casting rule for UFunc out=
parameters will be changed from 'unsafe' to 'same_kind'. (This also applies
to in-place operations like a += b, which is equivalent to np.add(a, b,
out=a).) Most usages which violate the 'same_kind' rule are likely bugs, so
this change may expose previously undetected errors in projects that depend
on NumPy. In this version of numpy, such usages will continue to succeed,
but will raise a DeprecationWarning.

Full-array boolean indexing has been optimized to use a different,
optimized code path.   This code path should produce the same results,
but any feedback about changes to your code would be appreciated.

Attempting to write to a read-only array (one with ``arr.flags.writeable``
set to ``False``) used to raise either a RuntimeError, ValueError, or
TypeError inconsistently, depending on which code path was taken. It now
consistently raises a ValueError.

The <ufunc>.reduce functions evaluate some reductions in a different order
than in previous versions of NumPy, generally providing higher performance.
Because of the nature of floating-point arithmetic, this may subtly change
some results, just as linking NumPy to a different BLAS implementations
such as MKL can.

If upgrading from 1.5, then generally in 1.6 and 1.7 there have been
substantial code added and some code paths altered, particularly in the
areas of type resolution and buffered iteration over universal functions.
This might have an impact on your code particularly if you relied on
accidental behavior in the past.

New features
============

Reduction UFuncs Generalize axis= Parameter
-------------------------------------------

Any ufunc.reduce function call, as well as other reductions like sum, prod,
any, all, max and min support the ability to choose a subset of the axes to
reduce over. Previously, one could say axis=None to mean all the axes or
axis=# to pick a single axis.  Now, one can also say axis=(#,#) to pick a
list of axes for reduction.

Reduction UFuncs New keepdims= Parameter
----------------------------------------

There is a new keepdims= parameter, which if set to True, doesn't throw
away the reduction axes but instead sets them to have size one.  When this
option is set, the reduction result will broadcast correctly to the
original operand which was reduced.

Datetime support
----------------

.. note:: The datetime API is *experimental* in 1.7.0, and may undergo changes
   in future versions of NumPy.

There have been a lot of fixes and enhancements to datetime64 compared
to NumPy 1.6:

* the parser is quite strict about only accepting ISO 8601 dates, with a few
  convenience extensions
* converts between units correctly
* datetime arithmetic works correctly
* business day functionality (allows the datetime to be used in contexts where
  only certain days of the week are valid)

The notes in `doc/source/reference/arrays.datetime.rst <https://github.com/numpy/numpy/blob/maintenance/1.7.x/doc/source/reference/arrays.datetime.rst>`_
(also available in the online docs at `arrays.datetime.html
<https://docs.scipy.org/doc/numpy/reference/arrays.datetime.html>`_) should be
consulted for more details.

Custom formatter for printing arrays
------------------------------------

See the new ``formatter`` parameter of the ``numpy.set_printoptions``
function.

New function numpy.random.choice
--------------------------------

A generic sampling function has been added which will generate samples from
a given array-like. The samples can be with or without replacement, and
with uniform or given non-uniform probabilities.

New function isclose
--------------------

Returns a boolean array where two arrays are element-wise equal within a
tolerance. Both relative and absolute tolerance can be specified.

Preliminary multi-dimensional support in the polynomial package
---------------------------------------------------------------

Axis keywords have been added to the integration and differentiation
functions and a tensor keyword was added to the evaluation functions.
These additions allow multi-dimensional coefficient arrays to be used in
those functions. New functions for evaluating 2-D and 3-D coefficient
arrays on grids or sets of points were added together with 2-D and 3-D
pseudo-Vandermonde matrices that can be used for fitting.


Ability to pad rank-n arrays
----------------------------

A pad module containing functions for padding n-dimensional arrays has been
added. The various private padding functions are exposed as options to a
public 'pad' function.  Example::

    pad(a, 5, mode='mean')

Current modes are ``constant``, ``edge``, ``linear_ramp``, ``maximum``,
``mean``, ``median``, ``minimum``, ``reflect``, ``symmetric``, ``wrap``, and
``<function>``.


New argument to searchsorted
----------------------------

The function searchsorted now accepts a 'sorter' argument that is a
permutation array that sorts the array to search.

Build system
------------

Added experimental support for the AArch64 architecture.

C API
-----

New function ``PyArray_FailUnlessWriteable`` provides a consistent interface
for checking array writeability -- any C code which works with arrays whose
WRITEABLE flag is not known to be True a priori, should make sure to call
this function before writing.

NumPy C Style Guide added (``doc/C_STYLE_GUIDE.rst.txt``).

Changes
=======

General
-------

The function np.concatenate tries to match the layout of its input arrays.
Previously, the layout did not follow any particular reason, and depended
in an undesirable way on the particular axis chosen for concatenation. A
bug was also fixed which silently allowed out of bounds axis arguments.

The ufuncs logical_or, logical_and, and logical_not now follow Python's
behavior with object arrays, instead of trying to call methods on the
objects. For example the expression (3 and 'test') produces the string
'test', and now np.logical_and(np.array(3, 'O'), np.array('test', 'O'))
produces 'test' as well.

The ``.base`` attribute on ndarrays, which is used on views to ensure that the
underlying array owning the memory is not deallocated prematurely, now
collapses out references when you have a view-of-a-view. For example::

    a = np.arange(10)
    b = a[1:]
    c = b[1:]

In numpy 1.6, ``c.base`` is ``b``, and ``c.base.base`` is ``a``. In numpy 1.7,
``c.base`` is ``a``.

To increase backwards compatibility for software which relies on the old
behaviour of ``.base``, we only 'skip over' objects which have exactly the same
type as the newly created view. This makes a difference if you use ``ndarray``
subclasses. For example, if we have a mix of ``ndarray`` and ``matrix`` objects
which are all views on the same original ``ndarray``::

    a = np.arange(10)
    b = np.asmatrix(a)
    c = b[0, 1:]
    d = c[0, 1:]

then ``d.base`` will be ``b``. This is because ``d`` is a ``matrix`` object,
and so the collapsing process only continues so long as it encounters other
``matrix`` objects. It considers ``c``, ``b``, and ``a`` in that order, and
``b`` is the last entry in that list which is a ``matrix`` object.

Casting Rules
-------------

Casting rules have undergone some changes in corner cases, due to the
NA-related work. In particular for combinations of scalar+scalar:

* the `longlong` type (`q`) now stays `longlong` for operations with any other
  number (`? b h i l q p B H I`), previously it was cast as `int_` (`l`). The
  `ulonglong` type (`Q`) now stays as `ulonglong` instead of `uint` (`L`).

* the `timedelta64` type (`m`) can now be mixed with any integer type (`b h i l
  q p B H I L Q P`), previously it raised `TypeError`.

For array + scalar, the above rules just broadcast except the case when
the array and scalars are unsigned/signed integers, then the result gets
converted to the array type (of possibly larger size) as illustrated by the
following examples::

    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(257)).dtype
    dtype('uint16')
    >>> (np.zeros((2,), dtype=np.int8) + np.uint16(257)).dtype
    dtype('int16')
    >>> (np.zeros((2,), dtype=np.int16) + np.uint32(2**17)).dtype
    dtype('int32')

Whether the size gets increased depends on the size of the scalar, for
example::

    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(255)).dtype
    dtype('uint8')
    >>> (np.zeros((2,), dtype=np.uint8) + np.int16(256)).dtype
    dtype('uint16')

Also a ``complex128`` scalar + ``float32`` array is cast to ``complex64``.

In NumPy 1.7 the `datetime64` type (`M`) must be constructed by explicitly
specifying the type as the second argument (e.g. ``np.datetime64(2000, 'Y')``).


Deprecations
============

General
-------

Specifying a custom string formatter with a `_format` array attribute is
deprecated. The new `formatter` keyword in ``numpy.set_printoptions`` or
``numpy.array2string`` can be used instead.

The deprecated imports in the polynomial package have been removed.

``concatenate`` now raises DepractionWarning for 1D arrays if ``axis != 0``.
Versions of numpy < 1.7.0 ignored axis argument value for 1D arrays. We
allow this for now, but in due course we will raise an error.

C-API
-----

Direct access to the fields of PyArrayObject* has been deprecated. Direct
access has been recommended against for many releases. Expect similar
deprecations for PyArray_Descr* and other core objects in the future as
preparation for NumPy 2.0.

The macros in old_defines.h are deprecated and will be removed in the next
major release (>= 2.0). The sed script tools/replace_old_macros.sed can be
used to replace these macros with the newer versions.

You can test your code against the deprecated C API by adding a line
composed of ``#define NPY_NO_DEPRECATED_API`` and the target version number,
such as ``NPY_1_7_API_VERSION``, before including any NumPy headers.

The ``NPY_CHAR`` member of the ``NPY_TYPES`` enum is deprecated and will be
removed in NumPy 1.8. See the discussion at
`gh-2801 <https://github.com/numpy/numpy/issues/2801>`_ for more details.
==========================
NumPy 1.13.3 Release Notes
==========================

This is a bugfix release for some problems found since 1.13.1. The most
important fixes are for CVE-2017-12852 and temporary elision. Users of earlier
versions of 1.13 should upgrade.

The Python versions supported are 2.7 and 3.4 - 3.6. The Python 3.6 wheels
available from PIP are built with Python 3.6.2 and should be compatible with
all previous versions of Python 3.6. It was cythonized with Cython 0.26.1,
which should be free of the bugs found in 0.27 while also being compatible with
Python 3.7-dev. The Windows wheels were built with OpenBlas instead ATLAS,
which should improve the performance of the linear algebra functions.

The NumPy 1.13.3 release is a re-release of 1.13.2, which suffered from a
bug in Cython 0.27.0.

Contributors
============

A total of 12 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Brandon Carter
* Charles Harris
* Eric Wieser
* Iryna Shcherbina +
* James Bourbeau +
* Jonathan Helmus
* Julian Taylor
* Matti Picus
* Michael Lamparski +
* Michael Seifert
* Ralf Gommers

Pull requests merged
====================

A total of 22 pull requests were merged for this release.

* #9390 BUG: Return the poly1d coefficients array directly
* #9555 BUG: Fix regression in 1.13.x in distutils.mingw32ccompiler.
* #9556 BUG: Fix true_divide when dtype=np.float64 specified.
* #9557 DOC: Fix some rst markup in numpy/doc/basics.py.
* #9558 BLD: Remove -xhost flag from IntelFCompiler.
* #9559 DOC: Removes broken docstring example (source code, png, pdf)...
* #9580 BUG: Add hypot and cabs functions to WIN32 blacklist.
* #9732 BUG: Make scalar function elision check if temp is writeable.
* #9736 BUG: Various fixes to np.gradient
* #9742 BUG: Fix np.pad for CVE-2017-12852
* #9744 BUG: Check for exception in sort functions, add tests
* #9745 DOC: Add whitespace after "versionadded::" directive so it actually...
* #9746 BUG: Memory leak in np.dot of size 0
* #9747 BUG: Adjust gfortran version search regex
* #9757 BUG: Cython 0.27 breaks NumPy on Python 3.
* #9764 BUG: Ensure `_npy_scaled_cexp{,f,l}` is defined when needed.
* #9765 BUG: PyArray_CountNonzero does not check for exceptions
* #9766 BUG: Fixes histogram monotonicity check for unsigned bin values
* #9767 BUG: Ensure consistent result dtype of count_nonzero
* #9771 BUG: MAINT: Fix mtrand for Cython 0.27.
* #9772 DOC: Create the 1.13.2 release notes.
* #9794 DOC: Create 1.13.3 release notes.
=========================
NumPy 1.8.1 Release Notes
=========================

This is a bugfix only release in the 1.8.x series.


Issues fixed
============

* gh-4276: Fix mean, var, std methods for object arrays
* gh-4262: remove insecure mktemp usage
* gh-2385: absolute(complex(inf)) raises invalid warning in python3
* gh-4024: Sequence assignment doesn't raise exception on shape mismatch
* gh-4027: Fix chunked reading of strings longer than BUFFERSIZE
* gh-4109: Fix object scalar return type of 0-d array indices
* gh-4018: fix missing check for memory allocation failure in ufuncs
* gh-4156: high order linalg.norm discards imaginary elements of complex arrays
* gh-4144: linalg: norm fails on longdouble, signed int
* gh-4094: fix NaT handling in _strided_to_strided_string_to_datetime
* gh-4051: fix uninitialized use in _strided_to_strided_string_to_datetime
* gh-4093: Loading compressed .npz file fails under Python 2.6.6
* gh-4138: segfault with non-native endian memoryview in python 3.4
* gh-4123: Fix missing NULL check in lexsort
* gh-4170: fix native-only long long check in memoryviews
* gh-4187: Fix large file support on 32 bit
* gh-4152: fromfile: ensure file handle positions are in sync in python3
* gh-4176: clang compatibility: Typos in conversion_utils
* gh-4223: Fetching a non-integer item caused array return
* gh-4197: fix minor memory leak in memoryview failure case
* gh-4206: fix build with single-threaded python
* gh-4220: add versionadded:: 1.8.0 to ufunc.at docstring
* gh-4267: improve handling of memory allocation failure
* gh-4267: fix use of capi without gil in ufunc.at
* gh-4261: Detect vendor versions of GNU Compilers
* gh-4253: IRR was returning nan instead of valid negative answer
* gh-4254: fix unnecessary byte order flag change for byte arrays
* gh-3263: numpy.random.shuffle clobbers mask of a MaskedArray
* gh-4270: np.random.shuffle not work with flexible dtypes
* gh-3173: Segmentation fault when 'size' argument to random.multinomial
* gh-2799: allow using unique with lists of complex
* gh-3504: fix linspace truncation for integer array scalar
* gh-4191: get_info('openblas') does not read libraries key
* gh-3348: Access violation in _descriptor_from_pep3118_format
* gh-3175: segmentation fault with numpy.array() from bytearray
* gh-4266: histogramdd - wrong result for entries very close to last boundary
* gh-4408: Fix stride_stricks.as_strided function for object arrays
* gh-4225: fix log1p and exmp1 return for np.inf on windows compiler builds
* gh-4359: Fix infinite recursion in str.format of flex arrays
* gh-4145: Incorrect shape of broadcast result with the exponent operator
* gh-4483: Fix commutativity of {dot,multiply,inner}(scalar, matrix_of_objs)
* gh-4466: Delay npyiter size check when size may change
* gh-4485: Buffered stride was erroneously marked fixed
* gh-4354: byte_bounds fails with datetime dtypes
* gh-4486: segfault/error converting from/to high-precision datetime64 objects
* gh-4428: einsum(None, None, None, None) causes segfault
* gh-4134: uninitialized use for for size 1 object reductions

Changes
=======

NDIter
------
When ``NpyIter_RemoveAxis`` is now called, the iterator range will be reset.

When a multi index is being tracked and an iterator is not buffered, it is
possible to use ``NpyIter_RemoveAxis``. In this case an iterator can shrink
in size. Because the total size of an iterator is limited, the iterator
may be too large before these calls. In this case its size will be set to ``-1``
and an error issued not at construction time but when removing the multi
index, setting the iterator range, or getting the next function.

This has no effect on currently working code, but highlights the necessity
of checking for an error return if these conditions can occur. In most
cases the arrays being iterated are as large as the iterator so that such
a problem cannot occur.

Optional reduced verbosity for np.distutils
-------------------------------------------
Set ``numpy.distutils.system_info.system_info.verbosity = 0`` and then
calls to ``numpy.distutils.system_info.get_info('blas_opt')`` will not
print anything on the output. This is mostly for other packages using
numpy.distutils.

Deprecations
============

C-API
-----

The utility function npy_PyFile_Dup and npy_PyFile_DupClose are broken by the
internal buffering python 3 applies to its file objects.
To fix this two new functions npy_PyFile_Dup2 and npy_PyFile_DupClose2 are
declared in npy_3kcompat.h and the old functions are deprecated.
Due to the fragile nature of these functions it is recommended to instead use
the python API when possible.
=========================
NumPy 1.7.2 Release Notes
=========================

This is a bugfix only release in the 1.7.x series.
It supports Python 2.4 - 2.7 and 3.1 - 3.3 and is the last series that
supports Python 2.4 - 2.5.


Issues fixed
============

* gh-3153: Do not reuse nditer buffers when not filled enough
* gh-3192: f2py crashes with UnboundLocalError exception
* gh-442: Concatenate with axis=None now requires equal number of array elements
* gh-2485: Fix for astype('S') string truncate issue
* gh-3312: bug in count_nonzero
* gh-2684: numpy.ma.average casts complex to float under certain conditions
* gh-2403: masked array with named components does not behave as expected
* gh-2495: np.ma.compress treated inputs in wrong order
* gh-576: add __len__ method to ma.mvoid
* gh-3364: reduce performance regression of mmap slicing
* gh-3421: fix non-swapping strided copies in GetStridedCopySwap
* gh-3373: fix small leak in datetime metadata initialization
* gh-2791: add platform specific python include directories to search paths
* gh-3168: fix undefined function and add integer divisions
* gh-3301: memmap does not work with TemporaryFile in python3
* gh-3057: distutils.misc_util.get_shared_lib_extension returns wrong debug extension
* gh-3472: add module extensions to load_library search list
* gh-3324: Make comparison function (gt, ge, ...) respect __array_priority__
* gh-3497: np.insert behaves incorrectly with argument 'axis=-1'
* gh-3541: make preprocessor tests consistent in halffloat.c
* gh-3458: array_ass_boolean_subscript() writes 'non-existent' data to array
* gh-2892: Regression in ufunc.reduceat with zero-sized index array
* gh-3608: Regression when filling struct from tuple
* gh-3701: add support for Python 3.4 ast.NameConstant
* gh-3712: do not assume that GIL is enabled in xerbla
* gh-3712: fix LAPACK error handling in lapack_litemodule
* gh-3728: f2py fix decref on wrong object
* gh-3743: Hash changed signature in Python 3.3
* gh-3793: scalar int hashing broken on 64 bit python3
* gh-3160: SandboxViolation easyinstalling 1.7.0 on Mac OS X 10.8.3
* gh-3871: npy_math.h has invalid isinf for Solaris with SUNWspro12.2
* gh-2561: Disable check for oldstyle classes in python3
* gh-3900: Ensure NotImplemented is passed on in MaskedArray ufunc's
* gh-2052: del scalar subscript causes segfault
* gh-3832: fix a few uninitialized uses and memleaks
* gh-3971: f2py changed string.lowercase to string.ascii_lowercase for python3
* gh-3480: numpy.random.binomial raised ValueError for n == 0
* gh-3992: hypot(inf, 0) shouldn't raise a warning, hypot(inf, inf) wrong result
* gh-4018: Segmentation fault dealing with very large arrays
* gh-4094: fix NaT handling in _strided_to_strided_string_to_datetime
* gh-4051: fix uninitialized use in _strided_to_strided_string_to_datetime
* gh-4123: lexsort segfault
* gh-4141: Fix a few issues that show up with python 3.4b1
==========================
NumPy 1.14.1 Release Notes
==========================

This is a bugfix release for some problems reported following the 1.14.0 release. The major
problems fixed are the following.

* Problems with the new array printing, particularly the printing of complex
  values, Please report any additional problems that may turn up.
* Problems with ``np.einsum`` due to the new ``optimized=True`` default. Some
  fixes for optimization have been applied and ``optimize=False`` is now the
  default.
* The sort order in ``np.unique`` when ``axis=<some-number>`` will now always
  be lexicographic in the subarray elements. In previous NumPy versions there
  was an optimization that could result in sorting the subarrays as unsigned
  byte strings.
* The change in 1.14.0 that multi-field indexing of structured arrays returns a
  view instead of a copy has been reverted but remains on track for NumPy 1.15.
  Affected users should read the 1.14.1 Numpy User Guide section
  "basics/structured arrays/accessing multiple fields" for advice on how to
  manage this transition.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python
3.6 wheels available from PIP are built with Python 3.6.2 and should be
compatible with all previous versions of Python 3.6. The source releases were
cythonized with Cython 0.26.1, which is known to **not** support the upcoming
Python 3.7 release.  People who wish to run Python 3.7 should check out the
NumPy repo and try building with the, as yet, unreleased master branch of
Cython.

Contributors
============

A total of 14 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Daniel Smith
* Dennis Weyland +
* Eric Larson
* Eric Wieser
* Jarrod Millman
* Kenichi Maehashi +
* Marten van Kerkwijk
* Mathieu Lamarre
* Sebastian Berg
* Simon Conseil
* Simon Gibbons
* xoviat

Pull requests merged
====================

A total of 36 pull requests were merged for this release.

* `#10339 <https://github.com/numpy/numpy/pull/10339>`__: BUG: restrict the __config__ modifications to win32
* `#10368 <https://github.com/numpy/numpy/pull/10368>`__: MAINT: Adjust type promotion in linalg.norm
* `#10375 <https://github.com/numpy/numpy/pull/10375>`__: BUG: add missing paren and remove quotes from repr of fieldless...
* `#10395 <https://github.com/numpy/numpy/pull/10395>`__: MAINT: Update download URL in setup.py.
* `#10396 <https://github.com/numpy/numpy/pull/10396>`__: BUG: fix einsum issue with unicode input and py2
* `#10397 <https://github.com/numpy/numpy/pull/10397>`__: BUG: fix error message not formatted in einsum
* `#10398 <https://github.com/numpy/numpy/pull/10398>`__: DOC: add documentation about how to handle new array printing
* `#10403 <https://github.com/numpy/numpy/pull/10403>`__: BUG: Set einsum optimize parameter default to `False`.
* `#10424 <https://github.com/numpy/numpy/pull/10424>`__: ENH: Fix repr of np.record objects to match np.void types #10412
* `#10425 <https://github.com/numpy/numpy/pull/10425>`__: MAINT: Update zesty to artful for i386 testing
* `#10431 <https://github.com/numpy/numpy/pull/10431>`__: REL: Add 1.14.1 release notes template
* `#10435 <https://github.com/numpy/numpy/pull/10435>`__: MAINT: Use ValueError for duplicate field names in lookup (backport)
* `#10534 <https://github.com/numpy/numpy/pull/10534>`__: BUG: Provide a better error message for out-of-order fields
* `#10536 <https://github.com/numpy/numpy/pull/10536>`__: BUG: Resize bytes columns in genfromtxt (backport of #10401)
* `#10537 <https://github.com/numpy/numpy/pull/10537>`__: BUG: multifield-indexing adds padding bytes: revert for 1.14.1
* `#10539 <https://github.com/numpy/numpy/pull/10539>`__: BUG: fix np.save issue with python 2.7.5
* `#10540 <https://github.com/numpy/numpy/pull/10540>`__: BUG: Add missing DECREF in Py2 int() cast
* `#10541 <https://github.com/numpy/numpy/pull/10541>`__: TST: Add circleci document testing to maintenance/1.14.x
* `#10542 <https://github.com/numpy/numpy/pull/10542>`__: BUG: complex repr has extra spaces, missing + (1.14 backport)
* `#10550 <https://github.com/numpy/numpy/pull/10550>`__: BUG: Set missing exception after malloc
* `#10557 <https://github.com/numpy/numpy/pull/10557>`__: BUG: In numpy.i, clear CARRAY flag if wrapped buffer is not C_CONTIGUOUS.
* `#10558 <https://github.com/numpy/numpy/pull/10558>`__: DEP: Issue FutureWarning when malformed records detected.
* `#10559 <https://github.com/numpy/numpy/pull/10559>`__: BUG: Fix einsum optimize logic for singleton dimensions
* `#10560 <https://github.com/numpy/numpy/pull/10560>`__: BUG: Fix calling ufuncs with a positional output argument.
* `#10561 <https://github.com/numpy/numpy/pull/10561>`__: BUG: Fix various Big-Endian test failures (ppc64)
* `#10562 <https://github.com/numpy/numpy/pull/10562>`__: BUG: Make dtype.descr error for out-of-order fields.
* `#10563 <https://github.com/numpy/numpy/pull/10563>`__: BUG: arrays not being flattened in `union1d`
* `#10607 <https://github.com/numpy/numpy/pull/10607>`__: MAINT: Update sphinxext submodule hash.
* `#10608 <https://github.com/numpy/numpy/pull/10608>`__: BUG: Revert sort optimization in np.unique.
* `#10609 <https://github.com/numpy/numpy/pull/10609>`__: BUG: infinite recursion in str of 0d subclasses
* `#10610 <https://github.com/numpy/numpy/pull/10610>`__: BUG: Align type definition with generated lapack
* `#10612 <https://github.com/numpy/numpy/pull/10612>`__: BUG/ENH: Improve output for structured non-void types
* `#10622 <https://github.com/numpy/numpy/pull/10622>`__: BUG: deallocate recursive closure in arrayprint.py (1.14 backport)
* `#10624 <https://github.com/numpy/numpy/pull/10624>`__: BUG: Correctly identify comma separated dtype strings
* `#10629 <https://github.com/numpy/numpy/pull/10629>`__: BUG: deallocate recursive closure in arrayprint.py (backport...
* `#10630 <https://github.com/numpy/numpy/pull/10630>`__: REL: Prepare for 1.14.1 release.
.. currentmodule:: numpy

==========================
NumPy 1.17.3 Release Notes
==========================

This release contains fixes for bugs reported against NumPy 1.17.2 along with a
some documentation improvements. The Python versions supported in this release
are 3.5-3.8.

Downstream developers should use Cython >= 0.29.13 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture.


Highlights
==========

- Wheels for Python 3.8
- Boolean ``matmul`` fixed to use booleans instead of integers.


Compatibility notes
===================

- The seldom used ``PyArray_DescrCheck`` macro has been changed/fixed.


Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Kevin Sheppard
* Matti Picus
* Ralf Gommers
* Sebastian Berg
* Warren Weckesser


Pull requests merged
====================

A total of 12 pull requests were merged for this release.

* `#14456 <https://github.com/numpy/numpy/pull/14456>`__: MAINT: clean up pocketfft modules inside numpy.fft namespace.
* `#14463 <https://github.com/numpy/numpy/pull/14463>`__: BUG: random.hypergeometic assumes npy_long is npy_int64, hung...
* `#14502 <https://github.com/numpy/numpy/pull/14502>`__: BUG: random: Revert gh-14458 and refix gh-14557.
* `#14504 <https://github.com/numpy/numpy/pull/14504>`__: BUG: add a specialized loop for boolean matmul.
* `#14506 <https://github.com/numpy/numpy/pull/14506>`__: MAINT: Update pytest version for Python 3.8
* `#14512 <https://github.com/numpy/numpy/pull/14512>`__: DOC: random: fix doc linking, was referencing private submodules.
* `#14513 <https://github.com/numpy/numpy/pull/14513>`__: BUG,MAINT: Some fixes and minor cleanup based on clang analysis
* `#14515 <https://github.com/numpy/numpy/pull/14515>`__: BUG: Fix randint when range is 2**32
* `#14519 <https://github.com/numpy/numpy/pull/14519>`__: MAINT: remove the entropy c-extension module
* `#14563 <https://github.com/numpy/numpy/pull/14563>`__: DOC: remove note about Pocketfft license file (non-existing here).
* `#14578 <https://github.com/numpy/numpy/pull/14578>`__: BUG: random: Create a legacy implementation of random.binomial.
* `#14687 <https://github.com/numpy/numpy/pull/14687>`__: BUG: properly define PyArray_DescrCheck
==========================
NumPy 1.16.1 Release Notes
==========================

The NumPy 1.16.1 release fixes bugs reported against the 1.16.0 release, and
also backports several enhancements from master that seem appropriate for a
release series that is the last to support Python 2.7. The wheels on PyPI are
linked with OpenBLAS v0.3.4+,  which should fix the known threading issues
found in previous OpenBLAS versions.

Downstream developers building this release should use Cython >= 0.29.2 and, if
using OpenBLAS, OpenBLAS > v0.3.4.

If you are installing using pip, you may encounter a problem with older
installed versions of NumPy that pip did not delete becoming mixed with the
current version, resulting in an ``ImportError``. That problem is particularly
common on Debian derived distributions due to a modified pip.  The fix is to
make sure all previous NumPy versions installed by pip have been removed. See
`#12736 <https://github.com/numpy/numpy/issues/12736>`__ for discussion of the
issue. Note that previously this problem resulted in an ``AttributeError``.


Contributors
============

A total of 16 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Antoine Pitrou
* Arcesio Castaneda Medina +
* Charles Harris
* Chris Markiewicz +
* Christoph Gohlke
* Christopher J. Markiewicz +
* Daniel Hrisca +
* EelcoPeacs +
* Eric Wieser
* Kevin Sheppard
* Matti Picus
* OBATA Akio +
* Ralf Gommers
* Sebastian Berg
* Stephan Hoyer
* Tyler Reddy


Enhancements
============

* `#12767 <https://github.com/numpy/numpy/pull/12767>`__: ENH: add mm->q floordiv
* `#12768 <https://github.com/numpy/numpy/pull/12768>`__: ENH: port np.core.overrides to C for speed
* `#12769 <https://github.com/numpy/numpy/pull/12769>`__: ENH: Add np.ctypeslib.as_ctypes_type(dtype), improve `np.ctypeslib.as_ctypes`
* `#12773 <https://github.com/numpy/numpy/pull/12773>`__: ENH: add "max difference" messages to np.testing.assert_array_equal...
* `#12820 <https://github.com/numpy/numpy/pull/12820>`__: ENH: Add mm->qm divmod
* `#12890 <https://github.com/numpy/numpy/pull/12890>`__: ENH: add _dtype_ctype to namespace for freeze analysis


Compatibility notes
===================

* The changed error message emitted by array comparison testing functions may
  affect doctests. See below for detail.

* Casting from double and single denormals to float16 has been corrected.  In
  some rare cases, this may result in results being rounded up instead of down,
  changing the last bit (ULP) of the result.


New Features
============

divmod operation is now supported for two ``timedelta64`` operands
------------------------------------------------------------------
The divmod operator now handles two ``np.timedelta64`` operands, with
type signature ``mm->qm``.


Improvements
============

Further improvements to ``ctypes`` support in ``np.ctypeslib``
--------------------------------------------------------------
A new `numpy.ctypeslib.as_ctypes_type` function has been added, which can be
used to converts a `dtype` into a best-guess `ctypes` type. Thanks to this
new function, `numpy.ctypeslib.as_ctypes` now supports a much wider range of
array types, including structures, booleans, and integers of non-native
endianness.

Array comparison assertions include maximum differences
-------------------------------------------------------
Error messages from array comparison tests such as
`np.testing.assert_allclose` now include "max absolute difference" and
"max relative difference," in addition to the previous "mismatch" percentage.
This information makes it easier to update absolute and relative error
tolerances.


Changes
=======

``timedelta64 % 0`` behavior adjusted to return ``NaT``
-------------------------------------------------------
The modulus operation with two ``np.timedelta64`` operands now returns
``NaT`` in the case of division by zero, rather than returning zero



==========================
NumPy 1.14.0 Release Notes
==========================

Numpy 1.14.0 is the result of seven months of work and contains a large number
of bug fixes and new features, along with several changes with potential
compatibility issues. The major change that users will notice are the
stylistic changes in the way numpy arrays and scalars are printed, a change
that will affect doctests. See below for details on how to preserve the
old style printing when needed.

A major decision affecting future development concerns the schedule for
dropping Python 2.7 support in the runup to 2020. The decision has been made to
support 2.7 for all releases made in 2018, with the last release being
designated a long term release with support for bug fixes extending through
2019. In 2019 support for 2.7 will be dropped in all new releases. More details
can be found in `NEP 12`_.

This release supports Python 2.7 and 3.4 - 3.6.

.. _`NEP 12`: http://www.numpy.org/neps/nep-0014-dropping-python2.7-proposal.html


Highlights
==========

* The `np.einsum` function uses BLAS when possible

* ``genfromtxt``, ``loadtxt``, ``fromregex`` and ``savetxt`` can now handle
  files with arbitrary Python supported encoding.

* Major improvements to printing of NumPy arrays and scalars.


New functions
=============

* ``parametrize``: decorator added to numpy.testing

* ``chebinterpolate``: Interpolate function at Chebyshev points.

* ``format_float_positional`` and ``format_float_scientific`` : format
  floating-point scalars unambiguously with control of rounding and padding.

* ``PyArray_ResolveWritebackIfCopy`` and ``PyArray_SetWritebackIfCopyBase``,
  new C-API functions useful in achieving PyPy compatibility.


Deprecations
============

* Using ``np.bool_`` objects in place of integers is deprecated.  Previously
  ``operator.index(np.bool_)`` was legal and allowed constructs such as
  ``[1, 2, 3][np.True_]``. That was misleading, as it behaved differently from
  ``np.array([1, 2, 3])[np.True_]``.

* Truth testing of an empty array is deprecated. To check if an array is not
  empty, use ``array.size > 0``.

* Calling ``np.bincount`` with ``minlength=None`` is deprecated.
  ``minlength=0`` should be used instead.

* Calling ``np.fromstring`` with the default value of the ``sep`` argument is
  deprecated.  When that argument is not provided, a broken version of
  ``np.frombuffer`` is used that silently accepts unicode strings and -- after
  encoding them as either utf-8 (python 3) or the default encoding
  (python 2) -- treats them as binary data. If reading binary data is
  desired, ``np.frombuffer`` should be used directly.

* The ``style`` option of array2string is deprecated in non-legacy printing mode.

* ``PyArray_SetUpdateIfCopyBase`` has been deprecated. For NumPy versions >= 1.14
  use ``PyArray_SetWritebackIfCopyBase`` instead, see `C API changes` below for
  more details.



* The use of ``UPDATEIFCOPY`` arrays is deprecated, see  `C API changes` below
  for details.  We will not be dropping support for those arrays, but they are
  not compatible with PyPy.


Future Changes
==============

* ``np.issubdtype`` will stop downcasting dtype-like arguments.
  It might be expected that ``issubdtype(np.float32, 'float64')`` and
  ``issubdtype(np.float32, np.float64)`` mean the same thing - however, there
  was an undocumented special case that translated the former into
  ``issubdtype(np.float32, np.floating)``, giving the surprising result of True.

  This translation now gives a warning that explains what translation is
  occurring.  In the future, the translation will be disabled, and the first
  example will be made equivalent to the second.

* ``np.linalg.lstsq`` default for ``rcond`` will be changed.  The ``rcond``
  parameter to ``np.linalg.lstsq`` will change its default to machine precision
  times the largest of the input array dimensions. A FutureWarning is issued
  when ``rcond`` is not passed explicitly.

* ``a.flat.__array__()`` will return a writeable copy of ``a`` when ``a`` is
  non-contiguous.  Previously it returned an UPDATEIFCOPY array when ``a`` was
  writeable. Currently it returns a non-writeable copy. See gh-7054 for a
  discussion of the issue.

* Unstructured void array's ``.item`` method will return a bytes object. In the
  future, calling ``.item()`` on arrays or scalars of ``np.void`` datatype will
  return a ``bytes`` object instead of a buffer or int array, the same as
  returned by ``bytes(void_scalar)``. This may affect code which assumed the
  return value was mutable, which will no longer be the case. A
  ``FutureWarning`` is now issued when this would occur.


Compatibility notes
===================

The mask of a masked array view is also a view rather than a copy
-----------------------------------------------------------------
There was a FutureWarning about this change in NumPy 1.11.x. In short, it is
now the case that, when changing a view of a masked array, changes to the mask
are propagated to the original. That was not previously the case. This change
affects slices in particular. Note that this does not yet work properly if the
mask of the original array is ``nomask`` and the mask of the view is changed.
See gh-5580 for an extended discussion. The original behavior of having a copy
of the mask can be obtained by calling the ``unshare_mask`` method of the view.

``np.ma.masked`` is no longer writeable
---------------------------------------
Attempts to mutate the ``masked`` constant now error, as the underlying arrays
are marked readonly. In the past, it was possible to get away with::

    # emulating a function that sometimes returns np.ma.masked
    val = random.choice([np.ma.masked, 10])
    var_arr = np.asarray(val)
    val_arr += 1  # now errors, previously changed np.ma.masked.data

``np.ma`` functions producing ``fill_value`` s have changed
-----------------------------------------------------------
Previously, ``np.ma.default_fill_value`` would return a 0d array, but
``np.ma.minimum_fill_value`` and ``np.ma.maximum_fill_value`` would return a
tuple of the fields. Instead, all three methods return a structured ``np.void``
object, which is what you would already find in the ``.fill_value`` attribute.

Additionally, the dtype guessing now matches that of ``np.array`` - so when
passing a python scalar ``x``, ``maximum_fill_value(x)`` is always the same as
``maximum_fill_value(np.array(x))``. Previously ``x = long(1)`` on Python 2
violated this assumption.

``a.flat.__array__()`` returns non-writeable arrays when ``a`` is non-contiguous
--------------------------------------------------------------------------------
The intent is that the UPDATEIFCOPY array previously returned when ``a`` was
non-contiguous will be replaced by a writeable copy in the future. This
temporary measure is aimed to notify folks who expect the underlying array be
modified in this situation that that will no longer be the case. The most
likely places for this to be noticed is when expressions of the form
``np.asarray(a.flat)`` are used, or when ``a.flat`` is passed as the out
parameter to a ufunc.

``np.tensordot`` now returns zero array when contracting over 0-length dimension
--------------------------------------------------------------------------------
Previously ``np.tensordot`` raised a ValueError when contracting over 0-length
dimension. Now it returns a zero array, which is consistent with the behaviour
of ``np.dot`` and ``np.einsum``.

``numpy.testing`` reorganized
-----------------------------
This is not expected to cause problems, but possibly something has been left
out. If you experience an unexpected import problem using ``numpy.testing``
let us know.

``np.asfarray`` no longer accepts non-dtypes through the ``dtype`` argument
---------------------------------------------------------------------------
This previously would accept ``dtype=some_array``, with the implied semantics
of ``dtype=some_array.dtype``. This was undocumented, unique across the numpy
functions, and if used would likely correspond to a typo.

1D ``np.linalg.norm`` preserves float input types, even for arbitrary orders
----------------------------------------------------------------------------
Previously, this would promote to ``float64`` when arbitrary orders were
passed, despite not doing so under the simple cases::

    >>> f32 = np.float32([[1, 2]])
    >>> np.linalg.norm(f32, 2.0, axis=-1).dtype
    dtype('float32')
    >>> np.linalg.norm(f32, 2.0001, axis=-1).dtype
    dtype('float64')  # numpy 1.13
    dtype('float32')  # numpy 1.14

This change affects only ``float32`` and ``float16`` arrays.

``count_nonzero(arr, axis=())`` now counts over no axes, not all axes
---------------------------------------------------------------------
Elsewhere, ``axis==()`` is always understood as "no axes", but
`count_nonzero` had a special case to treat this as "all axes". This was
inconsistent and surprising. The correct way to count over all axes has always
been to pass ``axis == None``.

``__init__.py`` files added to test directories
-----------------------------------------------
This is for pytest compatibility in the case of duplicate test file names in
the different directories. As a result, ``run_module_suite`` no longer works,
i.e., ``python <path-to-test-file>`` results in an error.

``.astype(bool)`` on unstructured void arrays now calls ``bool`` on each element
--------------------------------------------------------------------------------
On Python 2, ``void_array.astype(bool)`` would always return an array of
``True``, unless the dtype is ``V0``. On Python 3, this operation would usually
crash. Going forwards, `astype` matches the behavior of ``bool(np.void)``,
considering a buffer of all zeros as false, and anything else as true.
Checks for ``V0`` can still be done with ``arr.dtype.itemsize == 0``.

``MaskedArray.squeeze`` never returns ``np.ma.masked``
------------------------------------------------------
``np.squeeze`` is documented as returning a view, but the masked variant would
sometimes return ``masked``, which is not a view. This has been fixed, so that
the result is always a view on the original masked array.
This breaks any code that used ``masked_arr.squeeze() is np.ma.masked``, but
fixes code that writes to the result of `.squeeze()`.

Renamed first parameter of ``can_cast`` from ``from`` to ``from_``
------------------------------------------------------------------
The previous parameter name ``from`` is a reserved keyword in Python, which made
it difficult to pass the argument by name. This has been fixed by renaming
the parameter to ``from_``.

``isnat`` raises ``TypeError`` when passed wrong type
------------------------------------------------------
The ufunc ``isnat`` used to raise a ``ValueError`` when it was not passed
variables of type ``datetime`` or ``timedelta``. This has been changed to
raising a ``TypeError``.

``dtype.__getitem__`` raises ``TypeError`` when passed wrong type
-----------------------------------------------------------------
When indexed with a float, the dtype object used to raise ``ValueError``.

User-defined types now need to implement ``__str__`` and ``__repr__``
---------------------------------------------------------------------
Previously, user-defined types could fall back to a default implementation of
``__str__`` and ``__repr__`` implemented in numpy, but this has now been
removed. Now user-defined types will fall back to the python default
``object.__str__`` and ``object.__repr__``.

Many changes to array printing, disableable with the new "legacy" printing mode
-------------------------------------------------------------------------------
The ``str`` and ``repr`` of ndarrays and numpy scalars have been changed in
a variety of ways. These changes are likely to break downstream user's
doctests.

These new behaviors can be disabled to mostly reproduce numpy 1.13 behavior by
enabling the new 1.13 "legacy" printing mode. This is enabled by calling
``np.set_printoptions(legacy="1.13")``, or using the new ``legacy`` argument to
``np.array2string``, as ``np.array2string(arr, legacy='1.13')``.

In summary, the major changes are:

* For floating-point types:

  * The ``repr`` of float arrays often omits a space previously printed
    in the sign position. See the new ``sign`` option to ``np.set_printoptions``.
  * Floating-point arrays and scalars use a new algorithm for decimal
    representations, giving the shortest unique representation. This will
    usually shorten ``float16`` fractional output, and sometimes ``float32`` and
    ``float128`` output. ``float64`` should be unaffected.  See the new
    ``floatmode`` option to ``np.set_printoptions``.
  * Float arrays printed in scientific notation no longer use fixed-precision,
    and now instead show the shortest unique representation.
  * The ``str`` of floating-point scalars is no longer truncated in python2.

* For other data types:

  * Non-finite complex scalars print like ``nanj`` instead of ``nan*j``.
  * ``NaT`` values in datetime arrays are now properly aligned.
  * Arrays and scalars of ``np.void`` datatype are now printed using hex
    notation.

* For line-wrapping:

  * The "dtype" part of ndarray reprs will now be printed on the next line
    if there isn't space on the last line of array output.
  * The ``linewidth`` format option is now always respected.
    The `repr` or `str` of an array will never exceed this, unless a single
    element is too wide.
  * The last line of an array string will never have more elements than earlier
    lines.
  * An extra space is no longer inserted on the first line if the elements are
    too wide.

* For summarization (the use of ``...`` to shorten long arrays):

  * A trailing comma is no longer inserted for ``str``.
    Previously, ``str(np.arange(1001))`` gave
    ``'[   0    1    2 ...,  998  999 1000]'``, which has an extra comma.
  * For arrays of 2-D and beyond, when ``...`` is printed on its own line in
    order to summarize any but the last axis, newlines are now appended to that
    line to match its leading newlines and a trailing space character is
    removed.

* ``MaskedArray`` arrays now separate printed elements with commas, always
  print the dtype, and correctly wrap the elements of long arrays to multiple
  lines. If there is more than 1 dimension, the array attributes are now
  printed in a new "left-justified" printing style.
* ``recarray`` arrays no longer print a trailing space before their dtype, and
  wrap to the right number of columns.
* 0d arrays no longer have their own idiosyncratic implementations of ``str``
  and ``repr``. The ``style`` argument to ``np.array2string`` is deprecated.
* Arrays of ``bool`` datatype will omit the datatype in the ``repr``.
* User-defined ``dtypes`` (subclasses of ``np.generic``) now need to
  implement ``__str__`` and ``__repr__``.

Some of these changes are described in more detail below. If you need to retain
the previous behavior for doctests or other reasons, you may want to do
something like::

    # FIXME: We need the str/repr formatting used in Numpy < 1.14.
    try:
        np.set_printoptions(legacy='1.13')
    except TypeError:
        pass


C API changes
=============

PyPy compatible alternative to ``UPDATEIFCOPY`` arrays
------------------------------------------------------
``UPDATEIFCOPY`` arrays are contiguous copies of existing arrays, possibly with
different dimensions, whose contents are copied back to the original array when
their refcount goes to zero and they are deallocated. Because PyPy does not use
refcounts, they do not function correctly with PyPy. NumPy is in the process of
eliminating their use internally and two new C-API functions,

* ``PyArray_SetWritebackIfCopyBase``
* ``PyArray_ResolveWritebackIfCopy``,

have been added together with a complementary flag,
``NPY_ARRAY_WRITEBACKIFCOPY``. Using the new functionality also requires that
some flags be changed when new arrays are created, to wit:
``NPY_ARRAY_INOUT_ARRAY`` should be replaced by ``NPY_ARRAY_INOUT_ARRAY2`` and
``NPY_ARRAY_INOUT_FARRAY`` should be replaced by ``NPY_ARRAY_INOUT_FARRAY2``.
Arrays created with these new flags will then have the ``WRITEBACKIFCOPY``
semantics.

If PyPy compatibility is not a concern, these new functions can be ignored,
although there will be a ``DeprecationWarning``. If you do wish to pursue PyPy
compatibility, more information on these functions and their use may be found
in the c-api_ documentation and the example in how-to-extend_.

.. _c-api: https://github.com/numpy/numpy/blob/master/doc/source/reference/c-api.array.rst
.. _how-to-extend: https://github.com/numpy/numpy/blob/master/doc/source/user/c-info.how-to-extend.rst


New Features
============

Encoding argument for text IO functions
---------------------------------------
``genfromtxt``, ``loadtxt``, ``fromregex`` and ``savetxt`` can now handle files
with arbitrary encoding supported by Python via the encoding argument.
For backward compatibility the argument defaults to the special ``bytes`` value
which continues to treat text as raw byte values and continues to pass latin1
encoded bytes to custom converters.
Using any other value (including ``None`` for system default) will switch the
functions to real text IO so one receives unicode strings instead of bytes in
the resulting arrays.

External ``nose`` plugins are usable by ``numpy.testing.Tester``
----------------------------------------------------------------
``numpy.testing.Tester`` is now aware of ``nose`` plugins that are outside the
``nose`` built-in ones.  This allows using, for example, ``nose-timer`` like
so:  ``np.test(extra_argv=['--with-timer', '--timer-top-n', '20'])`` to
obtain the runtime of the 20 slowest tests.  An extra keyword ``timer`` was
also added to ``Tester.test``, so ``np.test(timer=20)`` will also report the 20
slowest tests.

``parametrize`` decorator added to ``numpy.testing``
----------------------------------------------------
A basic ``parametrize`` decorator is now available in ``numpy.testing``. It is
intended to allow rewriting yield based tests that have been deprecated in
pytest so as to facilitate the transition to pytest in the future. The nose
testing framework has not been supported for several years and looks like
abandonware.

The new ``parametrize`` decorator does not have the full functionality of the
one in pytest. It doesn't work for classes, doesn't support nesting, and does
not substitute variable names. Even so, it should be adequate to rewrite the
NumPy tests.

``chebinterpolate`` function added to ``numpy.polynomial.chebyshev``
--------------------------------------------------------------------
The new ``chebinterpolate`` function interpolates a given function at the
Chebyshev points of the first kind. A new ``Chebyshev.interpolate`` class
method adds support for interpolation over arbitrary intervals using the scaled
and shifted Chebyshev points of the first kind.

Support for reading lzma compressed text files in Python 3
----------------------------------------------------------
With Python versions containing the ``lzma`` module the text IO functions can
now transparently read from files with ``xz`` or ``lzma`` extension.

``sign`` option added to ``np.setprintoptions`` and ``np.array2string``
-----------------------------------------------------------------------
This option controls printing of the sign of floating-point types, and may be
one of the characters '-', '+' or ' '. With '+' numpy always prints the sign of
positive values, with ' ' it always prints a space (whitespace character) in
the sign position of positive values, and with '-' it will omit the sign
character for positive values. The new default is '-'.

This new default changes the float output relative to numpy 1.13. The old
behavior can be obtained in 1.13 "legacy" printing mode, see compatibility
notes above.

``hermitian`` option added to``np.linalg.matrix_rank``
------------------------------------------------------
The new ``hermitian`` option allows choosing between standard SVD based matrix
rank calculation and the more efficient eigenvalue based method for
symmetric/hermitian matrices.

``threshold`` and ``edgeitems`` options added to ``np.array2string``
--------------------------------------------------------------------
These options could previously be controlled using ``np.set_printoptions``, but
now can be changed on a per-call basis as arguments to ``np.array2string``.

``concatenate`` and ``stack`` gained an ``out`` argument
--------------------------------------------------------
A preallocated buffer of the desired dtype can now be used for the output of
these functions.

Support for PGI flang compiler on Windows
-----------------------------------------
The PGI flang compiler is a Fortran front end for LLVM released by NVIDIA under
the Apache 2 license. It can be invoked by ::

    python setup.py config --compiler=clang --fcompiler=flang install

There is little experience with this new compiler, so any feedback from people
using it will be appreciated.


Improvements
============

Numerator degrees of freedom in ``random.noncentral_f`` need only be positive.
------------------------------------------------------------------------------
Prior to NumPy 1.14.0, the numerator degrees of freedom needed to be > 1, but
the distribution is valid for values > 0, which is the new requirement.

The GIL is released for all ``np.einsum`` variations
----------------------------------------------------
Some specific loop structures which have an accelerated loop version
did not release the GIL prior to NumPy 1.14.0.  This oversight has been
fixed.

The `np.einsum` function will use BLAS when possible and optimize by default
----------------------------------------------------------------------------
The ``np.einsum`` function will now call ``np.tensordot`` when appropriate.
Because ``np.tensordot`` uses BLAS when possible, that will speed up execution.
By default, ``np.einsum`` will also attempt optimization as the overhead is
small relative to the potential improvement in speed.

``f2py`` now handles arrays of dimension 0
------------------------------------------
``f2py`` now allows for the allocation of arrays of dimension 0. This allows
for more consistent handling of corner cases downstream.

``numpy.distutils`` supports using MSVC and mingw64-gfortran together
---------------------------------------------------------------------
Numpy distutils now supports using Mingw64 gfortran and MSVC compilers
together. This enables the production of Python extension modules on Windows
containing Fortran code while retaining compatibility with the
binaries distributed by Python.org. Not all use cases are supported,
but most common ways to wrap Fortran for Python are functional.

Compilation in this mode is usually enabled automatically, and can be
selected via the ``--fcompiler`` and ``--compiler`` options to
``setup.py``. Moreover, linking Fortran codes to static OpenBLAS is
supported; by default a gfortran compatible static archive
``openblas.a`` is looked for.

``np.linalg.pinv`` now works on stacked matrices
------------------------------------------------
Previously it was limited to a single 2d array.

``numpy.save`` aligns data to 64 bytes instead of 16
----------------------------------------------------
Saving NumPy arrays in the ``npy`` format with ``numpy.save`` inserts
padding before the array data to align it at 64 bytes.  Previously
this was only 16 bytes (and sometimes less due to a bug in the code
for version 2).  Now the alignment is 64 bytes, which matches the
widest SIMD instruction set commonly available, and is also the most
common cache line size.  This makes ``npy`` files easier to use in
programs which open them with ``mmap``, especially on Linux where an
``mmap`` offset must be a multiple of the page size.

NPZ files now can be written without using temporary files
----------------------------------------------------------
In Python 3.6+ ``numpy.savez`` and ``numpy.savez_compressed`` now write
directly to a ZIP file, without creating intermediate temporary files.

Better support for empty structured and string types
----------------------------------------------------
Structured types can contain zero fields, and string dtypes can contain zero
characters. Zero-length strings still cannot be created directly, and must be
constructed through structured dtypes::

    str0 = np.empty(10, np.dtype([('v', str, N)]))['v']
    void0 = np.empty(10, np.void)

It was always possible to work with these, but the following operations are
now supported for these arrays:

 * `arr.sort()`
 * `arr.view(bytes)`
 * `arr.resize(...)`
 * `pickle.dumps(arr)`

Support for ``decimal.Decimal`` in ``np.lib.financial``
-------------------------------------------------------
Unless otherwise stated all functions within the ``financial`` package now
support using the ``decimal.Decimal`` built-in type.

Float printing now uses "dragon4" algorithm for shortest decimal representation
-------------------------------------------------------------------------------
The ``str`` and ``repr`` of floating-point values (16, 32, 64 and 128 bit) are
now printed to give the shortest decimal representation which uniquely
identifies the value from others of the same type. Previously this was only
true for ``float64`` values. The remaining float types will now often be shorter
than in numpy 1.13. Arrays printed in scientific notation now also use the
shortest scientific representation, instead of fixed precision as before.

 Additionally, the `str` of float scalars scalars will no longer be truncated
 in python2, unlike python2 `float`s.  `np.double` scalars now have a ``str``
 and ``repr`` identical to that of a python3 float.

New functions ``np.format_float_scientific`` and ``np.format_float_positional``
are provided to generate these decimal representations.

A new option ``floatmode`` has been added to ``np.set_printoptions`` and
``np.array2string``, which gives control over uniqueness and rounding of
printed elements in an array. The new default is ``floatmode='maxprec'`` with
``precision=8``, which will print at most 8 fractional digits, or fewer if an
element can be uniquely represented with fewer. A useful new mode is
``floatmode="unique"``, which will output enough digits to specify the array
elements uniquely.

Numpy complex-floating-scalars with values like ``inf*j`` or ``nan*j`` now
print as ``infj`` and ``nanj``, like the pure-python ``complex`` type.

The ``FloatFormat`` and ``LongFloatFormat`` classes are deprecated and should
both be replaced by ``FloatingFormat``. Similarly ``ComplexFormat`` and
``LongComplexFormat`` should be replaced by ``ComplexFloatingFormat``.

``void`` datatype elements are now printed in hex notation
----------------------------------------------------------
A hex representation compatible with the python ``bytes`` type is now printed
for unstructured ``np.void`` elements, e.g., ``V4`` datatype. Previously, in
python2 the raw void data of the element was printed to stdout, or in python3
the integer byte values were shown.

printing style for ``void`` datatypes is now independently customizable
-----------------------------------------------------------------------
The printing style of ``np.void`` arrays is now independently customizable
using the ``formatter`` argument to ``np.set_printoptions``, using the
``'void'`` key, instead of the catch-all ``numpystr`` key as before.

Reduced memory usage of ``np.loadtxt``
--------------------------------------
``np.loadtxt`` now reads files in chunks instead of all at once which decreases
its memory usage significantly for large files.


Changes
=======

Multiple-field indexing/assignment of structured arrays
-------------------------------------------------------
The indexing and assignment of structured arrays with multiple fields has
changed in a number of ways, as warned about in previous releases.

First, indexing a structured array with multiple fields, e.g.,
``arr[['f1', 'f3']]``, returns a view into the original array instead of a
copy. The returned view will have extra padding bytes corresponding to
intervening fields in the original array, unlike the copy in 1.13, which will
affect code such as ``arr[['f1', 'f3']].view(newdtype)``.

Second, assignment between structured arrays will now occur "by position"
instead of "by field name". The Nth field of the destination will be set to the
Nth field of the source regardless of field name, unlike in numpy versions 1.6
to 1.13 in which fields in the destination array were set to the
identically-named field in the source array or to 0 if the source did not have
a field.

Correspondingly, the order of fields in a structured dtypes now matters when
computing dtype equality. For example, with the dtypes ::

    x = dtype({'names': ['A', 'B'], 'formats': ['i4', 'f4'], 'offsets': [0, 4]})
    y = dtype({'names': ['B', 'A'], 'formats': ['f4', 'i4'], 'offsets': [4, 0]})

the expression ``x == y`` will now return ``False``, unlike before.
This makes dictionary based dtype specifications like
``dtype({'a': ('i4', 0), 'b': ('f4', 4)})`` dangerous in python < 3.6
since dict key order is not preserved in those versions.

Assignment from a structured array to a boolean array now raises a ValueError,
unlike in 1.13, where it always set the destination elements to ``True``.

Assignment from structured array with more than one field to a non-structured
array now raises a ValueError. In 1.13 this copied just the first field of the
source to the destination.

Using field "titles" in multiple-field indexing is now disallowed, as is
repeating a field name in a multiple-field index.

The documentation for structured arrays in the user guide has been
significantly updated to reflect these changes.

Integer and Void scalars are now unaffected by ``np.set_string_function``
-------------------------------------------------------------------------
Previously, unlike most other numpy scalars, the ``str`` and ``repr`` of
integer and void scalars could be controlled by ``np.set_string_function``.
This is no longer possible.

0d array printing changed, ``style`` arg of array2string deprecated
-------------------------------------------------------------------
Previously the ``str`` and ``repr`` of 0d arrays had idiosyncratic
implementations which returned ``str(a.item())`` and ``'array(' +
repr(a.item()) + ')'`` respectively for 0d array ``a``, unlike both numpy
scalars and higher dimension ndarrays.

Now, the ``str`` of a 0d array acts like a numpy scalar using ``str(a[()])``
and the ``repr`` acts like higher dimension arrays using ``formatter(a[()])``,
where  ``formatter``  can be specified using ``np.set_printoptions``. The
``style`` argument of ``np.array2string`` is deprecated.

This new behavior is disabled in 1.13 legacy printing mode, see compatibility
notes above.

Seeding ``RandomState`` using an array requires a 1-d array
-----------------------------------------------------------
``RandomState`` previously would accept empty arrays or arrays with 2 or more
dimensions, which resulted in either a failure to seed (empty arrays) or for
some of the passed values to be ignored when setting the seed.

``MaskedArray`` objects show a more useful ``repr``
---------------------------------------------------
The ``repr`` of a ``MaskedArray`` is now closer to the python code that would
produce it, with arrays now being shown with commas and dtypes. Like the other
formatting changes, this can be disabled with the 1.13 legacy printing mode in
order to help transition doctests.

The ``repr`` of ``np.polynomial`` classes is more explicit
----------------------------------------------------------
It now shows the domain and window parameters as keyword arguments to make
them more clear::

    >>> np.polynomial.Polynomial(range(4))
    Polynomial([0.,  1.,  2.,  3.], domain=[-1,  1], window=[-1,  1])
.. currentmodule:: numpy

==========================
NumPy 1.19.0 Release Notes
==========================
This NumPy release is marked by the removal of much technical debt: support for
Python 2 has been removed, many deprecations have been expired, and
documentation has been improved. The polishing of the random module continues
apace with bug fixes and better usability from Cython.

The Python versions supported for this release are 3.6-3.8. Downstream
developers should use Cython >= 0.29.16 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid problems on the Skylake architecture.


Highlights
==========

* Code compatibility with Python versions < 3.6 (including Python 2) was
  dropped from both the python and C code. The shims in ``numpy.compat`` will
  remain to support third-party packages, but they may be deprecated in a
  future release. Note that 1.19.x will *not* compile with earlier versions of
  Python due to the use of f-strings.

  (`gh-15233 <https://github.com/numpy/numpy/pull/15233>`__)


Expired deprecations
====================

``numpy.insert`` and ``numpy.delete`` can no longer be passed an axis on 0d arrays
----------------------------------------------------------------------------------
This concludes a deprecation from 1.9, where when an ``axis`` argument was
passed to a call to ``~numpy.insert`` and ``~numpy.delete`` on a 0d array, the
``axis`` and ``obj`` argument and indices would be completely ignored.
In these cases, ``insert(arr, "nonsense", 42, axis=0)`` would actually overwrite the
entire array, while ``delete(arr, "nonsense", axis=0)`` would be ``arr.copy()``

Now passing ``axis`` on a 0d array raises ``~numpy.AxisError``.

(`gh-15802 <https://github.com/numpy/numpy/pull/15802>`__)

``numpy.delete`` no longer ignores out-of-bounds indices
--------------------------------------------------------
This concludes deprecations from 1.8 and 1.9, where ``np.delete`` would ignore
both negative and out-of-bounds items in a sequence of indices. This was at
odds with its behavior when passed a single index.

Now out-of-bounds items throw ``IndexError``, and negative items index from the
end.

(`gh-15804 <https://github.com/numpy/numpy/pull/15804>`__)

``numpy.insert`` and ``numpy.delete`` no longer accept non-integral indices
---------------------------------------------------------------------------
This concludes a deprecation from 1.9, where sequences of non-integers indices
were allowed and cast to integers. Now passing sequences of non-integral
indices raises ``IndexError``, just like it does when passing a single
non-integral scalar.

(`gh-15805 <https://github.com/numpy/numpy/pull/15805>`__)

``numpy.delete`` no longer casts boolean indices to integers
------------------------------------------------------------
This concludes a deprecation from 1.8, where ``np.delete`` would cast boolean
arrays and scalars passed as an index argument into integer indices. The
behavior now is to treat boolean arrays as a mask, and to raise an error
on boolean scalars.

(`gh-15815 <https://github.com/numpy/numpy/pull/15815>`__)


Compatibility notes
===================

Changed random variate stream from ``numpy.random.Generator.dirichlet``
-----------------------------------------------------------------------
A bug in the generation of random variates for the Dirichlet distribution
with small 'alpha' values was fixed by using a different algorithm when
``max(alpha) < 0.1``.  Because of the change, the stream of variates
generated by ``dirichlet`` in this case will be different from previous
releases.

(`gh-14924 <https://github.com/numpy/numpy/pull/14924>`__)

Scalar promotion in ``PyArray_ConvertToCommonType``
---------------------------------------------------
The promotion of mixed scalars and arrays in ``PyArray_ConvertToCommonType``
has been changed to adhere to those used by ``np.result_type``.
This means that input such as ``(1000, np.array([1], dtype=np.uint8)))``
will now return ``uint16`` dtypes. In most cases the behaviour is unchanged.
Note that the use of this C-API function is generally discouraged.
This also fixes ``np.choose`` to behave the same way as the rest of NumPy
in this respect.

(`gh-14933 <https://github.com/numpy/numpy/pull/14933>`__)

Fasttake and fastputmask slots are deprecated and NULL'ed
---------------------------------------------------------
The fasttake and fastputmask slots are now never used and
must always be set to NULL. This will result in no change in behaviour.
However, if a user dtype should set one of these a DeprecationWarning
will be given.

(`gh-14942 <https://github.com/numpy/numpy/pull/14942>`__)

``np.ediff1d`` casting behaviour with ``to_end`` and ``to_begin``
-----------------------------------------------------------------
``np.ediff1d`` now uses the ``"same_kind"`` casting rule for
its additional ``to_end`` and ``to_begin`` arguments. This
ensures type safety except when the input array has a smaller
integer type than ``to_begin`` or ``to_end``.
In rare cases, the behaviour will be more strict than it was
previously in 1.16 and 1.17. This is necessary to solve issues
with floating point NaN.

(`gh-14981 <https://github.com/numpy/numpy/pull/14981>`__)

Converting of empty array-like objects to NumPy arrays
------------------------------------------------------
Objects with ``len(obj) == 0`` which implement an "array-like" interface,
meaning an object implementing ``obj.__array__()``,
``obj.__array_interface__``, ``obj.__array_struct__``, or the python
buffer interface and which are also sequences (i.e. Pandas objects)
will now always retain there shape correctly when converted to an array.
If such an object has a shape of ``(0, 1)`` previously, it could
be converted into an array of shape ``(0,)`` (losing all dimensions
after the first 0).

(`gh-14995 <https://github.com/numpy/numpy/pull/14995>`__)

Removed ``multiarray.int_asbuffer``
-----------------------------------
As part of the continued removal of Python 2 compatibility,
``multiarray.int_asbuffer`` was removed. On Python 3, it threw a
``NotImplementedError`` and was unused internally. It is expected that there
are no downstream use cases for this method with Python 3.

(`gh-15229 <https://github.com/numpy/numpy/pull/15229>`__)

``numpy.distutils.compat`` has been removed
-------------------------------------------
This module contained only the function ``get_exception()``, which was used as::

    try:
        ...
    except Exception:
        e = get_exception()

Its purpose was to handle the change in syntax introduced in Python 2.6, from
``except Exception, e:`` to ``except Exception as e:``, meaning it was only
necessary for codebases supporting Python 2.5 and older.

(`gh-15255 <https://github.com/numpy/numpy/pull/15255>`__)

``issubdtype`` no longer interprets ``float`` as ``np.floating``
----------------------------------------------------------------
``numpy.issubdtype`` had a FutureWarning since NumPy 1.14 which
has expired now. This means that certain input where the second
argument was neither a datatype nor a NumPy scalar type
(such as a string or a python type like ``int`` or ``float``)
will now be consistent with passing in ``np.dtype(arg2).type``.
This makes the result consistent with expectations and leads to
a false result in some cases which previously returned true.

(`gh-15773 <https://github.com/numpy/numpy/pull/15773>`__)

Change output of ``round`` on scalars to be consistent with Python
------------------------------------------------------------------

Output of the ``__round__`` dunder method and consequently the Python
built-in ``round`` has been changed to be a Python ``int`` to be consistent
with calling it on Python ``float`` objects when called with no arguments.
Previously, it would return a scalar of the ``np.dtype`` that was passed in.

(`gh-15840 <https://github.com/numpy/numpy/pull/15840>`__)

The ``numpy.ndarray`` constructor no longer interprets ``strides=()`` as ``strides=None``
-----------------------------------------------------------------------------------------
The former has changed to have the expected meaning of setting
``numpy.ndarray.strides`` to ``()``, while the latter continues to result in
strides being chosen automatically.

(`gh-15882 <https://github.com/numpy/numpy/pull/15882>`__)

C-Level string to datetime casts changed
----------------------------------------
The C-level casts from strings were simplified. This changed
also fixes string to datetime and timedelta casts to behave
correctly (i.e. like Python casts using ``string_arr.astype("M8")``
while previously the cast would behave like
``string_arr.astype(np.int_).astype("M8")``.
This only affects code using low-level C-API to do manual casts
(not full array casts) of single scalar values or using e.g.
``PyArray_GetCastFunc``, and should thus not affect the vast majority
of users.

(`gh-16068 <https://github.com/numpy/numpy/pull/16068>`__)

``SeedSequence`` with small seeds no longer conflicts with spawning
-------------------------------------------------------------------
Small seeds (less than ``2**96``) were previously implicitly 0-padded out to
128 bits, the size of the internal entropy pool. When spawned, the spawn key
was concatenated before the 0-padding. Since the first spawn key is ``(0,)``,
small seeds before the spawn created the same states as the first spawned
``SeedSequence``.  Now, the seed is explicitly 0-padded out to the internal
pool size before concatenating the spawn key. Spawned ``SeedSequences`` will
produce different results than in the previous release. Unspawned
``SeedSequences`` will still produce the same results.

(`gh-16551 <https://github.com/numpy/numpy/pull/16551>`__)


Deprecations
============

Deprecate automatic ``dtype=object`` for ragged input
-----------------------------------------------------
Calling ``np.array([[1, [1, 2, 3]])`` will issue a ``DeprecationWarning`` as
per `NEP 34`_. Users should explicitly use ``dtype=object`` to avoid the
warning.

.. _`NEP 34`: https://numpy.org/neps/nep-0034.html

(`gh-15119 <https://github.com/numpy/numpy/pull/15119>`__)

Passing ``shape=0`` to factory functions in ``numpy.rec`` is deprecated
-----------------------------------------------------------------------
``0`` is treated as a special case and is aliased to ``None`` in the functions:

* ``numpy.core.records.fromarrays``
* ``numpy.core.records.fromrecords``
* ``numpy.core.records.fromstring``
* ``numpy.core.records.fromfile``

In future, ``0`` will not be special cased, and will be treated as an array
length like any other integer.

(`gh-15217 <https://github.com/numpy/numpy/pull/15217>`__)

Deprecation of probably unused C-API functions
----------------------------------------------
The following C-API functions are probably unused and have been
deprecated:

* ``PyArray_GetArrayParamsFromObject``
* ``PyUFunc_GenericFunction``
* ``PyUFunc_SetUsesArraysAsData``

In most cases ``PyArray_GetArrayParamsFromObject`` should be replaced
by converting to an array, while ``PyUFunc_GenericFunction`` can be
replaced with ``PyObject_Call`` (see documentation for details).

(`gh-15427 <https://github.com/numpy/numpy/pull/15427>`__)

Converting certain types to dtypes is Deprecated
------------------------------------------------
The super classes of scalar types, such as ``np.integer``, ``np.generic``,
or ``np.inexact`` will now give a deprecation warning when converted
to a dtype (or used in a dtype keyword argument).
The reason for this is that ``np.integer`` is converted to ``np.int_``,
while it would be expected to represent *any* integer (e.g. also
``int8``, ``int16``, etc.
For example, ``dtype=np.floating`` is currently identical to
``dtype=np.float64``, even though also ``np.float32`` is a subclass of
``np.floating``.

(`gh-15534 <https://github.com/numpy/numpy/pull/15534>`__)

Deprecation of ``round`` for ``np.complexfloating`` scalars
-----------------------------------------------------------
Output of the ``__round__`` dunder method and consequently the Python built-in
``round`` has been deprecated on complex scalars. This does not affect
``np.round``.

(`gh-15840 <https://github.com/numpy/numpy/pull/15840>`__)

``numpy.ndarray.tostring()`` is deprecated in favor of ``tobytes()``
--------------------------------------------------------------------
``~numpy.ndarray.tobytes`` has existed since the 1.9 release, but until this
release ``~numpy.ndarray.tostring`` emitted no warning. The change to emit a
warning brings NumPy in line with the builtin ``array.array`` methods of the
same name.

(`gh-15867 <https://github.com/numpy/numpy/pull/15867>`__)


C API changes
=============

Better support for ``const`` dimensions in API functions
--------------------------------------------------------
The following functions now accept a constant array of ``npy_intp``:

* ``PyArray_BroadcastToShape``
* ``PyArray_IntTupleFromIntp``
* ``PyArray_OverflowMultiplyList``

Previously the caller would have to cast away the const-ness to call these
functions.

(`gh-15251 <https://github.com/numpy/numpy/pull/15251>`__)

Const qualify UFunc inner loops
-------------------------------
``UFuncGenericFunction`` now expects pointers to const ``dimension`` and
``strides`` as arguments. This means inner loops may no longer modify
either ``dimension`` or ``strides``. This change leads to an
``incompatible-pointer-types`` warning forcing users to either ignore
the compiler warnings or to const qualify their own loop signatures.

(`gh-15355 <https://github.com/numpy/numpy/pull/15355>`__)


New Features
============

``numpy.frompyfunc`` now accepts an identity argument
-----------------------------------------------------
This allows the :attr:``numpy.ufunc.identity`` attribute to be set on the
resulting ufunc, meaning it can be used for empty and multi-dimensional
calls to :meth:``numpy.ufunc.reduce``.

(`gh-8255 <https://github.com/numpy/numpy/pull/8255>`__)

``np.str_`` scalars now support the buffer protocol
---------------------------------------------------
``np.str_`` arrays are always stored as UCS4, so the corresponding scalars
now expose this through the buffer interface, meaning
``memoryview(np.str_('test'))`` now works.

(`gh-15385 <https://github.com/numpy/numpy/pull/15385>`__)

``subok`` option for ``numpy.copy``
-----------------------------------
A new kwarg, ``subok``, was added to ``numpy.copy`` to allow users to toggle
the behavior of ``numpy.copy`` with respect to array subclasses. The default
value is ``False`` which is consistent with the behavior of ``numpy.copy`` for
previous numpy versions. To create a copy that preserves an array subclass with
``numpy.copy``, call ``np.copy(arr, subok=True)``. This addition better
documents that the default behavior of ``numpy.copy`` differs from the
``numpy.ndarray.copy`` method which respects array subclasses by default.

(`gh-15685 <https://github.com/numpy/numpy/pull/15685>`__)

``numpy.linalg.multi_dot`` now accepts an ``out`` argument
----------------------------------------------------------

``out`` can be used to avoid creating unnecessary copies of the final product
computed by ``numpy.linalg.multidot``.

(`gh-15715 <https://github.com/numpy/numpy/pull/15715>`__)

``keepdims`` parameter for ``numpy.count_nonzero``
--------------------------------------------------
The parameter ``keepdims`` was added to ``numpy.count_nonzero``. The
parameter has the same meaning as it does in reduction functions such
as ``numpy.sum`` or ``numpy.mean``.

(`gh-15870 <https://github.com/numpy/numpy/pull/15870>`__)

``equal_nan`` parameter for ``numpy.array_equal``
-------------------------------------------------
The keyword argument ``equal_nan`` was added to ``numpy.array_equal``.
``equal_nan`` is a boolean value that toggles whether or not ``nan`` values are
considered equal in comparison (default is ``False``). This matches API used in
related functions such as ``numpy.isclose`` and ``numpy.allclose``.

(`gh-16128 <https://github.com/numpy/numpy/pull/16128>`__)


Improvements
============

Improve detection of CPU features
=================================
Replace ``npy_cpu_supports`` which was a gcc specific mechanism to test support
of AVX with more general functions ``npy_cpu_init`` and ``npy_cpu_have``, and
expose the results via a ``NPY_CPU_HAVE`` c-macro as well as a python-level
``__cpu_features__`` dictionary.

(`gh-13421 <https://github.com/numpy/numpy/pull/13421>`__)

Use 64-bit integer size on 64-bit platforms in fallback lapack_lite
-------------------------------------------------------------------
Use 64-bit integer size on 64-bit platforms in the fallback LAPACK library,
which is used when the system has no LAPACK installed, allowing it to deal with
linear algebra for large arrays.

(`gh-15218 <https://github.com/numpy/numpy/pull/15218>`__)

Use AVX512 intrinsic to implement ``np.exp`` when input is ``np.float64``
-------------------------------------------------------------------------
Use AVX512 intrinsic to implement ``np.exp`` when input is ``np.float64``,
which can improve the performance of ``np.exp`` with ``np.float64`` input 5-7x
faster than before. The ``_multiarray_umath.so`` module has grown about 63 KB
on linux64.

(`gh-15648 <https://github.com/numpy/numpy/pull/15648>`__)

Ability to disable madvise hugepages
------------------------------------
On Linux NumPy has previously added support for madavise hugepages which can
improve performance for very large arrays.  Unfortunately, on older Kernel
versions this led to performance regressions, thus by default the support has
been disabled on kernels before version 4.6. To override the default, you can
use the environment variable::

    NUMPY_MADVISE_HUGEPAGE=0

or set it to 1 to force enabling support. Note that this only makes
a difference if the operating system is set up to use madvise
transparent hugepage.

(`gh-15769 <https://github.com/numpy/numpy/pull/15769>`__)

``numpy.einsum`` accepts NumPy ``int64`` type in subscript list
---------------------------------------------------------------
There is no longer a type error thrown when ``numpy.einsum`` is passed
a NumPy ``int64`` array as its subscript list.

(`gh-16080 <https://github.com/numpy/numpy/pull/16080>`__)

``np.logaddexp2.identity`` changed to ``-inf``
----------------------------------------------
The ufunc ``~numpy.logaddexp2`` now has an identity of ``-inf``, allowing it to
be called on empty sequences.  This matches the identity of ``~numpy.logaddexp``.

(`gh-16102 <https://github.com/numpy/numpy/pull/16102>`__)


Changes
=======

Remove handling of extra argument to ``__array__``
--------------------------------------------------
A code path and test have been in the code since NumPy 0.4 for a two-argument
variant of ``__array__(dtype=None, context=None)``. It was activated when
calling ``ufunc(op)`` or ``ufunc.reduce(op)`` if ``op.__array__`` existed.
However that variant is not documented, and it is not clear what the intention
was for its use. It has been removed.

(`gh-15118 <https://github.com/numpy/numpy/pull/15118>`__)

``numpy.random._bit_generator`` moved to ``numpy.random.bit_generator``
-----------------------------------------------------------------------
In order to expose ``numpy.random.BitGenerator`` and
``numpy.random.SeedSequence`` to Cython, the ``_bitgenerator`` module is now
public as ``numpy.random.bit_generator``

Cython access to the random distributions is provided via a ``pxd`` file
------------------------------------------------------------------------
``c_distributions.pxd`` provides access to the c functions behind many of the
random distributions from Cython, making it convenient to use and extend them.

(`gh-15463 <https://github.com/numpy/numpy/pull/15463>`__)

Fixed ``eigh`` and ``cholesky`` methods in ``numpy.random.multivariate_normal``
-------------------------------------------------------------------------------
Previously, when passing ``method='eigh'`` or ``method='cholesky'``,
``numpy.random.multivariate_normal`` produced samples from the wrong
distribution. This is now fixed.

(`gh-15872 <https://github.com/numpy/numpy/pull/15872>`__)

Fixed the jumping implementation in ``MT19937.jumped``
------------------------------------------------------
This fix changes the stream produced from jumped MT19937 generators. It does
not affect the stream produced using ``RandomState`` or ``MT19937`` that
are directly seeded.

The translation of the jumping code for the MT19937 contained a reversed loop
ordering. ``MT19937.jumped`` matches the Makoto Matsumoto's original
implementation of the Horner and Sliding Window jump methods.

(`gh-16153 <https://github.com/numpy/numpy/pull/16153>`__)

=========================
NumPy 1.3.0 Release Notes
=========================

This minor includes numerous bug fixes, official python 2.6 support, and
several new features such as generalized ufuncs.

Highlights
==========

Python 2.6 support
------------------

Python 2.6 is now supported on all previously supported platforms, including
windows.

https://www.python.org/dev/peps/pep-0361/

Generalized ufuncs
------------------

There is a general need for looping over not only functions on scalars but also
over functions on vectors (or arrays), as explained on
http://scipy.org/scipy/numpy/wiki/GeneralLoopingFunctions. We propose to
realize this concept by generalizing the universal functions (ufuncs), and
provide a C implementation that adds ~500 lines to the numpy code base. In
current (specialized) ufuncs, the elementary function is limited to
element-by-element operations, whereas the generalized version supports
"sub-array" by "sub-array" operations. The Perl vector library PDL provides a
similar functionality and its terms are re-used in the following.

Each generalized ufunc has information associated with it that states what the
"core" dimensionality of the inputs is, as well as the corresponding
dimensionality of the outputs (the element-wise ufuncs have zero core
dimensions). The list of the core dimensions for all arguments is called the
"signature" of a ufunc. For example, the ufunc numpy.add has signature
"(),()->()" defining two scalar inputs and one scalar output.

Another example is (see the GeneralLoopingFunctions page) the function
inner1d(a,b) with a signature of "(i),(i)->()". This applies the inner product
along the last axis of each input, but keeps the remaining indices intact. For
example, where a is of shape (3,5,N) and b is of shape (5,N), this will return
an output of shape (3,5). The underlying elementary function is called 3*5
times. In the signature, we specify one core dimension "(i)" for each input and
zero core dimensions "()" for the output, since it takes two 1-d arrays and
returns a scalar. By using the same name "i", we specify that the two
corresponding dimensions should be of the same size (or one of them is of size
1 and will be broadcasted).

The dimensions beyond the core dimensions are called "loop" dimensions. In the
above example, this corresponds to (3,5).

The usual numpy "broadcasting" rules apply, where the signature determines how
the dimensions of each input/output object are split into core and loop
dimensions:

While an input array has a smaller dimensionality than the corresponding number
of core dimensions, 1's are pre-pended to its shape.  The core dimensions are
removed from all inputs and the remaining dimensions are broadcasted; defining
the loop dimensions.  The output is given by the loop dimensions plus the
output core dimensions.

Experimental Windows 64 bits support
------------------------------------

Numpy can now be built on windows 64 bits (amd64 only, not IA64), with both MS
compilers and mingw-w64 compilers:

This is *highly experimental*: DO NOT USE FOR PRODUCTION USE. See INSTALL.txt,
Windows 64 bits section for more information on limitations and how to build it
by yourself.

New features
============

Formatting issues
-----------------

Float formatting is now handled by numpy instead of the C runtime: this enables
locale independent formatting, more robust fromstring and related methods.
Special values (inf and nan) are also more consistent across platforms (nan vs
IND/NaN, etc...), and more consistent with recent python formatting work (in
2.6 and later).

Nan handling in max/min
-----------------------

The maximum/minimum ufuncs now reliably propagate nans. If one of the
arguments is a nan, then nan is returned. This affects np.min/np.max, amin/amax
and the array methods max/min. New ufuncs fmax and fmin have been added to deal
with non-propagating nans.

Nan handling in sign
--------------------

The ufunc sign now returns nan for the sign of anan.


New ufuncs
----------

#. fmax - same as maximum for integer types and non-nan floats. Returns the
   non-nan argument if one argument is nan and returns nan if both arguments
   are nan.
#. fmin - same as minimum for integer types and non-nan floats. Returns the
   non-nan argument if one argument is nan and returns nan if both arguments
   are nan.
#. deg2rad - converts degrees to radians, same as the radians ufunc.
#. rad2deg - converts radians to degrees, same as the degrees ufunc.
#. log2 - base 2 logarithm.
#. exp2 - base 2 exponential.
#. trunc - truncate floats to nearest integer towards zero.
#. logaddexp - add numbers stored as logarithms and return the logarithm
   of the result.
#. logaddexp2 - add numbers stored as base 2 logarithms and return the base 2
   logarithm of the result.

Masked arrays
-------------

Several new features and bug fixes, including:

	* structured arrays should now be fully supported by MaskedArray
	  (r6463, r6324, r6305, r6300, r6294...)
	* Minor bug fixes (r6356, r6352, r6335, r6299, r6298)
	* Improved support for __iter__ (r6326)
	* made baseclass, sharedmask and hardmask accessible to the user (but
	  read-only)
	* doc update

gfortran support on windows
---------------------------

Gfortran can now be used as a fortran compiler for numpy on windows, even when
the C compiler is Visual Studio (VS 2005 and above; VS 2003 will NOT work).
Gfortran + Visual studio does not work on windows 64 bits (but gcc + gfortran
does). It is unclear whether it will be possible to use gfortran and visual
studio at all on x64.

Arch option for windows binary
------------------------------

Automatic arch detection can now be bypassed from the command line for the superpack installed:

	numpy-1.3.0-superpack-win32.exe /arch=nosse

will install a numpy which works on any x86, even if the running computer
supports SSE set.

Deprecated features
===================

Histogram
---------

The semantics of histogram has been modified to fix long-standing issues
with outliers handling. The main changes concern

#. the definition of the bin edges, now including the rightmost edge, and
#. the handling of upper outliers, now ignored rather than tallied in the
   rightmost bin.

The previous behavior is still accessible using `new=False`, but this is
deprecated, and will be removed entirely in 1.4.0.

Documentation changes
=====================

A lot of documentation has been added. Both user guide and references can be
built from sphinx.

New C API
=========

Multiarray API
--------------

The following functions have been added to the multiarray C API:

	* PyArray_GetEndianness: to get runtime endianness

Ufunc API
---------

The following functions have been added to the ufunc API:

	* PyUFunc_FromFuncAndDataAndSignature: to declare a more general ufunc
	  (generalized ufunc).


New defines
-----------

New public C defines are available for ARCH specific code through numpy/npy_cpu.h:

	* NPY_CPU_X86: x86 arch (32 bits)
        * NPY_CPU_AMD64: amd64 arch (x86_64, NOT Itanium)
        * NPY_CPU_PPC: 32 bits ppc
        * NPY_CPU_PPC64: 64 bits ppc
        * NPY_CPU_SPARC: 32 bits sparc
        * NPY_CPU_SPARC64: 64 bits sparc
        * NPY_CPU_S390: S390
        * NPY_CPU_IA64: ia64
        * NPY_CPU_PARISC: PARISC

New macros for CPU endianness has been added as well (see internal changes
below for details):

	* NPY_BYTE_ORDER: integer
	* NPY_LITTLE_ENDIAN/NPY_BIG_ENDIAN defines

Those provide portable alternatives to glibc endian.h macros for platforms
without it.

Portable NAN, INFINITY, etc...
------------------------------

npy_math.h now makes available several portable macro to get NAN, INFINITY:

        * NPY_NAN: equivalent to NAN, which is a GNU extension
        * NPY_INFINITY: equivalent to C99 INFINITY
        * NPY_PZERO, NPY_NZERO: positive and negative zero respectively

Corresponding single and extended precision macros are available as well. All
references to NAN, or home-grown computation of NAN on the fly have been
removed for consistency.

Internal changes
================

numpy.core math configuration revamp
------------------------------------

This should make the porting to new platforms easier, and more robust. In
particular, the configuration stage does not need to execute any code on the
target platform, which is a first step toward cross-compilation.

https://www.numpy.org/neps/nep-0003-math_config_clean.html

umath refactor
--------------

A lot of code cleanup for umath/ufunc code (charris).

Improvements to build warnings
------------------------------

Numpy can now build with -W -Wall without warnings

https://www.numpy.org/neps/nep-0002-warnfix.html

Separate core math library
--------------------------

The core math functions (sin, cos, etc... for basic C types) have been put into
a separate library; it acts as a compatibility layer, to support most C99 maths
functions (real only for now). The library includes platform-specific fixes for
various maths functions, such as using those versions should be more robust
than using your platform functions directly. The API for existing functions is
exactly the same as the C99 math functions API; the only difference is the npy
prefix (npy_cos vs cos).

The core library will be made available to any extension in 1.4.0.

CPU arch detection
------------------

npy_cpu.h defines numpy specific CPU defines, such as NPY_CPU_X86, etc...
Those are portable across OS and toolchains, and set up when the header is
parsed, so that they can be safely used even in the case of cross-compilation
(the values is not set when numpy is built), or for multi-arch binaries (e.g.
fat binaries on Max OS X).

npy_endian.h defines numpy specific endianness defines, modeled on the glibc
endian.h. NPY_BYTE_ORDER  is equivalent to BYTE_ORDER, and one of
NPY_LITTLE_ENDIAN or NPY_BIG_ENDIAN is defined. As for CPU archs, those are set
when the header is parsed by the compiler, and as such can be used for
cross-compilation and multi-arch binaries.
.. currentmodule:: numpy

==========================
NumPy 1.19.5 Release Notes
==========================

NumPy 1.19.5 is a short bugfix release. Apart from fixing several bugs, the
main improvement is the update to OpenBLAS 0.3.13 that works around the windows
2004 bug while not breaking execution on other platforms. This release supports
Python 3.6-3.9 and is planned to be the last release in the 1.19.x cycle.

Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Christoph Gohlke
* Matti Picus
* Raghuveer Devulapalli
* Sebastian Berg
* Simon Graham +
* Veniamin Petrenko +
* Bernie Gray +

Pull requests merged
====================

A total of 11 pull requests were merged for this release.

* `#17756 <https://github.com/numpy/numpy/pull/17756>`__: BUG: Fix segfault due to out of bound pointer in floatstatus...
* `#17774 <https://github.com/numpy/numpy/pull/17774>`__: BUG: fix np.timedelta64('nat').__format__ throwing an exception
* `#17775 <https://github.com/numpy/numpy/pull/17775>`__: BUG: Fixed file handle leak in array_tofile.
* `#17786 <https://github.com/numpy/numpy/pull/17786>`__: BUG: Raise recursion error during dimension discovery
* `#17917 <https://github.com/numpy/numpy/pull/17917>`__: BUG: Fix subarray dtype used with too large count in fromfile
* `#17918 <https://github.com/numpy/numpy/pull/17918>`__: BUG: 'bool' object has no attribute 'ndim'
* `#17919 <https://github.com/numpy/numpy/pull/17919>`__: BUG: ensure _UFuncNoLoopError can be pickled
* `#17924 <https://github.com/numpy/numpy/pull/17924>`__: BLD: use BUFFERSIZE=20 in OpenBLAS
* `#18026 <https://github.com/numpy/numpy/pull/18026>`__: BLD: update to OpenBLAS 0.3.13
* `#18036 <https://github.com/numpy/numpy/pull/18036>`__: BUG: make a variable volatile to work around clang compiler bug
* `#18114 <https://github.com/numpy/numpy/pull/18114>`__: REL: Prepare for the NumPy 1.19.5 release.
.. currentmodule:: numpy

==========================
NumPy 1.18.4 Release Notes
==========================

This is the last planned release in the 1.18.x series. It reverts the
``bool("0")`` behavior introduced in 1.18.3 and fixes a bug in
``Generator.integers``. There is also a link to a new troubleshooting section
in the documentation included in the error message emitted when numpy import
fails.

The Python versions supported in this release are 3.5-3.8. Downstream
developers should use Cython >= 0.29.15 for Python 3.8 support and
OpenBLAS >= 3.7 to avoid errors on the Skylake architecture.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Sebastian Berg
* Warren Weckesser

Pull requests merged
====================

A total of 6 pull requests were merged for this release.

* `#16055 <https://github.com/numpy/numpy/pull/16055>`__: BLD: add i686 for 1.18 builds
* `#16090 <https://github.com/numpy/numpy/pull/16090>`__: BUG: random: ``Generator.integers(2**32)`` always returned 0.
* `#16091 <https://github.com/numpy/numpy/pull/16091>`__: BLD: fix path to libgfortran on macOS
* `#16109 <https://github.com/numpy/numpy/pull/16109>`__: REV: Reverts side-effect changes to casting
* `#16114 <https://github.com/numpy/numpy/pull/16114>`__: BLD: put openblas library in local directory on windows
* `#16132 <https://github.com/numpy/numpy/pull/16132>`__: DOC: Change import error "howto" to link to new troubleshooting...
==========================
NumPy 1.10.1 Release Notes
==========================

This release deals with a few build problems that showed up in 1.10.0. Most
users would not have seen these problems. The differences are:

* Compiling with msvc9 or msvc10 for 32 bit Windows now requires SSE2.
  This was the easiest fix for what looked to be some miscompiled code when
  SSE2 was not used. If you need to compile for 32 bit Windows systems
  without SSE2 support, mingw32 should still work.

* Make compiling with VS2008 python2.7 SDK easier

* Change Intel compiler options so that code will also be generated to
  support systems without SSE4.2.

* Some _config test functions needed an explicit integer return in
  order to avoid the openSUSE rpmlinter erring out.

* We ran into a problem with pipy not allowing reuse of filenames and a
  resulting proliferation of *.*.*.postN releases. Not only were the names
  getting out of hand, some packages were unable to work with the postN
  suffix.


Numpy 1.10.1 supports Python 2.6 - 2.7 and 3.2 - 3.5.


Commits:

45a3d84 DEP: Remove warning for `full` when dtype is set.
0c1a5df BLD: import setuptools to allow compile with VS2008 python2.7 sdk
04211c6 BUG: mask nan to 1 in ordered compare
826716f DOC: Document the reason msvc requires SSE2 on 32 bit platforms.
49fa187 BLD: enable SSE2 for 32-bit msvc 9 and 10 compilers
dcbc4cc MAINT: remove Wreturn-type warnings from config checks
d6564cb BLD: do not build exclusively for SSE4.2 processors
15cb66f BLD: do not build exclusively for SSE4.2 processors
c38bc08 DOC: fix var. reference in percentile docstring
78497f4 DOC: Sync 1.10.0-notes.rst in 1.10.x branch with master.

.. currentmodule:: numpy

==========================
NumPy 1.19.4 Release Notes
==========================

NumPy 1.19.4 is a quick release to revert the OpenBLAS library version.  It was
hoped that the 0.3.12 OpenBLAS version used in 1.19.3 would work around the
Microsoft fmod bug, but problems in some docker environments turned up. Instead,
1.19.4 will use the older library and run a sanity check on import, raising an
error if the problem is detected. Microsoft is aware of the problem and has
promised a fix, users should upgrade when it becomes available.

This release supports Python 3.6-3.9

Contributors
============

A total of 1 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#17679 <https://github.com/numpy/numpy/pull/17679>`__: MAINT: Add check for Windows 10 version 2004 bug.
* `#17680 <https://github.com/numpy/numpy/pull/17680>`__: REV: Revert OpenBLAS to 1.19.2 version for 1.19.4
.. currentmodule:: numpy

==========================
NumPy 1.18.2 Release Notes
==========================

This small release contains a fix for a performance regression in numpy/random
and several bug/maintenance updates.

The Python versions supported in this release are 3.5-3.8. Downstream
developers should use Cython >= 0.29.15 for Python 3.8 support and OpenBLAS >=
3.7 to avoid errors on the Skylake architecture.


Contributors
============

A total of 5 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Ganesh Kathiresan +
* Matti Picus
* Sebastian Berg
* przemb +


Pull requests merged
====================

A total of 7 pull requests were merged for this release.

* `#15675 <https://github.com/numpy/numpy/pull/15675>`__: TST: move _no_tracing to testing._private
* `#15676 <https://github.com/numpy/numpy/pull/15676>`__: MAINT: Large overhead in some random functions
* `#15677 <https://github.com/numpy/numpy/pull/15677>`__: TST: Do not create gfortran link in azure Mac testing.
* `#15679 <https://github.com/numpy/numpy/pull/15679>`__: BUG: Added missing error check in `ndarray.__contains__`
* `#15722 <https://github.com/numpy/numpy/pull/15722>`__: MAINT: use list-based APIs to call subprocesses
* `#15729 <https://github.com/numpy/numpy/pull/15729>`__: REL: Prepare for 1.18.2 release.
* `#15734 <https://github.com/numpy/numpy/pull/15734>`__: BUG: fix logic error when nm fails on 32-bit
.. currentmodule:: numpy

==========================
NumPy 1.18.1 Release Notes
==========================

This release contains fixes for bugs reported against NumPy 1.18.0.  Two bugs
in particular that caused widespread problems downstream were:

- The cython random extension test was not using a temporary directory for
  building, resulting in a permission violation. Fixed.

- Numpy distutils was appending `-std=c99` to all C compiler runs, leading to
  changed behavior and compile problems downstream. That flag is now only
  applied when building numpy C code.

The Python versions supported in this release are 3.5-3.8. Downstream
developers should use Cython >= 0.29.14 for Python 3.8 support and OpenBLAS >=
3.7 to avoid errors on the Skylake architecture.

Contributors
============

A total of 7 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Matti Picus
* Maxwell Aladago
* Pauli Virtanen
* Ralf Gommers
* Tyler Reddy
* Warren Weckesser

Pull requests merged
====================

A total of 13 pull requests were merged for this release.

* `#15158 <https://github.com/numpy/numpy/pull/15158>`__: MAINT: Update pavement.py for towncrier.
* `#15159 <https://github.com/numpy/numpy/pull/15159>`__: DOC: add moved modules to 1.18 release note
* `#15161 <https://github.com/numpy/numpy/pull/15161>`__: MAINT, DOC: Minor backports and updates for 1.18.x
* `#15176 <https://github.com/numpy/numpy/pull/15176>`__: TST: Add assert_array_equal test for big integer arrays
* `#15184 <https://github.com/numpy/numpy/pull/15184>`__: BUG: use tmp dir and check version for cython test (#15170)
* `#15220 <https://github.com/numpy/numpy/pull/15220>`__: BUG: distutils: fix msvc+gfortran openblas handling corner case
* `#15221 <https://github.com/numpy/numpy/pull/15221>`__: BUG: remove -std=c99 for c++ compilation (#15194)
* `#15222 <https://github.com/numpy/numpy/pull/15222>`__: MAINT: unskip test on win32
* `#15223 <https://github.com/numpy/numpy/pull/15223>`__: TST: add BLAS ILP64 run in Travis & Azure
* `#15245 <https://github.com/numpy/numpy/pull/15245>`__: MAINT: only add --std=c99 where needed
* `#15246 <https://github.com/numpy/numpy/pull/15246>`__: BUG: lib: Fix handling of integer arrays by gradient.
* `#15247 <https://github.com/numpy/numpy/pull/15247>`__: MAINT: Do not use private Python function in testing
* `#15250 <https://github.com/numpy/numpy/pull/15250>`__: REL: Prepare for the NumPy 1.18.1 release.
==========================
NumPy 1.14.5 Release Notes
==========================

This is a bugfix release for bugs reported following the 1.14.4 release. The
most significant fixes are:

* fixes for compilation errors on alpine and NetBSD

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python
3.6 wheels available from PIP are built with Python 3.6.2 and should be
compatible with all previous versions of Python 3.6. The source releases were
cythonized with Cython 0.28.2 and should work for the upcoming Python 3.7.

Contributors
============

A total of 1 person contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris

Pull requests merged
====================

A total of 2 pull requests were merged for this release.

* `#11274 <https://github.com/numpy/numpy/pull/11274>`__: BUG: Correct use of NPY_UNUSED.
* `#11294 <https://github.com/numpy/numpy/pull/11294>`__: BUG: Remove extra trailing parentheses.

==========================
NumPy 1.12.1 Release Notes
==========================

NumPy 1.12.1 supports Python 2.7 and 3.4 - 3.6 and fixes bugs and regressions
found in NumPy 1.12.0. In particular, the regression in f2py constant parsing
is fixed. Wheels for Linux, Windows, and OSX can be found on PyPI,

Bugs Fixed
==========

*  BUG: Fix wrong future nat warning and equiv type logic error...
*  BUG: Fix wrong masked median for some special cases
*  DOC: Place np.average in inline code
*  TST: Work around isfinite inconsistency on i386
*  BUG: Guard against replacing constants without '_' spec in f2py.
*  BUG: Fix mean for float 16 non-array inputs for 1.12
*  BUG: Fix calling python api with error set and minor leaks for...
*  BUG: Make iscomplexobj compatible with custom dtypes again
*  BUG: Fix undefined behaviour induced by bad __array_wrap__
*  BUG: Fix MaskedArray.__setitem__
*  BUG: PPC64el machines are POWER for Fortran in f2py
*  BUG: Look up methods on MaskedArray in `_frommethod`
*  BUG: Remove extra digit in binary_repr at limit
*  BUG: Fix deepcopy regression for empty arrays.
*  BUG: Fix ma.median for empty ndarrays
.. currentmodule:: numpy

==========================
NumPy 1.20.1 Release Notes
==========================

NumPy 1,20.1 is a rapid bugfix release fixing several bugs and regressions
reported after the 1.20.0 release.


Highlights
==========

- The distutils bug that caused problems with downstream projects is fixed.
- The ``random.shuffle`` regression is fixed.


Contributors
============

A total of 8 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Charles Harris
* Nicholas McKibben +
* Pearu Peterson
* Ralf Gommers
* Sebastian Berg
* Tyler Reddy
* @Aerysv +


Pull requests merged
====================

A total of 15 pull requests were merged for this release.

* `#18306 <https://github.com/numpy/numpy/pull/18306>`__: MAINT: Add missing placeholder annotations
* `#18310 <https://github.com/numpy/numpy/pull/18310>`__: BUG: Fix typo in ``numpy.__init__.py``
* `#18326 <https://github.com/numpy/numpy/pull/18326>`__: BUG: don't mutate list of fake libraries while iterating over...
* `#18327 <https://github.com/numpy/numpy/pull/18327>`__: MAINT: gracefully shuffle memoryviews
* `#18328 <https://github.com/numpy/numpy/pull/18328>`__: BUG: Use C linkage for random distributions
* `#18336 <https://github.com/numpy/numpy/pull/18336>`__: CI: fix when GitHub Actions builds trigger, and allow ci skips
* `#18337 <https://github.com/numpy/numpy/pull/18337>`__: BUG: Allow unmodified use of isclose, allclose, etc. with timedelta
* `#18345 <https://github.com/numpy/numpy/pull/18345>`__: BUG: Allow pickling all relevant DType types/classes
* `#18351 <https://github.com/numpy/numpy/pull/18351>`__: BUG: Fix missing signed_char dependency. Closes #18335.
* `#18352 <https://github.com/numpy/numpy/pull/18352>`__: DOC: Change license date 2020 -> 2021
* `#18353 <https://github.com/numpy/numpy/pull/18353>`__: CI: CircleCI seems to occasionally time out, increase the limit
* `#18354 <https://github.com/numpy/numpy/pull/18354>`__: BUG: Fix f2py bugs when wrapping F90 subroutines.
* `#18356 <https://github.com/numpy/numpy/pull/18356>`__: MAINT: crackfortran regex simplify
* `#18357 <https://github.com/numpy/numpy/pull/18357>`__: BUG: threads.h existence test requires GLIBC > 2.12.
* `#18359 <https://github.com/numpy/numpy/pull/18359>`__: REL: Prepare for the NumPy 1.20.1 release.
:orphan:

==========================
NumPy 1.xx.x Release Notes
==========================


Highlights
==========


New functions
=============


Deprecations
============


Future Changes
==============


Expired deprecations
====================


Compatibility notes
===================


C API changes
=============


New Features
============


Improvements
============


Changes
=======
.. currentmodule:: numpy

==========================
NumPy 1.22.0 Release Notes
==========================
NumPy 1.22.0 is a big release featuring the work of 153 contributors spread
over 609 pull requests. There have been many improvements, highlights are:

* Annotations of the main namespace are essentially complete. Upstream is a
  moving target, so there will likely be further improvements, but the major
  work is done. This is probably the most user visible enhancement in this
  release.
* A preliminary version of the proposed Array-API is provided. This is a step
  in creating a standard collection of functions that can be used across
  applications such as CuPy and JAX.
* NumPy now has a DLPack backend. DLPack provides a common interchange format
  for array (tensor) data.
* New methods for ``quantile``, ``percentile``, and related functions. The new
  methods provide a complete set of the methods commonly found in the
  literature.
* The universal functions have been refactored to implement most of
  :ref:`NEP 43 <NEP43>`.  This also unlocks the ability to experiment with the
  future DType API.
* A new configurable allocator for use by downstream projects.

These are in addition to the ongoing work to provide SIMD support for commonly
used functions, improvements to F2PY, and better documentation.

The Python versions supported in this release are 3.8-3.10, Python 3.7 has been
dropped. Note that 32 bit wheels are only provided for Python 3.8 and 3.9 on
Windows, all other wheels are 64 bits on account of Ubuntu, Fedora, and other
Linux distributions dropping 32 bit support. All 64 bit wheels are also linked
with 64 bit integer OpenBLAS, which should fix the occasional problems
encountered by folks using truly huge arrays.


Expired deprecations
====================

Deprecated numeric style dtype strings have been removed
--------------------------------------------------------
Using the strings ``"Bytes0"``, ``"Datetime64"``, ``"Str0"``, ``"Uint32"``,
and ``"Uint64"`` as a dtype will now raise a ``TypeError``.

(`gh-19539 <https://github.com/numpy/numpy/pull/19539>`__)

Expired deprecations for ``loads``, ``ndfromtxt``, and ``mafromtxt`` in npyio
-----------------------------------------------------------------------------
``numpy.loads`` was deprecated in v1.15, with the recommendation that users use
``pickle.loads`` instead.  ``ndfromtxt`` and ``mafromtxt`` were both deprecated
in v1.17 - users should use ``numpy.genfromtxt`` instead with the appropriate
value for the ``usemask`` parameter.

(`gh-19615 <https://github.com/numpy/numpy/pull/19615>`__)


Deprecations
============

Use delimiter rather than delimitor as kwarg in mrecords
--------------------------------------------------------
The misspelled keyword argument ``delimitor`` of
``numpy.ma.mrecords.fromtextfile()`` has been changed to ``delimiter``, using
it will emit a deprecation warning.

(`gh-19921 <https://github.com/numpy/numpy/pull/19921>`__)

Passing boolean ``kth`` values to (arg-)partition has been deprecated
---------------------------------------------------------------------
``numpy.partition`` and ``numpy.argpartition`` would previously accept boolean
values for the ``kth`` parameter, which would subsequently be converted into
integers. This behavior has now been deprecated.

(`gh-20000 <https://github.com/numpy/numpy/pull/20000>`__)

The ``np.MachAr`` class has been deprecated
-------------------------------------------
The ``numpy.MachAr`` class and ``finfo.machar <numpy.finfo>`` attribute have
been deprecated. Users are encouraged to access the property if interest
directly from the corresponding ``numpy.finfo`` attribute.

(`gh-20201 <https://github.com/numpy/numpy/pull/20201>`__)


Compatibility notes
===================

Distutils forces strict floating point model on clang
-----------------------------------------------------
NumPy now sets the ``-ftrapping-math`` option on clang to enforce correct
floating point error handling for universal functions.  Clang defaults to
non-IEEE and C99 conform behaviour otherwise.  This change (using the
equivalent but newer ``-ffp-exception-behavior=strict``) was attempted in NumPy
1.21, but was effectively never used.

(`gh-19479 <https://github.com/numpy/numpy/pull/19479>`__)

Removed floor division support for complex types
------------------------------------------------
Floor division of complex types will now result in a ``TypeError``

.. code-block:: python

    >>> a = np.arange(10) + 1j* np.arange(10)
    >>> a // 1
    TypeError: ufunc 'floor_divide' not supported for the input types...

(`gh-19135 <https://github.com/numpy/numpy/pull/19135>`__)

``numpy.vectorize`` functions now produce the same output class as the base function
------------------------------------------------------------------------------------
When a function that respects ``numpy.ndarray`` subclasses is vectorized using
``numpy.vectorize``, the vectorized function will now be subclass-safe also for
cases that a signature is given (i.e., when creating a ``gufunc``): the output
class will be the same as that returned by the first call to the underlying
function.

(`gh-19356 <https://github.com/numpy/numpy/pull/19356>`__)

Python 3.7 is no longer supported
---------------------------------
Python support has been dropped. This is rather strict, there are changes that
require Python >= 3.8.

(`gh-19665 <https://github.com/numpy/numpy/pull/19665>`__)

str/repr of complex dtypes now include space after punctuation
--------------------------------------------------------------
The repr of ``np.dtype({"names": ["a"], "formats": [int], "offsets": [2]})`` is
now ``dtype({'names': ['a'], 'formats': ['<i8'], 'offsets': [2], 'itemsize':
10})``, whereas spaces where previously omitted after colons and between
fields.

The old behavior can be restored via ``np.set_printoptions(legacy="1.21")``.

(`gh-19687 <https://github.com/numpy/numpy/pull/19687>`__)

Corrected ``advance`` in ``PCG64DSXM`` and ``PCG64``
----------------------------------------------------
Fixed a bug in the ``advance`` method of ``PCG64DSXM`` and ``PCG64``. The bug
only affects results when the step was larger than :math:`2^{64}` on platforms
that do not support 128-bit integers(e.g., Windows and 32-bit Linux).

(`gh-20049 <https://github.com/numpy/numpy/pull/20049>`__)

Change in generation of random 32 bit floating point variates
-------------------------------------------------------------
There was bug in the generation of 32 bit floating point values from the
uniform distribution that would result in the least significant bit of the
random variate always being 0.  This has been fixed.

This change affects the variates produced by the ``random.Generator`` methods
``random``, ``standard_normal``, ``standard_exponential``, and
``standard_gamma``, but only when the dtype is specified as ``numpy.float32``.

(`gh-20314 <https://github.com/numpy/numpy/pull/20314>`__)


C API changes
=============

Masked inner-loops cannot be customized anymore
-----------------------------------------------
The masked inner-loop selector is now never used.  A warning will be given in
the unlikely event that it was customized.

We do not expect that any code uses this.  If you do use it, you must unset the
selector on newer NumPy version.  Please also contact the NumPy developers, we
do anticipate providing a new, more specific, mechanism.

The customization was part of a never-implemented feature to allow for faster
masked operations.

(`gh-19259 <https://github.com/numpy/numpy/pull/19259>`__)

Experimental exposure of future DType and UFunc API
---------------------------------------------------
The new header ``experimental_public_dtype_api.h`` allows to experiment with
future API for improved universal function and especially user DType support.
At this time it is advisable to experiment using the development version
of NumPy since some changes are expected and new features will be unlocked.

(`gh-19919 <https://github.com/numpy/numpy/pull/19919>`__)


New Features
============

NEP 49 configurable allocators
------------------------------
As detailed in `NEP 49`_, the function used for allocation of the data segment
of a ndarray can be changed. The policy can be set globally or in a context.
For more information see the NEP and the :ref:`data_memory` reference docs.
Also add a ``NUMPY_WARN_IF_NO_MEM_POLICY`` override to warn on dangerous use
of transfering ownership by setting ``NPY_ARRAY_OWNDATA``.

.. _`NEP 49`: https://numpy.org/neps/nep-0049.html

(`gh-17582 <https://github.com/numpy/numpy/pull/17582>`__)

Implementation of the NEP 47 (adopting the array API standard)
--------------------------------------------------------------
An initial implementation of `NEP 47`_ (adoption the array API standard) has
been added as ``numpy.array_api``. The implementation is experimental and will
issue a UserWarning on import, as the `array API standard
<https://data-apis.org/array-api/latest/index.html>`_ is still in draft state.
``numpy.array_api`` is a conforming implementation of the array API standard,
which is also minimal, meaning that only those functions and behaviors that are
required by the standard are implemented (see the NEP for more info).
Libraries wishing to make use of the array API standard are encouraged to use
``numpy.array_api`` to check that they are only using functionality that is
guaranteed to be present in standard conforming implementations.

.. _`NEP 47`: https://numpy.org/neps/nep-0047-array-api-standard.html

(`gh-18585 <https://github.com/numpy/numpy/pull/18585>`__)

Generate C/C++ API reference documentation from comments blocks is now possible
-------------------------------------------------------------------------------
This feature depends on Doxygen_ in the generation process and on Breathe_ to
integrate it with Sphinx.

.. _`Doxygen`: https://www.doxygen.nl/index.html
.. _`Breathe`: https://breathe.readthedocs.io/en/latest/

(`gh-18884 <https://github.com/numpy/numpy/pull/18884>`__)

Assign the platform-specific ``c_intp`` precision via a mypy plugin
-------------------------------------------------------------------
The mypy_ plugin, introduced in `numpy/numpy#17843`_, has again been expanded:
the plugin now is now responsible for setting the platform-specific precision
of ``numpy.ctypeslib.c_intp``, the latter being used as data type for various
``numpy.ndarray.ctypes`` attributes.

Without the plugin, aforementioned type will default to ``ctypes.c_int64``.

To enable the plugin, one must add it to their mypy `configuration file`_:

.. code-block:: ini

    [mypy]
    plugins = numpy.typing.mypy_plugin


.. _mypy: http://mypy-lang.org/
.. _configuration file: https://mypy.readthedocs.io/en/stable/config_file.html
.. _`numpy/numpy#17843`: https://github.com/numpy/numpy/pull/17843

(`gh-19062 <https://github.com/numpy/numpy/pull/19062>`__)

Add NEP 47-compatible dlpack support
------------------------------------
Add a ``ndarray.__dlpack__()`` method which returns a ``dlpack`` C structure
wrapped in a ``PyCapsule``. Also add a ``np._from_dlpack(obj)`` function, where
``obj`` supports ``__dlpack__()``, and returns an ``ndarray``.

(`gh-19083 <https://github.com/numpy/numpy/pull/19083>`__)

``keepdims`` optional argument added to ``numpy.argmin``, ``numpy.argmax``
--------------------------------------------------------------------------
``keepdims`` argument is added to ``numpy.argmin``, ``numpy.argmax``.  If set
to ``True``, the axes which are reduced are left in the result as dimensions
with size one.  The resulting array has the same number of dimensions and will
broadcast with the input array.

(`gh-19211 <https://github.com/numpy/numpy/pull/19211>`__)

``bit_count`` to compute the number of 1-bits in an integer
-----------------------------------------------------------
Computes the number of 1-bits in the absolute value of the input.
This works on all the numpy integer types. Analogous to the builtin
``int.bit_count`` or ``popcount`` in C++.

.. code-block:: python

    >>> np.uint32(1023).bit_count()
    10
    >>> np.int32(-127).bit_count()
    7

(`gh-19355 <https://github.com/numpy/numpy/pull/19355>`__)

The ``ndim`` and ``axis`` attributes have been added to ``numpy.AxisError``
---------------------------------------------------------------------------
The ``ndim`` and ``axis`` parameters are now also stored as attributes
within each ``numpy.AxisError`` instance.

(`gh-19459 <https://github.com/numpy/numpy/pull/19459>`__)

Preliminary support for ``windows/arm64`` target
------------------------------------------------
``numpy`` added support for windows/arm64 target. Please note ``OpenBLAS``
support is not yet available for windows/arm64 target.

(`gh-19513 <https://github.com/numpy/numpy/pull/19513>`__)

Added support for LoongArch
---------------------------
LoongArch is a new instruction set, numpy compilation failure on LoongArch
architecture, so add the commit.

(`gh-19527 <https://github.com/numpy/numpy/pull/19527>`__)

A ``.clang-format`` file has been added
---------------------------------------
Clang-format is a C/C++ code formatter, together with the added
``.clang-format`` file, it produces code close enough to the NumPy
C_STYLE_GUIDE for general use. Clang-format version 12+ is required due to the
use of several new features, it is available in Fedora 34 and Ubuntu Focal
among other distributions.

(`gh-19754 <https://github.com/numpy/numpy/pull/19754>`__)

``is_integer`` is now available to ``numpy.floating`` and ``numpy.integer``
---------------------------------------------------------------------------
Based on its counterpart in Python ``float`` and ``int``, the numpy floating
point and integer types now support ``float.is_integer``. Returns ``True`` if
the number is finite with integral value, and ``False`` otherwise.

.. code-block:: python

    >>> np.float32(-2.0).is_integer()
    True
    >>> np.float64(3.2).is_integer()
    False
    >>> np.int32(-2).is_integer()
    True

(`gh-19803 <https://github.com/numpy/numpy/pull/19803>`__)

Symbolic parser for Fortran dimension specifications
----------------------------------------------------
A new symbolic parser has been added to f2py in order to correctly parse
dimension specifications. The parser is the basis for future improvements and
provides compatibility with Draft Fortran 202x.

(`gh-19805 <https://github.com/numpy/numpy/pull/19805>`__)

``ndarray``, ``dtype`` and ``number`` are now runtime-subscriptable
-------------------------------------------------------------------
Mimicking :pep:`585`, the ``numpy.ndarray``, ``numpy.dtype`` and
``numpy.number`` classes are now subscriptable for python 3.9 and later.
Consequently, expressions that were previously only allowed in .pyi stub files
or with the help of ``from __future__ import annotations`` are now also legal
during runtime.

.. code-block:: python

    >>> import numpy as np
    >>> from typing import Any

    >>> np.ndarray[Any, np.dtype[np.float64]]
    numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]

(`gh-19879 <https://github.com/numpy/numpy/pull/19879>`__)


Improvements
============

``ctypeslib.load_library`` can now take any path-like object
------------------------------------------------------------
All parameters in the can now take any :term:`python:path-like object`.
This includes the likes of strings, bytes and objects implementing the
:meth:`__fspath__<os.PathLike.__fspath__>` protocol.

(`gh-17530 <https://github.com/numpy/numpy/pull/17530>`__)

Add ``smallest_normal`` and ``smallest_subnormal`` attributes to ``finfo``
--------------------------------------------------------------------------
The attributes ``smallest_normal`` and ``smallest_subnormal`` are available as
an extension of ``finfo`` class for any floating-point data type. To use these
new attributes, write ``np.finfo(np.float64).smallest_normal`` or
``np.finfo(np.float64).smallest_subnormal``.

(`gh-18536 <https://github.com/numpy/numpy/pull/18536>`__)

``numpy.linalg.qr`` accepts stacked matrices as inputs
------------------------------------------------------
``numpy.linalg.qr`` is able to produce results for stacked matrices as inputs.
Moreover, the implementation of QR decomposition has been shifted to C from
Python.

(`gh-19151 <https://github.com/numpy/numpy/pull/19151>`__)

``numpy.fromregex`` now accepts ``os.PathLike`` implementations
---------------------------------------------------------------
``numpy.fromregex`` now accepts objects implementing the ``__fspath__<os.PathLike>``
protocol, *e.g.* ``pathlib.Path``.

(`gh-19680 <https://github.com/numpy/numpy/pull/19680>`__)

Add new methods for ``quantile`` and ``percentile``
---------------------------------------------------
``quantile`` and ``percentile`` now have have a ``method=`` keyword argument
supporting 13 different methods.  This replaces the ``interpolation=`` keyword
argument.

The methods are now aligned with nine methods which can be found in scientific
literature and the R language.  The remaining methods are the previous
discontinuous variations of the default "linear" one.

Please see the documentation of ``numpy.percentile`` for more information.

(`gh-19857 <https://github.com/numpy/numpy/pull/19857>`__)

Missing parameters have been added to the ``nan<x>`` functions
--------------------------------------------------------------
A number of the ``nan<x>`` functions previously lacked parameters that were
present in their ``<x>``-based counterpart, *e.g.* the ``where`` parameter was
present in ``numpy.mean`` but absent from ``numpy.nanmean``.

The following parameters have now been added to the ``nan<x>`` functions:

* nanmin: ``initial`` & ``where``
* nanmax: ``initial`` & ``where``
* nanargmin: ``keepdims`` & ``out``
* nanargmax: ``keepdims`` & ``out``
* nansum: ``initial`` & ``where``
* nanprod: ``initial`` & ``where``
* nanmean: ``where``
* nanvar: ``where``
* nanstd: ``where``

(`gh-20027 <https://github.com/numpy/numpy/pull/20027>`__)

Annotating the main Numpy namespace
-----------------------------------
Starting from the 1.20 release, PEP 484 type annotations have been included for
parts of the NumPy library; annotating the remaining functions being a work in
progress. With the release of 1.22 this process has been completed for the main
NumPy namespace, which is now fully annotated.

Besides the main namespace, a limited number of sub-packages contain
annotations as well. This includes, among others, ``numpy.testing``,
``numpy.linalg`` and ``numpy.random`` (available since 1.21).

(`gh-20217 <https://github.com/numpy/numpy/pull/20217>`__)

Vectorize umath module using AVX-512
-------------------------------------
By leveraging Intel Short Vector Math Library (SVML), 18 umath functions
(``exp2``, ``log2``, ``log10``, ``expm1``, ``log1p``, ``cbrt``, ``sin``,
``cos``, ``tan``, ``arcsin``, ``arccos``, ``arctan``, ``sinh``, ``cosh``,
``tanh``, ``arcsinh``, ``arccosh``, ``arctanh``) are vectorized using AVX-512
instruction set for both single and double precision implementations.  This
change is currently enabled only for Linux users and on processors with AVX-512
instruction set.  It provides an average speed up of 32x and 14x for single and
double precision functions respectively.

(`gh-19478 <https://github.com/numpy/numpy/pull/19478>`__)

OpenBLAS v0.3.18
----------------
Update the OpenBLAS used in testing and in wheels to v0.3.18

(`gh-20058 <https://github.com/numpy/numpy/pull/20058>`__)

.. currentmodule:: numpy

==========================
NumPy 1.21.2 Release Notes
==========================

The NumPy 1.21.2 is a maintenance release that fixes bugs discovered after
1.21.1. It also provides 64 bit manylinux Python 3.10.0rc1 wheels for
downstream testing. Note that Python 3.10 is not yet final. It also has
preliminary support for Windows on ARM64, but there is no OpenBLAS for that
platform and no wheels are available.

The Python versions supported for this release are 3.7-3.9. The 1.21.x series
is compatible with Python 3.10.0rc1 and Python 3.10 will be officially
supported after it is released. The previous problems with gcc-11.1 have been
fixed by gcc-11.2, check your version if you are using gcc-11.


Contributors
============

A total of 10 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Bas van Beek
* Carl Johnsen +
* Charles Harris
* Gwyn Ciesla +
* Matthieu Dartiailh
* Matti Picus
* Niyas Sait +
* Ralf Gommers
* Sayed Adel
* Sebastian Berg


Pull requests merged
====================

A total of 18 pull requests were merged for this release.

* `#19497 <https://github.com/numpy/numpy/pull/19497>`__: MAINT: set Python version for 1.21.x to ``<3.11``
* `#19533 <https://github.com/numpy/numpy/pull/19533>`__: BUG: Fix an issue wherein importing ``numpy.typing`` could raise
* `#19646 <https://github.com/numpy/numpy/pull/19646>`__: MAINT: Update Cython version for Python 3.10.
* `#19648 <https://github.com/numpy/numpy/pull/19648>`__: TST: Bump the python 3.10 test version from beta4 to rc1
* `#19651 <https://github.com/numpy/numpy/pull/19651>`__: TST: avoid distutils.sysconfig in runtests.py
* `#19652 <https://github.com/numpy/numpy/pull/19652>`__: MAINT: add missing dunder method to nditer type hints
* `#19656 <https://github.com/numpy/numpy/pull/19656>`__: BLD, SIMD: Fix testing extra checks when ``-Werror`` isn't applicable...
* `#19657 <https://github.com/numpy/numpy/pull/19657>`__: BUG: Remove logical object ufuncs with bool output
* `#19658 <https://github.com/numpy/numpy/pull/19658>`__: MAINT: Include .coveragerc in source distributions to support...
* `#19659 <https://github.com/numpy/numpy/pull/19659>`__: BUG: Fix bad write in masked iterator output copy paths
* `#19660 <https://github.com/numpy/numpy/pull/19660>`__: ENH: Add support for windows on arm targets
* `#19661 <https://github.com/numpy/numpy/pull/19661>`__: BUG: add base to templated arguments for platlib
* `#19662 <https://github.com/numpy/numpy/pull/19662>`__: BUG,DEP: Non-default UFunc signature/dtype usage should be deprecated
* `#19666 <https://github.com/numpy/numpy/pull/19666>`__: MAINT: Add Python 3.10 to supported versions.
* `#19668 <https://github.com/numpy/numpy/pull/19668>`__: TST,BUG: Sanitize path-separators when running ``runtest.py``
* `#19671 <https://github.com/numpy/numpy/pull/19671>`__: BLD: load extra flags when checking for libflame
* `#19676 <https://github.com/numpy/numpy/pull/19676>`__: BLD: update circleCI docker image
* `#19677 <https://github.com/numpy/numpy/pull/19677>`__: REL: Prepare for 1.21.2 release.
==========================
NumPy 1.10.4 Release Notes
==========================

This release is a bugfix source release motivated by a segfault regression.
No windows binaries are provided for this release, as there appear to be
bugs in the toolchain we use to generate those files. Hopefully that
problem will be fixed for the next release. In the meantime, we suggest
using one of the providers of windows binaries.

Compatibility notes
===================

* The trace function now calls the trace method on subclasses of ndarray,
  except for matrix, for which the current behavior is preserved. This is
  to help with the units package of AstroPy and hopefully will not cause
  problems.

Issues Fixed
============

* gh-6922 BUG: numpy.recarray.sort segfaults on Windows.
* gh-6937 BUG: busday_offset does the wrong thing with modifiedpreceding roll.
* gh-6949 BUG: Type is lost when slicing a subclass of recarray.

Merged PRs
==========

The following PRs have been merged into 1.10.4. When the PR is a backport,
the PR number for the original PR against master is listed.

* gh-6840 TST: Update travis testing script in 1.10.x
* gh-6843 BUG: Fix use of python 3 only FileNotFoundError in test_f2py.
* gh-6884 REL: Update pavement.py and setup.py to reflect current version.
* gh-6916 BUG: Fix test_f2py so it runs correctly in runtests.py.
* gh-6924 BUG: Fix segfault gh-6922.
* gh-6942 Fix datetime roll='modifiedpreceding' bug.
* gh-6943 DOC,BUG: Fix some latex generation problems.
* gh-6950 BUG trace is not subclass aware, np.trace(ma) != ma.trace().
* gh-6952 BUG recarray slices should preserve subclass.
=========================
NumPy 1.9.2 Release Notes
=========================

This is a bugfix only release in the 1.9.x series.

Issues fixed
============

* `#5316 <https://github.com/numpy/numpy/issues/5316>`__: fix too large dtype alignment of strings and complex types
* `#5424 <https://github.com/numpy/numpy/issues/5424>`__: fix ma.median when used on ndarrays
* `#5481 <https://github.com/numpy/numpy/issues/5481>`__: Fix astype for structured array fields of different byte order
* `#5354 <https://github.com/numpy/numpy/issues/5354>`__: fix segfault when clipping complex arrays
* `#5524 <https://github.com/numpy/numpy/issues/5524>`__: allow np.argpartition on non ndarrays
* `#5612 <https://github.com/numpy/numpy/issues/5612>`__: Fixes ndarray.fill to accept full range of uint64
* `#5155 <https://github.com/numpy/numpy/issues/5155>`__: Fix loadtxt with comments=None and a string None data
* `#4476 <https://github.com/numpy/numpy/issues/4476>`__: Masked array view fails if structured dtype has datetime component
* `#5388 <https://github.com/numpy/numpy/issues/5388>`__: Make RandomState.set_state and RandomState.get_state threadsafe
* `#5390 <https://github.com/numpy/numpy/issues/5390>`__: make seed, randint and shuffle threadsafe
* `#5374 <https://github.com/numpy/numpy/issues/5374>`__: Fixed incorrect assert_array_almost_equal_nulp documentation
* `#5393 <https://github.com/numpy/numpy/issues/5393>`__: Add support for ATLAS > 3.9.33.
* `#5313 <https://github.com/numpy/numpy/issues/5313>`__: PyArray_AsCArray caused segfault for 3d arrays
* `#5492 <https://github.com/numpy/numpy/issues/5492>`__: handle out of memory in rfftf
* `#4181 <https://github.com/numpy/numpy/issues/4181>`__: fix a few bugs in the random.pareto docstring
* `#5359 <https://github.com/numpy/numpy/issues/5359>`__: minor changes to linspace docstring
* `#4723 <https://github.com/numpy/numpy/issues/4723>`__: fix a compile issues on AIX
==========================
NumPy 1.14.6 Release Notes
==========================

This is a bugfix release for bugs reported following the 1.14.5 release. The
most significant fixes are:

* Fix for behavior change in ``ma.masked_values(shrink=True)``
* Fix the new cached allocations machinery to be thread safe.

The Python versions supported in this release are 2.7 and 3.4 - 3.7. The Python
3.6 wheels on PyPI should be compatible with all Python 3.6 versions.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Charles Harris
* Eric Wieser
* Julian Taylor
* Matti Picus

Pull requests merged
====================

A total of 4 pull requests were merged for this release.

* `#11985 <https://github.com/numpy/numpy/pull/11985>`__: BUG: fix cached allocations without the GIL
* `#11986 <https://github.com/numpy/numpy/pull/11986>`__: BUG: Undo behavior change in ma.masked_values(shrink=True)
* `#11987 <https://github.com/numpy/numpy/pull/11987>`__: BUG: fix refcount leak in PyArray_AdaptFlexibleDType
* `#11995 <https://github.com/numpy/numpy/pull/11995>`__: TST: Add Python 3.7 testing to NumPy 1.14.
=========================
NumPy 1.5.0 Release Notes
=========================


Highlights
==========

Python 3 compatibility
----------------------

This is the first NumPy release which is compatible with Python 3. Support for
Python 3 and Python 2 is done from a single code base. Extensive notes on
changes can be found at
`<https://web.archive.org/web/20100814160313/http://projects.scipy.org/numpy/browser/trunk/doc/Py3K.txt>`_.

Note that the Numpy testing framework relies on nose, which does not have a
Python 3 compatible release yet. A working Python 3 branch of nose can be found
at `<https://web.archive.org/web/20100817112505/http://bitbucket.org/jpellerin/nose3/>`_ however.

Porting of SciPy to Python 3 is expected to be completed soon.

:pep:`3118` compatibility
-------------------------

The new buffer protocol described by PEP 3118 is fully supported in this
version of Numpy. On Python versions >= 2.6 Numpy arrays expose the buffer
interface, and array(), asarray() and other functions accept new-style buffers
as input.


New features
============

Warning on casting complex to real
----------------------------------

Numpy now emits a `numpy.ComplexWarning` when a complex number is cast
into a real number. For example:

    >>> x = np.array([1,2,3])
    >>> x[:2] = np.array([1+2j, 1-2j])
    ComplexWarning: Casting complex values to real discards the imaginary part

The cast indeed discards the imaginary part, and this may not be the
intended behavior in all cases, hence the warning. This warning can be
turned off in the standard way:

    >>> import warnings
    >>> warnings.simplefilter("ignore", np.ComplexWarning)

Dot method for ndarrays
-----------------------

Ndarrays now have the dot product also as a method, which allows writing
chains of matrix products as

    >>> a.dot(b).dot(c)

instead of the longer alternative

    >>> np.dot(a, np.dot(b, c))

linalg.slogdet function
-----------------------

The slogdet function returns the sign and logarithm of the determinant
of a matrix. Because the determinant may involve the product of many
small/large values, the result is often more accurate than that obtained
by simple multiplication.

new header
----------

The new header file ndarraytypes.h contains the symbols from
ndarrayobject.h that do not depend on the PY_ARRAY_UNIQUE_SYMBOL and
NO_IMPORT/_ARRAY macros. Broadly, these symbols are types, typedefs,
and enumerations; the array function calls are left in
ndarrayobject.h. This allows users to include array-related types and
enumerations without needing to concern themselves with the macro
expansions and their side- effects.


Changes
=======

polynomial.polynomial
---------------------

* The polyint and polyder functions now check that the specified number
  integrations or derivations is a non-negative integer. The number 0 is
  a valid value for both functions.
* A degree method has been added to the Polynomial class.
* A trimdeg method has been added to the Polynomial class. It operates like
  truncate except that the argument is the desired degree of the result,
  not the number of coefficients.
* Polynomial.fit now uses None as the default domain for the fit. The default
  Polynomial domain can be specified by using [] as the domain value.
* Weights can be used in both polyfit and Polynomial.fit
* A linspace method has been added to the Polynomial class to ease plotting.
* The polymulx function was added.

polynomial.chebyshev
--------------------

* The chebint and chebder functions now check that the specified number
  integrations or derivations is a non-negative integer. The number 0 is
  a valid value for both functions.
* A degree method has been added to the Chebyshev class.
* A trimdeg method has been added to the Chebyshev class. It operates like
  truncate except that the argument is the desired degree of the result,
  not the number of coefficients.
* Chebyshev.fit now uses None as the default domain for the fit. The default
  Chebyshev domain can be specified by using [] as the domain value.
* Weights can be used in both chebfit and Chebyshev.fit
* A linspace method has been added to the Chebyshev class to ease plotting.
* The chebmulx function was added.
* Added functions for the Chebyshev points of the first and second kind.


histogram
---------

After a two years transition period, the old behavior of the histogram function
has been phased out, and the "new" keyword has been removed.

correlate
---------

The old behavior of correlate was deprecated in 1.4.0, the new behavior (the
usual definition for cross-correlation) is now the default.
==========================
NumPy 1.14.2 Release Notes
==========================

This is a bugfix release for some bugs reported following the 1.14.1 release. The major
problems dealt with are as follows.

* Residual bugs in the new array printing functionality.
* Regression resulting in a relocation problem with shared library.
* Improved PyPy compatibility.

The Python versions supported in this release are 2.7 and 3.4 - 3.6. The Python
3.6 wheels available from PIP are built with Python 3.6.2 and should be
compatible with all previous versions of Python 3.6. The source releases were
cythonized with Cython 0.26.1, which is known to **not** support the upcoming
Python 3.7 release.  People who wish to run Python 3.7 should check out the
NumPy repo and try building with the, as yet, unreleased master branch of
Cython.

Contributors
============

A total of 4 people contributed to this release.  People with a "+" by their
names contributed a patch for the first time.

* Allan Haldane
* Charles Harris
* Eric Wieser
* Pauli Virtanen

Pull requests merged
====================

A total of 5 pull requests were merged for this release.

* `#10674 <https://github.com/numpy/numpy/pull/10674>`__: BUG: Further back-compat fix for subclassed array repr
* `#10725 <https://github.com/numpy/numpy/pull/10725>`__: BUG: dragon4 fractional output mode adds too many trailing zeros
* `#10726 <https://github.com/numpy/numpy/pull/10726>`__: BUG: Fix f2py generated code to work on PyPy
* `#10727 <https://github.com/numpy/numpy/pull/10727>`__: BUG: Fix missing NPY_VISIBILITY_HIDDEN on npy_longdouble_to_PyLong
* `#10729 <https://github.com/numpy/numpy/pull/10729>`__: DOC: Create 1.14.2 notes and changelog.
.. _development-workflow:

====================
Development workflow
====================

You already have your own forked copy of the NumPy_ repository, by
following :ref:`forking`, :ref:`set-up-fork`, you have configured git_
by following :ref:`configure-git`, and have linked the upstream
repository as explained in :ref:`linking-to-upstream`.

What is described below is a recommended workflow with Git.

Basic workflow
##############

In short:

1. Start a new *feature branch* for each set of edits that you do.
   See :ref:`below <making-a-new-feature-branch>`.

2. Hack away! See :ref:`below <editing-workflow>`

3. When finished:

   - *Contributors*: push your feature branch to your own Github repo, and
     :ref:`create a pull request <asking-for-merging>`.

   - *Core developers*: If you want to push changes without
     further review, see the notes :ref:`below <pushing-to-main>`.

This way of working helps to keep work well organized and the history
as clear as possible.

.. seealso::

   There are many online tutorials to help you `learn git`_. For discussions
   of specific git workflows, see these discussions on `linux git workflow`_,
   and `ipython git workflow`_.

.. _making-a-new-feature-branch:

Making a new feature branch
===========================

First, fetch new commits from the ``upstream`` repository:

::

   git fetch upstream

Then, create a new branch based on the main branch of the upstream
repository::

   git checkout -b my-new-feature upstream/main


.. _editing-workflow:

The editing workflow
====================

Overview
--------

::

   # hack hack
   git status # Optional
   git diff # Optional
   git add modified_file
   git commit
   # push the branch to your own Github repo
   git push origin my-new-feature

In more detail
--------------

#. Make some changes. When you feel that you've made a complete, working set
   of related changes, move on to the next steps.

#. Optional: Check which files have changed with ``git status`` (see `git
   status`_).  You'll see a listing like this one::

     # On branch my-new-feature
     # Changed but not updated:
     #   (use "git add <file>..." to update what will be committed)
     #   (use "git checkout -- <file>..." to discard changes in working directory)
     #
     #	modified:   README
     #
     # Untracked files:
     #   (use "git add <file>..." to include in what will be committed)
     #
     #	INSTALL
     no changes added to commit (use "git add" and/or "git commit -a")

#. Optional: Compare the changes with the previous version using with ``git
   diff`` (`git diff`_). This brings up a simple text browser interface that
   highlights the difference between your files and the previous version.

#. Add any relevant modified or new files using  ``git add modified_file``
   (see `git add`_). This puts the files into a staging area, which is a queue
   of files that will be added to your next commit. Only add files that have
   related, complete changes. Leave files with unfinished changes for later
   commits.

#. To commit the staged files into the local copy of your repo, do ``git
   commit``. At this point, a text editor will open up to allow you to write a
   commit message. Read the :ref:`commit message
   section<writing-the-commit-message>` to be sure that you are writing a
   properly formatted and sufficiently detailed commit message. After saving
   your message and closing the editor, your commit will be saved. For trivial
   commits, a short commit message can be passed in through the command line
   using the ``-m`` flag. For example, ``git commit -am "ENH: Some message"``.

   In some cases, you will see this form of the commit command: ``git commit
   -a``. The extra ``-a`` flag automatically commits all modified files and
   removes all deleted files. This can save you some typing of numerous ``git
   add`` commands; however, it can add unwanted changes to a commit if you're
   not careful. For more information, see `why the -a flag?`_ - and the
   helpful use-case description in the `tangled working copy problem`_.

#. Push the changes to your forked repo on github_::

      git push origin my-new-feature

   For more information, see `git push`_.

.. note::

   Assuming you have followed the instructions in these pages, git will create
   a default link to your github_ repo called ``origin``.  In git >= 1.7 you
   can ensure that the link to origin is permanently set by using the
   ``--set-upstream`` option::

      git push --set-upstream origin my-new-feature

   From now on git_ will know that ``my-new-feature`` is related to the
   ``my-new-feature`` branch in your own github_ repo. Subsequent push calls
   are then simplified to the following::

      git push

   You have to use ``--set-upstream`` for each new branch that you create.


It may be the case that while you were working on your edits, new commits have
been added to ``upstream`` that affect your work. In this case, follow the
:ref:`rebasing-on-main` section of this document to apply those changes to
your branch.

.. _writing-the-commit-message:

Writing the commit message
--------------------------

Commit messages should be clear and follow a few basic rules.  Example::

   ENH: add functionality X to numpy.<submodule>.

   The first line of the commit message starts with a capitalized acronym
   (options listed below) indicating what type of commit this is.  Then a blank
   line, then more text if needed.  Lines shouldn't be longer than 72
   characters.  If the commit is related to a ticket, indicate that with
   "See #3456", "See ticket 3456", "Closes #3456" or similar.

Describing the motivation for a change, the nature of a bug for bug fixes or
some details on what an enhancement does are also good to include in a commit
message.  Messages should be understandable without looking at the code
changes.  A commit message like ``MAINT: fixed another one`` is an example of
what not to do; the reader has to go look for context elsewhere.

Standard acronyms to start the commit message with are::

   API: an (incompatible) API change
   BENCH: changes to the benchmark suite
   BLD: change related to building numpy
   BUG: bug fix
   DEP: deprecate something, or remove a deprecated object
   DEV: development tool or utility
   DOC: documentation
   ENH: enhancement
   MAINT: maintenance commit (refactoring, typos, etc.)
   REV: revert an earlier commit
   STY: style fix (whitespace, PEP8)
   TST: addition or modification of tests
   TYP: static typing
   REL: related to releasing numpy

Commands to skip continuous integration
```````````````````````````````````````

By default a lot of continuous integration (CI) jobs are run for every PR,
from running the test suite on different operating systems and hardware
platforms to building the docs. In some cases you already know that CI isn't
needed (or not all of it), for example if you work on CI config files, text in
the README, or other files that aren't involved in regular build, test or docs
sequences. In such cases you may explicitly skip CI by including one of these
fragments in your commit message::

   ``[ci skip]``: skip as much CI as possible (not all jobs can be skipped)
   ``[skip github]``: skip GitHub Actions "build numpy and run tests" jobs
   ``[skip travis]``: skip TravisCI jobs
   ``[skip azurepipelines]``: skip Azure jobs

*Note*: unfortunately not all CI systems implement this feature well, or at all.
CircleCI supports ``ci skip`` but has no command to skip only CircleCI.
Azure chooses to still run jobs with skip commands on PRs, the jobs only get
skipped on merging to master.


.. _workflow_mailing_list:

Get the mailing list's opinion
=======================================================

If you plan a new feature or API change, it's wisest to first email the
NumPy `mailing list <https://mail.python.org/mailman/listinfo/numpy-discussion>`_
asking for comment. If you haven't heard back in a week, it's
OK to ping the list again.

.. _asking-for-merging:

Asking for your changes to be merged with the main repo
=======================================================

When you feel your work is finished, you can create a pull request (PR). Github
has a nice help page that outlines the process for `filing pull requests`_.

If your changes involve modifications to the API or addition/modification of a
function, add a release note to the ``doc/release/upcoming_changes/``
directory, following the instructions and format in the
``doc/release/upcoming_changes/README.rst`` file.


.. _workflow_PR_timeline:

Getting your PR reviewed
========================

We review pull requests as soon as we can, typically within a week. If you get
no review comments within two weeks, feel free to ask for feedback by
adding a comment on your PR (this will notify maintainers).

If your PR is large or complicated, asking for input on the numpy-discussion
mailing list may also be useful.



.. _rebasing-on-main:

Rebasing on main
================

This updates your feature branch with changes from the upstream `NumPy
github`_ repo. If you do not absolutely need to do this, try to avoid doing
it, except perhaps when you are finished. The first step will be to update
the remote repository with new commits from upstream::

   git fetch upstream

Next, you need to update the feature branch::

   # go to the feature branch
   git checkout my-new-feature
   # make a backup in case you mess up
   git branch tmp my-new-feature
   # rebase on upstream main branch
   git rebase upstream/main

If you have made changes to files that have changed also upstream,
this may generate merge conflicts that you need to resolve. See
:ref:`below<recovering-from-mess-up>` for help in this case.

Finally, remove the backup branch upon a successful rebase::

   git branch -D tmp


.. note::

   Rebasing on main is preferred over merging upstream back to your
   branch. Using ``git merge`` and ``git pull`` is discouraged when
   working on feature branches.

.. _recovering-from-mess-up:

Recovering from mess-ups
========================

Sometimes, you mess up merges or rebases. Luckily, in Git it is
relatively straightforward to recover from such mistakes.

If you mess up during a rebase::

   git rebase --abort

If you notice you messed up after the rebase::

   # reset branch back to the saved point
   git reset --hard tmp

If you forgot to make a backup branch::

   # look at the reflog of the branch
   git reflog show my-feature-branch

   8630830 my-feature-branch@{0}: commit: BUG: io: close file handles immediately
   278dd2a my-feature-branch@{1}: rebase finished: refs/heads/my-feature-branch onto 11ee694744f2552d
   26aa21a my-feature-branch@{2}: commit: BUG: lib: make seek_gzip_factory not leak gzip obj
   ...

   # reset the branch to where it was before the botched rebase
   git reset --hard my-feature-branch@{2}

If you didn't actually mess up but there are merge conflicts, you need to
resolve those.  This can be one of the trickier things to get right.  For a
good description of how to do this, see `this article on merging conflicts`_.


Additional things you might want to do
######################################

.. _rewriting-commit-history:

Rewriting commit history
========================

.. note::

   Do this only for your own feature branches.

There's an embarrassing typo in a commit you made? Or perhaps you
made several false starts you would like the posterity not to see.

This can be done via *interactive rebasing*.

Suppose that the commit history looks like this::

    git log --oneline
    eadc391 Fix some remaining bugs
    a815645 Modify it so that it works
    2dec1ac Fix a few bugs + disable
    13d7934 First implementation
    6ad92e5 * masked is now an instance of a new object, MaskedConstant
    29001ed Add pre-nep for a couple of structured_array_extensions.
    ...

and ``6ad92e5`` is the last commit in the ``main`` branch. Suppose we
want to make the following changes:

* Rewrite the commit message for ``13d7934`` to something more sensible.
* Combine the commits ``2dec1ac``, ``a815645``, ``eadc391`` into a single one.

We do as follows::

    # make a backup of the current state
    git branch tmp HEAD
    # interactive rebase
    git rebase -i 6ad92e5

This will open an editor with the following text in it::

    pick 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    pick a815645 Modify it so that it works
    pick eadc391 Fix some remaining bugs

    # Rebase 6ad92e5..eadc391 onto 6ad92e5
    #
    # Commands:
    #  p, pick = use commit
    #  r, reword = use commit, but edit the commit message
    #  e, edit = use commit, but stop for amending
    #  s, squash = use commit, but meld into previous commit
    #  f, fixup = like "squash", but discard this commit's log message
    #
    # If you remove a line here THAT COMMIT WILL BE LOST.
    # However, if you remove everything, the rebase will be aborted.
    #

To achieve what we want, we will make the following changes to it::

    r 13d7934 First implementation
    pick 2dec1ac Fix a few bugs + disable
    f a815645 Modify it so that it works
    f eadc391 Fix some remaining bugs

This means that (i) we want to edit the commit message for
``13d7934``, and (ii) collapse the last three commits into one. Now we
save and quit the editor.

Git will then immediately bring up an editor for editing the commit
message. After revising it, we get the output::

    [detached HEAD 721fc64] FOO: First implementation
     2 files changed, 199 insertions(+), 66 deletions(-)
    [detached HEAD 0f22701] Fix a few bugs + disable
     1 files changed, 79 insertions(+), 61 deletions(-)
    Successfully rebased and updated refs/heads/my-feature-branch.

and the history looks now like this::

     0f22701 Fix a few bugs + disable
     721fc64 ENH: Sophisticated feature
     6ad92e5 * masked is now an instance of a new object, MaskedConstant

If it went wrong, recovery is again possible as explained :ref:`above
<recovering-from-mess-up>`.

Deleting a branch on github_
============================

::

   git checkout main
   # delete branch locally
   git branch -D my-unwanted-branch
   # delete branch on github
   git push origin --delete my-unwanted-branch

See also:
https://stackoverflow.com/questions/2003505/how-do-i-delete-a-git-branch-locally-and-remotely


Several people sharing a single repository
==========================================

If you want to work on some stuff with other people, where you are all
committing into the same repository, or even the same branch, then just
share it via github_.

First fork NumPy into your account, as from :ref:`forking`.

Then, go to your forked repository github page, say
``https://github.com/your-user-name/numpy``

Click on the 'Admin' button, and add anyone else to the repo as a
collaborator:

   .. image:: pull_button.png

Now all those people can do::

    git clone git@github.com:your-user-name/numpy.git

Remember that links starting with ``git@`` use the ssh protocol and are
read-write; links starting with ``git://`` are read-only.

Your collaborators can then commit directly into that repo with the
usual::

     git commit -am 'ENH - much better code'
     git push origin my-feature-branch # pushes directly into your repo

Exploring your repository
=========================

To see a graphical representation of the repository branches and
commits::

   gitk --all

To see a linear list of commits for this branch::

   git log

You can also look at the `network graph visualizer`_ for your github_
repo.

Backporting
===========

Backporting is the process of copying new feature/fixes committed in
`numpy/main`_ back to stable release branches. To do this you make a branch
off the branch you are backporting to, cherry pick the commits you want from
``numpy/main``, and then submit a pull request for the branch containing the
backport.

1. First, you need to make the branch you will work on. This needs to be
   based on the older version of NumPy (not main)::

    # Make a new branch based on numpy/maintenance/1.8.x,
    # backport-3324 is our new name for the branch.
    git checkout -b backport-3324 upstream/maintenance/1.8.x

2. Now you need to apply the changes from main to this branch using
   `git cherry-pick`_::

    # Update remote
    git fetch upstream
    # Check the commit log for commits to cherry pick
    git log upstream/main
    # This pull request included commits aa7a047 to c098283 (inclusive)
    # so you use the .. syntax (for a range of commits), the ^ makes the
    # range inclusive.
    git cherry-pick aa7a047^..c098283
    ...
    # Fix any conflicts, then if needed:
    git cherry-pick --continue

3. You might run into some conflicts cherry picking here. These are
   resolved the same way as merge/rebase conflicts. Except here you can
   use `git blame`_ to see the difference between main and the
   backported branch to make sure nothing gets screwed up.

4. Push the new branch to your Github repository::

    git push -u origin backport-3324

5. Finally make a pull request using Github. Make sure it is against the
   maintenance branch and not main, Github will usually suggest you
   make the pull request against main.

.. _pushing-to-main:

Pushing changes to the main repo
================================

*Requires commit rights to the main NumPy repo.*

When you have a set of "ready" changes in a feature branch ready for
NumPy's ``main`` or ``maintenance`` branches, you can push
them to ``upstream`` as follows:

1. First, merge or rebase on the target branch.

   a) Only a few, unrelated commits then prefer rebasing::

        git fetch upstream
        git rebase upstream/main

      See :ref:`rebasing-on-main`.

   b) If all of the commits are related, create a merge commit::

        git fetch upstream
        git merge --no-ff upstream/main

2. Check that what you are going to push looks sensible::

        git log -p upstream/main..
        git log --oneline --graph

3. Push to upstream::

        git push upstream my-feature-branch:main

.. note::

    It's usually a good idea to use the ``-n`` flag to ``git push`` to check
    first that you're about to push the changes you want to the place you
    want.


.. include:: gitwash/git_links.inc
===================
Releasing a version
===================

------------------------
How to Prepare a Release
------------------------

.. include:: ../../HOWTO_RELEASE.rst.txt

-----------------------
Step-by-Step Directions
-----------------------

.. include:: ../../RELEASE_WALKTHROUGH.rst.txt

========================
Advanced debugging tools
========================

If you reached here, you want to dive into, or use, more advanced tooling.
This is usually not necessary for first time contributors and most
day-to-day development.
These are used more rarely, for example close to a new NumPy release,
or when a large or particular complex change was made.

Since not all of these tools are used on a regular bases and only available
on some systems, please expect differences, issues, or quirks;
we will be happy to help if you get stuck and appreciate any improvements
or suggestions to these workflows.


Finding C errors with additional tooling
########################################

Most development will not require more than a typical debugging toolchain
as shown in :ref:`Debugging <debugging>`. 
But for example memory leaks can be particularly subtle or difficult to
narrow down.

We do not expect any of these tools to be run by most contributors.
However, you can ensure that we can track down such issues more easily easier:

* Tests should cover all code paths, including error paths.
* Try to write short and simple tests. If you have a very complicated test
  consider creating an additional simpler test as well.
  This can be helpful, because often it is only easy to find which test
  triggers an issue and not which line of the test.
* Never use ``np.empty`` if data is read/used. ``valgrind`` will notice this
  and report an error. When you do not care about values, you can generate
  random values instead.

This will help us catch any oversights before your change is released
and means you do not have to worry about making reference counting errors,
which can be intimidating.


Python debug build for finding memory leaks
===========================================

Debug builds of Python are easily available for example on ``debian`` systems,
and can be used on all platforms.
Running a test or terminal is usually as easy as::

    python3.8d runtests.py
    # or
    python3.8d runtests.py --ipython

and were already mentioned in :ref:`Debugging <debugging>`.

A Python debug build will help:

- Find bugs which may otherwise cause random behaviour.
  One example is when an object is still used after it has been deleted.

- Python debug builds allows to check correct reference counting.
  This works using the additional commands::

    sys.gettotalrefcount()
    sys.getallocatedblocks()


Use together with ``pytest``
----------------------------

Running the test suite only with a debug python build will not find many
errors on its own. An additional advantage of a debug build of Python is that
it allows detecting memory leaks.

A tool to make this easier is `pytest-leaks`_, which can be installed using ``pip``.
Unfortunately, ``pytest`` itself may leak memory, but good results can usually
(currently) be achieved by removing::

    @pytest.fixture(autouse=True)
    def add_np(doctest_namespace):
        doctest_namespace['np'] = numpy

    @pytest.fixture(autouse=True)
    def env_setup(monkeypatch):
        monkeypatch.setenv('PYTHONHASHSEED', '0')

from ``numpy/conftest.py`` (This may change with new ``pytest-leaks`` versions
or ``pytest`` updates).

This allows to run the test suite, or part of it, conveniently::

    python3.8d runtests.py -t numpy/core/tests/test_multiarray.py -- -R2:3 -s

where ``-R2:3`` is the ``pytest-leaks`` command (see its documentation), the
``-s`` causes output to print and may be necessary (in some versions captured
output was detected as a leak).

Note that some tests are known (or even designed) to leak references, we try
to mark them, but expect some false positives.

.. _pytest-leaks: https://github.com/abalkin/pytest-leaks

``valgrind``
============

Valgrind is a powerful tool to find certain memory access problems and should
be run on complicated C code.
Basic use of ``valgrind`` usually requires no more than::

    PYTHONMALLOC=malloc valgrind python runtests.py

where ``PYTHONMALLOC=malloc`` is necessary to avoid false positives from python
itself.
Depending on the system and valgrind version, you may see more false positives.
``valgrind`` supports "suppressions" to ignore some of these, and Python does
have a suppression file (and even a compile time option) which may help if you
find it necessary.

Valgrind helps:

- Find use of uninitialized variables/memory.

- Detect memory access violations (reading or writing outside of allocated
  memory).

- Find *many* memory leaks. Note that for *most* leaks the python
  debug build approach (and ``pytest-leaks``) is much more sensitive.
  The reason is that ``valgrind`` can only detect if memory is definitely
  lost. If::

      dtype = np.dtype(np.int64)
      arr.astype(dtype=dtype)

  Has incorrect reference counting for ``dtype``, this is a bug, but valgrind
  cannot see it because ``np.dtype(np.int64)`` always returns the same object.
  However, not all dtypes are singletons, so this might leak memory for
  different input.
  In rare cases NumPy uses ``malloc`` and not the Python memory allocators
  which are invisible to the Python debug build.
  ``malloc`` should normally be avoided, but there are some exceptions
  (e.g. the ``PyArray_Dims`` structure is public API and cannot use the
  Python allocators.)

Even though using valgrind for memory leak detection is slow and less sensitive
it can be a convenient: you can run most programs with valgrind without
modification.

Things to be aware of:

- Valgrind does not support the numpy ``longdouble``, this means that tests
  will fail or be flagged errors that are completely fine.

- Expect some errors before and after running your NumPy code.

- Caches can mean that errors (specifically memory leaks) may not be detected
  or are only detect at a later, unrelated time.

A big advantage of valgrind is that it has no requirements aside from valgrind
itself (although you probably want to use debug builds for better tracebacks).


Use together with ``pytest``
----------------------------
You can run the test suite with valgrind which may be sufficient
when you are only interested in a few tests::

    PYTHOMMALLOC=malloc valgrind python runtests.py \
     -t numpy/core/tests/test_multiarray.py -- --continue-on-collection-errors

Note the ``--continue-on-collection-errors``, which is currently necessary due to
missing ``longdouble`` support causing failures (this will usually not be
necessary if you do not run the full test suite).

If you wish to detect memory leaks you will also require ``--show-leak-kinds=definite``
and possibly more valgrind options.  Just as for ``pytest-leaks`` certain
tests are known to leak cause errors in valgrind and may or may not be marked
as such.

We have developed `pytest-valgrind`_ which:

- Reports errors for each test individually

- Narrows down memory leaks to individual tests (by default valgrind
  only checks for memory leaks after a program stops, which is very
  cumbersome).

Please refer to its ``README`` for more information (it includes an example
command for NumPy).

.. _pytest-valgrind: https://github.com/seberg/pytest-valgrind

.. currentmodule:: numpy

.. _c-code-explanations:

*************************
NumPy C code explanations
*************************

    Fanaticism consists of redoubling your efforts when you have forgotten
    your aim.
    --- *George Santayana*

    An authority is a person who can tell you more about something than
    you really care to know.
    --- *Unknown*

This page attempts to explain the logic behind some of the new
pieces of code. The purpose behind these explanations is to enable
somebody to be able to understand the ideas behind the implementation
somewhat more easily than just staring at the code. Perhaps in this
way, the algorithms can be improved on, borrowed from, and/or
optimized by more people.


Memory model
============

.. index::
   pair: ndarray; memory model

One fundamental aspect of the :class:`ndarray` is that an array is seen as a
"chunk" of memory starting at some location. The interpretation of
this memory depends on the :term:`stride` information. For each dimension in
an :math:`N`-dimensional array, an integer (:term:`stride`) dictates how many
bytes must be skipped to get to the next element in that dimension.
Unless you have a single-segment array, this :term:`stride` information must
be consulted when traversing through an array. It is not difficult to
write code that accepts strides, you just have to use ``char*``
pointers because strides are in units of bytes. Keep in mind also that
strides do not have to be unit-multiples of the element size. Also,
remember that if the number of dimensions of the array is 0 (sometimes
called a ``rank-0`` array), then the :term:`strides <stride>` and
:term:`dimensions <dimension>` variables are ``NULL``.

Besides the structural information contained in the strides and
dimensions members of the :c:type:`PyArrayObject`, the flags contain
important information about how the data may be accessed. In particular,
the :c:data:`NPY_ARRAY_ALIGNED` flag is set when the memory is on a
suitable boundary according to the datatype array. Even if you have
a :term:`contiguous` chunk of memory, you cannot just assume it is safe to
dereference a datatype-specific pointer to an element. Only if the
:c:data:`NPY_ARRAY_ALIGNED` flag is set, this is a safe operation. On
some platforms it will work but on others, like Solaris, it will cause
a bus error. The :c:data:`NPY_ARRAY_WRITEABLE` should also be ensured
if you plan on writing to the memory area of the array. It is also
possible to obtain a pointer to an unwritable memory area. Sometimes,
writing to the memory area when the :c:data:`NPY_ARRAY_WRITEABLE` flag is not
set will just be rude. Other times it can cause program crashes (*e.g.*
a data-area that is a read-only memory-mapped file).


Data-type encapsulation
=======================

.. seealso:: :ref:`arrays.dtypes`

.. index::
   single: dtype

The :ref:`datatype <arrays.dtypes>` is an important abstraction of the
:class:`ndarray`. Operations
will look to the datatype to provide the key functionality that is
needed to operate on the array. This functionality is provided in the
list of function pointers pointed to by the ``f`` member of the
:c:type:`PyArray_Descr` structure. In this way, the number of datatypes can be
extended simply by providing a :c:type:`PyArray_Descr` structure with suitable
function pointers in the ``f`` member. For built-in types, there are some
optimizations that bypass this mechanism, but the point of the datatype
abstraction is to allow new datatypes to be added.

One of the built-in datatypes, the :class:`void` datatype allows for
arbitrary :term:`structured types <structured data type>` containing 1 or more
fields as elements of the array. A :term:`field` is simply another datatype
object along with an offset into the current structured type. In order to
support arbitrarily nested fields, several recursive implementations of
datatype access are implemented for the void type. A common idiom is to cycle
through the elements of the dictionary and perform a specific operation based on
the datatype object stored at the given offset. These offsets can be
arbitrary numbers. Therefore, the possibility of encountering misaligned
data must be recognized and taken into account if necessary.


N-D Iterators
=============

.. seealso:: :ref:`arrays.nditer`

.. index::
   single: array iterator

A very common operation in much of NumPy code is the need to iterate
over all the elements of a general, strided, N-dimensional array. This
operation of a general-purpose N-dimensional loop is abstracted in the
notion of an iterator object. To write an N-dimensional loop, you only
have to create an iterator object from an ndarray, work with the
:c:member:`dataptr <PyArrayIterObject.dataptr>` member of the iterator object
structure and call the macro :c:func:`PyArray_ITER_NEXT` on the iterator
object to move to the next element. The ``next`` element is always in
C-contiguous order. The macro works by first special-casing the C-contiguous,
1-D, and 2-D cases which work very simply.

For the general case, the iteration works by keeping track of a list
of coordinate counters in the iterator object. At each iteration, the
last coordinate counter is increased (starting from 0). If this
counter is smaller than one less than the size of the array in that
dimension (a pre-computed and stored value), then the counter is
increased and the :c:member:`dataptr <PyArrayIterObject.dataptr>` member is
increased by the strides in that
dimension and the macro ends. If the end of a dimension is reached,
the counter for the last dimension is reset to zero and the
:c:member:`dataptr <PyArrayIterObject.dataptr>` is
moved back to the beginning of that dimension by subtracting the
strides value times one less than the number of elements in that
dimension (this is also pre-computed and stored in the
:c:member:`backstrides <PyArrayIterObject.backstrides>`
member of the iterator object). In this case, the macro does not end,
but a local dimension counter is decremented so that the next-to-last
dimension replaces the role that the last dimension played and the
previously-described tests are executed again on the next-to-last
dimension. In this way, the :c:member:`dataptr <PyArrayIterObject.dataptr>`
is adjusted appropriately for arbitrary striding.

The :c:member:`coordinates <PyArrayIterObject.coordinates>` member of the
:c:type:`PyArrayIterObject` structure maintains
the current N-d counter unless the underlying array is C-contiguous in
which case the coordinate counting is bypassed. The
:c:member:`index <PyArrayIterObject.index>` member of
the :c:type:`PyArrayIterObject` keeps track of the current flat index of the
iterator. It is updated by the :c:func:`PyArray_ITER_NEXT` macro.


Broadcasting
============

.. seealso:: :ref:`basics.broadcasting`

.. index::
   single: broadcasting

In Numeric, the ancestor of NumPy, broadcasting was implemented in several
lines of code buried deep in ``ufuncobject.c``. In NumPy, the notion of
broadcasting has been abstracted so that it can be performed in multiple places.
Broadcasting is handled by the function :c:func:`PyArray_Broadcast`. This
function requires a :c:type:`PyArrayMultiIterObject` (or something that is a
binary equivalent) to be passed in. The :c:type:`PyArrayMultiIterObject` keeps
track of the broadcast number of dimensions and size in each
dimension along with the total size of the broadcast result. It also
keeps track of the number of arrays being broadcast and a pointer to
an iterator for each of the arrays being broadcast.

The :c:func:`PyArray_Broadcast` function takes the iterators that have already
been defined and uses them to determine the broadcast shape in each
dimension (to create the iterators at the same time that broadcasting
occurs then use the :c:func:`PyArray_MultiIterNew` function).
Then, the iterators are
adjusted so that each iterator thinks it is iterating over an array
with the broadcast size. This is done by adjusting the iterators
number of dimensions, and the :term:`shape` in each dimension. This works
because the iterator strides are also adjusted. Broadcasting only
adjusts (or adds) length-1 dimensions. For these dimensions, the
strides variable is simply set to 0 so that the data-pointer for the
iterator over that array doesn't move as the broadcasting operation
operates over the extended dimension.

Broadcasting was always implemented in Numeric using 0-valued strides
for the extended dimensions. It is done in exactly the same way in
NumPy. The big difference is that now the array of strides is kept
track of in a :c:type:`PyArrayIterObject`, the iterators involved in a
broadcast result are kept track of in a :c:type:`PyArrayMultiIterObject`,
and the :c:func:`PyArray_Broadcast` call implements the
:ref:`general-broadcasting-rules`.


Array Scalars
=============

.. seealso:: :ref:`arrays.scalars`

.. index::
   single: array scalars

The array scalars offer a hierarchy of Python types that allow a one-to-one
correspondence between the datatype stored in an array and the
Python-type that is returned when an element is extracted from the
array. An exception to this rule was made with object arrays. Object
arrays are heterogeneous collections of arbitrary Python objects. When
you select an item from an object array, you get back the original
Python object (and not an object array scalar which does exist but is
rarely used for practical purposes).

The array scalars also offer the same methods and attributes as arrays
with the intent that the same code can be used to support arbitrary
dimensions (including 0-dimensions). The array scalars are read-only
(immutable) with the exception of the void scalar which can also be
written to so that structured array field setting works more naturally
(``a[0]['f1'] = value``).


Indexing
========

.. seealso:: :ref:`basics.indexing`, :ref:`arrays.indexing`

.. index::
   single: indexing

All Python indexing operations ``arr[index]`` are organized by first preparing
the index and finding the index type. The supported index types are:

* integer
* :const:`newaxis`
* :term:`python:slice`
* :py:data:`Ellipsis`
* integer arrays/array-likes (advanced)
* boolean (single boolean array); if there is more than one boolean array as
  the index or the shape does not match exactly, the boolean array will be
  converted to an integer array instead.
* 0-d boolean (and also integer); 0-d boolean arrays are a special
  case that has to be handled in the advanced indexing code. They signal
  that a 0-d boolean array had to be interpreted as an integer array.

As well as the scalar array special case signaling that an integer array
was interpreted as an integer index, which is important because an integer
array index forces a copy but is ignored if a scalar is returned (full integer
index). The prepared index is guaranteed to be valid with the exception of
out of bound values and broadcasting errors for advanced indexing. This
includes that an :py:data:`Ellipsis` is added for incomplete indices for
example when a two-dimensional array is indexed with a single integer.

The next step depends on the type of index which was found. If all
dimensions are indexed with an integer a scalar is returned or set. A
single boolean indexing array will call specialized boolean functions.
Indices containing an :py:data:`Ellipsis` or :term:`python:slice` but no
advanced indexing will always create a view into the old array by calculating
the new strides and memory offset.  This view can then either be returned or,
for assignments, filled using ``PyArray_CopyObject``. Note that
``PyArray_CopyObject`` may also be called on temporary arrays in other branches
to support complicated assignments when the array is of object :class:`dtype`.

Advanced indexing
-----------------

By far the most complex case is advanced indexing, which may or may not be
combined with typical view-based indexing. Here integer indices are
interpreted as view-based. Before trying to understand this, you may want
to make yourself familiar with its subtleties. The advanced indexing code
has three different branches and one special case:

* There is one indexing array and it, as well as the assignment array, can
  be iterated trivially. For example, they may be contiguous. Also, the
  indexing array must be of :class:`intp` type and the value array in
  assignments should be of the correct type. This is purely a fast path.
* There are only integer array indices so that no subarray exists.
* View-based and advanced indexing is mixed. In this case, the view-based
  indexing defines a collection of subarrays that are combined by the
  advanced indexing. For example, ``arr[[1, 2, 3], :]`` is created by
  vertically stacking the subarrays ``arr[1, :]``, ``arr[2, :]``, and
  ``arr[3, :]``.
* There is a subarray but it has exactly one element. This case can be handled
  as if there is no subarray but needs some care during setup.

Deciding what case applies, checking broadcasting, and determining the kind
of transposition needed are all done in :c:func:`PyArray_MapIterNew`. After
setting up, there are two cases. If there is no subarray or it only has one
element, no subarray iteration is necessary and an iterator is prepared
which iterates all indexing arrays *as well as* the result or value array.
If there is a subarray, there are three iterators prepared. One for the
indexing arrays, one for the result or value array (minus its subarray),
and one for the subarrays of the original and the result/assignment array.
The first two iterators give (or allow calculation) of the pointers into
the start of the subarray, which then allows restarting the subarray
iteration.

When advanced indices are next to each other transposing may be necessary.
All necessary transposing is handled by :c:func:`PyArray_MapIterSwapAxes` and
has to be handled by the caller unless :c:func:`PyArray_MapIterNew` is asked to
allocate the result.

After preparation, getting and setting are relatively straightforward,
although the different modes of iteration need to be considered. Unless
there is only a single indexing array during item getting, the validity of
the indices is checked beforehand. Otherwise, it is handled in the inner
loop itself for optimization.

.. _ufuncs-internals:

Universal functions
===================

.. seealso:: :ref:`ufuncs`, :ref:`ufuncs-basics`

.. index::
   single: ufunc

Universal functions are callable objects that take :math:`N` inputs
and produce :math:`M` outputs by wrapping basic 1-D loops that work
element-by-element into full easy-to-use functions that seamlessly
implement :ref:`broadcasting <basics.broadcasting>`,
:ref:`type-checking <ufuncs.casting>`,
:ref:`buffered coercion <use-of-internal-buffers>`, and
:ref:`output-argument handling <ufuncs-output-type>`. New universal functions
are normally created in C, although there is a mechanism for creating ufuncs
from Python functions (:func:`frompyfunc`). The user must supply a 1-D loop that
implements the basic function taking the input scalar values and
placing the resulting scalars into the appropriate output slots as
explained in implementation.


Setup
-----

Every :class:`ufunc` calculation involves some overhead related to setting up
the calculation. The practical significance of this overhead is that
even though the actual calculation of the ufunc is very fast, you will
be able to write array and type-specific code that will work faster
for small arrays than the ufunc. In particular, using ufuncs to
perform many calculations on 0-D arrays will be slower than other
Python-based solutions (the silently-imported ``scalarmath`` module exists
precisely to give array scalars the look-and-feel of ufunc based
calculations with significantly reduced overhead).

When a :class:`ufunc` is called, many things must be done. The information
collected from these setup operations is stored in a loop object. This
loop object is a C-structure (that could become a Python object but is
not initialized as such because it is only used internally). This loop
object has the layout needed to be used with :c:func:`PyArray_Broadcast`
so that the broadcasting can be handled in the same way as it is handled in
other sections of code.

The first thing done is to look up in the thread-specific global
dictionary the current values for the buffer-size, the error mask, and
the associated error object. The state of the error mask controls what
happens when an error condition is found. It should be noted that
checking of the hardware error flags is only performed after each 1-D
loop is executed. This means that if the input and output arrays are
contiguous and of the correct type so that a single 1-D loop is
performed, then the flags may not be checked until all elements of the
array have been calculated. Looking up these values in a thread-specific
dictionary takes time which is easily ignored for all but
very small arrays.

After checking, the thread-specific global variables, the inputs are
evaluated to determine how the ufunc should proceed and the input and
output arrays are constructed if necessary. Any inputs which are not
arrays are converted to arrays (using context if necessary). Which of
the inputs are scalars (and therefore converted to 0-D arrays) is
noted.

Next, an appropriate 1-D loop is selected from the 1-D loops available
to the :class:`ufunc` based on the input array types. This 1-D loop is selected
by trying to match the signature of the datatypes of the inputs
against the available signatures. The signatures corresponding to
built-in types are stored in the :attr:`ufunc.types` member of the ufunc
structure. The signatures corresponding to user-defined types are stored in a
linked list of function information with the head element stored as a
``CObject`` in the ``userloops`` dictionary keyed by the datatype number
(the first user-defined type in the argument list is used as the key).
The signatures are searched until a signature is found to which the
input arrays can all be cast safely (ignoring any scalar arguments
which are not allowed to determine the type of the result). The
implication of this search procedure is that "lesser types" should be
placed below "larger types" when the signatures are stored. If no 1-D
loop is found, then an error is reported. Otherwise, the ``argument_list``
is updated with the stored signature --- in case casting is necessary
and to fix the output types assumed by the 1-D loop.

If the ufunc has 2 inputs and 1 output and the second input is an
``Object`` array then a special-case check is performed so that
``NotImplemented`` is returned if the second input is not an ndarray, has
the :obj:`~numpy.class.__array_priority__` attribute, and has an ``__r{op}__``
special method. In this way, Python is signaled to give the other object a
chance to complete the operation instead of using generic object-array
calculations. This allows (for example) sparse matrices to override
the multiplication operator 1-D loop.

For input arrays that are smaller than the specified buffer size,
copies are made of all non-contiguous, misaligned, or out-of-byteorder
arrays to ensure that for small arrays, a single loop is
used. Then, array iterators are created for all the input arrays and
the resulting collection of iterators is broadcast to a single shape.

The output arguments (if any) are then processed and any missing
return arrays are constructed. If any provided output array doesn't
have the correct type (or is misaligned) and is smaller than the
buffer size, then a new output array is constructed with the special
:c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag set. At the end of the function,
:c:func:`PyArray_ResolveWritebackIfCopy` is called so that 
its contents will be copied back into the output array.
Iterators for the output arguments are then processed.

Finally, the decision is made about how to execute the looping
mechanism to ensure that all elements of the input arrays are combined
to produce the output arrays of the correct type. The options for loop
execution are one-loop (for :term`contiguous`, aligned, and correct data
type), strided-loop (for non-contiguous but still aligned and correct
data type), and a buffered loop (for misaligned or incorrect data
type situations). Depending on which execution method is called for,
the loop is then set up and computed.


Function call
-------------

This section describes how the basic universal function computation loop is
set up and executed for each of the three different kinds of execution. If
:c:data:`NPY_ALLOW_THREADS` is defined during compilation, then as long as
no object arrays are involved, the Python Global Interpreter Lock (GIL) is
released prior to calling the loops.  It is re-acquired if necessary to
handle error conditions. The hardware error flags are checked only after
the 1-D loop is completed.


One loop
^^^^^^^^

This is the simplest case of all. The ufunc is executed by calling the
underlying 1-D loop exactly once. This is possible only when we have
aligned data of the correct type (including byteorder) for both input
and output and all arrays have uniform strides (either :term:`contiguous`,
0-D, or 1-D). In this case, the 1-D computational loop is called once
to compute the calculation for the entire array. Note that the
hardware error flags are only checked after the entire calculation is
complete.


Strided loop
^^^^^^^^^^^^

When the input and output arrays are aligned and of the correct type,
but the striding is not uniform (non-contiguous and 2-D or larger),
then a second looping structure is employed for the calculation. This
approach converts all of the iterators for the input and output
arguments to iterate over all but the largest dimension. The inner
loop is then handled by the underlying 1-D computational loop. The
outer loop is a standard iterator loop on the converted iterators. The
hardware error flags are checked after each 1-D loop is completed.


Buffered loop
^^^^^^^^^^^^^

This is the code that handles the situation whenever the input and/or
output arrays are either misaligned or of the wrong datatype
(including being byteswapped) from what the underlying 1-D loop
expects. The arrays are also assumed to be non-contiguous. The code
works very much like the strided-loop except for the inner 1-D loop is
modified so that pre-processing is performed on the inputs and post-processing
is performed on the outputs in ``bufsize`` chunks (where
``bufsize`` is a user-settable parameter). The underlying 1-D
computational loop is called on data that is copied over (if it needs
to be). The setup code and the loop code is considerably more
complicated in this case because it has to handle:

- memory allocation of the temporary buffers

- deciding whether or not to use buffers on the input and output data
  (misaligned and/or wrong datatype)

- copying and possibly casting data for any inputs or outputs for which
  buffers are necessary.

- special-casing ``Object`` arrays so that reference counts are properly
  handled when copies and/or casts are necessary.

- breaking up the inner 1-D loop into ``bufsize`` chunks (with a possible
  remainder).

Again, the hardware error flags are checked at the end of each 1-D
loop.


Final output manipulation
-------------------------

Ufuncs allow other array-like classes to be passed seamlessly through
the interface in that inputs of a particular class will induce the
outputs to be of that same class. The mechanism by which this works is
the following. If any of the inputs are not ndarrays and define the
:obj:`~numpy.class.__array_wrap__` method, then the class with the largest
:obj:`~numpy.class.__array_priority__` attribute determines the type of all the
outputs (with the exception of any output arrays passed in). The
:obj:`~numpy.class.__array_wrap__` method of the input array will be called
with the ndarray being returned from the ufunc as its input. There are two
calling styles of the :obj:`~numpy.class.__array_wrap__` function supported.
The first takes the ndarray as the first argument and a tuple of "context" as
the second argument. The context is (ufunc, arguments, output argument
number). This is the first call tried. If a ``TypeError`` occurs, then the
function is called with just the ndarray as the first argument.


Methods
-------

There are three methods of ufuncs that require calculation similar to
the general-purpose ufuncs. These are :meth:`ufunc.reduce`,
:meth:`ufunc.accumulate`, and :meth:`ufunc.reduceat`. Each of these
methods requires a setup command followed by a
loop. There are four loop styles possible for the methods
corresponding to no-elements, one-element, strided-loop, and buffered-loop.
These are the same basic loop styles as implemented for the
general-purpose function call except for the no-element and one-element
cases which are special-cases occurring when the input array
objects have 0 and 1 elements respectively.


Setup
^^^^^

The setup function for all three methods is ``construct_reduce``.
This function creates a reducing loop object and fills it with the
parameters needed to complete the loop. All of the methods only work
on ufuncs that take 2-inputs and return 1 output. Therefore, the
underlying 1-D loop is selected assuming a signature of ``[otype,
otype, otype]`` where ``otype`` is the requested reduction
datatype. The buffer size and error handling are then retrieved from
(per-thread) global storage. For small arrays that are misaligned or
have incorrect datatype, a copy is made so that the un-buffered
section of code is used. Then, the looping strategy is selected. If
there is 1 element or 0 elements in the array, then a simple looping
method is selected. If the array is not misaligned and has the
correct datatype, then strided looping is selected. Otherwise,
buffered looping must be performed. Looping parameters are then
established, and the return array is constructed.  The output array is
of a different :term:`shape` depending on whether the method is
:meth:`reduce <ufunc.reduce>`, :meth:`accumulate <ufunc.accumulate>`, or
:meth:`reduceat <ufunc.reduceat>`. If an output array is already provided, then
its shape is checked. If the output array is not C-contiguous,
aligned, and of the correct data type, then a temporary copy is made
with the :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag set. In this way, the methods
will be able to work with a well-behaved output array but the result will be
copied back into the true output array when
:c:func:`PyArray_ResolveWritebackIfCopy` is called at function completion.
Finally, iterators are set up to loop over the correct :term:`axis`
(depending on the value of axis provided to the method) and the setup
routine returns to the actual computation routine.


:meth:`Reduce <ufunc.reduce>` 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. index::
   triple: ufunc; methods; reduce

All of the ufunc methods use the same underlying 1-D computational
loops with input and output arguments adjusted so that the appropriate
reduction takes place. For example, the key to the functioning of
:meth:`reduce <ufunc.reduce>` is that the 1-D loop is called with the output
and the second input pointing to the same position in memory and both having
a step-size of 0. The first input is pointing to the input array with a
step-size given by the appropriate stride for the selected axis. In this
way, the operation performed is

.. math::
   :nowrap:

   \begin{align*}
   o & = & i[0] \\
   o & = & i[k]\textrm{<op>}o\quad k=1\ldots N
   \end{align*}

where :math:`N+1` is the number of elements in the input, :math:`i`,
:math:`o` is the output, and :math:`i[k]` is the
:math:`k^{\textrm{th}}` element of :math:`i` along the selected axis.
This basic operation is repeated for arrays with greater than 1
dimension so that the reduction takes place for every 1-D sub-array
along the selected axis. An iterator with the selected dimension
removed handles this looping.

For buffered loops, care must be taken to copy and cast data before
the loop function is called because the underlying loop expects
aligned data of the correct datatype (including byteorder). The
buffered loop must handle this copying and casting prior to calling
the loop function on chunks no greater than the user-specified
``bufsize``.


:meth:`Accumulate <ufunc.accumulate>`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. index::
   triple: ufunc; methods; accumulate

The :meth:`accumulate <ufunc.accumulate>` method is very similar to
the :meth:`reduce <ufunc.reduce>` method in that
the output and the second input both point to the output. The
difference is that the second input points to memory one stride behind
the current output pointer. Thus, the operation performed is

.. math::
   :nowrap:

   \begin{align*}
   o[0] & = & i[0] \\
   o[k] & = & i[k]\textrm{<op>}o[k-1]\quad k=1\ldots N.
   \end{align*}

The output has the same shape as the input and each 1-D loop operates
over :math:`N` elements when the shape in the selected axis is :math:`N+1`.
Again, buffered loops take care to copy and cast the data before
calling the underlying 1-D computational loop.


:meth:`Reduceat <ufunc.reduceat>`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. index::
   triple: ufunc; methods; reduceat
   single: ufunc

The :meth:`reduceat <ufunc.reduceat>` function is a generalization of both the
:meth:`reduce <ufunc.reduce>` and :meth:`accumulate <ufunc.accumulate>`
functions. It implements a :meth:`reduce <ufunc.reduce>` over ranges of
the input array specified by indices. The extra indices argument is checked to
be sure that every input is not too large for the input array along
the selected dimension before the loop calculations take place. The
loop implementation is handled using code that is very similar to the
:meth:`reduce <ufunc.reduce>` code repeated as many times as there are elements
in the indices input. In particular: the first input pointer passed to the
underlying 1-D computational loop points to the input array at the
correct location indicated by the index array. In addition, the output
pointer and the second input pointer passed to the underlying 1-D loop
point to the same position in memory. The size of the 1-D
computational loop is fixed to be the difference between the current
index and the next index (when the current index is the last index,
then the next index is assumed to be the length of the array along the
selected dimension). In this way, the 1-D loop will implement a
:meth:`reduce <ufunc.reduce>` over the specified indices.

Misaligned or a loop datatype that does not match the input and/or
output datatype is handled using buffered code wherein data is
copied to a temporary buffer and cast to the correct datatype if
necessary prior to calling the underlying 1-D function. The temporary
buffers are created in (element) sizes no bigger than the user
settable buffer-size value. Thus, the loop must be flexible enough to
call the underlying 1-D computational loop enough times to complete
the total calculation in chunks no bigger than the buffer-size.
.. _development-gitpod:


Using Gitpod for NumPy development
=======================================================

This section of the documentation will guide you through:

*  using GitPod for your NumPy development environment
*  creating a personal fork of the NumPy repository on GitHub
*  a quick tour of Gitpod and VSCode
*  working on the NumPy documentation in Gitpod

Gitpod
-------

`Gitpod`_  is an open-source platform for automated and ready-to-code 
development environments. It enables developers to describe their dev 
environment as code and start instant and fresh development environments for 
each new task directly from your browser. This reduces the need to install local 
development environments and deal with incompatible dependencies.

Gitpod GitHub integration
--------------------------

To be able to use Gitpod, you will need to have the Gitpod app installed on your 
GitHub account, so if
you do not have an account yet, you will need to create one first.

Head over to the `Gitpod`_ website and click on the **Continue with GitHub** 
button. You will be redirected to the GitHub authentication page.
You will then be asked to install the `Gitpod GitHub app <https://github.com/marketplace/gitpod-io>`_.

Make sure to select **All repositories** access option to avoid issues with 
permissions later on. Click on the green **Install** button

.. image:: ./gitpod-imgs/installing-gitpod-io.png
   :alt: Gitpod repository access and installation screenshot

This will install the necessary hooks for the integration.

Forking the NumPy repository
-----------------------------

The best way to work on NumPy as a contributor is by making a fork of the 
repository first.

#. Browse to the `NumPy repository on GitHub`_ and `create your own fork`_.
#. Browse to your fork. Your fork will have a URL like 
   https://github.com/melissawm/NumPy, except with your GitHub username in place of ``melissawm``.

Starting Gitpod
----------------
Once you have authenticated to Gitpod through GitHub, you can install the 
`Gitpod browser extension <https://www.gitpod.io/docs/browser-extension>`_  
which will add a **Gitpod** button next to the **Code** button in the 
repository:

.. image:: ./gitpod-imgs/NumPy-github.png
   :alt: NumPy repository with Gitpod button screenshot

#. If you install the extension - you can click the **Gitpod** button to start 
   a new workspace.

#. Alternatively, if you do not want to install the browser extension, you can 
   visit https://gitpod.io/#https://github.com/USERNAME/NumPy replacing 
   ``USERNAME`` with your GitHub username.

#. In both cases, this will open a new tab on your web browser and start 
   building your development environment. Please note this can take a few 
   minutes.

#. Once the build is complete, you will be directed to your workspace, 
   including the VSCode editor and all the dependencies you need to work on 
   NumPy. The first time you start your workspace, you will notice that there 
   might be some actions running. This will ensure that you have a development 
   version of NumPy installed and that the docs are being pre-built for you.

#. When your workspace is ready, you can :ref:`test the build<testing-builds>` by 
   entering::

      $ python runtests.py -v

``runtests.py`` is another script in the NumPy root directory. It runs a suite 
of tests that make sure NumPy is working as it should, and ``-v`` activates the 
``--verbose`` option to show all the test output.

Quick workspace tour
---------------------
Gitpod uses VSCode as the editor. If you have not used this editor before, you 
can check the Getting started `VSCode docs`_ to familiarize yourself with it.

Your workspace will look similar to the image below:

.. image:: ./gitpod-imgs/gitpod-workspace.png
   :alt: Gitpod workspace screenshot

.. note::  By default, VSCode initializes with a light theme. You can change to 
   a dark theme by with the keyboard shortcut :kbd:`Cmd-K Cmd-T` in Mac or 
   :kbd:`Ctrl-K Ctrl-T` in Linux and Windows.

We have marked some important sections in the editor:

#. Your current Python interpreter - by default, this is ``numpy-dev`` and 
   should be displayed in the status bar and on your terminal. You do not need 
   to activate the conda environment as this will always be activated for you.
#. Your current branch is always displayed in the status bar. You can also use 
   this button to change or create branches.
#. GitHub Pull Requests extension - you can use this to work with Pull Requests 
   from your workspace.
#. Marketplace extensions - we have added some essential extensions to the NumPy 
   Gitpod. Still, you can also install other extensions or syntax highlighting 
   themes for your user, and these will be preserved for you.
#. Your workspace directory - by default, it is ``/workspace/numpy``. **Do not 
   change this** as this is the only directory preserved in Gitpod.

We have also pre-installed a few tools and VSCode extensions to help with the 
development experience:

*  `GitHub CLI <https://cli.github.com/>`_
*  `VSCode rst extension <https://marketplace.visualstudio.com/items?itemName=lextudio.restructuredtext>`_
*  `VSCode Live server extension <https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer>`_
*  `VSCode Gitlens extension <https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens>`_
*  `VSCode autodocstrings extension <https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring>`_
*  `VSCode Git Graph extension <https://marketplace.visualstudio.com/items?itemName=mhutchie.git-graph>`_

Development workflow with Gitpod
---------------------------------
The  :ref:`development-workflow` section of this documentation contains 
information regarding the NumPy development workflow. Make sure to check this 
before working on your contributions.

When using Gitpod, git is pre configured for you:

#. You do not need to configure your git username, and email as this should be 
   done for you as you authenticated through GitHub. You can check the git 
   configuration with the command ``git config --list`` in your terminal.
#. As you started your workspace from your own NumPy fork, you will by default 
   have both ``upstream`` and ``origin`` added as remotes. You can verify this by 
   typing ``git remote`` on your terminal or by clicking on the **branch name** 
   on the status bar (see image below).

   .. image:: ./gitpod-imgs/NumPy-gitpod-branches.png
      :alt: Gitpod workspace branches plugin screenshot

Rendering the NumPy documentation
----------------------------------
You can find the detailed documentation on how rendering the documentation with 
Sphinx works in the :ref:`howto-build-docs` section.

The documentation is pre-built during your workspace initialization. So once 
this task is completed, you have two main options to render the documentation 
in Gitpod.

Option 1: Using Liveserve
***************************

#. View the documentation in ``NumPy/doc/build/html``. You can start with 
   ``index.html`` and browse, or you can jump straight to the file you're 
   interested in.
#. To see the rendered version of a page, you can right-click on the ``.html`` 
   file and click on **Open with Live Serve**. Alternatively, you can open the 
   file in the editor and click on the **Go live** button on the status bar.

    .. image:: ./gitpod-imgs/vscode-statusbar.png
        :alt: Gitpod workspace VSCode start live serve screenshot

#. A simple browser will open to the right-hand side of the editor. We recommend 
   closing it and click on the **Open in browser** button in the pop-up.
#. To stop the server click on the **Port: 5500** button on the status bar.

Option 2: Using the rst extension
***********************************

A quick and easy way to see live changes in a ``.rst`` file as you work on it 
uses the rst extension with docutils.

.. note:: This will generate a simple live preview of the document without the 
    ``html`` theme, and some backlinks might not be added correctly. But it is an 
    easy and lightweight way to get instant feedback on your work.

#. Open any of the source documentation files located in ``doc/source`` in the 
   editor.
#. Open VSCode Command Palette with :kbd:`Cmd-Shift-P` in Mac or 
   :kbd:`Ctrl-Shift-P` in Linux and Windows. Start typing "restructured" 
   and choose either "Open preview" or "Open preview to the Side".

    .. image:: ./gitpod-imgs/vscode-rst.png
        :alt: Gitpod workspace VSCode open rst screenshot

#. As you work on the document, you will see a live rendering of it on the editor.

    .. image:: ./gitpod-imgs/rst-rendering.png
        :alt: Gitpod workspace VSCode rst rendering screenshot

If you want to see the final output with the ``html`` theme you will need to 
rebuild the docs with ``make html`` and use Live Serve as described in option 1.

FAQ's and troubleshooting
-------------------------

How long is my Gitpod workspace kept for?
*****************************************

Your stopped workspace will be kept for 14 days and deleted afterwards if you do 
not use them.

Can I come back to a previous workspace?
*****************************************

Yes, let's say you stepped away for a while and you want to carry on working on 
your NumPy contributions. You need to visit https://gitpod.io/workspaces and 
click on the workspace you want to spin up again. All your changes will be there 
as you last left them.

Can I install additional VSCode extensions?
*******************************************

Absolutely! Any extensions you installed will be installed in your own workspace 
and preserved.

I registered on Gitpod but I still cannot see a ``Gitpod`` button in my repositories.
*************************************************************************************

Head to https://gitpod.io/integrations and make sure you are logged in. 
Hover over GitHub and click on the three buttons that appear on the right. 
Click on edit permissions and make sure you have ``user:email``, 
``read:user``, and ``public_repo`` checked. Click on **Update Permissions** 
and confirm the changes in the GitHub application page.

.. image:: ./gitpod-imgs/gitpod-edit-permissions-gh.png
   :alt: Gitpod integrations - edit GH permissions screenshot

How long does my workspace stay active if I'm not using it?
***********************************************************

If you keep your workspace open in a browser tab but don't interact with it, 
it will shut down after 30 minutes. If you close the browser tab, it will 
shut down after 3 minutes.

My terminal is blank - there is no cursor and it's completely unresponsive
**************************************************************************

Unfortunately this is a known-issue on Gitpod's side. You can sort this 
issue in two ways:

#. Create a new Gitpod workspace altogether.
#. Head to your `Gitpod dashboard <https://gitpod.io/workspaces>`_ and locate 
   the running workspace. Hover on it and click on the **three dots menu** 
   and then click on **Stop**. When the workspace is completely stopped you 
   can click on its name to restart it again.   

.. image:: ./gitpod-imgs/gitpod-dashboard-stop.png
   :alt: Gitpod dashboard and workspace menu screenshot

I authenticated through GitHub but I still cannot commit to the repository through Gitpod. 
******************************************************************************************

Head to https://gitpod.io/integrations and make sure you are logged in. 
Hover over GitHub and click on the three buttons that appear on the right. 
Click on edit permissions and make sure you have ``public_repo`` checked.
Click on **Update Permissions** and confirm the changes in the 
GitHub application page.

.. image:: ./gitpod-imgs/gitpod-edit-permissions-repo.png
   :alt: Gitpod integrations - edit GH repository permissions screenshot

.. _Gitpod: https://www.gitpod.io/
.. _NumPy repository on GitHub: https://github.com/NumPy/NumPy
.. _create your own fork: https://help.github.com/en/articles/fork-a-repo
.. _VSCode docs: https://code.visualstudio.com/docs/getstarted/tips-and-tricks
.. _howto-build-docs:

=========================================
Building the NumPy API and reference docs
=========================================

If you only want to get the documentation, note that pre-built
versions can be found at

    https://numpy.org/doc/

in several different formats.

Development environments
------------------------

Before proceeding further it should be noted that the documentation is built with the ``make`` tool,
which is not natively available on Windows. MacOS or Linux users can jump
to :ref:`how-todoc.prerequisites`. It is recommended for Windows users to set up their development
environment on :ref:`Gitpod <development-gitpod>` or `Windows Subsystem
for Linux (WSL) <https://docs.microsoft.com/en-us/windows/wsl/install-win10>`_. WSL is a good option
for a persistent local set-up.

Gitpod
^^^^^^
Gitpod is an open-source platform that automatically creates the correct development environment right
in your browser, reducing the need to install local development environments and deal with
incompatible dependencies.

If you have good internet connectivity and want a temporary set-up,
it is often faster to build with Gitpod. Here are the in-depth instructions for
:ref:`building NumPy with Gitpod <development-gitpod>`.


.. _how-todoc.prerequisites:

Prerequisites
-------------

Building the NumPy documentation and API reference requires the following:

NumPy
^^^^^

Since large parts of the main documentation are obtained from NumPy via
``import numpy`` and examining the docstrings, you will need to first
:ref:`build <building-from-source>` and install it so that the correct version is imported.
NumPy has to be re-built and re-installed every time you fetch the latest version of the
repository, before generating the documentation. This ensures that the NumPy version and
the git repository version are in sync.

Note that you can e.g. install NumPy to a temporary location and set
the PYTHONPATH environment variable appropriately.
Alternatively, if using Python virtual environments (via e.g. ``conda``,
``virtualenv`` or the ``venv`` module), installing NumPy into a
new virtual environment is recommended.

Dependencies
^^^^^^^^^^^^

All of the necessary dependencies for building the NumPy docs except for
Doxygen_ can be installed with::

    pip install -r doc_requirements.txt

We currently use Sphinx_ along with Doxygen_ for generating the API and
reference documentation for NumPy. In addition, building the documentation
requires the Sphinx extension `plot_directive`, which is shipped with
:doc:`Matplotlib <matplotlib:index>`. We also use numpydoc_ to render docstrings in
the generated API documentation. :doc:`SciPy <scipy:index>`
is installed since some parts of the documentation require SciPy functions.

For installing Doxygen_, please check the official
`download <https://www.doxygen.nl/download.html#srcbin>`_ and
`installation <https://www.doxygen.nl/manual/install.html>`_ pages, or if you
are using Linux then you can install it through your distribution package manager.

.. note::

   Try to install a newer version of Doxygen_ > 1.8.10 otherwise you may get some
   warnings during the build.

Submodules
^^^^^^^^^^

If you obtained NumPy via git, also get the git submodules that contain
additional parts required for building the documentation::

    git submodule update --init

.. _Sphinx: http://www.sphinx-doc.org/
.. _numpydoc: https://numpydoc.readthedocs.io/en/latest/index.html
.. _Doxygen: https://www.doxygen.nl/index.html

Instructions
------------

Now you are ready to generate the docs, so write::

    cd doc
    make html

If all goes well, this will generate a
``build/html`` subdirectory in the ``/doc`` directory, containing the built documentation. If
you get a message about ``installed numpy != current repo git version``, you must
either override the check by setting ``GITVER`` or re-install NumPy.

If you have built NumPy into a virtual environment and get an error
that says ``numpy not found, cannot build documentation without...``,
you need to override the makefile ``PYTHON`` variable at the command
line, so instead of writing ``make  html`` write::

    make PYTHON=python html

To build the PDF documentation, do instead::

   make latex
   make -C build/latex all-pdf

You will need to have LaTeX_ installed for this, inclusive of support for
Greek letters.  For example, on Ubuntu xenial ``texlive-lang-greek`` and
``cm-super`` are needed.  Also, ``latexmk`` is needed on non-Windows systems.

Instead of the above, you can also do::

   make dist

which will rebuild NumPy, install it to a temporary location, and
build the documentation in all formats. This will most likely again
only work on Unix platforms.

The documentation for NumPy distributed at https://numpy.org/doc in html and
pdf format is also built with ``make dist``.  See `HOWTO RELEASE`_ for details
on how to update https://numpy.org/doc.

.. _LaTeX: https://www.latex-project.org/
.. _HOWTO RELEASE: https://github.com/numpy/numpy/blob/main/doc/HOWTO_RELEASE.rst.txt
.. _underthehood:

===========================================
Under-the-hood Documentation for developers
===========================================

These documents are intended as a low-level look into NumPy; focused
towards developers.

.. toctree::
   :maxdepth: 1

   internals
   internals.code-explanations
   alignment
.. currentmodule:: numpy

.. _alignment:

****************
Memory Alignment
****************

NumPy alignment goals
=====================

There are three use-cases related to memory alignment in NumPy (as of 1.14):

 1. Creating :term:`structured datatypes <structured data type>` with
    :term:`fields <field>` aligned like in a C-struct.
 2. Speeding up copy operations by using :class:`uint` assignment in instead of
    ``memcpy``.
 3. Guaranteeing safe aligned access for ufuncs/setitem/casting code.

NumPy uses two different forms of alignment to achieve these goals:
"True alignment" and "Uint alignment".

"True" alignment refers to the architecture-dependent alignment of an
equivalent C-type in C. For example, in x64 systems :attr:`float64` is
equivalent to ``double`` in C. On most systems, this has either an alignment of
4 or 8 bytes (and this can be controlled in GCC by the option
``malign-double``).  A variable is aligned in memory if its memory offset is a
multiple of its alignment. On some systems (eg. sparc) memory alignment is
required; on others, it gives a speedup.

"Uint" alignment depends on the size of a datatype. It is defined to be the
"True alignment" of the uint used by NumPy's copy-code to copy the datatype, or
undefined/unaligned if there is no equivalent uint. Currently, NumPy uses
``uint8``, ``uint16``, ``uint32``, ``uint64``, and ``uint64`` to copy data of
size 1, 2, 4, 8, 16 bytes respectively, and all other sized datatypes cannot
be uint-aligned.

For example, on a (typical Linux x64 GCC) system, the NumPy :attr:`complex64`
datatype is implemented as ``struct { float real, imag; }``. This has "true"
alignment of 4 and "uint" alignment of 8 (equal to the true alignment of
``uint64``).

Some cases where uint and true alignment are different (default GCC Linux):
   ======   =========   ========    ========
   arch     type        true-aln    uint-aln
   ======   =========   ========    ========
   x86_64   complex64          4           8
   x86_64   float128          16           8
   x86      float96            4          \-
   ======   =========   ========    ========


Variables in NumPy which control and describe alignment
=======================================================

There are 4 relevant uses of the word ``align`` used in NumPy:

 * The :attr:`dtype.alignment` attribute (``descr->alignment`` in C). This is
   meant to reflect the "true alignment" of the type. It has arch-dependent
   default values for all datatypes, except for the structured types created
   with ``align=True`` as described below.
 * The ``ALIGNED`` flag of an ndarray, computed in ``IsAligned`` and checked
   by :c:func:`PyArray_ISALIGNED`. This is computed from
   :attr:`dtype.alignment`.
   It is set to ``True`` if every item in the array is at a memory location
   consistent with :attr:`dtype.alignment`, which is the case if the
   ``data ptr`` and all strides of the array are multiples of that alignment.
 * The ``align`` keyword of the dtype constructor, which only affects
   :ref:`structured_arrays`. If the structure's field offsets are not manually
   provided, NumPy determines offsets automatically. In that case,
   ``align=True`` pads the structure so that each field is "true" aligned in
   memory and sets :attr:`dtype.alignment` to be the largest of the field
   "true" alignments. This is like what C-structs usually do. Otherwise if
   offsets or itemsize were manually provided ``align=True`` simply checks that
   all the fields are "true" aligned and that the total itemsize is a multiple
   of the largest field alignment. In either case :attr:`dtype.isalignedstruct`
   is also set to True.
 * ``IsUintAligned`` is used to determine if an ndarray is "uint aligned" in
   an analogous way to how ``IsAligned`` checks for true alignment.

Consequences of alignment
=========================

Here is how the variables above are used:

 1. Creating aligned structs: To know how to offset a field when
    ``align=True``, NumPy looks up ``field.dtype.alignment``. This includes
    fields that are nested structured arrays.
 2. Ufuncs: If the ``ALIGNED`` flag of an array is False, ufuncs will
    buffer/cast the array before evaluation. This is needed since ufunc inner
    loops access raw elements directly, which might fail on some archs if the
    elements are not true-aligned.
 3. Getitem/setitem/copyswap function: Similar to ufuncs, these functions
    generally have two code paths. If ``ALIGNED`` is False they will
    use a code path that buffers the arguments so they are true-aligned.
 4. Strided copy code: Here, "uint alignment" is used instead.  If the itemsize
    of an array is equal to 1, 2, 4, 8 or 16 bytes and the array is uint
    aligned then instead NumPy will do ``*(uintN*)dst) = *(uintN*)src)`` for
    appropriate N. Otherwise, NumPy copies by doing ``memcpy(dst, src, N)``.
 5. Nditer code: Since this often calls the strided copy code, it must
    check for "uint alignment".
 6. Cast code: This checks for "true" alignment, as it does
    ``*dst = CASTFUNC(*src)`` if aligned. Otherwise, it does
    ``memmove(srcval, src); dstval = CASTFUNC(srcval); memmove(dst, dstval)``
    where dstval/srcval are aligned.

Note that the strided-copy and strided-cast code are deeply intertwined and so
any arrays being processed by them must be both uint and true aligned, even
though the copy-code only needs uint alignment and the cast code only true
alignment.  If there is ever a big rewrite of this code it would be good to
allow them to use different alignments.


.. _reviewer-guidelines:

===================
Reviewer Guidelines
===================

Reviewing open pull requests (PRs) helps move the project forward. We encourage
people outside the project to get involved as well; it's a great way to get
familiar with the codebase.

Who can be a reviewer?
======================

Reviews can come from outside the NumPy team -- we welcome contributions from
domain experts (for instance, `linalg` or `fft`) or maintainers of other
projects. You do not need to be a NumPy maintainer (a NumPy team member with
permission to merge a PR) to review.

If we do not know you yet, consider introducing yourself in `the mailing list or
Slack <https://numpy.org/community/>`_ before you start reviewing pull requests.

Communication Guidelines
========================

- Every PR, good or bad, is an act of generosity. Opening with a positive
  comment will help the author feel rewarded, and your subsequent remarks may be
  heard more clearly. You may feel good also.
- Begin if possible with the large issues, so the author knows they've been
  understood. Resist the temptation to immediately go line by line, or to open
  with small pervasive issues.
- You are the face of the project, and NumPy some time ago decided `the kind of
  project it will be <https://numpy.org/code-of-conduct/>`_: open, empathetic,
  welcoming, friendly and patient. Be `kind
  <https://youtu.be/tzFWz5fiVKU?t=49m30s>`_ to contributors.
- Do not let perfect be the enemy of the good, particularly for documentation.
  If you find yourself making many small suggestions, or being too nitpicky on
  style or grammar, consider merging the current PR when all important concerns
  are addressed. Then, either push a commit directly (if you are a maintainer)
  or open a follow-up PR yourself.
- If you need help writing replies in reviews, check out some
  :ref:`standard replies for reviewing<saved-replies>`.

Reviewer Checklist
==================

- Is the intended behavior clear under all conditions? Some things to watch:
   - What happens with unexpected inputs like empty arrays or nan/inf values?
   - Are axis or shape arguments tested to be `int` or `tuples`?
   - Are unusual `dtypes` tested if a function supports those?
- Should variable names be improved for clarity or consistency?
- Should comments be added, or rather removed as unhelpful or extraneous?
- Does the documentation follow the :ref:`NumPy guidelines<howto-document>`? Are
  the docstrings properly formatted?
- Does the code follow NumPy's :ref:`Stylistic Guidelines<stylistic-guidelines>`?
- If you are a maintainer, and it is not obvious from the PR description, add a
  short explanation of what a branch did to the merge message and, if closing an
  issue, also add "Closes gh-123" where 123 is the issue number.
- For code changes, at least one maintainer (i.e. someone with commit rights)
  should review and approve a pull request. If you are the first to review a
  PR and approve of the changes use the GitHub `approve review
  <https://help.github.com/articles/reviewing-changes-in-pull-requests/>`_ tool
  to mark it as such. If a PR is straightforward, for example it's a clearly
  correct bug fix, it can be merged straight away. If it's more complex or
  changes public API, please leave it open for at least a couple of days so
  other maintainers get a chance to review.
- If you are a subsequent reviewer on an already approved PR, please use the
  same review method as for a new PR (focus on the larger issues, resist the
  temptation to add only a few nitpicks).  If you have commit rights and think
  no more review is needed, merge the PR.

For maintainers
---------------
  
- Make sure all automated CI tests pass before merging a PR, and that the
  :ref:`documentation builds <building-docs>` without any errors.
- In case of merge conflicts, ask the PR submitter to :ref:`rebase on main
  <rebasing-on-main>`.
- For PRs that add new features or are in some way complex, wait at least a day
  or two before merging it. That way, others get a chance to comment before the
  code goes in. Consider adding it to the release notes.
- When merging contributions, a committer is responsible for ensuring that those
  meet the requirements outlined in the :ref:`Development process guidelines
  <guidelines>` for NumPy. Also, check that new features and backwards
  compatibility breaks were discussed on the `numpy-discussion mailing list
  <https://mail.python.org/mailman/listinfo/numpy-discussion>`_.
- Squashing commits or cleaning up commit messages of a PR that you consider too
  messy is OK. Remember to retain the original author's name when doing this.
  Make sure commit messages follow the :ref:`rules for NumPy
  <writing-the-commit-message>`.
- When you want to reject a PR: if it's very obvious, you can just close it and
  explain why. If it's not, then it's a good idea to first explain why you
  think the PR is not suitable for inclusion in NumPy and then let a second
  committer comment or close.

GitHub Workflow
---------------

When reviewing pull requests, please use workflow tracking features on GitHub as
appropriate:

- After you have finished reviewing, if you want to ask for the submitter to
  make changes, change your review status to "Changes requested." This can be
  done on GitHub, PR page, Files changed tab, Review changes (button on the top
  right).
- If you're happy about the current status, mark the pull request as Approved
  (same way as Changes requested). Alternatively (for maintainers): merge
  the pull request, if you think it is ready to be merged.

It may be helpful to have a copy of the pull request code checked out on your
own machine so that you can play with it locally. You can use the `GitHub CLI
<https://docs.github.com/en/github/getting-started-with-github/github-cli>`_ to
do this by clicking the ``Open with`` button in the upper right-hand corner of
the PR page. 

Assuming you have your :ref:`development environment<development-environment>`
set up, you can now build the code and test it.

.. _saved-replies:

Standard replies for reviewing
==============================

It may be helpful to store some of these in GitHub's `saved
replies <https://github.com/settings/replies/>`_ for reviewing:

**Usage question**
    .. code-block:: md

        You are asking a usage question. The issue tracker is for bugs and new features.
        I'm going to close this issue, feel free to ask for help via our [help channels](https://numpy.org/gethelp/).

**You’re welcome to update the docs**
    .. code-block:: md

        Please feel free to offer a pull request updating the documentation if you feel it could be improved.

**Self-contained example for bug**
    .. code-block:: md

        Please provide a [self-contained example code](https://stackoverflow.com/help/mcve), including imports and data (if possible), so that other contributors can just run it and reproduce your issue.
        Ideally your example code should be minimal.

**Software versions**
    .. code-block:: md

        To help diagnose your issue, please paste the output of:
        ```
        python -c 'import numpy; print(numpy.version.version)'
        ```
        Thanks.

**Code blocks**
    .. code-block:: md

        Readability can be greatly improved if you [format](https://help.github.com/articles/creating-and-highlighting-code-blocks/) your code snippets and complete error messages appropriately.
        You can edit your issue descriptions and comments at any time to improve readability.
        This helps maintainers a lot. Thanks!

**Linking to code**
    .. code-block:: md

        For clarity's sake, you can link to code like [this](https://help.github.com/articles/creating-a-permanent-link-to-a-code-snippet/).

**Better description and title**
    .. code-block:: md

        Please make the title of the PR more descriptive.
        The title will become the commit message when this is merged.
        You should state what issue (or PR) it fixes/resolves in the description using the syntax described [here](https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword).

**Regression test needed**
    .. code-block:: md

        Please add a [non-regression test](https://en.wikipedia.org/wiki/Non-regression_testing) that would fail at main but pass in this PR.

**Don’t change unrelated**
    .. code-block:: md

        Please do not change unrelated lines. It makes your contribution harder to review and may introduce merge conflicts to other pull requests.

.. include:: gitwash/git_links.inc
.. currentmodule:: numpy

.. _numpy-internals:

*************************************
Internal organization of NumPy arrays
*************************************

It helps to understand a bit about how NumPy arrays are handled under the covers
to help understand NumPy better. This section will not go into great detail.
Those wishing to understand the full details are requested to refer to Travis
Oliphant's book `Guide to NumPy <http://web.mit.edu/dvp/Public/numpybook.pdf>`_.

NumPy arrays consist of two major components: the raw array data (from now on,
referred to as the data buffer), and the information about the raw array data.
The data buffer is typically what people think of as arrays in C or Fortran,
a :term:`contiguous` (and fixed) block of memory containing fixed-sized data
items. NumPy also contains a significant set of data that describes how to
interpret the data in the data buffer. This extra information contains (among
other things):

 1) The basic data element's size in bytes.
 2) The start of the data within the data buffer (an offset relative to the
    beginning of the data buffer).
 3) The number of :term:`dimensions <dimension>` and the size of each dimension.
 4) The separation between elements for each dimension (the :term:`stride`).
    This does not have to be a multiple of the element size.
 5) The byte order of the data (which may not be the native byte order).
 6) Whether the buffer is read-only.
 7) Information (via the :class:`dtype` object) about the interpretation of the
    basic data element. The basic data element may be as simple as an int or a
    float, or it may be a compound object (e.g.,
    :term:`struct-like <structured data type>`), a fixed character field,
    or Python object pointers.
 8) Whether the array is to be interpreted as :term:`C-order <C order>`
    or :term:`Fortran-order <Fortran order>`.

This arrangement allows for the very flexible use of arrays. One thing that it
allows is simple changes to the metadata to change the interpretation of the
array buffer. Changing the byteorder of the array is a simple change involving
no rearrangement of the data. The :term:`shape` of the array can be changed very
easily without changing anything in the data buffer or any data copying at all.

Among other things that are made possible is one can create a new array metadata
object that uses the same data buffer
to create a new :term:`view` of that data buffer that has a different
interpretation of the buffer (e.g., different shape, offset, byte order,
strides, etc) but shares the same data bytes. Many operations in NumPy do just
this such as :term:`slicing <python:slice>`. Other operations, such as
transpose, don't move data elements around in the array, but rather change the
information about the shape and strides so that the indexing of the array
changes, but the data in the doesn't move.

Typically these new versions of the array metadata but the same data buffer are
new views into the data buffer. There is a different :class:`ndarray` object,
but it uses the same data buffer. This is why it is necessary to force copies
through the use of the :func:`copy` method if one really wants to make a new
and independent copy of the data buffer.

New views into arrays mean the object reference counts for the data buffer
increase. Simply doing away with the original array object will not remove the
data buffer if other views of it still exist.

Multidimensional array indexing order issues
============================================

.. seealso:: :ref:`basics.indexing`

What is the right way to index
multi-dimensional arrays? Before you jump to conclusions about the one and
true way to index multi-dimensional arrays, it pays to understand why this is
a confusing issue. This section will try to explain in detail how NumPy
indexing works and why we adopt the convention we do for images, and when it
may be appropriate to adopt other conventions.

The first thing to understand is
that there are two conflicting conventions for indexing 2-dimensional arrays.
Matrix notation uses the first index to indicate which row is being selected and
the second index to indicate which column is selected. This is opposite the
geometrically oriented-convention for images where people generally think the
first index represents x position (i.e., column) and the second represents y
position (i.e., row). This alone is the source of much confusion;
matrix-oriented users and image-oriented users expect two different things with
regard to indexing.

The second issue to understand is how indices correspond
to the order in which the array is stored in memory. In Fortran, the first index
is the most rapidly varying index when moving through the elements of a
two-dimensional array as it is stored in memory. If you adopt the matrix
convention for indexing, then this means the matrix is stored one column at a
time (since the first index moves to the next row as it changes). Thus Fortran
is considered a Column-major language. C has just the opposite convention. In
C, the last index changes most rapidly as one moves through the array as
stored in memory. Thus C is a Row-major language. The matrix is stored by
rows. Note that in both cases it presumes that the matrix convention for
indexing is being used, i.e., for both Fortran and C, the first index is the
row. Note this convention implies that the indexing convention is invariant
and that the data order changes to keep that so.

But that's not the only way
to look at it. Suppose one has large two-dimensional arrays (images or
matrices) stored in data files. Suppose the data are stored by rows rather than
by columns. If we are to preserve our index convention (whether matrix or
image) that means that depending on the language we use, we may be forced to
reorder the data if it is read into memory to preserve our indexing
convention. For example, if we read row-ordered data into memory without
reordering, it will match the matrix indexing convention for C, but not for
Fortran. Conversely, it will match the image indexing convention for Fortran,
but not for C. For C, if one is using data stored in row order, and one wants
to preserve the image index convention, the data must be reordered when
reading into memory.

In the end, what you do for Fortran or C depends on
which is more important, not reordering data or preserving the indexing
convention. For large images, reordering data is potentially expensive, and
often the indexing convention is inverted to avoid that.

The situation with
NumPy makes this issue yet more complicated. The internal machinery of NumPy
arrays is flexible enough to accept any ordering of indices. One can simply
reorder indices by manipulating the internal :term:`stride` information for
arrays without reordering the data at all. NumPy will know how to map the new
index order to the data without moving the data.

So if this is true, why not choose
the index order that matches what you most expect? In particular, why not define
row-ordered images to use the image convention? (This is sometimes referred
to as the Fortran convention vs the C convention, thus the 'C' and 'FORTRAN'
order options for array ordering in NumPy.) The drawback of doing this is
potential performance penalties. It's common to access the data sequentially,
either implicitly in array operations or explicitly by looping over rows of an
image. When that is done, then the data will be accessed in non-optimal order.
As the first index is incremented, what is actually happening is that elements
spaced far apart in memory are being sequentially accessed, with usually poor
memory access speeds. For example, for a two-dimensional image ``im`` defined so
that ``im[0, 10]`` represents the value at ``x = 0``, ``y = 10``. To be
consistent with usual Python behavior then ``im[0]`` would represent a column
at ``x = 0``. Yet that data would be spread over the whole array since the data
are stored in row order. Despite the flexibility of NumPy's indexing, it can't
really paper over the fact basic operations are rendered inefficient because of
data order or that getting contiguous subarrays is still awkward (e.g.,
``im[:, 0]`` for the first row, vs ``im[0]``). Thus one can't use an idiom such
as for row in ``im``; for col in ``im`` does work, but doesn't yield contiguous
column data.

As it turns out, NumPy is
smart enough when dealing with :ref:`ufuncs <ufuncs-internals>` to determine
which index is the most rapidly varying one in memory and uses that for the
innermost loop. Thus for ufuncs, there is no large intrinsic advantage to
either approach in most cases. On the other hand, use of :attr:`ndarray.flat`
with a FORTRAN ordered array will lead to non-optimal memory access as adjacent
elements in the flattened array (iterator, actually) are not contiguous in
memory.

Indeed, the fact is that Python
indexing on lists and other sequences naturally leads to an outside-to-inside
ordering (the first index gets the largest grouping, the next largest,
and the last gets the smallest element). Since image data are normally stored
in rows, this corresponds to the position within rows being the last item
indexed.

If you do want to use Fortran ordering realize that
there are two approaches to consider: 1) accept that the first index is just not
the most rapidly changing in memory and have all your I/O routines reorder
your data when going from memory to disk or visa versa, or use NumPy's
mechanism for mapping the first index to the most rapidly varying data. We
recommend the former if possible. The disadvantage of the latter is that many
of NumPy's functions will yield arrays without Fortran ordering unless you are
careful to use the ``order`` keyword. Doing this would be highly inconvenient.

Otherwise, we recommend simply learning to reverse the usual order of indices
when accessing elements of an array. Granted, it goes against the grain, but
it is more in line with Python semantics and the natural order of the data.


.. _devindex:

#####################
Contributing to NumPy
#####################

Not a coder? Not a problem! NumPy is multi-faceted, and we can use a lot of help.
These are all activities we'd like to get help with (they're all important, so
we list them in alphabetical order):

- Code maintenance and development
- Community coordination
- DevOps
- Developing educational content & narrative documentation
- Fundraising
- Marketing
- Project management
- Translating content
- Website design and development
- Writing technical documentation

The rest of this document discusses working on the NumPy code base and documentation.
We're in the process of updating our descriptions of other activities and roles.
If you are interested in these other activities, please contact us!
You can do this via
the `numpy-discussion mailing list <https://mail.python.org/mailman/listinfo/numpy-discussion>`__,
or on `GitHub <https://github.com/numpy/numpy>`__ (open an issue or comment on a
relevant issue). These are our preferred communication channels (open source is open
by nature!), however if you prefer to discuss in private first, please reach out to
our community coordinators at `numpy-team@googlegroups.com` or `numpy-team.slack.com`
(send an email to `numpy-team@googlegroups.com` for an invite the first time).

Development process - summary
=============================

Here's the short summary, complete TOC links are below:

1. If you are a first-time contributor:

   * Go to `https://github.com/numpy/numpy
     <https://github.com/numpy/numpy>`_ and click the
     "fork" button to create your own copy of the project.

   * Clone the project to your local computer::

      git clone https://github.com/your-username/numpy.git

   * Change the directory::

      cd numpy

   * Add the upstream repository::

      git remote add upstream https://github.com/numpy/numpy.git

   * Now, `git remote -v` will show two remote repositories named:

     - ``upstream``, which refers to the ``numpy`` repository
     - ``origin``, which refers to your personal fork

2. Develop your contribution:

   * Pull the latest changes from upstream::

      git checkout main
      git pull upstream main

   * Create a branch for the feature you want to work on. Since the
     branch name will appear in the merge message, use a sensible name
     such as 'linspace-speedups'::

      git checkout -b linspace-speedups

   * Commit locally as you progress (``git add`` and ``git commit``)
     Use a :ref:`properly formatted<writing-the-commit-message>` commit message,
     write tests that fail before your change and pass afterward, run all the
     :ref:`tests locally<development-environment>`. Be sure to document any
     changed behavior in docstrings, keeping to the NumPy docstring
     :ref:`standard<howto-document>`.

3. To submit your contribution:

   * Push your changes back to your fork on GitHub::

      git push origin linspace-speedups

   * Enter your GitHub username and password (repeat contributors or advanced
     users can remove this step by connecting to GitHub with
     :ref:`SSH<set-up-and-configure-a-github-account>`).

   * Go to GitHub. The new branch will show up with a green Pull Request
     button. Make sure the title and message are clear, concise, and self-
     explanatory. Then click the button to submit it.

   * If your commit introduces a new feature or changes functionality, post on
     the `mailing list`_ to explain your changes. For bug fixes, documentation
     updates, etc., this is generally not necessary, though if you do not get
     any reaction, do feel free to ask for review.

4. Review process:

   * Reviewers (the other developers and interested community members) will
     write inline and/or general comments on your Pull Request (PR) to help
     you improve its implementation, documentation and style.  Every single
     developer working on the project has their code reviewed, and we've come
     to see it as friendly conversation from which we all learn and the
     overall code quality benefits.  Therefore, please don't let the review
     discourage you from contributing: its only aim is to improve the quality
     of project, not to criticize (we are, after all, very grateful for the
     time you're donating!). See our :ref:`Reviewer Guidelines
     <reviewer-guidelines>` for more information.

   * To update your PR, make your changes on your local repository, commit,
     **run tests, and only if they succeed** push to your fork. As soon as
     those changes are pushed up (to the same branch as before) the PR will
     update automatically. If you have no idea how to fix the test failures,
     you may push your changes anyway and ask for help in a PR comment.

   * Various continuous integration (CI) services are triggered after each PR
     update to build the code, run unit tests, measure code coverage and check
     coding style of your branch. The CI tests must pass before your PR can be
     merged. If CI fails, you can find out why by clicking on the "failed"
     icon (red cross) and inspecting the build and test log. To avoid overuse
     and waste of this resource,
     :ref:`test your work<recommended-development-setup>` locally before
     committing.

   * A PR must be **approved** by at least one core team member before merging.
     Approval means the core team member has carefully reviewed the changes,
     and the PR is ready for merging.

5. Document changes

   Beyond changes to a functions docstring and possible description in the
   general documentation, if your change introduces any user-facing
   modifications they may need to be mentioned in the release notes.
   To add your change to the release notes, you need to create a short file
   with a summary and place it in ``doc/release/upcoming_changes``.
   The file ``doc/release/upcoming_changes/README.rst`` details the format and
   filename conventions.

   If your change introduces a deprecation, make sure to discuss this first on
   GitHub or the mailing list first. If agreement on the deprecation is
   reached, follow :ref:`NEP 23 deprecation policy <NEP23>`  to add the deprecation.

6. Cross referencing issues

   If the PR relates to any issues, you can add the text ``xref gh-xxxx`` where
   ``xxxx`` is the number of the issue to github comments. Likewise, if the PR
   solves an issue, replace the ``xref`` with ``closes``, ``fixes`` or any of
   the other flavors `github accepts <https://help.github.com/en/articles/
   closing-issues-using-keywords>`_.

   In the source code, be sure to preface any issue or PR reference with
   ``gh-xxxx``.

For a more detailed discussion, read on and follow the links at the bottom of
this page.

Divergence between ``upstream/main`` and your feature branch
------------------------------------------------------------

If GitHub indicates that the branch of your Pull Request can no longer
be merged automatically, you have to incorporate changes that have been made
since you started into your branch. Our recommended way to do this is to
:ref:`rebase on main <rebasing-on-main>`.

.. _guidelines:

Guidelines
----------

* All code should have tests (see `test coverage`_ below for more details).
* All code should be `documented <https://numpydoc.readthedocs.io/
  en/latest/format.html#docstring-standard>`_.
* No changes are ever committed without review and approval by a core
  team member. Please ask politely on the PR or on the `mailing list`_ if you
  get no response to your pull request within a week.

.. _stylistic-guidelines:
  
Stylistic Guidelines
--------------------

* Set up your editor to follow `PEP 8 <https://www.python.org/dev/peps/
  pep-0008/>`_ (remove trailing white space, no tabs, etc.).  Check code with
  pyflakes / flake8.

* Use NumPy data types instead of strings (``np.uint8`` instead of
  ``"uint8"``).

* Use the following import conventions::

   import numpy as np

* For C code, see :ref:`NEP 45 <NEP45>`.


Test coverage
-------------

Pull requests (PRs) that modify code should either have new tests, or modify existing
tests to fail before the PR and pass afterwards. You should :ref:`run the tests
<development-environment>` before pushing a PR.

Running NumPy's test suite locally requires some additional packages, such as
``pytest`` and ``hypothesis``. The additional testing dependencies are listed
in ``test_requirements.txt`` in the top-level directory, and can conveniently
be installed with::

    pip install -r test_requirements.txt

Tests for a module should ideally cover all code in that module,
i.e., statement coverage should be at 100%.

To measure the test coverage, install
`pytest-cov <https://pytest-cov.readthedocs.io/en/latest/>`__
and then run::

  $ python runtests.py --coverage

This will create a report in ``build/coverage``, which can be viewed with::

  $ firefox build/coverage/index.html

.. _building-docs:

Building docs
-------------

To build docs, run ``make`` from the ``doc`` directory. ``make help`` lists
all targets. For example, to build the HTML documentation, you can run::

    make html

To get the appropriate dependencies and other requirements,
see :ref:`howto-build-docs`.

Fixing Warnings
~~~~~~~~~~~~~~~

-  "citation not found: R###" There is probably an underscore after a
   reference in the first line of a docstring (e.g. [1]\_). Use this
   method to find the source file: $ cd doc/build; grep -rin R####

-  "Duplicate citation R###, other instance in..."" There is probably a
   [2] without a [1] in one of the docstrings

Development process - details
=============================

The rest of the story

.. toctree::
   :maxdepth: 2

   Git Basics <gitwash/index>
   development_environment
   development_gitpod
   howto_build_docs
   development_workflow
   development_advanced_debugging
   reviewer_guidelines
   ../benchmarking
   NumPy C style guide <https://numpy.org/neps/nep-0045-c_style_guide.html>
   releasing
   governance/index
   howto-docs

NumPy-specific workflow is in :ref:`numpy-development-workflow
<development-workflow>`.

.. _`mailing list`: https://mail.python.org/mailman/listinfo/numpy-discussion
.. _development-environment:

Setting up and using your development environment
=================================================

.. _recommended-development-setup:

Recommended development setup
-----------------------------

Since NumPy contains parts written in C and Cython that need to be
compiled before use, make sure you have the necessary compilers and Python
development headers installed - see :ref:`building-from-source`. Building
NumPy as of version ``1.17`` requires a C99 compliant compiler.

Having compiled code also means that importing NumPy from the development
sources needs some additional steps, which are explained below.  For the rest
of this chapter we assume that you have set up your git repo as described in
:ref:`using-git`.

.. _testing-builds:

Testing builds
--------------

To build the development version of NumPy and run tests, spawn
interactive shells with the Python import paths properly set up etc.,
do one of::

    $ python runtests.py -v
    $ python runtests.py -v -s random
    $ python runtests.py -v -t numpy/core/tests/test_nditer.py::test_iter_c_order
    $ python runtests.py --ipython
    $ python runtests.py --python somescript.py
    $ python runtests.py --bench
    $ python runtests.py -g -m full

This builds NumPy first, so the first time it may take a few minutes.  If
you specify ``-n``, the tests are run against the version of NumPy (if
any) found on current PYTHONPATH.

When specifying a target using ``-s``, ``-t``, or ``--python``, additional
arguments may be forwarded to the target embedded by ``runtests.py`` by passing
the extra arguments after a bare ``--``. For example, to run a test method with
the ``--pdb`` flag forwarded to the target, run the following::

    $ python runtests.py -t numpy/tests/test_scripts.py::test_f2py -- --pdb

When using pytest as a target (the default), you can
`match test names using python operators`_ by passing the ``-k`` argument to pytest::

    $ python runtests.py -v -t numpy/core/tests/test_multiarray.py -- -k "MatMul and not vector"

.. note::

    Remember that all tests of NumPy should pass before committing your changes.

Using ``runtests.py`` is the recommended approach to running tests.
There are also a number of alternatives to it, for example in-place
build or installing to a virtualenv or a conda environment. See the FAQ below
for details.

.. note::

   Some of the tests in the test suite require a large amount of
   memory, and are skipped if your system does not have enough.

   To override the automatic detection of available memory, set the
   environment variable ``NPY_AVAILABLE_MEM``, for example
   ``NPY_AVAILABLE_MEM=32GB``, or using pytest ``--available-memory=32GB``
   target option.


Building in-place
-----------------

For development, you can set up an in-place build so that changes made to
``.py`` files have effect without rebuild. First, run::

    $ python setup.py build_ext -i

This allows you to import the in-place built NumPy *from the repo base
directory only*.  If you want the in-place build to be visible outside that
base dir, you need to point your ``PYTHONPATH`` environment variable to this
directory.  Some IDEs (`Spyder`_ for example) have utilities to manage
``PYTHONPATH``.  On Linux and OSX, you can run the command::

    $ export PYTHONPATH=$PWD

and on Windows::

    $ set PYTHONPATH=/path/to/numpy

Now editing a Python source file in NumPy allows you to immediately
test and use your changes (in ``.py`` files), by simply restarting the
interpreter.

Note that another way to do an inplace build visible outside the repo base dir
is with ``python setup.py develop``.  Instead of adjusting ``PYTHONPATH``, this
installs a ``.egg-link`` file into your site-packages as well as adjusts the
``easy-install.pth`` there, so its a more permanent (and magical) operation.


.. _Spyder: https://www.spyder-ide.org/

Other build options
-------------------

Build options can be discovered by running any of::

    $ python setup.py --help
    $ python setup.py --help-commands

It's possible to do a parallel build with ``numpy.distutils`` with the ``-j`` option;
see :ref:`parallel-builds` for more details.

A similar approach to in-place builds and use of ``PYTHONPATH`` but outside the
source tree is to use::

    $ pip install . --prefix /some/owned/folder
    $ export PYTHONPATH=/some/owned/folder/lib/python3.4/site-packages


NumPy uses a series of tests to probe the compiler and libc libraries for
functions. The results are stored in ``_numpyconfig.h`` and ``config.h`` files
using ``HAVE_XXX`` definitions. These tests are run during the ``build_src``
phase of the ``_multiarray_umath`` module in the ``generate_config_h`` and
``generate_numpyconfig_h`` functions. Since the output of these calls includes
many compiler warnings and errors, by default it is run quietly. If you wish
to see this output, you can run the ``build_src`` stage verbosely::

    $ python build build_src -v

Using virtual environments
--------------------------

A frequently asked question is "How do I set up a development version of NumPy
in parallel to a released version that I use to do my job/research?".

One simple way to achieve this is to install the released version in
site-packages, by using pip or conda for example, and set
up the development version in a virtual environment.

If you use conda, we recommend creating a separate virtual environment for
numpy development using the ``environment.yml`` file in the root of the repo
(this will create the environment and install all development dependencies at
once)::

    $ conda env create -f environment.yml  # `mamba` works too for this command
    $ conda activate numpy-dev

If you installed Python some other way than conda, first install
`virtualenv`_ (optionally use `virtualenvwrapper`_), then create your
virtualenv (named ``numpy-dev`` here) with::

    $ virtualenv numpy-dev

Now, whenever you want to switch to the virtual environment, you can use the
command ``source numpy-dev/bin/activate``, and ``deactivate`` to exit from the
virtual environment and back to your previous shell.


Running tests
-------------

Besides using ``runtests.py``, there are various ways to run the tests.  Inside
the interpreter, tests can be run like this::

    >>> np.test()  # doctest: +SKIPBLOCK
    >>> np.test('full')   # Also run tests marked as slow
    >>> np.test('full', verbose=2)   # Additionally print test name/file

    An example of a successful test :
    ``4686 passed, 362 skipped, 9 xfailed, 5 warnings in 213.99 seconds``

Or a similar way from the command line::

    $ python -c "import numpy as np; np.test()"

Tests can also be run with ``pytest numpy``, however then the NumPy-specific
plugin is not found which causes strange side effects

Running individual test files can be useful; it's much faster than running the
whole test suite or that of a whole module (example: ``np.random.test()``).
This can be done with::

    $ python path_to_testfile/test_file.py

That also takes extra arguments, like ``--pdb`` which drops you into the Python
debugger when a test fails or an exception is raised.

Running tests with `tox`_ is also supported.  For example, to build NumPy and
run the test suite with Python 3.7, use::

    $ tox -e py37

For more extensive information, see :ref:`testing-guidelines`

*Note: do not run the tests from the root directory of your numpy git repo without ``runtests.py``,
that will result in strange test errors.*

Running Linting
---------------
Lint checks can be performed on newly added lines of Python code.

Install all dependent packages using pip::

    $ python -m pip install -r linter_requirements.txt

To run lint checks before committing new code, run::

    $ python runtests.py --lint uncommitted

To check all changes in newly added Python code of current branch with target branch, run::

    $ python runtests.py --lint main

If there are no errors, the script exits with no message. In case of errors::

    $ python runtests.py --lint main
    ./numpy/core/tests/test_scalarmath.py:34:5: E303 too many blank lines (3)
    1       E303 too many blank lines (3)

It is advisable to run lint checks before pushing commits to a remote branch
since the linter runs as part of the CI pipeline.

For more details on Style Guidelines:

   - `Python Style Guide`_
   - `C Style Guide`_

Rebuilding & cleaning the workspace
-----------------------------------

Rebuilding NumPy after making changes to compiled code can be done with the
same build command as you used previously - only the changed files will be
re-built.  Doing a full build, which sometimes is necessary, requires cleaning
the workspace first.  The standard way of doing this is (*note: deletes any
uncommitted files!*)::

    $ git clean -xdf

When you want to discard all changes and go back to the last commit in the
repo, use one of::

    $ git checkout .
    $ git reset --hard


.. _debugging:

Debugging
---------

Another frequently asked question is "How do I debug C code inside NumPy?".
First, ensure that you have gdb installed on your system with the Python
extensions (often the default on Linux). You can see which version of
Python is running inside gdb to verify your setup::

    (gdb) python
    >import sys
    >print(sys.version_info)
    >end
    sys.version_info(major=3, minor=7, micro=0, releaselevel='final', serial=0)

Next you need to write a Python script that invokes the C code whose execution
you want to debug. For instance ``mytest.py``::

    import numpy as np
    x = np.arange(5)
    np.empty_like(x)

Now, you can run::

    $ gdb --args python runtests.py -g --python mytest.py

And then in the debugger::

    (gdb) break array_empty_like
    (gdb) run

The execution will now stop at the corresponding C function and you can step
through it as usual. A number of useful Python-specific commands are available.
For example to see where in the Python code you are, use ``py-list``.  For more
details, see `DebuggingWithGdb`_. Here are some commonly used commands:

   - ``list``: List specified function or line.
   - ``next``: Step program, proceeding through subroutine calls.
   - ``step``: Continue program being debugged, after signal or breakpoint.
   - ``print``: Print value of expression EXP.

Instead of plain ``gdb`` you can of course use your favourite
alternative debugger; run it on the python binary with arguments
``runtests.py -g --python mytest.py``.

Building NumPy with a Python built with debug support (on Linux distributions
typically packaged as ``python-dbg``) is highly recommended.



.. _DebuggingWithGdb: https://wiki.python.org/moin/DebuggingWithGdb
.. _tox: https://tox.readthedocs.io/
.. _virtualenv: http://www.virtualenv.org/
.. _virtualenvwrapper: http://www.doughellmann.com/projects/virtualenvwrapper/
.. _Waf: https://code.google.com/p/waf/
.. _`match test names using python operators`: https://docs.pytest.org/en/latest/usage.html#specifying-tests-selecting-tests
.. _`Python Style Guide`: https://www.python.org/dev/peps/pep-0008/
.. _`C Style Guide`: https://numpy.org/neps/nep-0045-c_style_guide.html

Understanding the code & getting started
----------------------------------------

The best strategy to better understand the code base is to pick something you
want to change and start reading the code to figure out how it works. When in
doubt, you can ask questions on the mailing list. It is perfectly okay if your
pull requests aren't perfect, the community is always happy to help. As a
volunteer project, things do sometimes get dropped and it's totally fine to
ping us if something has sat without a response for about two to four weeks.

So go ahead and pick something that annoys or confuses you about NumPy,
experiment with the code, hang around for discussions or go through the
reference documents to try to fix it. Things will fall in place and soon
you'll have a pretty good understanding of the project as a whole. Good Luck!
.. _howto-docs:

############################################
How to contribute to the NumPy documentation
############################################

This guide will help you decide what to contribute and how to submit it to the
official NumPy documentation.

***************************
Documentation team meetings
***************************

The NumPy community has set a firm goal of improving its documentation. We
hold regular documentation meetings on Zoom (dates are announced on the
`numpy-discussion mailing list
<https://mail.python.org/mailman/listinfo/numpy-discussion>`__), and everyone
is welcome. Reach out if you have questions or need
someone to guide you through your first steps -- we're happy to help.
Minutes are taken `on hackmd.io <https://hackmd.io/oB_boakvRqKR-_2jRV-Qjg>`__
and stored in the `NumPy Archive repository
<https://github.com/numpy/archive>`__.

*************
What's needed
*************

The :ref:`NumPy Documentation <numpy_docs_mainpage>` has the details covered.
API reference documentation is generated directly from
`docstrings <https://www.python.org/dev/peps/pep-0257/>`_ in the code when the
documentation is :ref:`built<howto-build-docs>`. Although we have mostly
complete reference documentation for each function and class exposed to users,
there is a lack of usage examples for some of them.

What we lack are docs with broader scope -- tutorials, how-tos, and
explanations. Reporting defects is another way to contribute. We discuss both.

******************
Contributing fixes
******************

We're eager to hear about and fix doc defects. But to attack the biggest
problems we end up having to defer or overlook some bug reports. Here are the
best defects to go after.

Top priority goes to **technical inaccuracies** -- a docstring missing a
parameter, a faulty description of a function/parameter/method, and so on.
Other "structural" defects like broken links also get priority. All these fixes
are easy to confirm and put in place. You can submit
a `pull request (PR) <https://numpy.org/devdocs/dev/index.html#devindex>`__
with the fix, if you know how to do that; otherwise please `open an issue
<https://github.com/numpy/numpy/issues>`__.

**Typos and misspellings** fall on a lower rung; we welcome hearing about them
but may not be able to fix them promptly. These too can be handled as pull
requests or issues.

Obvious **wording** mistakes (like leaving out a "not") fall into the typo
category, but other rewordings -- even for grammar -- require a judgment call,
which raises the bar. Test the waters by first presenting the fix as an issue.

Some functions/objects like numpy.ndarray.transpose, numpy.array etc. defined in
C-extension modules have their docstrings defined separately in `_add_newdocs.py
<https://github.com/numpy/numpy/blob/main/numpy/core/_add_newdocs.py>`__

**********************
Contributing new pages
**********************

Your frustrations using our documents are our best guide to what needs fixing.

If you write a missing doc you join the front line of open source, but it's
a meaningful contribution just to let us know what's missing. If you want to
compose a doc, run your thoughts by the `mailing list
<https://mail.python.org/mailman/listinfo/numpy-discussion>`__ for further
ideas and feedback. If you want to alert us to a gap,
`open an issue <https://github.com/numpy/numpy/issues>`__. See
`this issue <https://github.com/numpy/numpy/issues/15760>`__ for an example.

If you're looking for subjects, our formal roadmap for documentation is a
*NumPy Enhancement Proposal (NEP)*,
`NEP 44 - Restructuring the NumPy Documentation <https://www.numpy.org/neps/nep-0044-restructuring-numpy-docs>`__.
It identifies areas where our docs need help and lists several
additions we'd like to see, including :ref:`Jupyter notebooks <numpy_tutorials>`.

.. _tutorials_howtos_explanations:

Documentation framework
=======================

There are formulas for writing useful documents, and four formulas
cover nearly everything. There are four formulas because there are four
categories of document -- ``tutorial``, ``how-to guide``, ``explanation``,
and ``reference``. The insight that docs divide up this way belongs to
Daniele Procida and his `Diátaxis Framework <https://diataxis.fr/>`__. When you
begin a document or propose one, have in mind which of these types it will be.

.. _numpy_tutorials:

NumPy tutorials
===============

In addition to the documentation that is part of the NumPy source tree, you can
submit content in Jupyter Notebook format to the
`NumPy Tutorials <https://numpy.org/numpy-tutorials>`__ page. This
set of tutorials and educational materials is meant to provide high-quality
resources by the NumPy project, both for self-learning and for teaching classes
with. These resources are developed in a separate GitHub repository,
`numpy-tutorials <https://github.com/numpy/numpy-tutorials>`__, where you can
check out existing notebooks, open issues to suggest new topics or submit your
own tutorials as pull requests.

.. _contributing:

More on contributing
====================

Don't worry if English is not your first language, or if you can only come up
with a rough draft. Open source is a community effort. Do your best -- we'll
help fix issues.

Images and real-life data make text more engaging and powerful, but be sure
what you use is appropriately licensed and available. Here again, even a rough
idea for artwork can be polished by others.

For now, the only data formats accepted by NumPy are those also used by other
Python scientific libraries like pandas, SciPy, or Matplotlib. We're
developing a package to accept more formats; contact us for details.

NumPy documentation is kept in the source code tree. To get your document
into the docbase you must download the tree, :ref:`build it
<howto-build-docs>`, and submit a pull request. If GitHub and pull requests
are new to you, check our :ref:`Contributor Guide <devindex>`.

Our markup language is reStructuredText (rST), which is more elaborate than
Markdown. Sphinx, the tool many Python projects use to build and link project
documentation, converts the rST into HTML and other formats. For more on
rST, see the `Quick reStructuredText Guide
<https://docutils.sourceforge.io/docs/user/rst/quickref.html>`__ or the
`reStructuredText Primer
<http://www.sphinx-doc.org/en/stable/usage/restructuredtext/basics.html>`__


***********************
Contributing indirectly
***********************

If you run across outside material that would be a useful addition to the
NumPy docs, let us know by `opening an issue <https://github.com/numpy/numpy/issues>`__.

You don't have to contribute here to contribute to NumPy. You've contributed
if you write a tutorial on your blog, create a YouTube video, or answer questions
on Stack Overflow and other sites.


.. _howto-document:

*******************
Documentation style
*******************

.. _userdoc_guide:

User documentation
==================

- In general, we follow the
  `Google developer documentation style guide <https://developers.google.com/style>`_
  for the User Guide.

- NumPy style governs cases where:

      - Google has no guidance, or
      - We prefer not to use the Google style

  Our current rules:

      - We pluralize *index* as *indices* rather than
        `indexes <https://developers.google.com/style/word-list#letter-i>`_,
        following the precedent of :func:`numpy.indices`.

      - For consistency we also pluralize *matrix* as *matrices*.

- Grammatical issues inadequately addressed by the NumPy or Google rules are
  decided by the section on "Grammar and Usage" in the most recent edition of
  the `Chicago Manual of Style
  <https://en.wikipedia.org/wiki/The_Chicago_Manual_of_Style>`_.

- We welcome being
  `alerted <https://github.com/numpy/numpy/issues>`_ to cases
  we should add to the NumPy style rules.

.. _docstring_intro:

Docstrings
==========

When using `Sphinx <http://www.sphinx-doc.org/>`_ in combination with the
NumPy conventions, you should use the ``numpydoc`` extension so that your
docstrings will be handled correctly. For example, Sphinx will extract the
``Parameters`` section from your docstring and convert it into a field
list.  Using ``numpydoc`` will also avoid the reStructuredText errors produced
by plain Sphinx when it encounters NumPy docstring conventions like
section headers (e.g. ``-------------``) that sphinx does not expect to
find in docstrings.

It is available from:

* `numpydoc on PyPI <https://pypi.python.org/pypi/numpydoc>`_
* `numpydoc on GitHub <https://github.com/numpy/numpydoc/>`_

Note that for documentation within NumPy, it is not necessary to do
``import numpy as np`` at the beginning of an example.

Please use the ``numpydoc`` :ref:`formatting standard <numpydoc:format>` as
shown in their :ref:`example <numpydoc:example>`.

.. _doc_c_code:

Documenting C/C++ Code
======================

NumPy uses Doxygen_ to parse specially-formatted C/C++ comment blocks. This generates
XML files, which are  converted by Breathe_ into RST, which is used by Sphinx.

**It takes three steps to complete the documentation process**:

1. Writing the comment blocks
-----------------------------

Although there is still no commenting style set to follow, the Javadoc
is more preferable than the others due to the similarities with the current
existing non-indexed comment blocks.

.. note::
   Please see `"Documenting the code" <https://www.doxygen.nl/manual/docblocks.html>`__.

**This is what Javadoc style looks like**:

.. literalinclude:: examples/doxy_func.h

**And here is how it is rendered**:

.. doxygenfunction:: doxy_javadoc_example

**For line comment, you can use a triple forward slash. For example**:

.. literalinclude:: examples/doxy_class.hpp

**And here is how it is rendered**:

.. doxygenclass:: DoxyLimbo

Common Doxygen Tags:
++++++++++++++++++++

.. note::
   For more tags/commands, please take a look at https://www.doxygen.nl/manual/commands.html

``@brief``

Starts a paragraph that serves as a brief description. By default the first sentence
of the documentation block is automatically treated as a brief description, since
option `JAVADOC_AUTOBRIEF <https://www.doxygen.nl/manual/config.html#cfg_javadoc_autobrief>`__
is enabled within doxygen configurations.

``@details``

Just like ``@brief`` starts a brief description, ``@details`` starts the detailed description.
You can also start a new paragraph (blank line) then the ``@details`` command is not needed.

``@param``

Starts a parameter description for a function parameter with name <parameter-name>,
followed by a description of the parameter. The existence of the parameter is checked
and a warning is given if the documentation of this (or any other) parameter is missing
or not present in the function declaration or definition.

``@return``

Starts a return value description for a function.
Multiple adjacent ``@return`` commands will be joined into a single paragraph.
The ``@return`` description ends when a blank line or some other sectioning command is encountered.

``@code/@endcode``

Starts/Ends a block of code. A code block is treated differently from ordinary text.
It is interpreted as source code.

``@rst/@endrst``

Starts/Ends a block of reST markup.

Example
~~~~~~~
**Take a look at the following example**:

.. literalinclude:: examples/doxy_rst.h

**And here is how it is rendered**:

.. doxygenfunction:: doxy_reST_example

2. Feeding Doxygen
------------------

Not all headers files are collected automatically. You have to add the desired
C/C++ header paths within the sub-config files of Doxygen.

Sub-config files have the unique name ``.doxyfile``, which you can usually find near
directories that contain documented headers. You need to create a new config file if
there's not one located in a path close(2-depth) to the headers you want to add.

Sub-config files can accept any of Doxygen_ `configuration options <https://www.doxygen.nl/manual/config.html>`__,
but do not override or re-initialize any configuration option,
rather only use the concatenation operator "+=". For example::

   # to specfiy certain headers
   INPUT += @CUR_DIR/header1.h \
            @CUR_DIR/header2.h
   # to add all headers in certain path
   INPUT += @CUR_DIR/to/headers
   # to define certain macros
   PREDEFINED += C_MACRO(X)=X
   # to enable certain branches
   PREDEFINED += NPY_HAVE_FEATURE \
                 NPY_HAVE_FEATURE2

.. note::
    @CUR_DIR is a template constant returns the current
    dir path of the sub-config file.

3. Inclusion directives
-----------------------

Breathe_ provides a wide range of custom directives to allow
converting the documents generated by Doxygen_ into reST files.

.. note::
   For more information, please check out "`Directives & Config Variables <https://breathe.readthedocs.io/en/latest/directives.html>`__"

Common directives:
++++++++++++++++++

``doxygenfunction``

This directive generates the appropriate output for a single function.
The function name is required to be unique in the project.

.. code::

   .. doxygenfunction:: <function name>
       :outline:
       :no-link:

Checkout the `example <https://breathe.readthedocs.io/en/latest/function.html#function-example>`__
to see it in action.


``doxygenclass``

This directive generates the appropriate output for a single class.
It takes the standard project, path, outline and no-link options and
additionally the members, protected-members, private-members, undoc-members,
membergroups and members-only options:

.. code::

    .. doxygenclass:: <class name>
       :members: [...]
       :protected-members:
       :private-members:
       :undoc-members:
       :membergroups: ...
       :members-only:
       :outline:
       :no-link:

Checkout the `doxygenclass documentation <https://breathe.readthedocs.io/en/latest/class.html#class-example>_`
for more details and to see it in action.

``doxygennamespace``

This directive generates the appropriate output for the contents of a namespace.
It takes the standard project, path, outline and no-link options and additionally the content-only,
members, protected-members, private-members and undoc-members options.
To reference a nested namespace, the full namespaced path must be provided,
e.g. foo::bar for the bar namespace inside the foo namespace.

.. code::

    .. doxygennamespace:: <namespace>
       :content-only:
       :outline:
       :members:
       :protected-members:
       :private-members:
       :undoc-members:
       :no-link:

Checkout the `doxygennamespace documentation <https://breathe.readthedocs.io/en/latest/namespace.html#namespace-example>`__
for more details and to see it in action.

``doxygengroup``

This directive generates the appropriate output for the contents of a doxygen group.
A doxygen group can be declared with specific doxygen markup in the source comments
as covered in the doxygen `grouping documentation <https://www.doxygen.nl/manual/grouping.html>`__.

It takes the standard project, path, outline and no-link options and additionally the
content-only, members, protected-members, private-members and undoc-members options.

.. code::

    .. doxygengroup:: <group name>
       :content-only:
       :outline:
       :members:
       :protected-members:
       :private-members:
       :undoc-members:
       :no-link:
       :inner:

Checkout the `doxygengroup documentation <https://breathe.readthedocs.io/en/latest/group.html#group-example>`__
for more details and to see it in action.

.. _`Doxygen`: https://www.doxygen.nl/index.html
.. _`Breathe`: https://breathe.readthedocs.io/en/latest/


*********************
Documentation reading
*********************

- The leading organization of technical writers,
  `Write the Docs <https://www.writethedocs.org/>`__,
  holds conferences, hosts learning resources, and runs a Slack channel.

- "Every engineer is also a writer," says Google's
  `collection of technical writing resources <https://developers.google.com/tech-writing>`__,
  which includes free online courses for developers in planning and writing
  documents.

- `Software Carpentry's <https://software-carpentry.org/lessons>`__ mission is
  teaching software to researchers. In addition to hosting the curriculum, the
  website explains how to present ideas effectively.
.. _configure-git:

=================
Git configuration
=================

.. _git-config-basic:

Overview
========

Your personal git_ configurations are saved in the ``.gitconfig`` file in
your home directory.
Here is an example ``.gitconfig`` file::

  [user]
          name = Your Name
          email = you@yourdomain.example.com

  [alias]
          ci = commit -a
          co = checkout
          st = status -a
          stat = status -a
          br = branch
          wdiff = diff --color-words

  [core]
          editor = vim

  [merge]
          summary = true

You can edit this file directly or you can use the ``git config --global``
command::

  git config --global user.name "Your Name"
  git config --global user.email you@yourdomain.example.com
  git config --global alias.ci "commit -a"
  git config --global alias.co checkout
  git config --global alias.st "status -a"
  git config --global alias.stat "status -a"
  git config --global alias.br branch
  git config --global alias.wdiff "diff --color-words"
  git config --global core.editor vim
  git config --global merge.summary true

To set up on another computer, you can copy your ``~/.gitconfig`` file,
or run the commands above.

In detail
=========

user.name and user.email
------------------------

It is good practice to tell git_ who you are, for labeling any changes
you make to the code.  The simplest way to do this is from the command
line::

  git config --global user.name "Your Name"
  git config --global user.email you@yourdomain.example.com

This will write the settings into your git configuration file,  which
should now contain a user section with your name and email::

  [user]
        name = Your Name
        email = you@yourdomain.example.com

Of course you'll need to replace ``Your Name`` and ``you@yourdomain.example.com``
with your actual name and email address.

Aliases
-------

You might well benefit from some aliases to common commands.

For example, you might well want to be able to shorten ``git checkout``
to ``git co``.  Or you may want to alias ``git diff --color-words``
(which gives a nicely formatted output of the diff) to ``git wdiff``

The following ``git config --global`` commands::

  git config --global alias.ci "commit -a"
  git config --global alias.co checkout
  git config --global alias.st "status -a"
  git config --global alias.stat "status -a"
  git config --global alias.br branch
  git config --global alias.wdiff "diff --color-words"

will create an ``alias`` section in your ``.gitconfig`` file with contents
like this::

  [alias]
          ci = commit -a
          co = checkout
          st = status -a
          stat = status -a
          br = branch
          wdiff = diff --color-words

Editor
------

You may also want to make sure that your editor of choice is used ::

  git config --global core.editor vim

Merging
-------

To enforce summaries when doing merges (``~/.gitconfig`` file again)::

   [merge]
      log = true

Or from the command line::

  git config --global merge.log true


.. include:: git_links.inc
.. _development-setup:

##############################################################################
Setting up git for NumPy development
##############################################################################

To contribute code or documentation, you first need

#. git installed on your machine
#. a GitHub account
#. a fork of NumPy


******************************************************************************
Install git
******************************************************************************

You may already have git; check by typing ``git --version``. If it's
installed you'll see some variation of ``git version 2.11.0``.
If instead you see ``command is not recognized``, ``command not
found``, etc.,
`install git <https://git-scm.com/book/en/v2/Getting-Started-Installing-Git>`_.

Then set your name and email: ::

  git config --global user.email you@yourdomain.example.com
  git config --global user.name "Your Name"

.. _set-up-and-configure-a-github-account:

******************************************************************************
Create a GitHub account
******************************************************************************

If you don't have a GitHub account, visit https://github.com/join to create
one.

.. _forking:

******************************************************************************
Create a NumPy fork
******************************************************************************

``Forking`` has two steps -- visit GitHub to create a fork repo in your
account, then make a copy of it on your own machine.

Create the fork repo
==============================================================================

#. Log into your GitHub account.
#. Go to the `NumPy GitHub home <https://github.com/numpy/numpy>`_.
#. At the upper right of the page, click ``Fork``:

   .. image:: forking_button.png

   You'll see

   .. image:: forking_message.png

   and then you'll be taken to the home page of your forked copy:

   .. image:: forked_page.png


.. _set-up-fork:

Make the local copy
==============================================================================

#. In the directory where you want the copy created, run ::

    git clone https://github.com/your-user-name/numpy.git

   You'll see something like: ::

    $ git clone https://github.com/your-user-name/numpy.git
    Cloning into 'numpy'...
    remote: Enumerating objects: 12, done.
    remote: Counting objects: 100% (12/12), done.
    remote: Compressing objects: 100% (12/12), done.
    remote: Total 175837 (delta 0), reused 0 (delta 0), pack-reused 175825
    Receiving objects: 100% (175837/175837), 78.16 MiB | 9.87 MiB/s, done.
    Resolving deltas: 100% (139317/139317), done.

   A directory ``numpy`` is created on your machine. (If you already have
   a numpy directory, GitHub will choose a different name like ``numpy-1``.)
   ::

    $ ls -l
    total 0
    drwxrwxrwx 1 bjn bjn 4096 Jun 20 07:20 numpy

.. _linking-to-upstream:

#. Give the name ``upstream`` to the main NumPy repo: ::

    cd numpy
    git remote add upstream https://github.com/numpy/numpy.git

#. Set up your repository so ``git pull`` pulls from ``upstream`` by
   default: ::

    git config branch.main.remote upstream
    git config branch.main.merge refs/heads/main

******************************************************************************
Look it over
******************************************************************************

#. The branches shown by ``git branch -a`` will include

   - the ``main`` branch you just cloned on your own machine
   - the ``main`` branch from your fork on GitHub, which git named
     ``origin`` by default
   - the ``main`` branch on the main NumPy repo, which you named
     ``upstream``.

   ::

     main
     remotes/origin/main
     remotes/upstream/main

   If ``upstream`` isn't there, it will be added after you access the
   NumPy repo with a command like ``git fetch`` or ``git pull``.


#. The repos shown by ``git remote -v show`` will include your fork on GitHub
   and the main repo: ::

    upstream	https://github.com/numpy/numpy.git (fetch)
    upstream	https://github.com/numpy/numpy.git (push)
    origin	https://github.com/your-user-name/numpy.git (fetch)
    origin	https://github.com/your-user-name/numpy.git (push)

#. ``git config --list`` will include ::

    user.email=your_email@example.com
    user.name=Your Name
    remote.origin.url=git@github.com:your-github-id/numpy.git
    remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
    branch.main.remote=upstream
    branch.main.merge=refs/heads/main
    remote.upstream.url=https://github.com/numpy/numpy.git
    remote.upstream.fetch=+refs/heads/*:refs/remotes/upstream/*

.. include:: git_links.inc


******************************************************************************
Optional: set up SSH keys to avoid passwords
******************************************************************************

Cloning your NumPy fork repo required no password, because it read the remote
repo without changing it. Later, though, submitting your pull requests will
write to it, and GitHub will ask for your username and password -- even though
it's your own repo. You can eliminate this authentication without compromising
security by `setting up SSH keys \
<https://help.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh>`_.

**If you set up the keys before cloning**, the instructions above change
slightly. Instead of ::

  git clone https://github.com/your-user-name/numpy.git

run ::

  git clone git@github.com:your-user-name/numpy.git

and instead of showing an ``https`` URL,  ``git remote -v`` will show ::

  origin  git@github.com:your-user-name/numpy.git (fetch)
  origin  git@github.com:your-user-name/numpy.git (push)


**If you have cloned already** and want to start using SSH, see
`Switching remote URLs from HTTPS to SSH \
<https://help.github.com/en/github/using-git/changing-a-remotes-url#switching-remote-urls-from-https-to-ssh>`_.
.. _using-git:
.. _git-development:

=====================
 Git for development
=====================

These pages describe a general git_ and github_ workflow.

This is not a comprehensive git_ reference. It's tailored to the github_
hosting service. You may well find better or quicker ways of getting stuff done
with git_, but these should get you started.

For general resources for learning git_ see :ref:`git-resources`.

Have a look at the github_ install help pages available from `github help`_

.. _install-git:


Contents:

.. toctree::
   :maxdepth: 2

   git_intro
   following_latest
   development_setup
   configure_git
   dot2_dot3
   git_resources

.. include:: git_links.inc
.. _dot2-dot3:

========================================
 Two and three dots in difference specs
========================================

Thanks to Yarik Halchenko for this explanation.

Imagine a series of commits A, B, C, D...  Imagine that there are two
branches, *topic* and *main*.  You branched *topic* off *main* when
*main* was at commit 'E'.  The graph of the commits looks like this::


        A---B---C topic
        /
   D---E---F---G main

Then::

   git diff main..topic

will output the difference from G to C (i.e. with effects of F and G),
while::

   git diff main...topic

would output just differences in the topic branch (i.e. only A, B, and
C).
.. _git-resources:

=========================
Additional Git_ Resources
=========================

Tutorials and summaries
=======================

* `github help`_ has an excellent series of how-to guides.
* `learn.github`_ has an excellent series of tutorials
* The `pro git book`_ is a good in-depth book on git.
* A `git cheat sheet`_ is a page giving summaries of common commands.
* The `git user manual`_
* The `git tutorial`_
* The `git community book`_
* `git ready`_ - a nice series of tutorials
* `git casts`_ - video snippets giving git how-tos.
* `git magic`_ - extended introduction with intermediate detail
* The `git parable`_ is an easy read explaining the concepts behind git.
* Our own `git foundation`_ expands on the `git parable`_.
* Fernando Perez' git page - `Fernando's git page`_ - many links and tips
* A good but technical page on `git concepts`_
* `git svn crash course`_: git_ for those of us used to subversion_

Advanced git workflow
=====================

There are many ways of working with git_; here are some posts on the
rules of thumb that other projects have come up with:

* Linus Torvalds on `git management`_
* Linus Torvalds on `linux git workflow`_ .  Summary; use the git tools
  to make the history of your edits as clean as possible; merge from
  upstream edits as little as possible in branches where you are doing
  active development.

Manual pages online
===================

You can get these on your own machine with (e.g) ``git help push`` or
(same thing) ``git push --help``, but, for convenience, here are the
online manual pages for some common commands:

* `git add`_
* `git branch`_
* `git checkout`_
* `git clone`_
* `git commit`_
* `git config`_
* `git diff`_
* `git log`_
* `git pull`_
* `git push`_
* `git remote`_
* `git status`_

.. include:: git_links.inc
.. _following-latest:

These are the instructions if you just want to follow the latest
*NumPy* source, but you don't need to do any development for now.
If you do want to contribute a patch (excellent!) or do more extensive
NumPy development, see :ref:`development-workflow`.

The steps are:

* :ref:`install-git`
* get local copy of the git repository from Github_
* update local copy from time to time

Get the local copy of the code
==============================

From the command line::

   git clone https://github.com/numpy/numpy.git

You now have a copy of the code tree in the new ``numpy`` directory.
If this doesn't work you can try the alternative read-only url::

   git clone https://github.com/numpy/numpy.git

Updating the code
=================

From time to time you may want to pull down the latest code.  Do this with::

   cd numpy
   git fetch
   git merge --ff-only

The tree in ``numpy`` will now have the latest changes from the initial
repository.

.. _Github: https://github.com/numpy
Install git
===========

Developing with git can be done entirely without github. Git is a distributed
version control system. In order to use git on your machine you must `install
it`_.

.. include:: git_links.inc
================================================================
  NumPy project governance and decision-making
================================================================

The purpose of this document is to formalize the governance process
used by the NumPy project in both ordinary and extraordinary
situations, and to clarify how decisions are made and how the various
elements of our community interact, including the relationship between
open source collaborative development and work that may be funded by
for-profit or non-profit entities.

Summary
=======

NumPy is a community-owned and community-run project. To the maximum
extent possible, decisions about project direction are made by community
consensus (but note that "consensus" here has a somewhat technical
meaning that might not match everyone's expectations -- see below). Some
members of the community additionally contribute by serving on the NumPy
steering council, where they are responsible for facilitating the
establishment of community consensus, for stewarding project resources,
and -- in extreme cases -- for making project decisions if the normal
community-based process breaks down.

The Project
===========

The NumPy Project (The Project) is an open source software project
affiliated with the 501(c)3 NumFOCUS Foundation. The goal of The Project
is to develop open source software for array-based computing in Python,
and in particular the ``numpy`` package, along with related software
such as ``f2py`` and the NumPy Sphinx extensions. The Software developed
by The Project is released under the BSD (or similar) open source
license, developed openly and hosted on public GitHub repositories under
the ``numpy`` GitHub organization.

The Project is developed by a team of distributed developers, called
Contributors. Contributors are individuals who have contributed code,
documentation, designs or other work to the Project. Anyone can be a
Contributor. Contributors can be affiliated with any legal entity or
none. Contributors participate in the project by submitting, reviewing
and discussing GitHub Pull Requests and Issues and participating in open
and public Project discussions on GitHub, mailing lists, and other
channels. The foundation of Project participation is openness and
transparency.

The Project Community consists of all Contributors and Users of the
Project. Contributors work on behalf of and are responsible to the
larger Project Community and we strive to keep the barrier between
Contributors and Users as low as possible.

The Project is formally affiliated with the 501(c)3 NumFOCUS Foundation
(http://numfocus.org), which serves as its fiscal sponsor, may hold
project trademarks and other intellectual property, helps manage project
donations and acts as a parent legal entity. NumFOCUS is the only legal
entity that has a formal relationship with the project (see
Institutional Partners section below).

Governance
==========

This section describes the governance and leadership model of The
Project.

The foundations of Project governance are:

-  Openness & Transparency
-  Active Contribution
-  Institutional Neutrality

Consensus-based decision making by the community
------------------------------------------------

Normally, all project decisions will be made by consensus of all
interested Contributors. The primary goal of this approach is to ensure
that the people who are most affected by and involved in any given
change can contribute their knowledge in the confidence that their
voices will be heard, because thoughtful review from a broad community
is the best mechanism we know of for creating high-quality software.

The mechanism we use to accomplish this goal may be unfamiliar for those
who are not experienced with the cultural norms around free/open-source
software development. We provide a summary here, and highly recommend
that all Contributors additionally read `Chapter 4: Social and Political
Infrastructure <http://producingoss.com/en/producingoss.html#social-infrastructure>`_
of Karl Fogel's classic *Producing Open Source Software*, and in
particular the section on `Consensus-based
Democracy <http://producingoss.com/en/producingoss.html#consensus-democracy>`_,
for a more detailed discussion.

In this context, consensus does *not* require:

-  that we wait to solicit everybody's opinion on every change,
-  that we ever hold a vote on anything,
-  or that everybody is happy or agrees with every decision.

For us, what consensus means is that we entrust *everyone* with the
right to veto any change if they feel it necessary. While this may sound
like a recipe for obstruction and pain, this is not what happens.
Instead, we find that most people take this responsibility seriously,
and only invoke their veto when they judge that a serious problem is
being ignored, and that their veto is necessary to protect the project.
And in practice, it turns out that such vetoes are almost never formally
invoked, because their mere possibility ensures that Contributors are
motivated from the start to find some solution that everyone can live
with -- thus accomplishing our goal of ensuring that all interested
perspectives are taken into account.

How do we know when consensus has been achieved? In principle, this is
rather difficult, since consensus is defined by the absence of vetos,
which requires us to somehow prove a negative. In practice, we use a
combination of our best judgement (e.g., a simple and uncontroversial
bug fix posted on GitHub and reviewed by a core developer is probably
fine) and best efforts (e.g., all substantive API changes must be posted
to the mailing list in order to give the broader community a chance to
catch any problems and suggest improvements; we assume that anyone who
cares enough about NumPy to invoke their veto right should be on the
mailing list). If no-one bothers to comment on the mailing list after a
few days, then it's probably fine. And worst case, if a change is more
controversial than expected, or a crucial critique is delayed because
someone was on vacation, then it's no big deal: we apologize for
misjudging the situation, `back up, and sort things
out <http://producingoss.com/en/producingoss.html#version-control-relaxation>`_.

If one does need to invoke a formal veto, then it should consist of:

-  an unambiguous statement that a veto is being invoked,
-  an explanation of why it is being invoked, and
-  a description of what conditions (if any) would convince the vetoer
   to withdraw their veto.

If all proposals for resolving some issue are vetoed, then the status
quo wins by default.

In the worst case, if a Contributor is genuinely misusing their veto in
an obstructive fashion to the detriment of the project, then they can be
ejected from the project by consensus of the Steering Council -- see
below.

Steering Council
----------------

The Project will have a Steering Council that consists of Project
Contributors who have produced contributions that are substantial in
quality and quantity, and sustained over at least one year. The overall
role of the Council is to ensure, with input from the Community, the
long-term well-being of the project, both technically and as a
community.

During the everyday project activities, council members participate in
all discussions, code review and other project activities as peers with
all other Contributors and the Community. In these everyday activities,
Council Members do not have any special power or privilege through their
membership on the Council. However, it is expected that because of the
quality and quantity of their contributions and their expert knowledge
of the Project Software and Services that Council Members will provide
useful guidance, both technical and in terms of project direction, to
potentially less experienced contributors.

The Steering Council and its Members play a special role in certain
situations. In particular, the Council may, if necessary:

-  Make decisions about the overall scope, vision and direction of the
   project.
-  Make decisions about strategic collaborations with other
   organizations or individuals.
-  Make decisions about specific technical issues, features, bugs and
   pull requests. They are the primary mechanism of guiding the code
   review process and merging pull requests.
-  Make decisions about the Services that are run by The Project and
   manage those Services for the benefit of the Project and Community.
-  Update policy documents such as this one.
-  Make decisions when regular community discussion doesn’t produce
   consensus on an issue in a reasonable time frame.

However, the Council's primary responsibility is to facilitate the
ordinary community-based decision making procedure described above. If
we ever have to step in and formally override the community for the
health of the Project, then we will do so, but we will consider reaching
this point to indicate a failure in our leadership.

Council decision making
~~~~~~~~~~~~~~~~~~~~~~~

If it becomes necessary for the Steering Council to produce a formal
decision, then they will use a form of the `Apache Foundation voting
process <https://www.apache.org/foundation/voting.html>`_. This is a
formalized version of consensus, in which +1 votes indicate agreement,
-1 votes are vetoes (and must be accompanied with a rationale, as
above), and one can also vote fractionally (e.g. -0.5, +0.5) if one
wishes to express an opinion without registering a full veto. These
numeric votes are also often used informally as a way of getting a
general sense of people's feelings on some issue, and should not
normally be taken as formal votes. A formal vote only occurs if
explicitly declared, and if this does occur then the vote should be held
open for long enough to give all interested Council Members a chance to
respond -- at least one week.

In practice, we anticipate that for most Steering Council decisions
(e.g., voting in new members) a more informal process will suffice.

Council membership
~~~~~~~~~~~~~~~~~~

A list of current Steering Council Members is maintained at the
page `About Us <https://numpy.org/about/>`_.

To become eligible to join the Steering Council, an individual must be
a Project Contributor who has produced contributions that are
substantial in quality and quantity, and sustained over at least one
year. Potential Council Members are nominated by existing Council
members, and become members following consensus of the existing
Council members, and confirmation that the potential Member is
interested and willing to serve in that capacity. The Council will be
initially formed from the set of existing Core Developers who, as of
late 2015, have been significantly active over the last year.

When considering potential Members, the Council will look at candidates
with a comprehensive view of their contributions. This will include but
is not limited to code, code review, infrastructure work, mailing list
and chat participation, community help/building, education and outreach,
design work, etc. We are deliberately not setting arbitrary quantitative
metrics (like “100 commits in this repo”) to avoid encouraging behavior
that plays to the metrics rather than the project’s overall well-being.
We want to encourage a diverse array of backgrounds, viewpoints and
talents in our team, which is why we explicitly do not define code as
the sole metric on which council membership will be evaluated.

If a Council member becomes inactive in the project for a period of one
year, they will be considered for removal from the Council. Before
removal, inactive Member will be approached to see if they plan on
returning to active participation. If not they will be removed
immediately upon a Council vote. If they plan on returning to active
participation soon, they will be given a grace period of one year. If
they don’t return to active participation within that time period they
will be removed by vote of the Council without further grace period. All
former Council members can be considered for membership again at any
time in the future, like any other Project Contributor. Retired Council
members will be listed on the project website, acknowledging the period
during which they were active in the Council.

The Council reserves the right to eject current Members, if they are
deemed to be actively harmful to the project’s well-being, and attempts
at communication and conflict resolution have failed. This requires the
consensus of the remaining Members.


Conflict of interest
~~~~~~~~~~~~~~~~~~~~

It is expected that the Council Members will be employed at a wide range
of companies, universities and non-profit organizations. Because of
this, it is possible that Members will have conflict of interests. Such
conflict of interests include, but are not limited to:

-  Financial interests, such as investments, employment or contracting
   work, outside of The Project that may influence their work on The
   Project.
-  Access to proprietary information of their employer that could
   potentially leak into their work with the Project.

All members of the Council shall disclose to the rest of the Council any
conflict of interest they may have. Members with a conflict of interest
in a particular issue may participate in Council discussions on that
issue, but must recuse themselves from voting on the issue.

Private communications of the Council
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To the maximum extent possible, Council discussions and activities
will be public and done in collaboration and discussion with the
Project Contributors and Community. The Council will have a private
mailing list that will be used sparingly and only when a specific
matter requires privacy. When private communications and decisions are
needed, the Council will do its best to summarize those to the
Community after eliding personal/private/sensitive information that
should not be posted to the public internet.

Subcommittees
~~~~~~~~~~~~~

The Council can create subcommittees that provide leadership and
guidance for specific aspects of the project. Like the Council as a
whole, subcommittees should conduct their business in an open and public
manner unless privacy is specifically called for. Private subcommittee
communications should happen on the main private mailing list of the
Council unless specifically called for.

NumFOCUS Subcommittee
~~~~~~~~~~~~~~~~~~~~~

The Council will maintain one narrowly focused subcommittee to manage
its interactions with NumFOCUS.

-  The NumFOCUS Subcommittee is comprised of 5 persons who manage
   project funding that comes through NumFOCUS. It is expected that
   these funds will be spent in a manner that is consistent with the
   non-profit mission of NumFOCUS and the direction of the Project as
   determined by the full Council.
-  This Subcommittee shall NOT make decisions about the direction, scope
   or technical direction of the Project.
-  This Subcommittee will have 5 members, 4 of whom will be current
   Council Members and 1 of whom will be external to the Steering
   Council. No more than 2 Subcommittee Members can report to one person
   through employment or contracting work (including the reportee, i.e.
   the reportee + 1 is the max). This avoids effective majorities
   resting on one person.

The current membership of the NumFOCUS Subcommittee is listed at the
page `About Us <https://numpy.org/about/>`_.


Institutional Partners and Funding
==================================

The Steering Council are the primary leadership for the project. No
outside institution, individual or legal entity has the ability to own,
control, usurp or influence the project other than by participating in
the Project as Contributors and Council Members. However, because
institutions can be an important funding mechanism for the project, it
is important to formally acknowledge institutional participation in the
project. These are Institutional Partners.

An Institutional Contributor is any individual Project Contributor who
contributes to the project as part of their official duties at an
Institutional Partner. Likewise, an Institutional Council Member is any
Project Steering Council Member who contributes to the project as part
of their official duties at an Institutional Partner.

With these definitions, an Institutional Partner is any recognized legal
entity in the United States or elsewhere that employs at least 1
Institutional Contributor of Institutional Council Member. Institutional
Partners can be for-profit or non-profit entities.

Institutions become eligible to become an Institutional Partner by
employing individuals who actively contribute to The Project as part of
their official duties. To state this another way, the only way for a
Partner to influence the project is by actively contributing to the open
development of the project, in equal terms to any other member of the
community of Contributors and Council Members. Merely using Project
Software in institutional context does not allow an entity to become an
Institutional Partner. Financial gifts do not enable an entity to become
an Institutional Partner. Once an institution becomes eligible for
Institutional Partnership, the Steering Council must nominate and
approve the Partnership.

If at some point an existing Institutional Partner stops having any
contributing employees, then a one year grace period commences. If at
the end of this one year period they continue not to have any
contributing employees, then their Institutional Partnership will
lapse, and resuming it will require going through the normal process
for new Partnerships.

An Institutional Partner is free to pursue funding for their work on The
Project through any legal means. This could involve a non-profit
organization raising money from private foundations and donors or a
for-profit company building proprietary products and services that
leverage Project Software and Services. Funding acquired by
Institutional Partners to work on The Project is called Institutional
Funding. However, no funding obtained by an Institutional Partner can
override the Steering Council. If a Partner has funding to do NumPy work
and the Council decides to not pursue that work as a project, the
Partner is free to pursue it on their own. However in this situation,
that part of the Partner’s work will not be under the NumPy umbrella and
cannot use the Project trademarks in a way that suggests a formal
relationship.

Institutional Partner benefits are:

-  Acknowledgement on the NumPy websites, in talks and T-shirts.
-  Ability to acknowledge their own funding sources on the NumPy
   websites, in talks and T-shirts.
-  Ability to influence the project through the participation of their
   Council Member.
-  Council Members invited to NumPy Developer Meetings.

A list of current Institutional Partners is maintained at the page
`About Us <https://numpy.org/about/>`_.


Document history
================

https://github.com/numpy/numpy/commits/main/doc/source/dev/governance/governance.rst

Acknowledgements
================

Substantial portions of this document were adapted from the
`Jupyter/IPython project's governance document <https://github.com/jupyter/governance>`_

License
=======

To the extent possible under law, the authors have waived all
copyright and related or neighboring rights to the NumPy project
governance and decision-making document, as per the `CC-0 public
domain dedication / license
<https://creativecommons.org/publicdomain/zero/1.0/>`_.
#####################
NumPy governance
#####################

.. toctree::
   :maxdepth: 3

   governance
.. _routines.datetime:

Datetime Support Functions
**************************

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   datetime_as_string
   datetime_data


Business Day Functions
======================

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   busdaycalendar
   is_busday
   busday_offset
   busday_count
.. index::
   pair: array; interface
   pair: array; protocol

.. _arrays.interface:

*******************
The Array Interface
*******************

.. note::

   This page describes the numpy-specific API for accessing the contents of
   a numpy array from other C extensions. :pep:`3118` --
   :c:func:`The Revised Buffer Protocol <PyObject_GetBuffer>` introduces
   similar, standardized API to Python 2.6 and 3.0 for any extension
   module to use. Cython__'s buffer array support
   uses the :pep:`3118` API; see the `Cython numpy
   tutorial`__. Cython provides a way to write code that supports the buffer
   protocol with Python versions older than 2.6 because it has a
   backward-compatible implementation utilizing the array interface
   described here.

__ http://cython.org/
__ https://github.com/cython/cython/wiki/tutorials-numpy

:version: 3

The array interface (sometimes called array protocol) was created in
2005 as a means for array-like Python objects to re-use each other's
data buffers intelligently whenever possible. The homogeneous
N-dimensional array interface is a default mechanism for objects to
share N-dimensional array memory and information.  The interface
consists of a Python-side and a C-side using two attributes.  Objects
wishing to be considered an N-dimensional array in application code
should support at least one of these attributes.  Objects wishing to
support an N-dimensional array in application code should look for at
least one of these attributes and use the information provided
appropriately.

This interface describes homogeneous arrays in the sense that each
item of the array has the same "type".  This type can be very simple
or it can be a quite arbitrary and complicated C-like structure.

There are two ways to use the interface: A Python side and a C-side.
Both are separate attributes.

Python side
===========

This approach to the interface consists of the object having an
:data:`~object.__array_interface__` attribute.

.. data:: object.__array_interface__

   A dictionary of items (3 required and 5 optional).  The optional
   keys in the dictionary have implied defaults if they are not
   provided.

   The keys are:

   **shape** (required)
       Tuple whose elements are the array size in each dimension. Each
       entry is an integer (a Python :py:class:`int`).  Note that these
       integers could be larger than the platform ``int`` or ``long``
       could hold (a Python :py:class:`int` is a C ``long``). It is up to the code
       using this attribute to handle this appropriately; either by
       raising an error when overflow is possible, or by using
       ``long long`` as the C type for the shapes.

   **typestr** (required)
       A string providing the basic type of the homogeneous array The
       basic string format consists of 3 parts: a character describing
       the byteorder of the data (``<``: little-endian, ``>``:
       big-endian, ``|``: not-relevant), a character code giving the
       basic type of the array, and an integer providing the number of
       bytes the type uses.

       The basic type character codes are:

       =====  ================================================================
       ``t``  Bit field (following integer gives the number of
              bits in the bit field).
       ``b``  Boolean (integer type where all values are only True or False)
       ``i``  Integer
       ``u``  Unsigned integer
       ``f``  Floating point
       ``c``  Complex floating point
       ``m``  Timedelta
       ``M``  Datetime
       ``O``  Object (i.e. the memory contains a pointer to :c:type:`PyObject`)
       ``S``  String (fixed-length sequence of char)
       ``U``  Unicode (fixed-length sequence of :c:type:`Py_UNICODE`)
       ``V``  Other (void \* -- each item is a fixed-size chunk of memory)
       =====  ================================================================

   **descr** (optional)
       A list of tuples providing a more detailed description of the
       memory layout for each item in the homogeneous array.  Each
       tuple in the list has two or three elements.  Normally, this
       attribute would be used when *typestr* is ``V[0-9]+``, but this is
       not a requirement.  The only requirement is that the number of
       bytes represented in the *typestr* key is the same as the total
       number of bytes represented here.  The idea is to support
       descriptions of C-like structs that make up array
       elements.  The elements of each tuple in the list are

       1.  A string providing a name associated with this portion of
           the datatype.  This could also be a tuple of ``('full name',
	   'basic_name')`` where basic name would be a valid Python
           variable name representing the full name of the field.

       2. Either a basic-type description string as in *typestr* or
          another list (for nested structured types)

       3. An optional shape tuple providing how many times this part
          of the structure should be repeated.  No repeats are assumed
          if this is not given.  Very complicated structures can be
          described using this generic interface.  Notice, however,
          that each element of the array is still of the same
          data-type.  Some examples of using this interface are given
          below.

       **Default**: ``[('', typestr)]``

   **data** (optional)
       A 2-tuple whose first argument is an integer (a long integer
       if necessary) that points to the data-area storing the array
       contents.  This pointer must point to the first element of
       data (in other words any offset is always ignored in this
       case). The second entry in the tuple is a read-only flag (true
       means the data area is read-only).

       This attribute can also be an object exposing the
       :ref:`buffer interface <bufferobjects>` which
       will be used to share the data. If this key is not present (or
       returns None), then memory sharing will be done
       through the buffer interface of the object itself.  In this
       case, the offset key can be used to indicate the start of the
       buffer.  A reference to the object exposing the array interface
       must be stored by the new object if the memory area is to be
       secured.

       **Default**: None

   **strides** (optional)
       Either ``None`` to indicate a C-style contiguous array or
       a Tuple of strides which provides the number of bytes needed
       to jump to the next array element in the corresponding
       dimension. Each entry must be an integer (a Python
       :py:class:`int`). As with shape, the values may
       be larger than can be represented by a C ``int`` or ``long``; the
       calling code should handle this appropriately, either by
       raising an error, or by using ``long long`` in C. The
       default is ``None`` which implies a C-style contiguous
       memory buffer. In this model, the last dimension of the array
       varies the fastest.  For example, the default strides tuple
       for an object whose array entries are 8 bytes long and whose
       shape is ``(10, 20, 30)`` would be ``(4800, 240, 8)``

       **Default**: ``None`` (C-style contiguous)

   **mask** (optional)
       None or an object exposing the array interface.  All
       elements of the mask array should be interpreted only as true
       or not true indicating which elements of this array are valid.
       The shape of this object should be `"broadcastable"
       <arrays.broadcasting.broadcastable>` to the shape of the
       original array.

       **Default**: None (All array values are valid)

   **offset** (optional)
       An integer offset into the array data region. This can only be
       used when data is ``None`` or returns a :class:`buffer`
       object.

       **Default**: 0.

   **version** (required)
       An integer showing the version of the interface (i.e. 3 for
       this version).  Be careful not to use this to invalidate
       objects exposing future versions of the interface.


C-struct access
===============

This approach to the array interface allows for faster access to an
array using only one attribute lookup and a well-defined C-structure.

.. data:: object.__array_struct__

   A :c:type:`PyCapsule` whose ``pointer`` member contains a
   pointer to a filled :c:type:`PyArrayInterface` structure.  Memory
   for the structure is dynamically created and the :c:type:`PyCapsule`
   is also created with an appropriate destructor so the retriever of
   this attribute simply has to apply :c:func:`Py_DECREF()` to the
   object returned by this attribute when it is finished.  Also,
   either the data needs to be copied out, or a reference to the
   object exposing this attribute must be held to ensure the data is
   not freed.  Objects exposing the :obj:`__array_struct__` interface
   must also not reallocate their memory if other objects are
   referencing them.

The :c:type:`PyArrayInterface` structure is defined in ``numpy/ndarrayobject.h``
as::

  typedef struct {
    int two;              /* contains the integer 2 -- simple sanity check */
    int nd;               /* number of dimensions */
    char typekind;        /* kind in array --- character code of typestr */
    int itemsize;         /* size of each element */
    int flags;            /* flags indicating how the data should be interpreted */
                          /*   must set ARR_HAS_DESCR bit to validate descr */
    Py_intptr_t *shape;   /* A length-nd array of shape information */
    Py_intptr_t *strides; /* A length-nd array of stride information */
    void *data;           /* A pointer to the first element of the array */
    PyObject *descr;      /* NULL or data-description (same as descr key
                                  of __array_interface__) -- must set ARR_HAS_DESCR
                                  flag or this will be ignored. */
  } PyArrayInterface;

The flags member may consist of 5 bits showing how the data should be
interpreted and one bit showing how the Interface should be
interpreted.  The data-bits are :c:macro:`NPY_ARRAY_C_CONTIGUOUS` (0x1),
:c:macro:`NPY_ARRAY_F_CONTIGUOUS` (0x2), :c:macro:`NPY_ARRAY_ALIGNED` (0x100),
:c:macro:`NPY_ARRAY_NOTSWAPPED` (0x200), and :c:macro:`NPY_ARRAY_WRITEABLE` (0x400).  A final flag
:c:macro:`NPY_ARR_HAS_DESCR` (0x800) indicates whether or not this structure
has the arrdescr field.  The field should not be accessed unless this
flag is present.

   .. c:macro:: NPY_ARR_HAS_DESCR

.. admonition:: New since June 16, 2006:

   In the past most implementations used the ``desc`` member of the ``PyCObject``
   (now :c:type:`PyCapsule`) itself (do not confuse this with the "descr" member of
   the :c:type:`PyArrayInterface` structure above --- they are two separate
   things) to hold the pointer to the object exposing the interface.
   This is now an explicit part of the interface.  Be sure to take a
   reference to the object and call :c:func:`PyCapsule_SetContext` before
   returning the :c:type:`PyCapsule`, and configure a destructor to decref this
   reference.


Type description examples
=========================

For clarity it is useful to provide some examples of the type
description and corresponding :data:`~object.__array_interface__` 'descr'
entries.  Thanks to Scott Gilbert for these examples:

In every case, the 'descr' key is optional, but of course provides
more information which may be important for various applications::

     * Float data
         typestr == '>f4'
         descr == [('','>f4')]

     * Complex double
         typestr == '>c8'
         descr == [('real','>f4'), ('imag','>f4')]

     * RGB Pixel data
         typestr == '|V3'
         descr == [('r','|u1'), ('g','|u1'), ('b','|u1')]

     * Mixed endian (weird but could happen).
         typestr == '|V8' (or '>u8')
         descr == [('big','>i4'), ('little','<i4')]

     * Nested structure
         struct {
             int ival;
             struct {
                 unsigned short sval;
                 unsigned char bval;
                 unsigned char cval;
             } sub;
         }
         typestr == '|V8' (or '<u8' if you want)
         descr == [('ival','<i4'), ('sub', [('sval','<u2'), ('bval','|u1'), ('cval','|u1') ]) ]

     * Nested array
         struct {
             int ival;
             double data[16*4];
         }
         typestr == '|V516'
         descr == [('ival','>i4'), ('data','>f8',(16,4))]

     * Padded structure
         struct {
             int ival;
             double dval;
         }
         typestr == '|V16'
         descr == [('ival','>i4'),('','|V4'),('dval','>f8')]

It should be clear that any structured type could be described using this
interface.

Differences with Array interface (Version 2)
============================================

The version 2 interface was very similar.  The differences were
largely aesthetic.  In particular:

1. The PyArrayInterface structure had no descr member at the end
   (and therefore no flag ARR_HAS_DESCR)

2. The ``context`` member of the :c:type:`PyCapsule` (formally the ``desc``
   member of the ``PyCObject``) returned from ``__array_struct__`` was
   not specified.  Usually, it was the object exposing the array (so
   that a reference to it could be kept and destroyed when the
   C-object was destroyed). It is now an explicit requirement that this field
   be used in some way to hold a reference to the owning object.

   .. note::

       Until August 2020, this said:

           Now it must be a tuple whose first element is a string with
           "PyArrayInterface Version #" and whose second element is the object
           exposing the array.

       This design was retracted almost immediately after it was proposed, in
       <https://mail.python.org/pipermail/numpy-discussion/2006-June/020995.html>.
       Despite 14 years of documentation to the contrary, at no point was it
       valid to assume that ``__array_interface__`` capsules held this tuple
       content.

3. The tuple returned from ``__array_interface__['data']`` used to be a
   hex-string (now it is an integer or a long integer).

4. There was no ``__array_interface__`` attribute instead all of the keys
   (except for version) in the ``__array_interface__`` dictionary were
   their own attribute: Thus to obtain the Python-side information you
   had to access separately the attributes:

   * ``__array_data__``
   * ``__array_shape__``
   * ``__array_strides__``
   * ``__array_typestr__``
   * ``__array_descr__``
   * ``__array_offset__``
   * ``__array_mask__``
String operations
*****************

.. currentmodule:: numpy.char

.. module:: numpy.char

The `numpy.char` module provides a set of vectorized string
operations for arrays of type `numpy.str_` or `numpy.bytes_`.
All of them are based on the string methods in the Python standard library.

String operations
-----------------

.. autosummary::
   :toctree: generated/

   add
   multiply
   mod
   capitalize
   center
   decode
   encode
   expandtabs
   join
   ljust
   lower
   lstrip
   partition
   replace
   rjust
   rpartition
   rsplit
   rstrip
   split
   splitlines
   strip
   swapcase
   title
   translate
   upper
   zfill

Comparison
----------

Unlike the standard numpy comparison operators, the ones in the `char`
module strip trailing whitespace characters before performing the
comparison.

.. autosummary::
   :toctree: generated/

   equal
   not_equal
   greater_equal
   less_equal
   greater
   less
   compare_chararrays

String information
------------------

.. autosummary::
   :toctree: generated/

   count
   endswith
   find
   index
   isalpha
   isalnum
   isdecimal
   isdigit
   islower
   isnumeric
   isspace
   istitle
   isupper
   rfind
   rindex
   startswith
   str_len

Convenience class
-----------------

.. autosummary::
   :toctree: generated/

   array
   asarray
   chararray
Logic functions
***************

.. currentmodule:: numpy

Truth value testing
-------------------
.. autosummary::
   :toctree: generated/

   all
   any

Array contents
--------------
.. autosummary::
   :toctree: generated/

   isfinite
   isinf
   isnan
   isnat
   isneginf
   isposinf

Array type testing
------------------
.. autosummary::
   :toctree: generated/

   iscomplex
   iscomplexobj
   isfortran
   isreal
   isrealobj
   isscalar

Logical operations
------------------
.. autosummary::
   :toctree: generated/

   logical_and
   logical_or
   logical_not
   logical_xor

Comparison
----------
.. autosummary::
   :toctree: generated/

   allclose
   isclose
   array_equal
   array_equiv

.. autosummary::
   :toctree: generated/

   greater
   greater_equal
   less
   less_equal
   equal
   not_equal
.. _routines.dtype:

Data type routines
==================

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   can_cast
   promote_types
   min_scalar_type
   result_type
   common_type
   obj2sctype

Creating data types
-------------------
.. autosummary::
   :toctree: generated/

   dtype
   format_parser

Data type information
---------------------
.. autosummary::
   :toctree: generated/

   finfo
   iinfo
   MachAr

Data type testing
-----------------
.. autosummary::
   :toctree: generated/

   issctype
   issubdtype
   issubsctype
   issubclass_
   find_common_type

Miscellaneous
-------------
.. autosummary::
   :toctree: generated/

   typename
   sctype2char
   mintypecode
   maximum_sctype
.. _routines.ma:

Masked array operations
***********************

.. currentmodule:: numpy


Constants
=========

.. autosummary::
   :toctree: generated/

   ma.MaskType


Creation
========

From existing data
~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   ma.masked_array
   ma.array
   ma.copy
   ma.frombuffer
   ma.fromfunction

   ma.MaskedArray.copy


Ones and zeros
~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   ma.empty
   ma.empty_like
   ma.masked_all
   ma.masked_all_like
   ma.ones
   ma.ones_like
   ma.zeros
   ma.zeros_like


_____

Inspecting the array
====================

.. autosummary::
   :toctree: generated/

   ma.all
   ma.any
   ma.count
   ma.count_masked
   ma.getmask
   ma.getmaskarray
   ma.getdata
   ma.nonzero
   ma.shape
   ma.size
   ma.is_masked
   ma.is_mask
   ma.isMaskedArray
   ma.isMA
   ma.isarray


   ma.MaskedArray.all
   ma.MaskedArray.any
   ma.MaskedArray.count
   ma.MaskedArray.nonzero
   ma.shape
   ma.size


.. autosummary::

    ma.MaskedArray.data
    ma.MaskedArray.mask
    ma.MaskedArray.recordmask

_____

Manipulating a MaskedArray
==========================

Changing the shape
~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   ma.ravel
   ma.reshape
   ma.resize

   ma.MaskedArray.flatten
   ma.MaskedArray.ravel
   ma.MaskedArray.reshape
   ma.MaskedArray.resize


Modifying axes
~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.swapaxes
   ma.transpose

   ma.MaskedArray.swapaxes
   ma.MaskedArray.transpose


Changing the number of dimensions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.atleast_1d
   ma.atleast_2d
   ma.atleast_3d
   ma.expand_dims
   ma.squeeze

   ma.MaskedArray.squeeze

   ma.stack
   ma.column_stack
   ma.concatenate
   ma.dstack
   ma.hstack
   ma.hsplit
   ma.mr_
   ma.row_stack
   ma.vstack


Joining arrays
~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.concatenate
   ma.stack
   ma.vstack
   ma.hstack
   ma.dstack
   ma.column_stack
   ma.append


_____

Operations on masks
===================

Creating a mask
~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.make_mask
   ma.make_mask_none
   ma.mask_or
   ma.make_mask_descr


Accessing a mask
~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.getmask
   ma.getmaskarray
   ma.masked_array.mask


Finding masked data
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.flatnotmasked_contiguous
   ma.flatnotmasked_edges
   ma.notmasked_contiguous
   ma.notmasked_edges
   ma.clump_masked
   ma.clump_unmasked


Modifying a mask
~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.mask_cols
   ma.mask_or
   ma.mask_rowcols
   ma.mask_rows
   ma.harden_mask
   ma.soften_mask

   ma.MaskedArray.harden_mask
   ma.MaskedArray.soften_mask
   ma.MaskedArray.shrink_mask
   ma.MaskedArray.unshare_mask


_____

Conversion operations
======================

> to a masked array
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.asarray
   ma.asanyarray
   ma.fix_invalid
   ma.masked_equal
   ma.masked_greater
   ma.masked_greater_equal
   ma.masked_inside
   ma.masked_invalid
   ma.masked_less
   ma.masked_less_equal
   ma.masked_not_equal
   ma.masked_object
   ma.masked_outside
   ma.masked_values
   ma.masked_where


> to a ndarray
~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.compress_cols
   ma.compress_rowcols
   ma.compress_rows
   ma.compressed
   ma.filled

   ma.MaskedArray.compressed
   ma.MaskedArray.filled


> to another object
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.MaskedArray.tofile
   ma.MaskedArray.tolist
   ma.MaskedArray.torecords
   ma.MaskedArray.tobytes


Filling a masked array
~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.common_fill_value
   ma.default_fill_value
   ma.maximum_fill_value
   ma.minimum_fill_value
   ma.set_fill_value

   ma.MaskedArray.get_fill_value
   ma.MaskedArray.set_fill_value

.. autosummary::

    ma.MaskedArray.fill_value

_____

Masked arrays arithmetic
========================

Arithmetic
~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.anom
   ma.anomalies
   ma.average
   ma.conjugate
   ma.corrcoef
   ma.cov
   ma.cumsum
   ma.cumprod
   ma.mean
   ma.median
   ma.power
   ma.prod
   ma.std
   ma.sum
   ma.var

   ma.MaskedArray.anom
   ma.MaskedArray.cumprod
   ma.MaskedArray.cumsum
   ma.MaskedArray.mean
   ma.MaskedArray.prod
   ma.MaskedArray.std
   ma.MaskedArray.sum
   ma.MaskedArray.var


Minimum/maximum
~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.argmax
   ma.argmin
   ma.max
   ma.min
   ma.ptp
   ma.diff

   ma.MaskedArray.argmax
   ma.MaskedArray.argmin
   ma.MaskedArray.max
   ma.MaskedArray.min
   ma.MaskedArray.ptp


Sorting
~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.argsort
   ma.sort
   ma.MaskedArray.argsort
   ma.MaskedArray.sort


Algebra
~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.diag
   ma.dot
   ma.identity
   ma.inner
   ma.innerproduct
   ma.outer
   ma.outerproduct
   ma.trace
   ma.transpose

   ma.MaskedArray.trace
   ma.MaskedArray.transpose


Polynomial fit
~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.vander
   ma.polyfit


Clipping and rounding
~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.around
   ma.clip
   ma.round

   ma.MaskedArray.clip
   ma.MaskedArray.round


Miscellanea
~~~~~~~~~~~
.. autosummary::
   :toctree: generated/

   ma.allequal
   ma.allclose
   ma.apply_along_axis
   ma.apply_over_axes
   ma.arange
   ma.choose
   ma.ediff1d
   ma.indices
   ma.where
Optionally SciPy-accelerated routines (:mod:`numpy.dual`)
*********************************************************

.. automodule:: numpy.dual

Linear algebra
--------------

.. currentmodule:: numpy.linalg

.. autosummary::

   cholesky
   det
   eig
   eigh
   eigvals
   eigvalsh
   inv
   lstsq
   norm
   pinv
   solve
   svd

FFT
---

.. currentmodule:: numpy.fft

.. autosummary::

   fft
   fft2
   fftn
   ifft
   ifft2
   ifftn

Other
-----

.. currentmodule:: numpy

.. autosummary::

   i0
Statistics
==========

.. currentmodule:: numpy


Order statistics
----------------

.. autosummary::
   :toctree: generated/
   
   ptp
   percentile
   nanpercentile
   quantile
   nanquantile

Averages and variances
----------------------

.. autosummary::
   :toctree: generated/

   median
   average
   mean
   std
   var
   nanmedian
   nanmean
   nanstd
   nanvar

Correlating
-----------

.. autosummary::
   :toctree: generated/

   corrcoef
   correlate
   cov

Histograms
----------

.. autosummary::
   :toctree: generated/

   histogram
   histogram2d
   histogramdd
   bincount
   histogram_bin_edges
   digitize
:orphan:

.. automodule:: numpy.polynomial
   :no-members:
   :no-inherited-members:
   :no-special-members:

Configuration
-------------

.. autosummary:: 
   :toctree: generated/

   numpy.polynomial.set_default_printstyle
.. currentmodule:: numpy

.. _arrays.ndarray:

******************************************
The N-dimensional array (:class:`ndarray`)
******************************************

An :class:`ndarray` is a (usually fixed-size) multidimensional
container of items of the same type and size. The number of dimensions
and items in an array is defined by its :attr:`shape <ndarray.shape>`,
which is a :class:`tuple` of *N* non-negative integers that specify the
sizes of each dimension. The type of items in the array is specified by
a separate :ref:`data-type object (dtype) <arrays.dtypes>`, one of which
is associated with each ndarray.

As with other container objects in Python, the contents of an
:class:`ndarray` can be accessed and modified by :ref:`indexing or
slicing <arrays.indexing>` the array (using, for example, *N* integers),
and via the methods and attributes of the :class:`ndarray`.

.. index:: view, base

Different :class:`ndarrays <ndarray>` can share the same data, so that
changes made in one :class:`ndarray` may be visible in another. That
is, an ndarray can be a *"view"* to another ndarray, and the data it
is referring to is taken care of by the *"base"* ndarray. ndarrays can
also be views to memory owned by Python :class:`strings <str>` or
objects implementing the :class:`buffer` or :ref:`array
<arrays.interface>` interfaces.


.. admonition:: Example

   A 2-dimensional array of size 2 x 3, composed of 4-byte integer
   elements:

   >>> x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)
   >>> type(x)
   <class 'numpy.ndarray'>
   >>> x.shape
   (2, 3)
   >>> x.dtype
   dtype('int32')

   The array can be indexed using Python container-like syntax:

   >>> # The element of x in the *second* row, *third* column, namely, 6.
   >>> x[1, 2]
   6

   For example :ref:`slicing <arrays.indexing>` can produce views of
   the array:

   >>> y = x[:,1]
   >>> y
   array([2, 5], dtype=int32)
   >>> y[0] = 9 # this also changes the corresponding element in x
   >>> y
   array([9, 5], dtype=int32)
   >>> x
   array([[1, 9, 3],
          [4, 5, 6]], dtype=int32)


Constructing arrays
===================

New arrays can be constructed using the routines detailed in
:ref:`routines.array-creation`, and also by using the low-level
:class:`ndarray` constructor:

.. autosummary::
   :toctree: generated/

   ndarray

.. _arrays.ndarray.indexing:


Indexing arrays
===============

Arrays can be indexed using an extended Python slicing syntax,
``array[selection]``.  Similar syntax is also used for accessing
fields in a :term:`structured data type`.

.. seealso:: :ref:`Array Indexing <arrays.indexing>`.

.. _memory-layout:

Internal memory layout of an ndarray
====================================

An instance of class :class:`ndarray` consists of a contiguous
one-dimensional segment of computer memory (owned by the array, or by
some other object), combined with an indexing scheme that maps *N*
integers into the location of an item in the block.  The ranges in
which the indices can vary is specified by the :obj:`shape
<ndarray.shape>` of the array. How many bytes each item takes and how
the bytes are interpreted is defined by the :ref:`data-type object
<arrays.dtypes>` associated with the array.

.. index:: C-order, Fortran-order, row-major, column-major, stride,
  offset

A segment of memory is inherently 1-dimensional, and there are many
different schemes for arranging the items of an *N*-dimensional array
in a 1-dimensional block. NumPy is flexible, and :class:`ndarray`
objects can accommodate any *strided indexing scheme*. In a strided
scheme, the N-dimensional index :math:`(n_0, n_1, ..., n_{N-1})`
corresponds to the offset (in bytes):

.. math:: n_{\mathrm{offset}} = \sum_{k=0}^{N-1} s_k n_k

from the beginning of the memory block associated with the
array. Here, :math:`s_k` are integers which specify the :obj:`strides
<ndarray.strides>` of the array. The :term:`column-major` order (used,
for example, in the Fortran language and in *Matlab*) and
:term:`row-major` order (used in C) schemes are just specific kinds of
strided scheme, and correspond to memory that can be *addressed* by the strides:

.. math::

   s_k^{\mathrm{column}} = \mathrm{itemsize} \prod_{j=0}^{k-1} d_j ,
   \quad  s_k^{\mathrm{row}} = \mathrm{itemsize} \prod_{j=k+1}^{N-1} d_j .

.. index:: single-segment, contiguous, non-contiguous

where :math:`d_j` `= self.shape[j]`.

Both the C and Fortran orders are :term:`contiguous`, *i.e.,*
single-segment, memory layouts, in which every part of the
memory block can be accessed by some combination of the indices.

.. note::

    `Contiguous arrays` and `single-segment arrays` are synonymous
    and are used interchangeably throughout the documentation.

While a C-style and Fortran-style contiguous array, which has the corresponding
flags set, can be addressed with the above strides, the actual strides may be
different. This can happen in two cases:

    1. If ``self.shape[k] == 1`` then for any legal index ``index[k] == 0``.
       This means that in the formula for the offset :math:`n_k = 0` and thus
       :math:`s_k n_k = 0` and the value of :math:`s_k` `= self.strides[k]` is
       arbitrary.
    2. If an array has no elements (``self.size == 0``) there is no legal
       index and the strides are never used. Any array with no elements may be
       considered C-style and Fortran-style contiguous.

Point 1. means that ``self`` and ``self.squeeze()`` always have the same
contiguity and ``aligned`` flags value. This also means
that even a high dimensional array could be C-style and Fortran-style
contiguous at the same time.

.. index:: aligned

An array is considered aligned if the memory offsets for all elements and the
base offset itself is a multiple of `self.itemsize`. Understanding
`memory-alignment` leads to better performance on most hardware.

.. note::

    Points (1) and (2) can currently be disabled by the compile time
    environmental variable ``NPY_RELAXED_STRIDES_CHECKING=0``,
    which was the default before NumPy 1.10.
    No users should have to do this. ``NPY_RELAXED_STRIDES_DEBUG=1``
    can be used to help find errors when incorrectly relying on the strides
    in C-extension code (see below warning).

    You can check whether this option was enabled when your NumPy was
    built by looking at the value of ``np.ones((10,1),
    order='C').flags.f_contiguous``. If this is ``True``, then your
    NumPy has relaxed strides checking enabled.

.. warning::

    It does *not* generally hold that ``self.strides[-1] == self.itemsize``
    for C-style contiguous arrays or ``self.strides[0] == self.itemsize`` for
    Fortran-style contiguous arrays is true.

Data in new :class:`ndarrays <ndarray>` is in the :term:`row-major`
(C) order, unless otherwise specified, but, for example, :ref:`basic
array slicing <arrays.indexing>` often produces :term:`views <view>`
in a different scheme.

.. seealso: :ref:`Indexing <arrays.ndarray.indexing>`_

.. note::

   Several algorithms in NumPy work on arbitrarily strided arrays.
   However, some algorithms require single-segment arrays. When an
   irregularly strided array is passed in to such algorithms, a copy
   is automatically made.

.. _arrays.ndarray.attributes:

Array attributes
================

Array attributes reflect information that is intrinsic to the array
itself. Generally, accessing an array through its attributes allows
you to get and sometimes set intrinsic properties of the array without
creating a new array. The exposed attributes are the core parts of an
array and only some of them can be reset meaningfully without creating
a new array. Information on each attribute is given below.

Memory layout
-------------

The following attributes contain information about the memory layout
of the array:

.. autosummary::
   :toctree: generated/

   ndarray.flags
   ndarray.shape
   ndarray.strides
   ndarray.ndim
   ndarray.data
   ndarray.size
   ndarray.itemsize
   ndarray.nbytes
   ndarray.base

Data type
---------

.. seealso:: :ref:`Data type objects <arrays.dtypes>`

The data type object associated with the array can be found in the
:attr:`dtype <ndarray.dtype>` attribute:

.. autosummary::
   :toctree: generated/

   ndarray.dtype

Other attributes
----------------

.. autosummary::
   :toctree: generated/

   ndarray.T
   ndarray.real
   ndarray.imag
   ndarray.flat


.. _arrays.ndarray.array-interface:

Array interface
---------------

.. seealso:: :ref:`arrays.interface`.

==================================  ===================================
:obj:`~object.__array_interface__`  Python-side of the array interface
:obj:`~object.__array_struct__`     C-side of the array interface
==================================  ===================================

:mod:`ctypes` foreign function interface
----------------------------------------

.. autosummary::
   :toctree: generated/

   ndarray.ctypes

.. _array.ndarray.methods:

Array methods
=============

An :class:`ndarray` object has many methods which operate on or with
the array in some fashion, typically returning an array result. These
methods are briefly explained below. (Each method's docstring has a
more complete description.)

For the following methods there are also corresponding functions in
:mod:`numpy`: :func:`all`, :func:`any`, :func:`argmax`,
:func:`argmin`, :func:`argpartition`, :func:`argsort`, :func:`choose`,
:func:`clip`, :func:`compress`, :func:`copy`, :func:`cumprod`,
:func:`cumsum`, :func:`diagonal`, :func:`imag`, :func:`max <amax>`,
:func:`mean`, :func:`min <amin>`, :func:`nonzero`, :func:`partition`,
:func:`prod`, :func:`ptp`, :func:`put`, :func:`ravel`, :func:`real`,
:func:`repeat`, :func:`reshape`, :func:`round <around>`,
:func:`searchsorted`, :func:`sort`, :func:`squeeze`, :func:`std`,
:func:`sum`, :func:`swapaxes`, :func:`take`, :func:`trace`,
:func:`transpose`, :func:`var`.

Array conversion
----------------

.. autosummary::
   :toctree: generated/

   ndarray.item
   ndarray.tolist
   ndarray.itemset
   ndarray.tostring
   ndarray.tobytes
   ndarray.tofile
   ndarray.dump
   ndarray.dumps
   ndarray.astype
   ndarray.byteswap
   ndarray.copy
   ndarray.view
   ndarray.getfield
   ndarray.setflags
   ndarray.fill

Shape manipulation
------------------

For reshape, resize, and transpose, the single tuple argument may be
replaced with ``n`` integers which will be interpreted as an n-tuple.

.. autosummary::
   :toctree: generated/

   ndarray.reshape
   ndarray.resize
   ndarray.transpose
   ndarray.swapaxes
   ndarray.flatten
   ndarray.ravel
   ndarray.squeeze

Item selection and manipulation
-------------------------------

For array methods that take an *axis* keyword, it defaults to
*None*. If axis is *None*, then the array is treated as a 1-D
array. Any other value for *axis* represents the dimension along which
the operation should proceed.

.. autosummary::
   :toctree: generated/

   ndarray.take
   ndarray.put
   ndarray.repeat
   ndarray.choose
   ndarray.sort
   ndarray.argsort
   ndarray.partition
   ndarray.argpartition
   ndarray.searchsorted
   ndarray.nonzero
   ndarray.compress
   ndarray.diagonal

Calculation
-----------

.. index:: axis

Many of these methods take an argument named *axis*. In such cases,

- If *axis* is *None* (the default), the array is treated as a 1-D
  array and the operation is performed over the entire array. This
  behavior is also the default if self is a 0-dimensional array or
  array scalar. (An array scalar is an instance of the types/classes
  float32, float64, etc., whereas a 0-dimensional array is an ndarray
  instance containing precisely one array scalar.)

- If *axis* is an integer, then the operation is done over the given
  axis (for each 1-D subarray that can be created along the given axis).

.. admonition:: Example of the *axis* argument

   A 3-dimensional array of size 3 x 3 x 3, summed over each of its
   three axes

   >>> x = np.arange(27).reshape((3,3,3))
   >>> x
   array([[[ 0,  1,  2],
           [ 3,  4,  5],
           [ 6,  7,  8]],
          [[ 9, 10, 11],
           [12, 13, 14],
           [15, 16, 17]],
          [[18, 19, 20],
           [21, 22, 23],
           [24, 25, 26]]])
   >>> x.sum(axis=0)
   array([[27, 30, 33],
          [36, 39, 42],
          [45, 48, 51]])
   >>> # for sum, axis is the first keyword, so we may omit it,
   >>> # specifying only its value
   >>> x.sum(0), x.sum(1), x.sum(2)
   (array([[27, 30, 33],
           [36, 39, 42],
           [45, 48, 51]]),
    array([[ 9, 12, 15],
           [36, 39, 42],
           [63, 66, 69]]),
    array([[ 3, 12, 21],
           [30, 39, 48],
           [57, 66, 75]]))

The parameter *dtype* specifies the data type over which a reduction
operation (like summing) should take place. The default reduce data
type is the same as the data type of *self*. To avoid overflow, it can
be useful to perform the reduction using a larger data type.

For several methods, an optional *out* argument can also be provided
and the result will be placed into the output array given. The *out*
argument must be an :class:`ndarray` and have the same number of
elements. It can have a different data type in which case casting will
be performed.

.. autosummary::
   :toctree: generated/

   ndarray.max
   ndarray.argmax
   ndarray.min
   ndarray.argmin
   ndarray.ptp
   ndarray.clip
   ndarray.conj
   ndarray.round
   ndarray.trace
   ndarray.sum
   ndarray.cumsum
   ndarray.mean
   ndarray.var
   ndarray.std
   ndarray.prod
   ndarray.cumprod
   ndarray.all
   ndarray.any

Arithmetic, matrix multiplication, and comparison operations
============================================================

.. index:: comparison, arithmetic, matrix, operation, operator

Arithmetic and comparison operations on :class:`ndarrays <ndarray>`
are defined as element-wise operations, and generally yield
:class:`ndarray` objects as results.

Each of the arithmetic operations (``+``, ``-``, ``*``, ``/``, ``//``,
``%``, ``divmod()``, ``**`` or ``pow()``, ``<<``, ``>>``, ``&``,
``^``, ``|``, ``~``) and the comparisons (``==``, ``<``, ``>``,
``<=``, ``>=``, ``!=``) is equivalent to the corresponding
universal function (or :term:`ufunc` for short) in NumPy.  For
more information, see the section on :ref:`Universal Functions
<ufuncs>`.

Comparison operators:

.. autosummary::
   :toctree: generated/

   ndarray.__lt__
   ndarray.__le__
   ndarray.__gt__
   ndarray.__ge__
   ndarray.__eq__
   ndarray.__ne__

Truth value of an array (:class:`bool() <bool>`):

.. autosummary::
   :toctree: generated/

   ndarray.__bool__

.. note::

   Truth-value testing of an array invokes
   :meth:`ndarray.__bool__`, which raises an error if the number of
   elements in the array is larger than 1, because the truth value
   of such arrays is ambiguous. Use :meth:`.any() <ndarray.any>` and
   :meth:`.all() <ndarray.all>` instead to be clear about what is meant
   in such cases. (If the number of elements is 0, the array evaluates
   to ``False``.)


Unary operations:

.. autosummary::
   :toctree: generated/

   ndarray.__neg__
   ndarray.__pos__
   ndarray.__abs__
   ndarray.__invert__

Arithmetic:

.. autosummary::
   :toctree: generated/

   ndarray.__add__
   ndarray.__sub__
   ndarray.__mul__
   ndarray.__truediv__
   ndarray.__floordiv__
   ndarray.__mod__
   ndarray.__divmod__
   ndarray.__pow__
   ndarray.__lshift__
   ndarray.__rshift__
   ndarray.__and__
   ndarray.__or__
   ndarray.__xor__

.. note::

   - Any third argument to :func:`pow()` is silently ignored,
     as the underlying :func:`ufunc <power>` takes only two arguments.

   - Because :class:`ndarray` is a built-in type (written in C), the
     ``__r{op}__`` special methods are not directly defined.

   - The functions called to implement many arithmetic special methods
     for arrays can be modified using :class:`__array_ufunc__ <numpy.class.__array_ufunc__>`.

Arithmetic, in-place:

.. autosummary::
   :toctree: generated/

   ndarray.__iadd__
   ndarray.__isub__
   ndarray.__imul__
   ndarray.__itruediv__
   ndarray.__ifloordiv__
   ndarray.__imod__
   ndarray.__ipow__
   ndarray.__ilshift__
   ndarray.__irshift__
   ndarray.__iand__
   ndarray.__ior__
   ndarray.__ixor__

.. warning::

   In place operations will perform the calculation using the
   precision decided by the data type of the two operands, but will
   silently downcast the result (if necessary) so it can fit back into
   the array.  Therefore, for mixed precision calculations, ``A {op}=
   B`` can be different than ``A = A {op} B``. For example, suppose
   ``a = ones((3,3))``. Then, ``a += 3j`` is different than ``a = a +
   3j``: while they both perform the same computation, ``a += 3``
   casts the result to fit back in ``a``, whereas ``a = a + 3j``
   re-binds the name ``a`` to the result.

Matrix Multiplication:

.. autosummary::
   :toctree: generated/

   ndarray.__matmul__

.. note::

   Matrix operators ``@`` and ``@=`` were introduced in Python 3.5
   following :pep:`465`, and the ``@`` operator has been introduced in NumPy
   1.10.0. Further information can be found in the :func:`matmul` documentation.

Special methods
===============

For standard library functions:

.. autosummary::
   :toctree: generated/

   ndarray.__copy__
   ndarray.__deepcopy__
   ndarray.__reduce__
   ndarray.__setstate__

Basic customization:

.. autosummary::
   :toctree: generated/

   ndarray.__new__
   ndarray.__array__
   ndarray.__array_wrap__

Container customization: (see :ref:`Indexing <arrays.indexing>`)

.. autosummary::
   :toctree: generated/

   ndarray.__len__
   ndarray.__getitem__
   ndarray.__setitem__
   ndarray.__contains__

Conversion; the operations :class:`int() <int>`,
:class:`float() <float>` and :class:`complex() <complex>`.
They work only on arrays that have one element in them
and return the appropriate scalar.

.. autosummary::
   :toctree: generated/

   ndarray.__int__
   ndarray.__float__
   ndarray.__complex__

String representations:

.. autosummary::
   :toctree: generated/

   ndarray.__str__
   ndarray.__repr__

Utility method for typing:

.. autosummary::
   :toctree: generated/

   ndarray.__class_getitem__
Mathematical functions
**********************

.. currentmodule:: numpy

Trigonometric functions
-----------------------
.. autosummary::
   :toctree: generated/

   sin
   cos
   tan
   arcsin
   arccos
   arctan
   hypot
   arctan2
   degrees
   radians
   unwrap
   deg2rad
   rad2deg

Hyperbolic functions
--------------------
.. autosummary::
   :toctree: generated/

   sinh
   cosh
   tanh
   arcsinh
   arccosh
   arctanh

Rounding
--------
.. autosummary::
   :toctree: generated/

   around
   round_
   rint
   fix
   floor
   ceil
   trunc

Sums, products, differences
---------------------------
.. autosummary::
   :toctree: generated/

   prod
   sum
   nanprod
   nansum
   cumprod
   cumsum
   nancumprod
   nancumsum
   diff
   ediff1d
   gradient
   cross
   trapz

Exponents and logarithms
------------------------
.. autosummary::
   :toctree: generated/

   exp
   expm1
   exp2
   log
   log10
   log2
   log1p
   logaddexp
   logaddexp2

Other special functions
-----------------------
.. autosummary::
   :toctree: generated/

   i0
   sinc

Floating point routines
-----------------------
.. autosummary::
   :toctree: generated/

   signbit
   copysign
   frexp
   ldexp
   nextafter
   spacing

Rational routines
-----------------
.. autosummary::
   :toctree: generated/

   lcm
   gcd

Arithmetic operations
---------------------
.. autosummary::
   :toctree: generated/

   add
   reciprocal
   positive
   negative
   multiply
   divide
   power
   subtract
   true_divide
   floor_divide
   float_power

   fmod
   mod
   modf
   remainder
   divmod

Handling complex numbers
------------------------
.. autosummary::
   :toctree: generated/

   angle
   real
   imag
   conj
   conjugate

Extrema Finding
---------------
.. autosummary::
   :toctree: generated/

   maximum
   fmax
   amax
   nanmax
   
   minimum
   fmin
   amin
   nanmin
   

Miscellaneous
-------------
.. autosummary::
   :toctree: generated/

   convolve
   clip

   sqrt
   cbrt
   square

   absolute
   fabs
   sign
   heaviside
   
   nan_to_num
   real_if_close

   interp
.. currentmodule:: numpy

.. _arrays.datetime:

************************
Datetimes and Timedeltas
************************

.. versionadded:: 1.7.0

Starting in NumPy 1.7, there are core array data types which natively
support datetime functionality. The data type is called "datetime64",
so named because "datetime" is already taken by the datetime library
included in Python.


Basic Datetimes
===============

The most basic way to create datetimes is from strings in ISO 8601 date 
or datetime format. It is also possible to create datetimes from an integer by 
offset relative to the Unix epoch (00:00:00 UTC on 1 January 1970).
The unit for internal storage is automatically selected from the 
form of the string, and can be either a :ref:`date unit <arrays.dtypes.dateunits>` or a
:ref:`time unit <arrays.dtypes.timeunits>`. The date units are years ('Y'),
months ('M'), weeks ('W'), and days ('D'), while the time units are
hours ('h'), minutes ('m'), seconds ('s'), milliseconds ('ms'), and
some additional SI-prefix seconds-based units. The datetime64 data type
also accepts the string "NAT", in any combination of lowercase/uppercase
letters, for a "Not A Time" value.

.. admonition:: Example

    A simple ISO date:

    >>> np.datetime64('2005-02-25')
    numpy.datetime64('2005-02-25')
    
    From an integer and a date unit, 1 year since the UNIX epoch:

    >>> np.datetime64(1, 'Y')
    numpy.datetime64('1971')   

    Using months for the unit:

    >>> np.datetime64('2005-02')
    numpy.datetime64('2005-02')

    Specifying just the month, but forcing a 'days' unit:

    >>> np.datetime64('2005-02', 'D')
    numpy.datetime64('2005-02-01')

    From a date and time:

    >>> np.datetime64('2005-02-25T03:30')
    numpy.datetime64('2005-02-25T03:30')

    NAT (not a time):

    >>> np.datetime64('nat')
    numpy.datetime64('NaT')

When creating an array of datetimes from a string, it is still possible
to automatically select the unit from the inputs, by using the
datetime type with generic units.

.. admonition:: Example

    >>> np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64')
    array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype='datetime64[D]')

    >>> np.array(['2001-01-01T12:00', '2002-02-03T13:56:03.172'], dtype='datetime64')
    array(['2001-01-01T12:00:00.000', '2002-02-03T13:56:03.172'],
          dtype='datetime64[ms]')

An array of datetimes can be constructed from integers representing
POSIX timestamps with the given unit.

.. admonition:: Example

    >>> np.array([0, 1577836800], dtype='datetime64[s]')
    array(['1970-01-01T00:00:00', '2020-01-01T00:00:00'],
          dtype='datetime64[s]')

    >>> np.array([0, 1577836800000]).astype('datetime64[ms]')
    array(['1970-01-01T00:00:00.000', '2020-01-01T00:00:00.000'],
          dtype='datetime64[ms]')

The datetime type works with many common NumPy functions, for
example :func:`arange` can be used to generate ranges of dates.

.. admonition:: Example

    All the dates for one month:

    >>> np.arange('2005-02', '2005-03', dtype='datetime64[D]')
    array(['2005-02-01', '2005-02-02', '2005-02-03', '2005-02-04',
           '2005-02-05', '2005-02-06', '2005-02-07', '2005-02-08',
           '2005-02-09', '2005-02-10', '2005-02-11', '2005-02-12',
           '2005-02-13', '2005-02-14', '2005-02-15', '2005-02-16',
           '2005-02-17', '2005-02-18', '2005-02-19', '2005-02-20',
           '2005-02-21', '2005-02-22', '2005-02-23', '2005-02-24',
           '2005-02-25', '2005-02-26', '2005-02-27', '2005-02-28'],
          dtype='datetime64[D]')

The datetime object represents a single moment in time. If two
datetimes have different units, they may still be representing
the same moment of time, and converting from a bigger unit like
months to a smaller unit like days is considered a 'safe' cast
because the moment of time is still being represented exactly.

.. admonition:: Example

    >>> np.datetime64('2005') == np.datetime64('2005-01-01')
    True

    >>> np.datetime64('2010-03-14T15') == np.datetime64('2010-03-14T15:00:00.00')
    True

.. deprecated:: 1.11.0

  NumPy does not store timezone information. For backwards compatibility, datetime64
  still parses timezone offsets, which it handles by converting to
  UTC. This behaviour is deprecated and will raise an error in the
  future.


Datetime and Timedelta Arithmetic
=================================

NumPy allows the subtraction of two Datetime values, an operation which
produces a number with a time unit. Because NumPy doesn't have a physical
quantities system in its core, the timedelta64 data type was created
to complement datetime64. The arguments for timedelta64 are a number,
to represent the number of units, and a date/time unit, such as
(D)ay, (M)onth, (Y)ear, (h)ours, (m)inutes, or (s)econds. The timedelta64
data type also accepts the string "NAT" in place of the number for a "Not A Time" value.

.. admonition:: Example

    >>> np.timedelta64(1, 'D')
    numpy.timedelta64(1,'D')

    >>> np.timedelta64(4, 'h')
    numpy.timedelta64(4,'h')

    >>> np.timedelta64('nAt')
    numpy.timedelta64('NaT')

Datetimes and Timedeltas work together to provide ways for
simple datetime calculations.

.. admonition:: Example

    >>> np.datetime64('2009-01-01') - np.datetime64('2008-01-01')
    numpy.timedelta64(366,'D')

    >>> np.datetime64('2009') + np.timedelta64(20, 'D')
    numpy.datetime64('2009-01-21')

    >>> np.datetime64('2011-06-15T00:00') + np.timedelta64(12, 'h')
    numpy.datetime64('2011-06-15T12:00')

    >>> np.timedelta64(1,'W') / np.timedelta64(1,'D')
    7.0

    >>> np.timedelta64(1,'W') % np.timedelta64(10,'D')
    numpy.timedelta64(7,'D')

    >>> np.datetime64('nat') - np.datetime64('2009-01-01')
    numpy.timedelta64('NaT','D')

    >>> np.datetime64('2009-01-01') + np.timedelta64('nat')
    numpy.datetime64('NaT')

There are two Timedelta units ('Y', years and 'M', months) which are treated
specially, because how much time they represent changes depending
on when they are used. While a timedelta day unit is equivalent to
24 hours, there is no way to convert a month unit into days, because
different months have different numbers of days.

.. admonition:: Example

    >>> a = np.timedelta64(1, 'Y')

    >>> np.timedelta64(a, 'M')
    numpy.timedelta64(12,'M')

    >>> np.timedelta64(a, 'D')
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: Cannot cast NumPy timedelta64 scalar from metadata [Y] to [D] according to the rule 'same_kind'

Datetime Units
==============

The Datetime and Timedelta data types support a large number of time
units, as well as generic units which can be coerced into any of the
other units based on input data.

Datetimes are always stored based on POSIX time (though having a TAI
mode which allows for accounting of leap-seconds is proposed), with
an epoch of 1970-01-01T00:00Z. This means the supported dates are
always a symmetric interval around the epoch, called "time span" in the
table below.

The length of the span is the range of a 64-bit integer times the length
of the date or unit.  For example, the time span for 'W' (week) is exactly
7 times longer than the time span for 'D' (day), and the time span for
'D' (day) is exactly 24 times longer than the time span for 'h' (hour).

Here are the date units:

.. _arrays.dtypes.dateunits:

======== ================ ======================= ==========================
  Code       Meaning       Time span (relative)    Time span (absolute)
======== ================ ======================= ==========================
   Y       year             +/- 9.2e18 years        [9.2e18 BC, 9.2e18 AD]
   M       month            +/- 7.6e17 years        [7.6e17 BC, 7.6e17 AD]
   W       week             +/- 1.7e17 years        [1.7e17 BC, 1.7e17 AD]
   D       day              +/- 2.5e16 years        [2.5e16 BC, 2.5e16 AD]
======== ================ ======================= ==========================

And here are the time units:

.. _arrays.dtypes.timeunits:

======== ================ ======================= ==========================
  Code       Meaning       Time span (relative)    Time span (absolute)
======== ================ ======================= ==========================
   h       hour             +/- 1.0e15 years        [1.0e15 BC, 1.0e15 AD]
   m       minute           +/- 1.7e13 years        [1.7e13 BC, 1.7e13 AD]
   s       second           +/- 2.9e11 years        [2.9e11 BC, 2.9e11 AD]
   ms      millisecond      +/- 2.9e8 years         [ 2.9e8 BC,  2.9e8 AD]
us / μs    microsecond      +/- 2.9e5 years         [290301 BC, 294241 AD]
   ns      nanosecond       +/- 292 years           [  1678 AD,   2262 AD]
   ps      picosecond       +/- 106 days            [  1969 AD,   1970 AD]
   fs      femtosecond      +/- 2.6 hours           [  1969 AD,   1970 AD]
   as      attosecond       +/- 9.2 seconds         [  1969 AD,   1970 AD]
======== ================ ======================= ==========================

Business Day Functionality
==========================

To allow the datetime to be used in contexts where only certain days of
the week are valid, NumPy includes a set of "busday" (business day)
functions.

The default for busday functions is that the only valid days are Monday
through Friday (the usual business days).  The implementation is based on
a "weekmask" containing 7 Boolean flags to indicate valid days; custom
weekmasks are possible that specify other sets of valid days.

The "busday" functions can additionally check a list of "holiday" dates,
specific dates that are not valid days.

The function :func:`busday_offset` allows you to apply offsets
specified in business days to datetimes with a unit of 'D' (day).

.. admonition:: Example

    >>> np.busday_offset('2011-06-23', 1)
    numpy.datetime64('2011-06-24')

    >>> np.busday_offset('2011-06-23', 2)
    numpy.datetime64('2011-06-27')

When an input date falls on the weekend or a holiday,
:func:`busday_offset` first applies a rule to roll the
date to a valid business day, then applies the offset. The
default rule is 'raise', which simply raises an exception.
The rules most typically used are 'forward' and 'backward'.

.. admonition:: Example

    >>> np.busday_offset('2011-06-25', 2)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    ValueError: Non-business day date in busday_offset

    >>> np.busday_offset('2011-06-25', 0, roll='forward')
    numpy.datetime64('2011-06-27')

    >>> np.busday_offset('2011-06-25', 2, roll='forward')
    numpy.datetime64('2011-06-29')

    >>> np.busday_offset('2011-06-25', 0, roll='backward')
    numpy.datetime64('2011-06-24')

    >>> np.busday_offset('2011-06-25', 2, roll='backward')
    numpy.datetime64('2011-06-28')

In some cases, an appropriate use of the roll and the offset
is necessary to get a desired answer.

.. admonition:: Example

    The first business day on or after a date:

    >>> np.busday_offset('2011-03-20', 0, roll='forward')
    numpy.datetime64('2011-03-21')
    >>> np.busday_offset('2011-03-22', 0, roll='forward')
    numpy.datetime64('2011-03-22')

    The first business day strictly after a date:

    >>> np.busday_offset('2011-03-20', 1, roll='backward')
    numpy.datetime64('2011-03-21')
    >>> np.busday_offset('2011-03-22', 1, roll='backward')
    numpy.datetime64('2011-03-23')

The function is also useful for computing some kinds of days
like holidays. In Canada and the U.S., Mother's day is on
the second Sunday in May, which can be computed with a custom
weekmask.

.. admonition:: Example

    >>> np.busday_offset('2012-05', 1, roll='forward', weekmask='Sun')
    numpy.datetime64('2012-05-13')

When performance is important for manipulating many business dates
with one particular choice of weekmask and holidays, there is
an object :class:`busdaycalendar` which stores the data necessary
in an optimized form.

np.is_busday():
```````````````
To test a datetime64 value to see if it is a valid day, use :func:`is_busday`.

.. admonition:: Example

    >>> np.is_busday(np.datetime64('2011-07-15'))  # a Friday
    True
    >>> np.is_busday(np.datetime64('2011-07-16')) # a Saturday
    False
    >>> np.is_busday(np.datetime64('2011-07-16'), weekmask="Sat Sun")
    True
    >>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
    >>> np.is_busday(a)
    array([ True,  True,  True,  True,  True, False, False])

np.busday_count():
``````````````````
To find how many valid days there are in a specified range of datetime64
dates, use :func:`busday_count`:

.. admonition:: Example

    >>> np.busday_count(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
    5
    >>> np.busday_count(np.datetime64('2011-07-18'), np.datetime64('2011-07-11'))
    -5

If you have an array of datetime64 day values, and you want a count of
how many of them are valid dates, you can do this:

.. admonition:: Example

    >>> a = np.arange(np.datetime64('2011-07-11'), np.datetime64('2011-07-18'))
    >>> np.count_nonzero(np.is_busday(a))
    5



Custom Weekmasks
----------------

Here are several examples of custom weekmask values.  These examples
specify the "busday" default of Monday through Friday being valid days.

Some examples::

    # Positional sequences; positions are Monday through Sunday.
    # Length of the sequence must be exactly 7.
    weekmask = [1, 1, 1, 1, 1, 0, 0]
    # list or other sequence; 0 == invalid day, 1 == valid day
    weekmask = "1111100"
    # string '0' == invalid day, '1' == valid day

    # string abbreviations from this list: Mon Tue Wed Thu Fri Sat Sun
    weekmask = "Mon Tue Wed Thu Fri"
    # any amount of whitespace is allowed; abbreviations are case-sensitive.
    weekmask = "MonTue Wed  Thu\tFri"
Floating point error handling
*****************************

.. currentmodule:: numpy

Setting and getting error handling
----------------------------------

.. autosummary::
   :toctree: generated/

   seterr
   geterr
   seterrcall
   geterrcall
   errstate

Internal functions
------------------

.. autosummary::
   :toctree: generated/

   seterrobj
   geterrobj
Padding Arrays
==============

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   pad
.. versionadded:: 1.4.0

.. automodule:: numpy.polynomial.polynomial
   :no-members:
   :no-inherited-members:
   :no-special-members:
.. _routines.indexing:
.. _arrays.indexing:

Indexing routines
=================

.. seealso:: :ref:`basics.indexing`

.. currentmodule:: numpy

Generating index arrays
-----------------------
.. autosummary::
   :toctree: generated/

   c_
   r_
   s_
   nonzero
   where
   indices
   ix_
   ogrid
   ravel_multi_index
   unravel_index
   diag_indices
   diag_indices_from
   mask_indices
   tril_indices
   tril_indices_from
   triu_indices
   triu_indices_from

Indexing-like operations
------------------------
.. autosummary::
   :toctree: generated/

   take
   take_along_axis
   choose
   compress
   diag
   diagonal
   select
   lib.stride_tricks.sliding_window_view
   lib.stride_tricks.as_strided

Inserting data into arrays
--------------------------
.. autosummary::
   :toctree: generated/

   place
   put
   put_along_axis
   putmask
   fill_diagonal

Iterating over arrays
---------------------
.. autosummary::
   :toctree: generated/

   nditer
   ndenumerate
   ndindex
   nested_iters
   flatiter
   lib.Arrayterator
   iterable
Binary operations
*****************

.. currentmodule:: numpy

Elementwise bit operations
--------------------------
.. autosummary::
   :toctree: generated/

   bitwise_and
   bitwise_or
   bitwise_xor
   invert
   left_shift
   right_shift

Bit packing
-----------
.. autosummary::
   :toctree: generated/

   packbits
   unpackbits

Output formatting
-----------------
.. autosummary::
   :toctree: generated/

   binary_repr
:orphan:

*************************
NumPy C Code Explanations
*************************

.. This document has been moved to ../dev/internals.code-explanations.rst.

This document has been moved to :ref:`c-code-explanations`... _arrays.scalars:

*******
Scalars
*******

.. currentmodule:: numpy

Python defines only one type of a particular data class (there is only
one integer type, one floating-point type, etc.). This can be
convenient in applications that don't need to be concerned with all
the ways data can be represented in a computer.  For scientific
computing, however, more control is often needed.

In NumPy, there are 24 new fundamental Python types to describe
different types of scalars. These type descriptors are mostly based on
the types available in the C language that CPython is written in, with
several additional types compatible with Python's types.

Array scalars have the same attributes and methods as :class:`ndarrays
<ndarray>`. [#]_ This allows one to treat items of an array partly on
the same footing as arrays, smoothing out rough edges that result when
mixing scalar and array operations.

Array scalars live in a hierarchy (see the Figure below) of data
types. They can be detected using the hierarchy: For example,
``isinstance(val, np.generic)`` will return :py:data:`True` if *val* is
an array scalar object. Alternatively, what kind of array scalar is
present can be determined using other members of the data type
hierarchy. Thus, for example ``isinstance(val, np.complexfloating)``
will return :py:data:`True` if *val* is a complex valued type, while
``isinstance(val, np.flexible)`` will return true if *val* is one
of the flexible itemsize array types (:class:`str_`,
:class:`bytes_`, :class:`void`).

.. figure:: figures/dtype-hierarchy.png

   **Figure:** Hierarchy of type objects representing the array data
   types. Not shown are the two integer types :class:`intp` and
   :class:`uintp` which just point to the integer type that holds a
   pointer for the platform. All the number types can be obtained
   using bit-width names as well.


.. TODO - use something like this instead of the diagram above, as it generates
   links to the classes and is a vector graphic. Unfortunately it looks worse
   and the html <map> element providing the linked regions is misaligned.

   .. inheritance-diagram:: byte short intc int_ longlong ubyte ushort uintc uint ulonglong half single double longdouble csingle cdouble clongdouble bool_ datetime64 timedelta64 object_ bytes_ str_ void

.. [#] However, array scalars are immutable, so none of the array
       scalar attributes are settable.

.. _arrays.scalars.character-codes:

.. _arrays.scalars.built-in:

Built-in scalar types
=====================

The built-in scalar types are shown below. The C-like names are associated with character codes,
which are shown in their descriptions. Use of the character codes, however,
is discouraged.

Some of the scalar types are essentially equivalent to fundamental
Python types and therefore inherit from them as well as from the
generic array scalar type:

====================  ===========================  =============
Array scalar type     Related Python type          Inherits?
====================  ===========================  =============
:class:`int_`         :class:`int`                 Python 2 only
:class:`float_`       :class:`float`               yes
:class:`complex_`     :class:`complex`             yes
:class:`bytes_`       :class:`bytes`               yes
:class:`str_`         :class:`str`                 yes
:class:`bool_`        :class:`bool`                no
:class:`datetime64`   :class:`datetime.datetime`   no
:class:`timedelta64`  :class:`datetime.timedelta`  no
====================  ===========================  =============

The :class:`bool_` data type is very similar to the Python
:class:`bool` but does not inherit from it because Python's
:class:`bool` does not allow itself to be inherited from, and
on the C-level the size of the actual bool data is not the same as a
Python Boolean scalar.

.. warning::

   The :class:`int_` type does **not** inherit from the
   :class:`int` built-in under Python 3, because type :class:`int` is no
   longer a fixed-width integer type.

.. tip:: The default data type in NumPy is :class:`float_`.

.. autoclass:: numpy.generic
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.number
   :members: __init__
   :exclude-members: __init__

Integer types
~~~~~~~~~~~~~

.. autoclass:: numpy.integer
   :members: __init__
   :exclude-members: __init__

.. note::

   The numpy integer types mirror the behavior of C integers, and can therefore
   be subject to :ref:`overflow-errors`.

Signed integer types
++++++++++++++++++++

.. autoclass:: numpy.signedinteger
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.byte
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.short
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.intc
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.int_
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.longlong
   :members: __init__
   :exclude-members: __init__

Unsigned integer types
++++++++++++++++++++++

.. autoclass:: numpy.unsignedinteger
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.ubyte
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.ushort
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.uintc
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.uint
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.ulonglong
   :members: __init__
   :exclude-members: __init__

Inexact types
~~~~~~~~~~~~~

.. autoclass:: numpy.inexact
   :members: __init__
   :exclude-members: __init__

.. note::

   Inexact scalars are printed using the fewest decimal digits needed to
   distinguish their value from other values of the same datatype,
   by judicious rounding. See the ``unique`` parameter of
   `format_float_positional` and `format_float_scientific`.

   This means that variables with equal binary values but whose datatypes are of
   different precisions may display differently::

       >>> f16 = np.float16("0.1")
       >>> f32 = np.float32(f16)
       >>> f64 = np.float64(f32)
       >>> f16 == f32 == f64
       True
       >>> f16, f32, f64
       (0.1, 0.099975586, 0.0999755859375)

   Note that none of these floats hold the exact value :math:`\frac{1}{10}`;
   ``f16`` prints as ``0.1`` because it is as close to that value as possible,
   whereas the other types do not as they have more precision and therefore have
   closer values.

   Conversely, floating-point scalars of different precisions which approximate
   the same decimal value may compare unequal despite printing identically:

       >>> f16 = np.float16("0.1")
       >>> f32 = np.float32("0.1")
       >>> f64 = np.float64("0.1")
       >>> f16 == f32 == f64
       False
       >>> f16, f32, f64
       (0.1, 0.1, 0.1)

Floating-point types
++++++++++++++++++++

.. autoclass:: numpy.floating
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.half
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.single
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.double
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.longdouble
   :members: __init__
   :exclude-members: __init__

Complex floating-point types
++++++++++++++++++++++++++++

.. autoclass:: numpy.complexfloating
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.csingle
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.cdouble
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.clongdouble
   :members: __init__
   :exclude-members: __init__

Other types
~~~~~~~~~~~

.. autoclass:: numpy.bool_
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.datetime64
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.timedelta64
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.object_
   :members: __init__
   :exclude-members: __init__

.. note::

   The data actually stored in object arrays
   (*i.e.*, arrays having dtype :class:`object_`) are references to
   Python objects, not the objects themselves. Hence, object arrays
   behave more like usual Python :class:`lists <list>`, in the sense
   that their contents need not be of the same Python type.

   The object type is also special because an array containing
   :class:`object_` items does not return an :class:`object_` object
   on item access, but instead returns the actual object that
   the array item refers to.

.. index:: flexible

The following data types are **flexible**: they have no predefined
size and the data they describe can be of different length in different
arrays. (In the character codes ``#`` is an integer denoting how many
elements the data type consists of.)

.. autoclass:: numpy.flexible
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.bytes_
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.str_
   :members: __init__
   :exclude-members: __init__

.. autoclass:: numpy.void
   :members: __init__
   :exclude-members: __init__


.. warning::

   See :ref:`Note on string types<string-dtype-note>`.

   Numeric Compatibility: If you used old typecode characters in your
   Numeric code (which was never recommended), you will need to change
   some of them to the new characters. In particular, the needed
   changes are ``c -> S1``, ``b -> B``, ``1 -> b``, ``s -> h``, ``w ->
   H``, and ``u -> I``. These changes make the type character
   convention more consistent with other Python modules such as the
   :mod:`struct` module.

.. _sized-aliases:

Sized aliases
~~~~~~~~~~~~~

Along with their (mostly)
C-derived names, the integer, float, and complex data-types are also
available using a bit-width convention so that an array of the right
size can always be ensured. Two aliases (:class:`numpy.intp` and :class:`numpy.uintp`)
pointing to the integer type that is sufficiently large to hold a C pointer
are also provided.

.. note that these are documented with ..attribute because that is what
   autoclass does for aliases under the hood.

.. autoclass:: numpy.bool8

.. attribute:: int8
               int16
               int32
               int64

   Aliases for the signed integer types (one of `numpy.byte`, `numpy.short`,
   `numpy.intc`, `numpy.int_` and `numpy.longlong`) with the specified number
   of bits.

   Compatible with the C99 ``int8_t``, ``int16_t``, ``int32_t``, and
   ``int64_t``, respectively.

.. attribute:: uint8
               uint16
               uint32
               uint64

   Alias for the unsigned integer types (one of `numpy.ubyte`, `numpy.ushort`,
   `numpy.uintc`, `numpy.uint` and `numpy.ulonglong`) with the specified number
   of bits.

   Compatible with the C99 ``uint8_t``, ``uint16_t``, ``uint32_t``, and
   ``uint64_t``, respectively.

.. attribute:: intp

   Alias for the signed integer type (one of `numpy.byte`, `numpy.short`,
   `numpy.intc`, `numpy.int_` and `np.longlong`) that is the same size as a
   pointer.

   Compatible with the C ``intptr_t``.

   :Character code: ``'p'``

.. attribute:: uintp

   Alias for the unsigned integer type (one of `numpy.ubyte`, `numpy.ushort`,
   `numpy.uintc`, `numpy.uint` and `np.ulonglong`) that is the same size as a
   pointer.

   Compatible with the C ``uintptr_t``.

   :Character code: ``'P'``

.. autoclass:: numpy.float16

.. autoclass:: numpy.float32

.. autoclass:: numpy.float64

.. attribute:: float96
               float128

   Alias for `numpy.longdouble`, named after its size in bits.
   The existence of these aliases depends on the platform.

.. autoclass:: numpy.complex64

.. autoclass:: numpy.complex128

.. attribute:: complex192
               complex256

   Alias for `numpy.clongdouble`, named after its size in bits.
   The existence of these aliases depends on the platform.

Other aliases
~~~~~~~~~~~~~

The first two of these are conveniences which resemble the names of the
builtin types, in the same style as `bool_`, `int_`, `str_`, `bytes_`, and
`object_`:

.. autoclass:: numpy.float_

.. autoclass:: numpy.complex_

Some more use alternate naming conventions for extended-precision floats and
complex numbers:

.. autoclass:: numpy.longfloat

.. autoclass:: numpy.singlecomplex

.. autoclass:: numpy.cfloat

.. autoclass:: numpy.longcomplex

.. autoclass:: numpy.clongfloat

The following aliases originate from Python 2, and it is recommended that they
not be used in new code.

.. autoclass:: numpy.string_

.. autoclass:: numpy.unicode_

Attributes
==========

The array scalar objects have an :obj:`array priority
<class.__array_priority__>` of :c:data:`NPY_SCALAR_PRIORITY`
(-1,000,000.0). They also do not (yet) have a :attr:`ctypes <ndarray.ctypes>`
attribute. Otherwise, they share the same attributes as arrays:

.. autosummary::
   :toctree: generated/

   generic.flags
   generic.shape
   generic.strides
   generic.ndim
   generic.data
   generic.size
   generic.itemsize
   generic.base
   generic.dtype
   generic.real
   generic.imag
   generic.flat
   generic.T
   generic.__array_interface__
   generic.__array_struct__
   generic.__array_priority__
   generic.__array_wrap__


Indexing
========
.. seealso:: :ref:`arrays.indexing`, :ref:`arrays.dtypes`

Array scalars can be indexed like 0-dimensional arrays: if *x* is an
array scalar,

- ``x[()]`` returns a copy of array scalar
- ``x[...]`` returns a 0-dimensional :class:`ndarray`
- ``x['field-name']`` returns the array scalar in the field *field-name*.
  (*x* can have fields, for example, when it corresponds to a structured data type.)

Methods
=======

Array scalars have exactly the same methods as arrays. The default
behavior of these methods is to internally convert the scalar to an
equivalent 0-dimensional array and to call the corresponding array
method. In addition, math operations on array scalars are defined so
that the same hardware flags are set and used to interpret the results
as for :ref:`ufunc <ufuncs>`, so that the error state used for ufuncs
also carries over to the math on array scalars.

The exceptions to the above rules are given below:

.. autosummary::
   :toctree: generated/

   generic.__array__
   generic.__array_wrap__
   generic.squeeze
   generic.byteswap
   generic.__reduce__
   generic.__setstate__
   generic.setflags

Utility method for typing:

.. autosummary::
   :toctree: generated/

   number.__class_getitem__


Defining new types
==================

There are two ways to effectively define a new array scalar type
(apart from composing structured types :ref:`dtypes <arrays.dtypes>` from
the built-in scalar types): One way is to simply subclass the
:class:`ndarray` and overwrite the methods of interest. This will work to
a degree, but internally certain behaviors are fixed by the data type of
the array.  To fully customize the data type of an array you need to
define a new data-type, and register it with NumPy. Such new types can only
be defined in C, using the :ref:`NumPy C-API <c-api>`.
Functional programming
**********************

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   apply_along_axis
   apply_over_axes
   vectorize
   frompyfunc
   piecewise
Putting the Inner Loop in Cython
================================

Those who want really good performance out of their low level operations
should strongly consider directly using the iteration API provided
in C, but for those who are not comfortable with C or C++, Cython
is a good middle ground with reasonable performance tradeoffs. For
the :class:`~numpy.nditer` object, this means letting the iterator take care
of broadcasting, dtype conversion, and buffering, while giving the inner
loop to Cython.

For our example, we'll create a sum of squares function. To start,
let's implement this function in straightforward Python. We want to
support an 'axis' parameter similar to the numpy :func:`sum` function,
so we will need to construct a list for the `op_axes` parameter.
Here's how this looks.

.. admonition:: Example

    >>> def axis_to_axeslist(axis, ndim):
    ...     if axis is None:
    ...         return [-1] * ndim
    ...     else:
    ...         if type(axis) is not tuple:
    ...             axis = (axis,)
    ...         axeslist = [1] * ndim
    ...         for i in axis:
    ...             axeslist[i] = -1
    ...         ax = 0
    ...         for i in range(ndim):
    ...             if axeslist[i] != -1:
    ...                 axeslist[i] = ax
    ...                 ax += 1
    ...         return axeslist
    ...
    >>> def sum_squares_py(arr, axis=None, out=None):
    ...     axeslist = axis_to_axeslist(axis, arr.ndim)
    ...     it = np.nditer([arr, out], flags=['reduce_ok',
    ...                                       'buffered', 'delay_bufalloc'],
    ...                 op_flags=[['readonly'], ['readwrite', 'allocate']],
    ...                 op_axes=[None, axeslist],
    ...                 op_dtypes=['float64', 'float64'])
    ...     with it:
    ...         it.operands[1][...] = 0
    ...         it.reset()
    ...         for x, y in it:
    ...             y[...] += x*x
    ...         return it.operands[1]
    ...
    >>> a = np.arange(6).reshape(2,3)
    >>> sum_squares_py(a)
    array(55.)
    >>> sum_squares_py(a, axis=-1)
    array([  5.,  50.])

To Cython-ize this function, we replace the inner loop (y[...] += x*x) with
Cython code that's specialized for the float64 dtype. With the
'external_loop' flag enabled, the arrays provided to the inner loop will
always be one-dimensional, so very little checking needs to be done.

Here's the listing of sum_squares.pyx::

    import numpy as np
    cimport numpy as np
    cimport cython

    def axis_to_axeslist(axis, ndim):
        if axis is None:
            return [-1] * ndim
        else:
            if type(axis) is not tuple:
                axis = (axis,)
            axeslist = [1] * ndim
            for i in axis:
                axeslist[i] = -1
            ax = 0
            for i in range(ndim):
                if axeslist[i] != -1:
                    axeslist[i] = ax
                    ax += 1
            return axeslist

    @cython.boundscheck(False)
    def sum_squares_cy(arr, axis=None, out=None):
        cdef np.ndarray[double] x
        cdef np.ndarray[double] y
        cdef int size
        cdef double value

        axeslist = axis_to_axeslist(axis, arr.ndim)
        it = np.nditer([arr, out], flags=['reduce_ok', 'external_loop',
                                          'buffered', 'delay_bufalloc'],
                    op_flags=[['readonly'], ['readwrite', 'allocate']],
                    op_axes=[None, axeslist],
                    op_dtypes=['float64', 'float64'])
        with it:
            it.operands[1][...] = 0
            it.reset()
            for xarr, yarr in it:
                x = xarr
                y = yarr
                size = x.shape[0]
                for i in range(size):
                   value = x[i]
                   y[i] = y[i] + value * value
            return it.operands[1]

On this machine, building the .pyx file into a module looked like the
following, but you may have to find some Cython tutorials to tell you
the specifics for your system configuration.::

    $ cython sum_squares.pyx
    $ gcc -shared -pthread -fPIC -fwrapv -O2 -Wall -I/usr/include/python2.7 -fno-strict-aliasing -o sum_squares.so sum_squares.c

Running this from the Python interpreter produces the same answers
as our native Python/NumPy code did.

.. admonition:: Example

    >>> from sum_squares import sum_squares_cy #doctest: +SKIP
    >>> a = np.arange(6).reshape(2,3)
    >>> sum_squares_cy(a) #doctest: +SKIP
    array(55.0)
    >>> sum_squares_cy(a, axis=-1) #doctest: +SKIP
    array([  5.,  50.])

Doing a little timing in IPython shows that the reduced overhead and
memory allocation of the Cython inner loop is providing a very nice
speedup over both the straightforward Python code and an expression
using NumPy's built-in sum function.::

    >>> a = np.random.rand(1000,1000)

    >>> timeit sum_squares_py(a, axis=-1)
    10 loops, best of 3: 37.1 ms per loop

    >>> timeit np.sum(a*a, axis=-1)
    10 loops, best of 3: 20.9 ms per loop

    >>> timeit sum_squares_cy(a, axis=-1)
    100 loops, best of 3: 11.8 ms per loop

    >>> np.all(sum_squares_cy(a, axis=-1) == np.sum(a*a, axis=-1))
    True

    >>> np.all(sum_squares_py(a, axis=-1) == np.sum(a*a, axis=-1))
    True
**********************************
Packaging (:mod:`numpy.distutils`)
**********************************

.. module:: numpy.distutils

NumPy provides enhanced distutils functionality to make it easier to
build and install sub-packages, auto-generate code, and extension
modules that use Fortran-compiled libraries. To use features of NumPy
distutils, use the :func:`setup <core.setup>` command from
:mod:`numpy.distutils.core`. A useful :class:`Configuration
<misc_util.Configuration>` class is also provided in
:mod:`numpy.distutils.misc_util` that can make it easier to construct
keyword arguments to pass to the setup function (by passing the
dictionary obtained from the todict() method of the class). More
information is available in the :ref:`distutils-user-guide`.

The choice and location of linked libraries such as BLAS and LAPACK as well as
include paths and other such build options can be specified in a ``site.cfg``
file located in the NumPy root repository or a ``.numpy-site.cfg`` file in your
home directory. See the ``site.cfg.example`` example file included in the NumPy
repository or sdist for documentation.

.. index::
   single: distutils


Modules in :mod:`numpy.distutils`
=================================
.. toctree::
   :maxdepth: 2

   distutils/misc_util


.. currentmodule:: numpy.distutils

.. autosummary::
   :toctree: generated/

   ccompiler
   ccompiler_opt
   cpuinfo.cpu
   core.Extension
   exec_command
   log.set_verbosity
   system_info.get_info
   system_info.get_standard_file


Configuration class
===================

.. currentmodule:: numpy.distutils.misc_util

.. class:: Configuration(package_name=None, parent_name=None, top_path=None, package_path=None, **attrs)

    Construct a configuration instance for the given package name. If
    *parent_name* is not None, then construct the package as a
    sub-package of the *parent_name* package. If *top_path* and
    *package_path* are None then they are assumed equal to
    the path of the file this instance was created in. The setup.py
    files in the numpy distribution are good examples of how to use
    the :class:`Configuration` instance.

    .. automethod:: todict

    .. automethod:: get_distribution

    .. automethod:: get_subpackage

    .. automethod:: add_subpackage

    .. automethod:: add_data_files

    .. automethod:: add_data_dir

    .. automethod:: add_include_dirs

    .. automethod:: add_headers

    .. automethod:: add_extension

    .. automethod:: add_library

    .. automethod:: add_scripts

    .. automethod:: add_installed_library

    .. automethod:: add_npy_pkg_config

    .. automethod:: paths

    .. automethod:: get_config_cmd

    .. automethod:: get_build_temp_dir

    .. automethod:: have_f77c

    .. automethod:: have_f90c

    .. automethod:: get_version

    .. automethod:: make_svn_version_py

    .. automethod:: make_config_py

    .. automethod:: get_info

Building Installable C libraries
================================

Conventional C libraries (installed through `add_library`) are not installed, and
are just used during the build (they are statically linked).  An installable C
library is a pure C library, which does not depend on the python C runtime, and
is installed such that it may be used by third-party packages. To build and
install the C library, you just use the method `add_installed_library` instead of
`add_library`, which takes the same arguments except for an additional
``install_dir`` argument::

  .. hidden in a comment so as to be included in refguide but not rendered documentation
    >>> import numpy.distutils.misc_util
    >>> config = np.distutils.misc_util.Configuration(None, '', '.')
    >>> with open('foo.c', 'w') as f: pass

  >>> config.add_installed_library('foo', sources=['foo.c'], install_dir='lib')

npy-pkg-config files
--------------------

To make the necessary build options available to third parties, you could use
the `npy-pkg-config` mechanism implemented in `numpy.distutils`. This mechanism is
based on a .ini file which contains all the options. A .ini file is very
similar to .pc files as used by the pkg-config unix utility::

  [meta]
  Name: foo
  Version: 1.0
  Description: foo library

  [variables]
  prefix = /home/user/local
  libdir = ${prefix}/lib
  includedir = ${prefix}/include

  [default]
  cflags = -I${includedir}
  libs = -L${libdir} -lfoo

Generally, the file needs to be generated during the build, since it needs some
information known at build time only (e.g. prefix). This is mostly automatic if
one uses the `Configuration` method `add_npy_pkg_config`. Assuming we have a
template file foo.ini.in as follows::

  [meta]
  Name: foo
  Version: @version@
  Description: foo library

  [variables]
  prefix = @prefix@
  libdir = ${prefix}/lib
  includedir = ${prefix}/include

  [default]
  cflags = -I${includedir}
  libs = -L${libdir} -lfoo

and the following code in setup.py::

  >>> config.add_installed_library('foo', sources=['foo.c'], install_dir='lib')
  >>> subst = {'version': '1.0'}
  >>> config.add_npy_pkg_config('foo.ini.in', 'lib', subst_dict=subst)

This will install the file foo.ini into the directory package_dir/lib, and the
foo.ini file will be generated from foo.ini.in, where each ``@version@`` will be
replaced by ``subst_dict['version']``. The dictionary has an additional prefix
substitution rule automatically added, which contains the install prefix (since
this is not easy to get from setup.py).  npy-pkg-config files can also be
installed at the same location as used for numpy, using the path returned from
`get_npy_pkg_dir` function.

Reusing a C library from another package
----------------------------------------

Info are easily retrieved from the `get_info` function in
`numpy.distutils.misc_util`::

  >>> info = np.distutils.misc_util.get_info('npymath')
  >>> config.add_extension('foo', sources=['foo.c'], extra_info=info)
  <numpy.distutils.extension.Extension('foo') at 0x...>


An additional list of paths to look for .ini files can be given to `get_info`.

Conversion of ``.src`` files
============================

NumPy distutils supports automatic conversion of source files named
<somefile>.src. This facility can be used to maintain very similar
code blocks requiring only simple changes between blocks. During the
build phase of setup, if a template file named <somefile>.src is
encountered, a new file named <somefile> is constructed from the
template and placed in the build directory to be used instead. Two
forms of template conversion are supported. The first form occurs for
files named <file>.ext.src where ext is a recognized Fortran
extension (f, f90, f95, f77, for, ftn, pyf). The second form is used
for all other cases. See :ref:`templating`.
.. _maskedarray:

*************
Masked arrays
*************

Masked arrays are arrays that may have missing or invalid entries.
The :mod:`numpy.ma` module provides a nearly work-alike replacement for numpy
that supports data arrays with masks.

.. index::
   single: masked arrays

.. toctree::
   :maxdepth: 2

   maskedarray.generic
   maskedarray.baseclass
   routines.ma
.. _routines.polynomial:

Polynomials
***********

Polynomials in NumPy can be *created*, *manipulated*, and even *fitted* using
the :doc:`convenience classes <routines.polynomials.classes>`
of the `numpy.polynomial` package, introduced in NumPy 1.4.

Prior to NumPy 1.4, `numpy.poly1d` was the class of choice and it is still
available in order to maintain backward compatibility.
However, the newer `polynomial package <numpy.polynomial>` is more complete
and its `convenience classes <routines.polynomials.classes>` provide a
more consistent, better-behaved interface for working with polynomial
expressions.
Therefore :mod:`numpy.polynomial` is recommended for new coding.

.. note:: **Terminology**

   The term *polynomial module* refers to the old API defined in
   `numpy.lib.polynomial`, which includes the :class:`numpy.poly1d` class and
   the polynomial functions prefixed with *poly* accessible from the `numpy`
   namespace (e.g. `numpy.polyadd`, `numpy.polyval`, `numpy.polyfit`, etc.).

   The term *polynomial package* refers to the new API defined in 
   `numpy.polynomial`, which includes the convenience classes for the
   different kinds of polynomials (`numpy.polynomial.Polynomial`,
   `numpy.polynomial.Chebyshev`, etc.).

Transitioning from `numpy.poly1d` to `numpy.polynomial`
-------------------------------------------------------

As noted above, the :class:`poly1d class <numpy.poly1d>` and associated
functions defined in ``numpy.lib.polynomial``, such as `numpy.polyfit`
and `numpy.poly`, are considered legacy and should **not** be used in new
code.
Since NumPy version 1.4, the `numpy.polynomial` package is preferred for
working with polynomials.

Quick Reference
~~~~~~~~~~~~~~~

The following table highlights some of the main differences between the
legacy polynomial module and the polynomial package for common tasks.
The `~numpy.polynomial.polynomial.Polynomial` class is imported for brevity::

    from numpy.polynomial import Polynomial


+------------------------+------------------------------+---------------------------------------+
|  **How to...**         | Legacy (`numpy.poly1d`)      | `numpy.polynomial`                    |
+------------------------+------------------------------+---------------------------------------+
| Create a               | ``p = np.poly1d([1, 2, 3])`` | ``p = Polynomial([3, 2, 1])``         |
| polynomial object      |                              |                                       |
| from coefficients [1]_ |                              |                                       |
+------------------------+------------------------------+---------------------------------------+
| Create a polynomial    | ``r = np.poly([-1, 1])``     | ``p = Polynomial.fromroots([-1, 1])`` |
| object from roots      | ``p = np.poly1d(r)``         |                                       |
+------------------------+------------------------------+---------------------------------------+
| Fit a polynomial of    |                              |                                       |
| degree ``deg`` to data | ``np.polyfit(x, y, deg)``    | ``Polynomial.fit(x, y, deg)``         |
+------------------------+------------------------------+---------------------------------------+


.. [1] Note the reversed ordering of the coefficients

Transition Guide
~~~~~~~~~~~~~~~~

There are significant differences between ``numpy.lib.polynomial`` and
`numpy.polynomial`.
The most significant difference is the ordering of the coefficients for the
polynomial expressions.
The  various routines in `numpy.polynomial` all
deal with series whose coefficients go from degree zero upward,
which is the *reverse order* of the poly1d convention.
The easy way to remember this is that indices
correspond to degree, i.e., ``coef[i]`` is the coefficient of the term of
degree *i*.

Though the difference in convention may be confusing, it is straightforward to
convert from the legacy polynomial API to the new.
For example, the following demonstrates how you would convert a `numpy.poly1d`
instance representing the expression :math:`x^{2} + 2x + 3` to a
`~numpy.polynomial.polynomial.Polynomial` instance representing the same
expression::

    >>> p1d = np.poly1d([1, 2, 3])
    >>> p = np.polynomial.Polynomial(p1d.coef[::-1])

In addition to the ``coef`` attribute, polynomials from the polynomial
package also have ``domain`` and ``window`` attributes.
These attributes are most relevant when fitting
polynomials to data, though it should be noted that polynomials with
different ``domain`` and ``window`` attributes are not considered equal, and
can't be mixed in arithmetic::

    >>> p1 = np.polynomial.Polynomial([1, 2, 3])
    >>> p1
    Polynomial([1., 2., 3.], domain=[-1,  1], window=[-1,  1])
    >>> p2 = np.polynomial.Polynomial([1, 2, 3], domain=[-2, 2])
    >>> p1 == p2
    False
    >>> p1 + p2
    Traceback (most recent call last):
        ...
    TypeError: Domains differ

See the documentation for the
`convenience classes <routines.polynomials.classes>`_ for further details on
the ``domain`` and ``window`` attributes.

Another major difference between the legacy polynomial module and the
polynomial package is polynomial fitting. In the old module, fitting was
done via the `~numpy.polyfit` function. In the polynomial package, the
`~numpy.polynomial.polynomial.Polynomial.fit` class method is preferred. For
example, consider a simple linear fit to the following data:

.. ipython:: python

    rng = np.random.default_rng()
    x = np.arange(10)
    y = np.arange(10) + rng.standard_normal(10)

With the legacy polynomial module, a linear fit (i.e. polynomial of degree 1)
could be applied to these data with `~numpy.polyfit`:

.. ipython:: python

    np.polyfit(x, y, deg=1)

With the new polynomial API, the `~numpy.polynomial.polynomial.Polynomial.fit`
class method is preferred:

.. ipython:: python

    p_fitted = np.polynomial.Polynomial.fit(x, y, deg=1)
    p_fitted

Note that the coefficients are given *in the scaled domain* defined by the
linear mapping between the ``window`` and ``domain``.
`~numpy.polynomial.polynomial.Polynomial.convert` can be used to get the
coefficients in the unscaled data domain.

.. ipython:: python

    p_fitted.convert()

Documentation for the `~numpy.polynomial` Package
-------------------------------------------------

In addition to standard power series polynomials, the polynomial package
provides several additional kinds of polynomials including Chebyshev,
Hermite (two subtypes), Laguerre, and Legendre polynomials.
Each of these has an associated
`convenience class <routines.polynomials.classes>` available from the
`numpy.polynomial` namespace that provides a consistent interface for working
with polynomials regardless of their type.

.. toctree::
   :maxdepth: 1

   routines.polynomials.classes

Documentation pertaining to specific functions defined for each kind of
polynomial individually can be found in the corresponding module documentation:

.. toctree::
   :maxdepth: 1

   routines.polynomials.polynomial
   routines.polynomials.chebyshev
   routines.polynomials.hermite
   routines.polynomials.hermite_e
   routines.polynomials.laguerre
   routines.polynomials.legendre
   routines.polynomials.polyutils


Documentation for Legacy Polynomials
------------------------------------

.. toctree::
   :maxdepth: 2

   routines.polynomials.poly1d
.. _routines:

********
Routines
********

In this chapter routine docstrings are presented, grouped by functionality.
Many docstrings contain example code, which demonstrates basic usage
of the routine. The examples assume that NumPy is imported with::

  >>> import numpy as np

A convenient way to execute examples is the ``%doctest_mode`` mode of
IPython, which allows for pasting of multi-line examples and preserves
indentation.

.. toctree::
   :maxdepth: 2

   routines.array-creation
   routines.array-manipulation
   routines.bitwise
   routines.char
   routines.ctypeslib
   routines.datetime
   routines.dtype
   routines.dual
   routines.emath
   routines.err
   routines.fft
   routines.functional
   routines.help
   routines.io
   routines.linalg
   routines.logic
   routines.ma
   routines.math
   routines.matlib
   routines.other
   routines.padding
   routines.polynomials
   random/index
   routines.set
   routines.sort
   routines.statistics
   routines.testing
   routines.window
Poly1d
======

.. currentmodule:: numpy

Basics
------
.. autosummary::
   :toctree: generated/

   poly1d
   polyval
   poly
   roots

Fitting
-------
.. autosummary::
   :toctree: generated/

   polyfit

Calculus
--------
.. autosummary::
   :toctree: generated/

   polyder
   polyint

Arithmetic
----------
.. autosummary::
   :toctree: generated/

   polyadd
   polydiv
   polymul
   polysub

Warnings
--------
.. autosummary::
   :toctree: generated/

   RankWarning
Miscellaneous routines
**********************

.. toctree::

.. currentmodule:: numpy

Performance tuning
------------------
.. autosummary::
   :toctree: generated/

   setbufsize
   getbufsize

Memory ranges
-------------

.. autosummary::
   :toctree: generated/

   shares_memory
   may_share_memory
   byte_bounds

Array mixins
------------
.. autosummary::
   :toctree: generated/

   lib.mixins.NDArrayOperatorsMixin

NumPy version comparison
------------------------
.. autosummary::
   :toctree: generated/

   lib.NumpyVersion

Utility
-------

.. autosummary::
   :toctree: generated/

   get_include
   show_config
   deprecate
   deprecate_with_doc
   broadcast_shapes

Matlab-like Functions
---------------------
.. autosummary::
   :toctree: generated/

   who
   disp

Exceptions
----------
.. autosummary::
   :toctree: generated/

   AxisError
Testing the numpy.i Typemaps
============================

Introduction
------------

Writing tests for the ``numpy.i`` `SWIG <http://www.swig.org>`_
interface file is a combinatorial headache.  At present, 12 different
data types are supported, each with 74 different argument signatures,
for a total of 888 typemaps supported "out of the box".  Each of these
typemaps, in turn, might require several unit tests in order to verify
expected behavior for both proper and improper inputs.  Currently,
this results in more than 1,000 individual unit tests executed when
``make test`` is run in the ``numpy/tools/swig`` subdirectory.

To facilitate this many similar unit tests, some high-level
programming techniques are employed, including C and `SWIG`_ macros,
as well as Python inheritance.  The purpose of this document is to describe
the testing infrastructure employed to verify that the ``numpy.i``
typemaps are working as expected.

Testing Organization
--------------------

There are three independent testing frameworks supported, for one-,
two-, and three-dimensional arrays respectively.  For one-dimensional
arrays, there are two C++ files, a header and a source, named::

    Vector.h
    Vector.cxx

that contain prototypes and code for a variety of functions that have
one-dimensional arrays as function arguments.  The file::

    Vector.i

is a `SWIG`_ interface file that defines a python module ``Vector``
that wraps the functions in ``Vector.h`` while utilizing the typemaps
in ``numpy.i`` to correctly handle the C arrays.

The ``Makefile`` calls ``swig`` to generate ``Vector.py`` and
``Vector_wrap.cxx``, and also executes the ``setup.py`` script that
compiles ``Vector_wrap.cxx`` and links together the extension module
``_Vector.so`` or ``_Vector.dylib``, depending on the platform.  This
extension module and the proxy file ``Vector.py`` are both placed in a
subdirectory under the ``build`` directory.

The actual testing takes place with a Python script named::

    testVector.py

that uses the standard Python library module ``unittest``, which
performs several tests of each function defined in ``Vector.h`` for
each data type supported.

Two-dimensional arrays are tested in exactly the same manner.  The
above description applies, but with ``Matrix`` substituted for
``Vector``.  For three-dimensional tests, substitute ``Tensor`` for
``Vector``.  For four-dimensional tests, substitute ``SuperTensor``
for ``Vector``. For flat in-place array tests, substitute ``Flat``
for ``Vector``.
For the descriptions that follow, we will reference the
``Vector`` tests, but the same information applies to ``Matrix``,
``Tensor`` and ``SuperTensor`` tests.

The command ``make test`` will ensure that all of the test software is
built and then run all three test scripts.

Testing Header Files
--------------------

``Vector.h`` is a C++ header file that defines a C macro called
``TEST_FUNC_PROTOS`` that takes two arguments: ``TYPE``, which is a
data type name such as ``unsigned int``; and ``SNAME``, which is a
short name for the same data type with no spaces, e.g. ``uint``.  This
macro defines several function prototypes that have the prefix
``SNAME`` and have at least one argument that is an array of type
``TYPE``.  Those functions that have return arguments return a
``TYPE`` value.

``TEST_FUNC_PROTOS`` is then implemented for all of the data types
supported by ``numpy.i``:

  * ``signed char``
  * ``unsigned char``
  * ``short``
  * ``unsigned short``
  * ``int``
  * ``unsigned int``
  * ``long``
  * ``unsigned long``
  * ``long long``
  * ``unsigned long long``
  * ``float``
  * ``double``

Testing Source Files
--------------------

``Vector.cxx`` is a C++ source file that implements compilable code
for each of the function prototypes specified in ``Vector.h``.  It
defines a C macro ``TEST_FUNCS`` that has the same arguments and works
in the same way as ``TEST_FUNC_PROTOS`` does in ``Vector.h``.
``TEST_FUNCS`` is implemented for each of the 12 data types as above.

Testing SWIG Interface Files
----------------------------

``Vector.i`` is a `SWIG`_ interface file that defines python module
``Vector``.  It follows the conventions for using ``numpy.i`` as
described in this chapter.  It defines a `SWIG`_ macro
``%apply_numpy_typemaps`` that has a single argument ``TYPE``.
It uses the `SWIG`_ directive ``%apply`` to apply the provided
typemaps to the argument signatures found in ``Vector.h``.  This macro
is then implemented for all of the data types supported by
``numpy.i``.  It then does a ``%include "Vector.h"`` to wrap all of
the function prototypes in ``Vector.h`` using the typemaps in
``numpy.i``.

Testing Python Scripts
----------------------

After ``make`` is used to build the testing extension modules,
``testVector.py`` can be run to execute the tests.  As with other
scripts that use ``unittest`` to facilitate unit testing,
``testVector.py`` defines a class that inherits from
``unittest.TestCase``::

    class VectorTestCase(unittest.TestCase):

However, this class is not run directly.  Rather, it serves as a base
class to several other python classes, each one specific to a
particular data type.  The ``VectorTestCase`` class stores two strings
for typing information:

    **self.typeStr**
      A string that matches one of the ``SNAME`` prefixes used in
      ``Vector.h`` and ``Vector.cxx``.  For example, ``"double"``.

    **self.typeCode**
      A short (typically single-character) string that represents a
      data type in numpy and corresponds to ``self.typeStr``.  For
      example, if ``self.typeStr`` is ``"double"``, then
      ``self.typeCode`` should be ``"d"``.

Each test defined by the ``VectorTestCase`` class extracts the python
function it is trying to test by accessing the ``Vector`` module's
dictionary::

    length = Vector.__dict__[self.typeStr + "Length"]

In the case of double precision tests, this will return the python
function ``Vector.doubleLength``.

We then define a new test case class for each supported data type with
a short definition such as::

    class doubleTestCase(VectorTestCase):
        def __init__(self, methodName="runTest"):
            VectorTestCase.__init__(self, methodName)
            self.typeStr  = "double"
            self.typeCode = "d"

Each of these 12 classes is collected into a ``unittest.TestSuite``,
which is then executed.  Errors and failures are summed together and
returned as the exit argument.  Any non-zero result indicates that at
least one test did not pass.
Sorting, searching, and counting
================================

.. currentmodule:: numpy

Sorting
-------
.. autosummary::
   :toctree: generated/

   sort
   lexsort
   argsort
   ndarray.sort
   msort
   sort_complex
   partition
   argpartition

Searching
---------
.. autosummary::
   :toctree: generated/

   argmax
   nanargmax
   argmin
   nanargmin
   argwhere
   nonzero
   flatnonzero
   where
   searchsorted
   extract

Counting
--------
.. autosummary::
   :toctree: generated/

   count_nonzero
.. _global_state:

************
Global State
************

NumPy has a few import-time, compile-time, or runtime options
which change the global behaviour.
Most of these are related to performance or for debugging
purposes and will not be interesting to the vast majority
of users.


Performance-Related Options
===========================

Number of Threads used for Linear Algebra
-----------------------------------------

NumPy itself is normally intentionally limited to a single thread
during function calls, however it does support multiple Python
threads running at the same time.
Note that for performant linear algebra NumPy uses a BLAS backend
such as OpenBLAS or MKL, which may use multiple threads that may
be controlled by environment variables such as ``OMP_NUM_THREADS``
depending on what is used.
One way to control the number of threads is the package
`threadpoolctl <https://pypi.org/project/threadpoolctl/>`_


Madvise Hugepage on Linux
-------------------------

When working with very large arrays on modern Linux kernels,
you can experience a significant speedup when
`transparent hugepage <https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html>`_
is used.
The current system policy for transparent hugepages can be seen by::

    cat /sys/kernel/mm/transparent_hugepage/enabled

When set to ``madvise`` NumPy will typically use hugepages for a performance
boost. This behaviour can be modified by setting the environment variable::

    NUMPY_MADVISE_HUGEPAGE=0

or setting it to ``1`` to always enable it. When not set, the default
is to use madvise on Kernels 4.6 and newer. These kernels presumably
experience a large speedup with hugepage support.
This flag is checked at import time.


Interoperability-Related Options
================================

The array function protocol which allows array-like objects to
hook into the NumPy API is currently enabled by default.
This option exists since NumPy 1.16 and is enabled by default since
NumPy 1.17. It can be disabled using::

    NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0

See also :py:meth:`numpy.class.__array_function__` for more information.
This flag is checked at import time.


Debugging-Related Options
=========================

Relaxed Strides Checking
------------------------

The *compile-time* environment variables::

    NPY_RELAXED_STRIDES_DEBUG=0
    NPY_RELAXED_STRIDES_CHECKING=1

control how NumPy reports contiguity for arrays.
The default that it is enabled and the debug mode is disabled.
This setting should always be enabled. Setting the
debug option can be interesting for testing code written
in C which iterates through arrays that may or may not be
contiguous in memory.
Most users will have no reason to change these; for details
see the :ref:`memory layout <memory-layout>` documentation.


Warn if no memory allocation policy when deallocating data
----------------------------------------------------------

Some users might pass ownership of the data pointer to the ``ndarray`` by
setting the ``OWNDATA`` flag. If they do this without setting (manually) a
memory allocation policy, the default will be to call ``free``. If
``NUMPY_WARN_IF_NO_MEM_POLICY`` is set to ``"1"``, a ``RuntimeWarning`` will
be emitted. A better alternative is to use a ``PyCapsule`` with a deallocator
and set the ``ndarray.base``.
.. versionadded:: 1.4.0

.. automodule:: numpy.polynomial.chebyshev
   :no-members:
   :no-inherited-members:
   :no-special-members:
Set routines
============

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   lib.arraysetops

Making proper sets
------------------
.. autosummary::
   :toctree: generated/

   unique

Boolean operations
------------------
.. autosummary::
   :toctree: generated/

   in1d
   intersect1d
   isin
   setdiff1d
   setxor1d
   union1d
.. currentmodule:: numpy

.. _arrays.dtypes:

**********************************
Data type objects (:class:`dtype`)
**********************************

A data type object (an instance of :class:`numpy.dtype` class)
describes how the bytes in the fixed-size block of memory
corresponding to an array item should be interpreted. It describes the
following aspects of the data:

1. Type of the data (integer, float, Python object, etc.)
2. Size of the data (how many bytes is in *e.g.* the integer)
3. Byte order of the data (:term:`little-endian` or :term:`big-endian`)
4. If the data type is :term:`structured data type`, an aggregate of other
   data types, (*e.g.*, describing an array item consisting of
   an integer and a float),

   1. what are the names of the ":term:`fields <field>`" of the structure,
      by which they can be :ref:`accessed <arrays.indexing.fields>`,
   2. what is the data-type of each :term:`field`, and
   3. which part of the memory block each field takes.

5. If the data type is a sub-array, what is its shape and data type.

.. index::
   pair: dtype; scalar

To describe the type of scalar data, there are several :ref:`built-in
scalar types <arrays.scalars.built-in>` in NumPy for various precision
of integers, floating-point numbers, *etc*. An item extracted from an
array, *e.g.*, by indexing, will be a Python object whose type is the
scalar type associated with the data type of the array.

Note that the scalar types are not :class:`dtype` objects, even though
they can be used in place of one whenever a data type specification is
needed in NumPy.

.. index::
   pair: dtype; field

Structured data types are formed by creating a data type whose
:term:`field` contain other data types. Each field has a name by
which it can be :ref:`accessed <arrays.indexing.fields>`. The parent data
type should be of sufficient size to contain all its fields; the
parent is nearly always based on the :class:`void` type which allows
an arbitrary item size. Structured data types may also contain nested
structured sub-array data types in their fields.

.. index::
   pair: dtype; sub-array

Finally, a data type can describe items that are themselves arrays of
items of another data type. These sub-arrays must, however, be of a
fixed size.

If an array is created using a data-type describing a sub-array,
the dimensions of the sub-array are appended to the shape
of the array when the array is created. Sub-arrays in a field of a
structured type behave differently, see :ref:`arrays.indexing.fields`.

Sub-arrays always have a C-contiguous memory layout.

.. admonition:: Example

   A simple data type containing a 32-bit big-endian integer:
   (see :ref:`arrays.dtypes.constructing` for details on construction)

   >>> dt = np.dtype('>i4')
   >>> dt.byteorder
   '>'
   >>> dt.itemsize
   4
   >>> dt.name
   'int32'
   >>> dt.type is np.int32
   True

   The corresponding array scalar type is :class:`int32`.

.. admonition:: Example

   A structured data type containing a 16-character string (in field 'name')
   and a sub-array of two 64-bit floating-point number (in field 'grades'):

   >>> dt = np.dtype([('name', np.unicode_, 16), ('grades', np.float64, (2,))])
   >>> dt['name']
   dtype('<U16')
   >>> dt['grades']
   dtype(('<f8', (2,)))

   Items of an array of this data type are wrapped in an :ref:`array
   scalar <arrays.scalars>` type that also has two fields:

   >>> x = np.array([('Sarah', (8.0, 7.0)), ('John', (6.0, 7.0))], dtype=dt)
   >>> x[1]
   ('John', [6., 7.])
   >>> x[1]['grades']
   array([6.,  7.])
   >>> type(x[1])
   <class 'numpy.void'>
   >>> type(x[1]['grades'])
   <class 'numpy.ndarray'>

.. _arrays.dtypes.constructing:

Specifying and constructing data types
======================================

Whenever a data-type is required in a NumPy function or method, either
a :class:`dtype` object or something that can be converted to one can
be supplied.  Such conversions are done by the :class:`dtype`
constructor:

.. autosummary::
   :toctree: generated/

   dtype

What can be converted to a data-type object is described below:

:class:`dtype` object
   .. index::
      triple: dtype; construction; from dtype

   Used as-is.

None
   .. index::
      triple: dtype; construction; from None

   The default data type: :class:`float_`.

.. index::
   triple: dtype; construction; from type

Array-scalar types
    The 24 built-in :ref:`array scalar type objects
    <arrays.scalars.built-in>` all convert to an associated data-type object.
    This is true for their sub-classes as well.

    Note that not all data-type information can be supplied with a
    type-object: for example, `flexible` data-types have
    a default *itemsize* of 0, and require an explicitly given size
    to be useful.

    .. admonition:: Example

       >>> dt = np.dtype(np.int32)      # 32-bit integer
       >>> dt = np.dtype(np.complex128) # 128-bit complex floating-point number

Generic types
    The generic hierarchical type objects convert to corresponding
    type objects according to the associations:

    =====================================================  ===============
    :class:`number`, :class:`inexact`, :class:`floating`   :class:`float`
    :class:`complexfloating`                               :class:`cfloat`
    :class:`integer`, :class:`signedinteger`               :class:`int\_`
    :class:`unsignedinteger`                               :class:`uint`
    :class:`character`                                     :class:`string`
    :class:`generic`, :class:`flexible`                    :class:`void`
    =====================================================  ===============

    .. deprecated:: 1.19

        This conversion of generic scalar types is deprecated.
        This is because it can be unexpected in a context such as
        ``arr.astype(dtype=np.floating)``, which casts an array of ``float32``
        to an array of ``float64``, even though ``float32`` is a subdtype of
        ``np.floating``.


Built-in Python types
    Several python types are equivalent to a corresponding
    array scalar when used to generate a :class:`dtype` object:

    ================  ===============
    :class:`int`      :class:`int\_`
    :class:`bool`     :class:`bool\_`
    :class:`float`    :class:`float\_`
    :class:`complex`  :class:`cfloat`
    :class:`bytes`    :class:`bytes\_`
    :class:`str`      :class:`str\_`
    :class:`buffer`   :class:`void`
    (all others)      :class:`object_`
    ================  ===============

    Note that ``str`` refers to either null terminated bytes or unicode strings
    depending on the Python version. In code targeting both Python 2 and 3
    ``np.unicode_`` should be used as a dtype for strings.
    See :ref:`Note on string types<string-dtype-note>`.

    .. admonition:: Example

       >>> dt = np.dtype(float)   # Python-compatible floating-point number
       >>> dt = np.dtype(int)     # Python-compatible integer
       >>> dt = np.dtype(object)  # Python object

    .. note::

        All other types map to ``object_`` for convenience. Code should expect
        that such types may map to a specific (new) dtype in the future.

Types with ``.dtype``
    Any type object with a ``dtype`` attribute: The attribute will be
    accessed and used directly. The attribute must return something
    that is convertible into a dtype object.

.. index::
   triple: dtype; construction; from string

Several kinds of strings can be converted. Recognized strings can be
prepended with ``'>'`` (:term:`big-endian`), ``'<'``
(:term:`little-endian`), or ``'='`` (hardware-native, the default), to
specify the byte order.

One-character strings
    Each built-in data-type has a character code
    (the updated Numeric typecodes), that uniquely identifies it.

    .. admonition:: Example

       >>> dt = np.dtype('b')  # byte, native byte order
       >>> dt = np.dtype('>H') # big-endian unsigned short
       >>> dt = np.dtype('<f') # little-endian single-precision float
       >>> dt = np.dtype('d')  # double-precision floating-point number

Array-protocol type strings (see :ref:`arrays.interface`)
   The first character specifies the kind of data and the remaining
   characters specify the number of bytes per item, except for Unicode,
   where it is interpreted as the number of characters.  The item size
   must correspond to an existing type, or an error will be raised.  The
   supported kinds are

   ================   ========================
   ``'?'``            boolean
   ``'b'``            (signed) byte
   ``'B'``            unsigned byte
   ``'i'``            (signed) integer
   ``'u'``            unsigned integer
   ``'f'``            floating-point
   ``'c'``            complex-floating point
   ``'m'``            timedelta
   ``'M'``            datetime
   ``'O'``            (Python) objects
   ``'S'``, ``'a'``   zero-terminated bytes (not recommended)
   ``'U'``            Unicode string
   ``'V'``            raw data (:class:`void`)
   ================   ========================

   .. admonition:: Example

      >>> dt = np.dtype('i4')   # 32-bit signed integer
      >>> dt = np.dtype('f8')   # 64-bit floating-point number
      >>> dt = np.dtype('c16')  # 128-bit complex floating-point number
      >>> dt = np.dtype('a25')  # 25-length zero-terminated bytes
      >>> dt = np.dtype('U25')  # 25-character string

   .. _string-dtype-note:

   .. admonition:: Note on string types

    For backward compatibility with Python 2 the ``S`` and ``a`` typestrings
    remain zero-terminated bytes and `numpy.string_` continues to alias
    `numpy.bytes_`. To use actual strings in Python 3 use ``U`` or `numpy.str_`.
    For signed bytes that do not need zero-termination ``b`` or ``i1`` can be
    used.

String with comma-separated fields
   A short-hand notation for specifying the format of a structured data type is
   a comma-separated string of basic formats.

   A basic format in this context is an optional shape specifier
   followed by an array-protocol type string. Parenthesis are required
   on the shape if it has more than one dimension. NumPy allows a modification
   on the format in that any string that can uniquely identify the
   type can be used to specify the data-type in a field.
   The generated data-type fields are named ``'f0'``, ``'f1'``, ...,
   ``'f<N-1>'`` where N (>1) is the number of comma-separated basic
   formats in the string. If the optional shape specifier is provided,
   then the data-type for the corresponding field describes a sub-array.

   .. admonition:: Example

      - field named ``f0`` containing a 32-bit integer
      - field named ``f1`` containing a 2 x 3 sub-array
        of 64-bit floating-point numbers
      - field named ``f2`` containing a 32-bit floating-point number

      >>> dt = np.dtype("i4, (2,3)f8, f4")

      - field named ``f0`` containing a 3-character string
      - field named ``f1`` containing a sub-array of shape (3,)
        containing 64-bit unsigned integers
      - field named ``f2`` containing a 3 x 4 sub-array
        containing 10-character strings

      >>> dt = np.dtype("a3, 3u8, (3,4)a10")

Type strings
   Any string in :obj:`numpy.sctypeDict`.keys():

   .. admonition:: Example

      >>> dt = np.dtype('uint32')   # 32-bit unsigned integer
      >>> dt = np.dtype('float64')  # 64-bit floating-point number

.. index::
   triple: dtype; construction; from tuple

``(flexible_dtype, itemsize)``
    The first argument must be an object that is converted to a
    zero-sized flexible data-type object, the second argument is
    an integer providing the desired itemsize.

    .. admonition:: Example

       >>> dt = np.dtype((np.void, 10))  # 10-byte wide data block
       >>> dt = np.dtype(('U', 10))   # 10-character unicode string

``(fixed_dtype, shape)``
    .. index::
       pair: dtype; sub-array

    The first argument is any object that can be converted into a
    fixed-size data-type object. The second argument is the desired
    shape of this type. If the shape parameter is 1, then the
    data-type object used to be equivalent to fixed dtype. This behaviour is
    deprecated since NumPy 1.17 and will raise an error in the future.
    If *shape* is a tuple, then the new dtype defines a sub-array of the given
    shape.

    .. admonition:: Example

       >>> dt = np.dtype((np.int32, (2,2)))          # 2 x 2 integer sub-array
       >>> dt = np.dtype(('i4, (2,3)f8, f4', (2,3))) # 2 x 3 structured sub-array

.. index::
   triple: dtype; construction; from list

``[(field_name, field_dtype, field_shape), ...]``
   *obj* should be a list of fields where each field is described by a
   tuple of length 2 or 3. (Equivalent to the ``descr`` item in the
   :obj:`~object.__array_interface__` attribute.)

   The first element, *field_name*, is the field name (if this is
   ``''`` then a standard field name, ``'f#'``, is assigned).  The
   field name may also be a 2-tuple of strings where the first string
   is either a "title" (which may be any string or unicode string) or
   meta-data for the field which can be any object, and the second
   string is the "name" which must be a valid Python identifier.

   The second element, *field_dtype*, can be anything that can be
   interpreted as a data-type.

   The optional third element *field_shape* contains the shape if this
   field represents an array of the data-type in the second
   element. Note that a 3-tuple with a third argument equal to 1 is
   equivalent to a 2-tuple.

   This style does not accept *align* in the :class:`dtype`
   constructor as it is assumed that all of the memory is accounted
   for by the array interface description.

   .. admonition:: Example

      Data-type with fields ``big`` (big-endian 32-bit integer) and
      ``little`` (little-endian 32-bit integer):

      >>> dt = np.dtype([('big', '>i4'), ('little', '<i4')])

      Data-type with fields ``R``, ``G``, ``B``, ``A``, each being an
      unsigned 8-bit integer:

      >>> dt = np.dtype([('R','u1'), ('G','u1'), ('B','u1'), ('A','u1')])

.. index::
   triple: dtype; construction; from dict

``{'names': ..., 'formats': ..., 'offsets': ..., 'titles': ..., 'itemsize': ...}``
    This style has two required and three optional keys.  The *names*
    and *formats* keys are required. Their respective values are
    equal-length lists with the field names and the field formats.
    The field names must be strings and the field formats can be any
    object accepted by :class:`dtype` constructor.

    When the optional keys *offsets* and *titles* are provided,
    their values must each be lists of the same length as the *names*
    and *formats* lists. The *offsets* value is a list of byte offsets
    (limited to `ctypes.c_int`) for each field, while the *titles* value is a
    list of titles for each field (``None`` can be used if no title is
    desired for that field). The *titles* can be any object, but when a
    :class:`str` object will add another entry to the
    fields dictionary keyed by the title and referencing the same
    field tuple which will contain the title as an additional tuple
    member.

    The *itemsize* key allows the total size of the dtype to be
    set, and must be an integer large enough so all the fields
    are within the dtype. If the dtype being constructed is aligned,
    the *itemsize* must also be divisible by the struct alignment. Total dtype
    *itemsize* is limited to `ctypes.c_int`.

    .. admonition:: Example

       Data type with fields ``r``, ``g``, ``b``, ``a``, each being
       an 8-bit unsigned integer:

       >>> dt = np.dtype({'names': ['r','g','b','a'],
       ...                'formats': [np.uint8, np.uint8, np.uint8, np.uint8]})

       Data type with fields ``r`` and ``b`` (with the given titles),
       both being 8-bit unsigned integers, the first at byte position
       0 from the start of the field and the second at position 2:

       >>> dt = np.dtype({'names': ['r','b'], 'formats': ['u1', 'u1'],
       ...                'offsets': [0, 2],
       ...                'titles': ['Red pixel', 'Blue pixel']})


``{'field1': ..., 'field2': ..., ...}``
    This usage is discouraged, because it is ambiguous with the
    other dict-based construction method. If you have a field
    called 'names' and a field called 'formats' there will be
    a conflict.

    This style allows passing in the :attr:`fields <dtype.fields>`
    attribute of a data-type object.

    *obj* should contain string or unicode keys that refer to
    ``(data-type, offset)`` or ``(data-type, offset, title)`` tuples.

    .. admonition:: Example

       Data type containing field ``col1`` (10-character string at
       byte position 0), ``col2`` (32-bit float at byte position 10),
       and ``col3`` (integers at byte position 14):

       >>> dt = np.dtype({'col1': ('U10', 0), 'col2': (np.float32, 10),
       ...                'col3': (int, 14)})

``(base_dtype, new_dtype)``
    In NumPy 1.7 and later, this form allows `base_dtype` to be interpreted as
    a structured dtype. Arrays created with this dtype will have underlying
    dtype `base_dtype` but will have fields and flags taken from `new_dtype`.
    This is useful for creating custom structured dtypes, as done in
    :ref:`record arrays <arrays.classes.rec>`.

    This form also makes it possible to specify struct dtypes with overlapping
    fields, functioning like the 'union' type in C. This usage is discouraged,
    however, and the union mechanism is preferred.

    Both arguments must be convertible to data-type objects with the same total
    size.

    .. admonition:: Example

       32-bit integer, whose first two bytes are interpreted as an integer
       via field ``real``, and the following two bytes via field ``imag``.

       >>> dt = np.dtype((np.int32,{'real':(np.int16, 0),'imag':(np.int16, 2)}))

       32-bit integer, which is interpreted as consisting of a sub-array
       of shape ``(4,)`` containing 8-bit integers:

       >>> dt = np.dtype((np.int32, (np.int8, 4)))

       32-bit integer, containing fields ``r``, ``g``, ``b``, ``a`` that
       interpret the 4 bytes in the integer as four unsigned integers:

       >>> dt = np.dtype(('i4', [('r','u1'),('g','u1'),('b','u1'),('a','u1')]))


:class:`dtype`
==============

NumPy data type descriptions are instances of the :class:`dtype` class.

Attributes
----------

The type of the data is described by the following :class:`dtype`  attributes:

.. autosummary::
   :toctree: generated/

   dtype.type
   dtype.kind
   dtype.char
   dtype.num
   dtype.str

Size of the data is in turn described by:

.. autosummary::
   :toctree: generated/

   dtype.name
   dtype.itemsize

Endianness of this data:

.. autosummary::
   :toctree: generated/

   dtype.byteorder

Information about sub-data-types in a :term:`structured data type`:

.. autosummary::
   :toctree: generated/

   dtype.fields
   dtype.names

For data types that describe sub-arrays:

.. autosummary::
   :toctree: generated/

   dtype.subdtype
   dtype.shape

Attributes providing additional information:

.. autosummary::
   :toctree: generated/

   dtype.hasobject
   dtype.flags
   dtype.isbuiltin
   dtype.isnative
   dtype.descr
   dtype.alignment
   dtype.base

Metadata attached by the user:

.. autosummary::
   :toctree: generated/

    dtype.metadata


Methods
-------

Data types have the following method for changing the byte order:

.. autosummary::
   :toctree: generated/

   dtype.newbyteorder

The following methods implement the pickle protocol:

.. autosummary::
   :toctree: generated/

   dtype.__reduce__
   dtype.__setstate__

Utility method for typing:

.. autosummary::
   :toctree: generated/

   dtype.__class_getitem__

Comparison operations:

.. autosummary::
   :toctree: generated/

   dtype.__ge__
   dtype.__gt__
   dtype.__le__
   dtype.__lt__
numpy.i: a SWIG Interface File for NumPy
========================================

Introduction
------------

The Simple Wrapper and Interface Generator (or `SWIG
<http://www.swig.org>`_) is a powerful tool for generating wrapper
code for interfacing to a wide variety of scripting languages.
`SWIG`_ can parse header files, and using only the code prototypes,
create an interface to the target language.  But `SWIG`_ is not
omnipotent.  For example, it cannot know from the prototype::

    double rms(double* seq, int n);

what exactly ``seq`` is.  Is it a single value to be altered in-place?
Is it an array, and if so what is its length?  Is it input-only?
Output-only?  Input-output?  `SWIG`_ cannot determine these details,
and does not attempt to do so.

If we designed ``rms``, we probably made it a routine that takes an
input-only array of length ``n`` of ``double`` values called ``seq``
and returns the root mean square.  The default behavior of `SWIG`_,
however, will be to create a wrapper function that compiles, but is
nearly impossible to use from the scripting language in the way the C
routine was intended.

For Python, the preferred way of handling contiguous (or technically,
*strided*) blocks of homogeneous data is with NumPy, which provides full
object-oriented access to multidimensial arrays of data.  Therefore, the most
logical Python interface for the ``rms`` function would be (including doc
string)::

    def rms(seq):
        """
        rms: return the root mean square of a sequence
        rms(numpy.ndarray) -> double
        rms(list) -> double
        rms(tuple) -> double
        """

where ``seq`` would be a NumPy array of ``double`` values, and its
length ``n`` would be extracted from ``seq`` internally before being
passed to the C routine.  Even better, since NumPy supports
construction of arrays from arbitrary Python sequences, ``seq``
itself could be a nearly arbitrary sequence (so long as each element
can be converted to a ``double``) and the wrapper code would
internally convert it to a NumPy array before extracting its data
and length.

`SWIG`_ allows these types of conversions to be defined via a
mechanism called *typemaps*.  This document provides information on
how to use ``numpy.i``, a `SWIG`_ interface file that defines a series
of typemaps intended to make the type of array-related conversions
described above relatively simple to implement.  For example, suppose
that the ``rms`` function prototype defined above was in a header file
named ``rms.h``.  To obtain the Python interface discussed above, your
`SWIG`_ interface file would need the following::

    %{
    #define SWIG_FILE_WITH_INIT
    #include "rms.h"
    %}

    %include "numpy.i"

    %init %{
    import_array();
    %}

    %apply (double* IN_ARRAY1, int DIM1) {(double* seq, int n)};
    %include "rms.h"

Typemaps are keyed off a list of one or more function arguments,
either by type or by type and name.  We will refer to such lists as
*signatures*.  One of the many typemaps defined by ``numpy.i`` is used
above and has the signature ``(double* IN_ARRAY1, int DIM1)``.  The
argument names are intended to suggest that the ``double*`` argument
is an input array of one dimension and that the ``int`` represents the
size of that dimension.  This is precisely the pattern in the ``rms``
prototype.

Most likely, no actual prototypes to be wrapped will have the argument
names ``IN_ARRAY1`` and ``DIM1``.  We use the `SWIG`_ ``%apply``
directive to apply the typemap for one-dimensional input arrays of
type ``double`` to the actual prototype used by ``rms``.  Using
``numpy.i`` effectively, therefore, requires knowing what typemaps are
available and what they do.

A `SWIG`_ interface file that includes the `SWIG`_ directives given
above will produce wrapper code that looks something like::

     1 PyObject *_wrap_rms(PyObject *args) {
     2   PyObject *resultobj = 0;
     3   double *arg1 = (double *) 0 ;
     4   int arg2 ;
     5   double result;
     6   PyArrayObject *array1 = NULL ;
     7   int is_new_object1 = 0 ;
     8   PyObject * obj0 = 0 ;
     9
    10   if (!PyArg_ParseTuple(args,(char *)"O:rms",&obj0)) SWIG_fail;
    11   {
    12     array1 = obj_to_array_contiguous_allow_conversion(
    13                  obj0, NPY_DOUBLE, &is_new_object1);
    14     npy_intp size[1] = {
    15       -1
    16     };
    17     if (!array1 || !require_dimensions(array1, 1) ||
    18         !require_size(array1, size, 1)) SWIG_fail;
    19     arg1 = (double*) array1->data;
    20     arg2 = (int) array1->dimensions[0];
    21   }
    22   result = (double)rms(arg1,arg2);
    23   resultobj = SWIG_From_double((double)(result));
    24   {
    25     if (is_new_object1 && array1) Py_DECREF(array1);
    26   }
    27   return resultobj;
    28 fail:
    29   {
    30     if (is_new_object1 && array1) Py_DECREF(array1);
    31   }
    32   return NULL;
    33 }

The typemaps from ``numpy.i`` are responsible for the following lines
of code: 12--20, 25 and 30.  Line 10 parses the input to the ``rms``
function.  From the format string ``"O:rms"``, we can see that the
argument list is expected to be a single Python object (specified
by the ``O`` before the colon) and whose pointer is stored in
``obj0``.  A number of functions, supplied by ``numpy.i``, are called
to make and check the (possible) conversion from a generic Python
object to a NumPy array.  These functions are explained in the
section `Helper Functions`_, but hopefully their names are
self-explanatory.  At line 12 we use ``obj0`` to construct a NumPy
array.  At line 17, we check the validity of the result: that it is
non-null and that it has a single dimension of arbitrary length.  Once
these states are verified, we extract the data buffer and length in
lines 19 and 20 so that we can call the underlying C function at line
22.  Line 25 performs memory management for the case where we have
created a new array that is no longer needed.

This code has a significant amount of error handling.  Note the
``SWIG_fail`` is a macro for ``goto fail``, referring to the label at
line 28.  If the user provides the wrong number of arguments, this
will be caught at line 10.  If construction of the NumPy array
fails or produces an array with the wrong number of dimensions, these
errors are caught at line 17.  And finally, if an error is detected,
memory is still managed correctly at line 30.

Note that if the C function signature was in a different order::

    double rms(int n, double* seq);

that `SWIG`_ would not match the typemap signature given above with
the argument list for ``rms``.  Fortunately, ``numpy.i`` has a set of
typemaps with the data pointer given last::

    %apply (int DIM1, double* IN_ARRAY1) {(int n, double* seq)};

This simply has the effect of switching the definitions of ``arg1``
and ``arg2`` in lines 3 and 4 of the generated code above, and their
assignments in lines 19 and 20.

Using numpy.i
-------------

The ``numpy.i`` file is currently located in the ``tools/swig``
sub-directory under the ``numpy`` installation directory.  Typically,
you will want to copy it to the directory where you are developing
your wrappers.

A simple module that only uses a single `SWIG`_ interface file should
include the following::

    %{
    #define SWIG_FILE_WITH_INIT
    %}
    %include "numpy.i"
    %init %{
    import_array();
    %}

Within a compiled Python module, ``import_array()`` should only get
called once.  This could be in a C/C++ file that you have written and
is linked to the module.  If this is the case, then none of your
interface files should ``#define SWIG_FILE_WITH_INIT`` or call
``import_array()``.  Or, this initialization call could be in a
wrapper file generated by `SWIG`_ from an interface file that has the
``%init`` block as above.  If this is the case, and you have more than
one `SWIG`_ interface file, then only one interface file should
``#define SWIG_FILE_WITH_INIT`` and call ``import_array()``.

Available Typemaps
------------------

The typemap directives provided by ``numpy.i`` for arrays of different
data types, say ``double`` and ``int``, and dimensions of different
types, say ``int`` or ``long``, are identical to one another except
for the C and NumPy type specifications.  The typemaps are
therefore implemented (typically behind the scenes) via a macro::

    %numpy_typemaps(DATA_TYPE, DATA_TYPECODE, DIM_TYPE)

that can be invoked for appropriate ``(DATA_TYPE, DATA_TYPECODE,
DIM_TYPE)`` triplets.  For example::

    %numpy_typemaps(double, NPY_DOUBLE, int)
    %numpy_typemaps(int,    NPY_INT   , int)

The ``numpy.i`` interface file uses the ``%numpy_typemaps`` macro to
implement typemaps for the following C data types and ``int``
dimension types:

  * ``signed char``
  * ``unsigned char``
  * ``short``
  * ``unsigned short``
  * ``int``
  * ``unsigned int``
  * ``long``
  * ``unsigned long``
  * ``long long``
  * ``unsigned long long``
  * ``float``
  * ``double``

In the following descriptions, we reference a generic ``DATA_TYPE``, which
could be any of the C data types listed above, and ``DIM_TYPE`` which
should be one of the many types of integers.

The typemap signatures are largely differentiated on the name given to
the buffer pointer.  Names with ``FARRAY`` are for Fortran-ordered
arrays, and names with ``ARRAY`` are for C-ordered (or 1D arrays).

Input Arrays
````````````

Input arrays are defined as arrays of data that are passed into a
routine but are not altered in-place or returned to the user.  The
Python input array is therefore allowed to be almost any Python
sequence (such as a list) that can be converted to the requested type
of array.  The input array signatures are

1D:

  * ``(	DATA_TYPE IN_ARRAY1[ANY] )``
  * ``(	DATA_TYPE* IN_ARRAY1, int DIM1 )``
  * ``(	int DIM1, DATA_TYPE* IN_ARRAY1 )``

2D:

  * ``(	DATA_TYPE IN_ARRAY2[ANY][ANY] )``
  * ``(	DATA_TYPE* IN_ARRAY2, int DIM1, int DIM2 )``
  * ``(	int DIM1, int DIM2, DATA_TYPE* IN_ARRAY2 )``
  * ``(	DATA_TYPE* IN_FARRAY2, int DIM1, int DIM2 )``
  * ``(	int DIM1, int DIM2, DATA_TYPE* IN_FARRAY2 )``

3D:

  * ``(	DATA_TYPE IN_ARRAY3[ANY][ANY][ANY] )``
  * ``(	DATA_TYPE* IN_ARRAY3, int DIM1, int DIM2, int DIM3 )``
  * ``(	int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_ARRAY3 )``
  * ``(	DATA_TYPE* IN_FARRAY3, int DIM1, int DIM2, int DIM3 )``
  * ``(	int DIM1, int DIM2, int DIM3, DATA_TYPE* IN_FARRAY3 )``

4D:

  * ``(DATA_TYPE IN_ARRAY4[ANY][ANY][ANY][ANY])``
  * ``(DATA_TYPE* IN_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)``
  * ``(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* IN_ARRAY4)``
  * ``(DATA_TYPE* IN_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)``
  * ``(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* IN_FARRAY4)``

The first signature listed, ``( DATA_TYPE IN_ARRAY[ANY] )`` is for
one-dimensional arrays with hard-coded dimensions.  Likewise,
``( DATA_TYPE IN_ARRAY2[ANY][ANY] )`` is for two-dimensional arrays
with hard-coded dimensions, and similarly for three-dimensional.

In-Place Arrays
```````````````

In-place arrays are defined as arrays that are modified in-place.  The
input values may or may not be used, but the values at the time the
function returns are significant.  The provided Python argument
must therefore be a NumPy array of the required type.  The in-place
signatures are

1D:

  * ``(	DATA_TYPE INPLACE_ARRAY1[ANY] )``
  * ``(	DATA_TYPE* INPLACE_ARRAY1, int DIM1 )``
  * ``(	int DIM1, DATA_TYPE* INPLACE_ARRAY1 )``

2D:

  * ``(	DATA_TYPE INPLACE_ARRAY2[ANY][ANY] )``
  * ``(	DATA_TYPE* INPLACE_ARRAY2, int DIM1, int DIM2 )``
  * ``(	int DIM1, int DIM2, DATA_TYPE* INPLACE_ARRAY2 )``
  * ``(	DATA_TYPE* INPLACE_FARRAY2, int DIM1, int DIM2 )``
  * ``(	int DIM1, int DIM2, DATA_TYPE* INPLACE_FARRAY2 )``

3D:

  * ``(	DATA_TYPE INPLACE_ARRAY3[ANY][ANY][ANY] )``
  * ``(	DATA_TYPE* INPLACE_ARRAY3, int DIM1, int DIM2, int DIM3 )``
  * ``(	int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_ARRAY3 )``
  * ``(	DATA_TYPE* INPLACE_FARRAY3, int DIM1, int DIM2, int DIM3 )``
  * ``(	int DIM1, int DIM2, int DIM3, DATA_TYPE* INPLACE_FARRAY3 )``

4D:

  * ``(DATA_TYPE INPLACE_ARRAY4[ANY][ANY][ANY][ANY])``
  * ``(DATA_TYPE* INPLACE_ARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)``
  * ``(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, , DIM_TYPE DIM4, DATA_TYPE* INPLACE_ARRAY4)``
  * ``(DATA_TYPE* INPLACE_FARRAY4, DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4)``
  * ``(DIM_TYPE DIM1, DIM_TYPE DIM2, DIM_TYPE DIM3, DIM_TYPE DIM4, DATA_TYPE* INPLACE_FARRAY4)``

These typemaps now check to make sure that the ``INPLACE_ARRAY``
arguments use native byte ordering.  If not, an exception is raised.

There is also a "flat" in-place array for situations in which
you would like to modify or process each element, regardless of the
number of dimensions. One example is a "quantization" function that
quantizes each element of an array in-place, be it 1D, 2D or whatever.
This form checks for continuity but allows either C or Fortran ordering.

ND:

 * ``(DATA_TYPE* INPLACE_ARRAY_FLAT, DIM_TYPE DIM_FLAT)``


Argout Arrays
`````````````

Argout arrays are arrays that appear in the input arguments in C, but
are in fact output arrays.  This pattern occurs often when there is
more than one output variable and the single return argument is
therefore not sufficient.  In Python, the conventional way to return
multiple arguments is to pack them into a sequence (tuple, list, etc.)
and return the sequence.  This is what the argout typemaps do.  If a
wrapped function that uses these argout typemaps has more than one
return argument, they are packed into a tuple or list, depending on
the version of Python.  The Python user does not pass these
arrays in, they simply get returned.  For the case where a dimension
is specified, the python user must provide that dimension as an
argument.  The argout signatures are

1D:

  * ``(	DATA_TYPE ARGOUT_ARRAY1[ANY] )``
  * ``(	DATA_TYPE* ARGOUT_ARRAY1, int DIM1 )``
  * ``(	int DIM1, DATA_TYPE* ARGOUT_ARRAY1 )``

2D:

  * ``(	DATA_TYPE ARGOUT_ARRAY2[ANY][ANY] )``

3D:

  * ``(	DATA_TYPE ARGOUT_ARRAY3[ANY][ANY][ANY] )``

4D:

  * ``(	DATA_TYPE ARGOUT_ARRAY4[ANY][ANY][ANY][ANY] )``

These are typically used in situations where in C/C++, you would
allocate a(n) array(s) on the heap, and call the function to fill the
array(s) values.  In Python, the arrays are allocated for you and
returned as new array objects.

Note that we support ``DATA_TYPE*`` argout typemaps in 1D, but not 2D
or 3D.  This is because of a quirk with the `SWIG`_ typemap syntax and
cannot be avoided.  Note that for these types of 1D typemaps, the
Python function will take a single argument representing ``DIM1``.

Argout View Arrays
``````````````````

Argoutview arrays are for when your C code provides you with a view of
its internal data and does not require any memory to be allocated by
the user.  This can be dangerous.  There is almost no way to guarantee
that the internal data from the C code will remain in existence for
the entire lifetime of the NumPy array that encapsulates it.  If
the user destroys the object that provides the view of the data before
destroying the NumPy array, then using that array may result in bad
memory references or segmentation faults.  Nevertheless, there are
situations, working with large data sets, where you simply have no
other choice.

The C code to be wrapped for argoutview arrays are characterized by
pointers: pointers to the dimensions and double pointers to the data,
so that these values can be passed back to the user.  The argoutview
typemap signatures are therefore

1D:

  * ``( DATA_TYPE** ARGOUTVIEW_ARRAY1, DIM_TYPE* DIM1 )``
  * ``( DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEW_ARRAY1 )``

2D:

  * ``( DATA_TYPE** ARGOUTVIEW_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 )``
  * ``( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_ARRAY2 )``
  * ``( DATA_TYPE** ARGOUTVIEW_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2 )``
  * ``( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEW_FARRAY2 )``

3D:

  * ``( DATA_TYPE** ARGOUTVIEW_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)``
  * ``( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_ARRAY3)``
  * ``( DATA_TYPE** ARGOUTVIEW_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)``
  * ``( DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEW_FARRAY3)``

4D:

  * ``(DATA_TYPE** ARGOUTVIEW_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_ARRAY4)``
  * ``(DATA_TYPE** ARGOUTVIEW_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEW_FARRAY4)``

Note that arrays with hard-coded dimensions are not supported.  These
cannot follow the double pointer signatures of these typemaps.

Memory Managed Argout View Arrays
`````````````````````````````````

A recent addition to ``numpy.i`` are typemaps that permit argout
arrays with views into memory that is managed.  See the discussion `here
<http://blog.enthought.com/python/numpy-arrays-with-pre-allocated-memory>`_.

1D:

  * ``(DATA_TYPE** ARGOUTVIEWM_ARRAY1, DIM_TYPE* DIM1)``
  * ``(DIM_TYPE* DIM1, DATA_TYPE** ARGOUTVIEWM_ARRAY1)``

2D:

  * ``(DATA_TYPE** ARGOUTVIEWM_ARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_ARRAY2)``
  * ``(DATA_TYPE** ARGOUTVIEWM_FARRAY2, DIM_TYPE* DIM1, DIM_TYPE* DIM2)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DATA_TYPE** ARGOUTVIEWM_FARRAY2)``

3D:

  * ``(DATA_TYPE** ARGOUTVIEWM_ARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_ARRAY3)``
  * ``(DATA_TYPE** ARGOUTVIEWM_FARRAY3, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DATA_TYPE** ARGOUTVIEWM_FARRAY3)``

4D:

  * ``(DATA_TYPE** ARGOUTVIEWM_ARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_ARRAY4)``
  * ``(DATA_TYPE** ARGOUTVIEWM_FARRAY4, DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4)``
  * ``(DIM_TYPE* DIM1, DIM_TYPE* DIM2, DIM_TYPE* DIM3, DIM_TYPE* DIM4, DATA_TYPE** ARGOUTVIEWM_FARRAY4)``


Output Arrays
`````````````

The ``numpy.i`` interface file does not support typemaps for output
arrays, for several reasons.  First, C/C++ return arguments are
limited to a single value.  This prevents obtaining dimension
information in a general way.  Second, arrays with hard-coded lengths
are not permitted as return arguments.  In other words::

    double[3] newVector(double x, double y, double z);

is not legal C/C++ syntax.  Therefore, we cannot provide typemaps of
the form::

    %typemap(out) (TYPE[ANY]);

If you run into a situation where a function or method is returning a
pointer to an array, your best bet is to write your own version of the
function to be wrapped, either with ``%extend`` for the case of class
methods or ``%ignore`` and ``%rename`` for the case of functions.

Other Common Types: bool
````````````````````````

Note that C++ type ``bool`` is not supported in the list in the
`Available Typemaps`_ section.  NumPy bools are a single byte, while
the C++ ``bool`` is four bytes (at least on my system).  Therefore::

    %numpy_typemaps(bool, NPY_BOOL, int)

will result in typemaps that will produce code that reference
improper data lengths.  You can implement the following macro
expansion::

    %numpy_typemaps(bool, NPY_UINT, int)

to fix the data length problem, and `Input Arrays`_ will work fine,
but `In-Place Arrays`_ might fail type-checking.

Other Common Types: complex
```````````````````````````

Typemap conversions for complex floating-point types is also not
supported automatically.  This is because Python and NumPy are
written in C, which does not have native complex types.  Both
Python and NumPy implement their own (essentially equivalent)
``struct`` definitions for complex variables::

    /* Python */
    typedef struct {double real; double imag;} Py_complex;

    /* NumPy */
    typedef struct {float  real, imag;} npy_cfloat;
    typedef struct {double real, imag;} npy_cdouble;

We could have implemented::

    %numpy_typemaps(Py_complex , NPY_CDOUBLE, int)
    %numpy_typemaps(npy_cfloat , NPY_CFLOAT , int)
    %numpy_typemaps(npy_cdouble, NPY_CDOUBLE, int)

which would have provided automatic type conversions for arrays of
type ``Py_complex``, ``npy_cfloat`` and ``npy_cdouble``.  However, it
seemed unlikely that there would be any independent (non-Python,
non-NumPy) application code that people would be using `SWIG`_ to
generate a Python interface to, that also used these definitions
for complex types.  More likely, these application codes will define
their own complex types, or in the case of C++, use ``std::complex``.
Assuming these data structures are compatible with Python and
NumPy complex types, ``%numpy_typemap`` expansions as above (with
the user's complex type substituted for the first argument) should
work.

NumPy Array Scalars and SWIG
----------------------------

`SWIG`_ has sophisticated type checking for numerical types.  For
example, if your C/C++ routine expects an integer as input, the code
generated by `SWIG`_ will check for both Python integers and
Python long integers, and raise an overflow error if the provided
Python integer is too big to cast down to a C integer.  With the
introduction of NumPy scalar arrays into your Python code, you
might conceivably extract an integer from a NumPy array and attempt
to pass this to a `SWIG`_-wrapped C/C++ function that expects an
``int``, but the `SWIG`_ type checking will not recognize the NumPy
array scalar as an integer.  (Often, this does in fact work -- it
depends on whether NumPy recognizes the integer type you are using
as inheriting from the Python integer type on the platform you are
using.  Sometimes, this means that code that works on a 32-bit machine
will fail on a 64-bit machine.)

If you get a Python error that looks like the following::

    TypeError: in method 'MyClass_MyMethod', argument 2 of type 'int'

and the argument you are passing is an integer extracted from a
NumPy array, then you have stumbled upon this problem.  The
solution is to modify the `SWIG`_ type conversion system to accept
NumPy array scalars in addition to the standard integer types.
Fortunately, this capability has been provided for you.  Simply copy
the file::

    pyfragments.swg

to the working build directory for you project, and this problem will
be fixed.  It is suggested that you do this anyway, as it only
increases the capabilities of your Python interface.

Why is There a Second File?
```````````````````````````

The `SWIG`_ type checking and conversion system is a complicated
combination of C macros, `SWIG`_ macros, `SWIG`_ typemaps and `SWIG`_
fragments.  Fragments are a way to conditionally insert code into your
wrapper file if it is needed, and not insert it if not needed.  If
multiple typemaps require the same fragment, the fragment only gets
inserted into your wrapper code once.

There is a fragment for converting a Python integer to a C
``long``.  There is a different fragment that converts a Python
integer to a C ``int``, that calls the routine defined in the
``long`` fragment.  We can make the changes we want here by changing
the definition for the ``long`` fragment.  `SWIG`_ determines the
active definition for a fragment using a "first come, first served"
system.  That is, we need to define the fragment for ``long``
conversions prior to `SWIG`_ doing it internally.  `SWIG`_ allows us
to do this by putting our fragment definitions in the file
``pyfragments.swg``.  If we were to put the new fragment definitions
in ``numpy.i``, they would be ignored.

Helper Functions
----------------

The ``numpy.i`` file contains several macros and routines that it
uses internally to build its typemaps.  However, these functions may
be useful elsewhere in your interface file.  These macros and routines
are implemented as fragments, which are described briefly in the
previous section.  If you try to use one or more of the following
macros or functions, but your compiler complains that it does not
recognize the symbol, then you need to force these fragments to appear
in your code using::

    %fragment("NumPy_Fragments");

in your `SWIG`_ interface file.

Macros
``````

  **is_array(a)**
    Evaluates as true if ``a`` is non-``NULL`` and can be cast to a
    ``PyArrayObject*``.

  **array_type(a)**
    Evaluates to the integer data type code of ``a``, assuming ``a`` can
    be cast to a ``PyArrayObject*``.

  **array_numdims(a)**
    Evaluates to the integer number of dimensions of ``a``, assuming
    ``a`` can be cast to a ``PyArrayObject*``.

  **array_dimensions(a)**
    Evaluates to an array of type ``npy_intp`` and length
    ``array_numdims(a)``, giving the lengths of all of the dimensions
    of ``a``, assuming ``a`` can be cast to a ``PyArrayObject*``.

  **array_size(a,i)**
    Evaluates to the ``i``-th dimension size of ``a``, assuming ``a``
    can be cast to a ``PyArrayObject*``.

  **array_strides(a)**
    Evaluates to an array of type ``npy_intp`` and length
    ``array_numdims(a)``, giving the stridess of all of the dimensions
    of ``a``, assuming ``a`` can be cast to a ``PyArrayObject*``.  A
    stride is the distance in bytes between an element and its
    immediate neighbor along the same axis.

  **array_stride(a,i)**
    Evaluates to the ``i``-th stride of ``a``, assuming ``a`` can be
    cast to a ``PyArrayObject*``.

  **array_data(a)**
    Evaluates to a pointer of type ``void*`` that points to the data
    buffer of ``a``, assuming ``a`` can be cast to a ``PyArrayObject*``.

  **array_descr(a)**
    Returns a borrowed reference to the dtype property
    (``PyArray_Descr*``) of ``a``, assuming ``a`` can be cast to a
    ``PyArrayObject*``.

  **array_flags(a)**
    Returns an integer representing the flags of ``a``, assuming ``a``
    can be cast to a ``PyArrayObject*``.

  **array_enableflags(a,f)**
    Sets the flag represented by ``f`` of ``a``, assuming ``a`` can be
    cast to a ``PyArrayObject*``.

  **array_is_contiguous(a)**
    Evaluates as true if ``a`` is a contiguous array.  Equivalent to
    ``(PyArray_ISCONTIGUOUS(a))``.

  **array_is_native(a)**
    Evaluates as true if the data buffer of ``a`` uses native byte
    order.  Equivalent to ``(PyArray_ISNOTSWAPPED(a))``.

  **array_is_fortran(a)**
    Evaluates as true if ``a`` is FORTRAN ordered.

Routines
````````

  **pytype_string()**

    Return type: ``const char*``

    Arguments:

    * ``PyObject* py_obj``, a general Python object.

    Return a string describing the type of ``py_obj``.


  **typecode_string()**

    Return type: ``const char*``

    Arguments:

    * ``int typecode``, a NumPy integer typecode.

    Return a string describing the type corresponding to the NumPy
    ``typecode``.

  **type_match()**

    Return type: ``int``

    Arguments:

    * ``int actual_type``, the NumPy typecode of a NumPy array.

    * ``int desired_type``, the desired NumPy typecode.

    Make sure that ``actual_type`` is compatible with
    ``desired_type``.  For example, this allows character and
    byte types, or int and long types, to match.  This is now
    equivalent to ``PyArray_EquivTypenums()``.


  **obj_to_array_no_conversion()**

    Return type: ``PyArrayObject*``

    Arguments:

    * ``PyObject* input``, a general Python object.

    * ``int typecode``, the desired NumPy typecode.

    Cast ``input`` to a ``PyArrayObject*`` if legal, and ensure that
    it is of type ``typecode``.  If ``input`` cannot be cast, or the
    ``typecode`` is wrong, set a Python error and return ``NULL``.


  **obj_to_array_allow_conversion()**

    Return type: ``PyArrayObject*``

    Arguments:

    * ``PyObject* input``, a general Python object.

    * ``int typecode``, the desired NumPy typecode of the resulting
      array.

    * ``int* is_new_object``, returns a value of 0 if no conversion
      performed, else 1.

    Convert ``input`` to a NumPy array with the given ``typecode``.
    On success, return a valid ``PyArrayObject*`` with the correct
    type.  On failure, the Python error string will be set and the
    routine returns ``NULL``.


  **make_contiguous()**

    Return type: ``PyArrayObject*``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    * ``int* is_new_object``, returns a value of 0 if no conversion
      performed, else 1.

    * ``int min_dims``, minimum allowable dimensions.

    * ``int max_dims``, maximum allowable dimensions.

    Check to see if ``ary`` is contiguous.  If so, return the input
    pointer and flag it as not a new object.  If it is not contiguous,
    create a new ``PyArrayObject*`` using the original data, flag it
    as a new object and return the pointer.


  **make_fortran()**

    Return type: ``PyArrayObject*``

    Arguments

    * ``PyArrayObject* ary``, a NumPy array.

    * ``int* is_new_object``, returns a value of 0 if no conversion
      performed, else 1.

    Check to see if ``ary`` is Fortran contiguous.  If so, return the
    input pointer and flag it as not a new object.  If it is not
    Fortran contiguous, create a new ``PyArrayObject*`` using the
    original data, flag it as a new object and return the pointer.


  **obj_to_array_contiguous_allow_conversion()**

    Return type: ``PyArrayObject*``

    Arguments:

    * ``PyObject* input``, a general Python object.

    * ``int typecode``, the desired NumPy typecode of the resulting
      array.

    * ``int* is_new_object``, returns a value of 0 if no conversion
      performed, else 1.

    Convert ``input`` to a contiguous ``PyArrayObject*`` of the
    specified type.  If the input object is not a contiguous
    ``PyArrayObject*``, a new one will be created and the new object
    flag will be set.


  **obj_to_array_fortran_allow_conversion()**

    Return type: ``PyArrayObject*``

    Arguments:

    * ``PyObject* input``, a general Python object.

    * ``int typecode``, the desired NumPy typecode of the resulting
      array.

    * ``int* is_new_object``, returns a value of 0 if no conversion
      performed, else 1.

    Convert ``input`` to a Fortran contiguous ``PyArrayObject*`` of
    the specified type.  If the input object is not a Fortran
    contiguous ``PyArrayObject*``, a new one will be created and the
    new object flag will be set.


  **require_contiguous()**

    Return type: ``int``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    Test whether ``ary`` is contiguous.  If so, return 1.  Otherwise,
    set a Python error and return 0.


  **require_native()**

    Return type: ``int``

    Arguments:

    * ``PyArray_Object* ary``, a NumPy array.

    Require that ``ary`` is not byte-swapped.  If the array is not
    byte-swapped, return 1.  Otherwise, set a Python error and
    return 0.

  **require_dimensions()**

    Return type: ``int``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    * ``int exact_dimensions``, the desired number of dimensions.

    Require ``ary`` to have a specified number of dimensions.  If the
    array has the specified number of dimensions, return 1.
    Otherwise, set a Python error and return 0.


  **require_dimensions_n()**

    Return type: ``int``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    * ``int* exact_dimensions``, an array of integers representing
      acceptable numbers of dimensions.

    * ``int n``, the length of ``exact_dimensions``.

    Require ``ary`` to have one of a list of specified number of
    dimensions.  If the array has one of the specified number of
    dimensions, return 1.  Otherwise, set the Python error string
    and return 0.


  **require_size()**

    Return type: ``int``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    * ``npy_int* size``, an array representing the desired lengths of
      each dimension.

    * ``int n``, the length of ``size``.

    Require ``ary`` to have a specified shape.  If the array has the
    specified shape, return 1.  Otherwise, set the Python error
    string and return 0.


  **require_fortran()**

    Return type: ``int``

    Arguments:

    * ``PyArrayObject* ary``, a NumPy array.

    Require the given ``PyArrayObject`` to be Fortran ordered.  If
    the ``PyArrayObject`` is already Fortran ordered, do nothing.
    Else, set the Fortran ordering flag and recompute the strides.


Beyond the Provided Typemaps
----------------------------

There are many C or C++ array/NumPy array situations not covered by
a simple ``%include "numpy.i"`` and subsequent ``%apply`` directives.

A Common Example
````````````````

Consider a reasonable prototype for a dot product function::

    double dot(int len, double* vec1, double* vec2);

The Python interface that we want is::

    def dot(vec1, vec2):
        """
        dot(PyObject,PyObject) -> double
        """

The problem here is that there is one dimension argument and two array
arguments, and our typemaps are set up for dimensions that apply to a
single array (in fact, `SWIG`_ does not provide a mechanism for
associating ``len`` with ``vec2`` that takes two Python input
arguments).  The recommended solution is the following::

    %apply (int DIM1, double* IN_ARRAY1) {(int len1, double* vec1),
                                          (int len2, double* vec2)}
    %rename (dot) my_dot;
    %exception my_dot {
        $action
	if (PyErr_Occurred()) SWIG_fail;
    }
    %inline %{
    double my_dot(int len1, double* vec1, int len2, double* vec2) {
        if (len1 != len2) {
	    PyErr_Format(PyExc_ValueError,
                         "Arrays of lengths (%d,%d) given",
                         len1, len2);
	    return 0.0;
        }
        return dot(len1, vec1, vec2);
    }
    %}

If the header file that contains the prototype for ``double dot()``
also contains other prototypes that you want to wrap, so that you need
to ``%include`` this header file, then you will also need a ``%ignore
dot;`` directive, placed after the ``%rename`` and before the
``%include`` directives.  Or, if the function in question is a class
method, you will want to use ``%extend`` rather than ``%inline`` in
addition to ``%ignore``.

**A note on error handling:** Note that ``my_dot`` returns a
``double`` but that it can also raise a Python error.  The
resulting wrapper function will return a Python float
representation of 0.0 when the vector lengths do not match.  Since
this is not ``NULL``, the Python interpreter will not know to check
for an error.  For this reason, we add the ``%exception`` directive
above for ``my_dot`` to get the behavior we want (note that
``$action`` is a macro that gets expanded to a valid call to
``my_dot``).  In general, you will probably want to write a `SWIG`_
macro to perform this task.

Other Situations
````````````````

There are other wrapping situations in which ``numpy.i`` may be
helpful when you encounter them.

  * In some situations, it is possible that you could use the
    ``%numpy_typemaps`` macro to implement typemaps for your own
    types.  See the `Other Common Types: bool`_ or `Other Common
    Types: complex`_ sections for examples.  Another situation is if
    your dimensions are of a type other than ``int`` (say ``long`` for
    example)::

        %numpy_typemaps(double, NPY_DOUBLE, long)

  * You can use the code in ``numpy.i`` to write your own typemaps.
    For example, if you had a five-dimensional array as a function
    argument, you could cut-and-paste the appropriate four-dimensional
    typemaps into your interface file.  The modifications for the
    fourth dimension would be trivial.

  * Sometimes, the best approach is to use the ``%extend`` directive
    to define new methods for your classes (or overload existing ones)
    that take a ``PyObject*`` (that either is or can be converted to a
    ``PyArrayObject*``) instead of a pointer to a buffer.  In this
    case, the helper routines in ``numpy.i`` can be very useful.

  * Writing typemaps can be a bit nonintuitive.  If you have specific
    questions about writing `SWIG`_ typemaps for NumPy, the
    developers of ``numpy.i`` do monitor the
    `Numpy-discussion <mailto:Numpy-discussion@python.org>`_ and
    `Swig-user <mailto:Swig-user@lists.sourceforge.net>`_ mail lists.

A Final Note
````````````

When you use the ``%apply`` directive, as is usually necessary to use
``numpy.i``, it will remain in effect until you tell `SWIG`_ that it
shouldn't be.  If the arguments to the functions or methods that you
are wrapping have common names, such as ``length`` or ``vector``,
these typemaps may get applied in situations you do not expect or
want.  Therefore, it is always a good idea to add a ``%clear``
directive after you are done with a specific typemap::

    %apply (double* IN_ARRAY1, int DIM1) {(double* vector, int length)}
    %include "my_header.h"
    %clear (double* vector, int length);

In general, you should target these typemap signatures specifically
where you want them, and then clear them after you are done.

Summary
-------

Out of the box, ``numpy.i`` provides typemaps that support conversion
between NumPy arrays and C arrays:

  * That can be one of 12 different scalar types: ``signed char``,
    ``unsigned char``, ``short``, ``unsigned short``, ``int``,
    ``unsigned int``, ``long``, ``unsigned long``, ``long long``,
    ``unsigned long long``, ``float`` and ``double``.

  * That support 74 different argument signatures for each data type,
    including:

    + One-dimensional, two-dimensional, three-dimensional and
      four-dimensional arrays.

    + Input-only, in-place, argout, argoutview, and memory managed
      argoutview behavior.

    + Hard-coded dimensions, data-buffer-then-dimensions
      specification, and dimensions-then-data-buffer specification.

    + Both C-ordering ("last dimension fastest") or Fortran-ordering
      ("first dimension fastest") support for 2D, 3D and 4D arrays.

The ``numpy.i`` interface file also provides additional tools for
wrapper developers, including:

  * A `SWIG`_ macro (``%numpy_typemaps``) with three arguments for
    implementing the 74 argument signatures for the user's choice of
    (1) C data type, (2) NumPy data type (assuming they match), and
    (3) dimension type.

  * Fourteen C macros and fifteen C functions that can be used to
    write specialized typemaps, extensions, or inlined functions that
    handle cases not covered by the provided typemaps.  Note that the
    macros and functions are coded specifically to work with the NumPy
    C/API regardless of NumPy version number, both before and after
    the deprecation of some aspects of the API after version 1.6.
.. _routines.help:

NumPy-specific help functions
=============================

.. currentmodule:: numpy

Finding help
------------

.. autosummary::
   :toctree: generated/

   lookfor


Reading help
------------

.. autosummary::
   :toctree: generated/

   info
   source
.. module:: numpy.matlib

Matrix library (:mod:`numpy.matlib`)
************************************

.. currentmodule:: numpy

This module contains all functions in the :mod:`numpy` namespace, with
the following replacement functions that return :class:`matrices
<matrix>` instead of :class:`ndarrays <ndarray>`.

.. currentmodule:: numpy

Functions that are also in the numpy namespace and return matrices

.. autosummary::

   mat
   matrix
   asmatrix
   bmat


Replacement functions in `matlib`

.. currentmodule:: numpy.matlib

.. autosummary::
   :toctree: generated/

   empty
   zeros
   ones
   eye
   identity
   repmat
   rand
   randn
.. module:: numpy.testing

Test Support (:mod:`numpy.testing`)
===================================

.. currentmodule:: numpy.testing

Common test support for all numpy test scripts.

This single module should provide all the common functionality for numpy
tests in a single location, so that :ref:`test scripts
<development-environment>` can just import it and work right away. For
background, see the :ref:`testing-guidelines`


Asserts
-------
.. autosummary::
   :toctree: generated/

   assert_allclose
   assert_array_almost_equal_nulp
   assert_array_max_ulp
   assert_array_equal
   assert_array_less
   assert_equal
   assert_raises
   assert_raises_regex
   assert_warns
   assert_string_equal

Asserts (not recommended)
-------------------------
It is recommended to use one of `assert_allclose`,
`assert_array_almost_equal_nulp` or `assert_array_max_ulp` instead of these
functions for more consistent floating point comparisons.

.. autosummary::
   :toctree: generated/

   assert_almost_equal
   assert_approx_equal
   assert_array_almost_equal

Decorators
----------
.. autosummary::
   :toctree: generated/

   dec.deprecated
   dec.knownfailureif
   dec.setastest
   dec.skipif
   dec.slow
   decorate_methods

Test Running
------------
.. autosummary::
   :toctree: generated/

   Tester
   run_module_suite
   rundocs
   suppress_warnings

Guidelines
----------

.. toctree::

   testing
.. _arrays.classes:

#########################
Standard array subclasses
#########################

.. currentmodule:: numpy

.. for doctests
   >>> np.random.seed(1)

.. note::

    Subclassing a ``numpy.ndarray`` is possible but if your goal is to create
    an array with *modified* behavior, as do dask arrays for distributed
    computation and cupy arrays for GPU-based computation, subclassing is
    discouraged. Instead, using numpy's
    :ref:`dispatch mechanism <basics.dispatch>` is recommended.

The :class:`ndarray` can be inherited from (in Python or in C)
if desired. Therefore, it can form a foundation for many useful
classes. Often whether to sub-class the array object or to simply use
the core array component as an internal part of a new class is a
difficult decision, and can be simply a matter of choice. NumPy has
several tools for simplifying how your new object interacts with other
array objects, and so the choice may not be significant in the
end. One way to simplify the question is by asking yourself if the
object you are interested in can be replaced as a single array or does
it really require two or more arrays at its core.

Note that :func:`asarray` always returns the base-class ndarray. If
you are confident that your use of the array object can handle any
subclass of an ndarray, then :func:`asanyarray` can be used to allow
subclasses to propagate more cleanly through your subroutine. In
principal a subclass could redefine any aspect of the array and
therefore, under strict guidelines, :func:`asanyarray` would rarely be
useful. However, most subclasses of the array object will not
redefine certain aspects of the array object such as the buffer
interface, or the attributes of the array. One important example,
however, of why your subroutine may not be able to handle an arbitrary
subclass of an array is that matrices redefine the "*" operator to be
matrix-multiplication, rather than element-by-element multiplication.


Special attributes and methods
==============================

.. seealso:: :ref:`Subclassing ndarray <basics.subclassing>`

NumPy provides several hooks that classes can customize:

.. py:method:: class.__array_ufunc__(ufunc, method, *inputs, **kwargs)

   .. versionadded:: 1.13

   Any class, ndarray subclass or not, can define this method or set it to
   None in order to override the behavior of NumPy's ufuncs. This works
   quite similarly to Python's ``__mul__`` and other binary operation routines.

   - *ufunc* is the ufunc object that was called.
   - *method* is a string indicating which Ufunc method was called
     (one of ``"__call__"``, ``"reduce"``, ``"reduceat"``,
     ``"accumulate"``, ``"outer"``, ``"inner"``).
   - *inputs* is a tuple of the input arguments to the ``ufunc``.
   - *kwargs* is a dictionary containing the optional input arguments
     of the ufunc. If given, any ``out`` arguments, both positional
     and keyword, are passed as a :obj:`tuple` in *kwargs*. See the
     discussion in :ref:`ufuncs` for details.

   The method should return either the result of the operation, or
   :obj:`NotImplemented` if the operation requested is not implemented.

   If one of the input or output arguments has a :func:`__array_ufunc__`
   method, it is executed *instead* of the ufunc.  If more than one of the
   arguments implements :func:`__array_ufunc__`, they are tried in the
   order: subclasses before superclasses, inputs before outputs, otherwise
   left to right. The first routine returning something other than
   :obj:`NotImplemented` determines the result. If all of the
   :func:`__array_ufunc__` operations return :obj:`NotImplemented`, a
   :exc:`TypeError` is raised.

   .. note:: We intend to re-implement numpy functions as (generalized)
       Ufunc, in which case it will become possible for them to be
       overridden by the ``__array_ufunc__`` method.  A prime candidate is
       :func:`~numpy.matmul`, which currently is not a Ufunc, but could be
       relatively easily be rewritten as a (set of) generalized Ufuncs. The
       same may happen with functions such as :func:`~numpy.median`,
       :func:`~numpy.amin`, and :func:`~numpy.argsort`.

   Like with some other special methods in python, such as ``__hash__`` and
   ``__iter__``, it is possible to indicate that your class does *not*
   support ufuncs by setting ``__array_ufunc__ = None``. Ufuncs always raise
   :exc:`TypeError` when called on an object that sets
   ``__array_ufunc__ = None``.

   The presence of :func:`__array_ufunc__` also influences how
   :class:`ndarray` handles binary operations like ``arr + obj`` and ``arr
   < obj`` when ``arr`` is an :class:`ndarray` and ``obj`` is an instance
   of a custom class. There are two possibilities. If
   ``obj.__array_ufunc__`` is present and not None, then
   ``ndarray.__add__`` and friends will delegate to the ufunc machinery,
   meaning that ``arr + obj`` becomes ``np.add(arr, obj)``, and then
   :func:`~numpy.add` invokes ``obj.__array_ufunc__``. This is useful if you
   want to define an object that acts like an array.

   Alternatively, if ``obj.__array_ufunc__`` is set to None, then as a
   special case, special methods like ``ndarray.__add__`` will notice this
   and *unconditionally* raise :exc:`TypeError`. This is useful if you want to
   create objects that interact with arrays via binary operations, but
   are not themselves arrays. For example, a units handling system might have
   an object ``m`` representing the "meters" unit, and want to support the
   syntax ``arr * m`` to represent that the array has units of "meters", but
   not want to otherwise interact with arrays via ufuncs or otherwise. This
   can be done by setting ``__array_ufunc__ = None`` and defining ``__mul__``
   and ``__rmul__`` methods. (Note that this means that writing an
   ``__array_ufunc__`` that always returns :obj:`NotImplemented` is not
   quite the same as setting ``__array_ufunc__ = None``: in the former
   case, ``arr + obj`` will raise :exc:`TypeError`, while in the latter
   case it is possible to define a ``__radd__`` method to prevent this.)

   The above does not hold for in-place operators, for which :class:`ndarray`
   never returns :obj:`NotImplemented`.  Hence, ``arr += obj`` would always
   lead to a :exc:`TypeError`.  This is because for arrays in-place operations
   cannot generically be replaced by a simple reverse operation.  (For
   instance, by default, ``arr += obj`` would be translated to ``arr =
   arr + obj``, i.e., ``arr`` would be replaced, contrary to what is expected
   for in-place array operations.)

   .. note:: If you define ``__array_ufunc__``:

      - If you are not a subclass of :class:`ndarray`, we recommend your
        class define special methods like ``__add__`` and ``__lt__`` that
        delegate to ufuncs just like ndarray does.  An easy way to do this
        is to subclass from :class:`~numpy.lib.mixins.NDArrayOperatorsMixin`.
      - If you subclass :class:`ndarray`, we recommend that you put all your
        override logic in ``__array_ufunc__`` and not also override special
        methods. This ensures the class hierarchy is determined in only one
        place rather than separately by the ufunc machinery and by the binary
        operation rules (which gives preference to special methods of
        subclasses; the alternative way to enforce a one-place only hierarchy,
        of setting :func:`__array_ufunc__` to None, would seem very
        unexpected and thus confusing, as then the subclass would not work at
        all with ufuncs).
      - :class:`ndarray` defines its own :func:`__array_ufunc__`, which,
        evaluates the ufunc if no arguments have overrides, and returns
        :obj:`NotImplemented` otherwise. This may be useful for subclasses
        for which :func:`__array_ufunc__` converts any instances of its own
        class to :class:`ndarray`: it can then pass these on to its
        superclass using ``super().__array_ufunc__(*inputs, **kwargs)``,
        and finally return the results after possible back-conversion. The
        advantage of this practice is that it ensures that it is possible
        to have a hierarchy of subclasses that extend the behaviour. See
        :ref:`Subclassing ndarray <basics.subclassing>` for details.

   .. note:: If a class defines the :func:`__array_ufunc__` method,
      this disables the :func:`__array_wrap__`,
      :func:`__array_prepare__`, :data:`__array_priority__` mechanism
      described below for ufuncs (which may eventually be deprecated).

.. py:method:: class.__array_function__(func, types, args, kwargs)

   .. versionadded:: 1.16

   .. note::

       - In NumPy 1.17, the protocol is enabled by default, but can be disabled
         with ``NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0``.
       - In NumPy 1.16, you need to set the environment variable
         ``NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1`` before importing NumPy to use
         NumPy function overrides.
       - Eventually, expect to ``__array_function__`` to always be enabled.

   -  ``func`` is an arbitrary callable exposed by NumPy's public API,
      which was called in the form ``func(*args, **kwargs)``.
   -  ``types`` is a collection :py:class:`collections.abc.Collection`
      of unique argument types from the original NumPy function call that
      implement ``__array_function__``.
   -  The tuple ``args`` and dict ``kwargs`` are directly passed on from the
      original call.

   As a convenience for ``__array_function__`` implementors, ``types``
   provides all argument types with an ``'__array_function__'`` attribute.
   This allows implementors to quickly identify cases where they should defer
   to ``__array_function__`` implementations on other arguments.
   Implementations should not rely on the iteration order of ``types``.

   Most implementations of ``__array_function__`` will start with two
   checks:

   1.  Is the given function something that we know how to overload?
   2.  Are all arguments of a type that we know how to handle?

   If these conditions hold, ``__array_function__`` should return the result
   from calling its implementation for ``func(*args, **kwargs)``.  Otherwise,
   it should return the sentinel value ``NotImplemented``, indicating that the
   function is not implemented by these types.

   There are no general requirements on the return value from
   ``__array_function__``, although most sensible implementations should
   probably return array(s) with the same type as one of the function's
   arguments.

   It may also be convenient to define a custom decorators (``implements``
   below) for registering ``__array_function__`` implementations.

   .. code:: python

       HANDLED_FUNCTIONS = {}

       class MyArray:
           def __array_function__(self, func, types, args, kwargs):
               if func not in HANDLED_FUNCTIONS:
                   return NotImplemented
               # Note: this allows subclasses that don't override
               # __array_function__ to handle MyArray objects
               if not all(issubclass(t, MyArray) for t in types):
                   return NotImplemented
               return HANDLED_FUNCTIONS[func](*args, **kwargs)

       def implements(numpy_function):
           """Register an __array_function__ implementation for MyArray objects."""
           def decorator(func):
               HANDLED_FUNCTIONS[numpy_function] = func
               return func
           return decorator

       @implements(np.concatenate)
       def concatenate(arrays, axis=0, out=None):
           ...  # implementation of concatenate for MyArray objects

       @implements(np.broadcast_to)
       def broadcast_to(array, shape):
           ...  # implementation of broadcast_to for MyArray objects

   Note that it is not required for ``__array_function__`` implementations to
   include *all* of the corresponding NumPy function's optional arguments
   (e.g., ``broadcast_to`` above omits the irrelevant ``subok`` argument).
   Optional arguments are only passed in to ``__array_function__`` if they
   were explicitly used in the NumPy function call.

   Just like the case for builtin special methods like ``__add__``, properly
   written ``__array_function__`` methods should always return
   ``NotImplemented`` when an unknown type is encountered. Otherwise, it will
   be impossible to correctly override NumPy functions from another object
   if the operation also includes one of your objects.

   For the most part, the rules for dispatch with ``__array_function__``
   match those for ``__array_ufunc__``. In particular:

   -  NumPy will gather implementations of ``__array_function__`` from all
      specified inputs and call them in order: subclasses before
      superclasses, and otherwise left to right. Note that in some edge cases
      involving subclasses, this differs slightly from the
      `current behavior <https://bugs.python.org/issue30140>`_ of Python.
   -  Implementations of ``__array_function__`` indicate that they can
      handle the operation by returning any value other than
      ``NotImplemented``.
   -  If all ``__array_function__`` methods return ``NotImplemented``,
      NumPy will raise ``TypeError``.

   If no ``__array_function__`` methods exists, NumPy will default to calling
   its own implementation, intended for use on NumPy arrays. This case arises,
   for example, when all array-like arguments are Python numbers or lists.
   (NumPy arrays do have a ``__array_function__`` method, given below, but it
   always returns ``NotImplemented`` if any argument other than a NumPy array
   subclass implements ``__array_function__``.)

   One deviation from the current behavior of ``__array_ufunc__`` is that
   NumPy will only call ``__array_function__`` on the *first* argument of each
   unique type. This matches Python's `rule for calling reflected methods
   <https://docs.python.org/3/reference/datamodel.html#object.__ror__>`_, and
   this ensures that checking overloads has acceptable performance even when
   there are a large number of overloaded arguments.

.. py:method:: class.__array_finalize__(obj)

   This method is called whenever the system internally allocates a
   new array from *obj*, where *obj* is a subclass (subtype) of the
   :class:`ndarray`. It can be used to change attributes of *self*
   after construction (so as to ensure a 2-d matrix for example), or
   to update meta-information from the "parent." Subclasses inherit
   a default implementation of this method that does nothing.

.. py:method:: class.__array_prepare__(array, context=None)

   At the beginning of every :ref:`ufunc <ufuncs-output-type>`, this
   method is called on the input object with the highest array
   priority, or the output object if one was specified. The output
   array is passed in and whatever is returned is passed to the ufunc.
   Subclasses inherit a default implementation of this method which
   simply returns the output array unmodified. Subclasses may opt to
   use this method to transform the output array into an instance of
   the subclass and update metadata before returning the array to the
   ufunc for computation.

   .. note:: For ufuncs, it is hoped to eventually deprecate this method in
             favour of :func:`__array_ufunc__`.

.. py:method:: class.__array_wrap__(array, context=None)

   At the end of every :ref:`ufunc <ufuncs-output-type>`, this method
   is called on the input object with the highest array priority, or
   the output object if one was specified. The ufunc-computed array
   is passed in and whatever is returned is passed to the user.
   Subclasses inherit a default implementation of this method, which
   transforms the array into a new instance of the object's class.
   Subclasses may opt to use this method to transform the output array
   into an instance of the subclass and update metadata before
   returning the array to the user.

   .. note:: For ufuncs, it is hoped to eventually deprecate this method in
             favour of :func:`__array_ufunc__`.

.. py:attribute:: class.__array_priority__

   The value of this attribute is used to determine what type of
   object to return in situations where there is more than one
   possibility for the Python type of the returned object. Subclasses
   inherit a default value of 0.0 for this attribute.

   .. note:: For ufuncs, it is hoped to eventually deprecate this method in
             favour of :func:`__array_ufunc__`.

.. py:method:: class.__array__([dtype])

   If a class (ndarray subclass or not) having the :func:`__array__`
   method is used as the output object of an :ref:`ufunc
   <ufuncs-output-type>`, results will *not* be written to the object
   returned by :func:`__array__`. This practice will return ``TypeError``.


.. _matrix-objects:

Matrix objects
==============

.. index::
   single: matrix

.. note::
   It is strongly advised *not* to use the matrix subclass.  As described
   below, it makes writing functions that deal consistently with matrices
   and regular arrays very difficult. Currently, they are mainly used for
   interacting with ``scipy.sparse``. We hope to provide an alternative
   for this use, however, and eventually remove the ``matrix`` subclass.

:class:`matrix` objects inherit from the ndarray and therefore, they
have the same attributes and methods of ndarrays. There are six
important differences of matrix objects, however, that may lead to
unexpected results when you use matrices but expect them to act like
arrays:

1. Matrix objects can be created using a string notation to allow
   Matlab-style syntax where spaces separate columns and semicolons
   (';') separate rows.

2. Matrix objects are always two-dimensional. This has far-reaching
   implications, in that m.ravel() is still two-dimensional (with a 1
   in the first dimension) and item selection returns two-dimensional
   objects so that sequence behavior is fundamentally different than
   arrays.

3. Matrix objects over-ride multiplication to be
   matrix-multiplication. **Make sure you understand this for
   functions that you may want to receive matrices. Especially in
   light of the fact that asanyarray(m) returns a matrix when m is
   a matrix.**

4. Matrix objects over-ride power to be matrix raised to a power. The
   same warning about using power inside a function that uses
   asanyarray(...) to get an array object holds for this fact.

5. The default __array_priority\__ of matrix objects is 10.0, and
   therefore mixed operations with ndarrays always produce matrices.

6. Matrices have special attributes which make calculations easier.
   These are

   .. autosummary::
      :toctree: generated/

      matrix.T
      matrix.H
      matrix.I
      matrix.A

.. warning::

    Matrix objects over-ride multiplication, '*', and power, '**', to
    be matrix-multiplication and matrix power, respectively. If your
    subroutine can accept sub-classes and you do not convert to base-
    class arrays, then you must use the ufuncs multiply and power to
    be sure that you are performing the correct operation for all
    inputs.

The matrix class is a Python subclass of the ndarray and can be used
as a reference for how to construct your own subclass of the ndarray.
Matrices can be created from other matrices, strings, and anything
else that can be converted to an ``ndarray`` . The name "mat "is an
alias for "matrix "in NumPy.

.. autosummary::
   :toctree: generated/

   matrix
   asmatrix
   bmat

Example 1: Matrix creation from a string

>>> a = np.mat('1 2 3; 4 5 3')
>>> print((a*a.T).I)
    [[ 0.29239766 -0.13450292]
     [-0.13450292  0.08187135]]


Example 2: Matrix creation from nested sequence

>>> np.mat([[1,5,10],[1.0,3,4j]])
matrix([[  1.+0.j,   5.+0.j,  10.+0.j],
        [  1.+0.j,   3.+0.j,   0.+4.j]])

Example 3: Matrix creation from an array

>>> np.mat(np.random.rand(3,3)).T
matrix([[4.17022005e-01, 3.02332573e-01, 1.86260211e-01],
        [7.20324493e-01, 1.46755891e-01, 3.45560727e-01],
        [1.14374817e-04, 9.23385948e-02, 3.96767474e-01]])


Memory-mapped file arrays
=========================

.. index::
   single: memory maps

.. currentmodule:: numpy

Memory-mapped files are useful for reading and/or modifying small
segments of a large file with regular layout, without reading the
entire file into memory. A simple subclass of the ndarray uses a
memory-mapped file for the data buffer of the array. For small files,
the over-head of reading the entire file into memory is typically not
significant, however for large files using memory mapping can save
considerable resources.

Memory-mapped-file arrays have one additional method (besides those
they inherit from the ndarray): :meth:`.flush() <memmap.flush>` which
must be called manually by the user to ensure that any changes to the
array actually get written to disk.

.. autosummary::
   :toctree: generated/

   memmap
   memmap.flush

Example:

>>> a = np.memmap('newfile.dat', dtype=float, mode='w+', shape=1000)
>>> a[10] = 10.0
>>> a[30] = 30.0
>>> del a
>>> b = np.fromfile('newfile.dat', dtype=float)
>>> print(b[10], b[30])
10.0 30.0
>>> a = np.memmap('newfile.dat', dtype=float)
>>> print(a[10], a[30])
10.0 30.0


Character arrays (:mod:`numpy.char`)
====================================

.. seealso:: :ref:`routines.array-creation.char`

.. index::
   single: character arrays

.. note::
   The `chararray` class exists for backwards compatibility with
   Numarray, it is not recommended for new development. Starting from numpy
   1.4, if one needs arrays of strings, it is recommended to use arrays of
   `dtype` `object_`, `bytes_` or `str_`, and use the free functions
   in the `numpy.char` module for fast vectorized string operations.

These are enhanced arrays of either :class:`str_` type or
:class:`bytes_` type.  These arrays inherit from the
:class:`ndarray`, but specially-define the operations ``+``, ``*``,
and ``%`` on a (broadcasting) element-by-element basis.  These
operations are not available on the standard :class:`ndarray` of
character type. In addition, the :class:`chararray` has all of the
standard :class:`str` (and :class:`bytes`) methods,
executing them on an element-by-element basis. Perhaps the easiest
way to create a chararray is to use :meth:`self.view(chararray)
<ndarray.view>` where *self* is an ndarray of str or unicode
data-type. However, a chararray can also be created using the
:meth:`numpy.chararray` constructor, or via the
:func:`numpy.char.array <core.defchararray.array>` function:

.. autosummary::
   :toctree: generated/

   chararray
   core.defchararray.array

Another difference with the standard ndarray of str data-type is
that the chararray inherits the feature introduced by Numarray that
white-space at the end of any element in the array will be ignored
on item retrieval and comparison operations.


.. _arrays.classes.rec:

Record arrays (:mod:`numpy.rec`)
================================

.. seealso:: :ref:`routines.array-creation.rec`, :ref:`routines.dtype`,
             :ref:`arrays.dtypes`.

NumPy provides the :class:`recarray` class which allows accessing the
fields of a structured array as attributes, and a corresponding
scalar data type object :class:`record`.

.. currentmodule:: numpy

.. autosummary::
   :toctree: generated/

   recarray
   record

Masked arrays (:mod:`numpy.ma`)
===============================

.. seealso:: :ref:`maskedarray`

Standard container class
========================

.. currentmodule:: numpy

For backward compatibility and as a standard "container "class, the
UserArray from Numeric has been brought over to NumPy and named
:class:`numpy.lib.user_array.container` The container class is a
Python class whose self.array attribute is an ndarray. Multiple
inheritance is probably easier with numpy.lib.user_array.container
than with the ndarray itself and so it is included by default. It is
not documented here beyond mentioning its existence because you are
encouraged to use the ndarray class directly if you can.

.. autosummary::
   :toctree: generated/

   numpy.lib.user_array.container

.. index::
   single: user_array
   single: container class


Array Iterators
===============

.. currentmodule:: numpy

.. index::
   single: array iterator

Iterators are a powerful concept for array processing. Essentially,
iterators implement a generalized for-loop. If *myiter* is an iterator
object, then the Python code::

    for val in myiter:
        ...
        some code involving val
        ...

calls ``val = next(myiter)`` repeatedly until :exc:`StopIteration` is
raised by the iterator. There are several ways to iterate over an
array that may be useful: default iteration, flat iteration, and
:math:`N`-dimensional enumeration.


Default iteration
-----------------

The default iterator of an ndarray object is the default Python
iterator of a sequence type. Thus, when the array object itself is
used as an iterator. The default behavior is equivalent to::

    for i in range(arr.shape[0]):
        val = arr[i]

This default iterator selects a sub-array of dimension :math:`N-1`
from the array. This can be a useful construct for defining recursive
algorithms. To loop over the entire array requires :math:`N` for-loops.

>>> a = np.arange(24).reshape(3,2,4)+10
>>> for val in a:
...     print('item:', val)
item: [[10 11 12 13]
 [14 15 16 17]]
item: [[18 19 20 21]
 [22 23 24 25]]
item: [[26 27 28 29]
 [30 31 32 33]]


Flat iteration
--------------

.. autosummary::
   :toctree: generated/

   ndarray.flat

As mentioned previously, the flat attribute of ndarray objects returns
an iterator that will cycle over the entire array in C-style
contiguous order.

>>> for i, val in enumerate(a.flat):
...     if i%5 == 0: print(i, val)
0 10
5 15
10 20
15 25
20 30

Here, I've used the built-in enumerate iterator to return the iterator
index as well as the value.


N-dimensional enumeration
-------------------------

.. autosummary::
   :toctree: generated/

   ndenumerate

Sometimes it may be useful to get the N-dimensional index while
iterating. The ndenumerate iterator can achieve this.

>>> for i, val in np.ndenumerate(a):
...     if sum(i)%5 == 0: print(i, val)
(0, 0, 0) 10
(1, 1, 3) 25
(2, 0, 3) 29
(2, 1, 2) 32


Iterator for broadcasting
-------------------------

.. autosummary::
   :toctree: generated/

   broadcast

The general concept of broadcasting is also available from Python
using the :class:`broadcast` iterator. This object takes :math:`N`
objects as inputs and returns an iterator that returns tuples
providing each of the input sequence elements in the broadcasted
result.

>>> for val in np.broadcast([[1,0],[2,3]],[0,1]):
...     print(val)
(1, 0)
(0, 1)
(2, 0)
(3, 1)
Using the Convenience Classes
=============================

The convenience classes provided by the polynomial package are:

         ============    ================
         Name            Provides
         ============    ================
         Polynomial      Power series
         Chebyshev       Chebyshev series
         Legendre        Legendre series
         Laguerre        Laguerre series
         Hermite         Hermite series
         HermiteE        HermiteE series
         ============    ================

The series in this context are finite sums of the corresponding polynomial
basis functions multiplied by coefficients. For instance, a power series
looks like

.. math:: p(x) = 1 + 2x + 3x^2

and has coefficients :math:`[1, 2, 3]`. The Chebyshev series with the
same coefficients looks like


.. math:: p(x) = 1 T_0(x) + 2 T_1(x) + 3 T_2(x)

and more generally

.. math:: p(x) = \sum_{i=0}^n c_i T_i(x)

where in this case the :math:`T_n` are the Chebyshev functions of
degree :math:`n`, but could just as easily be the basis functions of
any of the other classes. The convention for all the classes is that
the coefficient :math:`c[i]` goes with the basis function of degree i.

All of the classes are immutable and have the same methods, and
especially they implement the Python numeric operators +, -, \*, //, %,
divmod, \*\*, ==, and !=. The last two can be a bit problematic due to
floating point roundoff errors. We now give a quick demonstration of the
various operations using NumPy version 1.7.0.

Basics
------

First we need a polynomial class and a polynomial instance to play with.
The classes can be imported directly from the polynomial package or from
the module of the relevant type. Here we import from the package and use
the conventional Polynomial class because of its familiarity::

   >>> from numpy.polynomial import Polynomial as P
   >>> p = P([1,2,3])
   >>> p
   Polynomial([1., 2., 3.], domain=[-1,  1], window=[-1,  1])

Note that there are three parts to the long version of the printout. The
first is the coefficients, the second is the domain, and the third is the
window::

   >>> p.coef
   array([1., 2., 3.])
   >>> p.domain
   array([-1,  1])
   >>> p.window
   array([-1,  1])

Printing a polynomial yields the polynomial expression in a more familiar
format::

   >>> print(p)
   1.0 + 2.0·x¹ + 3.0·x²

Note that the string representation of polynomials uses Unicode characters
by default (except on Windows) to express powers and subscripts. An ASCII-based
representation is also available (default on Windows). The polynomial string
format can be toggled at the package-level with the 
`~numpy.polynomial.set_default_printstyle` function::

   >>> np.polynomial.set_default_printstyle('ascii')
   >>> print(p)
   1.0 + 2.0 x**1 + 3.0 x**2

or controlled for individual polynomial instances with string formatting::

   >>> print(f"{p:unicode}")
   1.0 + 2.0·x¹ + 3.0·x²

We will deal with the domain and window when we get to fitting, for the moment
we ignore them and run through the basic algebraic and arithmetic operations.

Addition and Subtraction::

   >>> p + p
   Polynomial([2., 4., 6.], domain=[-1.,  1.], window=[-1.,  1.])
   >>> p - p
   Polynomial([0.], domain=[-1.,  1.], window=[-1.,  1.])

Multiplication::

   >>> p * p
   Polynomial([ 1.,   4.,  10.,  12.,   9.], domain=[-1.,  1.], window=[-1.,  1.])

Powers::

   >>> p**2
   Polynomial([ 1.,   4., 10., 12.,  9.], domain=[-1.,  1.], window=[-1.,  1.])

Division:

Floor division, '//', is the division operator for the polynomial classes,
polynomials are treated like integers in this regard. For Python versions <
3.x the '/' operator maps to '//', as it does for Python, for later
versions the '/' will only work for division by scalars. At some point it
will be deprecated::

   >>> p // P([-1, 1])
   Polynomial([5.,  3.], domain=[-1.,  1.], window=[-1.,  1.])

Remainder::

   >>> p % P([-1, 1])
   Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.])

Divmod::

   >>> quo, rem = divmod(p, P([-1, 1]))
   >>> quo
   Polynomial([5.,  3.], domain=[-1.,  1.], window=[-1.,  1.])
   >>> rem
   Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.])

Evaluation::

   >>> x = np.arange(5)
   >>> p(x)
   array([  1.,   6.,  17.,  34.,  57.])
   >>> x = np.arange(6).reshape(3,2)
   >>> p(x)
   array([[ 1.,   6.],
          [17.,  34.],
          [57.,  86.]])

Substitution:

Substitute a polynomial for x and expand the result. Here we substitute
p in itself leading to a new polynomial of degree 4 after expansion. If
the polynomials are regarded as functions this is composition of
functions::

   >>> p(p)
   Polynomial([ 6., 16., 36., 36., 27.], domain=[-1.,  1.], window=[-1.,  1.])

Roots::

   >>> p.roots()
   array([-0.33333333-0.47140452j, -0.33333333+0.47140452j])



It isn't always convenient to explicitly use Polynomial instances, so
tuples, lists, arrays, and scalars are automatically cast in the arithmetic
operations::

   >>> p + [1, 2, 3]
   Polynomial([2., 4., 6.], domain=[-1.,  1.], window=[-1.,  1.])
   >>> [1, 2, 3] * p
   Polynomial([ 1.,  4., 10., 12.,  9.], domain=[-1.,  1.], window=[-1.,  1.])
   >>> p / 2
   Polynomial([0.5, 1. , 1.5], domain=[-1.,  1.], window=[-1.,  1.])

Polynomials that differ in domain, window, or class can't be mixed in
arithmetic::

    >>> from numpy.polynomial import Chebyshev as T
    >>> p + P([1], domain=[0,1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 213, in __add__
    TypeError: Domains differ
    >>> p + P([1], window=[0,1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 215, in __add__
    TypeError: Windows differ
    >>> p + T([1])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "<string>", line 211, in __add__
    TypeError: Polynomial types differ


But different types can be used for substitution. In fact, this is how
conversion of Polynomial classes among themselves is done for type, domain,
and window casting::

    >>> p(T([0, 1]))
    Chebyshev([2.5, 2. , 1.5], domain=[-1.,  1.], window=[-1.,  1.])

Which gives the polynomial `p` in Chebyshev form. This works because
:math:`T_1(x) = x` and substituting :math:`x` for :math:`x` doesn't change
the original polynomial. However, all the multiplications and divisions
will be done using Chebyshev series, hence the type of the result.

It is intended that all polynomial instances are immutable, therefore
augmented operations (``+=``, ``-=``, etc.) and any other functionality that
would violate the immutablity of a polynomial instance are intentionally
unimplemented.

Calculus
--------

Polynomial instances can be integrated and differentiated.::

    >>> from numpy.polynomial import Polynomial as P
    >>> p = P([2, 6])
    >>> p.integ()
    Polynomial([0., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.])
    >>> p.integ(2)
    Polynomial([0., 0., 1., 1.], domain=[-1.,  1.], window=[-1.,  1.])

The first example integrates `p` once, the second example integrates it
twice. By default, the lower bound of the integration and the integration
constant are 0, but both can be specified.::

    >>> p.integ(lbnd=-1)
    Polynomial([-1.,  2.,  3.], domain=[-1.,  1.], window=[-1.,  1.])
    >>> p.integ(lbnd=-1, k=1)
    Polynomial([0., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.])

In the first case the lower bound of the integration is set to -1 and the
integration constant is 0. In the second the constant of integration is set
to 1 as well. Differentiation is simpler since the only option is the
number of times the polynomial is differentiated::

    >>> p = P([1, 2, 3])
    >>> p.deriv(1)
    Polynomial([2., 6.], domain=[-1.,  1.], window=[-1.,  1.])
    >>> p.deriv(2)
    Polynomial([6.], domain=[-1.,  1.], window=[-1.,  1.])


Other Polynomial Constructors
-----------------------------

Constructing polynomials by specifying coefficients is just one way of
obtaining a polynomial instance, they may also be created by specifying
their roots, by conversion from other polynomial types, and by least
squares fits. Fitting is discussed in its own section, the other methods
are demonstrated below::

    >>> from numpy.polynomial import Polynomial as P
    >>> from numpy.polynomial import Chebyshev as T
    >>> p = P.fromroots([1, 2, 3])
    >>> p
    Polynomial([-6., 11., -6.,  1.], domain=[-1.,  1.], window=[-1.,  1.])
    >>> p.convert(kind=T)
    Chebyshev([-9.  , 11.75, -3.  ,  0.25], domain=[-1.,  1.], window=[-1.,  1.])

The convert method can also convert domain and window::

    >>> p.convert(kind=T, domain=[0, 1])
    Chebyshev([-2.4375 ,  2.96875, -0.5625 ,  0.03125], domain=[0.,  1.], window=[-1.,  1.])
    >>> p.convert(kind=P, domain=[0, 1])
    Polynomial([-1.875,  2.875, -1.125,  0.125], domain=[0.,  1.], window=[-1.,  1.])

In numpy versions >= 1.7.0 the `basis` and `cast` class methods are also
available. The cast method works like the convert method while the basis
method returns the basis polynomial of given degree::

    >>> P.basis(3)
    Polynomial([0., 0., 0., 1.], domain=[-1.,  1.], window=[-1.,  1.])
    >>> T.cast(p)
    Chebyshev([-9.  , 11.75, -3. ,  0.25], domain=[-1.,  1.], window=[-1.,  1.])

Conversions between types can be useful, but it is *not* recommended
for routine use. The loss of numerical precision in passing from a
Chebyshev series of degree 50 to a Polynomial series of the same degree
can make the results of numerical evaluation essentially random.

Fitting
-------

Fitting is the reason that the `domain` and `window` attributes are part of
the convenience classes. To illustrate the problem, the values of the Chebyshev
polynomials up to degree 5 are plotted below.

.. plot::

    >>> import matplotlib.pyplot as plt
    >>> from numpy.polynomial import Chebyshev as T
    >>> x = np.linspace(-1, 1, 100)
    >>> for i in range(6):
    ...     ax = plt.plot(x, T.basis(i)(x), lw=2, label=f"$T_{i}$")
    ...
    >>> plt.legend(loc="upper left")
    >>> plt.show()

In the range -1 <= `x` <= 1 they are nice, equiripple functions lying between +/- 1.
The same plots over the range -2 <= `x` <= 2 look very different:

.. plot::

    >>> import matplotlib.pyplot as plt
    >>> from numpy.polynomial import Chebyshev as T
    >>> x = np.linspace(-2, 2, 100)
    >>> for i in range(6):
    ...     ax = plt.plot(x, T.basis(i)(x), lw=2, label=f"$T_{i}$")
    ...
    >>> plt.legend(loc="lower right")
    >>> plt.show()

As can be seen, the "good" parts have shrunk to insignificance. In using
Chebyshev polynomials for fitting we want to use the region where `x` is
between -1 and 1 and that is what the `window` specifies. However, it is
unlikely that the data to be fit has all its data points in that interval,
so we use `domain` to specify the interval where the data points lie. When
the fit is done, the domain is first mapped to the window by a linear
transformation and the usual least squares fit is done using the mapped
data points. The window and domain of the fit are part of the returned series
and are automatically used when computing values, derivatives, and such. If
they aren't specified in the call the fitting routine will use the default
window and the smallest domain that holds all the data points. This is
illustrated below for a fit to a noisy sine curve.

.. plot::

    >>> import numpy as np
    >>> import matplotlib.pyplot as plt
    >>> from numpy.polynomial import Chebyshev as T
    >>> np.random.seed(11)
    >>> x = np.linspace(0, 2*np.pi, 20)
    >>> y = np.sin(x) + np.random.normal(scale=.1, size=x.shape)
    >>> p = T.fit(x, y, 5)
    >>> plt.plot(x, y, 'o')
    >>> xx, yy = p.linspace()
    >>> plt.plot(xx, yy, lw=2)
    >>> p.domain
    array([0.        ,  6.28318531])
    >>> p.window
    array([-1.,  1.])
    >>> plt.show()
**************
NumPy and SWIG
**************

.. sectionauthor:: Bill Spotz


.. toctree::
   :maxdepth: 2

   swig.interface-file
   swig.testing
.. _arrays:

*************
Array objects
*************

.. currentmodule:: numpy

NumPy provides an N-dimensional array type, the :ref:`ndarray
<arrays.ndarray>`, which describes a collection of "items" of the same
type. The items can be :ref:`indexed <arrays.indexing>` using for
example N integers.

All ndarrays are :term:`homogeneous`: every item takes up the same size
block of memory, and all blocks are interpreted in exactly the same
way. How each item in the array is to be interpreted is specified by a
separate :ref:`data-type object <arrays.dtypes>`, one of which is associated
with every array. In addition to basic types (integers, floats,
*etc.*), the data type objects can also represent data structures.

An item extracted from an array, *e.g.*, by indexing, is represented
by a Python object whose type is one of the :ref:`array scalar types
<arrays.scalars>` built in NumPy. The array scalars allow easy manipulation
of also more complicated arrangements of data.

.. figure:: figures/threefundamental.png

   **Figure**
   Conceptual diagram showing the relationship between the three
   fundamental objects used to describe the data in an array: 1) the
   ndarray itself, 2) the data-type object that describes the layout
   of a single fixed-size element of the array, 3) the array-scalar
   Python object that is returned when a single element of the array
   is accessed.



.. toctree::
   :maxdepth: 2

   arrays.ndarray
   arrays.scalars
   arrays.dtypes
   arrays.indexing
   arrays.nditer
   arrays.classes
   maskedarray
   arrays.interface
   arrays.datetime
:orphan:

****************
Memory Alignment
****************

.. This document has been moved to ../dev/alignment.rst.

This document has been moved to :ref:`alignment`.




.. currentmodule:: numpy.ma

.. _maskedarray.generic:

.. module:: numpy.ma

The :mod:`numpy.ma` module
==========================

Rationale
---------

Masked arrays are arrays that may have missing or invalid entries.
The :mod:`numpy.ma` module provides a nearly work-alike replacement for numpy
that supports data arrays with masks.



What is a masked array?
-----------------------

In many circumstances, datasets can be incomplete or tainted by the presence
of invalid data. For example, a sensor may have failed to record a data, or
recorded an invalid value. The :mod:`numpy.ma` module provides a convenient
way to address this issue, by introducing masked arrays.

A masked array is the combination of a standard :class:`numpy.ndarray` and a
mask. A mask is either :attr:`nomask`, indicating that no value of the
associated array is invalid, or an array of booleans that determines for each
element of the associated array whether the value is valid or not. When an
element of the mask is ``False``, the corresponding element of the associated
array is valid and is said to be unmasked. When an element of the mask is
``True``, the corresponding element of the associated array is said to be
masked (invalid).

The package ensures that masked entries are not used in computations.

As an illustration, let's consider the following dataset::

   >>> import numpy as np
   >>> import numpy.ma as ma
   >>> x = np.array([1, 2, 3, -1, 5])

We wish to mark the fourth entry as invalid. The easiest is to create a masked
array::

   >>> mx = ma.masked_array(x, mask=[0, 0, 0, 1, 0])

We can now compute the mean of the dataset, without taking the invalid data
into account::

   >>> mx.mean()
   2.75


The :mod:`numpy.ma` module
--------------------------


The main feature of the :mod:`numpy.ma` module is the :class:`MaskedArray`
class, which is a subclass of :class:`numpy.ndarray`. The class, its
attributes and methods are described in more details in the
:ref:`MaskedArray class <maskedarray.baseclass>` section.

The :mod:`numpy.ma` module can be used as an addition to :mod:`numpy`: ::

   >>> import numpy as np
   >>> import numpy.ma as ma

To create an array with the second element invalid, we would do::

   >>> y = ma.array([1, 2, 3], mask = [0, 1, 0])

To create a masked array where all values close to 1.e20 are invalid, we would
do::

   >>> z = ma.masked_values([1.0, 1.e20, 3.0, 4.0], 1.e20)

For a complete discussion of creation methods for masked arrays please see
section :ref:`Constructing masked arrays <maskedarray.generic.constructing>`.




Using numpy.ma
==============

.. _maskedarray.generic.constructing:

Constructing masked arrays
--------------------------

There are several ways to construct a masked array.

* A first possibility is to directly invoke the :class:`MaskedArray` class.

* A second possibility is to use the two masked array constructors,
  :func:`array` and :func:`masked_array`.

  .. autosummary::
     :toctree: generated/

     array
     masked_array


* A third option is to take the view of an existing array. In that case, the
  mask of the view is set to :attr:`nomask` if the array has no named fields,
  or an array of boolean with the same structure as the array otherwise.

     >>> x = np.array([1, 2, 3])
     >>> x.view(ma.MaskedArray)
     masked_array(data=[1, 2, 3],
                  mask=False,
            fill_value=999999)
     >>> x = np.array([(1, 1.), (2, 2.)], dtype=[('a',int), ('b', float)])
     >>> x.view(ma.MaskedArray)
     masked_array(data=[(1, 1.0), (2, 2.0)],
                  mask=[(False, False), (False, False)],
            fill_value=(999999, 1.e+20),
                 dtype=[('a', '<i8'), ('b', '<f8')])

* Yet another possibility is to use any of the following functions:

  .. autosummary::
     :toctree: generated/

     asarray
     asanyarray
     fix_invalid
     masked_equal
     masked_greater
     masked_greater_equal
     masked_inside
     masked_invalid
     masked_less
     masked_less_equal
     masked_not_equal
     masked_object
     masked_outside
     masked_values
     masked_where



Accessing the data
------------------

The underlying data of a masked array can be accessed in several ways:

* through the :attr:`~MaskedArray.data` attribute. The output is a view of the
  array as a :class:`numpy.ndarray` or one of its subclasses, depending on the
  type of the underlying data at the masked array creation.

* through the :meth:`~MaskedArray.__array__` method. The output is then a
  :class:`numpy.ndarray`.

* by directly taking a view of the masked array as a :class:`numpy.ndarray`
  or one of its subclass (which is actually what using the
  :attr:`~MaskedArray.data` attribute does).

* by using the :func:`getdata` function.


None of these methods is completely satisfactory if some entries have been
marked as invalid. As a general rule, where a representation of the array is
required without any masked entries, it is recommended to fill the array with
the :meth:`filled` method.



Accessing the mask
------------------

The mask of a masked array is accessible through its :attr:`~MaskedArray.mask`
attribute. We must keep in mind that a ``True`` entry in the mask indicates an
*invalid* data.

Another possibility is to use the :func:`getmask` and :func:`getmaskarray`
functions. ``getmask(x)`` outputs the mask of ``x`` if ``x`` is a masked
array, and the special value :data:`nomask` otherwise. ``getmaskarray(x)``
outputs the mask of ``x`` if ``x`` is a masked array. If ``x`` has no invalid
entry or is not a masked array, the function outputs  a boolean array of
``False`` with as many elements as ``x``.




Accessing only the valid entries
---------------------------------

To retrieve only the valid entries, we can use the inverse of the mask as an
index. The inverse of the mask can be calculated with the
:func:`numpy.logical_not` function or simply with the ``~`` operator::

   >>> x = ma.array([[1, 2], [3, 4]], mask=[[0, 1], [1, 0]])
   >>> x[~x.mask]
   masked_array(data=[1, 4],
                mask=[False, False],
          fill_value=999999)

Another way to retrieve the valid data is to use the :meth:`compressed`
method, which returns a one-dimensional :class:`~numpy.ndarray` (or one of its
subclasses, depending on the value of the :attr:`~MaskedArray.baseclass`
attribute)::

   >>> x.compressed()
   array([1, 4])

Note that the output of :meth:`compressed` is always 1D.



Modifying the mask
------------------

Masking an entry
~~~~~~~~~~~~~~~~

The recommended way to mark one or several specific entries of a masked array
as invalid is to assign the special value :attr:`masked` to them::

   >>> x = ma.array([1, 2, 3])
   >>> x[0] = ma.masked
   >>> x
   masked_array(data=[--, 2, 3],
                mask=[ True, False, False],
          fill_value=999999)
   >>> y = ma.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
   >>> y[(0, 1, 2), (1, 2, 0)] = ma.masked
   >>> y
   masked_array(
     data=[[1, --, 3],
           [4, 5, --],
           [--, 8, 9]],
     mask=[[False,  True, False],
           [False, False,  True],
           [ True, False, False]],
     fill_value=999999)
   >>> z = ma.array([1, 2, 3, 4])
   >>> z[:-2] = ma.masked
   >>> z
   masked_array(data=[--, --, 3, 4],
                mask=[ True,  True, False, False],
          fill_value=999999)


A second possibility is to modify the :attr:`~MaskedArray.mask` directly,
but this usage is discouraged.

.. note::
   When creating a new masked array with a simple, non-structured datatype,
   the mask is initially set to the special value :attr:`nomask`, that
   corresponds roughly to the boolean ``False``. Trying to set an element of
   :attr:`nomask` will fail with a :exc:`TypeError` exception, as a boolean
   does not support item assignment.


All the entries of an array can be masked at once by assigning ``True`` to the
mask::

   >>> x = ma.array([1, 2, 3], mask=[0, 0, 1])
   >>> x.mask = True
   >>> x
   masked_array(data=[--, --, --],
                mask=[ True,  True,  True],
          fill_value=999999,
               dtype=int64)

Finally, specific entries can be masked and/or unmasked by assigning to the
mask a sequence of booleans::

   >>> x = ma.array([1, 2, 3])
   >>> x.mask = [0, 1, 0]
   >>> x
   masked_array(data=[1, --, 3],
                mask=[False,  True, False],
          fill_value=999999)

Unmasking an entry
~~~~~~~~~~~~~~~~~~

To unmask one or several specific entries, we can just assign one or several
new valid values to them::

   >>> x = ma.array([1, 2, 3], mask=[0, 0, 1])
   >>> x
   masked_array(data=[1, 2, --],
                mask=[False, False,  True],
          fill_value=999999)
   >>> x[-1] = 5
   >>> x
   masked_array(data=[1, 2, 5],
                mask=[False, False, False],
          fill_value=999999)

.. note::
   Unmasking an entry by direct assignment will silently fail if the masked
   array has a *hard* mask, as shown by the :attr:`~MaskedArray.hardmask`
   attribute. This feature was introduced to prevent overwriting the mask.
   To force the unmasking of an entry where the array has a hard mask,
   the mask must first to be softened using the :meth:`soften_mask` method
   before the allocation. It can be re-hardened with :meth:`harden_mask`::

      >>> x = ma.array([1, 2, 3], mask=[0, 0, 1], hard_mask=True)
      >>> x
      masked_array(data=[1, 2, --],
                   mask=[False, False,  True],
             fill_value=999999)
      >>> x[-1] = 5
      >>> x
      masked_array(data=[1, 2, --],
                   mask=[False, False,  True],
             fill_value=999999)
      >>> x.soften_mask()
      masked_array(data=[1, 2, --],
                   mask=[False, False,  True],
             fill_value=999999)
      >>> x[-1] = 5
      >>> x
      masked_array(data=[1, 2, 5],
                   mask=[False, False, False],
             fill_value=999999)
      >>> x.harden_mask()
      masked_array(data=[1, 2, 5],
                   mask=[False, False, False],
             fill_value=999999)


To unmask all masked entries of a masked array (provided the mask isn't a hard
mask), the simplest solution is to assign the constant :attr:`nomask` to the
mask::

   >>> x = ma.array([1, 2, 3], mask=[0, 0, 1])
   >>> x
   masked_array(data=[1, 2, --],
                mask=[False, False,  True],
          fill_value=999999)
   >>> x.mask = ma.nomask
   >>> x
   masked_array(data=[1, 2, 3],
                mask=[False, False, False],
          fill_value=999999)


Indexing and slicing
--------------------

As a :class:`MaskedArray` is a subclass of :class:`numpy.ndarray`, it inherits
its mechanisms for indexing and slicing.

When accessing a single entry of a masked array with no named fields, the
output is either a scalar (if the corresponding entry of the mask is
``False``) or the special value :attr:`masked` (if the corresponding entry of
the mask is ``True``)::

   >>> x = ma.array([1, 2, 3], mask=[0, 0, 1])
   >>> x[0]
   1
   >>> x[-1]
   masked
   >>> x[-1] is ma.masked
   True

If the masked array has named fields, accessing a single entry returns a
:class:`numpy.void` object if none of the fields are masked, or a 0d masked
array with the same dtype as the initial array if at least one of the fields
is masked.

   >>> y = ma.masked_array([(1,2), (3, 4)],
   ...                mask=[(0, 0), (0, 1)],
   ...               dtype=[('a', int), ('b', int)])
   >>> y[0]
   (1, 2)
   >>> y[-1]
   (3, --)


When accessing a slice, the output is a masked array whose
:attr:`~MaskedArray.data` attribute is a view of the original data, and whose
mask is either :attr:`nomask` (if there was no invalid entries in the original
array) or a view of the corresponding slice of the original mask. The view is
required to ensure propagation of any modification of the mask to the original.

   >>> x = ma.array([1, 2, 3, 4, 5], mask=[0, 1, 0, 0, 1])
   >>> mx = x[:3]
   >>> mx
   masked_array(data=[1, --, 3],
                mask=[False,  True, False],
          fill_value=999999)
   >>> mx[1] = -1
   >>> mx
   masked_array(data=[1, -1, 3],
                mask=[False, False, False],
          fill_value=999999)
   >>> x.mask
   array([False, False, False, False,  True])
   >>> x.data
   array([ 1, -1,  3,  4,  5])

Accessing a field of a masked array with structured datatype returns a
:class:`MaskedArray`.

Operations on masked arrays
---------------------------

Arithmetic and comparison operations are supported by masked arrays.
As much as possible, invalid entries of a masked array are not processed,
meaning that the corresponding :attr:`~MaskedArray.data` entries
*should* be the same before and after the operation.

.. warning::
   We need to stress that this behavior may not be systematic, that masked
   data may be affected by the operation in some cases and therefore users
   should not rely on this data remaining unchanged.

The :mod:`numpy.ma` module comes with a specific implementation of most
ufuncs. Unary and binary functions that have a validity domain (such as
:func:`~numpy.log` or :func:`~numpy.divide`) return the :data:`masked`
constant whenever the input is masked or falls outside the validity domain::

   >>> ma.log([-1, 0, 1, 2])
   masked_array(data=[--, --, 0.0, 0.6931471805599453],
                mask=[ True,  True, False, False],
          fill_value=1e+20)

Masked arrays also support standard numpy ufuncs. The output is then a masked
array. The result of a unary ufunc is masked wherever the input is masked. The
result of a binary ufunc is masked wherever any of the input is masked. If the
ufunc also returns the optional context output (a 3-element tuple containing
the name of the ufunc, its arguments and its domain), the context is processed
and entries of the output masked array are masked wherever the corresponding
input fall outside the validity domain::

   >>> x = ma.array([-1, 1, 0, 2, 3], mask=[0, 0, 0, 0, 1])
   >>> np.log(x)
   masked_array(data=[--, 0.0, --, 0.6931471805599453, --],
                mask=[ True, False,  True, False,  True],
          fill_value=1e+20)


Examples
========

Data with a given value representing missing data
-------------------------------------------------

Let's consider a list of elements, ``x``, where values of -9999. represent
missing data. We wish to compute the average value of the data and the vector
of anomalies (deviations from the average)::

   >>> import numpy.ma as ma
   >>> x = [0.,1.,-9999.,3.,4.]
   >>> mx = ma.masked_values (x, -9999.)
   >>> print(mx.mean())
   2.0
   >>> print(mx - mx.mean())
   [-2.0 -1.0 -- 1.0 2.0]
   >>> print(mx.anom())
   [-2.0 -1.0 -- 1.0 2.0]


Filling in the missing data
---------------------------

Suppose now that we wish to print that same data, but with the missing values
replaced by the average value.

   >>> print(mx.filled(mx.mean()))
   [0.  1.  2.  3.  4.]


Numerical operations
--------------------

Numerical operations can be easily performed without worrying about missing
values, dividing by zero, square roots of negative numbers, etc.::

   >>> import numpy.ma as ma
   >>> x = ma.array([1., -1., 3., 4., 5., 6.], mask=[0,0,0,0,1,0])
   >>> y = ma.array([1., 2., 0., 4., 5., 6.], mask=[0,0,0,0,0,1])
   >>> print(ma.sqrt(x/y))
   [1.0 -- -- 1.0 -- --]

Four values of the output are invalid: the first one comes from taking the
square root of a negative number, the second from the division by zero, and
the last two where the inputs were masked.


Ignoring extreme values
-----------------------

Let's consider an array ``d`` of floats between 0 and 1. We wish to
compute the average of the values of ``d`` while ignoring any data outside
the range ``[0.2, 0.9]``::

   >>> d = np.linspace(0, 1, 20)
   >>> print(d.mean() - ma.masked_outside(d, 0.2, 0.9).mean())
   -0.05263157894736836
.. sectionauthor:: adapted from "Guide to NumPy" by Travis E. Oliphant

.. currentmodule:: numpy

.. _ufuncs:

************************************
Universal functions (:class:`ufunc`)
************************************

.. seealso:: :ref:`ufuncs-basics`

A universal function (or :term:`ufunc` for short) is a function that
operates on :class:`ndarrays <numpy.ndarray>` in an element-by-element fashion,
supporting :ref:`array broadcasting <ufuncs.broadcasting>`, :ref:`type
casting <ufuncs.casting>`, and several other standard features. That
is, a ufunc is a ":term:`vectorized <vectorization>`" wrapper for a function
that takes a fixed number of specific inputs and produces a fixed number of
specific outputs. For detailed information on universal functions, see
:ref:`ufuncs-basics`.

:class:`ufunc`
==============

.. autosummary::
   :toctree: generated/

   numpy.ufunc

.. _ufuncs.kwargs:

Optional keyword arguments
--------------------------

All ufuncs take optional keyword arguments. Most of these represent
advanced usage and will not typically be used.

.. index::
   pair: ufunc; keyword arguments

.. rubric:: *out*

.. versionadded:: 1.6

The first output can be provided as either a positional or a keyword
parameter. Keyword 'out' arguments are incompatible with positional
ones.

.. versionadded:: 1.10

The 'out' keyword argument is expected to be a tuple with one entry per
output (which can be None for arrays to be allocated by the ufunc).
For ufuncs with a single output, passing a single array (instead of a
tuple holding a single array) is also valid.

Passing a single array in the 'out' keyword argument to a ufunc with
multiple outputs is deprecated, and will raise a warning in numpy 1.10,
and an error in a future release.

If 'out' is None (the default), a uninitialized return array is created.
The output array is then filled with the results of the ufunc in the places
that the broadcast 'where' is True. If 'where' is the scalar True (the
default), then this corresponds to the entire output being filled.
Note that outputs not explicitly filled are left with their
uninitialized values.

.. versionadded:: 1.13

Operations where ufunc input and output operands have memory overlap are
defined to be the same as for equivalent operations where there
is no memory overlap.  Operations affected make temporary copies
as needed to eliminate data dependency.  As detecting these cases
is computationally expensive, a heuristic is used, which may in rare
cases result in needless temporary copies.  For operations where the
data dependency is simple enough for the heuristic to analyze,
temporary copies will not be made even if the arrays overlap, if it
can be deduced copies are not necessary.  As an example,
``np.add(a, b, out=a)`` will not involve copies.

.. rubric:: *where*

.. versionadded:: 1.7

Accepts a boolean array which is broadcast together with the operands.
Values of True indicate to calculate the ufunc at that position, values
of False indicate to leave the value in the output alone. This argument
cannot be used for generalized ufuncs as those take non-scalar input.

Note that if an uninitialized return array is created, values of False
will leave those values **uninitialized**.

.. rubric:: *axes*

.. versionadded:: 1.15

A list of tuples with indices of axes a generalized ufunc should operate
on. For instance, for a signature of ``(i,j),(j,k)->(i,k)`` appropriate
for matrix multiplication, the base elements are two-dimensional matrices
and these are taken to be stored in the two last axes of each argument.
The corresponding axes keyword would be ``[(-2, -1), (-2, -1), (-2, -1)]``.
For simplicity, for generalized ufuncs that operate on 1-dimensional arrays
(vectors), a single integer is accepted instead of a single-element tuple,
and for generalized ufuncs for which all outputs are scalars, the output
tuples can be omitted.

.. rubric:: *axis*

.. versionadded:: 1.15

A single axis over which a generalized ufunc should operate. This is a
short-cut for ufuncs that operate over a single, shared core dimension,
equivalent to passing in ``axes`` with entries of ``(axis,)`` for each
single-core-dimension argument and ``()`` for all others.  For instance,
for a signature ``(i),(i)->()``, it is equivalent to passing in
``axes=[(axis,), (axis,), ()]``.

.. rubric:: *keepdims*

.. versionadded:: 1.15

If this is set to `True`, axes which are reduced over will be left in the
result as a dimension with size one, so that the result will broadcast
correctly against the inputs. This option can only be used for generalized
ufuncs that operate on inputs that all have the same number of core
dimensions and with outputs that have no core dimensions, i.e., with
signatures like ``(i),(i)->()`` or ``(m,m)->()``. If used, the location of
the dimensions in the output can be controlled with ``axes`` and ``axis``.

.. rubric:: *casting*

.. versionadded:: 1.6

May be 'no', 'equiv', 'safe', 'same_kind', or 'unsafe'.
See :func:`can_cast` for explanations of the parameter values.

Provides a policy for what kind of casting is permitted. For compatibility
with previous versions of NumPy, this defaults to 'unsafe' for numpy < 1.7.
In numpy 1.7 a transition to 'same_kind' was begun where ufuncs produce a
DeprecationWarning for calls which are allowed under the 'unsafe'
rules, but not under the 'same_kind' rules. From numpy 1.10 and
onwards, the default is 'same_kind'.

.. rubric:: *order*

.. versionadded:: 1.6

Specifies the calculation iteration order/memory layout of the output array.
Defaults to 'K'. 'C' means the output should be C-contiguous, 'F' means
F-contiguous, 'A' means F-contiguous if the inputs are F-contiguous and
not also not C-contiguous, C-contiguous otherwise, and 'K' means to match
the element ordering of the inputs as closely as possible.

.. rubric:: *dtype*

.. versionadded:: 1.6

Overrides the DType of the output arrays the same way as the *signature*.
This should ensure a matching precision of the calculation.  The exact
calculation DTypes chosen may depend on the ufunc and the inputs may be
cast to this DType to perform the calculation.

.. rubric:: *subok*

.. versionadded:: 1.6

Defaults to true. If set to false, the output will always be a strict
array, not a subtype.

.. rubric:: *signature*

Either a Dtype, a tuple of DTypes, or a special signature string
indicating the input and output types of a ufunc.

This argument allows the user to specify exact DTypes to be used for the
calculation.  Casting will be used as necessary. The actual DType of the
input arrays is not considered unless ``signature`` is ``None`` for
that array.

When all DTypes are fixed, a specific loop is chosen or an error raised
if no matching loop exists.
If some DTypes are not specified and left ``None``, the behaviour may
depend on the ufunc.
At this time, a list of available signatures is provided by the **types**
attribute of the ufunc.  (This list may be missing DTypes not defined
by NumPy.)

The ``signature`` only specifies the DType class/type.  For example, it
can specify that the operation should be ``datetime64`` or ``float64``
operation.  It does not specify the ``datetime64`` time-unit or the
``float64`` byte-order.

For backwards compatibility this argument can also be provided as *sig*,
although the long form is preferred.  Note that this should not be
confused with the generalized ufunc :ref:`signature <details-of-signature>`
that is stored in the **signature** attribute of the of the ufunc object.

.. rubric:: *extobj*

A list of length 3 specifying the ufunc buffer-size, the error
mode integer, and the error call-back function. Normally, these
values are looked up in a thread-specific dictionary. Passing them
here circumvents that look up and uses the low-level specification
provided for the error mode. This may be useful, for example, as
an optimization for calculations requiring many ufunc calls on
small arrays in a loop.



Attributes
----------

There are some informational attributes that universal functions
possess. None of the attributes can be set.

.. index::
   pair: ufunc; attributes


============  =================================================================
**__doc__**   A docstring for each ufunc. The first part of the docstring is
              dynamically generated from the number of outputs, the name, and
              the number of inputs. The second part of the docstring is
              provided at creation time and stored with the ufunc.

**__name__**  The name of the ufunc.
============  =================================================================

.. autosummary::
   :toctree: generated/

   ufunc.nin
   ufunc.nout
   ufunc.nargs
   ufunc.ntypes
   ufunc.types
   ufunc.identity
   ufunc.signature

.. _ufuncs.methods:

Methods
-------

.. index::
   pair: ufunc; methods

.. autosummary::
   :toctree: generated/

   ufunc.reduce
   ufunc.accumulate
   ufunc.reduceat
   ufunc.outer
   ufunc.at


.. warning::

    A reduce-like operation on an array with a data-type that has a
    range "too small" to handle the result will silently wrap. One
    should use `dtype` to increase the size of the data-type over which
    reduction takes place.


Available ufuncs
================

There are currently more than 60 universal functions defined in
:mod:`numpy` on one or more types, covering a wide variety of
operations. Some of these ufuncs are called automatically on arrays
when the relevant infix notation is used (*e.g.*, :func:`add(a, b) <add>`
is called internally when ``a + b`` is written and *a* or *b* is an
:class:`ndarray`). Nevertheless, you may still want to use the ufunc
call in order to use the optional output argument(s) to place the
output(s) in an object (or objects) of your choice.

Recall that each ufunc operates element-by-element. Therefore, each scalar
ufunc will be described as if acting on a set of scalar inputs to
return a set of scalar outputs.

.. note::

    The ufunc still returns its output(s) even if you use the optional
    output argument(s).

Math operations
---------------

.. autosummary::

    add
    subtract
    multiply
    matmul
    divide
    logaddexp
    logaddexp2
    true_divide
    floor_divide
    negative
    positive
    power
    float_power
    remainder
    mod
    fmod
    divmod
    absolute
    fabs
    rint
    sign
    heaviside
    conj
    conjugate
    exp
    exp2
    log
    log2
    log10
    expm1
    log1p
    sqrt
    square
    cbrt
    reciprocal
    gcd
    lcm

.. tip::

    The optional output arguments can be used to help you save memory
    for large calculations. If your arrays are large, complicated
    expressions can take longer than absolutely necessary due to the
    creation and (later) destruction of temporary calculation
    spaces. For example, the expression ``G = A * B + C`` is equivalent to
    ``T1 = A * B; G = T1 + C; del T1``. It will be more quickly executed
    as ``G = A * B; add(G, C, G)`` which is the same as
    ``G = A * B; G += C``.


Trigonometric functions
-----------------------
All trigonometric functions use radians when an angle is called for.
The ratio of degrees to radians is :math:`180^{\circ}/\pi.`

.. autosummary::

    sin
    cos
    tan
    arcsin
    arccos
    arctan
    arctan2
    hypot
    sinh
    cosh
    tanh
    arcsinh
    arccosh
    arctanh
    degrees
    radians
    deg2rad
    rad2deg

Bit-twiddling functions
-----------------------

These function all require integer arguments and they manipulate the
bit-pattern of those arguments.

.. autosummary::

    bitwise_and
    bitwise_or
    bitwise_xor
    invert
    left_shift
    right_shift

Comparison functions
--------------------

.. autosummary::

    greater
    greater_equal
    less
    less_equal
    not_equal
    equal

.. warning::

    Do not use the Python keywords ``and`` and ``or`` to combine
    logical array expressions. These keywords will test the truth
    value of the entire array (not element-by-element as you might
    expect). Use the bitwise operators & and \| instead.

.. autosummary::

    logical_and
    logical_or
    logical_xor
    logical_not

.. warning::

    The bit-wise operators & and \| are the proper way to perform
    element-by-element array comparisons. Be sure you understand the
    operator precedence: ``(a > 2) & (a < 5)`` is the proper syntax because
    ``a > 2 & a < 5`` will result in an error due to the fact that ``2 & a``
    is evaluated first.

.. autosummary::

    maximum

.. tip::

    The Python function ``max()`` will find the maximum over a one-dimensional
    array, but it will do so using a slower sequence interface. The reduce
    method of the maximum ufunc is much faster. Also, the ``max()`` method
    will not give answers you might expect for arrays with greater than
    one dimension. The reduce method of minimum also allows you to compute
    a total minimum over an array.

.. autosummary::

    minimum

.. warning::

    the behavior of ``maximum(a, b)`` is different than that of ``max(a, b)``.
    As a ufunc, ``maximum(a, b)`` performs an element-by-element comparison
    of `a` and `b` and chooses each element of the result according to which
    element in the two arrays is larger. In contrast, ``max(a, b)`` treats
    the objects `a` and `b` as a whole, looks at the (total) truth value of
    ``a > b`` and uses it to return either `a` or `b` (as a whole). A similar
    difference exists between ``minimum(a, b)`` and ``min(a, b)``.

.. autosummary::

    fmax
    fmin

Floating functions
------------------

Recall that all of these functions work element-by-element over an
array, returning an array output. The description details only a
single operation.

.. autosummary::

    isfinite
    isinf
    isnan
    isnat
    fabs
    signbit
    copysign
    nextafter
    spacing
    modf
    ldexp
    frexp
    fmod
    floor
    ceil
    trunc
.. versionadded:: 1.6.0

.. automodule:: numpy.polynomial.laguerre
   :no-members:
   :no-inherited-members:
   :no-special-members:
Array manipulation routines
***************************

.. currentmodule:: numpy

Basic operations
================
.. autosummary::
   :toctree: generated/

    copyto
    shape

Changing array shape
====================
.. autosummary::
   :toctree: generated/


   reshape
   ravel
   ndarray.flat
   ndarray.flatten

Transpose-like operations
=========================
.. autosummary::
   :toctree: generated/

   moveaxis
   rollaxis
   swapaxes
   ndarray.T
   transpose

Changing number of dimensions
=============================
.. autosummary::
   :toctree: generated/

   atleast_1d
   atleast_2d
   atleast_3d
   broadcast
   broadcast_to
   broadcast_arrays
   expand_dims
   squeeze

Changing kind of array
======================
.. autosummary::
   :toctree: generated/

   asarray
   asanyarray
   asmatrix
   asfarray
   asfortranarray
   ascontiguousarray
   asarray_chkfinite
   require

Joining arrays
==============
.. autosummary::
   :toctree: generated/

   concatenate
   stack
   block
   vstack
   hstack
   dstack
   column_stack
   row_stack

Splitting arrays
================
.. autosummary::
   :toctree: generated/

   split
   array_split
   dsplit
   hsplit
   vsplit

Tiling arrays
=============
.. autosummary::
   :toctree: generated/

   tile
   repeat

Adding and removing elements
============================
.. autosummary::
   :toctree: generated/

   delete
   insert
   append
   resize
   trim_zeros
   unique

Rearranging elements
====================
.. autosummary::
   :toctree: generated/

   flip
   fliplr
   flipud
   reshape
   roll
   rot90
:orphan:

***************
NumPy internals
***************

.. This document has been moved to ../dev/internals.rst.

This document has been moved to :ref:`numpy-internals`.

.. module:: numpy

.. _reference:

###############
NumPy Reference
###############

:Release: |version|
:Date: |today|

This reference manual details functions, modules, and objects
included in NumPy, describing what they are and what they do.
For learning how to use NumPy, see the :ref:`complete documentation <numpy_docs_mainpage>`.


.. toctree::
   :maxdepth: 2

   arrays
   constants
   ufuncs
   routines
   typing
   global_state
   distutils
   distutils_guide
   c-api/index
   simd/index
   swig


Acknowledgements
================

Large parts of this manual originate from Travis E. Oliphant's book
`Guide to NumPy <https://archive.org/details/NumPyBook>`__ (which generously
entered Public Domain in August 2008). The reference documentation for many of
the functions are written by numerous contributors and developers of
NumPy.
.. currentmodule:: numpy.ma

.. for doctests
   >>> from numpy import ma

.. _numpy.ma.constants:

Constants of the :mod:`numpy.ma` module
=======================================

In addition to the :class:`MaskedArray` class, the :mod:`numpy.ma` module
defines several constants.

.. data:: masked

   The :attr:`masked` constant is a special case of :class:`MaskedArray`,
   with a float datatype and a null shape. It is used to test whether a
   specific entry of a masked array is masked, or to mask one or several
   entries of a masked array::

      >>> x = ma.array([1, 2, 3], mask=[0, 1, 0])
      >>> x[1] is ma.masked
      True
      >>> x[-1] = ma.masked
      >>> x
      masked_array(data=[1, --, --],
                   mask=[False,  True,  True],
             fill_value=999999)


.. data:: nomask

   Value indicating that a masked array has no invalid entry.
   :attr:`nomask` is used internally to speed up computations when the mask
   is not needed. It is represented internally as ``np.False_``.


.. data:: masked_print_options

   String used in lieu of missing data when a masked array is printed.
   By default, this string is ``'--'``.




.. _maskedarray.baseclass:

The :class:`MaskedArray` class
==============================


.. class:: MaskedArray

A subclass of :class:`~numpy.ndarray` designed to manipulate numerical arrays with missing data.



An instance of :class:`MaskedArray` can be thought as the combination of several elements:

* The :attr:`~MaskedArray.data`, as a regular :class:`numpy.ndarray` of any shape or datatype (the data).
* A boolean :attr:`~numpy.ma.MaskedArray.mask` with the same shape as the data, where a ``True`` value indicates that the corresponding element of the data is invalid.
  The special value :const:`nomask` is also acceptable for arrays without named fields, and indicates that no data is invalid.
* A :attr:`~numpy.ma.MaskedArray.fill_value`, a value that may be used to replace the invalid entries in order to return a standard :class:`numpy.ndarray`.



.. _ma-attributes:

Attributes and properties of masked arrays
------------------------------------------

.. seealso:: :ref:`Array Attributes <arrays.ndarray.attributes>`

.. autoattribute:: MaskedArray.data

.. autoattribute:: MaskedArray.mask

.. autoattribute:: MaskedArray.recordmask

.. autoattribute:: MaskedArray.fill_value

.. autoattribute:: MaskedArray.baseclass

.. autoattribute:: MaskedArray.sharedmask

.. autoattribute:: MaskedArray.hardmask

As :class:`MaskedArray` is a subclass of :class:`~numpy.ndarray`, a masked array also inherits all the attributes and properties of a  :class:`~numpy.ndarray` instance.

.. autosummary::
   :toctree: generated/

   MaskedArray.base
   MaskedArray.ctypes
   MaskedArray.dtype
   MaskedArray.flags

   MaskedArray.itemsize
   MaskedArray.nbytes
   MaskedArray.ndim
   MaskedArray.shape
   MaskedArray.size
   MaskedArray.strides

   MaskedArray.imag
   MaskedArray.real

   MaskedArray.flat
   MaskedArray.__array_priority__



:class:`MaskedArray` methods
============================

.. seealso:: :ref:`Array methods <array.ndarray.methods>`


Conversion
----------

.. autosummary::
   :toctree: generated/

   MaskedArray.__float__
   MaskedArray.__int__

   MaskedArray.view
   MaskedArray.astype
   MaskedArray.byteswap

   MaskedArray.compressed
   MaskedArray.filled
   MaskedArray.tofile
   MaskedArray.toflex
   MaskedArray.tolist
   MaskedArray.torecords
   MaskedArray.tostring
   MaskedArray.tobytes


Shape manipulation
------------------

For reshape, resize, and transpose, the single tuple argument may be
replaced with ``n`` integers which will be interpreted as an n-tuple.

.. autosummary::
   :toctree: generated/

   MaskedArray.flatten
   MaskedArray.ravel
   MaskedArray.reshape
   MaskedArray.resize
   MaskedArray.squeeze
   MaskedArray.swapaxes
   MaskedArray.transpose
   MaskedArray.T


Item selection and manipulation
-------------------------------

For array methods that take an ``axis`` keyword, it defaults to None.
If axis is None, then the array is treated as a 1-D array.
Any other value for ``axis`` represents the dimension along which
the operation should proceed.

.. autosummary::
   :toctree: generated/

   MaskedArray.argmax
   MaskedArray.argmin
   MaskedArray.argsort
   MaskedArray.choose
   MaskedArray.compress
   MaskedArray.diagonal
   MaskedArray.fill
   MaskedArray.item
   MaskedArray.nonzero
   MaskedArray.put
   MaskedArray.repeat
   MaskedArray.searchsorted
   MaskedArray.sort
   MaskedArray.take


Pickling and copy
-----------------

.. autosummary::
   :toctree: generated/

   MaskedArray.copy
   MaskedArray.dump
   MaskedArray.dumps


Calculations
------------

.. autosummary::
   :toctree: generated/

   MaskedArray.all
   MaskedArray.anom
   MaskedArray.any
   MaskedArray.clip
   MaskedArray.conj
   MaskedArray.conjugate
   MaskedArray.cumprod
   MaskedArray.cumsum
   MaskedArray.max
   MaskedArray.mean
   MaskedArray.min
   MaskedArray.prod
   MaskedArray.product
   MaskedArray.ptp
   MaskedArray.round
   MaskedArray.std
   MaskedArray.sum
   MaskedArray.trace
   MaskedArray.var


Arithmetic and comparison operations
------------------------------------

.. index:: comparison, arithmetic, operation, operator

Comparison operators:
~~~~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.__lt__
   MaskedArray.__le__
   MaskedArray.__gt__
   MaskedArray.__ge__
   MaskedArray.__eq__
   MaskedArray.__ne__

Truth value of an array (:class:`bool() <bool>`):
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.__bool__


Arithmetic:
~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.__abs__
   MaskedArray.__add__
   MaskedArray.__radd__
   MaskedArray.__sub__
   MaskedArray.__rsub__
   MaskedArray.__mul__
   MaskedArray.__rmul__
   MaskedArray.__div__
   MaskedArray.__truediv__
   MaskedArray.__rtruediv__
   MaskedArray.__floordiv__
   MaskedArray.__rfloordiv__
   MaskedArray.__mod__
   MaskedArray.__rmod__
   MaskedArray.__divmod__
   MaskedArray.__rdivmod__
   MaskedArray.__pow__
   MaskedArray.__rpow__
   MaskedArray.__lshift__
   MaskedArray.__rlshift__
   MaskedArray.__rshift__
   MaskedArray.__rrshift__
   MaskedArray.__and__
   MaskedArray.__rand__
   MaskedArray.__or__
   MaskedArray.__ror__
   MaskedArray.__xor__
   MaskedArray.__rxor__


Arithmetic, in-place:
~~~~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.__iadd__
   MaskedArray.__isub__
   MaskedArray.__imul__
   MaskedArray.__idiv__
   MaskedArray.__itruediv__
   MaskedArray.__ifloordiv__
   MaskedArray.__imod__
   MaskedArray.__ipow__
   MaskedArray.__ilshift__
   MaskedArray.__irshift__
   MaskedArray.__iand__
   MaskedArray.__ior__
   MaskedArray.__ixor__


Representation
--------------

.. autosummary::
   :toctree: generated/

   MaskedArray.__repr__
   MaskedArray.__str__

   MaskedArray.ids
   MaskedArray.iscontiguous


Special methods
---------------

For standard library functions:

.. autosummary::
   :toctree: generated/

   MaskedArray.__copy__
   MaskedArray.__deepcopy__
   MaskedArray.__getstate__
   MaskedArray.__reduce__
   MaskedArray.__setstate__

Basic customization:

.. autosummary::
   :toctree: generated/

   MaskedArray.__new__
   MaskedArray.__array__
   MaskedArray.__array_wrap__

Container customization: (see :ref:`Indexing <arrays.indexing>`)

.. autosummary::
   :toctree: generated/

   MaskedArray.__len__
   MaskedArray.__getitem__
   MaskedArray.__setitem__
   MaskedArray.__delitem__
   MaskedArray.__contains__



Specific methods
----------------

Handling the mask
~~~~~~~~~~~~~~~~~

The following methods can be used to access information about the mask or to
manipulate the mask.

.. autosummary::
   :toctree: generated/

   MaskedArray.__setmask__

   MaskedArray.harden_mask
   MaskedArray.soften_mask
   MaskedArray.unshare_mask
   MaskedArray.shrink_mask


Handling the `fill_value`
~~~~~~~~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.get_fill_value
   MaskedArray.set_fill_value



Counting the missing elements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autosummary::
   :toctree: generated/

   MaskedArray.count
*********
Constants
*********

.. automodule:: numpy.doc.constants
.. module:: numpy.ctypeslib

***********************************************************
C-Types Foreign Function Interface (:mod:`numpy.ctypeslib`)
***********************************************************

.. currentmodule:: numpy.ctypeslib

.. autofunction:: as_array
.. autofunction:: as_ctypes
.. autofunction:: as_ctypes_type
.. autofunction:: load_library
.. autofunction:: ndpointer

.. class:: c_intp

    A `ctypes` signed integer type of the same size as `numpy.intp`.

    Depending on the platform, it can be an alias for either `~ctypes.c_int`,
    `~ctypes.c_long` or `~ctypes.c_longlong`.
.. versionadded:: 1.6.0

.. automodule:: numpy.polynomial.legendre
   :no-members:
   :no-inherited-members:
   :no-special-members:
.. _distutils-user-guide:

NumPy Distutils - Users Guide
=============================

.. include:: ../../DISTUTILS.rst.txt
   :start-line: 6
.. _routines.fft:
.. automodule:: numpy.fft
.. versionadded:: 1.6.0

.. automodule:: numpy.polynomial.hermite
   :no-members:
   :no-inherited-members:
   :no-special-members:
Polyutils
=========

.. automodule:: numpy.polynomial.polyutils
.. versionadded:: 1.6.0

.. automodule:: numpy.polynomial.hermite_e
   :no-members:
   :no-inherited-members:
   :no-special-members:
.. _routines.io:

Input and output
****************

.. currentmodule:: numpy

NumPy binary files (NPY, NPZ)
-----------------------------
.. autosummary::
   :toctree: generated/

   load
   save
   savez
   savez_compressed

The format of these binary file types is documented in
:py:mod:`numpy.lib.format`

Text files
----------
.. autosummary::
   :toctree: generated/

   loadtxt
   savetxt
   genfromtxt
   fromregex
   fromstring
   ndarray.tofile
   ndarray.tolist

Raw binary files
----------------

.. autosummary::

   fromfile
   ndarray.tofile

String formatting
-----------------
.. autosummary::
   :toctree: generated/

   array2string
   array_repr
   array_str
   format_float_positional
   format_float_scientific

Memory mapping files
--------------------
.. autosummary::
   :toctree: generated/

   memmap
   lib.format.open_memmap

Text formatting options
-----------------------
.. autosummary::
   :toctree: generated/

   set_printoptions
   get_printoptions
   set_string_function
   printoptions

Base-n representations
----------------------
.. autosummary::
   :toctree: generated/

   binary_repr
   base_repr

Data sources
------------
.. autosummary::
   :toctree: generated/

   DataSource

Binary Format Description
-------------------------
.. autosummary::
   :toctree: generated/

   lib.format
.. _routines.linalg:

.. module:: numpy.linalg

Linear algebra (:mod:`numpy.linalg`)
************************************

The NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient
low level implementations of standard linear algebra algorithms. Those
libraries may be provided by NumPy itself using C versions of a subset of their
reference implementations but, when possible, highly optimized libraries that
take advantage of specialized processor functionality are preferred. Examples
of such libraries are OpenBLAS_, MKL (TM), and ATLAS. Because those libraries
are multithreaded and processor dependent, environmental variables and external
packages such as threadpoolctl_ may be needed to control the number of threads
or specify the processor architecture.

.. _OpenBLAS: https://www.openblas.net/
.. _threadpoolctl: https://github.com/joblib/threadpoolctl

The SciPy library also contains a `~scipy.linalg` submodule, and there is
overlap in the functionality provided by the SciPy and NumPy submodules.  SciPy
contains functions not found in `numpy.linalg`, such as functions related to
LU decomposition and the Schur decomposition, multiple ways of calculating the
pseudoinverse, and matrix transcendentals such as the matrix logarithm.  Some
functions that exist in both have augmented functionality in `scipy.linalg`.
For example, `scipy.linalg.eig` can take a second matrix argument for solving
generalized eigenvalue problems.  Some functions in NumPy, however, have more
flexible broadcasting options.  For example, `numpy.linalg.solve` can handle
"stacked" arrays, while `scipy.linalg.solve` accepts only a single square
array as its first argument.

.. note::

   The term *matrix* as it is used on this page indicates a 2d `numpy.array`
   object, and *not* a `numpy.matrix` object. The latter is no longer
   recommended, even for linear algebra. See
   :ref:`the matrix object documentation<matrix-objects>` for
   more information.

The ``@`` operator
------------------

Introduced in NumPy 1.10.0, the ``@`` operator is preferable to
other methods when computing the matrix product between 2d arrays. The
:func:`numpy.matmul` function implements the ``@`` operator.

.. currentmodule:: numpy

Matrix and vector products
--------------------------

.. autosummary::
   :toctree: generated/

   dot
   linalg.multi_dot
   vdot
   inner
   outer
   matmul
   tensordot
   einsum
   einsum_path
   linalg.matrix_power
   kron

Decompositions
--------------
.. autosummary::
   :toctree: generated/

   linalg.cholesky
   linalg.qr
   linalg.svd

Matrix eigenvalues
------------------
.. autosummary::
   :toctree: generated/

   linalg.eig
   linalg.eigh
   linalg.eigvals
   linalg.eigvalsh

Norms and other numbers
-----------------------
.. autosummary::
   :toctree: generated/

   linalg.norm
   linalg.cond
   linalg.det
   linalg.matrix_rank
   linalg.slogdet
   trace

Solving equations and inverting matrices
----------------------------------------
.. autosummary::
   :toctree: generated/

   linalg.solve
   linalg.tensorsolve
   linalg.lstsq
   linalg.inv
   linalg.pinv
   linalg.tensorinv

Exceptions
----------
.. autosummary::
   :toctree: generated/

   linalg.LinAlgError

.. _routines.linalg-broadcasting:

Linear algebra on several matrices at once
------------------------------------------

.. versionadded:: 1.8.0

Several of the linear algebra routines listed above are able to
compute results for several matrices at once, if they are stacked into
the same array.

This is indicated in the documentation via input parameter
specifications such as ``a : (..., M, M) array_like``. This means that
if for instance given an input array ``a.shape == (N, M, M)``, it is
interpreted as a "stack" of N matrices, each of size M-by-M. Similar
specification applies to return values, for instance the determinant
has ``det : (...)`` and will in this case return an array of shape
``det(a).shape == (N,)``. This generalizes to linear algebra
operations on higher-dimensional arrays: the last 1 or 2 dimensions of
a multidimensional array are interpreted as vectors or matrices, as
appropriate for each operation.
.. currentmodule:: numpy

.. _arrays.nditer:

*********************
Iterating Over Arrays
*********************

.. note::

   Arrays support the iterator protocol and can be iterated over like Python
   lists. See the :ref:`quickstart.indexing-slicing-and-iterating` section in
   the Quickstart guide for basic usage and examples. The remainder of
   this document presents the :class:`nditer` object and covers more 
   advanced usage.

The iterator object :class:`nditer`, introduced in NumPy 1.6, provides
many flexible ways to visit all the elements of one or more arrays in
a systematic fashion. This page introduces some basic ways to use the
object for computations on arrays in Python, then concludes with how one
can accelerate the inner loop in Cython. Since the Python exposure of
:class:`nditer` is a relatively straightforward mapping of the C array
iterator API, these ideas will also provide help working with array
iteration from C or C++.

Single Array Iteration
======================

The most basic task that can be done with the :class:`nditer` is to
visit every element of an array. Each element is provided one by one
using the standard Python iterator interface.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> for x in np.nditer(a):
    ...     print(x, end=' ')
    ...
    0 1 2 3 4 5

An important thing to be aware of for this iteration is that the order
is chosen to match the memory layout of the array instead of using a
standard C or Fortran ordering. This is done for access efficiency,
reflecting the idea that by default one simply wants to visit each element
without concern for a particular ordering. We can see this by iterating
over the transpose of our previous array, compared to taking a copy
of that transpose in C order.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> for x in np.nditer(a.T):
    ...     print(x, end=' ')
    ...
    0 1 2 3 4 5

    >>> for x in np.nditer(a.T.copy(order='C')):
    ...     print(x, end=' ')
    ...
    0 3 1 4 2 5

The elements of both `a` and `a.T` get traversed in the same order,
namely the order they are stored in memory, whereas the elements of
`a.T.copy(order='C')` get visited in a different order because they
have been put into a different memory layout.

Controlling Iteration Order
---------------------------

There are times when it is important to visit the elements of an array
in a specific order, irrespective of the layout of the elements in memory.
The :class:`nditer` object provides an `order` parameter to control this
aspect of iteration. The default, having the behavior described above,
is order='K' to keep the existing order. This can be overridden with
order='C' for C order and order='F' for Fortran order.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> for x in np.nditer(a, order='F'):
    ...     print(x, end=' ')
    ...
    0 3 1 4 2 5
    >>> for x in np.nditer(a.T, order='C'):
    ...     print(x, end=' ')
    ...
    0 3 1 4 2 5

.. _nditer-context-manager:

Modifying Array Values
----------------------

By default, the :class:`nditer` treats the input operand as a read-only
object. To be able to modify the array elements, you must specify either
read-write or write-only mode using the `'readwrite'` or `'writeonly'`
per-operand flags.

The nditer will then yield writeable buffer arrays which you may modify. However,
because  the nditer must copy this buffer data back to the original array once
iteration is finished, you must signal when the iteration is ended, by one of two
methods. You may either:

 - used the nditer as a context manager using the `with` statement, and
   the temporary data will be written back when the context is exited.
 - call the iterator's `close` method once finished iterating, which will trigger
   the write-back.

The nditer can no longer be iterated once either `close` is called or its
context is exited.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> a
    array([[0, 1, 2],
           [3, 4, 5]])
    >>> with np.nditer(a, op_flags=['readwrite']) as it:
    ...    for x in it:
    ...        x[...] = 2 * x
    ...
    >>> a
    array([[ 0,  2,  4],
           [ 6,  8, 10]])

If you are writing code that needs to support older versions of numpy,
note that prior to 1.15, :class:`nditer` was not a context manager and
did not have a `close` method. Instead it relied on the destructor to
initiate the writeback of the buffer.

Using an External Loop
----------------------

In all the examples so far, the elements of `a` are provided by the
iterator one at a time, because all the looping logic is internal to the
iterator. While this is simple and convenient, it is not very efficient.
A better approach is to move the one-dimensional innermost loop into your
code, external to the iterator. This way, NumPy's vectorized operations
can be used on larger chunks of the elements being visited.

The :class:`nditer` will try to provide chunks that are
as large as possible to the inner loop. By forcing 'C' and 'F' order,
we get different external loop sizes. This mode is enabled by specifying
an iterator flag.

Observe that with the default of keeping native memory order, the
iterator is able to provide a single one-dimensional chunk, whereas
when forcing Fortran order, it has to provide three chunks of two
elements each.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> for x in np.nditer(a, flags=['external_loop']):
    ...     print(x, end=' ')
    ...
    [0 1 2 3 4 5]

    >>> for x in np.nditer(a, flags=['external_loop'], order='F'):
    ...     print(x, end=' ')
    ...
    [0 3] [1 4] [2 5]

Tracking an Index or Multi-Index
--------------------------------

During iteration, you may want to use the index of the current
element in a computation. For example, you may want to visit the
elements of an array in memory order, but use a C-order, Fortran-order,
or multidimensional index to look up values in a different array.

The index is tracked by the iterator object itself, and accessible
through the `index` or `multi_index` properties, depending on what was
requested. The examples below show printouts demonstrating the
progression of the index:

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> it = np.nditer(a, flags=['f_index'])
    >>> for x in it:
    ...     print("%d <%d>" % (x, it.index), end=' ')
    ...
    0 <0> 1 <2> 2 <4> 3 <1> 4 <3> 5 <5>

    >>> it = np.nditer(a, flags=['multi_index'])
    >>> for x in it:
    ...     print("%d <%s>" % (x, it.multi_index), end=' ')
    ...
    0 <(0, 0)> 1 <(0, 1)> 2 <(0, 2)> 3 <(1, 0)> 4 <(1, 1)> 5 <(1, 2)>

    >>> with np.nditer(a, flags=['multi_index'], op_flags=['writeonly']) as it:
    ...     for x in it:
    ...         x[...] = it.multi_index[1] - it.multi_index[0]
    ...
    >>> a
    array([[ 0,  1,  2],
           [-1,  0,  1]])

Tracking an index or multi-index is incompatible with using an external
loop, because it requires a different index value per element. If
you try to combine these flags, the :class:`nditer` object will
raise an exception.

.. admonition:: Example

    >>> a = np.zeros((2,3))
    >>> it = np.nditer(a, flags=['c_index', 'external_loop'])
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    ValueError: Iterator flag EXTERNAL_LOOP cannot be used if an index or multi-index is being tracked

Alternative Looping and Element Access
--------------------------------------

To make its properties more readily accessible during iteration,
:class:`nditer` has an alternative syntax for iterating, which works
explicitly with the iterator object itself. With this looping construct,
the current value is accessible by indexing into the iterator. Other
properties, such as tracked indices remain as before. The examples below
produce identical results to the ones in the previous section.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> it = np.nditer(a, flags=['f_index'])
    >>> while not it.finished:
    ...     print("%d <%d>" % (it[0], it.index), end=' ')
    ...     is_not_finished = it.iternext()
    ...
    0 <0> 1 <2> 2 <4> 3 <1> 4 <3> 5 <5>

    >>> it = np.nditer(a, flags=['multi_index'])
    >>> while not it.finished:
    ...     print("%d <%s>" % (it[0], it.multi_index), end=' ')
    ...     is_not_finished = it.iternext()
    ...
    0 <(0, 0)> 1 <(0, 1)> 2 <(0, 2)> 3 <(1, 0)> 4 <(1, 1)> 5 <(1, 2)>

    >>> with np.nditer(a, flags=['multi_index'], op_flags=['writeonly']) as it:
    ...     while not it.finished:
    ...         it[0] = it.multi_index[1] - it.multi_index[0]
    ...         is_not_finished = it.iternext()
    ...
    >>> a
    array([[ 0,  1,  2],
           [-1,  0,  1]])

Buffering the Array Elements
----------------------------

When forcing an iteration order, we observed that the external loop
option may provide the elements in smaller chunks because the elements
can't be visited in the appropriate order with a constant stride.
When writing C code, this is generally fine, however in pure Python code
this can cause a significant reduction in performance.

By enabling buffering mode, the chunks provided by the iterator to
the inner loop can be made larger, significantly reducing the overhead
of the Python interpreter. In the example forcing Fortran iteration order,
the inner loop gets to see all the elements in one go when buffering
is enabled.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3)
    >>> for x in np.nditer(a, flags=['external_loop'], order='F'):
    ...     print(x, end=' ')
    ...
    [0 3] [1 4] [2 5]

    >>> for x in np.nditer(a, flags=['external_loop','buffered'], order='F'):
    ...     print(x, end=' ')
    ...
    [0 3 1 4 2 5]

Iterating as a Specific Data Type
---------------------------------

There are times when it is necessary to treat an array as a different
data type than it is stored as. For instance, one may want to do all
computations on 64-bit floats, even if the arrays being manipulated
are 32-bit floats. Except when writing low-level C code, it's generally
better to let the iterator handle the copying or buffering instead
of casting the data type yourself in the inner loop.

There are two mechanisms which allow this to be done, temporary copies
and buffering mode. With temporary copies, a copy of the entire array is
made with the new data type, then iteration is done in the copy. Write
access is permitted through a mode which updates the original array after
all the iteration is complete. The major drawback of temporary copies is
that the temporary copy may consume a large amount of memory, particularly
if the iteration data type has a larger itemsize than the original one.

Buffering mode mitigates the memory usage issue and is more cache-friendly
than making temporary copies. Except for special cases, where the whole
array is needed at once outside the iterator, buffering is recommended
over temporary copying. Within NumPy, buffering is used by the ufuncs and
other functions to support flexible inputs with minimal memory overhead.

In our examples, we will treat the input array with a complex data type,
so that we can take square roots of negative numbers. Without enabling
copies or buffering mode, the iterator will raise an exception if the
data type doesn't match precisely.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3) - 3
    >>> for x in np.nditer(a, op_dtypes=['complex128']):
    ...     print(np.sqrt(x), end=' ')
    ...
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: Iterator operand required copying or buffering, but neither copying nor buffering was enabled

In copying mode, 'copy' is specified as a per-operand flag. This is
done to provide control in a per-operand fashion. Buffering mode is
specified as an iterator flag.

.. admonition:: Example

    >>> a = np.arange(6).reshape(2,3) - 3
    >>> for x in np.nditer(a, op_flags=['readonly','copy'],
    ...                 op_dtypes=['complex128']):
    ...     print(np.sqrt(x), end=' ')
    ...
    1.7320508075688772j 1.4142135623730951j 1j 0j (1+0j) (1.4142135623730951+0j)

    >>> for x in np.nditer(a, flags=['buffered'], op_dtypes=['complex128']):
    ...     print(np.sqrt(x), end=' ')
    ...
    1.7320508075688772j 1.4142135623730951j 1j 0j (1+0j) (1.4142135623730951+0j)


The iterator uses NumPy's casting rules to determine whether a specific
conversion is permitted. By default, it enforces 'safe' casting. This means,
for example, that it will raise an exception if you try to treat a
64-bit float array as a 32-bit float array. In many cases, the rule
'same_kind' is the most reasonable rule to use, since it will allow
conversion from 64 to 32-bit float, but not from float to int or from
complex to float.

.. admonition:: Example

    >>> a = np.arange(6.)
    >>> for x in np.nditer(a, flags=['buffered'], op_dtypes=['float32']):
    ...     print(x, end=' ')
    ...
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: Iterator operand 0 dtype could not be cast from dtype('float64') to dtype('float32') according to the rule 'safe'

    >>> for x in np.nditer(a, flags=['buffered'], op_dtypes=['float32'],
    ...                 casting='same_kind'):
    ...     print(x, end=' ')
    ...
    0.0 1.0 2.0 3.0 4.0 5.0

    >>> for x in np.nditer(a, flags=['buffered'], op_dtypes=['int32'], casting='same_kind'):
    ...     print(x, end=' ')
    ...
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: Iterator operand 0 dtype could not be cast from dtype('float64') to dtype('int32') according to the rule 'same_kind'

One thing to watch out for is conversions back to the original data
type when using a read-write or write-only operand. A common case is
to implement the inner loop in terms of 64-bit floats, and use 'same_kind'
casting to allow the other floating-point types to be processed as well.
While in read-only mode, an integer array could be provided, read-write
mode will raise an exception because conversion back to the array
would violate the casting rule.

.. admonition:: Example

    >>> a = np.arange(6)
    >>> for x in np.nditer(a, flags=['buffered'], op_flags=['readwrite'],
    ...                 op_dtypes=['float64'], casting='same_kind'):
    ...     x[...] = x / 2.0
    ...
    Traceback (most recent call last):
      File "<stdin>", line 2, in <module>
    TypeError: Iterator requested dtype could not be cast from dtype('float64') to dtype('int64'), the operand 0 dtype, according to the rule 'same_kind'

Broadcasting Array Iteration
============================

NumPy has a set of rules for dealing with arrays that have differing
shapes which are applied whenever functions take multiple operands
which combine element-wise. This is called
:ref:`broadcasting <ufuncs.broadcasting>`.  The :class:`nditer`
object can apply these rules for you when you need to write such a function.

As an example, we print out the result of broadcasting a one and
a two dimensional array together.

.. admonition:: Example

    >>> a = np.arange(3)
    >>> b = np.arange(6).reshape(2,3)
    >>> for x, y in np.nditer([a,b]):
    ...     print("%d:%d" % (x,y), end=' ')
    ...
    0:0 1:1 2:2 0:3 1:4 2:5

When a broadcasting error occurs, the iterator raises an exception
which includes the input shapes to help diagnose the problem.

.. admonition:: Example

    >>> a = np.arange(2)
    >>> b = np.arange(6).reshape(2,3)
    >>> for x, y in np.nditer([a,b]):
    ...     print("%d:%d" % (x,y), end=' ')
    ...
    Traceback (most recent call last):
    ...
    ValueError: operands could not be broadcast together with shapes (2,) (2,3)

Iterator-Allocated Output Arrays
--------------------------------

A common case in NumPy functions is to have outputs allocated based
on the broadcasting of the input, and additionally have an optional
parameter called 'out' where the result will be placed when it is
provided. The :class:`nditer` object provides a convenient idiom that
makes it very easy to support this mechanism.

We'll show how this works by creating a function `square` which squares
its input. Let's start with a minimal function definition excluding 'out'
parameter support.

.. admonition:: Example

    >>> def square(a):
    ...     with np.nditer([a, None]) as it:
    ...         for x, y in it:
    ...             y[...] = x*x
    ...         return it.operands[1]
    ...
    >>> square([1,2,3])
    array([1, 4, 9])

By default, the :class:`nditer` uses the flags 'allocate' and 'writeonly'
for operands that are passed in as None. This means we were able to provide
just the two operands to the iterator, and it handled the rest.

When adding the 'out' parameter, we have to explicitly provide those flags,
because if someone passes in an array as 'out', the iterator will default
to 'readonly', and our inner loop would fail. The reason 'readonly' is
the default for input arrays is to prevent confusion about unintentionally
triggering a reduction operation. If the default were 'readwrite', any
broadcasting operation would also trigger a reduction, a topic
which is covered later in this document.

While we're at it, let's also introduce the 'no_broadcast' flag, which
will prevent the output from being broadcast. This is important, because
we only want one input value for each output. Aggregating more than one
input value is a reduction operation which requires special handling.
It would already raise an error because reductions must be explicitly
enabled in an iterator flag, but the error message that results from
disabling broadcasting is much more understandable for end-users.
To see how to generalize the square function to a reduction, look
at the sum of squares function in the section about Cython.

For completeness, we'll also add the 'external_loop' and 'buffered'
flags, as these are what you will typically want for performance
reasons.

.. admonition:: Example

    >>> def square(a, out=None):
    ...     it = np.nditer([a, out],
    ...             flags = ['external_loop', 'buffered'],
    ...             op_flags = [['readonly'],
    ...                         ['writeonly', 'allocate', 'no_broadcast']])
    ...     with it:
    ...         for x, y in it:
    ...             y[...] = x*x
    ...         return it.operands[1]
    ...

    >>> square([1,2,3])
    array([1, 4, 9])

    >>> b = np.zeros((3,))
    >>> square([1,2,3], out=b)
    array([1.,  4.,  9.])
    >>> b
    array([1.,  4.,  9.])

    >>> square(np.arange(6).reshape(2,3), out=b)
    Traceback (most recent call last):
      ...
    ValueError: non-broadcastable output operand with shape (3,) doesn't
    match the broadcast shape (2,3)

Outer Product Iteration
-----------------------

Any binary operation can be extended to an array operation in an outer
product fashion like in :func:`outer`, and the :class:`nditer` object
provides a way to accomplish this by explicitly mapping the axes of
the operands.  It is also possible to do this with :const:`newaxis`
indexing, but we will show you how to directly use the nditer `op_axes`
parameter to accomplish this with no intermediate views.

We'll do a simple outer product, placing the dimensions of the first
operand before the dimensions of the second operand. The `op_axes`
parameter needs one list of axes for each operand, and provides a mapping
from the iterator's axes to the axes of the operand.

Suppose the first operand is one dimensional and the second operand is
two dimensional. The iterator will have three dimensions, so `op_axes`
will have two 3-element lists.  The first list picks out the one
axis of the first operand, and is -1 for the rest of the iterator axes,
with a final result of [0, -1, -1]. The second list picks out the two
axes of the second operand, but shouldn't overlap with the axes picked
out in the first operand. Its list is [-1, 0, 1]. The output operand
maps onto the iterator axes in the standard manner, so we can provide
None instead of constructing another list.

The operation in the inner loop is a straightforward multiplication.
Everything to do with the outer product is handled by the iterator setup.

.. admonition:: Example

    >>> a = np.arange(3)
    >>> b = np.arange(8).reshape(2,4)
    >>> it = np.nditer([a, b, None], flags=['external_loop'],
    ...             op_axes=[[0, -1, -1], [-1, 0, 1], None])
    >>> with it:
    ...     for x, y, z in it:
    ...         z[...] = x*y
    ...     result = it.operands[2]  # same as z
    ...
    >>> result
    array([[[ 0,  0,  0,  0],
            [ 0,  0,  0,  0]],
           [[ 0,  1,  2,  3],
            [ 4,  5,  6,  7]],
           [[ 0,  2,  4,  6],
            [ 8, 10, 12, 14]]])

Note that once the iterator is closed we can not access :func:`operands <nditer.operands>`
and must use a reference created inside the context manager.

Reduction Iteration
-------------------

Whenever a writeable operand has fewer elements than the full iteration space,
that operand is undergoing a reduction. The :class:`nditer` object requires
that any reduction operand be flagged as read-write, and only allows
reductions when 'reduce_ok' is provided as an iterator flag.

For a simple example, consider taking the sum of all elements in an array.

.. admonition:: Example

    >>> a = np.arange(24).reshape(2,3,4)
    >>> b = np.array(0)
    >>> with np.nditer([a, b], flags=['reduce_ok'],
    ...                     op_flags=[['readonly'], ['readwrite']]) as it:
    ...     for x,y in it:
    ...         y[...] += x
    ...
    >>> b
    array(276)
    >>> np.sum(a)
    276

Things are a little bit more tricky when combining reduction and allocated
operands. Before iteration is started, any reduction operand must be
initialized to its starting values. Here's how we can do this, taking
sums along the last axis of `a`.

.. admonition:: Example

    >>> a = np.arange(24).reshape(2,3,4)
    >>> it = np.nditer([a, None], flags=['reduce_ok'],
    ...             op_flags=[['readonly'], ['readwrite', 'allocate']],
    ...             op_axes=[None, [0,1,-1]])
    >>> with it:
    ...     it.operands[1][...] = 0
    ...     for x, y in it:
    ...         y[...] += x
    ...     result = it.operands[1]
    ...
    >>> result
    array([[ 6, 22, 38],
           [54, 70, 86]])
    >>> np.sum(a, axis=2)
    array([[ 6, 22, 38],
           [54, 70, 86]])

To do buffered reduction requires yet another adjustment during the
setup. Normally the iterator construction involves copying the first
buffer of data from the readable arrays into the buffer. Any reduction
operand is readable, so it may be read into a buffer. Unfortunately,
initialization of the operand after this buffering operation is complete
will not be reflected in the buffer that the iteration starts with, and
garbage results will be produced.

The iterator flag "delay_bufalloc" is there to allow
iterator-allocated reduction operands to exist together with buffering.
When this flag is set, the iterator will leave its buffers uninitialized
until it receives a reset, after which it will be ready for regular
iteration. Here's how the previous example looks if we also enable
buffering.

.. admonition:: Example

    >>> a = np.arange(24).reshape(2,3,4)
    >>> it = np.nditer([a, None], flags=['reduce_ok',
    ...                                  'buffered', 'delay_bufalloc'],
    ...             op_flags=[['readonly'], ['readwrite', 'allocate']],
    ...             op_axes=[None, [0,1,-1]])
    >>> with it:
    ...     it.operands[1][...] = 0
    ...     it.reset()
    ...     for x, y in it:
    ...         y[...] += x
    ...     result = it.operands[1]
    ...
    >>> result
    array([[ 6, 22, 38],
           [54, 70, 86]])

.. for doctests
   Include Cython section separately. Those tests are skipped entirely via an
   entry in RST_SKIPLIST

.. include:: arrays.nditer.cython.rst
Mathematical functions with automatic domain (:mod:`numpy.emath`)
***********************************************************************

.. currentmodule:: numpy

.. note:: :mod:`numpy.emath` is a preferred alias for :mod:`numpy.lib.scimath`,
          available after :mod:`numpy` is imported.

.. automodule:: numpy.lib.scimath
Window functions
================

.. currentmodule:: numpy

Various windows
---------------

.. autosummary::
   :toctree: generated/

   bartlett
   blackman
   hamming
   hanning
   kaiser
.. _testing-guidelines:

Testing Guidelines
==================

.. include:: ../../TESTS.rst.txt
   :start-line: 6
.. _routines.array-creation:

Array creation routines
=======================

.. seealso:: :ref:`Array creation <arrays.creation>`

.. currentmodule:: numpy

From shape or value
-------------------
.. autosummary::
   :toctree: generated/

   empty
   empty_like
   eye
   identity
   ones
   ones_like
   zeros
   zeros_like
   full
   full_like

From existing data
------------------
.. autosummary::
   :toctree: generated/

   array
   asarray
   asanyarray
   ascontiguousarray
   asmatrix
   copy
   frombuffer
   fromfile
   fromfunction
   fromiter
   fromstring
   loadtxt

.. _routines.array-creation.rec:

Creating record arrays (:mod:`numpy.rec`)
-----------------------------------------

.. note:: :mod:`numpy.rec` is the preferred alias for
   :mod:`numpy.core.records`.

.. autosummary::
   :toctree: generated/

   core.records.array
   core.records.fromarrays
   core.records.fromrecords
   core.records.fromstring
   core.records.fromfile

.. _routines.array-creation.char:

Creating character arrays (:mod:`numpy.char`)
---------------------------------------------

.. note:: :mod:`numpy.char` is the preferred alias for
   :mod:`numpy.core.defchararray`.

.. autosummary::
   :toctree: generated/

   core.defchararray.array
   core.defchararray.asarray

Numerical ranges
----------------
.. autosummary::
   :toctree: generated/

   arange
   linspace
   logspace
   geomspace
   meshgrid
   mgrid
   ogrid

Building matrices
-----------------
.. autosummary::
   :toctree: generated/

   diag
   diagflat
   tri
   tril
   triu
   vander

The Matrix class
----------------
.. autosummary::
   :toctree: generated/

   mat
   bmat
.. _typing:
.. automodule:: numpy.typing
distutils.misc_util
===================

.. automodule:: numpy.distutils.misc_util
   :members:
   :undoc-members:
   :exclude-members: Configuration
.. _new-or-different:

.. currentmodule:: numpy.random

What's New or Different
-----------------------

.. warning::

  The Box-Muller method used to produce NumPy's normals is no longer available
  in `Generator`.  It is not possible to reproduce the exact random
  values using ``Generator`` for the normal distribution or any other
  distribution that relies on the normal such as the `Generator.gamma` or
  `Generator.standard_t`. If you require bitwise backward compatible
  streams, use `RandomState`, i.e., `RandomState.gamma` or
  `RandomState.standard_t`.

Quick comparison of legacy :ref:`mtrand <legacy>` to the new `Generator`

================== ==================== =============
Feature            Older Equivalent     Notes
------------------ -------------------- -------------
`~.Generator`      `~.RandomState`      ``Generator`` requires a stream
                                        source, called a `BitGenerator`
                                        A number of these are provided.
                                        ``RandomState`` uses
                                        the Mersenne Twister `~.MT19937` by
                                        default, but can also be instantiated
                                        with any BitGenerator.
------------------ -------------------- -------------
``random``         ``random_sample``,   Access the values in a BitGenerator,
                   ``rand``             convert them to ``float64`` in the
                                        interval ``[0.0.,`` `` 1.0)``.
                                        In addition to the ``size`` kwarg, now
                                        supports ``dtype='d'`` or ``dtype='f'``,
                                        and an ``out`` kwarg to fill a user-
                                        supplied array.

                                        Many other distributions are also
                                        supported.
------------------ -------------------- -------------
``integers``       ``randint``,         Use the ``endpoint`` kwarg to adjust
                   ``random_integers``  the inclusion or exclution of the
                                        ``high`` interval endpoint
================== ==================== =============

And in more detail:

* Simulate from the complex normal distribution
  (`~.Generator.complex_normal`)
* The normal, exponential and gamma generators use 256-step Ziggurat
  methods which are 2-10 times faster than NumPy's default implementation in
  `~.Generator.standard_normal`, `~.Generator.standard_exponential` or
  `~.Generator.standard_gamma`.


.. ipython:: python

  from  numpy.random import Generator, PCG64
  import numpy.random
  rng = Generator(PCG64())
  %timeit -n 1 rng.standard_normal(100000)
  %timeit -n 1 numpy.random.standard_normal(100000)

.. ipython:: python

  %timeit -n 1 rng.standard_exponential(100000)
  %timeit -n 1 numpy.random.standard_exponential(100000)

.. ipython:: python

  %timeit -n 1 rng.standard_gamma(3.0, 100000)
  %timeit -n 1 numpy.random.standard_gamma(3.0, 100000)


* `~.Generator.integers` is now the canonical way to generate integer
  random numbers from a discrete uniform distribution. The ``rand`` and
  ``randn`` methods are only available through the legacy `~.RandomState`.
  This replaces both ``randint`` and the deprecated ``random_integers``.
* The Box-Muller method used to produce NumPy's normals is no longer available.
* All bit generators can produce doubles, uint64s and
  uint32s via CTypes (`~PCG64.ctypes`) and CFFI (`~PCG64.cffi`).
  This allows these bit generators to be used in numba.
* The bit generators can be used in downstream projects via
  Cython.
* Optional ``dtype`` argument that accepts ``np.float32`` or ``np.float64``
  to produce either single or double precision uniform random variables for
  select distributions

  * Uniforms (`~.Generator.random` and `~.Generator.integers`)
  * Normals (`~.Generator.standard_normal`)
  * Standard Gammas (`~.Generator.standard_gamma`)
  * Standard Exponentials (`~.Generator.standard_exponential`)

.. ipython:: python

  rng = Generator(PCG64(0))
  rng.random(3, dtype='d')
  rng.random(3, dtype='f')

* Optional ``out`` argument that allows existing arrays to be filled for
  select distributions

  * Uniforms (`~.Generator.random`)
  * Normals (`~.Generator.standard_normal`)
  * Standard Gammas (`~.Generator.standard_gamma`)
  * Standard Exponentials (`~.Generator.standard_exponential`)

  This allows multithreading to fill large arrays in chunks using suitable
  BitGenerators in parallel.

.. ipython:: python

  existing = np.zeros(4)
  rng.random(out=existing[:2])
  print(existing)

* Optional ``axis`` argument for methods like `~.Generator.choice`,
  `~.Generator.permutation` and `~.Generator.shuffle` that controls which
  axis an operation is performed over for multi-dimensional arrays.

.. ipython:: python

  rng = Generator(PCG64(123456789))
  a = np.arange(12).reshape((3, 4))
  a
  rng.choice(a, axis=1, size=5)
  rng.shuffle(a, axis=1)        # Shuffle in-place
  a
Performance
-----------

.. currentmodule:: numpy.random

Recommendation
**************

The recommended generator for general use is `PCG64` or its upgraded variant
`PCG64DXSM` for heavily-parallel use cases. They are statistically high quality,
full-featured, and fast on most platforms, but somewhat slow when compiled for
32-bit processes. See :ref:`upgrading-pcg64` for details on when heavy
parallelism would indicate using `PCG64DXSM`.

`Philox` is fairly slow, but its statistical properties have
very high quality, and it is easy to get an assuredly-independent stream by using
unique keys. If that is the style you wish to use for parallel streams, or you
are porting from another system that uses that style, then
`Philox` is your choice.

`SFC64` is statistically high quality and very fast. However, it
lacks jumpability. If you are not using that capability and want lots of speed,
even on 32-bit processes, this is your choice.

`MT19937` `fails some statistical tests`_ and is not especially
fast compared to modern PRNGs. For these reasons, we mostly do not recommend
using it on its own, only through the legacy `~.RandomState` for
reproducing old results. That said, it has a very long history as a default in
many systems.

.. _`fails some statistical tests`: https://www.iro.umontreal.ca/~lecuyer/myftp/papers/testu01.pdf

Timings
*******

The timings below are the time in ns to produce 1 random value from a
specific distribution.  The original `MT19937` generator is
much slower since it requires 2 32-bit values to equal the output of the
faster generators.

Integer performance has a similar ordering.

The pattern is similar for other, more complex generators. The normal
performance of the legacy `RandomState` generator is much
lower than the other since it uses the Box-Muller transform rather
than the Ziggurat method. The performance gap for Exponentials is also
large due to the cost of computing the log function to invert the CDF.
The column labeled MT19973 uses the same 32-bit generator as
`RandomState` but produces random variates using `Generator`.

.. csv-table::
    :header: ,MT19937,PCG64,PCG64DXSM,Philox,SFC64,RandomState
    :widths: 14,14,14,14,14,14,14

    32-bit Unsigned Ints,3.3,1.9,2.0,3.3,1.8,3.1
    64-bit Unsigned Ints,5.6,3.2,2.9,4.9,2.5,5.5
    Uniforms,5.9,3.1,2.9,5.0,2.6,6.0
    Normals,13.9,10.8,10.5,12.0,8.3,56.8
    Exponentials,9.1,6.0,5.8,8.1,5.4,63.9
    Gammas,37.2,30.8,28.9,34.0,27.5,77.0
    Binomials,21.3,17.4,17.6,19.3,15.6,21.4
    Laplaces,73.2,72.3,76.1,73.0,72.3,82.5
    Poissons,111.7,103.4,100.5,109.4,90.7,115.2

The next table presents the performance in percentage relative to values
generated by the legacy generator, ``RandomState(MT19937())``. The overall
performance was computed using a geometric mean.

.. csv-table::
    :header: ,MT19937,PCG64,PCG64DXSM,Philox,SFC64
    :widths: 14,14,14,14,14,14

    32-bit Unsigned Ints,96,162,160,96,175
    64-bit Unsigned Ints,97,171,188,113,218
    Uniforms,102,192,206,121,233
    Normals,409,526,541,471,684
    Exponentials,701,1071,1101,784,1179
    Gammas,207,250,266,227,281
    Binomials,100,123,122,111,138
    Laplaces,113,114,108,113,114
    Poissons,103,111,115,105,127
    Overall,159,219,225,174,251

.. note::

   All timings were taken using Linux on an AMD Ryzen 9 3900X processor.

Performance on different Operating Systems
******************************************
Performance differs across platforms due to compiler and hardware availability
(e.g., register width) differences. The default bit generator has been chosen
to perform well on 64-bit platforms.  Performance on 32-bit operating systems
is very different.

The values reported are normalized relative to the speed of MT19937 in
each table. A value of 100 indicates that the performance matches the MT19937.
Higher values indicate improved performance. These values cannot be compared
across tables.

64-bit Linux
~~~~~~~~~~~~

=====================   =========  =======  ===========  ========  =======
Distribution            MT19937    PCG64    PCG64DXSM    Philox    SFC64
=====================   =========  =======  ===========  ========  =======
32-bit Unsigned Ints          100      168         166        100      182
64-bit Unsigned Ints          100      176         193        116      224
Uniforms                      100      188         202        118      228
Normals                       100      128         132        115      167
Exponentials                  100      152         157        111      168
Overall                       100      161         168        112      192
=====================   =========  =======  ===========  ========  =======


64-bit Windows
~~~~~~~~~~~~~~
The relative performance on 64-bit Linux and 64-bit Windows is broadly similar
with the notable exception of the Philox generator.

=====================   =========  =======  ===========  ========  =======
Distribution              MT19937    PCG64    PCG64DXSM    Philox    SFC64
=====================   =========  =======  ===========  ========  =======
32-bit Unsigned Ints          100      155          131        29      150
64-bit Unsigned Ints          100      157          143        25      154
Uniforms                      100      151          144        24      155
Normals                       100      129          128        37      150
Exponentials                  100      150          145        28      159
**Overall**                   100      148          138        28      154
=====================   =========  =======  ===========  ========  =======


32-bit Windows
~~~~~~~~~~~~~~

The performance of 64-bit generators on 32-bit Windows is much lower than on 64-bit
operating systems due to register width. MT19937, the generator that has been
in NumPy since 2005, operates on 32-bit integers.

=====================   =========  =======  ===========  ========  =======
Distribution            MT19937    PCG64    PCG64DXSM    Philox    SFC64
=====================   =========  =======  ===========  ========  =======
32-bit Unsigned Ints          100       24           34        14       57
64-bit Unsigned Ints          100       21           32        14       74
Uniforms                      100       21           34        16       73
Normals                       100       36           57        28      101
Exponentials                  100       28           44        20       88
**Overall**                   100       25           39        18       77
=====================   =========  =======  ===========  ========  =======


.. note::

   Linux timings used Ubuntu 20.04 and GCC 9.3.0.  Windows timings were made on
   Windows 10 using Microsoft C/C++ Optimizing Compiler Version 19 (Visual
   Studio 2019). All timings were produced on an AMD Ryzen 9 3900X processor.
.. currentmodule:: numpy.random

Random Generator
----------------
The `~Generator` provides access to
a wide range of distributions, and served as a replacement for
:class:`~numpy.random.RandomState`.  The main difference between
the two is that ``Generator`` relies on an additional BitGenerator to
manage state and generate the random bits, which are then transformed into
random values from useful distributions. The default BitGenerator used by
``Generator`` is `~PCG64`.  The BitGenerator
can be changed by passing an instantized BitGenerator to ``Generator``.


.. autofunction:: default_rng

.. autoclass:: Generator
    :members: __init__
    :exclude-members: __init__

Accessing the BitGenerator
==========================
.. autosummary::
   :toctree: generated/

   ~numpy.random.Generator.bit_generator

Simple random data
==================
.. autosummary::
   :toctree: generated/

   ~numpy.random.Generator.integers
   ~numpy.random.Generator.random
   ~numpy.random.Generator.choice
   ~numpy.random.Generator.bytes

Permutations
============
The methods for randomly permuting a sequence are

.. autosummary::
   :toctree: generated/

   ~numpy.random.Generator.shuffle
   ~numpy.random.Generator.permutation
   ~numpy.random.Generator.permuted

The following table summarizes the behaviors of the methods.

+--------------+-------------------+------------------+
| method       | copy/in-place     | axis handling    |
+==============+===================+==================+
| shuffle      | in-place          | as if 1d         |
+--------------+-------------------+------------------+
| permutation  | copy              | as if 1d         |
+--------------+-------------------+------------------+
| permuted     | either (use 'out' | axis independent |
|              | for in-place)     |                  |
+--------------+-------------------+------------------+

The following subsections provide more details about the differences.

In-place vs. copy
~~~~~~~~~~~~~~~~~
The main difference between `Generator.shuffle` and `Generator.permutation`
is that `Generator.shuffle` operates in-place, while `Generator.permutation`
returns a copy.

By default, `Generator.permuted` returns a copy.  To operate in-place with
`Generator.permuted`, pass the same array as the first argument *and* as
the value of the ``out`` parameter.  For example,

    >>> rng = np.random.default_rng()
    >>> x = np.arange(0, 15).reshape(3, 5)
    >>> x #doctest: +SKIP
    array([[ 0,  1,  2,  3,  4],
           [ 5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14]])
    >>> y = rng.permuted(x, axis=1, out=x)
    >>> x #doctest: +SKIP
    array([[ 1,  0,  2,  4,  3],  # random
           [ 6,  7,  8,  9,  5],
           [10, 14, 11, 13, 12]])

Note that when ``out`` is given, the return value is ``out``:

    >>> y is x
    True

Handling the ``axis`` parameter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
An important distinction for these methods is how they handle the ``axis``
parameter.  Both `Generator.shuffle` and `Generator.permutation` treat the
input as a one-dimensional sequence, and the ``axis`` parameter determines
which dimension of the input array to use as the sequence. In the case of a
two-dimensional array, ``axis=0`` will, in effect, rearrange the rows of the
array, and  ``axis=1`` will rearrange the columns.  For example

    >>> rng = np.random.default_rng()
    >>> x = np.arange(0, 15).reshape(3, 5)
    >>> x
    array([[ 0,  1,  2,  3,  4],
           [ 5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14]])
    >>> rng.permutation(x, axis=1) #doctest: +SKIP
    array([[ 1,  3,  2,  0,  4],  # random
           [ 6,  8,  7,  5,  9],
           [11, 13, 12, 10, 14]])

Note that the columns have been rearranged "in bulk": the values within
each column have not changed.

The method `Generator.permuted` treats the ``axis`` parameter similar to
how `numpy.sort` treats it.  Each slice along the given axis is shuffled
independently of the others.  Compare the following example of the use of
`Generator.permuted` to the above example of `Generator.permutation`:

    >>> rng.permuted(x, axis=1) #doctest: +SKIP
    array([[ 1,  0,  2,  4,  3],  # random
           [ 5,  7,  6,  9,  8],
           [10, 14, 12, 13, 11]])

In this example, the values within each row (i.e. the values along
``axis=1``) have been shuffled independently.  This is not a "bulk"
shuffle of the columns.

Shuffling non-NumPy sequences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
`Generator.shuffle` works on non-NumPy sequences.  That is, if it is given
a sequence that is not a NumPy array, it shuffles that sequence in-place.
For example,

    >>> rng = np.random.default_rng()
    >>> a = ['A', 'B', 'C', 'D', 'E']
    >>> rng.shuffle(a)  # shuffle the list in-place
    >>> a #doctest: +SKIP
    ['B', 'D', 'A', 'E', 'C']  # random

Distributions
=============
.. autosummary::
   :toctree: generated/

   ~numpy.random.Generator.beta
   ~numpy.random.Generator.binomial
   ~numpy.random.Generator.chisquare
   ~numpy.random.Generator.dirichlet
   ~numpy.random.Generator.exponential
   ~numpy.random.Generator.f
   ~numpy.random.Generator.gamma
   ~numpy.random.Generator.geometric
   ~numpy.random.Generator.gumbel
   ~numpy.random.Generator.hypergeometric
   ~numpy.random.Generator.laplace
   ~numpy.random.Generator.logistic
   ~numpy.random.Generator.lognormal
   ~numpy.random.Generator.logseries
   ~numpy.random.Generator.multinomial
   ~numpy.random.Generator.multivariate_hypergeometric
   ~numpy.random.Generator.multivariate_normal
   ~numpy.random.Generator.negative_binomial
   ~numpy.random.Generator.noncentral_chisquare
   ~numpy.random.Generator.noncentral_f
   ~numpy.random.Generator.normal
   ~numpy.random.Generator.pareto
   ~numpy.random.Generator.poisson
   ~numpy.random.Generator.power
   ~numpy.random.Generator.rayleigh
   ~numpy.random.Generator.standard_cauchy
   ~numpy.random.Generator.standard_exponential
   ~numpy.random.Generator.standard_gamma
   ~numpy.random.Generator.standard_normal
   ~numpy.random.Generator.standard_t
   ~numpy.random.Generator.triangular
   ~numpy.random.Generator.uniform
   ~numpy.random.Generator.vonmises
   ~numpy.random.Generator.wald
   ~numpy.random.Generator.weibull
   ~numpy.random.Generator.zipf
.. currentmodule:: numpy.random

.. _extending:

Extending
---------
The BitGenerators have been designed to be extendable using standard tools for
high-performance Python -- numba and Cython.  The `~Generator` object can also
be used with user-provided BitGenerators as long as these export a small set of
required functions.

Numba
=====
Numba can be used with either CTypes or CFFI.  The current iteration of the
BitGenerators all export a small set of functions through both interfaces.

This example shows how numba can be used to produce gaussian samples using
a pure Python implementation which is then compiled.  The random numbers are
provided by ``ctypes.next_double``.

.. literalinclude:: ../../../../numpy/random/_examples/numba/extending.py
    :language: python
    :end-before: example 2

Both CTypes and CFFI allow the more complicated distributions to be used
directly in Numba after compiling the file distributions.c into a ``DLL`` or
``so``.  An example showing the use of a more complicated distribution is in
the `examples` section below.

.. _random_cython:

Cython
======

Cython can be used to unpack the ``PyCapsule`` provided by a BitGenerator.
This example uses `PCG64` and the example from above.  The usual caveats
for writing high-performance code using Cython -- removing bounds checks and
wrap around, providing array alignment information -- still apply.

.. literalinclude:: ../../../../numpy/random/_examples/cython/extending_distributions.pyx
    :language: cython
    :end-before: example 2

The BitGenerator can also be directly accessed using the members of the ``bitgen_t``
struct.

.. literalinclude:: ../../../../numpy/random/_examples/cython/extending_distributions.pyx
    :language: cython
    :start-after: example 2
    :end-before: example 3

Cython can be used to directly access the functions in
``numpy/random/c_distributions.pxd``. This requires linking with the
``npyrandom`` library located in ``numpy/random/lib``.

.. literalinclude:: ../../../../numpy/random/_examples/cython/extending_distributions.pyx
    :language: cython
    :start-after: example 3

See :ref:`extending_cython_example` for the complete listings of these examples
and a minimal ``setup.py`` to build the c-extension modules.

CFFI
====

CFFI can be used to directly access the functions in
``include/numpy/random/distributions.h``. Some "massaging" of the header
file is required:

.. literalinclude:: ../../../../numpy/random/_examples/cffi/extending.py
    :language: python
    :end-before: dlopen

Once the header is parsed by ``ffi.cdef``, the functions can be accessed
directly from the ``_generator`` shared object, using the `BitGenerator.cffi` interface.

.. literalinclude:: ../../../../numpy/random/_examples/cffi/extending.py
    :language: python
    :start-after: dlopen


New Bit Generators
==================
`~Generator` can be used with user-provided `~BitGenerator`\ s. The simplest
way to write a new BitGenerator is to examine the pyx file of one of the
existing BitGenerators. The key structure that must be provided is the
``capsule`` which contains a ``PyCapsule`` to a struct pointer of type
``bitgen_t``,

.. code-block:: c

  typedef struct bitgen {
    void *state;
    uint64_t (*next_uint64)(void *st);
    uint32_t (*next_uint32)(void *st);
    double (*next_double)(void *st);
    uint64_t (*next_raw)(void *st);
  } bitgen_t;

which provides 5 pointers. The first is an opaque pointer to the data structure
used by the BitGenerators.  The next three are function pointers which return
the next 64- and 32-bit unsigned integers, the next random double and the next
raw value.  This final function is used for testing and so can be set to
the next 64-bit unsigned integer function if not needed. Functions inside
``Generator`` use this structure as in

.. code-block:: c

  bitgen_state->next_uint64(bitgen_state->state)

Examples
========

.. toctree::
    Numba <examples/numba>
    CFFI + Numba <examples/numba_cffi> 
    Cython <examples/cython/index>
    CFFI <examples/cffi>
.. _upgrading-pcg64:

.. currentmodule:: numpy.random

Upgrading ``PCG64`` with ``PCG64DXSM``
--------------------------------------

Uses of the `PCG64` `BitGenerator` in a massively-parallel context have been
shown to have statistical weaknesses that were not apparent at the first
release in numpy 1.17. Most users will never observe this weakness and are
safe to continue to use `PCG64`. We have introduced a new `PCG64DXSM`
`BitGenerator` that will eventually become the new default `BitGenerator`
implementation used by `default_rng` in future releases. `PCG64DXSM` solves
the statistical weakness while preserving the performance and the features of
`PCG64`.

Does this affect me?
====================

If you

  1. only use a single `Generator` instance,
  2. only use `RandomState` or the functions in `numpy.random`,
  3. only use the `PCG64.jumped` method to generate parallel streams,
  4. explicitly use a `BitGenerator` other than `PCG64`,

then this weakness does not affect you at all. Carry on.

If you use moderate numbers of parallel streams created with `default_rng` or
`SeedSequence.spawn`, in the 1000s, then the chance of observing this weakness
is negligibly small. You can continue to use `PCG64` comfortably.

If you use very large numbers of parallel streams, in the millions, and draw
large amounts of numbers from each, then the chance of observing this weakness
can become non-negligible, if still small. An example of such a use case would
be a very large distributed reinforcement learning problem with millions of
long Monte Carlo playouts each generating billions of random number draws. Such
use cases should consider using `PCG64DXSM` explicitly or another
modern `BitGenerator` like `SFC64` or `Philox`, but it is unlikely that any
old results you may have calculated are invalid. In any case, the weakness is
a kind of `Birthday Paradox <https://en.wikipedia.org/wiki/Birthday_problem>`_
collision. That is, a single pair of parallel streams out of the millions,
considered together, might fail a stringent set of statistical tests of
randomness. The remaining millions of streams would all be perfectly fine, and
the effect of the bad pair in the whole calculation is very likely to be
swamped by the remaining streams in most applications.

.. _upgrading-pcg64-details:

Technical Details
=================

Like many PRNG algorithms, `PCG64` is constructed from a transition function,
which advances a 128-bit state, and an output function, that mixes the 128-bit
state into a 64-bit integer to be output. One of the guiding design principles
of the PCG family of PRNGs is to balance the computational cost (and
pseudorandomness strength) between the transition function and the output
function. The transition function is a 128-bit linear congruential generator
(LCG), which consists of multiplying the 128-bit state with a fixed
multiplication constant and then adding a user-chosen increment, in 128-bit
modular arithmetic. LCGs are well-analyzed PRNGs with known weaknesses, though
128-bit LCGs are large enough to pass stringent statistical tests on their own,
with only the trivial output function. The output function of `PCG64` is
intended to patch up some of those known weaknesses by doing "just enough"
scrambling of the bits to assist in the statistical properties without adding
too much computational cost.

One of these known weaknesses is that advancing the state of the LCG by steps
numbering a power of two (``bg.advance(2**N)``) will leave the lower ``N`` bits
identical to the state that was just left. For a single stream drawn from
sequentially, this is of little consequence. The remaining :math:`128-N` bits provide
plenty of pseudorandomness that will be mixed in for any practical ``N`` that can
be observed in a single stream, which is why one does not need to worry about
this if you only use a single stream in your application. Similarly, the
`PCG64.jumped` method uses a carefully chosen number of steps to avoid creating
these collisions. However, once you start creating "randomly-initialized"
parallel streams, either using OS entropy by calling `default_rng` repeatedly
or using `SeedSequence.spawn`, then we need to consider how many lower bits
need to "collide" in order to create a bad pair of streams, and then evaluate
the probability of creating such a collision.
`Empirically <https://github.com/numpy/numpy/issues/16313>`_, it has been
determined that if one shares the lower 58 bits of state and shares an
increment, then the pair of streams, when interleaved, will fail 
`PractRand <http://pracrand.sourceforge.net/>`_ in
a reasonable amount of time, after drawing a few gigabytes of data. Following
the standard Birthday Paradox calculations for a collision of 58 bits, we can
see that we can create :math:`2^{29}`, or about half a billion, streams which is when
the probability of such a collision becomes high. Half a billion streams is
quite high, and the amount of data each stream needs to draw before the
statistical correlations become apparent to even the strict ``PractRand`` tests
is in the gigabytes. But this is on the horizon for very large applications
like distributed reinforcement learning. There are reasons to expect that even
in these applications a collision probably will not have a practical effect in
the total result, since the statistical problem is constrained to just the
colliding pair.

Now, let us consider the case when the increment is not constrained to be the
same. Our implementation of `PCG64` seeds both the state and the increment;
that is, two calls to `default_rng` (almost certainly) have different states
and increments. Upon our first release, we believed that having the seeded
increment would provide a certain amount of extra protection, that one would
have to be "close" in both the state space and increment space in order to
observe correlations (``PractRand`` failures) in a pair of streams. If that were
true, then the "bottleneck" for collisions would be the 128-bit entropy pool
size inside of `SeedSequence` (and 128-bit collisions are in the
"preposterously unlikely" category). Unfortunately, this is not true.

One of the known properties of an LCG is that different increments create
*distinct* streams, but with a known relationship. Each LCG has an orbit that
traverses all :math:`2^{128}` different 128-bit states. Two LCGs with different
increments are related in that one can "rotate" the orbit of the first LCG
(advance it by a number of steps that we can compute from the two increments)
such that then both LCGs will always then have the same state, up to an
additive constant and maybe an inversion of the bits. If you then iterate both
streams in lockstep, then the states will *always* remain related by that same
additive constant (and the inversion, if present). Recall that `PCG64` is
constructed from both a transition function (the LCG) and an output function.
It was expected that the scrambling effect of the output function would have
been strong enough to make the distinct streams practically independent (i.e.
"passing the ``PractRand`` tests") unless the two increments were
pathologically related to each other (e.g. 1 and 3). The output function XSL-RR
of the then-standard PCG algorithm that we implemented in `PCG64` turns out to
be too weak to cover up for the 58-bit collision of the underlying LCG that we
described above. For any given pair of increments, the size of the "colliding"
space of states is the same, so for this weakness, the extra distinctness
provided by the increments does not translate into extra protection from
statistical correlations that ``PractRand`` can detect.

Fortunately, strengthening the output function is able to correct this weakness
and *does* turn the extra distinctness provided by differing increments into
additional protection from these low-bit collisions. To the `PCG author's
credit <https://github.com/numpy/numpy/issues/13635#issuecomment-506088698>`_,
she had developed a stronger output function in response to related discussions
during the long birth of the new `BitGenerator` system. We NumPy developers
chose to be "conservative" and use the XSL-RR variant that had undergone
a longer period of testing at that time. The DXSM output function adopts
a "xorshift-multiply" construction used in strong integer hashes that has much
better avalanche properties than the XSL-RR output function. While there are
"pathological" pairs of increments that induce "bad" additive constants that
relate the two streams, the vast majority of pairs induce "good" additive
constants that make the merely-distinct streams of LCG states into
practically-independent output streams. Indeed, now the claim we once made
about `PCG64` is actually true of `PCG64DXSM`: collisions are possible, but
both streams have to simultaneously be both "close" in the 128 bit state space
*and* "close" in the 127-bit increment space, so that would be less likely than
the negligible chance of colliding in the 128-bit internal `SeedSequence` pool.
The DXSM output function is more computationally intensive than XSL-RR, but
some optimizations in the LCG more than make up for the performance hit on most
machines, so `PCG64DXSM` is a good, safe upgrade. There are, of course, an
infinite number of stronger output functions that one could consider, but most
will have a greater computational cost, and the DXSM output function has now
received many CPU cycles of testing via ``PractRand`` at this time.
C API for random
----------------

.. currentmodule:: numpy.random

.. versionadded:: 1.19.0

Access to various distributions below is available via Cython or C-wrapper
libraries like CFFI. All the functions accept a :c:type:`bitgen_t` as their
first argument.  To access these from Cython or C, you must link with the
``npyrandom`` library which is part of the NumPy distribution, located in
``numpy/random/lib``.


.. c:type:: bitgen_t

    The :c:type:`bitgen_t` holds the current state of the BitGenerator and
    pointers to functions that return standard C types while advancing the
    state.

    .. code-block:: c

        struct bitgen:
            void *state
            npy_uint64 (*next_uint64)(void *st) nogil
            uint32_t (*next_uint32)(void *st) nogil
            double (*next_double)(void *st) nogil
            npy_uint64 (*next_raw)(void *st) nogil

        ctypedef bitgen bitgen_t

See :doc:`extending` for examples of using these functions.

The functions are named with the following conventions:

- "standard" refers to the reference values for any parameters. For instance
  "standard_uniform" means a uniform distribution on the interval ``0.0`` to
  ``1.0``

- "fill" functions will fill the provided ``out`` with ``cnt`` values.

- The functions without "standard" in their name require additional parameters
  to describe the distributions.

- Functions with ``inv`` in their name are based on the slower inverse method
  instead of a ziggurat lookup algorithm, which is significantly faster. The
  non-ziggurat variants are used in corner cases and for legacy compatibility.


.. c:function:: double random_standard_uniform(bitgen_t *bitgen_state)

.. c:function:: void random_standard_uniform_fill(bitgen_t* bitgen_state, npy_intp cnt, double *out)

.. c:function:: double random_standard_exponential(bitgen_t *bitgen_state)

.. c:function:: void random_standard_exponential_fill(bitgen_t *bitgen_state, npy_intp cnt, double *out)

.. c:function:: void random_standard_exponential_inv_fill(bitgen_t *bitgen_state, npy_intp cnt, double *out)

.. c:function:: double random_standard_normal(bitgen_t* bitgen_state)

.. c:function:: void random_standard_normal_fill(bitgen_t *bitgen_state, npy_intp count, double *out)

.. c:function:: void random_standard_normal_fill_f(bitgen_t *bitgen_state, npy_intp count, float *out)

.. c:function:: double random_standard_gamma(bitgen_t *bitgen_state, double shape)

.. c:function:: float random_standard_uniform_f(bitgen_t *bitgen_state)

.. c:function:: void random_standard_uniform_fill_f(bitgen_t* bitgen_state, npy_intp cnt, float *out)

.. c:function:: float random_standard_exponential_f(bitgen_t *bitgen_state)

.. c:function:: void random_standard_exponential_fill_f(bitgen_t *bitgen_state, npy_intp cnt, float *out)

.. c:function:: void random_standard_exponential_inv_fill_f(bitgen_t *bitgen_state, npy_intp cnt, float *out)

.. c:function:: float random_standard_normal_f(bitgen_t* bitgen_state)

.. c:function:: float random_standard_gamma_f(bitgen_t *bitgen_state, float shape)

.. c:function:: double random_normal(bitgen_t *bitgen_state, double loc, double scale)

.. c:function:: double random_gamma(bitgen_t *bitgen_state, double shape, double scale)

.. c:function:: float random_gamma_f(bitgen_t *bitgen_state, float shape, float scale)

.. c:function:: double random_exponential(bitgen_t *bitgen_state, double scale)

.. c:function:: double random_uniform(bitgen_t *bitgen_state, double lower, double range)

.. c:function:: double random_beta(bitgen_t *bitgen_state, double a, double b)

.. c:function:: double random_chisquare(bitgen_t *bitgen_state, double df)

.. c:function:: double random_f(bitgen_t *bitgen_state, double dfnum, double dfden)

.. c:function:: double random_standard_cauchy(bitgen_t *bitgen_state)

.. c:function:: double random_pareto(bitgen_t *bitgen_state, double a)

.. c:function:: double random_weibull(bitgen_t *bitgen_state, double a)

.. c:function:: double random_power(bitgen_t *bitgen_state, double a)

.. c:function:: double random_laplace(bitgen_t *bitgen_state, double loc, double scale)

.. c:function:: double random_gumbel(bitgen_t *bitgen_state, double loc, double scale)

.. c:function:: double random_logistic(bitgen_t *bitgen_state, double loc, double scale)

.. c:function:: double random_lognormal(bitgen_t *bitgen_state, double mean, double sigma)

.. c:function:: double random_rayleigh(bitgen_t *bitgen_state, double mode)

.. c:function:: double random_standard_t(bitgen_t *bitgen_state, double df)

.. c:function:: double random_noncentral_chisquare(bitgen_t *bitgen_state, double df, double nonc)
.. c:function:: double random_noncentral_f(bitgen_t *bitgen_state, double dfnum, double dfden, double nonc)
.. c:function:: double random_wald(bitgen_t *bitgen_state, double mean, double scale)

.. c:function:: double random_vonmises(bitgen_t *bitgen_state, double mu, double kappa)

.. c:function:: double random_triangular(bitgen_t *bitgen_state, double left, double mode, double right)

.. c:function:: npy_int64 random_poisson(bitgen_t *bitgen_state, double lam)

.. c:function:: npy_int64 random_negative_binomial(bitgen_t *bitgen_state, double n, double p)

.. c:type:: binomial_t

    .. code-block:: c

        typedef struct s_binomial_t {
          int has_binomial; /* !=0: following parameters initialized for binomial */
          double psave;
          RAND_INT_TYPE nsave;
          double r;
          double q;
          double fm;
          RAND_INT_TYPE m;
          double p1;
          double xm;
          double xl;
          double xr;
          double c;
          double laml;
          double lamr;
          double p2;
          double p3;
          double p4;
        } binomial_t;
     

.. c:function:: npy_int64 random_binomial(bitgen_t *bitgen_state, double p, npy_int64 n, binomial_t *binomial)

.. c:function:: npy_int64 random_logseries(bitgen_t *bitgen_state, double p)

.. c:function:: npy_int64 random_geometric_search(bitgen_t *bitgen_state, double p)

.. c:function:: npy_int64 random_geometric_inversion(bitgen_t *bitgen_state, double p)

.. c:function:: npy_int64 random_geometric(bitgen_t *bitgen_state, double p)

.. c:function:: npy_int64 random_zipf(bitgen_t *bitgen_state, double a)

.. c:function:: npy_int64 random_hypergeometric(bitgen_t *bitgen_state, npy_int64 good, npy_int64 bad, npy_int64 sample)

.. c:function:: npy_uint64 random_interval(bitgen_t *bitgen_state, npy_uint64 max)

.. c:function:: void random_multinomial(bitgen_t *bitgen_state, npy_int64 n, npy_int64 *mnix, double *pix, npy_intp d, binomial_t *binomial)

.. c:function:: int random_multivariate_hypergeometric_count(bitgen_t *bitgen_state, npy_int64 total, size_t num_colors, npy_int64 *colors, npy_int64 nsample, size_t num_variates, npy_int64 *variates)

.. c:function:: void random_multivariate_hypergeometric_marginals(bitgen_t *bitgen_state, npy_int64 total, size_t num_colors, npy_int64 *colors, npy_int64 nsample, size_t num_variates, npy_int64 *variates)

Generate a single integer

.. c:function:: npy_int64 random_positive_int64(bitgen_t *bitgen_state)

.. c:function:: npy_int32 random_positive_int32(bitgen_t *bitgen_state)

.. c:function:: npy_int64 random_positive_int(bitgen_t *bitgen_state)

.. c:function:: npy_uint64 random_uint(bitgen_t *bitgen_state)


Generate random uint64 numbers in closed interval [off, off + rng].

.. c:function:: npy_uint64 random_bounded_uint64(bitgen_t *bitgen_state, npy_uint64 off, npy_uint64 rng, npy_uint64 mask, bool use_masked)

Multithreaded Generation
========================

The four core distributions (:meth:`~.Generator.random`,
:meth:`~.Generator.standard_normal`, :meth:`~.Generator.standard_exponential`,
and :meth:`~.Generator.standard_gamma`) all allow existing arrays to be filled
using the ``out`` keyword argument. Existing arrays need to be contiguous and
well-behaved (writable and aligned). Under normal circumstances, arrays
created using the common constructors such as :meth:`numpy.empty` will satisfy
these requirements.

This example makes use of Python 3 :mod:`concurrent.futures` to fill an array
using multiple threads.  Threads are long-lived so that repeated calls do not
require any additional overheads from thread creation.

The random numbers generated are reproducible in the sense that the same
seed will produce the same outputs, given that the number of threads does not
change.

.. code-block:: ipython

    from numpy.random import default_rng, SeedSequence
    import multiprocessing
    import concurrent.futures
    import numpy as np

    class MultithreadedRNG:
        def __init__(self, n, seed=None, threads=None):
            if threads is None:
                threads = multiprocessing.cpu_count()
            self.threads = threads

            seq = SeedSequence(seed)
            self._random_generators = [default_rng(s)
                                       for s in seq.spawn(threads)]

            self.n = n
            self.executor = concurrent.futures.ThreadPoolExecutor(threads)
            self.values = np.empty(n)
            self.step = np.ceil(n / threads).astype(np.int_)

        def fill(self):
            def _fill(random_state, out, first, last):
                random_state.standard_normal(out=out[first:last])

            futures = {}
            for i in range(self.threads):
                args = (_fill,
                        self._random_generators[i],
                        self.values,
                        i * self.step,
                        (i + 1) * self.step)
                futures[self.executor.submit(*args)] = i
            concurrent.futures.wait(futures)

        def __del__(self):
            self.executor.shutdown(False)



The multithreaded random number generator can be used to fill an array.
The ``values`` attributes shows the zero-value before the fill and the
random value after.

.. code-block:: ipython

    In [2]: mrng = MultithreadedRNG(10000000, seed=12345)
       ...: print(mrng.values[-1])
    Out[2]: 0.0

    In [3]: mrng.fill()
       ...: print(mrng.values[-1])
    Out[3]: 2.4545724517479104

The time required to produce using multiple threads can be compared to
the time required to generate using a single thread.

.. code-block:: ipython

    In [4]: print(mrng.threads)
       ...: %timeit mrng.fill()

    Out[4]: 4
       ...: 32.8 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

The single threaded call directly uses the BitGenerator.

.. code-block:: ipython

    In [5]: values = np.empty(10000000)
       ...: rg = default_rng()
       ...: %timeit rg.standard_normal(out=values)

    Out[5]: 99.6 ms ± 222 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

The gains are substantial and the scaling is reasonable even for arrays that
are only moderately large. The gains are even larger when compared to a call
that does not use an existing array due to array creation overhead.

.. code-block:: ipython

    In [6]: rg = default_rng()
       ...: %timeit rg.standard_normal(10000000)

    Out[6]: 125 ms ± 309 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Note that if `threads` is not set by the user, it will be determined by
`multiprocessing.cpu_count()`.

.. code-block:: ipython

    In [7]: # simulate the behavior for `threads=None`, if the machine had only one thread
       ...: mrng = MultithreadedRNG(10000000, seed=12345, threads=1)
       ...: print(mrng.values[-1])
    Out[7]: 1.1800150052158556
.. _numpyrandom:

.. py:module:: numpy.random

.. currentmodule:: numpy.random

Random sampling (:mod:`numpy.random`)
=====================================

Numpy's random number routines produce pseudo random numbers using
combinations of a `BitGenerator` to create sequences and a `Generator`
to use those sequences to sample from different statistical distributions:

* BitGenerators: Objects that generate random numbers. These are typically
  unsigned integer words filled with sequences of either 32 or 64 random bits.
* Generators: Objects that transform sequences of random bits from a
  BitGenerator into sequences of numbers that follow a specific probability
  distribution (such as uniform, Normal or Binomial) within a specified
  interval.

Since Numpy version 1.17.0 the Generator can be initialized with a
number of different BitGenerators. It exposes many different probability
distributions. See `NEP 19 <https://www.numpy.org/neps/
nep-0019-rng-policy.html>`_ for context on the updated random Numpy number
routines. The legacy `RandomState` random number routines are still
available, but limited to a single BitGenerator. See :ref:`new-or-different` 
for a complete list of improvements and differences from the legacy
``RandomState``.

For convenience and backward compatibility, a single `RandomState`
instance's methods are imported into the numpy.random namespace, see
:ref:`legacy` for the complete list.

.. _random-quick-start:

Quick Start
-----------

Call `default_rng` to get a new instance of a `Generator`, then call its
methods to obtain samples from different distributions.  By default,
`Generator` uses bits provided by `PCG64` which has better statistical
properties than the legacy `MT19937` used in `RandomState`.

.. code-block:: python

  # Do this (new version)
  from numpy.random import default_rng
  rng = default_rng()
  vals = rng.standard_normal(10)
  more_vals = rng.standard_normal(10)

  # instead of this (legacy version)
  from numpy import random
  vals = random.standard_normal(10)
  more_vals = random.standard_normal(10)

`Generator` can be used as a replacement for `RandomState`. Both class
instances hold an internal `BitGenerator` instance to provide the bit
stream, it is accessible as ``gen.bit_generator``. Some long-overdue API
cleanup means that legacy and compatibility methods have been removed from
`Generator`

=================== ============== ============
`RandomState`       `Generator`    Notes
------------------- -------------- ------------
``random_sample``,  ``random``     Compatible with `random.random`
``rand``
------------------- -------------- ------------
``randint``,        ``integers``   Add an ``endpoint`` kwarg
``random_integers``
------------------- -------------- ------------
``tomaxint``        removed        Use ``integers(0, np.iinfo(np.int_).max,``
                                   ``endpoint=False)``
------------------- -------------- ------------
``seed``            removed        Use `SeedSequence.spawn`
=================== ============== ============

See :ref:`new-or-different` for more information.

Something like the following code can be used to support both ``RandomState``
and ``Generator``, with the understanding that the interfaces are slightly
different

.. code-block:: python

    try:
        rng_integers = rng.integers
    except AttributeError:
        rng_integers = rng.randint
    a = rng_integers(1000)

Seeds can be passed to any of the BitGenerators. The provided value is mixed
via `SeedSequence` to spread a possible sequence of seeds across a wider
range of initialization states for the BitGenerator. Here `PCG64` is used and
is wrapped with a `Generator`.

.. code-block:: python

  from numpy.random import Generator, PCG64
  rng = Generator(PCG64(12345))
  rng.standard_normal()
  
Here we use `default_rng` to create an instance of `Generator` to generate a 
random float:
 
>>> import numpy as np
>>> rng = np.random.default_rng(12345)
>>> print(rng)
Generator(PCG64)
>>> rfloat = rng.random()
>>> rfloat
0.22733602246716966
>>> type(rfloat)
<class 'float'>
 
Here we use `default_rng` to create an instance of `Generator` to generate 3 
random integers between 0 (inclusive) and 10 (exclusive):
    
>>> import numpy as np
>>> rng = np.random.default_rng(12345)
>>> rints = rng.integers(low=0, high=10, size=3)
>>> rints
array([6, 2, 7])
>>> type(rints[0])
<class 'numpy.int64'> 

Introduction
------------
The new infrastructure takes a different approach to producing random numbers
from the `RandomState` object.  Random number generation is separated into
two components, a bit generator and a random generator.

The `BitGenerator` has a limited set of responsibilities. It manages state
and provides functions to produce random doubles and random unsigned 32- and
64-bit values.

The `random generator <Generator>` takes the
bit generator-provided stream and transforms them into more useful
distributions, e.g., simulated normal random values. This structure allows
alternative bit generators to be used with little code duplication.

The `Generator` is the user-facing object that is nearly identical to the
legacy `RandomState`. It accepts a bit generator instance as an argument.
The default is currently `PCG64` but this may change in future versions. 
As a convenience NumPy  provides the `default_rng` function to hide these 
details:
  
>>> from numpy.random import default_rng
>>> rng = default_rng(12345)
>>> print(rng)
Generator(PCG64)
>>> print(rng.random())
0.22733602246716966
  
One can also instantiate `Generator` directly with a `BitGenerator` instance.

To use the default `PCG64` bit generator, one can instantiate it directly and 
pass it to `Generator`:

>>> from numpy.random import Generator, PCG64
>>> rng = Generator(PCG64(12345))
>>> print(rng)
Generator(PCG64)

Similarly to use the older `MT19937` bit generator (not recommended), one can
instantiate it directly and pass it to `Generator`:

>>> from numpy.random import Generator, MT19937
>>> rng = Generator(MT19937(12345))
>>> print(rng)
Generator(MT19937)

What's New or Different
~~~~~~~~~~~~~~~~~~~~~~~
.. warning::

  The Box-Muller method used to produce NumPy's normals is no longer available
  in `Generator`.  It is not possible to reproduce the exact random
  values using Generator for the normal distribution or any other
  distribution that relies on the normal such as the `RandomState.gamma` or
  `RandomState.standard_t`. If you require bitwise backward compatible
  streams, use `RandomState`.

* The Generator's normal, exponential and gamma functions use 256-step Ziggurat
  methods which are 2-10 times faster than NumPy's Box-Muller or inverse CDF
  implementations.
* Optional ``dtype`` argument that accepts ``np.float32`` or ``np.float64``
  to produce either single or double precision uniform random variables for
  select distributions
* Optional ``out`` argument that allows existing arrays to be filled for
  select distributions
* All BitGenerators can produce doubles, uint64s and uint32s via CTypes
  (`PCG64.ctypes`) and CFFI (`PCG64.cffi`). This allows the bit generators
  to be used in numba.
* The bit generators can be used in downstream projects via
  :ref:`Cython <random_cython>`.
* `Generator.integers` is now the canonical way to generate integer
  random numbers from a discrete uniform distribution. The ``rand`` and
  ``randn`` methods are only available through the legacy `RandomState`.
  The ``endpoint`` keyword can be used to specify open or closed intervals.
  This replaces both ``randint`` and the deprecated ``random_integers``.
* `Generator.random` is now the canonical way to generate floating-point
  random numbers, which replaces `RandomState.random_sample`,
  `RandomState.sample`, and `RandomState.ranf`. This is consistent with
  Python's `random.random`.
* All BitGenerators in numpy use `SeedSequence` to convert seeds into
  initialized states.
* The addition of an ``axis`` keyword argument to methods such as 
  `Generator.choice`, `Generator.permutation`,  and `Generator.shuffle` 
  improves support for sampling from and shuffling multi-dimensional arrays.

See :ref:`new-or-different` for a complete list of improvements and
differences from the traditional ``Randomstate``.

Parallel Generation
~~~~~~~~~~~~~~~~~~~

The included generators can be used in parallel, distributed applications in
one of three ways:

* :ref:`seedsequence-spawn`
* :ref:`independent-streams`
* :ref:`parallel-jumped`

Users with a very large amount of parallelism will want to consult
:ref:`upgrading-pcg64`.

Concepts
--------
.. toctree::
   :maxdepth: 1

   generator
   Legacy Generator (RandomState) <legacy>
   BitGenerators, SeedSequences <bit_generators/index>
   Upgrading PCG64 with PCG64DXSM <upgrading-pcg64>

Features
--------
.. toctree::
   :maxdepth: 2

   Parallel Applications <parallel>
   Multithreaded Generation <multithreading>
   new-or-different
   Comparing Performance <performance>
   c-api
   Examples of using Numba, Cython, CFFI <extending>

Original Source of the Generator and BitGenerators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This package was developed independently of NumPy and was integrated in version
1.17.0. The original repo is at https://github.com/bashtage/randomgen.
Parallel Random Number Generation
=================================

There are three strategies implemented that can be used to produce
repeatable pseudo-random numbers across multiple processes (local
or distributed).

.. currentmodule:: numpy.random

.. _seedsequence-spawn:

`~SeedSequence` spawning
------------------------

`~SeedSequence` `implements an algorithm`_ to process a user-provided seed,
typically as an integer of some size, and to convert it into an initial state for
a `~BitGenerator`. It uses hashing techniques to ensure that low-quality seeds
are turned into high quality initial states (at least, with very high
probability).

For example, `MT19937` has a state consisting of 624
`uint32` integers. A naive way to take a 32-bit integer seed would be to just set
the last element of the state to the 32-bit seed and leave the rest 0s. This is
a valid state for `MT19937`, but not a good one. The Mersenne Twister
algorithm `suffers if there are too many 0s`_. Similarly, two adjacent 32-bit
integer seeds (i.e. ``12345`` and ``12346``) would produce very similar
streams.

`~SeedSequence` avoids these problems by using successions of integer hashes
with good `avalanche properties`_ to ensure that flipping any bit in the input
has about a 50% chance of flipping any bit in the output. Two input seeds that
are very close to each other will produce initial states that are very far
from each other (with very high probability). It is also constructed in such
a way that you can provide arbitrary-sized integers or lists of integers.
`~SeedSequence` will take all of the bits that you provide and mix them
together to produce however many bits the consuming `~BitGenerator` needs to
initialize itself.

These properties together mean that we can safely mix together the usual
user-provided seed with simple incrementing counters to get `~BitGenerator`
states that are (to very high probability) independent of each other. We can
wrap this together into an API that is easy to use and difficult to misuse.

.. code-block:: python

  from numpy.random import SeedSequence, default_rng

  ss = SeedSequence(12345)

  # Spawn off 10 child SeedSequences to pass to child processes.
  child_seeds = ss.spawn(10)
  streams = [default_rng(s) for s in child_seeds]

.. end_block

Child `~SeedSequence` objects can also spawn to make grandchildren, and so on.
Each `~SeedSequence` has its position in the tree of spawned `~SeedSequence`
objects mixed in with the user-provided seed to generate independent (with very
high probability) streams.

.. code-block:: python

  grandchildren = child_seeds[0].spawn(4)
  grand_streams = [default_rng(s) for s in grandchildren]

.. end_block

This feature lets you make local decisions about when and how to split up
streams without coordination between processes. You do not have to preallocate
space to avoid overlapping or request streams from a common global service. This
general "tree-hashing" scheme is `not unique to numpy`_ but not yet widespread.
Python has increasingly-flexible mechanisms for parallelization available, and
this scheme fits in very well with that kind of use.

Using this scheme, an upper bound on the probability of a collision can be
estimated if one knows the number of streams that you derive. `~SeedSequence`
hashes its inputs, both the seed and the spawn-tree-path, down to a 128-bit
pool by default. The probability that there is a collision in
that pool, pessimistically-estimated ([1]_), will be about :math:`n^2*2^{-128}` where
`n` is the number of streams spawned. If a program uses an aggressive million
streams, about :math:`2^{20}`, then the probability that at least one pair of
them are identical is about :math:`2^{-88}`, which is in solidly-ignorable
territory ([2]_).

.. [1] The algorithm is carefully designed to eliminate a number of possible
       ways to collide. For example, if one only does one level of spawning, it
       is guaranteed that all states will be unique. But it's easier to
       estimate the naive upper bound on a napkin and take comfort knowing
       that the probability is actually lower.

.. [2] In this calculation, we can mostly ignore the amount of numbers drawn from each
       stream. See :ref:`upgrading-pcg64` for the technical details about
       `PCG64`. The other PRNGs we provide have some extra protection built in
       that avoids overlaps if the `~SeedSequence` pools differ in the
       slightest bit. `PCG64DXSM` has :math:`2^{127}` separate cycles
       determined by the seed in addition to the position in the
       :math:`2^{128}` long period for each cycle, so one has to both get on or
       near the same cycle *and* seed a nearby position in the cycle.
       `Philox` has completely independent cycles determined by the seed.
       `SFC64` incorporates a 64-bit counter so every unique seed is at
       least :math:`2^{64}` iterations away from any other seed. And
       finally, `MT19937` has just an unimaginably huge period. Getting
       a collision internal to `SeedSequence` is the way a failure would be
       observed.

.. _`implements an algorithm`: http://www.pcg-random.org/posts/developing-a-seed_seq-alternative.html
.. _`suffers if there are too many 0s`: http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/emt19937ar.html
.. _`avalanche properties`: https://en.wikipedia.org/wiki/Avalanche_effect
.. _`not unique to numpy`: https://www.iro.umontreal.ca/~lecuyer/myftp/papers/parallel-rng-imacs.pdf


.. _independent-streams:

Independent Streams
-------------------

`Philox` is a counter-based RNG based which generates values by
encrypting an incrementing counter using weak cryptographic primitives. The
seed determines the key that is used for the encryption. Unique keys create
unique, independent streams. `Philox` lets you bypass the
seeding algorithm to directly set the 128-bit key. Similar, but different, keys
will still create independent streams.

.. code-block:: python

  import secrets
  from numpy.random import Philox

  # 128-bit number as a seed
  root_seed = secrets.getrandbits(128)
  streams = [Philox(key=root_seed + stream_id) for stream_id in range(10)]

.. end_block

This scheme does require that you avoid reusing stream IDs. This may require
coordination between the parallel processes.


.. _parallel-jumped:

Jumping the BitGenerator state
------------------------------

``jumped`` advances the state of the BitGenerator *as-if* a large number of
random numbers have been drawn, and returns a new instance with this state.
The specific number of draws varies by BitGenerator, and ranges from
:math:`2^{64}` to :math:`2^{128}`.  Additionally, the *as-if* draws also depend
on the size of the default random number produced by the specific BitGenerator.
The BitGenerators that support ``jumped``, along with the period of the
BitGenerator, the size of the jump and the bits in the default unsigned random
are listed below.

+-----------------+-------------------------+-------------------------+-------------------------+
| BitGenerator    | Period                  |  Jump Size              | Bits per Draw           |
+=================+=========================+=========================+=========================+
| MT19937         | :math:`2^{19937}-1`     | :math:`2^{128}`         | 32                      |
+-----------------+-------------------------+-------------------------+-------------------------+
| PCG64           | :math:`2^{128}`         | :math:`~2^{127}` ([3]_) | 64                      |
+-----------------+-------------------------+-------------------------+-------------------------+
| PCG64DXSM       | :math:`2^{128}`         | :math:`~2^{127}` ([3]_) | 64                      |
+-----------------+-------------------------+-------------------------+-------------------------+
| Philox          | :math:`2^{256}`         | :math:`2^{128}`         | 64                      |
+-----------------+-------------------------+-------------------------+-------------------------+

.. [3] The jump size is :math:`(\phi-1)*2^{128}` where :math:`\phi` is the
       golden ratio. As the jumps wrap around the period, the actual distances
       between neighboring streams will slowly grow smaller than the jump size,
       but using the golden ratio this way is a classic method of constructing
       a low-discrepancy sequence that spreads out the states around the period
       optimally. You will not be able to jump enough to make those distances
       small enough to overlap in your lifetime.

``jumped`` can be used to produce long blocks which should be long enough to not
overlap.

.. code-block:: python

  import secrets
  from numpy.random import PCG64

  seed = secrets.getrandbits(128)
  blocked_rng = []
  rng = PCG64(seed)
  for i in range(10):
      blocked_rng.append(rng.jumped(i))

.. end_block

When using ``jumped``, one does have to take care not to jump to a stream that
was already used. In the above example, one could not later use
``blocked_rng[0].jumped()`` as it would overlap with ``blocked_rng[1]``. Like
with the independent streams, if the main process here wants to split off 10
more streams by jumping, then it needs to start with ``range(10, 20)``,
otherwise it would recreate the same streams. On the other hand, if you
carefully construct the streams, then you are guaranteed to have streams that
do not overlap.
.. currentmodule:: numpy.random

.. _legacy:

Legacy Random Generation
------------------------
The `RandomState` provides access to
legacy generators. This generator is considered frozen and will have
no further improvements.  It is guaranteed to produce the same values
as the final point release of NumPy v1.16. These all depend on Box-Muller
normals or inverse CDF exponentials or gammas. This class should only be used
if it is essential to have randoms that are identical to what
would have been produced by previous versions of NumPy.

`RandomState` adds additional information
to the state which is required when using Box-Muller normals since these
are produced in pairs. It is important to use
`RandomState.get_state`, and not the underlying bit generators
`state`, when accessing the state so that these extra values are saved.

Although we provide the `MT19937` BitGenerator for use independent of
`RandomState`, note that its default seeding uses `SeedSequence`
rather than the legacy seeding algorithm. `RandomState` will use the
legacy seeding algorithm. The methods to use the legacy seeding algorithm are
currently private as the main reason to use them is just to implement
`RandomState`. However, one can reset the state of `MT19937`
using the state of the `RandomState`:

.. code-block:: python

   from numpy.random import MT19937
   from numpy.random import RandomState

   rs = RandomState(12345)
   mt19937 = MT19937()
   mt19937.state = rs.get_state()
   rs2 = RandomState(mt19937)

   # Same output
   rs.standard_normal()
   rs2.standard_normal()

   rs.random()
   rs2.random()

   rs.standard_exponential()
   rs2.standard_exponential()


.. autoclass:: RandomState
    :members: __init__
    :exclude-members: __init__

Seeding and State
=================

.. autosummary::
   :toctree: generated/

   ~RandomState.get_state
   ~RandomState.set_state
   ~RandomState.seed

Simple random data
==================
.. autosummary::
   :toctree: generated/

   ~RandomState.rand
   ~RandomState.randn
   ~RandomState.randint
   ~RandomState.random_integers
   ~RandomState.random_sample
   ~RandomState.choice
   ~RandomState.bytes

Permutations
============
.. autosummary::
   :toctree: generated/

   ~RandomState.shuffle
   ~RandomState.permutation

Distributions
=============
.. autosummary::
   :toctree: generated/

   ~RandomState.beta
   ~RandomState.binomial
   ~RandomState.chisquare
   ~RandomState.dirichlet
   ~RandomState.exponential
   ~RandomState.f
   ~RandomState.gamma
   ~RandomState.geometric
   ~RandomState.gumbel
   ~RandomState.hypergeometric
   ~RandomState.laplace
   ~RandomState.logistic
   ~RandomState.lognormal
   ~RandomState.logseries
   ~RandomState.multinomial
   ~RandomState.multivariate_normal
   ~RandomState.negative_binomial
   ~RandomState.noncentral_chisquare
   ~RandomState.noncentral_f
   ~RandomState.normal
   ~RandomState.pareto
   ~RandomState.poisson
   ~RandomState.power
   ~RandomState.rayleigh
   ~RandomState.standard_cauchy
   ~RandomState.standard_exponential
   ~RandomState.standard_gamma
   ~RandomState.standard_normal
   ~RandomState.standard_t
   ~RandomState.triangular
   ~RandomState.uniform
   ~RandomState.vonmises
   ~RandomState.wald
   ~RandomState.weibull
   ~RandomState.zipf

Functions in `numpy.random`
===========================
Many of the RandomState methods above are exported as functions in
`numpy.random` This usage is discouraged, as it is implemented via a global
`RandomState` instance which is not advised on two counts:

- It uses global state, which means results will change as the code changes

- It uses a `RandomState` rather than the more modern `Generator`.

For backward compatible legacy reasons, we cannot change this. See
:ref:`random-quick-start`.

.. autosummary::
   :toctree: generated/

    beta
    binomial
    bytes
    chisquare
    choice
    dirichlet
    exponential
    f
    gamma
    geometric
    get_state
    gumbel
    hypergeometric
    laplace
    logistic
    lognormal
    logseries
    multinomial
    multivariate_normal
    negative_binomial
    noncentral_chisquare
    noncentral_f
    normal
    pareto
    permutation
    poisson
    power
    rand
    randint
    randn
    random
    random_integers
    random_sample
    ranf
    rayleigh
    sample
    seed
    set_state
    shuffle
    standard_cauchy
    standard_exponential
    standard_gamma
    standard_normal
    standard_t
    triangular
    uniform
    vonmises
    wald
    weibull
    zipf

Permuted Congruential Generator (64-bit, PCG64)
-----------------------------------------------

.. currentmodule:: numpy.random

.. autoclass:: PCG64
    :members: __init__
    :exclude-members: __init__

State
=====

.. autosummary::
   :toctree: generated/

   ~PCG64.state

Parallel generation
===================
.. autosummary::
   :toctree: generated/

   ~PCG64.advance
   ~PCG64.jumped

Extending
=========
.. autosummary::
   :toctree: generated/

   ~PCG64.cffi
   ~PCG64.ctypes
Permuted Congruential Generator (64-bit, PCG64 DXSM)
----------------------------------------------------

.. currentmodule:: numpy.random

.. autoclass:: PCG64DXSM
    :members: __init__
    :exclude-members: __init__

State
=====

.. autosummary::
   :toctree: generated/

   ~PCG64DXSM.state

Parallel generation
===================
.. autosummary::
   :toctree: generated/

   ~PCG64DXSM.advance
   ~PCG64DXSM.jumped

Extending
=========
.. autosummary::
   :toctree: generated/

   ~PCG64DXSM.cffi
   ~PCG64DXSM.ctypes
Philox Counter-based RNG
------------------------

.. currentmodule:: numpy.random

.. autoclass:: Philox
    :members: __init__
    :exclude-members: __init__

State
=====

.. autosummary::
   :toctree: generated/

   ~Philox.state

Parallel generation
===================
.. autosummary::
   :toctree: generated/

   ~Philox.advance
   ~Philox.jumped

Extending
=========
.. autosummary::
   :toctree: generated/

   ~Philox.cffi
   ~Philox.ctypes


SFC64 Small Fast Chaotic PRNG
-----------------------------

.. currentmodule:: numpy.random

.. autoclass:: SFC64
    :members: __init__
    :exclude-members: __init__

State
=====

.. autosummary::
   :toctree: generated/

   ~SFC64.state

Extending
=========
.. autosummary::
   :toctree: generated/

   ~SFC64.cffi
   ~SFC64.ctypes



.. currentmodule:: numpy.random

Bit Generators
--------------

The random values produced by :class:`~Generator`
originate in a BitGenerator.  The BitGenerators do not directly provide
random numbers and only contains methods used for seeding, getting or
setting the state, jumping or advancing the state, and for accessing
low-level wrappers for consumption by code that can efficiently
access the functions provided, e.g., `numba <https://numba.pydata.org>`_.

Supported BitGenerators
=======================

The included BitGenerators are:

* PCG-64 - The default. A fast generator that can be advanced by an arbitrary
  amount. See the documentation for :meth:`~.PCG64.advance`. PCG-64 has
  a period of :math:`2^{128}`. See the `PCG author's page`_ for more details
  about this class of PRNG.
* PCG-64 DXSM - An upgraded version of PCG-64 with better statistical
  properties in parallel contexts. See :ref:`upgrading-pcg64` for more
  information on these improvements.
* MT19937 - The standard Python BitGenerator. Adds a `MT19937.jumped`
  function that returns a new generator with state as-if :math:`2^{128}` draws have
  been made.
* Philox - A counter-based generator capable of being advanced an
  arbitrary number of steps or generating independent streams. See the
  `Random123`_ page for more details about this class of bit generators.
* SFC64 - A fast generator based on random invertible mappings. Usually the
  fastest generator of the four. See the `SFC author's page`_ for (a little)
  more detail.

.. _`PCG author's page`: http://www.pcg-random.org/
.. _`Random123`: https://www.deshawresearch.com/resources_random123.html
.. _`SFC author's page`: http://pracrand.sourceforge.net/RNG_engines.txt

.. autosummary::
    :toctree: generated/

    BitGenerator

.. toctree::
    :maxdepth: 1

    MT19937 <mt19937>
    PCG64 <pcg64>
    PCG64DXSM <pcg64dxsm>
    Philox <philox>
    SFC64 <sfc64>

Seeding and Entropy
-------------------

A BitGenerator provides a stream of random values. In order to generate
reproducible streams, BitGenerators support setting their initial state via a
seed. All of the provided BitGenerators will take an arbitrary-sized
non-negative integer, or a list of such integers, as a seed. BitGenerators
need to take those inputs and process them into a high-quality internal state
for the BitGenerator. All of the BitGenerators in numpy delegate that task to
`SeedSequence`, which uses hashing techniques to ensure that even low-quality
seeds generate high-quality initial states.

.. code-block:: python

    from numpy.random import PCG64

    bg = PCG64(12345678903141592653589793)

.. end_block

`~SeedSequence` is designed to be convenient for implementing best practices.
We recommend that a stochastic program defaults to using entropy from the OS so
that each run is different. The program should print out or log that entropy.
In order to reproduce a past value, the program should allow the user to
provide that value through some mechanism, a command-line argument is common,
so that the user can then re-enter that entropy to reproduce the result.
`~SeedSequence` can take care of everything except for communicating with the
user, which is up to you.

.. code-block:: python

    from numpy.random import PCG64, SeedSequence

    # Get the user's seed somehow, maybe through `argparse`.
    # If the user did not provide a seed, it should return `None`.
    seed = get_user_seed()
    ss = SeedSequence(seed)
    print('seed = {}'.format(ss.entropy))
    bg = PCG64(ss)

.. end_block

We default to using a 128-bit integer using entropy gathered from the OS. This
is a good amount of entropy to initialize all of the generators that we have in
numpy. We do not recommend using small seeds below 32 bits for general use.
Using just a small set of seeds to instantiate larger state spaces means that
there are some initial states that are impossible to reach. This creates some
biases if everyone uses such values.

There will not be anything *wrong* with the results, per se; even a seed of
0 is perfectly fine thanks to the processing that `~SeedSequence` does. If you
just need *some* fixed value for unit tests or debugging, feel free to use
whatever seed you like. But if you want to make inferences from the results or
publish them, drawing from a larger set of seeds is good practice.

If you need to generate a good seed "offline", then ``SeedSequence().entropy``
or using ``secrets.randbits(128)`` from the standard library are both
convenient ways.

If you need to run several stochastic simulations in parallel, best practice
is to construct a random generator instance for each simulation. 
To make sure that the random streams have distinct initial states, you can use
the `spawn` method of `~SeedSequence`. For instance, here we construct a list
of 12 instances:

.. code-block:: python

    from numpy.random import PCG64, SeedSequence
    
    # High quality initial entropy
    entropy = 0x87351080e25cb0fad77a44a3be03b491
    base_seq = SeedSequence(entropy)
    child_seqs = base_seq.spawn(12)    # a list of 12 SeedSequences
    generators = [PCG64(seq) for seq in child_seqs]

.. end_block


An alternative way is to use the fact that a `~SeedSequence` can be initialized
by a tuple of elements. Here we use a base entropy value and an integer
``worker_id``

.. code-block:: python

    from numpy.random import PCG64, SeedSequence

    # High quality initial entropy
    entropy = 0x87351080e25cb0fad77a44a3be03b491    
    sequences = [SeedSequence((entropy, worker_id)) for worker_id in range(12)]
    generators = [PCG64(seq) for seq in sequences]

.. end_block

Note that the sequences produced by the latter method will be distinct from
those constructed via `~SeedSequence.spawn`.


.. autosummary::
    :toctree: generated/

    SeedSequence
Mersenne Twister (MT19937)
--------------------------

.. currentmodule:: numpy.random

.. autoclass:: MT19937
    :members: __init__
    :exclude-members: __init__

State
=====

.. autosummary::
   :toctree: generated/

   ~MT19937.state

Parallel generation
===================
.. autosummary::
   :toctree: generated/

   ~MT19937.jumped

Extending
=========
.. autosummary::
   :toctree: generated/

   ~MT19937.cffi
   ~MT19937.ctypes


Extending via Numba and CFFI
----------------------------

.. literalinclude:: ../../../../../numpy/random/_examples/numba/extending_distributions.py
    :language: python
Extending via Numba
-------------------

.. literalinclude:: ../../../../../numpy/random/_examples/numba/extending.py
    :language: python
Extending via CFFI
------------------

.. literalinclude:: ../../../../../numpy/random/_examples/cffi/extending.py
    :language: python
extending_distributions.pyx
---------------------------

.. literalinclude:: ../../../../../../numpy/random/_examples/cython/extending_distributions.pyx
    :language: cython
setup.py
--------

.. literalinclude:: ../../../../../../numpy/random/_examples/cython/setup.py
    :language: python
extending.pyx
-------------

.. literalinclude:: ../../../../../../numpy/random/_examples/cython/extending.pyx
    :language: cython

.. _extending_cython_example:

Extending `numpy.random` via Cython
-----------------------------------


.. toctree::
    setup.py.rst
    extending.pyx
    extending_distributions.pyx
:orphan:

.. raw:: html

    <html>
        <head>
            <meta http-equiv="refresh" content="0; url=index.html"/>
        </head>
    </html>

The location of this document has been changed , if you are not
redirected in few seconds, `click here <index.html>`_.
*****************
CPU build options
*****************

Description
-----------

The following options are mainly used to change the default behavior of optimizations
that target certain CPU features:

- ``--cpu-baseline``: minimal set of required CPU features.
   Default value is ``min`` which provides the minimum CPU features that can
   safely run on a wide range of platforms within the processor family.

   .. note::

     During the runtime, NumPy modules will fail to load if any of specified features
     are not supported by the target CPU (raises Python runtime error).

- ``--cpu-dispatch``: dispatched set of additional CPU features.
   Default value is ``max -xop -fma4`` which enables all CPU
   features, except for AMD legacy features (in case of X86).

   .. note::

      During the runtime, NumPy modules will skip any specified features
      that are not available in the target CPU.

These options are accessible through :py:mod:`distutils` commands
`distutils.command.build`, `distutils.command.build_clib` and
`distutils.command.build_ext`.
They accept a set of :ref:`CPU features <opt-supported-features>`
or groups of features that gather several features or
:ref:`special options <opt-special-options>` that
perform a series of procedures.

.. note::

    If ``build_clib`` or ``build_ext`` are not specified by the user,
    the arguments of ``build`` will be used instead, which also holds the default values.

To customize both ``build_ext`` and ``build_clib``::

    cd /path/to/numpy
    python setup.py build --cpu-baseline="avx2 fma3" install --user

To customize only ``build_ext``::

    cd /path/to/numpy
    python setup.py build_ext --cpu-baseline="avx2 fma3" install --user

To customize only ``build_clib``::

    cd /path/to/numpy
    python setup.py build_clib --cpu-baseline="avx2 fma3" install --user

You can also customize CPU/build options through PIP command::

    pip install --no-use-pep517 --global-option=build \
    --global-option="--cpu-baseline=avx2 fma3" \
    --global-option="--cpu-dispatch=max" ./

Quick Start
-----------

In general, the default settings tend to not impose certain CPU features that
may not be available on some older processors. Raising the ceiling of the
baseline features will often improve performance and may also reduce
binary size.


The following are the most common scenarios that may require changing
the default settings:


I am building NumPy for my local use
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

And I do not intend to export the build to other users or target a
different CPU than what the host has.

Set `native` for baseline, or manually specify the CPU features in case of option
`native` isn't supported by your platform::

    python setup.py build --cpu-baseline="native" bdist

Building NumPy with extra CPU features isn't necessary for this case,
since all supported features are already defined within the baseline features::

    python setup.py build --cpu-baseline=native --cpu-dispatch=none bdist

.. note::

    A fatal error will be raised if `native` isn't supported by the host platform.

I do not want to support the old processors of the `x86` architecture
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since most of the CPUs nowadays support at least `AVX`, `F16C` features, you can use::

    python setup.py build --cpu-baseline="avx f16c" bdist

.. note::

    ``--cpu-baseline`` force combine all implied features, so there's no need
    to add SSE features.


I'm facing the same case above but with `ppc64` architecture
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Then raise the ceiling of the baseline features to Power8::

    python setup.py build --cpu-baseline="vsx2" bdist

Having issues with `AVX512` features?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You may have some reservations about including of `AVX512` or
any other CPU feature and you want to exclude from the dispatched features::

    python setup.py build --cpu-dispatch="max -avx512f -avx512cd \
    -avx512_knl -avx512_knm -avx512_skx -avx512_clx -avx512_cnl -avx512_icl" \
    bdist

.. _opt-supported-features:

Supported Features
------------------

The names of the features can express one feature or a group of features,
as shown in the following tables supported depend on the lowest interest:

.. note::

    The following features may not be supported by all compilers,
    also some compilers may produce different set of implied features
    when it comes to features like ``AVX512``, ``AVX2``, and ``FMA3``.
    See :ref:`opt-platform-differences` for more details.

.. include:: generated_tables/cpu_features.inc

.. _opt-special-options:

Special Options
---------------

- ``NONE``: enable no features.

- ``NATIVE``: Enables all CPU features that supported by the host CPU,
  this operation is based on the compiler flags (``-march=native``, ``-xHost``, ``/QxHost``)

- ``MIN``: Enables the minimum CPU features that can safely run on a wide range of platforms:

  .. table::
      :align: left

      ======================================  =======================================
       For Arch                               Implies
      ======================================  =======================================
       x86 (32-bit mode)                      ``SSE`` ``SSE2``
       x86_64                                 ``SSE`` ``SSE2`` ``SSE3``
       IBM/POWER (big-endian mode)            ``NONE``
       IBM/POWER (little-endian mode)         ``VSX`` ``VSX2``
       ARMHF                                  ``NONE``
       ARM64 A.K. AARCH64                     ``NEON`` ``NEON_FP16`` ``NEON_VFPV4``
                                              ``ASIMD``
       IBM/ZSYSTEM(S390X)                     ``NONE``
      ======================================  =======================================

- ``MAX``: Enables all supported CPU features by the compiler and platform.

- ``Operators-/+``: remove or add features, useful with options ``MAX``, ``MIN`` and ``NATIVE``.

Behaviors
---------

- CPU features and other options are case-insensitive, for example::

    python setup.py build --cpu-dispatch="SSE41 avx2 FMA3"

- The order of the requested optimizations doesn't matter::

    python setup.py build --cpu-dispatch="SSE41 AVX2 FMA3"
    # equivalent to
    python setup.py build --cpu-dispatch="FMA3 AVX2 SSE41"

- Either commas or spaces or '+' can be used as a separator,
  for example::

    python setup.py build --cpu-dispatch="avx2 avx512f"
    # or
    python setup.py build --cpu-dispatch=avx2,avx512f
    # or
    python setup.py build --cpu-dispatch="avx2+avx512f"

  all works but arguments should be enclosed in quotes or escaped
  by backslash if any spaces are used.

- ``--cpu-baseline`` combines all implied CPU features, for example::

    python setup.py build --cpu-baseline=sse42
    # equivalent to
    python setup.py build --cpu-baseline="sse sse2 sse3 ssse3 sse41 popcnt sse42"

- ``--cpu-baseline`` will be treated as "native" if compiler native flag
  ``-march=native`` or ``-xHost`` or ``/QxHost`` is enabled through environment variable
  `CFLAGS`::

    export CFLAGS="-march=native"
    python setup.py install --user
    # is equivalent to
    python setup.py build --cpu-baseline=native install --user

- ``--cpu-baseline`` escapes any specified features that aren't supported
  by the target platform or compiler rather than raising fatal errors.

   .. note::

        Since ``--cpu-baseline`` combines all implied features, the maximum
        supported of implied features will be enabled rather than escape all of them.
        For example::

           # Requesting `AVX2,FMA3` but the compiler only support **SSE** features
           python setup.py build --cpu-baseline="avx2 fma3"
           # is equivalent to
           python setup.py build --cpu-baseline="sse sse2 sse3 ssse3 sse41 popcnt sse42"

- ``--cpu-dispatch`` does not combain any of implied CPU features,
  so you must add them unless you want to disable one or all of them::

    # Only dispatches AVX2 and FMA3
    python setup.py build --cpu-dispatch=avx2,fma3
    # Dispatches AVX and SSE features
    python setup.py build --cpu-baseline=ssse3,sse41,sse42,avx,avx2,fma3

- ``--cpu-dispatch`` escapes any specified baseline features and also escapes
  any features not supported by the target platform or compiler without raising
  fatal errors.

Eventually, you should always check the final report through the build log
to verify the enabled features. See :ref:`opt-build-report` for more details.

.. _opt-platform-differences:

Platform differences
--------------------

Some exceptional conditions force us to link some features together when it come to
certain compilers or architectures, resulting in the impossibility of building them separately.

These conditions can be divided into two parts, as follows:

**Architectural compatibility**

The need to align certain CPU features that are assured to be supported by
successive generations of the same architecture, some cases:

- On ppc64le ``VSX(ISA 2.06)`` and ``VSX2(ISA 2.07)`` both imply one another since the
  first generation that supports little-endian mode is Power-8`(ISA 2.07)`
- On AArch64 ``NEON NEON_FP16 NEON_VFPV4 ASIMD`` implies each other since they are part of the
  hardware baseline.

For example::

    # On ARMv8/A64, specify NEON is going to enable Advanced SIMD
    # and all predecessor extensions
    python setup.py build --cpu-baseline=neon
    # which equivalent to
    python setup.py build --cpu-baseline="neon neon_fp16 neon_vfpv4 asimd"

.. note::

    Please take a deep look at :ref:`opt-supported-features`,
    in order to determine the features that imply one another.

**Compilation compatibility**

Some compilers don't provide independent support for all CPU features. For instance
**Intel**'s compiler doesn't provide separated flags for ``AVX2`` and ``FMA3``,
it makes sense since all Intel CPUs that comes with ``AVX2`` also support ``FMA3``,
but this approach is incompatible with other **x86** CPUs from **AMD** or **VIA**.

For example::

    # Specify AVX2 will force enables FMA3 on Intel compilers
    python setup.py build --cpu-baseline=avx2
    # which equivalent to
    python setup.py build --cpu-baseline="avx2 fma3"


The following tables only show the differences imposed by some compilers from the
general context that been shown in the :ref:`opt-supported-features` tables:

.. note::

    Features names with strikeout represent the unsupported CPU features.

.. raw:: html

    <style>
        .enabled-feature {color:green; font-weight:bold;}
        .disabled-feature {color:red; text-decoration: line-through;}
    </style>

.. role:: enabled
    :class: enabled-feature

.. role:: disabled
    :class: disabled-feature

.. include:: generated_tables/compilers-diff.inc

.. _opt-build-report:

Build report
------------

In most cases, the CPU build options do not produce any fatal errors that lead to hanging the build.
Most of the errors that may appear in the build log serve as heavy warnings due to the lack of some
expected CPU features by the compiler.

So we strongly recommend checking the final report log, to be aware of what kind of CPU features
are enabled and what are not.

You can find the final report of CPU optimizations at the end of the build log,
and here is how it looks on x86_64/gcc:

.. raw:: html

    <style>#build-report .highlight-bash pre{max-height:450px; overflow-y: scroll;}</style>

.. literalinclude:: log_example.txt
   :language: bash

As you see, there is a separate report for each of ``build_ext`` and ``build_clib``
that includes several sections, and each section has several values, representing the following:

**Platform**:

- :enabled:`Architecture`: The architecture name of target CPU. It should be one of
  ``x86``, ``x64``, ``ppc64``, ``ppc64le``, ``armhf``, ``aarch64``, ``s390x`` or ``unknown``.

- :enabled:`Compiler`: The compiler name. It should be one of
  gcc, clang, msvc, icc, iccw or unix-like.

**CPU baseline**:

- :enabled:`Requested`: The specific features and options to ``--cpu-baseline`` as-is.
- :enabled:`Enabled`: The final set of enabled CPU features.
- :enabled:`Flags`: The compiler flags that were used to all NumPy `C/C++` sources
  during the compilation except for temporary sources that have been used for generating
  the binary objects of dispatched features.
- :enabled:`Extra checks`: list of internal checks that activate certain functionality
  or intrinsics related to the enabled features, useful for debugging when it comes
  to developing SIMD kernels.

**CPU dispatch**:

- :enabled:`Requested`: The specific features and options to ``--cpu-dispatch`` as-is.
- :enabled:`Enabled`: The final set of enabled CPU features.
- :enabled:`Generated`: At the beginning of the next row of this property,
  the features for which optimizations have been generated are shown in the
  form of several sections with similar properties explained as follows:

  - :enabled:`One or multiple dispatched feature`: The implied CPU features.
  - :enabled:`Flags`: The compiler flags that been used for these features.
  - :enabled:`Extra checks`: Similar to the baseline but for these dispatched features.
  - :enabled:`Detect`: Set of CPU features that need be detected in runtime in order to
    execute the generated optimizations.
  - The lines that come after the above property and end with a ':' on a separate line,
    represent the paths of c/c++ sources that define the generated optimizations.

Runtime Trace
-------------
To be completed.
.. _numpysimd:
.. currentmodule:: numpysimd

***********************
CPU/SIMD Optimizations
***********************

NumPy comes with a flexible working mechanism that allows it to harness the SIMD
features that CPUs own, in order to provide faster and more stable performance
on all popular platforms. Currently, NumPy supports the X86, IBM/Power, ARM7 and ARM8
architectures.

The optimization process in NumPy is carried out in three layers:

- Code is *written* using the universal intrinsics which is a set of types, macros and
  functions that are mapped to each supported instruction-sets by using guards that
  will enable use of the them only when the compiler recognizes them.
  This allow us to generate multiple kernels for the same functionality,
  in which each generated kernel represents a set of instructions that related one
  or multiple certain CPU features. The first kernel represents the minimum (baseline)
  CPU features, and the other kernels represent the additional (dispatched) CPU features.

- At *compile* time, CPU build options are used to define the minimum and
  additional features to support, based on user choice and compiler support. The
  appropriate intrinsics are overlaid with the platform / architecture intrinsics,
  and multiple kernels are compiled.

- At *runtime import*, the CPU is probed for the set of supported CPU
  features. A mechanism is used to grab the pointer to the most appropriate
  kernel, and this will be the one called for the function.

.. note::

   NumPy community had a deep discussion before implementing this work,
   please check `NEP-38`_ for more clarification.

.. toctree::

    build-options
    how-it-works

.. _`NEP-38`: https://numpy.org/neps/nep-0038-SIMD-optimizations.html

**********************************
How does the CPU dispatcher work?
**********************************

NumPy dispatcher is based on multi-source compiling, which means taking
a certain source and compiling it multiple times with different compiler
flags and also with different **C** definitions that affect the code
paths. This enables certain instruction-sets for each compiled object
depending on the required optimizations and ends with linking the
returned objects together.

.. figure:: ../figures/opt-infra.png

This mechanism should support all compilers and it doesn't require any
compiler-specific extension, but at the same time it adds a few steps to
normal compilation that are explained as follows.

1- Configuration
~~~~~~~~~~~~~~~~

Configuring the required optimization by the user before starting to build the
source files via the two command arguments as explained above:

-  ``--cpu-baseline``: minimal set of required optimizations.

-  ``--cpu-dispatch``: dispatched set of additional optimizations.


2- Discovering the environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this part, we check the compiler and platform architecture
and cache some of the intermediary results to speed up rebuilding.

3- Validating the requested optimizations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By testing them against the compiler, and seeing what the compiler can
support according to the requested optimizations.

4- Generating the main configuration header
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The generated header ``_cpu_dispatch.h`` contains all the definitions and
headers of instruction-sets for the required optimizations that have been
validated during the previous step.

It also contains extra C definitions that are used for defining NumPy's
Python-level module attributes ``__cpu_baseline__`` and ``__cpu_dispatch__``.

**What is in this header?**

The example header was dynamically generated by gcc on an X86 machine.
The compiler supports ``--cpu-baseline="sse sse2 sse3"`` and
``--cpu-dispatch="ssse3 sse41"``, and the result is below.

.. code:: c

   // The header should be located at numpy/numpy/core/src/common/_cpu_dispatch.h
   /**NOTE
    ** C definitions prefixed with "NPY_HAVE_" represent
    ** the required optimzations.
    **
    ** C definitions prefixed with 'NPY__CPU_TARGET_' are protected and
    ** shouldn't be used by any NumPy C sources.
    */
   /******* baseline features *******/
   /** SSE **/
   #define NPY_HAVE_SSE 1
   #include <xmmintrin.h>
   /** SSE2 **/
   #define NPY_HAVE_SSE2 1
   #include <emmintrin.h>
   /** SSE3 **/
   #define NPY_HAVE_SSE3 1
   #include <pmmintrin.h>

   /******* dispatch-able features *******/
   #ifdef NPY__CPU_TARGET_SSSE3
     /** SSSE3 **/
     #define NPY_HAVE_SSSE3 1
     #include <tmmintrin.h>
   #endif
   #ifdef NPY__CPU_TARGET_SSE41
     /** SSE41 **/
     #define NPY_HAVE_SSE41 1
     #include <smmintrin.h>
   #endif

**Baseline features** are the minimal set of required optimizations configured
via ``--cpu-baseline``. They have no preprocessor guards and they're
always on, which means they can be used in any source.

Does this mean NumPy's infrastructure passes the compiler's flags of
baseline features to all sources?

Definitely, yes. But the :ref:`dispatch-able sources <dispatchable-sources>` are
treated differently.

What if the user specifies certain **baseline features** during the
build but at runtime the machine doesn't support even these
features? Will the compiled code be called via one of these definitions, or
maybe the compiler itself auto-generated/vectorized certain piece of code
based on the provided command line compiler flags?

During the loading of the NumPy module, there's a validation step
which detects this behavior. It will raise a Python runtime error to inform the
user. This is to prevent the CPU reaching an illegal instruction error causing
a segfault.

**Dispatch-able features** are our dispatched set of additional optimizations
that were configured via ``--cpu-dispatch``. They are not activated by
default and are always guarded by other C definitions prefixed with
``NPY__CPU_TARGET_``. C definitions ``NPY__CPU_TARGET_`` are only
enabled within **dispatch-able sources**.

.. _dispatchable-sources:

5- Dispatch-able sources and configuration statements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dispatch-able sources are special **C** files that can be compiled multiple
times with different compiler flags and also with different **C**
definitions. These affect code paths to enable certain
instruction-sets for each compiled object according to "**the
configuration statements**" that must be declared between a **C**
comment\ ``(/**/)`` and start with a special mark **@targets** at the
top of each dispatch-able source. At the same time, dispatch-able
sources will be treated as normal **C** sources if the optimization was
disabled by the command argument ``--disable-optimization`` .

**What are configuration statements?**

Configuration statements are sort of keywords combined together to
determine the required optimization for the dispatch-able source.

Example:

.. code:: c

   /*@targets avx2 avx512f vsx2 vsx3 asimd asimdhp */
   // C code

The keywords mainly represent the additional optimizations configured
through ``--cpu-dispatch``, but it can also represent other options such as:

- Target groups: pre-configured configuration statements used for
  managing the required optimizations from outside the dispatch-able source.

- Policies: collections of options used for changing the default
  behaviors or forcing the compilers to perform certain things.

- "baseline": a unique keyword represents the minimal optimizations
  that configured through ``--cpu-baseline``

**Numpy's infrastructure handles dispatch-able sources in four steps**:

- **(A) Recognition**: Just like source templates and F2PY, the
  dispatch-able sources requires a special extension ``*.dispatch.c``
  to mark C dispatch-able source files, and for C++
  ``*.dispatch.cpp`` or ``*.dispatch.cxx``
  **NOTE**: C++ not supported yet.

- **(B) Parsing and validating**: In this step, the
  dispatch-able sources that had been filtered by the previous step
  are parsed and validated by the configuration statements for each one
  of them one by one in order to determine the required optimizations.

- **(C) Wrapping**: This is the approach taken by NumPy's
  infrastructure, which has proved to be sufficiently flexible in order
  to compile a single source multiple times with different **C**
  definitions and flags that affect the code paths. The process is
  achieved by creating a temporary **C** source for each required
  optimization that related to the additional optimization, which
  contains the declarations of the **C** definitions and includes the
  involved source via the **C** directive **#include**. For more
  clarification take a look at the following code for AVX512F :

  .. code:: c

      /*
       * this definition is used by NumPy utilities as suffixes for the
       * exported symbols
       */
      #define NPY__CPU_TARGET_CURRENT AVX512F
      /*
       * The following definitions enable
       * definitions of the dispatch-able features that are defined within the main
       * configuration header. These are definitions for the implied features.
       */
      #define NPY__CPU_TARGET_SSE
      #define NPY__CPU_TARGET_SSE2
      #define NPY__CPU_TARGET_SSE3
      #define NPY__CPU_TARGET_SSSE3
      #define NPY__CPU_TARGET_SSE41
      #define NPY__CPU_TARGET_POPCNT
      #define NPY__CPU_TARGET_SSE42
      #define NPY__CPU_TARGET_AVX
      #define NPY__CPU_TARGET_F16C
      #define NPY__CPU_TARGET_FMA3
      #define NPY__CPU_TARGET_AVX2
      #define NPY__CPU_TARGET_AVX512F
      // our dispatch-able source
      #include "/the/absuolate/path/of/hello.dispatch.c"

- **(D) Dispatch-able configuration header**: The infrastructure
  generates a config header for each dispatch-able source, this header
  mainly contains two abstract **C** macros used for identifying the
  generated objects, so they can be used for runtime dispatching
  certain symbols from the generated objects by any **C** source. It is
  also used for forward declarations.

  The generated header takes the name of the dispatch-able source after
  excluding the extension and replace it with ``.h``, for example
  assume we have a dispatch-able source called ``hello.dispatch.c`` and
  contains the following:

  .. code:: c

      // hello.dispatch.c
      /*@targets baseline sse42 avx512f */
      #include <stdio.h>
      #include "numpy/utils.h" // NPY_CAT, NPY_TOSTR

      #ifndef NPY__CPU_TARGET_CURRENT
        // wrapping the dispatch-able source only happens to the additional optimizations
        // but if the keyword 'baseline' provided within the configuration statements,
        // the infrastructure will add extra compiling for the dispatch-able source by
        // passing it as-is to the compiler without any changes.
        #define CURRENT_TARGET(X) X
        #define NPY__CPU_TARGET_CURRENT baseline // for printing only
      #else
        // since we reach to this point, that's mean we're dealing with
          // the additional optimizations, so it could be SSE42 or AVX512F
        #define CURRENT_TARGET(X) NPY_CAT(NPY_CAT(X, _), NPY__CPU_TARGET_CURRENT)
      #endif
      // Macro 'CURRENT_TARGET' adding the current target as suffux to the exported symbols,
      // to avoid linking duplications, NumPy already has a macro called
      // 'NPY_CPU_DISPATCH_CURFX' similar to it, located at
      // numpy/numpy/core/src/common/npy_cpu_dispatch.h
      // NOTE: we tend to not adding suffixes to the baseline exported symbols
      void CURRENT_TARGET(simd_whoami)(const char *extra_info)
      {
          printf("I'm " NPY_TOSTR(NPY__CPU_TARGET_CURRENT) ", %s\n", extra_info);
      }

  Now assume you attached **hello.dispatch.c** to the source tree, then
  the infrastructure should generate a temporary config header called
  **hello.dispatch.h** that can be reached by any source in the source
  tree, and it should contain the following code :

  .. code:: c

      #ifndef NPY__CPU_DISPATCH_EXPAND_
        // To expand the macro calls in this header
          #define NPY__CPU_DISPATCH_EXPAND_(X) X
      #endif
      // Undefining the following macros, due to the possibility of including config headers
      // multiple times within the same source and since each config header represents
      // different required optimizations according to the specified configuration
      // statements in the dispatch-able source that derived from it.
      #undef NPY__CPU_DISPATCH_BASELINE_CALL
      #undef NPY__CPU_DISPATCH_CALL
      // nothing strange here, just a normal preprocessor callback
      // enabled only if 'baseline' specified within the configuration statements
      #define NPY__CPU_DISPATCH_BASELINE_CALL(CB, ...) \
        NPY__CPU_DISPATCH_EXPAND_(CB(__VA_ARGS__))
      // 'NPY__CPU_DISPATCH_CALL' is an abstract macro is used for dispatching
      // the required optimizations that specified within the configuration statements.
      //
      // @param CHK, Expected a macro that can be used to detect CPU features
      // in runtime, which takes a CPU feature name without string quotes and
      // returns the testing result in a shape of boolean value.
      // NumPy already has macro called "NPY_CPU_HAVE", which fits this requirement.
      //
      // @param CB, a callback macro that expected to be called multiple times depending
      // on the required optimizations, the callback should receive the following arguments:
      //  1- The pending calls of @param CHK filled up with the required CPU features,
      //     that need to be tested first in runtime before executing call belong to
      //     the compiled object.
      //  2- The required optimization name, same as in 'NPY__CPU_TARGET_CURRENT'
      //  3- Extra arguments in the macro itself
      //
      // By default the callback calls are sorted depending on the highest interest
      // unless the policy "$keep_sort" was in place within the configuration statements
      // see "Dive into the CPU dispatcher" for more clarification.
      #define NPY__CPU_DISPATCH_CALL(CHK, CB, ...) \
        NPY__CPU_DISPATCH_EXPAND_(CB((CHK(AVX512F)), AVX512F, __VA_ARGS__)) \
        NPY__CPU_DISPATCH_EXPAND_(CB((CHK(SSE)&&CHK(SSE2)&&CHK(SSE3)&&CHK(SSSE3)&&CHK(SSE41)), SSE41, __VA_ARGS__))

  An example of using the config header in light of the above:

  .. code:: c

      // NOTE: The following macros are only defined for demonstration purposes only.
      // NumPy already has a collections of macros located at
      // numpy/numpy/core/src/common/npy_cpu_dispatch.h, that covers all dispatching
      // and declarations scenarios.

      #include "numpy/npy_cpu_features.h" // NPY_CPU_HAVE
      #include "numpy/utils.h" // NPY_CAT, NPY_EXPAND

      // An example for setting a macro that calls all the exported symbols at once
      // after checking if they're supported by the running machine.
      #define DISPATCH_CALL_ALL(FN, ARGS) \
          NPY__CPU_DISPATCH_CALL(NPY_CPU_HAVE, DISPATCH_CALL_ALL_CB, FN, ARGS) \
          NPY__CPU_DISPATCH_BASELINE_CALL(DISPATCH_CALL_BASELINE_ALL_CB, FN, ARGS)
      // The preprocessor callbacks.
      // The same suffixes as we define it in the dispatch-able source.
      #define DISPATCH_CALL_ALL_CB(CHECK, TARGET_NAME, FN, ARGS) \
        if (CHECK) { NPY_CAT(NPY_CAT(FN, _), TARGET_NAME) ARGS; }
      #define DISPATCH_CALL_BASELINE_ALL_CB(FN, ARGS) \
        FN NPY_EXPAND(ARGS);

      // An example for setting a macro that calls the exported symbols of highest
      // interest optimization, after checking if they're supported by the running machine.
      #define DISPATCH_CALL_HIGH(FN, ARGS) \
        if (0) {} \
          NPY__CPU_DISPATCH_CALL(NPY_CPU_HAVE, DISPATCH_CALL_HIGH_CB, FN, ARGS) \
          NPY__CPU_DISPATCH_BASELINE_CALL(DISPATCH_CALL_BASELINE_HIGH_CB, FN, ARGS)
      // The preprocessor callbacks
      // The same suffixes as we define it in the dispatch-able source.
      #define DISPATCH_CALL_HIGH_CB(CHECK, TARGET_NAME, FN, ARGS) \
        else if (CHECK) { NPY_CAT(NPY_CAT(FN, _), TARGET_NAME) ARGS; }
      #define DISPATCH_CALL_BASELINE_HIGH_CB(FN, ARGS) \
        else { FN NPY_EXPAND(ARGS); }

      // NumPy has a macro called 'NPY_CPU_DISPATCH_DECLARE' can be used
      // for forward declrations any kind of prototypes based on
      // 'NPY__CPU_DISPATCH_CALL' and 'NPY__CPU_DISPATCH_BASELINE_CALL'.
      // However in this example, we just handle it manually.
      void simd_whoami(const char *extra_info);
      void simd_whoami_AVX512F(const char *extra_info);
      void simd_whoami_SSE41(const char *extra_info);

      void trigger_me(void)
      {
          // bring the auto-gernreated config header
          // which contains config macros 'NPY__CPU_DISPATCH_CALL' and
          // 'NPY__CPU_DISPATCH_BASELINE_CALL'.
          // it highely recomaned to include the config header before exectuing
        // the dispatching macros in case if there's another header in the scope.
          #include "hello.dispatch.h"
          DISPATCH_CALL_ALL(simd_whoami, ("all"))
          DISPATCH_CALL_HIGH(simd_whoami, ("the highest interest"))
          // An example of including multiple config headers in the same source
          // #include "hello2.dispatch.h"
          // DISPATCH_CALL_HIGH(another_function, ("the highest interest"))
      }
.. _data_memory:

Memory management in NumPy
==========================

The `numpy.ndarray` is a python class. It requires additional memory allocations
to hold `numpy.ndarray.strides`, `numpy.ndarray.shape` and
`numpy.ndarray.data` attributes. These attributes are specially allocated
after creating the python object in `__new__`. The ``strides`` and
``shape`` are stored in a piece of memory allocated internally.

The ``data`` allocation used to store the actual array values (which could be
pointers in the case of ``object`` arrays) can be very large, so NumPy has
provided interfaces to manage its allocation and release. This document details
how those interfaces work.

Historical overview
-------------------

Since version 1.7.0, NumPy has exposed a set of ``PyDataMem_*`` functions
(:c:func:`PyDataMem_NEW`, :c:func:`PyDataMem_FREE`, :c:func:`PyDataMem_RENEW`)
which are backed by `alloc`, `free`, `realloc` respectively. In that version
NumPy also exposed the `PyDataMem_EventHook` function (now deprecated)
described below, which wrap the OS-level calls.

Since those early days, Python also improved its memory management
capabilities, and began providing
various :ref:`management policies <memoryoverview>` beginning in version
3.4. These routines are divided into a set of domains, each domain has a
:c:type:`PyMemAllocatorEx` structure of routines for memory management. Python also
added a `tracemalloc` module to trace calls to the various routines. These
tracking hooks were added to the NumPy ``PyDataMem_*`` routines.

NumPy added a small cache of allocated memory in its internal
``npy_alloc_cache``, ``npy_alloc_cache_zero``, and ``npy_free_cache``
functions. These wrap ``alloc``, ``alloc-and-memset(0)`` and ``free``
respectively, but when ``npy_free_cache`` is called, it adds the pointer to a
short list of available blocks marked by size. These blocks can be re-used by
subsequent calls to ``npy_alloc*``, avoiding memory thrashing.

Configurable memory routines in NumPy (NEP 49)
----------------------------------------------

Users may wish to override the internal data memory routines with ones of their
own. Since NumPy does not use the Python domain strategy to manage data memory,
it provides an alternative set of C-APIs to change memory routines. There are
no Python domain-wide strategies for large chunks of object data, so those are
less suited to NumPy's needs. User who wish to change the NumPy data memory
management routines can use :c:func:`PyDataMem_SetHandler`, which uses a
:c:type:`PyDataMem_Handler` structure to hold pointers to functions used to
manage the data memory. The calls are still wrapped by internal routines to
call :c:func:`PyTraceMalloc_Track`, :c:func:`PyTraceMalloc_Untrack`, and will
use the deprecated :c:func:`PyDataMem_EventHookFunc` mechanism. Since the
functions may change during the lifetime of the process, each ``ndarray``
carries with it the functions used at the time of its instantiation, and these
will be used to reallocate or free the data memory of the instance.

.. c:type:: PyDataMem_Handler

    A struct to hold function pointers used to manipulate memory

    .. code-block:: c

        typedef struct {
            char name[127];  /* multiple of 64 to keep the struct aligned */
            uint8_t version; /* currently 1 */
            PyDataMemAllocator allocator;
        } PyDataMem_Handler;

    where the allocator structure is

    .. code-block:: c

        /* The declaration of free differs from PyMemAllocatorEx */ 
        typedef struct {
            void *ctx;
            void* (*malloc) (void *ctx, size_t size);
            void* (*calloc) (void *ctx, size_t nelem, size_t elsize);
            void* (*realloc) (void *ctx, void *ptr, size_t new_size);
            void (*free) (void *ctx, void *ptr, size_t size);
        } PyDataMemAllocator;

.. c:function:: PyObject * PyDataMem_SetHandler(PyObject *handler)

   Set a new allocation policy. If the input value is ``NULL``, will reset the
   policy to the default. Return the previous policy, or
   return ``NULL`` if an error has occurred. We wrap the user-provided functions
   so they will still call the python and numpy memory management callback
   hooks.
    
.. c:function:: PyObject * PyDataMem_GetHandler()

   Return the current policy that will be used to allocate data for the
   next ``PyArrayObject``. On failure, return ``NULL``.

For an example of setting up and using the PyDataMem_Handler, see the test in
:file:`numpy/core/tests/test_mem_policy.py`

.. c:function:: void PyDataMem_EventHookFunc(void *inp, void *outp, size_t size, void *user_data);

    This function will be called during data memory manipulation

.. c:function:: PyDataMem_EventHookFunc * PyDataMem_SetEventHook(PyDataMem_EventHookFunc *newhook, void *user_data, void **old_data)

    Sets the allocation event hook for numpy array data.
  
    Returns a pointer to the previous hook or ``NULL``.  If old_data is
    non-``NULL``, the previous user_data pointer will be copied to it.
  
    If not ``NULL``, hook will be called at the end of each ``PyDataMem_NEW/FREE/RENEW``:

    .. code-block:: c
   
        result = PyDataMem_NEW(size)        -> (*hook)(NULL, result, size, user_data)
        PyDataMem_FREE(ptr)                 -> (*hook)(ptr, NULL, 0, user_data)
        result = PyDataMem_RENEW(ptr, size) -> (*hook)(ptr, result, size, user_data)
  
    When the hook is called, the GIL will be held by the calling
    thread.  The hook should be written to be reentrant, if it performs
    operations that might cause new allocation events (such as the
    creation/destruction numpy objects, or creating/destroying Python
    objects which might cause a gc).

    Deprecated in v1.23

What happens when deallocating if there is no policy set
--------------------------------------------------------

A rare but useful technique is to allocate a buffer outside NumPy, use
:c:func:`PyArray_NewFromDescr` to wrap the buffer in a ``ndarray``, then switch
the ``OWNDATA`` flag to true. When the ``ndarray`` is released, the
appropriate function from the ``ndarray``'s ``PyDataMem_Handler`` should be
called to free the buffer. But the ``PyDataMem_Handler`` field was never set,
it will be ``NULL``. For backward compatibility, NumPy will call ``free()`` to
release the buffer. If ``NUMPY_WARN_IF_NO_MEM_POLICY`` is set to ``1``, a
warning will be emitted. The current default is not to emit a warning, this may
change in a future version of NumPy.

A better technique would be to use a ``PyCapsule`` as a base object:

.. code-block:: c

    /* define a PyCapsule_Destructor, using the correct deallocator for buff */
    void free_wrap(void *capsule){
        void * obj = PyCapsule_GetPointer(capsule, PyCapsule_GetName(capsule));
        free(obj); 
    };

    /* then inside the function that creates arr from buff */
    ...
    arr = PyArray_NewFromDescr(... buf, ...);
    if (arr == NULL) {
        return NULL;
    }
    capsule = PyCapsule_New(buf, "my_wrapped_buffer",
                            (PyCapsule_Destructor)&free_wrap);
    if (PyArray_SetBaseObject(arr, capsule) == -1) {
        Py_DECREF(arr);
        return NULL;
    }
    ...
System configuration
====================

.. sectionauthor:: Travis E. Oliphant

When NumPy is built, information about system configuration is
recorded, and is made available for extension modules using NumPy's C
API.  These are mostly defined in ``numpyconfig.h`` (included in
``ndarrayobject.h``). The public symbols are prefixed by ``NPY_*``.
NumPy also offers some functions for querying information about the
platform in use.

For private use, NumPy also constructs a ``config.h`` in the NumPy
include directory, which is not exported by NumPy (that is a python
extension which use the numpy C API will not see those symbols), to
avoid namespace pollution.


Data type sizes
---------------

The ``NPY_SIZEOF_{CTYPE}`` constants are defined so that sizeof
information is available to the pre-processor.

.. c:macro:: NPY_SIZEOF_SHORT

    sizeof(short)

.. c:macro:: NPY_SIZEOF_INT

    sizeof(int)

.. c:macro:: NPY_SIZEOF_LONG

    sizeof(long)

.. c:macro:: NPY_SIZEOF_LONGLONG

    sizeof(longlong) where longlong is defined appropriately on the
    platform.

.. c:macro:: NPY_SIZEOF_PY_LONG_LONG


.. c:macro:: NPY_SIZEOF_FLOAT

    sizeof(float)

.. c:macro:: NPY_SIZEOF_DOUBLE

    sizeof(double)

.. c:macro:: NPY_SIZEOF_LONG_DOUBLE

.. c:macro:: NPY_SIZEOF_LONGDOUBLE

    sizeof(longdouble)

.. c:macro:: NPY_SIZEOF_PY_INTPTR_T

.. c:macro:: NPY_SIZEOF_INTP

    Size of a pointer on this platform (sizeof(void \*))


Platform information
--------------------

.. c:macro:: NPY_CPU_X86
.. c:macro:: NPY_CPU_AMD64
.. c:macro:: NPY_CPU_IA64
.. c:macro:: NPY_CPU_PPC
.. c:macro:: NPY_CPU_PPC64
.. c:macro:: NPY_CPU_SPARC
.. c:macro:: NPY_CPU_SPARC64
.. c:macro:: NPY_CPU_S390
.. c:macro:: NPY_CPU_PARISC

    .. versionadded:: 1.3.0

    CPU architecture of the platform; only one of the above is
    defined.

    Defined in ``numpy/npy_cpu.h``

.. c:macro:: NPY_LITTLE_ENDIAN

.. c:macro:: NPY_BIG_ENDIAN

.. c:macro:: NPY_BYTE_ORDER

    .. versionadded:: 1.3.0

    Portable alternatives to the ``endian.h`` macros of GNU Libc.
    If big endian, :c:data:`NPY_BYTE_ORDER` == :c:data:`NPY_BIG_ENDIAN`, and
    similarly for little endian architectures.

    Defined in ``numpy/npy_endian.h``.

.. c:function:: int PyArray_GetEndianness()

    .. versionadded:: 1.3.0

    Returns the endianness of the current platform.
    One of :c:data:`NPY_CPU_BIG`, :c:data:`NPY_CPU_LITTLE`,
    or :c:data:`NPY_CPU_UNKNOWN_ENDIAN`.

    .. c:macro:: NPY_CPU_BIG

    .. c:macro:: NPY_CPU_LITTLE

    .. c:macro:: NPY_CPU_UNKNOWN_ENDIAN


Compiler directives
-------------------

.. c:macro:: NPY_LIKELY
.. c:macro:: NPY_UNLIKELY
.. c:macro:: NPY_UNUSED


Interrupt Handling
------------------

.. c:macro:: NPY_INTERRUPT_H
.. c:macro:: NPY_SIGSETJMP
.. c:macro:: NPY_SIGLONGJMP
.. c:macro:: NPY_SIGJMP_BUF
.. c:macro:: NPY_SIGINT_ON
.. c:macro:: NPY_SIGINT_OFF
NumPy core libraries
====================

.. sectionauthor:: David Cournapeau

.. versionadded:: 1.3.0

Starting from numpy 1.3.0, we are working on separating the pure C,
"computational" code from the python dependent code. The goal is twofolds:
making the code cleaner, and enabling code reuse by other extensions outside
numpy (scipy, etc...).

NumPy core math library
-----------------------

The numpy core math library ('npymath') is a first step in this direction. This
library contains most math-related C99 functionality, which can be used on
platforms where C99 is not well supported. The core math functions have the
same API as the C99 ones, except for the npy_* prefix.

The available functions are defined in <numpy/npy_math.h> - please refer to this header when
in doubt.

Floating point classification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. c:macro:: NPY_NAN

    This macro is defined to a NaN (Not a Number), and is guaranteed to have
    the signbit unset ('positive' NaN). The corresponding single and extension
    precision macro are available with the suffix F and L.

.. c:macro:: NPY_INFINITY

    This macro is defined to a positive inf. The corresponding single and
    extension precision macro are available with the suffix F and L.

.. c:macro:: NPY_PZERO

    This macro is defined to positive zero. The corresponding single and
    extension precision macro are available with the suffix F and L.

.. c:macro:: NPY_NZERO

    This macro is defined to negative zero (that is with the sign bit set). The
    corresponding single and extension precision macro are available with the
    suffix F and L.

.. c:macro:: npy_isnan(x)

    This is a macro, and is equivalent to C99 isnan: works for single, double
    and extended precision, and return a non 0 value if x is a NaN.

.. c:macro:: npy_isfinite(x)

    This is a macro, and is equivalent to C99 isfinite: works for single,
    double and extended precision, and return a non 0 value if x is neither a
    NaN nor an infinity.

.. c:macro:: npy_isinf(x)

    This is a macro, and is equivalent to C99 isinf: works for single, double
    and extended precision, and return a non 0 value if x is infinite (positive
    and negative).

.. c:macro:: npy_signbit(x)

    This is a macro, and is equivalent to C99 signbit: works for single, double
    and extended precision, and return a non 0 value if x has the signbit set
    (that is the number is negative).

.. c:macro:: npy_copysign(x, y)

    This is a function equivalent to C99 copysign: return x with the same sign
    as y. Works for any value, including inf and nan. Single and extended
    precisions are available with suffix f and l.

    .. versionadded:: 1.4.0

Useful math constants
~~~~~~~~~~~~~~~~~~~~~

The following math constants are available in ``npy_math.h``. Single
and extended precision are also available by adding the ``f`` and
``l`` suffixes respectively.

.. c:macro:: NPY_E

    Base of natural logarithm (:math:`e`)

.. c:macro:: NPY_LOG2E

    Logarithm to base 2 of the Euler constant (:math:`\frac{\ln(e)}{\ln(2)}`)

.. c:macro:: NPY_LOG10E

    Logarithm to base 10 of the Euler constant (:math:`\frac{\ln(e)}{\ln(10)}`)

.. c:macro:: NPY_LOGE2

    Natural logarithm of 2 (:math:`\ln(2)`)

.. c:macro:: NPY_LOGE10

    Natural logarithm of 10 (:math:`\ln(10)`)

.. c:macro:: NPY_PI

    Pi (:math:`\pi`)

.. c:macro:: NPY_PI_2

    Pi divided by 2 (:math:`\frac{\pi}{2}`)

.. c:macro:: NPY_PI_4

    Pi divided by 4 (:math:`\frac{\pi}{4}`)

.. c:macro:: NPY_1_PI

    Reciprocal of pi (:math:`\frac{1}{\pi}`)

.. c:macro:: NPY_2_PI

    Two times the reciprocal of pi (:math:`\frac{2}{\pi}`)

.. c:macro:: NPY_EULER

    The Euler constant
        :math:`\lim_{n\rightarrow\infty}({\sum_{k=1}^n{\frac{1}{k}}-\ln n})`

Low-level floating point manipulation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Those can be useful for precise floating point comparison.

.. c:function:: double npy_nextafter(double x, double y)

    This is a function equivalent to C99 nextafter: return next representable
    floating point value from x in the direction of y. Single and extended
    precisions are available with suffix f and l.

    .. versionadded:: 1.4.0

.. c:function:: double npy_spacing(double x)

    This is a function equivalent to Fortran intrinsic. Return distance between
    x and next representable floating point value from x, e.g. spacing(1) ==
    eps. spacing of nan and +/- inf return nan. Single and extended precisions
    are available with suffix f and l.

    .. versionadded:: 1.4.0

.. c:function:: void npy_set_floatstatus_divbyzero()

    Set the divide by zero floating point exception

    .. versionadded:: 1.6.0

.. c:function:: void npy_set_floatstatus_overflow()

    Set the overflow floating point exception

    .. versionadded:: 1.6.0

.. c:function:: void npy_set_floatstatus_underflow()

    Set the underflow floating point exception

    .. versionadded:: 1.6.0

.. c:function:: void npy_set_floatstatus_invalid()

    Set the invalid floating point exception

    .. versionadded:: 1.6.0

.. c:function:: int npy_get_floatstatus()

    Get floating point status. Returns a bitmask with following possible flags:

    * NPY_FPE_DIVIDEBYZERO
    * NPY_FPE_OVERFLOW
    * NPY_FPE_UNDERFLOW
    * NPY_FPE_INVALID

    Note that :c:func:`npy_get_floatstatus_barrier` is preferable as it prevents
    aggressive compiler optimizations reordering the call relative to
    the code setting the status, which could lead to incorrect results.

    .. versionadded:: 1.9.0

.. c:function:: int npy_get_floatstatus_barrier(char*)

    Get floating point status. A pointer to a local variable is passed in to
    prevent aggressive compiler optimizations from reordering this function call
    relative to the code setting the status, which could lead to incorrect
    results.

    Returns a bitmask with following possible flags:

    * NPY_FPE_DIVIDEBYZERO
    * NPY_FPE_OVERFLOW
    * NPY_FPE_UNDERFLOW
    * NPY_FPE_INVALID

    .. versionadded:: 1.15.0

.. c:function:: int npy_clear_floatstatus()

    Clears the floating point status. Returns the previous status mask.

    Note that :c:func:`npy_clear_floatstatus_barrier` is preferable as it
    prevents aggressive compiler optimizations reordering the call relative to
    the code setting the status, which could lead to incorrect results.

    .. versionadded:: 1.9.0

.. c:function:: int npy_clear_floatstatus_barrier(char*)

    Clears the floating point status. A pointer to a local variable is passed in to
    prevent aggressive compiler optimizations from reordering this function call.
    Returns the previous status mask.

    .. versionadded:: 1.15.0

Complex functions
~~~~~~~~~~~~~~~~~

.. versionadded:: 1.4.0

C99-like complex functions have been added. Those can be used if you wish to
implement portable C extensions. Since we still support platforms without C99
complex type, you need to restrict to C90-compatible syntax, e.g.:

.. code-block:: c

        /* a = 1 + 2i \*/
        npy_complex a = npy_cpack(1, 2);
        npy_complex b;

        b = npy_log(a);

Linking against the core math library in an extension
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.4.0

To use the core math library in your own extension, you need to add the npymath
compile and link options to your extension in your setup.py:

        .. hidden in a comment so as to be included in refguide but not rendered documentation
                >>> import numpy.distutils.misc_util
                >>> config = np.distutils.misc_util.Configuration(None, '', '.')
                >>> with open('foo.c', 'w') as f: pass

        >>> from numpy.distutils.misc_util import get_info
        >>> info = get_info('npymath')
        >>> _ = config.add_extension('foo', sources=['foo.c'], extra_info=info)

In other words, the usage of info is exactly the same as when using blas_info
and co.

Half-precision functions
~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.6.0

The header file <numpy/halffloat.h> provides functions to work with
IEEE 754-2008 16-bit floating point values. While this format is
not typically used for numerical computations, it is useful for
storing values which require floating point but do not need much precision.
It can also be used as an educational tool to understand the nature
of floating point round-off error.

Like for other types, NumPy includes a typedef npy_half for the 16 bit
float.  Unlike for most of the other types, you cannot use this as a
normal type in C, since it is a typedef for npy_uint16.  For example,
1.0 looks like 0x3c00 to C, and if you do an equality comparison
between the different signed zeros, you will get -0.0 != 0.0
(0x8000 != 0x0000), which is incorrect.

For these reasons, NumPy provides an API to work with npy_half values
accessible by including <numpy/halffloat.h> and linking to 'npymath'.
For functions that are not provided directly, such as the arithmetic
operations, the preferred method is to convert to float
or double and back again, as in the following example.

.. code-block:: c

        npy_half sum(int n, npy_half *array) {
            float ret = 0;
            while(n--) {
                ret += npy_half_to_float(*array++);
            }
            return npy_float_to_half(ret);
        }

External Links:

* `754-2008 IEEE Standard for Floating-Point Arithmetic`__
* `Half-precision Float Wikipedia Article`__.
* `OpenGL Half Float Pixel Support`__
* `The OpenEXR image format`__.

__ https://ieeexplore.ieee.org/document/4610935/
__ https://en.wikipedia.org/wiki/Half-precision_floating-point_format
__ https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_half_float_pixel.txt
__ https://www.openexr.com/about.html

.. c:macro:: NPY_HALF_ZERO

    This macro is defined to positive zero.

.. c:macro:: NPY_HALF_PZERO

    This macro is defined to positive zero.

.. c:macro:: NPY_HALF_NZERO

    This macro is defined to negative zero.

.. c:macro:: NPY_HALF_ONE

    This macro is defined to 1.0.

.. c:macro:: NPY_HALF_NEGONE

    This macro is defined to -1.0.

.. c:macro:: NPY_HALF_PINF

    This macro is defined to +inf.

.. c:macro:: NPY_HALF_NINF

    This macro is defined to -inf.

.. c:macro:: NPY_HALF_NAN

    This macro is defined to a NaN value, guaranteed to have its sign bit unset.

.. c:function:: float npy_half_to_float(npy_half h)

   Converts a half-precision float to a single-precision float.

.. c:function:: double npy_half_to_double(npy_half h)

   Converts a half-precision float to a double-precision float.

.. c:function:: npy_half npy_float_to_half(float f)

   Converts a single-precision float to a half-precision float.  The
   value is rounded to the nearest representable half, with ties going
   to the nearest even.  If the value is too small or too big, the
   system's floating point underflow or overflow bit will be set.

.. c:function:: npy_half npy_double_to_half(double d)

   Converts a double-precision float to a half-precision float.  The
   value is rounded to the nearest representable half, with ties going
   to the nearest even.  If the value is too small or too big, the
   system's floating point underflow or overflow bit will be set.

.. c:function:: int npy_half_eq(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 == h2).

.. c:function:: int npy_half_ne(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 != h2).

.. c:function:: int npy_half_le(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 <= h2).

.. c:function:: int npy_half_lt(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 < h2).

.. c:function:: int npy_half_ge(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 >= h2).

.. c:function:: int npy_half_gt(npy_half h1, npy_half h2)

   Compares two half-precision floats (h1 > h2).

.. c:function:: int npy_half_eq_nonan(npy_half h1, npy_half h2)

   Compares two half-precision floats that are known to not be NaN (h1 == h2).  If
   a value is NaN, the result is undefined.

.. c:function:: int npy_half_lt_nonan(npy_half h1, npy_half h2)

   Compares two half-precision floats that are known to not be NaN (h1 < h2).  If
   a value is NaN, the result is undefined.

.. c:function:: int npy_half_le_nonan(npy_half h1, npy_half h2)

   Compares two half-precision floats that are known to not be NaN (h1 <= h2).  If
   a value is NaN, the result is undefined.

.. c:function:: int npy_half_iszero(npy_half h)

   Tests whether the half-precision float has a value equal to zero.  This may be slightly
   faster than calling npy_half_eq(h, NPY_ZERO).

.. c:function:: int npy_half_isnan(npy_half h)

   Tests whether the half-precision float is a NaN.

.. c:function:: int npy_half_isinf(npy_half h)

   Tests whether the half-precision float is plus or minus Inf.

.. c:function:: int npy_half_isfinite(npy_half h)

   Tests whether the half-precision float is finite (not NaN or Inf).

.. c:function:: int npy_half_signbit(npy_half h)

   Returns 1 is h is negative, 0 otherwise.

.. c:function:: npy_half npy_half_copysign(npy_half x, npy_half y)

    Returns the value of x with the sign bit copied from y.  Works for any value,
    including Inf and NaN.

.. c:function:: npy_half npy_half_spacing(npy_half h)

    This is the same for half-precision float as npy_spacing and npy_spacingf
    described in the low-level floating point section.

.. c:function:: npy_half npy_half_nextafter(npy_half x, npy_half y)

    This is the same for half-precision float as npy_nextafter and npy_nextafterf
    described in the low-level floating point section.

.. c:function:: npy_uint16 npy_floatbits_to_halfbits(npy_uint32 f)

   Low-level function which converts a 32-bit single-precision float, stored
   as a uint32, into a 16-bit half-precision float.

.. c:function:: npy_uint16 npy_doublebits_to_halfbits(npy_uint64 d)

   Low-level function which converts a 64-bit double-precision float, stored
   as a uint64, into a 16-bit half-precision float.

.. c:function:: npy_uint32 npy_halfbits_to_floatbits(npy_uint16 h)

   Low-level function which converts a 16-bit half-precision float
   into a 32-bit single-precision float, stored as a uint32.

.. c:function:: npy_uint64 npy_halfbits_to_doublebits(npy_uint16 h)

   Low-level function which converts a 16-bit half-precision float
   into a 64-bit double-precision float, stored as a uint64.
UFunc API
=========

.. sectionauthor:: Travis E. Oliphant

.. index::
   pair: ufunc; C-API


Constants
---------

``UFUNC_ERR_{HANDLER}``
    .. c:macro:: UFUNC_ERR_IGNORE

    .. c:macro:: UFUNC_ERR_WARN

    .. c:macro:: UFUNC_ERR_RAISE

    .. c:macro:: UFUNC_ERR_CALL

``UFUNC_{THING}_{ERR}``
    .. c:macro:: UFUNC_MASK_DIVIDEBYZERO

    .. c:macro:: UFUNC_MASK_OVERFLOW

    .. c:macro:: UFUNC_MASK_UNDERFLOW

    .. c:macro:: UFUNC_MASK_INVALID

    .. c:macro:: UFUNC_SHIFT_DIVIDEBYZERO

    .. c:macro:: UFUNC_SHIFT_OVERFLOW

    .. c:macro:: UFUNC_SHIFT_UNDERFLOW

    .. c:macro:: UFUNC_SHIFT_INVALID

    .. c:macro:: UFUNC_FPE_DIVIDEBYZERO

    .. c:macro:: UFUNC_FPE_OVERFLOW

    .. c:macro:: UFUNC_FPE_UNDERFLOW

    .. c:macro:: UFUNC_FPE_INVALID

``PyUFunc_{VALUE}``
    .. c:macro:: PyUFunc_One

    .. c:macro:: PyUFunc_Zero

    .. c:macro:: PyUFunc_MinusOne

    .. c:macro:: PyUFunc_ReorderableNone

    .. c:macro:: PyUFunc_None

    .. c:macro:: PyUFunc_IdentityValue


Macros
------

.. c:macro:: NPY_LOOP_BEGIN_THREADS

    Used in universal function code to only release the Python GIL if
    loop->obj is not true (*i.e.* this is not an OBJECT array
    loop). Requires use of :c:macro:`NPY_BEGIN_THREADS_DEF` in variable
    declaration area.

.. c:macro:: NPY_LOOP_END_THREADS

    Used in universal function code to re-acquire the Python GIL if it
    was released (because loop->obj was not true).


Types
-----

.. c:type:: PyUFuncGenericFunction

    Pointers to functions that actually implement the underlying
    (element-by-element) function :math:`N` times with the following
    signature:

    .. c:function:: void loopfunc(\
            char** args, npy_intp const *dimensions, npy_intp const *steps, void* data)

        *args*

            An array of pointers to the actual data for the input and output
            arrays. The input arguments are given first followed by the output
            arguments.

        *dimensions*

            A pointer to the size of the dimension over which this function is
            looping.

        *steps*

            A pointer to the number of bytes to jump to get to the
            next element in this dimension for each of the input and
            output arguments.

        *data*

            Arbitrary data (extra arguments, function names, *etc.* )
            that can be stored with the ufunc and will be passed in
            when it is called. May be ``NULL``.

            .. versionchanged:: 1.23.0
               Accepts ``NULL`` `data` in addition to array of ``NULL`` values.

        This is an example of a func specialized for addition of doubles
        returning doubles.

        .. code-block:: c

            static void
            double_add(char **args,
                       npy_intp const *dimensions,
                       npy_intp const *steps,
                       void *extra)
            {
                npy_intp i;
                npy_intp is1 = steps[0], is2 = steps[1];
                npy_intp os = steps[2], n = dimensions[0];
                char *i1 = args[0], *i2 = args[1], *op = args[2];
                for (i = 0; i < n; i++) {
                    *((double *)op) = *((double *)i1) +
                                      *((double *)i2);
                    i1 += is1;
                    i2 += is2;
                    op += os;
                 }
            }


Functions
---------

.. c:function:: PyObject* PyUFunc_FromFuncAndData( \
        PyUFuncGenericFunction* func, void** data, char* types, int ntypes, \
        int nin, int nout, int identity, char* name, char* doc, int unused)

    Create a new broadcasting universal function from required variables.
    Each ufunc builds around the notion of an element-by-element
    operation. Each ufunc object contains pointers to 1-d loops
    implementing the basic functionality for each supported type.

    .. note::

       The *func*, *data*, *types*, *name*, and *doc* arguments are not
       copied by :c:func:`PyUFunc_FromFuncAndData`. The caller must ensure
       that the memory used by these arrays is not freed as long as the
       ufunc object is alive.

    :param func:
        Must point to an array containing *ntypes*
        :c:type:`PyUFuncGenericFunction` elements.

    :param data:
        Should be ``NULL`` or a pointer to an array of size *ntypes*.
        This array may contain arbitrary extra-data to be passed to
        the corresponding loop function in the func array, including
        ``NULL``.

    :param types:
       Length ``(nin + nout) * ntypes`` array of ``char`` encoding the
       `numpy.dtype.num` (built-in only) that the corresponding
       function in the ``func`` array accepts. For instance, for a comparison
       ufunc with three ``ntypes``, two ``nin`` and one ``nout``, where the
       first function accepts `numpy.int32` and the second
       `numpy.int64`, with both returning `numpy.bool_`, ``types`` would
       be ``(char[]) {5, 5, 0, 7, 7, 0}`` since ``NPY_INT32`` is 5,
       ``NPY_INT64`` is 7, and ``NPY_BOOL`` is 0.

       The bit-width names can also be used (e.g. :c:data:`NPY_INT32`,
       :c:data:`NPY_COMPLEX128` ) if desired.

       :ref:`ufuncs.casting` will be used at runtime to find the first
       ``func`` callable by the input/output provided.

    :param ntypes:
        How many different data-type-specific functions the ufunc has implemented.

    :param nin:
        The number of inputs to this operation.

    :param nout:
        The number of outputs

    :param identity:

        Either :c:data:`PyUFunc_One`, :c:data:`PyUFunc_Zero`,
        :c:data:`PyUFunc_MinusOne`, or :c:data:`PyUFunc_None`.
        This specifies what should be returned when
        an empty array is passed to the reduce method of the ufunc.
        The special value :c:data:`PyUFunc_IdentityValue` may only be used with
        the :c:func:`PyUFunc_FromFuncAndDataAndSignatureAndIdentity` method, to
        allow an arbitrary python object to be used as the identity.

    :param name:
        The name for the ufunc as a ``NULL`` terminated string.  Specifying
        a name of 'add' or 'multiply' enables a special behavior for
        integer-typed reductions when no dtype is given. If the input type is an
        integer (or boolean) data type smaller than the size of the `numpy.int_`
        data type, it will be internally upcast to the `numpy.int_` (or
        `numpy.uint`) data type.

    :param doc:
        Allows passing in a documentation string to be stored with the
        ufunc.  The documentation string should not contain the name
        of the function or the calling signature as that will be
        dynamically determined from the object and available when
        accessing the **__doc__** attribute of the ufunc.

    :param unused:
        Unused and present for backwards compatibility of the C-API.

.. c:function:: PyObject* PyUFunc_FromFuncAndDataAndSignature( \
        PyUFuncGenericFunction* func, void** data, char* types, int ntypes, \
        int nin, int nout, int identity, char* name, char* doc, int unused, char *signature)

   This function is very similar to PyUFunc_FromFuncAndData above, but has
   an extra *signature* argument, to define a
   :ref:`generalized universal functions <c-api.generalized-ufuncs>`.
   Similarly to how ufuncs are built around an element-by-element operation,
   gufuncs are around subarray-by-subarray operations, the
   :ref:`signature <details-of-signature>` defining the subarrays to operate on.

   :param signature:
        The signature for the new gufunc. Setting it to NULL is equivalent
        to calling PyUFunc_FromFuncAndData. A copy of the string is made,
        so the passed in buffer can be freed.

.. c:function:: PyObject* PyUFunc_FromFuncAndDataAndSignatureAndIdentity( \
        PyUFuncGenericFunction *func, void **data, char *types, int ntypes, \
        int nin, int nout, int identity, char *name, char *doc, int unused, \
        char *signature, PyObject *identity_value)

   This function is very similar to `PyUFunc_FromFuncAndDataAndSignature` above,
   but has an extra *identity_value* argument, to define an arbitrary identity
   for the ufunc when ``identity`` is passed as ``PyUFunc_IdentityValue``.

   :param identity_value:
        The identity for the new gufunc. Must be passed as ``NULL`` unless the
        ``identity`` argument is ``PyUFunc_IdentityValue``. Setting it to NULL
        is equivalent to calling PyUFunc_FromFuncAndDataAndSignature.


.. c:function:: int PyUFunc_RegisterLoopForType( \
        PyUFuncObject* ufunc, int usertype, PyUFuncGenericFunction function, \
        int* arg_types, void* data)

    This function allows the user to register a 1-d loop with an
    already- created ufunc to be used whenever the ufunc is called
    with any of its input arguments as the user-defined
    data-type. This is needed in order to make ufuncs work with
    built-in data-types. The data-type must have been previously
    registered with the numpy system. The loop is passed in as
    *function*. This loop can take arbitrary data which should be
    passed in as *data*. The data-types the loop requires are passed
    in as *arg_types* which must be a pointer to memory at least as
    large as ufunc->nargs.

.. c:function:: int PyUFunc_RegisterLoopForDescr( \
        PyUFuncObject* ufunc, PyArray_Descr* userdtype, \
        PyUFuncGenericFunction function, PyArray_Descr** arg_dtypes, void* data)

   This function behaves like PyUFunc_RegisterLoopForType above, except
   that it allows the user to register a 1-d loop using PyArray_Descr
   objects instead of dtype type num values. This allows a 1-d loop to be
   registered for structured array data-dtypes and custom data-types
   instead of scalar data-types.

.. c:function:: int PyUFunc_ReplaceLoopBySignature( \
        PyUFuncObject* ufunc, PyUFuncGenericFunction newfunc, int* signature, \
        PyUFuncGenericFunction* oldfunc)

    Replace a 1-d loop matching the given *signature* in the
    already-created *ufunc* with the new 1-d loop newfunc. Return the
    old 1-d loop function in *oldfunc*. Return 0 on success and -1 on
    failure. This function works only with built-in types (use
    :c:func:`PyUFunc_RegisterLoopForType` for user-defined types). A
    signature is an array of data-type numbers indicating the inputs
    followed by the outputs assumed by the 1-d loop.

.. c:function:: int PyUFunc_checkfperr(int errmask, PyObject* errobj)

    A simple interface to the IEEE error-flag checking support. The
    *errmask* argument is a mask of ``UFUNC_MASK_{ERR}`` bitmasks
    indicating which errors to check for (and how to check for
    them). The *errobj* must be a Python tuple with two elements: a
    string containing the name which will be used in any communication
    of error and either a callable Python object (call-back function)
    or :c:data:`Py_None`. The callable object will only be used if
    :c:data:`UFUNC_ERR_CALL` is set as the desired error checking
    method. This routine manages the GIL and is safe to call even
    after releasing the GIL. If an error in the IEEE-compatible
    hardware is determined a -1 is returned, otherwise a 0 is
    returned.

.. c:function:: void PyUFunc_clearfperr()

    Clear the IEEE error flags.

.. c:function:: void PyUFunc_GetPyValues( \
        char* name, int* bufsize, int* errmask, PyObject** errobj)

    Get the Python values used for ufunc processing from the
    thread-local storage area unless the defaults have been set in
    which case the name lookup is bypassed. The name is placed as a
    string in the first element of *\*errobj*. The second element is
    the looked-up function to call on error callback. The value of the
    looked-up buffer-size to use is passed into *bufsize*, and the
    value of the error mask is placed into *errmask*.


Generic functions
-----------------

At the core of every ufunc is a collection of type-specific functions
that defines the basic functionality for each of the supported types.
These functions must evaluate the underlying function :math:`N\geq1`
times. Extra-data may be passed in that may be used during the
calculation. This feature allows some general functions to be used as
these basic looping functions. The general function has all the code
needed to point variables to the right place and set up a function
call. The general function assumes that the actual function to call is
passed in as the extra data and calls it with the correct values. All
of these functions are suitable for placing directly in the array of
functions stored in the functions member of the PyUFuncObject
structure.

.. c:function:: void PyUFunc_f_f_As_d_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_d_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_f_f( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_g_g( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_F_F_As_D_D( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_F_F( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_D_D( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_G_G( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_e_e( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_e_e_As_f_f( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_e_e_As_d_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    Type specific, core 1-d functions for ufuncs where each
    calculation is obtained by calling a function taking one input
    argument and returning one output. This function is passed in
    ``func``. The letters correspond to dtypechar's of the supported
    data types ( ``e`` - half, ``f`` - float, ``d`` - double,
    ``g`` - long double, ``F`` - cfloat, ``D`` - cdouble,
    ``G`` - clongdouble). The argument *func* must support the same
    signature. The _As_X_X variants assume ndarray's of one data type
    but cast the values to use an underlying function that takes a
    different data type. Thus, :c:func:`PyUFunc_f_f_As_d_d` uses
    ndarrays of data type :c:data:`NPY_FLOAT` but calls out to a
    C-function that takes double and returns double.

.. c:function:: void PyUFunc_ff_f_As_dd_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_ff_f( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_dd_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_gg_g( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_FF_F_As_DD_D( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_DD_D( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_FF_F( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_GG_G( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_ee_e( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_ee_e_As_ff_f( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_ee_e_As_dd_d( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    Type specific, core 1-d functions for ufuncs where each
    calculation is obtained by calling a function taking two input
    arguments and returning one output. The underlying function to
    call is passed in as *func*. The letters correspond to
    dtypechar's of the specific data type supported by the
    general-purpose function. The argument ``func`` must support the
    corresponding signature. The ``_As_XX_X`` variants assume ndarrays
    of one data type but cast the values at each iteration of the loop
    to use the underlying function that takes a different data type.

.. c:function:: void PyUFunc_O_O( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

.. c:function:: void PyUFunc_OO_O( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    One-input, one-output, and two-input, one-output core 1-d functions
    for the :c:data:`NPY_OBJECT` data type. These functions handle reference
    count issues and return early on error. The actual function to call is
    *func* and it must accept calls with the signature ``(PyObject*)
    (PyObject*)`` for :c:func:`PyUFunc_O_O` or ``(PyObject*)(PyObject *,
    PyObject *)`` for :c:func:`PyUFunc_OO_O`.

.. c:function:: void PyUFunc_O_O_method( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    This general purpose 1-d core function assumes that *func* is a string
    representing a method of the input object. For each
    iteration of the loop, the Python object is extracted from the array
    and its *func* method is called returning the result to the output array.

.. c:function:: void PyUFunc_OO_O_method( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    This general purpose 1-d core function assumes that *func* is a
    string representing a method of the input object that takes one
    argument. The first argument in *args* is the method whose function is
    called, the second argument in *args* is the argument passed to the
    function. The output of the function is stored in the third entry
    of *args*.

.. c:function:: void PyUFunc_On_Om( \
        char** args, npy_intp const *dimensions, npy_intp const *steps, void* func)

    This is the 1-d core function used by the dynamic ufuncs created
    by umath.frompyfunc(function, nin, nout). In this case *func* is a
    pointer to a :c:type:`PyUFunc_PyFuncData` structure which has definition

    .. c:type:: PyUFunc_PyFuncData

       .. code-block:: c

           typedef struct {
               int nin;
               int nout;
               PyObject *callable;
           } PyUFunc_PyFuncData;

    At each iteration of the loop, the *nin* input objects are extracted
    from their object arrays and placed into an argument tuple, the Python
    *callable* is called with the input arguments, and the nout
    outputs are placed into their object arrays.


Importing the API
-----------------

.. c:macro:: PY_UFUNC_UNIQUE_SYMBOL

.. c:macro:: NO_IMPORT_UFUNC

.. c:function:: void import_ufunc(void)

    These are the constants and functions for accessing the ufunc
    C-API from extension modules in precisely the same way as the
    array C-API can be accessed. The ``import_ufunc`` () function must
    always be called (in the initialization subroutine of the
    extension module). If your extension module is in one file then
    that is all that is required. The other two constants are useful
    if your extension module makes use of multiple files. In that
    case, define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to something unique to
    your code and then in source files that do not contain the module
    initialization function but still need access to the UFUNC API,
    define :c:data:`PY_UFUNC_UNIQUE_SYMBOL` to the same name used previously
    and also define :c:data:`NO_IMPORT_UFUNC`.

    The C-API is actually an array of function pointers. This array is
    created (and pointed to by a global variable) by import_ufunc. The
    global variable is either statically defined or allowed to be seen
    by other files depending on the state of
    :c:data:`PY_UFUNC_UNIQUE_SYMBOL` and :c:data:`NO_IMPORT_UFUNC`.

.. index::
   pair: ufunc; C-API
Array API
=========

.. sectionauthor:: Travis E. Oliphant

|    The test of a first-rate intelligence is the ability to hold two
|    opposed ideas in the mind at the same time, and still retain the
|    ability to function.
|    --- *F. Scott Fitzgerald*

|    For a successful technology, reality must take precedence over public
|    relations, for Nature cannot be fooled.
|    --- *Richard P. Feynman*

.. index::
   pair: ndarray; C-API
   pair: C-API; array


Array structure and data access
-------------------------------

These macros access the :c:type:`PyArrayObject` structure members and are
defined in ``ndarraytypes.h``. The input argument, *arr*, can be any
:c:expr:`PyObject *` that is directly interpretable as a
:c:expr:`PyArrayObject *` (any instance of the :c:data:`PyArray_Type`
and its sub-types).

.. c:function:: int PyArray_NDIM(PyArrayObject *arr)

    The number of dimensions in the array.

.. c:function:: int PyArray_FLAGS(PyArrayObject* arr)

    Returns an integer representing the :ref:`array-flags<array-flags>`.

.. c:function:: int PyArray_TYPE(PyArrayObject* arr)

    Return the (builtin) typenumber for the elements of this array.

.. c:function:: int PyArray_SETITEM( \
        PyArrayObject* arr, void* itemptr, PyObject* obj)

    Convert obj and place it in the ndarray, *arr*, at the place
    pointed to by itemptr. Return -1 if an error occurs or 0 on
    success.

.. c:function:: void PyArray_ENABLEFLAGS(PyArrayObject* arr, int flags)

    .. versionadded:: 1.7

    Enables the specified array flags. This function does no validation,
    and assumes that you know what you're doing.

.. c:function:: void PyArray_CLEARFLAGS(PyArrayObject* arr, int flags)

    .. versionadded:: 1.7

    Clears the specified array flags. This function does no validation,
    and assumes that you know what you're doing.

.. c:function:: void *PyArray_DATA(PyArrayObject *arr)

.. c:function:: char *PyArray_BYTES(PyArrayObject *arr)

    These two macros are similar and obtain the pointer to the
    data-buffer for the array. The first macro can (and should be)
    assigned to a particular pointer where the second is for generic
    processing. If you have not guaranteed a contiguous and/or aligned
    array then be sure you understand how to access the data in the
    array to avoid memory and/or alignment problems.

.. c:function:: npy_intp *PyArray_DIMS(PyArrayObject *arr)

    Returns a pointer to the dimensions/shape of the array. The
    number of elements matches the number of dimensions
    of the array. Can return ``NULL`` for 0-dimensional arrays.

.. c:function:: npy_intp *PyArray_SHAPE(PyArrayObject *arr)

    .. versionadded:: 1.7

    A synonym for :c:func:`PyArray_DIMS`, named to be consistent with the
    `shape <numpy.ndarray.shape>` usage within Python.

.. c:function:: npy_intp *PyArray_STRIDES(PyArrayObject* arr)

    Returns a pointer to the strides of the array. The
    number of elements matches the number of dimensions
    of the array.

.. c:function:: npy_intp PyArray_DIM(PyArrayObject* arr, int n)

    Return the shape in the *n* :math:`^{\textrm{th}}` dimension.

.. c:function:: npy_intp PyArray_STRIDE(PyArrayObject* arr, int n)

    Return the stride in the *n* :math:`^{\textrm{th}}` dimension.

.. c:function:: npy_intp PyArray_ITEMSIZE(PyArrayObject* arr)

    Return the itemsize for the elements of this array.

    Note that, in the old API that was deprecated in version 1.7, this function
    had the return type ``int``.

.. c:function:: npy_intp PyArray_SIZE(PyArrayObject* arr)

    Returns the total size (in number of elements) of the array.

.. c:function:: npy_intp PyArray_Size(PyArrayObject* obj)

    Returns 0 if *obj* is not a sub-class of ndarray. Otherwise,
    returns the total number of elements in the array. Safer version
    of :c:func:`PyArray_SIZE` (*obj*).

.. c:function:: npy_intp PyArray_NBYTES(PyArrayObject* arr)

    Returns the total number of bytes consumed by the array.

.. c:function:: PyObject *PyArray_BASE(PyArrayObject* arr)

    This returns the base object of the array. In most cases, this
    means the object which owns the memory the array is pointing at.

    If you are constructing an array using the C API, and specifying
    your own memory, you should use the function :c:func:`PyArray_SetBaseObject`
    to set the base to an object which owns the memory.

    If the :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag is set, it has a different
    meaning, namely base is the array into which the current array will
    be copied upon copy resolution. This overloading of the base property
    for two functions is likely to change in a future version of NumPy.

.. c:function:: PyArray_Descr *PyArray_DESCR(PyArrayObject* arr)

    Returns a borrowed reference to the dtype property of the array.

.. c:function:: PyArray_Descr *PyArray_DTYPE(PyArrayObject* arr)

    .. versionadded:: 1.7

    A synonym for PyArray_DESCR, named to be consistent with the
    'dtype' usage within Python.

.. c:function:: PyObject *PyArray_GETITEM(PyArrayObject* arr, void* itemptr)

    Get a Python object of a builtin type from the ndarray, *arr*,
    at the location pointed to by itemptr. Return ``NULL`` on failure.

    `numpy.ndarray.item` is identical to PyArray_GETITEM.

.. c:function:: int PyArray_FinalizeFunc(PyArrayObject* arr, PyObject* obj)

    The function pointed to by the CObject
    :obj:`~numpy.class.__array_finalize__`.
    The first argument is the newly created sub-type. The second argument
    (if not NULL) is the "parent" array (if the array was created using
    slicing or some other operation where a clearly-distinguishable parent
    is present). This routine can do anything it wants to. It should
    return a -1 on error and 0 otherwise.


Data access
^^^^^^^^^^^

These functions and macros provide easy access to elements of the
ndarray from C. These work for all arrays. You may need to take care
when accessing the data in the array, however, if it is not in machine
byte-order, misaligned, or not writeable. In other words, be sure to
respect the state of the flags unless you know what you are doing, or
have previously guaranteed an array that is writeable, aligned, and in
machine byte-order using :c:func:`PyArray_FromAny`. If you wish to handle all
types of arrays, the copyswap function for each type is useful for
handling misbehaved arrays. Some platforms (e.g. Solaris) do not like
misaligned data and will crash if you de-reference a misaligned
pointer. Other platforms (e.g. x86 Linux) will just work more slowly
with misaligned data.

.. c:function:: void* PyArray_GetPtr(PyArrayObject* aobj, npy_intp* ind)

    Return a pointer to the data of the ndarray, *aobj*, at the
    N-dimensional index given by the c-array, *ind*, (which must be
    at least *aobj* ->nd in size). You may want to typecast the
    returned pointer to the data type of the ndarray.

.. c:function:: void* PyArray_GETPTR1(PyArrayObject* obj, npy_intp i)

.. c:function:: void* PyArray_GETPTR2( \
        PyArrayObject* obj, npy_intp i, npy_intp j)

.. c:function:: void* PyArray_GETPTR3( \
        PyArrayObject* obj, npy_intp i, npy_intp j, npy_intp k)

.. c:function:: void* PyArray_GETPTR4( \
        PyArrayObject* obj, npy_intp i, npy_intp j, npy_intp k, npy_intp l)

    Quick, inline access to the element at the given coordinates in
    the ndarray, *obj*, which must have respectively 1, 2, 3, or 4
    dimensions (this is not checked). The corresponding *i*, *j*,
    *k*, and *l* coordinates can be any integer but will be
    interpreted as ``npy_intp``. You may want to typecast the
    returned pointer to the data type of the ndarray.


Creating arrays
---------------


From scratch
^^^^^^^^^^^^

.. c:function:: PyObject* PyArray_NewFromDescr( \
        PyTypeObject* subtype, PyArray_Descr* descr, int nd, npy_intp const* dims, \
        npy_intp const* strides, void* data, int flags, PyObject* obj)

    This function steals a reference to *descr*. The easiest way to get one
    is using :c:func:`PyArray_DescrFromType`.

    This is the main array creation function. Most new arrays are
    created with this flexible function.

    The returned object is an object of Python-type *subtype*, which
    must be a subtype of :c:data:`PyArray_Type`.  The array has *nd*
    dimensions, described by *dims*. The data-type descriptor of the
    new array is *descr*.

    If *subtype* is of an array subclass instead of the base
    :c:data:`&PyArray_Type<PyArray_Type>`, then *obj* is the object to pass to
    the :obj:`~numpy.class.__array_finalize__` method of the subclass.

    If *data* is ``NULL``, then new unitinialized memory will be allocated and
    *flags* can be non-zero to indicate a Fortran-style contiguous array. Use
    :c:func:`PyArray_FILLWBYTE` to initialize the memory.

    If *data* is not ``NULL``, then it is assumed to point to the memory
    to be used for the array and the *flags* argument is used as the
    new flags for the array (except the state of :c:data:`NPY_ARRAY_OWNDATA`,
    :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag of the new array will be reset).

    In addition, if *data* is non-NULL, then *strides* can
    also be provided. If *strides* is ``NULL``, then the array strides
    are computed as C-style contiguous (default) or Fortran-style
    contiguous (*flags* is nonzero for *data* = ``NULL`` or *flags* &
    :c:data:`NPY_ARRAY_F_CONTIGUOUS` is nonzero non-NULL *data*). Any
    provided *dims* and *strides* are copied into newly allocated
    dimension and strides arrays for the new array object.

    :c:func:`PyArray_CheckStrides` can help verify non- ``NULL`` stride
    information.

    If ``data`` is provided, it must stay alive for the life of the array. One
    way to manage this is through :c:func:`PyArray_SetBaseObject`

.. c:function:: PyObject* PyArray_NewLikeArray( \
        PyArrayObject* prototype, NPY_ORDER order, PyArray_Descr* descr, \
        int subok)

    .. versionadded:: 1.6

    This function steals a reference to *descr* if it is not NULL.
    This array creation routine allows for the convenient creation of
    a new array matching an existing array's shapes and memory layout,
    possibly changing the layout and/or data type.

    When *order* is :c:data:`NPY_ANYORDER`, the result order is
    :c:data:`NPY_FORTRANORDER` if *prototype* is a fortran array,
    :c:data:`NPY_CORDER` otherwise.  When *order* is
    :c:data:`NPY_KEEPORDER`, the result order matches that of *prototype*, even
    when the axes of *prototype* aren't in C or Fortran order.

    If *descr* is NULL, the data type of *prototype* is used.

    If *subok* is 1, the newly created array will use the sub-type of
    *prototype* to create the new array, otherwise it will create a
    base-class array.

.. c:function:: PyObject* PyArray_New( \
        PyTypeObject* subtype, int nd, npy_intp const* dims, int type_num, \
        npy_intp const* strides, void* data, int itemsize, int flags, \
        PyObject* obj)

    This is similar to :c:func:`PyArray_NewFromDescr` (...) except you
    specify the data-type descriptor with *type_num* and *itemsize*,
    where *type_num* corresponds to a builtin (or user-defined)
    type. If the type always has the same number of bytes, then
    itemsize is ignored. Otherwise, itemsize specifies the particular
    size of this array.



.. warning::

    If data is passed to :c:func:`PyArray_NewFromDescr` or :c:func:`PyArray_New`,
    this memory must not be deallocated until the new array is
    deleted.  If this data came from another Python object, this can
    be accomplished using :c:func:`Py_INCREF` on that object and setting the
    base member of the new array to point to that object. If strides
    are passed in they must be consistent with the dimensions, the
    itemsize, and the data of the array.

.. c:function:: PyObject* PyArray_SimpleNew(int nd, npy_intp const* dims, int typenum)

    Create a new uninitialized array of type, *typenum*, whose size in
    each of *nd* dimensions is given by the integer array, *dims*.The memory
    for the array is uninitialized (unless typenum is :c:data:`NPY_OBJECT`
    in which case each element in the array is set to NULL). The
    *typenum* argument allows specification of any of the builtin
    data-types such as :c:data:`NPY_FLOAT` or :c:data:`NPY_LONG`. The
    memory for the array can be set to zero if desired using
    :c:func:`PyArray_FILLWBYTE` (return_object, 0).This function cannot be
    used to create a flexible-type array (no itemsize given).

.. c:function:: PyObject* PyArray_SimpleNewFromData( \
        int nd, npy_intp const* dims, int typenum, void* data)

    Create an array wrapper around *data* pointed to by the given
    pointer. The array flags will have a default that the data area is
    well-behaved and C-style contiguous. The shape of the array is
    given by the *dims* c-array of length *nd*. The data-type of the
    array is indicated by *typenum*. If data comes from another
    reference-counted Python object, the reference count on this object
    should be increased after the pointer is passed in, and the base member
    of the returned ndarray should point to the Python object that owns
    the data. This will ensure that the provided memory is not
    freed while the returned array is in existence.

.. c:function:: PyObject* PyArray_SimpleNewFromDescr( \
        int nd, npy_int const* dims, PyArray_Descr* descr)

    This function steals a reference to *descr*.

    Create a new array with the provided data-type descriptor, *descr*,
    of the shape determined by *nd* and *dims*.

.. c:function:: void PyArray_FILLWBYTE(PyObject* obj, int val)

    Fill the array pointed to by *obj* ---which must be a (subclass
    of) ndarray---with the contents of *val* (evaluated as a byte).
    This macro calls memset, so obj must be contiguous.

.. c:function:: PyObject* PyArray_Zeros( \
        int nd, npy_intp const* dims, PyArray_Descr* dtype, int fortran)

    Construct a new *nd* -dimensional array with shape given by *dims*
    and data type given by *dtype*. If *fortran* is non-zero, then a
    Fortran-order array is created, otherwise a C-order array is
    created. Fill the memory with zeros (or the 0 object if *dtype*
    corresponds to :c:type:`NPY_OBJECT` ).

.. c:function:: PyObject* PyArray_ZEROS( \
        int nd, npy_intp const* dims, int type_num, int fortran)

    Macro form of :c:func:`PyArray_Zeros` which takes a type-number instead
    of a data-type object.

.. c:function:: PyObject* PyArray_Empty( \
        int nd, npy_intp const* dims, PyArray_Descr* dtype, int fortran)

    Construct a new *nd* -dimensional array with shape given by *dims*
    and data type given by *dtype*. If *fortran* is non-zero, then a
    Fortran-order array is created, otherwise a C-order array is
    created. The array is uninitialized unless the data type
    corresponds to :c:type:`NPY_OBJECT` in which case the array is
    filled with :c:data:`Py_None`.

.. c:function:: PyObject* PyArray_EMPTY( \
        int nd, npy_intp const* dims, int typenum, int fortran)

    Macro form of :c:func:`PyArray_Empty` which takes a type-number,
    *typenum*, instead of a data-type object.

.. c:function:: PyObject* PyArray_Arange( \
        double start, double stop, double step, int typenum)

    Construct a new 1-dimensional array of data-type, *typenum*, that
    ranges from *start* to *stop* (exclusive) in increments of *step*
    . Equivalent to **arange** (*start*, *stop*, *step*, dtype).

.. c:function:: PyObject* PyArray_ArangeObj( \
        PyObject* start, PyObject* stop, PyObject* step, PyArray_Descr* descr)

    Construct a new 1-dimensional array of data-type determined by
    ``descr``, that ranges from ``start`` to ``stop`` (exclusive) in
    increments of ``step``. Equivalent to arange( ``start``,
    ``stop``, ``step``, ``typenum`` ).

.. c:function:: int PyArray_SetBaseObject(PyArrayObject* arr, PyObject* obj)

    .. versionadded:: 1.7

    This function **steals a reference** to ``obj`` and sets it as the
    base property of ``arr``.

    If you construct an array by passing in your own memory buffer as
    a parameter, you need to set the array's `base` property to ensure
    the lifetime of the memory buffer is appropriate.

    The return value is 0 on success, -1 on failure.

    If the object provided is an array, this function traverses the
    chain of `base` pointers so that each array points to the owner
    of the memory directly. Once the base is set, it may not be changed
    to another value.

From other objects
^^^^^^^^^^^^^^^^^^

.. c:function:: PyObject* PyArray_FromAny( \
        PyObject* op, PyArray_Descr* dtype, int min_depth, int max_depth, \
        int requirements, PyObject* context)

    This is the main function used to obtain an array from any nested
    sequence, or object that exposes the array interface, *op*. The
    parameters allow specification of the required *dtype*, the
    minimum (*min_depth*) and maximum (*max_depth*) number of
    dimensions acceptable, and other *requirements* for the array. This
    function **steals a reference** to the dtype argument, which needs
    to be a :c:type:`PyArray_Descr` structure
    indicating the desired data-type (including required
    byteorder). The *dtype* argument may be ``NULL``, indicating that any
    data-type (and byteorder) is acceptable. Unless
    :c:data:`NPY_ARRAY_FORCECAST` is present in ``flags``,
    this call will generate an error if the data
    type cannot be safely obtained from the object. If you want to use
    ``NULL`` for the *dtype* and ensure the array is notswapped then
    use :c:func:`PyArray_CheckFromAny`. A value of 0 for either of the
    depth parameters causes the parameter to be ignored. Any of the
    following array flags can be added (*e.g.* using \|) to get the
    *requirements* argument. If your code can handle general (*e.g.*
    strided, byte-swapped, or unaligned arrays) then *requirements*
    may be 0. Also, if *op* is not already an array (or does not
    expose the array interface), then a new array will be created (and
    filled from *op* using the sequence protocol). The new array will
    have :c:data:`NPY_ARRAY_DEFAULT` as its flags member. The *context*
    argument is unused.

    .. c:macro:: NPY_ARRAY_C_CONTIGUOUS

        Make sure the returned array is C-style contiguous

    .. c:macro:: NPY_ARRAY_F_CONTIGUOUS

        Make sure the returned array is Fortran-style contiguous.

    .. c:macro:: NPY_ARRAY_ALIGNED

        Make sure the returned array is aligned on proper boundaries for its
        data type. An aligned array has the data pointer and every strides
        factor as a multiple of the alignment factor for the data-type-
        descriptor.

    .. c:macro:: NPY_ARRAY_WRITEABLE

        Make sure the returned array can be written to.

    .. c:macro:: NPY_ARRAY_ENSURECOPY

        Make sure a copy is made of *op*. If this flag is not
        present, data is not copied if it can be avoided.

    .. c:macro:: NPY_ARRAY_ENSUREARRAY

        Make sure the result is a base-class ndarray. By
        default, if *op* is an instance of a subclass of
        ndarray, an instance of that same subclass is returned. If
        this flag is set, an ndarray object will be returned instead.

    .. c:macro:: NPY_ARRAY_FORCECAST

        Force a cast to the output type even if it cannot be done
        safely.  Without this flag, a data cast will occur only if it
        can be done safely, otherwise an error is raised.

    .. c:macro:: NPY_ARRAY_WRITEBACKIFCOPY

        If *op* is already an array, but does not satisfy the
        requirements, then a copy is made (which will satisfy the
        requirements). If this flag is present and a copy (of an object
        that is already an array) must be made, then the corresponding
        :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag is set in the returned
        copy and *op* is made to be read-only. You must be sure to call
        :c:func:`PyArray_ResolveWritebackIfCopy` to copy the contents
        back into *op* and the *op* array
        will be made writeable again. If *op* is not writeable to begin
        with, or if it is not already an array, then an error is raised.

    .. c:macro:: NPY_ARRAY_BEHAVED

        :c:data:`NPY_ARRAY_ALIGNED` \| :c:data:`NPY_ARRAY_WRITEABLE`

    .. c:macro:: NPY_ARRAY_CARRAY

        :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_BEHAVED`

    .. c:macro:: NPY_ARRAY_CARRAY_RO

        :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

    .. c:macro:: NPY_ARRAY_FARRAY

        :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_BEHAVED`

    .. c:macro:: NPY_ARRAY_FARRAY_RO

        :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

    .. c:macro:: NPY_ARRAY_DEFAULT

        :c:data:`NPY_ARRAY_CARRAY`

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_ARRAY_IN_ARRAY

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

    .. c:macro:: NPY_ARRAY_IN_FARRAY

        :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

.. c:macro:: NPY_OUT_ARRAY

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_WRITEABLE` \|
    :c:data:`NPY_ARRAY_ALIGNED`

.. c:macro:: NPY_ARRAY_OUT_ARRAY

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED` \|
    :c:data:`NPY_ARRAY_WRITEABLE`

    .. c:macro:: NPY_ARRAY_OUT_FARRAY

        :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_WRITEABLE` \|
        :c:data:`NPY_ARRAY_ALIGNED`

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_ARRAY_INOUT_ARRAY

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_WRITEABLE` \|
    :c:data:`NPY_ARRAY_ALIGNED` \| :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`

    .. c:macro:: NPY_ARRAY_INOUT_FARRAY

        :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_WRITEABLE` \|
        :c:data:`NPY_ARRAY_ALIGNED` \| :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`

.. c:function:: int PyArray_GetArrayParamsFromObject( \
        PyObject* op, PyArray_Descr* requested_dtype, npy_bool writeable, \
        PyArray_Descr** out_dtype, int* out_ndim, npy_intp* out_dims, \
        PyArrayObject** out_arr, PyObject* context)

    .. deprecated:: NumPy 1.19

        Unless NumPy is made aware of an issue with this, this function
        is scheduled for rapid removal without replacement.

    .. versionchanged:: NumPy 1.19

        `context` is never used. Its use results in an error.

    .. versionadded:: 1.6

.. c:function:: PyObject* PyArray_CheckFromAny( \
        PyObject* op, PyArray_Descr* dtype, int min_depth, int max_depth, \
        int requirements, PyObject* context)

    Nearly identical to :c:func:`PyArray_FromAny` (...) except
    *requirements* can contain :c:data:`NPY_ARRAY_NOTSWAPPED` (over-riding the
    specification in *dtype*) and :c:data:`NPY_ARRAY_ELEMENTSTRIDES` which
    indicates that the array should be aligned in the sense that the
    strides are multiples of the element size.

    In versions 1.6 and earlier of NumPy, the following flags
    did not have the _ARRAY_ macro namespace in them. That form
    of the constant names is deprecated in 1.7.

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_ARRAY_NOTSWAPPED

    Make sure the returned array has a data-type descriptor that is in
    machine byte-order, over-riding any specification in the *dtype*
    argument. Normally, the byte-order requirement is determined by
    the *dtype* argument. If this flag is set and the dtype argument
    does not indicate a machine byte-order descriptor (or is NULL and
    the object is already an array with a data-type descriptor that is
    not in machine byte- order), then a new data-type descriptor is
    created and used with its byte-order field set to native.

    .. c:macro:: NPY_ARRAY_BEHAVED_NS

        :c:data:`NPY_ARRAY_ALIGNED` \| :c:data:`NPY_ARRAY_WRITEABLE` \|
        :c:data:`NPY_ARRAY_NOTSWAPPED`

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_ARRAY_ELEMENTSTRIDES

    Make sure the returned array has strides that are multiples of the
    element size.

.. c:function:: PyObject* PyArray_FromArray( \
        PyArrayObject* op, PyArray_Descr* newtype, int requirements)

    Special case of :c:func:`PyArray_FromAny` for when *op* is already an
    array but it needs to be of a specific *newtype* (including
    byte-order) or has certain *requirements*.

.. c:function:: PyObject* PyArray_FromStructInterface(PyObject* op)

    Returns an ndarray object from a Python object that exposes the
    :obj:`~object.__array_struct__` attribute and follows the array interface
    protocol. If the object does not contain this attribute then a
    borrowed reference to :c:data:`Py_NotImplemented` is returned.

.. c:function:: PyObject* PyArray_FromInterface(PyObject* op)

    Returns an ndarray object from a Python object that exposes the
    :obj:`~object.__array_interface__` attribute following the array interface
    protocol. If the object does not contain this attribute then a
    borrowed reference to :c:data:`Py_NotImplemented` is returned.

.. c:function:: PyObject* PyArray_FromArrayAttr( \
        PyObject* op, PyArray_Descr* dtype, PyObject* context)

    Return an ndarray object from a Python object that exposes the
    :obj:`~numpy.class.__array__` method. The :obj:`~numpy.class.__array__`
    method can take 0, or 1 argument ``([dtype])``. ``context`` is unused.

.. c:function:: PyObject* PyArray_ContiguousFromAny( \
        PyObject* op, int typenum, int min_depth, int max_depth)

    This function returns a (C-style) contiguous and behaved function
    array from any nested sequence or array interface exporting
    object, *op*, of (non-flexible) type given by the enumerated
    *typenum*, of minimum depth *min_depth*, and of maximum depth
    *max_depth*. Equivalent to a call to :c:func:`PyArray_FromAny` with
    requirements set to :c:data:`NPY_ARRAY_DEFAULT` and the type_num member of the
    type argument set to *typenum*.

.. c:function:: PyObject* PyArray_ContiguousFromObject( \
        PyObject* op, int typenum, int min_depth, int max_depth)

    This function returns a well-behaved C-style contiguous array from any nested
    sequence or array-interface exporting object. The minimum number of dimensions
    the array can have is given by `min_depth` while the maximum is `max_depth`.
    This is equivalent to call :c:func:`PyArray_FromAny` with requirements
    :c:data:`NPY_ARRAY_DEFAULT` and :c:data:`NPY_ARRAY_ENSUREARRAY`.

.. c:function:: PyObject* PyArray_FromObject( \
        PyObject* op, int typenum, int min_depth, int max_depth)

    Return an aligned and in native-byteorder array from any nested
    sequence or array-interface exporting object, op, of a type given by
    the enumerated typenum. The minimum number of dimensions the array can
    have is given by min_depth while the maximum is max_depth. This is
    equivalent to a call to :c:func:`PyArray_FromAny` with requirements set to
    BEHAVED.

.. c:function:: PyObject* PyArray_EnsureArray(PyObject* op)

    This function **steals a reference** to ``op`` and makes sure that
    ``op`` is a base-class ndarray. It special cases array scalars,
    but otherwise calls :c:func:`PyArray_FromAny` ( ``op``, NULL, 0, 0,
    :c:data:`NPY_ARRAY_ENSUREARRAY`, NULL).

.. c:function:: PyObject* PyArray_FromString( \
        char* string, npy_intp slen, PyArray_Descr* dtype, npy_intp num, \
        char* sep)

    Construct a one-dimensional ndarray of a single type from a binary
    or (ASCII) text ``string`` of length ``slen``. The data-type of
    the array to-be-created is given by ``dtype``. If num is -1, then
    **copy** the entire string and return an appropriately sized
    array, otherwise, ``num`` is the number of items to **copy** from
    the string. If ``sep`` is NULL (or ""), then interpret the string
    as bytes of binary data, otherwise convert the sub-strings
    separated by ``sep`` to items of data-type ``dtype``. Some
    data-types may not be readable in text mode and an error will be
    raised if that occurs. All errors return NULL.

.. c:function:: PyObject* PyArray_FromFile( \
        FILE* fp, PyArray_Descr* dtype, npy_intp num, char* sep)

    Construct a one-dimensional ndarray of a single type from a binary
    or text file. The open file pointer is ``fp``, the data-type of
    the array to be created is given by ``dtype``. This must match
    the data in the file. If ``num`` is -1, then read until the end of
    the file and return an appropriately sized array, otherwise,
    ``num`` is the number of items to read. If ``sep`` is NULL (or
    ""), then read from the file in binary mode, otherwise read from
    the file in text mode with ``sep`` providing the item
    separator. Some array types cannot be read in text mode in which
    case an error is raised.

.. c:function:: PyObject* PyArray_FromBuffer( \
        PyObject* buf, PyArray_Descr* dtype, npy_intp count, npy_intp offset)

    Construct a one-dimensional ndarray of a single type from an
    object, ``buf``, that exports the (single-segment) buffer protocol
    (or has an attribute __buffer\__ that returns an object that
    exports the buffer protocol). A writeable buffer will be tried
    first followed by a read- only buffer. The :c:data:`NPY_ARRAY_WRITEABLE`
    flag of the returned array will reflect which one was
    successful. The data is assumed to start at ``offset`` bytes from
    the start of the memory location for the object. The type of the
    data in the buffer will be interpreted depending on the data- type
    descriptor, ``dtype.`` If ``count`` is negative then it will be
    determined from the size of the buffer and the requested itemsize,
    otherwise, ``count`` represents how many elements should be
    converted from the buffer.

.. c:function:: int PyArray_CopyInto(PyArrayObject* dest, PyArrayObject* src)

    Copy from the source array, ``src``, into the destination array,
    ``dest``, performing a data-type conversion if necessary. If an
    error occurs return -1 (otherwise 0). The shape of ``src`` must be
    broadcastable to the shape of ``dest``. The data areas of dest
    and src must not overlap.

.. c:function:: int PyArray_CopyObject(PyArrayObject* dest, PyObject* src)

    Assign an object ``src`` to a NumPy array ``dest`` according to
    array-coercion rules. This is basically identical to
    :c:func:`PyArray_FromAny`, but assigns directly to the output array.
    Returns 0 on success and -1 on failures.

.. c:function:: int PyArray_MoveInto(PyArrayObject* dest, PyArrayObject* src)

    Move data from the source array, ``src``, into the destination
    array, ``dest``, performing a data-type conversion if
    necessary. If an error occurs return -1 (otherwise 0). The shape
    of ``src`` must be broadcastable to the shape of ``dest``. The
    data areas of dest and src may overlap.

.. c:function:: PyArrayObject* PyArray_GETCONTIGUOUS(PyObject* op)

    If ``op`` is already (C-style) contiguous and well-behaved then
    just return a reference, otherwise return a (contiguous and
    well-behaved) copy of the array. The parameter op must be a
    (sub-class of an) ndarray and no checking for that is done.

.. c:function:: PyObject* PyArray_FROM_O(PyObject* obj)

    Convert ``obj`` to an ndarray. The argument can be any nested
    sequence or object that exports the array interface. This is a
    macro form of :c:func:`PyArray_FromAny` using ``NULL``, 0, 0, 0 for the
    other arguments. Your code must be able to handle any data-type
    descriptor and any combination of data-flags to use this macro.

.. c:function:: PyObject* PyArray_FROM_OF(PyObject* obj, int requirements)

    Similar to :c:func:`PyArray_FROM_O` except it can take an argument
    of *requirements* indicating properties the resulting array must
    have. Available requirements that can be enforced are
    :c:data:`NPY_ARRAY_C_CONTIGUOUS`, :c:data:`NPY_ARRAY_F_CONTIGUOUS`,
    :c:data:`NPY_ARRAY_ALIGNED`, :c:data:`NPY_ARRAY_WRITEABLE`,
    :c:data:`NPY_ARRAY_NOTSWAPPED`, :c:data:`NPY_ARRAY_ENSURECOPY`,
    :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`, :c:data:`NPY_ARRAY_FORCECAST`, and
    :c:data:`NPY_ARRAY_ENSUREARRAY`. Standard combinations of flags can also
    be used:

.. c:function:: PyObject* PyArray_FROM_OT(PyObject* obj, int typenum)

    Similar to :c:func:`PyArray_FROM_O` except it can take an argument of
    *typenum* specifying the type-number the returned array.

.. c:function:: PyObject* PyArray_FROM_OTF( \
        PyObject* obj, int typenum, int requirements)

    Combination of :c:func:`PyArray_FROM_OF` and :c:func:`PyArray_FROM_OT`
    allowing both a *typenum* and a *flags* argument to be provided.

.. c:function:: PyObject* PyArray_FROMANY( \
        PyObject* obj, int typenum, int min, int max, int requirements)

    Similar to :c:func:`PyArray_FromAny` except the data-type is
    specified using a typenumber. :c:func:`PyArray_DescrFromType`
    (*typenum*) is passed directly to :c:func:`PyArray_FromAny`. This
    macro also adds :c:data:`NPY_ARRAY_DEFAULT` to requirements if
    :c:data:`NPY_ARRAY_ENSURECOPY` is passed in as requirements.

.. c:function:: PyObject *PyArray_CheckAxis( \
        PyObject* obj, int* axis, int requirements)

    Encapsulate the functionality of functions and methods that take
    the axis= keyword and work properly with None as the axis
    argument. The input array is ``obj``, while ``*axis`` is a
    converted integer (so that >=MAXDIMS is the None value), and
    ``requirements`` gives the needed properties of ``obj``. The
    output is a converted version of the input so that requirements
    are met and if needed a flattening has occurred. On output
    negative values of ``*axis`` are converted and the new value is
    checked to ensure consistency with the shape of ``obj``.


Dealing with types
------------------


General check of Python Type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. c:function:: int PyArray_Check(PyObject *op)

    Evaluates true if *op* is a Python object whose type is a sub-type
    of :c:data:`PyArray_Type`.

.. c:function:: int PyArray_CheckExact(PyObject *op)

    Evaluates true if *op* is a Python object with type
    :c:data:`PyArray_Type`.

.. c:function:: int PyArray_HasArrayInterface(PyObject *op, PyObject *out)

    If ``op`` implements any part of the array interface, then ``out``
    will contain a new reference to the newly created ndarray using
    the interface or ``out`` will contain ``NULL`` if an error during
    conversion occurs. Otherwise, out will contain a borrowed
    reference to :c:data:`Py_NotImplemented` and no error condition is set.

.. c:function:: int PyArray_HasArrayInterfaceType(\
        PyObject *op, PyArray_Descr *dtype, PyObject *context, PyObject *out)

    If ``op`` implements any part of the array interface, then ``out``
    will contain a new reference to the newly created ndarray using
    the interface or ``out`` will contain ``NULL`` if an error during
    conversion occurs. Otherwise, out will contain a borrowed
    reference to Py_NotImplemented and no error condition is set.
    This version allows setting of the dtype in the part of the array interface
    that looks for the :obj:`~numpy.class.__array__` attribute. `context` is
    unused.

.. c:function:: int PyArray_IsZeroDim(PyObject *op)

    Evaluates true if *op* is an instance of (a subclass of)
    :c:data:`PyArray_Type` and has 0 dimensions.

.. c:macro:: PyArray_IsScalar(op, cls)

    Evaluates true if *op* is an instance of ``Py{cls}ArrType_Type``.

.. c:function:: int PyArray_CheckScalar(PyObject *op)

    Evaluates true if *op* is either an array scalar (an instance of a
    sub-type of :c:data:`PyGenericArr_Type` ), or an instance of (a
    sub-class of) :c:data:`PyArray_Type` whose dimensionality is 0.

.. c:function:: int PyArray_IsPythonNumber(PyObject *op)

    Evaluates true if *op* is an instance of a builtin numeric type (int,
    float, complex, long, bool)

.. c:function:: int PyArray_IsPythonScalar(PyObject *op)

    Evaluates true if *op* is a builtin Python scalar object (int,
    float, complex, bytes, str, long, bool).

.. c:function:: int PyArray_IsAnyScalar(PyObject *op)

    Evaluates true if *op* is either a Python scalar object (see
    :c:func:`PyArray_IsPythonScalar`) or an array scalar (an instance of a sub-
    type of :c:data:`PyGenericArr_Type` ).

.. c:function:: int PyArray_CheckAnyScalar(PyObject *op)

    Evaluates true if *op* is a Python scalar object (see
    :c:func:`PyArray_IsPythonScalar`), an array scalar (an instance of a
    sub-type of :c:data:`PyGenericArr_Type`) or an instance of a sub-type of
    :c:data:`PyArray_Type` whose dimensionality is 0.


Data-type checking
^^^^^^^^^^^^^^^^^^

For the typenum macros, the argument is an integer representing an
enumerated array data type. For the array type checking macros the
argument must be a :c:expr:`PyObject *` that can be directly interpreted as a
:c:expr:`PyArrayObject *`.

.. c:function:: int PyTypeNum_ISUNSIGNED(int num)

.. c:function:: int PyDataType_ISUNSIGNED(PyArray_Descr *descr)

.. c:function:: int PyArray_ISUNSIGNED(PyArrayObject *obj)

    Type represents an unsigned integer.

.. c:function:: int PyTypeNum_ISSIGNED(int num)

.. c:function:: int PyDataType_ISSIGNED(PyArray_Descr *descr)

.. c:function:: int PyArray_ISSIGNED(PyArrayObject *obj)

    Type represents a signed integer.

.. c:function:: int PyTypeNum_ISINTEGER(int num)

.. c:function:: int PyDataType_ISINTEGER(PyArray_Descr* descr)

.. c:function:: int PyArray_ISINTEGER(PyArrayObject *obj)

    Type represents any integer.

.. c:function:: int PyTypeNum_ISFLOAT(int num)

.. c:function:: int PyDataType_ISFLOAT(PyArray_Descr* descr)

.. c:function:: int PyArray_ISFLOAT(PyArrayObject *obj)

    Type represents any floating point number.

.. c:function:: int PyTypeNum_ISCOMPLEX(int num)

.. c:function:: int PyDataType_ISCOMPLEX(PyArray_Descr* descr)

.. c:function:: int PyArray_ISCOMPLEX(PyArrayObject *obj)

    Type represents any complex floating point number.

.. c:function:: int PyTypeNum_ISNUMBER(int num)

.. c:function:: int PyDataType_ISNUMBER(PyArray_Descr* descr)

.. c:function:: int PyArray_ISNUMBER(PyArrayObject *obj)

    Type represents any integer, floating point, or complex floating point
    number.

.. c:function:: int PyTypeNum_ISSTRING(int num)

.. c:function:: int PyDataType_ISSTRING(PyArray_Descr* descr)

.. c:function:: int PyArray_ISSTRING(PyArrayObject *obj)

    Type represents a string data type.

.. c:function:: int PyTypeNum_ISPYTHON(int num)

.. c:function:: int PyDataType_ISPYTHON(PyArray_Descr* descr)

.. c:function:: int PyArray_ISPYTHON(PyArrayObject *obj)

    Type represents an enumerated type corresponding to one of the
    standard Python scalar (bool, int, float, or complex).

.. c:function:: int PyTypeNum_ISFLEXIBLE(int num)

.. c:function:: int PyDataType_ISFLEXIBLE(PyArray_Descr* descr)

.. c:function:: int PyArray_ISFLEXIBLE(PyArrayObject *obj)

    Type represents one of the flexible array types ( :c:data:`NPY_STRING`,
    :c:data:`NPY_UNICODE`, or :c:data:`NPY_VOID` ).

.. c:function:: int PyDataType_ISUNSIZED(PyArray_Descr* descr)

    Type has no size information attached, and can be resized. Should only be
    called on flexible dtypes. Types that are attached to an array will always
    be sized, hence the array form of this macro not existing.

    .. versionchanged:: 1.18

    For structured datatypes with no fields this function now returns False.

.. c:function:: int PyTypeNum_ISUSERDEF(int num)

.. c:function:: int PyDataType_ISUSERDEF(PyArray_Descr* descr)

.. c:function:: int PyArray_ISUSERDEF(PyArrayObject *obj)

    Type represents a user-defined type.

.. c:function:: int PyTypeNum_ISEXTENDED(int num)

.. c:function:: int PyDataType_ISEXTENDED(PyArray_Descr* descr)

.. c:function:: int PyArray_ISEXTENDED(PyArrayObject *obj)

    Type is either flexible or user-defined.

.. c:function:: int PyTypeNum_ISOBJECT(int num)

.. c:function:: int PyDataType_ISOBJECT(PyArray_Descr* descr)

.. c:function:: int PyArray_ISOBJECT(PyArrayObject *obj)

    Type represents object data type.

.. c:function:: int PyTypeNum_ISBOOL(int num)

.. c:function:: int PyDataType_ISBOOL(PyArray_Descr* descr)

.. c:function:: int PyArray_ISBOOL(PyArrayObject *obj)

    Type represents Boolean data type.

.. c:function:: int PyDataType_HASFIELDS(PyArray_Descr* descr)

.. c:function:: int PyArray_HASFIELDS(PyArrayObject *obj)

    Type has fields associated with it.

.. c:function:: int PyArray_ISNOTSWAPPED(PyArrayObject *m)

    Evaluates true if the data area of the ndarray *m* is in machine
    byte-order according to the array's data-type descriptor.

.. c:function:: int PyArray_ISBYTESWAPPED(PyArrayObject *m)

    Evaluates true if the data area of the ndarray *m* is **not** in
    machine byte-order according to the array's data-type descriptor.

.. c:function:: npy_bool PyArray_EquivTypes( \
        PyArray_Descr* type1, PyArray_Descr* type2)

    Return :c:data:`NPY_TRUE` if *type1* and *type2* actually represent
    equivalent types for this platform (the fortran member of each
    type is ignored). For example, on 32-bit platforms,
    :c:data:`NPY_LONG` and :c:data:`NPY_INT` are equivalent. Otherwise
    return :c:data:`NPY_FALSE`.

.. c:function:: npy_bool PyArray_EquivArrTypes( \
        PyArrayObject* a1, PyArrayObject * a2)

    Return :c:data:`NPY_TRUE` if *a1* and *a2* are arrays with equivalent
    types for this platform.

.. c:function:: npy_bool PyArray_EquivTypenums(int typenum1, int typenum2)

    Special case of :c:func:`PyArray_EquivTypes` (...) that does not accept
    flexible data types but may be easier to call.

.. c:function:: int PyArray_EquivByteorders(int b1, int b2)

    True if byteorder characters *b1* and *b2* ( :c:data:`NPY_LITTLE`,
    :c:data:`NPY_BIG`, :c:data:`NPY_NATIVE`, :c:data:`NPY_IGNORE` ) are
    either equal or equivalent as to their specification of a native
    byte order. Thus, on a little-endian machine :c:data:`NPY_LITTLE`
    and :c:data:`NPY_NATIVE` are equivalent where they are not
    equivalent on a big-endian machine.


Converting data types
^^^^^^^^^^^^^^^^^^^^^

.. c:function:: PyObject* PyArray_Cast(PyArrayObject* arr, int typenum)

    Mainly for backwards compatibility to the Numeric C-API and for
    simple casts to non-flexible types. Return a new array object with
    the elements of *arr* cast to the data-type *typenum* which must
    be one of the enumerated types and not a flexible type.

.. c:function:: PyObject* PyArray_CastToType( \
        PyArrayObject* arr, PyArray_Descr* type, int fortran)

    Return a new array of the *type* specified, casting the elements
    of *arr* as appropriate. The fortran argument specifies the
    ordering of the output array.

.. c:function:: int PyArray_CastTo(PyArrayObject* out, PyArrayObject* in)

    As of 1.6, this function simply calls :c:func:`PyArray_CopyInto`,
    which handles the casting.

    Cast the elements of the array *in* into the array *out*. The
    output array should be writeable, have an integer-multiple of the
    number of elements in the input array (more than one copy can be
    placed in out), and have a data type that is one of the builtin
    types.  Returns 0 on success and -1 if an error occurs.

.. c:function:: PyArray_VectorUnaryFunc* PyArray_GetCastFunc( \
        PyArray_Descr* from, int totype)

    Return the low-level casting function to cast from the given
    descriptor to the builtin type number. If no casting function
    exists return ``NULL`` and set an error. Using this function
    instead of direct access to *from* ->f->cast will allow support of
    any user-defined casting functions added to a descriptors casting
    dictionary.

.. c:function:: int PyArray_CanCastSafely(int fromtype, int totype)

    Returns non-zero if an array of data type *fromtype* can be cast
    to an array of data type *totype* without losing information. An
    exception is that 64-bit integers are allowed to be cast to 64-bit
    floating point values even though this can lose precision on large
    integers so as not to proliferate the use of long doubles without
    explicit requests. Flexible array types are not checked according
    to their lengths with this function.

.. c:function:: int PyArray_CanCastTo( \
        PyArray_Descr* fromtype, PyArray_Descr* totype)

    :c:func:`PyArray_CanCastTypeTo` supersedes this function in
    NumPy 1.6 and later.

    Equivalent to PyArray_CanCastTypeTo(fromtype, totype, NPY_SAFE_CASTING).

.. c:function:: int PyArray_CanCastTypeTo( \
        PyArray_Descr* fromtype, PyArray_Descr* totype, NPY_CASTING casting)

    .. versionadded:: 1.6

    Returns non-zero if an array of data type *fromtype* (which can
    include flexible types) can be cast safely to an array of data
    type *totype* (which can include flexible types) according to
    the casting rule *casting*. For simple types with :c:data:`NPY_SAFE_CASTING`,
    this is basically a wrapper around :c:func:`PyArray_CanCastSafely`, but
    for flexible types such as strings or unicode, it produces results
    taking into account their sizes. Integer and float types can only be cast
    to a string or unicode type using :c:data:`NPY_SAFE_CASTING` if the string
    or unicode type is big enough to hold the max value of the integer/float
    type being cast from.

.. c:function:: int PyArray_CanCastArrayTo( \
        PyArrayObject* arr, PyArray_Descr* totype, NPY_CASTING casting)

    .. versionadded:: 1.6

    Returns non-zero if *arr* can be cast to *totype* according
    to the casting rule given in *casting*.  If *arr* is an array
    scalar, its value is taken into account, and non-zero is also
    returned when the value will not overflow or be truncated to
    an integer when converting to a smaller type.

    This is almost the same as the result of
    PyArray_CanCastTypeTo(PyArray_MinScalarType(arr), totype, casting),
    but it also handles a special case arising because the set
    of uint values is not a subset of the int values for types with the
    same number of bits.

.. c:function:: PyArray_Descr* PyArray_MinScalarType(PyArrayObject* arr)

    .. versionadded:: 1.6

    If *arr* is an array, returns its data type descriptor, but if
    *arr* is an array scalar (has 0 dimensions), it finds the data type
    of smallest size to which the value may be converted
    without overflow or truncation to an integer.

    This function will not demote complex to float or anything to
    boolean, but will demote a signed integer to an unsigned integer
    when the scalar value is positive.

.. c:function:: PyArray_Descr* PyArray_PromoteTypes( \
        PyArray_Descr* type1, PyArray_Descr* type2)

    .. versionadded:: 1.6

    Finds the data type of smallest size and kind to which *type1* and
    *type2* may be safely converted. This function is symmetric and
    associative. A string or unicode result will be the proper size for
    storing the max value of the input types converted to a string or unicode.

.. c:function:: PyArray_Descr* PyArray_ResultType( \
        npy_intp narrs, PyArrayObject **arrs, npy_intp ndtypes, \
        PyArray_Descr **dtypes)

    .. versionadded:: 1.6

    This applies type promotion to all the inputs,
    using the NumPy rules for combining scalars and arrays, to
    determine the output type of a set of operands.  This is the
    same result type that ufuncs produce. The specific algorithm
    used is as follows.

    Categories are determined by first checking which of boolean,
    integer (int/uint), or floating point (float/complex) the maximum
    kind of all the arrays and the scalars are.

    If there are only scalars or the maximum category of the scalars
    is higher than the maximum category of the arrays,
    the data types are combined with :c:func:`PyArray_PromoteTypes`
    to produce the return value.

    Otherwise, PyArray_MinScalarType is called on each array, and
    the resulting data types are all combined with
    :c:func:`PyArray_PromoteTypes` to produce the return value.

    The set of int values is not a subset of the uint values for types
    with the same number of bits, something not reflected in
    :c:func:`PyArray_MinScalarType`, but handled as a special case in
    PyArray_ResultType.

.. c:function:: int PyArray_ObjectType(PyObject* op, int mintype)

    This function is superseded by :c:func:`PyArray_MinScalarType` and/or
    :c:func:`PyArray_ResultType`.

    This function is useful for determining a common type that two or
    more arrays can be converted to. It only works for non-flexible
    array types as no itemsize information is passed. The *mintype*
    argument represents the minimum type acceptable, and *op*
    represents the object that will be converted to an array. The
    return value is the enumerated typenumber that represents the
    data-type that *op* should have.

.. c:function:: void PyArray_ArrayType( \
        PyObject* op, PyArray_Descr* mintype, PyArray_Descr* outtype)

    This function is superseded by :c:func:`PyArray_ResultType`.

    This function works similarly to :c:func:`PyArray_ObjectType` (...)
    except it handles flexible arrays. The *mintype* argument can have
    an itemsize member and the *outtype* argument will have an
    itemsize member at least as big but perhaps bigger depending on
    the object *op*.

.. c:function:: PyArrayObject** PyArray_ConvertToCommonType( \
        PyObject* op, int* n)

    The functionality this provides is largely superseded by iterator
    :c:type:`NpyIter` introduced in 1.6, with flag
    :c:data:`NPY_ITER_COMMON_DTYPE` or with the same dtype parameter for
    all operands.

    Convert a sequence of Python objects contained in *op* to an array
    of ndarrays each having the same data type. The type is selected
    in the same way as `PyArray_ResultType`. The length of the sequence is
    returned in *n*, and an *n* -length array of :c:type:`PyArrayObject`
    pointers is the return value (or ``NULL`` if an error occurs).
    The returned array must be freed by the caller of this routine
    (using :c:func:`PyDataMem_FREE` ) and all the array objects in it
    ``DECREF`` 'd or a memory-leak will occur. The example template-code
    below shows a typically usage:

    .. versionchanged:: 1.18.0
       A mix of scalars and zero-dimensional arrays now produces a type
       capable of holding the scalar value.
       Previously priority was given to the dtype of the arrays.

    .. code-block:: c

        mps = PyArray_ConvertToCommonType(obj, &n);
        if (mps==NULL) return NULL;
        {code}
        <before return>
        for (i=0; i<n; i++) Py_DECREF(mps[i]);
        PyDataMem_FREE(mps);
        {return}

.. c:function:: char* PyArray_Zero(PyArrayObject* arr)

    A pointer to newly created memory of size *arr* ->itemsize that
    holds the representation of 0 for that type. The returned pointer,
    *ret*, **must be freed** using :c:func:`PyDataMem_FREE` (ret) when it is
    not needed anymore.

.. c:function:: char* PyArray_One(PyArrayObject* arr)

    A pointer to newly created memory of size *arr* ->itemsize that
    holds the representation of 1 for that type. The returned pointer,
    *ret*, **must be freed** using :c:func:`PyDataMem_FREE` (ret) when it
    is not needed anymore.

.. c:function:: int PyArray_ValidType(int typenum)

    Returns :c:data:`NPY_TRUE` if *typenum* represents a valid type-number
    (builtin or user-defined or character code). Otherwise, this
    function returns :c:data:`NPY_FALSE`.


User-defined data types
^^^^^^^^^^^^^^^^^^^^^^^

.. c:function:: void PyArray_InitArrFuncs(PyArray_ArrFuncs* f)

    Initialize all function pointers and members to ``NULL``.

.. c:function:: int PyArray_RegisterDataType(PyArray_Descr* dtype)

    Register a data-type as a new user-defined data type for
    arrays. The type must have most of its entries filled in. This is
    not always checked and errors can produce segfaults. In
    particular, the typeobj member of the ``dtype`` structure must be
    filled with a Python type that has a fixed-size element-size that
    corresponds to the elsize member of *dtype*. Also the ``f``
    member must have the required functions: nonzero, copyswap,
    copyswapn, getitem, setitem, and cast (some of the cast functions
    may be ``NULL`` if no support is desired). To avoid confusion, you
    should choose a unique character typecode but this is not enforced
    and not relied on internally.

    A user-defined type number is returned that uniquely identifies
    the type. A pointer to the new structure can then be obtained from
    :c:func:`PyArray_DescrFromType` using the returned type number. A -1 is
    returned if an error occurs.  If this *dtype* has already been
    registered (checked only by the address of the pointer), then
    return the previously-assigned type-number.

.. c:function:: int PyArray_RegisterCastFunc( \
        PyArray_Descr* descr, int totype, PyArray_VectorUnaryFunc* castfunc)

    Register a low-level casting function, *castfunc*, to convert
    from the data-type, *descr*, to the given data-type number,
    *totype*. Any old casting function is over-written. A ``0`` is
    returned on success or a ``-1`` on failure.

.. c:function:: int PyArray_RegisterCanCast( \
        PyArray_Descr* descr, int totype, NPY_SCALARKIND scalar)

    Register the data-type number, *totype*, as castable from
    data-type object, *descr*, of the given *scalar* kind. Use
    *scalar* = :c:data:`NPY_NOSCALAR` to register that an array of data-type
    *descr* can be cast safely to a data-type whose type_number is
    *totype*. The return value is 0 on success or -1 on failure.

.. c:function:: int PyArray_TypeNumFromName( \
        char const *str)

   Given a string return the type-number for the data-type with that string as
   the type-object name.
   Returns ``NPY_NOTYPE`` without setting an error if no type can be found.
   Only works for user-defined data-types.

Special functions for NPY_OBJECT
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. c:function:: int PyArray_INCREF(PyArrayObject* op)

    Used for an array, *op*, that contains any Python objects. It
    increments the reference count of every object in the array
    according to the data-type of *op*. A -1 is returned if an error
    occurs, otherwise 0 is returned.

.. c:function:: void PyArray_Item_INCREF(char* ptr, PyArray_Descr* dtype)

    A function to INCREF all the objects at the location *ptr*
    according to the data-type *dtype*. If *ptr* is the start of a
    structured type with an object at any offset, then this will (recursively)
    increment the reference count of all object-like items in the
    structured type.

.. c:function:: int PyArray_XDECREF(PyArrayObject* op)

    Used for an array, *op*, that contains any Python objects. It
    decrements the reference count of every object in the array
    according to the data-type of *op*. Normal return value is 0. A
    -1 is returned if an error occurs.

.. c:function:: void PyArray_Item_XDECREF(char* ptr, PyArray_Descr* dtype)

    A function to XDECREF all the object-like items at the location
    *ptr* as recorded in the data-type, *dtype*. This works
    recursively so that if ``dtype`` itself has fields with data-types
    that contain object-like items, all the object-like fields will be
    XDECREF ``'d``.

.. c:function:: void PyArray_FillObjectArray(PyArrayObject* arr, PyObject* obj)

    Fill a newly created array with a single value obj at all
    locations in the structure with object data-types. No checking is
    performed but *arr* must be of data-type :c:type:`NPY_OBJECT` and be
    single-segment and uninitialized (no previous objects in
    position). Use :c:func:`PyArray_XDECREF` (*arr*) if you need to
    decrement all the items in the object array prior to calling this
    function.

.. c:function:: int PyArray_SetWritebackIfCopyBase(PyArrayObject* arr, PyArrayObject* base)

    Precondition: ``arr`` is a copy of ``base`` (though possibly with different
    strides, ordering, etc.) Sets the :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag
    and ``arr->base``, and set ``base`` to READONLY. Call
    :c:func:`PyArray_ResolveWritebackIfCopy` before calling
    `Py_DECREF` in order copy any changes back to ``base`` and
    reset the READONLY flag.

    Returns 0 for success, -1 for failure.

.. _array-flags:

Array flags
-----------

The ``flags`` attribute of the ``PyArrayObject`` structure contains
important information about the memory used by the array (pointed to
by the data member) This flag information must be kept accurate or
strange results and even segfaults may result.

There are 6 (binary) flags that describe the memory area used by the
data buffer.  These constants are defined in ``arrayobject.h`` and
determine the bit-position of the flag.  Python exposes a nice
attribute- based interface as well as a dictionary-like interface for
getting (and, if appropriate, setting) these flags.

Memory areas of all kinds can be pointed to by an ndarray, necessitating
these flags.  If you get an arbitrary ``PyArrayObject`` in C-code, you
need to be aware of the flags that are set.  If you need to guarantee
a certain kind of array (like :c:data:`NPY_ARRAY_C_CONTIGUOUS` and
:c:data:`NPY_ARRAY_BEHAVED`), then pass these requirements into the
PyArray_FromAny function.


Basic Array Flags
^^^^^^^^^^^^^^^^^

An ndarray can have a data segment that is not a simple contiguous
chunk of well-behaved memory you can manipulate. It may not be aligned
with word boundaries (very important on some platforms). It might have
its data in a different byte-order than the machine recognizes. It
might not be writeable. It might be in Fortran-contiguous order. The
array flags are used to indicate what can be said about data
associated with an array.

In versions 1.6 and earlier of NumPy, the following flags
did not have the _ARRAY_ macro namespace in them. That form
of the constant names is deprecated in 1.7.

.. c:macro:: NPY_ARRAY_C_CONTIGUOUS

    The data area is in C-style contiguous order (last index varies the
    fastest).

.. c:macro:: NPY_ARRAY_F_CONTIGUOUS

    The data area is in Fortran-style contiguous order (first index varies
    the fastest).

.. note::

    Arrays can be both C-style and Fortran-style contiguous simultaneously.
    This is clear for 1-dimensional arrays, but can also be true for higher
    dimensional arrays.

    Even for contiguous arrays a stride for a given dimension
    ``arr.strides[dim]`` may be *arbitrary* if ``arr.shape[dim] == 1``
    or the array has no elements.
    It does *not* generally hold that ``self.strides[-1] == self.itemsize``
    for C-style contiguous arrays or ``self.strides[0] == self.itemsize`` for
    Fortran-style contiguous arrays is true. The correct way to access the
    ``itemsize`` of an array from the C API is ``PyArray_ITEMSIZE(arr)``.

    .. seealso:: :ref:`Internal memory layout of an ndarray <arrays.ndarray>`

.. c:macro:: NPY_ARRAY_OWNDATA

    The data area is owned by this array. Should never be set manually, instead
    create a ``PyObject`` wrapping the data and set the array's base to that
    object. For an example, see the test in ``test_mem_policy``.

.. c:macro:: NPY_ARRAY_ALIGNED

    The data area and all array elements are aligned appropriately.

.. c:macro:: NPY_ARRAY_WRITEABLE

    The data area can be written to.

    Notice that the above 3 flags are defined so that a new, well-
    behaved array has these flags defined as true.

.. c:macro:: NPY_ARRAY_WRITEBACKIFCOPY

    The data area represents a (well-behaved) copy whose information
    should be transferred back to the original when
    :c:func:`PyArray_ResolveWritebackIfCopy` is called.

    This is a special flag that is set if this array represents a copy
    made because a user required certain flags in
    :c:func:`PyArray_FromAny` and a copy had to be made of some other
    array (and the user asked for this flag to be set in such a
    situation). The base attribute then points to the "misbehaved"
    array (which is set read_only). :c:func`PyArray_ResolveWritebackIfCopy`
    will copy its contents back to the "misbehaved"
    array (casting if necessary) and will reset the "misbehaved" array
    to :c:data:`NPY_ARRAY_WRITEABLE`. If the "misbehaved" array was not
    :c:data:`NPY_ARRAY_WRITEABLE` to begin with then :c:func:`PyArray_FromAny`
    would have returned an error because :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`
    would not have been possible.

:c:func:`PyArray_UpdateFlags` (obj, flags) will update the ``obj->flags``
for ``flags`` which can be any of :c:data:`NPY_ARRAY_C_CONTIGUOUS`,
:c:data:`NPY_ARRAY_F_CONTIGUOUS`, :c:data:`NPY_ARRAY_ALIGNED`, or
:c:data:`NPY_ARRAY_WRITEABLE`.


Combinations of array flags
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. c:macro:: NPY_ARRAY_BEHAVED

    :c:data:`NPY_ARRAY_ALIGNED` \| :c:data:`NPY_ARRAY_WRITEABLE`

.. c:macro:: NPY_ARRAY_CARRAY

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_BEHAVED`

.. c:macro:: NPY_ARRAY_CARRAY_RO

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

.. c:macro:: NPY_ARRAY_FARRAY

    :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_BEHAVED`

.. c:macro:: NPY_ARRAY_FARRAY_RO

    :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`

.. c:macro:: NPY_ARRAY_DEFAULT

    :c:data:`NPY_ARRAY_CARRAY`

.. c:macro:: NPY_ARRAY_UPDATE_ALL

    :c:data:`NPY_ARRAY_C_CONTIGUOUS` \| :c:data:`NPY_ARRAY_F_CONTIGUOUS` \| :c:data:`NPY_ARRAY_ALIGNED`


Flag-like constants
^^^^^^^^^^^^^^^^^^^

These constants are used in :c:func:`PyArray_FromAny` (and its macro forms) to
specify desired properties of the new array.

.. c:macro:: NPY_ARRAY_FORCECAST

    Cast to the desired type, even if it can't be done without losing
    information.

.. c:macro:: NPY_ARRAY_ENSURECOPY

    Make sure the resulting array is a copy of the original.

.. c:macro:: NPY_ARRAY_ENSUREARRAY

    Make sure the resulting object is an actual ndarray, and not a sub-class.


Flag checking
^^^^^^^^^^^^^

For all of these macros *arr* must be an instance of a (subclass of)
:c:data:`PyArray_Type`.

.. c:function:: int PyArray_CHKFLAGS(PyObject *arr, int flags)

    The first parameter, arr, must be an ndarray or subclass. The
    parameter, *flags*, should be an integer consisting of bitwise
    combinations of the possible flags an array can have:
    :c:data:`NPY_ARRAY_C_CONTIGUOUS`, :c:data:`NPY_ARRAY_F_CONTIGUOUS`,
    :c:data:`NPY_ARRAY_OWNDATA`, :c:data:`NPY_ARRAY_ALIGNED`,
    :c:data:`NPY_ARRAY_WRITEABLE`, :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`.

.. c:function:: int PyArray_IS_C_CONTIGUOUS(PyObject *arr)

    Evaluates true if *arr* is C-style contiguous.

.. c:function:: int PyArray_IS_F_CONTIGUOUS(PyObject *arr)

    Evaluates true if *arr* is Fortran-style contiguous.

.. c:function:: int PyArray_ISFORTRAN(PyObject *arr)

    Evaluates true if *arr* is Fortran-style contiguous and *not*
    C-style contiguous. :c:func:`PyArray_IS_F_CONTIGUOUS`
    is the correct way to test for Fortran-style contiguity.

.. c:function:: int PyArray_ISWRITEABLE(PyObject *arr)

    Evaluates true if the data area of *arr* can be written to

.. c:function:: int PyArray_ISALIGNED(PyObject *arr)

    Evaluates true if the data area of *arr* is properly aligned on
    the machine.

.. c:function:: int PyArray_ISBEHAVED(PyObject *arr)

    Evaluates true if the data area of *arr* is aligned and writeable
    and in machine byte-order according to its descriptor.

.. c:function:: int PyArray_ISBEHAVED_RO(PyObject *arr)

    Evaluates true if the data area of *arr* is aligned and in machine
    byte-order.

.. c:function:: int PyArray_ISCARRAY(PyObject *arr)

    Evaluates true if the data area of *arr* is C-style contiguous,
    and :c:func:`PyArray_ISBEHAVED` (*arr*) is true.

.. c:function:: int PyArray_ISFARRAY(PyObject *arr)

    Evaluates true if the data area of *arr* is Fortran-style
    contiguous and :c:func:`PyArray_ISBEHAVED` (*arr*) is true.

.. c:function:: int PyArray_ISCARRAY_RO(PyObject *arr)

    Evaluates true if the data area of *arr* is C-style contiguous,
    aligned, and in machine byte-order.

.. c:function:: int PyArray_ISFARRAY_RO(PyObject *arr)

    Evaluates true if the data area of *arr* is Fortran-style
    contiguous, aligned, and in machine byte-order **.**

.. c:function:: int PyArray_ISONESEGMENT(PyObject *arr)

    Evaluates true if the data area of *arr* consists of a single
    (C-style or Fortran-style) contiguous segment.

.. c:function:: void PyArray_UpdateFlags(PyArrayObject* arr, int flagmask)

    The :c:data:`NPY_ARRAY_C_CONTIGUOUS`, :c:data:`NPY_ARRAY_ALIGNED`, and
    :c:data:`NPY_ARRAY_F_CONTIGUOUS` array flags can be "calculated" from the
    array object itself. This routine updates one or more of these
    flags of *arr* as specified in *flagmask* by performing the
    required calculation.


.. warning::

    It is important to keep the flags updated (using
    :c:func:`PyArray_UpdateFlags` can help) whenever a manipulation with an
    array is performed that might cause them to change. Later
    calculations in NumPy that rely on the state of these flags do not
    repeat the calculation to update them.

.. c:function:: int PyArray_FailUnlessWriteable(PyArrayObject *obj, const char *name)

    This function does nothing and returns 0 if *obj* is writeable.
    It raises an exception and returns -1 if *obj* is not writeable.
    It may also do other house-keeping, such as issuing warnings on
    arrays which are transitioning to become views. Always call this
    function at some point before writing to an array.

    *name* is a name for the array, used to give better error messages.
    It can be something like "assignment destination", "output array",
    or even just "array".

Array method alternative API
----------------------------


Conversion
^^^^^^^^^^

.. c:function:: PyObject* PyArray_GetField( \
        PyArrayObject* self, PyArray_Descr* dtype, int offset)

    Equivalent to :meth:`ndarray.getfield<numpy.ndarray.getfield>`
    (*self*, *dtype*, *offset*). This function `steals a reference
    <https://docs.python.org/3/c-api/intro.html?reference-count-details>`_
    to `PyArray_Descr` and returns a new array of the given `dtype` using
    the data in the current array at a specified `offset` in bytes. The
    `offset` plus the itemsize of the new array type must be less than ``self
    ->descr->elsize`` or an error is raised. The same shape and strides
    as the original array are used. Therefore, this function has the
    effect of returning a field from a structured array. But, it can also
    be used to select specific bytes or groups of bytes from any array
    type.

.. c:function:: int PyArray_SetField( \
        PyArrayObject* self, PyArray_Descr* dtype, int offset, PyObject* val)

    Equivalent to :meth:`ndarray.setfield<numpy.ndarray.setfield>` (*self*, *val*, *dtype*, *offset*
    ). Set the field starting at *offset* in bytes and of the given
    *dtype* to *val*. The *offset* plus *dtype* ->elsize must be less
    than *self* ->descr->elsize or an error is raised. Otherwise, the
    *val* argument is converted to an array and copied into the field
    pointed to. If necessary, the elements of *val* are repeated to
    fill the destination array, But, the number of elements in the
    destination must be an integer multiple of the number of elements
    in *val*.

.. c:function:: PyObject* PyArray_Byteswap(PyArrayObject* self, npy_bool inplace)

    Equivalent to :meth:`ndarray.byteswap<numpy.ndarray.byteswap>` (*self*, *inplace*). Return an array
    whose data area is byteswapped. If *inplace* is non-zero, then do
    the byteswap inplace and return a reference to self. Otherwise,
    create a byteswapped copy and leave self unchanged.

.. c:function:: PyObject* PyArray_NewCopy(PyArrayObject* old, NPY_ORDER order)

    Equivalent to :meth:`ndarray.copy<numpy.ndarray.copy>` (*self*, *fortran*). Make a copy of the
    *old* array. The returned array is always aligned and writeable
    with data interpreted the same as the old array. If *order* is
    :c:data:`NPY_CORDER`, then a C-style contiguous array is returned. If
    *order* is :c:data:`NPY_FORTRANORDER`, then a Fortran-style contiguous
    array is returned. If *order is* :c:data:`NPY_ANYORDER`, then the array
    returned is Fortran-style contiguous only if the old one is;
    otherwise, it is C-style contiguous.

.. c:function:: PyObject* PyArray_ToList(PyArrayObject* self)

    Equivalent to :meth:`ndarray.tolist<numpy.ndarray.tolist>` (*self*). Return a nested Python list
    from *self*.

.. c:function:: PyObject* PyArray_ToString(PyArrayObject* self, NPY_ORDER order)

    Equivalent to :meth:`ndarray.tobytes<numpy.ndarray.tobytes>` (*self*, *order*). Return the bytes
    of this array in a Python string.

.. c:function:: PyObject* PyArray_ToFile( \
        PyArrayObject* self, FILE* fp, char* sep, char* format)

    Write the contents of *self* to the file pointer *fp* in C-style
    contiguous fashion. Write the data as binary bytes if *sep* is the
    string ""or ``NULL``. Otherwise, write the contents of *self* as
    text using the *sep* string as the item separator. Each item will
    be printed to the file.  If the *format* string is not ``NULL`` or
    "", then it is a Python print statement format string showing how
    the items are to be written.

.. c:function:: int PyArray_Dump(PyObject* self, PyObject* file, int protocol)

    Pickle the object in *self* to the given *file* (either a string
    or a Python file object). If *file* is a Python string it is
    considered to be the name of a file which is then opened in binary
    mode. The given *protocol* is used (if *protocol* is negative, or
    the highest available is used). This is a simple wrapper around
    cPickle.dump(*self*, *file*, *protocol*).

.. c:function:: PyObject* PyArray_Dumps(PyObject* self, int protocol)

    Pickle the object in *self* to a Python string and return it. Use
    the Pickle *protocol* provided (or the highest available if
    *protocol* is negative).

.. c:function:: int PyArray_FillWithScalar(PyArrayObject* arr, PyObject* obj)

    Fill the array, *arr*, with the given scalar object, *obj*. The
    object is first converted to the data type of *arr*, and then
    copied into every location. A -1 is returned if an error occurs,
    otherwise 0 is returned.

.. c:function:: PyObject* PyArray_View( \
        PyArrayObject* self, PyArray_Descr* dtype, PyTypeObject *ptype)

    Equivalent to :meth:`ndarray.view<numpy.ndarray.view>` (*self*, *dtype*). Return a new
    view of the array *self* as possibly a different data-type, *dtype*,
    and different array subclass *ptype*.

    If *dtype* is ``NULL``, then the returned array will have the same
    data type as *self*. The new data-type must be consistent with the
    size of *self*. Either the itemsizes must be identical, or *self* must
    be single-segment and the total number of bytes must be the same.
    In the latter case the dimensions of the returned array will be
    altered in the last (or first for Fortran-style contiguous arrays)
    dimension. The data area of the returned array and self is exactly
    the same.


Shape Manipulation
^^^^^^^^^^^^^^^^^^

.. c:function:: PyObject* PyArray_Newshape( \
        PyArrayObject* self, PyArray_Dims* newshape, NPY_ORDER order)

    Result will be a new array (pointing to the same memory location
    as *self* if possible), but having a shape given by *newshape*.
    If the new shape is not compatible with the strides of *self*,
    then a copy of the array with the new specified shape will be
    returned.

.. c:function:: PyObject* PyArray_Reshape(PyArrayObject* self, PyObject* shape)

    Equivalent to :meth:`ndarray.reshape<numpy.ndarray.reshape>` (*self*, *shape*) where *shape* is a
    sequence. Converts *shape* to a :c:type:`PyArray_Dims` structure and
    calls :c:func:`PyArray_Newshape` internally.
    For back-ward compatibility -- Not recommended

.. c:function:: PyObject* PyArray_Squeeze(PyArrayObject* self)

    Equivalent to :meth:`ndarray.squeeze<numpy.ndarray.squeeze>` (*self*). Return a new view of *self*
    with all of the dimensions of length 1 removed from the shape.

.. warning::

    matrix objects are always 2-dimensional. Therefore,
    :c:func:`PyArray_Squeeze` has no effect on arrays of matrix sub-class.

.. c:function:: PyObject* PyArray_SwapAxes(PyArrayObject* self, int a1, int a2)

    Equivalent to :meth:`ndarray.swapaxes<numpy.ndarray.swapaxes>` (*self*, *a1*, *a2*). The returned
    array is a new view of the data in *self* with the given axes,
    *a1* and *a2*, swapped.

.. c:function:: PyObject* PyArray_Resize( \
        PyArrayObject* self, PyArray_Dims* newshape, int refcheck, \
        NPY_ORDER fortran)

    Equivalent to :meth:`ndarray.resize<numpy.ndarray.resize>` (*self*, *newshape*, refcheck
    ``=`` *refcheck*, order= fortran ). This function only works on
    single-segment arrays. It changes the shape of *self* inplace and
    will reallocate the memory for *self* if *newshape* has a
    different total number of elements then the old shape. If
    reallocation is necessary, then *self* must own its data, have
    *self* - ``>base==NULL``, have *self* - ``>weakrefs==NULL``, and
    (unless refcheck is 0) not be referenced by any other array.
    The fortran argument can be :c:data:`NPY_ANYORDER`, :c:data:`NPY_CORDER`,
    or :c:data:`NPY_FORTRANORDER`. It currently has no effect. Eventually
    it could be used to determine how the resize operation should view
    the data when constructing a differently-dimensioned array.
    Returns None on success and NULL on error.

.. c:function:: PyObject* PyArray_Transpose( \
        PyArrayObject* self, PyArray_Dims* permute)

    Equivalent to :meth:`ndarray.transpose<numpy.ndarray.transpose>` (*self*, *permute*). Permute the
    axes of the ndarray object *self* according to the data structure
    *permute* and return the result. If *permute* is ``NULL``, then
    the resulting array has its axes reversed. For example if *self*
    has shape :math:`10\times20\times30`, and *permute* ``.ptr`` is
    (0,2,1) the shape of the result is :math:`10\times30\times20.` If
    *permute* is ``NULL``, the shape of the result is
    :math:`30\times20\times10.`

.. c:function:: PyObject* PyArray_Flatten(PyArrayObject* self, NPY_ORDER order)

    Equivalent to :meth:`ndarray.flatten<numpy.ndarray.flatten>` (*self*, *order*). Return a 1-d copy
    of the array. If *order* is :c:data:`NPY_FORTRANORDER` the elements are
    scanned out in Fortran order (first-dimension varies the
    fastest). If *order* is :c:data:`NPY_CORDER`, the elements of ``self``
    are scanned in C-order (last dimension varies the fastest). If
    *order* :c:data:`NPY_ANYORDER`, then the result of
    :c:func:`PyArray_ISFORTRAN` (*self*) is used to determine which order
    to flatten.

.. c:function:: PyObject* PyArray_Ravel(PyArrayObject* self, NPY_ORDER order)

    Equivalent to *self*.ravel(*order*). Same basic functionality
    as :c:func:`PyArray_Flatten` (*self*, *order*) except if *order* is 0
    and *self* is C-style contiguous, the shape is altered but no copy
    is performed.


Item selection and manipulation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. c:function:: PyObject* PyArray_TakeFrom( \
        PyArrayObject* self, PyObject* indices, int axis, PyArrayObject* ret, \
        NPY_CLIPMODE clipmode)

    Equivalent to :meth:`ndarray.take<numpy.ndarray.take>` (*self*, *indices*, *axis*, *ret*,
    *clipmode*) except *axis* =None in Python is obtained by setting
    *axis* = :c:data:`NPY_MAXDIMS` in C. Extract the items from self
    indicated by the integer-valued *indices* along the given *axis.*
    The clipmode argument can be :c:data:`NPY_RAISE`, :c:data:`NPY_WRAP`, or
    :c:data:`NPY_CLIP` to indicate what to do with out-of-bound indices. The
    *ret* argument can specify an output array rather than having one
    created internally.

.. c:function:: PyObject* PyArray_PutTo( \
        PyArrayObject* self, PyObject* values, PyObject* indices, \
        NPY_CLIPMODE clipmode)

    Equivalent to *self*.put(*values*, *indices*, *clipmode*
    ). Put *values* into *self* at the corresponding (flattened)
    *indices*. If *values* is too small it will be repeated as
    necessary.

.. c:function:: PyObject* PyArray_PutMask( \
        PyArrayObject* self, PyObject* values, PyObject* mask)

    Place the *values* in *self* wherever corresponding positions
    (using a flattened context) in *mask* are true. The *mask* and
    *self* arrays must have the same total number of elements. If
    *values* is too small, it will be repeated as necessary.

.. c:function:: PyObject* PyArray_Repeat( \
        PyArrayObject* self, PyObject* op, int axis)

    Equivalent to :meth:`ndarray.repeat<numpy.ndarray.repeat>` (*self*, *op*, *axis*). Copy the
    elements of *self*, *op* times along the given *axis*. Either
    *op* is a scalar integer or a sequence of length *self*
    ->dimensions[ *axis* ] indicating how many times to repeat each
    item along the axis.

.. c:function:: PyObject* PyArray_Choose( \
        PyArrayObject* self, PyObject* op, PyArrayObject* ret, \
        NPY_CLIPMODE clipmode)

    Equivalent to :meth:`ndarray.choose<numpy.ndarray.choose>` (*self*, *op*, *ret*, *clipmode*).
    Create a new array by selecting elements from the sequence of
    arrays in *op* based on the integer values in *self*. The arrays
    must all be broadcastable to the same shape and the entries in
    *self* should be between 0 and len(*op*). The output is placed
    in *ret* unless it is ``NULL`` in which case a new output is
    created. The *clipmode* argument determines behavior for when
    entries in *self* are not between 0 and len(*op*).

    .. c:macro:: NPY_RAISE

        raise a ValueError;

    .. c:macro:: NPY_WRAP

        wrap values < 0 by adding len(*op*) and values >=len(*op*)
        by subtracting len(*op*) until they are in range;

    .. c:macro:: NPY_CLIP

        all values are clipped to the region [0, len(*op*) ).


.. c:function:: PyObject* PyArray_Sort(PyArrayObject* self, int axis, NPY_SORTKIND kind)

    Equivalent to :meth:`ndarray.sort<numpy.ndarray.sort>` (*self*, *axis*, *kind*).
    Return an array with the items of *self* sorted along *axis*. The array
    is sorted using the algorithm denoted by *kind*, which is an integer/enum pointing
    to the type of sorting algorithms used.

.. c:function:: PyObject* PyArray_ArgSort(PyArrayObject* self, int axis)

    Equivalent to :meth:`ndarray.argsort<numpy.ndarray.argsort>` (*self*, *axis*).
    Return an array of indices such that selection of these indices
    along the given ``axis`` would return a sorted version of *self*. If *self* ->descr
    is a data-type with fields defined, then self->descr->names is used
    to determine the sort order. A comparison where the first field is equal
    will use the second field and so on. To alter the sort order of a
    structured array, create a new data-type with a different order of names
    and construct a view of the array with that new data-type.

.. c:function:: PyObject* PyArray_LexSort(PyObject* sort_keys, int axis)

    Given a sequence of arrays (*sort_keys*) of the same shape,
    return an array of indices (similar to :c:func:`PyArray_ArgSort` (...))
    that would sort the arrays lexicographically. A lexicographic sort
    specifies that when two keys are found to be equal, the order is
    based on comparison of subsequent keys. A merge sort (which leaves
    equal entries unmoved) is required to be defined for the
    types. The sort is accomplished by sorting the indices first using
    the first *sort_key* and then using the second *sort_key* and so
    forth. This is equivalent to the lexsort(*sort_keys*, *axis*)
    Python command. Because of the way the merge-sort works, be sure
    to understand the order the *sort_keys* must be in (reversed from
    the order you would use when comparing two elements).

    If these arrays are all collected in a structured array, then
    :c:func:`PyArray_Sort` (...) can also be used to sort the array
    directly.

.. c:function:: PyObject* PyArray_SearchSorted( \
        PyArrayObject* self, PyObject* values, NPY_SEARCHSIDE side, \
        PyObject* perm)

    Equivalent to :meth:`ndarray.searchsorted<numpy.ndarray.searchsorted>` (*self*, *values*, *side*,
    *perm*). Assuming *self* is a 1-d array in ascending order, then the
    output is an array of indices the same shape as *values* such that, if
    the elements in *values* were inserted before the indices, the order of
    *self* would be preserved. No checking is done on whether or not self is
    in ascending order.

    The *side* argument indicates whether the index returned should be that of
    the first suitable location (if :c:data:`NPY_SEARCHLEFT`) or of the last
    (if :c:data:`NPY_SEARCHRIGHT`).

    The *sorter* argument, if not ``NULL``, must be a 1D array of integer
    indices the same length as *self*, that sorts it into ascending order.
    This is typically the result of a call to :c:func:`PyArray_ArgSort` (...)
    Binary search is used to find the required insertion points.

.. c:function:: int PyArray_Partition( \
        PyArrayObject *self, PyArrayObject * ktharray, int axis, \
        NPY_SELECTKIND which)

    Equivalent to :meth:`ndarray.partition<numpy.ndarray.partition>` (*self*, *ktharray*, *axis*,
    *kind*). Partitions the array so that the values of the element indexed by
    *ktharray* are in the positions they would be if the array is fully sorted
    and places all elements smaller than the kth before and all elements equal
    or greater after the kth element. The ordering of all elements within the
    partitions is undefined.
    If *self*->descr is a data-type with fields defined, then
    self->descr->names is used to determine the sort order. A comparison where
    the first field is equal will use the second field and so on. To alter the
    sort order of a structured array, create a new data-type with a different
    order of names and construct a view of the array with that new data-type.
    Returns zero on success and -1 on failure.

.. c:function:: PyObject* PyArray_ArgPartition( \
        PyArrayObject *op, PyArrayObject * ktharray, int axis, \
        NPY_SELECTKIND which)

    Equivalent to :meth:`ndarray.argpartition<numpy.ndarray.argpartition>` (*self*, *ktharray*, *axis*,
    *kind*). Return an array of indices such that selection of these indices
    along the given ``axis`` would return a partitioned version of *self*.

.. c:function:: PyObject* PyArray_Diagonal( \
        PyArrayObject* self, int offset, int axis1, int axis2)

    Equivalent to :meth:`ndarray.diagonal<numpy.ndarray.diagonal>` (*self*, *offset*, *axis1*, *axis2*
    ). Return the *offset* diagonals of the 2-d arrays defined by
    *axis1* and *axis2*.

.. c:function:: npy_intp PyArray_CountNonzero(PyArrayObject* self)

    .. versionadded:: 1.6

    Counts the number of non-zero elements in the array object *self*.

.. c:function:: PyObject* PyArray_Nonzero(PyArrayObject* self)

    Equivalent to :meth:`ndarray.nonzero<numpy.ndarray.nonzero>` (*self*). Returns a tuple of index
    arrays that select elements of *self* that are nonzero. If (nd=
    :c:func:`PyArray_NDIM` ( ``self`` ))==1, then a single index array is
    returned. The index arrays have data type :c:data:`NPY_INTP`. If a
    tuple is returned (nd :math:`\neq` 1), then its length is nd.

.. c:function:: PyObject* PyArray_Compress( \
        PyArrayObject* self, PyObject* condition, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.compress<numpy.ndarray.compress>` (*self*, *condition*, *axis*
    ). Return the elements along *axis* corresponding to elements of
    *condition* that are true.


Calculation
^^^^^^^^^^^

.. tip::

    Pass in :c:data:`NPY_MAXDIMS` for axis in order to achieve the same
    effect that is obtained by passing in ``axis=None`` in Python
    (treating the array as a 1-d array).


.. note::

    The out argument specifies where to place the result. If out is
    NULL, then the output array is created, otherwise the output is
    placed in out which must be the correct size and type. A new
    reference to the output array is always returned even when out
    is not NULL. The caller of the routine has the responsibility
    to ``Py_DECREF`` out if not NULL or a memory-leak will occur.


.. c:function:: PyObject* PyArray_ArgMax( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.argmax<numpy.ndarray.argmax>` (*self*, *axis*). Return the index of
    the largest element of *self* along *axis*.

.. c:function:: PyObject* PyArray_ArgMin( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.argmin<numpy.ndarray.argmin>` (*self*, *axis*). Return the index of
    the smallest element of *self* along *axis*.

.. c:function:: PyObject* PyArray_Max( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.max<numpy.ndarray.max>` (*self*, *axis*). Returns the largest
    element of *self* along the given *axis*. When the result is a single
    element, returns a numpy scalar instead of an ndarray.

.. c:function:: PyObject* PyArray_Min( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.min<numpy.ndarray.min>` (*self*, *axis*). Return the smallest
    element of *self* along the given *axis*. When the result is a single
    element, returns a numpy scalar instead of an ndarray.


.. c:function:: PyObject* PyArray_Ptp( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.ptp<numpy.ndarray.ptp>` (*self*, *axis*). Return the difference
    between the largest element of *self* along *axis* and the
    smallest element of *self* along *axis*. When the result is a single
    element, returns a numpy scalar instead of an ndarray.




.. note::

    The rtype argument specifies the data-type the reduction should
    take place over. This is important if the data-type of the array
    is not "large" enough to handle the output. By default, all
    integer data-types are made at least as large as :c:data:`NPY_LONG`
    for the "add" and "multiply" ufuncs (which form the basis for
    mean, sum, cumsum, prod, and cumprod functions).

.. c:function:: PyObject* PyArray_Mean( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.mean<numpy.ndarray.mean>` (*self*, *axis*, *rtype*). Returns the
    mean of the elements along the given *axis*, using the enumerated
    type *rtype* as the data type to sum in. Default sum behavior is
    obtained using :c:data:`NPY_NOTYPE` for *rtype*.

.. c:function:: PyObject* PyArray_Trace( \
        PyArrayObject* self, int offset, int axis1, int axis2, int rtype, \
        PyArrayObject* out)

    Equivalent to :meth:`ndarray.trace<numpy.ndarray.trace>` (*self*, *offset*, *axis1*, *axis2*,
    *rtype*). Return the sum (using *rtype* as the data type of
    summation) over the *offset* diagonal elements of the 2-d arrays
    defined by *axis1* and *axis2* variables. A positive offset
    chooses diagonals above the main diagonal. A negative offset
    selects diagonals below the main diagonal.

.. c:function:: PyObject* PyArray_Clip( \
        PyArrayObject* self, PyObject* min, PyObject* max)

    Equivalent to :meth:`ndarray.clip<numpy.ndarray.clip>` (*self*, *min*, *max*). Clip an array,
    *self*, so that values larger than *max* are fixed to *max* and
    values less than *min* are fixed to *min*.

.. c:function:: PyObject* PyArray_Conjugate(PyArrayObject* self)

    Equivalent to :meth:`ndarray.conjugate<numpy.ndarray.conjugate>` (*self*).
    Return the complex conjugate of *self*. If *self* is not of
    complex data type, then return *self* with a reference.

.. c:function:: PyObject* PyArray_Round( \
        PyArrayObject* self, int decimals, PyArrayObject* out)

    Equivalent to :meth:`ndarray.round<numpy.ndarray.round>` (*self*, *decimals*, *out*). Returns
    the array with elements rounded to the nearest decimal place. The
    decimal place is defined as the :math:`10^{-\textrm{decimals}}`
    digit so that negative *decimals* cause rounding to the nearest 10's, 100's, etc. If out is ``NULL``, then the output array is created, otherwise the output is placed in *out* which must be the correct size and type.

.. c:function:: PyObject* PyArray_Std( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.std<numpy.ndarray.std>` (*self*, *axis*, *rtype*). Return the
    standard deviation using data along *axis* converted to data type
    *rtype*.

.. c:function:: PyObject* PyArray_Sum( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.sum<numpy.ndarray.sum>` (*self*, *axis*, *rtype*). Return 1-d
    vector sums of elements in *self* along *axis*. Perform the sum
    after converting data to data type *rtype*.

.. c:function:: PyObject* PyArray_CumSum( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.cumsum<numpy.ndarray.cumsum>` (*self*, *axis*, *rtype*). Return
    cumulative 1-d sums of elements in *self* along *axis*. Perform
    the sum after converting data to data type *rtype*.

.. c:function:: PyObject* PyArray_Prod( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.prod<numpy.ndarray.prod>` (*self*, *axis*, *rtype*). Return 1-d
    products of elements in *self* along *axis*. Perform the product
    after converting data to data type *rtype*.

.. c:function:: PyObject* PyArray_CumProd( \
        PyArrayObject* self, int axis, int rtype, PyArrayObject* out)

    Equivalent to :meth:`ndarray.cumprod<numpy.ndarray.cumprod>` (*self*, *axis*, *rtype*). Return
    1-d cumulative products of elements in ``self`` along ``axis``.
    Perform the product after converting data to data type ``rtype``.

.. c:function:: PyObject* PyArray_All( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.all<numpy.ndarray.all>` (*self*, *axis*). Return an array with
    True elements for every 1-d sub-array of ``self`` defined by
    ``axis`` in which all the elements are True.

.. c:function:: PyObject* PyArray_Any( \
        PyArrayObject* self, int axis, PyArrayObject* out)

    Equivalent to :meth:`ndarray.any<numpy.ndarray.any>` (*self*, *axis*). Return an array with
    True elements for every 1-d sub-array of *self* defined by *axis*
    in which any of the elements are True.

Functions
---------


Array Functions
^^^^^^^^^^^^^^^

.. c:function:: int PyArray_AsCArray( \
        PyObject** op, void* ptr, npy_intp* dims, int nd, int typenum, \
        int itemsize)

    Sometimes it is useful to access a multidimensional array as a
    C-style multi-dimensional array so that algorithms can be
    implemented using C's a[i][j][k] syntax. This routine returns a
    pointer, *ptr*, that simulates this kind of C-style array, for
    1-, 2-, and 3-d ndarrays.

    :param op:

        The address to any Python object. This Python object will be replaced
        with an equivalent well-behaved, C-style contiguous, ndarray of the
        given data type specified by the last two arguments. Be sure that
        stealing a reference in this way to the input object is justified.

    :param ptr:

        The address to a (ctype* for 1-d, ctype** for 2-d or ctype*** for 3-d)
        variable where ctype is the equivalent C-type for the data type. On
        return, *ptr* will be addressable as a 1-d, 2-d, or 3-d array.

    :param dims:

        An output array that contains the shape of the array object. This
        array gives boundaries on any looping that will take place.

    :param nd:

        The dimensionality of the array (1, 2, or 3).

    :param typenum:

        The expected data type of the array.

    :param itemsize:

        This argument is only needed when *typenum* represents a
        flexible array. Otherwise it should be 0.

.. note::

    The simulation of a C-style array is not complete for 2-d and 3-d
    arrays. For example, the simulated arrays of pointers cannot be passed
    to subroutines expecting specific, statically-defined 2-d and 3-d
    arrays. To pass to functions requiring those kind of inputs, you must
    statically define the required array and copy data.

.. c:function:: int PyArray_Free(PyObject* op, void* ptr)

    Must be called with the same objects and memory locations returned
    from :c:func:`PyArray_AsCArray` (...). This function cleans up memory
    that otherwise would get leaked.

.. c:function:: PyObject* PyArray_Concatenate(PyObject* obj, int axis)

    Join the sequence of objects in *obj* together along *axis* into a
    single array. If the dimensions or types are not compatible an
    error is raised.

.. c:function:: PyObject* PyArray_InnerProduct(PyObject* obj1, PyObject* obj2)

    Compute a product-sum over the last dimensions of *obj1* and
    *obj2*. Neither array is conjugated.

.. c:function:: PyObject* PyArray_MatrixProduct(PyObject* obj1, PyObject* obj)

    Compute a product-sum over the last dimension of *obj1* and the
    second-to-last dimension of *obj2*. For 2-d arrays this is a
    matrix-product. Neither array is conjugated.

.. c:function:: PyObject* PyArray_MatrixProduct2( \
        PyObject* obj1, PyObject* obj, PyArrayObject* out)

    .. versionadded:: 1.6

    Same as PyArray_MatrixProduct, but store the result in *out*.  The
    output array must have the correct shape, type, and be
    C-contiguous, or an exception is raised.

.. c:function:: PyObject* PyArray_EinsteinSum( \
        char* subscripts, npy_intp nop, PyArrayObject** op_in, \
        PyArray_Descr* dtype, NPY_ORDER order, NPY_CASTING casting, \
        PyArrayObject* out)

    .. versionadded:: 1.6

    Applies the Einstein summation convention to the array operands
    provided, returning a new array or placing the result in *out*.
    The string in *subscripts* is a comma separated list of index
    letters. The number of operands is in *nop*, and *op_in* is an
    array containing those operands. The data type of the output can
    be forced with *dtype*, the output order can be forced with *order*
    (:c:data:`NPY_KEEPORDER` is recommended), and when *dtype* is specified,
    *casting* indicates how permissive the data conversion should be.

    See the :func:`~numpy.einsum` function for more details.

.. c:function:: PyObject* PyArray_CopyAndTranspose(PyObject * op)

    A specialized copy and transpose function that works only for 2-d
    arrays. The returned array is a transposed copy of *op*.

.. c:function:: PyObject* PyArray_Correlate( \
        PyObject* op1, PyObject* op2, int mode)

    Compute the 1-d correlation of the 1-d arrays *op1* and *op2*
    . The correlation is computed at each output point by multiplying
    *op1* by a shifted version of *op2* and summing the result. As a
    result of the shift, needed values outside of the defined range of
    *op1* and *op2* are interpreted as zero. The mode determines how
    many shifts to return: 0 - return only shifts that did not need to
    assume zero- values; 1 - return an object that is the same size as
    *op1*, 2 - return all possible shifts (any overlap at all is
    accepted).

    .. rubric:: Notes

    This does not compute the usual correlation: if op2 is larger than op1, the
    arguments are swapped, and the conjugate is never taken for complex arrays.
    See PyArray_Correlate2 for the usual signal processing correlation.

.. c:function:: PyObject* PyArray_Correlate2( \
        PyObject* op1, PyObject* op2, int mode)

    Updated version of PyArray_Correlate, which uses the usual definition of
    correlation for 1d arrays. The correlation is computed at each output point
    by multiplying *op1* by a shifted version of *op2* and summing the result.
    As a result of the shift, needed values outside of the defined range of
    *op1* and *op2* are interpreted as zero. The mode determines how many
    shifts to return: 0 - return only shifts that did not need to assume zero-
    values; 1 - return an object that is the same size as *op1*, 2 - return all
    possible shifts (any overlap at all is accepted).

    .. rubric:: Notes

    Compute z as follows::

      z[k] = sum_n op1[n] * conj(op2[n+k])

.. c:function:: PyObject* PyArray_Where( \
        PyObject* condition, PyObject* x, PyObject* y)

    If both ``x`` and ``y`` are ``NULL``, then return
    :c:func:`PyArray_Nonzero` (*condition*). Otherwise, both *x* and *y*
    must be given and the object returned is shaped like *condition*
    and has elements of *x* and *y* where *condition* is respectively
    True or False.


Other functions
^^^^^^^^^^^^^^^

.. c:function:: npy_bool PyArray_CheckStrides( \
        int elsize, int nd, npy_intp numbytes, npy_intp const* dims, \
        npy_intp const* newstrides)

    Determine if *newstrides* is a strides array consistent with the
    memory of an *nd* -dimensional array with shape ``dims`` and
    element-size, *elsize*. The *newstrides* array is checked to see
    if jumping by the provided number of bytes in each direction will
    ever mean jumping more than *numbytes* which is the assumed size
    of the available memory segment. If *numbytes* is 0, then an
    equivalent *numbytes* is computed assuming *nd*, *dims*, and
    *elsize* refer to a single-segment array. Return :c:data:`NPY_TRUE` if
    *newstrides* is acceptable, otherwise return :c:data:`NPY_FALSE`.

.. c:function:: npy_intp PyArray_MultiplyList(npy_intp const* seq, int n)

.. c:function:: int PyArray_MultiplyIntList(int const* seq, int n)

    Both of these routines multiply an *n* -length array, *seq*, of
    integers and return the result. No overflow checking is performed.

.. c:function:: int PyArray_CompareLists(npy_intp const* l1, npy_intp const* l2, int n)

    Given two *n* -length arrays of integers, *l1*, and *l2*, return
    1 if the lists are identical; otherwise, return 0.


Auxiliary Data With Object Semantics
------------------------------------

.. versionadded:: 1.7.0

.. c:type:: NpyAuxData

When working with more complex dtypes which are composed of other dtypes,
such as the struct dtype, creating inner loops that manipulate the dtypes
requires carrying along additional data. NumPy supports this idea
through a struct :c:type:`NpyAuxData`, mandating a few conventions so that
it is possible to do this.

Defining an :c:type:`NpyAuxData` is similar to defining a class in C++,
but the object semantics have to be tracked manually since the API is in C.
Here's an example for a function which doubles up an element using
an element copier function as a primitive.

.. code-block:: c

    typedef struct {
        NpyAuxData base;
        ElementCopier_Func *func;
        NpyAuxData *funcdata;
    } eldoubler_aux_data;

    void free_element_doubler_aux_data(NpyAuxData *data)
    {
        eldoubler_aux_data *d = (eldoubler_aux_data *)data;
        /* Free the memory owned by this auxdata */
        NPY_AUXDATA_FREE(d->funcdata);
        PyArray_free(d);
    }

    NpyAuxData *clone_element_doubler_aux_data(NpyAuxData *data)
    {
        eldoubler_aux_data *ret = PyArray_malloc(sizeof(eldoubler_aux_data));
        if (ret == NULL) {
            return NULL;
        }

        /* Raw copy of all data */
        memcpy(ret, data, sizeof(eldoubler_aux_data));

        /* Fix up the owned auxdata so we have our own copy */
        ret->funcdata = NPY_AUXDATA_CLONE(ret->funcdata);
        if (ret->funcdata == NULL) {
            PyArray_free(ret);
            return NULL;
        }

        return (NpyAuxData *)ret;
    }

    NpyAuxData *create_element_doubler_aux_data(
                                ElementCopier_Func *func,
                                NpyAuxData *funcdata)
    {
        eldoubler_aux_data *ret = PyArray_malloc(sizeof(eldoubler_aux_data));
        if (ret == NULL) {
            PyErr_NoMemory();
            return NULL;
        }
        memset(&ret, 0, sizeof(eldoubler_aux_data));
        ret->base->free = &free_element_doubler_aux_data;
        ret->base->clone = &clone_element_doubler_aux_data;
        ret->func = func;
        ret->funcdata = funcdata;

        return (NpyAuxData *)ret;
    }

.. c:type:: NpyAuxData_FreeFunc

    The function pointer type for NpyAuxData free functions.

.. c:type:: NpyAuxData_CloneFunc

    The function pointer type for NpyAuxData clone functions. These
    functions should never set the Python exception on error, because
    they may be called from a multi-threaded context.

.. c:function:: void NPY_AUXDATA_FREE(NpyAuxData *auxdata)

    A macro which calls the auxdata's free function appropriately,
    does nothing if auxdata is NULL.

.. c:function:: NpyAuxData *NPY_AUXDATA_CLONE(NpyAuxData *auxdata)

    A macro which calls the auxdata's clone function appropriately,
    returning a deep copy of the auxiliary data.

Array Iterators
---------------

As of NumPy 1.6.0, these array iterators are superseded by
the new array iterator, :c:type:`NpyIter`.

An array iterator is a simple way to access the elements of an
N-dimensional array quickly and efficiently. Section `2
<#sec-array-iterator>`__ provides more description and examples of
this useful approach to looping over an array.

.. c:function:: PyObject* PyArray_IterNew(PyObject* arr)

    Return an array iterator object from the array, *arr*. This is
    equivalent to *arr*. **flat**. The array iterator object makes
    it easy to loop over an N-dimensional non-contiguous array in
    C-style contiguous fashion.

.. c:function:: PyObject* PyArray_IterAllButAxis(PyObject* arr, int* axis)

    Return an array iterator that will iterate over all axes but the
    one provided in *\*axis*. The returned iterator cannot be used
    with :c:func:`PyArray_ITER_GOTO1D`. This iterator could be used to
    write something similar to what ufuncs do wherein the loop over
    the largest axis is done by a separate sub-routine. If *\*axis* is
    negative then *\*axis* will be set to the axis having the smallest
    stride and that axis will be used.

.. c:function:: PyObject *PyArray_BroadcastToShape( \
        PyObject* arr, npy_intp const *dimensions, int nd)

    Return an array iterator that is broadcast to iterate as an array
    of the shape provided by *dimensions* and *nd*.

.. c:function:: int PyArrayIter_Check(PyObject* op)

    Evaluates true if *op* is an array iterator (or instance of a
    subclass of the array iterator type).

.. c:function:: void PyArray_ITER_RESET(PyObject* iterator)

    Reset an *iterator* to the beginning of the array.

.. c:function:: void PyArray_ITER_NEXT(PyObject* iterator)

    Incremement the index and the dataptr members of the *iterator* to
    point to the next element of the array. If the array is not
    (C-style) contiguous, also increment the N-dimensional coordinates
    array.

.. c:function:: void *PyArray_ITER_DATA(PyObject* iterator)

    A pointer to the current element of the array.

.. c:function:: void PyArray_ITER_GOTO( \
        PyObject* iterator, npy_intp* destination)

    Set the *iterator* index, dataptr, and coordinates members to the
    location in the array indicated by the N-dimensional c-array,
    *destination*, which must have size at least *iterator*
    ->nd_m1+1.

.. c:function:: void PyArray_ITER_GOTO1D(PyObject* iterator, npy_intp index)

    Set the *iterator* index and dataptr to the location in the array
    indicated by the integer *index* which points to an element in the
    C-styled flattened array.

.. c:function:: int PyArray_ITER_NOTDONE(PyObject* iterator)

    Evaluates TRUE as long as the iterator has not looped through all of
    the elements, otherwise it evaluates FALSE.


Broadcasting (multi-iterators)
------------------------------

.. c:function:: PyObject* PyArray_MultiIterNew(int num, ...)

    A simplified interface to broadcasting. This function takes the
    number of arrays to broadcast and then *num* extra ( :c:type:`PyObject *<PyObject>`
    ) arguments. These arguments are converted to arrays and iterators
    are created. :c:func:`PyArray_Broadcast` is then called on the resulting
    multi-iterator object. The resulting, broadcasted mult-iterator
    object is then returned. A broadcasted operation can then be
    performed using a single loop and using :c:func:`PyArray_MultiIter_NEXT`
    (..)

.. c:function:: void PyArray_MultiIter_RESET(PyObject* multi)

    Reset all the iterators to the beginning in a multi-iterator
    object, *multi*.

.. c:function:: void PyArray_MultiIter_NEXT(PyObject* multi)

    Advance each iterator in a multi-iterator object, *multi*, to its
    next (broadcasted) element.

.. c:function:: void *PyArray_MultiIter_DATA(PyObject* multi, int i)

    Return the data-pointer of the *i* :math:`^{\textrm{th}}` iterator
    in a multi-iterator object.

.. c:function:: void PyArray_MultiIter_NEXTi(PyObject* multi, int i)

    Advance the pointer of only the *i* :math:`^{\textrm{th}}` iterator.

.. c:function:: void PyArray_MultiIter_GOTO( \
        PyObject* multi, npy_intp* destination)

    Advance each iterator in a multi-iterator object, *multi*, to the
    given :math:`N` -dimensional *destination* where :math:`N` is the
    number of dimensions in the broadcasted array.

.. c:function:: void PyArray_MultiIter_GOTO1D(PyObject* multi, npy_intp index)

    Advance each iterator in a multi-iterator object, *multi*, to the
    corresponding location of the *index* into the flattened
    broadcasted array.

.. c:function:: int PyArray_MultiIter_NOTDONE(PyObject* multi)

    Evaluates TRUE as long as the multi-iterator has not looped
    through all of the elements (of the broadcasted result), otherwise
    it evaluates FALSE.

.. c:function:: int PyArray_Broadcast(PyArrayMultiIterObject* mit)

    This function encapsulates the broadcasting rules. The *mit*
    container should already contain iterators for all the arrays that
    need to be broadcast. On return, these iterators will be adjusted
    so that iteration over each simultaneously will accomplish the
    broadcasting. A negative number is returned if an error occurs.

.. c:function:: int PyArray_RemoveSmallest(PyArrayMultiIterObject* mit)

    This function takes a multi-iterator object that has been
    previously "broadcasted," finds the dimension with the smallest
    "sum of strides" in the broadcasted result and adapts all the
    iterators so as not to iterate over that dimension (by effectively
    making them of length-1 in that dimension). The corresponding
    dimension is returned unless *mit* ->nd is 0, then -1 is
    returned. This function is useful for constructing ufunc-like
    routines that broadcast their inputs correctly and then call a
    strided 1-d version of the routine as the inner-loop.  This 1-d
    version is usually optimized for speed and for this reason the
    loop should be performed over the axis that won't require large
    stride jumps.

Neighborhood iterator
---------------------

.. versionadded:: 1.4.0

Neighborhood iterators are subclasses of the iterator object, and can be used
to iter over a neighborhood of a point. For example, you may want to iterate
over every voxel of a 3d image, and for every such voxel, iterate over an
hypercube. Neighborhood iterator automatically handle boundaries, thus making
this kind of code much easier to write than manual boundaries handling, at the
cost of a slight overhead.

.. c:function:: PyObject* PyArray_NeighborhoodIterNew( \
        PyArrayIterObject* iter, npy_intp bounds, int mode, \
        PyArrayObject* fill_value)

    This function creates a new neighborhood iterator from an existing
    iterator.  The neighborhood will be computed relatively to the position
    currently pointed by *iter*, the bounds define the shape of the
    neighborhood iterator, and the mode argument the boundaries handling mode.

    The *bounds* argument is expected to be a (2 * iter->ao->nd) arrays, such
    as the range bound[2*i]->bounds[2*i+1] defines the range where to walk for
    dimension i (both bounds are included in the walked coordinates). The
    bounds should be ordered for each dimension (bounds[2*i] <= bounds[2*i+1]).

    The mode should be one of:

    .. c:macro:: NPY_NEIGHBORHOOD_ITER_ZERO_PADDING

            Zero padding. Outside bounds values will be 0.

    .. c:macro:: NPY_NEIGHBORHOOD_ITER_ONE_PADDING

            One padding, Outside bounds values will be 1.

    .. c:macro:: NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING

            Constant padding. Outside bounds values will be the
            same as the first item in fill_value.

    .. c:macro:: NPY_NEIGHBORHOOD_ITER_MIRROR_PADDING

            Mirror padding. Outside bounds values will be as if the
            array items were mirrored. For example, for the array [1, 2, 3, 4],
            x[-2] will be 2, x[-2] will be 1, x[4] will be 4, x[5] will be 1,
            etc...

    .. c:macro:: NPY_NEIGHBORHOOD_ITER_CIRCULAR_PADDING

            Circular padding. Outside bounds values will be as if the array
            was repeated. For example, for the array [1, 2, 3, 4], x[-2] will
            be 3, x[-2] will be 4, x[4] will be 1, x[5] will be 2, etc...

    If the mode is constant filling (`NPY_NEIGHBORHOOD_ITER_CONSTANT_PADDING`),
    fill_value should point to an array object which holds the filling value
    (the first item will be the filling value if the array contains more than
    one item). For other cases, fill_value may be NULL.

    - The iterator holds a reference to iter
    - Return NULL on failure (in which case the reference count of iter is not
      changed)
    - iter itself can be a Neighborhood iterator: this can be useful for .e.g
      automatic boundaries handling
    - the object returned by this function should be safe to use as a normal
      iterator
    - If the position of iter is changed, any subsequent call to
      PyArrayNeighborhoodIter_Next is undefined behavior, and
      PyArrayNeighborhoodIter_Reset must be called.
    - If the position of iter is not the beginning of the data and the
      underlying data for iter is contiguous, the iterator will point to the
      start of the data instead of position pointed by iter.
      To avoid this situation, iter should be moved to the required position
      only after the creation of iterator, and PyArrayNeighborhoodIter_Reset
      must be called.

    .. code-block:: c

       PyArrayIterObject *iter;
       PyArrayNeighborhoodIterObject *neigh_iter;
       iter = PyArray_IterNew(x);

       /*For a 3x3 kernel */
       bounds = {-1, 1, -1, 1};
       neigh_iter = (PyArrayNeighborhoodIterObject*)PyArray_NeighborhoodIterNew(
            iter, bounds, NPY_NEIGHBORHOOD_ITER_ZERO_PADDING, NULL);

       for(i = 0; i < iter->size; ++i) {
            for (j = 0; j < neigh_iter->size; ++j) {
                    /* Walk around the item currently pointed by iter->dataptr */
                    PyArrayNeighborhoodIter_Next(neigh_iter);
            }

            /* Move to the next point of iter */
            PyArrayIter_Next(iter);
            PyArrayNeighborhoodIter_Reset(neigh_iter);
       }

.. c:function:: int PyArrayNeighborhoodIter_Reset( \
        PyArrayNeighborhoodIterObject* iter)

    Reset the iterator position to the first point of the neighborhood. This
    should be called whenever the iter argument given at
    PyArray_NeighborhoodIterObject is changed (see example)

.. c:function:: int PyArrayNeighborhoodIter_Next( \
        PyArrayNeighborhoodIterObject* iter)

    After this call, iter->dataptr points to the next point of the
    neighborhood. Calling this function after every point of the
    neighborhood has been visited is undefined.

Array mapping
-------------

Array mapping is the machinery behind advanced indexing.

.. c:function:: PyObject* PyArray_MapIterArray(PyArrayObject *a, \
                 PyObject *index)

    Use advanced indexing to iterate an array.

.. c:function:: void PyArray_MapIterSwapAxes(PyArrayMapIterObject *mit, \
                PyArrayObject **ret, int getmap)

    Swap the axes to or from their inserted form. ``MapIter`` always puts the
    advanced (array) indices first in the iteration. But if they are
    consecutive, it will insert/transpose them back before returning.
    This is stored as ``mit->consec != 0`` (the place where they are inserted).
    For assignments, the opposite happens: the values to be assigned are
    transposed (``getmap=1`` instead of ``getmap=0``). ``getmap=0`` and
    ``getmap=1`` undo the other operation.

.. c:function:: void PyArray_MapIterNext(PyArrayMapIterObject *mit)

    This function needs to update the state of the map iterator
    and point ``mit->dataptr`` to the memory-location of the next object.

    Note that this function never handles an extra operand but provides
    compatibility for an old (exposed) API.

.. c:function:: PyObject* PyArray_MapIterArrayCopyIfOverlap(PyArrayObject *a, \
                PyObject *index, int copy_if_overlap, PyArrayObject *extra_op)

    Similar to :c:func:`PyArray_MapIterArray` but with an additional
    ``copy_if_overlap`` argument. If ``copy_if_overlap != 0``, checks if ``a``
    has memory overlap with any of the arrays in ``index`` and with
    ``extra_op``, and make copies as appropriate to avoid problems if the
    input is modified during the iteration. ``iter->array`` may contain a
    copied array (WRITEBACKIFCOPY set).

Array Scalars
-------------

.. c:function:: PyObject* PyArray_Return(PyArrayObject* arr)

    This function steals a reference to *arr*.

    This function checks to see if *arr* is a 0-dimensional array and,
    if so, returns the appropriate array scalar. It should be used
    whenever 0-dimensional arrays could be returned to Python.

.. c:function:: PyObject* PyArray_Scalar( \
        void* data, PyArray_Descr* dtype, PyObject* base)

    Return an array scalar object of the given *dtype* by **copying**
    from memory pointed to by *data*.  *base* is expected to be the
    array object that is the owner of the data.  *base* is required
    if `dtype` is a ``void`` scalar, or if the ``NPY_USE_GETITEM``
    flag is set and it is known that the ``getitem`` method uses
    the ``arr`` argument without checking if it is ``NULL``.  Otherwise
    `base` may be ``NULL``.

    If the data is not in native byte order (as indicated by
    ``dtype->byteorder``) then this function will byteswap the data,
    because array scalars are always in correct machine-byte order.

.. c:function:: PyObject* PyArray_ToScalar(void* data, PyArrayObject* arr)

    Return an array scalar object of the type and itemsize indicated
    by the array object *arr* copied from the memory pointed to by
    *data* and swapping if the data in *arr* is not in machine
    byte-order.

.. c:function:: PyObject* PyArray_FromScalar( \
        PyObject* scalar, PyArray_Descr* outcode)

    Return a 0-dimensional array of type determined by *outcode* from
    *scalar* which should be an array-scalar object. If *outcode* is
    NULL, then the type is determined from *scalar*.

.. c:function:: void PyArray_ScalarAsCtype(PyObject* scalar, void* ctypeptr)

    Return in *ctypeptr* a pointer to the actual value in an array
    scalar. There is no error checking so *scalar* must be an
    array-scalar object, and ctypeptr must have enough space to hold
    the correct type. For flexible-sized types, a pointer to the data
    is copied into the memory of *ctypeptr*, for all other types, the
    actual data is copied into the address pointed to by *ctypeptr*.

.. c:function:: void PyArray_CastScalarToCtype( \
        PyObject* scalar, void* ctypeptr, PyArray_Descr* outcode)

    Return the data (cast to the data type indicated by *outcode*)
    from the array-scalar, *scalar*, into the memory pointed to by
    *ctypeptr* (which must be large enough to handle the incoming
    memory).

.. c:function:: PyObject* PyArray_TypeObjectFromType(int type)

    Returns a scalar type-object from a type-number, *type*
    . Equivalent to :c:func:`PyArray_DescrFromType` (*type*)->typeobj
    except for reference counting and error-checking. Returns a new
    reference to the typeobject on success or ``NULL`` on failure.

.. c:function:: NPY_SCALARKIND PyArray_ScalarKind( \
        int typenum, PyArrayObject** arr)

    See the function :c:func:`PyArray_MinScalarType` for an alternative
    mechanism introduced in NumPy 1.6.0.

    Return the kind of scalar represented by *typenum* and the array
    in *\*arr* (if *arr* is not ``NULL`` ). The array is assumed to be
    rank-0 and only used if *typenum* represents a signed integer. If
    *arr* is not ``NULL`` and the first element is negative then
    :c:data:`NPY_INTNEG_SCALAR` is returned, otherwise
    :c:data:`NPY_INTPOS_SCALAR` is returned. The possible return values
    are the enumerated values in :c:type:`NPY_SCALARKIND`.

.. c:function:: int PyArray_CanCoerceScalar( \
        char thistype, char neededtype, NPY_SCALARKIND scalar)

    See the function :c:func:`PyArray_ResultType` for details of
    NumPy type promotion, updated in NumPy 1.6.0.

    Implements the rules for scalar coercion. Scalars are only
    silently coerced from thistype to neededtype if this function
    returns nonzero.  If scalar is :c:data:`NPY_NOSCALAR`, then this
    function is equivalent to :c:func:`PyArray_CanCastSafely`. The rule is
    that scalars of the same KIND can be coerced into arrays of the
    same KIND. This rule means that high-precision scalars will never
    cause low-precision arrays of the same KIND to be upcast.


Data-type descriptors
---------------------



.. warning::

    Data-type objects must be reference counted so be aware of the
    action on the data-type reference of different C-API calls. The
    standard rule is that when a data-type object is returned it is a
    new reference.  Functions that take :c:expr:`PyArray_Descr *` objects and
    return arrays steal references to the data-type their inputs
    unless otherwise noted. Therefore, you must own a reference to any
    data-type object used as input to such a function.

.. c:function:: int PyArray_DescrCheck(PyObject* obj)

    Evaluates as true if *obj* is a data-type object ( :c:expr:`PyArray_Descr *` ).

.. c:function:: PyArray_Descr* PyArray_DescrNew(PyArray_Descr* obj)

    Return a new data-type object copied from *obj* (the fields
    reference is just updated so that the new object points to the
    same fields dictionary if any).

.. c:function:: PyArray_Descr* PyArray_DescrNewFromType(int typenum)

    Create a new data-type object from the built-in (or
    user-registered) data-type indicated by *typenum*. All builtin
    types should not have any of their fields changed. This creates a
    new copy of the :c:type:`PyArray_Descr` structure so that you can fill
    it in as appropriate. This function is especially needed for
    flexible data-types which need to have a new elsize member in
    order to be meaningful in array construction.

.. c:function:: PyArray_Descr* PyArray_DescrNewByteorder( \
        PyArray_Descr* obj, char newendian)

    Create a new data-type object with the byteorder set according to
    *newendian*. All referenced data-type objects (in subdescr and
    fields members of the data-type object) are also changed
    (recursively).

    The value of *newendian* is one of these macros:
..
    dedent the enumeration of flags to avoid missing references sphinx warnings 

.. c:macro:: NPY_IGNORE
             NPY_SWAP
             NPY_NATIVE
             NPY_LITTLE
             NPY_BIG

    If a byteorder of :c:data:`NPY_IGNORE` is encountered it
    is left alone. If newendian is :c:data:`NPY_SWAP`, then all byte-orders
    are swapped. Other valid newendian values are :c:data:`NPY_NATIVE`,
    :c:data:`NPY_LITTLE`, and :c:data:`NPY_BIG` which all cause
    the returned data-typed descriptor (and all it's
    referenced data-type descriptors) to have the corresponding byte-
    order.

.. c:function:: PyArray_Descr* PyArray_DescrFromObject( \
        PyObject* op, PyArray_Descr* mintype)

    Determine an appropriate data-type object from the object *op*
    (which should be a "nested" sequence object) and the minimum
    data-type descriptor mintype (which can be ``NULL`` ). Similar in
    behavior to array(*op*).dtype. Don't confuse this function with
    :c:func:`PyArray_DescrConverter`. This function essentially looks at
    all the objects in the (nested) sequence and determines the
    data-type from the elements it finds.

.. c:function:: PyArray_Descr* PyArray_DescrFromScalar(PyObject* scalar)

    Return a data-type object from an array-scalar object. No checking
    is done to be sure that *scalar* is an array scalar. If no
    suitable data-type can be determined, then a data-type of
    :c:data:`NPY_OBJECT` is returned by default.

.. c:function:: PyArray_Descr* PyArray_DescrFromType(int typenum)

    Returns a data-type object corresponding to *typenum*. The
    *typenum* can be one of the enumerated types, a character code for
    one of the enumerated types, or a user-defined type. If you want to use a
    flexible size array, then you need to ``flexible typenum`` and set the
    results ``elsize`` parameter to the desired size. The typenum is one of the
    :c:data:`NPY_TYPES`.

.. c:function:: int PyArray_DescrConverter(PyObject* obj, PyArray_Descr** dtype)

    Convert any compatible Python object, *obj*, to a data-type object
    in *dtype*. A large number of Python objects can be converted to
    data-type objects. See :ref:`arrays.dtypes` for a complete
    description. This version of the converter converts None objects
    to a :c:data:`NPY_DEFAULT_TYPE` data-type object. This function can
    be used with the "O&" character code in :c:func:`PyArg_ParseTuple`
    processing.

.. c:function:: int PyArray_DescrConverter2( \
        PyObject* obj, PyArray_Descr** dtype)

    Convert any compatible Python object, *obj*, to a data-type
    object in *dtype*. This version of the converter converts None
    objects so that the returned data-type is ``NULL``. This function
    can also be used with the "O&" character in PyArg_ParseTuple
    processing.

.. c:function:: int Pyarray_DescrAlignConverter( \
        PyObject* obj, PyArray_Descr** dtype)

    Like :c:func:`PyArray_DescrConverter` except it aligns C-struct-like
    objects on word-boundaries as the compiler would.

.. c:function:: int Pyarray_DescrAlignConverter2( \
        PyObject* obj, PyArray_Descr** dtype)

    Like :c:func:`PyArray_DescrConverter2` except it aligns C-struct-like
    objects on word-boundaries as the compiler would.

.. c:function:: PyObject *PyArray_FieldNames(PyObject* dict)

    Take the fields dictionary, *dict*, such as the one attached to a
    data-type object and construct an ordered-list of field names such
    as is stored in the names field of the :c:type:`PyArray_Descr` object.


Conversion Utilities
--------------------


For use with :c:func:`PyArg_ParseTuple`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

All of these functions can be used in :c:func:`PyArg_ParseTuple` (...) with
the "O&" format specifier to automatically convert any Python object
to the required C-object. All of these functions return
:c:data:`NPY_SUCCEED` if successful and :c:data:`NPY_FAIL` if not. The first
argument to all of these function is a Python object. The second
argument is the **address** of the C-type to convert the Python object
to.


.. warning::

    Be sure to understand what steps you should take to manage the
    memory when using these conversion functions. These functions can
    require freeing memory, and/or altering the reference counts of
    specific objects based on your use.

.. c:function:: int PyArray_Converter(PyObject* obj, PyObject** address)

    Convert any Python object to a :c:type:`PyArrayObject`. If
    :c:func:`PyArray_Check` (*obj*) is TRUE then its reference count is
    incremented and a reference placed in *address*. If *obj* is not
    an array, then convert it to an array using :c:func:`PyArray_FromAny`
    . No matter what is returned, you must DECREF the object returned
    by this routine in *address* when you are done with it.

.. c:function:: int PyArray_OutputConverter( \
        PyObject* obj, PyArrayObject** address)

    This is a default converter for output arrays given to
    functions. If *obj* is :c:data:`Py_None` or ``NULL``, then *\*address*
    will be ``NULL`` but the call will succeed. If :c:func:`PyArray_Check` (
    *obj*) is TRUE then it is returned in *\*address* without
    incrementing its reference count.

.. c:function:: int PyArray_IntpConverter(PyObject* obj, PyArray_Dims* seq)

    Convert any Python sequence, *obj*, smaller than :c:data:`NPY_MAXDIMS`
    to a C-array of :c:type:`npy_intp`. The Python object could also be a
    single number. The *seq* variable is a pointer to a structure with
    members ptr and len. On successful return, *seq* ->ptr contains a
    pointer to memory that must be freed, by calling :c:func:`PyDimMem_FREE`,
    to avoid a memory leak. The restriction on memory size allows this
    converter to be conveniently used for sequences intended to be
    interpreted as array shapes.

.. c:function:: int PyArray_BufferConverter(PyObject* obj, PyArray_Chunk* buf)

    Convert any Python object, *obj*, with a (single-segment) buffer
    interface to a variable with members that detail the object's use
    of its chunk of memory. The *buf* variable is a pointer to a
    structure with base, ptr, len, and flags members. The
    :c:type:`PyArray_Chunk` structure is binary compatible with the
    Python's buffer object (through its len member on 32-bit platforms
    and its ptr member on 64-bit platforms or in Python 2.5). On
    return, the base member is set to *obj* (or its base if *obj* is
    already a buffer object pointing to another object). If you need
    to hold on to the memory be sure to INCREF the base member. The
    chunk of memory is pointed to by *buf* ->ptr member and has length
    *buf* ->len. The flags member of *buf* is :c:data:`NPY_ARRAY_ALIGNED`
    with the :c:data:`NPY_ARRAY_WRITEABLE` flag set if *obj* has
    a writeable buffer interface.

.. c:function:: int PyArray_AxisConverter(PyObject* obj, int* axis)

    Convert a Python object, *obj*, representing an axis argument to
    the proper value for passing to the functions that take an integer
    axis. Specifically, if *obj* is None, *axis* is set to
    :c:data:`NPY_MAXDIMS` which is interpreted correctly by the C-API
    functions that take axis arguments.

.. c:function:: int PyArray_BoolConverter(PyObject* obj, npy_bool* value)

    Convert any Python object, *obj*, to :c:data:`NPY_TRUE` or
    :c:data:`NPY_FALSE`, and place the result in *value*.

.. c:function:: int PyArray_ByteorderConverter(PyObject* obj, char* endian)

    Convert Python strings into the corresponding byte-order
    character:
    '>', '<', 's', '=', or '\|'.

.. c:function:: int PyArray_SortkindConverter(PyObject* obj, NPY_SORTKIND* sort)

    Convert Python strings into one of :c:data:`NPY_QUICKSORT` (starts
    with 'q' or 'Q'), :c:data:`NPY_HEAPSORT` (starts with 'h' or 'H'),
    :c:data:`NPY_MERGESORT` (starts with 'm' or 'M') or :c:data:`NPY_STABLESORT`
    (starts with 't' or 'T'). :c:data:`NPY_MERGESORT` and :c:data:`NPY_STABLESORT`
    are aliased to each other for backwards compatibility and may refer to one
    of several stable sorting algorithms depending on the data type.

.. c:function:: int PyArray_SearchsideConverter( \
        PyObject* obj, NPY_SEARCHSIDE* side)

    Convert Python strings into one of :c:data:`NPY_SEARCHLEFT` (starts with 'l'
    or 'L'), or :c:data:`NPY_SEARCHRIGHT` (starts with 'r' or 'R').

.. c:function:: int PyArray_OrderConverter(PyObject* obj, NPY_ORDER* order)

   Convert the Python strings 'C', 'F', 'A', and 'K' into the :c:type:`NPY_ORDER`
   enumeration :c:data:`NPY_CORDER`, :c:data:`NPY_FORTRANORDER`,
   :c:data:`NPY_ANYORDER`, and :c:data:`NPY_KEEPORDER`.

.. c:function:: int PyArray_CastingConverter( \
        PyObject* obj, NPY_CASTING* casting)

   Convert the Python strings 'no', 'equiv', 'safe', 'same_kind', and
   'unsafe' into the :c:type:`NPY_CASTING` enumeration :c:data:`NPY_NO_CASTING`,
   :c:data:`NPY_EQUIV_CASTING`, :c:data:`NPY_SAFE_CASTING`,
   :c:data:`NPY_SAME_KIND_CASTING`, and :c:data:`NPY_UNSAFE_CASTING`.

.. c:function:: int PyArray_ClipmodeConverter( \
        PyObject* object, NPY_CLIPMODE* val)

    Convert the Python strings 'clip', 'wrap', and 'raise' into the
    :c:type:`NPY_CLIPMODE` enumeration :c:data:`NPY_CLIP`, :c:data:`NPY_WRAP`,
    and :c:data:`NPY_RAISE`.

.. c:function:: int PyArray_ConvertClipmodeSequence( \
        PyObject* object, NPY_CLIPMODE* modes, int n)

   Converts either a sequence of clipmodes or a single clipmode into
   a C array of :c:type:`NPY_CLIPMODE` values. The number of clipmodes *n*
   must be known before calling this function. This function is provided
   to help functions allow a different clipmode for each dimension.

Other conversions
^^^^^^^^^^^^^^^^^

.. c:function:: int PyArray_PyIntAsInt(PyObject* op)

    Convert all kinds of Python objects (including arrays and array
    scalars) to a standard integer. On error, -1 is returned and an
    exception set. You may find useful the macro:

    .. code-block:: c

        #define error_converting(x) (((x) == -1) && PyErr_Occurred())

.. c:function:: npy_intp PyArray_PyIntAsIntp(PyObject* op)

    Convert all kinds of Python objects (including arrays and array
    scalars) to a (platform-pointer-sized) integer. On error, -1 is
    returned and an exception set.

.. c:function:: int PyArray_IntpFromSequence( \
        PyObject* seq, npy_intp* vals, int maxvals)

    Convert any Python sequence (or single Python number) passed in as
    *seq* to (up to) *maxvals* pointer-sized integers and place them
    in the *vals* array. The sequence can be smaller then *maxvals* as
    the number of converted objects is returned.

.. c:function:: int PyArray_TypestrConvert(int itemsize, int gentype)

    Convert typestring characters (with *itemsize*) to basic
    enumerated data types. The typestring character corresponding to
    signed and unsigned integers, floating point numbers, and
    complex-floating point numbers are recognized and converted. Other
    values of gentype are returned. This function can be used to
    convert, for example, the string 'f4' to :c:data:`NPY_FLOAT32`.


Miscellaneous
-------------


Importing the API
^^^^^^^^^^^^^^^^^

In order to make use of the C-API from another extension module, the
:c:func:`import_array` function must be called. If the extension module is
self-contained in a single .c file, then that is all that needs to be
done. If, however, the extension module involves multiple files where
the C-API is needed then some additional steps must be taken.

.. c:function:: void import_array(void)

    This function must be called in the initialization section of a
    module that will make use of the C-API. It imports the module
    where the function-pointer table is stored and points the correct
    variable to it.

.. c:macro:: PY_ARRAY_UNIQUE_SYMBOL

.. c:macro:: NO_IMPORT_ARRAY

    Using these #defines you can use the C-API in multiple files for a
    single extension module. In each file you must define
    :c:macro:`PY_ARRAY_UNIQUE_SYMBOL` to some name that will hold the
    C-API (*e.g.* myextension_ARRAY_API). This must be done **before**
    including the numpy/arrayobject.h file. In the module
    initialization routine you call :c:func:`import_array`. In addition,
    in the files that do not have the module initialization
    sub_routine define :c:macro:`NO_IMPORT_ARRAY` prior to including
    numpy/arrayobject.h.

    Suppose I have two files coolmodule.c and coolhelper.c which need
    to be compiled and linked into a single extension module. Suppose
    coolmodule.c contains the required initcool module initialization
    function (with the import_array() function called). Then,
    coolmodule.c would have at the top:

    .. code-block:: c

        #define PY_ARRAY_UNIQUE_SYMBOL cool_ARRAY_API
        #include numpy/arrayobject.h

    On the other hand, coolhelper.c would contain at the top:

    .. code-block:: c

        #define NO_IMPORT_ARRAY
        #define PY_ARRAY_UNIQUE_SYMBOL cool_ARRAY_API
        #include numpy/arrayobject.h

    You can also put the common two last lines into an extension-local
    header file as long as you make sure that NO_IMPORT_ARRAY is
    #defined before #including that file.

    Internally, these #defines work as follows:

        * If neither is defined, the C-API is declared to be
          ``static void**``, so it is only visible within the
          compilation unit that #includes numpy/arrayobject.h.
        * If :c:macro:`PY_ARRAY_UNIQUE_SYMBOL` is #defined, but
          :c:macro:`NO_IMPORT_ARRAY` is not, the C-API is declared to
          be ``void**``, so that it will also be visible to other
          compilation units.
        * If :c:macro:`NO_IMPORT_ARRAY` is #defined, regardless of
          whether :c:macro:`PY_ARRAY_UNIQUE_SYMBOL` is, the C-API is
          declared to be ``extern void**``, so it is expected to
          be defined in another compilation unit.
        * Whenever :c:macro:`PY_ARRAY_UNIQUE_SYMBOL` is #defined, it
          also changes the name of the variable holding the C-API, which
          defaults to ``PyArray_API``, to whatever the macro is
          #defined to.

Checking the API Version
^^^^^^^^^^^^^^^^^^^^^^^^

Because python extensions are not used in the same way as usual libraries on
most platforms, some errors cannot be automatically detected at build time or
even runtime. For example, if you build an extension using a function available
only for numpy >= 1.3.0, and you import the extension later with numpy 1.2, you
will not get an import error (but almost certainly a segmentation fault when
calling the function). That's why several functions are provided to check for
numpy versions. The macros :c:data:`NPY_VERSION`  and
:c:data:`NPY_FEATURE_VERSION` corresponds to the numpy version used to build the
extension, whereas the versions returned by the functions
:c:func:`PyArray_GetNDArrayCVersion` and :c:func:`PyArray_GetNDArrayCFeatureVersion`
corresponds to the runtime numpy's version.

The rules for ABI and API compatibilities can be summarized as follows:

    * Whenever :c:data:`NPY_VERSION` != ``PyArray_GetNDArrayCVersion()``, the
      extension has to be recompiled (ABI incompatibility).
    * :c:data:`NPY_VERSION` == ``PyArray_GetNDArrayCVersion()`` and
      :c:data:`NPY_FEATURE_VERSION` <= ``PyArray_GetNDArrayCFeatureVersion()`` means
      backward compatible changes.

ABI incompatibility is automatically detected in every numpy's version. API
incompatibility detection was added in numpy 1.4.0. If you want to supported
many different numpy versions with one extension binary, you have to build your
extension with the lowest :c:data:`NPY_FEATURE_VERSION` as possible.

.. c:macro:: NPY_VERSION

    The current version of the ndarray object (check to see if this
    variable is defined to guarantee the ``numpy/arrayobject.h`` header is
    being used).

.. c:macro:: NPY_FEATURE_VERSION

    The current version of the C-API.

.. c:function:: unsigned int PyArray_GetNDArrayCVersion(void)

    This just returns the value :c:data:`NPY_VERSION`. :c:data:`NPY_VERSION`
    changes whenever a backward incompatible change at the ABI level. Because
    it is in the C-API, however, comparing the output of this function from the
    value defined in the current header gives a way to test if the C-API has
    changed thus requiring a re-compilation of extension modules that use the
    C-API. This is automatically checked in the function :c:func:`import_array`.

.. c:function:: unsigned int PyArray_GetNDArrayCFeatureVersion(void)

    .. versionadded:: 1.4.0

    This just returns the value :c:data:`NPY_FEATURE_VERSION`.
    :c:data:`NPY_FEATURE_VERSION` changes whenever the API changes (e.g. a
    function is added). A changed value does not always require a recompile.

Internal Flexibility
^^^^^^^^^^^^^^^^^^^^

.. c:function:: int PyArray_SetNumericOps(PyObject* dict)

    NumPy stores an internal table of Python callable objects that are
    used to implement arithmetic operations for arrays as well as
    certain array calculation methods. This function allows the user
    to replace any or all of these Python objects with their own
    versions. The keys of the dictionary, *dict*, are the named
    functions to replace and the paired value is the Python callable
    object to use. Care should be taken that the function used to
    replace an internal array operation does not itself call back to
    that internal array operation (unless you have designed the
    function to handle that), or an unchecked infinite recursion can
    result (possibly causing program crash). The key names that
    represent operations that can be replaced are:

        **add**, **subtract**, **multiply**, **divide**,
        **remainder**, **power**, **square**, **reciprocal**,
        **ones_like**, **sqrt**, **negative**, **positive**,
        **absolute**, **invert**, **left_shift**, **right_shift**,
        **bitwise_and**, **bitwise_xor**, **bitwise_or**,
        **less**, **less_equal**, **equal**, **not_equal**,
        **greater**, **greater_equal**, **floor_divide**,
        **true_divide**, **logical_or**, **logical_and**,
        **floor**, **ceil**, **maximum**, **minimum**, **rint**.


    These functions are included here because they are used at least once
    in the array object's methods. The function returns -1 (without
    setting a Python Error) if one of the objects being assigned is not
    callable.

    .. deprecated:: 1.16

.. c:function:: PyObject* PyArray_GetNumericOps(void)

    Return a Python dictionary containing the callable Python objects
    stored in the internal arithmetic operation table. The keys of
    this dictionary are given in the explanation for :c:func:`PyArray_SetNumericOps`.

    .. deprecated:: 1.16

.. c:function:: void PyArray_SetStringFunction(PyObject* op, int repr)

    This function allows you to alter the tp_str and tp_repr methods
    of the array object to any Python function. Thus you can alter
    what happens for all arrays when str(arr) or repr(arr) is called
    from Python. The function to be called is passed in as *op*. If
    *repr* is non-zero, then this function will be called in response
    to repr(arr), otherwise the function will be called in response to
    str(arr). No check on whether or not *op* is callable is
    performed. The callable passed in to *op* should expect an array
    argument and should return a string to be printed.


Memory management
^^^^^^^^^^^^^^^^^

.. c:function:: char* PyDataMem_NEW(size_t nbytes)

.. c:function:: void PyDataMem_FREE(char* ptr)

.. c:function:: char* PyDataMem_RENEW(void * ptr, size_t newbytes)

    Macros to allocate, free, and reallocate memory. These macros are used
    internally to create arrays.

.. c:function:: npy_intp*  PyDimMem_NEW(int nd)

.. c:function:: void PyDimMem_FREE(char* ptr)

.. c:function:: npy_intp* PyDimMem_RENEW(void* ptr, size_t newnd)

    Macros to allocate, free, and reallocate dimension and strides memory.

.. c:function:: void* PyArray_malloc(size_t nbytes)

.. c:function:: void PyArray_free(void* ptr)

.. c:function:: void* PyArray_realloc(npy_intp* ptr, size_t nbytes)

    These macros use different memory allocators, depending on the
    constant :c:data:`NPY_USE_PYMEM`. The system malloc is used when
    :c:data:`NPY_USE_PYMEM` is 0, if :c:data:`NPY_USE_PYMEM` is 1, then
    the Python memory allocator is used.

    .. c:macro:: NPY_USE_PYMEM

.. c:function:: int PyArray_ResolveWritebackIfCopy(PyArrayObject* obj)

    If ``obj.flags`` has :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`, this function
    clears the flags, `DECREF` s
    `obj->base` and makes it writeable, and sets ``obj->base`` to NULL. It then
    copies ``obj->data`` to `obj->base->data`, and returns the error state of
    the copy operation. This is the opposite of
    :c:func:`PyArray_SetWritebackIfCopyBase`. Usually this is called once
    you are finished with ``obj``, just before ``Py_DECREF(obj)``. It may be called
    multiple times, or with ``NULL`` input. See also
    :c:func:`PyArray_DiscardWritebackIfCopy`.

    Returns 0 if nothing was done, -1 on error, and 1 if action was taken.

Threading support
^^^^^^^^^^^^^^^^^

These macros are only meaningful if :c:data:`NPY_ALLOW_THREADS`
evaluates True during compilation of the extension module. Otherwise,
these macros are equivalent to whitespace. Python uses a single Global
Interpreter Lock (GIL) for each Python process so that only a single
thread may execute at a time (even on multi-cpu machines). When
calling out to a compiled function that may take time to compute (and
does not have side-effects for other threads like updated global
variables), the GIL should be released so that other Python threads
can run while the time-consuming calculations are performed. This can
be accomplished using two groups of macros. Typically, if one macro in
a group is used in a code block, all of them must be used in the same
code block. Currently, :c:data:`NPY_ALLOW_THREADS` is defined to the
python-defined :c:data:`WITH_THREADS` constant unless the environment
variable ``NPY_NOSMP`` is set in which case
:c:data:`NPY_ALLOW_THREADS` is defined to be 0.

.. c:macro:: NPY_ALLOW_THREADS 

.. c:macro:: WITH_THREADS

Group 1
"""""""

    This group is used to call code that may take some time but does not
    use any Python C-API calls. Thus, the GIL should be released during
    its calculation.

    .. c:macro:: NPY_BEGIN_ALLOW_THREADS

        Equivalent to :c:macro:`Py_BEGIN_ALLOW_THREADS` except it uses
        :c:data:`NPY_ALLOW_THREADS` to determine if the macro if
        replaced with white-space or not.

    .. c:macro:: NPY_END_ALLOW_THREADS

        Equivalent to :c:macro:`Py_END_ALLOW_THREADS` except it uses
        :c:data:`NPY_ALLOW_THREADS` to determine if the macro if
        replaced with white-space or not.

    .. c:macro:: NPY_BEGIN_THREADS_DEF

        Place in the variable declaration area. This macro sets up the
        variable needed for storing the Python state.

    .. c:macro:: NPY_BEGIN_THREADS

        Place right before code that does not need the Python
        interpreter (no Python C-API calls). This macro saves the
        Python state and releases the GIL.

    .. c:macro:: NPY_END_THREADS

        Place right after code that does not need the Python
        interpreter. This macro acquires the GIL and restores the
        Python state from the saved variable.

    .. c:function:: void NPY_BEGIN_THREADS_DESCR(PyArray_Descr *dtype)

        Useful to release the GIL only if *dtype* does not contain
        arbitrary Python objects which may need the Python interpreter
        during execution of the loop.

    .. c:function:: void NPY_END_THREADS_DESCR(PyArray_Descr *dtype)

        Useful to regain the GIL in situations where it was released
        using the BEGIN form of this macro.

    .. c:function:: void NPY_BEGIN_THREADS_THRESHOLDED(int loop_size)

        Useful to release the GIL only if *loop_size* exceeds a
        minimum threshold, currently set to 500. Should be matched
        with a :c:macro:`NPY_END_THREADS` to regain the GIL.

Group 2
"""""""

    This group is used to re-acquire the Python GIL after it has been
    released. For example, suppose the GIL has been released (using the
    previous calls), and then some path in the code (perhaps in a
    different subroutine) requires use of the Python C-API, then these
    macros are useful to acquire the GIL. These macros accomplish
    essentially a reverse of the previous three (acquire the LOCK saving
    what state it had) and then re-release it with the saved state.

    .. c:macro:: NPY_ALLOW_C_API_DEF

        Place in the variable declaration area to set up the necessary
        variable.

    .. c:macro:: NPY_ALLOW_C_API

        Place before code that needs to call the Python C-API (when it is
        known that the GIL has already been released).

    .. c:macro:: NPY_DISABLE_C_API

        Place after code that needs to call the Python C-API (to re-release
        the GIL).

.. tip::

    Never use semicolons after the threading support macros.


Priority
^^^^^^^^

.. c:macro:: NPY_PRIORITY

    Default priority for arrays.

.. c:macro:: NPY_SUBTYPE_PRIORITY

    Default subtype priority.

.. c:macro:: NPY_SCALAR_PRIORITY

    Default scalar priority (very small)

.. c:function:: double PyArray_GetPriority(PyObject* obj, double def)

    Return the :obj:`~numpy.class.__array_priority__` attribute (converted to a
    double) of *obj* or *def* if no attribute of that name
    exists. Fast returns that avoid the attribute lookup are provided
    for objects of type :c:data:`PyArray_Type`.


Default buffers
^^^^^^^^^^^^^^^

.. c:macro:: NPY_BUFSIZE

    Default size of the user-settable internal buffers.

.. c:macro:: NPY_MIN_BUFSIZE

    Smallest size of user-settable internal buffers.

.. c:macro:: NPY_MAX_BUFSIZE

    Largest size allowed for the user-settable buffers.


Other constants
^^^^^^^^^^^^^^^

.. c:macro:: NPY_NUM_FLOATTYPE

    The number of floating-point types

.. c:macro:: NPY_MAXDIMS

    The maximum number of dimensions allowed in arrays.

.. c:macro:: NPY_MAXARGS

    The maximum number of array arguments that can be used in functions.

.. c:macro:: NPY_FALSE

    Defined as 0 for use with Bool.

.. c:macro:: NPY_TRUE

    Defined as 1 for use with Bool.

.. c:macro:: NPY_FAIL

    The return value of failed converter functions which are called using
    the "O&" syntax in :c:func:`PyArg_ParseTuple`-like functions.

.. c:macro:: NPY_SUCCEED

    The return value of successful converter functions which are called
    using the "O&" syntax in :c:func:`PyArg_ParseTuple`-like functions.


Miscellaneous Macros
^^^^^^^^^^^^^^^^^^^^

.. c:function:: int PyArray_SAMESHAPE(PyArrayObject *a1, PyArrayObject *a2)

    Evaluates as True if arrays *a1* and *a2* have the same shape.

.. c:macro:: PyArray_MAX(a,b)

    Returns the maximum of *a* and *b*. If (*a*) or (*b*) are
    expressions they are evaluated twice.

.. c:macro:: PyArray_MIN(a,b)

    Returns the minimum of *a* and *b*. If (*a*) or (*b*) are
    expressions they are evaluated twice.

.. c:macro:: PyArray_CLT(a,b)

.. c:macro:: PyArray_CGT(a,b)

.. c:macro:: PyArray_CLE(a,b)

.. c:macro:: PyArray_CGE(a,b)

.. c:macro:: PyArray_CEQ(a,b)

.. c:macro:: PyArray_CNE(a,b)

    Implements the complex comparisons between two complex numbers
    (structures with a real and imag member) using NumPy's definition
    of the ordering which is lexicographic: comparing the real parts
    first and then the complex parts if the real parts are equal.

.. c:function:: npy_intp PyArray_REFCOUNT(PyObject* op)

    Returns the reference count of any Python object.

.. c:function:: void PyArray_DiscardWritebackIfCopy(PyObject* obj)

    If ``obj.flags`` has :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`, this function
    clears the flags, `DECREF` s
    `obj->base` and makes it writeable, and sets ``obj->base`` to NULL. In
    contrast to :c:func:`PyArray_DiscardWritebackIfCopy` it makes no attempt
    to copy the data from `obj->base` This undoes
    :c:func:`PyArray_SetWritebackIfCopyBase`. Usually this is called after an
    error when you are finished with ``obj``, just before ``Py_DECREF(obj)``.
    It may be called multiple times, or with ``NULL`` input.

.. c:function:: void PyArray_XDECREF_ERR(PyObject* obj)

    Deprecated in 1.14, use :c:func:`PyArray_DiscardWritebackIfCopy`
    followed by ``Py_XDECREF``

    DECREF's an array object which may have the
    :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`
    flag set without causing the contents to be copied back into the
    original array. Resets the :c:data:`NPY_ARRAY_WRITEABLE` flag on the base
    object. This is useful for recovering from an error condition when
    writeback semantics are used, but will lead to wrong results.


Enumerated Types
^^^^^^^^^^^^^^^^

.. c:enum:: NPY_SORTKIND

    A special variable-type which can take on different values to indicate
    the sorting algorithm being used.

    .. c:enumerator:: NPY_QUICKSORT

    .. c:enumerator:: NPY_HEAPSORT

    .. c:enumerator:: NPY_MERGESORT

    .. c:enumerator:: NPY_STABLESORT

        Used as an alias of :c:data:`NPY_MERGESORT` and vica versa.

    .. c:enumerator:: NPY_NSORTS

       Defined to be the number of sorts. It is fixed at three by the need for
       backwards compatibility, and consequently :c:data:`NPY_MERGESORT` and
       :c:data:`NPY_STABLESORT` are aliased to each other and may refer to one
       of several stable sorting algorithms depending on the data type.


.. c:enum:: NPY_SCALARKIND

    A special variable type indicating the number of "kinds" of
    scalars distinguished in determining scalar-coercion rules. This
    variable can take on the values:

    .. c:enumerator:: NPY_NOSCALAR

    .. c:enumerator:: NPY_BOOL_SCALAR

    .. c:enumerator:: NPY_INTPOS_SCALAR

    .. c:enumerator:: NPY_INTNEG_SCALAR

    .. c:enumerator:: NPY_FLOAT_SCALAR

    .. c:enumerator:: NPY_COMPLEX_SCALAR

    .. c:enumerator:: NPY_OBJECT_SCALAR

    .. c:enumerator:: NPY_NSCALARKINDS

       Defined to be the number of scalar kinds
       (not including :c:data:`NPY_NOSCALAR`).

.. c:enum:: NPY_ORDER

    An enumeration type indicating the element order that an array should be
    interpreted in. When a brand new array is created, generally
    only **NPY_CORDER** and **NPY_FORTRANORDER** are used, whereas
    when one or more inputs are provided, the order can be based on them.

    .. c:enumerator:: NPY_ANYORDER

        Fortran order if all the inputs are Fortran, C otherwise.

    .. c:enumerator:: NPY_CORDER

        C order.

    .. c:enumerator:: NPY_FORTRANORDER

        Fortran order.

    .. c:enumerator:: NPY_KEEPORDER

        An order as close to the order of the inputs as possible, even
        if the input is in neither C nor Fortran order.

.. c:enum:: NPY_CLIPMODE

    A variable type indicating the kind of clipping that should be
    applied in certain functions.

    .. c:enumerator:: NPY_RAISE

        The default for most operations, raises an exception if an index
        is out of bounds.

    .. c:enumerator:: NPY_CLIP

        Clips an index to the valid range if it is out of bounds.

    .. c:enumerator:: NPY_WRAP

        Wraps an index to the valid range if it is out of bounds.

.. c:enum:: NPY_SEARCHSIDE

    A variable type indicating whether the index returned should be that of
    the first suitable location (if :c:data:`NPY_SEARCHLEFT`) or of the last
    (if :c:data:`NPY_SEARCHRIGHT`).

    .. c:enumerator:: NPY_SEARCHLEFT

    .. c:enumerator:: NPY_SEARCHRIGHT

.. c:enum:: NPY_SELECTKIND

    A variable type indicating the selection algorithm being used.

    .. c:enumerator:: NPY_INTROSELECT

.. c:enum:: NPY_CASTING

    .. versionadded:: 1.6

    An enumeration type indicating how permissive data conversions should
    be. This is used by the iterator added in NumPy 1.6, and is intended
    to be used more broadly in a future version.

    .. c:enumerator:: NPY_NO_CASTING

        Only allow identical types.

    .. c:enumerator:: NPY_EQUIV_CASTING

       Allow identical and casts involving byte swapping.

    .. c:enumerator:: NPY_SAFE_CASTING

       Only allow casts which will not cause values to be rounded,
       truncated, or otherwise changed.

    .. c:enumerator:: NPY_SAME_KIND_CASTING

       Allow any safe casts, and casts between types of the same kind.
       For example, float64 -> float32 is permitted with this rule.

    .. c:enumerator:: NPY_UNSAFE_CASTING

       Allow any cast, no matter what kind of data loss may occur.

.. index::
   pair: ndarray; C-API
Data Type API
=============

.. sectionauthor:: Travis E. Oliphant

The standard array can have 24 different data types (and has some
support for adding your own types). These data types all have an
enumerated type, an enumerated type-character, and a corresponding
array scalar Python type object (placed in a hierarchy). There are
also standard C typedefs to make it easier to manipulate elements of
the given data type. For the numeric types, there are also bit-width
equivalent C typedefs and named typenumbers that make it easier to
select the precision desired.

.. warning::

    The names for the types in c code follows c naming conventions
    more closely. The Python names for these types follow Python
    conventions.  Thus, :c:data:`NPY_FLOAT` picks up a 32-bit float in
    C, but :class:`numpy.float_` in Python corresponds to a 64-bit
    double. The bit-width names can be used in both Python and C for
    clarity.


Enumerated Types
----------------

.. c:enumerator:: NPY_TYPES

There is a list of enumerated types defined providing the basic 24
data types plus some useful generic names. Whenever the code requires
a type number, one of these enumerated types is requested. The types
are all called ``NPY_{NAME}``:

.. c:enumerator:: NPY_BOOL

    The enumeration value for the boolean type, stored as one byte.
    It may only be set to the values 0 and 1.

.. c:enumerator:: NPY_BYTE
.. c:enumerator:: NPY_INT8

    The enumeration value for an 8-bit/1-byte signed integer.

.. c:enumerator:: NPY_SHORT
.. c:enumerator:: NPY_INT16

    The enumeration value for a 16-bit/2-byte signed integer.

.. c:enumerator:: NPY_INT
.. c:enumerator:: NPY_INT32

    The enumeration value for a 32-bit/4-byte signed integer.

.. c:enumerator:: NPY_LONG

    Equivalent to either NPY_INT or NPY_LONGLONG, depending on the
    platform.

.. c:enumerator:: NPY_LONGLONG
.. c:enumerator:: NPY_INT64

    The enumeration value for a 64-bit/8-byte signed integer.

.. c:enumerator:: NPY_UBYTE
.. c:enumerator:: NPY_UINT8

    The enumeration value for an 8-bit/1-byte unsigned integer.

.. c:enumerator:: NPY_USHORT
.. c:enumerator:: NPY_UINT16

    The enumeration value for a 16-bit/2-byte unsigned integer.

.. c:enumerator:: NPY_UINT
.. c:enumerator:: NPY_UINT32

    The enumeration value for a 32-bit/4-byte unsigned integer.

.. c:enumerator:: NPY_ULONG

    Equivalent to either NPY_UINT or NPY_ULONGLONG, depending on the
    platform.

.. c:enumerator:: NPY_ULONGLONG
.. c:enumerator:: NPY_UINT64

    The enumeration value for a 64-bit/8-byte unsigned integer.

.. c:enumerator:: NPY_HALF
.. c:enumerator:: NPY_FLOAT16

    The enumeration value for a 16-bit/2-byte IEEE 754-2008 compatible floating
    point type.

.. c:enumerator:: NPY_FLOAT
.. c:enumerator:: NPY_FLOAT32

    The enumeration value for a 32-bit/4-byte IEEE 754 compatible floating
    point type.

.. c:enumerator:: NPY_DOUBLE
.. c:enumerator:: NPY_FLOAT64

    The enumeration value for a 64-bit/8-byte IEEE 754 compatible floating
    point type.

.. c:enumerator:: NPY_LONGDOUBLE

    The enumeration value for a platform-specific floating point type which is
    at least as large as NPY_DOUBLE, but larger on many platforms.

.. c:enumerator:: NPY_CFLOAT
.. c:enumerator:: NPY_COMPLEX64

    The enumeration value for a 64-bit/8-byte complex type made up of
    two NPY_FLOAT values.

.. c:enumerator:: NPY_CDOUBLE
.. c:enumerator:: NPY_COMPLEX128

    The enumeration value for a 128-bit/16-byte complex type made up of
    two NPY_DOUBLE values.

.. c:enumerator:: NPY_CLONGDOUBLE

    The enumeration value for a platform-specific complex floating point
    type which is made up of two NPY_LONGDOUBLE values.

.. c:enumerator:: NPY_DATETIME

    The enumeration value for a data type which holds dates or datetimes with
    a precision based on selectable date or time units.

.. c:enumerator:: NPY_TIMEDELTA

    The enumeration value for a data type which holds lengths of times in
    integers of selectable date or time units.

.. c:enumerator:: NPY_STRING

    The enumeration value for ASCII strings of a selectable size. The
    strings have a fixed maximum size within a given array.

.. c:enumerator:: NPY_UNICODE

    The enumeration value for UCS4 strings of a selectable size. The
    strings have a fixed maximum size within a given array.

.. c:enumerator:: NPY_OBJECT

    The enumeration value for references to arbitrary Python objects.

.. c:enumerator:: NPY_VOID

    Primarily used to hold struct dtypes, but can contain arbitrary
    binary data.

Some useful aliases of the above types are

.. c:enumerator:: NPY_INTP

    The enumeration value for a signed integer type which is the same
    size as a (void \*) pointer. This is the type used by all
    arrays of indices.

.. c:enumerator:: NPY_UINTP

    The enumeration value for an unsigned integer type which is the
    same size as a (void \*) pointer.

.. c:enumerator:: NPY_MASK

    The enumeration value of the type used for masks, such as with
    the :c:data:`NPY_ITER_ARRAYMASK` iterator flag. This is equivalent
    to :c:data:`NPY_UINT8`.

.. c:enumerator:: NPY_DEFAULT_TYPE

    The default type to use when no dtype is explicitly specified, for
    example when calling np.zero(shape). This is equivalent to
    :c:data:`NPY_DOUBLE`.

Other useful related constants are

.. c:macro:: NPY_NTYPES

    The total number of built-in NumPy types. The enumeration covers
    the range from 0 to NPY_NTYPES-1.

.. c:macro:: NPY_NOTYPE

    A signal value guaranteed not to be a valid type enumeration number.

.. c:macro:: NPY_USERDEF

    The start of type numbers used for Custom Data types.

The various character codes indicating certain types are also part of
an enumerated list. References to type characters (should they be
needed at all) should always use these enumerations. The form of them
is ``NPY_{NAME}LTR`` where ``{NAME}`` can be

    **BOOL**, **BYTE**, **UBYTE**, **SHORT**, **USHORT**, **INT**,
    **UINT**, **LONG**, **ULONG**, **LONGLONG**, **ULONGLONG**,
    **HALF**, **FLOAT**, **DOUBLE**, **LONGDOUBLE**, **CFLOAT**,
    **CDOUBLE**, **CLONGDOUBLE**, **DATETIME**, **TIMEDELTA**,
    **OBJECT**, **STRING**, **VOID**

    **INTP**, **UINTP**

    **GENBOOL**, **SIGNED**, **UNSIGNED**, **FLOATING**, **COMPLEX**

The latter group of ``{NAME}s`` corresponds to letters used in the array
interface typestring specification.


Defines
-------

Max and min values for integers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``NPY_MAX_INT{bits}``, ``NPY_MAX_UINT{bits}``, ``NPY_MIN_INT{bits}``
    These are defined for ``{bits}`` = 8, 16, 32, 64, 128, and 256 and provide
    the maximum (minimum) value of the corresponding (unsigned) integer
    type. Note: the actual integer type may not be available on all
    platforms (i.e. 128-bit and 256-bit integers are rare).

``NPY_MIN_{type}``
    This is defined for ``{type}`` = **BYTE**, **SHORT**, **INT**,
    **LONG**, **LONGLONG**, **INTP**

``NPY_MAX_{type}``
    This is defined for all defined for ``{type}`` = **BYTE**, **UBYTE**,
    **SHORT**, **USHORT**, **INT**, **UINT**, **LONG**, **ULONG**,
    **LONGLONG**, **ULONGLONG**, **INTP**, **UINTP**


Number of bits in data types
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

All ``NPY_SIZEOF_{CTYPE}`` constants have corresponding
``NPY_BITSOF_{CTYPE}`` constants defined. The ``NPY_BITSOF_{CTYPE}``
constants provide the number of bits in the data type.  Specifically,
the available ``{CTYPE}s`` are

    **BOOL**, **CHAR**, **SHORT**, **INT**, **LONG**,
    **LONGLONG**, **FLOAT**, **DOUBLE**, **LONGDOUBLE**


Bit-width references to enumerated typenums
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

All of the numeric data types (integer, floating point, and complex)
have constants that are defined to be a specific enumerated type
number. Exactly which enumerated type a bit-width type refers to is
platform dependent. In particular, the constants available are
``PyArray_{NAME}{BITS}`` where ``{NAME}`` is **INT**, **UINT**,
**FLOAT**, **COMPLEX** and ``{BITS}`` can be 8, 16, 32, 64, 80, 96, 128,
160, 192, 256, and 512.  Obviously not all bit-widths are available on
all platforms for all the kinds of numeric types. Commonly 8-, 16-,
32-, 64-bit integers; 32-, 64-bit floats; and 64-, 128-bit complex
types are available.


Integer that can hold a pointer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The constants **NPY_INTP** and **NPY_UINTP** refer to an
enumerated integer type that is large enough to hold a pointer on the
platform. Index arrays should always be converted to **NPY_INTP**
, because the dimension of the array is of type npy_intp.


C-type names
------------

There are standard variable types for each of the numeric data types
and the bool data type. Some of these are already available in the
C-specification. You can create variables in extension code with these
types.


Boolean
^^^^^^^

.. c:type:: npy_bool

    unsigned char; The constants :c:data:`NPY_FALSE` and
    :c:data:`NPY_TRUE` are also defined.


(Un)Signed Integer
^^^^^^^^^^^^^^^^^^

Unsigned versions of the integers can be defined by pre-pending a 'u'
to the front of the integer name.

.. c:type:: npy_byte

    char

.. c:type:: npy_ubyte

    unsigned char

.. c:type:: npy_short

    short

.. c:type:: npy_ushort

    unsigned short

.. c:type:: npy_int

    int

.. c:type:: npy_uint

    unsigned int

.. c:type:: npy_int16

    16-bit integer

.. c:type:: npy_uint16

    16-bit unsigned integer

.. c:type:: npy_int32

    32-bit integer

.. c:type:: npy_uint32

    32-bit unsigned integer

.. c:type:: npy_int64

    64-bit integer

.. c:type:: npy_uint64

    64-bit unsigned integer

.. c:type:: npy_long

    long int

.. c:type:: npy_ulong

    unsigned long int

.. c:type:: npy_longlong

    long long int

.. c:type:: npy_ulonglong

    unsigned long long int

.. c:type:: npy_intp

    Py_intptr_t (an integer that is the size of a pointer on
    the platform).

.. c:type:: npy_uintp

    unsigned Py_intptr_t (an integer that is the size of a pointer on
    the platform).


(Complex) Floating point
^^^^^^^^^^^^^^^^^^^^^^^^

.. c:type:: npy_half

    16-bit float

.. c:type:: npy_float

    32-bit float

.. c:type:: npy_cfloat

    32-bit complex float

.. c:type:: npy_double

    64-bit double

.. c:type:: npy_cdouble

    64-bit complex double

.. c:type:: npy_longdouble

    long double

.. c:type:: npy_clongdouble

    long complex double

complex types are structures with **.real** and **.imag** members (in
that order).


Bit-width names
^^^^^^^^^^^^^^^

There are also typedefs for signed integers, unsigned integers,
floating point, and complex floating point types of specific bit-
widths. The available type names are

    ``npy_int{bits}``, ``npy_uint{bits}``, ``npy_float{bits}``,
    and ``npy_complex{bits}``

where ``{bits}`` is the number of bits in the type and can be **8**,
**16**, **32**, **64**, 128, and 256 for integer types; 16, **32**
, **64**, 80, 96, 128, and 256 for floating-point types; and 32,
**64**, **128**, 160, 192, and 512 for complex-valued types. Which
bit-widths are available is platform dependent. The bolded bit-widths
are usually available on all platforms.


Printf Formatting
-----------------

For help in printing, the following strings are defined as the correct
format specifier in printf and related commands.

.. c:macro:: NPY_LONGLONG_FMT

.. c:macro:: NPY_ULONGLONG_FMT

.. c:macro:: NPY_INTP_FMT

.. c:macro:: NPY_UINTP_FMT

.. c:macro:: NPY_LONGDOUBLE_FMT
.. _c-api:

###########
NumPy C-API
###########

.. sectionauthor:: Travis E. Oliphant

|    Beware of the man who won't be bothered with details.
|    --- *William Feather, Sr.*

|    The truth is out there.
|    --- *Chris Carter, The X Files*


NumPy provides a C-API to enable users to extend the system and get
access to the array object for use in other routines. The best way to
truly understand the C-API is to read the source code. If you are
unfamiliar with (C) source code, however, this can be a daunting
experience at first. Be assured that the task becomes easier with
practice, and you may be surprised at how simple the C-code can be to
understand. Even if you don't think you can write C-code from scratch,
it is much easier to understand and modify already-written source code
than create it *de novo*.

Python extensions are especially straightforward to understand because
they all have a very similar structure. Admittedly, NumPy is not a
trivial extension to Python, and may take a little more snooping to
grasp. This is especially true because of the code-generation
techniques, which simplify maintenance of very similar code, but can
make the code a little less readable to beginners. Still, with a
little persistence, the code can be opened to your understanding. It
is my hope, that this guide to the C-API can assist in the process of
becoming familiar with the compiled-level work that can be done with
NumPy in order to squeeze that last bit of necessary speed out of your
code.

.. currentmodule:: numpy-c-api

.. toctree::
   :maxdepth: 2

   types-and-structures
   config
   dtype
   array
   iterator
   ufunc
   generalized-ufuncs
   coremath
   deprecations
   data_memory
.. _c-api.generalized-ufuncs:

==================================
Generalized Universal Function API
==================================

There is a general need for looping over not only functions on scalars
but also over functions on vectors (or arrays).
This concept is realized in NumPy by generalizing the universal functions
(ufuncs).  In regular ufuncs, the elementary function is limited to
element-by-element operations, whereas the generalized version (gufuncs)
supports "sub-array" by "sub-array" operations.  The Perl vector library PDL
provides a similar functionality and its terms are re-used in the following.

Each generalized ufunc has information associated with it that states
what the "core" dimensionality of the inputs is, as well as the
corresponding dimensionality of the outputs (the element-wise ufuncs
have zero core dimensions).  The list of the core dimensions for all
arguments is called the "signature" of a ufunc.  For example, the
ufunc numpy.add has signature ``(),()->()`` defining two scalar inputs
and one scalar output.

Another example is the function ``inner1d(a, b)`` with a signature of
``(i),(i)->()``.  This applies the inner product along the last axis of
each input, but keeps the remaining indices intact.
For example, where ``a`` is of shape ``(3, 5, N)`` and ``b`` is of shape
``(5, N)``, this will return an output of shape ``(3,5)``.
The underlying elementary function is called ``3 * 5`` times.  In the
signature, we specify one core dimension ``(i)`` for each input and zero core
dimensions ``()`` for the output, since it takes two 1-d arrays and
returns a scalar.  By using the same name ``i``, we specify that the two
corresponding dimensions should be of the same size.

The dimensions beyond the core dimensions are called "loop" dimensions.  In
the above example, this corresponds to ``(3, 5)``.

The signature determines how the dimensions of each input/output array are
split into core and loop dimensions:

#. Each dimension in the signature is matched to a dimension of the
   corresponding passed-in array, starting from the end of the shape tuple.
   These are the core dimensions, and they must be present in the arrays, or
   an error will be raised.
#. Core dimensions assigned to the same label in the signature (e.g. the
   ``i`` in ``inner1d``'s ``(i),(i)->()``) must have exactly matching sizes,
   no broadcasting is performed.
#. The core dimensions are removed from all inputs and the remaining
   dimensions are broadcast together, defining the loop dimensions.
#. The shape of each output is determined from the loop dimensions plus the
   output's core dimensions

Typically, the size of all core dimensions in an output will be determined by
the size of a core dimension with the same label in an input array. This is
not a requirement, and it is possible to define a signature where a label
comes up for the first time in an output, although some precautions must be
taken when calling such a function. An example would be the function
``euclidean_pdist(a)``, with signature ``(n,d)->(p)``, that given an array of
``n`` ``d``-dimensional vectors, computes all unique pairwise Euclidean
distances among them. The output dimension ``p`` must therefore be equal to
``n * (n - 1) / 2``, but it is the caller's responsibility to pass in an
output array of the right size. If the size of a core dimension of an output
cannot be determined from a passed in input or output array, an error will be
raised.

Note: Prior to NumPy 1.10.0, less strict checks were in place: missing core
dimensions were created by prepending 1's to the shape as necessary, core
dimensions with the same label were broadcast together, and undetermined
dimensions were created with size 1.


Definitions
-----------

Elementary Function
    Each ufunc consists of an elementary function that performs the
    most basic operation on the smallest portion of array arguments
    (e.g. adding two numbers is the most basic operation in adding two
    arrays).  The ufunc applies the elementary function multiple times
    on different parts of the arrays.  The input/output of elementary
    functions can be vectors; e.g., the elementary function of inner1d
    takes two vectors as input.

Signature
    A signature is a string describing the input/output dimensions of
    the elementary function of a ufunc.  See section below for more
    details.

Core Dimension
    The dimensionality of each input/output of an elementary function
    is defined by its core dimensions (zero core dimensions correspond
    to a scalar input/output).  The core dimensions are mapped to the
    last dimensions of the input/output arrays.

Dimension Name
    A dimension name represents a core dimension in the signature.
    Different dimensions may share a name, indicating that they are of
    the same size.

Dimension Index
    A dimension index is an integer representing a dimension name. It
    enumerates the dimension names according to the order of the first
    occurrence of each name in the signature.

.. _details-of-signature:

Details of Signature
--------------------

The signature defines "core" dimensionality of input and output
variables, and thereby also defines the contraction of the
dimensions.  The signature is represented by a string of the
following format:

* Core dimensions of each input or output array are represented by a
  list of dimension names in parentheses, ``(i_1,...,i_N)``; a scalar
  input/output is denoted by ``()``.  Instead of ``i_1``, ``i_2``,
  etc, one can use any valid Python variable name.
* Dimension lists for different arguments are separated by ``","``.
  Input/output arguments are separated by ``"->"``.
* If one uses the same dimension name in multiple locations, this
  enforces the same size of the corresponding dimensions.

The formal syntax of signatures is as follows::

    <Signature>            ::= <Input arguments> "->" <Output arguments>
    <Input arguments>      ::= <Argument list>
    <Output arguments>     ::= <Argument list>
    <Argument list>        ::= nil | <Argument> | <Argument> "," <Argument list>
    <Argument>             ::= "(" <Core dimension list> ")"
    <Core dimension list>  ::= nil | <Core dimension> |
                               <Core dimension> "," <Core dimension list>
    <Core dimension>       ::= <Dimension name> <Dimension modifier>
    <Dimension name>       ::= valid Python variable name | valid integer
    <Dimension modifier>   ::= nil | "?"

Notes:

#. All quotes are for clarity.
#. Unmodified core dimensions that share the same name must have the same size.
   Each dimension name typically corresponds to one level of looping in the
   elementary function's implementation.
#. White spaces are ignored.
#. An integer as a dimension name freezes that dimension to the value.
#. If the name is suffixed with the "?" modifier, the dimension is a core
   dimension only if it exists on all inputs and outputs that share it;
   otherwise it is ignored (and replaced by a dimension of size 1 for the
   elementary function).

Here are some examples of signatures:

+-------------+----------------------------+-----------------------------------+
| name        | signature                  | common usage                      |
+=============+============================+===================================+
| add         | ``(),()->()``              | binary ufunc                      |
+-------------+----------------------------+-----------------------------------+
| sum1d       | ``(i)->()``                | reduction                         |
+-------------+----------------------------+-----------------------------------+
| inner1d     | ``(i),(i)->()``            | vector-vector multiplication      |
+-------------+----------------------------+-----------------------------------+
| matmat      | ``(m,n),(n,p)->(m,p)``     | matrix multiplication             |
+-------------+----------------------------+-----------------------------------+
| vecmat      | ``(n),(n,p)->(p)``         | vector-matrix multiplication      |
+-------------+----------------------------+-----------------------------------+
| matvec      | ``(m,n),(n)->(m)``         | matrix-vector multiplication      |
+-------------+----------------------------+-----------------------------------+
| matmul      | ``(m?,n),(n,p?)->(m?,p?)`` | combination of the four above     |
+-------------+----------------------------+-----------------------------------+
| outer_inner | ``(i,t),(j,t)->(i,j)``     | inner over the last dimension,    |
|             |                            | outer over the second to last,    |
|             |                            | and loop/broadcast over the rest. |
+-------------+----------------------------+-----------------------------------+
|  cross1d    | ``(3),(3)->(3)``           | cross product where the last      |
|             |                            | dimension is frozen and must be 3 |
+-------------+----------------------------+-----------------------------------+

.. _frozen:

The last is an instance of freezing a core dimension and can be used to
improve ufunc performance

C-API for implementing Elementary Functions
-------------------------------------------

The current interface remains unchanged, and ``PyUFunc_FromFuncAndData``
can still be used to implement (specialized) ufuncs, consisting of
scalar elementary functions.

One can use ``PyUFunc_FromFuncAndDataAndSignature`` to declare a more
general ufunc.  The argument list is the same as
``PyUFunc_FromFuncAndData``, with an additional argument specifying the
signature as C string.

Furthermore, the callback function is of the same type as before,
``void (*foo)(char **args, intp *dimensions, intp *steps, void *func)``.
When invoked, ``args`` is a list of length ``nargs`` containing
the data of all input/output arguments.  For a scalar elementary
function, ``steps`` is also of length ``nargs``, denoting the strides used
for the arguments. ``dimensions`` is a pointer to a single integer
defining the size of the axis to be looped over.

For a non-trivial signature, ``dimensions`` will also contain the sizes
of the core dimensions as well, starting at the second entry.  Only
one size is provided for each unique dimension name and the sizes are
given according to the first occurrence of a dimension name in the
signature.

The first ``nargs`` elements of ``steps`` remain the same as for scalar
ufuncs.  The following elements contain the strides of all core
dimensions for all arguments in order.

For example, consider a ufunc with signature ``(i,j),(i)->()``.  In
this case, ``args`` will contain three pointers to the data of the
input/output arrays ``a``, ``b``, ``c``.  Furthermore, ``dimensions`` will be
``[N, I, J]`` to define the size of ``N`` of the loop and the sizes ``I`` and ``J``
for the core dimensions ``i`` and ``j``.  Finally, ``steps`` will be
``[a_N, b_N, c_N, a_i, a_j, b_i]``, containing all necessary strides.

*****************************
Python Types and C-Structures
*****************************

.. sectionauthor:: Travis E. Oliphant

Several new types are defined in the C-code. Most of these are
accessible from Python, but a few are not exposed due to their limited
use. Every new Python type has an associated :c:expr:`PyObject *` with an
internal structure that includes a pointer to a "method table" that
defines how the new object behaves in Python. When you receive a
Python object into C code, you always get a pointer to a
:c:type:`PyObject` structure. Because a :c:type:`PyObject` structure is
very generic and defines only :c:macro:`PyObject_HEAD`, by itself it
is not very interesting. However, different objects contain more
details after the :c:macro:`PyObject_HEAD` (but you have to cast to the
correct type to access them --- or use accessor functions or macros).


New Python Types Defined
========================

Python types are the functional equivalent in C of classes in Python.
By constructing a new Python type you make available a new object for
Python. The ndarray object is an example of a new type defined in C.
New types are defined in C by two basic steps:

1. creating a C-structure (usually named ``Py{Name}Object``) that is
   binary- compatible with the :c:type:`PyObject` structure itself but holds
   the additional information needed for that particular object;

2. populating the :c:type:`PyTypeObject` table (pointed to by the ob_type
   member of the :c:type:`PyObject` structure) with pointers to functions
   that implement the desired behavior for the type.

Instead of special method names which define behavior for Python
classes, there are "function tables" which point to functions that
implement the desired results. Since Python 2.2, the PyTypeObject
itself has become dynamic which allows C types that can be "sub-typed
"from other C-types in C, and sub-classed in Python. The children
types inherit the attributes and methods from their parent(s).

There are two major new types: the ndarray ( :c:data:`PyArray_Type` )
and the ufunc ( :c:data:`PyUFunc_Type` ). Additional types play a
supportive role: the :c:data:`PyArrayIter_Type`, the
:c:data:`PyArrayMultiIter_Type`, and the :c:data:`PyArrayDescr_Type`
. The :c:data:`PyArrayIter_Type` is the type for a flat iterator for an
ndarray (the object that is returned when getting the flat
attribute). The :c:data:`PyArrayMultiIter_Type` is the type of the
object returned when calling ``broadcast`` (). It handles iteration
and broadcasting over a collection of nested sequences. Also, the
:c:data:`PyArrayDescr_Type` is the data-type-descriptor type whose
instances describe the data.  Finally, there are 21 new scalar-array
types which are new Python scalars corresponding to each of the
fundamental data types available for arrays. An additional 10 other
types are place holders that allow the array scalars to fit into a
hierarchy of actual Python types.


PyArray_Type and PyArrayObject
------------------------------

.. c:var:: PyTypeObject PyArray_Type

   The Python type of the ndarray is :c:data:`PyArray_Type`. In C, every
   ndarray is a pointer to a :c:type:`PyArrayObject` structure. The ob_type
   member of this structure contains a pointer to the :c:data:`PyArray_Type`
   typeobject.

.. c:type:: PyArrayObject
            NPY_AO

   The :c:type:`PyArrayObject` C-structure contains all of the required
   information for an array. All instances of an ndarray (and its
   subclasses) will have this structure.  For future compatibility,
   these structure members should normally be accessed using the
   provided macros. If you need a shorter name, then you can make use
   of :c:type:`NPY_AO` (deprecated) which is defined to be equivalent to
   :c:type:`PyArrayObject`. Direct access to the struct fields are
   deprecated. Use the ``PyArray_*(arr)`` form instead.
   As of NumPy 1.20, the size of this struct is not considered part of
   the NumPy ABI (see note at the end of the member list).

   .. code-block:: c

      typedef struct PyArrayObject {
          PyObject_HEAD
          char *data;
          int nd;
          npy_intp *dimensions;
          npy_intp *strides;
          PyObject *base;
          PyArray_Descr *descr;
          int flags;
          PyObject *weakreflist;
          /* version dependent private members */
      } PyArrayObject;

   .. c:macro:: PyObject_HEAD

       This is needed by all Python objects. It consists of (at least)
       a reference count member ( ``ob_refcnt`` ) and a pointer to the
       typeobject ( ``ob_type`` ). (Other elements may also be present
       if Python was compiled with special options see
       Include/object.h in the Python source tree for more
       information). The ob_type member points to a Python type
       object.

   .. c:member:: char *data

       Accessible via :c:data:`PyArray_DATA`, this data member is a
       pointer to the first element of the array. This pointer can
       (and normally should) be recast to the data type of the array.

   .. c:member:: int nd

       An integer providing the number of dimensions for this
       array. When nd is 0, the array is sometimes called a rank-0
       array. Such arrays have undefined dimensions and strides and
       cannot be accessed. Macro :c:data:`PyArray_NDIM` defined in
       ``ndarraytypes.h`` points to this data member. :c:data:`NPY_MAXDIMS`
       is the largest number of dimensions for any array.

   .. c:member:: npy_intp dimensions

       An array of integers providing the shape in each dimension as
       long as nd :math:`\geq` 1. The integer is always large enough
       to hold a pointer on the platform, so the dimension size is
       only limited by memory. :c:data:`PyArray_DIMS` is the macro
       associated with this data member.

   .. c:member:: npy_intp *strides

       An array of integers providing for each dimension the number of
       bytes that must be skipped to get to the next element in that
       dimension. Associated with macro :c:data:`PyArray_STRIDES`.

   .. c:member:: PyObject *base

       Pointed to by :c:data:`PyArray_BASE`, this member is used to hold a
       pointer to another Python object that is related to this array.
       There are two use cases:

       - If this array does not own its own memory, then base points to the
         Python object that owns it (perhaps another array object)
       - If this array has the :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag set,
         then this array is a working copy of a "misbehaved" array.

       When ``PyArray_ResolveWritebackIfCopy`` is called, the array pointed to
       by base will be updated with the contents of this array.

   .. c:member:: PyArray_Descr *descr

       A pointer to a data-type descriptor object (see below). The
       data-type descriptor object is an instance of a new built-in
       type which allows a generic description of memory. There is a
       descriptor structure for each data type supported. This
       descriptor structure contains useful information about the type
       as well as a pointer to a table of function pointers to
       implement specific functionality. As the name suggests, it is
       associated with the macro :c:data:`PyArray_DESCR`.

   .. c:member:: int flags

       Pointed to by the macro :c:data:`PyArray_FLAGS`, this data member represents
       the flags indicating how the memory pointed to by data is to be
       interpreted. Possible flags are :c:data:`NPY_ARRAY_C_CONTIGUOUS`,
       :c:data:`NPY_ARRAY_F_CONTIGUOUS`, :c:data:`NPY_ARRAY_OWNDATA`,
       :c:data:`NPY_ARRAY_ALIGNED`, :c:data:`NPY_ARRAY_WRITEABLE`,
       :c:data:`NPY_ARRAY_WRITEBACKIFCOPY`.

   .. c:member:: PyObject *weakreflist

       This member allows array objects to have weak references (using the
       weakref module).

   .. note::

      Further members are considered private and version dependent. If the size
      of the struct is important for your code, special care must be taken.
      A possible use-case when this is relevant is subclassing in C.
      If your code relies on ``sizeof(PyArrayObject)`` to be constant,
      you must add the following check at import time:

      .. code-block:: c

         if (sizeof(PyArrayObject) < PyArray_Type.tp_basicsize) {
             PyErr_SetString(PyExc_ImportError,
                "Binary incompatibility with NumPy, must recompile/update X.");
             return NULL;
         }

      To ensure that your code does not have to be compiled for a specific
      NumPy version, you may add a constant, leaving room for changes in NumPy.
      A solution guaranteed to be compatible with any future NumPy version
      requires the use of a runtime calculate offset and allocation size.


PyArrayDescr_Type and PyArray_Descr
-----------------------------------

.. c:var:: PyTypeObject PyArrayDescr_Type

   The :c:data:`PyArrayDescr_Type` is the built-in type of the
   data-type-descriptor objects used to describe how the bytes comprising
   the array are to be interpreted.  There are 21 statically-defined
   :c:type:`PyArray_Descr` objects for the built-in data-types. While these
   participate in reference counting, their reference count should never
   reach zero.  There is also a dynamic table of user-defined
   :c:type:`PyArray_Descr` objects that is also maintained. Once a
   data-type-descriptor object is "registered" it should never be
   deallocated either. The function :c:func:`PyArray_DescrFromType` (...) can
   be used to retrieve a :c:type:`PyArray_Descr` object from an enumerated
   type-number (either built-in or user- defined).

.. c:type:: PyArray_Descr

   The :c:type:`PyArray_Descr` structure lies at the heart of the
   :c:data:`PyArrayDescr_Type`. While it is described here for
   completeness, it should be considered internal to NumPy and manipulated via
   ``PyArrayDescr_*`` or ``PyDataType*`` functions and macros. The size of this
   structure is subject to change across versions of NumPy. To ensure
   compatibility:

   - Never declare a non-pointer instance of the struct
   - Never perform pointer arithmetic
   - Never use ``sizof(PyArray_Descr)``

   It has the following structure:

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          PyTypeObject *typeobj;
          char kind;
          char type;
          char byteorder;
          char flags;
          int type_num;
          int elsize;
          int alignment;
          PyArray_ArrayDescr *subarray;
          PyObject *fields;
          PyObject *names;
          PyArray_ArrFuncs *f;
          PyObject *metadata;
          NpyAuxData *c_metadata;
          npy_hash_t hash;
      } PyArray_Descr;

   .. c:member:: PyTypeObject *typeobj

       Pointer to a typeobject that is the corresponding Python type for
       the elements of this array. For the builtin types, this points to
       the corresponding array scalar. For user-defined types, this
       should point to a user-defined typeobject. This typeobject can
       either inherit from array scalars or not. If it does not inherit
       from array scalars, then the :c:data:`NPY_USE_GETITEM` and
       :c:data:`NPY_USE_SETITEM` flags should be set in the ``flags`` member.

   .. c:member:: char kind

       A character code indicating the kind of array (using the array
       interface typestring notation). A 'b' represents Boolean, a 'i'
       represents signed integer, a 'u' represents unsigned integer, 'f'
       represents floating point, 'c' represents complex floating point, 'S'
       represents 8-bit zero-terminated bytes, 'U' represents 32-bit/character
       unicode string, and 'V' represents arbitrary.

   .. c:member:: char type

       A traditional character code indicating the data type.

   .. c:member:: char byteorder

       A character indicating the byte-order: '>' (big-endian), '<' (little-
       endian), '=' (native), '\|' (irrelevant, ignore). All builtin data-
       types have byteorder '='.

   .. c:member:: char flags

       A data-type bit-flag that determines if the data-type exhibits object-
       array like behavior. Each bit in this member is a flag which are named
       as:

   .. c:member:: int alignment

       Non-NULL if this type is an array (C-contiguous) of some other type


..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_ITEM_REFCOUNT

    Indicates that items of this data-type must be reference
    counted (using :c:func:`Py_INCREF` and :c:func:`Py_DECREF` ).

       .. c:macro:: NPY_ITEM_HASOBJECT

           Same as :c:data:`NPY_ITEM_REFCOUNT`.

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: NPY_LIST_PICKLE

    Indicates arrays of this data-type must be converted to a list
    before pickling.

.. c:macro:: NPY_ITEM_IS_POINTER

    Indicates the item is a pointer to some other data-type

.. c:macro:: NPY_NEEDS_INIT

    Indicates memory for this data-type must be initialized (set
    to 0) on creation.

.. c:macro:: NPY_NEEDS_PYAPI

    Indicates this data-type requires the Python C-API during
    access (so don't give up the GIL if array access is going to
    be needed).

.. c:macro:: NPY_USE_GETITEM

    On array access use the ``f->getitem`` function pointer
    instead of the standard conversion to an array scalar. Must
    use if you don't define an array scalar to go along with
    the data-type.

.. c:macro:: NPY_USE_SETITEM

    When creating a 0-d array from an array scalar use
    ``f->setitem`` instead of the standard copy from an array
    scalar. Must use if you don't define an array scalar to go
    along with the data-type.

       .. c:macro:: NPY_FROM_FIELDS

           The bits that are inherited for the parent data-type if these
           bits are set in any field of the data-type. Currently (
           :c:data:`NPY_NEEDS_INIT` \| :c:data:`NPY_LIST_PICKLE` \|
           :c:data:`NPY_ITEM_REFCOUNT` \| :c:data:`NPY_NEEDS_PYAPI` ).

       .. c:macro:: NPY_OBJECT_DTYPE_FLAGS

           Bits set for the object data-type: ( :c:data:`NPY_LIST_PICKLE`
           \| :c:data:`NPY_USE_GETITEM` \| :c:data:`NPY_ITEM_IS_POINTER` \|
           :c:data:`NPY_ITEM_REFCOUNT` \| :c:data:`NPY_NEEDS_INIT` \|
           :c:data:`NPY_NEEDS_PYAPI`).

       .. c:function:: int PyDataType_FLAGCHK(PyArray_Descr *dtype, int flags)

           Return true if all the given flags are set for the data-type
           object.

       .. c:function:: int PyDataType_REFCHK(PyArray_Descr *dtype)

           Equivalent to :c:func:`PyDataType_FLAGCHK` (*dtype*,
           :c:data:`NPY_ITEM_REFCOUNT`).

   .. c:member:: int type_num

       A number that uniquely identifies the data type. For new data-types,
       this number is assigned when the data-type is registered.

   .. c:member:: int elsize

       For data types that are always the same size (such as long), this
       holds the size of the data type. For flexible data types where
       different arrays can have a different elementsize, this should be
       0.

   .. c:member:: int alignment

       A number providing alignment information for this data type.
       Specifically, it shows how far from the start of a 2-element
       structure (whose first element is a ``char`` ), the compiler
       places an item of this type: ``offsetof(struct {char c; type v;},
       v)``

   .. c:member:: PyArray_ArrayDescr *subarray

       If this is non- ``NULL``, then this data-type descriptor is a
       C-style contiguous array of another data-type descriptor. In
       other-words, each element that this descriptor describes is
       actually an array of some other base descriptor. This is most
       useful as the data-type descriptor for a field in another
       data-type descriptor. The fields member should be ``NULL`` if this
       is non- ``NULL`` (the fields member of the base descriptor can be
       non- ``NULL`` however).

       .. c:type:: PyArray_ArrayDescr

           .. code-block:: c

              typedef struct {
                  PyArray_Descr *base;
                  PyObject *shape;
              } PyArray_ArrayDescr;

           .. c:member:: PyArray_Descr *base

               The data-type-descriptor object of the base-type.

           .. c:member:: PyObject *shape

               The shape (always C-style contiguous) of the sub-array as a Python
               tuple.

   .. c:member:: PyObject *fields

       If this is non-NULL, then this data-type-descriptor has fields
       described by a Python dictionary whose keys are names (and also
       titles if given) and whose values are tuples that describe the
       fields. Recall that a data-type-descriptor always describes a
       fixed-length set of bytes. A field is a named sub-region of that
       total, fixed-length collection. A field is described by a tuple
       composed of another data- type-descriptor and a byte
       offset. Optionally, the tuple may contain a title which is
       normally a Python string. These tuples are placed in this
       dictionary keyed by name (and also title if given).

   .. c:member:: PyObject *names

       An ordered tuple of field names. It is NULL if no field is
       defined.

   .. c:member:: PyArray_ArrFuncs *f

       A pointer to a structure containing functions that the type needs
       to implement internal features. These functions are not the same
       thing as the universal functions (ufuncs) described later. Their
       signatures can vary arbitrarily.

   .. c:member:: PyObject *metadata

       Metadata about this dtype.

   .. c:member:: NpyAuxData *c_metadata

       Metadata specific to the C implementation
       of the particular dtype. Added for NumPy 1.7.0.

   .. c:type:: npy_hash_t
   .. c:member:: npy_hash_t *hash

       Currently unused. Reserved for future use in caching
       hash values.

.. c:type:: PyArray_ArrFuncs

    Functions implementing internal features. Not all of these
    function pointers must be defined for a given type. The required
    members are ``nonzero``, ``copyswap``, ``copyswapn``, ``setitem``,
    ``getitem``, and ``cast``. These are assumed to be non- ``NULL``
    and ``NULL`` entries will cause a program crash. The other
    functions may be ``NULL`` which will just mean reduced
    functionality for that data-type. (Also, the nonzero function will
    be filled in with a default function if it is ``NULL`` when you
    register a user-defined data-type).

    .. code-block:: c

       typedef struct {
           PyArray_VectorUnaryFunc *cast[NPY_NTYPES];
           PyArray_GetItemFunc *getitem;
           PyArray_SetItemFunc *setitem;
           PyArray_CopySwapNFunc *copyswapn;
           PyArray_CopySwapFunc *copyswap;
           PyArray_CompareFunc *compare;
           PyArray_ArgFunc *argmax;
           PyArray_DotFunc *dotfunc;
           PyArray_ScanFunc *scanfunc;
           PyArray_FromStrFunc *fromstr;
           PyArray_NonzeroFunc *nonzero;
           PyArray_FillFunc *fill;
           PyArray_FillWithScalarFunc *fillwithscalar;
           PyArray_SortFunc *sort[NPY_NSORTS];
           PyArray_ArgSortFunc *argsort[NPY_NSORTS];
           PyObject *castdict;
           PyArray_ScalarKindFunc *scalarkind;
           int **cancastscalarkindto;
           int *cancastto;
           PyArray_FastClipFunc *fastclip;  /* deprecated */
           PyArray_FastPutmaskFunc *fastputmask;  /* deprecated */
           PyArray_FastTakeFunc *fasttake;  /* deprecated */
           PyArray_ArgFunc *argmin;
       } PyArray_ArrFuncs;

    The concept of a behaved segment is used in the description of the
    function pointers. A behaved segment is one that is aligned and in
    native machine byte-order for the data-type. The ``nonzero``,
    ``copyswap``, ``copyswapn``, ``getitem``, and ``setitem``
    functions can (and must) deal with mis-behaved arrays. The other
    functions require behaved memory segments.

    .. c:member:: void cast( \
            void *from, void *to, npy_intp n, void *fromarr, void *toarr)

        An array of function pointers to cast from the current type to
        all of the other builtin types. Each function casts a
        contiguous, aligned, and notswapped buffer pointed at by
        *from* to a contiguous, aligned, and notswapped buffer pointed
        at by *to* The number of items to cast is given by *n*, and
        the arguments *fromarr* and *toarr* are interpreted as
        PyArrayObjects for flexible arrays to get itemsize
        information.

    .. c:member:: PyObject *getitem(void *data, void *arr)

        A pointer to a function that returns a standard Python object
        from a single element of the array object *arr* pointed to by
        *data*. This function must be able to deal with "misbehaved
        "(misaligned and/or swapped) arrays correctly.

    .. c:member:: int setitem(PyObject *item, void *data, void *arr)

        A pointer to a function that sets the Python object *item*
        into the array, *arr*, at the position pointed to by *data*
        . This function deals with "misbehaved" arrays. If successful,
        a zero is returned, otherwise, a negative one is returned (and
        a Python error set).

    .. c:member:: void copyswapn( \
            void *dest, npy_intp dstride, void *src, npy_intp sstride, \
            npy_intp n, int swap, void *arr)

    .. c:member:: void copyswap(void *dest, void *src, int swap, void *arr)

        These members are both pointers to functions to copy data from
        *src* to *dest* and *swap* if indicated. The value of arr is
        only used for flexible ( :c:data:`NPY_STRING`, :c:data:`NPY_UNICODE`,
        and :c:data:`NPY_VOID` ) arrays (and is obtained from
        ``arr->descr->elsize`` ). The second function copies a single
        value, while the first loops over n values with the provided
        strides. These functions can deal with misbehaved *src*
        data. If *src* is NULL then no copy is performed. If *swap* is
        0, then no byteswapping occurs. It is assumed that *dest* and
        *src* do not overlap. If they overlap, then use ``memmove``
        (...) first followed by ``copyswap(n)`` with NULL valued
        ``src``.

    .. c:member:: int compare(const void* d1, const void* d2, void* arr)

        A pointer to a function that compares two elements of the
        array, ``arr``, pointed to by ``d1`` and ``d2``. This
        function requires behaved (aligned and not swapped) arrays.
        The return value is 1 if * ``d1`` > * ``d2``, 0 if * ``d1`` == *
        ``d2``, and -1 if * ``d1`` < * ``d2``. The array object ``arr`` is
        used to retrieve itemsize and field information for flexible arrays.

    .. c:member:: int argmax( \
            void* data, npy_intp n, npy_intp* max_ind, void* arr)

        A pointer to a function that retrieves the index of the
        largest of ``n`` elements in ``arr`` beginning at the element
        pointed to by ``data``. This function requires that the
        memory segment be contiguous and behaved. The return value is
        always 0. The index of the largest element is returned in
        ``max_ind``.

    .. c:member:: void dotfunc( \
            void* ip1, npy_intp is1, void* ip2, npy_intp is2, void* op, \
            npy_intp n, void* arr)

        A pointer to a function that multiplies two ``n`` -length
        sequences together, adds them, and places the result in
        element pointed to by ``op`` of ``arr``. The start of the two
        sequences are pointed to by ``ip1`` and ``ip2``. To get to
        the next element in each sequence requires a jump of ``is1``
        and ``is2`` *bytes*, respectively. This function requires
        behaved (though not necessarily contiguous) memory.

    .. c:member:: int scanfunc(FILE* fd, void* ip, void* arr)

        A pointer to a function that scans (scanf style) one element
        of the corresponding type from the file descriptor ``fd`` into
        the array memory pointed to by ``ip``. The array is assumed
        to be behaved. 
        The last argument ``arr`` is the array to be scanned into.
        Returns number of receiving arguments successfully assigned (which
        may be zero in case a matching failure occurred before the first
        receiving argument was assigned), or EOF if input failure occurs 
        before the first receiving argument was assigned.
        This function should be called without holding the Python GIL, and
        has to grab it for error reporting.

    .. c:member:: int fromstr(char* str, void* ip, char** endptr, void* arr)

        A pointer to a function that converts the string pointed to by
        ``str`` to one element of the corresponding type and places it
        in the memory location pointed to by ``ip``. After the
        conversion is completed, ``*endptr`` points to the rest of the
        string. The last argument ``arr`` is the array into which ip
        points (needed for variable-size data- types). Returns 0 on
        success or -1 on failure. Requires a behaved array.
        This function should be called without holding the Python GIL, and
        has to grab it for error reporting.

    .. c:member:: npy_bool nonzero(void* data, void* arr)

        A pointer to a function that returns TRUE if the item of
        ``arr`` pointed to by ``data`` is nonzero. This function can
        deal with misbehaved arrays.

    .. c:member:: void fill(void* data, npy_intp length, void* arr)

        A pointer to a function that fills a contiguous array of given
        length with data. The first two elements of the array must
        already be filled- in. From these two values, a delta will be
        computed and the values from item 3 to the end will be
        computed by repeatedly adding this computed delta. The data
        buffer must be well-behaved.

    .. c:member:: void fillwithscalar( \
            void* buffer, npy_intp length, void* value, void* arr)

        A pointer to a function that fills a contiguous ``buffer`` of
        the given ``length`` with a single scalar ``value`` whose
        address is given. The final argument is the array which is
        needed to get the itemsize for variable-length arrays.

    .. c:member:: int sort(void* start, npy_intp length, void* arr)

        An array of function pointers to a particular sorting
        algorithms. A particular sorting algorithm is obtained using a
        key (so far :c:data:`NPY_QUICKSORT`, :c:data:`NPY_HEAPSORT`,
        and :c:data:`NPY_MERGESORT` are defined). These sorts are done
        in-place assuming contiguous and aligned data.

    .. c:member:: int argsort( \
            void* start, npy_intp* result, npy_intp length, void *arr)

        An array of function pointers to sorting algorithms for this
        data type. The same sorting algorithms as for sort are
        available. The indices producing the sort are returned in
        ``result`` (which must be initialized with indices 0 to
        ``length-1`` inclusive).

    .. c:member:: PyObject *castdict

        Either ``NULL`` or a dictionary containing low-level casting
        functions for user- defined data-types. Each function is
        wrapped in a :c:expr:`PyCapsule *` and keyed by
        the data-type number.

    .. c:member:: NPY_SCALARKIND scalarkind(PyArrayObject* arr)

        A function to determine how scalars of this type should be
        interpreted. The argument is ``NULL`` or a 0-dimensional array
        containing the data (if that is needed to determine the kind
        of scalar). The return value must be of type
        :c:type:`NPY_SCALARKIND`.

    .. c:member:: int **cancastscalarkindto

        Either ``NULL`` or an array of :c:type:`NPY_NSCALARKINDS`
        pointers. These pointers should each be either ``NULL`` or a
        pointer to an array of integers (terminated by
        :c:data:`NPY_NOTYPE`) indicating data-types that a scalar of
        this data-type of the specified kind can be cast to safely
        (this usually means without losing precision).

    .. c:member:: int *cancastto

        Either ``NULL`` or an array of integers (terminated by
        :c:data:`NPY_NOTYPE` ) indicated data-types that this data-type
        can be cast to safely (this usually means without losing
        precision).

    .. c:member:: void fastclip( \
            void *in, npy_intp n_in, void *min, void *max, void *out)

        .. deprecated:: 1.17
            The use of this function will give a deprecation warning when
            ``np.clip``. Instead of this function, the datatype must
            instead use ``PyUFunc_RegisterLoopForDescr`` to attach a custom
            loop to ``np.core.umath.clip``, ``np.minimum``, and ``np.maximum``.

        .. deprecated:: 1.19
            Setting this function is deprecated and should always be ``NULL``,
            if set, it will be ignored.

        A function that reads ``n_in`` items from ``in``, and writes to
        ``out`` the read value if it is within the limits pointed to by
        ``min`` and ``max``, or the corresponding limit if outside. The
        memory segments must be contiguous and behaved, and either
        ``min`` or ``max`` may be ``NULL``, but not both.

    .. c:member:: void fastputmask( \
            void *in, void *mask, npy_intp n_in, void *values, npy_intp nv)

        .. deprecated:: 1.19
            Setting this function is deprecated and should always be ``NULL``,
            if set, it will be ignored.

        A function that takes a pointer ``in`` to an array of ``n_in``
        items, a pointer ``mask`` to an array of ``n_in`` boolean
        values, and a pointer ``vals`` to an array of ``nv`` items.
        Items from ``vals`` are copied into ``in`` wherever the value
        in ``mask`` is non-zero, tiling ``vals`` as needed if
        ``nv < n_in``. All arrays must be contiguous and behaved.

    .. c:member:: void fasttake( \
            void *dest, void *src, npy_intp *indarray, npy_intp nindarray, \
            npy_intp n_outer, npy_intp m_middle, npy_intp nelem, \
            NPY_CLIPMODE clipmode)

        .. deprecated:: 1.19
            Setting this function is deprecated and should always be ``NULL``,
            if set, it will be ignored.

        A function that takes a pointer ``src`` to a C contiguous,
        behaved segment, interpreted as a 3-dimensional array of shape
        ``(n_outer, nindarray, nelem)``, a pointer ``indarray`` to a
        contiguous, behaved segment of ``m_middle`` integer indices,
        and a pointer ``dest`` to a C contiguous, behaved segment,
        interpreted as a 3-dimensional array of shape
        ``(n_outer, m_middle, nelem)``. The indices in ``indarray`` are
        used to index ``src`` along the second dimension, and copy the
        corresponding chunks of ``nelem`` items into ``dest``.
        ``clipmode`` (which can take on the values :c:data:`NPY_RAISE`,
        :c:data:`NPY_WRAP` or :c:data:`NPY_CLIP`) determines how will
        indices smaller than 0 or larger than ``nindarray`` will be
        handled.

    .. c:member:: int argmin( \
            void* data, npy_intp n, npy_intp* min_ind, void* arr)

        A pointer to a function that retrieves the index of the
        smallest of ``n`` elements in ``arr`` beginning at the element
        pointed to by ``data``. This function requires that the
        memory segment be contiguous and behaved. The return value is
        always 0. The index of the smallest element is returned in
        ``min_ind``.


The :c:data:`PyArray_Type` typeobject implements many of the features of
:c:type:`Python objects <PyTypeObject>` including the :c:member:`tp_as_number
<PyTypeObject.tp_as_number>`, :c:member:`tp_as_sequence
<PyTypeObject.tp_as_sequence>`, :c:member:`tp_as_mapping
<PyTypeObject.tp_as_mapping>`, and :c:member:`tp_as_buffer
<PyTypeObject.tp_as_buffer>` interfaces. The :c:type:`rich comparison
<richcmpfunc>`) is also used along with new-style attribute lookup for
member (:c:member:`tp_members <PyTypeObject.tp_members>`) and properties
(:c:member:`tp_getset <PyTypeObject.tp_getset>`).
The :c:data:`PyArray_Type` can also be sub-typed.

.. tip::

    The ``tp_as_number`` methods use a generic approach to call whatever
    function has been registered for handling the operation.  When the
    ``_multiarray_umath module`` is imported, it sets the numeric operations
    for all arrays to the corresponding ufuncs. This choice can be changed with
    :c:func:`PyUFunc_ReplaceLoopBySignature` The ``tp_str`` and ``tp_repr``
    methods can also be altered using :c:func:`PyArray_SetStringFunction`.


PyUFunc_Type and PyUFuncObject
------------------------------

.. c:var:: PyTypeObject PyUFunc_Type

   The ufunc object is implemented by creation of the
   :c:data:`PyUFunc_Type`. It is a very simple type that implements only
   basic getattribute behavior, printing behavior, and has call
   behavior which allows these objects to act like functions. The
   basic idea behind the ufunc is to hold a reference to fast
   1-dimensional (vector) loops for each data type that supports the
   operation. These one-dimensional loops all have the same signature
   and are the key to creating a new ufunc. They are called by the
   generic looping code as appropriate to implement the N-dimensional
   function. There are also some generic 1-d loops defined for
   floating and complexfloating arrays that allow you to define a
   ufunc using a single scalar function (*e.g.* atanh).


.. c:type:: PyUFuncObject

   The core of the ufunc is the :c:type:`PyUFuncObject` which contains all
   the information needed to call the underlying C-code loops that
   perform the actual work. While it is described here for completeness, it
   should be considered internal to NumPy and manipulated via ``PyUFunc_*``
   functions. The size of this structure is subject to change across versions
   of NumPy. To ensure compatibility:

   - Never declare a non-pointer instance of the struct
   - Never perform pointer arithmetic
   - Never use ``sizeof(PyUFuncObject)``

   It has the following structure:

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          int nin;
          int nout;
          int nargs;
          int identity;
          PyUFuncGenericFunction *functions;
          void **data;
          int ntypes;
          int reserved1;
          const char *name;
          char *types;
          const char *doc;
          void *ptr;
          PyObject *obj;
          PyObject *userloops;
          int core_enabled;
          int core_num_dim_ix;
          int *core_num_dims;
          int *core_dim_ixs;
          int *core_offsets;
          char *core_signature;
          PyUFunc_TypeResolutionFunc *type_resolver;
          PyUFunc_LegacyInnerLoopSelectionFunc *legacy_inner_loop_selector;
          void *reserved2;
          npy_uint32 *op_flags;
          npy_uint32 *iter_flags;
          /* new in API version 0x0000000D */
          npy_intp *core_dim_sizes;
          npy_uint32 *core_dim_flags;
          PyObject *identity_value;
          /* Further private slots (size depends on the NumPy version) */
      } PyUFuncObject;

   .. c:macro: PyObject_HEAD

       required for all Python objects.

   .. c:member:: int nin

       The number of input arguments.

   .. c:member:: int nout

       The number of output arguments.

   .. c:member:: int nargs

       The total number of arguments (*nin* + *nout*). This must be
       less than :c:data:`NPY_MAXARGS`.

   .. c:member:: int identity

       Either :c:data:`PyUFunc_One`, :c:data:`PyUFunc_Zero`,
       :c:data:`PyUFunc_MinusOne`, :c:data:`PyUFunc_None`,
       :c:data:`PyUFunc_ReorderableNone`, or
       :c:data:`PyUFunc_IdentityValue` to indicate
       the identity for this operation. It is only used for a
       reduce-like call on an empty array.

   .. c:member:: void functions( \
          char** args, npy_intp* dims, npy_intp* steps, void* extradata)

       An array of function pointers --- one for each data type
       supported by the ufunc. This is the vector loop that is called
       to implement the underlying function *dims* [0] times. The
       first argument, *args*, is an array of *nargs* pointers to
       behaved memory. Pointers to the data for the input arguments
       are first, followed by the pointers to the data for the output
       arguments. How many bytes must be skipped to get to the next
       element in the sequence is specified by the corresponding entry
       in the *steps* array. The last argument allows the loop to
       receive extra information.  This is commonly used so that a
       single, generic vector loop can be used for multiple
       functions. In this case, the actual scalar function to call is
       passed in as *extradata*. The size of this function pointer
       array is ntypes.

   .. c:member:: void **data

       Extra data to be passed to the 1-d vector loops or ``NULL`` if
       no extra-data is needed. This C-array must be the same size (
       *i.e.* ntypes) as the functions array. ``NULL`` is used if
       extra_data is not needed. Several C-API calls for UFuncs are
       just 1-d vector loops that make use of this extra data to
       receive a pointer to the actual function to call.

   .. c:member:: int ntypes

       The number of supported data types for the ufunc. This number
       specifies how many different 1-d loops (of the builtin data
       types) are available.

   .. c:member:: int reserved1

       Unused.

   .. c:member:: char *name

       A string name for the ufunc. This is used dynamically to build
       the __doc\__ attribute of ufuncs.

   .. c:member:: char *types

       An array of :math:`nargs \times ntypes` 8-bit type_numbers
       which contains the type signature for the function for each of
       the supported (builtin) data types. For each of the *ntypes*
       functions, the corresponding set of type numbers in this array
       shows how the *args* argument should be interpreted in the 1-d
       vector loop. These type numbers do not have to be the same type
       and mixed-type ufuncs are supported.

   .. c:member:: char *doc

       Documentation for the ufunc. Should not contain the function
       signature as this is generated dynamically when __doc\__ is
       retrieved.

   .. c:member:: void *ptr

       Any dynamically allocated memory. Currently, this is used for
       dynamic ufuncs created from a python function to store room for
       the types, data, and name members.

   .. c:member:: PyObject *obj

       For ufuncs dynamically created from python functions, this member
       holds a reference to the underlying Python function.

   .. c:member:: PyObject *userloops

       A dictionary of user-defined 1-d vector loops (stored as CObject
       ptrs) for user-defined types. A loop may be registered by the
       user for any user-defined type. It is retrieved by type number.
       User defined type numbers are always larger than
       :c:data:`NPY_USERDEF`.

   .. c:member:: int core_enabled

       0 for scalar ufuncs; 1 for generalized ufuncs

   .. c:member:: int core_num_dim_ix

       Number of distinct core dimension names in the signature

   .. c:member:: int *core_num_dims

       Number of core dimensions of each argument

   .. c:member:: int *core_dim_ixs

       Dimension indices in a flattened form; indices of argument ``k`` are
       stored in ``core_dim_ixs[core_offsets[k] : core_offsets[k] +
       core_numdims[k]]``

   .. c:member:: int *core_offsets

       Position of 1st core dimension of each argument in ``core_dim_ixs``,
       equivalent to cumsum(``core_num_dims``)

   .. c:member:: char *core_signature

       Core signature string

   .. c:member:: PyUFunc_TypeResolutionFunc *type_resolver

       A function which resolves the types and fills an array with the dtypes
       for the inputs and outputs

   .. c:member:: PyUFunc_LegacyInnerLoopSelectionFunc *legacy_inner_loop_selector

       .. deprecated:: 1.22

            Some fallback support for this slot exists, but will be removed
            eventually.  A universal function that relied on this will
            have to be ported eventually.
            See ref:`NEP 41 <NEP41>` and ref:`NEP 43 <NEP43>`

   .. c:member:: void *reserved2

       For a possible future loop selector with a different signature.

   .. c:member:: npy_uint32 op_flags

       Override the default operand flags for each ufunc operand.

   .. c:member:: npy_uint32 iter_flags

       Override the default nditer flags for the ufunc.

   Added in API version 0x0000000D

   .. c:member:: npy_intp *core_dim_sizes

       For each distinct core dimension, the possible
       :ref:`frozen <frozen>` size if
       :c:data:`UFUNC_CORE_DIM_SIZE_INFERRED` is ``0``

   .. c:member:: npy_uint32 *core_dim_flags

       For each distinct core dimension, a set of ``UFUNC_CORE_DIM*`` flags

..
  dedented to allow internal linking, pending a refactoring

.. c:macro:: UFUNC_CORE_DIM_CAN_IGNORE

    if the dim name ends in ``?``

.. c:macro:: UFUNC_CORE_DIM_SIZE_INFERRED

    if the dim size will be determined from the operands
    and not from a :ref:`frozen <frozen>` signature

   .. c:member:: PyObject *identity_value

       Identity for reduction, when :c:member:`PyUFuncObject.identity`
       is equal to :c:data:`PyUFunc_IdentityValue`.

PyArrayIter_Type and PyArrayIterObject
--------------------------------------

.. c:var:: PyTypeObject PyArrayIter_Type

   This is an iterator object that makes it easy to loop over an
   N-dimensional array. It is the object returned from the flat
   attribute of an ndarray. It is also used extensively throughout the
   implementation internals to loop over an N-dimensional array. The
   tp_as_mapping interface is implemented so that the iterator object
   can be indexed (using 1-d indexing), and a few methods are
   implemented through the tp_methods table. This object implements the
   next method and can be used anywhere an iterator can be used in
   Python.

.. c:type:: PyArrayIterObject

   The C-structure corresponding to an object of :c:data:`PyArrayIter_Type` is
   the :c:type:`PyArrayIterObject`. The :c:type:`PyArrayIterObject` is used to
   keep track of a pointer into an N-dimensional array. It contains associated
   information used to quickly march through the array. The pointer can
   be adjusted in three basic ways: 1) advance to the "next" position in
   the array in a C-style contiguous fashion, 2) advance to an arbitrary
   N-dimensional coordinate in the array, and 3) advance to an arbitrary
   one-dimensional index into the array. The members of the
   :c:type:`PyArrayIterObject` structure are used in these
   calculations. Iterator objects keep their own dimension and strides
   information about an array. This can be adjusted as needed for
   "broadcasting," or to loop over only specific dimensions.

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          int   nd_m1;
          npy_intp  index;
          npy_intp  size;
          npy_intp  coordinates[NPY_MAXDIMS];
          npy_intp  dims_m1[NPY_MAXDIMS];
          npy_intp  strides[NPY_MAXDIMS];
          npy_intp  backstrides[NPY_MAXDIMS];
          npy_intp  factors[NPY_MAXDIMS];
          PyArrayObject *ao;
          char  *dataptr;
          npy_bool  contiguous;
      } PyArrayIterObject;

   .. c:member:: int nd_m1

       :math:`N-1` where :math:`N` is the number of dimensions in the
       underlying array.

   .. c:member:: npy_intp index

       The current 1-d index into the array.

   .. c:member:: npy_intp size

       The total size of the underlying array.

   .. c:member:: npy_intp *coordinates

       An :math:`N` -dimensional index into the array.

   .. c:member:: npy_intp *dims_m1

       The size of the array minus 1 in each dimension.

   .. c:member:: npy_intp *strides

       The strides of the array. How many bytes needed to jump to the next
       element in each dimension.

   .. c:member:: npy_intp *backstrides

       How many bytes needed to jump from the end of a dimension back
       to its beginning. Note that ``backstrides[k] == strides[k] *
       dims_m1[k]``, but it is stored here as an optimization.

   .. c:member:: npy_intp *factors

       This array is used in computing an N-d index from a 1-d index. It
       contains needed products of the dimensions.

   .. c:member:: PyArrayObject *ao

       A pointer to the underlying ndarray this iterator was created to
       represent.

   .. c:member:: char *dataptr

       This member points to an element in the ndarray indicated by the
       index.

   .. c:member:: npy_bool contiguous

       This flag is true if the underlying array is
       :c:data:`NPY_ARRAY_C_CONTIGUOUS`. It is used to simplify
       calculations when possible.


How to use an array iterator on a C-level is explained more fully in
later sections. Typically, you do not need to concern yourself with
the internal structure of the iterator object, and merely interact
with it through the use of the macros :c:func:`PyArray_ITER_NEXT` (it),
:c:func:`PyArray_ITER_GOTO` (it, dest), or :c:func:`PyArray_ITER_GOTO1D`
(it, index). All of these macros require the argument *it* to be a
:c:expr:`PyArrayIterObject *`.


PyArrayMultiIter_Type and PyArrayMultiIterObject
------------------------------------------------

.. c:var:: PyTypeObject PyArrayMultiIter_Type

   This type provides an iterator that encapsulates the concept of
   broadcasting. It allows :math:`N` arrays to be broadcast together
   so that the loop progresses in C-style contiguous fashion over the
   broadcasted array. The corresponding C-structure is the
   :c:type:`PyArrayMultiIterObject` whose memory layout must begin any
   object, *obj*, passed in to the :c:func:`PyArray_Broadcast` (obj)
   function. Broadcasting is performed by adjusting array iterators so
   that each iterator represents the broadcasted shape and size, but
   has its strides adjusted so that the correct element from the array
   is used at each iteration.


.. c:type:: PyArrayMultiIterObject

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          int numiter;
          npy_intp size;
          npy_intp index;
          int nd;
          npy_intp dimensions[NPY_MAXDIMS];
          PyArrayIterObject *iters[NPY_MAXDIMS];
      } PyArrayMultiIterObject;

   .. c:macro: PyObject_HEAD

       Needed at the start of every Python object (holds reference count
       and type identification).

   .. c:member:: int numiter

       The number of arrays that need to be broadcast to the same shape.

   .. c:member:: npy_intp size

       The total broadcasted size.

   .. c:member:: npy_intp index

       The current (1-d) index into the broadcasted result.

   .. c:member:: int nd

       The number of dimensions in the broadcasted result.

   .. c:member:: npy_intp *dimensions

       The shape of the broadcasted result (only ``nd`` slots are used).

   .. c:member:: PyArrayIterObject **iters

       An array of iterator objects that holds the iterators for the
       arrays to be broadcast together. On return, the iterators are
       adjusted for broadcasting.

PyArrayNeighborhoodIter_Type and PyArrayNeighborhoodIterObject
--------------------------------------------------------------

.. c:var:: PyTypeObject PyArrayNeighborhoodIter_Type

   This is an iterator object that makes it easy to loop over an
   N-dimensional neighborhood.

.. c:type:: PyArrayNeighborhoodIterObject

   The C-structure corresponding to an object of
   :c:data:`PyArrayNeighborhoodIter_Type` is the
   :c:type:`PyArrayNeighborhoodIterObject`.

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          int nd_m1;
          npy_intp index, size;
          npy_intp coordinates[NPY_MAXDIMS]
          npy_intp dims_m1[NPY_MAXDIMS];
          npy_intp strides[NPY_MAXDIMS];
          npy_intp backstrides[NPY_MAXDIMS];
          npy_intp factors[NPY_MAXDIMS];
          PyArrayObject *ao;
          char *dataptr;
          npy_bool contiguous;
          npy_intp bounds[NPY_MAXDIMS][2];
          npy_intp limits[NPY_MAXDIMS][2];
          npy_intp limits_sizes[NPY_MAXDIMS];
          npy_iter_get_dataptr_t translate;
          npy_intp nd;
          npy_intp dimensions[NPY_MAXDIMS];
          PyArrayIterObject* _internal_iter;
          char* constant;
          int mode;
      } PyArrayNeighborhoodIterObject;

PyArrayFlags_Type and PyArrayFlagsObject
----------------------------------------

.. c:var:: PyTypeObject PyArrayFlags_Type

   When the flags attribute is retrieved from Python, a special
   builtin object of this type is constructed. This special type makes
   it easier to work with the different flags by accessing them as
   attributes or by accessing them as if the object were a dictionary
   with the flag names as entries.

.. c:type:: PyArrayFlagsObject

   .. code-block:: c

      typedef struct PyArrayFlagsObject {
              PyObject_HEAD
              PyObject *arr;
              int flags;
      } PyArrayFlagsObject;


ScalarArrayTypes
----------------

There is a Python type for each of the different built-in data types
that can be present in the array Most of these are simple wrappers
around the corresponding data type in C. The C-names for these types
are ``Py{TYPE}ArrType_Type`` where ``{TYPE}`` can be

    **Bool**, **Byte**, **Short**, **Int**, **Long**, **LongLong**,
    **UByte**, **UShort**, **UInt**, **ULong**, **ULongLong**,
    **Half**, **Float**, **Double**, **LongDouble**, **CFloat**,
    **CDouble**, **CLongDouble**, **String**, **Unicode**, **Void**, and
    **Object**.

These type names are part of the C-API and can therefore be created in
extension C-code. There is also a ``PyIntpArrType_Type`` and a
``PyUIntpArrType_Type`` that are simple substitutes for one of the
integer types that can hold a pointer on the platform. The structure
of these scalar objects is not exposed to C-code. The function
:c:func:`PyArray_ScalarAsCtype` (..) can be used to extract the C-type
value from the array scalar and the function :c:func:`PyArray_Scalar`
(...) can be used to construct an array scalar from a C-value.


Other C-Structures
==================

A few new C-structures were found to be useful in the development of
NumPy. These C-structures are used in at least one C-API call and are
therefore documented here. The main reason these structures were
defined is to make it easy to use the Python ParseTuple C-API to
convert from Python objects to a useful C-Object.


PyArray_Dims
------------

.. c:type:: PyArray_Dims

   This structure is very useful when shape and/or strides information
   is supposed to be interpreted. The structure is:

   .. code-block:: c

      typedef struct {
          npy_intp *ptr;
          int len;
      } PyArray_Dims;

   The members of this structure are

   .. c:member:: npy_intp *ptr

       A pointer to a list of (:c:type:`npy_intp`) integers which
       usually represent array shape or array strides.

   .. c:member:: int len

       The length of the list of integers. It is assumed safe to
       access *ptr* [0] to *ptr* [len-1].


PyArray_Chunk
-------------

.. c:type:: PyArray_Chunk

   This is equivalent to the buffer object structure in Python up to
   the ptr member. On 32-bit platforms (*i.e.* if :c:data:`NPY_SIZEOF_INT`
   == :c:data:`NPY_SIZEOF_INTP`), the len member also matches an equivalent
   member of the buffer object. It is useful to represent a generic
   single-segment chunk of memory.

   .. code-block:: c

      typedef struct {
          PyObject_HEAD
          PyObject *base;
          void *ptr;
          npy_intp len;
          int flags;
      } PyArray_Chunk;

   The members are

   .. c:macro: PyObject_HEAD

       Necessary for all Python objects. Included here so that the
       :c:type:`PyArray_Chunk` structure matches that of the buffer object
       (at least to the len member).

   .. c:member:: PyObject *base

       The Python object this chunk of memory comes from. Needed so that
       memory can be accounted for properly.

   .. c:member:: void *ptr

       A pointer to the start of the single-segment chunk of memory.

   .. c:member:: npy_intp len

       The length of the segment in bytes.

   .. c:member:: int flags

       Any data flags (*e.g.* :c:data:`NPY_ARRAY_WRITEABLE` ) that should
       be used to interpret the memory.


PyArrayInterface
----------------

.. seealso:: :ref:`arrays.interface`

.. c:type:: PyArrayInterface

   The :c:type:`PyArrayInterface` structure is defined so that NumPy and
   other extension modules can use the rapid array interface
   protocol. The :obj:`~object.__array_struct__` method of an object that
   supports the rapid array interface protocol should return a
   :c:type:`PyCapsule` that contains a pointer to a :c:type:`PyArrayInterface`
   structure with the relevant details of the array. After the new
   array is created, the attribute should be ``DECREF``'d which will
   free the :c:type:`PyArrayInterface` structure. Remember to ``INCREF`` the
   object (whose :obj:`~object.__array_struct__` attribute was retrieved) and
   point the base member of the new :c:type:`PyArrayObject` to this same
   object. In this way the memory for the array will be managed
   correctly.

   .. code-block:: c

      typedef struct {
          int two;
          int nd;
          char typekind;
          int itemsize;
          int flags;
          npy_intp *shape;
          npy_intp *strides;
          void *data;
          PyObject *descr;
      } PyArrayInterface;

   .. c:member:: int two

       the integer 2 as a sanity check.

   .. c:member:: int nd

       the number of dimensions in the array.

   .. c:member:: char typekind

       A character indicating what kind of array is present according to the
       typestring convention with 't' -> bitfield, 'b' -> Boolean, 'i' ->
       signed integer, 'u' -> unsigned integer, 'f' -> floating point, 'c' ->
       complex floating point, 'O' -> object, 'S' -> (byte-)string, 'U' ->
       unicode, 'V' -> void.

   .. c:member:: int itemsize

       The number of bytes each item in the array requires.

   .. c:member:: int flags

       Any of the bits :c:data:`NPY_ARRAY_C_CONTIGUOUS` (1),
       :c:data:`NPY_ARRAY_F_CONTIGUOUS` (2), :c:data:`NPY_ARRAY_ALIGNED` (0x100),
       :c:data:`NPY_ARRAY_NOTSWAPPED` (0x200), or :c:data:`NPY_ARRAY_WRITEABLE`
       (0x400) to indicate something about the data. The
       :c:data:`NPY_ARRAY_ALIGNED`, :c:data:`NPY_ARRAY_C_CONTIGUOUS`, and
       :c:data:`NPY_ARRAY_F_CONTIGUOUS` flags can actually be determined from
       the other parameters. The flag :c:data:`NPY_ARR_HAS_DESCR`
       (0x800) can also be set to indicate to objects consuming the
       version 3 array interface that the descr member of the
       structure is present (it will be ignored by objects consuming
       version 2 of the array interface).

   .. c:member:: npy_intp *shape

       An array containing the size of the array in each dimension.

   .. c:member:: npy_intp *strides

       An array containing the number of bytes to jump to get to the next
       element in each dimension.

   .. c:member:: void *data

       A pointer *to* the first element of the array.

   .. c:member:: PyObject *descr

       A Python object describing the data-type in more detail (same
       as the *descr* key in :obj:`~object.__array_interface__`). This can be
       ``NULL`` if *typekind* and *itemsize* provide enough
       information. This field is also ignored unless
       :c:data:`NPY_ARR_HAS_DESCR` flag is on in *flags*.


Internally used structures
--------------------------

Internally, the code uses some additional Python objects primarily for
memory management. These types are not accessible directly from
Python, and are not exposed to the C-API. They are included here only
for completeness and assistance in understanding the code.


.. c:type:: PyUFuncLoopObject

   A loose wrapper for a C-structure that contains the information
   needed for looping. This is useful if you are trying to understand
   the ufunc looping code. The :c:type:`PyUFuncLoopObject` is the associated
   C-structure. It is defined in the ``ufuncobject.h`` header.

.. c:type:: PyUFuncReduceObject

   A loose wrapper for the C-structure that contains the information
   needed for reduce-like methods of ufuncs. This is useful if you are
   trying to understand the reduce, accumulate, and reduce-at
   code. The :c:type:`PyUFuncReduceObject` is the associated C-structure. It
   is defined in the ``ufuncobject.h`` header.

.. c:type:: PyUFunc_Loop1d

   A simple linked-list of C-structures containing the information needed
   to define a 1-d loop for a ufunc for every defined signature of a
   user-defined data-type.

.. c:var:: PyTypeObject PyArrayMapIter_Type

   Advanced indexing is handled with this Python type. It is simply a
   loose wrapper around the C-structure containing the variables
   needed for advanced array indexing. The associated C-structure,
   ``PyArrayMapIterObject``, is useful if you are trying to
   understand the advanced-index mapping code. It is defined in the
   ``arrayobject.h`` header. This type is not exposed to Python and
   could be replaced with a C-structure. As a Python type it takes
   advantage of reference- counted memory management.
Array Iterator API
==================

.. sectionauthor:: Mark Wiebe

.. index::
   pair: iterator; C-API
   pair: C-API; iterator

.. versionadded:: 1.6

Array Iterator
--------------

The array iterator encapsulates many of the key features in ufuncs,
allowing user code to support features like output parameters,
preservation of memory layouts, and buffering of data with the wrong
alignment or type, without requiring difficult coding.

This page documents the API for the iterator.
The iterator is named ``NpyIter`` and functions are
named ``NpyIter_*``.

There is an :ref:`introductory guide to array iteration <arrays.nditer>`
which may be of interest for those using this C API. In many instances,
testing out ideas by creating the iterator in Python is a good idea
before writing the C iteration code.

Simple Iteration Example
------------------------

The best way to become familiar with the iterator is to look at its
usage within the NumPy codebase itself. For example, here is a slightly
tweaked version of the code for :c:func:`PyArray_CountNonzero`, which counts the
number of non-zero elements in an array.

.. code-block:: c

    npy_intp PyArray_CountNonzero(PyArrayObject* self)
    {
        /* Nonzero boolean function */
        PyArray_NonzeroFunc* nonzero = PyArray_DESCR(self)->f->nonzero;

        NpyIter* iter;
        NpyIter_IterNextFunc *iternext;
        char** dataptr;
        npy_intp nonzero_count;
        npy_intp* strideptr,* innersizeptr;

        /* Handle zero-sized arrays specially */
        if (PyArray_SIZE(self) == 0) {
            return 0;
        }

        /*
         * Create and use an iterator to count the nonzeros.
         *   flag NPY_ITER_READONLY
         *     - The array is never written to.
         *   flag NPY_ITER_EXTERNAL_LOOP
         *     - Inner loop is done outside the iterator for efficiency.
         *   flag NPY_ITER_NPY_ITER_REFS_OK
         *     - Reference types are acceptable.
         *   order NPY_KEEPORDER
         *     - Visit elements in memory order, regardless of strides.
         *       This is good for performance when the specific order
         *       elements are visited is unimportant.
         *   casting NPY_NO_CASTING
         *     - No casting is required for this operation.
         */
        iter = NpyIter_New(self, NPY_ITER_READONLY|
                                 NPY_ITER_EXTERNAL_LOOP|
                                 NPY_ITER_REFS_OK,
                            NPY_KEEPORDER, NPY_NO_CASTING,
                            NULL);
        if (iter == NULL) {
            return -1;
        }

        /*
         * The iternext function gets stored in a local variable
         * so it can be called repeatedly in an efficient manner.
         */
        iternext = NpyIter_GetIterNext(iter, NULL);
        if (iternext == NULL) {
            NpyIter_Deallocate(iter);
            return -1;
        }
        /* The location of the data pointer which the iterator may update */
        dataptr = NpyIter_GetDataPtrArray(iter);
        /* The location of the stride which the iterator may update */
        strideptr = NpyIter_GetInnerStrideArray(iter);
        /* The location of the inner loop size which the iterator may update */
        innersizeptr = NpyIter_GetInnerLoopSizePtr(iter);

        nonzero_count = 0;
        do {
            /* Get the inner loop data/stride/count values */
            char* data = *dataptr;
            npy_intp stride = *strideptr;
            npy_intp count = *innersizeptr;

            /* This is a typical inner loop for NPY_ITER_EXTERNAL_LOOP */
            while (count--) {
                if (nonzero(data, self)) {
                    ++nonzero_count;
                }
                data += stride;
            }

            /* Increment the iterator to the next inner loop */
        } while(iternext(iter));

        NpyIter_Deallocate(iter);

        return nonzero_count;
    }

Simple Multi-Iteration Example
------------------------------

Here is a simple copy function using the iterator.  The ``order`` parameter
is used to control the memory layout of the allocated result, typically
:c:data:`NPY_KEEPORDER` is desired.

.. code-block:: c

    PyObject *CopyArray(PyObject *arr, NPY_ORDER order)
    {
        NpyIter *iter;
        NpyIter_IterNextFunc *iternext;
        PyObject *op[2], *ret;
        npy_uint32 flags;
        npy_uint32 op_flags[2];
        npy_intp itemsize, *innersizeptr, innerstride;
        char **dataptrarray;

        /*
         * No inner iteration - inner loop is handled by CopyArray code
         */
        flags = NPY_ITER_EXTERNAL_LOOP;
        /*
         * Tell the constructor to automatically allocate the output.
         * The data type of the output will match that of the input.
         */
        op[0] = arr;
        op[1] = NULL;
        op_flags[0] = NPY_ITER_READONLY;
        op_flags[1] = NPY_ITER_WRITEONLY | NPY_ITER_ALLOCATE;

        /* Construct the iterator */
        iter = NpyIter_MultiNew(2, op, flags, order, NPY_NO_CASTING,
                                op_flags, NULL);
        if (iter == NULL) {
            return NULL;
        }

        /*
         * Make a copy of the iternext function pointer and
         * a few other variables the inner loop needs.
         */
        iternext = NpyIter_GetIterNext(iter, NULL);
        innerstride = NpyIter_GetInnerStrideArray(iter)[0];
        itemsize = NpyIter_GetDescrArray(iter)[0]->elsize;
        /*
         * The inner loop size and data pointers may change during the
         * loop, so just cache the addresses.
         */
        innersizeptr = NpyIter_GetInnerLoopSizePtr(iter);
        dataptrarray = NpyIter_GetDataPtrArray(iter);

        /*
         * Note that because the iterator allocated the output,
         * it matches the iteration order and is packed tightly,
         * so we don't need to check it like the input.
         */
        if (innerstride == itemsize) {
            do {
                memcpy(dataptrarray[1], dataptrarray[0],
                                        itemsize * (*innersizeptr));
            } while (iternext(iter));
        } else {
            /* For efficiency, should specialize this based on item size... */
            npy_intp i;
            do {
                npy_intp size = *innersizeptr;
                char *src = dataptrarray[0], *dst = dataptrarray[1];
                for(i = 0; i < size; i++, src += innerstride, dst += itemsize) {
                    memcpy(dst, src, itemsize);
                }
            } while (iternext(iter));
        }

        /* Get the result from the iterator object array */
        ret = NpyIter_GetOperandArray(iter)[1];
        Py_INCREF(ret);

        if (NpyIter_Deallocate(iter) != NPY_SUCCEED) {
            Py_DECREF(ret);
            return NULL;
        }

        return ret;
    }


Iterator Data Types
---------------------

The iterator layout is an internal detail, and user code only sees
an incomplete struct.

.. c:type:: NpyIter

    This is an opaque pointer type for the iterator. Access to its contents
    can only be done through the iterator API.

.. c:type:: NpyIter_Type

   This is the type which exposes the iterator to Python. Currently, no
   API is exposed which provides access to the values of a Python-created
   iterator. If an iterator is created in Python, it must be used in Python
   and vice versa. Such an API will likely be created in a future version.

.. c:type:: NpyIter_IterNextFunc

   This is a function pointer for the iteration loop, returned by
   :c:func:`NpyIter_GetIterNext`.

.. c:type:: NpyIter_GetMultiIndexFunc

   This is a function pointer for getting the current iterator multi-index,
   returned by :c:func:`NpyIter_GetGetMultiIndex`.

Construction and Destruction
----------------------------

.. c:function:: NpyIter* NpyIter_New( \
        PyArrayObject* op, npy_uint32 flags, NPY_ORDER order, \
        NPY_CASTING casting, PyArray_Descr* dtype)

    Creates an iterator for the given numpy array object ``op``.

    Flags that may be passed in ``flags`` are any combination
    of the global and per-operand flags documented in
    :c:func:`NpyIter_MultiNew`, except for :c:data:`NPY_ITER_ALLOCATE`.

    Any of the :c:type:`NPY_ORDER` enum values may be passed to ``order``.  For
    efficient iteration, :c:type:`NPY_KEEPORDER` is the best option, and
    the other orders enforce the particular iteration pattern.

    Any of the :c:type:`NPY_CASTING` enum values may be passed to ``casting``.
    The values include :c:data:`NPY_NO_CASTING`, :c:data:`NPY_EQUIV_CASTING`,
    :c:data:`NPY_SAFE_CASTING`, :c:data:`NPY_SAME_KIND_CASTING`, and
    :c:data:`NPY_UNSAFE_CASTING`.  To allow the casts to occur, copying or
    buffering must also be enabled.

    If ``dtype`` isn't ``NULL``, then it requires that data type.
    If copying is allowed, it will make a temporary copy if the data
    is castable.  If :c:data:`NPY_ITER_UPDATEIFCOPY` is enabled, it will
    also copy the data back with another cast upon iterator destruction.

    Returns NULL if there is an error, otherwise returns the allocated
    iterator.

    To make an iterator similar to the old iterator, this should work.

    .. code-block:: c

        iter = NpyIter_New(op, NPY_ITER_READWRITE,
                            NPY_CORDER, NPY_NO_CASTING, NULL);

    If you want to edit an array with aligned ``double`` code,
    but the order doesn't matter, you would use this.

    .. code-block:: c

        dtype = PyArray_DescrFromType(NPY_DOUBLE);
        iter = NpyIter_New(op, NPY_ITER_READWRITE|
                            NPY_ITER_BUFFERED|
                            NPY_ITER_NBO|
                            NPY_ITER_ALIGNED,
                            NPY_KEEPORDER,
                            NPY_SAME_KIND_CASTING,
                            dtype);
        Py_DECREF(dtype);

.. c:function:: NpyIter* NpyIter_MultiNew( \
        npy_intp nop, PyArrayObject** op, npy_uint32 flags, NPY_ORDER order, \
        NPY_CASTING casting, npy_uint32* op_flags, PyArray_Descr** op_dtypes)

    Creates an iterator for broadcasting the ``nop`` array objects provided
    in ``op``, using regular NumPy broadcasting rules.

    Any of the :c:type:`NPY_ORDER` enum values may be passed to ``order``.  For
    efficient iteration, :c:data:`NPY_KEEPORDER` is the best option, and the
    other orders enforce the particular iteration pattern.  When using
    :c:data:`NPY_KEEPORDER`, if you also want to ensure that the iteration is
    not reversed along an axis, you should pass the flag
    :c:data:`NPY_ITER_DONT_NEGATE_STRIDES`.

    Any of the :c:type:`NPY_CASTING` enum values may be passed to ``casting``.
    The values include :c:data:`NPY_NO_CASTING`, :c:data:`NPY_EQUIV_CASTING`,
    :c:data:`NPY_SAFE_CASTING`, :c:data:`NPY_SAME_KIND_CASTING`, and
    :c:data:`NPY_UNSAFE_CASTING`.  To allow the casts to occur, copying or
    buffering must also be enabled.

    If ``op_dtypes`` isn't ``NULL``, it specifies a data type or ``NULL``
    for each ``op[i]``.

    Returns NULL if there is an error, otherwise returns the allocated
    iterator.

    Flags that may be passed in ``flags``, applying to the whole
    iterator, are:
..
    dedent the enumeration of flags to avoid missing references sphinx warnings 

.. c:macro:: NPY_ITER_C_INDEX

    Causes the iterator to track a raveled flat index matching C
    order. This option cannot be used with :c:data:`NPY_ITER_F_INDEX`.

.. c:macro:: NPY_ITER_F_INDEX

    Causes the iterator to track a raveled flat index matching Fortran
    order. This option cannot be used with :c:data:`NPY_ITER_C_INDEX`.

.. c:macro:: NPY_ITER_MULTI_INDEX

    Causes the iterator to track a multi-index.
    This prevents the iterator from coalescing axes to
    produce bigger inner loops. If the loop is also not buffered
    and no index is being tracked (`NpyIter_RemoveAxis` can be called),
    then the iterator size can be ``-1`` to indicate that the iterator
    is too large. This can happen due to complex broadcasting and
    will result in errors being created when the setting the iterator
    range, removing the multi index, or getting the next function.
    However, it is possible to remove axes again and use the iterator
    normally if the size is small enough after removal.

.. c:macro:: NPY_ITER_EXTERNAL_LOOP

    Causes the iterator to skip iteration of the innermost
    loop, requiring the user of the iterator to handle it.

    This flag is incompatible with :c:data:`NPY_ITER_C_INDEX`,
    :c:data:`NPY_ITER_F_INDEX`, and :c:data:`NPY_ITER_MULTI_INDEX`.

.. c:macro:: NPY_ITER_DONT_NEGATE_STRIDES

    This only affects the iterator when :c:type:`NPY_KEEPORDER` is
    specified for the order parameter.  By default with
    :c:type:`NPY_KEEPORDER`, the iterator reverses axes which have
    negative strides, so that memory is traversed in a forward
    direction.  This disables this step.  Use this flag if you
    want to use the underlying memory-ordering of the axes,
    but don't want an axis reversed. This is the behavior of
    ``numpy.ravel(a, order='K')``, for instance.

.. c:macro:: NPY_ITER_COMMON_DTYPE

    Causes the iterator to convert all the operands to a common
    data type, calculated based on the ufunc type promotion rules.
    Copying or buffering must be enabled.

    If the common data type is known ahead of time, don't use this
    flag.  Instead, set the requested dtype for all the operands.

.. c:macro:: NPY_ITER_REFS_OK

    Indicates that arrays with reference types (object
    arrays or structured arrays containing an object type)
    may be accepted and used in the iterator.  If this flag
    is enabled, the caller must be sure to check whether
    :c:expr:`NpyIter_IterationNeedsAPI(iter)` is true, in which case
    it may not release the GIL during iteration.

.. c:macro:: NPY_ITER_ZEROSIZE_OK

    Indicates that arrays with a size of zero should be permitted.
    Since the typical iteration loop does not naturally work with
    zero-sized arrays, you must check that the IterSize is larger
    than zero before entering the iteration loop.
    Currently only the operands are checked, not a forced shape.

.. c:macro:: NPY_ITER_REDUCE_OK

    Permits writeable operands with a dimension with zero
    stride and size greater than one.  Note that such operands
    must be read/write.

    When buffering is enabled, this also switches to a special
    buffering mode which reduces the loop length as necessary to
    not trample on values being reduced.

    Note that if you want to do a reduction on an automatically
    allocated output, you must use :c:func:`NpyIter_GetOperandArray`
    to get its reference, then set every value to the reduction
    unit before doing the iteration loop.  In the case of a
    buffered reduction, this means you must also specify the
    flag :c:data:`NPY_ITER_DELAY_BUFALLOC`, then reset the iterator
    after initializing the allocated operand to prepare the
    buffers.

.. c:macro:: NPY_ITER_RANGED

    Enables support for iteration of sub-ranges of the full
    ``iterindex`` range ``[0, NpyIter_IterSize(iter))``.  Use
    the function :c:func:`NpyIter_ResetToIterIndexRange` to specify
    a range for iteration.

    This flag can only be used with :c:data:`NPY_ITER_EXTERNAL_LOOP`
    when :c:data:`NPY_ITER_BUFFERED` is enabled.  This is because
    without buffering, the inner loop is always the size of the
    innermost iteration dimension, and allowing it to get cut up
    would require special handling, effectively making it more
    like the buffered version.

.. c:macro:: NPY_ITER_BUFFERED

    Causes the iterator to store buffering data, and use buffering
    to satisfy data type, alignment, and byte-order requirements.
    To buffer an operand, do not specify the :c:data:`NPY_ITER_COPY`
    or :c:data:`NPY_ITER_UPDATEIFCOPY` flags, because they will
    override buffering.  Buffering is especially useful for Python
    code using the iterator, allowing for larger chunks
    of data at once to amortize the Python interpreter overhead.

    If used with :c:data:`NPY_ITER_EXTERNAL_LOOP`, the inner loop
    for the caller may get larger chunks than would be possible
    without buffering, because of how the strides are laid out.

    Note that if an operand is given the flag :c:data:`NPY_ITER_COPY`
    or :c:data:`NPY_ITER_UPDATEIFCOPY`, a copy will be made in preference
    to buffering.  Buffering will still occur when the array was
    broadcast so elements need to be duplicated to get a constant
    stride.

    In normal buffering, the size of each inner loop is equal
    to the buffer size, or possibly larger if
    :c:data:`NPY_ITER_GROWINNER` is specified.  If
    :c:data:`NPY_ITER_REDUCE_OK` is enabled and a reduction occurs,
    the inner loops may become smaller depending
    on the structure of the reduction.

.. c:macro:: NPY_ITER_GROWINNER

    When buffering is enabled, this allows the size of the inner
    loop to grow when buffering isn't necessary.  This option
    is best used if you're doing a straight pass through all the
    data, rather than anything with small cache-friendly arrays
    of temporary values for each inner loop.

.. c:macro:: NPY_ITER_DELAY_BUFALLOC

    When buffering is enabled, this delays allocation of the
    buffers until :c:func:`NpyIter_Reset` or another reset function is
    called.  This flag exists to avoid wasteful copying of
    buffer data when making multiple copies of a buffered
    iterator for multi-threaded iteration.

    Another use of this flag is for setting up reduction operations.
    After the iterator is created, and a reduction output
    is allocated automatically by the iterator (be sure to use
    READWRITE access), its value may be initialized to the reduction
    unit.  Use :c:func:`NpyIter_GetOperandArray` to get the object.
    Then, call :c:func:`NpyIter_Reset` to allocate and fill the buffers
    with their initial values.

.. c:macro:: NPY_ITER_COPY_IF_OVERLAP

    If any write operand has overlap with any read operand, eliminate all
    overlap by making temporary copies (enabling UPDATEIFCOPY for write
    operands, if necessary). A pair of operands has overlap if there is
    a memory address that contains data common to both arrays.

    Because exact overlap detection has exponential runtime
    in the number of dimensions, the decision is made based
    on heuristics, which has false positives (needless copies in unusual
    cases) but has no false negatives.

    If any read/write overlap exists, this flag ensures the result of the
    operation is the same as if all operands were copied.
    In cases where copies would need to be made, **the result of the
    computation may be undefined without this flag!**

    Flags that may be passed in ``op_flags[i]``, where ``0 <= i < nop``:
..
    dedent the enumeration of flags to avoid missing references sphinx warnings 

.. c:macro:: NPY_ITER_READWRITE
.. c:macro:: NPY_ITER_READONLY
.. c:macro:: NPY_ITER_WRITEONLY

    Indicate how the user of the iterator will read or write
    to ``op[i]``.  Exactly one of these flags must be specified
    per operand. Using ``NPY_ITER_READWRITE`` or ``NPY_ITER_WRITEONLY``
    for a user-provided operand may trigger `WRITEBACKIFCOPY``
    semantics. The data will be written back to the original array
    when ``NpyIter_Deallocate`` is called.

.. c:macro:: NPY_ITER_COPY

    Allow a copy of ``op[i]`` to be made if it does not
    meet the data type or alignment requirements as specified
    by the constructor flags and parameters.

.. c:macro:: NPY_ITER_UPDATEIFCOPY

    Triggers :c:data:`NPY_ITER_COPY`, and when an array operand
    is flagged for writing and is copied, causes the data
    in a copy to be copied back to ``op[i]`` when
    ``NpyIter_Deallocate`` is called.

    If the operand is flagged as write-only and a copy is needed,
    an uninitialized temporary array will be created and then copied
    to back to ``op[i]`` on calling ``NpyIter_Deallocate``, instead of
    doing the unnecessary copy operation.

.. c:macro:: NPY_ITER_NBO
.. c:macro:: NPY_ITER_ALIGNED
.. c:macro:: NPY_ITER_CONTIG

    Causes the iterator to provide data for ``op[i]``
    that is in native byte order, aligned according to
    the dtype requirements, contiguous, or any combination.

    By default, the iterator produces pointers into the
    arrays provided, which may be aligned or unaligned, and
    with any byte order.  If copying or buffering is not
    enabled and the operand data doesn't satisfy the constraints,
    an error will be raised.

    The contiguous constraint applies only to the inner loop,
    successive inner loops may have arbitrary pointer changes.

    If the requested data type is in non-native byte order,
    the NBO flag overrides it and the requested data type is
    converted to be in native byte order.

.. c:macro:: NPY_ITER_ALLOCATE

    This is for output arrays, and requires that the flag
    :c:data:`NPY_ITER_WRITEONLY` or :c:data:`NPY_ITER_READWRITE`
    be set.  If ``op[i]`` is NULL, creates a new array with
    the final broadcast dimensions, and a layout matching
    the iteration order of the iterator.

    When ``op[i]`` is NULL, the requested data type
    ``op_dtypes[i]`` may be NULL as well, in which case it is
    automatically generated from the dtypes of the arrays which
    are flagged as readable.  The rules for generating the dtype
    are the same is for UFuncs.  Of special note is handling
    of byte order in the selected dtype.  If there is exactly
    one input, the input's dtype is used as is.  Otherwise,
    if more than one input dtypes are combined together, the
    output will be in native byte order.

    After being allocated with this flag, the caller may retrieve
    the new array by calling :c:func:`NpyIter_GetOperandArray` and
    getting the i-th object in the returned C array.  The caller
    must call Py_INCREF on it to claim a reference to the array.

.. c:macro:: NPY_ITER_NO_SUBTYPE

    For use with :c:data:`NPY_ITER_ALLOCATE`, this flag disables
    allocating an array subtype for the output, forcing
    it to be a straight ndarray.

    TODO: Maybe it would be better to introduce a function
    ``NpyIter_GetWrappedOutput`` and remove this flag?

.. c:macro:: NPY_ITER_NO_BROADCAST

    Ensures that the input or output matches the iteration
    dimensions exactly.

.. c:macro:: NPY_ITER_ARRAYMASK

    .. versionadded:: 1.7

    Indicates that this operand is the mask to use for
    selecting elements when writing to operands which have
    the :c:data:`NPY_ITER_WRITEMASKED` flag applied to them.
    Only one operand may have :c:data:`NPY_ITER_ARRAYMASK` flag
    applied to it.

    The data type of an operand with this flag should be either
    :c:data:`NPY_BOOL`, :c:data:`NPY_MASK`, or a struct dtype
    whose fields are all valid mask dtypes. In the latter case,
    it must match up with a struct operand being WRITEMASKED,
    as it is specifying a mask for each field of that array.

    This flag only affects writing from the buffer back to
    the array. This means that if the operand is also
    :c:data:`NPY_ITER_READWRITE` or :c:data:`NPY_ITER_WRITEONLY`,
    code doing iteration can write to this operand to
    control which elements will be untouched and which ones will be
    modified. This is useful when the mask should be a combination
    of input masks.

.. c:macro:: NPY_ITER_WRITEMASKED

    .. versionadded:: 1.7

    This array is the mask for all `writemasked <numpy.nditer>`
    operands. Code uses the ``writemasked`` flag which indicates 
    that only elements where the chosen ARRAYMASK operand is True
    will be written to. In general, the iterator does not enforce
    this, it is up to the code doing the iteration to follow that
    promise.

    When ``writemasked`` flag is used, and this operand is buffered,
    this changes how data is copied from the buffer into the array.
    A masked copying routine is used, which only copies the
    elements in the buffer for which ``writemasked``
    returns true from the corresponding element in the ARRAYMASK
    operand.

.. c:macro:: NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE

    In memory overlap checks, assume that operands with
    ``NPY_ITER_OVERLAP_ASSUME_ELEMENTWISE`` enabled are accessed only
    in the iterator order.

    This enables the iterator to reason about data dependency,
    possibly avoiding unnecessary copies.

    This flag has effect only if ``NPY_ITER_COPY_IF_OVERLAP`` is enabled
    on the iterator.

.. c:function:: NpyIter* NpyIter_AdvancedNew( \
        npy_intp nop, PyArrayObject** op, npy_uint32 flags, NPY_ORDER order, \
        NPY_CASTING casting, npy_uint32* op_flags, PyArray_Descr** op_dtypes, \
        int oa_ndim, int** op_axes, npy_intp const* itershape, npy_intp buffersize)

    Extends :c:func:`NpyIter_MultiNew` with several advanced options providing
    more control over broadcasting and buffering.

    If -1/NULL values are passed to ``oa_ndim``, ``op_axes``, ``itershape``,
    and ``buffersize``, it is equivalent to :c:func:`NpyIter_MultiNew`.

    The parameter ``oa_ndim``, when not zero or -1, specifies the number of
    dimensions that will be iterated with customized broadcasting.
    If it is provided, ``op_axes`` must and ``itershape`` can also be provided.
    The ``op_axes`` parameter let you control in detail how the
    axes of the operand arrays get matched together and iterated.
    In ``op_axes``, you must provide an array of ``nop`` pointers
    to ``oa_ndim``-sized arrays of type ``npy_intp``.  If an entry
    in ``op_axes`` is NULL, normal broadcasting rules will apply.
    In ``op_axes[j][i]`` is stored either a valid axis of ``op[j]``, or
    -1 which means ``newaxis``.  Within each ``op_axes[j]`` array, axes
    may not be repeated.  The following example is how normal broadcasting
    applies to a 3-D array, a 2-D array, a 1-D array and a scalar.

    **Note**: Before NumPy 1.8 ``oa_ndim == 0` was used for signalling
    that ``op_axes`` and ``itershape`` are unused. This is deprecated and
    should be replaced with -1. Better backward compatibility may be
    achieved by using :c:func:`NpyIter_MultiNew` for this case.

    .. code-block:: c

        int oa_ndim = 3;               /* # iteration axes */
        int op0_axes[] = {0, 1, 2};    /* 3-D operand */
        int op1_axes[] = {-1, 0, 1};   /* 2-D operand */
        int op2_axes[] = {-1, -1, 0};  /* 1-D operand */
        int op3_axes[] = {-1, -1, -1}  /* 0-D (scalar) operand */
        int* op_axes[] = {op0_axes, op1_axes, op2_axes, op3_axes};

    The ``itershape`` parameter allows you to force the iterator
    to have a specific iteration shape. It is an array of length
    ``oa_ndim``. When an entry is negative, its value is determined
    from the operands. This parameter allows automatically allocated
    outputs to get additional dimensions which don't match up with
    any dimension of an input.

    If ``buffersize`` is zero, a default buffer size is used,
    otherwise it specifies how big of a buffer to use.  Buffers
    which are powers of 2 such as 4096 or 8192 are recommended.

    Returns NULL if there is an error, otherwise returns the allocated
    iterator.

.. c:function:: NpyIter* NpyIter_Copy(NpyIter* iter)

    Makes a copy of the given iterator.  This function is provided
    primarily to enable multi-threaded iteration of the data.

    *TODO*: Move this to a section about multithreaded iteration.

    The recommended approach to multithreaded iteration is to
    first create an iterator with the flags
    :c:data:`NPY_ITER_EXTERNAL_LOOP`, :c:data:`NPY_ITER_RANGED`,
    :c:data:`NPY_ITER_BUFFERED`, :c:data:`NPY_ITER_DELAY_BUFALLOC`, and
    possibly :c:data:`NPY_ITER_GROWINNER`.  Create a copy of this iterator
    for each thread (minus one for the first iterator).  Then, take
    the iteration index range ``[0, NpyIter_GetIterSize(iter))`` and
    split it up into tasks, for example using a TBB parallel_for loop.
    When a thread gets a task to execute, it then uses its copy of
    the iterator by calling :c:func:`NpyIter_ResetToIterIndexRange` and
    iterating over the full range.

    When using the iterator in multi-threaded code or in code not
    holding the Python GIL, care must be taken to only call functions
    which are safe in that context.  :c:func:`NpyIter_Copy` cannot be safely
    called without the Python GIL, because it increments Python
    references.  The ``Reset*`` and some other functions may be safely
    called by passing in the ``errmsg`` parameter as non-NULL, so that
    the functions will pass back errors through it instead of setting
    a Python exception.

    :c:func:`NpyIter_Deallocate` must be called for each copy.

.. c:function:: int NpyIter_RemoveAxis(NpyIter* iter, int axis)

    Removes an axis from iteration.  This requires that
    :c:data:`NPY_ITER_MULTI_INDEX` was set for iterator creation, and does
    not work if buffering is enabled or an index is being tracked. This
    function also resets the iterator to its initial state.

    This is useful for setting up an accumulation loop, for example.
    The iterator can first be created with all the dimensions, including
    the accumulation axis, so that the output gets created correctly.
    Then, the accumulation axis can be removed, and the calculation
    done in a nested fashion.

    **WARNING**: This function may change the internal memory layout of
    the iterator.  Any cached functions or pointers from the iterator
    must be retrieved again! The iterator range will be reset as well.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.


.. c:function:: int NpyIter_RemoveMultiIndex(NpyIter* iter)

    If the iterator is tracking a multi-index, this strips support for them,
    and does further iterator optimizations that are possible if multi-indices
    are not needed.  This function also resets the iterator to its initial
    state.

    **WARNING**: This function may change the internal memory layout of
    the iterator.  Any cached functions or pointers from the iterator
    must be retrieved again!

    After calling this function, :c:expr:`NpyIter_HasMultiIndex(iter)` will
    return false.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: int NpyIter_EnableExternalLoop(NpyIter* iter)

    If :c:func:`NpyIter_RemoveMultiIndex` was called, you may want to enable the
    flag :c:data:`NPY_ITER_EXTERNAL_LOOP`.  This flag is not permitted
    together with :c:data:`NPY_ITER_MULTI_INDEX`, so this function is provided
    to enable the feature after :c:func:`NpyIter_RemoveMultiIndex` is called.
    This function also resets the iterator to its initial state.

    **WARNING**: This function changes the internal logic of the iterator.
    Any cached functions or pointers from the iterator must be retrieved
    again!

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: int NpyIter_Deallocate(NpyIter* iter)

    Deallocates the iterator object and resolves any needed writebacks.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: int NpyIter_Reset(NpyIter* iter, char** errmsg)

    Resets the iterator back to its initial state, at the beginning
    of the iteration range.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

.. c:function:: int NpyIter_ResetToIterIndexRange( \
        NpyIter* iter, npy_intp istart, npy_intp iend, char** errmsg)

    Resets the iterator and restricts it to the ``iterindex`` range
    ``[istart, iend)``.  See :c:func:`NpyIter_Copy` for an explanation of
    how to use this for multi-threaded iteration.  This requires that
    the flag :c:data:`NPY_ITER_RANGED` was passed to the iterator constructor.

    If you want to reset both the ``iterindex`` range and the base
    pointers at the same time, you can do the following to avoid
    extra buffer copying (be sure to add the return code error checks
    when you copy this code).

    .. code-block:: c

        /* Set to a trivial empty range */
        NpyIter_ResetToIterIndexRange(iter, 0, 0);
        /* Set the base pointers */
        NpyIter_ResetBasePointers(iter, baseptrs);
        /* Set to the desired range */
        NpyIter_ResetToIterIndexRange(iter, istart, iend);

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

.. c:function:: int NpyIter_ResetBasePointers( \
        NpyIter *iter, char** baseptrs, char** errmsg)

    Resets the iterator back to its initial state, but using the values
    in ``baseptrs`` for the data instead of the pointers from the arrays
    being iterated.  This functions is intended to be used, together with
    the ``op_axes`` parameter, by nested iteration code with two or more
    iterators.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

    *TODO*: Move the following into a special section on nested iterators.

    Creating iterators for nested iteration requires some care.  All
    the iterator operands must match exactly, or the calls to
    :c:func:`NpyIter_ResetBasePointers` will be invalid.  This means that
    automatic copies and output allocation should not be used haphazardly.
    It is possible to still use the automatic data conversion and casting
    features of the iterator by creating one of the iterators with
    all the conversion parameters enabled, then grabbing the allocated
    operands with the :c:func:`NpyIter_GetOperandArray` function and passing
    them into the constructors for the rest of the iterators.

    **WARNING**: When creating iterators for nested iteration,
    the code must not use a dimension more than once in the different
    iterators.  If this is done, nested iteration will produce
    out-of-bounds pointers during iteration.

    **WARNING**: When creating iterators for nested iteration, buffering
    can only be applied to the innermost iterator.  If a buffered iterator
    is used as the source for ``baseptrs``, it will point into a small buffer
    instead of the array and the inner iteration will be invalid.

    The pattern for using nested iterators is as follows.

    .. code-block:: c

        NpyIter *iter1, *iter1;
        NpyIter_IterNextFunc *iternext1, *iternext2;
        char **dataptrs1;

        /*
         * With the exact same operands, no copies allowed, and
         * no axis in op_axes used both in iter1 and iter2.
         * Buffering may be enabled for iter2, but not for iter1.
         */
        iter1 = ...; iter2 = ...;

        iternext1 = NpyIter_GetIterNext(iter1);
        iternext2 = NpyIter_GetIterNext(iter2);
        dataptrs1 = NpyIter_GetDataPtrArray(iter1);

        do {
            NpyIter_ResetBasePointers(iter2, dataptrs1);
            do {
                /* Use the iter2 values */
            } while (iternext2(iter2));
        } while (iternext1(iter1));

.. c:function:: int NpyIter_GotoMultiIndex(NpyIter* iter, npy_intp const* multi_index)

    Adjusts the iterator to point to the ``ndim`` indices
    pointed to by ``multi_index``.  Returns an error if a multi-index
    is not being tracked, the indices are out of bounds,
    or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: int NpyIter_GotoIndex(NpyIter* iter, npy_intp index)

    Adjusts the iterator to point to the ``index`` specified.
    If the iterator was constructed with the flag
    :c:data:`NPY_ITER_C_INDEX`, ``index`` is the C-order index,
    and if the iterator was constructed with the flag
    :c:data:`NPY_ITER_F_INDEX`, ``index`` is the Fortran-order
    index.  Returns an error if there is no index being tracked,
    the index is out of bounds, or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: npy_intp NpyIter_GetIterSize(NpyIter* iter)

    Returns the number of elements being iterated.  This is the product
    of all the dimensions in the shape.  When a multi index is being tracked
    (and `NpyIter_RemoveAxis` may be called) the size may be ``-1`` to
    indicate an iterator is too large.  Such an iterator is invalid, but
    may become valid after `NpyIter_RemoveAxis` is called. It is not
    necessary to check for this case.

.. c:function:: npy_intp NpyIter_GetIterIndex(NpyIter* iter)

    Gets the ``iterindex`` of the iterator, which is an index matching
    the iteration order of the iterator.

.. c:function:: void NpyIter_GetIterIndexRange( \
        NpyIter* iter, npy_intp* istart, npy_intp* iend)

    Gets the ``iterindex`` sub-range that is being iterated.  If
    :c:data:`NPY_ITER_RANGED` was not specified, this always returns the
    range ``[0, NpyIter_IterSize(iter))``.

.. c:function:: int NpyIter_GotoIterIndex(NpyIter* iter, npy_intp iterindex)

    Adjusts the iterator to point to the ``iterindex`` specified.
    The IterIndex is an index matching the iteration order of the iterator.
    Returns an error if the ``iterindex`` is out of bounds,
    buffering is enabled, or inner loop iteration is disabled.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: npy_bool NpyIter_HasDelayedBufAlloc(NpyIter* iter)

    Returns 1 if the flag :c:data:`NPY_ITER_DELAY_BUFALLOC` was passed
    to the iterator constructor, and no call to one of the Reset
    functions has been done yet, 0 otherwise.

.. c:function:: npy_bool NpyIter_HasExternalLoop(NpyIter* iter)

    Returns 1 if the caller needs to handle the inner-most 1-dimensional
    loop, or 0 if the iterator handles all looping. This is controlled
    by the constructor flag :c:data:`NPY_ITER_EXTERNAL_LOOP` or
    :c:func:`NpyIter_EnableExternalLoop`.

.. c:function:: npy_bool NpyIter_HasMultiIndex(NpyIter* iter)

    Returns 1 if the iterator was created with the
    :c:data:`NPY_ITER_MULTI_INDEX` flag, 0 otherwise.

.. c:function:: npy_bool NpyIter_HasIndex(NpyIter* iter)

    Returns 1 if the iterator was created with the
    :c:data:`NPY_ITER_C_INDEX` or :c:data:`NPY_ITER_F_INDEX`
    flag, 0 otherwise.

.. c:function:: npy_bool NpyIter_RequiresBuffering(NpyIter* iter)

    Returns 1 if the iterator requires buffering, which occurs
    when an operand needs conversion or alignment and so cannot
    be used directly.

.. c:function:: npy_bool NpyIter_IsBuffered(NpyIter* iter)

    Returns 1 if the iterator was created with the
    :c:data:`NPY_ITER_BUFFERED` flag, 0 otherwise.

.. c:function:: npy_bool NpyIter_IsGrowInner(NpyIter* iter)

    Returns 1 if the iterator was created with the
    :c:data:`NPY_ITER_GROWINNER` flag, 0 otherwise.

.. c:function:: npy_intp NpyIter_GetBufferSize(NpyIter* iter)

    If the iterator is buffered, returns the size of the buffer
    being used, otherwise returns 0.

.. c:function:: int NpyIter_GetNDim(NpyIter* iter)

    Returns the number of dimensions being iterated.  If a multi-index
    was not requested in the iterator constructor, this value
    may be smaller than the number of dimensions in the original
    objects.

.. c:function:: int NpyIter_GetNOp(NpyIter* iter)

    Returns the number of operands in the iterator.

.. c:function:: npy_intp* NpyIter_GetAxisStrideArray(NpyIter* iter, int axis)

    Gets the array of strides for the specified axis. Requires that
    the iterator be tracking a multi-index, and that buffering not
    be enabled.

    This may be used when you want to match up operand axes in
    some fashion, then remove them with :c:func:`NpyIter_RemoveAxis` to
    handle their processing manually.  By calling this function
    before removing the axes, you can get the strides for the
    manual processing.

    Returns ``NULL`` on error.

.. c:function:: int NpyIter_GetShape(NpyIter* iter, npy_intp* outshape)

    Returns the broadcast shape of the iterator in ``outshape``.
    This can only be called on an iterator which is tracking a multi-index.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: PyArray_Descr** NpyIter_GetDescrArray(NpyIter* iter)

    This gives back a pointer to the ``nop`` data type Descrs for
    the objects being iterated.  The result points into ``iter``,
    so the caller does not gain any references to the Descrs.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it.

.. c:function:: PyObject** NpyIter_GetOperandArray(NpyIter* iter)

    This gives back a pointer to the ``nop`` operand PyObjects
    that are being iterated.  The result points into ``iter``,
    so the caller does not gain any references to the PyObjects.

.. c:function:: PyObject* NpyIter_GetIterView(NpyIter* iter, npy_intp i)

    This gives back a reference to a new ndarray view, which is a view
    into the i-th object in the array :c:func:`NpyIter_GetOperandArray()`,
    whose dimensions and strides match the internal optimized
    iteration pattern.  A C-order iteration of this view is equivalent
    to the iterator's iteration order.

    For example, if an iterator was created with a single array as its
    input, and it was possible to rearrange all its axes and then
    collapse it into a single strided iteration, this would return
    a view that is a one-dimensional array.

.. c:function:: void NpyIter_GetReadFlags(NpyIter* iter, char* outreadflags)

    Fills ``nop`` flags. Sets ``outreadflags[i]`` to 1 if
    ``op[i]`` can be read from, and to 0 if not.

.. c:function:: void NpyIter_GetWriteFlags(NpyIter* iter, char* outwriteflags)

    Fills ``nop`` flags. Sets ``outwriteflags[i]`` to 1 if
    ``op[i]`` can be written to, and to 0 if not.

.. c:function:: int NpyIter_CreateCompatibleStrides( \
        NpyIter* iter, npy_intp itemsize, npy_intp* outstrides)

    Builds a set of strides which are the same as the strides of an
    output array created using the :c:data:`NPY_ITER_ALLOCATE` flag, where NULL
    was passed for op_axes.  This is for data packed contiguously,
    but not necessarily in C or Fortran order. This should be used
    together with :c:func:`NpyIter_GetShape` and :c:func:`NpyIter_GetNDim`
    with the flag :c:data:`NPY_ITER_MULTI_INDEX` passed into the constructor.

    A use case for this function is to match the shape and layout of
    the iterator and tack on one or more dimensions.  For example,
    in order to generate a vector per input value for a numerical gradient,
    you pass in ndim*itemsize for itemsize, then add another dimension to
    the end with size ndim and stride itemsize.  To do the Hessian matrix,
    you do the same thing but add two dimensions, or take advantage of
    the symmetry and pack it into 1 dimension with a particular encoding.

    This function may only be called if the iterator is tracking a multi-index
    and if :c:data:`NPY_ITER_DONT_NEGATE_STRIDES` was used to prevent an axis
    from being iterated in reverse order.

    If an array is created with this method, simply adding 'itemsize'
    for each iteration will traverse the new array matching the
    iterator.

    Returns ``NPY_SUCCEED`` or ``NPY_FAIL``.

.. c:function:: npy_bool NpyIter_IsFirstVisit(NpyIter* iter, int iop)

    .. versionadded:: 1.7

    Checks to see whether this is the first time the elements of the
    specified reduction operand which the iterator points at are being
    seen for the first time. The function returns a reasonable answer
    for reduction operands and when buffering is disabled. The answer
    may be incorrect for buffered non-reduction operands.

    This function is intended to be used in EXTERNAL_LOOP mode only,
    and will produce some wrong answers when that mode is not enabled.

    If this function returns true, the caller should also check the inner
    loop stride of the operand, because if that stride is 0, then only
    the first element of the innermost external loop is being visited
    for the first time.

    *WARNING*: For performance reasons, 'iop' is not bounds-checked,
    it is not confirmed that 'iop' is actually a reduction operand,
    and it is not confirmed that EXTERNAL_LOOP mode is enabled. These
    checks are the responsibility of the caller, and should be done
    outside of any inner loops.

Functions For Iteration
-----------------------

.. c:function:: NpyIter_IterNextFunc* NpyIter_GetIterNext( \
        NpyIter* iter, char** errmsg)

    Returns a function pointer for iteration.  A specialized version
    of the function pointer may be calculated by this function
    instead of being stored in the iterator structure. Thus, to
    get good performance, it is required that the function pointer
    be saved in a variable rather than retrieved for each loop iteration.

    Returns NULL if there is an error.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

    The typical looping construct is as follows.

    .. code-block:: c

        NpyIter_IterNextFunc *iternext = NpyIter_GetIterNext(iter, NULL);
        char** dataptr = NpyIter_GetDataPtrArray(iter);

        do {
            /* use the addresses dataptr[0], ... dataptr[nop-1] */
        } while(iternext(iter));

    When :c:data:`NPY_ITER_EXTERNAL_LOOP` is specified, the typical
    inner loop construct is as follows.

    .. code-block:: c

        NpyIter_IterNextFunc *iternext = NpyIter_GetIterNext(iter, NULL);
        char** dataptr = NpyIter_GetDataPtrArray(iter);
        npy_intp* stride = NpyIter_GetInnerStrideArray(iter);
        npy_intp* size_ptr = NpyIter_GetInnerLoopSizePtr(iter), size;
        npy_intp iop, nop = NpyIter_GetNOp(iter);

        do {
            size = *size_ptr;
            while (size--) {
                /* use the addresses dataptr[0], ... dataptr[nop-1] */
                for (iop = 0; iop < nop; ++iop) {
                    dataptr[iop] += stride[iop];
                }
            }
        } while (iternext());

    Observe that we are using the dataptr array inside the iterator, not
    copying the values to a local temporary.  This is possible because
    when ``iternext()`` is called, these pointers will be overwritten
    with fresh values, not incrementally updated.

    If a compile-time fixed buffer is being used (both flags
    :c:data:`NPY_ITER_BUFFERED` and :c:data:`NPY_ITER_EXTERNAL_LOOP`), the
    inner size may be used as a signal as well.  The size is guaranteed
    to become zero when ``iternext()`` returns false, enabling the
    following loop construct.  Note that if you use this construct,
    you should not pass :c:data:`NPY_ITER_GROWINNER` as a flag, because it
    will cause larger sizes under some circumstances.

    .. code-block:: c

        /* The constructor should have buffersize passed as this value */
        #define FIXED_BUFFER_SIZE 1024

        NpyIter_IterNextFunc *iternext = NpyIter_GetIterNext(iter, NULL);
        char **dataptr = NpyIter_GetDataPtrArray(iter);
        npy_intp *stride = NpyIter_GetInnerStrideArray(iter);
        npy_intp *size_ptr = NpyIter_GetInnerLoopSizePtr(iter), size;
        npy_intp i, iop, nop = NpyIter_GetNOp(iter);

        /* One loop with a fixed inner size */
        size = *size_ptr;
        while (size == FIXED_BUFFER_SIZE) {
            /*
             * This loop could be manually unrolled by a factor
             * which divides into FIXED_BUFFER_SIZE
             */
            for (i = 0; i < FIXED_BUFFER_SIZE; ++i) {
                /* use the addresses dataptr[0], ... dataptr[nop-1] */
                for (iop = 0; iop < nop; ++iop) {
                    dataptr[iop] += stride[iop];
                }
            }
            iternext();
            size = *size_ptr;
        }

        /* Finish-up loop with variable inner size */
        if (size > 0) do {
            size = *size_ptr;
            while (size--) {
                /* use the addresses dataptr[0], ... dataptr[nop-1] */
                for (iop = 0; iop < nop; ++iop) {
                    dataptr[iop] += stride[iop];
                }
            }
        } while (iternext());

.. c:function:: NpyIter_GetMultiIndexFunc *NpyIter_GetGetMultiIndex( \
        NpyIter* iter, char** errmsg)

    Returns a function pointer for getting the current multi-index
    of the iterator.  Returns NULL if the iterator is not tracking
    a multi-index.  It is recommended that this function
    pointer be cached in a local variable before the iteration
    loop.

    Returns NULL if there is an error.  If errmsg is non-NULL,
    no Python exception is set when ``NPY_FAIL`` is returned.
    Instead, \*errmsg is set to an error message.  When errmsg is
    non-NULL, the function may be safely called without holding
    the Python GIL.

.. c:function:: char** NpyIter_GetDataPtrArray(NpyIter* iter)

    This gives back a pointer to the ``nop`` data pointers.  If
    :c:data:`NPY_ITER_EXTERNAL_LOOP` was not specified, each data
    pointer points to the current data item of the iterator.  If
    no inner iteration was specified, it points to the first data
    item of the inner loop.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it.  This function may be safely
    called without holding the Python GIL.

.. c:function:: char** NpyIter_GetInitialDataPtrArray(NpyIter* iter)

   Gets the array of data pointers directly into the arrays (never
   into the buffers), corresponding to iteration index 0.

   These pointers are different from the pointers accepted by
   ``NpyIter_ResetBasePointers``, because the direction along
   some axes may have been reversed.

   This function may be safely called without holding the Python GIL.

.. c:function:: npy_intp* NpyIter_GetIndexPtr(NpyIter* iter)

    This gives back a pointer to the index being tracked, or NULL
    if no index is being tracked.  It is only usable if one of
    the flags :c:data:`NPY_ITER_C_INDEX` or :c:data:`NPY_ITER_F_INDEX`
    were specified during construction.

When the flag :c:data:`NPY_ITER_EXTERNAL_LOOP` is used, the code
needs to know the parameters for doing the inner loop.  These
functions provide that information.

.. c:function:: npy_intp* NpyIter_GetInnerStrideArray(NpyIter* iter)

    Returns a pointer to an array of the ``nop`` strides,
    one for each iterated object, to be used by the inner loop.

    This pointer may be cached before the iteration loop, calling
    ``iternext`` will not change it. This function may be safely
    called without holding the Python GIL.

    **WARNING**: While the pointer may be cached, its values may
    change if the iterator is buffered.

.. c:function:: npy_intp* NpyIter_GetInnerLoopSizePtr(NpyIter* iter)

    Returns a pointer to the number of iterations the
    inner loop should execute.

    This address may be cached before the iteration loop, calling
    ``iternext`` will not change it.  The value itself may change during
    iteration, in particular if buffering is enabled.  This function
    may be safely called without holding the Python GIL.

.. c:function:: void NpyIter_GetInnerFixedStrideArray( \
        NpyIter* iter, npy_intp* out_strides)

    Gets an array of strides which are fixed, or will not change during
    the entire iteration.  For strides that may change, the value
    NPY_MAX_INTP is placed in the stride.

    Once the iterator is prepared for iteration (after a reset if
    :c:data:`NPY_ITER_DELAY_BUFALLOC` was used), call this to get the strides
    which may be used to select a fast inner loop function.  For example,
    if the stride is 0, that means the inner loop can always load its
    value into a variable once, then use the variable throughout the loop,
    or if the stride equals the itemsize, a contiguous version for that
    operand may be used.

    This function may be safely called without holding the Python GIL.

.. index::
    pair: iterator; C-API

Converting from Previous NumPy Iterators
----------------------------------------

The old iterator API includes functions like PyArrayIter_Check,
PyArray_Iter* and PyArray_ITER_*.  The multi-iterator array includes
PyArray_MultiIter*, PyArray_Broadcast, and PyArray_RemoveSmallest.  The
new iterator design replaces all of this functionality with a single object
and associated API.  One goal of the new API is that all uses of the
existing iterator should be replaceable with the new iterator without
significant effort. In 1.6, the major exception to this is the neighborhood
iterator, which does not have corresponding features in this iterator.

Here is a conversion table for which functions to use with the new iterator:

=====================================  ===================================================
*Iterator Functions*
:c:func:`PyArray_IterNew`              :c:func:`NpyIter_New`
:c:func:`PyArray_IterAllButAxis`       :c:func:`NpyIter_New` + ``axes`` parameter **or**
                                       Iterator flag :c:data:`NPY_ITER_EXTERNAL_LOOP`
:c:func:`PyArray_BroadcastToShape`     **NOT SUPPORTED** (Use the support for
                                       multiple operands instead.)
:c:func:`PyArrayIter_Check`            Will need to add this in Python exposure
:c:func:`PyArray_ITER_RESET`           :c:func:`NpyIter_Reset`
:c:func:`PyArray_ITER_NEXT`            Function pointer from :c:func:`NpyIter_GetIterNext`
:c:func:`PyArray_ITER_DATA`            :c:func:`NpyIter_GetDataPtrArray`
:c:func:`PyArray_ITER_GOTO`            :c:func:`NpyIter_GotoMultiIndex`
:c:func:`PyArray_ITER_GOTO1D`          :c:func:`NpyIter_GotoIndex` or
                                       :c:func:`NpyIter_GotoIterIndex`
:c:func:`PyArray_ITER_NOTDONE`         Return value of ``iternext`` function pointer
*Multi-iterator Functions*
:c:func:`PyArray_MultiIterNew`         :c:func:`NpyIter_MultiNew`
:c:func:`PyArray_MultiIter_RESET`      :c:func:`NpyIter_Reset`
:c:func:`PyArray_MultiIter_NEXT`       Function pointer from :c:func:`NpyIter_GetIterNext`
:c:func:`PyArray_MultiIter_DATA`       :c:func:`NpyIter_GetDataPtrArray`
:c:func:`PyArray_MultiIter_NEXTi`      **NOT SUPPORTED** (always lock-step iteration)
:c:func:`PyArray_MultiIter_GOTO`       :c:func:`NpyIter_GotoMultiIndex`
:c:func:`PyArray_MultiIter_GOTO1D`     :c:func:`NpyIter_GotoIndex` or
                                       :c:func:`NpyIter_GotoIterIndex`
:c:func:`PyArray_MultiIter_NOTDONE`    Return value of ``iternext`` function pointer
:c:func:`PyArray_Broadcast`            Handled by :c:func:`NpyIter_MultiNew`
:c:func:`PyArray_RemoveSmallest`       Iterator flag :c:data:`NPY_ITER_EXTERNAL_LOOP`
*Other Functions*
:c:func:`PyArray_ConvertToCommonType`  Iterator flag :c:data:`NPY_ITER_COMMON_DTYPE`
=====================================  ===================================================
C API Deprecations
==================

Background
----------

The API exposed by NumPy for third-party extensions has grown over
years of releases, and has allowed programmers to directly access
NumPy functionality from C. This API can be best described as
"organic".   It has emerged from multiple competing desires and from
multiple points of view over the years, strongly influenced by the
desire to make it easy for users to move to NumPy from Numeric and
Numarray.   The core API originated with Numeric in 1995 and there are
patterns such as the heavy use of macros written to mimic Python's
C-API as well as account for compiler technology of the late 90's.
There is also only a small group of volunteers who have had very little
time to spend on improving this API.

There is an ongoing effort to improve the API.
It is important in this effort
to ensure that code that compiles for NumPy 1.X continues to
compile for NumPy 1.X.  At the same time, certain API's will be marked
as deprecated so that future-looking code can avoid these API's and
follow better practices.

Another important role played by deprecation markings in the C API is to move
towards hiding internal details of the NumPy implementation. For those
needing direct, easy, access to the data of ndarrays, this will not
remove this ability. Rather, there are many potential performance
optimizations which require changing the implementation details, and
NumPy developers have been unable to try them because of the high
value of preserving ABI compatibility. By deprecating this direct
access, we will in the future be able to improve NumPy's performance
in ways we cannot presently.

Deprecation Mechanism NPY_NO_DEPRECATED_API
-------------------------------------------

In C, there is no equivalent to the deprecation warnings that Python
supports. One way to do deprecations is to flag them in the
documentation and release notes, then remove or change the deprecated
features in a future major version (NumPy 2.0 and beyond).  Minor
versions of NumPy should not have major C-API changes, however, that
prevent code that worked on a previous minor release.  For example, we
will do our best to ensure that code that compiled and worked on NumPy
1.4 should continue to work on NumPy 1.7 (but perhaps with compiler
warnings).

To use the NPY_NO_DEPRECATED_API mechanism, you need to #define it to
the target API version of NumPy before #including any NumPy headers.
If you want to confirm that your code is clean against 1.7, use:

.. code-block:: c

    #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION

On compilers which support a #warning mechanism, NumPy issues a
compiler warning if you do not define the symbol NPY_NO_DEPRECATED_API.
This way, the fact that there are deprecations will be flagged for
third-party developers who may not have read the release notes closely.
========================
Advanced F2PY use cases
========================

Adding user-defined functions to F2PY generated modules
=========================================================

User-defined Python C/API functions can be defined inside
signature files using ``usercode`` and ``pymethoddef`` statements
(they must be used inside the ``python module`` block). For
example, the following signature file ``spam.pyf``

.. include:: ./code/spam.pyf
   :literal:

wraps the C library function ``system()``::

  f2py -c spam.pyf

In Python this can then be used as:

.. literalinclude:: ./code/results/spam_session.dat
   :language: python

Adding user-defined variables
==============================

The following example illustrates how to add user-defined variables to a F2PY
generated extension module by modifying the dictionary of a F2PY generated
module. Consider the following signature file (compiled with ``f2py -c var.pyf``):

.. literalinclude:: ./code/var.pyf
  :language: fortran

Notice that the second ``usercode`` statement must be defined inside
an ``interface`` block and the module dictionary is available through
the variable ``d`` (see ``varmodule.c`` generated by ``f2py var.pyf`` for
additional details).

Usage in Python:

.. literalinclude:: ./code/results/var_session.dat
  :language: python


Dealing with KIND specifiers
============================

Currently, F2PY can handle only ``<type spec>(kind=<kindselector>)``
declarations where ``<kindselector>`` is a numeric integer (e.g. 1, 2,
4,...), but not a function call ``KIND(..)`` or any other
expression. F2PY needs to know what would be the corresponding C type
and a general solution for that would be too complicated to implement.

However, F2PY provides a hook to overcome this difficulty, namely,
users can define their own <Fortran type> to <C type> maps. For
example, if Fortran 90 code contains::

    REAL(kind=KIND(0.0D0)) ...

then create a mapping file containing a Python dictionary::

    {'real': {'KIND(0.0D0)': 'double'}}

for instance.

Use the ``--f2cmap`` command-line option to pass the file name to F2PY.
By default, F2PY assumes file name is ``.f2py_f2cmap`` in the current
working directory.

More generally, the f2cmap file must contain a dictionary
with items::

    <Fortran typespec> : {<selector_expr>:<C type>}

that defines mapping between Fortran type::

    <Fortran typespec>([kind=]<selector_expr>)

and the corresponding <C type>. The <C type> can be one of the following::

    char
    signed_char
    short
    int
    long_long
    float
    double
    long_double
    complex_float
    complex_double
    complex_long_double
    string

For more information, see the F2Py source code ``numpy/f2py/capi_maps.py``.
===========
Using F2PY
===========

F2PY can be used either as a command line tool ``f2py`` or as a Python
module ``numpy.f2py``. While we try to provide the command line tool as part
of the numpy setup, some platforms like Windows make it difficult to
reliably put the executables on the ``PATH``. We will refer to ``f2py``
in this document but you may have to run it as a module::

   python -m numpy.f2py

If you run ``f2py`` with no arguments, and the line ``numpy Version`` at the
end matches the NumPy version printed from ``python -m numpy.f2py``, then you
can use the shorter version. If not, or if you cannot run ``f2py``, you should
replace all calls to ``f2py`` here with the longer version.

Command ``f2py``
=================

When used as a command line tool, ``f2py`` has three major modes,
distinguished by the usage of ``-c`` and ``-h`` switches:

Signature file generation
^^^^^^^^^^^^^^^^^^^^^^^^^^

1. To scan Fortran sources and generate a signature file, use

   .. code-block:: sh

     f2py -h <filename.pyf> <options> <fortran files>   \
       [[ only: <fortran functions>  : ]                \
        [ skip: <fortran functions>  : ]]...            \
       [<fortran files> ...]

   .. note::

    A Fortran source file can contain many routines, and it is often
    not necessary to allow all routines be usable from Python. In such cases,
    either specify which routines should be wrapped (in the ``only: .. :`` part)
    or which routines F2PY should ignored (in the ``skip: .. :`` part).

   If ``<filename.pyf>`` is specified as ``stdout`` then signatures
   are written to standard output instead of a file.

   Among other options (see below), the following can be used
   in this mode:

   ``--overwrite-signature``
     Overwrites an existing signature file.

Extension module construction
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

2. To construct an extension module, use

   .. code-block:: sh

     f2py -m <modulename> <options> <fortran files>   \
       [[ only: <fortran functions>  : ]              \
        [ skip: <fortran functions>  : ]]...          \
       [<fortran files> ...]

   The constructed extension module is saved as
   ``<modulename>module.c`` to the current directory.

   Here ``<fortran files>`` may also contain signature files.
   Among other options (see below), the following options can be used
   in this mode:

   ``--debug-capi``
     Adds debugging hooks to the extension module. When using this extension
     module, various diagnostic information about the wrapper is written to
     the standard output, for example, the values of variables, the steps taken,
     etc.

   ``-include'<includefile>'``
     Add a CPP ``#include`` statement to the extension module source.
     ``<includefile>`` should be given in one of the following forms

       .. code-block:: cpp

        "filename.ext"
        <filename.ext>

     The include statement is inserted just before the wrapper
     functions. This feature enables using arbitrary C functions
     (defined in ``<includefile>``) in F2PY generated wrappers.

     .. note:: This option is deprecated. Use ``usercode`` statement to specify C code snippets directly in signature files.

   ``--[no-]wrap-functions``
     Create Fortran subroutine wrappers to Fortran functions.
     ``--wrap-functions`` is default because it ensures maximum
     portability and compiler independence.

   ``--include-paths <path1>:<path2>:..``
     Search include files from given directories.

   ``--help-link [<list of resources names>]``
     List system resources found by ``numpy_distutils/system_info.py``.
     For example, try ``f2py --help-link lapack_opt``.

Building a module
^^^^^^^^^^^^^^^^^

3. To build an extension module, use

   .. code-block:: sh

     f2py -c <options> <fortran files>       \
       [[ only: <fortran functions>  : ]     \
        [ skip: <fortran functions>  : ]]... \
       [ <fortran/c source files> ] [ <.o, .a, .so files> ]
 
   If ``<fortran files>`` contains a signature file, then the source for
   an extension module is constructed, all Fortran and C sources are
   compiled, and finally all object and library files are linked to the
   extension module ``<modulename>.so`` which is saved into the current
   directory.

   If ``<fortran files>`` does not contain a signature file, then an
   extension module is constructed by scanning all Fortran source codes
   for routine signatures, before proceeding to build the extension module.
 
   Among other options (see below) and options described for previous
   modes, the following options can be used in this mode:
 
   ``--help-fcompiler``
     List the available Fortran compilers.
   ``--help-compiler`` **[depreciated]**
     List the available Fortran compilers.
   ``--fcompiler=<Vendor>``
     Specify a Fortran compiler type by vendor.
   ``--f77exec=<path>``
     Specify the path to a F77 compiler
   ``--fcompiler-exec=<path>`` **[depreciated]**
     Specify the path to a F77 compiler
   ``--f90exec=<path>``
     Specify the path to a F90 compiler
   ``--f90compiler-exec=<path>`` **[depreciated]**
     Specify the path to a F90 compiler
   ``--f77flags=<string>``
     Specify F77 compiler flags
   ``--f90flags=<string>``
     Specify F90 compiler flags
   ``--opt=<string>``
     Specify optimization flags
   ``--arch=<string>``
     Specify architecture specific optimization flags
   ``--noopt``
     Compile without optimization flags
   ``--noarch``
     Compile without arch-dependent optimization flags
   ``--debug``
     Compile with debugging information
   ``-l<libname>``
     Use the library ``<libname>`` when linking.
   ``-D<macro>[=<defn=1>]``
     Define macro ``<macro>`` as ``<defn>``.
   ``-U<macro>``
     Define macro ``<macro>``
   ``-I<dir>``
     Append directory ``<dir>`` to the list of directories searched for
     include files.
   ``-L<dir>``
     Add directory ``<dir>`` to the list of directories to  be  searched
     for ``-l``.
   ``link-<resource>``
     Link the extension module with <resource> as defined by
     ``numpy_distutils/system_info.py``. E.g. to link with optimized
     LAPACK libraries (vecLib on MacOSX, ATLAS elsewhere), use
     ``--link-lapack_opt``. See also ``--help-link`` switch.

   .. note:: The ``f2py -c`` option must be applied either to an existing ``.pyf`` file (plus the source/object/library files) or one must specify the ``-m <modulename>`` option (plus the sources/object/library files). Use one of the following options:

   .. code-block:: sh

         f2py -c -m fib1 fib1.f

   or

   .. code-block:: sh

         f2py -m fib1 fib1.f -h fib1.pyf
         f2py -c fib1.pyf fib1.f

   For more information, see the `Building C and C++ Extensions`__ Python documentation for details.

   __ https://docs.python.org/3/extending/building.html


   When building an extension module, a combination of the following
   macros may be required for non-gcc Fortran compilers:

   .. code-block:: sh

     -DPREPEND_FORTRAN
     -DNO_APPEND_FORTRAN
     -DUPPERCASE_FORTRAN
 
   To test the performance of F2PY generated interfaces, use
   ``-DF2PY_REPORT_ATEXIT``. Then a report of various timings is
   printed out at the exit of Python. This feature may not work on
   all platforms, currently only Linux platform is supported.
 
   To see whether F2PY generated interface performs copies of array
   arguments, use ``-DF2PY_REPORT_ON_ARRAY_COPY=<int>``. When the size
   of an array argument is larger than ``<int>``, a message about
   the coping is sent to ``stderr``.

Other options
^^^^^^^^^^^^^

``-m <modulename>``
  Name of an extension module. Default is ``untitled``.

  .. warning:: Don't use this option if a signature file (\*.pyf) is used.
``--[no-]lower``
  Do [not] lower the cases in ``<fortran files>``.  By default,
  ``--lower`` is assumed with ``-h`` switch, and ``--no-lower``
  without the ``-h`` switch.
``-include<header>``
  Writes additional headers in the C wrapper, can be passed multiple times,
  generates #include <header> each time. Note that this is meant to be passed in
  single quotes and without spaces, for example ``'-include<stdbool.h>'``
``--build-dir <dirname>``
  All F2PY generated files are created in ``<dirname>``.  Default is
  ``tempfile.mkdtemp()``.
``--quiet``
  Run quietly.
``--verbose``
  Run with extra verbosity.
``-v``
  Print the F2PY version and exit.

Execute ``f2py`` without any options to get an up-to-date list of
available options.

Python module ``numpy.f2py``
============================

.. warning::

  The current Python interface to the ``f2py`` module is not mature and
  may change in the future.


.. automodule:: numpy.f2py
    :members:

.. _f2py-getting-started:

======================================
 Three ways to wrap - getting started
======================================

Wrapping Fortran or C functions to Python using F2PY consists of the
following steps:

* Creating the so-called signature file that contains descriptions of
  wrappers to Fortran or C functions, also called the signatures of the
  functions. For Fortran routines, F2PY can create an initial
  signature file by scanning Fortran source codes and
  tracking all relevant information needed to create wrapper
  functions.

  * Optionally, F2PY created signature files can be edited to optimize
    wrapper functions, to make them "smarter" and more "Pythonic".

* F2PY reads a signature file and writes a Python C/API module containing
  Fortran/C/Python bindings.
* F2PY compiles all sources and builds an extension module containing
  the wrappers.

  * In building the extension modules, F2PY uses ``numpy_distutils`` which
    supports a number of Fortran 77/90/95 compilers, including Gnu, Intel, Sun
    Fortran, SGI MIPSpro, Absoft, NAG, Compaq etc.

Depending on the situation, these steps can be carried out in a single composite
command or step-by-step; in which case some steps can be omitted or combined
with others.

Below, we describe three typical approaches of using F2PY. These can be read in
order of increasing effort, but also cater to different access levels depending
on whether the Fortran code can be freely modified.

The following example Fortran 77 code will be used for
illustration, save it as ``fib1.f``:

.. literalinclude:: ./code/fib1.f
   :language: fortran


The quick way
==============

The quickest way to wrap the Fortran subroutine ``FIB`` for use in Python is to
run

::

  python -m numpy.f2py -c fib1.f -m fib1

This command compiles and wraps ``fib1.f`` (``-c``) to create the extension
module ``fib1.so`` (``-m``) in the current directory. A list of command line
options can be seen by executing ``python -m numpy.f2py``.  Now, in Python the
Fortran subroutine ``FIB`` is accessible via ``fib1.fib``::

  >>> import numpy as np
  >>> import fib1
  >>> print(fib1.fib.__doc__)
  fib(a,[n])

  Wrapper for ``fib``.

  Parameters
  ----------
  a : input rank-1 array('d') with bounds (n)

  Other Parameters
  ----------------
  n : input int, optional
      Default: len(a)

  >>> a = np.zeros(8, 'd')
  >>> fib1.fib(a)
  >>> print(a)
  [  0.   1.   1.   2.   3.   5.   8.  13.]

.. note::

  * Note that F2PY recognized that the second argument ``n`` is the
    dimension of the first array argument ``a``. Since by default all
    arguments are input-only arguments, F2PY concludes that ``n`` can
    be optional with the default value ``len(a)``.

  * One can use different values for optional ``n``::

      >>> a1 = np.zeros(8, 'd')
      >>> fib1.fib(a1, 6)
      >>> print(a1)
      [ 0.  1.  1.  2.  3.  5.  0.  0.]

    but an exception is raised when it is incompatible with the input
    array ``a``::

      >>> fib1.fib(a, 10)
      Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
      fib.error: (len(a)>=n) failed for 1st keyword n: fib:n=10
      >>>

    F2PY implements basic compatibility checks between related
    arguments in order to avoid unexpected crashes.

  * When a NumPy array, that is Fortran contiguous and has a ``dtype``
    corresponding to a presumed Fortran type, is used as an input array
    argument, then its C pointer is directly passed to Fortran.

    Otherwise F2PY makes a contiguous copy (with the proper ``dtype``) of
    the input array and passes a C pointer of the copy to the Fortran
    subroutine. As a result, any possible changes to the (copy of)
    input array have no effect to the original argument, as
    demonstrated below::

      >>> a = np.ones(8, 'i')
      >>> fib1.fib(a)
      >>> print(a)
      [1 1 1 1 1 1 1 1]

    Clearly, this is unexpected, as Fortran typically passes by reference. That
    the above example worked with ``dtype=float`` is considered accidental.

    F2PY provides an ``intent(inplace)`` attribute that modifies
    the attributes of an input array so that any changes made by
    Fortran routine will be reflected in the input argument. For example,
    if one specifies the ``intent(inplace) a`` directive (see subsequent
    sections on how), then the example above would read::

      >>> a = np.ones(8, 'i')
      >>> fib1.fib(a)
      >>> print(a)
      [  0.   1.   1.   2.   3.   5.   8.  13.]

    However, the recommended way to have changes made by Fortran subroutine
    propagate to Python is to use the ``intent(out)`` attribute. That approach is
    more efficient and also cleaner.

  * The usage of ``fib1.fib`` in Python is very similar to using ``FIB`` in
    Fortran. However, using *in situ* output arguments in Python is poor style,
    as there are no safety mechanisms in Python to protect against wrong
    argument types. When using Fortran or C, compilers discover any type
    mismatches during the compilation process, but in Python the types must be
    checked at runtime. Consequently, using *in situ* output arguments in Python
    may lead to difficult to find bugs, not to mention the fact that the
    codes will be less readable when all required type checks are implemented.

  Though the approach to wrapping Fortran routines for Python discussed so far is
  very straightforward, it has several drawbacks (see the comments above).
  The drawbacks are due to the fact that there is no way for F2PY to determine
  the actual intention of the arguments; that is there is ambiguity in
  distinguishing between input and output arguments. Consequently, F2PY assumes
  that all arguments are input arguments by default.

  However, there are ways (see below) to remove this ambiguity by "teaching"
  F2PY about the true intentions of function arguments, and F2PY is then able to
  generate more explicit, easier to use, and less error prone wrappers for
  Fortran functions.

The smart way
==============

Let us apply the steps for wrapping Fortran functions to Python one by
one.

* First, we create a signature file from ``fib1.f`` by running:

  ::

    python -m numpy.f2py fib1.f -m fib2 -h fib1.pyf

  The signature file is saved to ``fib1.pyf`` (see the ``-h`` flag) and
  its contents are shown below.

  .. literalinclude:: ./code/fib1.pyf
     :language: fortran

* Next, we'll teach F2PY that the argument ``n`` is an input argument (using the
  ``intent(in)`` attribute) and that the result, i.e., the contents of ``a``
  after calling the Fortran function ``FIB``, should be returned to Python (using
  the ``intent(out)`` attribute). In addition, an array ``a`` should be created
  dynamically using the size determined by the input argument ``n`` (using the
  ``depend(n)`` attribute to indicate this dependence relation).

  The contents of a suitably modified version of ``fib1.pyf`` (saved as
  ``fib2.pyf``) is as follows:

  .. literalinclude:: ./code/fib2.pyf
     :language: fortran

* Finally, we build the extension module with ``numpy.distutils`` by running:

  ::

    python -m numpy.f2py -c fib2.pyf fib1.f

In Python::

  >>> import fib2
  >>> print(fib2.fib.__doc__)
  a = fib(n)

  Wrapper for ``fib``.

  Parameters
  ----------
  n : input int

  Returns
  -------
  a : rank-1 array('d') with bounds (n)

  >>> print(fib2.fib(8))
  [  0.   1.   1.   2.   3.   5.   8.  13.]

.. note::

  * The signature of ``fib2.fib`` now more closely corresponds to the
    intention of Fortran subroutine ``FIB``: given the number ``n``,
    ``fib2.fib`` returns the first ``n`` Fibonacci numbers as a NumPy array.
    The new Python signature ``fib2.fib`` also rules out the unexpected behaviour in ``fib1.fib``.

  * Note that by default, using a single ``intent(out)`` also implies
    ``intent(hide)``. Arguments that have the ``intent(hide)`` attribute
    specified will not be listed in the argument list of a wrapper function.

The quick and smart way
========================

The "smart way" of wrapping Fortran functions, as explained above, is
suitable for wrapping (e.g. third party) Fortran codes for which
modifications to their source codes are not desirable nor even
possible.

However, if editing Fortran codes is acceptable, then the generation of an
intermediate signature file can be skipped in most cases. F2PY specific
attributes can be inserted directly into Fortran source codes using F2PY
directives. A F2PY directive consists of special comment lines (starting with
``Cf2py`` or ``!f2py``, for example) which are ignored by Fortran compilers but
interpreted by F2PY as normal lines.

Consider a modified version of the previous Fortran code with F2PY directives,
saved as ``fib3.f``:

.. literalinclude:: ./code/fib3.f
   :language: fortran

Building the extension module can be now carried out in one command::

  python -m numpy.f2py -c -m fib3 fib3.f

Notice that the resulting wrapper to ``FIB`` is as "smart" (unambiguous) as in
the previous case::

  >>> import fib3
  >>> print(fib3.fib.__doc__)
  a = fib(n)

  Wrapper for ``fib``.

  Parameters
  ----------
  n : input int

  Returns
  -------
  a : rank-1 array('d') with bounds (n)

  >>> print(fib3.fib(8))
  [  0.   1.   1.   2.   3.   5.   8.  13.]
==================
 Signature file
==================

The syntax specification for signature files (.pyf files) is modeled on the
Fortran 90/95 language specification. Almost all Fortran 90/95 standard
constructs are understood, both in free and fixed format (recall that Fortran 77
is a subset of Fortran 90/95). F2PY introduces some extensions to the Fortran
90/95 language specification that help in the design of the Fortran to Python
interface, making it more "Pythonic".

Signature files may contain arbitrary Fortran code so that any Fortran 90/95
codes can be treated as signature files. F2PY silently ignores
Fortran constructs that are irrelevant for creating the interface.
However, this also means that syntax errors are not caught by F2PY and will only
be caught when the library is built.

In general, the contents of the signature files are case-sensitive. When
scanning Fortran codes to generate a signature file, F2PY lowers all
cases automatically except in multi-line blocks or when the ``--no-lower``
option is used.

The syntax of signature files is presented below.

Python module block
=====================

A signature file may contain one (recommended) or more ``python
module`` blocks. The ``python module`` block describes the contents of
a Python/C extension module ``<modulename>module.c`` that F2PY
generates.

.. warning::

   Exception: if ``<modulename>`` contains a substring ``__user__``, then the
   corresponding ``python module`` block describes the signatures of call-back
   functions (see :ref:`Call-back arguments`).

A ``python module`` block has the following structure::

  python module <modulename>
    [<usercode statement>]...
    [
    interface
      <usercode statement>
      <Fortran block data signatures>
      <Fortran/C routine signatures>
    end [interface]
    ]...
    [
    interface
      module <F90 modulename>
        [<F90 module data type declarations>]
        [<F90 module routine signatures>]
      end [module [<F90 modulename>]]
    end [interface]
    ]...
  end [python module [<modulename>]]

Here brackets ``[]`` indicate an optional section, dots ``...`` indicate one or
more of a previous section. So, ``[]...`` is to be read as zero or more of a
previous section.


Fortran/C routine signatures
=============================

The signature of a Fortran routine has the following structure::

  [<typespec>] function | subroutine <routine name> \
                [ ( [<arguments>] ) ] [ result ( <entityname> ) ]
    [<argument/variable type declarations>]
    [<argument/variable attribute statements>]
    [<use statements>]
    [<common block statements>]
    [<other statements>]
  end [ function | subroutine [<routine name>] ]

From a Fortran routine signature F2PY generates a Python/C extension
function that has the following signature::

  def <routine name>(<required arguments>[,<optional arguments>]):
       ...
       return <return variables>

The signature of a Fortran block data has the following structure::

  block data [ <block data name> ]
    [<variable type declarations>]
    [<variable attribute statements>]
    [<use statements>]
    [<common block statements>]
    [<include statements>]
  end [ block data [<block data name>] ]

Type declarations
=================

The definition of the ``<argument/variable type declaration>`` part
is

::

  <typespec> [ [<attrspec>] :: ] <entitydecl>

where

::

  <typespec> := byte | character [<charselector>]
             | complex [<kindselector>] | real [<kindselector>]
             | double complex | double precision
             | integer [<kindselector>] | logical [<kindselector>]

  <charselector> := * <charlen>
                 | ( [len=] <len> [ , [kind=] <kind>] )
                 | ( kind= <kind> [ , len= <len> ] )
  <kindselector> := * <intlen> | ( [kind=] <kind> )

  <entitydecl> := <name> [ [ * <charlen> ] [ ( <arrayspec> ) ]
                        | [ ( <arrayspec> ) ] * <charlen> ]
                       | [ / <init_expr> / | = <init_expr> ] \
                         [ , <entitydecl> ]

and

* ``<attrspec>`` is a comma separated list of attributes_;

* ``<arrayspec>`` is a comma separated list of dimension bounds;

* ``<init_expr>`` is a `C expression`__;

* ``<intlen>`` may be negative integer for ``integer`` type
  specifications. In such cases ``integer*<negintlen>`` represents
  unsigned C integers;

__ `C expressions`_

If an argument has no ``<argument type declaration>``, its type is
determined by applying ``implicit`` rules to its name.

Statements
==========

Attribute statements
^^^^^^^^^^^^^^^^^^^^^

* The ``<argument/variable attribute statement>`` is
  ``<argument/variable type declaration>`` without ``<typespec>``.
* In addition, in an attribute statement one cannot use other
  attributes, also ``<entitydecl>`` can be only a list of names.

Use statements
^^^^^^^^^^^^^^^

* The definition of the ``<use statement>`` part is

  ::

    use <modulename> [ , <rename_list> | , ONLY : <only_list> ]

  where

  ::

     <rename_list> := <local_name> => <use_name> [ , <rename_list> ]

* Currently F2PY uses ``use`` statement only for linking call-back
  modules and ``external`` arguments (call-back functions), see
  :ref:`Call-back arguments`.

Common block statements
^^^^^^^^^^^^^^^^^^^^^^^

* The definition of the ``<common block statement>`` part is

  ::

    common / <common name> / <shortentitydecl>

  where

  ::

    <shortentitydecl> := <name> [ ( <arrayspec> ) ] [ , <shortentitydecl> ]

* If a ``python module`` block contains two or more ``common`` blocks
  with the same name, the variables from the additional declarations
  are appended.  The types of variables in ``<shortentitydecl>`` are
  defined using ``<argument type declarations>``. Note that the
  corresponding ``<argument type declarations>`` may contain array
  specifications; then these need not be specified in ``<shortentitydecl>``.

Other statements
^^^^^^^^^^^^^^^^^

* The ``<other statement>`` part refers to any other Fortran language
  constructs that are not described above. F2PY ignores most of them
  except the following:

  + ``call`` statements and function calls of ``external`` arguments
    (`more details`__?);

    __ external_

  + ``include`` statements
      ::

        include '<filename>'
        include "<filename>"

      If a file ``<filename>`` does not exist, the ``include``
      statement is ignored. Otherwise, the file ``<filename>`` is
      included to a signature file.  ``include`` statements can be used
      in any part of a signature file, also outside the Fortran/C
      routine signature blocks.

  + ``implicit`` statements
      ::

        implicit none
	implicit <list of implicit maps>

      where

      ::

        <implicit map> := <typespec> ( <list of letters or range of letters> )

      Implicit rules are used to determine the type specification of
      a variable (from the first-letter of its name) if the variable
      is not defined using ``<variable type declaration>``.  Default
      implicit rules are given by:

      ::

        implicit real (a-h,o-z,$_), integer (i-m)

  + ``entry`` statements
      ::

        entry <entry name> [([<arguments>])]

      F2PY generates wrappers for all entry names using the signature
      of the routine block.

      .. note::

        The ``entry`` statement can be used to describe the signature of an
        arbitrary subroutine or function allowing F2PY to generate a number of
        wrappers from only one routine block signature. There are few
        restrictions while doing this: ``fortranname`` cannot be used,
        ``callstatement`` and ``callprotoargument`` can be used only if they are
        valid for all entry routines, etc.

F2PY statements
^^^^^^^^^^^^^^^^

  In addition, F2PY introduces the following statements:

``threadsafe``
  Uses a ``Py_BEGIN_ALLOW_THREADS .. Py_END_ALLOW_THREADS`` block
  around the call to Fortran/C function.

``callstatement <C-expr|multi-line block>``
  Replaces the  F2PY generated call statement to Fortran/C function with
  ``<C-expr|multi-line block>``. The wrapped Fortran/C function is available
  as ``(*f2py_func)``.

  To raise an exception, set ``f2py_success = 0`` in ``<C-expr|multi-line
  block>``.

``callprotoargument <C-typespecs>``
  When the ``callstatement`` statement is used then F2PY may not
  generate proper prototypes for Fortran/C functions (because
  ``<C-expr>`` may contain any function calls and F2PY has no way
  to determine what should be the proper prototype).

  With this statement you can explicitly specify the arguments of the
  corresponding prototype::

    extern <return type> FUNC_F(<routine name>,<ROUTINE NAME>)(<callprotoargument>);

``fortranname [<actual Fortran/C routine name>]``
  F2PY allows for the use of an arbitrary ``<routine name>`` for a given
  Fortran/C function. Then this statement is used for the ``<actual
  Fortran/C routine name>``.

  If ``fortranname`` statement is used without
  ``<actual Fortran/C routine name>`` then a dummy wrapper is
  generated.

``usercode <multi-line block>``
  When this is used inside a ``python module`` block, the given C code will
  be inserted to generated C/API source just before wrapper function
  definitions.

  Here you can define arbitrary C functions to be used for the
  initialization of optional arguments.

  For example, if ``usercode`` is used twice inside ``python module`` block
  then the second multi-line block is inserted after the definition of
  the external routines.

  When used inside ``<routine signature>``, then the given C code will be
  inserted into the corresponding wrapper function just after the
  declaration of  variables but before any C statements. So, the
  ``usercode`` follow-up can contain both declarations and C statements.

  When used inside the first ``interface`` block, then the given C code will
  be inserted at the end of the initialization function of the extension
  module. This is how the extension modules dictionary can be modified and
  has many use-cases; for example, to define additional variables.

``pymethoddef <multiline block>``
  This is a multi-line block which will be inserted into the definition of a
  module methods ``PyMethodDef``-array. It must be a comma-separated list of
  C arrays (see `Extending and Embedding`__ Python documentation for
  details).  ``pymethoddef`` statement can be used only inside ``python
  module`` block.

  __ https://docs.python.org/extending/index.html

Attributes
============

The following attributes are used by F2PY:

``optional``
  The corresponding argument is moved to the end of ``<optional
  arguments>`` list. A default value for an optional argument can be
  specified via ``<init_expr>``, see the ``entitydecl`` definition.


  .. note::

   * The default value must be given as a valid C expression.
   * Whenever ``<init_expr>`` is used, ``optional`` attribute
     is set automatically by F2PY.
   * For an optional array argument, all its dimensions must be bounded.

``required``
  The corresponding argument with this attribute considered mandatory. This is
  the default. ``required`` should only be specified if there is a need to
  disable the automatic ``optional`` setting when ``<init_expr>`` is used.

  If a Python ``None`` object is used as a required argument, the
  argument is treated as optional. That is, in the case of array
  argument, the memory is allocated. If ``<init_expr>`` is given, then the
  corresponding initialization is carried out.

``dimension(<arrayspec>)``
  The corresponding variable is considered as an array with dimensions given in
  ``<arrayspec>``.

``intent(<intentspec>)``
  This specifies the "intention" of the corresponding
  argument. ``<intentspec>`` is a comma separated list of the
  following keys:

  * ``in``
      The corresponding argument is considered to be input-only. This means that the value of
      the argument is passed to a Fortran/C function and that the function is
      expected to not change the value of this argument.

  * ``inout``
      The corresponding argument is marked for input/output or as an *in situ* output
      argument. ``intent(inout)`` arguments can be only "contiguous" NumPy
      arrays with proper type and size. Here "contiguous" can be either in the
      Fortran or C sense. The latter  coincides with the default contiguous
      concept used in NumPy and is effective only if ``intent(c)`` is used. F2PY
      assumes Fortran contiguous arguments by default.

      .. note::

         Using ``intent(inout)`` is generally not recommended, use ``intent(in,out)`` instead.

     See also the ``intent(inplace)`` attribute.

  * ``inplace``
      The corresponding argument is considered to be an input/output or *in situ* output
      argument. ``intent(inplace)`` arguments must be NumPy arrays of a proper
      size. If the type of an array is not "proper" or the array is
      non-contiguous then the array will be modified in-place to fix the type and
      make it contiguous.

      .. note::

        Using ``intent(inplace)`` is generally not recommended either.

        For example, when slices have been taken from an ``intent(inplace)`` argument
        then after in-place changes, the data pointers for the slices may point to
        an unallocated memory area.


  * ``out``
      The corresponding argument is considered to be a return variable. It is appended to the
      ``<returned variables>`` list. Using ``intent(out)`` sets ``intent(hide)``
      automatically, unless  ``intent(in)`` or ``intent(inout)`` are specified
      as well.

      By default, returned multidimensional arrays are Fortran-contiguous. If
      ``intent(c)`` attribute is used, then the returned multidimensional arrays
      are C-contiguous.

  * ``hide``
      The corresponding argument is removed from the list of required or optional
      arguments. Typically ``intent(hide)`` is used with ``intent(out)``
      or when ``<init_expr>`` completely determines the value of the
      argument like in the following example::

        integer intent(hide),depend(a) :: n = len(a)
        real intent(in),dimension(n) :: a

  * ``c``
      The corresponding argument is treated as a C scalar or C array argument. For the case
      of a scalar argument, its value is passed to a C function as a C scalar
      argument (recall that Fortran scalar arguments are actually C pointer
      arguments).  For array arguments, the wrapper function is assumed to treat
      multidimensional arrays as C-contiguous arrays.

      There is no need to use ``intent(c)`` for one-dimensional
      arrays, irrespective of whether the wrapped function is in Fortran or C.
      This is because the concepts of Fortran- and C contiguity overlap in
      one-dimensional cases.

      If ``intent(c)`` is used as a statement but without an entity
      declaration list, then F2PY adds the ``intent(c)`` attribute to all
      arguments.

      Also, when wrapping C functions, one must use ``intent(c)``
      attribute for ``<routine name>`` in order to disable Fortran
      specific ``F_FUNC(..,..)`` macros.

  * ``cache``
      The corresponding argument is treated as junk memory. No Fortran nor C contiguity
      checks are carried out. Using ``intent(cache)`` makes sense only for array
      arguments, also in conjunction with ``intent(hide)`` or ``optional``
      attributes.

  * ``copy``
      Ensures that the original contents of ``intent(in)`` argument is
      preserved. Typically used with the ``intent(in,out)`` attribute. F2PY
      creates an optional argument ``overwrite_<argument name>`` with the
      default value ``0``.

  * ``overwrite``
      This indicates that the original contents of the ``intent(in)`` argument
      may be altered by the Fortran/C function.  F2PY creates an optional
      argument ``overwrite_<argument name>`` with the default value ``1``.

  * ``out=<new name>``
      Replaces the returned name with ``<new name>`` in the ``__doc__`` string
      of the wrapper function.

  * ``callback``
      Constructs an external function suitable for calling Python functions
      from Fortran. ``intent(callback)`` must be specified before the
      corresponding ``external`` statement. If the 'argument' is not in
      the argument list then it will be added to Python wrapper but only
      by initializing an external function.

      .. note::

         Use ``intent(callback)`` in situations where the Fortran/C code assumes
         that the user implemented a function with a given prototype and linked
         it to an executable. Don't use ``intent(callback)`` if the function
         appears in the argument list of a Fortran routine.

      With ``intent(hide)`` or ``optional`` attributes specified and using a
      wrapper function without specifying the callback argument in the argument
      list; then the call-back function is assumed to be found in the  namespace
      of the F2PY generated extension module where it can be set as a module
      attribute by a user.

  * ``aux``
      Defines an auxiliary C variable in the F2PY generated wrapper function.
      Useful to save parameter values so that they can be accessed in
      initialization expressions for other variables.

      .. note::

         ``intent(aux)`` silently implies ``intent(c)``.

  The following rules apply:

  * If none of ``intent(in | inout | out | hide)`` are specified,
    ``intent(in)`` is assumed.

    * ``intent(in,inout)`` is ``intent(in)``;

    * ``intent(in,hide)`` or ``intent(inout,hide)`` is ``intent(hide)``;

    * ``intent(out)`` is ``intent(out,hide)`` unless ``intent(in)`` or
      ``intent(inout)`` is specified.

  * If ``intent(copy)`` or ``intent(overwrite)`` is used, then an additional
    optional argument is introduced with a name ``overwrite_<argument name>``
    and a default value 0 or 1, respectively.

    * ``intent(inout,inplace)`` is ``intent(inplace)``;

    * ``intent(in,inplace)`` is ``intent(inplace)``;

    * ``intent(hide)`` disables ``optional`` and ``required``.

``check([<C-booleanexpr>])``
  Performs a consistency check on the arguments by evaluating
  ``<C-booleanexpr>``; if ``<C-booleanexpr>`` returns 0, an exception is raised.

  .. note::

     If ``check(..)`` is not used then F2PY automatically generates a few
     standard checks (e.g.  in a case of an array argument, it checks for the
     proper shape and size). Use ``check()`` to disable checks
     generated by F2PY.

``depend([<names>])``
  This declares that the corresponding argument depends on the values
  of variables in the ``<names>`` list. For example, ``<init_expr>``
  may use the values of other arguments.  Using information given by
  ``depend(..)`` attributes, F2PY ensures that arguments are
  initialized in a proper order. If the ``depend(..)`` attribute is not
  used then F2PY determines dependence relations automatically. Use
  ``depend()`` to disable the dependence relations generated by F2PY.

  When you edit dependence relations that were initially generated by
  F2PY, be careful not to break the dependence relations of other
  relevant variables. Another thing to watch out for is cyclic
  dependencies. F2PY is able to detect cyclic dependencies
  when constructing wrappers and it complains if any are found.

``allocatable``
  The corresponding variable is a Fortran 90 allocatable array defined as
  Fortran 90 module data.

.. _external:

``external``
  The corresponding argument is a function provided by user. The
  signature of this call-back function can be defined

  - in ``__user__`` module block,
  - or by demonstrative (or real, if the signature file is a real Fortran
    code) call in the ``<other statements>`` block.

  For example, F2PY generates from:

  .. code-block:: fortran

    external cb_sub, cb_fun
    integer n
    real a(n),r
    call cb_sub(a,n)
    r = cb_fun(4)

  the following call-back signatures:

  .. code-block:: fortran

    subroutine cb_sub(a,n)
        real dimension(n) :: a
        integer optional,check(len(a)>=n),depend(a) :: n=len(a)
    end subroutine cb_sub
    function cb_fun(e_4_e) result (r)
        integer :: e_4_e
        real :: r
    end function cb_fun

  The corresponding user-provided Python function are then:

  .. code-block:: python

    def cb_sub(a,[n]):
        ...
        return
    def cb_fun(e_4_e):
        ...
        return r

  See also the ``intent(callback)`` attribute.

``parameter``
  This indicates that the corresponding variable is a parameter and it must have
  a fixed value. F2PY replaces all parameter occurrences by their corresponding
  values.

Extensions
============

F2PY directives
^^^^^^^^^^^^^^^^

The F2PY directives allow using F2PY signature file constructs in
Fortran 77/90 source codes. With this feature one  can (almost) completely skip
the intermediate signature file generation and apply F2PY directly to Fortran
source codes.

F2PY directives have the following form::

  <comment char>f2py ...

where allowed comment characters for fixed and free format Fortran
codes are ``cC*!#`` and ``!``, respectively. Everything that follows
``<comment char>f2py`` is ignored by a compiler but read by F2PY as a
normal non-comment  Fortran line:

.. note::
  When F2PY finds a line with F2PY directive, the directive is first
  replaced by 5 spaces and then the line is reread.

For fixed format Fortran codes, ``<comment char>`` must be at the
first column of a file, of course. For free format Fortran codes,
the F2PY directives can appear anywhere in a file.

C expressions
^^^^^^^^^^^^^^

C expressions are used in the following parts of signature files:

* ``<init_expr>`` for variable initialization;
* ``<C-booleanexpr>`` of the ``check`` attribute;
* ``<arrayspec>`` of the ``dimension`` attribute;
* ``callstatement`` statement, here also a C multi-line block can be used.

A C expression may contain:

* standard C constructs;
* functions from ``math.h`` and ``Python.h``;
* variables from the argument list, presumably initialized before
  according to given dependence relations;
* the following CPP macros:

  * ``rank(<name>)``
    Returns the rank of an array ``<name>``.

  * ``shape(<name>,<n>)``
    Returns the ``<n>``-th dimension of an array ``<name>``.

  * ``len(<name>)``
    Returns the length of an array ``<name>``.

  * ``size(<name>)``
    Returns the size of an array ``<name>``.

  * ``slen(<name>)``
    Returns the length of a string ``<name>``.

For initializing an array ``<array name>``, F2PY generates a loop over
all indices and dimensions that executes the following
pseudo-statement::

  <array name>(_i[0],_i[1],...) = <init_expr>;

where ``_i[<i>]`` refers to the ``<i>``-th index value and that runs
from ``0`` to ``shape(<array name>,<i>)-1``.

For example, a function ``myrange(n)`` generated from the following
signature

.. code-block::

       subroutine myrange(a,n)
         fortranname        ! myrange is a dummy wrapper
         integer intent(in) :: n
         real*8 intent(c,out),dimension(n),depend(n) :: a = _i[0]
       end subroutine myrange

is equivalent to ``numpy.arange(n,dtype=float)``.

.. warning::

  F2PY may lower cases also in C expressions when scanning Fortran codes
  (see ``--[no]-lower`` option).

Multi-line blocks
^^^^^^^^^^^^^^^^^^

A multi-line block starts with ``'''`` (triple single-quotes) and ends
with ``'''`` in some *strictly* subsequent line.  Multi-line blocks can
be used only within .pyf files. The contents of a multi-line block can
be arbitrary (except that it cannot contain ``'''``) and no
transformations (e.g. lowering cases) are applied to it.

Currently, multi-line blocks can be used in the following constructs:

* as a C expression of the ``callstatement`` statement;

* as a C type specification of the ``callprotoargument`` statement;

* as a C code block of the ``usercode`` statement;

* as a list of C arrays of the ``pymethoddef`` statement;

* as a documentation string.
==================================
Using F2PY bindings in Python
==================================

All wrappers for Fortran/C routines, common blocks, or for Fortran
90 module data generated by F2PY are exposed to Python as ``fortran``
type objects. Routine wrappers are callable ``fortran`` type objects
while wrappers to Fortran data have attributes referring to data
objects.

All ``fortran`` type objects have an attribute ``_cpointer`` that contains a
``CObject`` referring to the C pointer of the corresponding Fortran/C function
or variable at the C level. Such ``CObjects`` can be used as a callback argument
for F2PY generated functions to bypass the Python C/API layer for calling Python
functions from Fortran or C when the computational aspects of such functions are
implemented in C or Fortran and wrapped with F2PY (or any other tool capable of
providing the ``CObject`` of a function).

Consider a Fortran 77 file ```ftype.f``:

  .. literalinclude:: ./code/ftype.f
     :language: fortran

and a wrapper built using ``f2py -c ftype.f -m ftype``.

In Python:

  .. literalinclude:: ./code/results/ftype_session.dat
     :language: python


Scalar arguments
=================

In general, a scalar argument for a F2PY generated wrapper function can
be an ordinary Python scalar (integer, float, complex number) as well as
an arbitrary sequence object (list, tuple, array, string) of
scalars. In the latter case, the first element of the sequence object
is passed to Fortran routine as a scalar argument.

.. note::

   * When type-casting is required and there is possible loss of information via
     narrowing e.g. when type-casting float to integer or complex to float, F2PY
     *does not* raise an exception.

     * For complex to real type-casting only the real part of a complex number is used.

   * ``intent(inout)`` scalar arguments are assumed to be array objects in
     order to have *in situ* changes be effective. It is recommended to use
     arrays with proper type but also other types work.

Consider the following Fortran 77 code:

  .. literalinclude:: ./code/scalar.f
     :language: fortran

and wrap it using ``f2py -c -m scalar scalar.f``.

In Python:

  .. literalinclude:: ./code/results/scalar_session.dat
     :language: python


String arguments
=================

F2PY generated wrapper functions accept almost any Python object as
a string argument, since ``str`` is applied for non-string objects.
Exceptions are NumPy arrays that must have type code ``'c'`` or
``'1'`` when used as string arguments.

A string can have an arbitrary length when used as a string argument
for an F2PY generated wrapper function. If the length is greater than
expected, the string is truncated silently. If the length is smaller than
expected, additional memory is allocated and filled with ``\0``.

Because Python strings are immutable, an ``intent(inout)`` argument
expects an array version of a string in order to have *in situ* changes be effective.

Consider the following Fortran 77 code:

  .. literalinclude:: ./code/string.f
     :language: fortran

and wrap it using ``f2py -c -m mystring string.f``.

Python session:

  .. literalinclude:: ./code/results/string_session.dat
     :language: python


Array arguments
================

In general, array arguments for F2PY generated wrapper functions accept
arbitrary sequences that can be transformed to NumPy array objects. There are
two notable exceptions:

* ``intent(inout)`` array arguments must always be proper-contiguous (defined below) and have a
  compatible ``dtype``, otherwise an exception is raised.
* ``intent(inplace)`` array arguments  will be changed *in situ* if the argument
  has a different type than expected (see the ``intent(inplace)`` attribute for
  more information).

In general, if a NumPy array is proper-contiguous and has a proper type then it
is directly passed to the wrapped Fortran/C function. Otherwise, an element-wise
copy of the input array is made and the copy, being proper-contiguous and with
proper type, is used as the array argument.

There are two types of proper-contiguous NumPy arrays:

* Fortran-contiguous arrays refer to data that is stored columnwise,
  i.e. the indexing of data as stored in memory starts from the lowest
  dimension;
* C-contiguous, or simply contiguous arrays, refer to data that is stored
  rowwise, i.e. the indexing of data as stored in memory starts from the highest
  dimension.

For one-dimensional arrays these notions coincide.

For example, a 2x2 array ``A`` is Fortran-contiguous if its elements
are stored in memory in the following order::

  A[0,0] A[1,0] A[0,1] A[1,1]

and C-contiguous if the order is as follows::

  A[0,0] A[0,1] A[1,0] A[1,1]

To test whether an array is C-contiguous, use the ``.flags.c_contiguous``
attribute of NumPy arrays.  To test for Fortran contiguity, use the
``.flags.f_contiguous`` attribute.

Usually there is no need to worry about how the arrays are stored in memory and
whether the wrapped functions, being either Fortran or C functions, assume one
or another storage order. F2PY automatically ensures that wrapped functions get
arguments with the proper storage order; the underlying algorithm is designed to
make copies of arrays only when absolutely necessary. However, when dealing with
very large multidimensional input arrays with sizes close to the size of the
physical memory in your computer, then care must be taken to ensure the usage of
proper-contiguous and proper type arguments.

To transform input arrays to column major storage order before passing
them to Fortran routines, use the function ``numpy.asfortranarray(<array>)``.

Consider the following Fortran 77 code:

  .. literalinclude:: ./code/array.f
     :language: fortran

and wrap it using ``f2py -c -m arr array.f -DF2PY_REPORT_ON_ARRAY_COPY=1``.

In Python:

  .. literalinclude:: ./code/results/array_session.dat
     :language: python

.. _Call-back arguments:

Call-back arguments
====================

F2PY supports calling Python functions from Fortran or C codes.

Consider the following Fortran 77 code:

  .. literalinclude:: ./code/callback.f
     :language: fortran

and wrap it using ``f2py -c -m callback callback.f``.

In Python:

  .. literalinclude:: ./code/results/callback_session.dat
     :language: python

In the above example F2PY was able to guess accurately the signature
of the call-back function. However, sometimes F2PY cannot establish the
appropriate signature; in these cases the signature of the call-back
function must be explicitly defined in the signature file.

To facilitate this, signature files may contain special modules (the names of
these modules contain the special ``__user__`` sub-string) that defines the
various signatures for call-back functions.  Callback arguments in routine
signatures have the ``external`` attribute (see also the ``intent(callback)``
attribute). To relate a callback argument with its signature in a ``__user__``
module block, a ``use`` statement can be utilized as illustrated below. The same
signature for a callback argument can be referred to in different routine
signatures.

We use the same Fortran 77 code as in the previous example but now
we will pretend that F2PY was not able to guess the signatures of
call-back arguments correctly. First, we create an initial signature
file ``callback2.pyf`` using F2PY::

    f2py -m callback2 -h callback2.pyf callback.f

Then modify it as follows

  .. include:: ./code/callback2.pyf
     :literal:

Finally, we build the extension module using ``f2py -c callback2.pyf callback.f``.

An example Python session for this snippet would be identical to the previous
example except that the argument names would differ.

Sometimes a Fortran package may require that users provide routines
that the package will use. F2PY can construct an interface to such
routines so that Python functions can be called from Fortran.

Consider the following Fortran 77 subroutine that takes an array as its input
and applies a function ``func`` to its elements.

  .. literalinclude:: ./code/calculate.f
     :language: fortran

The Fortran code expects that the function ``func`` has been defined externally.
In order to use a Python function for ``func``, it must have an attribute
``intent(callback)`` and, it must be specified before the ``external`` statement.

Finally, build an extension module using ``f2py -c -m foo calculate.f``

In Python:

  .. literalinclude:: ./code/results/calculate_session.dat
     :language: python

The function is included as an argument to the python function call to the
Fortran subroutine even though it was *not* in the Fortran subroutine argument
list. The "external" keyword refers to the C function generated by f2py, not the
python function itself. The python function is essentially being supplied to the
C function.

The callback function may also be explicitly set in the module.
Then it is not necessary to pass the function in the argument list to
the Fortran function. This may be desired if the Fortran function calling
the python callback function is itself called by another Fortran function.

Consider the following Fortran 77 subroutine:

  .. literalinclude:: ./code/extcallback.f
     :language: fortran

and wrap it using ``f2py -c -m pfromf extcallback.f``.

In Python:

  .. literalinclude:: ./code/results/extcallback_session.dat
     :language: python

Resolving arguments to call-back functions
===========================================

F2PY generated interfaces are very flexible with respect to call-back
arguments.  For each call-back argument an additional optional
argument ``<name>_extra_args`` is introduced by F2PY. This argument
can be used to pass extra arguments to user provided call-back
functions.

If a F2PY generated wrapper function expects the following call-back
argument::

  def fun(a_1,...,a_n):
     ...
     return x_1,...,x_k

but the following Python function

::

  def gun(b_1,...,b_m):
     ...
     return y_1,...,y_l

is provided by a user, and in addition,

::

  fun_extra_args = (e_1,...,e_p)

is used, then the following rules are applied when a Fortran or C
function evaluates the call-back argument ``gun``:

* If ``p == 0`` then ``gun(a_1, ..., a_q)`` is called, here
  ``q = min(m, n)``.
* If ``n + p <= m`` then ``gun(a_1, ..., a_n, e_1, ..., e_p)`` is called.
* If ``p <= m < n + p`` then ``gun(a_1, ..., a_q, e_1, ..., e_p)`` is called, here
  ``q=m-p``.
* If ``p > m`` then ``gun(e_1, ..., e_m)`` is called.
* If ``n + p`` is less than the number of required arguments to ``gun``
  then an exception is raised.

If the function ``gun`` may return any number of objects as a tuple; then
the following rules are applied:

* If ``k < l``, then ``y_{k + 1}, ..., y_l`` are ignored.
* If ``k > l``, then only ``x_1, ..., x_l`` are set.


Common blocks
==============

F2PY generates wrappers to ``common`` blocks defined in a routine
signature block. Common blocks are visible to all Fortran codes linked
to the current extension module, but not to other extension modules
(this restriction is due to the way Python imports shared libraries).  In
Python, the F2PY wrappers to ``common`` blocks are ``fortran`` type
objects that have (dynamic) attributes related to the data members of
the common blocks. When accessed, these attributes return as NumPy array
objects (multidimensional arrays are Fortran-contiguous) which
directly link to data members in common blocks. Data members can be
changed by direct assignment or by in-place changes to the
corresponding array objects.

Consider the following Fortran 77 code:

  .. literalinclude:: ./code/common.f
     :language: fortran

and wrap it using ``f2py -c -m common common.f``.

In Python:

  .. literalinclude:: ./code/results/common_session.dat
     :language: python


Fortran 90 module data
=======================

The F2PY interface to Fortran 90 module data is similar to the handling of Fortran 77
common blocks.

Consider the following Fortran 90 code:

  .. literalinclude:: ./code/moddata.f90
     :language: fortran

and wrap it using ``f2py -c -m moddata moddata.f90``.

In Python:

  .. literalinclude:: ./code/results/moddata_session.dat
     :language: python


Allocatable arrays
===================

F2PY has basic support for Fortran 90 module allocatable arrays.

Consider the following Fortran 90 code:

  .. literalinclude:: ./code/allocarr.f90
     :language: fortran

and wrap it using ``f2py -c -m allocarr allocarr.f90``.

In Python:

  .. literalinclude:: ./code/results/allocarr_session.dat
     :language: python
.. _f2py:

=====================================
F2PY user guide and reference manual
=====================================

The purpose of the ``F2PY`` --*Fortran to Python interface generator*-- utility
is to provide a connection between Python and Fortran
languages.  F2PY is a part of NumPy_ (``numpy.f2py``) and also available as a
standalone command line tool ``f2py`` when ``numpy`` is installed that
facilitates creating/building Python C/API extension modules that make it
possible

* to call Fortran 77/90/95 external subroutines and Fortran 90/95
  module subroutines as well as C functions;
* to access Fortran 77 ``COMMON`` blocks and Fortran 90/95 module data,
  including allocatable arrays

from Python.

.. toctree::
   :maxdepth: 2

   usage
   f2py.getting-started
   python-usage
   signature-file
   buildtools/index
   advanced

.. _Python: https://www.python.org/
.. _NumPy: https://www.numpy.org/
.. _f2py-skbuild:

============================
Using via ``scikit-build``
============================

``scikit-build`` provides two separate concepts geared towards the users of Python extension modules.

1. A ``setuptools`` replacement (legacy behaviour)
2. A series of ``cmake`` modules with definitions which help building Python extensions

.. note::

   It is possible to use ``scikit-build``'s ``cmake`` modules to `bypass the
   cmake setup mechanism`_ completely, and to write targets which call ``f2py
   -c``. This usage is **not recommended** since the point of these build system
   documents are to move away from the internal ``numpy.distutils`` methods.

For situations where no ``setuptools`` replacements are required or wanted (i.e.
if ``wheels`` are not needed), it is recommended to instead use the vanilla
``cmake`` setup described in :ref:`f2py-cmake`.

Fibonacci Walkthrough (F77)
===========================

We will consider the ``fib``  example from :ref:`f2py-getting-started` section.

.. literalinclude:: ./../code/fib1.f
    :language: fortran

``CMake`` modules only
^^^^^^^^^^^^^^^^^^^^^^^

Consider using the following ``CMakeLists.txt``.

.. literalinclude:: ./../code/CMakeLists_skbuild.txt
   :language: cmake

Much of the logic is the same as in :ref:`f2py-cmake`, however notably here the
appropriate module suffix is generated via ``sysconfig.get_config_var("SO")``.
The resulting extension can be built and loaded in the standard workflow.

.. code:: bash

    ls .
    # CMakeLists.txt fib1.f
    cmake -S . -B build
    cmake --build build
    cd build
    python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"
    # [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]


``setuptools`` replacement
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

   **As of November 2021**

   The behavior described here of driving the ``cmake`` build of a module is
   considered to be legacy behaviour and should not be depended on.

The utility of ``scikit-build`` lies in being able to drive the generation of
more than extension modules, in particular a common usage pattern is the
generation of Python distributables (for example for PyPI).

The workflow with ``scikit-build`` straightforwardly supports such packaging requirements. Consider augmenting the project with a ``setup.py`` as defined:

.. literalinclude:: ./../code/setup_skbuild.py
   :language: python

Along with a commensurate ``pyproject.toml``

.. literalinclude:: ./../code/pyproj_skbuild.toml
   :language: toml

Together these can build the extension using ``cmake`` in tandem with other
standard ``setuptools`` outputs. Running ``cmake`` through ``setup.py`` is
mostly used when it is necessary to integrate with extension modules not built
with ``cmake``.

.. code:: bash

    ls .
    # CMakeLists.txt fib1.f pyproject.toml setup.py
    python setup.py build_ext --inplace
    python -c "import numpy as np; import fibby.fibby; a = np.zeros(9); fibby.fibby.fib(a); print (a)"
    # [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]

Where we have modified the path to the module as ``--inplace`` places the
extension module in a subfolder.

.. _bypass the cmake setup mechanism: https://scikit-build.readthedocs.io/en/latest/cmake-modules/F2PY.html
.. _f2py-distutils:

=============================
Using via `numpy.distutils`
=============================

.. currentmodule:: numpy.distutils.core

:mod:`numpy.distutils` is part of NumPy, and extends the standard Python
``distutils`` module to deal with Fortran sources and F2PY signature files, e.g.
compile Fortran sources, call F2PY to construct extension modules, etc.

.. topic:: Example

  Consider the following ``setup_file.py`` for the ``fib`` and ``scalar``
  examples from :ref:`f2py-getting-started` section:

  .. literalinclude:: ./../code/setup_example.py
    :language: python

  Running

  .. code-block:: bash

    python setup_example.py build

  will build two extension modules ``scalar`` and ``fib2`` to the
  build directory.
   
Extensions to ``distutils``
===========================

:mod:`numpy.distutils` extends ``distutils`` with the following features:

* :class:`Extension` class argument ``sources`` may contain Fortran source
  files. In addition, the list ``sources`` may contain at most one
  F2PY signature file, and in this case, the name of an Extension module must
  match with the ``<modulename>`` used in signature file. It is
  assumed that an F2PY signature file contains exactly one ``python
  module`` block.

  If ``sources`` do not contain a signature file, then F2PY is used to scan
  Fortran source files to construct wrappers to the Fortran codes.

  Additional options to the F2PY executable can be given using the
  :class:`Extension` class argument ``f2py_options``.

* The following new ``distutils`` commands are defined:

  ``build_src``
    to construct Fortran wrapper extension modules, among many other things.
  ``config_fc``
    to change Fortran compiler options.

  Additionally, the ``build_ext`` and ``build_clib`` commands are also enhanced
  to support Fortran sources.

  Run

  .. code-block:: bash

    python <setup.py file> config_fc build_src build_ext --help

  to see available options for these commands.

* When building Python packages containing Fortran sources, one
  can choose different Fortran compilers by using the ``build_ext``
  command option ``--fcompiler=<Vendor>``. Here ``<Vendor>`` can be one of the
  following names (on ``linux`` systems)::

    absoft compaq fujitsu g95 gnu gnu95 intel intele intelem lahey nag nagfor nv pathf95 pg vast

  See ``numpy_distutils/fcompiler.py`` for an up-to-date list of
  supported compilers for different platforms, or run

  .. code-block:: bash

     python -m numpy.f2py -c --help-fcompiler
.. _f2py-meson:

===================
Using via ``meson``
===================

The key advantage gained by leveraging ``meson`` over the techniques described
in :ref:`f2py-distutils` is that this feeds into existing systems and larger
projects with ease. ``meson`` has a rather pythonic syntax which makes it more
comfortable and amenable to extension for ``python`` users.

.. note::

    Meson needs to be at-least ``0.46.0`` in order to resolve the ``python`` include directories.


Fibonacci Walkthrough (F77)
===========================


We will need the generated ``C`` wrapper before we can use a general purpose
build system like ``meson``. We will acquire this by:

.. code-block:: bash

    python -n numpy.f2py fib1.f -m fib2

Now, consider the following ``meson.build`` file for the ``fib`` and ``scalar``
examples from :ref:`f2py-getting-started` section:

.. literalinclude:: ./../code/meson.build
    :language: meson

At this point the build will complete, but the import will fail:

.. code-block:: bash

   meson setup builddir
   meson compile -C builddir
   cd builddir
   python -c 'import fib2'
   Traceback (most recent call last):
   File "<string>", line 1, in <module>
   ImportError: fib2.cpython-39-x86_64-linux-gnu.so: undefined symbol: FIB_
   # Check this isn't a false positive
   nm -A fib2.cpython-39-x86_64-linux-gnu.so | grep FIB_
   fib2.cpython-39-x86_64-linux-gnu.so: U FIB_

Recall that the original example, as reproduced below, was in SCREAMCASE:

.. literalinclude:: ./../code/fib1.f
   :language: fortran

With the standard approach, the subroutine exposed to ``python`` is ``fib`` and
not ``FIB``. This means we have a few options. One approach (where possible) is
to lowercase the original Fortran file with say:

.. code-block:: bash

   tr "[:upper:]" "[:lower:]" < fib1.f > fib1.f
   python -n numpy.f2py fib1.f -m fib2
   meson --wipe builddir
   meson compile -C builddir
   cd builddir
   python -c 'import fib2'

However this requires the ability to modify the source which is not always
possible. The easiest way to solve this is to let ``f2py`` deal with it:

.. code-block:: bash

   python -n numpy.f2py fib1.f -m fib2 --lower
   meson --wipe builddir
   meson compile -C builddir
   cd builddir
   python -c 'import fib2'


Automating wrapper generation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A major pain point in the workflow defined above, is the manual tracking of
inputs. Although it would require more effort to figure out the actual outputs
for reasons discussed in :ref:`f2py-bldsys`.

However, we can augment our workflow in a straightforward to take into account
files for which the outputs are known when the build system is set up.

.. literalinclude:: ./../code/meson_upd.build
    :language: meson

This can be compiled and run as before.

.. code-block:: bash

    rm -rf builddir
    meson setup builddir
    meson compile -C builddir
    cd builddir
    python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"
    # [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]

Salient points
===============

It is worth keeping in mind the following:

* ``meson`` will default to passing ``-fimplicit-none`` under ``gfortran`` by
  default, which differs from that of the standard ``np.distutils`` behaviour

* It is not possible to use SCREAMCASE in this context, so either the contents
  of the ``.f`` file or the generated wrapper ``.c`` needs to be lowered to
  regular letters; which can be facilitated by the ``--lower`` option of
  ``F2PY``
.. _f2py-cmake:

===================
Using via ``cmake``
===================

In terms of complexity, ``cmake`` falls between ``make`` and ``meson``. The
learning curve is steeper since CMake syntax is not pythonic and is closer to
``make`` with environment variables.

However, the trade-off is enhanced flexibility and support for most architectures
and compilers. An introduction to the syntax is out of scope for this document,
but this `extensive CMake collection`_ of resources is great.

.. note::

   ``cmake`` is very popular for mixed-language systems, however support for
   ``f2py`` is not particularly native or pleasant; and a more natural approach
   is to consider :ref:`f2py-skbuild`

Fibonacci Walkthrough (F77)
===========================

Returning to the ``fib``  example from :ref:`f2py-getting-started` section.

.. literalinclude:: ./../code/fib1.f
    :language: fortran

We do not need to explicitly generate the ``python -m numpy.f2py fib1.f``
output, which is ``fib1module.c``, which is beneficial. With this; we can now
initialize a ``CMakeLists.txt`` file as follows:

.. literalinclude:: ./../code/CMakeLists.txt
    :language: cmake

A key element of the ``CMakeLists.txt`` file defined above is that the
``add_custom_command`` is used to generate the wrapper ``C`` files and then
added as a dependency of the actual shared library target via a
``add_custom_target`` directive which prevents the command from running every
time. Additionally, the method used for obtaining the ``fortranobject.c`` file
can also be used to grab the ``numpy`` headers on older ``cmake`` versions.

This then works in the same manner as the other modules, although the naming
conventions are different and the output library is not automatically prefixed
with the ``cython`` information.

.. code:: bash

    ls .
    # CMakeLists.txt fib1.f
    cmake -S . -B build
    cmake --build build
    cd build
    python -c "import numpy as np; import fibby; a = np.zeros(9); fibby.fib(a); print (a)"
    # [ 0.  1.  1.  2.  3.  5.  8. 13. 21.]

This is particularly useful where an existing toolchain already exists and
``scikit-build`` or other additional ``python`` dependencies are discouraged.

.. _extensive CMake collection: https://cliutils.gitlab.io/modern-cmake/
.. _f2py-bldsys:

=======================
F2PY and Build Systems
=======================

In this section we will cover the various popular build systems and their usage
with ``f2py``.

.. note::
   **As of November 2021**

   The default build system for ``F2PY`` has traditionally been the through the
   enhanced ``numpy.distutils`` module. This module is based on ``distutils`` which
   will be removed in ``Python 3.12.0`` in **October 2023**; ``setuptools`` does not
   have support for Fortran or ``F2PY`` and it is unclear if it will be supported
   in the future. Alternative methods are thus increasingly more important.


Basic Concepts
===============

Building an extension module which includes Python and Fortran consists of:

- Fortran source(s)
- One or more generated files from ``f2py``

  + A ``C`` wrapper file is always created
  + Code with modules require an additional ``.f90`` wrapper
  + Code with functions generate an additional ``.f`` wrapper

- ``fortranobject.{c,h}``

  + Distributed with ``numpy``
  + Can be queried via ``python -c "import numpy.f2py; print(numpy.f2py.get_include())"``

- NumPy headers

  + Can be queried via ``python -c "import numpy; print(numpy.get_include())"``

- Python libraries and development headers

Broadly speaking there are three cases which arise when considering the outputs of ``f2py``:

Fortran 77 programs
   - Input file ``blah.f``
   - Generates

     + ``blahmodule.c``
     + ``blah-f2pywrappers.f``

   When no ``COMMON`` blocks are present only a ``C`` wrapper file is generated.
   Wrappers are also generated to rewrite assumed shape arrays as automatic
   arrays.

Fortran 90 programs
   - Input file ``blah.f90``
   - Generates:

     + ``blahmodule.c``
     + ``blah-f2pywrappers.f``
     + ``blah-f2pywrappers2.f90``

   The ``f90`` wrapper is used to handle code which is subdivided into
   modules. The ``f`` wrapper makes ``subroutines`` for  ``functions``. It
   rewrites assumed shape arrays as automatic arrays.

Signature files
   - Input file ``blah.pyf``
   - Generates:

     + ``blahmodule.c``
     + ``blah-f2pywrappers2.f90`` (occasionally)
     + ``blah-f2pywrappers.f`` (occasionally)

   Signature files ``.pyf`` do not signal their language standard via the file
   extension, they may generate the F90 and F77 specific wrappers depending on
   their contents; which shifts the burden of checking for generated files onto
   the build system.

.. note::

   The signature file output situation is being reconsidered in `issue 20385`_ .


In theory keeping the above requirements in hand, any build system can be
adapted to generate ``f2py`` extension modules. Here we will cover a subset of
the more popular systems.

.. note::
   ``make`` has no place in a modern multi-language setup, and so is not
   discussed further.

Build Systems
==============

.. toctree::
   :maxdepth: 2

   distutils
   meson
   cmake
   skbuild

.. _`issue 20385`: https://github.com/numpy/numpy/issues/20385
.. _basics.broadcasting:
.. _array-broadcasting-in-numpy:

************
Broadcasting
************

.. seealso::
    :class:`numpy.broadcast`   


The term broadcasting describes how NumPy treats arrays with different
shapes during arithmetic operations. Subject to certain constraints,
the smaller array is "broadcast" across the larger array so that they
have compatible shapes. Broadcasting provides a means of vectorizing
array operations so that looping occurs in C instead of Python. It does
this without making needless copies of data and usually leads to
efficient algorithm implementations. There are, however, cases where
broadcasting is a bad idea because it leads to inefficient use of memory
that slows computation.

NumPy operations are usually done on pairs of arrays on an
element-by-element basis.  In the simplest case, the two arrays must
have exactly the same shape, as in the following example:

  >>> a = np.array([1.0, 2.0, 3.0])
  >>> b = np.array([2.0, 2.0, 2.0])
  >>> a * b
  array([2.,  4.,  6.])

NumPy's broadcasting rule relaxes this constraint when the arrays'
shapes meet certain constraints. The simplest broadcasting example occurs
when an array and a scalar value are combined in an operation:

>>> a = np.array([1.0, 2.0, 3.0])
>>> b = 2.0
>>> a * b
array([2.,  4.,  6.])

The result is equivalent to the previous example where ``b`` was an array.
We can think of the scalar ``b`` being *stretched* during the arithmetic
operation into an array with the same shape as ``a``. The new elements in
``b``, as shown in :ref:`broadcasting.figure-1`, are simply copies of the
original scalar. The stretching analogy is
only conceptual.  NumPy is smart enough to use the original scalar value
without actually making copies so that broadcasting operations are as
memory and computationally efficient as possible.

.. figure:: broadcasting_1.png
    :alt: A scalar is broadcast to match the shape of the 1-d array it
          is being multiplied to.
    :name: broadcasting.figure-1

    *Figure 1*

    *In the simplest example of broadcasting, the scalar* ``b`` *is
    stretched to become an array of same shape as* ``a`` *so the shapes
    are compatible for element-by-element multiplication.*

The code in the second example is more efficient than that in the first
because broadcasting moves less memory around during the multiplication
(``b`` is a scalar rather than an array).

.. _general-broadcasting-rules:

General Broadcasting Rules
==========================
When operating on two arrays, NumPy compares their shapes element-wise.
It starts with the trailing (i.e. rightmost) dimensions and works its
way left.  Two dimensions are compatible when

1) they are equal, or
2) one of them is 1

If these conditions are not met, a
``ValueError: operands could not be broadcast together`` exception is 
thrown, indicating that the arrays have incompatible shapes. The size of 
the resulting array is the size that is not 1 along each axis of the inputs.

Arrays do not need to have the same *number* of dimensions.  For example,
if you have a ``256x256x3`` array of RGB values, and you want to scale
each color in the image by a different value, you can multiply the image
by a one-dimensional array with 3 values. Lining up the sizes of the
trailing axes of these arrays according to the broadcast rules, shows that
they are compatible::

  Image  (3d array): 256 x 256 x 3
  Scale  (1d array):             3
  Result (3d array): 256 x 256 x 3

When either of the dimensions compared is one, the other is
used.  In other words, dimensions with size 1 are stretched or "copied"
to match the other.

In the following example, both the ``A`` and ``B`` arrays have axes with
length one that are expanded to a larger size during the broadcast
operation::

  A      (4d array):  8 x 1 x 6 x 1
  B      (3d array):      7 x 1 x 5
  Result (4d array):  8 x 7 x 6 x 5


.. _arrays.broadcasting.broadcastable:

Broadcastable arrays
====================

.. index:: broadcastable

A set of arrays is called "broadcastable" to the same shape if
the above rules produce a valid result.

For example, if ``a.shape`` is (5,1), ``b.shape`` is (1,6), ``c.shape`` is (6,)
and ``d.shape`` is () so that *d* is a scalar, then *a*, *b*, *c*,
and *d* are all broadcastable to dimension (5,6); and

- *a* acts like a (5,6) array where ``a[:,0]`` is broadcast to the other
  columns,

- *b* acts like a (5,6) array where ``b[0,:]`` is broadcast
  to the other rows,

- *c* acts like a (1,6) array and therefore like a (5,6) array
  where ``c[:]`` is broadcast to every row, and finally,

- *d* acts like a (5,6) array where the single value is repeated.

Here are some more examples::

  A      (2d array):  5 x 4
  B      (1d array):      1
  Result (2d array):  5 x 4

  A      (2d array):  5 x 4
  B      (1d array):      4
  Result (2d array):  5 x 4

  A      (3d array):  15 x 3 x 5
  B      (3d array):  15 x 1 x 5
  Result (3d array):  15 x 3 x 5

  A      (3d array):  15 x 3 x 5
  B      (2d array):       3 x 5
  Result (3d array):  15 x 3 x 5

  A      (3d array):  15 x 3 x 5
  B      (2d array):       3 x 1
  Result (3d array):  15 x 3 x 5

Here are examples of shapes that do not broadcast::

  A      (1d array):  3
  B      (1d array):  4 # trailing dimensions do not match

  A      (2d array):      2 x 1
  B      (3d array):  8 x 4 x 3 # second from last dimensions mismatched

An example of broadcasting when a 1-d array is added to a 2-d array::

  >>> a = np.array([[ 0.0,  0.0,  0.0],
  ...               [10.0, 10.0, 10.0],
  ...               [20.0, 20.0, 20.0],
  ...               [30.0, 30.0, 30.0]])
  >>> b = np.array([1.0, 2.0, 3.0])
  >>> a + b
  array([[  1.,   2.,   3.],
          [11.,  12.,  13.],
          [21.,  22.,  23.],
          [31.,  32.,  33.]])
  >>> b = np.array([1.0, 2.0, 3.0, 4.0])
  >>> a + b 
  Traceback (most recent call last):
  ValueError: operands could not be broadcast together with shapes (4,3) (4,)

As shown in :ref:`broadcasting.figure-2`, ``b`` is added to each row of ``a``.
In :ref:`broadcasting.figure-3`, an exception is raised because of the
incompatible shapes.

.. figure:: broadcasting_2.png
    :alt: A 1-d array with shape (3) is strectched to match the 2-d array of
          shape (4, 3) it is being added to, and the result is a 2-d array of shape
          (4, 3).
    :name: broadcasting.figure-2

    *Figure 2*

    *A one dimensional array added to a two dimensional array results in
    broadcasting if number of 1-d array elements matches the number of 2-d
    array columns.*

.. figure:: broadcasting_3.png
    :alt: A huge cross over the 2-d array of shape (4, 3) and the 1-d array
          of shape (4) shows that they can not be broadcast due to mismatch
          of shapes and thus produce no result.
    :name: broadcasting.figure-3

    *Figure 3*

    *When the trailing dimensions of the arrays are unequal, broadcasting fails
    because it is impossible to align the values in the rows of the 1st array
    with the elements of the 2nd arrays for element-by-element addition.*

Broadcasting provides a convenient way of taking the outer product (or
any other outer operation) of two arrays. The following example shows an
outer addition operation of two 1-d arrays::

  >>> a = np.array([0.0, 10.0, 20.0, 30.0])
  >>> b = np.array([1.0, 2.0, 3.0])
  >>> a[:, np.newaxis] + b
  array([[ 1.,   2.,   3.],
         [11.,  12.,  13.],
         [21.,  22.,  23.],
         [31.,  32.,  33.]])

.. figure:: broadcasting_4.png
    :alt: A 2-d array of shape (4, 1) and a 1-d array of shape (3) are
          stretched to match their shapes and produce a resultant array
          of shape (4, 3).
    :name: broadcasting.figure-4

    *Figure 4*

    *In some cases, broadcasting stretches both arrays to form an output array
    larger than either of the initial arrays.*

Here the ``newaxis`` index operator inserts a new axis into ``a``,
making it a two-dimensional ``4x1`` array.  Combining the ``4x1`` array
with ``b``, which has shape ``(3,)``, yields a ``4x3`` array.

A Practical Example: Vector Quantization
========================================

Broadcasting comes up quite often in real world problems. A typical example
occurs in the vector quantization (VQ) algorithm used in information theory,
classification, and other related areas. The basic operation in VQ finds
the closest point in a set of points, called ``codes`` in VQ jargon, to a given
point, called the ``observation``. In the very simple, two-dimensional case
shown below, the values in ``observation`` describe the weight and height of an
athlete to be classified. The ``codes`` represent different classes of
athletes. [#f1]_ Finding the closest point requires calculating the distance
between observation and each of the codes. The shortest distance provides the
best match. In this example, ``codes[0]`` is the closest class indicating that
the athlete is likely a basketball player.

  >>> from numpy import array, argmin, sqrt, sum
  >>> observation = array([111.0, 188.0])
  >>> codes = array([[102.0, 203.0],
  ...                [132.0, 193.0],
  ...                [45.0, 155.0],
  ...                [57.0, 173.0]])
  >>> diff = codes - observation    # the broadcast happens here
  >>> dist = sqrt(sum(diff**2,axis=-1))
  >>> argmin(dist)
  0

In this example, the ``observation`` array is stretched to match
the shape of the ``codes`` array::

  Observation      (1d array):      2
  Codes            (2d array):  4 x 2
  Diff             (2d array):  4 x 2

.. figure:: broadcasting_5.png
    :alt: A height versus weight graph that shows data of a female
          gymnast, marathon runner, basketball player, football
          lineman and the athlete to be classified. Shortest distance
          is found between the basketball player and the athlete
          to be classified. 
    :name: broadcasting.figure-5

    *Figure 5*

    *The basic operation of vector quantization calculates the distance between
    an object to be classified, the dark square, and multiple known codes, the
    gray circles. In this simple case, the codes represent individual classes.
    More complex cases use multiple codes per class.*

Typically, a large number of ``observations``, perhaps read from a database,
are compared to a set of ``codes``. Consider this scenario::

  Observation      (2d array):      10 x 3
  Codes            (2d array):       5 x 3
  Diff             (3d array):  5 x 10 x 3 

The three-dimensional array, ``diff``, is a consequence of broadcasting, not a
necessity for the calculation. Large data sets will generate a large
intermediate array that is computationally inefficient. Instead, if each
observation is calculated individually using a Python loop around the code
in the two-dimensional example above, a much smaller array is used.

Broadcasting is a powerful tool for writing short and usually intuitive code
that does its computations very efficiently in C. However, there are cases
when broadcasting uses unnecessarily large amounts of memory for a particular
algorithm. In these cases, it is better to write the algorithm's outer loop in
Python. This may also produce more readable code, as algorithms that use
broadcasting tend to become more difficult to interpret as the number of
dimensions in the broadcast increases.

.. rubric:: Footnotes

.. [#f1]
    In this example, weight has more impact on the distance calculation
    than height because of the larger values. In practice, it is important to
    normalize the height and weight, often by their standard deviation across the
    data set, so that both have equal influence on the distance calculation.
.. sectionauthor:: Pierre Gerard-Marchant <pierregmcode@gmail.com>

*********************************************
Importing data with :func:`~numpy.genfromtxt`
*********************************************

NumPy provides several functions to create arrays from tabular data.
We focus here on the :func:`~numpy.genfromtxt` function.

In a nutshell, :func:`~numpy.genfromtxt` runs two main loops.  The first
loop converts each line of the file in a sequence of strings.  The second
loop converts each string to the appropriate data type.  This mechanism is
slower than a single loop, but gives more flexibility.  In particular,
:func:`~numpy.genfromtxt` is able to take missing data into account, when
other faster and simpler functions like :func:`~numpy.loadtxt` cannot.

.. note::

   When giving examples, we will use the following conventions::

       >>> import numpy as np
       >>> from io import StringIO



Defining the input
==================

The only mandatory argument of :func:`~numpy.genfromtxt` is the source of
the data. It can be a string, a list of strings, a generator or an open
file-like object with a ``read`` method, for example, a file or 
:class:`io.StringIO` object. If a single string is provided, it is assumed
to be the name of a local or remote file. If a list of strings or a generator
returning strings is provided, each string is treated as one line in a file.
When the URL of a remote file is passed, the file is automatically downloaded
to the current directory and opened.

Recognized file types are text files and archives.  Currently, the function
recognizes ``gzip`` and ``bz2`` (``bzip2``) archives.  The type of
the archive is determined from the extension of the file: if the filename
ends with ``'.gz'``, a ``gzip`` archive is expected; if it ends with
``'bz2'``, a ``bzip2`` archive is assumed.



Splitting the lines into columns
================================

The ``delimiter`` argument
--------------------------

Once the file is defined and open for reading, :func:`~numpy.genfromtxt`
splits each non-empty line into a sequence of strings.  Empty or commented
lines are just skipped.  The ``delimiter`` keyword is used to define
how the splitting should take place.

Quite often, a single character marks the separation between columns.  For
example, comma-separated files (CSV) use a comma (``,``) or a semicolon
(``;``) as delimiter::

   >>> data = u"1, 2, 3\n4, 5, 6"
   >>> np.genfromtxt(StringIO(data), delimiter=",")
   array([[1.,  2.,  3.],
          [4.,  5.,  6.]])

Another common separator is ``"\t"``, the tabulation character.  However,
we are not limited to a single character, any string will do.  By default,
:func:`~numpy.genfromtxt` assumes ``delimiter=None``, meaning that the line
is split along white spaces (including tabs) and that consecutive white
spaces are considered as a single white space.

Alternatively, we may be dealing with a fixed-width file, where columns are
defined as a given number of characters.  In that case, we need to set
``delimiter`` to a single integer (if all the columns have the same
size) or to a sequence of integers (if columns can have different sizes)::

   >>> data = u"  1  2  3\n  4  5 67\n890123  4"
   >>> np.genfromtxt(StringIO(data), delimiter=3)
   array([[  1.,    2.,    3.],
          [  4.,    5.,   67.],
          [890.,  123.,    4.]])
   >>> data = u"123456789\n   4  7 9\n   4567 9"
   >>> np.genfromtxt(StringIO(data), delimiter=(4, 3, 2))
   array([[1234.,   567.,    89.],
          [   4.,     7.,     9.],
          [   4.,   567.,     9.]])


The ``autostrip`` argument
--------------------------

By default, when a line is decomposed into a series of strings, the
individual entries are not stripped of leading nor trailing white spaces.
This behavior can be overwritten by setting the optional argument
``autostrip`` to a value of ``True``::

   >>> data = u"1, abc , 2\n 3, xxx, 4"
   >>> # Without autostrip
   >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5")
   array([['1', ' abc ', ' 2'],
          ['3', ' xxx', ' 4']], dtype='<U5')
   >>> # With autostrip
   >>> np.genfromtxt(StringIO(data), delimiter=",", dtype="|U5", autostrip=True)
   array([['1', 'abc', '2'],
          ['3', 'xxx', '4']], dtype='<U5')


The ``comments`` argument
-------------------------

The optional argument ``comments`` is used to define a character
string that marks the beginning of a comment.  By default,
:func:`~numpy.genfromtxt` assumes ``comments='#'``.  The comment marker may
occur anywhere on the line.  Any character present after the comment
marker(s) is simply ignored::

   >>> data = u"""#
   ... # Skip me !
   ... # Skip me too !
   ... 1, 2
   ... 3, 4
   ... 5, 6 #This is the third line of the data
   ... 7, 8
   ... # And here comes the last line
   ... 9, 0
   ... """
   >>> np.genfromtxt(StringIO(data), comments="#", delimiter=",")
   array([[1., 2.],
          [3., 4.],
          [5., 6.],
          [7., 8.],
          [9., 0.]])

.. versionadded:: 1.7.0

    When ``comments`` is set to ``None``, no lines are treated as comments.

.. note::

   There is one notable exception to this behavior: if the optional argument
   ``names=True``, the first commented line will be examined for names.


Skipping lines and choosing columns
===================================

The ``skip_header`` and ``skip_footer`` arguments
---------------------------------------------------------------

The presence of a header in the file can hinder data processing.  In that
case, we need to use the ``skip_header`` optional argument.  The
values of this argument must be an integer which corresponds to the number
of lines to skip at the beginning of the file, before any other action is
performed.  Similarly, we can skip the last ``n`` lines of the file by
using the ``skip_footer`` attribute and giving it a value of ``n``::

   >>> data = u"\n".join(str(i) for i in range(10))
   >>> np.genfromtxt(StringIO(data),)
   array([0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])
   >>> np.genfromtxt(StringIO(data),
   ...               skip_header=3, skip_footer=5)
   array([3.,  4.])

By default, ``skip_header=0`` and ``skip_footer=0``, meaning that no lines
are skipped.


The ``usecols`` argument
------------------------

In some cases, we are not interested in all the columns of the data but
only a few of them.  We can select which columns to import with the
``usecols`` argument.  This argument accepts a single integer or a
sequence of integers corresponding to the indices of the columns to import.
Remember that by convention, the first column has an index of 0.  Negative
integers behave the same as regular Python negative indexes.

For example, if we want to import only the first and the last columns, we
can use ``usecols=(0, -1)``::

   >>> data = u"1 2 3\n4 5 6"
   >>> np.genfromtxt(StringIO(data), usecols=(0, -1))
   array([[1.,  3.],
          [4.,  6.]])

If the columns have names, we can also select which columns to import by
giving their name to the ``usecols`` argument, either as a sequence
of strings or a comma-separated string::

   >>> data = u"1 2 3\n4 5 6"
   >>> np.genfromtxt(StringIO(data),
   ...               names="a, b, c", usecols=("a", "c"))
   array([(1., 3.), (4., 6.)], dtype=[('a', '<f8'), ('c', '<f8')])
   >>> np.genfromtxt(StringIO(data),
   ...               names="a, b, c", usecols=("a, c"))
       array([(1., 3.), (4., 6.)], dtype=[('a', '<f8'), ('c', '<f8')])




Choosing the data type
======================

The main way to control how the sequences of strings we have read from the
file are converted to other types is to set the ``dtype`` argument.
Acceptable values for this argument are:

* a single type, such as ``dtype=float``.
  The output will be 2D with the given dtype, unless a name has been
  associated with each column with the use of the ``names`` argument
  (see below).  Note that ``dtype=float`` is the default for
  :func:`~numpy.genfromtxt`.
* a sequence of types, such as ``dtype=(int, float, float)``.
* a comma-separated string, such as ``dtype="i4,f8,|U3"``.
* a dictionary with two keys ``'names'`` and ``'formats'``.
* a sequence of tuples ``(name, type)``, such as
  ``dtype=[('A', int), ('B', float)]``.
* an existing :class:`numpy.dtype` object.
* the special value ``None``.
  In that case, the type of the columns will be determined from the data
  itself (see below).

In all the cases but the first one, the output will be a 1D array with a
structured dtype.  This dtype has as many fields as items in the sequence.
The field names are defined with the ``names`` keyword.


When ``dtype=None``, the type of each column is determined iteratively from
its data.  We start by checking whether a string can be converted to a
boolean (that is, if the string matches ``true`` or ``false`` in lower
cases); then whether it can be converted to an integer, then to a float,
then to a complex and eventually to a string.

The option ``dtype=None`` is provided for convenience.  However, it is
significantly slower than setting the dtype explicitly.



Setting the names
=================

The ``names`` argument
----------------------

A natural approach when dealing with tabular data is to allocate a name to
each column.  A first possibility is to use an explicit structured dtype,
as mentioned previously::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, dtype=[(_, int) for _ in "abc"])
   array([(1, 2, 3), (4, 5, 6)],
         dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])

Another simpler possibility is to use the ``names`` keyword with a
sequence of strings or a comma-separated string::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, names="A, B, C")
   array([(1., 2., 3.), (4., 5., 6.)],
         dtype=[('A', '<f8'), ('B', '<f8'), ('C', '<f8')])

In the example above, we used the fact that by default, ``dtype=float``.
By giving a sequence of names, we are forcing the output to a structured
dtype.

We may sometimes need to define the column names from the data itself.  In
that case, we must use the ``names`` keyword with a value of
``True``.  The names will then be read from the first line (after the
``skip_header`` ones), even if the line is commented out::

   >>> data = StringIO("So it goes\n#a b c\n1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, skip_header=1, names=True)
   array([(1., 2., 3.), (4., 5., 6.)],
         dtype=[('a', '<f8'), ('b', '<f8'), ('c', '<f8')])

The default value of ``names`` is ``None``.  If we give any other
value to the keyword, the new names will overwrite the field names we may
have defined with the dtype::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> ndtype=[('a',int), ('b', float), ('c', int)]
   >>> names = ["A", "B", "C"]
   >>> np.genfromtxt(data, names=names, dtype=ndtype)
   array([(1, 2., 3), (4, 5., 6)],
         dtype=[('A', '<i8'), ('B', '<f8'), ('C', '<i8')])


The ``defaultfmt`` argument
---------------------------

If ``names=None`` but a structured dtype is expected, names are defined
with the standard NumPy default of ``"f%i"``, yielding names like ``f0``,
``f1`` and so forth::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, dtype=(int, float, int))
   array([(1, 2., 3), (4, 5., 6)],
         dtype=[('f0', '<i8'), ('f1', '<f8'), ('f2', '<i8')])

In the same way, if we don't give enough names to match the length of the
dtype, the missing names will be defined with this default template::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, dtype=(int, float, int), names="a")
   array([(1, 2., 3), (4, 5., 6)],
         dtype=[('a', '<i8'), ('f0', '<f8'), ('f1', '<i8')])

We can overwrite this default with the ``defaultfmt`` argument, that
takes any format string::

   >>> data = StringIO("1 2 3\n 4 5 6")
   >>> np.genfromtxt(data, dtype=(int, float, int), defaultfmt="var_%02i")
   array([(1, 2., 3), (4, 5., 6)],
         dtype=[('var_00', '<i8'), ('var_01', '<f8'), ('var_02', '<i8')])

.. note::

   We need to keep in mind that ``defaultfmt`` is used only if some names
   are expected but not defined.


Validating names
----------------

NumPy arrays with a structured dtype can also be viewed as
:class:`~numpy.recarray`, where a field can be accessed as if it were an
attribute.  For that reason, we may need to make sure that the field name
doesn't contain any space or invalid character, or that it does not
correspond to the name of a standard attribute (like ``size`` or
``shape``), which would confuse the interpreter.  :func:`~numpy.genfromtxt`
accepts three optional arguments that provide a finer control on the names:

   ``deletechars``
      Gives a string combining all the characters that must be deleted from
      the name. By default, invalid characters are
      ``~!@#$%^&*()-=+~\|]}[{';:
      /?.>,<``.
   ``excludelist``
      Gives a list of the names to exclude, such as ``return``, ``file``,
      ``print``...  If one of the input name is part of this list, an
      underscore character (``'_'``) will be appended to it.
   ``case_sensitive``
      Whether the names should be case-sensitive (``case_sensitive=True``),
      converted to upper case (``case_sensitive=False`` or
      ``case_sensitive='upper'``) or to lower case
      (``case_sensitive='lower'``).



Tweaking the conversion
=======================

The ``converters`` argument
---------------------------

Usually, defining a dtype is sufficient to define how the sequence of
strings must be converted.  However, some additional control may sometimes
be required.  For example, we may want to make sure that a date in a format
``YYYY/MM/DD`` is converted to a :class:`~datetime.datetime` object, or that
a string like ``xx%`` is properly converted to a float between 0 and 1.  In
such cases, we should define conversion functions with the ``converters``
arguments.

The value of this argument is typically a dictionary with column indices or
column names as keys and a conversion functions as values.  These
conversion functions can either be actual functions or lambda functions. In
any case, they should accept only a string as input and output only a
single element of the wanted type.

In the following example, the second column is converted from as string
representing a percentage to a float between 0 and 1::

   >>> convertfunc = lambda x: float(x.strip(b"%"))/100.
   >>> data = u"1, 2.3%, 45.\n6, 78.9%, 0"
   >>> names = ("i", "p", "n")
   >>> # General case .....
   >>> np.genfromtxt(StringIO(data), delimiter=",", names=names)
   array([(1., nan, 45.), (6., nan, 0.)],
         dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])

We need to keep in mind that by default, ``dtype=float``.  A float is
therefore expected for the second column.  However, the strings ``' 2.3%'``
and ``' 78.9%'`` cannot be converted to float and we end up having
``np.nan`` instead.  Let's now use a converter::

   >>> # Converted case ...
   >>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
   ...               converters={1: convertfunc})
   array([(1., 0.023, 45.), (6., 0.789, 0.)],
         dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])

The same results can be obtained by using the name of the second column
(``"p"``) as key instead of its index (1)::

   >>> # Using a name for the converter ...
   >>> np.genfromtxt(StringIO(data), delimiter=",", names=names,
   ...               converters={"p": convertfunc})
   array([(1., 0.023, 45.), (6., 0.789, 0.)],
         dtype=[('i', '<f8'), ('p', '<f8'), ('n', '<f8')])


Converters can also be used to provide a default for missing entries.  In
the following example, the converter ``convert`` transforms a stripped
string into the corresponding float or into -999 if the string is empty.
We need to explicitly strip the string from white spaces as it is not done
by default::

   >>> data = u"1, , 3\n 4, 5, 6"
   >>> convert = lambda x: float(x.strip() or -999)
   >>> np.genfromtxt(StringIO(data), delimiter=",",
   ...               converters={1: convert})
   array([[   1., -999.,    3.],
          [   4.,    5.,    6.]])




Using missing and filling values
--------------------------------

Some entries may be missing in the dataset we are trying to import.  In a
previous example, we used a converter to transform an empty string into a
float.  However, user-defined converters may rapidly become cumbersome to
manage.

The :func:`~numpy.genfromtxt` function provides two other complementary
mechanisms: the ``missing_values`` argument is used to recognize
missing data and a second argument, ``filling_values``, is used to
process these missing data.

``missing_values``
------------------

By default, any empty string is marked as missing.  We can also consider
more complex strings, such as ``"N/A"`` or ``"???"`` to represent missing
or invalid data.  The ``missing_values`` argument accepts three kinds
of values:

   a string or a comma-separated string
      This string will be used as the marker for missing data for all the
      columns
   a sequence of strings
      In that case, each item is associated to a column, in order.
   a dictionary
      Values of the dictionary are strings or sequence of strings.  The
      corresponding keys can be column indices (integers) or column names
      (strings). In addition, the special key ``None`` can be used to
      define a default applicable to all columns.


``filling_values``
------------------

We know how to recognize missing data, but we still need to provide a value
for these missing entries.  By default, this value is determined from the
expected dtype according to this table:

=============  ==============
Expected type  Default
=============  ==============
``bool``       ``False``
``int``        ``-1``
``float``      ``np.nan``
``complex``    ``np.nan+0j``
``string``     ``'???'``
=============  ==============

We can get a finer control on the conversion of missing values with the
``filling_values`` optional argument.  Like
``missing_values``, this argument accepts different kind of values:

   a single value
      This will be the default for all columns
   a sequence of values
      Each entry will be the default for the corresponding column
   a dictionary
      Each key can be a column index or a column name, and the
      corresponding value should be a single object.  We can use the
      special key ``None`` to define a default for all columns.

In the following example, we suppose that the missing values are flagged
with ``"N/A"`` in the first column and by ``"???"`` in the third column.
We wish to transform these missing values to 0 if they occur in the first
and second column, and to -999 if they occur in the last column::

    >>> data = u"N/A, 2, 3\n4, ,???"
    >>> kwargs = dict(delimiter=",",
    ...               dtype=int,
    ...               names="a,b,c",
    ...               missing_values={0:"N/A", 'b':" ", 2:"???"},
    ...               filling_values={0:0, 'b':0, 2:-999})
    >>> np.genfromtxt(StringIO(data), **kwargs)
    array([(0, 2, 3), (4, 0, -999)],
          dtype=[('a', '<i8'), ('b', '<i8'), ('c', '<i8')])


``usemask``
-----------

We may also want to keep track of the occurrence of missing data by
constructing a boolean mask, with ``True`` entries where data was missing
and ``False`` otherwise.  To do that, we just have to set the optional
argument ``usemask`` to ``True`` (the default is ``False``).  The
output array will then be a :class:`~numpy.ma.MaskedArray`.


.. unpack=None, loose=True, invalid_raise=True)


Shortcut functions
==================

In addition to :func:`~numpy.genfromtxt`, the ``numpy.lib.npyio`` module
provides several convenience functions derived from
:func:`~numpy.genfromtxt`.  These functions work the same way as the
original, but they have different default values.

``numpy.lib.npyio.recfromtxt``
   Returns a standard :class:`numpy.recarray` (if ``usemask=False``) or a
   ``numpy.ma.mrecords.MaskedRecords`` array (if ``usemaske=True``).  The
   default dtype is ``dtype=None``, meaning that the types of each column
   will be automatically determined.
``numpy.lib.npyio.recfromcsv``
   Like ``numpy.lib.npyio.recfromtxt``, but with a default ``delimiter=","``.
*************
Miscellaneous
*************

IEEE 754 Floating Point Special Values
--------------------------------------

Special values defined in numpy: nan, inf,

NaNs can be used as a poor-man's mask (if you don't care what the
original value was)

Note: cannot use equality to test NaNs. E.g.: ::

 >>> myarr = np.array([1., 0., np.nan, 3.])
 >>> np.nonzero(myarr == np.nan)
 (array([], dtype=int64),)
 >>> np.nan == np.nan  # is always False! Use special numpy functions instead.
 False
 >>> myarr[myarr == np.nan] = 0. # doesn't work
 >>> myarr
 array([  1.,   0.,  nan,   3.])
 >>> myarr[np.isnan(myarr)] = 0. # use this instead find
 >>> myarr
 array([1.,  0.,  0.,  3.])

Other related special value functions: ::

 isinf():    True if value is inf
 isfinite(): True if not nan or inf
 nan_to_num(): Map nan to 0, inf to max float, -inf to min float

The following corresponds to the usual functions except that nans are excluded
from the results: ::

 nansum()
 nanmax()
 nanmin()
 nanargmax()
 nanargmin()

 >>> x = np.arange(10.)
 >>> x[3] = np.nan
 >>> x.sum()
 nan
 >>> np.nansum(x)
 42.0

How numpy handles numerical exceptions
--------------------------------------

The default is to ``'warn'`` for ``invalid``, ``divide``, and ``overflow``
and ``'ignore'`` for ``underflow``.  But this can be changed, and it can be
set individually for different kinds of exceptions. The different behaviors
are:

 - 'ignore' : Take no action when the exception occurs.
 - 'warn'   : Print a `RuntimeWarning` (via the Python `warnings` module).
 - 'raise'  : Raise a `FloatingPointError`.
 - 'call'   : Call a function specified using the `seterrcall` function.
 - 'print'  : Print a warning directly to ``stdout``.
 - 'log'    : Record error in a Log object specified by `seterrcall`.

These behaviors can be set for all kinds of errors or specific ones:

 - all       : apply to all numeric exceptions
 - invalid   : when NaNs are generated
 - divide    : divide by zero (for integers as well!)
 - overflow  : floating point overflows
 - underflow : floating point underflows

Note that integer divide-by-zero is handled by the same machinery.
These behaviors are set on a per-thread basis.

Examples
--------

::

 >>> oldsettings = np.seterr(all='warn')
 >>> np.zeros(5,dtype=np.float32)/0.
 Traceback (most recent call last):
 ...
 RuntimeWarning: invalid value encountered in divide
 >>> j = np.seterr(under='ignore')
 >>> np.array([1.e-100])**10
 array([0.])
 >>> j = np.seterr(invalid='raise')
 >>> np.sqrt(np.array([-1.]))
 Traceback (most recent call last):
 ...
 FloatingPointError: invalid value encountered in sqrt
 >>> def errorhandler(errstr, errflag):
 ...      print("saw stupid error!")
 >>> np.seterrcall(errorhandler)
 >>> j = np.seterr(all='call')
 >>> np.zeros(5, dtype=np.int32)/0
 saw stupid error!
 array([nan, nan, nan, nan, nan])
 >>> j = np.seterr(**oldsettings) # restore previous
 ...                              # error-handling settings

Interfacing to C
----------------
Only a survey of the choices. Little detail on how each works.

1) Bare metal, wrap your own C-code manually.

 - Plusses:

   - Efficient
   - No dependencies on other tools

 - Minuses:

   - Lots of learning overhead:

     - need to learn basics of Python C API
     - need to learn basics of numpy C API
     - need to learn how to handle reference counting and love it.

   - Reference counting often difficult to get right.

     - getting it wrong leads to memory leaks, and worse, segfaults

   - API will change for Python 3.0!

2) Cython

 - Plusses:

   - avoid learning C API's
   - no dealing with reference counting
   - can code in pseudo python and generate C code
   - can also interface to existing C code
   - should shield you from changes to Python C api
   - has become the de-facto standard within the scientific Python community
   - fast indexing support for arrays

 - Minuses:

   - Can write code in non-standard form which may become obsolete
   - Not as flexible as manual wrapping

3) ctypes

 - Plusses:

   - part of Python standard library
   - good for interfacing to existing shareable libraries, particularly
     Windows DLLs
   - avoids API/reference counting issues
   - good numpy support: arrays have all these in their ctypes
     attribute: ::

       a.ctypes.data
       a.ctypes.data_as
       a.ctypes.shape
       a.ctypes.shape_as
       a.ctypes.strides
       a.ctypes.strides_as

 - Minuses:

   - can't use for writing code to be turned into C extensions, only a wrapper
     tool.

4) SWIG (automatic wrapper generator)

 - Plusses:

   - around a long time
   - multiple scripting language support
   - C++ support
   - Good for wrapping large (many functions) existing C libraries

 - Minuses:

   - generates lots of code between Python and the C code
   - can cause performance problems that are nearly impossible to optimize
     out
   - interface files can be hard to write
   - doesn't necessarily avoid reference counting issues or needing to know
     API's

5) scipy.weave

 - Plusses:

   - can turn many numpy expressions into C code
   - dynamic compiling and loading of generated C code
   - can embed pure C code in Python module and have weave extract, generate
     interfaces and compile, etc.

 - Minuses:

   - Future very uncertain: it's the only part of Scipy not ported to Python 3
     and is effectively deprecated in favor of Cython.

6) Psyco

 - Plusses:

   - Turns pure python into efficient machine code through jit-like
     optimizations
   - very fast when it optimizes well

 - Minuses:

   - Only on intel (windows?)
   - Doesn't do much for numpy?

Interfacing to Fortran:
-----------------------
The clear choice to wrap Fortran code is
`f2py <https://docs.scipy.org/doc/numpy/f2py/>`_.

Pyfort is an older alternative, but not supported any longer.
Fwrap is a newer project that looked promising but isn't being developed any
longer.

Interfacing to C++:
-------------------
 1) Cython
 2) CXX
 3) Boost.python
 4) SWIG
 5) SIP (used mainly in PyQT)



****************************************
NumPy: the absolute basics for beginners
****************************************

.. currentmodule:: numpy

Welcome to the absolute beginner's guide to NumPy! If you have comments or
suggestions, please don’t hesitate to `reach out
<https://numpy.org/community/>`_!


Welcome to NumPy!
-----------------

NumPy (**Numerical Python**) is an open source Python library that's used in
almost every field of science and engineering. It's the universal standard for
working with numerical data in Python, and it's at the core of the scientific
Python and PyData ecosystems. NumPy users include everyone from beginning coders
to experienced researchers doing state-of-the-art scientific and industrial
research and development. The NumPy API is used extensively in Pandas, SciPy,
Matplotlib, scikit-learn, scikit-image and most other data science and
scientific Python packages.

The NumPy library contains multidimensional array and matrix data structures
(you'll find more information about this in later sections). It provides
**ndarray**, a homogeneous n-dimensional array object, with methods to
efficiently operate on it. NumPy can be used to perform a wide variety of
mathematical operations on arrays.  It adds powerful data structures to Python
that guarantee efficient calculations with arrays and matrices and it supplies
an enormous library of high-level mathematical functions that operate on these
arrays and matrices.

Learn more about :ref:`NumPy here <whatisnumpy>`!

Installing NumPy
----------------

To install NumPy, we strongly recommend using a scientific Python distribution.
If you're looking for the full instructions for installing NumPy on your
operating system, see `Installing NumPy <https://numpy.org/install/>`_.



If you already have Python, you can install NumPy with::

  conda install numpy

or ::

  pip install numpy

If you don't have Python yet, you might want to consider using `Anaconda
<https://www.anaconda.com/>`_. It's the easiest way to get started. The good
thing about getting this distribution is the fact that you don’t need to worry
too much about separately installing NumPy or any of the major packages that
you’ll be using for your data analyses, like pandas, Scikit-Learn, etc.

How to import NumPy
-------------------

To access NumPy and its functions import it in your Python code like this::

  import numpy as np

We shorten the imported name to ``np`` for better readability of code using
NumPy. This is a widely adopted convention that you should follow so that
anyone working with your code can easily understand it.

Reading the example code
------------------------

If you aren't already comfortable with reading tutorials that contain a lot of code,
you might not know how to interpret a code block that looks
like this::

  >>> a = np.arange(6)
  >>> a2 = a[np.newaxis, :]
  >>> a2.shape
  (1, 6)

If you aren't familiar with this style, it's very easy to understand.
If you see ``>>>``, you're looking at **input**, or the code that
you would enter. Everything that doesn't have ``>>>`` in front of it
is **output**, or the results of running your code. This is the style
you see when you run ``python`` on the command line, but if you're using
IPython, you might see a different style. Note that it is not part of the
code and will cause an error if typed or pasted into the Python
shell. It can be safely typed or pasted into the IPython shell; the ``>>>``
is ignored.


What’s the difference between a Python list and a NumPy array?
--------------------------------------------------------------

NumPy gives you an enormous range of fast and efficient ways of creating arrays
and manipulating numerical data inside them. While a Python list can contain
different data types within a single list, all of the elements in a NumPy array
should be homogeneous. The mathematical operations that are meant to be performed
on arrays would be extremely inefficient if the arrays weren't homogeneous.

**Why use NumPy?**

NumPy arrays are faster and more compact than Python lists. An array consumes
less memory and is convenient to use. NumPy uses much less memory to store data
and it provides a mechanism of specifying the data types. This allows the code
to be optimized even further.

What is an array?
-----------------

An array is a central data structure of the NumPy library. An array is a grid of
values and it contains information about the raw data, how to locate an element,
and how to interpret an element. It has a grid of elements that can be indexed
in :ref:`various ways <quickstart.indexing-slicing-and-iterating>`.
The elements are all of the same type, referred to as the array ``dtype``.

An array can be indexed by a tuple of nonnegative integers, by booleans, by
another array, or by integers. The ``rank`` of the array is the number of
dimensions. The ``shape`` of the array is a tuple of integers giving the size of
the array along each dimension.

One way we can initialize NumPy arrays is from Python lists, using nested lists
for two- or higher-dimensional data.

For example::

  >>> a = np.array([1, 2, 3, 4, 5, 6])

or::

  >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

We can access the elements in the array using square brackets. When you're
accessing elements, remember that indexing in NumPy starts at 0. That means that
if you want to access the first element in your array, you'll be accessing
element "0".

::

  >>> print(a[0])
  [1 2 3 4]


More information about arrays
-----------------------------

*This section covers* ``1D array``, ``2D array``, ``ndarray``, ``vector``, ``matrix``

------

You might occasionally hear an array referred to as a "ndarray," which is
shorthand for "N-dimensional array." An N-dimensional array is simply an array
with any number of dimensions. You might also hear **1-D**, or one-dimensional
array, **2-D**, or two-dimensional array, and so on. The NumPy ``ndarray`` class
is used to represent both matrices and vectors. A **vector** is an array with a
single dimension (there's no difference
between row and column vectors), while a **matrix** refers to an
array with two dimensions. For **3-D** or higher dimensional arrays, the term
**tensor** is also commonly used.

**What are the attributes of an array?**

An array is usually a fixed-size container of items of the same type and size.
The number of dimensions and items in an array is defined by its shape. The
shape of an array is a tuple of non-negative integers that specify the sizes of
each dimension.

In NumPy, dimensions are called **axes**. This means that if you have a 2D array
that looks like this::

  [[0., 0., 0.],
   [1., 1., 1.]]

Your array has 2 axes. The first axis has a length of 2 and the second axis has
a length of 3.

Just like in other Python container objects, the contents of an array can be
accessed and modified by indexing or slicing the array. Unlike the typical container
objects, different arrays can share the same data, so changes made on one array might
be visible in another.

Array **attributes** reflect information intrinsic to the array itself. If you
need to get, or even set, properties of an array without creating a new array,
you can often access an array through its attributes.

:ref:`Read more about array attributes here <arrays.ndarray>` and learn about
:ref:`array objects here <arrays>`.


How to create a basic array
---------------------------


*This section covers* ``np.array()``, ``np.zeros()``, ``np.ones()``,
``np.empty()``, ``np.arange()``, ``np.linspace()``, ``dtype``

-----

To create a NumPy array, you can use the function ``np.array()``.

All you need to do to create a simple array is pass a list to it. If you choose
to, you can also specify the type of data in your list.
:ref:`You can find more information about data types here <arrays.dtypes>`. ::

    >>> import numpy as np
    >>> a = np.array([1, 2, 3])

You can visualize your array this way:

.. image:: images/np_array.png

*Be aware that these visualizations are meant to simplify ideas and give you a basic understanding of NumPy concepts and mechanics. Arrays and array operations are much more complicated than are captured here!*

Besides creating an array from a sequence of elements, you can easily create an
array filled with ``0``'s::

  >>> np.zeros(2)
  array([0., 0.])

Or an array filled with ``1``'s::

  >>> np.ones(2)
  array([1., 1.])

Or even an empty array! The function ``empty`` creates an array whose initial
content is random and depends on the state of the memory. The reason to use
``empty`` over ``zeros`` (or something similar) is speed - just make sure to
fill every element afterwards! ::

  >>> # Create an empty array with 2 elements
  >>> np.empty(2) #doctest: +SKIP
  array([3.14, 42.  ])  # may vary

You can create an array with a range of elements::

  >>> np.arange(4)
  array([0, 1, 2, 3])

And even an array that contains a range of evenly spaced intervals. To do this,
you will specify the **first number**, **last number**, and the **step size**. ::

  >>> np.arange(2, 9, 2)
  array([2, 4, 6, 8])

You can also use ``np.linspace()`` to create an array with values that are
spaced linearly in a specified interval::

  >>> np.linspace(0, 10, num=5)
  array([ 0. ,  2.5,  5. ,  7.5, 10. ])

**Specifying your data type**

While the default data type is floating point (``np.float64``), you can explicitly
specify which data type you want using the ``dtype`` keyword. ::

  >>> x = np.ones(2, dtype=np.int64)
  >>> x
  array([1, 1])

:ref:`Learn more about creating arrays here <quickstart.array-creation>`

Adding, removing, and sorting elements
--------------------------------------

*This section covers* ``np.sort()``, ``np.concatenate()``

-----

Sorting an element is simple with ``np.sort()``. You can specify the axis, kind,
and order when you call the function.

If you start with this array::

  >>> arr = np.array([2, 1, 5, 3, 7, 4, 6, 8])

You can quickly sort the numbers in ascending order with::

  >>> np.sort(arr)
  array([1, 2, 3, 4, 5, 6, 7, 8])

In addition to sort, which returns a sorted copy of an array, you can use:

- `argsort`, which is an indirect sort along a specified axis,
- `lexsort`, which is an indirect stable sort on multiple keys,
- `searchsorted`, which will find elements in a sorted array, and
- `partition`, which is a partial sort.

To read more about sorting an array, see: `sort`.

If you start with these arrays::

  >>> a = np.array([1, 2, 3, 4])
  >>> b = np.array([5, 6, 7, 8])

You can concatenate them with ``np.concatenate()``. ::

  >>> np.concatenate((a, b))
  array([1, 2, 3, 4, 5, 6, 7, 8])

Or, if you start with these arrays::

  >>> x = np.array([[1, 2], [3, 4]])
  >>> y = np.array([[5, 6]])

You can concatenate them with::

  >>> np.concatenate((x, y), axis=0)
  array([[1, 2],
         [3, 4],
         [5, 6]])

In order to remove elements from an array, it's simple to use indexing to select
the elements that you want to keep.

To read more about concatenate, see: `concatenate`.


How do you know the shape and size of an array?
-----------------------------------------------

*This section covers* ``ndarray.ndim``, ``ndarray.size``, ``ndarray.shape``

-----

``ndarray.ndim`` will tell you the number of axes, or dimensions, of the array.

``ndarray.size`` will tell you the total number of elements of the array. This
is the *product* of the elements of the array's shape.

``ndarray.shape`` will display a tuple of integers that indicate the number of
elements stored along each dimension of the array. If, for example, you have a
2-D array with 2 rows and 3 columns, the shape of your array is ``(2, 3)``.

For example, if you create this array::

  >>> array_example = np.array([[[0, 1, 2, 3],
  ...                            [4, 5, 6, 7]],
  ...
  ...                           [[0, 1, 2, 3],
  ...                            [4, 5, 6, 7]],
  ...
  ...                           [[0 ,1 ,2, 3],
  ...                            [4, 5, 6, 7]]])

To find the number of dimensions of the array, run::

  >>> array_example.ndim
  3

To find the total number of elements in the array, run::

  >>> array_example.size
  24

And to find the shape of your array, run::

  >>> array_example.shape
  (3, 2, 4)


Can you reshape an array?
-------------------------

*This section covers* ``arr.reshape()``

-----

**Yes!**

Using ``arr.reshape()`` will give a new shape to an array without changing the
data. Just remember that when you use the reshape method, the array you want to
produce needs to have the same number of elements as the original array. If you
start with an array with 12 elements, you'll need to make sure that your new
array also has a total of 12 elements.

If you start with this array::

  >>> a = np.arange(6)
  >>> print(a)
  [0 1 2 3 4 5]

You can use ``reshape()`` to reshape your array. For example, you can reshape
this array to an array with three rows and two columns::

  >>> b = a.reshape(3, 2)
  >>> print(b)
  [[0 1]
   [2 3]
   [4 5]]

With ``np.reshape``, you can specify a few optional parameters::

  >>> np.reshape(a, newshape=(1, 6), order='C')
  array([[0, 1, 2, 3, 4, 5]])

``a`` is the array to be reshaped.

``newshape`` is the new shape you want. You can specify an integer or a tuple of
integers. If you specify an integer, the result will be an array of that length.
The shape should be compatible with the original shape.

``order:`` ``C`` means to read/write the elements using C-like index order,
``F`` means to read/write the elements using Fortran-like index order, ``A``
means to read/write the elements in Fortran-like index order if a is Fortran
contiguous in memory, C-like order otherwise. (This is an optional parameter and
doesn't need to be specified.)

If you want to learn more about C and Fortran order, you can
:ref:`read more about the internal organization of NumPy arrays here <numpy-internals>`.
Essentially, C and Fortran orders have to do with how indices correspond
to the order the array is stored in memory. In Fortran, when moving through
the elements of a two-dimensional array as it is stored in memory, the **first**
index is the most rapidly varying index. As the first index moves to the next
row as it changes, the matrix is stored one column at a time.
This is why Fortran is thought of as a **Column-major language**.
In C on the other hand, the **last** index changes
the most rapidly. The matrix is stored by rows, making it a **Row-major
language**. What you do for C or Fortran depends on whether it's more important
to preserve the indexing convention or not reorder the data.

:ref:`Learn more about shape manipulation here <quickstart.shape-manipulation>`.


How to convert a 1D array into a 2D array (how to add a new axis to an array)
-----------------------------------------------------------------------------

*This section covers* ``np.newaxis``, ``np.expand_dims``

-----

You can use ``np.newaxis`` and ``np.expand_dims`` to increase the dimensions of
your existing array.

Using ``np.newaxis`` will increase the dimensions of your array by one dimension
when used once. This means that a **1D** array will become a **2D** array, a
**2D** array will become a **3D** array, and so on.

For example, if you start with this array::

  >>> a = np.array([1, 2, 3, 4, 5, 6])
  >>> a.shape
  (6,)

You can use ``np.newaxis`` to add a new axis::

  >>> a2 = a[np.newaxis, :]
  >>> a2.shape
  (1, 6)

You can explicitly convert a 1D array with either a row vector or a column
vector using ``np.newaxis``. For example, you can convert a 1D array to a row
vector by inserting an axis along the first dimension::

  >>> row_vector = a[np.newaxis, :]
  >>> row_vector.shape
  (1, 6)

Or, for a column vector, you can insert an axis along the second dimension::

  >>> col_vector = a[:, np.newaxis]
  >>> col_vector.shape
  (6, 1)

You can also expand an array by inserting a new axis at a specified position
with ``np.expand_dims``.

For example, if you start with this array::

  >>> a = np.array([1, 2, 3, 4, 5, 6])
  >>> a.shape
  (6,)

You can use ``np.expand_dims`` to add an axis at index position 1 with::

  >>> b = np.expand_dims(a, axis=1)
  >>> b.shape
  (6, 1)

You can add an axis at index position 0 with::

  >>> c = np.expand_dims(a, axis=0)
  >>> c.shape
  (1, 6)

Find more information about :ref:`newaxis here <arrays.indexing>` and
``expand_dims`` at `expand_dims`.


Indexing and slicing
--------------------

You can index and slice NumPy arrays in the same ways you can slice Python
lists. ::

  >>> data = np.array([1, 2, 3])

  >>> data[1]
  2
  >>> data[0:2]
  array([1, 2])
  >>> data[1:]
  array([2, 3])
  >>> data[-2:]
  array([2, 3])

You can visualize it this way:

.. image:: images/np_indexing.png


You may want to take a section of your array or specific array elements to use
in further analysis or additional operations. To do that, you'll need to subset,
slice, and/or index your arrays.

If you want to select values from your array that fulfill certain conditions,
it's straightforward with NumPy.

For example, if you start with this array::

  >>> a = np.array([[1 , 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can easily print all of the values in the array that are less than 5. ::

  >>> print(a[a < 5])
  [1 2 3 4]

You can also select, for example, numbers that are equal to or greater than 5,
and use that condition to index an array. ::

  >>> five_up = (a >= 5)
  >>> print(a[five_up])
  [ 5  6  7  8  9 10 11 12]

You can select elements that are divisible by 2::

  >>> divisible_by_2 = a[a%2==0]
  >>> print(divisible_by_2)
  [ 2  4  6  8 10 12]

Or you can select elements that satisfy two conditions using the ``&`` and ``|``
operators::

  >>> c = a[(a > 2) & (a < 11)]
  >>> print(c)
  [ 3  4  5  6  7  8  9 10]

You can also make use of the logical operators **&** and **|** in order to
return boolean values that specify whether or not the values in an array fulfill
a certain condition. This can be useful with arrays that contain names or other
categorical values. ::

  >>> five_up = (a > 5) | (a == 5)
  >>> print(five_up)
  [[False False False False]
   [ True  True  True  True]
   [ True  True  True True]]

You can also use ``np.nonzero()`` to select elements or indices from an array.

Starting with this array::

  >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can use ``np.nonzero()`` to print the indices of elements that are, for
example, less than 5::

  >>> b = np.nonzero(a < 5)
  >>> print(b)
  (array([0, 0, 0, 0]), array([0, 1, 2, 3]))

In this example, a tuple of arrays was returned: one for each dimension. The
first array represents the row indices where these values are found, and the
second array represents the column indices where the values are found.

If you want to generate a list of coordinates where the elements exist, you can
zip the arrays, iterate over the list of coordinates, and print them. For
example::

  >>> list_of_coordinates= list(zip(b[0], b[1]))

  >>> for coord in list_of_coordinates:
  ...     print(coord)
  (0, 0)
  (0, 1)
  (0, 2)
  (0, 3)

You can also use ``np.nonzero()`` to print the elements in an array that are less
than 5 with::

  >>> print(a[b])
  [1 2 3 4]

If the element you're looking for doesn't exist in the array, then the returned
array of indices will be empty. For example::

  >>> not_there = np.nonzero(a == 42)
  >>> print(not_there)
  (array([], dtype=int64), array([], dtype=int64))

Learn more about :ref:`indexing and slicing here <quickstart.indexing-slicing-and-iterating>`
and :ref:`here <basics.indexing>`.

Read more about using the nonzero function at: `nonzero`.


How to create an array from existing data
-----------------------------------------

*This section covers* ``slicing and indexing``, ``np.vstack()``, ``np.hstack()``,
``np.hsplit()``, ``.view()``, ``copy()``

-----

You can easily create a new array from a section of an existing array.

Let's say you have this array:

::

  >>> a = np.array([1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

You can create a new array from a section of your array any time by specifying
where you want to slice your array. ::

  >>> arr1 = a[3:8]
  >>> arr1
  array([4, 5, 6, 7, 8])

Here, you grabbed a section of your array from index position 3 through index
position 8.

You can also stack two existing arrays, both vertically and horizontally. Let's
say you have two arrays, ``a1`` and ``a2``::

  >>> a1 = np.array([[1, 1],
  ...                [2, 2]])

  >>> a2 = np.array([[3, 3],
  ...                [4, 4]])

You can stack them vertically with ``vstack``::

  >>> np.vstack((a1, a2))
  array([[1, 1],
         [2, 2],
         [3, 3],
         [4, 4]])

Or stack them horizontally with ``hstack``::

  >>> np.hstack((a1, a2))
  array([[1, 1, 3, 3],
         [2, 2, 4, 4]])

You can split an array into several smaller arrays using ``hsplit``. You can
specify either the number of equally shaped arrays to return or the columns
*after* which the division should occur.

Let's say you have this array::

  >>> x = np.arange(1, 25).reshape(2, 12)
  >>> x
  array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
         [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]])

If you wanted to split this array into three equally shaped arrays, you would
run::

  >>> np.hsplit(x, 3)
    [array([[ 1,  2,  3,  4],
           [13, 14, 15, 16]]), array([[ 5,  6,  7,  8],
           [17, 18, 19, 20]]), array([[ 9, 10, 11, 12],
           [21, 22, 23, 24]])]

If you wanted to split your array after the third and fourth column, you'd run::

  >>> np.hsplit(x, (3, 4))
    [array([[ 1,  2,  3],
           [13, 14, 15]]), array([[ 4],
           [16]]), array([[ 5,  6,  7,  8,  9, 10, 11, 12],
           [17, 18, 19, 20, 21, 22, 23, 24]])]

:ref:`Learn more about stacking and splitting arrays here <quickstart.stacking-arrays>`.

You can use the ``view`` method to create a new array object that looks at the
same data as the original array (a *shallow copy*).

Views are an important NumPy concept! NumPy functions, as well as operations
like indexing and slicing, will return views whenever possible. This saves
memory and is faster (no copy of the data has to be made). However it's
important to be aware of this - modifying data in a view also modifies the
original array!

Let's say you create this array::

  >>> a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

Now we create an array ``b1`` by slicing ``a`` and modify the first element of
``b1``. This will modify the corresponding element in ``a`` as well! ::

  >>> b1 = a[0, :]
  >>> b1
  array([1, 2, 3, 4])
  >>> b1[0] = 99
  >>> b1
  array([99,  2,  3,  4])
  >>> a
  array([[99,  2,  3,  4],
         [ 5,  6,  7,  8],
         [ 9, 10, 11, 12]])

Using the ``copy`` method will make a complete copy of the array and its data (a
*deep copy*). To use this on your array, you could run::

  >>> b2 = a.copy()

:ref:`Learn more about copies and views here <quickstart.copies-and-views>`.


Basic array operations
----------------------

*This section covers addition, subtraction, multiplication, division, and more*

-----

Once you've created your arrays, you can start to work with them.  Let's say,
for example, that you've created two arrays, one called "data" and one called
"ones"

.. image:: images/np_array_dataones.png

You can add the arrays together with the plus sign.

::

  >>> data = np.array([1, 2])
  >>> ones = np.ones(2, dtype=int)
  >>> data + ones
  array([2, 3])

.. image:: images/np_data_plus_ones.png

You can, of course, do more than just addition!

::

  >>> data - ones
  array([0, 1])
  >>> data * data
  array([1, 4])
  >>> data / data
  array([1., 1.])

.. image:: images/np_sub_mult_divide.png

Basic operations are simple with NumPy. If you want to find the sum of the
elements in an array, you'd use ``sum()``. This works for 1D arrays, 2D arrays,
and arrays in higher dimensions. ::

  >>> a = np.array([1, 2, 3, 4])

  >>> a.sum()
  10

To add the rows or the columns in a 2D array, you would specify the axis.

If you start with this array::

  >>> b = np.array([[1, 1], [2, 2]])

You can sum over the axis of rows with::

  >>> b.sum(axis=0)
  array([3, 3])

You can sum over the axis of columns with::

  >>> b.sum(axis=1)
  array([2, 4])

:ref:`Learn more about basic operations here <quickstart.basic-operations>`.


Broadcasting
------------

There are times when you might want to carry out an operation between an array
and a single number (also called *an operation between a vector and a scalar*)
or between arrays of two different sizes. For example, your array (we'll call it
"data") might contain information about distance in miles but you want to
convert the information to kilometers. You can perform this operation with::

  >>> data = np.array([1.0, 2.0])
  >>> data * 1.6
  array([1.6, 3.2])

.. image:: images/np_multiply_broadcasting.png

NumPy understands that the multiplication should happen with each cell. That
concept is called **broadcasting**. Broadcasting is a mechanism that allows
NumPy to perform operations on arrays of different shapes. The dimensions of
your array must be compatible, for example, when the dimensions of both arrays
are equal or when one of them is 1. If the dimensions are not compatible, you
will get a ``ValueError``.

:ref:`Learn more about broadcasting here <basics.broadcasting>`.


More useful array operations
----------------------------

*This section covers maximum, minimum, sum, mean, product, standard deviation, and more*

-----

NumPy also performs aggregation functions. In addition to ``min``, ``max``, and
``sum``, you can easily run ``mean`` to get the average, ``prod`` to get the
result of multiplying the elements together, ``std`` to get the standard
deviation, and more. ::

  >>> data.max()
  2.0
  >>> data.min()
  1.0
  >>> data.sum()
  3.0

.. image:: images/np_aggregation.png

Let's start with this array, called "a" ::

  >>> a = np.array([[0.45053314, 0.17296777, 0.34376245, 0.5510652],
  ...               [0.54627315, 0.05093587, 0.40067661, 0.55645993],
  ...               [0.12697628, 0.82485143, 0.26590556, 0.56917101]])

It's very common to want to aggregate along a row or column. By default, every
NumPy aggregation function will return the aggregate of the entire array. To
find the sum or the minimum of the elements in your array, run::

  >>> a.sum()
  4.8595784

Or::

  >>> a.min()
  0.05093587

You can specify on which axis you want the aggregation function to be computed.
For example, you can find the minimum value within each column by specifying
``axis=0``. ::

  >>> a.min(axis=0)
  array([0.12697628, 0.05093587, 0.26590556, 0.5510652 ])

The four values listed above correspond to the number of columns in your array.
With a four-column array, you will get four values as your result.

Read more about :ref:`array methods here <array.ndarray.methods>`.


Creating matrices
-----------------

You can pass Python lists of lists to create a 2-D array (or "matrix") to
represent them in NumPy. ::

  >>> data = np.array([[1, 2], [3, 4], [5, 6]])
  >>> data
  array([[1, 2],
         [3, 4],
         [5, 6]])

.. image:: images/np_create_matrix.png

Indexing and slicing operations are useful when you're manipulating matrices::

  >>> data[0, 1]
  2
  >>> data[1:3]
  array([[3, 4],
         [5, 6]])
  >>> data[0:2, 0]
  array([1, 3])

.. image:: images/np_matrix_indexing.png

You can aggregate matrices the same way you aggregated vectors::

  >>> data.max()
  6
  >>> data.min()
  1
  >>> data.sum()
  21

.. image:: images/np_matrix_aggregation.png

You can aggregate all the values in a matrix and you can aggregate them across
columns or rows using the ``axis`` parameter. To illustrate this point, let's
look at a slightly modified dataset::

  >>> data = np.array([[1, 2], [5, 3], [4, 6]])
  >>> data
  array([[1, 2],
         [5, 3],
         [4, 6]])
  >>> data.max(axis=0)
  array([5, 6])
  >>> data.max(axis=1)
  array([2, 5, 6])

.. image:: images/np_matrix_aggregation_row.png

Once you've created your matrices, you can add and multiply them using
arithmetic operators if you have two matrices that are the same size. ::

  >>> data = np.array([[1, 2], [3, 4]])
  >>> ones = np.array([[1, 1], [1, 1]])
  >>> data + ones
  array([[2, 3],
         [4, 5]])

.. image:: images/np_matrix_arithmetic.png

You can do these arithmetic operations on matrices of different sizes, but only
if one matrix has only one column or one row. In this case, NumPy will use its
broadcast rules for the operation. ::

  >>> data = np.array([[1, 2], [3, 4], [5, 6]])
  >>> ones_row = np.array([[1, 1]])
  >>> data + ones_row
  array([[2, 3],
         [4, 5],
         [6, 7]])

.. image:: images/np_matrix_broadcasting.png

Be aware that when NumPy prints N-dimensional arrays, the last axis is looped
over the fastest while the first axis is the slowest. For instance::

  >>> np.ones((4, 3, 2))
  array([[[1., 1.],
          [1., 1.],
          [1., 1.]],
  <BLANKLINE>
         [[1., 1.],
          [1., 1.],
          [1., 1.]],
  <BLANKLINE>
         [[1., 1.],
          [1., 1.],
          [1., 1.]],
  <BLANKLINE>
         [[1., 1.],
          [1., 1.],
          [1., 1.]]])

There are often instances where we want NumPy to initialize the values of an
array. NumPy offers functions like ``ones()`` and ``zeros()``, and the
``random.Generator`` class for random number generation for that.
All you need to do is pass in the number of elements you want it to generate::

  >>> np.ones(3)
  array([1., 1., 1.])
  >>> np.zeros(3)
  array([0., 0., 0.])
  >>> rng = np.random.default_rng()  # the simplest way to generate random numbers
  >>> rng.random(3) #doctest: +SKIP
  array([0.63696169, 0.26978671, 0.04097352])

.. image:: images/np_ones_zeros_random.png

You can also use ``ones()``, ``zeros()``, and ``random()`` to create
a 2D array if you give them a tuple describing the dimensions of the matrix::

  >>> np.ones((3, 2))
  array([[1., 1.],
         [1., 1.],
         [1., 1.]])
  >>> np.zeros((3, 2))
  array([[0., 0.],
         [0., 0.],
         [0., 0.]])
  >>> rng.random((3, 2)) #doctest: +SKIP
  array([[0.01652764, 0.81327024],
         [0.91275558, 0.60663578],
         [0.72949656, 0.54362499]])  # may vary

.. image:: images/np_ones_zeros_matrix.png

Read more about creating arrays, filled with ``0``'s, ``1``'s, other values or
uninitialized, at :ref:`array creation routines <routines.array-creation>`.


Generating random numbers
-------------------------

The use of random number generation is an important part of the configuration
and evaluation of many numerical and machine learning algorithms. Whether you
need to randomly initialize weights in an artificial neural network, split data
into random sets, or randomly shuffle your dataset, being able to generate
random numbers (actually, repeatable pseudo-random numbers) is essential.

With ``Generator.integers``, you can generate random integers from low (remember
that this is inclusive with NumPy) to high (exclusive). You can set
``endpoint=True`` to make the high number inclusive.

You can generate a 2 x 4 array of random integers between 0 and 4 with::

  >>> rng.integers(5, size=(2, 4)) #doctest: +SKIP
  array([[2, 1, 1, 0],
         [0, 0, 0, 4]])  # may vary

:ref:`Read more about random number generation here <numpyrandom>`.


How to get unique items and counts
----------------------------------

*This section covers* ``np.unique()``

-----

You can find the unique elements in an array easily with ``np.unique``.

For example, if you start with this array::

  >>> a = np.array([11, 11, 12, 13, 14, 15, 16, 17, 12, 13, 11, 14, 18, 19, 20])

you can use ``np.unique`` to print the unique values in your array::

  >>> unique_values = np.unique(a)
  >>> print(unique_values)
  [11 12 13 14 15 16 17 18 19 20]

To get the indices of unique values in a NumPy array (an array of first index
positions of unique values in the array), just pass the ``return_index``
argument in ``np.unique()`` as well as your array. ::

  >>> unique_values, indices_list = np.unique(a, return_index=True)
  >>> print(indices_list)
  [ 0  2  3  4  5  6  7 12 13 14]

You can pass the ``return_counts`` argument in ``np.unique()`` along with your
array to get the frequency count of unique values in a NumPy array. ::

  >>> unique_values, occurrence_count = np.unique(a, return_counts=True)
  >>> print(occurrence_count)
  [3 2 2 2 1 1 1 1 1 1]

This also works with 2D arrays!
If you start with this array::

  >>> a_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [1, 2, 3, 4]])

You can find unique values with::

  >>> unique_values = np.unique(a_2d)
  >>> print(unique_values)
  [ 1  2  3  4  5  6  7  8  9 10 11 12]

If the axis argument isn't passed, your 2D array will be flattened.

If you want to get the unique rows or columns, make sure to pass the ``axis``
argument. To find the unique rows, specify ``axis=0`` and for columns, specify
``axis=1``. ::

  >>> unique_rows = np.unique(a_2d, axis=0)
  >>> print(unique_rows)
  [[ 1  2  3  4]
   [ 5  6  7  8]
   [ 9 10 11 12]]

To get the unique rows, index position, and occurrence count, you can use::

  >>> unique_rows, indices, occurrence_count = np.unique(
  ...      a_2d, axis=0, return_counts=True, return_index=True)
  >>> print(unique_rows)
  [[ 1  2  3  4]
   [ 5  6  7  8]
   [ 9 10 11 12]]
  >>> print(indices)
  [0 1 2]
  >>> print(occurrence_count)
  [2 1 1]

To learn more about finding the unique elements in an array, see `unique`.


Transposing and reshaping a matrix
----------------------------------

*This section covers* ``arr.reshape()``, ``arr.transpose()``, ``arr.T``

-----

It's common to need to transpose your matrices. NumPy arrays have the property
``T`` that allows you to transpose a matrix.

.. image:: images/np_transposing_reshaping.png

You may also need to switch the dimensions of a matrix. This can happen when,
for example, you have a model that expects a certain input shape that is
different from your dataset. This is where the ``reshape`` method can be useful.
You simply need to pass in the new dimensions that you want for the matrix. ::

  >>> data.reshape(2, 3)
  array([[1, 2, 3],
         [4, 5, 6]])
  >>> data.reshape(3, 2)
  array([[1, 2],
         [3, 4],
         [5, 6]])

.. image:: images/np_reshape.png

You can also use ``.transpose()`` to reverse or change the axes of an array
according to the values you specify.

If you start with this array::

  >>> arr = np.arange(6).reshape((2, 3))
  >>> arr
  array([[0, 1, 2],
         [3, 4, 5]])

You can transpose your array with ``arr.transpose()``. ::

  >>> arr.transpose()
  array([[0, 3],
         [1, 4],
         [2, 5]])

You can also use ``arr.T``::

    >>> arr.T
    array([[0, 3],
           [1, 4],
           [2, 5]])

To learn more about transposing and reshaping arrays, see `transpose` and
`reshape`.


How to reverse an array
-----------------------

*This section covers* ``np.flip()``

-----

NumPy's ``np.flip()`` function allows you to flip, or reverse, the contents of
an array along an axis. When using ``np.flip()``, specify the array you would like
to reverse and the axis. If you don't specify the axis, NumPy will reverse the
contents along all of the axes of your input array.

**Reversing a 1D array**

If you begin with a 1D array like this one::

  >>> arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

You can reverse it with::

  >>> reversed_arr = np.flip(arr)

If you want to print your reversed array, you can run::

  >>> print('Reversed Array: ', reversed_arr)
  Reversed Array:  [8 7 6 5 4 3 2 1]

**Reversing a 2D array**

A 2D array works much the same way.

If you start with this array::

  >>> arr_2d = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can reverse the content in all of the rows and all of the columns with::

  >>> reversed_arr = np.flip(arr_2d)
  >>> print(reversed_arr)
  [[12 11 10  9]
   [ 8  7  6  5]
   [ 4  3  2  1]]

You can easily reverse only the *rows* with::

  >>> reversed_arr_rows = np.flip(arr_2d, axis=0)
  >>> print(reversed_arr_rows)
  [[ 9 10 11 12]
   [ 5  6  7  8]
   [ 1  2  3  4]]

Or reverse only the *columns* with::

  >>> reversed_arr_columns = np.flip(arr_2d, axis=1)
  >>> print(reversed_arr_columns)
  [[ 4  3  2  1]
   [ 8  7  6  5]
   [12 11 10  9]]

You can also reverse the contents of only one column or row. For example, you
can reverse the contents of the row at index position 1 (the second row)::

  >>> arr_2d[1] = np.flip(arr_2d[1])
  >>> print(arr_2d)
  [[ 1  2  3  4]
   [ 8  7  6  5]
   [ 9 10 11 12]]

You can also reverse the column at index position 1 (the second column)::

  >>> arr_2d[:,1] = np.flip(arr_2d[:,1])
  >>> print(arr_2d)
  [[ 1 10  3  4]
   [ 8  7  6  5]
   [ 9  2 11 12]]

Read more about reversing arrays at `flip`.


Reshaping and flattening multidimensional arrays
------------------------------------------------

*This section covers* ``.flatten()``, ``ravel()``

-----

There are two popular ways to flatten an array: ``.flatten()`` and ``.ravel()``.
The primary difference between the two is that the new array created using
``ravel()`` is actually a reference to the parent array (i.e., a "view"). This
means that any changes to the new array will affect the parent array as well.
Since ``ravel`` does not create a copy, it's memory efficient.

If you start with this array::

  >>> x = np.array([[1 , 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

You can use ``flatten`` to flatten your array into a 1D array. ::

  >>> x.flatten()
  array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])

When you use ``flatten``, changes to your new array won't change the parent
array.

For example::

  >>> a1 = x.flatten()
  >>> a1[0] = 99
  >>> print(x)  # Original array
  [[ 1  2  3  4]
   [ 5  6  7  8]
   [ 9 10 11 12]]
  >>> print(a1)  # New array
  [99  2  3  4  5  6  7  8  9 10 11 12]

But when you use ``ravel``, the changes you make to the new array will affect
the parent array.

For example::

  >>> a2 = x.ravel()
  >>> a2[0] = 98
  >>> print(x)  # Original array
  [[98  2  3  4]
   [ 5  6  7  8]
   [ 9 10 11 12]]
  >>> print(a2)  # New array
  [98  2  3  4  5  6  7  8  9 10 11 12]

Read more about ``flatten`` at `ndarray.flatten` and ``ravel`` at `ravel`.


How to access the docstring for more information
------------------------------------------------

*This section covers* ``help()``, ``?``, ``??``

-----

When it comes to the data science ecosystem, Python and NumPy are built with the
user in mind. One of the best examples of this is the built-in access to
documentation. Every object contains the reference to a string, which is known
as the **docstring**. In most cases, this docstring contains a quick and concise
summary of the object and how to use it. Python has a built-in ``help()``
function that can help you access this information. This means that nearly any
time you need more information, you can use ``help()`` to quickly find the
information that you need.

For example::

  >>> help(max)
  Help on built-in function max in module builtins:
  <BLANKLINE>
  max(...)
      max(iterable, *[, default=obj, key=func]) -> value
      max(arg1, arg2, *args, *[, key=func]) -> value
  <BLANKLINE>
      With a single iterable argument, return its biggest item. The
      default keyword-only argument specifies an object to return if
      the provided iterable is empty.
      With two or more arguments, return the largest argument.
  <BLANKLINE>


Because access to additional information is so useful, IPython uses the ``?``
character as a shorthand for accessing this documentation along with other
relevant information. IPython is a command shell for interactive computing in
multiple languages.
`You can find more information about IPython here <https://ipython.org/>`_.

For example:

.. code-block:: ipython

  In [0]: max?
  max(iterable, *[, default=obj, key=func]) -> value
  max(arg1, arg2, *args, *[, key=func]) -> value

  With a single iterable argument, return its biggest item. The
  default keyword-only argument specifies an object to return if
  the provided iterable is empty.
  With two or more arguments, return the largest argument.
  Type:      builtin_function_or_method

You can even use this notation for object methods and objects themselves.

Let's say you create this array::

  >>> a = np.array([1, 2, 3, 4, 5, 6])

Then you can obtain a lot of useful information (first details about ``a`` itself,
followed by the docstring of ``ndarray`` of which ``a`` is an instance):

.. code-block:: ipython

  In [1]: a?
  Type:            ndarray
  String form:     [1 2 3 4 5 6]
  Length:          6
  File:            ~/anaconda3/lib/python3.7/site-packages/numpy/__init__.py
  Docstring:       <no docstring>
  Class docstring:
  ndarray(shape, dtype=float, buffer=None, offset=0,
          strides=None, order=None)

  An array object represents a multidimensional, homogeneous array
  of fixed-size items.  An associated data-type object describes the
  format of each element in the array (its byte-order, how many bytes it
  occupies in memory, whether it is an integer, a floating point number,
  or something else, etc.)

  Arrays should be constructed using `array`, `zeros` or `empty` (refer
  to the See Also section below).  The parameters given here refer to
  a low-level method (`ndarray(...)`) for instantiating an array.

  For more information, refer to the `numpy` module and examine the
  methods and attributes of an array.

  Parameters
  ----------
  (for the __new__ method; see Notes below)

  shape : tuple of ints
          Shape of created array.
  ...

This also works for functions and other objects that **you** create. Just
remember to include a docstring with your function using a string literal
(``""" """`` or ``''' '''`` around your documentation).

For example, if you create this function::

  >>> def double(a):
  ...   '''Return a * 2'''
  ...   return a * 2

You can obtain information about the function:

.. code-block:: ipython

  In [2]: double?
  Signature: double(a)
  Docstring: Return a * 2
  File:      ~/Desktop/<ipython-input-23-b5adf20be596>
  Type:      function

You can reach another level of information by reading the source code of the
object you're interested in. Using a double question mark (``??``) allows you to
access the source code.

For example:

.. code-block:: ipython

  In [3]: double??
  Signature: double(a)
  Source:
  def double(a):
      '''Return a * 2'''
      return a * 2
  File:      ~/Desktop/<ipython-input-23-b5adf20be596>
  Type:      function

If the object in question is compiled in a language other than Python, using
``??`` will return the same information as ``?``. You'll find this with a lot of
built-in objects and types, for example:

.. code-block:: ipython

  In [4]: len?
  Signature: len(obj, /)
  Docstring: Return the number of items in a container.
  Type:      builtin_function_or_method

and :

.. code-block:: ipython

  In [5]: len??
  Signature: len(obj, /)
  Docstring: Return the number of items in a container.
  Type:      builtin_function_or_method

have the same output because they were compiled in a programming language other
than Python.


Working with mathematical formulas
----------------------------------

The ease of implementing mathematical formulas that work on arrays is one of
the things that make NumPy so widely used in the scientific Python community.

For example, this is the mean square error formula (a central formula used in
supervised machine learning models that deal with regression):

.. image:: images/np_MSE_formula.png

Implementing this formula is simple and straightforward in NumPy:

.. image:: images/np_MSE_implementation.png

What makes this work so well is that ``predictions`` and ``labels`` can contain
one or a thousand values. They only need to be the same size.

You can visualize it this way:

.. image:: images/np_mse_viz1.png

In this example, both the predictions and labels vectors contain three values,
meaning ``n`` has a value of three. After we carry out subtractions the values
in the vector are squared. Then NumPy sums the values, and your result is the
error value for that prediction and a score for the quality of the model.

.. image:: images/np_mse_viz2.png

.. image:: images/np_MSE_explanation2.png


How to save and load NumPy objects
----------------------------------

*This section covers* ``np.save``, ``np.savez``, ``np.savetxt``,
``np.load``, ``np.loadtxt``

-----

You will, at some point, want to save your arrays to disk and load them back
without having to re-run the code. Fortunately, there are several ways to save
and load objects with NumPy. The ndarray objects can be saved to and loaded from
the disk files with ``loadtxt`` and ``savetxt`` functions that handle normal
text files, ``load`` and ``save`` functions that handle NumPy binary files with
a **.npy** file extension, and a ``savez`` function that handles NumPy files
with a **.npz** file extension.

The **.npy** and **.npz** files store data, shape, dtype, and other information
required to reconstruct the ndarray in a way that allows the array to be
correctly retrieved, even when the file is on another machine with different
architecture.

If you want to store a single ndarray object, store it as a .npy file using
``np.save``. If you want to store more than one ndarray object in a single file,
save it as a .npz file using ``np.savez``. You can also save several arrays
into a single file in compressed npz format with `savez_compressed`.

It's easy to save and load and array with ``np.save()``. Just make sure to
specify the array you want to save and a file name. For example, if you create
this array::

  >>> a = np.array([1, 2, 3, 4, 5, 6])

You can save it as "filename.npy" with::

  >>> np.save('filename', a)

You can use ``np.load()`` to reconstruct your array. ::

  >>> b = np.load('filename.npy')

If you want to check your array, you can run:::

  >>> print(b)
  [1 2 3 4 5 6]

You can save a NumPy array as a plain text file like a **.csv** or **.txt** file
with ``np.savetxt``.

For example, if you create this array::

  >>> csv_arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

You can easily save it as a .csv file with the name "new_file.csv" like this::

  >>> np.savetxt('new_file.csv', csv_arr)

You can quickly and easily load your saved text file using ``loadtxt()``::

  >>> np.loadtxt('new_file.csv')
  array([1., 2., 3., 4., 5., 6., 7., 8.])

The ``savetxt()`` and ``loadtxt()`` functions accept additional optional
parameters such as header, footer, and delimiter. While text files can be easier
for sharing, .npy and .npz files are smaller and faster to read. If you need more
sophisticated handling of your text file (for example, if you need to work with
lines that contain missing values), you will want to use the `genfromtxt`
function.

With `savetxt`, you can specify headers, footers, comments, and more.

Learn more about :ref:`input and output routines here <routines.io>`.


Importing and exporting a CSV
-----------------------------

.. save a csv

   >>> with open('music.csv', 'w') as fid:
   ...     n = fid.write('Artist,Genre,Listeners,Plays\n')
   ...     n = fid.write('Billie Holiday,Jazz,1300000,27000000\n')
   ...     n = fid.write('Jimmie Hendrix,Rock,2700000,70000000\n')
   ...     n = fid.write('Miles Davis,Jazz,1500000,48000000\n')
   ...     n = fid.write('SIA,Pop,2000000,74000000\n')



It's simple to read in a CSV that contains existing information. The best and
easiest way to do this is to use
`Pandas <https://pandas.pydata.org>`_. ::

  >>> import pandas as pd

  >>> # If all of your columns are the same type:
  >>> x = pd.read_csv('music.csv', header=0).values
  >>> print(x)
  [['Billie Holiday' 'Jazz' 1300000 27000000]
   ['Jimmie Hendrix' 'Rock' 2700000 70000000]
   ['Miles Davis' 'Jazz' 1500000 48000000]
   ['SIA' 'Pop' 2000000 74000000]]

  >>> # You can also simply select the columns you need:
  >>> x = pd.read_csv('music.csv', usecols=['Artist', 'Plays']).values
  >>> print(x)
  [['Billie Holiday' 27000000]
   ['Jimmie Hendrix' 70000000]
   ['Miles Davis' 48000000]
   ['SIA' 74000000]]

.. image:: images/np_pandas.png

It's simple to use Pandas in order to export your array as well. If you are new
to NumPy, you may want to  create a Pandas dataframe from the values in your
array and then write the data frame to a CSV file with Pandas.

If you created this array "a" ::

  >>> a = np.array([[-2.58289208,  0.43014843, -1.24082018, 1.59572603],
  ...               [ 0.99027828, 1.17150989,  0.94125714, -0.14692469],
  ...               [ 0.76989341,  0.81299683, -0.95068423, 0.11769564],
  ...               [ 0.20484034,  0.34784527,  1.96979195, 0.51992837]])

.. for doctests
   The continuous integration truncates dataframe display without this setting.
   >>> pd.set_option('display.max_columns', 10)

You could create a Pandas dataframe ::

  >>> df = pd.DataFrame(a)
  >>> print(df)
            0         1         2         3
  0 -2.582892  0.430148 -1.240820  1.595726
  1  0.990278  1.171510  0.941257 -0.146925
  2  0.769893  0.812997 -0.950684  0.117696
  3  0.204840  0.347845  1.969792  0.519928

You can easily save your dataframe with::

  >>> df.to_csv('pd.csv')

And read your CSV with::

  >>> data = pd.read_csv('pd.csv')

.. image:: images/np_readcsv.png

You can also save your array with the NumPy ``savetxt`` method. ::

  >>> np.savetxt('np.csv', a, fmt='%.2f', delimiter=',', header='1,  2,  3,  4')

If you're using the command line, you can read your saved CSV any time with a
command such as::

  $ cat np.csv
  #  1,  2,  3,  4
  -2.58,0.43,-1.24,1.60
  0.99,1.17,0.94,-0.15
  0.77,0.81,-0.95,0.12
  0.20,0.35,1.97,0.52

Or you can open the file any time with a text editor!

If you're interested in learning more about Pandas, take a look at the
`official Pandas documentation <https://pandas.pydata.org/index.html>`_.
Learn how to install Pandas with the
`official Pandas installation information <https://pandas.pydata.org/pandas-docs/stable/install.html>`_.


Plotting arrays with Matplotlib
-------------------------------

If you need to generate a plot for your values, it's very simple with
`Matplotlib <https://matplotlib.org/>`_.

For example, you may have an array like this one::

  >>> a = np.array([2, 1, 5, 7, 4, 6, 8, 14, 10, 9, 18, 20, 22])

If you already have Matplotlib installed, you can import it with::

  >>> import matplotlib.pyplot as plt

  # If you're using Jupyter Notebook, you may also want to run the following
  # line of code to display your code in the notebook:

  %matplotlib inline

All you need to do to plot your values is run::

  >>> plt.plot(a)

  # If you are running from a command line, you may need to do this:
  # >>> plt.show()

.. plot:: user/plots/matplotlib1.py
   :align: center
   :include-source: 0

For example, you can plot a 1D array like this::

  >>> x = np.linspace(0, 5, 20)
  >>> y = np.linspace(0, 10, 20)
  >>> plt.plot(x, y, 'purple') # line
  >>> plt.plot(x, y, 'o')      # dots

.. plot:: user/plots/matplotlib2.py
   :align: center
   :include-source: 0

With Matplotlib, you have access to an enormous number of visualization options. ::

  >>> fig = plt.figure()
  >>> ax = fig.add_subplot(projection='3d')
  >>> X = np.arange(-5, 5, 0.15)
  >>> Y = np.arange(-5, 5, 0.15)
  >>> X, Y = np.meshgrid(X, Y)
  >>> R = np.sqrt(X**2 + Y**2)
  >>> Z = np.sin(R)

  >>> ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis')

.. plot:: user/plots/matplotlib3.py
   :align: center
   :include-source: 0


To read more about Matplotlib and what it can do, take a look at
`the official documentation <https://matplotlib.org/>`_.
For directions regarding installing Matplotlib, see the official
`installation section <https://matplotlib.org/users/installing.html>`_.


-------------------------------------------------------

*Image credits: Jay Alammar http://jalammar.github.io/*
:orphan:

===========================
Array Broadcasting in Numpy
===========================

.. 
   Originally part of the scipy.org wiki, available `here
   <https://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc>`_ or from the
   `github repo
   <https://github.com/scipy/old-wiki/blob/gh-pages/pages/EricsBroadcastingDoc.html>`_

.. note::
    Please refer to the updated :doc:`basics.broadcasting` document.

.. _basics.indexing:

****************************************
Indexing on :class:`ndarrays <.ndarray>`
****************************************

.. seealso::

   :ref:`Indexing routines <routines.indexing>`

.. sectionauthor:: adapted from "Guide to NumPy" by Travis E. Oliphant

.. currentmodule:: numpy

.. index:: indexing, slicing

:class:`ndarrays <ndarray>` can be indexed using the standard Python
``x[obj]`` syntax, where *x* is the array and *obj* the selection.
There are different kinds of indexing available depending on *obj*:
basic indexing, advanced indexing and field access.

Most of the following examples show the use of indexing when
referencing data in an array. The examples work just as well
when assigning to an array. See :ref:`assigning-values-to-indexed-arrays` for
specific examples and explanations on how assignments work.

Note that in Python, ``x[(exp1, exp2, ..., expN)]`` is equivalent to
``x[exp1, exp2, ..., expN]``; the latter is just syntactic sugar
for the former.

.. _basic-indexing:

Basic indexing
--------------

.. _single-element-indexing:

Single element indexing
^^^^^^^^^^^^^^^^^^^^^^^

Single element indexing works
exactly like that for other standard Python sequences. It is 0-based,
and accepts negative indices for indexing from the end of the array. ::

    >>> x = np.arange(10)
    >>> x[2]
    2
    >>> x[-2]
    8

It is not necessary to
separate each dimension's index into its own set of square brackets. ::

    >>> x.shape = (2, 5)  # now x is 2-dimensional
    >>> x[1, 3]
    8
    >>> x[1, -1]
    9

Note that if one indexes a multidimensional array with fewer indices
than dimensions, one gets a subdimensional array. For example: ::

    >>> x[0]
    array([0, 1, 2, 3, 4])

That is, each index specified selects the array corresponding to the
rest of the dimensions selected. In the above example, choosing 0
means that the remaining dimension of length 5 is being left unspecified,
and that what is returned is an array of that dimensionality and size.
It must be noted that the returned array is a :term:`view`, i.e., it is not a
copy of the original, but points to the same values in memory as does the
original array.
In  this case, the 1-D array at the first position (0) is returned.
So using a single index on the returned array, results in a single
element being returned. That is: ::

    >>> x[0][2]
    2

So note that ``x[0, 2] == x[0][2]`` though the second case is more
inefficient as a new temporary array is created after the first index
that is subsequently indexed by 2.

.. note::

    NumPy uses C-order indexing. That means that the last
    index usually represents the most rapidly changing memory location,
    unlike Fortran or IDL, where the first index represents the most
    rapidly changing location in memory. This difference represents a
    great potential for confusion.

.. _slicing-and-striding:

Slicing and striding
^^^^^^^^^^^^^^^^^^^^

Basic slicing extends Python's basic concept of slicing to N
dimensions. Basic slicing occurs when *obj* is a :class:`slice` object
(constructed by ``start:stop:step`` notation inside of brackets), an
integer, or a tuple of slice objects and integers. :py:data:`Ellipsis`
and :const:`newaxis` objects can be interspersed with these as
well.

.. deprecated:: 1.15.0

  In order to remain backward compatible with a common usage in
  Numeric, basic slicing is also initiated if the selection object is
  any non-ndarray and non-tuple sequence (such as a :class:`list`) containing
  :class:`slice` objects, the :py:data:`Ellipsis` object, or the :const:`newaxis`
  object, but not for integer arrays or other embedded sequences.

.. index::
   triple: ndarray; special methods; getitem
   triple: ndarray; special methods; setitem
   single: ellipsis
   single: newaxis

The simplest case of indexing with *N* integers returns an :ref:`array
scalar <arrays.scalars>` representing the corresponding item.  As in
Python, all indices are zero-based: for the *i*-th index :math:`n_i`,
the valid range is :math:`0 \le n_i < d_i` where :math:`d_i` is the
*i*-th element of the shape of the array.  Negative indices are
interpreted as counting from the end of the array (*i.e.*, if
:math:`n_i < 0`, it means :math:`n_i + d_i`).


All arrays generated by basic slicing are always :term:`views <view>`
of the original array.

.. note::

    NumPy slicing creates a :term:`view` instead of a copy as in the case of
    built-in Python sequences such as string, tuple and list.
    Care must be taken when extracting
    a small portion from a large array which becomes useless after the
    extraction, because the small portion extracted contains a reference
    to the large original array whose memory will not be released until
    all arrays derived from it are garbage-collected. In such cases an
    explicit ``copy()`` is recommended.

The standard rules of sequence slicing apply to basic slicing on a
per-dimension basis (including using a step index). Some useful
concepts to remember include:

- The basic slice syntax is ``i:j:k`` where *i* is the starting index,
  *j* is the stopping index, and *k* is the step (:math:`k\neq0`).
  This selects the *m* elements (in the corresponding dimension) with
  index values *i*, *i + k*, ..., *i + (m - 1) k* where
  :math:`m = q + (r\neq0)` and *q* and *r* are the quotient and remainder
  obtained by dividing *j - i* by *k*: *j - i = q k + r*, so that
  *i + (m - 1) k < j*. 
  For example::

     >>> x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
     >>> x[1:7:2]
     array([1, 3, 5])

- Negative *i* and *j* are interpreted as *n + i* and *n + j* where
  *n* is the number of elements in the corresponding dimension.
  Negative *k* makes stepping go towards smaller indices.
  From the above example::

      >>> x[-2:10]
      array([8, 9])
      >>> x[-3:3:-1]
      array([7, 6, 5, 4])

- Assume *n* is the number of elements in the dimension being
  sliced. Then, if *i* is not given it defaults to 0 for *k > 0* and
  *n - 1* for *k < 0* . If *j* is not given it defaults to *n* for *k > 0*
  and *-n-1* for *k < 0* . If *k* is not given it defaults to 1. Note that
  ``::`` is the same as ``:`` and means select all indices along this
  axis.
  From the above example::

      >>> x[5:]
      array([5, 6, 7, 8, 9])

- If the number of objects in the selection tuple is less than
  *N*, then ``:`` is assumed for any subsequent dimensions.
  For example::

      >>> x = np.array([[[1],[2],[3]], [[4],[5],[6]]])
      >>> x.shape
      (2, 3, 1)
      >>> x[1:2]
      array([[[4],
              [5],
              [6]]])

- An integer, *i*, returns the same values as ``i:i+1``
  **except** the dimensionality of the returned object is reduced by
  1. In particular, a selection tuple with the *p*-th
  element an integer (and all other entries ``:``) returns the
  corresponding sub-array with dimension *N - 1*. If *N = 1*
  then the returned object is an array scalar. These objects are
  explained in :ref:`arrays.scalars`.

- If the selection tuple has all entries ``:`` except the
  *p*-th entry which is a slice object ``i:j:k``,
  then the returned array has dimension *N* formed by
  concatenating the sub-arrays returned by integer indexing of
  elements *i*, *i+k*, ..., *i + (m - 1) k < j*,

- Basic slicing with more than one non-``:`` entry in the slicing
  tuple, acts like repeated application of slicing using a single
  non-``:`` entry, where the non-``:`` entries are successively taken
  (with all other non-``:`` entries replaced by ``:``). Thus,
  ``x[ind1, ..., ind2,:]`` acts like ``x[ind1][..., ind2, :]`` under basic
  slicing.

  .. warning:: The above is **not** true for advanced indexing.

- You may use slicing to set values in the array, but (unlike lists) you
  can never grow the array. The size of the value to be set in
  ``x[obj] = value`` must be (broadcastable) to the same shape as
  ``x[obj]``.

- A slicing tuple can always be constructed as *obj*
  and used in the ``x[obj]`` notation. Slice objects can be used in
  the construction in place of the ``[start:stop:step]``
  notation. For example, ``x[1:10:5, ::-1]`` can also be implemented
  as ``obj = (slice(1, 10, 5), slice(None, None, -1)); x[obj]`` . This
  can be useful for constructing generic code that works on arrays
  of arbitrary dimensions. See :ref:`dealing-with-variable-indices`
  for more information.

.. index::
   pair: ndarray; view

.. _dimensional-indexing-tools:

Dimensional indexing tools
^^^^^^^^^^^^^^^^^^^^^^^^^^

There are some tools to facilitate the easy matching of array shapes with
expressions and in assignments.

:py:data:`Ellipsis` expands to the number of ``:`` objects needed for the
selection tuple to index all dimensions. In most cases, this means that the
length of the expanded selection tuple is ``x.ndim``. There may only be a
single ellipsis present.
From the above example::

    >>> x[..., 0]
    array([[1, 2, 3],
          [4, 5, 6]])

This is equivalent to::

    >>> x[:, :, 0]
    array([[1, 2, 3],
          [4, 5, 6]])

Each :const:`newaxis` object in the selection tuple serves to expand
the dimensions of the resulting selection by one unit-length
dimension.  The added dimension is the position of the :const:`newaxis`
object in the selection tuple. :const:`newaxis` is an alias for
``None``, and ``None`` can be used in place of this with the same result.
From the above example::

    >>> x[:, np.newaxis, :, :].shape
    (2, 1, 3, 1)
    >>> x[:, None, :, :].shape
    (2, 1, 3, 1)

This can be handy to combine two
arrays in a way that otherwise would require explicit reshaping
operations. For example::

    >>> x = np.arange(5)
    >>> x[:, np.newaxis] + x[np.newaxis, :]
    array([[0, 1, 2, 3, 4],
          [1, 2, 3, 4, 5],
          [2, 3, 4, 5, 6],
          [3, 4, 5, 6, 7],
          [4, 5, 6, 7, 8]])


.. _advanced-indexing:

Advanced indexing
-----------------

Advanced indexing is triggered when the selection object, *obj*, is a
non-tuple sequence object, an :class:`ndarray` (of data type integer or bool),
or a tuple with at least one sequence object or ndarray (of data type
integer or bool). There are two types of advanced indexing: integer
and Boolean.

Advanced indexing always returns a *copy* of the data (contrast with
basic slicing that returns a :term:`view`).

.. warning::

   The definition of advanced indexing means that ``x[(1, 2, 3),]`` is
   fundamentally different than ``x[(1, 2, 3)]``. The latter is
   equivalent to ``x[1, 2, 3]`` which will trigger basic selection while
   the former will trigger advanced indexing. Be sure to understand
   why this occurs.

   Also recognize that ``x[[1, 2, 3]]`` will trigger advanced indexing,
   whereas due to the deprecated Numeric compatibility mentioned above,
   ``x[[1, 2, slice(None)]]`` will trigger basic slicing.

Integer array indexing
^^^^^^^^^^^^^^^^^^^^^^

Integer array indexing allows selection of arbitrary items in the array
based on their *N*-dimensional index. Each integer array represents a number
of indices into that dimension.

Negative values are permitted in the index arrays and work as they do with
single indices or slices::

    >>> x = np.arange(10, 1, -1)
    >>> x
    array([10,  9,  8,  7,  6,  5,  4,  3,  2])
    >>> x[np.array([3, 3, 1, 8])]
    array([7, 7, 9, 2])
    >>> x[np.array([3, 3, -3, 8])]
    array([7, 7, 4, 2])

If the index values are out of bounds then an ``IndexError`` is thrown::

    >>> x = np.array([[1, 2], [3, 4], [5, 6]])
    >>> x[np.array([1, -1])]
    array([[3, 4],
          [5, 6]])
    >>> x[np.array([3, 4])]
    Traceback (most recent call last):
      ...
    IndexError: index 3 is out of bounds for axis 0 with size 3

When the index consists of as many integer arrays as dimensions of the array
being indexed, the indexing is straightforward, but different from slicing.

Advanced indices always are :ref:`broadcast<basics.broadcasting>` and
iterated as *one*::

     result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],
                                ..., ind_N[i_1, ..., i_M]]

Note that the resulting shape is identical to the (broadcast) indexing array
shapes ``ind_1, ..., ind_N``. If the indices cannot be broadcast to the
same shape, an exception ``IndexError: shape mismatch: indexing arrays could
not be broadcast together with shapes...`` is raised. 

Indexing with multidimensional index arrays tend
to be more unusual uses, but they are permitted, and they are useful for some
problems. We’ll start with the simplest multidimensional case::

    >>> y = np.arange(35).reshape(5, 7)
    >>> y
    array([[ 0,  1,  2,  3,  4,  5,  6],
           [ 7,  8,  9, 10, 11, 12, 13],
           [14, 15, 16, 17, 18, 19, 20],
           [21, 22, 23, 24, 25, 26, 27],
           [28, 29, 30, 31, 32, 33, 34]])
    >>> y[np.array([0, 2, 4]), np.array([0, 1, 2])]
    array([ 0, 15, 30])

In this case, if the index arrays have a matching shape, and there is an
index array for each dimension of the array being indexed, the resultant
array has the same shape as the index arrays, and the values correspond
to the index set for each position in the index arrays. In this example,
the first index value is 0 for both index arrays, and thus the first value
of the resultant array is ``y[0, 0]``. The next value is ``y[2, 1]``, and
the last is ``y[4, 2]``.

If the index arrays do not have the same shape, there is an attempt to
broadcast them to the same shape. If they cannot be broadcast to the same
shape, an exception is raised::

    >>> y[np.array([0, 2, 4]), np.array([0, 1])]
    Traceback (most recent call last):
      ...
    IndexError: shape mismatch: indexing arrays could not be broadcast
    together with shapes (3,) (2,)

The broadcasting mechanism permits index arrays to be combined with
scalars for other indices. The effect is that the scalar value is used
for all the corresponding values of the index arrays::

    >>> y[np.array([0, 2, 4]), 1]
    array([ 1, 15, 29])

Jumping to the next level of complexity, it is possible to only partially
index an array with index arrays. It takes a bit of thought to understand
what happens in such cases. For example if we just use one index array
with y::

    >>> y[np.array([0, 2, 4])]
    array([[ 0,  1,  2,  3,  4,  5,  6],
          [14, 15, 16, 17, 18, 19, 20],
          [28, 29, 30, 31, 32, 33, 34]])

It results in the construction of a new array where each value of the
index array selects one row from the array being indexed and the resultant
array has the resulting shape (number of index elements, size of row).

In general, the shape of the resultant array will be the concatenation of
the shape of the index array (or the shape that all the index arrays were
broadcast to) with the shape of any unused dimensions (those not indexed)
in the array being indexed.

.. rubric:: Example

From each row, a specific element should be selected. The row index is just
``[0, 1, 2]`` and the column index specifies the element to choose for the
corresponding row, here ``[0, 1, 0]``. Using both together the task
can be solved using advanced indexing::

    >>> x = np.array([[1, 2], [3, 4], [5, 6]])
    >>> x[[0, 1, 2], [0, 1, 0]]
    array([1, 4, 5])

To achieve a behaviour similar to the basic slicing above, broadcasting can be
used. The function :func:`ix_` can help with this broadcasting. This is best
understood with an example.

.. rubric:: Example

From a 4x3 array the corner elements should be selected using advanced
indexing. Thus all elements for which the column is one of ``[0, 2]`` and
the row is one of ``[0, 3]`` need to be selected. To use advanced indexing
one needs to select all elements *explicitly*. Using the method explained
previously one could write::

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> rows = np.array([[0, 0],
    ...                  [3, 3]], dtype=np.intp)
    >>> columns = np.array([[0, 2],
    ...                     [0, 2]], dtype=np.intp)
    >>> x[rows, columns]
    array([[ 0,  2],
           [ 9, 11]])

However, since the indexing arrays above just repeat themselves,
broadcasting can be used (compare operations such as
``rows[:, np.newaxis] + columns``) to simplify this::

    >>> rows = np.array([0, 3], dtype=np.intp)
    >>> columns = np.array([0, 2], dtype=np.intp)
    >>> rows[:, np.newaxis]
    array([[0],
           [3]])
    >>> x[rows[:, np.newaxis], columns]
    array([[ 0,  2],
           [ 9, 11]])

This broadcasting can also be achieved using the function :func:`ix_`:

    >>> x[np.ix_(rows, columns)]
    array([[ 0,  2],
           [ 9, 11]])

Note that without the ``np.ix_`` call, only the diagonal elements would
be selected::

    >>> x[rows, columns]
    array([ 0, 11])

This difference is the most important thing to remember about
indexing with multiple advanced indices.

.. rubric:: Example

A real-life example of where advanced indexing may be useful is for a color
lookup table where we want to map the values of an image into RGB triples for
display. The lookup table could have a shape (nlookup, 3). Indexing
such an array with an image with shape (ny, nx) with dtype=np.uint8
(or any integer type so long as values are with the bounds of the
lookup table) will result in an array of shape (ny, nx, 3) where a
triple of RGB values is associated with each pixel location.

.. _boolean-indexing:

Boolean array indexing
^^^^^^^^^^^^^^^^^^^^^^

This advanced indexing occurs when *obj* is an array object of Boolean
type, such as may be returned from comparison operators. A single
boolean index array is practically identical to ``x[obj.nonzero()]`` where,
as described above, :meth:`obj.nonzero() <ndarray.nonzero>` returns a
tuple (of length :attr:`obj.ndim <ndarray.ndim>`) of integer index
arrays showing the :py:data:`True` elements of *obj*. However, it is
faster when ``obj.shape == x.shape``.

If ``obj.ndim == x.ndim``, ``x[obj]`` returns a 1-dimensional array
filled with the elements of *x* corresponding to the :py:data:`True`
values of *obj*.  The search order will be :term:`row-major`,
C-style. If *obj* has :py:data:`True` values at entries that are outside
of the bounds of *x*, then an index error will be raised. If *obj* is
smaller than *x* it is identical to filling it with :py:data:`False`.

A common use case for this is filtering for desired element values.
For example, one may wish to select all entries from an array which
are not :const:`NaN`::

    >>> x = np.array([[1., 2.], [np.nan, 3.], [np.nan, np.nan]])
    >>> x[~np.isnan(x)]
    array([1., 2., 3.])

Or wish to add a constant to all negative elements::

    >>> x = np.array([1., -1., -2., 3])
    >>> x[x < 0] += 20
    >>> x
    array([ 1., 19., 18., 3.])

In general if an index includes a Boolean array, the result will be
identical to inserting ``obj.nonzero()`` into the same position
and using the integer array indexing mechanism described above.
``x[ind_1, boolean_array, ind_2]`` is equivalent to
``x[(ind_1,) + boolean_array.nonzero() + (ind_2,)]``.

If there is only one Boolean array and no integer indexing array present,
this is straightforward. Care must only be taken to make sure that the
boolean index has *exactly* as many dimensions as it is supposed to work
with.

In general, when the boolean array has fewer dimensions than the array being
indexed, this is equivalent to ``x[b, ...]``, which means x is indexed by b
followed by as many ``:`` as are needed to fill out the rank of x. Thus the
shape of the result is one dimension containing the number of True elements of
the boolean array, followed by the remaining dimensions of the array being
indexed::

    >>> x = np.arange(35).reshape(5, 7)
    >>> b = x > 20
    >>> b[:, 5]
    array([False, False, False,  True,  True])
    >>> x[b[:, 5]]
    array([[21, 22, 23, 24, 25, 26, 27],
          [28, 29, 30, 31, 32, 33, 34]])

Here the 4th and 5th rows are selected from the indexed array and
combined to make a 2-D array.

.. rubric:: Example

From an array, select all rows which sum up to less or equal two::

    >>> x = np.array([[0, 1], [1, 1], [2, 2]])
    >>> rowsum = x.sum(-1)
    >>> x[rowsum <= 2, :]
    array([[0, 1],
           [1, 1]])


Combining multiple Boolean indexing arrays or a Boolean with an integer
indexing array can best be understood with the
:meth:`obj.nonzero() <ndarray.nonzero>` analogy. The function :func:`ix_`
also supports boolean arrays and will work without any surprises.

.. rubric:: Example

Use boolean indexing to select all rows adding up to an even
number. At the same time columns 0 and 2 should be selected with an
advanced integer index. Using the :func:`ix_` function this can be done
with::

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> rows = (x.sum(-1) % 2) == 0
    >>> rows
    array([False,  True, False,  True])
    >>> columns = [0, 2]
    >>> x[np.ix_(rows, columns)]
    array([[ 3,  5],
           [ 9, 11]])

Without the ``np.ix_`` call, only the diagonal elements would be
selected.

Or without ``np.ix_`` (compare the integer array examples)::

    >>> rows = rows.nonzero()[0]
    >>> x[rows[:, np.newaxis], columns]
    array([[ 3,  5],
           [ 9, 11]])

.. rubric:: Example

Use a 2-D boolean array of shape (2, 3)
with four True elements to select rows from a 3-D array of shape
(2, 3, 5) results in a 2-D result of shape (4, 5)::

    >>> x = np.arange(30).reshape(2, 3, 5)
    >>> x
    array([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14]],
          [[15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]]])
    >>> b = np.array([[True, True, False], [False, True, True]])
    >>> x[b]
    array([[ 0,  1,  2,  3,  4],
          [ 5,  6,  7,  8,  9],
          [20, 21, 22, 23, 24],
          [25, 26, 27, 28, 29]])


.. _combining-advanced-and-basic-indexing:

Combining advanced and basic indexing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When there is at least one slice (``:``), ellipsis (``...``) or :const:`newaxis`
in the index (or the array has more dimensions than there are advanced indices),
then the behaviour can be more complicated. It is like concatenating the
indexing result for each advanced index element.

In the simplest case, there is only a *single* advanced index combined with
a slice. For example::

    >>> y = np.arange(35).reshape(5,7)
    >>> y[np.array([0, 2, 4]), 1:3]
    array([[ 1,  2],
           [15, 16],
           [29, 30]])

In effect, the slice and index array operation are independent. The slice
operation extracts columns with index 1 and 2, (i.e. the 2nd and 3rd columns),
followed by the index array operation which extracts rows with index 0, 2 and 4
(i.e the first, third and fifth rows). This is equivalent to::

    >>> y[:, 1:3][np.array([0, 2, 4]), :]
    array([[ 1,  2],
           [15, 16],
           [29, 30]])

A single advanced index can, for example, replace a slice and the result array
will be the same. However, it is a copy and may have a different memory layout.
A slice is preferable when it is possible.
For example::

    >>> x = np.array([[ 0,  1,  2],
    ...               [ 3,  4,  5],
    ...               [ 6,  7,  8],
    ...               [ 9, 10, 11]])
    >>> x[1:2, 1:3]
    array([[4, 5]])
    >>> x[1:2, [1, 2]]
    array([[4, 5]])

The easiest way to understand a combination of *multiple* advanced indices may
be to think in terms of the resulting shape. There are two parts to the indexing
operation, the subspace defined by the basic indexing (excluding integers) and
the subspace from the advanced indexing part. Two cases of index combination
need to be distinguished:

* The advanced indices are separated by a slice, :py:data:`Ellipsis` or
  :const:`newaxis`. For example ``x[arr1, :, arr2]``.
* The advanced indices are all next to each other.
  For example ``x[..., arr1, arr2, :]`` but *not* ``x[arr1, :, 1]``
  since ``1`` is an advanced index in this regard.

In the first case, the dimensions resulting from the advanced indexing
operation come first in the result array, and the subspace dimensions after
that.
In the second case, the dimensions from the advanced indexing operations
are inserted into the result array at the same spot as they were in the
initial array (the latter logic is what makes simple advanced indexing
behave just like slicing). 

.. rubric:: Example

Suppose ``x.shape`` is (10, 20, 30) and ``ind`` is a (2, 3, 4)-shaped
indexing :class:`intp` array, then ``result = x[..., ind, :]`` has
shape (10, 2, 3, 4, 30) because the (20,)-shaped subspace has been
replaced with a (2, 3, 4)-shaped broadcasted indexing subspace. If
we let *i, j, k* loop over the (2, 3, 4)-shaped subspace then
``result[..., i, j, k, :] = x[..., ind[i, j, k], :]``. This example
produces the same result as :meth:`x.take(ind, axis=-2) <ndarray.take>`.

.. rubric:: Example

Let ``x.shape`` be (10, 20, 30, 40, 50) and suppose ``ind_1``
and ``ind_2`` can be broadcast to the shape (2, 3, 4). Then
``x[:, ind_1, ind_2]`` has shape (10, 2, 3, 4, 40, 50) because the
(20, 30)-shaped subspace from X has been replaced with the
(2, 3, 4) subspace from the indices. However,
``x[:, ind_1, :, ind_2]`` has shape (2, 3, 4, 10, 30, 50) because there
is no unambiguous place to drop in the indexing subspace, thus
it is tacked-on to the beginning. It is always possible to use
:meth:`.transpose() <ndarray.transpose>` to move the subspace
anywhere desired. Note that this example cannot be replicated
using :func:`take`.

.. rubric:: Example

Slicing can be combined with broadcasted boolean indices::

    >>> x = np.arange(35).reshape(5, 7)
    >>> b = x > 20
    >>> b
    array([[False, False, False, False, False, False, False],
          [False, False, False, False, False, False, False],
          [False, False, False, False, False, False, False],
          [ True,  True,  True,  True,  True,  True,  True],
          [ True,  True,  True,  True,  True,  True,  True]])
    >>> x[b[:, 5], 1:3]
    array([[22, 23],
          [29, 30]])


.. _arrays.indexing.fields:

Field access
-------------

.. seealso:: :ref:`structured_arrays`

If the :class:`ndarray` object is a structured array the :term:`fields <field>`
of the array can be accessed by indexing the array with strings,
dictionary-like.

Indexing ``x['field-name']`` returns a new :term:`view` to the array,
which is of the same shape as *x* (except when the field is a
sub-array) but of data type ``x.dtype['field-name']`` and contains
only the part of the data in the specified field. Also,
:ref:`record array <arrays.classes.rec>` scalars can be "indexed" this way.

Indexing into a structured array can also be done with a list of field names,
e.g. ``x[['field-name1', 'field-name2']]``. As of NumPy 1.16, this returns a
view containing only those fields. In older versions of NumPy, it returned a
copy. See the user guide section on :ref:`structured_arrays` for more
information on multifield indexing.

If the accessed field is a sub-array, the dimensions of the sub-array
are appended to the shape of the result.
For example::

   >>> x = np.zeros((2, 2), dtype=[('a', np.int32), ('b', np.float64, (3, 3))])
   >>> x['a'].shape
   (2, 2)
   >>> x['a'].dtype
   dtype('int32')
   >>> x['b'].shape
   (2, 2, 3, 3)
   >>> x['b'].dtype
   dtype('float64')

.. _flat-iterator-indexing:

Flat Iterator indexing
----------------------

:attr:`x.flat <ndarray.flat>` returns an iterator that will iterate
over the entire array (in C-contiguous style with the last index
varying the fastest). This iterator object can also be indexed using
basic slicing or advanced indexing as long as the selection object is
not a tuple. This should be clear from the fact that :attr:`x.flat
<ndarray.flat>` is a 1-dimensional view. It can be used for integer
indexing with 1-dimensional C-style-flat indices. The shape of any
returned array is therefore the shape of the integer indexing object.

.. index::
   single: indexing
   single: ndarray


.. _assigning-values-to-indexed-arrays:

Assigning values to indexed arrays
----------------------------------

As mentioned, one can select a subset of an array to assign to using
a single index, slices, and index and mask arrays. The value being
assigned to the indexed array must be shape consistent (the same shape
or broadcastable to the shape the index produces). For example, it is
permitted to assign a constant to a slice: ::

 >>> x = np.arange(10)
 >>> x[2:7] = 1

or an array of the right size: ::

 >>> x[2:7] = np.arange(5)

Note that assignments may result in changes if assigning
higher types to lower types (like floats to ints) or even
exceptions (assigning complex to floats or ints): ::

 >>> x[1] = 1.2
 >>> x[1]
 1
 >>> x[1] = 1.2j
 Traceback (most recent call last):
   ...
 TypeError: can't convert complex to int


Unlike some of the references (such as array and mask indices)
assignments are always made to the original data in the array
(indeed, nothing else would make sense!). Note though, that some
actions may not work as one may naively expect. This particular
example is often surprising to people: ::

 >>> x = np.arange(0, 50, 10)
 >>> x
 array([ 0, 10, 20, 30, 40])
 >>> x[np.array([1, 1, 3, 1])] += 1
 >>> x
 array([ 0, 11, 20, 31, 40])

Where people expect that the 1st location will be incremented by 3.
In fact, it will only be incremented by 1. The reason is that
a new array is extracted from the original (as a temporary) containing
the values at 1, 1, 3, 1, then the value 1 is added to the temporary,
and then the temporary is assigned back to the original array. Thus
the value of the array at ``x[1] + 1`` is assigned to ``x[1]`` three times,
rather than being incremented 3 times.

.. _dealing-with-variable-indices:

Dealing with variable numbers of indices within programs
--------------------------------------------------------

The indexing syntax is very powerful but limiting when dealing with
a variable number of indices. For example, if you want to write
a function that can handle arguments with various numbers of
dimensions without having to write special case code for each
number of possible dimensions, how can that be done? If one
supplies to the index a tuple, the tuple will be interpreted
as a list of indices. For example::

 >>> z = np.arange(81).reshape(3, 3, 3, 3)
 >>> indices = (1, 1, 1, 1)
 >>> z[indices]
 40

So one can use code to construct tuples of any number of indices
and then use these within an index.

Slices can be specified within programs by using the slice() function
in Python. For example: ::

 >>> indices = (1, 1, 1, slice(0, 2))  # same as [1, 1, 1, 0:2]
 >>> z[indices]
 array([39, 40])

Likewise, ellipsis can be specified by code by using the Ellipsis
object: ::

 >>> indices = (1, Ellipsis, 1)  # same as [1, ..., 1]
 >>> z[indices]
 array([[28, 31, 34],
        [37, 40, 43],
        [46, 49, 52]])

For this reason, it is possible to use the output from the 
:meth:`np.nonzero() <ndarray.nonzero>` function directly as an index since
it always returns a tuple of index arrays.

Because of the special treatment of tuples, they are not automatically
converted to an array as a list would be. As an example: ::

 >>> z[[1, 1, 1, 1]]  # produces a large array
 array([[[[27, 28, 29],
          [30, 31, 32], ...
 >>> z[(1, 1, 1, 1)]  # returns a single value
 40


Detailed notes
--------------

These are some detailed notes, which are not of importance for day to day
indexing (in no particular order):

* The native NumPy indexing type is ``intp`` and may differ from the
  default integer array type. ``intp`` is the smallest data type
  sufficient to safely index any array; for advanced indexing it may be
  faster than other types.
* For advanced assignments, there is in general no guarantee for the
  iteration order. This means that if an element is set more than once,
  it is not possible to predict the final result.
* An empty (tuple) index is a full scalar index into a zero-dimensional array.
  ``x[()]`` returns a *scalar* if ``x`` is zero-dimensional and a view
  otherwise. On the other hand, ``x[...]`` always returns a view.
* If a zero-dimensional array is present in the index *and* it is a full
  integer index the result will be a *scalar* and not a zero-dimensional array.
  (Advanced indexing is not triggered.)
* When an ellipsis (``...``) is present but has no size (i.e. replaces zero
  ``:``) the result will still always be an array. A view if no advanced index
  is present, otherwise a copy.
* The ``nonzero`` equivalence for Boolean arrays does not hold for zero
  dimensional boolean arrays.
* When the result of an advanced indexing operation has no elements but an
  individual index is out of bounds, whether or not an ``IndexError`` is
  raised is undefined (e.g. ``x[[], [123]]`` with ``123`` being out of bounds).
* When a *casting* error occurs during assignment (for example updating a
  numerical array using a sequence of strings), the array being assigned
  to may end up in an unpredictable partially updated state.
  However, if any other error (such as an out of bounds index) occurs, the
  array will remain unchanged.
* The memory layout of an advanced indexing result is optimized for each
  indexing operation and no particular memory order can be assumed.
* When using a subclass (especially one which manipulates its shape), the
  default ``ndarray.__setitem__`` behaviour will call ``__getitem__`` for
  *basic* indexing but not for *advanced* indexing. For such a subclass it may
  be preferable to call ``ndarray.__setitem__`` with a *base class* ndarray
  view on the data. This *must* be done if the subclasses ``__getitem__`` does
  not return views.
.. _basics.subclassing:

*******************
Subclassing ndarray
*******************

Introduction
------------

Subclassing ndarray is relatively simple, but it has some complications
compared to other Python objects.  On this page we explain the machinery
that allows you to subclass ndarray, and the implications for
implementing a subclass.

ndarrays and object creation
============================

Subclassing ndarray is complicated by the fact that new instances of
ndarray classes can come about in three different ways.  These are:

#. Explicit constructor call - as in ``MySubClass(params)``.  This is
   the usual route to Python instance creation.
#. View casting - casting an existing ndarray as a given subclass
#. New from template - creating a new instance from a template
   instance. Examples include returning slices from a subclassed array,
   creating return types from ufuncs, and copying arrays.  See
   :ref:`new-from-template` for more details

The last two are characteristics of ndarrays - in order to support
things like array slicing.  The complications of subclassing ndarray are
due to the mechanisms numpy has to support these latter two routes of
instance creation.

.. _view-casting:

View casting
------------

*View casting* is the standard ndarray mechanism by which you take an
ndarray of any subclass, and return a view of the array as another
(specified) subclass:

>>> import numpy as np
>>> # create a completely useless ndarray subclass
>>> class C(np.ndarray): pass
>>> # create a standard ndarray
>>> arr = np.zeros((3,))
>>> # take a view of it, as our useless subclass
>>> c_arr = arr.view(C)
>>> type(c_arr)
<class '__main__.C'>

.. _new-from-template:

Creating new from template
--------------------------

New instances of an ndarray subclass can also come about by a very
similar mechanism to :ref:`view-casting`, when numpy finds it needs to
create a new instance from a template instance.  The most obvious place
this has to happen is when you are taking slices of subclassed arrays.
For example:

>>> v = c_arr[1:]
>>> type(v) # the view is of type 'C'
<class '__main__.C'>
>>> v is c_arr # but it's a new instance
False

The slice is a *view* onto the original ``c_arr`` data.  So, when we
take a view from the ndarray, we return a new ndarray, of the same
class, that points to the data in the original.

There are other points in the use of ndarrays where we need such views,
such as copying arrays (``c_arr.copy()``), creating ufunc output arrays
(see also :ref:`array-wrap`), and reducing methods (like
``c_arr.mean()``).

Relationship of view casting and new-from-template
--------------------------------------------------

These paths both use the same machinery.  We make the distinction here,
because they result in different input to your methods.  Specifically,
:ref:`view-casting` means you have created a new instance of your array
type from any potential subclass of ndarray.  :ref:`new-from-template`
means you have created a new instance of your class from a pre-existing
instance, allowing you - for example - to copy across attributes that
are particular to your subclass.

Implications for subclassing
----------------------------

If we subclass ndarray, we need to deal not only with explicit
construction of our array type, but also :ref:`view-casting` or
:ref:`new-from-template`.  NumPy has the machinery to do this, and it is
this machinery that makes subclassing slightly non-standard.

There are two aspects to the machinery that ndarray uses to support
views and new-from-template in subclasses.

The first is the use of the ``ndarray.__new__`` method for the main work
of object initialization, rather then the more usual ``__init__``
method.  The second is the use of the ``__array_finalize__`` method to
allow subclasses to clean up after the creation of views and new
instances from templates.

A brief Python primer on ``__new__`` and ``__init__``
=====================================================

``__new__`` is a standard Python method, and, if present, is called
before ``__init__`` when we create a class instance. See the `python
__new__ documentation
<https://docs.python.org/reference/datamodel.html#object.__new__>`_ for more detail.

For example, consider the following Python code:

>>> class C:
>>>     def __new__(cls, *args):
>>>         print('Cls in __new__:', cls)
>>>         print('Args in __new__:', args)
>>>         # The `object` type __new__ method takes a single argument.
>>>         return object.__new__(cls)
>>>     def __init__(self, *args):
>>>         print('type(self) in __init__:', type(self))
>>>         print('Args in __init__:', args)

meaning that we get:

>>> c = C('hello')
Cls in __new__: <class 'C'>
Args in __new__: ('hello',)
type(self) in __init__: <class 'C'>
Args in __init__: ('hello',)

When we call ``C('hello')``, the ``__new__`` method gets its own class
as first argument, and the passed argument, which is the string
``'hello'``.  After python calls ``__new__``, it usually (see below)
calls our ``__init__`` method, with the output of ``__new__`` as the
first argument (now a class instance), and the passed arguments
following.

As you can see, the object can be initialized in the ``__new__``
method or the ``__init__`` method, or both, and in fact ndarray does
not have an ``__init__`` method, because all the initialization is
done in the ``__new__`` method.

Why use ``__new__`` rather than just the usual ``__init__``?  Because
in some cases, as for ndarray, we want to be able to return an object
of some other class.  Consider the following:

.. testcode::

  class D(C):
      def __new__(cls, *args):
          print('D cls is:', cls)
          print('D args in __new__:', args)
          return C.__new__(C, *args)

      def __init__(self, *args):
          # we never get here
          print('In D __init__')

meaning that:

>>> obj = D('hello')
D cls is: <class 'D'>
D args in __new__: ('hello',)
Cls in __new__: <class 'C'>
Args in __new__: ('hello',)
>>> type(obj)
<class 'C'>

The definition of ``C`` is the same as before, but for ``D``, the
``__new__`` method returns an instance of class ``C`` rather than
``D``.  Note that the ``__init__`` method of ``D`` does not get
called.  In general, when the ``__new__`` method returns an object of
class other than the class in which it is defined, the ``__init__``
method of that class is not called.

This is how subclasses of the ndarray class are able to return views
that preserve the class type.  When taking a view, the standard
ndarray machinery creates the new ndarray object with something
like::

  obj = ndarray.__new__(subtype, shape, ...

where ``subdtype`` is the subclass.  Thus the returned view is of the
same class as the subclass, rather than being of class ``ndarray``.

That solves the problem of returning views of the same type, but now
we have a new problem.  The machinery of ndarray can set the class
this way, in its standard methods for taking views, but the ndarray
``__new__`` method knows nothing of what we have done in our own
``__new__`` method in order to set attributes, and so on.  (Aside -
why not call ``obj = subdtype.__new__(...`` then?  Because we may not
have a ``__new__`` method with the same call signature).

The role of ``__array_finalize__``
==================================

``__array_finalize__`` is the mechanism that numpy provides to allow
subclasses to handle the various ways that new instances get created.

Remember that subclass instances can come about in these three ways:

#. explicit constructor call (``obj = MySubClass(params)``).  This will
   call the usual sequence of ``MySubClass.__new__`` then (if it exists)
   ``MySubClass.__init__``.
#. :ref:`view-casting`
#. :ref:`new-from-template`

Our ``MySubClass.__new__`` method only gets called in the case of the
explicit constructor call, so we can't rely on ``MySubClass.__new__`` or
``MySubClass.__init__`` to deal with the view casting and
new-from-template.  It turns out that ``MySubClass.__array_finalize__``
*does* get called for all three methods of object creation, so this is
where our object creation housekeeping usually goes.

* For the explicit constructor call, our subclass will need to create a
  new ndarray instance of its own class.  In practice this means that
  we, the authors of the code, will need to make a call to
  ``ndarray.__new__(MySubClass,...)``, a class-hierarchy prepared call to
  ``super().__new__(cls, ...)``, or do view casting of an existing array
  (see below)
* For view casting and new-from-template, the equivalent of
  ``ndarray.__new__(MySubClass,...`` is called, at the C level.

The arguments that ``__array_finalize__`` receives differ for the three
methods of instance creation above.

The following code allows us to look at the call sequences and arguments:

.. testcode::

   import numpy as np

   class C(np.ndarray):
       def __new__(cls, *args, **kwargs):
           print('In __new__ with class %s' % cls)
           return super().__new__(cls, *args, **kwargs)

       def __init__(self, *args, **kwargs):
           # in practice you probably will not need or want an __init__
           # method for your subclass
           print('In __init__ with class %s' % self.__class__)

       def __array_finalize__(self, obj):
           print('In array_finalize:')
           print('   self type is %s' % type(self))
           print('   obj type is %s' % type(obj))


Now:

>>> # Explicit constructor
>>> c = C((10,))
In __new__ with class <class 'C'>
In array_finalize:
   self type is <class 'C'>
   obj type is <type 'NoneType'>
In __init__ with class <class 'C'>
>>> # View casting
>>> a = np.arange(10)
>>> cast_a = a.view(C)
In array_finalize:
   self type is <class 'C'>
   obj type is <type 'numpy.ndarray'>
>>> # Slicing (example of new-from-template)
>>> cv = c[:1]
In array_finalize:
   self type is <class 'C'>
   obj type is <class 'C'>

The signature of ``__array_finalize__`` is::

    def __array_finalize__(self, obj):

One sees that the ``super`` call, which goes to
``ndarray.__new__``, passes ``__array_finalize__`` the new object, of our
own class (``self``) as well as the object from which the view has been
taken (``obj``).  As you can see from the output above, the ``self`` is
always a newly created instance of our subclass, and the type of ``obj``
differs for the three instance creation methods:

* When called from the explicit constructor, ``obj`` is ``None``
* When called from view casting, ``obj`` can be an instance of any
  subclass of ndarray, including our own.
* When called in new-from-template, ``obj`` is another instance of our
  own subclass, that we might use to update the new ``self`` instance.

Because ``__array_finalize__`` is the only method that always sees new
instances being created, it is the sensible place to fill in instance
defaults for new object attributes, among other tasks.

This may be clearer with an example.

Simple example - adding an extra attribute to ndarray
-----------------------------------------------------

.. testcode::

  import numpy as np

  class InfoArray(np.ndarray):

      def __new__(subtype, shape, dtype=float, buffer=None, offset=0,
                  strides=None, order=None, info=None):
          # Create the ndarray instance of our type, given the usual
          # ndarray input arguments.  This will call the standard
          # ndarray constructor, but return an object of our type.
          # It also triggers a call to InfoArray.__array_finalize__
          obj = super().__new__(subtype, shape, dtype,
                                buffer, offset, strides, order)
          # set the new 'info' attribute to the value passed
          obj.info = info
          # Finally, we must return the newly created object:
          return obj

      def __array_finalize__(self, obj):
          # ``self`` is a new object resulting from
          # ndarray.__new__(InfoArray, ...), therefore it only has
          # attributes that the ndarray.__new__ constructor gave it -
          # i.e. those of a standard ndarray.
          #
          # We could have got to the ndarray.__new__ call in 3 ways:
          # From an explicit constructor - e.g. InfoArray():
          #    obj is None
          #    (we're in the middle of the InfoArray.__new__
          #    constructor, and self.info will be set when we return to
          #    InfoArray.__new__)
          if obj is None: return
          # From view casting - e.g arr.view(InfoArray):
          #    obj is arr
          #    (type(obj) can be InfoArray)
          # From new-from-template - e.g infoarr[:3]
          #    type(obj) is InfoArray
          #
          # Note that it is here, rather than in the __new__ method,
          # that we set the default value for 'info', because this
          # method sees all creation of default objects - with the
          # InfoArray.__new__ constructor, but also with
          # arr.view(InfoArray).
          self.info = getattr(obj, 'info', None)
          # We do not need to return anything


Using the object looks like this:

  >>> obj = InfoArray(shape=(3,)) # explicit constructor
  >>> type(obj)
  <class 'InfoArray'>
  >>> obj.info is None
  True
  >>> obj = InfoArray(shape=(3,), info='information')
  >>> obj.info
  'information'
  >>> v = obj[1:] # new-from-template - here - slicing
  >>> type(v)
  <class 'InfoArray'>
  >>> v.info
  'information'
  >>> arr = np.arange(10)
  >>> cast_arr = arr.view(InfoArray) # view casting
  >>> type(cast_arr)
  <class 'InfoArray'>
  >>> cast_arr.info is None
  True

This class isn't very useful, because it has the same constructor as the
bare ndarray object, including passing in buffers and shapes and so on.
We would probably prefer the constructor to be able to take an already
formed ndarray from the usual numpy calls to ``np.array`` and return an
object.

Slightly more realistic example - attribute added to existing array
-------------------------------------------------------------------

Here is a class that takes a standard ndarray that already exists, casts
as our type, and adds an extra attribute.

.. testcode::

  import numpy as np

  class RealisticInfoArray(np.ndarray):

      def __new__(cls, input_array, info=None):
          # Input array is an already formed ndarray instance
          # We first cast to be our class type
          obj = np.asarray(input_array).view(cls)
          # add the new attribute to the created instance
          obj.info = info
          # Finally, we must return the newly created object:
          return obj

      def __array_finalize__(self, obj):
          # see InfoArray.__array_finalize__ for comments
          if obj is None: return
          self.info = getattr(obj, 'info', None)


So:

  >>> arr = np.arange(5)
  >>> obj = RealisticInfoArray(arr, info='information')
  >>> type(obj)
  <class 'RealisticInfoArray'>
  >>> obj.info
  'information'
  >>> v = obj[1:]
  >>> type(v)
  <class 'RealisticInfoArray'>
  >>> v.info
  'information'

.. _array-ufunc:

``__array_ufunc__`` for ufuncs
------------------------------

  .. versionadded:: 1.13

A subclass can override what happens when executing numpy ufuncs on it by
overriding the default ``ndarray.__array_ufunc__`` method. This method is
executed *instead* of the ufunc and should return either the result of the
operation, or :obj:`NotImplemented` if the operation requested is not
implemented.

The signature of ``__array_ufunc__`` is::

    def __array_ufunc__(ufunc, method, *inputs, **kwargs):

    - *ufunc* is the ufunc object that was called.
    - *method* is a string indicating how the Ufunc was called, either
      ``"__call__"`` to indicate it was called directly, or one of its
      :ref:`methods<ufuncs.methods>`: ``"reduce"``, ``"accumulate"``,
      ``"reduceat"``, ``"outer"``, or ``"at"``.
    - *inputs* is a tuple of the input arguments to the ``ufunc``
    - *kwargs* contains any optional or keyword arguments passed to the
      function. This includes any ``out`` arguments, which are always
      contained in a tuple.

A typical implementation would convert any inputs or outputs that are
instances of one's own class, pass everything on to a superclass using
``super()``, and finally return the results after possible
back-conversion. An example, taken from the test case
``test_ufunc_override_with_super`` in ``core/tests/test_umath.py``, is the
following.

.. testcode::

    input numpy as np

    class A(np.ndarray):
        def __array_ufunc__(self, ufunc, method, *inputs, out=None, **kwargs):
            args = []
            in_no = []
            for i, input_ in enumerate(inputs):
                if isinstance(input_, A):
                    in_no.append(i)
                    args.append(input_.view(np.ndarray))
                else:
                    args.append(input_)

            outputs = out
            out_no = []
            if outputs:
                out_args = []
                for j, output in enumerate(outputs):
                    if isinstance(output, A):
                        out_no.append(j)
                        out_args.append(output.view(np.ndarray))
                    else:
                        out_args.append(output)
                kwargs['out'] = tuple(out_args)
            else:
                outputs = (None,) * ufunc.nout

            info = {}
            if in_no:
                info['inputs'] = in_no
            if out_no:
                info['outputs'] = out_no

            results = super().__array_ufunc__(ufunc, method, *args, **kwargs)
            if results is NotImplemented:
                return NotImplemented

            if method == 'at':
                if isinstance(inputs[0], A):
                    inputs[0].info = info
                return

            if ufunc.nout == 1:
                results = (results,)

            results = tuple((np.asarray(result).view(A)
                             if output is None else output)
                            for result, output in zip(results, outputs))
            if results and isinstance(results[0], A):
                results[0].info = info

            return results[0] if len(results) == 1 else results

So, this class does not actually do anything interesting: it just
converts any instances of its own to regular ndarray (otherwise, we'd
get infinite recursion!), and adds an ``info`` dictionary that tells
which inputs and outputs it converted. Hence, e.g.,

>>> a = np.arange(5.).view(A)
>>> b = np.sin(a)
>>> b.info
{'inputs': [0]}
>>> b = np.sin(np.arange(5.), out=(a,))
>>> b.info
{'outputs': [0]}
>>> a = np.arange(5.).view(A)
>>> b = np.ones(1).view(A)
>>> c = a + b
>>> c.info
{'inputs': [0, 1]}
>>> a += b
>>> a.info
{'inputs': [0, 1], 'outputs': [0]}

Note that another approach would be to use ``getattr(ufunc,
methods)(*inputs, **kwargs)`` instead of the ``super`` call. For this example,
the result would be identical, but there is a difference if another operand
also defines ``__array_ufunc__``. E.g., lets assume that we evalulate
``np.add(a, b)``, where ``b`` is an instance of another class ``B`` that has
an override.  If you use ``super`` as in the example,
``ndarray.__array_ufunc__`` will notice that ``b`` has an override, which
means it cannot evaluate the result itself. Thus, it will return
`NotImplemented` and so will our class ``A``. Then, control will be passed
over to ``b``, which either knows how to deal with us and produces a result,
or does not and returns `NotImplemented`, raising a ``TypeError``.

If instead, we replace our ``super`` call with ``getattr(ufunc, method)``, we
effectively do ``np.add(a.view(np.ndarray), b)``. Again, ``B.__array_ufunc__``
will be called, but now it sees an ``ndarray`` as the other argument. Likely,
it will know how to handle this, and return a new instance of the ``B`` class
to us. Our example class is not set up to handle this, but it might well be
the best approach if, e.g., one were to re-implement ``MaskedArray`` using
``__array_ufunc__``.

As a final note: if the ``super`` route is suited to a given class, an
advantage of using it is that it helps in constructing class hierarchies.
E.g., suppose that our other class ``B`` also used the ``super`` in its
``__array_ufunc__`` implementation, and we created a class ``C`` that depended
on both, i.e., ``class C(A, B)`` (with, for simplicity, not another
``__array_ufunc__`` override). Then any ufunc on an instance of ``C`` would
pass on to ``A.__array_ufunc__``, the ``super`` call in ``A`` would go to
``B.__array_ufunc__``, and the ``super`` call in ``B`` would go to
``ndarray.__array_ufunc__``, thus allowing ``A`` and ``B`` to collaborate.

.. _array-wrap:

``__array_wrap__`` for ufuncs and other functions
-------------------------------------------------

Prior to numpy 1.13, the behaviour of ufuncs could only be tuned using
``__array_wrap__`` and ``__array_prepare__``. These two allowed one to
change the output type of a ufunc, but, in contrast to
``__array_ufunc__``, did not allow one to make any changes to the inputs.
It is hoped to eventually deprecate these, but ``__array_wrap__`` is also
used by other numpy functions and methods, such as ``squeeze``, so at the
present time is still needed for full functionality.

Conceptually, ``__array_wrap__`` "wraps up the action" in the sense of
allowing a subclass to set the type of the return value and update
attributes and metadata.  Let's show how this works with an example.  First
we return to the simpler example subclass, but with a different name and
some print statements:

.. testcode::

  import numpy as np

  class MySubClass(np.ndarray):

      def __new__(cls, input_array, info=None):
          obj = np.asarray(input_array).view(cls)
          obj.info = info
          return obj

      def __array_finalize__(self, obj):
          print('In __array_finalize__:')
          print('   self is %s' % repr(self))
          print('   obj is %s' % repr(obj))
          if obj is None: return
          self.info = getattr(obj, 'info', None)

      def __array_wrap__(self, out_arr, context=None):
          print('In __array_wrap__:')
          print('   self is %s' % repr(self))
          print('   arr is %s' % repr(out_arr))
          # then just call the parent
          return super().__array_wrap__(self, out_arr, context)

We run a ufunc on an instance of our new array:

>>> obj = MySubClass(np.arange(5), info='spam')
In __array_finalize__:
   self is MySubClass([0, 1, 2, 3, 4])
   obj is array([0, 1, 2, 3, 4])
>>> arr2 = np.arange(5)+1
>>> ret = np.add(arr2, obj)
In __array_wrap__:
   self is MySubClass([0, 1, 2, 3, 4])
   arr is array([1, 3, 5, 7, 9])
In __array_finalize__:
   self is MySubClass([1, 3, 5, 7, 9])
   obj is MySubClass([0, 1, 2, 3, 4])
>>> ret
MySubClass([1, 3, 5, 7, 9])
>>> ret.info
'spam'

Note that the ufunc (``np.add``) has called the ``__array_wrap__`` method
with arguments ``self`` as ``obj``, and ``out_arr`` as the (ndarray) result
of the addition.  In turn, the default ``__array_wrap__``
(``ndarray.__array_wrap__``) has cast the result to class ``MySubClass``,
and called ``__array_finalize__`` - hence the copying of the ``info``
attribute.  This has all happened at the C level.

But, we could do anything we wanted:

.. testcode::

  class SillySubClass(np.ndarray):

      def __array_wrap__(self, arr, context=None):
          return 'I lost your data'

>>> arr1 = np.arange(5)
>>> obj = arr1.view(SillySubClass)
>>> arr2 = np.arange(5)
>>> ret = np.multiply(obj, arr2)
>>> ret
'I lost your data'

So, by defining a specific ``__array_wrap__`` method for our subclass,
we can tweak the output from ufuncs. The ``__array_wrap__`` method
requires ``self``, then an argument - which is the result of the ufunc -
and an optional parameter *context*. This parameter is returned by
ufuncs as a 3-element tuple: (name of the ufunc, arguments of the ufunc,
domain of the ufunc), but is not set by other numpy functions. Though,
as seen above, it is possible to do otherwise, ``__array_wrap__`` should
return an instance of its containing class.  See the masked array
subclass for an implementation.

In addition to ``__array_wrap__``, which is called on the way out of the
ufunc, there is also an ``__array_prepare__`` method which is called on
the way into the ufunc, after the output arrays are created but before any
computation has been performed. The default implementation does nothing
but pass through the array. ``__array_prepare__`` should not attempt to
access the array data or resize the array, it is intended for setting the
output array type, updating attributes and metadata, and performing any
checks based on the input that may be desired before computation begins.
Like ``__array_wrap__``, ``__array_prepare__`` must return an ndarray or
subclass thereof or raise an error.

Extra gotchas - custom ``__del__`` methods and ndarray.base
-----------------------------------------------------------

One of the problems that ndarray solves is keeping track of memory
ownership of ndarrays and their views.  Consider the case where we have
created an ndarray, ``arr`` and have taken a slice with ``v = arr[1:]``.
The two objects are looking at the same memory.  NumPy keeps track of
where the data came from for a particular array or view, with the
``base`` attribute:

>>> # A normal ndarray, that owns its own data
>>> arr = np.zeros((4,))
>>> # In this case, base is None
>>> arr.base is None
True
>>> # We take a view
>>> v1 = arr[1:]
>>> # base now points to the array that it derived from
>>> v1.base is arr
True
>>> # Take a view of a view
>>> v2 = v1[1:]
>>> # base points to the original array that it was derived from
>>> v2.base is arr
True

In general, if the array owns its own memory, as for ``arr`` in this
case, then ``arr.base`` will be None - there are some exceptions to this
- see the numpy book for more details.

The ``base`` attribute is useful in being able to tell whether we have
a view or the original array.  This in turn can be useful if we need
to know whether or not to do some specific cleanup when the subclassed
array is deleted.  For example, we may only want to do the cleanup if
the original array is deleted, but not the views.  For an example of
how this can work, have a look at the ``memmap`` class in
``numpy.core``.

Subclassing and Downstream Compatibility
----------------------------------------

When sub-classing ``ndarray`` or creating duck-types that mimic the ``ndarray``
interface, it is your responsibility to decide how aligned your APIs will be
with those of numpy. For convenience, many numpy functions that have a corresponding
``ndarray`` method (e.g., ``sum``, ``mean``, ``take``, ``reshape``) work by checking
if the first argument to a function has a method of the same name. If it exists, the
method is called instead of coercing the arguments to a numpy array.

For example, if you want your sub-class or duck-type to be compatible with
numpy's ``sum`` function, the method signature for this object's ``sum`` method
should be the following:

.. testcode::

    def sum(self, axis=None, dtype=None, out=None, keepdims=False):
    ...

This is the exact same method signature for ``np.sum``, so now if a user calls
``np.sum`` on this object, numpy will call the object's own ``sum`` method and
pass in these arguments enumerated above in the signature, and no errors will
be raised because the signatures are completely compatible with each other.

If, however, you decide to deviate from this signature and do something like this:

.. testcode::

   def sum(self, axis=None, dtype=None):
   ...

This object is no longer compatible with ``np.sum`` because if you call ``np.sum``,
it will pass in unexpected arguments ``out`` and ``keepdims``, causing a TypeError
to be raised.

If you wish to maintain compatibility with numpy and its subsequent versions (which
might add new keyword arguments) but do not want to surface all of numpy's arguments,
your function's signature should accept ``**kwargs``. For example:

.. testcode::

   def sum(self, axis=None, dtype=None, **unused_kwargs):
   ...

This object is now compatible with ``np.sum`` again because any extraneous arguments
(i.e. keywords that are not ``axis`` or ``dtype``) will be hidden away in the
``**unused_kwargs`` parameter.


====================
Using Python as glue
====================

|    There is no conversation more boring than the one where everybody
|    agrees.
|    --- *Michel de Montaigne*

|    Duct tape is like the force. It has a light side, and a dark side, and
|    it holds the universe together.
|    --- *Carl Zwanzig*

Many people like to say that Python is a fantastic glue language.
Hopefully, this Chapter will convince you that this is true. The first
adopters of Python for science were typically people who used it to
glue together large application codes running on super-computers. Not
only was it much nicer to code in Python than in a shell script or
Perl, in addition, the ability to easily extend Python made it
relatively easy to create new classes and types specifically adapted
to the problems being solved. From the interactions of these early
contributors, Numeric emerged as an array-like object that could be
used to pass data between these applications.

As Numeric has matured and developed into NumPy, people have been able
to write more code directly in NumPy. Often this code is fast-enough
for production use, but there are still times that there is a need to
access compiled code. Either to get that last bit of efficiency out of
the algorithm or to make it easier to access widely-available codes
written in C/C++ or Fortran.

This chapter will review many of the tools that are available for the
purpose of accessing code written in other compiled languages. There
are many resources available for learning to call other compiled
libraries from Python and the purpose of this Chapter is not to make
you an expert. The main goal is to make you aware of some of the
possibilities so that you will know what to "Google" in order to learn more.


Calling other compiled libraries from Python
============================================

While Python is a great language and a pleasure to code in, its
dynamic nature results in overhead that can cause some code ( *i.e.*
raw computations inside of for loops) to be up 10-100 times slower
than equivalent code written in a static compiled language. In
addition, it can cause memory usage to be larger than necessary as
temporary arrays are created and destroyed during computation. For
many types of computing needs, the extra slow-down and memory
consumption can often not be spared (at least for time- or memory-
critical portions of your code). Therefore one of the most common
needs is to call out from Python code to a fast, machine-code routine
(e.g. compiled using C/C++ or Fortran). The fact that this is
relatively easy to do is a big reason why Python is such an excellent
high-level language for scientific and engineering programming.

Their are two basic approaches to calling compiled code: writing an
extension module that is then imported to Python using the import
command, or calling a shared-library subroutine directly from Python
using the `ctypes <https://docs.python.org/3/library/ctypes.html>`_
module.  Writing an extension module is the most common method.

.. warning::

    Calling C-code from Python can result in Python crashes if you are not
    careful. None of the approaches in this chapter are immune. You have
    to know something about the way data is handled by both NumPy and by
    the third-party library being used.


Hand-generated wrappers
=======================

Extension modules were discussed in :ref:`writing-an-extension`.
The most basic way to interface with compiled code is to write
an extension module and construct a module method that calls
the compiled code. For improved readability, your method should
take advantage of the ``PyArg_ParseTuple`` call to convert between
Python objects and C data-types. For standard C data-types there
is probably already a built-in converter. For others you may need 
to write your own converter and use the ``"O&"`` format string which
allows you to specify a function that will be used to perform the
conversion from the Python object to whatever C-structures are needed.

Once the conversions to the appropriate C-structures and C data-types
have been performed, the next step in the wrapper is to call the
underlying function. This is straightforward if the underlying
function is in C or C++. However, in order to call Fortran code you
must be familiar with how Fortran subroutines are called from C/C++
using your compiler and platform. This can vary somewhat platforms and
compilers (which is another reason f2py makes life much simpler for
interfacing Fortran code) but generally involves underscore mangling
of the name and the fact that all variables are passed by reference
(i.e. all arguments are pointers).

The advantage of the hand-generated wrapper is that you have complete
control over how the C-library gets used and called which can lead to
a lean and tight interface with minimal over-head. The disadvantage is
that you have to write, debug, and maintain C-code, although most of
it can be adapted using the time-honored technique of
"cutting-pasting-and-modifying" from other extension modules. Because,
the procedure of calling out to additional C-code is fairly
regimented, code-generation procedures have been developed to make
this process easier. One of these code-generation techniques is
distributed with NumPy and allows easy integration with Fortran and
(simple) C code. This package, f2py, will be covered briefly in the
next section.


f2py
====

F2py allows you to automatically construct an extension module that
interfaces to routines in Fortran 77/90/95 code. It has the ability to
parse Fortran 77/90/95 code and automatically generate Python
signatures for the subroutines it encounters, or you can guide how the
subroutine interfaces with Python by constructing an interface-definition-file
(or modifying the f2py-produced one).

.. index::
   single: f2py

Creating source for a basic extension module
--------------------------------------------

Probably the easiest way to introduce f2py is to offer a simple
example. Here is one of the subroutines contained in a file named
:file:`add.f`

.. code-block:: fortran

    C
          SUBROUTINE ZADD(A,B,C,N)
    C
          DOUBLE COMPLEX A(*)
          DOUBLE COMPLEX B(*)
          DOUBLE COMPLEX C(*)
          INTEGER N
          DO 20 J = 1, N
             C(J) = A(J)+B(J)
     20   CONTINUE
          END

This routine simply adds the elements in two contiguous arrays and
places the result in a third. The memory for all three arrays must be
provided by the calling routine. A very basic interface to this
routine can be automatically generated by f2py::

    f2py -m add add.f

You should be able to run this command assuming your search-path is
set-up properly. This command will produce an extension module named
:file:`addmodule.c` in the current directory. This extension module can now be
compiled and used from Python just like any other extension module.


Creating a compiled extension module
------------------------------------

You can also get f2py to both compile :file:`add.f` along with the produced
extension module leaving only a shared-library extension file that can
be imported from Python::

    f2py -c -m add add.f

This command leaves a file named add.{ext} in the current directory
(where {ext} is the appropriate extension for a Python extension
module on your platform --- so, pyd, *etc.* ). This module may then be
imported from Python. It will contain a method for each subroutine in
add (zadd, cadd, dadd, sadd). The docstring of each method contains
information about how the module method may be called::

    >>> import add
    >>> print(add.zadd.__doc__)
    zadd(a,b,c,n)

    Wrapper for ``zadd``.

    Parameters
    ----------
    a : input rank-1 array('D') with bounds (*)
    b : input rank-1 array('D') with bounds (*)
    c : input rank-1 array('D') with bounds (*)
    n : input int

Improving the basic interface
-----------------------------

The default interface is a very literal translation of the Fortran
code into Python. The Fortran array arguments must now be NumPy arrays
and the integer argument should be an integer. The interface will
attempt to convert all arguments to their required types (and shapes)
and issue an error if unsuccessful. However, because it knows nothing
about the semantics of the arguments (such that C is an output and n
should really match the array sizes), it is possible to abuse this
function in ways that can cause Python to crash. For example::

    >>> add.zadd([1, 2, 3], [1, 2], [3, 4], 1000)

will cause a program crash on most systems. Under the covers, the
lists are being converted to proper arrays but then the underlying add
loop is told to cycle way beyond the borders of the allocated memory.

In order to improve the interface, directives should be provided. This
is accomplished by constructing an interface definition file. It is
usually best to start from the interface file that f2py can produce
(where it gets its default behavior from). To get f2py to generate the
interface file use the -h option::

    f2py -h add.pyf -m add add.f

This command leaves the file add.pyf in the current directory. The
section of this file corresponding to zadd is:

.. code-block:: fortran

    subroutine zadd(a,b,c,n) ! in :add:add.f
       double complex dimension(*) :: a
       double complex dimension(*) :: b
       double complex dimension(*) :: c
       integer :: n
    end subroutine zadd

By placing intent directives and checking code, the interface can be
cleaned up quite a bit until the Python module method is both easier
to use and more robust.

.. code-block:: fortran

    subroutine zadd(a,b,c,n) ! in :add:add.f
       double complex dimension(n) :: a
       double complex dimension(n) :: b
       double complex intent(out),dimension(n) :: c
       integer intent(hide),depend(a) :: n=len(a)
    end subroutine zadd

The intent directive, intent(out) is used to tell f2py that ``c`` is
an output variable and should be created by the interface before being
passed to the underlying code. The intent(hide) directive tells f2py
to not allow the user to specify the variable, ``n``, but instead to
get it from the size of ``a``. The depend( ``a`` ) directive is
necessary to tell f2py that the value of n depends on the input a (so
that it won't try to create the variable n until the variable a is
created).

After modifying ``add.pyf``, the new Python module file can be generated
by compiling both ``add.f`` and ``add.pyf``::

    f2py -c add.pyf add.f

The new interface has docstring::

    >>> import add
    >>> print(add.zadd.__doc__)
    c = zadd(a,b)

    Wrapper for ``zadd``.

    Parameters
    ----------
    a : input rank-1 array('D') with bounds (n)
    b : input rank-1 array('D') with bounds (n)

    Returns
    -------
    c : rank-1 array('D') with bounds (n)

Now, the function can be called in a much more robust way::

    >>> add.zadd([1, 2, 3], [4, 5, 6])
    array([5.+0.j, 7.+0.j, 9.+0.j])

Notice the automatic conversion to the correct format that occurred.


Inserting directives in Fortran source
--------------------------------------

The nice interface can also be generated automatically by placing the
variable directives as special comments in the original Fortran code.
Thus, if the source code is modified to contain:

.. code-block:: fortran

    C
          SUBROUTINE ZADD(A,B,C,N)
    C
    CF2PY INTENT(OUT) :: C
    CF2PY INTENT(HIDE) :: N
    CF2PY DOUBLE COMPLEX :: A(N)
    CF2PY DOUBLE COMPLEX :: B(N)
    CF2PY DOUBLE COMPLEX :: C(N)
          DOUBLE COMPLEX A(*)
          DOUBLE COMPLEX B(*)
          DOUBLE COMPLEX C(*)
          INTEGER N
          DO 20 J = 1, N
             C(J) = A(J) + B(J)
     20   CONTINUE
          END

Then, one can compile the extension module using::

    f2py -c -m add add.f

The resulting signature for the function add.zadd is exactly the same
one that was created previously. If the original source code had
contained ``A(N)`` instead of ``A(*)`` and so forth with ``B`` and ``C``,
then nearly the same interface can be obtained by placing the
``INTENT(OUT) :: C`` comment line in the source code. The only difference
is that ``N`` would be an optional input that would default to the length
of ``A``.


A filtering example
-------------------

For comparison with the other methods to be discussed. Here is another
example of a function that filters a two-dimensional array of double
precision floating-point numbers using a fixed averaging filter. The
advantage of using Fortran to index into multi-dimensional arrays
should be clear from this example.

.. code-block::

          SUBROUTINE DFILTER2D(A,B,M,N)
    C
          DOUBLE PRECISION A(M,N)
          DOUBLE PRECISION B(M,N)
          INTEGER N, M
    CF2PY INTENT(OUT) :: B
    CF2PY INTENT(HIDE) :: N
    CF2PY INTENT(HIDE) :: M
          DO 20 I = 2,M-1
             DO 40 J=2,N-1
                B(I,J) = A(I,J) +
         $           (A(I-1,J)+A(I+1,J) +
         $            A(I,J-1)+A(I,J+1) )*0.5D0 +
         $           (A(I-1,J-1) + A(I-1,J+1) +
         $            A(I+1,J-1) + A(I+1,J+1))*0.25D0
     40      CONTINUE
     20   CONTINUE
          END

This code can be compiled and linked into an extension module named
filter using::

    f2py -c -m filter filter.f

This will produce an extension module named filter.so in the current
directory with a method named dfilter2d that returns a filtered
version of the input.


Calling f2py from Python
------------------------

The f2py program is written in Python and can be run from inside your code
to compile Fortran code at runtime, as follows:

.. code-block:: python

    from numpy import f2py
    with open("add.f") as sourcefile:
        sourcecode = sourcefile.read()
    f2py.compile(sourcecode, modulename='add')
    import add

The source string can be any valid Fortran code. If you want to save
the extension-module source code then a suitable file-name can be
provided by the ``source_fn`` keyword to the compile function.


Automatic extension module generation
-------------------------------------

If you want to distribute your f2py extension module, then you only
need to include the .pyf file and the Fortran code. The distutils
extensions in NumPy allow you to define an extension module entirely
in terms of this interface file. A valid ``setup.py`` file allowing
distribution of the ``add.f`` module (as part of the package
``f2py_examples`` so that it would be loaded as ``f2py_examples.add``) is:

.. code-block:: python

    def configuration(parent_package='', top_path=None)
        from numpy.distutils.misc_util import Configuration
        config = Configuration('f2py_examples',parent_package, top_path)
        config.add_extension('add', sources=['add.pyf','add.f'])
        return config

    if __name__ == '__main__':
        from numpy.distutils.core import setup
        setup(**configuration(top_path='').todict())

Installation of the new package is easy using::

    pip install .

assuming you have the proper permissions to write to the main site-
packages directory for the version of Python you are using. For the
resulting package to work, you need to create a file named ``__init__.py``
(in the same directory as ``add.pyf``). Notice the extension module is
defined entirely in terms of the ``add.pyf`` and ``add.f`` files. The
conversion of the .pyf file to a .c file is handled by `numpy.disutils`.


Conclusion
----------

The interface definition file (.pyf) is how you can fine-tune the interface
between Python and Fortran. There is decent documentation for f2py at
:ref:`f2py`. There is also more information on using f2py (including how to use
it to wrap C codes) at the `"Interfacing With Other Languages" heading of the
SciPy Cookbook.
<https://scipy-cookbook.readthedocs.io/items/idx_interfacing_with_other_languages.html>`_

The f2py method of linking compiled code is currently the most
sophisticated and integrated approach. It allows clean separation of
Python with compiled code while still allowing for separate
distribution of the extension module. The only draw-back is that it
requires the existence of a Fortran compiler in order for a user to
install the code. However, with the existence of the free-compilers
g77, gfortran, and g95, as well as high-quality commercial compilers,
this restriction is not particularly onerous. In our opinion, Fortran
is still the easiest way to write fast and clear code for scientific
computing. It handles complex numbers, and multi-dimensional indexing
in the most straightforward way. Be aware, however, that some Fortran
compilers will not be able to optimize code as well as good hand-
written C-code.

.. index::
   single: f2py


Cython
======

`Cython <http://cython.org>`_ is a compiler for a Python dialect that adds
(optional) static typing for speed, and allows mixing C or C++ code
into your modules. It produces C or C++ extensions that can be compiled
and imported in Python code.

If you are writing an extension module that will include quite a bit of your
own algorithmic code as well, then Cython is a good match. Among its
features is the ability to easily and quickly
work with multidimensional arrays.

.. index::
   single: cython

Notice that Cython is an extension-module generator only. Unlike f2py,
it includes no automatic facility for compiling and linking
the extension module (which must be done in the usual fashion). It
does provide a modified distutils class called ``build_ext`` which lets
you build an extension module from a ``.pyx`` source. Thus, you could
write in a ``setup.py`` file:

.. code-block:: python

    from Cython.Distutils import build_ext
    from distutils.extension import Extension
    from distutils.core import setup
    import numpy

    setup(name='mine', description='Nothing',
          ext_modules=[Extension('filter', ['filter.pyx'],
                                 include_dirs=[numpy.get_include()])],
          cmdclass = {'build_ext':build_ext})

Adding the NumPy include directory is, of course, only necessary if
you are using NumPy arrays in the extension module (which is what we
assume you are using Cython for). The distutils extensions in NumPy
also include support for automatically producing the extension-module
and linking it from a ``.pyx`` file. It works so that if the user does
not have Cython installed, then it looks for a file with the same
file-name but a ``.c`` extension which it then uses instead of trying
to produce the ``.c`` file again.

If you just use Cython to compile a standard Python module, then you
will get a C extension module that typically runs a bit faster than the
equivalent Python module. Further speed increases can be gained by using
the ``cdef`` keyword to statically define C variables.

Let's look at two examples we've seen before to see how they might be
implemented using Cython. These examples were compiled into extension
modules using Cython 0.21.1.


Complex addition in Cython
--------------------------

Here is part of a Cython module named ``add.pyx`` which implements the
complex addition functions we previously implemented using f2py:

.. code-block:: cython

    cimport cython
    cimport numpy as np
    import numpy as np

    # We need to initialize NumPy.
    np.import_array()

    #@cython.boundscheck(False)
    def zadd(in1, in2):
        cdef double complex[:] a = in1.ravel()
        cdef double complex[:] b = in2.ravel()

        out = np.empty(a.shape[0], np.complex64)
        cdef double complex[:] c = out.ravel()

        for i in range(c.shape[0]):
            c[i].real = a[i].real + b[i].real
            c[i].imag = a[i].imag + b[i].imag

        return out

This module shows use of the ``cimport`` statement to load the definitions
from the ``numpy.pxd`` header that ships with Cython. It looks like NumPy is
imported twice; ``cimport`` only makes the NumPy C-API available, while the
regular ``import`` causes a Python-style import at runtime and makes it
possible to call into the familiar NumPy Python API.

The example also demonstrates Cython's "typed memoryviews", which are like
NumPy arrays at the C level, in the sense that they are shaped and strided
arrays that know their own extent (unlike a C array addressed through a bare
pointer). The syntax ``double complex[:]`` denotes a one-dimensional array
(vector) of doubles, with arbitrary strides. A contiguous array of ints would
be ``int[::1]``, while a matrix of floats would be ``float[:, :]``.

Shown commented is the ``cython.boundscheck`` decorator, which turns
bounds-checking for memory view accesses on or off on a per-function basis.
We can use this to further speed up our code, at the expense of safety
(or a manual check prior to entering the loop).

Other than the view syntax, the function is immediately readable to a Python
programmer. Static typing of the variable ``i`` is implicit. Instead of the
view syntax, we could also have used Cython's special NumPy array syntax,
but the view syntax is preferred.


Image filter in Cython
----------------------

The two-dimensional example we created using Fortran is just as easy to write
in Cython:

.. code-block:: cython

    cimport numpy as np
    import numpy as np

    np.import_array()

    def filter(img):
        cdef double[:, :] a = np.asarray(img, dtype=np.double)
        out = np.zeros(img.shape, dtype=np.double)
        cdef double[:, ::1] b = out

        cdef np.npy_intp i, j

        for i in range(1, a.shape[0] - 1):
            for j in range(1, a.shape[1] - 1):
                b[i, j] = (a[i, j]
                           + .5 * (  a[i-1, j] + a[i+1, j]
                                   + a[i, j-1] + a[i, j+1])
                           + .25 * (  a[i-1, j-1] + a[i-1, j+1]
                                    + a[i+1, j-1] + a[i+1, j+1]))

        return out

This 2-d averaging filter runs quickly because the loop is in C and
the pointer computations are done only as needed. If the code above is
compiled as a module ``image``, then a 2-d image, ``img``, can be filtered
using this code very quickly using:

.. code-block:: python

    import image
    out = image.filter(img)

Regarding the code, two things are of note: firstly, it is impossible to
return a memory view to Python. Instead, a NumPy array ``out`` is first
created, and then a view ``b`` onto this array is used for the computation.
Secondly, the view ``b`` is typed ``double[:, ::1]``. This means 2-d array
with contiguous rows, i.e., C matrix order. Specifying the order explicitly
can speed up some algorithms since they can skip stride computations.


Conclusion
----------

Cython is the extension mechanism of choice for several scientific Python
libraries, including Scipy, Pandas, SAGE, scikit-image and scikit-learn,
as well as the XML processing library LXML.
The language and compiler are well-maintained.

There are several disadvantages of using Cython:

1. When coding custom algorithms, and sometimes when wrapping existing C
   libraries, some familiarity with C is required. In particular, when using
   C memory management (``malloc`` and friends), it's easy to introduce
   memory leaks. However, just compiling a Python module renamed to ``.pyx``
   can already speed it up, and adding a few type declarations can give
   dramatic speedups in some code.

2. It is easy to lose a clean separation between Python and C which makes
   re-using your C-code for other non-Python-related projects more
   difficult.

3. The C-code generated by Cython is hard to read and modify (and typically
   compiles with annoying but harmless warnings).

One big advantage of Cython-generated extension modules is that they are
easy to distribute. In summary, Cython is a very capable tool for either
gluing C code or generating an extension module quickly and should not be
over-looked. It is especially useful for people that can't or won't write
C or Fortran code.

.. index::
   single: cython


ctypes
======

`Ctypes <https://docs.python.org/3/library/ctypes.html>`_
is a Python extension module, included in the stdlib, that
allows you to call an arbitrary function in a shared library directly
from Python. This approach allows you to interface with C-code directly
from Python. This opens up an enormous number of libraries for use from
Python. The drawback, however, is that coding mistakes can lead to ugly
program crashes very easily (just as can happen in C) because there is
little type or bounds checking done on the parameters. This is especially
true when array data is passed in as a pointer to a raw memory
location. The responsibility is then on you that the subroutine will
not access memory outside the actual array area. But, if you don't
mind living a little dangerously ctypes can be an effective tool for
quickly taking advantage of a large shared library (or writing
extended functionality in your own shared library).

.. index::
   single: ctypes

Because the ctypes approach exposes a raw interface to the compiled
code it is not always tolerant of user mistakes. Robust use of the
ctypes module typically involves an additional layer of Python code in
order to check the data types and array bounds of objects passed to
the underlying subroutine. This additional layer of checking (not to
mention the conversion from ctypes objects to C-data-types that ctypes
itself performs), will make the interface slower than a hand-written
extension-module interface. However, this overhead should be negligible
if the C-routine being called is doing any significant amount of work.
If you are a great Python programmer with weak C skills, ctypes is an
easy way to write a useful interface to a (shared) library of compiled
code.

To use ctypes you must

1. Have a shared library.

2. Load the shared library.

3. Convert the Python objects to ctypes-understood arguments.

4. Call the function from the library with the ctypes arguments.


Having a shared library
-----------------------

There are several requirements for a shared library that can be used
with ctypes that are platform specific. This guide assumes you have
some familiarity with making a shared library on your system (or
simply have a shared library available to you). Items to remember are:

- A shared library must be compiled in a special way ( *e.g.* using
  the ``-shared`` flag with gcc).

- On some platforms (*e.g.* Windows), a shared library requires a
  .def file that specifies the functions to be exported. For example a
  mylib.def file might contain::

      LIBRARY mylib.dll
      EXPORTS
      cool_function1
      cool_function2

  Alternatively, you may be able to use the storage-class specifier
  ``__declspec(dllexport)`` in the C-definition of the function to avoid
  the need for this ``.def`` file.

There is no standard way in Python distutils to create a standard
shared library (an extension module is a "special" shared library
Python understands) in a cross-platform manner. Thus, a big
disadvantage of ctypes at the time of writing this book is that it is
difficult to distribute in a cross-platform manner a Python extension
that uses ctypes and includes your own code which should be compiled
as a shared library on the users system.


Loading the shared library
--------------------------

A simple, but robust way to load the shared library is to get the
absolute path name and load it using the cdll object of ctypes:

.. code-block:: python

    lib = ctypes.cdll[<full_path_name>]

However, on Windows accessing an attribute of the ``cdll`` method will
load the first DLL by that name found in the current directory or on
the PATH. Loading the absolute path name requires a little finesse for
cross-platform work since the extension of shared libraries varies.
There is a ``ctypes.util.find_library`` utility available that can
simplify the process of finding the library to load but it is not
foolproof. Complicating matters, different platforms have different
default extensions used by shared libraries (e.g. .dll -- Windows, .so
-- Linux, .dylib -- Mac OS X). This must also be taken into account if
you are using ctypes to wrap code that needs to work on several
platforms.

NumPy provides a convenience function called
``ctypeslib.load_library`` (name, path). This function takes the name
of the shared library (including any prefix like 'lib' but excluding
the extension) and a path where the shared library can be located. It
returns a ctypes library object or raises an ``OSError`` if the library
cannot be found or raises an ``ImportError`` if the ctypes module is not
available. (Windows users: the ctypes library object loaded using
``load_library`` is always loaded assuming cdecl calling convention.
See the ctypes documentation under ``ctypes.windll`` and/or ``ctypes.oledll``
for ways to load libraries under other calling conventions).

The functions in the shared library are available as attributes of the
ctypes library object (returned from ``ctypeslib.load_library``) or
as items using ``lib['func_name']`` syntax. The latter method for
retrieving a function name is particularly useful if the function name
contains characters that are not allowable in Python variable names.


Converting arguments
--------------------

Python ints/longs, strings, and unicode objects are automatically
converted as needed to equivalent ctypes arguments The None object is
also converted automatically to a NULL pointer. All other Python
objects must be converted to ctypes-specific types. There are two ways
around this restriction that allow ctypes to integrate with other
objects.

1. Don't set the argtypes attribute of the function object and define an
   ``_as_parameter_`` method for the object you want to pass in. The
   ``_as_parameter_`` method must return a Python int which will be passed
   directly to the function.

2. Set the argtypes attribute to a list whose entries contain objects
   with a classmethod named from_param that knows how to convert your
   object to an object that ctypes can understand (an int/long, string,
   unicode, or object with the ``_as_parameter_`` attribute).

NumPy uses both methods with a preference for the second method
because it can be safer. The ctypes attribute of the ndarray returns
an object that has an ``_as_parameter_`` attribute which returns an
integer representing the address of the ndarray to which it is
associated. As a result, one can pass this ctypes attribute object
directly to a function expecting a pointer to the data in your
ndarray. The caller must be sure that the ndarray object is of the
correct type, shape, and has the correct flags set or risk nasty
crashes if the data-pointer to inappropriate arrays are passed in.

To implement the second method, NumPy provides the class-factory
function :func:`ndpointer` in the :mod:`numpy.ctypeslib` module. This
class-factory function produces an appropriate class that can be
placed in an argtypes attribute entry of a ctypes function. The class
will contain a from_param method which ctypes will use to convert any
ndarray passed in to the function to a ctypes-recognized object. In
the process, the conversion will perform checking on any properties of
the ndarray that were specified by the user in the call to :func:`ndpointer`.
Aspects of the ndarray that can be checked include the data-type, the
number-of-dimensions, the shape, and/or the state of the flags on any
array passed. The return value of the from_param method is the ctypes
attribute of the array which (because it contains the ``_as_parameter_``
attribute pointing to the array data area) can be used by ctypes
directly.

The ctypes attribute of an ndarray is also endowed with additional
attributes that may be convenient when passing additional information
about the array into a ctypes function. The attributes **data**,
**shape**, and **strides** can provide ctypes compatible types
corresponding to the data-area, the shape, and the strides of the
array. The data attribute returns a ``c_void_p`` representing a
pointer to the data area. The shape and strides attributes each return
an array of ctypes integers (or None representing a NULL pointer, if a
0-d array). The base ctype of the array is a ctype integer of the same
size as a pointer on the platform. There are also methods
``data_as({ctype})``, ``shape_as(<base ctype>)``, and ``strides_as(<base
ctype>)``. These return the data as a ctype object of your choice and
the shape/strides arrays using an underlying base type of your choice.
For convenience, the ``ctypeslib`` module also contains ``c_intp`` as
a ctypes integer data-type whose size is the same as the size of
``c_void_p`` on the platform (its value is None if ctypes is not
installed).


Calling the function
--------------------

The function is accessed as an attribute of or an item from the loaded
shared-library. Thus, if ``./mylib.so`` has a function named
``cool_function1``, it may be accessed either as:

.. code-block:: python

    lib = numpy.ctypeslib.load_library('mylib','.')
    func1 = lib.cool_function1  # or equivalently
    func1 = lib['cool_function1']

In ctypes, the return-value of a function is set to be 'int' by
default. This behavior can be changed by setting the restype attribute
of the function. Use None for the restype if the function has no
return value ('void'):

.. code-block:: python

    func1.restype = None

As previously discussed, you can also set the argtypes attribute of
the function in order to have ctypes check the types of the input
arguments when the function is called. Use the :func:`ndpointer` factory
function to generate a ready-made class for data-type, shape, and
flags checking on your new function. The :func:`ndpointer` function has the
signature

.. function:: ndpointer(dtype=None, ndim=None, shape=None, flags=None)

    Keyword arguments with the value ``None`` are not checked.
    Specifying a keyword enforces checking of that aspect of the
    ndarray on conversion to a ctypes-compatible object. The dtype
    keyword can be any object understood as a data-type object. The
    ndim keyword should be an integer, and the shape keyword should be
    an integer or a sequence of integers. The flags keyword specifies
    the minimal flags that are required on any array passed in. This
    can be specified as a string of comma separated requirements, an
    integer indicating the requirement bits OR'd together, or a flags
    object returned from the flags attribute of an array with the
    necessary requirements.

Using an ndpointer class in the argtypes method can make it
significantly safer to call a C function using ctypes and the data-
area of an ndarray. You may still want to wrap the function in an
additional Python wrapper to make it user-friendly (hiding some
obvious arguments and making some arguments output arguments). In this
process, the ``requires`` function in NumPy may be useful to return the right
kind of array from a given input.


Complete example
----------------

In this example, we will demonstrate how the addition function and the filter
function implemented previously using the other approaches can be
implemented using ctypes. First, the C code which implements the
algorithms contains the functions ``zadd``, ``dadd``, ``sadd``, ``cadd``,
and ``dfilter2d``. The ``zadd`` function is:

.. code-block:: c

    /* Add arrays of contiguous data */
    typedef struct {double real; double imag;} cdouble;
    typedef struct {float real; float imag;} cfloat;
    void zadd(cdouble *a, cdouble *b, cdouble *c, long n)
    {
        while (n--) {
            c->real = a->real + b->real;
            c->imag = a->imag + b->imag;
            a++; b++; c++;
        }
    }

with similar code for ``cadd``, ``dadd``, and ``sadd`` that handles complex
float, double, and float data-types, respectively:

.. code-block:: c

    void cadd(cfloat *a, cfloat *b, cfloat *c, long n)
    {
            while (n--) {
                    c->real = a->real + b->real;
                    c->imag = a->imag + b->imag;
                    a++; b++; c++;
            }
    }
    void dadd(double *a, double *b, double *c, long n)
    {
            while (n--) {
                    *c++ = *a++ + *b++;
            }
    }
    void sadd(float *a, float *b, float *c, long n)
    {
            while (n--) {
                    *c++ = *a++ + *b++;
            }
    }

The ``code.c`` file also contains the function ``dfilter2d``:

.. code-block:: c

    /*
     * Assumes b is contiguous and has strides that are multiples of
     * sizeof(double)
     */
    void
    dfilter2d(double *a, double *b, ssize_t *astrides, ssize_t *dims)
    {
        ssize_t i, j, M, N, S0, S1;
        ssize_t r, c, rm1, rp1, cp1, cm1;

        M = dims[0]; N = dims[1];
        S0 = astrides[0]/sizeof(double);
        S1 = astrides[1]/sizeof(double);
        for (i = 1; i < M - 1; i++) {
            r = i*S0;
            rp1 = r + S0;
            rm1 = r - S0;
            for (j = 1; j < N - 1; j++) {
                c = j*S1;
                cp1 = j + S1;
                cm1 = j - S1;
                b[i*N + j] = a[r + c] +
                    (a[rp1 + c] + a[rm1 + c] +
                     a[r + cp1] + a[r + cm1])*0.5 +
                    (a[rp1 + cp1] + a[rp1 + cm1] +
                     a[rm1 + cp1] + a[rm1 + cp1])*0.25;
            }
        }
    }

A possible advantage this code has over the Fortran-equivalent code is
that it takes arbitrarily strided (i.e. non-contiguous arrays) and may
also run faster depending on the optimization capability of your
compiler. But, it is an obviously more complicated than the simple code
in ``filter.f``. This code must be compiled into a shared library. On my
Linux system this is accomplished using::

    gcc -o code.so -shared code.c

Which creates a shared_library named code.so in the current directory.
On Windows don't forget to either add ``__declspec(dllexport)`` in front
of void on the line preceding each function definition, or write a
``code.def`` file that lists the names of the functions to be exported.

A suitable Python interface to this shared library should be
constructed. To do this create a file named interface.py with the
following lines at the top:

.. code-block:: python

    __all__ = ['add', 'filter2d']

    import numpy as np
    import os

    _path = os.path.dirname('__file__')
    lib = np.ctypeslib.load_library('code', _path)
    _typedict = {'zadd' : complex, 'sadd' : np.single,
                 'cadd' : np.csingle, 'dadd' : float}
    for name in _typedict.keys():
        val = getattr(lib, name)
        val.restype = None
        _type = _typedict[name]
        val.argtypes = [np.ctypeslib.ndpointer(_type,
                          flags='aligned, contiguous'),
                        np.ctypeslib.ndpointer(_type,
                          flags='aligned, contiguous'),
                        np.ctypeslib.ndpointer(_type,
                          flags='aligned, contiguous,'\
                                'writeable'),
                        np.ctypeslib.c_intp]

This code loads the shared library named ``code.{ext}`` located in the
same path as this file. It then adds a return type of void to the
functions contained in the library. It also adds argument checking to
the functions in the library so that ndarrays can be passed as the
first three arguments along with an integer (large enough to hold a
pointer on the platform) as the fourth argument.

Setting up the filtering function is similar and allows the filtering
function to be called with ndarray arguments as the first two
arguments and with pointers to integers (large enough to handle the
strides and shape of an ndarray) as the last two arguments.:

.. code-block:: python

    lib.dfilter2d.restype=None
    lib.dfilter2d.argtypes = [np.ctypeslib.ndpointer(float, ndim=2,
                                           flags='aligned'),
                              np.ctypeslib.ndpointer(float, ndim=2,
                                     flags='aligned, contiguous,'\
                                           'writeable'),
                              ctypes.POINTER(np.ctypeslib.c_intp),
                              ctypes.POINTER(np.ctypeslib.c_intp)]

Next, define a simple selection function that chooses which addition
function to call in the shared library based on the data-type:

.. code-block:: python

    def select(dtype):
        if dtype.char in ['?bBhHf']:
            return lib.sadd, single
        elif dtype.char in ['F']:
            return lib.cadd, csingle
        elif dtype.char in ['DG']:
            return lib.zadd, complex
        else:
            return lib.dadd, float
        return func, ntype

Finally, the two functions to be exported by the interface can be
written simply as:

.. code-block:: python

    def add(a, b):
        requires = ['CONTIGUOUS', 'ALIGNED']
        a = np.asanyarray(a)
        func, dtype = select(a.dtype)
        a = np.require(a, dtype, requires)
        b = np.require(b, dtype, requires)
        c = np.empty_like(a)
        func(a,b,c,a.size)
        return c

and:

.. code-block:: python

    def filter2d(a):
        a = np.require(a, float, ['ALIGNED'])
        b = np.zeros_like(a)
        lib.dfilter2d(a, b, a.ctypes.strides, a.ctypes.shape)
        return b


Conclusion
----------

.. index::
   single: ctypes

Using ctypes is a powerful way to connect Python with arbitrary
C-code. Its advantages for extending Python include

- clean separation of C code from Python code

    - no need to learn a new syntax except Python and C

    - allows re-use of C code

    - functionality in shared libraries written for other purposes can be
      obtained with a simple Python wrapper and search for the library.


- easy integration with NumPy through the ctypes attribute

- full argument checking with the ndpointer class factory

Its disadvantages include

- It is difficult to distribute an extension module made using ctypes
  because of a lack of support for building shared libraries in
  distutils.

- You must have shared-libraries of your code (no static libraries).

- Very little support for C++ code and its different library-calling
  conventions. You will probably need a C wrapper around C++ code to use
  with ctypes (or just use Boost.Python instead).

Because of the difficulty in distributing an extension module made
using ctypes, f2py and Cython are still the easiest ways to extend Python
for package creation. However, ctypes is in some cases a useful alternative.
This should bring more features to ctypes that should
eliminate the difficulty in extending Python and distributing the
extension using ctypes.


Additional tools you may find useful
====================================

These tools have been found useful by others using Python and so are
included here. They are discussed separately because they are
either older ways to do things now handled by f2py, Cython, or ctypes
(SWIG, PyFort) or because of a lack of reasonable documentation (SIP, Boost).
Links to these methods are not included since the most relevant
can be found using Google or some other search engine, and any links provided
here would be quickly dated. Do not assume that inclusion in this list means
that the package deserves attention. Information about these packages are
collected here because many people have found them useful and we'd like to give
you as many options as possible for tackling the problem of easily integrating
your code.


SWIG
----

.. index::
   single: swig

Simplified Wrapper and Interface Generator (SWIG) is an old and fairly
stable method for wrapping C/C++-libraries to a large variety of other
languages. It does not specifically understand NumPy arrays but can be
made usable with NumPy through the use of typemaps. There are some
sample typemaps in the numpy/tools/swig directory under numpy.i together
with an example module that makes use of them. SWIG excels at wrapping
large C/C++ libraries because it can (almost) parse their headers and
auto-produce an interface. Technically, you need to generate a ``.i``
file that defines the interface. Often, however, this ``.i`` file can
be parts of the header itself. The interface usually needs a bit of
tweaking to be very useful. This ability to parse C/C++ headers and
auto-generate the interface still makes SWIG a useful approach to
adding functionalilty from C/C++ into Python, despite the other
methods that have emerged that are more targeted to Python. SWIG can
actually target extensions for several languages, but the typemaps
usually have to be language-specific. Nonetheless, with modifications
to the Python-specific typemaps, SWIG can be used to interface a
library with other languages such as Perl, Tcl, and Ruby.

My experience with SWIG has been generally positive in that it is
relatively easy to use and quite powerful. It has been used
often before becoming more proficient at writing C-extensions.
However, writing custom interfaces with SWIG is often troublesome because it
must be done using the concept of typemaps which are not Python
specific and are written in a C-like syntax. Therefore, other gluing strategies
are preferred and SWIG would be probably considered only to
wrap a very-large C/C++ library. Nonetheless, there are others who use
SWIG quite happily.


SIP
---

.. index::
   single: SIP

SIP is another tool for wrapping C/C++ libraries that is Python
specific and appears to have very good support for C++. Riverbank
Computing developed SIP in order to create Python bindings to the QT
library. An interface file must be written to generate the binding,
but the interface file looks a lot like a C/C++ header file. While SIP
is not a full C++ parser, it understands quite a bit of C++ syntax as
well as its own special directives that allow modification of how the
Python binding is accomplished. It also allows the user to define
mappings between Python types and C/C++ structures and classes.


Boost Python
------------

.. index::
   single: Boost.Python

Boost is a repository of C++ libraries and Boost.Python is one of
those libraries which provides a concise interface for binding C++
classes and functions to Python. The amazing part of the Boost.Python
approach is that it works entirely in pure C++ without introducing a
new syntax. Many users of C++ report that Boost.Python makes it
possible to combine the best of both worlds in a seamless fashion. Using Boost
to wrap simple C-subroutines is usually over-kill. Its primary purpose is to
make C++ classes available in Python. So, if you have a set of C++ classes that
need to be integrated cleanly into Python, consider learning about and using
Boost.Python.


PyFort
------

PyFort is a nice tool for wrapping Fortran and Fortran-like C-code
into Python with support for Numeric arrays. It was written by Paul
Dubois, a distinguished computer scientist and the very first
maintainer of Numeric (now retired). It is worth mentioning in the
hopes that somebody will update PyFort to work with NumPy arrays as
well which now support either Fortran or C-style contiguous arrays.
*******************
How to extend NumPy
*******************

|    That which is static and repetitive is boring. That which is dynamic
|    and random is confusing. In between lies art.
|    --- *John A. Locke*

|    Science is a differential equation. Religion is a boundary condition.
|    --- *Alan Turing*


.. _writing-an-extension:

Writing an extension module
===========================

While the ndarray object is designed to allow rapid computation in
Python, it is also designed to be general-purpose and satisfy a wide-
variety of computational needs. As a result, if absolute speed is
essential, there is no replacement for a well-crafted, compiled loop
specific to your application and hardware. This is one of the reasons
that numpy includes f2py so that an easy-to-use mechanisms for linking
(simple) C/C++ and (arbitrary) Fortran code directly into Python are
available. You are encouraged to use and improve this mechanism. The
purpose of this section is not to document this tool but to document
the more basic steps to writing an extension module that this tool
depends on.

.. index::
   single: extension module

When an extension module is written, compiled, and installed to
somewhere in the Python path (sys.path), the code can then be imported
into Python as if it were a standard python file. It will contain
objects and methods that have been defined and compiled in C code. The
basic steps for doing this in Python are well-documented and you can
find more information in the documentation for Python itself available
online at `www.python.org <https://www.python.org>`_ .

In addition to the Python C-API, there is a full and rich C-API for NumPy
allowing sophisticated manipulations on a C-level. However, for most
applications, only a few API calls will typically be used. For example, if you
need to just extract a pointer to memory along with some shape information to
pass to another calculation routine, then you will use very different calls
than if you are trying to create a new array-like type or add a new data type
for ndarrays. This chapter documents the API calls and macros that are most
commonly used.


Required subroutine
===================

There is exactly one function that must be defined in your C-code in
order for Python to use it as an extension module. The function must
be called init{name} where {name} is the name of the module from
Python. This function must be declared so that it is visible to code
outside of the routine. Besides adding the methods and constants you
desire, this subroutine must also contain calls like ``import_array()``
and/or ``import_ufunc()`` depending on which C-API is needed. Forgetting
to place these commands will show itself as an ugly segmentation fault
(crash) as soon as any C-API subroutine is actually called. It is
actually possible to have multiple init{name} functions in a single
file in which case multiple modules will be defined by that file.
However, there are some tricks to get that to work correctly and it is
not covered here.

A minimal ``init{name}`` method looks like:

.. code-block:: c

    PyMODINIT_FUNC
    init{name}(void)
    {
       (void)Py_InitModule({name}, mymethods);
       import_array();
    }

The mymethods must be an array (usually statically declared) of
PyMethodDef structures which contain method names, actual C-functions,
a variable indicating whether the method uses keyword arguments or
not, and docstrings. These are explained in the next section. If you
want to add constants to the module, then you store the returned value
from Py_InitModule which is a module object. The most general way to
add items to the module is to get the module dictionary using
PyModule_GetDict(module). With the module dictionary, you can add
whatever you like to the module manually. An easier way to add objects
to the module is to use one of three additional Python C-API calls
that do not require a separate extraction of the module dictionary.
These are documented in the Python documentation, but repeated here
for convenience:

.. c:function:: int PyModule_AddObject( \
        PyObject* module, char* name, PyObject* value)

.. c:function:: int PyModule_AddIntConstant( \
        PyObject* module, char* name, long value)

.. c:function:: int PyModule_AddStringConstant( \
        PyObject* module, char* name, char* value)

    All three of these functions require the *module* object (the
    return value of Py_InitModule). The *name* is a string that
    labels the value in the module. Depending on which function is
    called, the *value* argument is either a general object
    (:c:func:`PyModule_AddObject` steals a reference to it), an integer
    constant, or a string constant.


Defining functions
==================

The second argument passed in to the Py_InitModule function is a
structure that makes it easy to define functions in the module. In
the example given above, the mymethods structure would have been
defined earlier in the file (usually right before the init{name}
subroutine) to:

.. code-block:: c

    static PyMethodDef mymethods[] = {
        { nokeywordfunc,nokeyword_cfunc,
          METH_VARARGS,
          Doc string},
        { keywordfunc, keyword_cfunc,
          METH_VARARGS|METH_KEYWORDS,
          Doc string},
        {NULL, NULL, 0, NULL} /* Sentinel */
    }

Each entry in the mymethods array is a :c:type:`PyMethodDef` structure
containing 1) the Python name, 2) the C-function that implements the
function, 3) flags indicating whether or not keywords are accepted for
this function, and 4) The docstring for the function. Any number of
functions may be defined for a single module by adding more entries to
this table. The last entry must be all NULL as shown to act as a
sentinel. Python looks for this entry to know that all of the
functions for the module have been defined.

The last thing that must be done to finish the extension module is to
actually write the code that performs the desired functions. There are
two kinds of functions: those that don't accept keyword arguments, and
those that do.


Functions without keyword arguments
-----------------------------------

Functions that don't accept keyword arguments should be written as:

.. code-block:: c

    static PyObject*
    nokeyword_cfunc (PyObject *dummy, PyObject *args)
    {
        /* convert Python arguments */
        /* do function */
        /* return something */
    }

The dummy argument is not used in this context and can be safely
ignored. The *args* argument contains all of the arguments passed in
to the function as a tuple. You can do anything you want at this
point, but usually the easiest way to manage the input arguments is to
call :c:func:`PyArg_ParseTuple` (args, format_string,
addresses_to_C_variables...) or :c:func:`PyArg_UnpackTuple` (tuple, "name",
min, max, ...). A good description of how to use the first function is
contained in the Python C-API reference manual under section 5.5
(Parsing arguments and building values). You should pay particular
attention to the "O&" format which uses converter functions to go
between the Python object and the C object. All of the other format
functions can be (mostly) thought of as special cases of this general
rule. There are several converter functions defined in the NumPy C-API
that may be of use. In particular, the :c:func:`PyArray_DescrConverter`
function is very useful to support arbitrary data-type specification.
This function transforms any valid data-type Python object into a
:c:expr:`PyArray_Descr *` object. Remember to pass in the address of the
C-variables that should be filled in.

There are lots of examples of how to use :c:func:`PyArg_ParseTuple`
throughout the NumPy source code. The standard usage is like this:

.. code-block:: c

    PyObject *input;
    PyArray_Descr *dtype;
    if (!PyArg_ParseTuple(args, "OO&", &input,
                          PyArray_DescrConverter,
                          &dtype)) return NULL;

It is important to keep in mind that you get a *borrowed* reference to
the object when using the "O" format string. However, the converter
functions usually require some form of memory handling. In this
example, if the conversion is successful, *dtype* will hold a new
reference to a :c:expr:`PyArray_Descr *` object, while *input* will hold a
borrowed reference. Therefore, if this conversion were mixed with
another conversion (say to an integer) and the data-type conversion
was successful but the integer conversion failed, then you would need
to release the reference count to the data-type object before
returning. A typical way to do this is to set *dtype* to ``NULL``
before calling :c:func:`PyArg_ParseTuple` and then use :c:func:`Py_XDECREF`
on *dtype* before returning.

After the input arguments are processed, the code that actually does
the work is written (likely calling other functions as needed). The
final step of the C-function is to return something. If an error is
encountered then ``NULL`` should be returned (making sure an error has
actually been set). If nothing should be returned then increment
:c:data:`Py_None` and return it. If a single object should be returned then
it is returned (ensuring that you own a reference to it first). If
multiple objects should be returned then you need to return a tuple.
The :c:func:`Py_BuildValue` (format_string, c_variables...) function makes
it easy to build tuples of Python objects from C variables. Pay
special attention to the difference between 'N' and 'O' in the format
string or you can easily create memory leaks. The 'O' format string
increments the reference count of the :c:expr:`PyObject *` C-variable it
corresponds to, while the 'N' format string steals a reference to the
corresponding :c:expr:`PyObject *` C-variable. You should use 'N' if you have
already created a reference for the object and just want to give that
reference to the tuple. You should use 'O' if you only have a borrowed
reference to an object and need to create one to provide for the
tuple.


Functions with keyword arguments
--------------------------------

These functions are very similar to functions without keyword
arguments. The only difference is that the function signature is:

.. code-block:: c

    static PyObject*
    keyword_cfunc (PyObject *dummy, PyObject *args, PyObject *kwds)
    {
    ...
    }

The kwds argument holds a Python dictionary whose keys are the names
of the keyword arguments and whose values are the corresponding
keyword-argument values. This dictionary can be processed however you
see fit. The easiest way to handle it, however, is to replace the
:c:func:`PyArg_ParseTuple` (args, format_string, addresses...) function with
a call to :c:func:`PyArg_ParseTupleAndKeywords` (args, kwds, format_string,
char \*kwlist[], addresses...). The kwlist parameter to this function
is a ``NULL`` -terminated array of strings providing the expected
keyword arguments.  There should be one string for each entry in the
format_string. Using this function will raise a TypeError if invalid
keyword arguments are passed in.

For more help on this function please see section 1.8 (Keyword
Parameters for Extension Functions) of the Extending and Embedding
tutorial in the Python documentation.


Reference counting
------------------

The biggest difficulty when writing extension modules is reference
counting. It is an important reason for the popularity of f2py, weave,
Cython, ctypes, etc.... If you mis-handle reference counts you can get
problems from memory-leaks to segmentation faults. The only strategy I
know of to handle reference counts correctly is blood, sweat, and
tears. First, you force it into your head that every Python variable
has a reference count. Then, you understand exactly what each function
does to the reference count of your objects, so that you can properly
use DECREF and INCREF when you need them. Reference counting can
really test the amount of patience and diligence you have towards your
programming craft. Despite the grim depiction, most cases of reference
counting are quite straightforward with the most common difficulty
being not using DECREF on objects before exiting early from a routine
due to some error. In second place, is the common error of not owning
the reference on an object that is passed to a function or macro that
is going to steal the reference ( *e.g.* :c:func:`PyTuple_SET_ITEM`, and
most functions that take :c:type:`PyArray_Descr` objects).

.. index::
   single: reference counting

Typically you get a new reference to a variable when it is created or
is the return value of some function (there are some prominent
exceptions, however --- such as getting an item out of a tuple or a
dictionary). When you own the reference, you are responsible to make
sure that :c:func:`Py_DECREF` (var) is called when the variable is no
longer necessary (and no other function has "stolen" its
reference). Also, if you are passing a Python object to a function
that will "steal" the reference, then you need to make sure you own it
(or use :c:func:`Py_INCREF` to get your own reference). You will also
encounter the notion of borrowing a reference. A function that borrows
a reference does not alter the reference count of the object and does
not expect to "hold on "to the reference. It's just going to use the
object temporarily.  When you use :c:func:`PyArg_ParseTuple` or
:c:func:`PyArg_UnpackTuple` you receive a borrowed reference to the
objects in the tuple and should not alter their reference count inside
your function. With practice, you can learn to get reference counting
right, but it can be frustrating at first.

One common source of reference-count errors is the :c:func:`Py_BuildValue`
function. Pay careful attention to the difference between the 'N'
format character and the 'O' format character. If you create a new
object in your subroutine (such as an output array), and you are
passing it back in a tuple of return values, then you should most-
likely use the 'N' format character in :c:func:`Py_BuildValue`. The 'O'
character will increase the reference count by one. This will leave
the caller with two reference counts for a brand-new array.  When the
variable is deleted and the reference count decremented by one, there
will still be that extra reference count, and the array will never be
deallocated. You will have a reference-counting induced memory leak.
Using the 'N' character will avoid this situation as it will return to
the caller an object (inside the tuple) with a single reference count.

.. index::
   single: reference counting




Dealing with array objects
==========================

Most extension modules for NumPy will need to access the memory for an
ndarray object (or one of it's sub-classes). The easiest way to do
this doesn't require you to know much about the internals of NumPy.
The method is to

1. Ensure you are dealing with a well-behaved array (aligned, in machine
   byte-order and single-segment) of the correct type and number of
   dimensions.

    1. By converting it from some Python object using
       :c:func:`PyArray_FromAny` or a macro built on it.

    2. By constructing a new ndarray of your desired shape and type
       using :c:func:`PyArray_NewFromDescr` or a simpler macro or function
       based on it.


2. Get the shape of the array and a pointer to its actual data.

3. Pass the data and shape information on to a subroutine or other
   section of code that actually performs the computation.

4. If you are writing the algorithm, then I recommend that you use the
   stride information contained in the array to access the elements of
   the array (the :c:func:`PyArray_GetPtr` macros make this painless). Then,
   you can relax your requirements so as not to force a single-segment
   array and the data-copying that might result.

Each of these sub-topics is covered in the following sub-sections.


Converting an arbitrary sequence object
---------------------------------------

The main routine for obtaining an array from any Python object that
can be converted to an array is :c:func:`PyArray_FromAny`. This
function is very flexible with many input arguments. Several macros
make it easier to use the basic function. :c:func:`PyArray_FROM_OTF` is
arguably the most useful of these macros for the most common uses.  It
allows you to convert an arbitrary Python object to an array of a
specific builtin data-type ( *e.g.* float), while specifying a
particular set of requirements ( *e.g.* contiguous, aligned, and
writeable). The syntax is

:c:func:`PyArray_FROM_OTF`
    Return an ndarray from any Python object, *obj*, that can be
    converted to an array. The number of dimensions in the returned
    array is determined by the object. The desired data-type of the
    returned array is provided in *typenum* which should be one of the
    enumerated types. The *requirements* for the returned array can be
    any combination of standard array flags.  Each of these arguments
    is explained in more detail below. You receive a new reference to
    the array on success. On failure, ``NULL`` is returned and an
    exception is set.

    *obj*
        The object can be any Python object convertible to an ndarray.
        If the object is already (a subclass of) the ndarray that
        satisfies the requirements then a new reference is returned.
        Otherwise, a new array is constructed. The contents of *obj*
        are copied to the new array unless the array interface is used
        so that data does not have to be copied. Objects that can be
        converted to an array include: 1) any nested sequence object,
        2) any object exposing the array interface, 3) any object with
        an :obj:`~numpy.class.__array__` method (which should return an ndarray),
        and 4) any scalar object (becomes a zero-dimensional
        array). Sub-classes of the ndarray that otherwise fit the
        requirements will be passed through. If you want to ensure
        a base-class ndarray, then use :c:data:`NPY_ARRAY_ENSUREARRAY` in the
        requirements flag. A copy is made only if necessary. If you
        want to guarantee a copy, then pass in :c:data:`NPY_ARRAY_ENSURECOPY`
        to the requirements flag.

    *typenum*
        One of the enumerated types or :c:data:`NPY_NOTYPE` if the data-type
        should be determined from the object itself. The C-based names
        can be used:

            :c:data:`NPY_BOOL`, :c:data:`NPY_BYTE`, :c:data:`NPY_UBYTE`,
            :c:data:`NPY_SHORT`, :c:data:`NPY_USHORT`, :c:data:`NPY_INT`,
            :c:data:`NPY_UINT`, :c:data:`NPY_LONG`, :c:data:`NPY_ULONG`,
            :c:data:`NPY_LONGLONG`, :c:data:`NPY_ULONGLONG`, :c:data:`NPY_DOUBLE`,
            :c:data:`NPY_LONGDOUBLE`, :c:data:`NPY_CFLOAT`, :c:data:`NPY_CDOUBLE`,
            :c:data:`NPY_CLONGDOUBLE`, :c:data:`NPY_OBJECT`.

        Alternatively, the bit-width names can be used as supported on the
        platform. For example:

            :c:data:`NPY_INT8`, :c:data:`NPY_INT16`, :c:data:`NPY_INT32`,
            :c:data:`NPY_INT64`, :c:data:`NPY_UINT8`,
            :c:data:`NPY_UINT16`, :c:data:`NPY_UINT32`,
            :c:data:`NPY_UINT64`, :c:data:`NPY_FLOAT32`,
            :c:data:`NPY_FLOAT64`, :c:data:`NPY_COMPLEX64`,
            :c:data:`NPY_COMPLEX128`.

        The object will be converted to the desired type only if it
        can be done without losing precision. Otherwise ``NULL`` will
        be returned and an error raised. Use :c:data:`NPY_ARRAY_FORCECAST` in the
        requirements flag to override this behavior.

    *requirements*
        The memory model for an ndarray admits arbitrary strides in
        each dimension to advance to the next element of the array.
        Often, however, you need to interface with code that expects a
        C-contiguous or a Fortran-contiguous memory layout. In
        addition, an ndarray can be misaligned (the address of an
        element is not at an integral multiple of the size of the
        element) which can cause your program to crash (or at least
        work more slowly) if you try and dereference a pointer into
        the array data. Both of these problems can be solved by
        converting the Python object into an array that is more
        "well-behaved" for your specific usage.

        The requirements flag allows specification of what kind of
        array is acceptable. If the object passed in does not satisfy
        this requirements then a copy is made so that the returned
        object will satisfy the requirements. these ndarray can use a
        very generic pointer to memory.  This flag allows specification
        of the desired properties of the returned array object. All
        of the flags are explained in the detailed API chapter. The
        flags most commonly needed are :c:data:`NPY_ARRAY_IN_ARRAY`,
        :c:data:`NPY_OUT_ARRAY`, and :c:data:`NPY_ARRAY_INOUT_ARRAY`:

        :c:data:`NPY_ARRAY_IN_ARRAY`
            This flag is useful for arrays that must be in C-contiguous
            order and aligned. These kinds of arrays are usually input 
            arrays for some algorithm.

        :c:data:`NPY_ARRAY_OUT_ARRAY`
            This flag is useful to specify an array that is
            in C-contiguous order, is aligned, and can be written to
            as well. Such an array is usually returned as output
            (although normally such output arrays are created from
            scratch).

        :c:data:`NPY_ARRAY_INOUT_ARRAY`
            This flag is useful to specify an array that will be used for both
            input and output. :c:func:`PyArray_ResolveWritebackIfCopy`
            must be called before :c:func:`Py_DECREF` at
            the end of the interface routine to write back the temporary data
            into the original array passed in. Use
            of the :c:data:`NPY_ARRAY_WRITEBACKIFCOPY` flag requires that the
            input object is already an array (because other objects cannot
            be automatically updated in this fashion). If an error
            occurs use :c:func:`PyArray_DiscardWritebackIfCopy` (obj) on an
            array with these flags set. This will set the underlying base array
            writable without causing the contents to be copied
            back into the original array.


        Other useful flags that can be OR'd as additional requirements are:

        :c:data:`NPY_ARRAY_FORCECAST`
            Cast to the desired type, even if it can't be done without losing
            information.

        :c:data:`NPY_ARRAY_ENSURECOPY`
            Make sure the resulting array is a copy of the original.

        :c:data:`NPY_ARRAY_ENSUREARRAY`
            Make sure the resulting object is an actual ndarray and not a sub-
            class.

.. note::

    Whether or not an array is byte-swapped is determined by the
    data-type of the array. Native byte-order arrays are always
    requested by :c:func:`PyArray_FROM_OTF` and so there is no need for
    a :c:data:`NPY_ARRAY_NOTSWAPPED` flag in the requirements argument. There
    is also no way to get a byte-swapped array from this routine.


Creating a brand-new ndarray
----------------------------

Quite often, new arrays must be created from within extension-module
code. Perhaps an output array is needed and you don't want the caller
to have to supply it. Perhaps only a temporary array is needed to hold
an intermediate calculation. Whatever the need there are simple ways
to get an ndarray object of whatever data-type is needed. The most
general function for doing this is :c:func:`PyArray_NewFromDescr`. All array
creation functions go through this heavily re-used code. Because of
its flexibility, it can be somewhat confusing to use. As a result,
simpler forms exist that are easier to use. These forms are part of the
:c:func:`PyArray_SimpleNew` family of functions, which simplify the interface
by providing default values for common use cases.


Getting at ndarray memory and accessing elements of the ndarray
---------------------------------------------------------------

If obj is an ndarray (:c:expr:`PyArrayObject *`), then the data-area of the
ndarray is pointed to by the void* pointer :c:func:`PyArray_DATA` (obj) or
the char* pointer :c:func:`PyArray_BYTES` (obj). Remember that (in general)
this data-area may not be aligned according to the data-type, it may
represent byte-swapped data, and/or it may not be writeable. If the
data area is aligned and in native byte-order, then how to get at a
specific element of the array is determined only by the array of
npy_intp variables, :c:func:`PyArray_STRIDES` (obj). In particular, this
c-array of integers shows how many **bytes** must be added to the
current element pointer to get to the next element in each dimension.
For arrays less than 4-dimensions there are ``PyArray_GETPTR{k}``
(obj, ...) macros where {k} is the integer 1, 2, 3, or 4 that make
using the array strides easier. The arguments .... represent {k} non-
negative integer indices into the array. For example, suppose ``E`` is
a 3-dimensional ndarray. A (void*) pointer to the element ``E[i,j,k]``
is obtained as :c:func:`PyArray_GETPTR3` (E, i, j, k).

As explained previously, C-style contiguous arrays and Fortran-style
contiguous arrays have particular striding patterns. Two array flags
(:c:data:`NPY_ARRAY_C_CONTIGUOUS` and :c:data:`NPY_ARRAY_F_CONTIGUOUS`) indicate
whether or not the striding pattern of a particular array matches the
C-style contiguous or Fortran-style contiguous or neither. Whether or
not the striding pattern matches a standard C or Fortran one can be
tested Using :c:func:`PyArray_IS_C_CONTIGUOUS` (obj) and
:c:func:`PyArray_ISFORTRAN` (obj) respectively. Most third-party
libraries expect contiguous arrays.  But, often it is not difficult to
support general-purpose striding. I encourage you to use the striding
information in your own code whenever possible, and reserve
single-segment requirements for wrapping third-party code. Using the
striding information provided with the ndarray rather than requiring a
contiguous striding reduces copying that otherwise must be made.


Example
=======

.. index::
   single: extension module

The following example shows how you might write a wrapper that accepts
two input arguments (that will be converted to an array) and an output
argument (that must be an array). The function returns None and
updates the output array. Note the updated use of WRITEBACKIFCOPY semantics
for NumPy v1.14 and above

.. code-block:: c

    static PyObject *
    example_wrapper(PyObject *dummy, PyObject *args)
    {
        PyObject *arg1=NULL, *arg2=NULL, *out=NULL;
        PyObject *arr1=NULL, *arr2=NULL, *oarr=NULL;

        if (!PyArg_ParseTuple(args, "OOO!", &arg1, &arg2,
            &PyArray_Type, &out)) return NULL;

        arr1 = PyArray_FROM_OTF(arg1, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);
        if (arr1 == NULL) return NULL;
        arr2 = PyArray_FROM_OTF(arg2, NPY_DOUBLE, NPY_ARRAY_IN_ARRAY);
        if (arr2 == NULL) goto fail;
    #if NPY_API_VERSION >= 0x0000000c
        oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY2);
    #else
        oarr = PyArray_FROM_OTF(out, NPY_DOUBLE, NPY_ARRAY_INOUT_ARRAY);
    #endif
        if (oarr == NULL) goto fail;

        /* code that makes use of arguments */
        /* You will probably need at least
           nd = PyArray_NDIM(<..>)    -- number of dimensions
           dims = PyArray_DIMS(<..>)  -- npy_intp array of length nd
                                         showing length in each dim.
           dptr = (double *)PyArray_DATA(<..>) -- pointer to data.

           If an error occurs goto fail.
         */

        Py_DECREF(arr1);
        Py_DECREF(arr2);
    #if NPY_API_VERSION >= 0x0000000c
        PyArray_ResolveWritebackIfCopy(oarr);
    #endif
        Py_DECREF(oarr);
        Py_INCREF(Py_None);
        return Py_None;

     fail:
        Py_XDECREF(arr1);
        Py_XDECREF(arr2);
    #if NPY_API_VERSION >= 0x0000000c
        PyArray_DiscardWritebackIfCopy(oarr);
    #endif
        Py_XDECREF(oarr);
        return NULL;
    }
.. _numpy-for-matlab-users:

======================
NumPy for MATLAB users
======================

Introduction
============

MATLAB® and NumPy have a lot in common, but NumPy was created to work with
Python, not to be a MATLAB clone.  This guide will help MATLAB users get started
with NumPy. 

.. raw:: html

   <style>
   table.docutils td { border: solid 1px #ccc; }
   </style>

Some key differences
====================

.. list-table::
   :class: docutils

   * - In MATLAB, the basic type, even for scalars, is a
       multidimensional array. Array assignments in MATLAB are stored as
       2D arrays of double precision floating point numbers, unless you
       specify the number of dimensions and type.  Operations on the 2D
       instances of these arrays are modeled on matrix operations in
       linear algebra. 

     - In NumPy, the basic type is a multidimensional ``array``.  Array
       assignments in NumPy are usually stored as :ref:`n-dimensional arrays<arrays>` with the
       minimum type required to hold the objects in sequence, unless you
       specify the number of dimensions and type. NumPy performs
       operations element-by-element, so multiplying 2D arrays with
       ``*`` is not a matrix multiplication -- it's an
       element-by-element multiplication. (The ``@`` operator, available
       since Python 3.5, can be used for conventional matrix
       multiplication.)

   * - MATLAB numbers indices from 1; ``a(1)`` is the first element.
       :ref:`See note INDEXING <numpy-for-matlab-users.notes>`
     - NumPy, like Python, numbers indices from 0; ``a[0]`` is the first
       element.

   * - MATLAB's scripting language was created for linear algebra so the
       syntax for some array manipulations is more compact than
       NumPy's. On the other hand, the API for adding GUIs and creating 
       full-fledged applications is more or less an afterthought.
     - NumPy is  based on Python, a
       general-purpose language.  The advantage to NumPy
       is access to Python libraries including: `SciPy
       <https://www.scipy.org/>`_, `Matplotlib <https://matplotlib.org/>`_,
       `Pandas <https://pandas.pydata.org/>`_, `OpenCV <https://opencv.org/>`_,
       and more. In addition, Python is often `embedded as a scripting language
       <https://en.wikipedia.org/wiki/List_of_Python_software#Embedded_as_a_scripting_language>`_
       in other software, allowing NumPy to be used there too. 

   * - MATLAB array slicing uses pass-by-value semantics, with a lazy
       copy-on-write scheme to prevent creating copies until they are
       needed. Slicing operations copy parts of the array.
     - NumPy array slicing uses pass-by-reference, that does not copy
       the arguments. Slicing operations are views into an array.
   

Rough equivalents
=======================================

The table below gives rough equivalents for some common MATLAB
expressions. These are similar expressions, not equivalents. For
details, see the :ref:`documentation<reference>`.

In the table below, it is assumed that you have executed the following
commands in Python:

::

    import numpy as np
    from scipy import io, integrate, linalg, signal
    from scipy.sparse.linalg import eigs

Also assume below that if the Notes talk about "matrix" that the
arguments are two-dimensional entities.

General purpose equivalents
---------------------------

.. list-table::
   :header-rows: 1

   * - MATLAB
     - NumPy
     - Notes

   * - ``help func``
     - ``info(func)`` or ``help(func)`` or ``func?`` (in IPython)
     - get help on the function *func*

   * - ``which func``
     - :ref:`see note HELP <numpy-for-matlab-users.notes>`
     - find out where *func* is defined

   * - ``type func``
     - ``np.source(func)`` or ``func??`` (in IPython)
     - print source for *func* (if not a native function)

   * - ``% comment``
     - ``# comment``
     - comment a line of code with the text ``comment``

   * - ::

         for i=1:3
             fprintf('%i\n',i)
         end

     - ::

         for i in range(1, 4):
            print(i)

     - use a for-loop to print the numbers 1, 2, and 3 using :py:class:`range <range>`

   * - ``a && b``
     - ``a and b``
     - short-circuiting logical AND operator (:ref:`Python native operator <python:boolean>`);
       scalar arguments only

   * - ``a || b``
     - ``a or b``
     - short-circuiting logical OR operator (:ref:`Python native operator <python:boolean>`);
       scalar arguments only

   * - .. code:: matlab
        
        >> 4 == 4
        ans = 1
        >> 4 == 5
        ans = 0

     - ::

        >>> 4 == 4
        True
        >>> 4 == 5
        False

     - The :ref:`boolean objects <python:bltin-boolean-values>`
       in Python are ``True`` and ``False``, as opposed to MATLAB
       logical types of ``1`` and ``0``. 

   * - .. code:: matlab

         a=4
         if a==4
             fprintf('a = 4\n')
         elseif a==5
             fprintf('a = 5\n')
         end

     - ::

         a = 4
         if a == 4:
             print('a = 4')
         elif a == 5: 
             print('a = 5')

     - create an if-else statement to check if ``a`` is 4 or 5 and print result

   * - ``1*i``, ``1*j``,  ``1i``, ``1j``
     - ``1j``
     - complex numbers

   * - ``eps``
     - ``np.finfo(float).eps`` or ``np.spacing(1)``
     - Upper bound to relative error due to rounding in 64-bit floating point
       arithmetic.

   * - ``load data.mat``
     - ``io.loadmat('data.mat')``
     - Load MATLAB variables saved to the file ``data.mat``. (Note: When saving arrays to
       ``data.mat`` in MATLAB/Octave, use a recent binary format. :func:`scipy.io.loadmat`
       will create a dictionary with the saved arrays and further information.)

   * - ``ode45``
     - ``integrate.solve_ivp(f)``
     - integrate an ODE with Runge-Kutta 4,5

   * - ``ode15s``
     - ``integrate.solve_ivp(f, method='BDF')``
     - integrate an ODE with BDF method


Linear algebra equivalents
--------------------------

.. list-table::
   :header-rows: 1

   * - MATLAB
     - NumPy
     - Notes

   * - ``ndims(a)``
     - ``np.ndim(a)`` or ``a.ndim``
     - number of dimensions of array ``a``

   * - ``numel(a)``
     - ``np.size(a)`` or ``a.size``
     - number of elements of array ``a``

   * - ``size(a)``
     - ``np.shape(a)`` or ``a.shape``
     - "size" of array ``a``

   * - ``size(a,n)``
     - ``a.shape[n-1]``
     - get the number of elements of the n-th dimension of array ``a``. (Note
       that MATLAB uses 1 based indexing while Python uses 0 based indexing,
       See note :ref:`INDEXING <numpy-for-matlab-users.notes>`)

   * - ``[ 1 2 3; 4 5 6 ]``
     - ``np.array([[1. ,2. ,3.], [4. ,5. ,6.]])``
     - define a 2x3 2D array

   * - ``[ a b; c d ]``
     - ``np.block([[a, b], [c, d]])``
     - construct a matrix from blocks ``a``, ``b``, ``c``, and ``d``

   * - ``a(end)``
     - ``a[-1]``
     - access last element in MATLAB vector (1xn or nx1) or 1D NumPy array
       ``a`` (length n)

   * - ``a(2,5)``
     - ``a[1, 4]``
     - access element in second row, fifth column in 2D array ``a``

   * - ``a(2,:)``
     - ``a[1]`` or  ``a[1, :]``
     - entire second row of 2D array ``a``

   * - ``a(1:5,:)``
     - ``a[0:5]`` or ``a[:5]`` or ``a[0:5, :]``
     - first 5 rows of 2D array ``a``

   * - ``a(end-4:end,:)``
     - ``a[-5:]``
     - last 5 rows of 2D array ``a``

   * - ``a(1:3,5:9)``
     - ``a[0:3, 4:9]``
     - The first through third rows and fifth through ninth columns of a 2D array, ``a``. 

   * - ``a([2,4,5],[1,3])``
     - ``a[np.ix_([1, 3, 4], [0, 2])]``
     - rows 2,4 and 5 and columns 1 and 3.  This allows the matrix to be
       modified, and doesn't require a regular slice.

   * - ``a(3:2:21,:)``
     - ``a[2:21:2,:]``
     - every other row of ``a``, starting with the third and going to the
       twenty-first

   * - ``a(1:2:end,:)``
     - ``a[ ::2,:]``
     - every other row of ``a``, starting with the first

   * - ``a(end:-1:1,:)``  or ``flipud(a)``
     -  ``a[::-1,:]``
     - ``a`` with rows in reverse order

   * - ``a([1:end 1],:)``
     -  ``a[np.r_[:len(a),0]]``
     - ``a`` with copy of the first row appended to the end

   * - ``a.'``
     - ``a.transpose()`` or ``a.T``
     - transpose of ``a``

   * - ``a'``
     - ``a.conj().transpose()`` or ``a.conj().T``
     - conjugate transpose of ``a``

   * - ``a * b``
     - ``a @ b``
     - matrix multiply

   * - ``a .* b``
     - ``a * b``
     - element-wise multiply

   * - ``a./b``
     - ``a/b``
     - element-wise divide

   * - ``a.^3``
     - ``a**3``
     - element-wise exponentiation

   * - ``(a > 0.5)``
     - ``(a > 0.5)``
     - matrix whose i,jth element is (a_ij > 0.5).  The MATLAB result is an
       array of logical values 0 and 1.  The NumPy result is an array of the boolean
       values ``False`` and ``True``.

   * - ``find(a > 0.5)``
     - ``np.nonzero(a > 0.5)``
     - find the indices where (``a`` > 0.5)

   * - ``a(:,find(v > 0.5))``
     - ``a[:,np.nonzero(v > 0.5)[0]]``
     - extract the columns of ``a`` where vector v > 0.5

   * - ``a(:,find(v>0.5))``
     - ``a[:, v.T > 0.5]``
     - extract the columns of ``a`` where column vector v > 0.5

   * - ``a(a<0.5)=0``
     - ``a[a < 0.5]=0``
     - ``a`` with elements less than 0.5 zeroed out

   * - ``a .* (a>0.5)``
     - ``a * (a > 0.5)``
     - ``a`` with elements less than 0.5 zeroed out

   * - ``a(:) = 3``
     - ``a[:] = 3``
     - set all values to the same scalar value

   * - ``y=x``
     - ``y = x.copy()``
     - NumPy assigns by reference

   * - ``y=x(2,:)``
     - ``y = x[1, :].copy()``
     - NumPy slices are by reference

   * - ``y=x(:)``
     - ``y = x.flatten()``
     - turn array into vector (note that this forces a copy). To obtain the
       same data ordering as in MATLAB, use ``x.flatten('F')``.

   * - ``1:10``
     - ``np.arange(1., 11.)`` or ``np.r_[1.:11.]`` or  ``np.r_[1:10:10j]``
     - create an increasing vector (see note :ref:`RANGES
       <numpy-for-matlab-users.notes>`)

   * - ``0:9``
     - ``np.arange(10.)`` or  ``np.r_[:10.]`` or  ``np.r_[:9:10j]``
     - create an increasing vector (see note :ref:`RANGES
       <numpy-for-matlab-users.notes>`)

   * - ``[1:10]'``
     - ``np.arange(1.,11.)[:, np.newaxis]``
     - create a column vector

   * - ``zeros(3,4)``
     - ``np.zeros((3, 4))``
     - 3x4 two-dimensional array full of 64-bit floating point zeros

   * - ``zeros(3,4,5)``
     - ``np.zeros((3, 4, 5))``
     - 3x4x5 three-dimensional array full of 64-bit floating point zeros

   * - ``ones(3,4)``
     - ``np.ones((3, 4))``
     - 3x4 two-dimensional array full of 64-bit floating point ones

   * - ``eye(3)``
     - ``np.eye(3)``
     - 3x3 identity matrix

   * - ``diag(a)``
     - ``np.diag(a)``
     - returns a vector of the diagonal elements of 2D array, ``a``

   * - ``diag(v,0)``
     - ``np.diag(v, 0)``
     - returns a square diagonal matrix whose nonzero values are the elements of
       vector, ``v``

   * - .. code:: matlab
         
         rng(42,'twister')
         rand(3,4)

     - ::

         from numpy.random import default_rng
         rng = default_rng(42)
         rng.random(3, 4) 

       or older version: ``random.rand((3, 4))``

     - generate a random 3x4 array with default random number generator and
       seed = 42

   * - ``linspace(1,3,4)``
     - ``np.linspace(1,3,4)``
     - 4 equally spaced samples between 1 and 3, inclusive

   * - ``[x,y]=meshgrid(0:8,0:5)``
     - ``np.mgrid[0:9.,0:6.]`` or ``np.meshgrid(r_[0:9.],r_[0:6.]``
     - two 2D arrays: one of x values, the other of y values

   * -
     - ``ogrid[0:9.,0:6.]`` or ``np.ix_(np.r_[0:9.],np.r_[0:6.]``
     - the best way to eval functions on a grid

   * - ``[x,y]=meshgrid([1,2,4],[2,4,5])``
     - ``np.meshgrid([1,2,4],[2,4,5])``
     -

   * -
     - ``ix_([1,2,4],[2,4,5])``
     - the best way to eval functions on a grid

   * - ``repmat(a, m, n)``
     - ``np.tile(a, (m, n))``
     - create m by n copies of ``a``

   * - ``[a b]``
     - ``np.concatenate((a,b),1)`` or ``np.hstack((a,b))`` or
       ``np.column_stack((a,b))`` or ``np.c_[a,b]``
     - concatenate columns of ``a`` and ``b``

   * - ``[a; b]``
     - ``np.concatenate((a,b))`` or ``np.vstack((a,b))`` or ``np.r_[a,b]``
     - concatenate rows of ``a`` and ``b``

   * - ``max(max(a))``
     - ``a.max()`` or ``np.nanmax(a)``
     - maximum element of ``a`` (with ndims(a)<=2 for MATLAB, if there are
       NaN's, ``nanmax`` will ignore these and return largest value)

   * - ``max(a)``
     - ``a.max(0)``
     - maximum element of each column of array ``a``

   * - ``max(a,[],2)``
     - ``a.max(1)``
     - maximum element of each row of array ``a``

   * - ``max(a,b)``
     - ``np.maximum(a, b)``
     - compares ``a`` and ``b`` element-wise, and returns the maximum value
       from each pair

   * - ``norm(v)``
     - ``np.sqrt(v @ v)`` or ``np.linalg.norm(v)``
     - L2 norm of vector ``v``

   * - ``a & b``
     - ``logical_and(a,b)``
     - element-by-element AND operator (NumPy ufunc) :ref:`See note
       LOGICOPS <numpy-for-matlab-users.notes>`

   * - ``a | b``
     - ``np.logical_or(a,b)``
     - element-by-element OR operator (NumPy ufunc) :ref:`See note LOGICOPS
       <numpy-for-matlab-users.notes>`

   * - ``bitand(a,b)``
     - ``a & b``
     - bitwise AND operator (Python native and NumPy ufunc)

   * - ``bitor(a,b)``
     - ``a | b``
     - bitwise OR operator (Python native and NumPy ufunc)

   * - ``inv(a)``
     - ``linalg.inv(a)``
     - inverse of square 2D array ``a``

   * - ``pinv(a)``
     - ``linalg.pinv(a)``
     - pseudo-inverse of 2D array ``a``

   * - ``rank(a)``
     - ``linalg.matrix_rank(a)``
     - matrix rank of a 2D array ``a``

   * - ``a\b``
     - ``linalg.solve(a, b)`` if ``a`` is square; ``linalg.lstsq(a, b)``
       otherwise
     - solution of a x = b for x

   * - ``b/a``
     - Solve ``a.T x.T = b.T`` instead
     - solution of x a = b for x

   * - ``[U,S,V]=svd(a)``
     - ``U, S, Vh = linalg.svd(a), V = Vh.T``
     - singular value decomposition of ``a``

   * - ``c=chol(a)`` where ``a==c'*c``
     - ``c = linalg.cholesky(a)`` where ``a == c@c.T``
     - Cholesky factorization of a 2D array (``chol(a)`` in MATLAB returns an
       upper triangular 2D array, but :func:`~scipy.linalg.cholesky` returns a lower
       triangular 2D array)

   * - ``[V,D]=eig(a)``
     - ``D,V = linalg.eig(a)``
     - eigenvalues :math:`\lambda` and eigenvectors :math:`\bar{v}` of ``a``,
       where :math:`\lambda\bar{v}=\mathbf{a}\bar{v}`

   * - ``[V,D]=eig(a,b)``
     - ``D,V = linalg.eig(a, b)``
     - eigenvalues :math:`\lambda` and eigenvectors :math:`\bar{v}` of
       ``a``, ``b``
       where :math:`\lambda\mathbf{b}\bar{v}=\mathbf{a}\bar{v}`

   * - ``[V,D]=eigs(a,3)``
     - ``D,V = eigs(a, k = 3)``
     - find the ``k=3`` largest eigenvalues and eigenvectors of 2D array, ``a``

   * - ``[Q,R,P]=qr(a,0)``
     - ``Q,R = linalg.qr(a)``
     - QR decomposition

   * - ``[L,U,P]=lu(a)`` where ``a==P'*L*U``
     - ``P,L,U = linalg.lu(a)`` where ``a == P@L@U``
     - LU decomposition (note: P(MATLAB) == transpose(P(NumPy)))

   * - ``conjgrad``
     - ``cg``
     - Conjugate gradients solver

   * - ``fft(a)``
     - ``np.fft(a)``
     - Fourier transform of ``a``

   * - ``ifft(a)``
     - ``np.ifft(a)``
     - inverse Fourier transform of ``a``

   * - ``sort(a)``
     - ``np.sort(a)`` or ``a.sort(axis=0)``
     - sort each column of a 2D array, ``a``

   * - ``sort(a, 2)``
     - ``np.sort(a, axis = 1)`` or ``a.sort(axis = 1)``
     - sort the each row of 2D array, ``a``

   * - ``[b,I]=sortrows(a,1)``
     - ``I = np.argsort(a[:, 0]); b = a[I,:]``
     - save the array ``a`` as array ``b`` with rows sorted by the first column

   * - ``x = Z\y``
     - ``x = linalg.lstsq(Z, y)``
     - perform a linear regression of the form :math:`\mathbf{Zx}=\mathbf{y}`

   * - ``decimate(x, q)``
     - ``signal.resample(x, np.ceil(len(x)/q))``
     - downsample with low-pass filtering

   * - ``unique(a)``
     - ``np.unique(a)``
     - a vector of unique values in array ``a``

   * - ``squeeze(a)``
     - ``a.squeeze()``
     - remove singleton dimensions of array ``a``. Note that MATLAB will always
       return arrays of 2D or higher while NumPy will return arrays of 0D or
       higher

.. _numpy-for-matlab-users.notes:

Notes
=====

\ **Submatrix**: Assignment to a submatrix can be done with lists of
indices using the ``ix_`` command. E.g., for 2D array ``a``, one might
do: ``ind=[1, 3]; a[np.ix_(ind, ind)] += 100``.

\ **HELP**: There is no direct equivalent of MATLAB's ``which`` command,
but the commands :func:`help` and :func:`numpy.source` will usually list the filename
where the function is located. Python also has an ``inspect`` module (do
``import inspect``) which provides a ``getfile`` that often works.

\ **INDEXING**: MATLAB uses one based indexing, so the initial element
of a sequence has index 1. Python uses zero based indexing, so the
initial element of a sequence has index 0. Confusion and flamewars arise
because each has advantages and disadvantages. One based indexing is
consistent with common human language usage, where the "first" element
of a sequence has index 1. Zero based indexing `simplifies
indexing <https://groups.google.com/group/comp.lang.python/msg/1bf4d925dfbf368?q=g:thl3498076713d&hl=en>`__.
See also `a text by prof.dr. Edsger W.
Dijkstra <https://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html>`__.

\ **RANGES**: In MATLAB, ``0:5`` can be used as both a range literal
and a 'slice' index (inside parentheses); however, in Python, constructs
like ``0:5`` can *only* be used as a slice index (inside square
brackets). Thus the somewhat quirky ``r_`` object was created to allow
NumPy to have a similarly terse range construction mechanism. Note that
``r_`` is not called like a function or a constructor, but rather
*indexed* using square brackets, which allows the use of Python's slice
syntax in the arguments.

\ **LOGICOPS**: ``&`` or ``|`` in NumPy is bitwise AND/OR, while in MATLAB &
and ``|`` are logical AND/OR. The two can appear to work the same,
but there are important differences. If you would have used MATLAB's ``&``
or ``|`` operators, you should use the NumPy ufuncs
``logical_and``/``logical_or``. The notable differences between MATLAB's and
NumPy's ``&`` and ``|`` operators are:

-  Non-logical {0,1} inputs: NumPy's output is the bitwise AND of the
   inputs. MATLAB treats any non-zero value as 1 and returns the logical
   AND. For example ``(3 & 4)`` in NumPy is ``0``, while in MATLAB both ``3``
   and ``4``
   are considered logical true and ``(3 & 4)`` returns ``1``.

-  Precedence: NumPy's & operator is higher precedence than logical
   operators like ``<`` and ``>``; MATLAB's is the reverse.

If you know you have boolean arguments, you can get away with using
NumPy's bitwise operators, but be careful with parentheses, like this: ``z
= (x > 1) & (x < 2)``. The absence of NumPy operator forms of ``logical_and``
and ``logical_or`` is an unfortunate consequence of Python's design.

**RESHAPE and LINEAR INDEXING**: MATLAB always allows multi-dimensional
arrays to be accessed using scalar or linear indices, NumPy does not.
Linear indices are common in MATLAB programs, e.g. ``find()`` on a matrix
returns them, whereas NumPy's find behaves differently. When converting
MATLAB code it might be necessary to first reshape a matrix to a linear
sequence, perform some indexing operations and then reshape back. As
reshape (usually) produces views onto the same storage, it should be
possible to do this fairly efficiently. Note that the scan order used by
reshape in NumPy defaults to the 'C' order, whereas MATLAB uses the
Fortran order. If you are simply converting to a linear sequence and
back this doesn't matter. But if you are converting reshapes from MATLAB
code which relies on the scan order, then this MATLAB code: ``z =
reshape(x,3,4);`` should become ``z = x.reshape(3,4,order='F').copy()`` in
NumPy.

'array' or 'matrix'? Which should I use?
========================================

Historically, NumPy has provided a special matrix type, `np.matrix`, which
is a subclass of ndarray which makes binary operations linear algebra
operations. You may see it used in some existing code instead of `np.array`.
So, which one to use?

Short answer
------------

**Use arrays**.

-  They support multidimensional array algebra that is supported in MATLAB
-  They are the standard vector/matrix/tensor type of NumPy. Many NumPy
   functions return arrays, not matrices.
-  There is a clear distinction between element-wise operations and
   linear algebra operations.
-  You can have standard vectors or row/column vectors if you like.

Until Python 3.5 the only disadvantage of using the array type was that you
had to use ``dot`` instead of ``*`` to multiply (reduce) two tensors
(scalar product, matrix vector multiplication etc.). Since Python 3.5 you
can use the matrix multiplication ``@`` operator.

Given the above, we intend to deprecate ``matrix`` eventually.

Long answer
-----------

NumPy contains both an ``array`` class and a ``matrix`` class. The
``array`` class is intended to be a general-purpose n-dimensional array
for many kinds of numerical computing, while ``matrix`` is intended to
facilitate linear algebra computations specifically. In practice there
are only a handful of key differences between the two.

-  Operators ``*`` and ``@``, functions ``dot()``, and ``multiply()``:

   -  For ``array``, **``*`` means element-wise multiplication**, while
      **``@`` means matrix multiplication**; they have associated functions
      ``multiply()`` and ``dot()``.  (Before Python 3.5, ``@`` did not exist
      and one had to use ``dot()`` for matrix multiplication).
   -  For ``matrix``, **``*`` means matrix multiplication**, and for
      element-wise multiplication one has to use the ``multiply()`` function.

-  Handling of vectors (one-dimensional arrays)

   -  For ``array``, the **vector shapes 1xN, Nx1, and N are all different
      things**. Operations like ``A[:,1]`` return a one-dimensional array of
      shape N, not a two-dimensional array of shape Nx1. Transpose on a
      one-dimensional ``array`` does nothing.
   -  For ``matrix``, **one-dimensional arrays are always upconverted to 1xN
      or Nx1 matrices** (row or column vectors). ``A[:,1]`` returns a
      two-dimensional matrix of shape Nx1.

-  Handling of higher-dimensional arrays (ndim > 2)

   -  ``array`` objects **can have number of dimensions > 2**;
   -  ``matrix`` objects **always have exactly two dimensions**.

-  Convenience attributes

   -  ``array`` **has a .T attribute**, which returns the transpose of
      the data.
   -  ``matrix`` **also has .H, .I, and .A attributes**, which return
      the conjugate transpose, inverse, and ``asarray()`` of the matrix,
      respectively.

-  Convenience constructor

   -  The ``array`` constructor **takes (nested) Python sequences as
      initializers**. As in, ``array([[1,2,3],[4,5,6]])``.
   -  The ``matrix`` constructor additionally **takes a convenient
      string initializer**. As in ``matrix("[1 2 3; 4 5 6]")``.

There are pros and cons to using both:

-  ``array``

   -  ``:)`` Element-wise multiplication is easy: ``A*B``.
   -  ``:(`` You have to remember that matrix multiplication has its own
      operator, ``@``.
   -  ``:)`` You can treat one-dimensional arrays as *either* row or column
      vectors. ``A @ v`` treats ``v`` as a column vector, while
      ``v @ A`` treats ``v`` as a row vector. This can save you having to
      type a lot of transposes.
   -  ``:)`` ``array`` is the "default" NumPy type, so it gets the most
      testing, and is the type most likely to be returned by 3rd party
      code that uses NumPy.
   -  ``:)`` Is quite at home handling data of any number of dimensions.
   -  ``:)`` Closer in semantics to tensor algebra, if you are familiar
      with that.
   -  ``:)`` *All* operations (``*``, ``/``, ``+``, ``-`` etc.) are
      element-wise.
   -  ``:(`` Sparse matrices from ``scipy.sparse`` do not interact as well
      with arrays.

-  ``matrix``

   -  ``:\\`` Behavior is more like that of MATLAB matrices.
   -  ``<:(`` Maximum of two-dimensional. To hold three-dimensional data you
      need ``array`` or perhaps a Python list of ``matrix``.
   -  ``<:(`` Minimum of two-dimensional. You cannot have vectors. They must be
      cast as single-column or single-row matrices.
   -  ``<:(`` Since ``array`` is the default in NumPy, some functions may
      return an ``array`` even if you give them a ``matrix`` as an
      argument. This shouldn't happen with NumPy functions (if it does
      it's a bug), but 3rd party code based on NumPy may not honor type
      preservation like NumPy does.
   -  ``:)`` ``A*B`` is matrix multiplication, so it looks just like you write
      it in linear algebra (For Python >= 3.5 plain arrays have the same
      convenience with the ``@`` operator).
   -  ``<:(`` Element-wise multiplication requires calling a function,
      ``multiply(A,B)``.
   -  ``<:(`` The use of operator overloading is a bit illogical: ``*``
      does not work element-wise but ``/`` does.
   -  Interaction with ``scipy.sparse`` is a bit cleaner.

The ``array`` is thus much more advisable to use.  Indeed, we intend to
deprecate ``matrix`` eventually.

Customizing your environment
============================

In MATLAB the main tool available to you for customizing the
environment is to modify the search path with the locations of your
favorite functions. You can put such customizations into a startup
script that MATLAB will run on startup.

NumPy, or rather Python, has similar facilities.

-  To modify your Python search path to include the locations of your
   own modules, define the ``PYTHONPATH`` environment variable.

-  To have a particular script file executed when the interactive Python
   interpreter is started, define the ``PYTHONSTARTUP`` environment
   variable to contain the name of your startup script.

Unlike MATLAB, where anything on your path can be called immediately,
with Python you need to first do an 'import' statement to make functions
in a particular file accessible.

For example you might make a startup script that looks like this (Note:
this is just an example, not a statement of "best practices"):

::

    # Make all numpy available via shorter 'np' prefix
    import numpy as np
    # 
    # Make the SciPy linear algebra functions available as linalg.func()
    # e.g. linalg.lu, linalg.eig (for general l*B@u==A@u solution)
    from scipy import linalg
    #
    # Define a Hermitian function
    def hermitian(A, **kwargs):
        return np.conj(A,**kwargs).T
    # Make a shortcut for hermitian:
    #    hermitian(A) --> H(A)
    H = hermitian

To use the deprecated `matrix` and other `matlib` functions:

::
    
    # Make all matlib functions accessible at the top level via M.func()
    import numpy.matlib as M
    # Make some matlib functions accessible directly at the top level via, e.g. rand(3,3)
    from numpy.matlib import matrix,rand,zeros,ones,empty,eye

Links
=====

Another somewhat outdated MATLAB/NumPy cross-reference can be found at
http://mathesaurus.sf.net/

An extensive list of tools for scientific work with Python can be
found in the `topical software page <https://scipy.org/topical-software.html>`__.

See
`List of Python software: scripting
<https://en.wikipedia.org/wiki/List_of_Python_software#Embedded_as_a_scripting_language>`_
for a list of software that use Python as a scripting language

MATLAB® and SimuLink® are registered trademarks of The MathWorks, Inc.
.. _basics.copies-and-views:

****************
Copies and views
****************

When operating on NumPy arrays, it is possible to access the internal data
buffer directly using a :ref:`view <view>` without copying data around. This
ensures good performance but can also cause unwanted problems if the user is
not aware of how this works. Hence, it is important to know the difference
between these two terms and to know which operations return copies and
which return views.

The NumPy array is a data structure consisting of two parts:
the :term:`contiguous` data buffer with the actual data elements and the
metadata that contains information about the data buffer. The metadata
includes data type, strides, and other important information that helps
manipulate the :class:`.ndarray` easily. See the :ref:`numpy-internals`
section for a detailed look.

.. _view:

View
====

It is possible to access the array differently by just changing certain
metadata like :term:`stride` and :term:`dtype` without changing the
data buffer. This creates a new way of looking at the data and these new
arrays are called views. The data buffer remains the same, so any changes made
to a view reflects in the original copy. A view can be forced through the
:meth:`.ndarray.view` method.

Copy
====

When a new array is created by duplicating the data buffer as well as the
metadata, it is called a copy. Changes made to the copy
do not reflect on the original array. Making a copy is slower and
memory-consuming but sometimes necessary. A copy can be forced by using
:meth:`.ndarray.copy`.

.. _indexing-operations:

Indexing operations
===================

.. seealso:: :ref:`basics.indexing`

Views are created when elements can be addressed with offsets and strides
in the original array. Hence, basic indexing always creates views.
For example::

    >>> x = np.arange(10)
    >>> x
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    >>> y = x[1:3]  # creates a view
    >>> y
    array([1, 2])
    >>> x[1:3] = [10, 11]
    >>> x
    array([ 0, 10, 11,  3,  4,  5,  6,  7,  8,  9])
    >>> y
    array([10, 11])  

Here, ``y`` gets changed when ``x`` is changed because it is a view.

:ref:`advanced-indexing`, on the other hand, always creates copies.
For example::

    >>> x = np.arange(9).reshape(3, 3)
    >>> x
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])
    >>> y = x[[1, 2]]
    >>> y
    array([[3, 4, 5],
           [6, 7, 8]])
    >>> y.base is None
    True

Here, ``y`` is a copy, as signified by the :attr:`base <.ndarray.base>`
attribute. We can also confirm this by assigning new values to ``x[[1, 2]]``
which in turn will not affect ``y`` at all::

    >>> x[[1, 2]] = [[10, 11, 12], [13, 14, 15]]
    >>> x
    array([[ 0,  1,  2],
           [10, 11, 12],
           [13, 14, 15]])
    >>> y
    array([[3, 4, 5],
           [6, 7, 8]])

It must be noted here that during the assignment of ``x[[1, 2]]`` no view
or copy is created as the assignment happens in-place. 


Other operations
================

The :func:`numpy.reshape` function creates a view where possible or a copy
otherwise. In most cases, the strides can be modified to reshape the
array with a view. However, in some cases where the array becomes
non-contiguous (perhaps after a :meth:`.ndarray.transpose` operation),
the reshaping cannot be done by modifying strides and requires a copy.
In these cases, we can raise an error by assigning the new shape to the
shape attribute of the array. For example::

    >>> x = np.ones((2, 3))
    >>> y = x.T  # makes the array non-contiguous
    >>> y
    array([[1., 1.],
           [1., 1.],
           [1., 1.]])
    >>> z = y.view()
    >>> z.shape = 6
    Traceback (most recent call last):
       ...
    AttributeError: Incompatible shape for in-place modification. Use
    `.reshape()` to make a copy with the desired shape.

Taking the example of another operation, :func:`.ravel` returns a contiguous
flattened view of the array wherever possible. On the other hand,
:meth:`.ndarray.flatten` always returns a flattened copy of the array.
However, to guarantee a view in most cases, ``x.reshape(-1)`` may be preferable.

How to tell if the array is a view or a copy
============================================

The :attr:`base <.ndarray.base>` attribute of the ndarray makes it easy
to tell if an array is a view or a copy. The base attribute of a view returns
the original array while it returns ``None`` for a copy.

    >>> x = np.arange(9)
    >>> x
    array([0, 1, 2, 3, 4, 5, 6, 7, 8])
    >>> y = x.reshape(3, 3)
    >>> y
    array([[0, 1, 2],
           [3, 4, 5],
           [6, 7, 8]])
    >>> y.base  # .reshape() creates a view
    array([0, 1, 2, 3, 4, 5, 6, 7, 8])
    >>> z = y[[2, 1]]
    >>> z
    array([[6, 7, 8],
           [3, 4, 5]])
    >>> z.base is None  # advanced indexing creates a copy
    True

Note that the ``base`` attribute should not be used to determine
if an ndarray object is *new*; only if it is a view or a copy
of another ndarray.
.. _whatisnumpy:

**************
What is NumPy?
**************

NumPy is the fundamental package for scientific computing in Python.
It is a Python library that provides a multidimensional array object,
various derived objects (such as masked arrays and matrices), and an
assortment of routines for fast operations on arrays, including
mathematical, logical, shape manipulation, sorting, selecting, I/O,
discrete Fourier transforms, basic linear algebra, basic statistical
operations, random simulation and much more.

At the core of the NumPy package, is the `ndarray` object.  This
encapsulates *n*-dimensional arrays of homogeneous data types, with
many operations being performed in compiled code for performance.
There are several important differences between NumPy arrays and the
standard Python sequences:

- NumPy arrays have a fixed size at creation, unlike Python lists
  (which can grow dynamically). Changing the size of an `ndarray` will
  create a new array and delete the original.

- The elements in a NumPy array are all required to be of the same
  data type, and thus will be the same size in memory.  The exception:
  one can have arrays of (Python, including NumPy) objects, thereby
  allowing for arrays of different sized elements.

- NumPy arrays facilitate advanced mathematical and other types of
  operations on large numbers of data.  Typically, such operations are
  executed more efficiently and with less code than is possible using
  Python's built-in sequences.

- A growing plethora of scientific and mathematical Python-based
  packages are using NumPy arrays; though these typically support
  Python-sequence input, they convert such input to NumPy arrays prior
  to processing, and they often output NumPy arrays.  In other words,
  in order to efficiently use much (perhaps even most) of today's
  scientific/mathematical Python-based software, just knowing how to
  use Python's built-in sequence types is insufficient - one also
  needs to know how to use NumPy arrays.

The points about sequence size and speed are particularly important in
scientific computing.  As a simple example, consider the case of
multiplying each element in a 1-D sequence with the corresponding
element in another sequence of the same length.  If the data are
stored in two Python lists, ``a`` and ``b``, we could iterate over
each element::

  c = []
  for i in range(len(a)):
      c.append(a[i]*b[i])

This produces the correct answer, but if ``a`` and ``b`` each contain
millions of numbers, we will pay the price for the inefficiencies of
looping in Python.  We could accomplish the same task much more
quickly in C by writing (for clarity we neglect variable declarations
and initializations, memory allocation, etc.)

::

  for (i = 0; i < rows; i++) {
    c[i] = a[i]*b[i];
  }

This saves all the overhead involved in interpreting the Python code
and manipulating Python objects, but at the expense of the benefits
gained from coding in Python.  Furthermore, the coding work required
increases with the dimensionality of our data. In the case of a 2-D
array, for example, the C code (abridged as before) expands to

::

  for (i = 0; i < rows; i++) {
    for (j = 0; j < columns; j++) {
      c[i][j] = a[i][j]*b[i][j];
    }
  }

NumPy gives us the best of both worlds: element-by-element operations
are the "default mode" when an `ndarray` is involved, but the
element-by-element operation is speedily executed by pre-compiled C
code.  In NumPy

::

  c = a * b

does what the earlier examples do, at near-C speeds, but with the code
simplicity we expect from something based on Python. Indeed, the NumPy
idiom is even simpler!  This last example illustrates two of NumPy's
features which are the basis of much of its power: vectorization and
broadcasting.

.. _whatis-vectorization:

Why is NumPy Fast?
------------------

Vectorization describes the absence of any explicit looping, indexing,
etc., in the code - these things are taking place, of course, just
"behind the scenes" in optimized, pre-compiled C code.  Vectorized
code has many advantages, among which are:

- vectorized code is more concise and easier to read

- fewer lines of code generally means fewer bugs

- the code more closely resembles standard mathematical notation
  (making it easier, typically, to correctly code mathematical
  constructs)

- vectorization results in more "Pythonic" code. Without
  vectorization, our code would be littered with inefficient and
  difficult to read ``for`` loops.

Broadcasting is the term used to describe the implicit
element-by-element behavior of operations; generally speaking, in
NumPy all operations, not just arithmetic operations, but
logical, bit-wise, functional, etc., behave in this implicit
element-by-element fashion, i.e., they broadcast.  Moreover, in the
example above, ``a`` and ``b`` could be multidimensional arrays of the
same shape, or a scalar and an array, or even two arrays of with
different shapes, provided that the smaller array is "expandable" to
the shape of the larger in such a way that the resulting broadcast is
unambiguous. For detailed "rules" of broadcasting see
:ref:`Broadcasting <basics.broadcasting>`.

Who Else Uses NumPy?
--------------------

NumPy fully supports an object-oriented approach, starting, once
again, with `ndarray`.  For example, `ndarray` is a class, possessing
numerous methods and attributes.  Many of its methods are mirrored by
functions in the outer-most NumPy namespace, allowing the programmer
to code in whichever paradigm they prefer. This flexibility has allowed the
NumPy array dialect and NumPy `ndarray` class to become the *de-facto* language
of multi-dimensional data interchange used in Python.
===================
NumPy quickstart
===================

.. currentmodule:: numpy

.. testsetup::

   >>> import numpy as np
   >>> import sys

Prerequisites
=============

You'll need to know a bit of Python. For a refresher, see the `Python
tutorial <https://docs.python.org/tutorial/>`__.

To work the examples, you'll need ``matplotlib`` installed
in addition to NumPy.

**Learner profile**

This is a quick overview of arrays in NumPy. It demonstrates how n-dimensional
(:math:`n>=2`) arrays are represented and can be manipulated. In particular, if
you don't know how to apply common functions to n-dimensional arrays (without
using for-loops), or if you want to understand axis and shape properties for
n-dimensional arrays, this article might be of help.

**Learning Objectives**

After reading, you should be able to:

- Understand the difference between one-, two- and n-dimensional arrays in
  NumPy;
- Understand how to apply some linear algebra operations to n-dimensional
  arrays without using for-loops;
- Understand axis and shape properties for n-dimensional arrays.

.. _quickstart.the-basics:

The Basics
==========

NumPy's main object is the homogeneous multidimensional array. It is a
table of elements (usually numbers), all of the same type, indexed by a
tuple of non-negative integers. In NumPy dimensions are called *axes*.

For example, the array for the coordinates of a point in 3D space,
``[1, 2, 1]``, has one axis. That axis has 3 elements in it, so we say
it has a length of 3. In the example pictured below, the array has 2 
axes. The first axis has a length of 2, the second axis has a length of 
3.

::

    [[1., 0., 0.],
     [0., 1., 2.]]

NumPy's array class is called ``ndarray``. It is also known by the alias
``array``. Note that ``numpy.array`` is not the same as the Standard
Python Library class ``array.array``, which only handles one-dimensional
arrays and offers less functionality. The more important attributes of
an ``ndarray`` object are:

ndarray.ndim
    the number of axes (dimensions) of the array.
ndarray.shape
    the dimensions of the array. This is a tuple of integers indicating
    the size of the array in each dimension. For a matrix with *n* rows
    and *m* columns, ``shape`` will be ``(n,m)``. The length of the
    ``shape`` tuple is therefore the number of axes, ``ndim``.
ndarray.size
    the total number of elements of the array. This is equal to the
    product of the elements of ``shape``.
ndarray.dtype
    an object describing the type of the elements in the array. One can
    create or specify dtype's using standard Python types. Additionally
    NumPy provides types of its own. numpy.int32, numpy.int16, and
    numpy.float64 are some examples.
ndarray.itemsize
    the size in bytes of each element of the array. For example, an
    array of elements of type ``float64`` has ``itemsize`` 8 (=64/8),
    while one of type ``complex32`` has ``itemsize`` 4 (=32/8). It is
    equivalent to ``ndarray.dtype.itemsize``.
ndarray.data
    the buffer containing the actual elements of the array. Normally, we
    won't need to use this attribute because we will access the elements
    in an array using indexing facilities.

An example
----------

    >>> import numpy as np
    >>> a = np.arange(15).reshape(3, 5)
    >>> a
    array([[ 0,  1,  2,  3,  4],
           [ 5,  6,  7,  8,  9],
           [10, 11, 12, 13, 14]])
    >>> a.shape
    (3, 5)
    >>> a.ndim
    2
    >>> a.dtype.name
    'int64'
    >>> a.itemsize
    8
    >>> a.size
    15
    >>> type(a)
    <class 'numpy.ndarray'>
    >>> b = np.array([6, 7, 8])
    >>> b
    array([6, 7, 8])
    >>> type(b)
    <class 'numpy.ndarray'>

.. _quickstart.array-creation:

Array Creation
--------------

There are several ways to create arrays.

For example, you can create an array from a regular Python list or tuple
using the ``array`` function. The type of the resulting array is deduced
from the type of the elements in the sequences.

::

    >>> import numpy as np
    >>> a = np.array([2, 3, 4])
    >>> a
    array([2, 3, 4])
    >>> a.dtype
    dtype('int64')
    >>> b = np.array([1.2, 3.5, 5.1])
    >>> b.dtype
    dtype('float64')

A frequent error consists in calling ``array`` with multiple arguments,
rather than providing a single sequence as an argument.

::

    >>> a = np.array(1, 2, 3, 4)    # WRONG
    Traceback (most recent call last):
      ...
    TypeError: array() takes from 1 to 2 positional arguments but 4 were given
    >>> a = np.array([1, 2, 3, 4])  # RIGHT

``array`` transforms sequences of sequences into two-dimensional arrays,
sequences of sequences of sequences into three-dimensional arrays, and
so on.

::

    >>> b = np.array([(1.5, 2, 3), (4, 5, 6)])
    >>> b
    array([[1.5, 2. , 3. ],
           [4. , 5. , 6. ]])

The type of the array can also be explicitly specified at creation time:

::

    >>> c = np.array([[1, 2], [3, 4]], dtype=complex)
    >>> c
    array([[1.+0.j, 2.+0.j],
           [3.+0.j, 4.+0.j]])

Often, the elements of an array are originally unknown, but its size is
known. Hence, NumPy offers several functions to create
arrays with initial placeholder content. These minimize the necessity of
growing arrays, an expensive operation.

The function ``zeros`` creates an array full of zeros, the function
``ones`` creates an array full of ones, and the function ``empty``
creates an array whose initial content is random and depends on the
state of the memory. By default, the dtype of the created array is
``float64``, but it can be specified via the key word argument ``dtype``.

::

    >>> np.zeros((3, 4))
    array([[0., 0., 0., 0.],
           [0., 0., 0., 0.],
           [0., 0., 0., 0.]])
    >>> np.ones((2, 3, 4), dtype=np.int16)
    array([[[1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1]],
    <BLANKLINE>
           [[1, 1, 1, 1],
            [1, 1, 1, 1],
            [1, 1, 1, 1]]], dtype=int16)
    >>> np.empty((2, 3)) #doctest: +SKIP
    array([[3.73603959e-262, 6.02658058e-154, 6.55490914e-260],  # may vary
           [5.30498948e-313, 3.14673309e-307, 1.00000000e+000]])

To create sequences of numbers, NumPy provides the ``arange`` function
which is analogous to the Python built-in ``range``, but returns an
array.

::

    >>> np.arange(10, 30, 5)
    array([10, 15, 20, 25])
    >>> np.arange(0, 2, 0.3)  # it accepts float arguments
    array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8])

When ``arange`` is used with floating point arguments, it is generally
not possible to predict the number of elements obtained, due to the
finite floating point precision. For this reason, it is usually better
to use the function ``linspace`` that receives as an argument the number
of elements that we want, instead of the step::

    >>> from numpy import pi
    >>> np.linspace(0, 2, 9)                   # 9 numbers from 0 to 2
    array([0.  , 0.25, 0.5 , 0.75, 1.  , 1.25, 1.5 , 1.75, 2.  ])
    >>> x = np.linspace(0, 2 * pi, 100)        # useful to evaluate function at lots of points
    >>> f = np.sin(x)

.. seealso::
    `array`,
    `zeros`,
    `zeros_like`,
    `ones`,
    `ones_like`,
    `empty`,
    `empty_like`,
    `arange`,
    `linspace`,
    `numpy.random.Generator.rand`,
    `numpy.random.Generator.randn`,
    `fromfunction`,
    `fromfile`

Printing Arrays
---------------

When you print an array, NumPy displays it in a similar way to nested
lists, but with the following layout:

-  the last axis is printed from left to right,
-  the second-to-last is printed from top to bottom,
-  the rest are also printed from top to bottom, with each slice
   separated from the next by an empty line.

One-dimensional arrays are then printed as rows, bidimensionals as
matrices and tridimensionals as lists of matrices.

::

    >>> a = np.arange(6)                    # 1d array
    >>> print(a)
    [0 1 2 3 4 5]
    >>> 
    >>> b = np.arange(12).reshape(4, 3)     # 2d array
    >>> print(b)
    [[ 0  1  2]
     [ 3  4  5]
     [ 6  7  8]
     [ 9 10 11]]
    >>> 
    >>> c = np.arange(24).reshape(2, 3, 4)  # 3d array
    >>> print(c)
    [[[ 0  1  2  3]
      [ 4  5  6  7]
      [ 8  9 10 11]]
    <BLANKLINE>
     [[12 13 14 15]
      [16 17 18 19]
      [20 21 22 23]]]

See :ref:`below <quickstart.shape-manipulation>` to get
more details on ``reshape``.

If an array is too large to be printed, NumPy automatically skips the
central part of the array and only prints the corners::

    >>> print(np.arange(10000))
    [   0    1    2 ... 9997 9998 9999]
    >>> 
    >>> print(np.arange(10000).reshape(100, 100))
    [[   0    1    2 ...   97   98   99]
     [ 100  101  102 ...  197  198  199]
     [ 200  201  202 ...  297  298  299]
     ...
     [9700 9701 9702 ... 9797 9798 9799]
     [9800 9801 9802 ... 9897 9898 9899]
     [9900 9901 9902 ... 9997 9998 9999]]

To disable this behaviour and force NumPy to print the entire array, you
can change the printing options using ``set_printoptions``.

::

    >>> np.set_printoptions(threshold=sys.maxsize)  # sys module should be imported


.. _quickstart.basic-operations:

Basic Operations
----------------

Arithmetic operators on arrays apply *elementwise*. A new array is
created and filled with the result.

::

    >>> a = np.array([20, 30, 40, 50])
    >>> b = np.arange(4)
    >>> b
    array([0, 1, 2, 3])
    >>> c = a - b
    >>> c
    array([20, 29, 38, 47])
    >>> b**2
    array([0, 1, 4, 9])
    >>> 10 * np.sin(a)
    array([ 9.12945251, -9.88031624,  7.4511316 , -2.62374854])
    >>> a < 35
    array([ True,  True, False, False])

Unlike in many matrix languages, the product operator ``*`` operates
elementwise in NumPy arrays. The matrix product can be performed using
the ``@`` operator (in python >=3.5) or the ``dot`` function or method::

    >>> A = np.array([[1, 1],
    ...               [0, 1]])
    >>> B = np.array([[2, 0],
    ...               [3, 4]])
    >>> A * B     # elementwise product
    array([[2, 0],
           [0, 4]])
    >>> A @ B     # matrix product
    array([[5, 4],
           [3, 4]])
    >>> A.dot(B)  # another matrix product
    array([[5, 4],
           [3, 4]])

Some operations, such as ``+=`` and ``*=``, act in place to modify an
existing array rather than create a new one.

::

    >>> rg = np.random.default_rng(1)  # create instance of default random number generator
    >>> a = np.ones((2, 3), dtype=int)
    >>> b = rg.random((2, 3))
    >>> a *= 3
    >>> a
    array([[3, 3, 3],
           [3, 3, 3]])
    >>> b += a
    >>> b
    array([[3.51182162, 3.9504637 , 3.14415961],
           [3.94864945, 3.31183145, 3.42332645]])
    >>> a += b  # b is not automatically converted to integer type
    Traceback (most recent call last):
        ...
    numpy.core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'

When operating with arrays of different types, the type of the resulting
array corresponds to the more general or precise one (a behavior known
as upcasting).

::

    >>> a = np.ones(3, dtype=np.int32)
    >>> b = np.linspace(0, pi, 3)
    >>> b.dtype.name
    'float64'
    >>> c = a + b
    >>> c
    array([1.        , 2.57079633, 4.14159265])
    >>> c.dtype.name
    'float64'
    >>> d = np.exp(c * 1j)
    >>> d
    array([ 0.54030231+0.84147098j, -0.84147098+0.54030231j,
           -0.54030231-0.84147098j])
    >>> d.dtype.name
    'complex128'

Many unary operations, such as computing the sum of all the elements in
the array, are implemented as methods of the ``ndarray`` class.

::

    >>> a = rg.random((2, 3))
    >>> a
    array([[0.82770259, 0.40919914, 0.54959369],
           [0.02755911, 0.75351311, 0.53814331]])
    >>> a.sum()
    3.1057109529998157
    >>> a.min()
    0.027559113243068367
    >>> a.max()
    0.8277025938204418

By default, these operations apply to the array as though it were a list
of numbers, regardless of its shape. However, by specifying the ``axis``
parameter you can apply an operation along the specified axis of an
array::

    >>> b = np.arange(12).reshape(3, 4)
    >>> b
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>>
    >>> b.sum(axis=0)     # sum of each column
    array([12, 15, 18, 21])
    >>>
    >>> b.min(axis=1)     # min of each row
    array([0, 4, 8])
    >>>
    >>> b.cumsum(axis=1)  # cumulative sum along each row
    array([[ 0,  1,  3,  6],
           [ 4,  9, 15, 22],
           [ 8, 17, 27, 38]])


Universal Functions
-------------------

NumPy provides familiar mathematical functions such as sin, cos, and
exp. In NumPy, these are called "universal
functions" (\ ``ufunc``). Within NumPy, these functions
operate elementwise on an array, producing an array as output.

::

    >>> B = np.arange(3)
    >>> B
    array([0, 1, 2])
    >>> np.exp(B)
    array([1.        , 2.71828183, 7.3890561 ])
    >>> np.sqrt(B)
    array([0.        , 1.        , 1.41421356])
    >>> C = np.array([2., -1., 4.])
    >>> np.add(B, C)
    array([2., 0., 6.])

.. seealso::

    `all`,
    `any`,
    `apply_along_axis`,
    `argmax`,
    `argmin`,
    `argsort`,
    `average`,
    `bincount`,
    `ceil`,
    `clip`,
    `conj`,
    `corrcoef`,
    `cov`,
    `cross`,
    `cumprod`,
    `cumsum`,
    `diff`,
    `dot`,
    `floor`,
    `inner`,
    `invert`,
    `lexsort`,
    `max`,
    `maximum`,
    `mean`,
    `median`,
    `min`,
    `minimum`,
    `nonzero`,
    `outer`,
    `prod`,
    `re`,
    `round`,
    `sort`,
    `std`,
    `sum`,
    `trace`,
    `transpose`,
    `var`,
    `vdot`,
    `vectorize`,
    `where`

.. _quickstart.indexing-slicing-and-iterating:

Indexing, Slicing and Iterating
-------------------------------

**One-dimensional** arrays can be indexed, sliced and iterated over,
much like
`lists <https://docs.python.org/tutorial/introduction.html#lists>`__
and other Python sequences.

::

    >>> a = np.arange(10)**3
    >>> a
    array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729])
    >>> a[2]
    8
    >>> a[2:5]
    array([ 8, 27, 64])
    >>> # equivalent to a[0:6:2] = 1000;
    >>> # from start to position 6, exclusive, set every 2nd element to 1000
    >>> a[:6:2] = 1000
    >>> a
    array([1000,    1, 1000,   27, 1000,  125,  216,  343,  512,  729])
    >>> a[::-1]  # reversed a
    array([ 729,  512,  343,  216,  125, 1000,   27, 1000,    1, 1000])
    >>> for i in a:
    ...     print(i**(1 / 3.))
    ...
    9.999999999999998
    1.0
    9.999999999999998
    3.0
    9.999999999999998
    4.999999999999999
    5.999999999999999
    6.999999999999999
    7.999999999999999
    8.999999999999998


**Multidimensional** arrays can have one index per axis. These indices
are given in a tuple separated by commas::

    >>> def f(x, y):
    ...     return 10 * x + y
    ...
    >>> b = np.fromfunction(f, (5, 4), dtype=int)
    >>> b
    array([[ 0,  1,  2,  3],
           [10, 11, 12, 13],
           [20, 21, 22, 23],
           [30, 31, 32, 33],
           [40, 41, 42, 43]])
    >>> b[2, 3]
    23
    >>> b[0:5, 1]  # each row in the second column of b
    array([ 1, 11, 21, 31, 41])
    >>> b[:, 1]    # equivalent to the previous example
    array([ 1, 11, 21, 31, 41])
    >>> b[1:3, :]  # each column in the second and third row of b
    array([[10, 11, 12, 13],
           [20, 21, 22, 23]])

When fewer indices are provided than the number of axes, the missing
indices are considered complete slices\ ``:``

::

    >>> b[-1]   # the last row. Equivalent to b[-1, :]
    array([40, 41, 42, 43])

The expression within brackets in ``b[i]`` is treated as an ``i``
followed by as many instances of ``:`` as needed to represent the
remaining axes. NumPy also allows you to write this using dots as
``b[i, ...]``.

The **dots** (``...``) represent as many colons as needed to produce a
complete indexing tuple. For example, if ``x`` is an array with 5
axes, then

-  ``x[1, 2, ...]`` is equivalent to ``x[1, 2, :, :, :]``,
-  ``x[..., 3]`` to ``x[:, :, :, :, 3]`` and
-  ``x[4, ..., 5, :]`` to ``x[4, :, :, 5, :]``.

::

    >>> c = np.array([[[  0,  1,  2],  # a 3D array (two stacked 2D arrays)
    ...                [ 10, 12, 13]],
    ...               [[100, 101, 102],
    ...                [110, 112, 113]]])
    >>> c.shape
    (2, 2, 3)
    >>> c[1, ...]  # same as c[1, :, :] or c[1]
    array([[100, 101, 102],
           [110, 112, 113]])
    >>> c[..., 2]  # same as c[:, :, 2]
    array([[  2,  13],
           [102, 113]])

**Iterating** over multidimensional arrays is done with respect to the
first axis::

    >>> for row in b:
    ...     print(row)
    ...
    [0 1 2 3]
    [10 11 12 13]
    [20 21 22 23]
    [30 31 32 33]
    [40 41 42 43]

However, if one wants to perform an operation on each element in the
array, one can use the ``flat`` attribute which is an
`iterator <https://docs.python.org/tutorial/classes.html#iterators>`__
over all the elements of the array::

    >>> for element in b.flat:
    ...     print(element)
    ...
    0
    1
    2
    3
    10
    11
    12
    13
    20
    21
    22
    23
    30
    31
    32
    33
    40
    41
    42
    43

.. seealso::

    :ref:`basics.indexing`,
    :ref:`arrays.indexing` (reference),
    `newaxis`,
    `ndenumerate`,
    `indices`

.. _quickstart.shape-manipulation:

Shape Manipulation
==================

Changing the shape of an array
------------------------------

An array has a shape given by the number of elements along each axis::

    >>> a = np.floor(10 * rg.random((3, 4)))
    >>> a
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])
    >>> a.shape
    (3, 4)

The shape of an array can be changed with various commands. Note that the
following three commands all return a modified array, but do not change
the original array::

    >>> a.ravel()  # returns the array, flattened
    array([3., 7., 3., 4., 1., 4., 2., 2., 7., 2., 4., 9.])
    >>> a.reshape(6, 2)  # returns the array with a modified shape
    array([[3., 7.],
           [3., 4.],
           [1., 4.],
           [2., 2.],
           [7., 2.],
           [4., 9.]])
    >>> a.T  # returns the array, transposed
    array([[3., 1., 7.],
           [7., 4., 2.],
           [3., 2., 4.],
           [4., 2., 9.]])
    >>> a.T.shape
    (4, 3)
    >>> a.shape
    (3, 4)

The order of the elements in the array resulting from ``ravel`` is
normally "C-style", that is, the rightmost index "changes the fastest",
so the element after ``a[0, 0]`` is ``a[0, 1]``. If the array is reshaped to some
other shape, again the array is treated as "C-style". NumPy normally
creates arrays stored in this order, so ``ravel`` will usually not need to
copy its argument, but if the array was made by taking slices of another
array or created with unusual options, it may need to be copied. The
functions ``ravel`` and ``reshape`` can also be instructed, using an
optional argument, to use FORTRAN-style arrays, in which the leftmost
index changes the fastest.

The `reshape` function returns its
argument with a modified shape, whereas the
`ndarray.resize` method modifies the array
itself::

    >>> a
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])
    >>> a.resize((2, 6))
    >>> a
    array([[3., 7., 3., 4., 1., 4.],
           [2., 2., 7., 2., 4., 9.]])

If a dimension is given as ``-1`` in a reshaping operation, the other
dimensions are automatically calculated::

    >>> a.reshape(3, -1)
    array([[3., 7., 3., 4.],
           [1., 4., 2., 2.],
           [7., 2., 4., 9.]])

.. seealso::

   `ndarray.shape`,
   `reshape`,
   `resize`,
   `ravel`


.. _quickstart.stacking-arrays:

Stacking together different arrays
----------------------------------

Several arrays can be stacked together along different axes::

    >>> a = np.floor(10 * rg.random((2, 2)))
    >>> a
    array([[9., 7.],
           [5., 2.]])
    >>> b = np.floor(10 * rg.random((2, 2)))
    >>> b
    array([[1., 9.],
           [5., 1.]])
    >>> np.vstack((a, b))
    array([[9., 7.],
           [5., 2.],
           [1., 9.],
           [5., 1.]])
    >>> np.hstack((a, b))
    array([[9., 7., 1., 9.],
           [5., 2., 5., 1.]])

The function `column_stack` stacks 1D arrays as columns into a 2D array.
It is equivalent to `hstack` only for 2D arrays::

    >>> from numpy import newaxis
    >>> np.column_stack((a, b))  # with 2D arrays
    array([[9., 7., 1., 9.],
           [5., 2., 5., 1.]])
    >>> a = np.array([4., 2.])
    >>> b = np.array([3., 8.])
    >>> np.column_stack((a, b))  # returns a 2D array
    array([[4., 3.],
           [2., 8.]])
    >>> np.hstack((a, b))        # the result is different
    array([4., 2., 3., 8.])
    >>> a[:, newaxis]  # view `a` as a 2D column vector
    array([[4.],
           [2.]])
    >>> np.column_stack((a[:, newaxis], b[:, newaxis]))
    array([[4., 3.],
           [2., 8.]])
    >>> np.hstack((a[:, newaxis], b[:, newaxis]))  # the result is the same
    array([[4., 3.],
           [2., 8.]])

On the other hand, the function `row_stack` is equivalent to `vstack`
for any input arrays. In fact, `row_stack` is an alias for `vstack`::

    >>> np.column_stack is np.hstack
    False
    >>> np.row_stack is np.vstack
    True

In general, for arrays with more than two dimensions,
`hstack` stacks along their second
axes, `vstack` stacks along their
first axes, and `concatenate`
allows for an optional arguments giving the number of the axis along
which the concatenation should happen.

**Note**

In complex cases, `r_` and `c_` are useful for creating arrays by stacking
numbers along one axis. They allow the use of range literals ``:``. ::

       >>> np.r_[1:4, 0, 4]
       array([1, 2, 3, 0, 4])

When used with arrays as arguments,
`r_` and
`c_` are similar to
`vstack` and
`hstack` in their default behavior,
but allow for an optional argument giving the number of the axis along
which to concatenate.

.. seealso::

   `hstack`,
   `vstack`,
   `column_stack`,
   `concatenate`,
   `c_`,
   `r_`

Splitting one array into several smaller ones
---------------------------------------------

Using `hsplit`, you can split an
array along its horizontal axis, either by specifying the number of
equally shaped arrays to return, or by specifying the columns after
which the division should occur::

    >>> a = np.floor(10 * rg.random((2, 12)))
    >>> a
    array([[6., 7., 6., 9., 0., 5., 4., 0., 6., 8., 5., 2.],
           [8., 5., 5., 7., 1., 8., 6., 7., 1., 8., 1., 0.]])
    >>> # Split `a` into 3
    >>> np.hsplit(a, 3)
    [array([[6., 7., 6., 9.],
           [8., 5., 5., 7.]]), array([[0., 5., 4., 0.],
           [1., 8., 6., 7.]]), array([[6., 8., 5., 2.],
           [1., 8., 1., 0.]])]
    >>> # Split `a` after the third and the fourth column
    >>> np.hsplit(a, (3, 4))
    [array([[6., 7., 6.],
           [8., 5., 5.]]), array([[9.],
           [7.]]), array([[0., 5., 4., 0., 6., 8., 5., 2.],
           [1., 8., 6., 7., 1., 8., 1., 0.]])]

`vsplit` splits along the vertical
axis, and `array_split` allows
one to specify along which axis to split.


.. _quickstart.copies-and-views:

Copies and Views
================

When operating and manipulating arrays, their data is sometimes copied
into a new array and sometimes not. This is often a source of confusion
for beginners. There are three cases:

No Copy at All
--------------

Simple assignments make no copy of objects or their data.

::

    >>> a = np.array([[ 0,  1,  2,  3],
    ...               [ 4,  5,  6,  7],
    ...               [ 8,  9, 10, 11]])
    >>> b = a            # no new object is created
    >>> b is a           # a and b are two names for the same ndarray object
    True

Python passes mutable objects as references, so function calls make no
copy.

::

    >>> def f(x):
    ...     print(id(x))
    ...
    >>> id(a)  # id is a unique identifier of an object #doctest: +SKIP
    148293216  # may vary
    >>> f(a)   #doctest: +SKIP
    148293216  # may vary

View or Shallow Copy
--------------------

Different array objects can share the same data. The ``view`` method
creates a new array object that looks at the same data.

::

    >>> c = a.view()
    >>> c is a
    False
    >>> c.base is a            # c is a view of the data owned by a
    True
    >>> c.flags.owndata
    False
    >>>
    >>> c = c.reshape((2, 6))  # a's shape doesn't change
    >>> a.shape
    (3, 4)
    >>> c[0, 4] = 1234         # a's data changes
    >>> a
    array([[   0,    1,    2,    3],
           [1234,    5,    6,    7],
           [   8,    9,   10,   11]])

Slicing an array returns a view of it::

    >>> s = a[:, 1:3]
    >>> s[:] = 10  # s[:] is a view of s. Note the difference between s = 10 and s[:] = 10
    >>> a
    array([[   0,   10,   10,    3],
           [1234,   10,   10,    7],
           [   8,   10,   10,   11]])

Deep Copy
---------

The ``copy`` method makes a complete copy of the array and its data.

::

    >>> d = a.copy()  # a new array object with new data is created
    >>> d is a
    False
    >>> d.base is a  # d doesn't share anything with a
    False
    >>> d[0, 0] = 9999
    >>> a
    array([[   0,   10,   10,    3],
           [1234,   10,   10,    7],
           [   8,   10,   10,   11]])


Sometimes ``copy`` should be called after slicing if the original array is not required anymore.
For example, suppose ``a`` is a huge intermediate result and the final result ``b`` only contains
a small fraction of ``a``, a deep copy should be made when constructing ``b`` with slicing::

    >>> a = np.arange(int(1e8))
    >>> b = a[:100].copy()
    >>> del a  # the memory of ``a`` can be released.

If ``b = a[:100]`` is used instead, ``a`` is referenced by ``b`` and will persist in memory
even if ``del a`` is executed.

Functions and Methods Overview
------------------------------

Here is a list of some useful NumPy functions and methods names
ordered in categories. See :ref:`routines` for the full list.

Array Creation
    `arange`,
    `array`,
    `copy`,
    `empty`,
    `empty_like`,
    `eye`,
    `fromfile`,
    `fromfunction`,
    `identity`,
    `linspace`,
    `logspace`,
    `mgrid`,
    `ogrid`,
    `ones`,
    `ones_like`,
    `r_`,
    `zeros`,
    `zeros_like`
Conversions
   `ndarray.astype`,
   `atleast_1d`,
   `atleast_2d`,
   `atleast_3d`,
   `mat`
Manipulations
    `array_split`,
    `column_stack`,
    `concatenate`,
    `diagonal`,
    `dsplit`,
    `dstack`,
    `hsplit`,
    `hstack`,
    `ndarray.item`,
    `newaxis`,
    `ravel`,
    `repeat`,
    `reshape`,
    `resize`,
    `squeeze`,
    `swapaxes`,
    `take`,
    `transpose`,
    `vsplit`,
    `vstack`
Questions
    `all`,
    `any`,
    `nonzero`,
    `where`
Ordering
    `argmax`,
    `argmin`,
    `argsort`,
    `max`,
    `min`,
    `ptp`,
    `searchsorted`,
    `sort`
Operations
    `choose`,
    `compress`,
    `cumprod`,
    `cumsum`,
    `inner`,
    `ndarray.fill`,
    `imag`,
    `prod`,
    `put`,
    `putmask`,
    `real`,
    `sum`
Basic Statistics
    `cov`,
    `mean`,
    `std`,
    `var`
Basic Linear Algebra
    `cross`,
    `dot`,
    `outer`,
    `linalg.svd`,
    `vdot`

Less Basic
==========

.. _broadcasting-rules:

Broadcasting rules
------------------

Broadcasting allows universal functions to deal in a meaningful way with
inputs that do not have exactly the same shape.

The first rule of broadcasting is that if all input arrays do not have
the same number of dimensions, a "1" will be repeatedly prepended to the
shapes of the smaller arrays until all the arrays have the same number
of dimensions.

The second rule of broadcasting ensures that arrays with a size of 1
along a particular dimension act as if they had the size of the array
with the largest shape along that dimension. The value of the array
element is assumed to be the same along that dimension for the
"broadcast" array.

After application of the broadcasting rules, the sizes of all arrays
must match. More details can be found in :ref:`basics.broadcasting`.

Advanced indexing and index tricks
==================================

NumPy offers more indexing facilities than regular Python sequences. In
addition to indexing by integers and slices, as we saw before, arrays
can be indexed by arrays of integers and arrays of booleans.

Indexing with Arrays of Indices
-------------------------------

::

    >>> a = np.arange(12)**2  # the first 12 square numbers
    >>> i = np.array([1, 1, 3, 8, 5])  # an array of indices
    >>> a[i]  # the elements of `a` at the positions `i`
    array([ 1,  1,  9, 64, 25])
    >>> 
    >>> j = np.array([[3, 4], [9, 7]])  # a bidimensional array of indices
    >>> a[j]  # the same shape as `j`
    array([[ 9, 16],
           [81, 49]])

When the indexed array ``a`` is multidimensional, a single array of
indices refers to the first dimension of ``a``. The following example
shows this behavior by converting an image of labels into a color image
using a palette.

::

    >>> palette = np.array([[0, 0, 0],         # black
    ...                     [255, 0, 0],       # red
    ...                     [0, 255, 0],       # green
    ...                     [0, 0, 255],       # blue
    ...                     [255, 255, 255]])  # white
    >>> image = np.array([[0, 1, 2, 0],  # each value corresponds to a color in the palette
    ...                   [0, 3, 4, 0]])
    >>> palette[image]  # the (2, 4, 3) color image
    array([[[  0,   0,   0],
            [255,   0,   0],
            [  0, 255,   0],
            [  0,   0,   0]],
    <BLANKLINE>
           [[  0,   0,   0],
            [  0,   0, 255],
            [255, 255, 255],
            [  0,   0,   0]]])

We can also give indexes for more than one dimension. The arrays of
indices for each dimension must have the same shape.

::

    >>> a = np.arange(12).reshape(3, 4)
    >>> a
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> i = np.array([[0, 1],  # indices for the first dim of `a`
    ...               [1, 2]])
    >>> j = np.array([[2, 1],  # indices for the second dim
    ...               [3, 3]])
    >>> 
    >>> a[i, j]  # i and j must have equal shape
    array([[ 2,  5],
           [ 7, 11]])
    >>> 
    >>> a[i, 2]
    array([[ 2,  6],
           [ 6, 10]])
    >>> 
    >>> a[:, j]
    array([[[ 2,  1],
            [ 3,  3]],
    <BLANKLINE>
           [[ 6,  5],
            [ 7,  7]],
    <BLANKLINE>
           [[10,  9],
            [11, 11]]])

In Python, ``arr[i, j]`` is exactly the same as ``arr[(i, j)]``---so we can
put ``i`` and ``j`` in a ``tuple`` and then do the indexing with that.

::

    >>> l = (i, j)
    >>> # equivalent to a[i, j]
    >>> a[l]
    array([[ 2,  5],
           [ 7, 11]])

However, we can not do this by putting ``i`` and ``j`` into an array,
because this array will be interpreted as indexing the first dimension
of ``a``.

::

    >>> s = np.array([i, j])
    >>> # not what we want
    >>> a[s]
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    IndexError: index 3 is out of bounds for axis 0 with size 3
    >>> # same as `a[i, j]`
    >>> a[tuple(s)]
    array([[ 2,  5],
           [ 7, 11]])

Another common use of indexing with arrays is the search of the maximum
value of time-dependent series::

    >>> time = np.linspace(20, 145, 5)  # time scale
    >>> data = np.sin(np.arange(20)).reshape(5, 4)  # 4 time-dependent series
    >>> time
    array([ 20.  ,  51.25,  82.5 , 113.75, 145.  ])
    >>> data
    array([[ 0.        ,  0.84147098,  0.90929743,  0.14112001],
           [-0.7568025 , -0.95892427, -0.2794155 ,  0.6569866 ],
           [ 0.98935825,  0.41211849, -0.54402111, -0.99999021],
           [-0.53657292,  0.42016704,  0.99060736,  0.65028784],
           [-0.28790332, -0.96139749, -0.75098725,  0.14987721]])
    >>> # index of the maxima for each series
    >>> ind = data.argmax(axis=0)
    >>> ind
    array([2, 0, 3, 1])
    >>> # times corresponding to the maxima
    >>> time_max = time[ind]
    >>> 
    >>> data_max = data[ind, range(data.shape[1])]  # => data[ind[0], 0], data[ind[1], 1]...
    >>> time_max
    array([ 82.5 ,  20.  , 113.75,  51.25])
    >>> data_max
    array([0.98935825, 0.84147098, 0.99060736, 0.6569866 ])
    >>> np.all(data_max == data.max(axis=0))
    True

You can also use indexing with arrays as a target to assign to::

    >>> a = np.arange(5)
    >>> a
    array([0, 1, 2, 3, 4])
    >>> a[[1, 3, 4]] = 0
    >>> a
    array([0, 0, 2, 0, 0])

However, when the list of indices contains repetitions, the assignment
is done several times, leaving behind the last value::

    >>> a = np.arange(5)
    >>> a[[0, 0, 2]] = [1, 2, 3]
    >>> a
    array([2, 1, 3, 3, 4])

This is reasonable enough, but watch out if you want to use Python's
``+=`` construct, as it may not do what you expect::

    >>> a = np.arange(5)
    >>> a[[0, 0, 2]] += 1
    >>> a
    array([1, 1, 3, 3, 4])

Even though 0 occurs twice in the list of indices, the 0th element is
only incremented once. This is because Python requires ``a += 1`` to be
equivalent to ``a = a + 1``.

Indexing with Boolean Arrays
----------------------------

When we index arrays with arrays of (integer) indices we are providing
the list of indices to pick. With boolean indices the approach is
different; we explicitly choose which items in the array we want and
which ones we don't.

The most natural way one can think of for boolean indexing is to use
boolean arrays that have *the same shape* as the original array::

    >>> a = np.arange(12).reshape(3, 4)
    >>> b = a > 4
    >>> b  # `b` is a boolean with `a`'s shape
    array([[False, False, False, False],
           [False,  True,  True,  True],
           [ True,  True,  True,  True]])
    >>> a[b]  # 1d array with the selected elements
    array([ 5,  6,  7,  8,  9, 10, 11])

This property can be very useful in assignments::

    >>> a[b] = 0  # All elements of `a` higher than 4 become 0
    >>> a
    array([[0, 1, 2, 3],
           [4, 0, 0, 0],
           [0, 0, 0, 0]])

You can look at the following
example to see
how to use boolean indexing to generate an image of the `Mandelbrot
set <https://en.wikipedia.org/wiki/Mandelbrot_set>`__:

.. plot::

    >>> import numpy as np
    >>> import matplotlib.pyplot as plt
    >>> def mandelbrot(h, w, maxit=20, r=2):
    ...     """Returns an image of the Mandelbrot fractal of size (h,w)."""
    ...     x = np.linspace(-2.5, 1.5, 4*h+1)
    ...     y = np.linspace(-1.5, 1.5, 3*w+1)
    ...     A, B = np.meshgrid(x, y)
    ...     C = A + B*1j
    ...     z = np.zeros_like(C)
    ...     divtime = maxit + np.zeros(z.shape, dtype=int)
    ...
    ...     for i in range(maxit):
    ...         z = z**2 + C
    ...         diverge = abs(z) > r                    # who is diverging
    ...         div_now = diverge & (divtime == maxit)  # who is diverging now
    ...         divtime[div_now] = i                    # note when
    ...         z[diverge] = r                          # avoid diverging too much
    ...
    ...     return divtime
    >>> plt.clf()
    >>> plt.imshow(mandelbrot(400, 400))

The second way of indexing with booleans is more similar to integer
indexing; for each dimension of the array we give a 1D boolean array
selecting the slices we want::

    >>> a = np.arange(12).reshape(3, 4)
    >>> b1 = np.array([False, True, True])         # first dim selection
    >>> b2 = np.array([True, False, True, False])  # second dim selection
    >>> 
    >>> a[b1, :]                                   # selecting rows
    array([[ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> 
    >>> a[b1]                                      # same thing
    array([[ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> 
    >>> a[:, b2]                                   # selecting columns
    array([[ 0,  2],
           [ 4,  6],
           [ 8, 10]])
    >>> 
    >>> a[b1, b2]                                  # a weird thing to do
    array([ 4, 10])

Note that the length of the 1D boolean array must coincide with the
length of the dimension (or axis) you want to slice. In the previous
example, ``b1`` has length 3 (the number of *rows* in ``a``), and
``b2`` (of length 4) is suitable to index the 2nd axis (columns) of
``a``.

The ix_() function
-------------------

The `ix_` function can be used to combine different vectors so as to
obtain the result for each n-uplet. For example, if you want to compute
all the a+b\*c for all the triplets taken from each of the vectors a, b
and c::

    >>> a = np.array([2, 3, 4, 5])
    >>> b = np.array([8, 5, 4])
    >>> c = np.array([5, 4, 6, 8, 3])
    >>> ax, bx, cx = np.ix_(a, b, c)
    >>> ax
    array([[[2]],
    <BLANKLINE>
           [[3]],
    <BLANKLINE>
           [[4]],
    <BLANKLINE>
           [[5]]])
    >>> bx
    array([[[8],
            [5],
            [4]]])
    >>> cx
    array([[[5, 4, 6, 8, 3]]])
    >>> ax.shape, bx.shape, cx.shape
    ((4, 1, 1), (1, 3, 1), (1, 1, 5))
    >>> result = ax + bx * cx
    >>> result
    array([[[42, 34, 50, 66, 26],
            [27, 22, 32, 42, 17],
            [22, 18, 26, 34, 14]],
    <BLANKLINE>
           [[43, 35, 51, 67, 27],
            [28, 23, 33, 43, 18],
            [23, 19, 27, 35, 15]],
    <BLANKLINE>
           [[44, 36, 52, 68, 28],
            [29, 24, 34, 44, 19],
            [24, 20, 28, 36, 16]],
    <BLANKLINE>
           [[45, 37, 53, 69, 29],
            [30, 25, 35, 45, 20],
            [25, 21, 29, 37, 17]]])
    >>> result[3, 2, 4]
    17
    >>> a[3] + b[2] * c[4]
    17

You could also implement the reduce as follows::

    >>> def ufunc_reduce(ufct, *vectors):
    ...    vs = np.ix_(*vectors)
    ...    r = ufct.identity
    ...    for v in vs:
    ...        r = ufct(r, v)
    ...    return r

and then use it as::

    >>> ufunc_reduce(np.add, a, b, c)
    array([[[15, 14, 16, 18, 13],
            [12, 11, 13, 15, 10],
            [11, 10, 12, 14,  9]],
    <BLANKLINE>
           [[16, 15, 17, 19, 14],
            [13, 12, 14, 16, 11],
            [12, 11, 13, 15, 10]],
    <BLANKLINE>
           [[17, 16, 18, 20, 15],
            [14, 13, 15, 17, 12],
            [13, 12, 14, 16, 11]],
    <BLANKLINE>
           [[18, 17, 19, 21, 16],
            [15, 14, 16, 18, 13],
            [14, 13, 15, 17, 12]]])

The advantage of this version of reduce compared to the normal
ufunc.reduce is that it makes use of the
:ref:`broadcasting rules <broadcasting-rules>`
in order to avoid creating an argument array the size of the output
times the number of vectors.

Indexing with strings
---------------------

See :ref:`structured_arrays`.

Tricks and Tips
===============

Here we give a list of short and useful tips.

"Automatic" Reshaping
---------------------

To change the dimensions of an array, you can omit one of the sizes
which will then be deduced automatically::

    >>> a = np.arange(30)
    >>> b = a.reshape((2, -1, 3))  # -1 means "whatever is needed"
    >>> b.shape
    (2, 5, 3)
    >>> b
    array([[[ 0,  1,  2],
            [ 3,  4,  5],
            [ 6,  7,  8],
            [ 9, 10, 11],
            [12, 13, 14]],
    <BLANKLINE>
           [[15, 16, 17],
            [18, 19, 20],
            [21, 22, 23],
            [24, 25, 26],
            [27, 28, 29]]])

Vector Stacking
---------------

How do we construct a 2D array from a list of equally-sized row vectors?
In MATLAB this is quite easy: if ``x`` and ``y`` are two vectors of the
same length you only need do ``m=[x;y]``. In NumPy this works via the
functions ``column_stack``, ``dstack``, ``hstack`` and ``vstack``,
depending on the dimension in which the stacking is to be done. For
example::

    >>> x = np.arange(0, 10, 2)
    >>> y = np.arange(5)
    >>> m = np.vstack([x, y])
    >>> m
    array([[0, 2, 4, 6, 8],
           [0, 1, 2, 3, 4]])
    >>> xy = np.hstack([x, y])
    >>> xy
    array([0, 2, 4, 6, 8, 0, 1, 2, 3, 4])

The logic behind those functions in more than two dimensions can be
strange.

.. seealso::

   :doc:`numpy-for-matlab-users`

Histograms
----------

The NumPy ``histogram`` function applied to an array returns a pair of
vectors: the histogram of the array and a vector of the bin edges. Beware:
``matplotlib`` also has a function to build histograms (called ``hist``,
as in Matlab) that differs from the one in NumPy. The main difference is
that ``pylab.hist`` plots the histogram automatically, while
``numpy.histogram`` only generates the data.

.. plot::

    >>> import numpy as np
    >>> rg = np.random.default_rng(1)
    >>> import matplotlib.pyplot as plt
    >>> # Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2
    >>> mu, sigma = 2, 0.5
    >>> v = rg.normal(mu, sigma, 10000)
    >>> # Plot a normalized histogram with 50 bins
    >>> plt.hist(v, bins=50, density=True)       # matplotlib version (plot)
    (array...)
    >>> # Compute the histogram with numpy and then plot it
    >>> (n, bins) = np.histogram(v, bins=50, density=True)  # NumPy version (no plot)
    >>> plt.plot(.5 * (bins[1:] + bins[:-1]), n) #doctest: +SKIP

With Matplotlib >=3.4 you can also use ``plt.stairs(n, bins)``.


Further reading
===============

-  The `Python tutorial <https://docs.python.org/tutorial/>`__
-  :ref:`reference`
-  `SciPy Tutorial <https://docs.scipy.org/doc/scipy/reference/tutorial/index.html>`__
-  `SciPy Lecture Notes <https://scipy-lectures.org>`__
-  A `matlab, R, IDL, NumPy/SciPy dictionary <http://mathesaurus.sf.net/>`__
-  :doc:`tutorial-svd <content/tutorial-svd>`
*****************
Beyond the Basics
*****************

|    The voyage of discovery is not in seeking new landscapes but in having
|    new eyes.
|    --- *Marcel Proust*

|    Discovery is seeing what everyone else has seen and thinking what no
|    one else has thought.
|    --- *Albert Szent-Gyorgi*


Iterating over elements in the array
====================================

.. _`sec:array_iterator`:

Basic Iteration
---------------

One common algorithmic requirement is to be able to walk over all
elements in a multidimensional array. The array iterator object makes
this easy to do in a generic way that works for arrays of any
dimension. Naturally, if you know the number of dimensions you will be
using, then you can always write nested for loops to accomplish the
iteration. If, however, you want to write code that works with any
number of dimensions, then you can make use of the array iterator. An
array iterator object is returned when accessing the .flat attribute
of an array.

.. index::
   single: array iterator

Basic usage is to call :c:func:`PyArray_IterNew` ( ``array`` ) where array
is an ndarray object (or one of its sub-classes). The returned object
is an array-iterator object (the same object returned by the .flat
attribute of the ndarray). This object is usually cast to
PyArrayIterObject* so that its members can be accessed. The only
members that are needed are ``iter->size`` which contains the total
size of the array, ``iter->index``, which contains the current 1-d
index into the array, and ``iter->dataptr`` which is a pointer to the
data for the current element of the array.  Sometimes it is also
useful to access ``iter->ao`` which is a pointer to the underlying
ndarray object.

After processing data at the current element of the array, the next
element of the array can be obtained using the macro
:c:func:`PyArray_ITER_NEXT` ( ``iter`` ). The iteration always proceeds in a
C-style contiguous fashion (last index varying the fastest). The
:c:func:`PyArray_ITER_GOTO` ( ``iter``, ``destination`` ) can be used to
jump to a particular point in the array, where ``destination`` is an
array of npy_intp data-type with space to handle at least the number
of dimensions in the underlying array. Occasionally it is useful to
use :c:func:`PyArray_ITER_GOTO1D` ( ``iter``, ``index`` ) which will jump
to the 1-d index given by the value of ``index``. The most common
usage, however, is given in the following example.

.. code-block:: c

    PyObject *obj; /* assumed to be some ndarray object */
    PyArrayIterObject *iter;
    ...
    iter = (PyArrayIterObject *)PyArray_IterNew(obj);
    if (iter == NULL) goto fail;   /* Assume fail has clean-up code */
    while (iter->index < iter->size) {
        /* do something with the data at it->dataptr */
        PyArray_ITER_NEXT(it);
    }
    ...

You can also use :c:func:`PyArrayIter_Check` ( ``obj`` ) to ensure you have
an iterator object and :c:func:`PyArray_ITER_RESET` ( ``iter`` ) to reset an
iterator object back to the beginning of the array.

It should be emphasized at this point that you may not need the array
iterator if your array is already contiguous (using an array iterator
will work but will be slower than the fastest code you could write).
The major purpose of array iterators is to encapsulate iteration over
N-dimensional arrays with arbitrary strides. They are used in many,
many places in the NumPy source code itself. If you already know your
array is contiguous (Fortran or C), then simply adding the element-
size to a running pointer variable will step you through the array
very efficiently. In other words, code like this will probably be
faster for you in the contiguous case (assuming doubles).

.. code-block:: c

    npy_intp size;
    double *dptr;  /* could make this any variable type */
    size = PyArray_SIZE(obj);
    dptr = PyArray_DATA(obj);
    while(size--) {
       /* do something with the data at dptr */
       dptr++;
    }


Iterating over all but one axis
-------------------------------

A common algorithm is to loop over all elements of an array and
perform some function with each element by issuing a function call. As
function calls can be time consuming, one way to speed up this kind of
algorithm is to write the function so it takes a vector of data and
then write the iteration so the function call is performed for an
entire dimension of data at a time. This increases the amount of work
done per function call, thereby reducing the function-call over-head
to a small(er) fraction of the total time. Even if the interior of the
loop is performed without a function call it can be advantageous to
perform the inner loop over the dimension with the highest number of
elements to take advantage of speed enhancements available on micro-
processors that use pipelining to enhance fundamental operations.

The :c:func:`PyArray_IterAllButAxis` ( ``array``, ``&dim`` ) constructs an
iterator object that is modified so that it will not iterate over the
dimension indicated by dim. The only restriction on this iterator
object, is that the :c:func:`PyArray_ITER_GOTO1D` ( ``it``, ``ind`` ) macro
cannot be used (thus flat indexing won't work either if you pass this
object back to Python --- so you shouldn't do this). Note that the
returned object from this routine is still usually cast to
PyArrayIterObject \*. All that's been done is to modify the strides
and dimensions of the returned iterator to simulate iterating over
array[...,0,...] where 0 is placed on the
:math:`\textrm{dim}^{\textrm{th}}` dimension. If dim is negative, then
the dimension with the largest axis is found and used.


Iterating over multiple arrays
------------------------------

Very often, it is desirable to iterate over several arrays at the
same time. The universal functions are an example of this kind of
behavior. If all you want to do is iterate over arrays with the same
shape, then simply creating several iterator objects is the standard
procedure. For example, the following code iterates over two arrays
assumed to be the same shape and size (actually obj1 just has to have
at least as many total elements as does obj2):

.. code-block:: c

    /* It is already assumed that obj1 and obj2
       are ndarrays of the same shape and size.
    */
    iter1 = (PyArrayIterObject *)PyArray_IterNew(obj1);
    if (iter1 == NULL) goto fail;
    iter2 = (PyArrayIterObject *)PyArray_IterNew(obj2);
    if (iter2 == NULL) goto fail;  /* assume iter1 is DECREF'd at fail */
    while (iter2->index < iter2->size)  {
        /* process with iter1->dataptr and iter2->dataptr */
        PyArray_ITER_NEXT(iter1);
        PyArray_ITER_NEXT(iter2);
    }


Broadcasting over multiple arrays
---------------------------------

.. index::
   single: broadcasting

When multiple arrays are involved in an operation, you may want to use the
same broadcasting rules that the math operations (*i.e.* the ufuncs) use.
This can be done easily using the :c:type:`PyArrayMultiIterObject`.  This is
the object returned from the Python command numpy.broadcast and it is almost
as easy to use from C. The function
:c:func:`PyArray_MultiIterNew` ( ``n``, ``...`` ) is used (with ``n`` input
objects in place of ``...`` ). The input objects can be arrays or anything
that can be converted into an array. A pointer to a PyArrayMultiIterObject is
returned.  Broadcasting has already been accomplished which adjusts the
iterators so that all that needs to be done to advance to the next element in
each array is for PyArray_ITER_NEXT to be called for each of the inputs. This
incrementing is automatically performed by
:c:func:`PyArray_MultiIter_NEXT` ( ``obj`` ) macro (which can handle a
multiterator ``obj`` as either a :c:expr:`PyArrayMultiIterObject *` or a
:c:expr:`PyObject *`). The data from input number ``i`` is available using
:c:func:`PyArray_MultiIter_DATA` ( ``obj``, ``i`` ). An example of using this
feature follows.

.. code-block:: c

    mobj = PyArray_MultiIterNew(2, obj1, obj2);
    size = mobj->size;
    while(size--) {
        ptr1 = PyArray_MultiIter_DATA(mobj, 0);
        ptr2 = PyArray_MultiIter_DATA(mobj, 1);
        /* code using contents of ptr1 and ptr2 */
        PyArray_MultiIter_NEXT(mobj);
    }

The function :c:func:`PyArray_RemoveSmallest` ( ``multi`` ) can be used to
take a multi-iterator object and adjust all the iterators so that
iteration does not take place over the largest dimension (it makes
that dimension of size 1). The code being looped over that makes use
of the pointers will very-likely also need the strides data for each
of the iterators. This information is stored in
multi->iters[i]->strides.

.. index::
   single: array iterator

There are several examples of using the multi-iterator in the NumPy
source code as it makes N-dimensional broadcasting-code very simple to
write. Browse the source for more examples.

.. _user.user-defined-data-types:

User-defined data-types
=======================

NumPy comes with 24 builtin data-types. While this covers a large
majority of possible use cases, it is conceivable that a user may have
a need for an additional data-type. There is some support for adding
an additional data-type into the NumPy system. This additional data-
type will behave much like a regular data-type except ufuncs must have
1-d loops registered to handle it separately. Also checking for
whether or not other data-types can be cast "safely" to and from this
new type or not will always return "can cast" unless you also register
which types your new data-type can be cast to and from.

The NumPy source code includes an example of a custom data-type as part
of its test suite. The file ``_rational_tests.c.src`` in the source code
directory  ``numpy/numpy/core/src/umath/`` contains an implementation of
a data-type that represents a rational number as the ratio of two 32 bit
integers.

.. index::
   pair: dtype; adding new


Adding the new data-type
------------------------

To begin to make use of the new data-type, you need to first define a
new Python type to hold the scalars of your new data-type. It should
be acceptable to inherit from one of the array scalars if your new
type has a binary compatible layout. This will allow your new data
type to have the methods and attributes of array scalars. New data-
types must have a fixed memory size (if you want to define a data-type
that needs a flexible representation, like a variable-precision
number, then use a pointer to the object as the data-type). The memory
layout of the object structure for the new Python type must be
PyObject_HEAD followed by the fixed-size memory needed for the data-
type. For example, a suitable structure for the new Python type is:

.. code-block:: c

    typedef struct {
       PyObject_HEAD;
       some_data_type obval;
       /* the name can be whatever you want */
    } PySomeDataTypeObject;

After you have defined a new Python type object, you must then define
a new :c:type:`PyArray_Descr` structure whose typeobject member will contain a
pointer to the data-type you've just defined. In addition, the
required functions in the ".f" member must be defined: nonzero,
copyswap, copyswapn, setitem, getitem, and cast. The more functions in
the ".f" member you define, however, the more useful the new data-type
will be.  It is very important to initialize unused functions to NULL.
This can be achieved using :c:func:`PyArray_InitArrFuncs` (f).

Once a new :c:type:`PyArray_Descr` structure is created and filled with the
needed information and useful functions you call
:c:func:`PyArray_RegisterDataType` (new_descr). The return value from this
call is an integer providing you with a unique type_number that
specifies your data-type. This type number should be stored and made
available by your module so that other modules can use it to recognize
your data-type (the other mechanism for finding a user-defined
data-type number is to search based on the name of the type-object
associated with the data-type using :c:func:`PyArray_TypeNumFromName` ).


Registering a casting function
------------------------------

You may want to allow builtin (and other user-defined) data-types to
be cast automatically to your data-type. In order to make this
possible, you must register a casting function with the data-type you
want to be able to cast from. This requires writing low-level casting
functions for each conversion you want to support and then registering
these functions with the data-type descriptor. A low-level casting
function has the signature.

.. c:function:: void castfunc( \
        void* from, void* to, npy_intp n, void* fromarr, void* toarr)

    Cast ``n`` elements ``from`` one type ``to`` another. The data to
    cast from is in a contiguous, correctly-swapped and aligned chunk
    of memory pointed to by from. The buffer to cast to is also
    contiguous, correctly-swapped and aligned. The fromarr and toarr
    arguments should only be used for flexible-element-sized arrays
    (string, unicode, void).

An example castfunc is:

.. code-block:: c

    static void
    double_to_float(double *from, float* to, npy_intp n,
                    void* ignore1, void* ignore2) {
        while (n--) {
              (*to++) = (double) *(from++);
        }
    }

This could then be registered to convert doubles to floats using the
code:

.. code-block:: c

    doub = PyArray_DescrFromType(NPY_DOUBLE);
    PyArray_RegisterCastFunc(doub, NPY_FLOAT,
         (PyArray_VectorUnaryFunc *)double_to_float);
    Py_DECREF(doub);


Registering coercion rules
--------------------------

By default, all user-defined data-types are not presumed to be safely
castable to any builtin data-types. In addition builtin data-types are
not presumed to be safely castable to user-defined data-types. This
situation limits the ability of user-defined data-types to participate
in the coercion system used by ufuncs and other times when automatic
coercion takes place in NumPy. This can be changed by registering
data-types as safely castable from a particular data-type object. The
function :c:func:`PyArray_RegisterCanCast` (from_descr, totype_number,
scalarkind) should be used to specify that the data-type object
from_descr can be cast to the data-type with type number
totype_number. If you are not trying to alter scalar coercion rules,
then use :c:enumerator:`NPY_NOSCALAR` for the scalarkind argument.

If you want to allow your new data-type to also be able to share in
the scalar coercion rules, then you need to specify the scalarkind
function in the data-type object's ".f" member to return the kind of
scalar the new data-type should be seen as (the value of the scalar is
available to that function). Then, you can register data-types that
can be cast to separately for each scalar kind that may be returned
from your user-defined data-type. If you don't register scalar
coercion handling, then all of your user-defined data-types will be
seen as :c:enumerator:`NPY_NOSCALAR`.


Registering a ufunc loop
------------------------

You may also want to register low-level ufunc loops for your data-type
so that an ndarray of your data-type can have math applied to it
seamlessly. Registering a new loop with exactly the same arg_types
signature, silently replaces any previously registered loops for that
data-type.

Before you can register a 1-d loop for a ufunc, the ufunc must be
previously created. Then you call :c:func:`PyUFunc_RegisterLoopForType`
(...) with the information needed for the loop. The return value of
this function is ``0`` if the process was successful and ``-1`` with
an error condition set if it was not successful.

.. index::
   pair: dtype; adding new


Subtyping the ndarray in C
==========================

One of the lesser-used features that has been lurking in Python since
2.2 is the ability to sub-class types in C. This facility is one of
the important reasons for basing NumPy off of the Numeric code-base
which was already in C. A sub-type in C allows much more flexibility
with regards to memory management. Sub-typing in C is not difficult
even if you have only a rudimentary understanding of how to create new
types for Python. While it is easiest to sub-type from a single parent
type, sub-typing from multiple parent types is also possible. Multiple
inheritance in C is generally less useful than it is in Python because
a restriction on Python sub-types is that they have a binary
compatible memory layout. Perhaps for this reason, it is somewhat
easier to sub-type from a single parent type.

.. index::
   pair: ndarray; subtyping

All C-structures corresponding to Python objects must begin with
:c:macro:`PyObject_HEAD` (or :c:macro:`PyObject_VAR_HEAD`). In the same
way, any sub-type must have a C-structure that begins with exactly the
same memory layout as the parent type (or all of the parent types in
the case of multiple-inheritance). The reason for this is that Python
may attempt to access a member of the sub-type structure as if it had
the parent structure ( *i.e.* it will cast a given pointer to a
pointer to the parent structure and then dereference one of it's
members). If the memory layouts are not compatible, then this attempt
will cause unpredictable behavior (eventually leading to a memory
violation and program crash).

One of the elements in :c:macro:`PyObject_HEAD` is a pointer to a
type-object structure. A new Python type is created by creating a new
type-object structure and populating it with functions and pointers to
describe the desired behavior of the type. Typically, a new
C-structure is also created to contain the instance-specific
information needed for each object of the type as well. For example,
:c:data:`&PyArray_Type<PyArray_Type>` is a pointer to the type-object table for the ndarray
while a :c:expr:`PyArrayObject *` variable is a pointer to a particular instance
of an ndarray (one of the members of the ndarray structure is, in
turn, a pointer to the type- object table :c:data:`&PyArray_Type<PyArray_Type>`). Finally
:c:func:`PyType_Ready` (<pointer_to_type_object>) must be called for
every new Python type.


Creating sub-types
------------------

To create a sub-type, a similar procedure must be followed except
only behaviors that are different require new entries in the type-
object structure. All other entries can be NULL and will be filled in
by :c:func:`PyType_Ready` with appropriate functions from the parent
type(s). In particular, to create a sub-type in C follow these steps:

1. If needed create a new C-structure to handle each instance of your
   type. A typical C-structure would be:

   .. code-block:: c

        typedef _new_struct {
            PyArrayObject base;
            /* new things here */
        } NewArrayObject;

   Notice that the full PyArrayObject is used as the first entry in order
   to ensure that the binary layout of instances of the new type is
   identical to the PyArrayObject.

2. Fill in a new Python type-object structure with pointers to new
   functions that will over-ride the default behavior while leaving any
   function that should remain the same unfilled (or NULL). The tp_name
   element should be different.

3. Fill in the tp_base member of the new type-object structure with a
   pointer to the (main) parent type object. For multiple-inheritance,
   also fill in the tp_bases member with a tuple containing all of the
   parent objects in the order they should be used to define inheritance.
   Remember, all parent-types must have the same C-structure for multiple
   inheritance to work properly.

4. Call :c:func:`PyType_Ready` (<pointer_to_new_type>). If this function
   returns a negative number, a failure occurred and the type is not
   initialized. Otherwise, the type is ready to be used. It is
   generally important to place a reference to the new type into the
   module dictionary so it can be accessed from Python.

More information on creating sub-types in C can be learned by reading
PEP 253 (available at https://www.python.org/dev/peps/pep-0253).


Specific features of ndarray sub-typing
---------------------------------------

Some special methods and attributes are used by arrays in order to
facilitate the interoperation of sub-types with the base ndarray type.

The __array_finalize\__ method
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. attribute:: ndarray.__array_finalize__

   Several array-creation functions of the ndarray allow
   specification of a particular sub-type to be created. This allows
   sub-types to be handled seamlessly in many routines. When a
   sub-type is created in such a fashion, however, neither the
   __new_\_ method nor the __init\__ method gets called. Instead, the
   sub-type is allocated and the appropriate instance-structure
   members are filled in. Finally, the :obj:`~numpy.class.__array_finalize__`
   attribute is looked-up in the object dictionary. If it is present
   and not None, then it can be either a CObject containing a pointer
   to a :c:func:`PyArray_FinalizeFunc` or it can be a method taking a
   single argument (which could be None)

   If the :obj:`~numpy.class.__array_finalize__` attribute is a CObject, then the pointer
   must be a pointer to a function with the signature:

   .. code-block:: c

       (int) (PyArrayObject *, PyObject *)

   The first argument is the newly created sub-type. The second argument
   (if not NULL) is the "parent" array (if the array was created using
   slicing or some other operation where a clearly-distinguishable parent
   is present). This routine can do anything it wants to. It should
   return a -1 on error and 0 otherwise.

   If the :obj:`~numpy.class.__array_finalize__` attribute is not None nor a CObject,
   then it must be a Python method that takes the parent array as an
   argument (which could be None if there is no parent), and returns
   nothing. Errors in this method will be caught and handled.


The __array_priority\__ attribute
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. attribute:: ndarray.__array_priority__

   This attribute allows simple but flexible determination of which sub-
   type should be considered "primary" when an operation involving two or
   more sub-types arises. In operations where different sub-types are
   being used, the sub-type with the largest :obj:`~numpy.class.__array_priority__`
   attribute will determine the sub-type of the output(s). If two sub-
   types have the same :obj:`~numpy.class.__array_priority__` then the sub-type of the
   first argument determines the output. The default
   :obj:`~numpy.class.__array_priority__` attribute returns a value of 0.0 for the base
   ndarray type and 1.0 for a sub-type. This attribute can also be
   defined by objects that are not sub-types of the ndarray and can be
   used to determine which :obj:`~numpy.class.__array_wrap__` method should be called for
   the return output.

The __array_wrap\__ method
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. attribute:: ndarray.__array_wrap__

   Any class or type can define this method which should take an ndarray
   argument and return an instance of the type. It can be seen as the
   opposite of the :obj:`~numpy.class.__array__` method. This method is used by the
   ufuncs (and other NumPy functions) to allow other objects to pass
   through. For Python >2.4, it can also be used to write a decorator
   that converts a function that works only with ndarrays to one that
   works with any type with :obj:`~numpy.class.__array__` and :obj:`~numpy.class.__array_wrap__` methods.

.. index::
   pair: ndarray; subtyping
.. _how-to-io:

##############################################################################
Reading and writing files
##############################################################################

This page tackles common applications; for the full collection of I/O
routines, see :ref:`routines.io`.


******************************************************************************
Reading text and CSV_ files
******************************************************************************

.. _CSV: https://en.wikipedia.org/wiki/Comma-separated_values

With no missing values
==============================================================================

Use :func:`numpy.loadtxt`.

With missing values
==============================================================================

Use :func:`numpy.genfromtxt`.

:func:`numpy.genfromtxt` will either

  - return a :ref:`masked array<maskedarray.generic>`
    **masking out missing values** (if ``usemask=True``), or

  - **fill in the missing value** with the value specified in
    ``filling_values`` (default is ``np.nan`` for float, -1 for int).

With non-whitespace delimiters
------------------------------------------------------------------------------
::

    >>> print(open("csv.txt").read())  # doctest: +SKIP
    1, 2, 3
    4,, 6
    7, 8, 9


Masked-array output
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
::

    >>> np.genfromtxt("csv.txt", delimiter=",", usemask=True)  # doctest: +SKIP
    masked_array(
      data=[[1.0, 2.0, 3.0],
            [4.0, --, 6.0],
            [7.0, 8.0, 9.0]],
      mask=[[False, False, False],
            [False,  True, False],
            [False, False, False]],
      fill_value=1e+20)

Array output
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
::

    >>> np.genfromtxt("csv.txt", delimiter=",")  # doctest: +SKIP
    array([[ 1.,  2.,  3.],
           [ 4., nan,  6.],
           [ 7.,  8.,  9.]])

Array output, specified fill-in value
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
::

    >>> np.genfromtxt("csv.txt", delimiter=",", dtype=np.int8, filling_values=99)  # doctest: +SKIP
    array([[ 1,  2,  3],
           [ 4, 99,  6],
           [ 7,  8,  9]], dtype=int8)

Whitespace-delimited
-------------------------------------------------------------------------------

:func:`numpy.genfromtxt` can also parse whitespace-delimited data files
that have missing values if

* **Each field has a fixed width**: Use the width as the `delimiter` argument.
  ::

    # File with width=4. The data does not have to be justified (for example,
    # the 2 in row 1), the last column can be less than width (for example, the 6
    # in row 2), and no delimiting character is required (for instance 8888 and 9
    # in row 3)

    >>> f = open("fixedwidth.txt").read()  # doctest: +SKIP
    >>> print(f)  # doctest: +SKIP
    1   2      3
    44      6
    7   88889

    # Showing spaces as ^
    >>> print(f.replace(" ","^"))  # doctest: +SKIP
    1^^^2^^^^^^3
    44^^^^^^6
    7^^^88889

    >>> np.genfromtxt("fixedwidth.txt", delimiter=4)  # doctest: +SKIP
    array([[1.000e+00, 2.000e+00, 3.000e+00],
           [4.400e+01,       nan, 6.000e+00],
           [7.000e+00, 8.888e+03, 9.000e+00]])

* **A special value (e.g. "x") indicates a missing field**: Use it as the
  `missing_values` argument.
  ::

    >>> print(open("nan.txt").read())  # doctest: +SKIP
    1 2 3
    44 x 6
    7  8888 9

    >>> np.genfromtxt("nan.txt", missing_values="x")  # doctest: +SKIP
    array([[1.000e+00, 2.000e+00, 3.000e+00],
           [4.400e+01,       nan, 6.000e+00],
           [7.000e+00, 8.888e+03, 9.000e+00]])

* **You want to skip the rows with missing values**: Set
  `invalid_raise=False`.
  ::

    >>> print(open("skip.txt").read())  # doctest: +SKIP
    1 2   3
    44    6
    7 888 9

    >>> np.genfromtxt("skip.txt", invalid_raise=False)  # doctest: +SKIP
    __main__:1: ConversionWarning: Some errors were detected !
        Line #2 (got 2 columns instead of 3)
    array([[  1.,   2.,   3.],
           [  7., 888.,   9.]])


* **The delimiter whitespace character is different from the whitespace that
  indicates missing data**. For instance, if columns are delimited by ``\t``,
  then missing data will be recognized if it consists of one
  or more spaces.
  ::

    >>> f = open("tabs.txt").read()  # doctest: +SKIP
    >>> print(f)  # doctest: +SKIP
    1       2       3
    44              6
    7       888     9

    # Tabs vs. spaces
    >>> print(f.replace("\t","^"))  # doctest: +SKIP
    1^2^3
    44^ ^6
    7^888^9

    >>> np.genfromtxt("tabs.txt", delimiter="\t", missing_values=" +")  # doctest: +SKIP
    array([[  1.,   2.,   3.],
           [ 44.,  nan,   6.],
           [  7., 888.,   9.]])

******************************************************************************
Read a file in .npy or .npz format
******************************************************************************

Choices:

  - Use :func:`numpy.load`. It can read files generated by any of
    :func:`numpy.save`, :func:`numpy.savez`, or :func:`numpy.savez_compressed`.

  - Use memory mapping. See `numpy.lib.format.open_memmap`.

******************************************************************************
Write to a file to be read back by NumPy
******************************************************************************

Binary
===============================================================================

Use
:func:`numpy.save`, or to store multiple arrays :func:`numpy.savez`
or :func:`numpy.savez_compressed`.

For :ref:`security and portability <how-to-io-pickle-file>`, set
``allow_pickle=False`` unless the dtype contains Python objects, which
requires pickling.

Masked arrays :any:`can't currently be saved <MaskedArray.tofile>`,
nor can other arbitrary array subclasses.

Human-readable
==============================================================================

:func:`numpy.save` and :func:`numpy.savez` create binary files. To **write a
human-readable file**, use :func:`numpy.savetxt`. The array can only be 1- or
2-dimensional, and there's no ` savetxtz` for multiple files.

Large arrays
==============================================================================

See :ref:`how-to-io-large-arrays`.

******************************************************************************
Read an arbitrarily formatted binary file ("binary blob")
******************************************************************************

Use a :doc:`structured array <basics.rec>`.

**Example:**

The ``.wav`` file header is a 44-byte block preceding ``data_size`` bytes of the
actual sound data::

    chunk_id         "RIFF"
    chunk_size       4-byte unsigned little-endian integer
    format           "WAVE"
    fmt_id           "fmt "
    fmt_size         4-byte unsigned little-endian integer
    audio_fmt        2-byte unsigned little-endian integer
    num_channels     2-byte unsigned little-endian integer
    sample_rate      4-byte unsigned little-endian integer
    byte_rate        4-byte unsigned little-endian integer
    block_align      2-byte unsigned little-endian integer
    bits_per_sample  2-byte unsigned little-endian integer
    data_id          "data"
    data_size        4-byte unsigned little-endian integer

The ``.wav`` file header as a NumPy structured dtype::

    wav_header_dtype = np.dtype([
        ("chunk_id", (bytes, 4)), # flexible-sized scalar type, item size 4
        ("chunk_size", "<u4"),    # little-endian unsigned 32-bit integer
        ("format", "S4"),         # 4-byte string, alternate spelling of (bytes, 4)
        ("fmt_id", "S4"),
        ("fmt_size", "<u4"),
        ("audio_fmt", "<u2"),     #
        ("num_channels", "<u2"),  # .. more of the same ...
        ("sample_rate", "<u4"),   #
        ("byte_rate", "<u4"),
        ("block_align", "<u2"),
        ("bits_per_sample", "<u2"),
        ("data_id", "S4"),
        ("data_size", "<u4"),
        #
        # the sound data itself cannot be represented here:
        # it does not have a fixed size
    ])

    header = np.fromfile(f, dtype=wave_header_dtype, count=1)[0]

This ``.wav`` example is for illustration; to read a ``.wav`` file in real
life, use Python's built-in module :mod:`wave`.

(Adapted from Pauli Virtanen, :ref:`advanced_numpy`, licensed
under `CC BY 4.0 <https://creativecommons.org/licenses/by/4.0/>`_.)

.. _how-to-io-large-arrays:

******************************************************************************
Write or read large arrays
******************************************************************************

**Arrays too large to fit in memory** can be treated like ordinary in-memory
arrays using memory mapping.

- Raw array data written with :func:`numpy.ndarray.tofile` or
  :func:`numpy.ndarray.tobytes` can be read with :func:`numpy.memmap`::

      array = numpy.memmap("mydata/myarray.arr", mode="r", dtype=np.int16, shape=(1024, 1024))

- Files output by :func:`numpy.save` (that is, using the numpy format) can be read
  using :func:`numpy.load` with the ``mmap_mode`` keyword argument::

      large_array[some_slice] = np.load("path/to/small_array", mmap_mode="r")

Memory mapping lacks features like data chunking and compression; more
full-featured formats and libraries usable with NumPy include:

* **HDF5**: `h5py <https://www.h5py.org/>`_ or `PyTables <https://www.pytables.org/>`_.
* **Zarr**: `here <https://zarr.readthedocs.io/en/stable/tutorial.html#reading-and-writing-data>`_.
* **NetCDF**: :class:`scipy.io.netcdf_file`.

For tradeoffs among memmap, Zarr, and HDF5, see
`pythonspeed.com <https://pythonspeed.com/articles/mmap-vs-zarr-hdf5/>`_.

******************************************************************************
Write files for reading by other (non-NumPy) tools
******************************************************************************

Formats for **exchanging data** with other tools include HDF5, Zarr, and
NetCDF (see :ref:`how-to-io-large-arrays`).

******************************************************************************
Write or read a JSON file
******************************************************************************

NumPy arrays are **not** directly
`JSON serializable <https://github.com/numpy/numpy/issues/12481>`_.


.. _how-to-io-pickle-file:

******************************************************************************
Save/restore using a pickle file
******************************************************************************

Avoid when possible; :doc:`pickles <python:library/pickle>` are not secure
against erroneous or maliciously constructed data.

Use :func:`numpy.save` and :func:`numpy.load`.  Set ``allow_pickle=False``,
unless the array dtype includes Python objects, in which case pickling is
required.

******************************************************************************
Convert from a pandas DataFrame to a NumPy array
******************************************************************************

See :meth:`pandas.DataFrame.to_numpy`.

******************************************************************************
 Save/restore using `~numpy.ndarray.tofile` and `~numpy.fromfile`
******************************************************************************

In general, prefer :func:`numpy.save` and :func:`numpy.load`.

:func:`numpy.ndarray.tofile` and :func:`numpy.fromfile` lose information on
endianness and precision and so are unsuitable for anything but scratch
storage.

.. _basics.dispatch:

*******************************
Writing custom array containers
*******************************

Numpy's dispatch mechanism, introduced in numpy version v1.16 is the
recommended approach for writing custom N-dimensional array containers that are
compatible with the numpy API and provide custom implementations of numpy
functionality. Applications include `dask <http://dask.pydata.org>`_ arrays, an
N-dimensional array distributed across multiple nodes, and `cupy
<https://docs-cupy.chainer.org/en/stable/>`_ arrays, an N-dimensional array on
a GPU.

To get a feel for writing custom array containers, we'll begin with a simple
example that has rather narrow utility but illustrates the concepts involved.

>>> import numpy as np
>>> class DiagonalArray:
...     def __init__(self, N, value):
...         self._N = N
...         self._i = value
...     def __repr__(self):
...         return f"{self.__class__.__name__}(N={self._N}, value={self._i})"
...     def __array__(self, dtype=None):
...         return self._i * np.eye(self._N, dtype=dtype)

Our custom array can be instantiated like:

>>> arr = DiagonalArray(5, 1)
>>> arr
DiagonalArray(N=5, value=1)

We can convert to a numpy array using :func:`numpy.array` or
:func:`numpy.asarray`, which will call its ``__array__`` method to obtain a
standard ``numpy.ndarray``.

>>> np.asarray(arr)
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])

If we operate on ``arr`` with a numpy function, numpy will again use the
``__array__`` interface to convert it to an array and then apply the function
in the usual way.

>>> np.multiply(arr, 2)
array([[2., 0., 0., 0., 0.],
       [0., 2., 0., 0., 0.],
       [0., 0., 2., 0., 0.],
       [0., 0., 0., 2., 0.],
       [0., 0., 0., 0., 2.]])


Notice that the return type is a standard ``numpy.ndarray``.

>>> type(np.multiply(arr, 2))
<class 'numpy.ndarray'>

How can we pass our custom array type through this function? Numpy allows a
class to indicate that it would like to handle computations in a custom-defined
way through the interfaces ``__array_ufunc__`` and ``__array_function__``. Let's
take one at a time, starting with ``_array_ufunc__``. This method covers
:ref:`ufuncs`, a class of functions that includes, for example,
:func:`numpy.multiply` and :func:`numpy.sin`.

The ``__array_ufunc__`` receives:

- ``ufunc``, a function like ``numpy.multiply``
- ``method``, a string, differentiating between ``numpy.multiply(...)`` and
  variants like ``numpy.multiply.outer``, ``numpy.multiply.accumulate``, and so
  on.  For the common case, ``numpy.multiply(...)``, ``method == '__call__'``.
- ``inputs``, which could be a mixture of different types
- ``kwargs``, keyword arguments passed to the function

For this example we will only handle the method ``__call__``

>>> from numbers import Number
>>> class DiagonalArray:
...     def __init__(self, N, value):
...         self._N = N
...         self._i = value
...     def __repr__(self):
...         return f"{self.__class__.__name__}(N={self._N}, value={self._i})"
...     def __array__(self, dtype=None):
...         return self._i * np.eye(self._N, dtype=dtype)
...     def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
...         if method == '__call__':
...             N = None
...             scalars = []
...             for input in inputs:
...                 if isinstance(input, Number):
...                     scalars.append(input)
...                 elif isinstance(input, self.__class__):
...                     scalars.append(input._i)
...                     if N is not None:
...                         if N != self._N:
...                             raise TypeError("inconsistent sizes")
...                     else:
...                         N = self._N
...                 else:
...                     return NotImplemented
...             return self.__class__(N, ufunc(*scalars, **kwargs))
...         else:
...             return NotImplemented

Now our custom array type passes through numpy functions.

>>> arr = DiagonalArray(5, 1)
>>> np.multiply(arr, 3)
DiagonalArray(N=5, value=3)
>>> np.add(arr, 3)
DiagonalArray(N=5, value=4)
>>> np.sin(arr)
DiagonalArray(N=5, value=0.8414709848078965)

At this point ``arr + 3`` does not work.

>>> arr + 3
Traceback (most recent call last):
...
TypeError: unsupported operand type(s) for +: 'DiagonalArray' and 'int'

To support it, we need to define the Python interfaces ``__add__``, ``__lt__``,
and so on to dispatch to the corresponding ufunc. We can achieve this
conveniently by inheriting from the mixin
:class:`~numpy.lib.mixins.NDArrayOperatorsMixin`.

>>> import numpy.lib.mixins
>>> class DiagonalArray(numpy.lib.mixins.NDArrayOperatorsMixin):
...     def __init__(self, N, value):
...         self._N = N
...         self._i = value
...     def __repr__(self):
...         return f"{self.__class__.__name__}(N={self._N}, value={self._i})"
...     def __array__(self, dtype=None):
...         return self._i * np.eye(self._N, dtype=dtype)
...     def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
...         if method == '__call__':
...             N = None
...             scalars = []
...             for input in inputs:
...                 if isinstance(input, Number):
...                     scalars.append(input)
...                 elif isinstance(input, self.__class__):
...                     scalars.append(input._i)
...                     if N is not None:
...                         if N != self._N:
...                             raise TypeError("inconsistent sizes")
...                     else:
...                         N = self._N
...                 else:
...                     return NotImplemented
...             return self.__class__(N, ufunc(*scalars, **kwargs))
...         else:
...             return NotImplemented

>>> arr = DiagonalArray(5, 1)
>>> arr + 3
DiagonalArray(N=5, value=4)
>>> arr > 0
DiagonalArray(N=5, value=True)

Now let's tackle ``__array_function__``. We'll create dict that maps numpy
functions to our custom variants.

>>> HANDLED_FUNCTIONS = {}
>>> class DiagonalArray(numpy.lib.mixins.NDArrayOperatorsMixin):
...     def __init__(self, N, value):
...         self._N = N
...         self._i = value
...     def __repr__(self):
...         return f"{self.__class__.__name__}(N={self._N}, value={self._i})"
...     def __array__(self, dtype=None):
...         return self._i * np.eye(self._N, dtype=dtype)
...     def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
...         if method == '__call__':
...             N = None
...             scalars = []
...             for input in inputs:
...                 # In this case we accept only scalar numbers or DiagonalArrays.
...                 if isinstance(input, Number):
...                     scalars.append(input)
...                 elif isinstance(input, self.__class__):
...                     scalars.append(input._i)
...                     if N is not None:
...                         if N != self._N:
...                             raise TypeError("inconsistent sizes")
...                     else:
...                         N = self._N
...                 else:
...                     return NotImplemented
...             return self.__class__(N, ufunc(*scalars, **kwargs))
...         else:
...             return NotImplemented
...     def __array_function__(self, func, types, args, kwargs):
...         if func not in HANDLED_FUNCTIONS:
...             return NotImplemented
...         # Note: this allows subclasses that don't override
...         # __array_function__ to handle DiagonalArray objects.
...         if not all(issubclass(t, self.__class__) for t in types):
...             return NotImplemented
...         return HANDLED_FUNCTIONS[func](*args, **kwargs)
...

A convenient pattern is to define a decorator ``implements`` that can be used
to add functions to ``HANDLED_FUNCTIONS``.

>>> def implements(np_function):
...    "Register an __array_function__ implementation for DiagonalArray objects."
...    def decorator(func):
...        HANDLED_FUNCTIONS[np_function] = func
...        return func
...    return decorator
...

Now we write implementations of numpy functions for ``DiagonalArray``.
For completeness, to support the usage ``arr.sum()`` add a method ``sum`` that
calls ``numpy.sum(self)``, and the same for ``mean``.

>>> @implements(np.sum)
... def sum(arr):
...     "Implementation of np.sum for DiagonalArray objects"
...     return arr._i * arr._N
...
>>> @implements(np.mean)
... def mean(arr):
...     "Implementation of np.mean for DiagonalArray objects"
...     return arr._i / arr._N
...
>>> arr = DiagonalArray(5, 1)
>>> np.sum(arr)
5
>>> np.mean(arr)
0.2

If the user tries to use any numpy functions not included in
``HANDLED_FUNCTIONS``, a ``TypeError`` will be raised by numpy, indicating that
this operation is not supported. For example, concatenating two
``DiagonalArrays`` does not produce another diagonal array, so it is not
supported.

>>> np.concatenate([arr, arr])
Traceback (most recent call last):
...
TypeError: no implementation found for 'numpy.concatenate' on types that implement __array_function__: [<class '__main__.DiagonalArray'>]

Additionally, our implementations of ``sum`` and ``mean`` do not accept the
optional arguments that numpy's implementation does.

>>> np.sum(arr, axis=0)
Traceback (most recent call last):
...
TypeError: sum() got an unexpected keyword argument 'axis'


The user always has the option of converting to a normal ``numpy.ndarray`` with
:func:`numpy.asarray` and using standard numpy from there.

>>> np.concatenate([np.asarray(arr), np.asarray(arr)])
array([[1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.],
       [1., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0.],
       [0., 0., 1., 0., 0.],
       [0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 1.]])

Refer to the `dask source code <https://github.com/dask/dask>`_ and
`cupy source code <https://github.com/cupy/cupy>`_  for more fully-worked
examples of custom array containers.

See also :doc:`NEP 18<neps:nep-0018-array-function-protocol>`.

******************
NumPy fundamentals
******************

These documents clarify concepts, design decisions, and technical
constraints in NumPy. This is a great place to understand the
fundamental NumPy ideas and philosophy. 

.. toctree::
   :maxdepth: 1

   basics.creation
   basics.indexing
   basics.io
   basics.types
   basics.broadcasting
   basics.byteswapping
   basics.rec
   basics.dispatch
   basics.subclassing
   basics.ufuncs
   basics.copies
.. _arrays.creation:

**************
Array creation
**************

.. seealso:: :ref:`Array creation routines <routines.array-creation>`

Introduction
============

There are 6 general mechanisms for creating arrays:

1) Conversion from other Python structures (i.e. lists and tuples)
2) Intrinsic NumPy array creation functions (e.g. arange, ones, zeros,
   etc.)
3) Replicating, joining, or mutating existing arrays
4) Reading arrays from disk, either from standard or custom formats
5) Creating arrays from raw bytes through the use of strings or buffers
6) Use of special library functions (e.g., random)

You can use these methods to create ndarrays or :ref:`structured_arrays`.
This document will cover general methods for ndarray creation. 

1) Converting Python sequences to NumPy Arrays
==============================================

NumPy arrays can be defined using Python sequences such as lists and
tuples. Lists and tuples are defined using ``[...]`` and ``(...)``,
respectively. Lists and tuples can define ndarray creation:

* a list of numbers will create a 1D array, 
* a list of lists will create a 2D array, 
* further nested lists will create higher-dimensional arrays. In general, any array object is called an **ndarray** in NumPy.

::

  >>> a1D = np.array([1, 2, 3, 4])
  >>> a2D = np.array([[1, 2], [3, 4]])
  >>> a3D = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

When you use :func:`numpy.array` to define a new array, you should
consider the :doc:`dtype <basics.types>` of the elements in the array,
which can be specified explicitly. This feature gives you
more control over the underlying data structures and how the elements
are handled in C/C++ functions. If you are not careful with ``dtype``
assignments, you can get unwanted overflow, as such 

::

  >>> a = np.array([127, 128, 129], dtype=np.int8)
  >>> a
  array([ 127, -128, -127], dtype=int8)

An 8-bit signed integer represents integers from -128 to 127.
Assigning the ``int8`` array to integers outside of this range results
in overflow. This feature can often be misunderstood. If you
perform calculations with mismatching ``dtypes``, you can get unwanted
results,  for example::

    >>> a = np.array([2, 3, 4], dtype=np.uint32)
    >>> b = np.array([5, 6, 7], dtype=np.uint32)
    >>> c_unsigned32 = a - b
    >>> print('unsigned c:', c_unsigned32, c_unsigned32.dtype)
    unsigned c: [4294967293 4294967293 4294967293] uint32
    >>> c_signed32 = a - b.astype(np.int32)
    >>> print('signed c:', c_signed32, c_signed32.dtype)
    signed c: [-3 -3 -3] int64

Notice when you perform operations with two arrays of the same
``dtype``: ``uint32``, the resulting array is the same type. When you
perform operations with different ``dtype``, NumPy will 
assign a new type that satisfies all of the array elements involved in
the computation, here ``uint32`` and ``int32`` can both be represented in
as ``int64``. 

The default NumPy behavior is to create arrays in either 32 or 64-bit signed
integers (platform dependent and matches C int size) or double precision
floating point numbers, int32/int64 and float, respectively. If you expect your
integer arrays to be a specific type, then you need to specify the dtype while
you create the array.

2) Intrinsic NumPy array creation functions
===========================================
..
  40 functions seems like a small number, but the routies.array-creation
  has ~47. I'm sure there are more. 

NumPy has over 40 built-in functions for creating arrays as laid
out in the :ref:`Array creation routines <routines.array-creation>`.
These functions can be split into roughly three categories, based on the
dimension of the array they create:

1) 1D arrays
2) 2D arrays
3) ndarrays

1 - 1D array creation functions
-------------------------------

The 1D array creation functions e.g. :func:`numpy.linspace` and
:func:`numpy.arange` generally need at least two inputs, ``start`` and
``stop``. 

:func:`numpy.arange` creates arrays with regularly incrementing values.
Check the documentation for complete information and examples. A few
examples are shown::

 >>> np.arange(10)
 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
 >>> np.arange(2, 10, dtype=float)
 array([2., 3., 4., 5., 6., 7., 8., 9.])
 >>> np.arange(2, 3, 0.1)
 array([2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9])

Note: best practice for :func:`numpy.arange` is to use integer start, end, and
step values. There are some subtleties regarding ``dtype``. In the second
example, the ``dtype`` is defined. In the third example, the array is
``dtype=float`` to accommodate the step size of ``0.1``. Due to roundoff error,
the ``stop`` value is sometimes included. 

:func:`numpy.linspace` will create arrays with a specified number of elements, and
spaced equally between the specified beginning and end values. For
example: ::

 >>> np.linspace(1., 4., 6)
 array([1. ,  1.6,  2.2,  2.8,  3.4,  4. ])

The advantage of this creation function is that you guarantee the
number of elements and the starting and end point. The previous
``arange(start, stop, step)`` will not include the value ``stop``.

2 - 2D array creation functions
-------------------------------

The 2D array creation functions e.g. :func:`numpy.eye`, :func:`numpy.diag`, and :func:`numpy.vander`
define properties of special matrices represented as 2D arrays. 

``np.eye(n, m)`` defines a 2D identity matrix. The elements where i=j (row index and column index are equal) are 1
and the rest are 0, as such::

 >>> np.eye(3)
 array([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
 >>> np.eye(3, 5)
 array([[1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.]])

:func:`numpy.diag` can define either a square 2D array with given values along
the diagonal *or* if given a 2D array returns a 1D array that is
only the diagonal elements. The two array creation functions can be helpful while
doing linear algebra, as such::
 
 >>> np.diag([1, 2, 3])
 array([[1, 0, 0],
        [0, 2, 0],
        [0, 0, 3]])
 >>> np.diag([1, 2, 3], 1)
 array([[0, 1, 0, 0],
        [0, 0, 2, 0],
        [0, 0, 0, 3],
        [0, 0, 0, 0]])
 >>> a = np.array([[1, 2], [3, 4]])
 >>> np.diag(a)
 array([1, 4])

``vander(x, n)`` defines a Vandermonde matrix as a 2D NumPy array. Each column
of the Vandermonde matrix is a decreasing power of the input 1D array or
list or tuple,
``x`` where the highest polynomial order is ``n-1``. This array creation
routine is helpful in generating linear least squares models, as such::
 
 >>> np.vander(np.linspace(0, 2, 5), 2)
 array([[0. , 1. ],
       [0.5, 1. ],
       [1. , 1. ],
       [1.5, 1. ],
       [2. , 1. ]])
 >>> np.vander([1, 2, 3, 4], 2)
 array([[1, 1],
        [2, 1],
        [3, 1],
        [4, 1]])
 >>> np.vander((1, 2, 3, 4), 4)
 array([[ 1,  1,  1,  1],
        [ 8,  4,  2,  1],
        [27,  9,  3,  1],
        [64, 16,  4,  1]])
 
3 - general ndarray creation functions
--------------------------------------

The ndarray creation functions e.g. :func:`numpy.ones`,
:func:`numpy.zeros`, and :meth:`~numpy.random.Generator.random` define
arrays based upon the desired shape.  The  ndarray creation functions
can create arrays with any dimension by specifying how many dimensions
and length along that dimension in a tuple or list. 

:func:`numpy.zeros` will create an array filled with 0 values with the
specified shape. The default dtype is ``float64``::

 >>> np.zeros((2, 3))
 array([[0., 0., 0.], 
        [0., 0., 0.]])
 >>> np.zeros((2, 3, 2))
 array([[[0., 0.],
         [0., 0.],
         [0., 0.]],
 <BLANKLINE>        
        [[0., 0.],
         [0., 0.],
         [0., 0.]]])

:func:`numpy.ones` will create an array filled with 1 values. It is identical to
``zeros`` in all other respects as such::

 >>> np.ones((2, 3))
 array([[1., 1., 1.], 
        [1., 1., 1.]])
 >>> np.ones((2, 3, 2))
 array([[[1., 1.],
         [1., 1.],
         [1., 1.]],
 <BLANKLINE>
        [[1., 1.],
         [1., 1.],
         [1., 1.]]])

The :meth:`~numpy.random.Generator.random` method of the result of
``default_rng`` will create an array filled with random
values between 0 and 1. It is included with the :func:`numpy.random`
library. Below, two arrays are created with shapes (2,3) and (2,3,2),
respectively. The seed is set to 42 so you can reproduce these
pseudorandom numbers::

 >>> from numpy.random import default_rng
 >>> default_rng(42).random((2,3))
 array([[0.77395605, 0.43887844, 0.85859792],
        [0.69736803, 0.09417735, 0.97562235]])
 >>> default_rng(42).random((2,3,2))
 array([[[0.77395605, 0.43887844],
         [0.85859792, 0.69736803],
         [0.09417735, 0.97562235]],
        [[0.7611397 , 0.78606431],
         [0.12811363, 0.45038594],
         [0.37079802, 0.92676499]]])

:func:`numpy.indices` will create a set of arrays (stacked as a one-higher
dimensioned array), one per dimension with each representing variation in that
dimension: ::

 >>> np.indices((3,3))
 array([[[0, 0, 0], 
         [1, 1, 1], 
         [2, 2, 2]], 
        [[0, 1, 2], 
         [0, 1, 2], 
         [0, 1, 2]]])

This is particularly useful for evaluating functions of multiple dimensions on
a regular grid.

3) Replicating, joining, or mutating existing arrays
====================================================

Once you have created arrays, you can replicate, join, or mutate those
existing arrays to create new arrays. When you assign an array or its
elements to a new variable, you have to explicitly :func:`numpy.copy` the array,
otherwise the variable is a view into the original array. Consider the
following example::

 >>> a = np.array([1, 2, 3, 4, 5, 6])
 >>> b = a[:2]
 >>> b += 1
 >>> print('a =', a, '; b =', b)
 a = [2 3 3 4 5 6] ; b = [2 3]

In this example, you did not create a new array. You created a variable,
``b`` that viewed the first 2 elements of ``a``. When you added 1 to ``b`` you
would get the same result by adding 1 to ``a[:2]``. If you want to create a
*new* array, use the :func:`numpy.copy` array creation routine as such::

 >>> a = np.array([1, 2, 3, 4])
 >>> b = a[:2].copy()
 >>> b += 1
 >>> print('a = ', a, 'b = ', b)
 a =  [1 2 3 4] b =  [2 3]

For more information and examples look at :ref:`Copies and Views
<quickstart.copies-and-views>`.

There are a number of routines to join existing arrays e.g. :func:`numpy.vstack`,
:func:`numpy.hstack`, and :func:`numpy.block`. Here is an example of joining four 2-by-2
arrays into a 4-by-4 array using ``block``::

 >>> A = np.ones((2, 2))
 >>> B = np.eye(2, 2)
 >>> C = np.zeros((2, 2))
 >>> D = np.diag((-3, -4))
 >>> np.block([[A, B], [C, D]])
 array([[ 1.,  1.,  1.,  0.],
        [ 1.,  1.,  0.,  1.],
        [ 0.,  0., -3.,  0.],
        [ 0.,  0.,  0., -4.]])

Other routines use similar syntax to join ndarrays. Check the
routine's documentation for further examples and syntax. 

4) Reading arrays from disk, either from standard or custom formats
===================================================================

This is the most common case of large array creation. The details depend
greatly on the format of data on disk. This section gives general pointers on
how to handle various formats. For more detailed examples of IO look at
:ref:`How to Read and Write files <how-to-io>`. 

Standard Binary Formats
-----------------------

Various fields have standard formats for array data. The following lists the
ones with known Python libraries to read them and return NumPy arrays (there
may be others for which it is possible to read and convert to NumPy arrays so
check the last section as well)
::

 HDF5: h5py
 FITS: Astropy

Examples of formats that cannot be read directly but for which it is not hard to
convert are those formats supported by libraries like PIL (able to read and
write many image formats such as jpg, png, etc).

Common ASCII Formats
--------------------

Delimited files such as comma separated value (csv) and tab separated
value (tsv) files are used for programs like Excel and LabView. Python
functions can read and parse these files line-by-line. NumPy has two
standard routines for importing a file with delimited data :func:`numpy.loadtxt`
and :func:`numpy.genfromtxt`. These functions have more involved use cases in
:doc:`how-to-io`. A simple example given a ``simple.csv``:

.. code-block:: bash

 $ cat simple.csv
 x, y
 0, 0
 1, 1
 2, 4
 3, 9

Importing ``simple.csv`` is accomplished using :func:`loadtxt`::

 >>> np.loadtxt('simple.csv', delimiter = ',', skiprows = 1) # doctest: +SKIP
 array([[0., 0.],
        [1., 1.],
        [2., 4.],
        [3., 9.]])


More generic ASCII files can be read using `scipy.io` and `Pandas
<https://pandas.pydata.org/>`_.

5) Creating arrays from raw bytes through the use of strings or buffers
=======================================================================

There are a variety of approaches one can use. If the file has a relatively
simple format then one can write a simple I/O library and use the NumPy
``fromfile()`` function and ``.tofile()`` method to read and write NumPy arrays
directly (mind your byteorder though!) If a good C or C++ library exists that
read the data, one can wrap that library with a variety of techniques though
that certainly is much more work and requires significantly more advanced
knowledge to interface with C or C++.

6) Use of special library functions (e.g., SciPy, Pandas, and OpenCV)
=====================================================================

NumPy is the fundamental library for array containers in the Python Scientific Computing
stack. Many Python libraries, including SciPy, Pandas, and OpenCV, use NumPy ndarrays
as the common format for data exchange, These libraries can create,
operate on, and work with NumPy arrays. 
**************
I/O with NumPy
**************

.. toctree::
   :maxdepth: 2

   basics.io.genfromtxt
.. _basics.types:

**********
Data types
**********

.. seealso:: :ref:`Data type objects <arrays.dtypes>`

Array types and conversions between types
=========================================

NumPy supports a much greater variety of numerical types than Python does.
This section shows which are available, and how to modify an array's data-type.

The primitive types supported are tied closely to those in C:

.. list-table::
    :header-rows: 1

    * - Numpy type
      - C type
      - Description

    * - `numpy.bool_`
      - ``bool``
      - Boolean (True or False) stored as a byte

    * - `numpy.byte`
      - ``signed char``
      - Platform-defined

    * - `numpy.ubyte`
      - ``unsigned char``
      - Platform-defined

    * - `numpy.short`
      - ``short``
      - Platform-defined

    * - `numpy.ushort`
      - ``unsigned short``
      - Platform-defined

    * - `numpy.intc`
      - ``int``
      - Platform-defined

    * - `numpy.uintc`
      - ``unsigned int``
      - Platform-defined

    * - `numpy.int_`
      - ``long``
      - Platform-defined

    * - `numpy.uint`
      - ``unsigned long``
      - Platform-defined

    * - `numpy.longlong`
      - ``long long``
      - Platform-defined

    * - `numpy.ulonglong`
      - ``unsigned long long``
      - Platform-defined

    * - `numpy.half` / `numpy.float16`
      -
      - Half precision float:
        sign bit, 5 bits exponent, 10 bits mantissa

    * - `numpy.single`
      - ``float``
      - Platform-defined single precision float:
        typically sign bit, 8 bits exponent, 23 bits mantissa

    * - `numpy.double`
      - ``double``
      - Platform-defined double precision float:
        typically sign bit, 11 bits exponent, 52 bits mantissa.

    * - `numpy.longdouble`
      - ``long double``
      - Platform-defined extended-precision float

    * - `numpy.csingle`
      - ``float complex``
      - Complex number, represented by two single-precision floats (real and imaginary components)

    * - `numpy.cdouble`
      - ``double complex``
      - Complex number, represented by two double-precision floats (real and imaginary components).

    * - `numpy.clongdouble`
      - ``long double complex``
      - Complex number, represented by two extended-precision floats (real and imaginary components).


Since many of these have platform-dependent definitions, a set of fixed-size
aliases are provided (See :ref:`sized-aliases`).



NumPy numerical types are instances of ``dtype`` (data-type) objects, each
having unique characteristics.  Once you have imported NumPy using

  ::

    >>> import numpy as np

the dtypes are available as ``np.bool_``, ``np.float32``, etc.

Advanced types, not listed above, are explored in
section :ref:`structured_arrays`.

There are 5 basic numerical types representing booleans (bool), integers (int),
unsigned integers (uint) floating point (float) and complex. Those with numbers
in their name indicate the bitsize of the type (i.e. how many bits are needed
to represent a single value in memory).  Some types, such as ``int`` and
``intp``, have differing bitsizes, dependent on the platforms (e.g. 32-bit
vs. 64-bit machines).  This should be taken into account when interfacing
with low-level code (such as C or Fortran) where the raw memory is addressed.

Data-types can be used as functions to convert python numbers to array scalars
(see the array scalar section for an explanation), python sequences of numbers
to arrays of that type, or as arguments to the dtype keyword that many numpy
functions or methods accept. Some examples::

    >>> import numpy as np
    >>> x = np.float32(1.0)
    >>> x
    1.0
    >>> y = np.int_([1,2,4])
    >>> y
    array([1, 2, 4])
    >>> z = np.arange(3, dtype=np.uint8)
    >>> z
    array([0, 1, 2], dtype=uint8)

Array types can also be referred to by character codes, mostly to retain
backward compatibility with older packages such as Numeric.  Some
documentation may still refer to these, for example::

  >>> np.array([1, 2, 3], dtype='f')
  array([1.,  2.,  3.], dtype=float32)

We recommend using dtype objects instead.

To convert the type of an array, use the .astype() method (preferred) or
the type itself as a function. For example: ::

    >>> z.astype(float)
    array([0.,  1.,  2.])
    >>> np.int8(z)
    array([0, 1, 2], dtype=int8)

Note that, above, we use the *Python* float object as a dtype.  NumPy knows
that ``int`` refers to ``np.int_``, ``bool`` means ``np.bool_``,
that ``float`` is ``np.float_`` and ``complex`` is ``np.complex_``.
The other data-types do not have Python equivalents.

To determine the type of an array, look at the dtype attribute::

    >>> z.dtype
    dtype('uint8')

dtype objects also contain information about the type, such as its bit-width
and its byte-order.  The data type can also be used indirectly to query
properties of the type, such as whether it is an integer::

    >>> d = np.dtype(int)
    >>> d #doctest: +SKIP
    dtype('int32')

    >>> np.issubdtype(d, np.integer)
    True

    >>> np.issubdtype(d, np.floating)
    False


Array Scalars
=============

NumPy generally returns elements of arrays as array scalars (a scalar
with an associated dtype).  Array scalars differ from Python scalars, but
for the most part they can be used interchangeably (the primary
exception is for versions of Python older than v2.x, where integer array
scalars cannot act as indices for lists and tuples).  There are some
exceptions, such as when code requires very specific attributes of a scalar
or when it checks specifically whether a value is a Python scalar. Generally,
problems are easily fixed by explicitly converting array scalars
to Python scalars, using the corresponding Python type function
(e.g., ``int``, ``float``, ``complex``, ``str``, ``unicode``).

The primary advantage of using array scalars is that
they preserve the array type (Python may not have a matching scalar type
available, e.g. ``int16``).  Therefore, the use of array scalars ensures
identical behaviour between arrays and scalars, irrespective of whether the
value is inside an array or not.  NumPy scalars also have many of the same
methods arrays do.

.. _overflow-errors:

Overflow Errors
===============

The fixed size of NumPy numeric types may cause overflow errors when a value
requires more memory than available in the data type. For example, 
`numpy.power` evaluates ``100 ** 8`` correctly for 64-bit integers,
but gives 1874919424 (incorrect) for a 32-bit integer.

    >>> np.power(100, 8, dtype=np.int64)
    10000000000000000
    >>> np.power(100, 8, dtype=np.int32)
    1874919424

The behaviour of NumPy and Python integer types differs significantly for
integer overflows and may confuse users expecting NumPy integers to behave
similar to Python's ``int``. Unlike NumPy, the size of Python's ``int`` is
flexible. This means Python integers may expand to accommodate any integer and
will not overflow.

NumPy provides `numpy.iinfo` and `numpy.finfo` to verify the
minimum or maximum values of NumPy integer and floating point values
respectively ::

    >>> np.iinfo(int) # Bounds of the default integer on this system.
    iinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)
    >>> np.iinfo(np.int32) # Bounds of a 32-bit integer
    iinfo(min=-2147483648, max=2147483647, dtype=int32)
    >>> np.iinfo(np.int64) # Bounds of a 64-bit integer
    iinfo(min=-9223372036854775808, max=9223372036854775807, dtype=int64)

If 64-bit integers are still too small the result may be cast to a
floating point number. Floating point numbers offer a larger, but inexact,
range of possible values.

    >>> np.power(100, 100, dtype=np.int64) # Incorrect even with 64-bit int
    0
    >>> np.power(100, 100, dtype=np.float64)
    1e+200

Extended Precision
==================

Python's floating-point numbers are usually 64-bit floating-point numbers,
nearly equivalent to ``np.float64``. In some unusual situations it may be
useful to use floating-point numbers with more precision. Whether this
is possible in numpy depends on the hardware and on the development
environment: specifically, x86 machines provide hardware floating-point
with 80-bit precision, and while most C compilers provide this as their
``long double`` type, MSVC (standard for Windows builds) makes
``long double`` identical to ``double`` (64 bits). NumPy makes the
compiler's ``long double`` available as ``np.longdouble`` (and
``np.clongdouble`` for the complex numbers). You can find out what your
numpy provides with ``np.finfo(np.longdouble)``.

NumPy does not provide a dtype with more precision than C's
``long double``; in particular, the 128-bit IEEE quad precision
data type (FORTRAN's ``REAL*16``) is not available.

For efficient memory alignment, ``np.longdouble`` is usually stored
padded with zero bits, either to 96 or 128 bits. Which is more efficient
depends on hardware and development environment; typically on 32-bit
systems they are padded to 96 bits, while on 64-bit systems they are
typically padded to 128 bits. ``np.longdouble`` is padded to the system
default; ``np.float96`` and ``np.float128`` are provided for users who
want specific padding. In spite of the names, ``np.float96`` and
``np.float128`` provide only as much precision as ``np.longdouble``,
that is, 80 bits on most x86 machines and 64 bits in standard
Windows builds.

Be warned that even if ``np.longdouble`` offers more precision than
python ``float``, it is easy to lose that extra precision, since
python often forces values to pass through ``float``. For example,
the ``%`` formatting operator requires its arguments to be converted
to standard python types, and it is therefore impossible to preserve
extended precision even if many decimal places are requested. It can
be useful to test your code with the value
``1 + np.finfo(np.longdouble).eps``.


.. sectionauthor:: adapted from "Guide to NumPy" by Travis E. Oliphant

.. _ufuncs-basics:

********************************************
Universal functions (:class:`.ufunc`) basics
********************************************

.. seealso:: :ref:`ufuncs`

.. index: ufunc, universal function, arithmetic, operation

A universal function (or :term:`ufunc` for short) is a function that
operates on :class:`ndarrays <numpy.ndarray>` in an element-by-element fashion,
supporting :ref:`array broadcasting <ufuncs.broadcasting>`, :ref:`type
casting <ufuncs.casting>`, and several other standard features. That
is, a ufunc is a ":term:`vectorized <vectorization>`" wrapper for a function
that takes a fixed number of specific inputs and produces a fixed number of
specific outputs.

In NumPy, universal functions are instances of the
:class:`numpy.ufunc` class. Many of the built-in functions are
implemented in compiled C code. The basic ufuncs operate on scalars, but
there is also a generalized kind for which the basic elements are sub-arrays
(vectors, matrices, etc.), and broadcasting is done over other dimensions.
The simplest example is the addition operator::

    >>> np.array([0,2,3,4]) + np.array([1,1,-1,2])
    array([1, 3, 2, 6])

One can also produce custom :class:`numpy.ufunc` instances using the
:func:`numpy.frompyfunc` factory function.


Ufunc methods
=============

All ufuncs have four methods. They can be found at
:ref:`ufuncs.methods`. However, these methods only make sense on scalar
ufuncs that take two input arguments and return one output argument.
Attempting to call these methods on other ufuncs will cause a
:exc:`ValueError`.

The reduce-like methods all take an *axis* keyword, a *dtype*
keyword, and an *out* keyword, and the arrays must all have dimension >= 1.
The *axis* keyword specifies the axis of the array over which the reduction
will take place (with negative values counting backwards). Generally, it is an
integer, though for :meth:`numpy.ufunc.reduce`, it can also be a tuple of
``int`` to reduce over several axes at once, or ``None``, to reduce over all
axes. For example::

   >>> x = np.arange(9).reshape(3,3)
   >>> x
   array([[0, 1, 2],
         [3, 4, 5],
         [6, 7, 8]])
   >>> np.add.reduce(x, 1)
   array([ 3, 12, 21])
   >>> np.add.reduce(x, (0, 1))
   36

The *dtype* keyword allows you to manage a very common problem that arises
when naively using :meth:`.ufunc.reduce`. Sometimes you may
have an array of a certain data type and wish to add up all of its
elements, but the result does not fit into the data type of the
array. This commonly happens if you have an array of single-byte
integers. The *dtype* keyword allows you to alter the data type over which
the reduction takes place (and therefore the type of the output). Thus,
you can ensure that the output is a data type with precision large enough
to handle your output. The responsibility of altering the reduce type is
mostly up to you. There is one exception: if no *dtype* is given for a
reduction on the "add" or "multiply" operations, then if the input type is
an integer (or Boolean) data-type and smaller than the size of the
:class:`numpy.int_` data type, it will be internally upcast to the :class:`.int_`
(or :class:`numpy.uint`) data-type. In the previous example::

   >>> x.dtype 
   dtype('int64')
   >>> np.multiply.reduce(x, dtype=float)
   array([ 0., 28., 80.])

Finally, the *out* keyword allows you to
provide an output array (for single-output ufuncs, which are currently the only
ones supported; for future extension, however, a tuple with a single argument
can be passed in). If *out* is given, the *dtype* argument is ignored.
Considering ``x`` from the previous example::

   >>> y = np.zeros(3, dtype=int)
   >>> y
   array([0, 0, 0])
   >>> np.multiply.reduce(x, dtype=float, out=y)
   array([ 0, 28, 80])

Ufuncs also have a fifth method, :func:`numpy.ufunc.at`, that allows in place
operations to be performed using advanced indexing. No
:ref:`buffering <use-of-internal-buffers>` is used on the dimensions where
advanced indexing is used, so the advanced index can
list an item more than once and the operation will be performed on the result
of the previous operation for that item.


.. _ufuncs-output-type:

Output type determination
=========================

The output of the ufunc (and its methods) is not necessarily an
:class:`ndarray <numpy.ndarray>`, if all input arguments are not
:class:`ndarrays <numpy.ndarray>`. Indeed, if any input defines an
:obj:`~.class.__array_ufunc__` method,
control will be passed completely to that function, i.e., the ufunc is
:ref:`overridden <ufuncs.overrides>`.

If none of the inputs overrides the ufunc, then
all output arrays will be passed to the
:obj:`~.class.__array_prepare__` and
:obj:`~.class.__array_wrap__` methods of the input (besides
:class:`ndarrays <.ndarray>`, and scalars) that defines it **and** has
the highest :obj:`~.class.__array_priority__`
of any other input to the universal function. The default
:obj:`~.class.__array_priority__` of the
ndarray is 0.0, and the default :obj:`~.class.__array_priority__` of a subtype
is 0.0. Matrices have :obj:`~.class.__array_priority__` equal to 10.0.

All ufuncs can also take output arguments. If necessary, output will
be cast to the data-type(s) of the provided output array(s). If a class
with an :obj:`~.class.__array__` method is used for the output,
results will be written to the object returned by :obj:`~.class.__array__`.
Then, if the class also has an :obj:`~.class.__array_prepare__` method, it is
called so metadata may be determined based on the context of the ufunc (the
context consisting of the ufunc itself, the arguments passed to the ufunc, and
the ufunc domain.) The array object returned by
:obj:`~.class.__array_prepare__` is passed to the ufunc for computation.
Finally, if the class also has an :obj:`~.class.__array_wrap__` method, the
returned :class:`.ndarray` result will be passed to that method just before
passing control back to the caller.

.. _ufuncs.broadcasting:

Broadcasting
============

.. seealso:: :doc:`Broadcasting basics <basics.broadcasting>`

.. index:: broadcasting

Each universal function takes array inputs and produces array outputs
by performing the core function element-wise on the inputs (where an
element is generally a scalar, but can be a vector or higher-order
sub-array for generalized ufuncs). Standard
:ref:`broadcasting rules <general-broadcasting-rules>` are applied
so that inputs not sharing exactly the
same shapes can still be usefully operated on. 

By these rules, if an input has a dimension size of 1 in its shape, the
first data entry in that dimension will be used for all calculations along
that dimension. In other words, the stepping machinery of the
:term:`ufunc` will simply not step along that dimension (the
:ref:`stride <memory-layout>` will be 0 for that dimension).
   

.. _ufuncs.casting:

Type casting rules
==================

.. index::
   pair: ufunc; casting rules

.. note::

   In NumPy 1.6.0, a type promotion API was created to encapsulate the
   mechanism for determining output types. See the functions
   :func:`numpy.result_type`, :func:`numpy.promote_types`, and
   :func:`numpy.min_scalar_type` for more details.

At the core of every ufunc is a one-dimensional strided loop that
implements the actual function for a specific type combination. When a
ufunc is created, it is given a static list of inner loops and a
corresponding list of type signatures over which the ufunc operates.
The ufunc machinery uses this list to determine which inner loop to
use for a particular case. You can inspect the :attr:`.types
<.ufunc.types>` attribute for a particular ufunc to see which type
combinations have a defined inner loop and which output type they
produce (:ref:`character codes <arrays.scalars.character-codes>` are used
in said output for brevity).

Casting must be done on one or more of the inputs whenever the ufunc
does not have a core loop implementation for the input types provided.
If an implementation for the input types cannot be found, then the
algorithm searches for an implementation with a type signature to
which all of the inputs can be cast "safely." The first one it finds
in its internal list of loops is selected and performed, after all
necessary type casting. Recall that internal copies during ufuncs (even
for casting) are limited to the size of an internal buffer (which is user
settable).

.. note::

    Universal functions in NumPy are flexible enough to have mixed type
    signatures. Thus, for example, a universal function could be defined
    that works with floating-point and integer values. See
    :func:`numpy.ldexp` for an example.

By the above description, the casting rules are essentially
implemented by the question of when a data type can be cast "safely"
to another data type. The answer to this question can be determined in
Python with a function call: :func:`can_cast(fromtype, totype)
<numpy.can_cast>`. The example below shows the results of this call for
the 24 internally supported types on the author's 64-bit system. You
can generate this table for your system with the code given in the example.

.. rubric:: Example

Code segment showing the "can cast safely" table for a 64-bit system.
Generally the output depends on the system; your system might result in
a different table.

>>> mark = {False: ' -', True: ' Y'}
>>> def print_table(ntypes):
...     print('X ' + ' '.join(ntypes))
...     for row in ntypes:
...         print(row, end='')
...         for col in ntypes:
...             print(mark[np.can_cast(row, col)], end='')
...         print()
...
>>> print_table(np.typecodes['All'])
X ? b h i l q p B H I L Q P e f d g F D G S U V O M m
? Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y - Y
b - Y Y Y Y Y Y - - - - - - Y Y Y Y Y Y Y Y Y Y Y - Y
h - - Y Y Y Y Y - - - - - - - Y Y Y Y Y Y Y Y Y Y - Y
i - - - Y Y Y Y - - - - - - - - Y Y - Y Y Y Y Y Y - Y
l - - - - Y Y Y - - - - - - - - Y Y - Y Y Y Y Y Y - Y
q - - - - Y Y Y - - - - - - - - Y Y - Y Y Y Y Y Y - Y
p - - - - Y Y Y - - - - - - - - Y Y - Y Y Y Y Y Y - Y
B - - Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y - Y
H - - - Y Y Y Y - Y Y Y Y Y - Y Y Y Y Y Y Y Y Y Y - Y
I - - - - Y Y Y - - Y Y Y Y - - Y Y - Y Y Y Y Y Y - Y
L - - - - - - - - - - Y Y Y - - Y Y - Y Y Y Y Y Y - -
Q - - - - - - - - - - Y Y Y - - Y Y - Y Y Y Y Y Y - -
P - - - - - - - - - - Y Y Y - - Y Y - Y Y Y Y Y Y - -
e - - - - - - - - - - - - - Y Y Y Y Y Y Y Y Y Y Y - -
f - - - - - - - - - - - - - - Y Y Y Y Y Y Y Y Y Y - -
d - - - - - - - - - - - - - - - Y Y - Y Y Y Y Y Y - -
g - - - - - - - - - - - - - - - - Y - - Y Y Y Y Y - -
F - - - - - - - - - - - - - - - - - Y Y Y Y Y Y Y - -
D - - - - - - - - - - - - - - - - - - Y Y Y Y Y Y - -
G - - - - - - - - - - - - - - - - - - - Y Y Y Y Y - -
S - - - - - - - - - - - - - - - - - - - - Y Y Y Y - -
U - - - - - - - - - - - - - - - - - - - - - Y Y Y - -
V - - - - - - - - - - - - - - - - - - - - - - Y Y - -
O - - - - - - - - - - - - - - - - - - - - - - - Y - -
M - - - - - - - - - - - - - - - - - - - - - - Y Y Y -
m - - - - - - - - - - - - - - - - - - - - - - Y Y - Y

You should note that, while included in the table for completeness,
the 'S', 'U', and 'V' types cannot be operated on by ufuncs. Also,
note that on a 32-bit system the integer types may have different
sizes, resulting in a slightly altered table.

Mixed scalar-array operations use a different set of casting rules
that ensure that a scalar cannot "upcast" an array unless the scalar is
of a fundamentally different kind of data (i.e., under a different
hierarchy in the data-type hierarchy) than the array.  This rule
enables you to use scalar constants in your code (which, as Python
types, are interpreted accordingly in ufuncs) without worrying about
whether the precision of the scalar constant will cause upcasting on
your large (small precision) array.

.. _use-of-internal-buffers:

Use of internal buffers
=======================

.. index:: buffers

Internally, buffers are used for misaligned data, swapped data, and
data that has to be converted from one data type to another. The size
of internal buffers is settable on a per-thread basis. There can
be up to :math:`2 (n_{\mathrm{inputs}} + n_{\mathrm{outputs}})`
buffers of the specified size created to handle the data from all the
inputs and outputs of a ufunc. The default size of a buffer is
10,000 elements. Whenever buffer-based calculation would be needed,
but all input arrays are smaller than the buffer size, those
misbehaved or incorrectly-typed arrays will be copied before the
calculation proceeds. Adjusting the size of the buffer may therefore
alter the speed at which ufunc calculations of various sorts are
completed. A simple interface for setting this variable is accessible
using the function :func:`numpy.setbufsize`.


Error handling
==============

.. index:: error handling

Universal functions can trip special floating-point status registers
in your hardware (such as divide-by-zero). If available on your
platform, these registers will be regularly checked during
calculation. Error handling is controlled on a per-thread basis,
and can be configured using the functions :func:`numpy.seterr` and
:func:`numpy.seterrcall`.
   

.. _ufuncs.overrides:

Overriding ufunc behavior
=========================

Classes (including ndarray subclasses) can override how ufuncs act on
them by defining certain special methods.  For details, see
:ref:`arrays.classes`.
.. currentmodule:: numpy

.. _how-to-index.rst:

*****************************************
How to index :class:`ndarrays <.ndarray>`
*****************************************

.. seealso:: :ref:`basics.indexing`

This page tackles common examples. For an in-depth look into indexing, refer
to :ref:`basics.indexing`.

Access specific/arbitrary rows and columns
==========================================

Use :ref:`basic-indexing` features like :ref:`slicing-and-striding`, and
:ref:`dimensional-indexing-tools`.

    >>> a = np.arange(30).reshape(2, 3, 5)
    >>> a
    array([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14]],
    <BLANKLINE>
            [[15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]]])
    >>> a[0, 2, :]
    array([10, 11, 12, 13, 14])
    >>> a[0, :, 3]
    array([ 3,  8, 13])
    
Note that the output from indexing operations can have different shape from the
original object. To preserve the original dimensions after indexing, you can
use :func:`newaxis`. To use other such tools, refer to
:ref:`dimensional-indexing-tools`.

    >>> a[0, :, 3].shape
    (3,)
    >>> a[0, :, 3, np.newaxis].shape
    (3, 1)
    >>> a[0, :, 3, np.newaxis, np.newaxis].shape
    (3, 1, 1)

Variables can also be used to index::

    >>> y = 0
    >>> a[y, :, y+3]
    array([ 3,  8, 13])

Refer to :ref:`dealing-with-variable-indices` to see how to use
:term:`python:slice` and :py:data:`Ellipsis` in your index variables.

Index columns
-------------

To index columns, you have to index the last axis. Use
:ref:`dimensional-indexing-tools` to get the desired number of dimensions::

    >>> a = np.arange(24).reshape(2, 3, 4)
    >>> a
    array([[[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]],
    <BLANKLINE>
           [[12, 13, 14, 15],
            [16, 17, 18, 19],
            [20, 21, 22, 23]]])
    >>> a[..., 3]
    array([[ 3,  7, 11],
           [15, 19, 23]])

To index specific elements in each column, make use of :ref:`advanced-indexing`
as below::

    >>> arr = np.arange(3*4).reshape(3, 4)
    >>> arr
    array([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]])
    >>> column_indices = [[1, 3], [0, 2], [2, 2]]
    >>> np.arange(arr.shape[0])
    array([0, 1, 2])
    >>> row_indices = np.arange(arr.shape[0])[:, np.newaxis]
    >>> row_indices
    array([[0],
           [1],
           [2]])

Use the ``row_indices`` and ``column_indices`` for advanced
indexing::

    >>> arr[row_indices, column_indices]
    array([[ 1,  3],
           [ 4,  6],
           [10, 10]])

Index along a specific axis
---------------------------

Use :meth:`take`. See also :meth:`take_along_axis` and
:meth:`put_along_axis`.

    >>> a = np.arange(30).reshape(2, 3, 5)
    >>> a
    array([[[ 0,  1,  2,  3,  4],
            [ 5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14]],
    <BLANKLINE>
            [[15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [25, 26, 27, 28, 29]]])
    >>> np.take(a, [2, 3], axis=2)
    array([[[ 2,  3],
            [ 7,  8],
            [12, 13]],
    <BLANKLINE>
            [[17, 18],
            [22, 23],
            [27, 28]]])
    >>> np.take(a, [2], axis=1)
    array([[[10, 11, 12, 13, 14]],
    <BLANKLINE>
            [[25, 26, 27, 28, 29]]])

Create subsets of larger matrices
=================================

Use :ref:`slicing-and-striding` to access chunks of a large array::

    >>> a = np.arange(100).reshape(10, 10)
    >>> a
    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
            [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
            [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
            [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
            [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
            [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
            [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
            [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])
    >>> a[2:5, 2:5]
    array([[22, 23, 24],
           [32, 33, 34],
           [42, 43, 44]])
    >>> a[2:5, 1:3]
    array([[21, 22],
           [31, 32],
           [41, 42]])
    >>> a[:5, :5]
    array([[ 0,  1,  2,  3,  4],
           [10, 11, 12, 13, 14],
           [20, 21, 22, 23, 24],
           [30, 31, 32, 33, 34],
           [40, 41, 42, 43, 44]])

The same thing can be done with advanced indexing in a slightly more complex
way. Remember that
:ref:`advanced indexing creates a copy <indexing-operations>`::

    >>> a[np.arange(5)[:, None], np.arange(5)[None, :]]
    array([[ 0,  1,  2,  3,  4],
           [10, 11, 12, 13, 14],
           [20, 21, 22, 23, 24],
           [30, 31, 32, 33, 34],
           [40, 41, 42, 43, 44]])

You can also use :meth:`mgrid` to generate indices::

    >>> indices = np.mgrid[0:6:2]
    >>> indices
    array([0, 2, 4])
    >>> a[:, indices]
    array([[ 0,  2,  4],
           [10, 12, 14],
           [20, 22, 24],
           [30, 32, 34],
           [40, 42, 44],
           [50, 52, 54],
           [60, 62, 64],
           [70, 72, 74],
           [80, 82, 84],
           [90, 92, 94]])

Filter values
=============

Non-zero elements
-----------------

Use :meth:`nonzero` to get a tuple of array indices of non-zero elements 
corresponding to every dimension::

	>>> z = np.array([[1, 2, 3, 0], [0, 0, 5, 3], [4, 6, 0, 0]])
       >>> z
       array([[1, 2, 3, 0],
              [0, 0, 5, 3],
              [4, 6, 0, 0]])
       >>> np.nonzero(z)
       (array([0, 0, 0, 1, 1, 2, 2]), array([0, 1, 2, 2, 3, 0, 1]))

Use :meth:`flatnonzero` to fetch indices of elements that are non-zero in
the flattened version of the ndarray::

	>>> np.flatnonzero(z)
	array([0, 1, 2, 6, 7, 8, 9])

Arbitrary conditions
--------------------

Use :meth:`where` to generate indices based on conditions and then
use :ref:`advanced-indexing`.

    >>> a = np.arange(30).reshape(2, 3, 5)
    >>> indices = np.where(a % 2 == 0)
    >>> indices
    (array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), 
    array([0, 0, 0, 1, 1, 2, 2, 2, 0, 0, 1, 1, 1, 2, 2]), 
    array([0, 2, 4, 1, 3, 0, 2, 4, 1, 3, 0, 2, 4, 1, 3]))
    >>> a[indices]
    array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28])

Or, use :ref:`boolean-indexing`::

    >>> a > 14
    array([[[False, False, False, False, False],
            [False, False, False, False, False],
            [False, False, False, False, False]],
    <BLANKLINE>
           [[ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True]]])
    >>> a[a > 14]
    array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])

Replace values after filtering
------------------------------

Use assignment with filtering to replace desired values::

    >>> p = np.arange(-10, 10).reshape(2, 2, 5)
    >>> p
    array([[[-10,  -9,  -8,  -7,  -6],
            [ -5,  -4,  -3,  -2,  -1]],
    <BLANKLINE>
           [[  0,   1,   2,   3,   4],
            [  5,   6,   7,   8,   9]]])
    >>> q = p < 0
    >>> q
    array([[[ True,  True,  True,  True,  True],
            [ True,  True,  True,  True,  True]],
    <BLANKLINE>
           [[False, False, False, False, False],
            [False, False, False, False, False]]])
    >>> p[q] = 0
    >>> p
    array([[[0, 0, 0, 0, 0],
            [0, 0, 0, 0, 0]],
    <BLANKLINE>
           [[0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9]]])

Fetch indices of max/min values
===============================

Use :meth:`argmax` and :meth:`argmin`::

    >>> a = np.arange(30).reshape(2, 3, 5)
    >>> np.argmax(a)
    29
    >>> np.argmin(a)
    0

Use the ``axis`` keyword to get the indices of maximum and minimum
values along a specific axis::

    >>> np.argmax(a, axis=0)
    array([[1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1],
           [1, 1, 1, 1, 1]])
    >>> np.argmax(a, axis=1)
    array([[2, 2, 2, 2, 2],
           [2, 2, 2, 2, 2]])
    >>> np.argmax(a, axis=2)
    array([[4, 4, 4],
           [4, 4, 4]])
    <BLANKLINE>
    >>> np.argmin(a, axis=1)
    array([[0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0]])
    >>> np.argmin(a, axis=2)
    array([[0, 0, 0],
           [0, 0, 0]])

Set ``keepdims`` to ``True`` to keep the axes which are reduced in the
result as dimensions with size one::

    >>> np.argmin(a, axis=2, keepdims=True)
    array([[[0],
            [0],
            [0]],
    <BLANKLINE>
           [[0],
            [0],
            [0]]])
    >>> np.argmax(a, axis=1, keepdims=True)
    array([[[2, 2, 2, 2, 2]],
    <BLANKLINE>
           [[2, 2, 2, 2, 2]]])

Index the same ndarray multiple times efficiently
=================================================

It must be kept in mind that basic indexing produces :term:`views <view>`
and advanced indexing produces :term:`copies <copy>`, which are
computationally less efficient. Hence, you should take care to use basic
indexing wherever possible instead of advanced indexing.

Further reading
===============

Nicolas Rougier's `100 NumPy exercises <https://github.com/rougier/numpy-100>`_
provide a good insight into how indexing is combined with other operations.
Exercises `6`_, `8`_, `10`_, `15`_, `16`_, `19`_, `20`_, `45`_, `59`_,
`64`_, `65`_, `70`_, `71`_, `72`_, `76`_, `80`_, `81`_, `84`_, `87`_, `90`_,
`93`_, `94`_ are specially focused on indexing. 

.. _6: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#6-create-a-null-vector-of-size-10-but-the-fifth-value-which-is-1-
.. _8: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#8-reverse-a-vector-first-element-becomes-last-
.. _10: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#10-find-indices-of-non-zero-elements-from-120040-
.. _15: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#15-create-a-2d-array-with-1-on-the-border-and-0-inside-
.. _16: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#16-how-to-add-a-border-filled-with-0s-around-an-existing-array-
.. _19: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#19-create-a-8x8-matrix-and-fill-it-with-a-checkerboard-pattern-
.. _20: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#20-consider-a-678-shape-array-what-is-the-index-xyz-of-the-100th-element-
.. _45: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#45-create-random-vector-of-size-10-and-replace-the-maximum-value-by-0-
.. _59: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#59-how-to-sort-an-array-by-the-nth-column-
.. _64: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#64-consider-a-given-vector-how-to-add-1-to-each-element-indexed-by-a-second-vector-be-careful-with-repeated-indices-
.. _65: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#65-how-to-accumulate-elements-of-a-vector-x-to-an-array-f-based-on-an-index-list-i-
.. _70: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#70-consider-the-vector-1-2-3-4-5-how-to-build-a-new-vector-with-3-consecutive-zeros-interleaved-between-each-value-
.. _71: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#71-consider-an-array-of-dimension-553-how-to-mulitply-it-by-an-array-with-dimensions-55-
.. _72: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#72-how-to-swap-two-rows-of-an-array-
.. _76: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#76-consider-a-one-dimensional-array-z-build-a-two-dimensional-array-whose-first-row-is-z0z1z2-and-each-subsequent-row-is--shifted-by-1-last-row-should-be-z-3z-2z-1-
.. _80: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#80-consider-an-arbitrary-array-write-a-function-that-extract-a-subpart-with-a-fixed-shape-and-centered-on-a-given-element-pad-with-a-fill-value-when-necessary-
.. _81: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#81-consider-an-array-z--1234567891011121314-how-to-generate-an-array-r--1234-2345-3456--11121314-
.. _84: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#84-extract-all-the-contiguous-3x3-blocks-from-a-random-10x10-matrix-
.. _87: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#87-consider-a-16x16-array-how-to-get-the-block-sum-block-size-is-4x4-
.. _90: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#90-given-an-arbitrary-number-of-vectors-build-the-cartesian-product-every-combinations-of-every-item-
.. _93: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#93-consider-two-arrays-a-and-b-of-shape-83-and-22-how-to-find-rows-of-a-that-contain-elements-of-each-row-of-b-regardless-of-the-order-of-the-elements-in-b-
.. _94: https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises_with_solutions.md#94-considering-a-10x3-matrix-extract-rows-with-unequal-values-eg-223-.. _howtos:

################
NumPy How Tos
################

These documents are intended as recipes to common tasks using NumPy. For
detailed reference documentation of the functions and classes contained in
the package, see the :ref:`API reference <reference>`.

.. toctree::
   :maxdepth: 1

   how-to-how-to
   how-to-io
   how-to-index
.. _building-from-source:

Building from source
====================

There are two options for building NumPy- building with Gitpod or locally from
source. Your choice depends on your operating system and familiarity with the
command line.

Gitpod
------------

Gitpod is an open-source platform that automatically creates
the correct development environment right in your browser, reducing the need to
install local development environments and deal with incompatible dependencies.

If you are a Windows user, unfamiliar with using the command line or building
NumPy for the first time, it is often faster to build with Gitpod. Here are the
in-depth instructions for building NumPy with `building NumPy with Gitpod`_.

.. _building NumPy with Gitpod: https://numpy.org/devdocs/dev/development_gitpod.html

Building locally
------------------

Building locally on your machine gives you
more granular control. If you are a MacOS or Linux user familiar with using the
command line, you can continue with building NumPy locally by following the
instructions below.

..
  This page is referenced from numpy/numpy/__init__.py. Please keep its
  location in sync with the link there.

Prerequisites
-------------

Building NumPy requires the following software installed:

1) Python 3.6.x or newer

   Please note that the Python development headers also need to be installed,
   e.g., on Debian/Ubuntu one needs to install both `python3` and
   `python3-dev`. On Windows and macOS this is normally not an issue.

2) Compilers

   Much of NumPy is written in C.  You will need a C compiler that complies
   with the C99 standard.

   While a FORTRAN 77 compiler is not necessary for building NumPy, it is
   needed to run the ``numpy.f2py`` tests. These tests are skipped if the
   compiler is not auto-detected.

   Note that NumPy is developed mainly using GNU compilers and tested on
   MSVC and Clang compilers. Compilers from other vendors such as Intel,
   Absoft, Sun, NAG, Compaq, Vast, Portland, Lahey, HP, IBM are only
   supported in the form of community feedback, and may not work out of the
   box.  GCC 4.x (and later) compilers are recommended. On ARM64 (aarch64)
   GCC 8.x (and later) are recommended.

3) Linear Algebra libraries

   NumPy does not require any external linear algebra libraries to be
   installed. However, if these are available, NumPy's setup script can detect
   them and use them for building. A number of different LAPACK library setups
   can be used, including optimized LAPACK libraries such as OpenBLAS or MKL.
   The choice and location of these libraries as well as include paths and
   other such build options can be specified in a ``site.cfg`` file located in
   the NumPy root repository or a ``.numpy-site.cfg`` file in your home
   directory. See the ``site.cfg.example`` example file included in the NumPy
   repository or sdist for documentation, and below for specifying search
   priority from environmental variables.

4) Cython

   For building NumPy, you'll need a recent version of Cython.

Basic Installation
------------------

To install NumPy, run::

    pip install .

To perform an in-place build that can be run from the source folder run::

    python setup.py build_ext --inplace

*Note: for build instructions to do development work on NumPy itself, see*
:ref:`development-environment`.

Testing
-------

Make sure to test your builds. To ensure everything stays in shape, see if
all tests pass::

    $ python runtests.py -v -m full

For detailed info on testing, see :ref:`testing-builds`.

.. _parallel-builds:

Parallel builds
~~~~~~~~~~~~~~~

It's possible to do a parallel build with::

    python setup.py build -j 4 install --prefix $HOME/.local

This will compile numpy on 4 CPUs and install it into the specified prefix.
to perform a parallel in-place build, run::

    python setup.py build_ext --inplace -j 4

The number of build jobs can also be specified via the environment variable
``NPY_NUM_BUILD_JOBS``.

Choosing the fortran compiler
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Compilers are auto-detected; building with a particular compiler can be done
with ``--fcompiler``.  E.g. to select gfortran::

    python setup.py build --fcompiler=gnu95

For more information see::

    python setup.py build --help-fcompiler

How to check the ABI of BLAS/LAPACK libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One relatively simple and reliable way to check for the compiler used to
build a library is to use ldd on the library. If libg2c.so is a dependency,
this means that g77 has been used (note: g77 is no longer supported for
building NumPy). If libgfortran.so is a dependency, gfortran has been used.
If both are dependencies, this means both have been used, which is almost
always a very bad idea.

.. _accelerated-blas-lapack-libraries:

Accelerated BLAS/LAPACK libraries
---------------------------------

NumPy searches for optimized linear algebra libraries such as BLAS and LAPACK.
There are specific orders for searching these libraries, as described below and
in the ``site.cfg.example`` file.

BLAS
~~~~

Note that both BLAS and CBLAS interfaces are needed for a properly
optimized build of NumPy.

The default order for the libraries are:

1. MKL
2. BLIS
3. OpenBLAS
4. ATLAS
5. BLAS (NetLIB)

The detection of BLAS libraries may be bypassed by defining the environment
variable ``NPY_BLAS_LIBS`` , which should contain the exact linker flags you
want to use (interface is assumed to be Fortran 77).  Also define
``NPY_CBLAS_LIBS`` (even empty if CBLAS is contained in your BLAS library) to
trigger use of CBLAS and avoid slow fallback code for matrix calculations.

If you wish to build against OpenBLAS but you also have BLIS available one
may predefine the order of searching via the environment variable
``NPY_BLAS_ORDER`` which is a comma-separated list of the above names which
is used to determine what to search for, for instance::

      NPY_BLAS_ORDER=ATLAS,blis,openblas,MKL python setup.py build

will prefer to use ATLAS, then BLIS, then OpenBLAS and as a last resort MKL.
If neither of these exists the build will fail (names are compared
lower case).

Alternatively one may use ``!`` or ``^`` to negate all items::

        NPY_BLAS_ORDER='^blas,atlas' python setup.py build

will allow using anything **but** NetLIB BLAS and ATLAS libraries, the order
of the above list is retained.

One cannot mix negation and positives, nor have multiple negations, such
cases will raise an error.

LAPACK
~~~~~~

The default order for the libraries are:

1. MKL
2. OpenBLAS
3. libFLAME
4. ATLAS
5. LAPACK (NetLIB)

The detection of LAPACK libraries may be bypassed by defining the environment
variable ``NPY_LAPACK_LIBS``, which should contain the exact linker flags you
want to use (language is assumed to be Fortran 77).

If you wish to build against OpenBLAS but you also have MKL available one
may predefine the order of searching via the environment variable
``NPY_LAPACK_ORDER`` which is a comma-separated list of the above names,
for instance::

      NPY_LAPACK_ORDER=ATLAS,openblas,MKL python setup.py build

will prefer to use ATLAS, then OpenBLAS and as a last resort MKL.
If neither of these exists the build will fail (names are compared
lower case).

Alternatively one may use ``!`` or ``^`` to negate all items::

        NPY_LAPACK_ORDER='^lapack' python setup.py build

will allow using anything **but** the NetLIB LAPACK library, the order of
the above list is retained.

One cannot mix negation and positives, nor have multiple negations, such
cases will raise an error.

.. deprecated:: 1.20
  The native libraries on macOS, provided by Accelerate, are not fit for use
  in NumPy since they have bugs that cause wrong output under easily
  reproducible conditions. If the vendor fixes those bugs, the library could
  be reinstated, but until then users compiling for themselves should use
  another linear algebra library or use the built-in (but slower) default,
  see the next section.


Disabling ATLAS and other accelerated libraries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Usage of ATLAS and other accelerated libraries in NumPy can be disabled
via::

    NPY_BLAS_ORDER= NPY_LAPACK_ORDER= python setup.py build

or::

    BLAS=None LAPACK=None ATLAS=None python setup.py build


64-bit BLAS and LAPACK
~~~~~~~~~~~~~~~~~~~~~~

You can tell Numpy to use 64-bit BLAS/LAPACK libraries by setting the
environment variable::

    NPY_USE_BLAS_ILP64=1

when building Numpy. The following 64-bit BLAS/LAPACK libraries are
supported:

1. OpenBLAS ILP64 with ``64_`` symbol suffix (``openblas64_``)
2. OpenBLAS ILP64 without symbol suffix (``openblas_ilp64``)

The order in which they are preferred is determined by
``NPY_BLAS_ILP64_ORDER`` and ``NPY_LAPACK_ILP64_ORDER`` environment
variables. The default value is ``openblas64_,openblas_ilp64``.

.. note::

   Using non-symbol-suffixed 64-bit BLAS/LAPACK in a program that also
   uses 32-bit BLAS/LAPACK can cause crashes under certain conditions
   (e.g. with embedded Python interpreters on Linux).

   The 64-bit OpenBLAS with ``64_`` symbol suffix is obtained by
   compiling OpenBLAS with settings::

       make INTERFACE64=1 SYMBOLSUFFIX=64_

   The symbol suffix avoids the symbol name clashes between 32-bit and
   64-bit BLAS/LAPACK libraries.


Supplying additional compiler flags
-----------------------------------

Additional compiler flags can be supplied by setting the ``OPT``,
``FOPT`` (for Fortran), and ``CC`` environment variables.
When providing options that should improve the performance of the code
ensure that you also set ``-DNDEBUG`` so that debugging code is not
executed.

Cross compilation
-----------------

Although ``numpy.distutils`` and ``setuptools`` do not directly support cross
compilation, it is possible to build NumPy on one system for different
architectures with minor modifications to the build environment. This may be
desirable, for example, to use the power of a high-performance desktop to
create a NumPy package for a low-power, single-board computer. Because the
``setup.py`` scripts are unaware of cross-compilation environments and tend to
make decisions based on the environment detected on the build system, it is
best to compile for the same type of operating system that runs on the builder.
Attempting to compile a Mac version of NumPy on Windows, for example, is likely
to be met with challenges not considered here.

For the purpose of this discussion, the nomenclature adopted by `meson`_ will
be used: the "build" system is that which will be running the NumPy build
process, while the "host" is the platform on which the compiled package will be
run. A native Python interpreter, the setuptools and Cython packages and the
desired cross compiler must be available for the build system. In addition, a
Python interpreter and its development headers as well as any external linear
algebra libraries must be available for the host platform. For convenience, it
is assumed that all host software is available under a separate prefix
directory, here called ``$CROSS_PREFIX``.

.. _meson: https://mesonbuild.com/Cross-compilation.html#cross-compilation

When building and installing NumPy for a host system, the ``CC`` environment
variable must provide the path the cross compiler that will be used to build
NumPy C extensions. It may also be necessary to set the ``LDSHARED``
environment variable to the path to the linker that can link compiled objects
for the host system. The compiler must be told where it can find Python
libraries and development headers. On Unix-like systems, this generally
requires adding, *e.g.*, the following parameters to the ``CFLAGS`` environment
variable::

    -I${CROSS_PREFIX}/usr/include
    -I${CROSS_PREFIX}/usr/include/python3.y

for Python version 3.y. (Replace the "y" in this path with the actual minor
number of the installed Python runtime.) Likewise, the linker should be told
where to find host libraries by adding a parameter to the ``LDFLAGS``
environment variable::

    -L${CROSS_PREFIX}/usr/lib

To make sure Python-specific system configuration options are provided for the
intended host and not the build system, set::

    _PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_${ARCH_TRIPLET}

where ``${ARCH_TRIPLET}`` is an architecture-dependent suffix appropriate for
the host architecture. (This should be the name of a ``_sysconfigdata`` file,
without the ``.py`` extension, found in the host Python library directory.)

When using external linear algebra libraries, include and library directories
should be provided for the desired libraries in ``site.cfg`` as described
above and in the comments of the ``site.cfg.example`` file included in the
NumPy repository or sdist. In this example, set::

    include_dirs = ${CROSS_PREFIX}/usr/include
    library_dirs = ${CROSS_PREFIX}/usr/lib

under appropriate sections of the file to allow ``numpy.distutils`` to find the
libraries.

As of NumPy 1.22.0, a vendored copy of SVML will be built on ``x86_64`` Linux
hosts to provide AVX-512 acceleration of floating-point operations. When using
an ``x86_64`` Linux build system to cross compile NumPy for hosts other than
``x86_64`` Linux, set the environment variable ``NPY_DISABLE_SVML`` to prevent
the NumPy build script from incorrectly attempting to cross-compile this
platform-specific library::

    NPY_DISABLE_SVML=1

With the environment configured, NumPy may be built as it is natively::

    python setup.py build

When the ``wheel`` package is available, the cross-compiled package may be
packed into a wheel for installation on the host with::

    python setup.py bdist_wheel

It may be possible to use ``pip`` to build a wheel, but ``pip`` configures its
own environment; adapting the ``pip`` environment to cross-compilation is
beyond the scope of this guide.

The cross-compiled package may also be installed into the host prefix for
cross-compilation of other packages using, *e.g.*, the command::

    python setup.py install --prefix=${CROSS_PREFIX}

When cross compiling other packages that depend on NumPy, the host
npy-pkg-config file must be made available. For further discussion, refer to
`numpy distutils documentation`_.

.. _numpy distutils documentation: https://numpy.org/devdocs/reference/distutils.html#numpy.distutils.misc_util.Configuration.add_npy_pkg_config
**********************
Writing your own ufunc
**********************

| I have the Power!
| --- *He-Man*


.. _`sec:Creating-a-new`:

Creating a new universal function
=================================

.. index::
   pair: ufunc; adding new

Before reading this, it may help to familiarize yourself with the basics
of C extensions for Python by reading/skimming the tutorials in Section 1
of `Extending and Embedding the Python Interpreter
<https://docs.python.org/extending/index.html>`_ and in :doc:`How to extend
NumPy <c-info.how-to-extend>`

The umath module is a computer-generated C-module that creates many
ufuncs. It provides a great many examples of how to create a universal
function. Creating your own ufunc that will make use of the ufunc
machinery is not difficult either. Suppose you have a function that
you want to operate element-by-element over its inputs. By creating a
new ufunc you will obtain a function that handles

- broadcasting

- N-dimensional looping

- automatic type-conversions with minimal memory usage

- optional output arrays

It is not difficult to create your own ufunc. All that is required is
a 1-d loop for each data-type you want to support. Each 1-d loop must
have a specific signature, and only ufuncs for fixed-size data-types
can be used. The function call used to create a new ufunc to work on
built-in data-types is given below. A different mechanism is used to
register ufuncs for user-defined data-types.

In the next several sections we give example code that can be
easily modified to create your own ufuncs. The examples are
successively more complete or complicated versions of the logit
function, a common function in statistical modeling. Logit is also
interesting because, due to the magic of IEEE standards (specifically
IEEE 754), all of the logit functions created below
automatically have the following behavior.

>>> logit(0)
-inf
>>> logit(1)
inf
>>> logit(2)
nan
>>> logit(-2)
nan

This is wonderful because the function writer doesn't have to
manually propagate infs or nans.

.. _`sec:Non-numpy-example`:

Example Non-ufunc extension
===========================

.. index::
   pair: ufunc; adding new

For comparison and general edification of the reader we provide
a simple implementation of a C extension of ``logit`` that uses no
numpy.

To do this we need two files. The first is the C file which contains
the actual code, and the second is the ``setup.py`` file used to create
the module.

    .. code-block:: c

        #define PY_SSIZE_T_CLEAN
        #include <Python.h>
        #include <math.h>

        /*
         * spammodule.c
         * This is the C code for a non-numpy Python extension to
         * define the logit function, where logit(p) = log(p/(1-p)).
         * This function will not work on numpy arrays automatically.
         * numpy.vectorize must be called in python to generate
         * a numpy-friendly function.
         *
         * Details explaining the Python-C API can be found under
         * 'Extending and Embedding' and 'Python/C API' at
         * docs.python.org .
         */


        /* This declares the logit function */
        static PyObject *spam_logit(PyObject *self, PyObject *args);

        /*
         * This tells Python what methods this module has.
         * See the Python-C API for more information.
         */
        static PyMethodDef SpamMethods[] = {
            {"logit",
                spam_logit,
                METH_VARARGS, "compute logit"},
            {NULL, NULL, 0, NULL}
        };

        /*
         * This actually defines the logit function for
         * input args from Python.
         */

        static PyObject *spam_logit(PyObject *self, PyObject *args)
        {
            double p;

            /* This parses the Python argument into a double */
            if(!PyArg_ParseTuple(args, "d", &p)) {
                return NULL;
            }

            /* THE ACTUAL LOGIT FUNCTION */
            p = p/(1-p);
            p = log(p);

            /*This builds the answer back into a python object */
            return Py_BuildValue("d", p);
        }

        /* This initiates the module using the above definitions. */
        static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "spam",
            NULL,
            -1,
            SpamMethods,
            NULL,
            NULL,
            NULL,
            NULL
        };

        PyMODINIT_FUNC PyInit_spam(void)
        {
            PyObject *m;
            m = PyModule_Create(&moduledef);
            if (!m) {
                return NULL;
            }
            return m;
        }

To use the ``setup.py file``, place ``setup.py`` and ``spammodule.c``
in the same folder. Then ``python setup.py build`` will build the module to
import, or ``python setup.py install`` will install the module to your
site-packages directory.

    .. code-block:: python

        '''
            setup.py file for spammodule.c

            Calling
            $python setup.py build_ext --inplace
            will build the extension library in the current file.

            Calling
            $python setup.py build
            will build a file that looks like ./build/lib*, where
            lib* is a file that begins with lib. The library will
            be in this file and end with a C library extension,
            such as .so

            Calling
            $python setup.py install
            will install the module in your site-packages file.

            See the distutils section of
            'Extending and Embedding the Python Interpreter'
            at docs.python.org for more information.
        '''


        from distutils.core import setup, Extension

        module1 = Extension('spam', sources=['spammodule.c'],
                                include_dirs=['/usr/local/lib'])

        setup(name = 'spam',
                version='1.0',
                description='This is my spam package',
                ext_modules = [module1])


Once the spam module is imported into python, you can call logit
via ``spam.logit``. Note that the function used above cannot be applied
as-is to numpy arrays. To do so we must call :py:func:`numpy.vectorize`
on it. For example, if a python interpreter is opened in the file containing
the spam library or spam has been installed, one can perform the
following commands:

>>> import numpy as np
>>> import spam
>>> spam.logit(0)
-inf
>>> spam.logit(1)
inf
>>> spam.logit(0.5)
0.0
>>> x = np.linspace(0,1,10)
>>> spam.logit(x)
TypeError: only length-1 arrays can be converted to Python scalars
>>> f = np.vectorize(spam.logit)
>>> f(x)
array([       -inf, -2.07944154, -1.25276297, -0.69314718, -0.22314355,
    0.22314355,  0.69314718,  1.25276297,  2.07944154,         inf])

THE RESULTING LOGIT FUNCTION IS NOT FAST! ``numpy.vectorize`` simply
loops over ``spam.logit``. The loop is done at the C level, but the numpy
array is constantly being parsed and build back up. This is expensive.
When the author compared ``numpy.vectorize(spam.logit)`` against the
logit ufuncs constructed below, the logit ufuncs were almost exactly
4 times faster. Larger or smaller speedups are, of course, possible
depending on the nature of the function.


.. _`sec:NumPy-one-loop`:

Example NumPy ufunc for one dtype
=================================

.. index::
   pair: ufunc; adding new

For simplicity we give a ufunc for a single dtype, the ``'f8'``
``double``. As in the previous section, we first give the ``.c`` file
and then the ``setup.py`` file used to create the module containing the
ufunc.

The place in the code corresponding to the actual computations for
the ufunc are marked with ``/\* BEGIN main ufunc computation \*/`` and
``/\* END main ufunc computation \*/``. The code in between those lines is
the primary thing that must be changed to create your own ufunc.

    .. code-block:: c

        #define PY_SSIZE_T_CLEAN
        #include <Python.h>
        #include "numpy/ndarraytypes.h"
        #include "numpy/ufuncobject.h"
        #include "numpy/npy_3kcompat.h"
        #include <math.h>

        /*
         * single_type_logit.c
         * This is the C code for creating your own
         * NumPy ufunc for a logit function.
         *
         * In this code we only define the ufunc for
         * a single dtype. The computations that must
         * be replaced to create a ufunc for
         * a different function are marked with BEGIN
         * and END.
         *
         * Details explaining the Python-C API can be found under
         * 'Extending and Embedding' and 'Python/C API' at
         * docs.python.org .
         */

        static PyMethodDef LogitMethods[] = {
            {NULL, NULL, 0, NULL}
        };

        /* The loop definition must precede the PyMODINIT_FUNC. */

        static void double_logit(char **args, const npy_intp *dimensions,
                                 const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in = args[0], *out = args[1];
            npy_intp in_step = steps[0], out_step = steps[1];

            double tmp;

            for (i = 0; i < n; i++) {
                /* BEGIN main ufunc computation */
                tmp = *(double *)in;
                tmp /= 1 - tmp;
                *((double *)out) = log(tmp);
                /* END main ufunc computation */

                in += in_step;
                out += out_step;
            }
        }

        /* This a pointer to the above function */
        PyUFuncGenericFunction funcs[1] = {&double_logit};

        /* These are the input and return dtypes of logit.*/
        static char types[2] = {NPY_DOUBLE, NPY_DOUBLE};

        static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "npufunc",
            NULL,
            -1,
            LogitMethods,
            NULL,
            NULL,
            NULL,
            NULL
        };

        PyMODINIT_FUNC PyInit_npufunc(void)
        {
            PyObject *m, *logit, *d;
            m = PyModule_Create(&moduledef);
            if (!m) {
                return NULL;
            }

            import_array();
            import_umath();

            logit = PyUFunc_FromFuncAndData(funcs, NULL, types, 1, 1, 1,
                                            PyUFunc_None, "logit",
                                            "logit_docstring", 0);

            d = PyModule_GetDict(m);

            PyDict_SetItemString(d, "logit", logit);
            Py_DECREF(logit);

            return m;
        }

This is a ``setup.py file`` for the above code. As before, the module
can be build via calling ``python setup.py build`` at the command prompt,
or installed to site-packages via ``python setup.py install``. The module
can also be placed into a local folder e.g. ``npufunc_directory`` below
using ``python setup.py build_ext --inplace``.

    .. code-block:: python

        '''
            setup.py file for single_type_logit.c
            Note that since this is a numpy extension
            we use numpy.distutils instead of
            distutils from the python standard library.

            Calling
            $python setup.py build_ext --inplace
            will build the extension library in the npufunc_directory.

            Calling
            $python setup.py build
            will build a file that looks like ./build/lib*, where
            lib* is a file that begins with lib. The library will
            be in this file and end with a C library extension,
            such as .so

            Calling
            $python setup.py install
            will install the module in your site-packages file.

            See the distutils section of
            'Extending and Embedding the Python Interpreter'
            at docs.python.org  and the documentation
            on numpy.distutils for more information.
        '''


        def configuration(parent_package='', top_path=None):
            from numpy.distutils.misc_util import Configuration

            config = Configuration('npufunc_directory',
                                   parent_package,
                                   top_path)
            config.add_extension('npufunc', ['single_type_logit.c'])

            return config

        if __name__ == "__main__":
            from numpy.distutils.core import setup
            setup(configuration=configuration)

After the above has been installed, it can be imported and used as follows.

>>> import numpy as np
>>> import npufunc
>>> npufunc.logit(0.5)
0.0
>>> a = np.linspace(0,1,5)
>>> npufunc.logit(a)
array([       -inf, -1.09861229,  0.        ,  1.09861229,         inf])



.. _`sec:NumPy-many-loop`:

Example NumPy ufunc with multiple dtypes
========================================

.. index::
   pair: ufunc; adding new

We finally give an example of a full ufunc, with inner loops for
half-floats, floats, doubles, and long doubles. As in the previous
sections we first give the ``.c`` file and then the corresponding
``setup.py`` file.

The places in the code corresponding to the actual computations for
the ufunc are marked with ``/\* BEGIN main ufunc computation \*/`` and
``/\* END main ufunc computation \*/``. The code in between those lines
is the primary thing that must be changed to create your own ufunc.


    .. code-block:: c

        #define PY_SSIZE_T_CLEAN
        #include <Python.h>
        #include "numpy/ndarraytypes.h"
        #include "numpy/ufuncobject.h"
        #include "numpy/halffloat.h"
        #include <math.h>

        /*
         * multi_type_logit.c
         * This is the C code for creating your own
         * NumPy ufunc for a logit function.
         *
         * Each function of the form type_logit defines the
         * logit function for a different numpy dtype. Each
         * of these functions must be modified when you
         * create your own ufunc. The computations that must
         * be replaced to create a ufunc for
         * a different function are marked with BEGIN
         * and END.
         *
         * Details explaining the Python-C API can be found under
         * 'Extending and Embedding' and 'Python/C API' at
         * docs.python.org .
         *
         */

        static PyMethodDef LogitMethods[] = {
            {NULL, NULL, 0, NULL}
        };

        /* The loop definitions must precede the PyMODINIT_FUNC. */

        static void long_double_logit(char **args, const npy_intp *dimensions,
                                      const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in = args[0], *out = args[1];
            npy_intp in_step = steps[0], out_step = steps[1];

            long double tmp;

            for (i = 0; i < n; i++) {
                /* BEGIN main ufunc computation */
                tmp = *(long double *)in;
                tmp /= 1 - tmp;
                *((long double *)out) = logl(tmp);
                /* END main ufunc computation */

                in += in_step;
                out += out_step;
            }
        }

        static void double_logit(char **args, const npy_intp *dimensions,
                                 const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in = args[0], *out = args[1];
            npy_intp in_step = steps[0], out_step = steps[1];

            double tmp;

            for (i = 0; i < n; i++) {
                /* BEGIN main ufunc computation */
                tmp = *(double *)in;
                tmp /= 1 - tmp;
                *((double *)out) = log(tmp);
                /* END main ufunc computation */

                in += in_step;
                out += out_step;
            }
        }

        static void float_logit(char **args, const npy_intp *dimensions,
                               const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in = args[0], *out = args[1];
            npy_intp in_step = steps[0], out_step = steps[1];

            float tmp;

            for (i = 0; i < n; i++) {
                /* BEGIN main ufunc computation */
                tmp = *(float *)in;
                tmp /= 1 - tmp;
                *((float *)out) = logf(tmp);
                /* END main ufunc computation */

                in += in_step;
                out += out_step;
            }
        }


        static void half_float_logit(char **args, const npy_intp *dimensions,
                                    const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in = args[0], *out = args[1];
            npy_intp in_step = steps[0], out_step = steps[1];

            float tmp;

            for (i = 0; i < n; i++) {

                /* BEGIN main ufunc computation */
                tmp = npy_half_to_float(*(npy_half *)in);
                tmp /= 1 - tmp;
                tmp = logf(tmp);
                *((npy_half *)out) = npy_float_to_half(tmp);
                /* END main ufunc computation */

                in += in_step;
                out += out_step;
            }
        }


        /*This gives pointers to the above functions*/
        PyUFuncGenericFunction funcs[4] = {&half_float_logit,
                                           &float_logit,
                                           &double_logit,
                                           &long_double_logit};

        static char types[8] = {NPY_HALF, NPY_HALF,
                                NPY_FLOAT, NPY_FLOAT,
                                NPY_DOUBLE, NPY_DOUBLE,
                                NPY_LONGDOUBLE, NPY_LONGDOUBLE};

        static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "npufunc",
            NULL,
            -1,
            LogitMethods,
            NULL,
            NULL,
            NULL,
            NULL
        };

        PyMODINIT_FUNC PyInit_npufunc(void)
        {
            PyObject *m, *logit, *d;
            m = PyModule_Create(&moduledef);
            if (!m) {
                return NULL;
            }

            import_array();
            import_umath();

            logit = PyUFunc_FromFuncAndData(funcs, NULL, types, 4, 1, 1,
                                            PyUFunc_None, "logit",
                                            "logit_docstring", 0);

            d = PyModule_GetDict(m);

            PyDict_SetItemString(d, "logit", logit);
            Py_DECREF(logit);

            return m;
        }

This is a ``setup.py`` file for the above code. As before, the module
can be build via calling ``python setup.py build`` at the command prompt,
or installed to site-packages via ``python setup.py install``.

    .. code-block:: python

        '''
            setup.py file for multi_type_logit.c
            Note that since this is a numpy extension
            we use numpy.distutils instead of
            distutils from the python standard library.

            Calling
            $python setup.py build_ext --inplace
            will build the extension library in the current file.

            Calling
            $python setup.py build
            will build a file that looks like ./build/lib*, where
            lib* is a file that begins with lib. The library will
            be in this file and end with a C library extension,
            such as .so

            Calling
            $python setup.py install
            will install the module in your site-packages file.

            See the distutils section of
            'Extending and Embedding the Python Interpreter'
            at docs.python.org  and the documentation
            on numpy.distutils for more information.
        '''


        def configuration(parent_package='', top_path=None):
            from numpy.distutils.misc_util import Configuration, get_info

            #Necessary for the half-float d-type.
            info = get_info('npymath')

            config = Configuration('npufunc_directory',
                                    parent_package,
                                    top_path)
            config.add_extension('npufunc',
                                    ['multi_type_logit.c'],
                                    extra_info=info)

            return config

        if __name__ == "__main__":
            from numpy.distutils.core import setup
            setup(configuration=configuration)

After the above has been installed, it can be imported and used as follows.

>>> import numpy as np
>>> import npufunc
>>> npufunc.logit(0.5)
0.0
>>> a = np.linspace(0,1,5)
>>> npufunc.logit(a)
array([       -inf, -1.09861229,  0.        ,  1.09861229,         inf])



.. _`sec:NumPy-many-arg`:

Example NumPy ufunc with multiple arguments/return values
=========================================================

Our final example is a ufunc with multiple arguments. It is a modification
of the code for a logit ufunc for data with a single dtype. We
compute ``(A * B, logit(A * B))``.

We only give the C code as the setup.py file is exactly the same as
the ``setup.py`` file in `Example NumPy ufunc for one dtype`_, except that
the line

    .. code-block:: python

        config.add_extension('npufunc', ['single_type_logit.c'])

is replaced with

    .. code-block:: python

        config.add_extension('npufunc', ['multi_arg_logit.c'])

The C file is given below. The ufunc generated takes two arguments ``A``
and ``B``. It returns a tuple whose first element is ``A * B`` and whose second
element is ``logit(A * B)``. Note that it automatically supports broadcasting,
as well as all other properties of a ufunc.

    .. code-block:: c

        #define PY_SSIZE_T_CLEAN
        #include <Python.h>
        #include "numpy/ndarraytypes.h"
        #include "numpy/ufuncobject.h"
        #include "numpy/halffloat.h"
        #include <math.h>

        /*
         * multi_arg_logit.c
         * This is the C code for creating your own
         * NumPy ufunc for a multiple argument, multiple
         * return value ufunc. The places where the
         * ufunc computation is carried out are marked
         * with comments.
         *
         * Details explaining the Python-C API can be found under
         * 'Extending and Embedding' and 'Python/C API' at
         * docs.python.org.
         */

        static PyMethodDef LogitMethods[] = {
            {NULL, NULL, 0, NULL}
        };

        /* The loop definition must precede the PyMODINIT_FUNC. */

        static void double_logitprod(char **args, const npy_intp *dimensions,
                                     const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp n = dimensions[0];
            char *in1 = args[0], *in2 = args[1];
            char *out1 = args[2], *out2 = args[3];
            npy_intp in1_step = steps[0], in2_step = steps[1];
            npy_intp out1_step = steps[2], out2_step = steps[3];

            double tmp;

            for (i = 0; i < n; i++) {
                /* BEGIN main ufunc computation */
                tmp = *(double *)in1;
                tmp *= *(double *)in2;
                *((double *)out1) = tmp;
                *((double *)out2) = log(tmp / (1 - tmp));
                /* END main ufunc computation */

                in1 += in1_step;
                in2 += in2_step;
                out1 += out1_step;
                out2 += out2_step;
            }
        }

        /*This a pointer to the above function*/
        PyUFuncGenericFunction funcs[1] = {&double_logitprod};

        /* These are the input and return dtypes of logit.*/

        static char types[4] = {NPY_DOUBLE, NPY_DOUBLE,
                                NPY_DOUBLE, NPY_DOUBLE};

        static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "npufunc",
            NULL,
            -1,
            LogitMethods,
            NULL,
            NULL,
            NULL,
            NULL
        };

        PyMODINIT_FUNC PyInit_npufunc(void)
        {
            PyObject *m, *logit, *d;
            m = PyModule_Create(&moduledef);
            if (!m) {
                return NULL;
            }

            import_array();
            import_umath();

            logit = PyUFunc_FromFuncAndData(funcs, NULL, types, 1, 2, 2,
                                            PyUFunc_None, "logit",
                                            "logit_docstring", 0);

            d = PyModule_GetDict(m);

            PyDict_SetItemString(d, "logit", logit);
            Py_DECREF(logit);

            return m;
        }


.. _`sec:NumPy-struct-dtype`:

Example NumPy ufunc with structured array dtype arguments
=========================================================

This example shows how to create a ufunc for a structured array dtype.
For the example we show a trivial ufunc for adding two arrays with dtype
``'u8,u8,u8'``. The process is a bit different from the other examples since
a call to :c:func:`PyUFunc_FromFuncAndData` doesn't fully register ufuncs for
custom dtypes and structured array dtypes. We need to also call
:c:func:`PyUFunc_RegisterLoopForDescr` to finish setting up the ufunc.

We only give the C code as the ``setup.py`` file is exactly the same as
the ``setup.py`` file in `Example NumPy ufunc for one dtype`_, except that
the line

    .. code-block:: python

        config.add_extension('npufunc', ['single_type_logit.c'])

is replaced with

    .. code-block:: python

        config.add_extension('npufunc', ['add_triplet.c'])

The C file is given below.

    .. code-block:: c

        #define PY_SSIZE_T_CLEAN
        #include <Python.h>
        #include "numpy/ndarraytypes.h"
        #include "numpy/ufuncobject.h"
        #include "numpy/npy_3kcompat.h"
        #include <math.h>

        /*
         * add_triplet.c
         * This is the C code for creating your own
         * NumPy ufunc for a structured array dtype.
         *
         * Details explaining the Python-C API can be found under
         * 'Extending and Embedding' and 'Python/C API' at
         * docs.python.org.
         */

        static PyMethodDef StructUfuncTestMethods[] = {
            {NULL, NULL, 0, NULL}
        };

        /* The loop definition must precede the PyMODINIT_FUNC. */

        static void add_uint64_triplet(char **args, const npy_intp *dimensions,
                                       const npy_intp *steps, void *data)
        {
            npy_intp i;
            npy_intp is1 = steps[0];
            npy_intp is2 = steps[1];
            npy_intp os = steps[2];
            npy_intp n = dimensions[0];
            uint64_t *x, *y, *z;

            char *i1 = args[0];
            char *i2 = args[1];
            char *op = args[2];

            for (i = 0; i < n; i++) {

                x = (uint64_t *)i1;
                y = (uint64_t *)i2;
                z = (uint64_t *)op;

                z[0] = x[0] + y[0];
                z[1] = x[1] + y[1];
                z[2] = x[2] + y[2];

                i1 += is1;
                i2 += is2;
                op += os;
            }
        }

        /* This a pointer to the above function */
        PyUFuncGenericFunction funcs[1] = {&add_uint64_triplet};

        /* These are the input and return dtypes of add_uint64_triplet. */
        static char types[3] = {NPY_UINT64, NPY_UINT64, NPY_UINT64};

        static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "struct_ufunc_test",
            NULL,
            -1,
            StructUfuncTestMethods,
            NULL,
            NULL,
            NULL,
            NULL
        };

        PyMODINIT_FUNC PyInit_struct_ufunc_test(void)
        {
            PyObject *m, *add_triplet, *d;
            PyObject *dtype_dict;
            PyArray_Descr *dtype;
            PyArray_Descr *dtypes[3];

            m = PyModule_Create(&moduledef);

            if (m == NULL) {
                return NULL;
            }

            import_array();
            import_umath();

            /* Create a new ufunc object */
            add_triplet = PyUFunc_FromFuncAndData(NULL, NULL, NULL, 0, 2, 1,
                                                  PyUFunc_None, "add_triplet",
                                                  "add_triplet_docstring", 0);

            dtype_dict = Py_BuildValue("[(s, s), (s, s), (s, s)]",
                                       "f0", "u8", "f1", "u8", "f2", "u8");
            PyArray_DescrConverter(dtype_dict, &dtype);
            Py_DECREF(dtype_dict);

            dtypes[0] = dtype;
            dtypes[1] = dtype;
            dtypes[2] = dtype;

            /* Register ufunc for structured dtype */
            PyUFunc_RegisterLoopForDescr(add_triplet,
                                         dtype,
                                         &add_uint64_triplet,
                                         dtypes,
                                         NULL);

            d = PyModule_GetDict(m);

            PyDict_SetItemString(d, "add_triplet", add_triplet);
            Py_DECREF(add_triplet);
            return m;
        }

.. index::
   pair: ufunc; adding new

The returned ufunc object is a callable Python object. It should be
placed in a (module) dictionary under the same name as was used in the
name argument to the ufunc-creation routine. The following example is
adapted from the umath module

    .. code-block:: c

        static PyUFuncGenericFunction atan2_functions[] = {
                              PyUFunc_ff_f, PyUFunc_dd_d,
                              PyUFunc_gg_g, PyUFunc_OO_O_method};
        static void *atan2_data[] = {
                              (void *)atan2f, (void *)atan2,
                              (void *)atan2l, (void *)"arctan2"};
        static char atan2_signatures[] = {
                      NPY_FLOAT, NPY_FLOAT, NPY_FLOAT,
                      NPY_DOUBLE, NPY_DOUBLE, NPY_DOUBLE,
                      NPY_LONGDOUBLE, NPY_LONGDOUBLE, NPY_LONGDOUBLE
                      NPY_OBJECT, NPY_OBJECT, NPY_OBJECT};
        ...
        /* in the module initialization code */
        PyObject *f, *dict, *module;
        ...
        dict = PyModule_GetDict(module);
        ...
        f = PyUFunc_FromFuncAndData(atan2_functions,
            atan2_data, atan2_signatures, 4, 2, 1,
            PyUFunc_None, "arctan2",
            "a safe and correct arctan(x1/x2)", 0);
        PyDict_SetItemString(dict, "arctan2", f);
        Py_DECREF(f);
        ...
:orphan:

.. _user:

################
NumPy user guide
################

This guide is an overview and explains the important features;
details are found in :ref:`reference`.

.. toctree::
   :maxdepth: 1

   whatisnumpy
   Installation <https://numpy.org/install/>
   quickstart
   absolute_beginners
   basics
   misc
   numpy-for-matlab-users
   building
   c-info
   NumPy Tutorials <https://numpy.org/numpy-tutorials/features.html>
   howtos_index
   depending_on_numpy


.. Links to these files are placed directly in the top-level html
   (doc/source/_templates/indexcontent.html, which appears for the URLs
   numpy.org/devdocs and numpy.org/doc/XX) and are not in any toctree, so
   we include them here to avoid a "WARNING: document isn't included in any
   toctree" message

.. toctree::
   :hidden:

   ../f2py/index
   ../glossary
   ../dev/underthehood
   ../bugs
   ../release
   ../license
#################
Using NumPy C-API
#################

.. toctree::

   c-info.how-to-extend
   c-info.python-as-glue
   c-info.ufunc-tutorial
   c-info.beyond-basics
.. _for-downstream-package-authors:

For downstream package authors
==============================

This document aims to explain some best practices for authoring a package that
depends on NumPy.


Understanding NumPy's versioning and API/ABI stability
------------------------------------------------------

NumPy uses a standard, :pep:`440` compliant, versioning scheme:
``major.minor.bugfix``. A *major* release is highly unusual (NumPy is still at
version ``1.xx``) and if it happens it will likely indicate an ABI break.
*Minor* versions are released regularly, typically every 6 months. Minor
versions contain new features, deprecations, and removals of previously
deprecated code. *Bugfix* releases are made even more frequently; they do not
contain any new features or deprecations.

It is important to know that NumPy, like Python itself and most other
well known scientific Python projects, does **not** use semantic versioning.
Instead, backwards incompatible API changes require deprecation warnings for at
least two releases. For more details, see :ref:`NEP23`.

NumPy has both a Python API and a C API. The C API can be used directly or via
Cython, f2py, or other such tools. If your package uses the C API, then ABI
(application binary interface) stability of NumPy is important. NumPy's ABI is
forward but not backward compatible. This means: binaries compiled against a
given version of NumPy will still run correctly with newer NumPy versions, but
not with older versions.


Testing against the NumPy main branch or pre-releases
-----------------------------------------------------

For large, actively maintained packages that depend on NumPy, we recommend
testing against the development version of NumPy in CI. To make this easy,
nightly builds are provided as wheels at
https://anaconda.org/scipy-wheels-nightly/.
This helps detect regressions in NumPy that need fixing before the next NumPy
release.  Furthermore, we recommend to raise errors on warnings in CI for this
job, either all warnings or otherwise at least ``DeprecationWarning`` and
``FutureWarning``. This gives you an early warning about changes in NumPy to
adapt your code.


Adding a dependency on NumPy
----------------------------

Build-time dependency
`````````````````````

If a package either uses the NumPy C API directly or it uses some other tool
that depends on it like Cython or Pythran, NumPy is a *build-time* dependency
of the package. Because the NumPy ABI is only forward compatible, you must
build your own binaries (wheels or other package formats) against the lowest
NumPy version that you support (or an even older version).

Picking the correct NumPy version to build against for each Python version and
platform can get complicated. There are a couple of ways to do this.
Build-time dependencies are specified in ``pyproject.toml`` (see PEP 517),
which is the file used to build wheels by PEP 517 compliant tools (e.g.,
when using ``pip wheel``).

You can specify everything manually in ``pyproject.toml``, or you can instead
rely on the `oldest-supported-numpy <https://github.com/scipy/oldest-supported-numpy/>`__
metapackage. ``oldest-supported-numpy`` will specify the correct NumPy version
at build time for wheels, taking into account Python version, Python
implementation (CPython or PyPy), operating system and hardware platform. It
will specify the oldest NumPy version that supports that combination of
characteristics.  Note: for platforms for which NumPy provides wheels on PyPI,
it will be the first version with wheels (even if some older NumPy version
happens to build).

For conda-forge it's a little less complicated: there's dedicated handling for
NumPy in build-time and runtime dependencies, so typically this is enough
(see `here <https://conda-forge.org/docs/maintainer/knowledge_base.html#building-against-numpy>`__ for docs)::

    host:
      - numpy
    run:
      - {{ pin_compatible('numpy') }}

.. note::

    ``pip`` has ``--no-use-pep517`` and ``--no-build-isolation`` flags that may
    ignore ``pyproject.toml`` or treat it differently - if users use those
    flags, they are responsible for installing the correct build dependencies
    themselves.

    ``conda`` will always use ``-no-build-isolation``; dependencies for conda
    builds are given in the conda recipe (``meta.yaml``), the ones in
    ``pyproject.toml`` have no effect.

    Please do not use ``setup_requires`` (it is deprecated and may invoke
    ``easy_install``).

Because for NumPy you have to care about ABI compatibility, you
specify the version with ``==`` to the lowest supported version. For your other
build dependencies you can probably be looser, however it's still important to
set lower and upper bounds for each dependency. It's fine to specify either a
range or a specific version for a dependency like ``wheel`` or ``setuptools``.
It's recommended to set the upper bound of the range to the latest already
released version of ``wheel`` and ``setuptools`` - this prevents future
releases from breaking your packages on PyPI.


Runtime dependency & version ranges
```````````````````````````````````

NumPy itself and many core scientific Python packages have agreed on a schedule
for dropping support for old Python and NumPy versions: :ref:`NEP29`. We
recommend all packages depending on NumPy to follow the recommendations in NEP
29.

For *run-time dependencies*, you specify the range of versions in
``install_requires`` in ``setup.py`` (assuming you use ``numpy.distutils`` or
``setuptools`` to build). Getting the upper bound right for NumPy is slightly
tricky. If we don't set any bound, a too-new version will be pulled in a few
years down the line, and NumPy may have deprecated and removed some API that
your package depended on by then. On the other hand if you set the upper bound
to the newest already-released version, then as soon as a new NumPy version is
released there will be no matching version of your package that works with it.

What to do here depends on your release frequency. Given that NumPy releases
come in a 6-monthly cadence and that features that get deprecated in NumPy
should stay around for another two releases, a good upper bound is
``<1.(xx+3).0`` - where ``xx`` is the minor version of the latest
already-released NumPy. This is safe to do if you release at least once a year.
If your own releases are much less frequent, you may set the upper bound a
little further into the future - this is a trade-off between a future NumPy
version _maybe_ removing something you rely on, and the upper bound being
exceeded which _may_ lead to your package being hard to install in combination
with other packages relying on the latest NumPy.


.. note::


    SciPy has more documentation on how it builds wheels and deals with its
    build-time and runtime dependencies
    `here <https://scipy.github.io/devdocs/dev/core-dev/index.html#distributing>`__.

    NumPy and SciPy wheel build CI may also be useful as a reference, it can be
    found `here for NumPy <https://github.com/MacPython/numpy-wheels>`__ and
    `here for SciPy <https://github.com/MacPython/scipy-wheels>`__.
*************
Byte-swapping
*************

Introduction to byte ordering and ndarrays
==========================================

The ``ndarray`` is an object that provide a python array interface to data
in memory.

It often happens that the memory that you want to view with an array is
not of the same byte ordering as the computer on which you are running
Python.

For example, I might be working on a computer with a little-endian CPU -
such as an Intel Pentium, but I have loaded some data from a file
written by a computer that is big-endian.  Let's say I have loaded 4
bytes from a file written by a Sun (big-endian) computer.  I know that
these 4 bytes represent two 16-bit integers.  On a big-endian machine, a
two-byte integer is stored with the Most Significant Byte (MSB) first,
and then the Least Significant Byte (LSB). Thus the bytes are, in memory order:

#. MSB integer 1
#. LSB integer 1
#. MSB integer 2
#. LSB integer 2

Let's say the two integers were in fact 1 and 770.  Because 770 = 256 *
3 + 2, the 4 bytes in memory would contain respectively: 0, 1, 3, 2.
The bytes I have loaded from the file would have these contents:

>>> big_end_buffer = bytearray([0,1,3,2])
>>> big_end_buffer
bytearray(b'\x00\x01\x03\x02')

We might want to use an ``ndarray`` to access these integers.  In that
case, we can create an array around this memory, and tell numpy that
there are two integers, and that they are 16 bit and big-endian:

>>> import numpy as np
>>> big_end_arr = np.ndarray(shape=(2,),dtype='>i2', buffer=big_end_buffer)
>>> big_end_arr[0]
1
>>> big_end_arr[1]
770

Note the array ``dtype`` above of ``>i2``.  The ``>`` means 'big-endian'
(``<`` is little-endian) and ``i2`` means 'signed 2-byte integer'.  For
example, if our data represented a single unsigned 4-byte little-endian
integer, the dtype string would be ``<u4``.

In fact, why don't we try that?

>>> little_end_u4 = np.ndarray(shape=(1,),dtype='<u4', buffer=big_end_buffer)
>>> little_end_u4[0] == 1 * 256**1 + 3 * 256**2 + 2 * 256**3
True

Returning to our ``big_end_arr`` - in this case our underlying data is
big-endian (data endianness) and we've set the dtype to match (the dtype
is also big-endian).  However, sometimes you need to flip these around.

.. warning::

    Scalars currently do not include byte order information, so extracting
    a scalar from an array will return an integer in native byte order.
    Hence:

    >>> big_end_arr[0].dtype.byteorder == little_end_u4[0].dtype.byteorder
    True

Changing byte ordering
======================

As you can imagine from the introduction, there are two ways you can
affect the relationship between the byte ordering of the array and the
underlying memory it is looking at:

* Change the byte-ordering information in the array dtype so that it
  interprets the underlying data as being in a different byte order.
  This is the role of ``arr.newbyteorder()``
* Change the byte-ordering of the underlying data, leaving the dtype
  interpretation as it was.  This is what ``arr.byteswap()`` does.

The common situations in which you need to change byte ordering are:

#. Your data and dtype endianness don't match, and you want to change
   the dtype so that it matches the data.
#. Your data and dtype endianness don't match, and you want to swap the
   data so that they match the dtype
#. Your data and dtype endianness match, but you want the data swapped
   and the dtype to reflect this

Data and dtype endianness don't match, change dtype to match data
-----------------------------------------------------------------

We make something where they don't match:

>>> wrong_end_dtype_arr = np.ndarray(shape=(2,),dtype='<i2', buffer=big_end_buffer)
>>> wrong_end_dtype_arr[0]
256

The obvious fix for this situation is to change the dtype so it gives
the correct endianness:

>>> fixed_end_dtype_arr = wrong_end_dtype_arr.newbyteorder()
>>> fixed_end_dtype_arr[0]
1

Note the array has not changed in memory:

>>> fixed_end_dtype_arr.tobytes() == big_end_buffer
True

Data and type endianness don't match, change data to match dtype
----------------------------------------------------------------

You might want to do this if you need the data in memory to be a certain
ordering.  For example you might be writing the memory out to a file
that needs a certain byte ordering.

>>> fixed_end_mem_arr = wrong_end_dtype_arr.byteswap()
>>> fixed_end_mem_arr[0]
1

Now the array *has* changed in memory:

>>> fixed_end_mem_arr.tobytes() == big_end_buffer
False

Data and dtype endianness match, swap data and dtype
----------------------------------------------------

You may have a correctly specified array dtype, but you need the array
to have the opposite byte order in memory, and you want the dtype to
match so the array values make sense.  In this case you just do both of
the previous operations:

>>> swapped_end_arr = big_end_arr.byteswap().newbyteorder()
>>> swapped_end_arr[0]
1
>>> swapped_end_arr.tobytes() == big_end_buffer
False

An easier way of casting the data to a specific dtype and byte ordering
can be achieved with the ndarray astype method:

>>> swapped_end_arr = big_end_arr.astype('<i2')
>>> swapped_end_arr[0]
1
>>> swapped_end_arr.tobytes() == big_end_buffer
False


.. _structured_arrays:

*****************
Structured arrays 
*****************

Introduction
============

Structured arrays are ndarrays whose datatype is a composition of simpler
datatypes organized as a sequence of named :term:`fields <field>`. For example,
::

 >>> x = np.array([('Rex', 9, 81.0), ('Fido', 3, 27.0)],
 ...              dtype=[('name', 'U10'), ('age', 'i4'), ('weight', 'f4')])
 >>> x
 array([('Rex', 9, 81.), ('Fido', 3, 27.)],
       dtype=[('name', '<U10'), ('age', '<i4'), ('weight', '<f4')])

Here ``x`` is a one-dimensional array of length two whose datatype is a
structure with three fields: 1. A string of length 10 or less named 'name', 2.
a 32-bit integer named 'age', and 3. a 32-bit float named 'weight'.

If you index ``x`` at position 1 you get a structure::

 >>> x[1]
 ('Fido', 3, 27.)

You can access and modify individual fields of a structured array by indexing
with the field name::

 >>> x['age']
 array([9, 3], dtype=int32)
 >>> x['age'] = 5
 >>> x
 array([('Rex', 5, 81.), ('Fido', 5, 27.)],
       dtype=[('name', '<U10'), ('age', '<i4'), ('weight', '<f4')])

Structured datatypes are designed to be able to mimic 'structs' in the C
language, and share a similar memory layout. They are meant for interfacing with
C code and for low-level manipulation of structured buffers, for example for
interpreting binary blobs. For these purposes they support specialized features
such as subarrays, nested datatypes, and unions, and allow control over the
memory layout of the structure.

Users looking to manipulate tabular data, such as stored in csv files, may find
other pydata projects more suitable, such as xarray, pandas, or DataArray.
These provide a high-level interface for tabular data analysis and are better
optimized for that use. For instance, the C-struct-like memory layout of
structured arrays in numpy can lead to poor cache behavior in comparison.

.. _defining-structured-types:

Structured Datatypes
====================

A structured datatype can be thought of as a sequence of bytes of a certain
length (the structure's :term:`itemsize`) which is interpreted as a collection
of fields. Each field has a name, a datatype, and a byte offset within the
structure. The datatype of a field may be any numpy datatype including other
structured datatypes, and it may also be a :term:`subarray data type` which
behaves like an ndarray of a specified shape. The offsets of the fields are
arbitrary, and fields may even overlap. These offsets are usually determined
automatically by numpy, but can also be specified.

Structured Datatype Creation
----------------------------

Structured datatypes may be created using the function :func:`numpy.dtype`.
There are 4 alternative forms of specification which vary in flexibility and
conciseness. These are further documented in the
:ref:`Data Type Objects <arrays.dtypes.constructing>` reference page, and in
summary they are:

1.   A list of tuples, one tuple per field

     Each tuple has the form ``(fieldname, datatype, shape)`` where shape is
     optional. ``fieldname`` is a string (or tuple if titles are used, see
     :ref:`Field Titles <titles>` below), ``datatype`` may be any object
     convertible to a datatype, and ``shape`` is a tuple of integers specifying
     subarray shape.

      >>> np.dtype([('x', 'f4'), ('y', np.float32), ('z', 'f4', (2, 2))])
      dtype([('x', '<f4'), ('y', '<f4'), ('z', '<f4', (2, 2))])

     If ``fieldname`` is the empty string ``''``, the field will be given a
     default name of the form ``f#``, where ``#`` is the integer index of the
     field, counting from 0 from the left::

      >>> np.dtype([('x', 'f4'), ('', 'i4'), ('z', 'i8')])
      dtype([('x', '<f4'), ('f1', '<i4'), ('z', '<i8')])

     The byte offsets of the fields within the structure and the total
     structure itemsize are determined automatically.

2.   A string of comma-separated dtype specifications

     In this shorthand notation any of the :ref:`string dtype specifications
     <arrays.dtypes.constructing>` may be used in a string and separated by
     commas. The itemsize and byte offsets of the fields are determined
     automatically, and the field names are given the default names ``f0``,
     ``f1``, etc. ::

      >>> np.dtype('i8, f4, S3')
      dtype([('f0', '<i8'), ('f1', '<f4'), ('f2', 'S3')])
      >>> np.dtype('3int8, float32, (2, 3)float64')
      dtype([('f0', 'i1', (3,)), ('f1', '<f4'), ('f2', '<f8', (2, 3))])

3.   A dictionary of field parameter arrays

     This is the most flexible form of specification since it allows control
     over the byte-offsets of the fields and the itemsize of the structure.

     The dictionary has two required keys, 'names' and 'formats', and four
     optional keys, 'offsets', 'itemsize', 'aligned' and 'titles'. The values
     for 'names' and 'formats' should respectively be a list of field names and
     a list of dtype specifications, of the same length. The optional 'offsets'
     value should be a list of integer byte-offsets, one for each field within
     the structure. If 'offsets' is not given the offsets are determined
     automatically. The optional 'itemsize' value should be an integer
     describing the total size in bytes of the dtype, which must be large
     enough to contain all the fields.
     ::

      >>> np.dtype({'names': ['col1', 'col2'], 'formats': ['i4', 'f4']})
      dtype([('col1', '<i4'), ('col2', '<f4')])
      >>> np.dtype({'names': ['col1', 'col2'],
      ...           'formats': ['i4', 'f4'],
      ...           'offsets': [0, 4],
      ...           'itemsize': 12})
      dtype({'names': ['col1', 'col2'], 'formats': ['<i4', '<f4'], 'offsets': [0, 4], 'itemsize': 12})

     Offsets may be chosen such that the fields overlap, though this will mean
     that assigning to one field may clobber any overlapping field's data. As
     an exception, fields of :class:`numpy.object_` type cannot overlap with
     other fields, because of the risk of clobbering the internal object
     pointer and then dereferencing it.

     The optional 'aligned' value can be set to ``True`` to make the automatic
     offset computation use aligned offsets (see :ref:`offsets-and-alignment`),
     as if the 'align' keyword argument of :func:`numpy.dtype` had been set to
     True.

     The optional 'titles' value should be a list of titles of the same length
     as 'names', see :ref:`Field Titles <titles>` below.

4.   A dictionary of field names

     The use of this form of specification is discouraged, but documented here
     because older numpy code may use it. The keys of the dictionary are the
     field names and the values are tuples specifying type and offset::

      >>> np.dtype({'col1': ('i1', 0), 'col2': ('f4', 1)})
      dtype([('col1', 'i1'), ('col2', '<f4')])

     This form is discouraged because Python dictionaries do not preserve order
     in Python versions before Python 3.6, and the order of the fields in a
     structured dtype has meaning. :ref:`Field Titles <titles>` may be
     specified by using a 3-tuple, see below.

Manipulating and Displaying Structured Datatypes
------------------------------------------------

The list of field names of a structured datatype can be found in the ``names``
attribute of the dtype object::

 >>> d = np.dtype([('x', 'i8'), ('y', 'f4')])
 >>> d.names
 ('x', 'y')

The field names may be modified by assigning to the ``names`` attribute using a
sequence of strings of the same length.

The dtype object also has a dictionary-like attribute, ``fields``, whose keys
are the field names (and :ref:`Field Titles <titles>`, see below) and whose
values are tuples containing the dtype and byte offset of each field. ::

 >>> d.fields
 mappingproxy({'x': (dtype('int64'), 0), 'y': (dtype('float32'), 8)})

Both the ``names`` and ``fields`` attributes will equal ``None`` for
unstructured arrays. The recommended way to test if a dtype is structured is
with `if dt.names is not None` rather than `if dt.names`, to account for dtypes
with 0 fields.

The string representation of a structured datatype is shown in the "list of
tuples" form if possible, otherwise numpy falls back to using the more general
dictionary form.

.. _offsets-and-alignment:

Automatic Byte Offsets and Alignment
------------------------------------

Numpy uses one of two methods to automatically determine the field byte offsets
and the overall itemsize of a structured datatype, depending on whether
``align=True`` was specified as a keyword argument to :func:`numpy.dtype`.

By default (``align=False``), numpy will pack the fields together such that
each field starts at the byte offset the previous field ended, and the fields
are contiguous in memory. ::

 >>> def print_offsets(d):
 ...     print("offsets:", [d.fields[name][1] for name in d.names])
 ...     print("itemsize:", d.itemsize)
 >>> print_offsets(np.dtype('u1, u1, i4, u1, i8, u2'))
 offsets: [0, 1, 2, 6, 7, 15]
 itemsize: 17

If ``align=True`` is set, numpy will pad the structure in the same way many C
compilers would pad a C-struct. Aligned structures can give a performance
improvement in some cases, at the cost of increased datatype size. Padding
bytes are inserted between fields such that each field's byte offset will be a
multiple of that field's alignment, which is usually equal to the field's size
in bytes for simple datatypes, see :c:member:`PyArray_Descr.alignment`.  The
structure will also have trailing padding added so that its itemsize is a
multiple of the largest field's alignment. ::

 >>> print_offsets(np.dtype('u1, u1, i4, u1, i8, u2', align=True))
 offsets: [0, 1, 4, 8, 16, 24]
 itemsize: 32

Note that although almost all modern C compilers pad in this way by default,
padding in C structs is C-implementation-dependent so this memory layout is not
guaranteed to exactly match that of a corresponding struct in a C program. Some
work may be needed, either on the numpy side or the C side, to obtain exact
correspondence.

If offsets were specified using the optional ``offsets`` key in the
dictionary-based dtype specification, setting ``align=True`` will check that
each field's offset is a multiple of its size and that the itemsize is a
multiple of the largest field size, and raise an exception if not.

If the offsets of the fields and itemsize of a structured array satisfy the
alignment conditions, the array will have the ``ALIGNED`` :attr:`flag
<numpy.ndarray.flags>` set.

A convenience function :func:`numpy.lib.recfunctions.repack_fields` converts an
aligned dtype or array to a packed one and vice versa. It takes either a dtype
or structured ndarray as an argument, and returns a copy with fields re-packed,
with or without padding bytes.

.. _titles:

Field Titles
------------

In addition to field names, fields may also have an associated :term:`title`,
an alternate name, which is sometimes used as an additional description or
alias for the field. The title may be used to index an array, just like a
field name.

To add titles when using the list-of-tuples form of dtype specification, the
field name may be specified as a tuple of two strings instead of a single
string, which will be the field's title and field name respectively. For
example::

 >>> np.dtype([(('my title', 'name'), 'f4')])
 dtype([(('my title', 'name'), '<f4')])

When using the first form of dictionary-based specification, the titles may be
supplied as an extra ``'titles'`` key as described above. When using the second
(discouraged) dictionary-based specification, the title can be supplied by
providing a 3-element tuple ``(datatype, offset, title)`` instead of the usual
2-element tuple::

 >>> np.dtype({'name': ('i4', 0, 'my title')})
 dtype([(('my title', 'name'), '<i4')])

The ``dtype.fields`` dictionary will contain titles as keys, if any
titles are used.  This means effectively that a field with a title will be
represented twice in the fields dictionary. The tuple values for these fields
will also have a third element, the field title. Because of this, and because
the ``names`` attribute preserves the field order while the ``fields``
attribute may not, it is recommended to iterate through the fields of a dtype
using the ``names`` attribute of the dtype, which will not list titles, as
in::

 >>> for name in d.names:
 ...     print(d.fields[name][:2])
 (dtype('int64'), 0)
 (dtype('float32'), 8)

Union types
-----------

Structured datatypes are implemented in numpy to have base type
:class:`numpy.void` by default, but it is possible to interpret other numpy
types as structured types using the ``(base_dtype, dtype)`` form of dtype
specification described in
:ref:`Data Type Objects <arrays.dtypes.constructing>`.  Here, ``base_dtype`` is
the desired underlying dtype, and fields and flags will be copied from
``dtype``. This dtype is similar to a 'union' in C.

Indexing and Assignment to Structured arrays
============================================

Assigning data to a Structured Array
------------------------------------

There are a number of ways to assign values to a structured array: Using python
tuples, using scalar values, or using other structured arrays.

Assignment from Python Native Types (Tuples)
````````````````````````````````````````````

The simplest way to assign values to a structured array is using python tuples.
Each assigned value should be a tuple of length equal to the number of fields
in the array, and not a list or array as these will trigger numpy's
broadcasting rules. The tuple's elements are assigned to the successive fields
of the array, from left to right::

 >>> x = np.array([(1, 2, 3), (4, 5, 6)], dtype='i8, f4, f8')
 >>> x[1] = (7, 8, 9)
 >>> x
 array([(1, 2., 3.), (7, 8., 9.)],
      dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '<f8')])

Assignment from Scalars
```````````````````````

A scalar assigned to a structured element will be assigned to all fields. This
happens when a scalar is assigned to a structured array, or when an
unstructured array is assigned to a structured array::

 >>> x = np.zeros(2, dtype='i8, f4, ?, S1')
 >>> x[:] = 3
 >>> x
 array([(3, 3., True, b'3'), (3, 3., True, b'3')],
       dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '?'), ('f3', 'S1')])
 >>> x[:] = np.arange(2)
 >>> x
 array([(0, 0., False, b'0'), (1, 1., True, b'1')],
       dtype=[('f0', '<i8'), ('f1', '<f4'), ('f2', '?'), ('f3', 'S1')])

Structured arrays can also be assigned to unstructured arrays, but only if the
structured datatype has just a single field::

 >>> twofield = np.zeros(2, dtype=[('A', 'i4'), ('B', 'i4')])
 >>> onefield = np.zeros(2, dtype=[('A', 'i4')])
 >>> nostruct = np.zeros(2, dtype='i4')
 >>> nostruct[:] = twofield
 Traceback (most recent call last):
 ...
 TypeError: Cannot cast array data from dtype([('A', '<i4'), ('B', '<i4')]) to dtype('int32') according to the rule 'unsafe'

Assignment from other Structured Arrays
```````````````````````````````````````

Assignment between two structured arrays occurs as if the source elements had
been converted to tuples and then assigned to the destination elements. That
is, the first field of the source array is assigned to the first field of the
destination array, and the second field likewise, and so on, regardless of
field names. Structured arrays with a different number of fields cannot be
assigned to each other. Bytes of the destination structure which are not
included in any of the fields are unaffected. ::

 >>> a = np.zeros(3, dtype=[('a', 'i8'), ('b', 'f4'), ('c', 'S3')])
 >>> b = np.ones(3, dtype=[('x', 'f4'), ('y', 'S3'), ('z', 'O')])
 >>> b[:] = a
 >>> b
 array([(0., b'0.0', b''), (0., b'0.0', b''), (0., b'0.0', b'')],
       dtype=[('x', '<f4'), ('y', 'S3'), ('z', 'O')])


Assignment involving subarrays
``````````````````````````````

When assigning to fields which are subarrays, the assigned value will first be
broadcast to the shape of the subarray.

Indexing Structured Arrays
--------------------------

Accessing Individual Fields
```````````````````````````

Individual fields of a structured array may be accessed and modified by indexing
the array with the field name. ::

 >>> x = np.array([(1, 2), (3, 4)], dtype=[('foo', 'i8'), ('bar', 'f4')])
 >>> x['foo']
 array([1, 3])
 >>> x['foo'] = 10
 >>> x
 array([(10, 2.), (10, 4.)],
       dtype=[('foo', '<i8'), ('bar', '<f4')])

The resulting array is a view into the original array. It shares the same
memory locations and writing to the view will modify the original array. ::

 >>> y = x['bar']
 >>> y[:] = 11
 >>> x
 array([(10, 11.), (10, 11.)],
       dtype=[('foo', '<i8'), ('bar', '<f4')])

This view has the same dtype and itemsize as the indexed field, so it is
typically a non-structured array, except in the case of nested structures.

 >>> y.dtype, y.shape, y.strides
 (dtype('float32'), (2,), (12,))

If the accessed field is a subarray, the dimensions of the subarray
are appended to the shape of the result::

   >>> x = np.zeros((2, 2), dtype=[('a', np.int32), ('b', np.float64, (3, 3))])
   >>> x['a'].shape
   (2, 2)
   >>> x['b'].shape
   (2, 2, 3, 3)

Accessing Multiple Fields
```````````````````````````

One can index and assign to a structured array with a multi-field index, where
the index is a list of field names.

.. warning::
    The behavior of multi-field indexes changed from Numpy 1.15 to Numpy 1.16.

The result of indexing with a multi-field index is a view into the original
array, as follows::

 >>> a = np.zeros(3, dtype=[('a', 'i4'), ('b', 'i4'), ('c', 'f4')])
 >>> a[['a', 'c']]
 array([(0, 0.), (0, 0.), (0, 0.)],
      dtype={'names': ['a', 'c'], 'formats': ['<i4', '<f4'], 'offsets': [0, 8], 'itemsize': 12})

Assignment to the view modifies the original array. The view's fields will be
in the order they were indexed. Note that unlike for single-field indexing, the
dtype of the view has the same itemsize as the original array, and has fields
at the same offsets as in the original array, and unindexed fields are merely
missing.

.. warning::
    In Numpy 1.15, indexing an array with a multi-field index returned a copy of
    the result above, but with fields packed together in memory as if
    passed through :func:`numpy.lib.recfunctions.repack_fields`.

    The new behavior as of Numpy 1.16 leads to extra "padding" bytes at the
    location of unindexed fields compared to 1.15. You will need to update any
    code which depends on the data having a "packed" layout. For instance code
    such as::

     >>> a[['a', 'c']].view('i8')  # Fails in Numpy 1.16
     Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
     ValueError: When changing to a smaller dtype, its size must be a divisor of the size of original dtype

    will need to be changed. This code has raised a ``FutureWarning`` since
    Numpy 1.12, and similar code has raised ``FutureWarning`` since 1.7.

    In 1.16 a number of functions have been introduced in the
    :mod:`numpy.lib.recfunctions` module to help users account for this
    change. These are
    :func:`numpy.lib.recfunctions.repack_fields`.
    :func:`numpy.lib.recfunctions.structured_to_unstructured`,
    :func:`numpy.lib.recfunctions.unstructured_to_structured`,
    :func:`numpy.lib.recfunctions.apply_along_fields`,
    :func:`numpy.lib.recfunctions.assign_fields_by_name`,  and
    :func:`numpy.lib.recfunctions.require_fields`.

    The function :func:`numpy.lib.recfunctions.repack_fields` can always be
    used to reproduce the old behavior, as it will return a packed copy of the
    structured array. The code above, for example, can be replaced with:

     >>> from numpy.lib.recfunctions import repack_fields
     >>> repack_fields(a[['a', 'c']]).view('i8')  # supported in 1.16
     array([0, 0, 0])

    Furthermore, numpy now provides a new function
    :func:`numpy.lib.recfunctions.structured_to_unstructured` which is a safer
    and more efficient alternative for users who wish to convert structured
    arrays to unstructured arrays, as the view above is often indeded to do.
    This function allows safe conversion to an unstructured type taking into
    account padding, often avoids a copy, and also casts the datatypes
    as needed, unlike the view. Code such as:

     >>> b = np.zeros(3, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])
     >>> b[['x', 'z']].view('f4')
     array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)

    can be made safer by replacing with:

     >>> from numpy.lib.recfunctions import structured_to_unstructured
     >>> structured_to_unstructured(b[['x', 'z']])
     array([[0., 0.],
            [0., 0.],
            [0., 0.]], dtype=float32)


Assignment to an array with a multi-field index modifies the original array::

 >>> a[['a', 'c']] = (2, 3)
 >>> a
 array([(2, 0, 3.), (2, 0, 3.), (2, 0, 3.)],
       dtype=[('a', '<i4'), ('b', '<i4'), ('c', '<f4')])

This obeys the structured array assignment rules described above. For example,
this means that one can swap the values of two fields using appropriate
multi-field indexes::

 >>> a[['a', 'c']] = a[['c', 'a']]

Indexing with an Integer to get a Structured Scalar
```````````````````````````````````````````````````

Indexing a single element of a structured array (with an integer index) returns
a structured scalar::

 >>> x = np.array([(1, 2., 3.)], dtype='i, f, f')
 >>> scalar = x[0]
 >>> scalar
 (1, 2., 3.)
 >>> type(scalar)
 <class 'numpy.void'>

Unlike other numpy scalars, structured scalars are mutable and act like views
into the original array, such that modifying the scalar will modify the
original array. Structured scalars also support access and assignment by field
name::

 >>> x = np.array([(1, 2), (3, 4)], dtype=[('foo', 'i8'), ('bar', 'f4')])
 >>> s = x[0]
 >>> s['bar'] = 100
 >>> x
 array([(1, 100.), (3, 4.)],
       dtype=[('foo', '<i8'), ('bar', '<f4')])

Similarly to tuples, structured scalars can also be indexed with an integer::

 >>> scalar = np.array([(1, 2., 3.)], dtype='i, f, f')[0]
 >>> scalar[0]
 1
 >>> scalar[1] = 4

Thus, tuples might be thought of as the native Python equivalent to numpy's
structured types, much like native python integers are the equivalent to
numpy's integer types. Structured scalars may be converted to a tuple by
calling `numpy.ndarray.item`::

 >>> scalar.item(), type(scalar.item())
 ((1, 4.0, 3.0), <class 'tuple'>)

Viewing Structured Arrays Containing Objects
--------------------------------------------

In order to prevent clobbering object pointers in fields of
:class:`object` type, numpy currently does not allow views of structured
arrays containing objects.

Structure Comparison
--------------------

If the dtypes of two void structured arrays are equal, testing the equality of
the arrays will result in a boolean array with the dimensions of the original
arrays, with elements set to ``True`` where all fields of the corresponding
structures are equal. Structured dtypes are equal if the field names,
dtypes and titles are the same, ignoring endianness, and the fields are in
the same order::

 >>> a = np.zeros(2, dtype=[('a', 'i4'), ('b', 'i4')])
 >>> b = np.ones(2, dtype=[('a', 'i4'), ('b', 'i4')])
 >>> a == b
 array([False, False])

Currently, if the dtypes of two void structured arrays are not equivalent the
comparison fails, returning the scalar value ``False``. This behavior is
deprecated as of numpy 1.10 and will raise an error or perform elementwise
comparison in the future.

The ``<`` and ``>`` operators always return ``False`` when comparing void
structured arrays, and arithmetic and bitwise operations are not supported.

Record Arrays
=============

As an optional convenience numpy provides an ndarray subclass,
:class:`numpy.recarray` that allows access to fields of structured arrays by
attribute instead of only by index.
Record arrays use a special datatype, :class:`numpy.record`, that allows
field access by attribute on the structured scalars obtained from the array.
The ``numpy.rec`` module provides functions for creating recarrays from
various objects.
Additional helper functions for creating and manipulating structured arrays
can be found in :mod:`numpy.lib.recfunctions`.

The simplest way to create a record array is with
:func:`numpy.rec.array <numpy.core.records.array>`::

 >>> recordarr = np.rec.array([(1, 2., 'Hello'), (2, 3., "World")],
 ...                    dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'S10')])
 >>> recordarr.bar
 array([2., 3.], dtype=float32)
 >>> recordarr[1:2]
 rec.array([(2, 3., b'World')],
       dtype=[('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')])
 >>> recordarr[1:2].foo
 array([2], dtype=int32)
 >>> recordarr.foo[1:2]
 array([2], dtype=int32)
 >>> recordarr[1].baz
 b'World'

:func:`numpy.rec.array <numpy.core.records.array>` can convert a wide variety
of arguments into record arrays, including structured arrays::

 >>> arr = np.array([(1, 2., 'Hello'), (2, 3., "World")],
 ...             dtype=[('foo', 'i4'), ('bar', 'f4'), ('baz', 'S10')])
 >>> recordarr = np.rec.array(arr)

The ``numpy.rec`` module provides a number of other convenience functions for
creating record arrays, see :ref:`record array creation routines
<routines.array-creation.rec>`.

A record array representation of a structured array can be obtained using the
appropriate `view <numpy-ndarray-view>`_::

 >>> arr = np.array([(1, 2., 'Hello'), (2, 3., "World")],
 ...                dtype=[('foo', 'i4'),('bar', 'f4'), ('baz', 'a10')])
 >>> recordarr = arr.view(dtype=np.dtype((np.record, arr.dtype)),
 ...                      type=np.recarray)

For convenience, viewing an ndarray as type :class:`numpy.recarray` will
automatically convert to :class:`numpy.record` datatype, so the dtype can be left
out of the view::

 >>> recordarr = arr.view(np.recarray)
 >>> recordarr.dtype
 dtype((numpy.record, [('foo', '<i4'), ('bar', '<f4'), ('baz', 'S10')]))

To get back to a plain ndarray both the dtype and type must be reset. The
following view does so, taking into account the unusual case that the
recordarr was not a structured type::

 >>> arr2 = recordarr.view(recordarr.dtype.fields or recordarr.dtype, np.ndarray)

Record array fields accessed by index or by attribute are returned as a record
array if the field has a structured type but as a plain ndarray otherwise. ::

 >>> recordarr = np.rec.array([('Hello', (1, 2)), ("World", (3, 4))],
 ...                 dtype=[('foo', 'S6'),('bar', [('A', int), ('B', int)])])
 >>> type(recordarr.foo)
 <class 'numpy.ndarray'>
 >>> type(recordarr.bar)
 <class 'numpy.recarray'>

Note that if a field has the same name as an ndarray attribute, the ndarray
attribute takes precedence. Such fields will be inaccessible by attribute but
will still be accessible by index.


Recarray Helper Functions
-------------------------

.. automodule:: numpy.lib.recfunctions
    :members:
:orphan:

****************
Installing NumPy
****************

See `Installing NumPy <https://numpy.org/install/>`_... _how-to-how-to:

##############################################################################
How to write a NumPy how-to
##############################################################################

How-tos get straight to the point -- they

  - answer a focused question, or
  - narrow a broad question into focused questions that the user can
    choose among.

******************************************************************************
A stranger has asked for directions...
******************************************************************************

**"I need to refuel my car."**

******************************************************************************
Give a brief but explicit answer
******************************************************************************

  - `"Three kilometers/miles, take a right at Hayseed Road, it's on your left."`

Add helpful details for newcomers ("Hayseed Road", even though it's the only
turnoff at three km/mi). But not irrelevant ones:

  - Don't also give directions from Route 7.
  - Don't explain why the town has only one filling station.

If there's related background (tutorial, explanation, reference, alternative
approach), bring it to the user's attention with a link ("Directions from Route 7,"
"Why so few filling stations?").


******************************************************************************
Delegate
******************************************************************************

  - `"Three km/mi, take a right at Hayseed Road, follow the signs."`

If the information is already documented and succinct enough for a how-to,
just link to it, possibly after an introduction ("Three km/mi, take a right").

******************************************************************************
If the question is broad, narrow and redirect it
******************************************************************************

 **"I want to see the sights."**

The `See the sights` how-to should link to a set of narrower how-tos:

- Find historic buildings
- Find scenic lookouts
- Find the town center

and these might in turn link to still narrower how-tos -- so the town center
page might link to

   - Find the court house
   - Find city hall

By organizing how-tos this way, you not only display the options for people
who need to narrow their question, you also have provided answers for users
who start with narrower questions ("I want to see historic buildings," "Which
way to city hall?").

******************************************************************************
If there are many steps, break them up
******************************************************************************

If a how-to has many steps:

  - Consider breaking a step out into an individual how-to and linking to it.
  - Include subheadings. They help readers grasp what's coming and return
    where they left off.

******************************************************************************
Why write how-tos when there's Stack Overflow, Reddit, Gitter...?
******************************************************************************

 - We have authoritative answers.
 - How-tos make the site less forbidding to non-experts.
 - How-tos bring people into the site and help them discover other information
   that's here .
 - Creating how-tos helps us see NumPy usability through new eyes.

******************************************************************************
Aren't how-tos and tutorials the same thing?
******************************************************************************

People use the terms "how-to" and "tutorial" interchangeably, but we draw a
distinction, following Daniele Procida's `taxonomy of documentation`_.

 .. _`taxonomy of documentation`: https://documentation.divio.com/

Documentation needs to meet users where they are.  `How-tos` offer get-it-done
information; the user wants steps to copy and doesn't necessarily want to
understand NumPy. `Tutorials` are warm-fuzzy information; the user wants a
feel for some aspect of NumPy (and again, may or may not care about deeper
knowledge).

We distinguish both tutorials and how-tos from `Explanations`, which are
deep dives intended to give understanding rather than immediate assistance,
and `References`, which give complete, authoritative data on some concrete
part of NumPy (like its API) but aren't obligated to paint a broader picture.

For more on tutorials, see :doc:`content/tutorial-style-guide`

******************************************************************************
Is this page an example of a how-to?
******************************************************************************

Yes -- until the sections with question-mark headings; they explain rather
than giving directions. In a how-to, those would be links.
:orphan:

.. Reason for orphan: This page is referenced by the installation
   instructions, which have moved from Sphinx to https://numpy.org/install.
   All install links in Sphinx now point there, leaving no Sphinx references
   to this page.


***************************
Troubleshooting ImportError
***************************

.. note::

    Since this information may be updated regularly, please ensure you are
    viewing the most `up-to-date version <https://numpy.org/devdocs/user/troubleshooting-importerror.html>`_.


ImportError
===========

In certain cases a failed installation or setup issue can cause you to
see the following error message::

    IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

    Importing the numpy c-extensions failed. This error can happen for
    different reasons, often due to issues with your setup.

The error also has additional information to help you troubleshoot:

* Your Python version
* Your NumPy version

Please check both of these carefully to see if they are what you expect.
You may need to check your ``PATH`` or ``PYTHONPATH`` environment variables
(see `Check Environment Variables`_ below).

The following sections list commonly reported issues depending on your setup.
If you have an issue/solution that you think should appear please open a
NumPy issue so that it will be added.

There are a few commonly reported issues depending on your system/setup.
If none of the following tips help you, please be sure to note the following:

* how you installed Python
* how you installed NumPy
* your operating system
* whether or not you have multiple versions of Python installed
* if you built from source, your compiler versions and ideally a build log

when investigating further and asking for support.


Using Python from ``conda`` (Anaconda)
--------------------------------------

Please make sure that you have activated your conda environment.
See also the `conda user-guide <https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment>`_.
If you use an external editor/development environment it will have to be set
up correctly.  See below for solutions for some common setups.

Using PyCharm with Anaconda/conda Python
----------------------------------------

There are fairly common issues when using PyCharm together with Anaconda,
please see the `PyCharm support <https://www.jetbrains.com/help/pycharm/conda-support-creating-conda-virtual-environment.html>`_

Using VSCode with Anaconda/conda Python (or environments)
---------------------------------------------------------

A commonly reported issue is related to the environment activation within
VSCode. Please see the `VSCode support <https://code.visualstudio.com/docs/python/environments>`_
for information on how to correctly set up VSCode with virtual environments
or conda.

Using Eclipse/PyDev with Anaconda/conda Python (or environments)
----------------------------------------------------------------

Please see the
`Anaconda Documentation <https://docs.anaconda.com/anaconda/user-guide/tasks/integration/eclipse-pydev/>`_
on how to properly configure Eclipse/PyDev to use Anaconda Python with specific
conda environments.


Raspberry Pi
------------

There are sometimes issues reported on Raspberry Pi setups when installing
using ``pip3 install`` (or ``pip`` install). These will typically mention::

    libf77blas.so.3: cannot open shared object file: No such file or directory


The solution will be to either::

    sudo apt-get install libatlas-base-dev

to install the missing libraries expected by the self-compiled NumPy
(ATLAS is a possible provider of linear algebra).

*Alternatively* use the NumPy provided by Raspbian. In which case run::

    pip3 uninstall numpy  # remove previously installed version
    apt install python3-numpy


Debug build on Windows
----------------------

Rather than building your project in ``DEBUG`` mode on windows, try
building in ``RELEASE`` mode with debug symbols and no optimization.
Full ``DEBUG`` mode on windows changes the names of the DLLs python
expects to find, so if you wish to truly work in ``DEBUG`` mode you will
need to recompile the entire stack of python modules you work with
including NumPy


All Setups
----------

Occasionally there may be simple issues with old or bad installations
of NumPy. In this case you may just try to uninstall and reinstall NumPy.
Make sure that NumPy is not found after uninstalling.


Development Setup
-----------------

If you are using a development setup, make sure to run ``git clean -xdf``
to delete all files not under version control (be careful not to lose
any modifications you made, e.g. ``site.cfg``).
In many cases files from old builds may lead to incorrect builds.


Check Environment Variables
---------------------------

In general how to set and check your environment variables depends on
your system. If you can open a correct python shell, you can also run the
following in python::

    import os
    print("PYTHONPATH:", os.environ.get('PYTHONPATH'))
    print("PATH:", os.environ.get('PATH'))

This may mainly help you if you are not running the python and/or NumPy
version you are expecting to run.
``alen`` and ``asscalar`` removed
---------------------------------

The deprecated ``np.alen`` and ``np.asscalar`` functions were removed.
Deprecate PyDataMem_SetEventHook
--------------------------------

The ability to track allocations is now built-in to python via ``tracemalloc``.
The hook function ``PyDataMem_SetEventHook`` has been deprecated and the
demonstration of its use in tool/allocation_tracking has been removed.
``ndarray.__array_finalize__`` is now callable
----------------------------------------------
This means subclasses can now use ``super().__array_finalize__(obj)``
without worrying whether ``ndarray`` is their superclass or not.
The actual call remains a no-op.
Changing to dtype of a different size now requires contiguity of only the last axis
-----------------------------------------------------------------------------------

Previously, viewing an array with a dtype of a different itemsize required that
the entire array be C-contiguous. This limitation would unnecessarily force the
user to make contiguous copies of non-contiguous arrays before being able to
change the dtype.

This change affects not only ``ndarray.view``, but other construction
mechanisms, including the discouraged direct assignment to ``ndarray.dtype``.

This change expires the deprecation regarding the viewing of F-contiguous
arrays, described elsewhere in the release notes.
New parameter ``ndmin`` added to ``genfromtxt``
-------------------------------------------------------------------------
This parameter behaves the same as ``ndmin`` from `~numpy.loadtxt`.* Setting ``__array_finalize__`` to ``None`` is deprecated.  It must now be
  a method and may wish to call ``super().__array_finalize__(obj)`` after
  checking for ``None`` or if the NumPy version is sufficiently new.
Changing to dtype of different size in F-contiguous arrays no longer permitted
------------------------------------------------------------------------------

Behavior deprecated in NumPy 1.11.0 allowed the following counterintuitive result::

    >>> x = np.array(["aA", "bB", "cC", "dD", "eE", "fF"]).reshape(1, 2, 3).transpose()
    >>> x.view('U1')  # deprecated behavior, shape (6, 2, 1)
    DeprecationWarning: ...
    array([[['a'],
            ['d']],
    
           [['A'],
            ['D']],
    
           [['b'],
            ['e']],
    
           [['B'],
            ['E']],
    
           [['c'],
            ['f']],
    
           [['C'],
            ['F']]], dtype='<U1')

Now that the deprecation has expired, dtype reassignment only happens along the
last axis, so the above will result in::

    >>> x.view('U1')  # new behavior, shape (3, 2, 2)
    array([[['a', 'A'],
            ['d', 'D']],
    
           [['b', 'B'],
            ['e', 'E']],
    
           [['c', 'C'],
            ['f', 'F']]], dtype='<U1')

When the last axis is not contiguous, an error is now raised in place of the `DeprecationWarning`::

    >>> x = np.array(["aA", "bB", "cC", "dD", "eE", "fF"]).reshape(2, 3).transpose()
    >>> x.view('U1')
    ValueError: To change to a dtype of a different size, the last axis must be contiguous

The new behavior is equivalent to the more intuitive::

    >>> x.copy().view('U1')

To replicate the old behavior on F-but-not-C-contiguous arrays, use::

    >>> x.T.view('U1').T
f2py supports reading access type attributes from derived type statements
-------------------------------------------------------------------------
As a result, one does not need to use `public` or `private` statements to
specify derived type access properties.
crackfortran has support for operator and assignment overloading
----------------------------------------------------------------
``crackfortran`` parser now understands operator and assignment
definitions in a module. They are added in the ``body`` list of the
module which contains a new key ``implementedby`` listing the names
of the subroutines or functions implementing the operator or
assignment.
Remove deprecated ``NPY_ARRAY_UPDATEIFCOPY``
--------------------------------------------

The array flag ``UPDATEIFCOPY`` and enum ``NPY_ARRAY_UPDATEIFCOPY`` were
deprecated in 1.14. They were replaced by ``WRITEBACKIFCOPY`` which require
calling ``PyArray_ResoveWritebackIfCopy`` before the array is deallocated. Also
removed the associated (and deprecated) ``PyArray_XDECREF_ERR``.
:orphan:

Changelog
=========

This directory contains "news fragments" which are short files that contain a
small **ReST**-formatted text that will be added to the next what's new page.

Make sure to use full sentences with correct case and punctuation, and please
try to use Sphinx intersphinx using backticks. The fragment should have a
header line and an underline using ``------``

Each file should be named like ``<PULL REQUEST>.<TYPE>.rst``, where
``<PULL REQUEST>`` is a pull request number, and ``<TYPE>`` is one of:

* ``new_function``: New user facing functions.
* ``deprecation``: Changes existing code to emit a DeprecationWarning.
* ``future``: Changes existing code to emit a FutureWarning.
* ``expired``: Removal of a deprecated part of the API.
* ``compatibility``: A change which requires users to change code and is not
  backwards compatible. (Not to be used for removal of deprecated features.)
* ``c_api``: Changes in the Numpy C-API exported functions
* ``new_feature``: New user facing features like ``kwargs``.
* ``improvement``: General improvements and edge-case changes which are
  not new features or compatibility related.
* ``performance``: Performance changes that should not affect other behaviour.
* ``change``: Other changes
* ``highlight``: Adds a highlight bullet point to use as a possibly highlight
  of the release.

It is possible to add two files with different categories (and text) if both
are relevant. For example a change may improve performance but have some
compatibility concerns.

Most categories should be formatted as paragraphs with a heading.
So for example: ``123.new_feature.rst`` would have the content::

    ``my_new_feature`` option for `my_favorite_function`
    ----------------------------------------------------
    The ``my_new_feature`` option is now available for `my_favorite_function`.
    To use it, write ``np.my_favorite_function(..., my_new_feature=True)``.

``highlight`` is usually formatted as bulled points making the fragment
``* This is a highlight``.

Note the use of single-backticks to get an internal link (assuming
``my_favorite_function`` is exported from the ``numpy`` namespace),
and double-backticks for code.

If you are unsure what pull request type to use, don't hesitate to ask in your
PR.

You can install ``towncrier`` and run ``towncrier build --draft --version 1.18``
if you want to get a preview of how your change will look in the final release
notes.

.. note::

    This README was adapted from the pytest changelog readme under the terms of
    the MIT licence.

{% set title = "NumPy {} Release Notes".format(versiondata.version) %}
{{ "=" * title|length }}
{{ title }}
{{ "=" * title|length }}

{% for section, _ in sections.items() %}
{% set underline = underlines[0] %}{% if section %}{{ section }}
{{ underline * section|length }}{% set underline = underlines[1] %}

{% endif %}
{% if sections[section] %}
{% for category, val in definitions.items() if category in sections[section] %}

{{ definitions[category]['name'] }}
{{ underline * definitions[category]['name']|length }}

{% if definitions[category]['showcontent'] %}
{% for text, values in sections[section][category].items() %}
{{ text }}

{{ get_indent(text) }}({{values|join(', ') }})

{% endfor %}
{% else %}
- {{ sections[section][category]['']|join(', ') }}

{% endif %}
{% if sections[section][category]|length == 0 %}
No significant changes.

{% else %}
{% endif %}
{% endfor %}
{% else %}
No significant changes.


{% endif %}
{% endfor %}
