About the Copyright Holders
===========================

*   Copyright (c) 2008-2011 AQR Capital Management, LLC

    AQR Capital Management began pandas development in 2008. Development was
    led by Wes McKinney. AQR released the source under this license in 2009.
*   Copyright (c) 2011-2012, Lambda Foundry, Inc.

    Wes is now an employee of Lambda Foundry, and remains the pandas project
    lead.
*   Copyright (c) 2011-2012, PyData Development Team

    The PyData Development Team is the collection of developers of the PyData
    project. This includes all of the PyData sub-projects, including pandas. The
    core team that coordinates development on GitHub can be found here:
    https://github.com/pydata.

Full credits for pandas contributors can be found in the documentation.

Our Copyright Policy
====================

PyData uses a shared copyright model. Each contributor maintains copyright
over their contributions to PyData. However, it is important to note that
these contributions are typically only changes to the repositories. Thus,
the PyData source code, in its entirety, is not the copyright of any single
person or institution. Instead, it is the collective copyright of the
entire PyData Development Team. If individual contributors want to maintain
a record of what changes/contributions they have specific copyright on,
they should indicate their copyright in the commit message of the change
when they commit the change to one of the PyData repositories.

With this in mind, the following banner should be used in any source code
file to indicate the copyright and license terms:

```
#-----------------------------------------------------------------------------
# Copyright (c) 2012, PyData Development Team
# All rights reserved.
#
# Distributed under the terms of the BSD Simplified License.
#
# The full license is in the LICENSE file, distributed with this software.
#-----------------------------------------------------------------------------
```

Other licenses can be found in the LICENSES directory.

License
=======

pandas is distributed under a 3-clause ("Simplified" or "New") BSD
license. Parts of NumPy, SciPy, numpydoc, bottleneck, which all have
BSD-compatible licenses, are included. Their licenses follow the pandas
license.
<div align="center">
  <img src="https://pandas.pydata.org/static/img/pandas.svg"><br>
</div>

-----------------

# pandas: powerful Python data analysis toolkit
[![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/)
[![Conda Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/anaconda/pandas/)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134)
[![Package Status](https://img.shields.io/pypi/status/pandas.svg)](https://pypi.org/project/pandas/)
[![License](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/main/LICENSE)
[![Azure Build Status](https://dev.azure.com/pandas-dev/pandas/_apis/build/status/pandas-dev.pandas?branch=main)](https://dev.azure.com/pandas-dev/pandas/_build/latest?definitionId=1&branch=main)
[![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=main)](https://codecov.io/gh/pandas-dev/pandas)
[![Downloads](https://static.pepy.tech/personalized-badge/pandas?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads%20per%20month)](https://pepy.tech/project/pandas)
[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pydata/pandas)
[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)

## What is it?

**pandas** is a Python package that provides fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, **real world** data analysis in Python. Additionally, it has
the broader goal of becoming **the most powerful and flexible open source data
analysis / manipulation tool available in any language**. It is already well on
its way towards this goal.

## Main Features
Here are just a few of the things that pandas does well:

  - Easy handling of [**missing data**][missing-data] (represented as
    `NaN`, `NA`, or `NaT`) in floating point as well as non-floating point data
  - Size mutability: columns can be [**inserted and
    deleted**][insertion-deletion] from DataFrame and higher dimensional
    objects
  - Automatic and explicit [**data alignment**][alignment]: objects can
    be explicitly aligned to a set of labels, or the user can simply
    ignore the labels and let `Series`, `DataFrame`, etc. automatically
    align the data for you in computations
  - Powerful, flexible [**group by**][groupby] functionality to perform
    split-apply-combine operations on data sets, for both aggregating
    and transforming data
  - Make it [**easy to convert**][conversion] ragged,
    differently-indexed data in other Python and NumPy data structures
    into DataFrame objects
  - Intelligent label-based [**slicing**][slicing], [**fancy
    indexing**][fancy-indexing], and [**subsetting**][subsetting] of
    large data sets
  - Intuitive [**merging**][merging] and [**joining**][joining] data
    sets
  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of
    data sets
  - [**Hierarchical**][mi] labeling of axes (possible to have multiple
    labels per tick)
  - Robust IO tools for loading data from [**flat files**][flat-files]
    (CSV and delimited), [**Excel files**][excel], [**databases**][db],
    and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]
  - [**Time series**][timeseries]-specific functionality: date range
    generation and frequency conversion, moving window statistics,
    date shifting and lagging


   [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
   [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
   [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
   [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
   [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
   [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
   [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
   [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
   [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
   [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
   [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
   [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
   [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
   [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
   [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
   [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
   [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality

## Where to get it
The source code is currently hosted on GitHub at:
https://github.com/pandas-dev/pandas

Binary installers for the latest released version are available at the [Python
Package Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://docs.conda.io/en/latest/).

```sh
# conda
conda install pandas
```

```sh
# or PyPI
pip install pandas
```

## Dependencies
- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)
- [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)
- [pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations](https://github.com/stub42/pytz)

See the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.

## Installation from sources
To install pandas from source you need [Cython](https://cython.org/) in addition to the normal
dependencies above. Cython can be installed from PyPI:

```sh
pip install cython
```

In the `pandas` directory (same one where you found this file after
cloning the git repo), execute:

```sh
python setup.py install
```

or for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):


```sh
python -m pip install -e . --no-build-isolation --no-use-pep517
```

If you have `make`, you can also use `make develop` to run the same command.

or alternatively

```sh
python setup.py develop
```

See the full instructions for [installing from source](https://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source).

## License
[BSD 3](LICENSE)

## Documentation
The official documentation is hosted on PyData.org: https://pandas.pydata.org/pandas-docs/stable

## Background
Work on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and
has been under active development since then.

## Getting Help

For usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).
Further, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).

## Discussion and Development
Most development discussions take place on GitHub in this repo. Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Gitter channel](https://gitter.im/pydata/pandas) is available for quick development related questions.

## Contributing to pandas [![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)

All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.

A detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.

If you are simply looking to start working with the pandas codebase, navigate to the [GitHub "issues" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.

You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).

Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!

Feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Gitter](https://gitter.im/pydata/pandas).

As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/pandas/blob/main/.github/CODE_OF_CONDUCT.md)
Release Notes
=============

The list of changes to Pandas between each release can be found
[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full
details, see the commit logs at https://github.com/pandas-dev/pandas.
Directory containing the pandas website (hosted at https://pandas.pydata.org).

The website sources are in `web/pandas/`, which also include a `config.yml` file
containing the settings to build the website. The website is generated with the
command `./pandas_web.py pandas`. See `./pandas_web.py --help` and the header of
the script for more information and options.

After building the website, to navigate it, it is needed to access the web using
an http server (a not open the local files with the browser, since the links and
the image sources are absolute to where they are served from). The easiest way
to run an http server locally is to run `python -m http.server` from the
`web/build/` directory.
# Donate to pandas

<div id="salsalabs-donate-container">
</div>
<script type="text/javascript"
        src="https://default.salsalabs.org/api/widget/template/4ba4e328-1855-47c8-9a89-63e4757d2151/?tId=salsalabs-donate-container">
</script>

_pandas_ is a Sponsored Project of [NumFOCUS](https://numfocus.org/), a 501(c)(3) nonprofit charity in the United States.
NumFOCUS provides _pandas_ with fiscal, legal, and administrative support to help ensure the
health and sustainability of the project. Visit numfocus.org for more information.

Donations to _pandas_ are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible
to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.
# Try pandas online

<section>
    <pre data-executable>
import pandas
fibonacci = pandas.Series([1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144])
fibonacci.sum()
    </pre>
    <script src="https://combinatronics.com/ines/juniper/v0.1.0/dist/juniper.min.js"></script>
    <script>new Juniper({ repo: 'datapythonista/pandas-web' })</script>
</section>

## Interactive tutorials

You can also try _pandas_ on [Binder](https://mybinder.org/) for one of the next topics:

- Exploratory analysis of US presidents
- Preprocessing the Titanic dataset to train a machine learning model
- Forecasting the stock market

_(links will be added soon)_
# Contribute to pandas

_pandas_ is and will always be **free**. To make the development sustainable, we need _pandas_ users, corporate
and individual, to support the development by providing their time and money.

You can find more information about current developers in the [team page](about/team.html),
and about current sponsors in the [sponsors page](about/sponsors.html).

<section>
    <div class="container mt-5">
      <div class="row text-center">
        <div class="col-md-4">
          <span class="fa-stack fa-4x">
            <i class="fas fa-circle fa-stack-2x pink"></i>
            <i class="fas fa-building fa-stack-1x fa-inverse"></i>
          </span>
          <h4 class="service-heading mt-3 fw-bold blue">Corporate support</h4>
          <p class="text-muted">
            pandas depends on companies and institutions using the software to support its development. Hiring
            people to work on pandas, or letting existing employees to contribute to the
            software. Or sponsoring pandas with funds, so the project can hire people to
            progress on the <a href="about/roadmap.html">pandas roadmap</a>.
          </p>
          <p>More information in the <a href="about/sponsors.html">sponsors page</a></p>
        </div>
        <div class="col-md-4">
          <span class="fa-stack fa-4x">
            <i class="fas fa-circle fa-stack-2x pink"></i>
            <i class="fas fa-users fa-stack-1x fa-inverse"></i>
          </span>
          <h4 class="service-heading mt-3 fw-bold blue">Individual contributors</h4>
          <p class="text-muted">
            pandas is mostly developed by volunteers. All kind of contributions are welcome,
            such as contributions to the code, to the website (including graphical designers),
            to the documentation (including translators) and others. There are tasks for all
            levels, including beginners.
          </p>
          <p>More information in the <a href="{{ base_url }}/docs/development/index.html">contributing page</a></p>
        </div>
        <div class="col-md-4">
          <span class="fa-stack fa-4x">
            <i class="fas fa-circle fa-stack-2x pink"></i>
            <i class="fas fa-dollar-sign fa-stack-1x fa-inverse"></i>
          </span>
          <h4 class="service-heading mt-3 fw-bold blue">Donations</h4>
          <p class="text-muted">
            Individual donations are appreciated, and are used for things like the project
            infrastructure, travel expenses for our volunteer contributors to attend
            the in-person sprints, or to give small grants to develop features.
          </p>
          <p>Make your donation in the <a href="donate.html">donate page</a></p>
        </div>
      </div>
    </div>
</section>
# Getting started

## Installation instructions

The next steps provides the easiest and recommended way to set up your
environment to use pandas. Other installation options can be found in
the [advanced installation page]({{ base_url}}/docs/getting_started/install.html).

1. Download [Anaconda](https://www.anaconda.com/distribution/) for your operating system and
   the latest Python version, run the installer, and follow the steps. Please note:

    - It is not needed (and discouraged) to install Anaconda as root or administrator.
    - When asked if you wish to initialize Anaconda3, answer yes.
    - Restart the terminal after completing the installation.

    Detailed instructions on how to install Anaconda can be found in the
    [Anaconda documentation](https://docs.anaconda.com/anaconda/install/).

2. In the Anaconda prompt (or terminal in Linux or MacOS), start JupyterLab:

    <img class="img-fluid" alt="" src="{{ base_url }}/static/img/install/anaconda_prompt.png"/>

3. In JupyterLab, create a new (Python 3) notebook:

    <img class="img-fluid" alt="" src="{{ base_url }}/static/img/install/jupyterlab_home.png"/>

4. In the first cell of the notebook, you can import pandas and check the version with:

    <img class="img-fluid" alt="" src="{{ base_url }}/static/img/install/pandas_import_and_version.png"/>

5. Now you are ready to use pandas, and you can write your code in the next cells.

## Tutorials

You can learn more about pandas in the [tutorials]({{ base_url }}/docs/getting_started/intro_tutorials/),
and more about JupyterLab in the
[JupyterLab documentation](https://jupyterlab.readthedocs.io/en/stable/user/interface.html).

## Books

The book we recommend to learn pandas is [Python for Data Analysis](https://amzn.to/2KI5JJw),
by [Wes McKinney](https://wesmckinney.com/), creator of pandas.

<a href="https://amzn.to/2KI5JJw">
    <img alt="Python for Data Analysis" src="{{ base_url }}/static/img/pydata_book.gif"/>
</a>

## Videos

<iframe width="560" height="315" frameborder="0"
src="https://www.youtube.com/embed/_T8LGqJtuGc"
allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
allowfullscreen></iframe>

## Cheat sheet

[pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
# About pandas

## History of development

In 2008, _pandas_ development began at [AQR Capital Management](https://www.aqr.com).
By the end of 2009 it had been [open sourced](https://en.wikipedia.org/wiki/Open_source),
and is actively supported today by a community of like-minded individuals around the world who
contribute their valuable time and energy to help make open source _pandas_
possible. Thank you to [all of our contributors](team.html).

Since 2015, _pandas_ is a [NumFOCUS sponsored project](https://numfocus.org/sponsored-projects).
This will help ensure the success of development of _pandas_ as a world-class open-source project.

### Timeline

- **2008**:  Development of _pandas_ started
- **2009**: _pandas_ becomes open source
- **2012**: First edition of _Python for Data Analysis_ is published
- **2015**: _pandas_ becomes a [NumFOCUS sponsored project](https://numfocus.org/sponsored-projects)
- **2018**: First in-person core developer sprint

## Library Highlights

- A fast and efficient **DataFrame** object for data manipulation with
  integrated indexing;

- Tools for **reading and writing data** between in-memory data structures and
  different formats: CSV and text files, Microsoft Excel, SQL databases, and
  the fast HDF5 format;

- Intelligent **data alignment** and integrated handling of **missing data**:
  gain automatic label-based alignment in computations and easily manipulate
  messy data into an orderly form;

- Flexible **reshaping** and pivoting of data sets;

- Intelligent label-based **slicing**, **fancy indexing**, and **subsetting**
  of large data sets;

- Columns can be inserted and deleted from data structures for **size
  mutability**;

- Aggregating or transforming data with a powerful **group by** engine
  allowing split-apply-combine operations on data sets;

- High performance **merging and joining** of data sets;

- **Hierarchical axis indexing** provides an intuitive way of working with
  high-dimensional data in a lower-dimensional data structure;

- **Time series**-functionality: date range generation and frequency
  conversion, moving window statistics, date shifting and lagging.
  Even create domain-specific time offsets and join time
  series without losing data;

- Highly **optimized for performance**, with critical code paths written in
  [Cython](http://www.cython.org/) or C.

- Python with *pandas* is in use in a wide variety of **academic and
  commercial** domains, including Finance, Neuroscience, Economics,
  Statistics, Advertising, Web Analytics, and more.

## Mission

_pandas_ aims to be the fundamental high-level building block for doing practical,
real world data analysis in Python.
Additionally, it has the broader goal of becoming the most powerful and flexible
open source data analysis / manipulation tool available in any language.

## Vision

A world where data analytics and manipulation software is:

- Accessible to everyone
- Free for users to use and modify
- Flexible
- Powerful
- Easy to use
- Fast

## Values

Is in the core of _pandas_ to be respectful and welcoming with everybody,
users, contributors and the broader community. Regardless of level of experience,
gender, gender identity and expression, sexual orientation, disability,
personal appearance, body size, race, ethnicity, age, religion, or nationality.
# Citing and logo

## Citing pandas

If you use _pandas_ for a scientific publication, we would appreciate citations to the published software and the
following paper:

- [pandas on Zenodo](https://zenodo.org/record/3715232#.XoqFyC2ZOL8),
   Please find us on Zenodo and replace with the citation for the version you are using. You can replace the full author
   list from there with "The pandas development team" like in the example below.

        @software{reback2020pandas,
            author       = {The pandas development team},
            title        = {pandas-dev/pandas: Pandas},
            month        = feb,
            year         = 2020,
            publisher    = {Zenodo},
            version      = {latest},
            doi          = {10.5281/zenodo.3509134},
            url          = {https://doi.org/10.5281/zenodo.3509134}
        }

- [Data structures for statistical computing in python](https://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf),
   McKinney, Proceedings of the 9th Python in Science Conference, Volume 445, 2010.

        @InProceedings{ mckinney-proc-scipy-2010,
          author    = { {W}es {M}c{K}inney },
          title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
          booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
          pages     = { 56 - 61 },
          year      = { 2010 },
          editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
          doi       = { 10.25080/Majora-92bf1922-00a }
        }

## Brand and logo

When using the project name _pandas_, please use it in lower case, even at the beginning of a sentence.

The official logos of _pandas_ are:

### Primary logo

<table class="table logo">
    <tr>
        <td>
            <img alt="" src="{{ base_url }}/static/img/pandas.svg"/>
        </td>
        <td style="background-color: #150458">
            <img alt="" src="{{ base_url }}/static/img/pandas_white.svg"/>
        </td>
    </tr>
</table>

### Secondary logo

<table class="table logo">
    <tr>
        <td>
            <img alt="" src="{{ base_url }}/static/img/pandas_secondary.svg"/>
        </td>
        <td style="background-color: #150458">
            <img alt="" src="{{ base_url }}/static/img/pandas_secondary_white.svg"/>
        </td>
    </tr>
</table>

### Logo mark

<table class="table logo">
    <tr>
        <td>
            <img alt="" src="{{ base_url }}/static/img/pandas_mark.svg"/>
        </td>
        <td style="background-color: #150458">
            <img alt="" src="{{ base_url }}/static/img/pandas_mark_white.svg"/>
        </td>
    </tr>
</table>

### Logo usage

The pandas logo is available in full color and white accent.
The full color logo should only appear against white backgrounds.
The white accent logo should go against contrasting color background.

When using the logo, please follow the next directives:

- Primary logo should never be seen under 1 inch in size for printing and 72px for web
- The secondary logo should never be seen under 0.75 inch in size for printing and 55px for web
- Leave enough margin around the logo (leave the height of the logo in the top, bottom and both sides)
- Do not distort the logo by changing its proportions
- Do not place text or other elements on top of the logo

### Colors

<table class="table">
    <tr>
        <td style="text-align: center;">
            <svg xmlns="http://www.w3.org/2000/svg" width="100" height="100">
                <circle cx="50" cy="50" r="50" fill="#150458"/>
            </svg>
            <br/>
            <b style="color: #150458;">Blue</b><br/>
            RGB: R21 G4 B88<br/>
            HEX: #150458
        </td>
        <td style="text-align: center;">
            <svg xmlns="http://www.w3.org/2000/svg" width="100" height="100">
                <circle cx="50" cy="50" r="50" fill="#ffca00"/>
            </svg>
            <br/>
            <b style="color: #150458;">Yellow</b><br/>
            RGB: R255 G202 B0<br/>
            HEX: #FFCA00
        </td>
        <td style="text-align: center;">
            <svg xmlns="http://www.w3.org/2000/svg" width="100" height="100">
                <circle cx="50" cy="50" r="50" fill="#e70488"/>
            </svg>
            <br/>
            <b style="color: #150458;">Pink</b><br/>
            RGB: R231 G4 B136<br/>
            HEX: #E70488
        </td>
    </tr>
</table>
# Sponsors

## NumFOCUS

![](https://numfocus.org/wp-content/uploads/2018/01/optNumFocus_LRG.png)

_pandas_ is a Sponsored Project of [NumFOCUS](https://numfocus.org/), a 501(c)(3) nonprofit charity in the United States.
NumFOCUS provides _pandas_ with fiscal, legal, and administrative support to help ensure the
health and sustainability of the project. Visit numfocus.org for more information.

Donations to _pandas_ are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible
to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.

## Become a sponsor

As a free and open source project, _pandas_ relies on the support of the community of users for its development.
If you work for an organization that uses and benefits from _pandas_, please consider supporting pandas. There
are different ways, such as employing people to work on pandas, funding the project, or becoming a
[NumFOCUS sponsor](https://numfocus.org/sponsors) to support the broader ecosystem. Please contact us at
[admin@numfocus.org](mailto:admin@numfocus.org) to discuss.

## Institutional partners

Institutional partners are companies and universities that support the project by employing contributors.
Current institutional partners include:

<ul>
    {% for company in sponsors.active if company.kind == "partner" %}
        <li><a href="{{ company.url }}">{{ company.name }}</a>: {{ company.description }}</li>
    {% endfor %}
</ul>

## Sponsors

Sponsors are organizations that provide funding for pandas. Current sponsors include:

<ul>
    {% for company in sponsors.active if company.kind == "regular" %}
        <li><a href="{{ company.url }}">{{ company.name }}</a>: {{ company.description }}</li>
    {% endfor %}
</ul>

## In-kind sponsors

In-kind sponsors are organizations that support pandas development with goods or services.
Current in-kind sponsors include:

<ul>
    {% for company in sponsors.inkind %}
        <li><a href="{{ company.url }}">{{ company.name }}</a>: {{ company.description }}</li>
    {% endfor %}
</ul>

## Past institutional partners

<ul>
    {% for company in sponsors.past if company.kind == "partner" %}
        <li><a href="{{ company.url }}">{{ company.name }}</a></li>
    {% endfor %}
</ul>
# Team

## Contributors

_pandas_ is made with love by more than [2,000 volunteer contributors](https://github.com/pandas-dev/pandas/graphs/contributors).

If you want to support pandas development, you can find information in the [donations page](../donate.html).

## Maintainers

<div class="card-group maintainers">
    {% for person in maintainers.people %}
        <div class="card">
            <img class="card-img-top" alt="" src="{{ person.avatar_url }}"/>
            <div class="card-body">
                <h6 class="card-title">
                    {% if person.blog %}
                        <a href="{{ person.blog }}">
                            {{ person.name or person.login }}
                        </a>
                    {% else %}
                        {{ person.name or person.login }}
                    {% endif %}
                </h6>
                <p class="card-text small"><a href="{{ person.html_url }}">{{ person.login }}</a></p>
            </div>
        </div>
    {% endfor %}
</div>

## Diversity and Inclusion

> _pandas_ expressly welcomes and encourages contributions from anyone who faces under-representation, discrimination in the technology industry
> or anyone willing to increase the diversity of our team.
> We have identified visible gaps and obstacles in sustaining diversity and inclusion in the open-source communities and we are proactive in increasing
> the diversity of our team.
> We have a [code of conduct](../community/coc.html) to ensure a friendly and welcoming environment.
> Please send an email to [pandas-code-of-conduct-committee](mailto:pandas-coc@googlegroups.com), if you think we can do a
> better job at achieving this goal.

## Governance

Wes McKinney is the Benevolent Dictator for Life (BDFL).

The project governance is available in the [project governance documents](https://github.com/pandas-dev/pandas-governance).

## Code of conduct committee

<ul>
    {% for person in maintainers.coc %}
        <li>{{ person }}</li>
    {% endfor %}
</ul>

## NumFOCUS committee

<ul>
    {% for person in maintainers.numfocus %}
        <li>{{ person }}</li>
    {% endfor %}
</ul>

## Emeritus maintainers

<ul>
    {% for person in maintainers.emeritus %}
        <li>{{ person }}</li>
    {% endfor %}
</ul>
# Roadmap

This page provides an overview of the major themes in pandas'
development. Each of these items requires a relatively large amount of
effort to implement. These may be achieved more quickly with dedicated
funding or interest from contributors.

An item being on the roadmap does not mean that it will *necessarily*
happen, even with unlimited funding. During the implementation period we
may discover issues preventing the adoption of the feature.

Additionally, an item *not* being on the roadmap does not exclude it
from inclusion in pandas. The roadmap is intended for larger,
fundamental changes to the project that are likely to take months or
years of developer time. Smaller-scoped items will continue to be
tracked on our [issue tracker](https://github.com/pandas-dev/pandas/issues).

See [Roadmap evolution](#roadmap-evolution) for proposing
changes to this document.

## Extensibility

Pandas `extending.extension-types` allow
for extending NumPy types with custom data types and array storage.
Pandas uses extension types internally, and provides an interface for
3rd-party libraries to define their own custom data types.

Many parts of pandas still unintentionally convert data to a NumPy
array. These problems are especially pronounced for nested data.

We'd like to improve the handling of extension arrays throughout the
library, making their behavior more consistent with the handling of
NumPy arrays. We'll do this by cleaning up pandas' internals and
adding new methods to the extension array interface.

## String data type

Currently, pandas stores text data in an `object` -dtype NumPy array.
The current implementation has two primary drawbacks: First, `object`
-dtype is not specific to strings: any Python object can be stored in an
`object` -dtype array, not just strings. Second: this is not efficient.
The NumPy memory model isn't especially well-suited to variable width
text data.

To solve the first issue, we propose a new extension type for string
data. This will initially be opt-in, with users explicitly requesting
`dtype="string"`. The array backing this string dtype may initially be
the current implementation: an `object` -dtype NumPy array of Python
strings.

To solve the second issue (performance), we'll explore alternative
in-memory array libraries (for example, Apache Arrow). As part of the
work, we may need to implement certain operations expected by pandas
users (for example the algorithm used in, `Series.str.upper`). That work
may be done outside of pandas.

## Apache Arrow interoperability

[Apache Arrow](https://arrow.apache.org) is a cross-language development
platform for in-memory data. The Arrow logical types are closely aligned
with typical pandas use cases.

We'd like to provide better-integrated support for Arrow memory and
data types within pandas. This will let us take advantage of its I/O
capabilities and provide for better interoperability with other
languages and libraries using Arrow.

## Block manager rewrite

We'd like to replace pandas current internal data structures (a
collection of 1 or 2-D arrays) with a simpler collection of 1-D arrays.

Pandas internal data model is quite complex. A DataFrame is made up of
one or more 2-dimensional "blocks", with one or more blocks per dtype.
This collection of 2-D arrays is managed by the BlockManager.

The primary benefit of the BlockManager is improved performance on
certain operations (construction from a 2D array, binary operations,
reductions across the columns), especially for wide DataFrames. However,
the BlockManager substantially increases the complexity and maintenance
burden of pandas.

By replacing the BlockManager we hope to achieve

-   Substantially simpler code
-   Easier extensibility with new logical types
-   Better user control over memory use and layout
-   Improved micro-performance
-   Option to provide a C / Cython API to pandas' internals

See [these design
documents](https://dev.pandas.io/pandas2/internal-architecture.html#removal-of-blockmanager-new-dataframe-internals)
for more.

## Decoupling of indexing and internals

The code for getting and setting values in pandas' data structures
needs refactoring. In particular, we must clearly separate code that
converts keys (e.g., the argument to `DataFrame.loc`) to positions from
code that uses these positions to get or set values. This is related to
the proposed BlockManager rewrite. Currently, the BlockManager sometimes
uses label-based, rather than position-based, indexing. We propose that
it should only work with positional indexing, and the translation of
keys to positions should be entirely done at a higher level.

Indexing is a complicated API with many subtleties. This refactor will
require care and attention. More details are discussed at
<https://github.com/pandas-dev/pandas/wiki/(Tentative)-rules-for-restructuring-indexing-code>

## Numba-accelerated operations

[Numba](https://numba.pydata.org) is a JIT compiler for Python code.
We'd like to provide ways for users to apply their own Numba-jitted
functions where pandas accepts user-defined functions (for example,
`Series.apply`,
`DataFrame.apply`,
`DataFrame.applymap`, and in groupby and
window contexts). This will improve the performance of
user-defined-functions in these operations by staying within compiled
code.

## Documentation improvements

We'd like to improve the content, structure, and presentation of the
pandas documentation. Some specific goals include

-   Overhaul the HTML theme with a modern, responsive design
    (`15556`)
-   Improve the "Getting Started" documentation, designing and writing
    learning paths for users different backgrounds (e.g. brand new to
    programming, familiar with other languages like R, already familiar
    with Python).
-   Improve the overall organization of the documentation and specific
    subsections of the documentation to make navigation and finding
    content easier.

## Performance monitoring

Pandas uses [airspeed velocity](https://asv.readthedocs.io/en/stable/)
to monitor for performance regressions. ASV itself is a fabulous tool,
but requires some additional work to be integrated into an open source
project's workflow.

The [asv-runner](https://github.com/asv-runner) organization, currently
made up of pandas maintainers, provides tools built on top of ASV. We
have a physical machine for running a number of project's benchmarks,
and tools managing the benchmark runs and reporting on results.

We'd like to fund improvements and maintenance of these tools to

-   Be more stable. Currently, they're maintained on the nights and
    weekends when a maintainer has free time.
-   Tune the system for benchmarks to improve stability, following
    <https://pyperf.readthedocs.io/en/latest/system.html>
-   Build a GitHub bot to request ASV runs *before* a PR is merged.
    Currently, the benchmarks are only run nightly.

## Roadmap Evolution

Pandas continues to evolve. The direction is primarily determined by
community interest. Everyone is welcome to review existing items on the
roadmap and to propose a new item.

Each item on the roadmap should be a short summary of a larger design
proposal. The proposal should include

1.  Short summary of the changes, which would be appropriate for
    inclusion in the roadmap if accepted.
2.  Motivation for the changes.
3.  An explanation of why the change is in scope for pandas.
4.  Detailed design: Preferably with example-usage (even if not
    implemented yet) and API documentation
5.  API Change: Any API changes that may result from the proposal.

That proposal may then be submitted as a GitHub issue, where the pandas
maintainers can review and comment on the design. The [pandas mailing
list](https://mail.python.org/mailman/listinfo/pandas-dev) should be
notified of the proposal.

When there's agreement that an implementation would be welcome, the
roadmap should be updated to include the summary and a link to the
discussion issue.
# Code of conduct

As contributors and maintainers of this project, and in the interest of
fostering an open and welcoming community, we pledge to respect all people who
contribute through reporting issues, posting feature requests, updating
documentation, submitting pull requests or patches, and other activities.

We are committed to making participation in this project a harassment-free
experience for everyone, regardless of level of experience, gender, gender
identity and expression, sexual orientation, disability, personal appearance,
body size, race, ethnicity, age, religion, or nationality.

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery
* Personal attacks
* Trolling or insulting/derogatory comments
* Public or private harassment
* Publishing other's private information, such as physical or electronic
  addresses, without explicit permission
* Other unethical or unprofessional conduct

Furthermore, we encourage inclusive behavior - for example,
please don’t say “hey guys!” but “hey everyone!”.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

By adopting this Code of Conduct, project maintainers commit themselves to
fairly and consistently applying these principles to every aspect of managing
this project. Project maintainers who do not follow or enforce the Code of
Conduct may be permanently removed from the project team.

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community.

A working group of community members is committed to promptly addressing any
reported issues. The working group is made up of pandas contributors and users.
Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the working group by e-mail (pandas-coc@googlegroups.com).
Messages sent to this e-mail address will not be publicly visible but only to
the working group members. The working group currently includes

<ul>
    {% for person in maintainers.coc %}
    <li>{{ person }}</li>
    {% endfor %}
</ul>

All complaints will be reviewed and investigated and will result in a response
that is deemed necessary and appropriate to the circumstances. Maintainers are
obligated to maintain confidentiality with regard to the reporter of an
incident.

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 1.3.0, available at
[https://www.contributor-covenant.org/version/1/3/0/][version],
and the [Swift Code of Conduct][swift].

[homepage]: https://www.contributor-covenant.org
[version]: https://www.contributor-covenant.org/version/1/3/0/
[swift]: https://swift.org/community/#code-of-conduct
# Ecosystem

Increasingly, packages are being built on top of pandas to address
specific needs in data preparation, analysis and visualization. This is
encouraging because it means pandas is not only helping users to handle
their data tasks but also that it provides a better starting point for
developers to build powerful and more focused data tools. The creation
of libraries that complement pandas' functionality also allows pandas
development to remain focused around its original requirements.

This is an inexhaustive list of projects that build on pandas in order
to provide tools in the PyData space. For a list of projects that depend
on pandas, see the [libraries.io usage page for
pandas](https://libraries.io/pypi/pandas/usage) or [search pypi for
pandas](https://pypi.org/search/?q=pandas).

We'd like to make it easier for users to find these projects, if you
know of other substantial projects that you feel should be on this list,
please let us know.

## Statistics and machine learning

### [Statsmodels](https://www.statsmodels.org/)

Statsmodels is the prominent Python "statistics and econometrics
library" and it has a long-standing special relationship with pandas.
Statsmodels provides powerful statistics, econometrics, analysis and
modeling functionality that is out of pandas' scope. Statsmodels
leverages pandas objects as the underlying data container for
computation.

### [sklearn-pandas](https://github.com/paulgb/sklearn-pandas)

Use pandas DataFrames in your [scikit-learn](https://scikit-learn.org/)
ML pipeline.

### [Featuretools](https://github.com/alteryx/featuretools/)

Featuretools is a Python library for automated feature engineering built
on top of pandas. It excels at transforming temporal and relational
datasets into feature matrices for machine learning using reusable
feature engineering "primitives". Users can contribute their own
primitives in Python and share them with the rest of the community.

### [Compose](https://github.com/alteryx/compose)

Compose is a machine learning tool for labeling data and prediction engineering.
It allows you to structure the labeling process by parameterizing
prediction problems and transforming time-driven relational data into
target values with cutoff times that can be used for supervised learning.

## Visualization

### [Altair](https://altair-viz.github.io/)

Altair is a declarative statistical visualization library for Python.
With Altair, you can spend more time understanding your data and its
meaning. Altair's API is simple, friendly and consistent and built on
top of the powerful Vega-Lite JSON specification. This elegant
simplicity produces beautiful and effective visualizations with a
minimal amount of code. Altair works with Pandas DataFrames.

### [Bokeh](https://bokeh.pydata.org)

Bokeh is a Python interactive visualization library for large datasets
that natively uses the latest web technologies. Its goal is to provide
elegant, concise construction of novel graphics in the style of
Protovis/D3, while delivering high-performance interactivity over large
data to thin clients.

[Pandas-Bokeh](https://github.com/PatrikHlobil/Pandas-Bokeh) provides a
high level API for Bokeh that can be loaded as a native Pandas plotting
backend via

```
pd.set_option("plotting.backend", "pandas_bokeh")
```

It is very similar to the matplotlib plotting backend, but provides
interactive web-based charts and maps.

### [seaborn](https://seaborn.pydata.org)

Seaborn is a Python visualization library based on
[matplotlib](https://matplotlib.org). It provides a high-level,
dataset-oriented interface for creating attractive statistical graphics.
The plotting functions in seaborn understand pandas objects and leverage
pandas grouping operations internally to support concise specification
of complex visualizations. Seaborn also goes beyond matplotlib and
pandas with the option to perform statistical estimation while plotting,
aggregating across observations and visualizing the fit of statistical
models to emphasize patterns in a dataset.

### [plotnine](https://github.com/has2k1/plotnine/)

Hadley Wickham's [ggplot2](https://ggplot2.tidyverse.org/) is a
foundational exploratory visualization package for the R language. Based
on ["The Grammar of
Graphics"](https://www.cs.uic.edu/~wilkinson/TheGrammarOfGraphics/GOG.html)
it provides a powerful, declarative and extremely general way to
generate bespoke plots of any kind of data.
Various implementations to other languages are available.
A good implementation for Python users is [has2k1/plotnine](https://github.com/has2k1/plotnine/).

### [IPython Vega](https://github.com/vega/ipyvega)

[IPython Vega](https://github.com/vega/ipyvega) leverages [Vega](https://github.com/vega/vega) to create plots within Jupyter Notebook.

### [Plotly](https://plot.ly/python)

[Plotly's](https://plot.ly/) [Python API](https://plot.ly/python/)
enables interactive figures and web shareability. Maps, 2D, 3D, and
live-streaming graphs are rendered with WebGL and
[D3.js](https://d3js.org/). The library supports plotting directly from
a pandas DataFrame and cloud-based collaboration. Users of [matplotlib,
ggplot for Python, and
Seaborn](https://plot.ly/python/matplotlib-to-plotly-tutorial/) can
convert figures into interactive web-based plots. Plots can be drawn in
[IPython Notebooks](https://plot.ly/ipython-notebooks/) , edited with R
or MATLAB, modified in a GUI, or embedded in apps and dashboards. Plotly
is free for unlimited sharing, and has
[cloud](https://plot.ly/product/plans/),
[offline](https://plot.ly/python/offline/), or
[on-premise](https://plot.ly/product/enterprise/) accounts for private
use.

### [QtPandas](https://github.com/draperjames/qtpandas)

Spun off from the main pandas library, the
[qtpandas](https://github.com/draperjames/qtpandas) library enables
DataFrame visualization and manipulation in PyQt4 and PySide
applications.

## IDE

### [IPython](https://ipython.org/documentation.html)

IPython is an interactive command shell and distributed computing
environment. IPython tab completion works with Pandas methods and also
attributes like DataFrame columns.

### [Jupyter Notebook / Jupyter Lab](https://jupyter.org)

Jupyter Notebook is a web application for creating Jupyter notebooks. A
Jupyter notebook is a JSON document containing an ordered list of
input/output cells which can contain code, text, mathematics, plots and
rich media. Jupyter notebooks can be converted to a number of open
standard output formats (HTML, HTML presentation slides, LaTeX, PDF,
ReStructuredText, Markdown, Python) through 'Download As' in the web
interface and `jupyter convert` in a shell.

Pandas DataFrames implement `_repr_html_`and `_repr_latex` methods which
are utilized by Jupyter Notebook for displaying (abbreviated) HTML or
LaTeX tables. LaTeX output is properly escaped. (Note: HTML tables may
or may not be compatible with non-HTML Jupyter output formats.)

See `Options and Settings <options>` and
`Available Options <options.available>`
for pandas `display.` settings.

### [quantopian/qgrid](https://github.com/quantopian/qgrid)

qgrid is "an interactive grid for sorting and filtering DataFrames in
IPython Notebook" built with SlickGrid.

### [Spyder](https://www.spyder-ide.org/)

Spyder is a cross-platform PyQt-based IDE combining the editing,
analysis, debugging and profiling functionality of a software
development tool with the data exploration, interactive execution, deep
inspection and rich visualization capabilities of a scientific
environment like MATLAB or Rstudio.

Its [Variable
Explorer](https://docs.spyder-ide.org/variableexplorer.html) allows
users to view, manipulate and edit pandas `Index`, `Series`, and
`DataFrame` objects like a "spreadsheet", including copying and
modifying values, sorting, displaying a "heatmap", converting data
types and more. Pandas objects can also be renamed, duplicated, new
columns added, copyed/pasted to/from the clipboard (as TSV), and
saved/loaded to/from a file. Spyder can also import data from a variety
of plain text and binary files or the clipboard into a new pandas
DataFrame via a sophisticated import wizard.

Most pandas classes, methods and data attributes can be autocompleted in
Spyder's [Editor](https://docs.spyder-ide.org/editor.html) and [IPython
Console](https://docs.spyder-ide.org/ipythonconsole.html), and Spyder's
[Help pane](https://docs.spyder-ide.org/help.html) can retrieve and
render Numpydoc documentation on pandas objects in rich text with Sphinx
both automatically and on-demand.

## API

### [pandas-datareader](https://github.com/pydata/pandas-datareader)

`pandas-datareader` is a remote data access library for pandas
(PyPI:`pandas-datareader`). It is based on functionality that was
located in `pandas.io.data` and `pandas.io.wb` but was split off in
v0.19. See more in the [pandas-datareader
docs](https://pandas-datareader.readthedocs.io/en/latest/):

The following data feeds are available:

- Google Finance
- Tiingo
- Morningstar
- IEX
- Robinhood
- Enigma
- Quandl
- FRED
- Fama/French
- World Bank
- OECD
- Eurostat
- TSP Fund Data
- Nasdaq Trader Symbol Definitions
- Stooq Index Data
- MOEX Data

### [quandl/Python](https://github.com/quandl/Python)

Quandl API for Python wraps the Quandl REST API to return Pandas
DataFrames with timeseries indexes.

### [pydatastream](https://github.com/vfilimonov/pydatastream)

PyDatastream is a Python interface to the [Thomson Dataworks Enterprise
(DWE/Datastream)](http://dataworks.thomson.com/Dataworks/Enterprise/1.0/)
SOAP API to return indexed Pandas DataFrames with financial data. This
package requires valid credentials for this API (non free).

### [pandaSDMX](https://pandasdmx.readthedocs.io)

pandaSDMX is a library to retrieve and acquire statistical data and
metadata disseminated in [SDMX](https://www.sdmx.org) 2.1, an
ISO-standard widely used by institutions such as statistics offices,
central banks, and international organisations. pandaSDMX can expose
datasets and related structural metadata including data flows,
code-lists, and data structure definitions as pandas Series or
MultiIndexed DataFrames.

### [fredapi](https://github.com/mortada/fredapi)

fredapi is a Python interface to the [Federal Reserve Economic Data
(FRED)](https://fred.stlouisfed.org/) provided by the Federal Reserve
Bank of St. Louis. It works with both the FRED database and ALFRED
database that contains point-in-time data (i.e. historic data
revisions). fredapi provides a wrapper in Python to the FRED HTTP API,
and also provides several convenient methods for parsing and analyzing
point-in-time data from ALFRED. fredapi makes use of pandas and returns
data in a Series or DataFrame. This module requires a FRED API key that
you can obtain for free on the FRED website.

## Domain specific

### [Geopandas](https://github.com/kjordahl/geopandas)

Geopandas extends pandas data objects to include geographic information
which support geometric operations. If your work entails maps and
geographical coordinates, and you love pandas, you should take a close
look at Geopandas.

### [xarray](https://github.com/pydata/xarray)

xarray brings the labeled data power of pandas to the physical sciences
by providing N-dimensional variants of the core pandas data structures.
It aims to provide a pandas-like and pandas-compatible toolkit for
analytics on multi-dimensional arrays, rather than the tabular data for
which pandas excels.

## Out-of-core

### [Blaze](https://blaze.pydata.org/)

Blaze provides a standard API for doing computations with various
in-memory and on-disk backends: NumPy, Pandas, SQLAlchemy, MongoDB,
PyTables, PySpark.

### [Dask](https://dask.readthedocs.io/en/latest/)

Dask is a flexible parallel computing library for analytics. Dask
provides a familiar `DataFrame` interface for out-of-core, parallel and
distributed computing.

### [Dask-ML](https://dask-ml.readthedocs.io/en/latest/)

Dask-ML enables parallel and distributed machine learning using Dask
alongside existing machine learning libraries like Scikit-Learn,
XGBoost, and TensorFlow.

### [Koalas](https://koalas.readthedocs.io/en/latest/)

Koalas provides a familiar pandas DataFrame interface on top of Apache
Spark. It enables users to leverage multi-cores on one machine or a
cluster of machines to speed up or scale their DataFrame code.

### [Odo](http://odo.pydata.org)

Odo provides a uniform API for moving data between different formats. It
uses pandas own `read_csv` for CSV IO and leverages many existing
packages such as PyTables, h5py, and pymongo to move data between non
pandas formats. Its graph based approach is also extensible by end users
for custom formats that may be too specific for the core of odo.

### [Ray](https://ray.readthedocs.io/en/latest/pandas_on_ray.html)

Pandas on Ray is an early stage DataFrame library that wraps Pandas and
transparently distributes the data and computation. The user does not
need to know how many cores their system has, nor do they need to
specify how to distribute the data. In fact, users can continue using
their previous Pandas notebooks while experiencing a considerable
speedup from Pandas on Ray, even on a single machine. Only a
modification of the import statement is needed, as we demonstrate below.
Once you've changed your import statement, you're ready to use Pandas on
Ray just like you would Pandas.

```
# import pandas as pd
import ray.dataframe as pd
```

### [Vaex](https://docs.vaex.io/)

Increasingly, packages are being built on top of pandas to address
specific needs in data preparation, analysis and visualization. Vaex is
a python library for Out-of-Core DataFrames (similar to Pandas), to
visualize and explore big tabular datasets. It can calculate statistics
such as mean, sum, count, standard deviation etc, on an N-dimensional
grid up to a billion (10^9^) objects/rows per second. Visualization is
done using histograms, density plots and 3d volume rendering, allowing
interactive exploration of big data. Vaex uses memory mapping, zero
memory copy policy and lazy computations for best performance (no memory
wasted).

- ``vaex.from_pandas``
- ``vaex.to_pandas_df``

## Data cleaning and validation

### [pyjanitor](https://github.com/ericmjl/pyjanitor/)

Pyjanitor provides a clean API for cleaning data, using method chaining.

### [Engarde](https://engarde.readthedocs.io/en/latest/)

Engarde is a lightweight library used to explicitly state your
assumptions about your datasets and check that they're *actually* true.

## Extension data types

Pandas provides an interface for defining
`extension types <extending.extension-types>` to extend NumPy's type system. The following libraries
implement that interface to provide types not found in NumPy or pandas,
which work well with pandas' data containers.

### [cyberpandas](https://cyberpandas.readthedocs.io/en/latest)

Cyberpandas provides an extension type for storing arrays of IP
Addresses. These arrays can be stored inside pandas' Series and
DataFrame.

### [Pandas-Genomics](https://pandas-genomics.readthedocs.io/en/latest/)

Pandas-Genomics provides an extension type and extension array for working
 with genomics data.  It also includes `genomics` accessors for many useful properties
 and methods related to QC and analysis of genomics data.

### [Pint-Pandas](https://github.com/hgrecco/pint-pandas)

Pint-Pandas provides an extension type for storing numeric arrays with units.
These arrays can be stored inside pandas' Series and DataFrame. Operations
between Series and DataFrame columns which use pint's extension array are then
units aware.

## Accessors

A directory of projects providing
`extension accessors <extending.register-accessors>`. This is for users to discover new accessors and for library
authors to coordinate on the namespace.

  | Library                                                              | Accessor   |  Classes              |
  | ---------------------------------------------------------------------|------------|-----------------------|
  | [cyberpandas](https://cyberpandas.readthedocs.io/en/latest)          | `ip`       | `Series`              |
  | [pdvega](https://altair-viz.github.io/pdvega/)                       | `vgplot`   | `Series`, `DataFrame` |
  | [pandas-genomics](https://pandas-genomics.readthedocs.io/en/latest/) | `genomics` | `Series`, `DataFrame` |
  | [pandas_path](https://github.com/drivendataorg/pandas-path/)         | `path`     | `Index`, `Series`     |
  | [pint-pandas](https://github.com/hgrecco/pint-pandas)                | `pint`     | `Series`, `DataFrame` |
  | [composeml](https://github.com/alteryx/compose)                      | `slice`    | `DataFrame`           |
  | [woodwork](https://github.com/alteryx/woodwork)                      | `slice`    | `Series`, `DataFrame` |
## Development tools

### [pandas-stubs](https://github.com/VirtusLab/pandas-stubs)

While pandas repository is partially typed, the package itself doesn't expose this information for external use.
Install pandas-stubs to enable basic type coverage of pandas API.

Learn more by reading through these issues [14468](https://github.com/pandas-dev/pandas/issues/14468),
[26766](https://github.com/pandas-dev/pandas/issues/26766), [28142](https://github.com/pandas-dev/pandas/issues/28142).

See installation and usage instructions on the [github page](https://github.com/VirtusLab/pandas-stubs).
Title: pandas extension arrays
Date: 2019-01-04

# pandas extension arrays

Extensibility was a major theme in pandas development over the last couple of
releases. This post introduces the pandas extension array interface: the
motivation behind it and how it might affect you as a pandas user. Finally, we
look at how extension arrays may shape the future of pandas.

Extension Arrays are just one of the changes in pandas 0.24.0. See the
[whatsnew][whatsnew] for a full changelog.

## The Motivation

Pandas is built on top of NumPy. You could roughly define a Series as a wrapper
around a NumPy array, and a DataFrame as a collection of Series with a shared
index. That's not entirely correct for several reasons, but I want to focus on
the "wrapper around a NumPy array" part. It'd be more correct to say "wrapper
around an array-like object".

Pandas mostly uses NumPy's builtin data representation; we've restricted it in
places and extended it in others. For example, pandas' early users cared greatly
about timezone-aware datetimes, which NumPy doesn't support. So pandas
internally defined a `DatetimeTZ` dtype (which mimics a NumPy dtype), and
allowed you to use that dtype in `Index`, `Series`, and as a column in a
`DataFrame`. That dtype carried around the tzinfo, but wasn't itself a valid
NumPy dtype.

As another example, consider `Categorical`. This actually composes *two* arrays:
one for the `categories` and one for the `codes`. But it can be stored in a
`DataFrame` like any other column.

Each of these extension types pandas added is useful on its own, but carries a
high maintenance cost. Large sections of the codebase need to be aware of how to
handle a NumPy array or one of these other kinds of special arrays. This made
adding new extension types to pandas very difficult.

Anaconda, Inc. had a client who regularly dealt with datasets with IP addresses.
They wondered if it made sense to add an [IPArray][IPArray] to pandas. In the
end, we didn't think it passed the cost-benefit test for inclusion in pandas
*itself*, but we were interested in defining an interface for third-party
extensions to pandas. Any object implementing this interface would be allowed in
pandas. I was able to write [cyberpandas][cyberpandas] outside of pandas, but it
feels like using any other dtype built into pandas.

## The Current State

As of pandas 0.24.0, all of pandas' internal extension arrays (Categorical,
Datetime with Timezone, Period, Interval, and Sparse) are now built on top of
the ExtensionArray interface. Users shouldn't notice many changes. The main
thing you'll notice is that things are cast to `object` dtype in fewer places,
meaning your code will run faster and your types will be more stable. This
includes storing `Period` and `Interval` data in `Series` (which were previously
cast to object dtype).

Additionally, we'll be able to add *new* extension arrays with relative ease.
For example, 0.24.0 (optionally) solved one of pandas longest-standing pain
points: missing values casting integer-dtype values to float.


```python
>>> int_ser = pd.Series([1, 2], index=[0, 2])
>>> int_ser
0    1
2    2
dtype: int64

>>> int_ser.reindex([0, 1, 2])
0    1.0
1    NaN
2    2.0
dtype: float64
```

With the new [IntegerArray][IntegerArray] and nullable integer dtypes, we can
natively represent integer data with missing values.

```python
>>> int_ser = pd.Series([1, 2], index=[0, 2], dtype=pd.Int64Dtype())
>>> int_ser
0    1
2    2
dtype: Int64

>>> int_ser.reindex([0, 1, 2])
0      1
1    NaN
2      2
dtype: Int64
```

One thing it does slightly change how you should access the raw (unlabeled)
arrays stored inside a Series or Index, which is occasionally useful. Perhaps
the method you're calling only works with NumPy arrays, or perhaps you want to
disable automatic alignment.

In the past, you'd hear things like "Use `.values` to extract the NumPy array
from a Series or DataFrame." If it were a good resource, they'd tell you that's
not *entirely* true, since there are some exceptions. I'd like to delve into
those exceptions.

The fundamental problem with `.values` is that it serves two purposes:

1. Extracting the array backing a Series, Index, or DataFrame
2. Converting the Series, Index, or DataFrame to a NumPy array

As we saw above, the "array" backing a Series or Index might not be a NumPy
array, it may instead be an extension array (from pandas or a third-party
library). For example, consider `Categorical`,

```python
>>> cat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])
>>> ser = pd.Series(cat)
>>> ser
0    a
1    b
2    a
dtype: category
Categories (3, object): ['a', 'b', 'c']

>>> ser.values
[a, b, a]
Categories (3, object): ['a', 'b', 'c']
```

In this case `.values` is a Categorical, not a NumPy array. For period-dtype
data, `.values` returns a NumPy array of `Period` objects, which is expensive to
create. For timezone-aware data, `.values` converts to UTC and *drops* the
timezone info. These kind of surprises (different types, or expensive or lossy
conversions) stem from trying to shoehorn these extension arrays into a NumPy
array. But the entire point of an extension array is for representing data NumPy
*can't* natively represent.

To solve the `.values` problem, we've split its roles into two dedicated methods:

1. Use `.array` to get a zero-copy reference to the underlying data
2. Use `.to_numpy()` to get a (potentially expensive, lossy) NumPy array of the
   data.

So with our Categorical example,

```python
>>> ser.array
[a, b, a]
Categories (3, object): ['a', 'b', 'c']

>>> ser.to_numpy()
array(['a', 'b', 'a'], dtype=object)
```

To summarize:

- `.array` will *always* be a an ExtensionArray, and is always a zero-copy
   reference back to the data.
- `.to_numpy()` is *always* a NumPy array, so you can reliably call
   ndarray-specific methods on it.

You shouldn't ever need `.values` anymore.

## Possible Future Paths

Extension Arrays open up quite a few exciting opportunities. Currently, pandas
represents string data using Python objects in a NumPy array, which is slow.
Libraries like [Apache Arrow][arrow] provide native support for variable-length
strings, and the [Fletcher][fletcher] library provides pandas extension arrays
for Arrow arrays. It will allow [GeoPandas][geopandas] to store geometry data
more efficiently. Pandas (or third-party libraries) will be able to support
nested data, data with units, geo data, GPU arrays. Keep an eye on the
[pandas ecosystem][eco] page, which will keep track of third-party extension
arrays. It's an exciting time for pandas development.

## Other Thoughts

I'd like to emphasize that this is an *interface*, and not a concrete array
implementation. We are *not* reimplementing NumPy here in pandas. Rather, this
is a way to take any array-like data structure (one or more NumPy arrays, an
Apache Arrow array, a CuPy array) and place it inside a DataFrame. I think
getting pandas out of the array business, and instead thinking about
higher-level tabular data things, is a healthy development for the project.

This works perfectly with NumPy's [`__array_ufunc__`][ufunc] protocol and
[NEP-18][nep18]. You'll be able to use the familiar NumPy API on objects that
aren't backed by NumPy memory.

## Upgrade

These new goodies are all available in the recently released pandas 0.24.

conda:

    conda install -c conda-forge pandas

pip:

    pip install --upgrade pandas

As always, we're happy to hear feedback on the [mailing list][ml],
[@pandas-dev][twitter], or [issue tracker][tracker].

Thanks to the many contributors, maintainers, and [institutional
partners][partners] involved in the pandas community.


[IPArray]: https://github.com/pandas-dev/pandas/issues/18767
[cyberpandas]: https://cyberpandas.readthedocs.io
[IntegerArray]: http://pandas.pydata.org/pandas-docs/version/0.24/reference/api/pandas.arrays.IntegerArray.html
[fletcher]: https://github.com/xhochy/fletcher
[arrow]: https://arrow.apache.org
[ufunc]: https://numpy.org/neps/nep-0013-ufunc-overrides.html
[nep18]: https://www.numpy.org/neps/nep-0018-array-function-protocol.html
[ml]: https://mail.python.org/mailman/listinfo/pandas-dev
[twitter]: https://twitter.com/pandas_dev
[tracker]: https://github.com/pandas-dev/pandas/issues
[partners]: https://github.com/pandas-dev/pandas-governance/blob/master/people.md
[eco]: http://pandas.pydata.org/pandas-docs/stable/ecosystem.html#extension-data-types
[whatsnew]: http://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html
[geopandas]: https://github.com/geopandas/geopandas
Title: 2019 pandas user survey
Date: 2019-08-22

<style type="text/css">
table td {
    background: none;
}

table tr.even td {
    background: none;
}

table {
	text-shadow: none;
}

</style>

# 2019 pandas user survey

Pandas recently conducted a user survey to help guide future development.
Thanks to everyone who participated! This post presents the high-level results.

This analysis and the raw data can be found [on GitHub](https://github.com/pandas-dev/pandas-user-surveys) and run on Binder

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pandas-dev/pandas-user-surveys/master?filepath=2019.ipynb)


We had about 1250 repsonses over the 15 days we ran the survey in the summer of 2019.

## About the Respondents

There was a fair amount of representation across pandas experience and frequeny of use, though the majority of respondents are on the more experienced side.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_4_0.png)




![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_5_0.png)


We included a few questions that were also asked in the [Python Developers Survey](https://www.jetbrains.com/research/python-developers-survey-2018/) so we could compare Pandas' population to Python's.

90% of our respondents use Python as a primary language (compared with 84% from the PSF survey).





    Yes    90.67%
    No      9.33%
    Name: Is Python your main language?, dtype: object



Windows users are well represented (see [Steve Dower's talk](https://www.youtube.com/watch?v=uoI57uMdDD4) on this topic).





    Linux      61.57%
    Windows    60.21%
    MacOS      42.75%
    Name: What Operating Systems do you use?, dtype: object



For environment isolation, [conda](https://conda.io/en/latest/) was the most popular.




![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_13_0.png)


Most repondents are Python 3 only.





    3        92.39%
    2 & 3     6.80%
    2         0.81%
    Name: Python 2 or 3?, dtype: object



## Pandas APIs

It can be hard for open source projects to know what features are actually used. We asked a few questions to get an idea.

CSV and Excel are (for better or worse) the most popular formats.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_18_0.png)


In preperation for a possible refactor of pandas internals, we wanted to get a sense for
how common wide (100s of columns or more) DataFrames are.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_20_0.png)


Pandas is slowly growing new exentension types. Categoricals are the most popular,
and the nullable integer type is already almost as popular as datetime with timezone.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_22_0.png)


More and better examples seem to be a high-priority development item.
Pandas recently received a NumFOCUS grant to improve our documentation,
which we're using to write tutorial-style documentation, which should help
meet this need.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_24_0.png)


We also asked about specific, commonly-requested features.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_26_0.png)


Of these, the clear standout is "scaling" to large datasets. A couple observations:

1. Perhaps pandas' documentation should do a better job of promoting libraries that provide scalable dataframes (like [Dask](https://dask.org), [vaex](https://dask.org), and [modin](https://modin.readthedocs.io/en/latest/))
2. Memory efficiency (perhaps from a native string data type, fewer internal copies, etc.) is a valuable goal.

After that, the next-most critical improvement is integer missing values. Those were actually added in [Pandas 0.24](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.24.0.html#optional-integer-na-support), but they're not the default, and there's still some incompatibilites with the rest of pandas API.

Pandas is a less conservative library than, say, NumPy. We're approaching 1.0, but on the way we've made many deprecations and some outright API breaking changes. Fortunately, most people are OK with the tradeoff.





    Yes    94.89%
    No      5.11%
    Name: Is Pandas stable enough for you?, dtype: object



There's a perception (which is shared by many of the pandas maintainers) that the pandas API is too large. To measure that, we asked whether users thought that pandas' API was too large, too small, or just right.



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_31_0.png)


Finally, we asked for an overall satisfaction with the library, from 1 (not very unsatisfied) to 5 (very satisfied).



![png]({{ base_url }}/static/img/blog/2019-user-survey/2019_33_0.png)


Most people are very satisfied. The average response is 4.39. I look forward to tracking this number over time.

If you're analyzing the raw data, be sure to share the results with us [@pandas_dev](https://twitter.com/pandas_dev).
Title: pandas 1.0
Date: 2020-01-29

# pandas 1.0

Today pandas celebrates its 1.0.0 release. In many ways this is just a normal release with a host of new features, performance improvements, and bug fixes, which are documented in our [release notes](https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html). But it’s also something a bit more — a milestone for the project beyond just the commits. We wanted to take some time to reflect on where we've been and where we're going.

## Reflections

The world of scientific Python has changed a lot since pandas was started.  In 2011, [the ecosystem was fragmented](https://wesmckinney.com/blog/a-roadmap-for-rich-scientific-data-structures-in-python/): a standard *rich* data structure for statistics and data science had yet to emerge. This echos a similar story for NumPy, which consolidated array efforts that were [previously fragmented](https://numpy.org/old_array_packages.html).

Over the subsequent years, pandas emerged as a *de facto* standard. It’s used by data scientists and analysts and as a data structure for other libraries to build on top of. StackOverflow [cited pandas](https://stackoverflow.blog/2017/09/14/python-growing-quickly/) as one of the reasons for Python being the fastest growing major programming language.

![Growth of pandas](https://149351115.v2.pressablecdn.com/wp-content/uploads/2017/09/related_tags_over_time-1-1000x1000.png)

Today, the ecosystem is in another phase of exploration.
Several new DataFrame implementations are cropping up to fill needs not met by pandas.
We're [working with those projects](https://datapythonista.me/blog/dataframe-summit-at-euroscipy.html) to establish shared standards and semantics for rich data structures.

## Community and Project Health

This release cycle is the first to involve any kind of grant funding for pandas. [Pandas received funding](https://chanzuckerberg.com/eoss/proposals/) as part of the CZI’s [*Essential Open Source Software for Science*](https://medium.com/@cziscience/the-invisible-foundations-of-biomedicine-4ab7f8d4f5dd) [program](https://medium.com/@cziscience/the-invisible-foundations-of-biomedicine-4ab7f8d4f5dd). The pandas project relies overwhelmingly on volunteer contributors. These volunteer contributions are shepherded and augmented by some maintainers who are given time from their employers — our [institutional partners](https://github.com/pandas-dev/pandas-governance/blob/master/people.md#institutional-partners). The largest work item in our grant award was library maintenance, which specifically includes working with community members to address our large backlog of open issues and pull requests.

While a “1.0.0” version might seem arbitrary or anti-climactic (given that pandas as a codebase is nearly 12 years old), we see it as a symbolic milestone celebrating the growth of our core developer team and depth of our contributor base.  Few open source projects are ever truly “done” and pandas is no different. We recognize the essential role that pandas now occupies, and we intend to continue to evolve the project and adapt to the needs of the world’s data wranglers.

## Going Forward

Our [roadmap](https://pandas.pydata.org/pandas-docs/version/1.0.0/development/roadmap.html) contains an up-to-date listing of where we see the project heading over the next few years.
Needless to say, there's still plenty to do.

Check out the [release notes](https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html) and visit the [installation page](https://pandas.pydata.org/pandas-docs/version/1.0.0/getting_started/install.html) for instructions on updating to pandas 1.0.
To report a security vulnerability to pandas, please go to https://tidelift.com/security and see the instructions there.
# Contributor Code of Conduct

As contributors and maintainers of this project, and in the interest of
fostering an open and welcoming community, we pledge to respect all people who
contribute through reporting issues, posting feature requests, updating
documentation, submitting pull requests or patches, and other activities.

We are committed to making participation in this project a harassment-free
experience for everyone, regardless of level of experience, gender, gender
identity and expression, sexual orientation, disability, personal appearance,
body size, race, ethnicity, age, religion, or nationality.

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery
* Personal attacks
* Trolling or insulting/derogatory comments
* Public or private harassment
* Publishing other's private information, such as physical or electronic
  addresses, without explicit permission
* Other unethical or unprofessional conduct

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

By adopting this Code of Conduct, project maintainers commit themselves to
fairly and consistently applying these principles to every aspect of managing
this project. Project maintainers who do not follow or enforce the Code of
Conduct may be permanently removed from the project team.

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community.

A working group of community members is committed to promptly addressing any
reported issues. The working group is made up of pandas contributors and users.
Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the working group by e-mail (pandas-coc@googlegroups.com).
Messages sent to this e-mail address will not be publicly visible but only to
the working group members. The working group currently includes

- Safia Abdalla
- Tom Augspurger
- Joris Van den Bossche
- Camille Scott
- Nathaniel Smith

All complaints will be reviewed and investigated and will result in a response
that is deemed necessary and appropriate to the circumstances. Maintainers are
obligated to maintain confidentiality with regard to the reporter of an
incident.

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 1.3.0, available at
[https://www.contributor-covenant.org/version/1/3/0/][version],
and the [Swift Code of Conduct][swift].

[homepage]: https://www.contributor-covenant.org
[version]: https://www.contributor-covenant.org/version/1/3/0/
[swift]: https://swift.org/community/#code-of-conduct
# Contributing to pandas

A detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.
- [ ] closes #xxxx (Replace xxxx with the Github issue number)
- [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature
- [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).
- [ ] Added an entry in the latest `doc/source/whatsnew/vX.X.X.rst` file if fixing a bug or adding a new feature.
---

name: Feature Request
about: Suggest an idea for pandas
title: "ENH:"
labels: "Enhancement, Needs Triage"

---

#### Is your feature request related to a problem?

[this should provide a description of what the problem is, e.g. "I wish I could use pandas to do [...]"]

#### Describe the solution you'd like

[this should provide a description of the feature request, e.g. "`DataFrame.foo` should get a new parameter `bar` that [...]", try to write a docstring for the desired feature]

#### API breaking implications

[this should provide a description of how this feature will affect the API]

#### Describe alternatives you've considered

[this should provide a description of any alternative solutions or features you've considered]

#### Additional context

[add any other context, code examples, or references to existing implementations about the feature request here]

```python
# Your code here, if applicable

```
{{ fullname }}
{{ underline }}

.. currentmodule:: {{ module.split('.')[0] }}

.. autoaccessorcallable:: {{ (module.split('.')[1:] + [objname]) | join('.') }}.__call__
{{ fullname }}
{{ underline }}

.. currentmodule:: {{ module.split('.')[0] }}

.. autoaccessormethod:: {{ (module.split('.')[1:] + [objname]) | join('.') }}
{{ fullname }}
{{ underline }}

.. currentmodule:: {{ module.split('.')[0] }}

.. autoaccessorattribute:: {{ (module.split('.')[1:] + [objname]) | join('.') }}
{{ fullname }}
{{ underline }}

.. currentmodule:: {{ module }}

.. autoclass:: {{ objname }}
{% extends "!autosummary/class.rst" %}

{% block methods %}
{% if methods %}

..
   HACK -- the point here is that we don't want this to appear in the output, but the autosummary should still generate the pages.
   .. autosummary::
      :toctree:
      {% for item in all_methods %}
      {%- if not item.startswith('_') or item in ['__call__'] %}
      {{ name }}.{{ item }}
      {%- endif -%}
      {%- endfor %}

{% endif %}
{% endblock %}

{% block attributes %}
{% if attributes %}

..
   HACK -- the point here is that we don't want this to appear in the output, but the autosummary should still generate the pages.
   .. autosummary::
      :toctree:
      {% for item in all_attributes %}
      {%- if not item.startswith('_') %}
      {{ name }}.{{ item }}
      {%- endif -%}
      {%- endfor %}

{% endif %}
{% endblock %}
{{ fullname }}
{{ underline }}

.. currentmodule:: {{ module.split('.')[0] }}

.. autoaccessor:: {{ (module.split('.')[1:] + [objname]) | join('.') }}
:orphan:

.. _ecosystem:

{{ header }}

****************
pandas ecosystem
****************

Increasingly, packages are being built on top of pandas to address specific needs
in data preparation, analysis and visualization.
This is encouraging because it means pandas is not only helping users to handle
their data tasks but also that it provides a better starting point for developers to
build powerful and more focused data tools.
The creation of libraries that complement pandas' functionality also allows pandas
development to remain focused around it's original requirements.

This is an inexhaustive list of projects that build on pandas in order to provide
tools in the PyData space. For a list of projects that depend on pandas,
see the
`Github network dependents for pandas <https://github.com/pandas-dev/pandas/network/dependents>`_
or `search pypi for pandas <https://pypi.org/search/?q=pandas>`_.

We'd like to make it easier for users to find these projects, if you know of other
substantial projects that you feel should be on this list, please let us know.

.. _ecosystem.data_cleaning_and_validation:

Data cleaning and validation
----------------------------

`Pyjanitor <https://github.com/pyjanitor-devs/pyjanitor>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pyjanitor provides a clean API for cleaning data, using method chaining.

`Pandera <https://pandera.readthedocs.io/en/stable/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pandera provides a flexible and expressive API for performing data validation on dataframes
to make data processing pipelines more readable and robust.
Dataframes contain information that pandera explicitly validates at runtime. This is useful in
production-critical data pipelines or reproducible research settings.

`pandas-path <https://github.com/drivendataorg/pandas-path/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since Python 3.4, `pathlib <https://docs.python.org/3/library/pathlib.html>`_ has been
included in the Python standard library. Path objects provide a simple
and delightful way to interact with the file system. The pandas-path package enables the
Path API for pandas through a custom accessor ``.path``. Getting just the filenames from
a series of full file paths is as simple as ``my_files.path.name``. Other convenient operations like
joining paths, replacing file extensions, and checking if files exist are also available.

.. _ecosystem.stats:

Statistics and machine learning
-------------------------------

`pandas-tfrecords <https://pypi.org/project/pandas-tfrecords/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Easy saving pandas dataframe to tensorflow tfrecords format and reading tfrecords to pandas.

`Statsmodels <https://www.statsmodels.org/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Statsmodels is the prominent Python "statistics and econometrics library" and it has
a long-standing special relationship with pandas. Statsmodels provides powerful statistics,
econometrics, analysis and modeling functionality that is out of pandas' scope.
Statsmodels leverages pandas objects as the underlying data container for computation.

`sklearn-pandas <https://github.com/scikit-learn-contrib/sklearn-pandas>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use pandas DataFrames in your `scikit-learn <https://scikit-learn.org/>`__
ML pipeline.

`Featuretools <https://github.com/alteryx/featuretools/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Featuretools is a Python library for automated feature engineering built on top of pandas. It excels at transforming temporal and relational datasets into feature matrices for machine learning using reusable feature engineering "primitives". Users can contribute their own primitives in Python and share them with the rest of the community.

`Compose <https://github.com/alteryx/compose>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Compose is a machine learning tool for labeling data and prediction engineering. It allows you to structure the labeling process by parameterizing prediction problems and transforming time-driven relational data into target values with cutoff times that can be used for supervised learning.

`STUMPY <https://github.com/TDAmeritrade/stumpy>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

STUMPY is a powerful and scalable Python library for modern time series analysis.
At its core, STUMPY efficiently computes something called a
`matrix profile <https://stumpy.readthedocs.io/en/latest/Tutorial_The_Matrix_Profile.html>`__,
which can be used for a wide variety of time series data mining tasks.

.. _ecosystem.visualization:

Visualization
-------------

`Pandas has its own Styler class for table visualization <user_guide/style.ipynb>`_, and while
:ref:`pandas also has built-in support for data visualization through charts with matplotlib <visualization>`,
there are a number of other pandas-compatible libraries.

`Altair <https://altair-viz.github.io/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Altair is a declarative statistical visualization library for Python.
With Altair, you can spend more time understanding your data and its
meaning. Altair's API is simple, friendly and consistent and built on
top of the powerful Vega-Lite JSON specification. This elegant
simplicity produces beautiful and effective visualizations with a
minimal amount of code. Altair works with pandas DataFrames.


`Bokeh <https://docs.bokeh.org/en/latest/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Bokeh is a Python interactive visualization library for large datasets that natively uses
the latest web technologies. Its goal is to provide elegant, concise construction of novel
graphics in the style of Protovis/D3, while delivering high-performance interactivity over
large data to thin clients.

`Pandas-Bokeh <https://github.com/PatrikHlobil/Pandas-Bokeh>`__ provides a high level API
for Bokeh that can be loaded as a native pandas plotting backend via

.. code:: python

    pd.set_option("plotting.backend", "pandas_bokeh")

It is very similar to the matplotlib plotting backend, but provides interactive
web-based charts and maps.


`Seaborn <https://seaborn.pydata.org>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Seaborn is a Python visualization library based on
`matplotlib <https://matplotlib.org>`__. It provides a high-level, dataset-oriented
interface for creating attractive statistical graphics. The plotting functions
in seaborn understand pandas objects and leverage pandas grouping operations
internally to support concise specification of complex visualizations. Seaborn
also goes beyond matplotlib and pandas with the option to perform statistical
estimation while plotting, aggregating across observations and visualizing the
fit of statistical models to emphasize patterns in a dataset.

`plotnine <https://github.com/has2k1/plotnine/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Hadley Wickham's `ggplot2 <https://ggplot2.tidyverse.org/>`__ is a foundational exploratory visualization package for the R language.
Based on `"The Grammar of Graphics" <https://www.cs.uic.edu/~wilkinson/TheGrammarOfGraphics/GOG.html>`__ it
provides a powerful, declarative and extremely general way to generate bespoke plots of any kind of data.
Various implementations to other languages are available.
A good implementation for Python users is `has2k1/plotnine <https://github.com/has2k1/plotnine/>`__.

`IPython vega <https://github.com/vega/ipyvega>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`IPython Vega <https://github.com/vega/ipyvega>`__ leverages `Vega
<https://github.com/vega/vega>`__ to create plots within Jupyter Notebook.

`Plotly <https://poltly.com/python>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`Plotly’s <https://poltly.com/>`__ `Python API <https://poltly.com/python/>`__ enables interactive figures and web shareability. Maps, 2D, 3D, and live-streaming graphs are rendered with WebGL and `D3.js <https://d3js.org/>`__. The library supports plotting directly from a pandas DataFrame and cloud-based collaboration. Users of `matplotlib, ggplot for Python, and Seaborn <https://poltly.com/python/matplotlib-to-plotly-tutorial/>`__ can convert figures into interactive web-based plots. Plots can be drawn in `IPython Notebooks <https://plotly.com/ipython-notebooks/>`__ , edited with R or MATLAB, modified in a GUI, or embedded in apps and dashboards. Plotly is free for unlimited sharing, and has `offline <https://poltly.com/python/offline/>`__, or `on-premise <https://poltly.com/product/enterprise/>`__ accounts for private use.

`Lux <https://github.com/lux-org/lux>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`Lux <https://github.com/lux-org/lux>`__ is a Python library that facilitates fast and easy experimentation with data by automating the visual data exploration process. To use Lux, simply add an extra import alongside pandas:

.. code:: python

    import lux
    import pandas as pd

    df = pd.read_csv("data.csv")
    df  # discover interesting insights!

By printing out a dataframe, Lux automatically `recommends a set of visualizations <https://raw.githubusercontent.com/lux-org/lux-resources/master/readme_img/demohighlight.gif>`__ that highlights interesting trends and patterns in the dataframe. Users can leverage any existing pandas commands without modifying their code, while being able to visualize their pandas data structures (e.g., DataFrame, Series, Index) at the same time. Lux also offers a `powerful, intuitive language <https://lux-api.readthedocs.io/en/latest/source/guide/vis.html>`__ that allow users to create  `Altair <https://altair-viz.github.io/>`__, `matplotlib <https://matplotlib.org>`__, or `Vega-Lite <https://vega.github.io/vega-lite/>`__ visualizations without having to think at the level of code.

`Qtpandas <https://github.com/draperjames/qtpandas>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Spun off from the main pandas library, the `qtpandas <https://github.com/draperjames/qtpandas>`__
library enables DataFrame visualization and manipulation in PyQt4 and PySide applications.

`D-Tale <https://github.com/man-group/dtale>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

D-Tale is a lightweight web client for visualizing pandas data structures. It
provides a rich spreadsheet-style grid which acts as a wrapper for a lot of
pandas functionality (query, sort, describe, corr...) so users can quickly
manipulate their data. There is also an interactive chart-builder using Plotly
Dash allowing users to build nice portable visualizations. D-Tale can be
invoked with the following command

.. code:: python

    import dtale

    dtale.show(df)

D-Tale integrates seamlessly with Jupyter notebooks, Python terminals, Kaggle
& Google Colab. Here are some demos of the `grid <http://alphatechadmin.pythonanywhere.com/dtale/main/1>`__.

`hvplot <https://hvplot.holoviz.org/index.html>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

hvPlot is a high-level plotting API for the PyData ecosystem built on `HoloViews <https://holoviews.org/>`__.
It can be loaded as a native pandas plotting backend via

.. code:: python

    pd.set_option("plotting.backend", "hvplot")

.. _ecosystem.ide:

IDE
---

`IPython <https://ipython.org/documentation.html>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

IPython is an interactive command shell and distributed computing
environment. IPython tab completion works with pandas methods and also
attributes like DataFrame columns.

`Jupyter Notebook / Jupyter Lab <https://jupyter.org>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jupyter Notebook is a web application for creating Jupyter notebooks.
A Jupyter notebook is a JSON document containing an ordered list
of input/output cells which can contain code, text, mathematics, plots
and rich media.
Jupyter notebooks can be converted to a number of open standard output formats
(HTML, HTML presentation slides, LaTeX, PDF, ReStructuredText, Markdown,
Python) through 'Download As' in the web interface and ``jupyter convert``
in a shell.

pandas DataFrames implement ``_repr_html_`` and ``_repr_latex`` methods
which are utilized by Jupyter Notebook for displaying
(abbreviated) HTML or LaTeX tables. LaTeX output is properly escaped.
(Note: HTML tables may or may not be
compatible with non-HTML Jupyter output formats.)

See :ref:`Options and Settings <options>` and
:ref:`Available Options <options.available>`
for pandas ``display.`` settings.

`Quantopian/qgrid <https://github.com/quantopian/qgrid>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

qgrid is "an interactive grid for sorting and filtering
DataFrames in IPython Notebook" built with SlickGrid.

`Spyder <https://www.spyder-ide.org/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Spyder is a cross-platform PyQt-based IDE combining the editing, analysis,
debugging and profiling functionality of a software development tool with the
data exploration, interactive execution, deep inspection and rich visualization
capabilities of a scientific environment like MATLAB or Rstudio.

Its `Variable Explorer <https://docs.spyder-ide.org/current/panes/variableexplorer.html>`__
allows users to view, manipulate and edit pandas ``Index``, ``Series``,
and ``DataFrame`` objects like a "spreadsheet", including copying and modifying
values, sorting, displaying a "heatmap", converting data types and more.
pandas objects can also be renamed, duplicated, new columns added,
copied/pasted to/from the clipboard (as TSV), and saved/loaded to/from a file.
Spyder can also import data from a variety of plain text and binary files
or the clipboard into a new pandas DataFrame via a sophisticated import wizard.

Most pandas classes, methods and data attributes can be autocompleted in
Spyder's `Editor <https://docs.spyder-ide.org/current/panes/editor.html>`__ and
`IPython Console <https://docs.spyder-ide.org/current/panes/ipythonconsole.html>`__,
and Spyder's `Help pane <https://docs.spyder-ide.org/current/panes/help.html>`__ can retrieve
and render Numpydoc documentation on pandas objects in rich text with Sphinx
both automatically and on-demand.


.. _ecosystem.api:

API
---

`pandas-datareader <https://github.com/pydata/pandas-datareader>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
``pandas-datareader`` is a remote data access library for pandas (PyPI:``pandas-datareader``).
It is based on functionality that was located in ``pandas.io.data`` and ``pandas.io.wb`` but was
split off in v0.19.
See more in the  `pandas-datareader docs <https://pandas-datareader.readthedocs.io/en/latest/>`_:

The following data feeds are available:

 * Google Finance
 * Tiingo
 * Morningstar
 * IEX
 * Robinhood
 * Enigma
 * Quandl
 * FRED
 * Fama/French
 * World Bank
 * OECD
 * Eurostat
 * TSP Fund Data
 * Nasdaq Trader Symbol Definitions
 * Stooq Index Data
 * MOEX Data

`Quandl/Python <https://github.com/quandl/quandl-python>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Quandl API for Python wraps the Quandl REST API to return
pandas DataFrames with timeseries indexes.

`Pydatastream <https://github.com/vfilimonov/pydatastream>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
PyDatastream is a Python interface to the
`Refinitiv Datastream (DWS) <https://www.refinitiv.com/en/products/datastream-macroeconomic-analysis>`__
REST API to return indexed pandas DataFrames with financial data.
This package requires valid credentials for this API (non free).

`pandaSDMX <https://pandasdmx.readthedocs.io/en/v1.0/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandaSDMX is a library to retrieve and acquire statistical data
and metadata disseminated in
`SDMX <https://www.sdmx.org>`_ 2.1, an ISO-standard
widely used by institutions such as statistics offices, central banks,
and international organisations. pandaSDMX can expose datasets and related
structural metadata including data flows, code-lists,
and data structure definitions as pandas Series
or MultiIndexed DataFrames.

`fredapi <https://github.com/mortada/fredapi>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
fredapi is a Python interface to the `Federal Reserve Economic Data (FRED) <https://fred.stlouisfed.org/>`__
provided by the Federal Reserve Bank of St. Louis. It works with both the FRED database and ALFRED database that
contains point-in-time data (i.e. historic data revisions). fredapi provides a wrapper in Python to the FRED
HTTP API, and also provides several convenient methods for parsing and analyzing point-in-time data from ALFRED.
fredapi makes use of pandas and returns data in a Series or DataFrame. This module requires a FRED API key that
you can obtain for free on the FRED website.

`dataframe_sql <https://github.com/zbrookle/dataframe_sql>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
``dataframe_sql`` is a Python package that translates SQL syntax directly into
operations on pandas DataFrames. This is useful when migrating from a database to
using pandas or for users more comfortable with SQL looking for a way to interface
with pandas.


.. _ecosystem.domain:

Domain specific
---------------

`Geopandas <https://github.com/geopandas/geopandas>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Geopandas extends pandas data objects to include geographic information which support
geometric operations. If your work entails maps and geographical coordinates, and
you love pandas, you should take a close look at Geopandas.

`staircase <https://github.com/staircase-dev/staircase>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

staircase is a data analysis package, built upon pandas and numpy, for modelling and
manipulation of mathematical step functions. It provides a rich variety of arithmetic
operations, relational operations, logical operations, statistical operations and
aggregations for step functions defined over real numbers, datetime and timedelta domains.


`xarray <https://github.com/pydata/xarray>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

xarray brings the labeled data power of pandas to the physical sciences by
providing N-dimensional variants of the core pandas data structures. It aims to
provide a pandas-like and pandas-compatible toolkit for analytics on multi-
dimensional arrays, rather than the tabular data for which pandas excels.


.. _ecosystem.io:

IO
--

`BCPandas <https://github.com/yehoshuadimarsky/bcpandas>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

BCPandas provides high performance writes from pandas to Microsoft SQL Server,
far exceeding the performance of the native ``df.to_sql`` method. Internally, it uses
Microsoft's BCP utility, but the complexity is fully abstracted away from the end user.
Rigorously tested, it is a complete replacement for ``df.to_sql``.

`Deltalake <https://pypi.org/project/deltalake>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Deltalake python package lets you access tables stored in
`Delta Lake <https://delta.io/>`__ natively in Python without the need to use Spark or
JVM. It provides the ``delta_table.to_pyarrow_table().to_pandas()`` method to convert
any Delta table into Pandas dataframe.


.. _ecosystem.out-of-core:

Out-of-core
-----------

`Blaze <https://blaze.pydata.org/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Blaze provides a standard API for doing computations with various
in-memory and on-disk backends: NumPy, pandas, SQLAlchemy, MongoDB, PyTables,
PySpark.

`Cylon <https://cylondata.org/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Cylon is a fast, scalable, distributed memory parallel runtime with a pandas
like Python DataFrame API. ”Core Cylon” is implemented with C++ using Apache
Arrow format to represent the data in-memory. Cylon DataFrame API implements
most of the core operators of pandas such as merge, filter, join, concat,
group-by, drop_duplicates, etc. These operators are designed to work across
thousands of cores to scale applications. It can interoperate with pandas
DataFrame by reading data from pandas or converting data to pandas so users
can selectively scale parts of their pandas DataFrame applications.

.. code:: python

    from pycylon import read_csv, DataFrame, CylonEnv
    from pycylon.net import MPIConfig

    # Initialize Cylon distributed environment
    config: MPIConfig = MPIConfig()
    env: CylonEnv = CylonEnv(config=config, distributed=True)

    df1: DataFrame = read_csv('/tmp/csv1.csv')
    df2: DataFrame = read_csv('/tmp/csv2.csv')

    # Using 1000s of cores across the cluster to compute the join
    df3: Table = df1.join(other=df2, on=[0], algorithm="hash", env=env)

    print(df3)

`Dask <https://docs.dask.org/en/latest/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dask is a flexible parallel computing library for analytics. Dask
provides a familiar ``DataFrame`` interface for out-of-core, parallel and distributed computing.

`Dask-ML <https://dask-ml.readthedocs.io/en/latest/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dask-ML enables parallel and distributed machine learning using Dask alongside existing machine learning libraries like Scikit-Learn, XGBoost, and TensorFlow.

`Ibis <https://ibis-project.org/docs/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ibis offers a standard way to write analytics code, that can be run in multiple engines. It helps in bridging the gap between local Python environments (like pandas) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.).


`Koalas <https://koalas.readthedocs.io/en/latest/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Koalas provides a familiar pandas DataFrame interface on top of Apache Spark. It enables users to leverage multi-cores on one machine or a cluster of machines to speed up or scale their DataFrame code.

`Modin <https://github.com/modin-project/modin>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``modin.pandas`` DataFrame is a parallel and distributed drop-in replacement
for pandas. This means that you can use Modin with existing pandas code or write
new code with the existing pandas API. Modin can leverage your entire machine or
cluster to speed up and scale your pandas workloads, including traditionally
time-consuming tasks like ingesting data (``read_csv``, ``read_excel``,
``read_parquet``, etc.).

.. code:: python

    # import pandas as pd
    import modin.pandas as pd

    df = pd.read_csv("big.csv")  # use all your cores!

`Odo <http://odo.pydata.org/en/latest/>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Odo provides a uniform API for moving data between different formats. It uses
pandas own ``read_csv`` for CSV IO and leverages many existing packages such as
PyTables, h5py, and pymongo to move data between non pandas formats. Its graph
based approach is also extensible by end users for custom formats that may be
too specific for the core of odo.

`Pandarallel <https://github.com/nalepae/pandarallel>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Pandarallel provides a simple way to parallelize your pandas operations on all your CPUs by changing only one line of code.
If also displays progress bars.

.. code:: python

    from pandarallel import pandarallel

    pandarallel.initialize(progress_bar=True)

    # df.apply(func)
    df.parallel_apply(func)


`Vaex <https://vaex.io/docs/index.html>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Increasingly, packages are being built on top of pandas to address specific needs in data preparation, analysis and visualization. Vaex is a Python library for Out-of-Core DataFrames (similar to pandas), to visualize and explore big tabular datasets. It can calculate statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid up to a billion (10\ :sup:`9`) objects/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).

 * vaex.from_pandas
 * vaex.to_pandas_df

.. _ecosystem.extensions:

Extension data types
--------------------

pandas provides an interface for defining
:ref:`extension types <extending.extension-types>` to extend NumPy's type
system. The following libraries implement that interface to provide types not
found in NumPy or pandas, which work well with pandas' data containers.

`Cyberpandas`_
~~~~~~~~~~~~~~

Cyberpandas provides an extension type for storing arrays of IP Addresses. These
arrays can be stored inside pandas' Series and DataFrame.

`Pandas-Genomics`_
~~~~~~~~~~~~~~~~~~

Pandas-Genomics provides extension types, extension arrays, and extension accessors for working with genomics data

`Pint-Pandas`_
~~~~~~~~~~~~~~

``Pint-Pandas <https://github.com/hgrecco/pint-pandas>`` provides an extension type for
storing numeric arrays with units. These arrays can be stored inside pandas'
Series and DataFrame. Operations between Series and DataFrame columns which
use pint's extension array are then units aware.

`Text Extensions for Pandas`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``Text Extensions for Pandas <https://ibm.biz/text-extensions-for-pandas>``
provides extension types to cover common data structures for representing natural language
data, plus library integrations that convert the outputs of popular natural language
processing libraries into Pandas DataFrames.

.. _ecosystem.accessors:

Accessors
---------

A directory of projects providing
:ref:`extension accessors <extending.register-accessors>`. This is for users to
discover new accessors and for library authors to coordinate on the namespace.

================== ============ ==================================== ===============================================================================
Library            Accessor     Classes                              Description
================== ============ ==================================== ===============================================================================
`cyberpandas`_     ``ip``       ``Series``                           Provides common operations for working with IP addresses.
`pdvega`_          ``vgplot``   ``Series``, ``DataFrame``            Provides plotting functions from the Altair_ library.
`pandas-genomics`_ ``genomics`` ``Series``, ``DataFrame``            Provides common operations for quality control and analysis of genomics data.
`pandas_path`_     ``path``     ``Index``, ``Series``                Provides `pathlib.Path`_ functions for Series.
`pint-pandas`_     ``pint``     ``Series``, ``DataFrame``            Provides units support for numeric Series and DataFrames.
`composeml`_       ``slice``    ``DataFrame``                        Provides a generator for enhanced data slicing.
`datatest`_        ``validate`` ``Series``, ``DataFrame``, ``Index`` Provides validation, differences, and acceptance managers.
`woodwork`_        ``ww``       ``Series``, ``DataFrame``            Provides physical, logical, and semantic data typing information for Series and DataFrames.
`staircase`_       ``sc``       ``Series``                           Provides methods for querying, aggregating and plotting step functions
================== ============ ==================================== ===============================================================================

.. _cyberpandas: https://cyberpandas.readthedocs.io/en/latest
.. _pdvega: https://altair-viz.github.io/pdvega/
.. _Altair: https://altair-viz.github.io/
.. _pandas-genomics: https://pandas-genomics.readthedocs.io/en/latest/
.. _pandas_path: https://github.com/drivendataorg/pandas-path/
.. _pathlib.Path: https://docs.python.org/3/library/pathlib.html
.. _pint-pandas: https://github.com/hgrecco/pint-pandas
.. _composeml: https://github.com/alteryx/compose
.. _datatest: https://datatest.readthedocs.io/en/stable/
.. _woodwork: https://github.com/alteryx/woodwork
.. _staircase: https://www.staircase.dev/

Development tools
-----------------

`pandas-stubs <https://github.com/VirtusLab/pandas-stubs>`__
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While pandas repository is partially typed, the package itself doesn't expose this information for external use.
Install pandas-stubs to enable basic type coverage of pandas API.

Learn more by reading through :issue:`14468`, :issue:`26766`, :issue:`28142`.

See installation and usage instructions on the `github page <https://github.com/VirtusLab/pandas-stubs>`__.
.. _whatsnew_0233:

What's new in 0.23.3 (July 7, 2018)
-----------------------------------

{{ header }}

This release fixes a build issue with the sdist for Python 3.7 (:issue:`21785`)
There are no other changes.

.. _whatsnew_0.23.3.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.23.2..v0.23.3
.. _whatsnew_124:

What's new in 1.2.4 (April 12, 2021)
------------------------------------

These are the changes in pandas 1.2.4. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_124.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`DataFrame.sum` when ``min_count`` greater than the :class:`DataFrame` shape was passed resulted in a ``ValueError`` (:issue:`39738`)
- Fixed regression in :meth:`DataFrame.to_json` raising ``AttributeError`` when run on PyPy (:issue:`39837`)
- Fixed regression in (in)equality comparison of ``pd.NaT`` with a non-datetimelike numpy array returning a scalar instead of an array (:issue:`40722`)
- Fixed regression in :meth:`DataFrame.where` not returning a copy in the case of an all True condition (:issue:`39595`)
- Fixed regression in :meth:`DataFrame.replace` raising ``IndexError`` when ``regex`` was a multi-key dictionary (:issue:`39338`)
- Fixed regression in repr of floats in an ``object`` column not respecting ``float_format`` when printed in the console or outputted through :meth:`DataFrame.to_string`, :meth:`DataFrame.to_html`, and :meth:`DataFrame.to_latex` (:issue:`40024`)
- Fixed regression in NumPy ufuncs such as ``np.add`` not passing through all arguments for :class:`DataFrame` (:issue:`40662`)

.. ---------------------------------------------------------------------------

.. _whatsnew_124.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.3..v1.2.4
.. _whatsnew_130:

What's new in 1.3.0 (July 2, 2021)
----------------------------------

These are the changes in pandas 1.3.0. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. warning::

   When reading new Excel 2007+ (``.xlsx``) files, the default argument
   ``engine=None`` to :func:`read_excel` will now result in using the
   `openpyxl <https://openpyxl.readthedocs.io/en/stable/>`_ engine in all cases
   when the option :attr:`io.excel.xlsx.reader` is set to ``"auto"``.
   Previously, some cases would use the
   `xlrd <https://xlrd.readthedocs.io/en/latest/>`_ engine instead. See
   :ref:`What's new 1.2.0 <whatsnew_120>` for background on this change.

.. ---------------------------------------------------------------------------

.. _whatsnew_130.enhancements:

Enhancements
~~~~~~~~~~~~

.. _whatsnew_130.enhancements.read_csv_json_http_headers:

Custom HTTP(s) headers when reading csv or json files
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When reading from a remote URL that is not handled by fsspec (e.g. HTTP and
HTTPS) the dictionary passed to ``storage_options`` will be used to create the
headers included in the request.  This can be used to control the User-Agent
header or send other custom headers (:issue:`36688`).
For example:

.. ipython:: python

    headers = {"User-Agent": "pandas"}
    df = pd.read_csv(
        "https://download.bls.gov/pub/time.series/cu/cu.item",
        sep="\t",
        storage_options=headers
    )

.. _whatsnew_130.enhancements.read_to_xml:

Read and write XML documents
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We added I/O support to read and render shallow versions of `XML`_ documents with
:func:`read_xml` and :meth:`DataFrame.to_xml`. Using `lxml`_ as parser,
both XPath 1.0 and XSLT 1.0 are available. (:issue:`27554`)

.. _XML: https://www.w3.org/standards/xml/core
.. _lxml: https://lxml.de

.. code-block:: ipython

    In [1]: xml = """<?xml version='1.0' encoding='utf-8'?>
       ...: <data>
       ...:  <row>
       ...:     <shape>square</shape>
       ...:     <degrees>360</degrees>
       ...:     <sides>4.0</sides>
       ...:  </row>
       ...:  <row>
       ...:     <shape>circle</shape>
       ...:     <degrees>360</degrees>
       ...:     <sides/>
       ...:  </row>
       ...:  <row>
       ...:     <shape>triangle</shape>
       ...:     <degrees>180</degrees>
       ...:     <sides>3.0</sides>
       ...:  </row>
       ...:  </data>"""

    In [2]: df = pd.read_xml(xml)
    In [3]: df
    Out[3]:
          shape  degrees  sides
    0    square      360    4.0
    1    circle      360    NaN
    2  triangle      180    3.0

    In [4]: df.to_xml()
    Out[4]:
    <?xml version='1.0' encoding='utf-8'?>
    <data>
      <row>
        <index>0</index>
        <shape>square</shape>
        <degrees>360</degrees>
        <sides>4.0</sides>
      </row>
      <row>
        <index>1</index>
        <shape>circle</shape>
        <degrees>360</degrees>
        <sides/>
      </row>
      <row>
        <index>2</index>
        <shape>triangle</shape>
        <degrees>180</degrees>
        <sides>3.0</sides>
      </row>
    </data>

For more, see :ref:`io.xml` in the user guide on IO tools.

.. _whatsnew_130.enhancements.styler:

Styler enhancements
^^^^^^^^^^^^^^^^^^^

We provided some focused development on :class:`.Styler`. See also the `Styler documentation <../user_guide/style.ipynb>`_
which has been revised and improved (:issue:`39720`, :issue:`39317`, :issue:`40493`).

 - The method :meth:`.Styler.set_table_styles` can now accept more natural CSS language for arguments, such as ``'color:red;'`` instead of ``[('color', 'red')]`` (:issue:`39563`)
 - The methods :meth:`.Styler.highlight_null`, :meth:`.Styler.highlight_min`, and :meth:`.Styler.highlight_max` now allow custom CSS highlighting instead of the default background coloring (:issue:`40242`)
 - :meth:`.Styler.apply` now accepts functions that return an ``ndarray`` when ``axis=None``, making it now consistent with the ``axis=0`` and ``axis=1`` behavior (:issue:`39359`)
 - When incorrectly formatted CSS is given via :meth:`.Styler.apply` or :meth:`.Styler.applymap`, an error is now raised upon rendering (:issue:`39660`)
 - :meth:`.Styler.format` now accepts the keyword argument ``escape`` for optional HTML and LaTeX escaping (:issue:`40388`, :issue:`41619`)
 - :meth:`.Styler.background_gradient` has gained the argument ``gmap`` to supply a specific gradient map for shading (:issue:`22727`)
 - :meth:`.Styler.clear` now clears :attr:`Styler.hidden_index` and :attr:`Styler.hidden_columns` as well (:issue:`40484`)
 - Added the method :meth:`.Styler.highlight_between` (:issue:`39821`)
 - Added the method :meth:`.Styler.highlight_quantile` (:issue:`40926`)
 - Added the method :meth:`.Styler.text_gradient` (:issue:`41098`)
 - Added the method :meth:`.Styler.set_tooltips` to allow hover tooltips; this can be used enhance interactive displays (:issue:`21266`, :issue:`40284`)
 - Added the parameter ``precision`` to the method :meth:`.Styler.format` to control the display of floating point numbers (:issue:`40134`)
 - :class:`.Styler` rendered HTML output now follows the `w3 HTML Style Guide <https://www.w3schools.com/html/html5_syntax.asp>`_ (:issue:`39626`)
 - Many features of the :class:`.Styler` class are now either partially or fully usable on a DataFrame with a non-unique indexes or columns (:issue:`41143`)
 - One has greater control of the display through separate sparsification of the index or columns using the :ref:`new styler options <options.available>`, which are also usable via :func:`option_context` (:issue:`41142`)
 - Added the option ``styler.render.max_elements`` to avoid browser overload when styling large DataFrames (:issue:`40712`)
 - Added the method :meth:`.Styler.to_latex` (:issue:`21673`, :issue:`42320`), which also allows some limited CSS conversion (:issue:`40731`)
 - Added the method :meth:`.Styler.to_html` (:issue:`13379`)
 - Added the method :meth:`.Styler.set_sticky` to make index and column headers permanently visible in scrolling HTML frames (:issue:`29072`)

.. _whatsnew_130.enhancements.dataframe_honors_copy_with_dict:

DataFrame constructor honors ``copy=False`` with dict
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When passing a dictionary to :class:`DataFrame` with ``copy=False``,
a copy will no longer be made (:issue:`32960`).

.. ipython:: python

    arr = np.array([1, 2, 3])
    df = pd.DataFrame({"A": arr, "B": arr.copy()}, copy=False)
    df

``df["A"]`` remains a view on ``arr``:

.. ipython:: python

    arr[0] = 0
    assert df.iloc[0, 0] == 0

The default behavior when not passing ``copy`` will remain unchanged, i.e.
a copy will be made.

.. _whatsnew_130.enhancements.arrow_string:

PyArrow backed string data type
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've enhanced the :class:`StringDtype`, an extension type dedicated to string data.
(:issue:`39908`)

It is now possible to specify a ``storage`` keyword option to :class:`StringDtype`. Use
pandas options or specify the dtype using ``dtype='string[pyarrow]'`` to allow the
StringArray to be backed by a PyArrow array instead of a NumPy array of Python objects.

The PyArrow backed StringArray requires pyarrow 1.0.0 or greater to be installed.

.. warning::

   ``string[pyarrow]`` is currently considered experimental. The implementation
   and parts of the API may change without warning.

.. ipython:: python

   pd.Series(['abc', None, 'def'], dtype=pd.StringDtype(storage="pyarrow"))

You can use the alias ``"string[pyarrow]"`` as well.

.. ipython:: python

   s = pd.Series(['abc', None, 'def'], dtype="string[pyarrow]")
   s

You can also create a PyArrow backed string array using pandas options.

.. ipython:: python

    with pd.option_context("string_storage", "pyarrow"):
        s = pd.Series(['abc', None, 'def'], dtype="string")
    s

The usual string accessor methods work. Where appropriate, the return type of the Series
or columns of a DataFrame will also have string dtype.

.. ipython:: python

   s.str.upper()
   s.str.split('b', expand=True).dtypes

String accessor methods returning integers will return a value with :class:`Int64Dtype`

.. ipython:: python

   s.str.count("a")

.. _whatsnew_130.enhancements.centered_datetimelike_rolling_window:

Centered datetime-like rolling windows
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When performing rolling calculations on DataFrame and Series
objects with a datetime-like index, a centered datetime-like window can now be
used (:issue:`38780`).
For example:

.. ipython:: python

    df = pd.DataFrame(
        {"A": [0, 1, 2, 3, 4]}, index=pd.date_range("2020", periods=5, freq="1D")
    )
    df
    df.rolling("2D", center=True).mean()


.. _whatsnew_130.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- :meth:`DataFrame.rolling`, :meth:`Series.rolling`, :meth:`DataFrame.expanding`, and :meth:`Series.expanding` now support a ``method`` argument with a ``'table'`` option that performs the windowing operation over an entire :class:`DataFrame`. See :ref:`Window Overview <window.overview>` for performance and functional benefits (:issue:`15095`, :issue:`38995`)
- :class:`.ExponentialMovingWindow` now support a ``online`` method that can perform ``mean`` calculations in an online fashion. See :ref:`Window Overview <window.overview>` (:issue:`41673`)
- Added :meth:`MultiIndex.dtypes` (:issue:`37062`)
- Added ``end`` and ``end_day`` options for the ``origin`` argument in :meth:`DataFrame.resample` (:issue:`37804`)
- Improved error message when ``usecols`` and ``names`` do not match for :func:`read_csv` and ``engine="c"`` (:issue:`29042`)
- Improved consistency of error messages when passing an invalid ``win_type`` argument in :ref:`Window methods <api.window>` (:issue:`15969`)
- :func:`read_sql_query` now accepts a ``dtype`` argument to cast the columnar data from the SQL database based on user input (:issue:`10285`)
- :func:`read_csv` now raising ``ParserWarning`` if length of header or given names does not match length of data when ``usecols`` is not specified (:issue:`21768`)
- Improved integer type mapping from pandas to SQLAlchemy when using :meth:`DataFrame.to_sql` (:issue:`35076`)
- :func:`to_numeric` now supports downcasting of nullable ``ExtensionDtype`` objects (:issue:`33013`)
- Added support for dict-like names in :class:`MultiIndex.set_names` and :class:`MultiIndex.rename` (:issue:`20421`)
- :func:`read_excel` can now auto-detect .xlsb files and older .xls files (:issue:`35416`, :issue:`41225`)
- :class:`ExcelWriter` now accepts an ``if_sheet_exists`` parameter to control the behavior of append mode when writing to existing sheets (:issue:`40230`)
- :meth:`.Rolling.sum`, :meth:`.Expanding.sum`, :meth:`.Rolling.mean`, :meth:`.Expanding.mean`, :meth:`.ExponentialMovingWindow.mean`, :meth:`.Rolling.median`, :meth:`.Expanding.median`, :meth:`.Rolling.max`, :meth:`.Expanding.max`, :meth:`.Rolling.min`, and :meth:`.Expanding.min` now support `Numba <http://numba.pydata.org/>`_ execution with the ``engine`` keyword (:issue:`38895`, :issue:`41267`)
- :meth:`DataFrame.apply` can now accept NumPy unary operators as strings, e.g. ``df.apply("sqrt")``, which was already the case for :meth:`Series.apply` (:issue:`39116`)
- :meth:`DataFrame.apply` can now accept non-callable DataFrame properties as strings, e.g. ``df.apply("size")``, which was already the case for :meth:`Series.apply` (:issue:`39116`)
- :meth:`DataFrame.applymap` can now accept kwargs to pass on to the user-provided ``func`` (:issue:`39987`)
- Passing a :class:`DataFrame` indexer to ``iloc`` is now disallowed for :meth:`Series.__getitem__` and :meth:`DataFrame.__getitem__` (:issue:`39004`)
- :meth:`Series.apply` can now accept list-like or dictionary-like arguments that aren't lists or dictionaries, e.g. ``ser.apply(np.array(["sum", "mean"]))``, which was already the case for :meth:`DataFrame.apply` (:issue:`39140`)
- :meth:`DataFrame.plot.scatter` can now accept a categorical column for the argument ``c`` (:issue:`12380`, :issue:`31357`)
- :meth:`Series.loc` now raises a helpful error message when the Series has a :class:`MultiIndex` and the indexer has too many dimensions (:issue:`35349`)
- :func:`read_stata` now supports reading data from compressed files (:issue:`26599`)
- Added support for parsing ``ISO 8601``-like timestamps with negative signs to :class:`Timedelta` (:issue:`37172`)
- Added support for unary operators in :class:`FloatingArray` (:issue:`38749`)
- :class:`RangeIndex` can now be constructed by passing a ``range`` object directly e.g. ``pd.RangeIndex(range(3))`` (:issue:`12067`)
- :meth:`Series.round` and :meth:`DataFrame.round` now work with nullable integer and floating dtypes (:issue:`38844`)
- :meth:`read_csv` and :meth:`read_json` expose the argument ``encoding_errors`` to control how encoding errors are handled (:issue:`39450`)
- :meth:`.GroupBy.any` and :meth:`.GroupBy.all` use Kleene logic with nullable data types (:issue:`37506`)
- :meth:`.GroupBy.any` and :meth:`.GroupBy.all` return a ``BooleanDtype`` for columns with nullable data types (:issue:`33449`)
- :meth:`.GroupBy.any` and :meth:`.GroupBy.all` raising with ``object`` data containing ``pd.NA`` even when ``skipna=True`` (:issue:`37501`)
- :meth:`.GroupBy.rank` now supports object-dtype data (:issue:`38278`)
- Constructing a :class:`DataFrame` or :class:`Series` with the ``data`` argument being a Python iterable that is *not* a NumPy ``ndarray`` consisting of NumPy scalars will now result in a dtype with a precision the maximum of the NumPy scalars; this was already the case when ``data`` is a NumPy ``ndarray`` (:issue:`40908`)
- Add keyword ``sort`` to :func:`pivot_table` to allow non-sorting of the result (:issue:`39143`)
- Add keyword ``dropna`` to :meth:`DataFrame.value_counts` to allow counting rows that include ``NA`` values (:issue:`41325`)
- :meth:`Series.replace` will now cast results to ``PeriodDtype`` where possible instead of ``object`` dtype (:issue:`41526`)
- Improved error message in ``corr`` and ``cov`` methods on :class:`.Rolling`, :class:`.Expanding`, and :class:`.ExponentialMovingWindow` when ``other`` is not a :class:`DataFrame` or :class:`Series` (:issue:`41741`)
- :meth:`Series.between` can now accept ``left`` or ``right`` as arguments to ``inclusive`` to include only the left or right boundary (:issue:`40245`)
- :meth:`DataFrame.explode` now supports exploding multiple columns. Its ``column`` argument now also accepts a list of str or tuples for exploding on multiple columns at the same time (:issue:`39240`)
- :meth:`DataFrame.sample` now accepts the ``ignore_index`` argument to reset the index after sampling, similar to :meth:`DataFrame.drop_duplicates` and :meth:`DataFrame.sort_values` (:issue:`38581`)

.. ---------------------------------------------------------------------------

.. _whatsnew_130.notable_bug_fixes:

Notable bug fixes
~~~~~~~~~~~~~~~~~

These are bug fixes that might have notable behavior changes.

.. _whatsnew_130.notable_bug_fixes.categorical_unique_maintains_dtype:

``Categorical.unique`` now always maintains same dtype as original
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, when calling :meth:`Categorical.unique` with categorical data, unused categories in the new array
would be removed, making the dtype of the new array different than the
original (:issue:`18291`)

As an example of this, given:

.. ipython:: python

        dtype = pd.CategoricalDtype(['bad', 'neutral', 'good'], ordered=True)
        cat = pd.Categorical(['good', 'good', 'bad', 'bad'], dtype=dtype)
        original = pd.Series(cat)
        unique = original.unique()

*Previous behavior*:

.. code-block:: ipython

    In [1]: unique
    ['good', 'bad']
    Categories (2, object): ['bad' < 'good']
    In [2]: original.dtype == unique.dtype
    False

*New behavior*:

.. ipython:: python

        unique
        original.dtype == unique.dtype

.. _whatsnew_130.notable_bug_fixes.combine_first_preserves_dtype:

Preserve dtypes in :meth:`DataFrame.combine_first`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.combine_first` will now preserve dtypes (:issue:`7509`)

.. ipython:: python

   df1 = pd.DataFrame({"A": [1, 2, 3], "B": [1, 2, 3]}, index=[0, 1, 2])
   df1
   df2 = pd.DataFrame({"B": [4, 5, 6], "C": [1, 2, 3]}, index=[2, 3, 4])
   df2
   combined = df1.combine_first(df2)

*Previous behavior*:

.. code-block:: ipython

   In [1]: combined.dtypes
   Out[2]:
   A    float64
   B    float64
   C    float64
   dtype: object

*New behavior*:

.. ipython:: python

   combined.dtypes

.. _whatsnew_130.notable_bug_fixes.groupby_preserves_dtype:

Groupby methods agg and transform no longer changes return dtype for callables
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously the methods :meth:`.DataFrameGroupBy.aggregate`,
:meth:`.SeriesGroupBy.aggregate`, :meth:`.DataFrameGroupBy.transform`, and
:meth:`.SeriesGroupBy.transform` might cast the result dtype when the argument ``func``
is callable, possibly leading to undesirable results (:issue:`21240`). The cast would
occur if the result is numeric and casting back to the input dtype does not change any
values as measured by ``np.allclose``. Now no such casting occurs.

.. ipython:: python

    df = pd.DataFrame({'key': [1, 1], 'a': [True, False], 'b': [True, True]})
    df

*Previous behavior*:

.. code-block:: ipython

    In [5]: df.groupby('key').agg(lambda x: x.sum())
    Out[5]:
            a  b
    key
    1    True  2

*New behavior*:

.. ipython:: python

    df.groupby('key').agg(lambda x: x.sum())

.. _whatsnew_130.notable_bug_fixes.groupby_reductions_float_result:

``float`` result for :meth:`.GroupBy.mean`, :meth:`.GroupBy.median`, and :meth:`.GroupBy.var`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, these methods could result in different dtypes depending on the input values.
Now, these methods will always return a float dtype. (:issue:`41137`)

.. ipython:: python

    df = pd.DataFrame({'a': [True], 'b': [1], 'c': [1.0]})

*Previous behavior*:

.. code-block:: ipython

    In [5]: df.groupby(df.index).mean()
    Out[5]:
            a  b    c
    0    True  1  1.0

*New behavior*:

.. ipython:: python

    df.groupby(df.index).mean()

.. _whatsnew_130.notable_bug_fixes.setitem_column_try_inplace:

Try operating inplace when setting values with ``loc`` and ``iloc``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When setting an entire column using ``loc`` or ``iloc``, pandas will try to
insert the values into the existing data rather than create an entirely new array.

.. ipython:: python

   df = pd.DataFrame(range(3), columns=["A"], dtype="float64")
   values = df.values
   new = np.array([5, 6, 7], dtype="int64")
   df.loc[[0, 1, 2], "A"] = new

In both the new and old behavior, the data in ``values`` is overwritten, but in
the old behavior the dtype of ``df["A"]`` changed to ``int64``.

*Previous behavior*:

.. code-block:: ipython

   In [1]: df.dtypes
   Out[1]:
   A    int64
   dtype: object
   In [2]: np.shares_memory(df["A"].values, new)
   Out[2]: False
   In [3]: np.shares_memory(df["A"].values, values)
   Out[3]: False

In pandas 1.3.0, ``df`` continues to share data with ``values``

*New behavior*:

.. ipython:: python

   df.dtypes
   np.shares_memory(df["A"], new)
   np.shares_memory(df["A"], values)


.. _whatsnew_130.notable_bug_fixes.setitem_never_inplace:

Never operate inplace when setting ``frame[keys] = values``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When setting multiple columns using ``frame[keys] = values`` new arrays will
replace pre-existing arrays for these keys, which will *not* be over-written
(:issue:`39510`).  As a result, the columns will retain the dtype(s) of ``values``,
never casting to the dtypes of the existing arrays.

.. ipython:: python

   df = pd.DataFrame(range(3), columns=["A"], dtype="float64")
   df[["A"]] = 5

In the old behavior, ``5`` was cast to ``float64`` and inserted into the existing
array backing ``df``:

*Previous behavior*:

.. code-block:: ipython

   In [1]: df.dtypes
   Out[1]:
   A    float64

In the new behavior, we get a new array, and retain an integer-dtyped ``5``:

*New behavior*:

.. ipython:: python

   df.dtypes


.. _whatsnew_130.notable_bug_fixes.setitem_with_bool_casting:

Consistent casting with setting into Boolean Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Setting non-boolean values into a :class:`Series` with ``dtype=bool`` now consistently
casts to ``dtype=object`` (:issue:`38709`)

.. ipython:: python

   orig = pd.Series([True, False])
   ser = orig.copy()
   ser.iloc[1] = np.nan
   ser2 = orig.copy()
   ser2.iloc[1] = 2.0

*Previous behavior*:

.. code-block:: ipython

   In [1]: ser
   Out [1]:
   0    1.0
   1    NaN
   dtype: float64

   In [2]:ser2
   Out [2]:
   0    True
   1     2.0
   dtype: object

*New behavior*:

.. ipython:: python

   ser
   ser2


.. _whatsnew_130.notable_bug_fixes.rolling_groupby_column:

GroupBy.rolling no longer returns grouped-by column in values
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The group-by column will now be dropped from the result of a
``groupby.rolling`` operation (:issue:`32262`)

.. ipython:: python

    df = pd.DataFrame({"A": [1, 1, 2, 3], "B": [0, 1, 2, 3]})
    df

*Previous behavior*:

.. code-block:: ipython

    In [1]: df.groupby("A").rolling(2).sum()
    Out[1]:
           A    B
    A
    1 0  NaN  NaN
    1    2.0  1.0
    2 2  NaN  NaN
    3 3  NaN  NaN

*New behavior*:

.. ipython:: python

    df.groupby("A").rolling(2).sum()

.. _whatsnew_130.notable_bug_fixes.rolling_var_precision:

Removed artificial truncation in rolling variance and standard deviation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`.Rolling.std` and :meth:`.Rolling.var` will no longer
artificially truncate results that are less than ``~1e-8`` and ``~1e-15`` respectively to
zero (:issue:`37051`, :issue:`40448`, :issue:`39872`).

However, floating point artifacts may now exist in the results when rolling over larger values.

.. ipython:: python

   s = pd.Series([7, 5, 5, 5])
   s.rolling(3).var()

.. _whatsnew_130.notable_bug_fixes.rolling_groupby_multiindex:

GroupBy.rolling with MultiIndex no longer drops levels in the result
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`GroupBy.rolling` will no longer drop levels of a :class:`DataFrame`
with a :class:`MultiIndex` in the result. This can lead to a perceived duplication of levels in the resulting
:class:`MultiIndex`, but this change restores the behavior that was present in version 1.1.3 (:issue:`38787`, :issue:`38523`).


.. ipython:: python

   index = pd.MultiIndex.from_tuples([('idx1', 'idx2')], names=['label1', 'label2'])
   df = pd.DataFrame({'a': [1], 'b': [2]}, index=index)
   df

*Previous behavior*:

.. code-block:: ipython

    In [1]: df.groupby('label1').rolling(1).sum()
    Out[1]:
              a    b
    label1
    idx1    1.0  2.0

*New behavior*:

.. ipython:: python

    df.groupby('label1').rolling(1).sum()


.. ---------------------------------------------------------------------------

.. _whatsnew_130.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_130.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Some minimum supported versions of dependencies were updated.
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| numpy           | 1.17.3          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| pytz            | 2017.3          |    X     |         |
+-----------------+-----------------+----------+---------+
| python-dateutil | 2.7.3           |    X     |         |
+-----------------+-----------------+----------+---------+
| bottleneck      | 1.2.1           |          |         |
+-----------------+-----------------+----------+---------+
| numexpr         | 2.7.0           |          |    X    |
+-----------------+-----------------+----------+---------+
| pytest (dev)    | 6.0             |          |    X    |
+-----------------+-----------------+----------+---------+
| mypy (dev)      | 0.812           |          |    X    |
+-----------------+-----------------+----------+---------+
| setuptools      | 38.6.0          |          |    X    |
+-----------------+-----------------+----------+---------+

For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
| beautifulsoup4  | 4.6.0           |         |
+-----------------+-----------------+---------+
| fastparquet     | 0.4.0           |    X    |
+-----------------+-----------------+---------+
| fsspec          | 0.7.4           |         |
+-----------------+-----------------+---------+
| gcsfs           | 0.6.0           |         |
+-----------------+-----------------+---------+
| lxml            | 4.3.0           |         |
+-----------------+-----------------+---------+
| matplotlib      | 2.2.3           |         |
+-----------------+-----------------+---------+
| numba           | 0.46.0          |         |
+-----------------+-----------------+---------+
| openpyxl        | 3.0.0           |    X    |
+-----------------+-----------------+---------+
| pyarrow         | 0.17.0          |    X    |
+-----------------+-----------------+---------+
| pymysql         | 0.8.1           |    X    |
+-----------------+-----------------+---------+
| pytables        | 3.5.1           |         |
+-----------------+-----------------+---------+
| s3fs            | 0.4.0           |         |
+-----------------+-----------------+---------+
| scipy           | 1.2.0           |         |
+-----------------+-----------------+---------+
| sqlalchemy      | 1.3.0           |    X    |
+-----------------+-----------------+---------+
| tabulate        | 0.8.7           |    X    |
+-----------------+-----------------+---------+
| xarray          | 0.12.0          |         |
+-----------------+-----------------+---------+
| xlrd            | 1.2.0           |         |
+-----------------+-----------------+---------+
| xlsxwriter      | 1.0.2           |         |
+-----------------+-----------------+---------+
| xlwt            | 1.3.0           |         |
+-----------------+-----------------+---------+
| pandas-gbq      | 0.12.0          |         |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

.. _whatsnew_130.api_breaking.other:

Other API changes
^^^^^^^^^^^^^^^^^
- Partially initialized :class:`CategoricalDtype` objects (i.e. those with ``categories=None``) will no longer compare as equal to fully initialized dtype objects (:issue:`38516`)
- Accessing ``_constructor_expanddim`` on a :class:`DataFrame` and ``_constructor_sliced`` on a :class:`Series` now raise an ``AttributeError``. Previously a ``NotImplementedError`` was raised (:issue:`38782`)
- Added new ``engine`` and ``**engine_kwargs`` parameters to :meth:`DataFrame.to_sql` to support other future "SQL engines". Currently we still only use ``SQLAlchemy`` under the hood, but more engines are planned to be supported such as `turbodbc <https://turbodbc.readthedocs.io/en/latest/>`_ (:issue:`36893`)
- Removed redundant ``freq`` from :class:`PeriodIndex` string representation (:issue:`41653`)
- :meth:`ExtensionDtype.construct_array_type` is now a required method instead of an optional one for :class:`ExtensionDtype` subclasses (:issue:`24860`)
- Calling ``hash`` on non-hashable pandas objects will now raise ``TypeError`` with the built-in error message (e.g. ``unhashable type: 'Series'``). Previously it would raise a custom message such as ``'Series' objects are mutable, thus they cannot be hashed``. Furthermore, ``isinstance(<Series>, abc.collections.Hashable)`` will now return ``False`` (:issue:`40013`)
- :meth:`.Styler.from_custom_template` now has two new arguments for template names, and removed the old ``name``, due to template inheritance having been introducing for better parsing (:issue:`42053`). Subclassing modifications to Styler attributes are also needed.

.. _whatsnew_130.api_breaking.build:

Build
^^^^^
- Documentation in ``.pptx`` and ``.pdf`` formats are no longer included in wheels or source distributions. (:issue:`30741`)

.. ---------------------------------------------------------------------------

.. _whatsnew_130.deprecations:

Deprecations
~~~~~~~~~~~~

.. _whatsnew_130.deprecations.nuisance_columns:

Deprecated dropping nuisance columns in DataFrame reductions and DataFrameGroupBy operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Calling a reduction (e.g. ``.min``, ``.max``, ``.sum``) on a :class:`DataFrame` with
``numeric_only=None`` (the default), columns where the reduction raises a ``TypeError``
are silently ignored and dropped from the result.

This behavior is deprecated. In a future version, the ``TypeError`` will be raised,
and users will need to select only valid columns before calling the function.

For example:

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2, 3, 4], "B": pd.date_range("2016-01-01", periods=4)})
   df

*Old behavior*:

.. code-block:: ipython

    In [3]: df.prod()
    Out[3]:
    Out[3]:
    A    24
    dtype: int64

*Future behavior*:

.. code-block:: ipython

    In [4]: df.prod()
    ...
    TypeError: 'DatetimeArray' does not implement reduction 'prod'

    In [5]: df[["A"]].prod()
    Out[5]:
    A    24
    dtype: int64


Similarly, when applying a function to :class:`DataFrameGroupBy`, columns on which
the function raises ``TypeError`` are currently silently ignored and dropped
from the result.

This behavior is deprecated.  In a future version, the ``TypeError``
will be raised, and users will need to select only valid columns before calling
the function.

For example:

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2, 3, 4], "B": pd.date_range("2016-01-01", periods=4)})
   gb = df.groupby([1, 1, 2, 2])

*Old behavior*:

.. code-block:: ipython

    In [4]: gb.prod(numeric_only=False)
    Out[4]:
    A
    1   2
    2  12

*Future behavior*:

.. code-block:: ipython

    In [5]: gb.prod(numeric_only=False)
    ...
    TypeError: datetime64 type does not support prod operations

    In [6]: gb[["A"]].prod(numeric_only=False)
    Out[6]:
        A
    1   2
    2  12

.. _whatsnew_130.deprecations.other:

Other Deprecations
^^^^^^^^^^^^^^^^^^
- Deprecated allowing scalars to be passed to the :class:`Categorical` constructor (:issue:`38433`)
- Deprecated constructing :class:`CategoricalIndex` without passing list-like data (:issue:`38944`)
- Deprecated allowing subclass-specific keyword arguments in the :class:`Index` constructor, use the specific subclass directly instead (:issue:`14093`, :issue:`21311`, :issue:`22315`, :issue:`26974`)
- Deprecated the :meth:`astype` method of datetimelike (``timedelta64[ns]``, ``datetime64[ns]``, ``Datetime64TZDtype``, ``PeriodDtype``) to convert to integer dtypes, use ``values.view(...)`` instead (:issue:`38544`). This deprecation was later reverted in pandas 1.4.0.
- Deprecated :meth:`MultiIndex.is_lexsorted` and :meth:`MultiIndex.lexsort_depth`, use :meth:`MultiIndex.is_monotonic_increasing` instead (:issue:`32259`)
- Deprecated keyword ``try_cast`` in :meth:`Series.where`, :meth:`Series.mask`, :meth:`DataFrame.where`, :meth:`DataFrame.mask`; cast results manually if desired (:issue:`38836`)
- Deprecated comparison of :class:`Timestamp` objects with ``datetime.date`` objects.  Instead of e.g. ``ts <= mydate`` use ``ts <= pd.Timestamp(mydate)`` or ``ts.date() <= mydate`` (:issue:`36131`)
- Deprecated :attr:`Rolling.win_type` returning ``"freq"`` (:issue:`38963`)
- Deprecated :attr:`Rolling.is_datetimelike` (:issue:`38963`)
- Deprecated :class:`DataFrame` indexer for :meth:`Series.__setitem__` and :meth:`DataFrame.__setitem__` (:issue:`39004`)
- Deprecated :meth:`ExponentialMovingWindow.vol` (:issue:`39220`)
- Using ``.astype`` to convert between ``datetime64[ns]`` dtype and :class:`DatetimeTZDtype` is deprecated and will raise in a future version, use ``obj.tz_localize`` or ``obj.dt.tz_localize`` instead (:issue:`38622`)
- Deprecated casting ``datetime.date`` objects to ``datetime64`` when used as ``fill_value`` in :meth:`DataFrame.unstack`, :meth:`DataFrame.shift`, :meth:`Series.shift`, and :meth:`DataFrame.reindex`, pass ``pd.Timestamp(dateobj)`` instead (:issue:`39767`)
- Deprecated :meth:`.Styler.set_na_rep` and :meth:`.Styler.set_precision` in favor of :meth:`.Styler.format` with ``na_rep`` and ``precision`` as existing and new input arguments respectively (:issue:`40134`, :issue:`40425`)
- Deprecated :meth:`.Styler.where` in favor of using an alternative formulation with :meth:`Styler.applymap` (:issue:`40821`)
- Deprecated allowing partial failure in :meth:`Series.transform` and :meth:`DataFrame.transform` when ``func`` is list-like or dict-like and raises anything but ``TypeError``; ``func`` raising anything but a ``TypeError`` will raise in a future version (:issue:`40211`)
- Deprecated arguments ``error_bad_lines`` and ``warn_bad_lines`` in :meth:`read_csv` and :meth:`read_table` in favor of argument ``on_bad_lines`` (:issue:`15122`)
- Deprecated support for ``np.ma.mrecords.MaskedRecords`` in the :class:`DataFrame` constructor, pass ``{name: data[name] for name in data.dtype.names}`` instead (:issue:`40363`)
- Deprecated using :func:`merge`, :meth:`DataFrame.merge`, and :meth:`DataFrame.join` on a different number of levels (:issue:`34862`)
- Deprecated the use of ``**kwargs`` in :class:`.ExcelWriter`; use the keyword argument ``engine_kwargs`` instead (:issue:`40430`)
- Deprecated the ``level`` keyword for :class:`DataFrame` and :class:`Series` aggregations; use groupby instead (:issue:`39983`)
- Deprecated the ``inplace`` parameter of :meth:`Categorical.remove_categories`, :meth:`Categorical.add_categories`, :meth:`Categorical.reorder_categories`, :meth:`Categorical.rename_categories`, :meth:`Categorical.set_categories` and will be removed in a future version (:issue:`37643`)
- Deprecated :func:`merge` producing duplicated columns through the ``suffixes`` keyword  and already existing columns (:issue:`22818`)
- Deprecated setting :attr:`Categorical._codes`, create a new :class:`Categorical` with the desired codes instead (:issue:`40606`)
- Deprecated the ``convert_float`` optional argument in :func:`read_excel` and :meth:`ExcelFile.parse` (:issue:`41127`)
- Deprecated behavior of :meth:`DatetimeIndex.union` with mixed timezones; in a future version both will be cast to UTC instead of object dtype (:issue:`39328`)
- Deprecated using ``usecols`` with out of bounds indices for :func:`read_csv` with ``engine="c"`` (:issue:`25623`)
- Deprecated special treatment of lists with first element a Categorical in the :class:`DataFrame` constructor; pass as ``pd.DataFrame({col: categorical, ...})`` instead (:issue:`38845`)
- Deprecated behavior of :class:`DataFrame` constructor when a ``dtype`` is passed and the data cannot be cast to that dtype. In a future version, this will raise instead of being silently ignored (:issue:`24435`)
- Deprecated the :attr:`Timestamp.freq` attribute.  For the properties that use it (``is_month_start``, ``is_month_end``, ``is_quarter_start``, ``is_quarter_end``, ``is_year_start``, ``is_year_end``), when you have a ``freq``, use e.g. ``freq.is_month_start(ts)`` (:issue:`15146`)
- Deprecated construction of :class:`Series` or :class:`DataFrame` with ``DatetimeTZDtype`` data and ``datetime64[ns]`` dtype.  Use ``Series(data).dt.tz_localize(None)`` instead (:issue:`41555`, :issue:`33401`)
- Deprecated behavior of :class:`Series` construction with large-integer values and small-integer dtype silently overflowing; use ``Series(data).astype(dtype)`` instead (:issue:`41734`)
- Deprecated behavior of :class:`DataFrame` construction with floating data and integer dtype casting even when lossy; in a future version this will remain floating, matching :class:`Series` behavior (:issue:`41770`)
- Deprecated inference of ``timedelta64[ns]``, ``datetime64[ns]``, or ``DatetimeTZDtype`` dtypes in :class:`Series` construction when data containing strings is passed and no ``dtype`` is passed (:issue:`33558`)
- In a future version, constructing :class:`Series` or :class:`DataFrame` with ``datetime64[ns]`` data and ``DatetimeTZDtype`` will treat the data as wall-times instead of as UTC times (matching DatetimeIndex behavior). To treat the data as UTC times, use ``pd.Series(data).dt.tz_localize("UTC").dt.tz_convert(dtype.tz)`` or ``pd.Series(data.view("int64"), dtype=dtype)`` (:issue:`33401`)
- Deprecated passing lists as ``key`` to :meth:`DataFrame.xs` and :meth:`Series.xs` (:issue:`41760`)
- Deprecated boolean arguments of ``inclusive`` in :meth:`Series.between` to have ``{"left", "right", "neither", "both"}`` as standard argument values (:issue:`40628`)
- Deprecated passing arguments as positional for all of the following, with exceptions noted (:issue:`41485`):

  - :func:`concat` (other than ``objs``)
  - :func:`read_csv` (other than ``filepath_or_buffer``)
  - :func:`read_table` (other than ``filepath_or_buffer``)
  - :meth:`DataFrame.clip` and :meth:`Series.clip` (other than ``upper`` and ``lower``)
  - :meth:`DataFrame.drop_duplicates` (except for ``subset``), :meth:`Series.drop_duplicates`, :meth:`Index.drop_duplicates` and :meth:`MultiIndex.drop_duplicates`
  - :meth:`DataFrame.drop` (other than ``labels``) and :meth:`Series.drop`
  - :meth:`DataFrame.dropna` and :meth:`Series.dropna`
  - :meth:`DataFrame.ffill`, :meth:`Series.ffill`, :meth:`DataFrame.bfill`, and :meth:`Series.bfill`
  - :meth:`DataFrame.fillna` and :meth:`Series.fillna` (apart from ``value``)
  - :meth:`DataFrame.interpolate` and :meth:`Series.interpolate` (other than ``method``)
  - :meth:`DataFrame.mask` and :meth:`Series.mask` (other than ``cond`` and ``other``)
  - :meth:`DataFrame.reset_index` (other than ``level``) and :meth:`Series.reset_index`
  - :meth:`DataFrame.set_axis` and :meth:`Series.set_axis` (other than ``labels``)
  - :meth:`DataFrame.set_index` (other than ``keys``)
  - :meth:`DataFrame.sort_index` and :meth:`Series.sort_index`
  - :meth:`DataFrame.sort_values` (other than ``by``) and :meth:`Series.sort_values`
  - :meth:`DataFrame.where` and :meth:`Series.where` (other than ``cond`` and ``other``)
  - :meth:`Index.set_names` and :meth:`MultiIndex.set_names` (except for ``names``)
  - :meth:`MultiIndex.codes` (except for ``codes``)
  - :meth:`MultiIndex.set_levels` (except for ``levels``)
  - :meth:`Resampler.interpolate` (other than ``method``)


.. ---------------------------------------------------------------------------


.. _whatsnew_130.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~
- Performance improvement in :meth:`IntervalIndex.isin` (:issue:`38353`)
- Performance improvement in :meth:`Series.mean` for nullable data types (:issue:`34814`)
- Performance improvement in :meth:`Series.isin` for nullable data types (:issue:`38340`)
- Performance improvement in :meth:`DataFrame.fillna` with ``method="pad"`` or ``method="backfill"`` for nullable floating and nullable integer dtypes (:issue:`39953`)
- Performance improvement in :meth:`DataFrame.corr` for ``method=kendall`` (:issue:`28329`)
- Performance improvement in :meth:`DataFrame.corr` for ``method=spearman`` (:issue:`40956`, :issue:`41885`)
- Performance improvement in :meth:`.Rolling.corr` and :meth:`.Rolling.cov` (:issue:`39388`)
- Performance improvement in :meth:`.RollingGroupby.corr`, :meth:`.ExpandingGroupby.corr`, :meth:`.ExpandingGroupby.corr` and :meth:`.ExpandingGroupby.cov` (:issue:`39591`)
- Performance improvement in :func:`unique` for object data type (:issue:`37615`)
- Performance improvement in :func:`json_normalize` for basic cases (including separators) (:issue:`40035` :issue:`15621`)
- Performance improvement in :class:`.ExpandingGroupby` aggregation methods (:issue:`39664`)
- Performance improvement in :class:`.Styler` where render times are more than 50% reduced and now matches :meth:`DataFrame.to_html` (:issue:`39972` :issue:`39952`, :issue:`40425`)
- The method :meth:`.Styler.set_td_classes` is now as performant as :meth:`.Styler.apply` and :meth:`.Styler.applymap`, and even more so in some cases (:issue:`40453`)
- Performance improvement in :meth:`.ExponentialMovingWindow.mean` with ``times`` (:issue:`39784`)
- Performance improvement in :meth:`.GroupBy.apply` when requiring the Python fallback implementation (:issue:`40176`)
- Performance improvement in the conversion of a PyArrow Boolean array to a pandas nullable Boolean array (:issue:`41051`)
- Performance improvement for concatenation of data with type :class:`CategoricalDtype` (:issue:`40193`)
- Performance improvement in :meth:`.GroupBy.cummin` and :meth:`.GroupBy.cummax` with nullable data types (:issue:`37493`)
- Performance improvement in :meth:`Series.nunique` with nan values (:issue:`40865`)
- Performance improvement in :meth:`DataFrame.transpose`, :meth:`Series.unstack` with ``DatetimeTZDtype`` (:issue:`40149`)
- Performance improvement in :meth:`Series.plot` and :meth:`DataFrame.plot` with entry point lazy loading (:issue:`41492`)

.. ---------------------------------------------------------------------------

.. _whatsnew_130.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^
- Bug in :class:`CategoricalIndex` incorrectly failing to raise ``TypeError`` when scalar data is passed (:issue:`38614`)
- Bug in ``CategoricalIndex.reindex`` failed when the :class:`Index` passed was not categorical but whose values were all labels in the category (:issue:`28690`)
- Bug where constructing a :class:`Categorical` from an object-dtype array of ``date`` objects did not round-trip correctly with ``astype`` (:issue:`38552`)
- Bug in constructing a :class:`DataFrame` from an ``ndarray`` and a :class:`CategoricalDtype` (:issue:`38857`)
- Bug in setting categorical values into an object-dtype column in a :class:`DataFrame` (:issue:`39136`)
- Bug in :meth:`DataFrame.reindex` was raising an ``IndexError`` when the new index contained duplicates and the old index was a :class:`CategoricalIndex` (:issue:`38906`)
- Bug in :meth:`Categorical.fillna` with a tuple-like category raising ``NotImplementedError`` instead of ``ValueError`` when filling with a non-category tuple (:issue:`41914`)

Datetimelike
^^^^^^^^^^^^
- Bug in :class:`DataFrame` and :class:`Series` constructors sometimes dropping nanoseconds from :class:`Timestamp` (resp. :class:`Timedelta`) ``data``, with ``dtype=datetime64[ns]`` (resp. ``timedelta64[ns]``) (:issue:`38032`)
- Bug in :meth:`DataFrame.first` and :meth:`Series.first` with an offset of one month returning an incorrect result when the first day is the last day of a month (:issue:`29623`)
- Bug in constructing a :class:`DataFrame` or :class:`Series` with mismatched ``datetime64`` data and ``timedelta64`` dtype, or vice-versa, failing to raise a ``TypeError`` (:issue:`38575`, :issue:`38764`, :issue:`38792`)
- Bug in constructing a :class:`Series` or :class:`DataFrame` with a ``datetime`` object out of bounds for ``datetime64[ns]`` dtype or a ``timedelta`` object out of bounds for ``timedelta64[ns]`` dtype (:issue:`38792`, :issue:`38965`)
- Bug in :meth:`DatetimeIndex.intersection`, :meth:`DatetimeIndex.symmetric_difference`, :meth:`PeriodIndex.intersection`, :meth:`PeriodIndex.symmetric_difference` always returning object-dtype when operating with :class:`CategoricalIndex` (:issue:`38741`)
- Bug in :meth:`DatetimeIndex.intersection` giving incorrect results with non-Tick frequencies with ``n != 1`` (:issue:`42104`)
- Bug in :meth:`Series.where` incorrectly casting ``datetime64`` values to ``int64`` (:issue:`37682`)
- Bug in :class:`Categorical` incorrectly typecasting ``datetime`` object to ``Timestamp`` (:issue:`38878`)
- Bug in comparisons between :class:`Timestamp` object and ``datetime64`` objects just outside the implementation bounds for nanosecond ``datetime64`` (:issue:`39221`)
- Bug in :meth:`Timestamp.round`, :meth:`Timestamp.floor`, :meth:`Timestamp.ceil` for values near the implementation bounds of :class:`Timestamp` (:issue:`39244`)
- Bug in :meth:`Timedelta.round`, :meth:`Timedelta.floor`, :meth:`Timedelta.ceil` for values near the implementation bounds of :class:`Timedelta` (:issue:`38964`)
- Bug in :func:`date_range` incorrectly creating :class:`DatetimeIndex` containing ``NaT`` instead of raising ``OutOfBoundsDatetime`` in corner cases (:issue:`24124`)
- Bug in :func:`infer_freq` incorrectly fails to infer 'H' frequency of :class:`DatetimeIndex` if the latter has a timezone and crosses DST boundaries (:issue:`39556`)
- Bug in :class:`Series` backed by :class:`DatetimeArray` or :class:`TimedeltaArray` sometimes failing to set the array's ``freq`` to ``None`` (:issue:`41425`)

Timedelta
^^^^^^^^^
- Bug in constructing :class:`Timedelta` from ``np.timedelta64`` objects with non-nanosecond units that are out of bounds for ``timedelta64[ns]`` (:issue:`38965`)
- Bug in constructing a :class:`TimedeltaIndex` incorrectly accepting ``np.datetime64("NaT")`` objects (:issue:`39462`)
- Bug in constructing :class:`Timedelta` from an input string with only symbols and no digits failed to raise an error (:issue:`39710`)
- Bug in :class:`TimedeltaIndex` and :func:`to_timedelta` failing to raise when passed non-nanosecond ``timedelta64`` arrays that overflow when converting to ``timedelta64[ns]`` (:issue:`40008`)

Timezones
^^^^^^^^^
- Bug in different ``tzinfo`` objects representing UTC not being treated as equivalent (:issue:`39216`)
- Bug in ``dateutil.tz.gettz("UTC")`` not being recognized as equivalent to other UTC-representing tzinfos (:issue:`39276`)

Numeric
^^^^^^^
- Bug in :meth:`DataFrame.quantile`, :meth:`DataFrame.sort_values` causing incorrect subsequent indexing behavior (:issue:`38351`)
- Bug in :meth:`DataFrame.sort_values` raising an :class:`IndexError` for empty ``by`` (:issue:`40258`)
- Bug in :meth:`DataFrame.select_dtypes` with ``include=np.number`` would drop numeric ``ExtensionDtype`` columns (:issue:`35340`)
- Bug in :meth:`DataFrame.mode` and :meth:`Series.mode` not keeping consistent integer :class:`Index` for empty input (:issue:`33321`)
- Bug in :meth:`DataFrame.rank` when the DataFrame contained ``np.inf`` (:issue:`32593`)
- Bug in :meth:`DataFrame.rank` with ``axis=0`` and columns holding incomparable types raising an ``IndexError`` (:issue:`38932`)
- Bug in :meth:`Series.rank`, :meth:`DataFrame.rank`, and :meth:`.GroupBy.rank` treating the most negative ``int64`` value as missing (:issue:`32859`)
- Bug in :meth:`DataFrame.select_dtypes` different behavior between Windows and Linux with ``include="int"`` (:issue:`36596`)
- Bug in :meth:`DataFrame.apply` and :meth:`DataFrame.agg` when passed the argument ``func="size"`` would operate on the entire ``DataFrame`` instead of rows or columns (:issue:`39934`)
- Bug in :meth:`DataFrame.transform` would raise a ``SpecificationError`` when passed a dictionary and columns were missing; will now raise a ``KeyError`` instead (:issue:`40004`)
- Bug in :meth:`.GroupBy.rank` giving incorrect results with ``pct=True`` and equal values between consecutive groups (:issue:`40518`)
- Bug in :meth:`Series.count` would result in an ``int32`` result on 32-bit platforms when argument ``level=None`` (:issue:`40908`)
- Bug in :class:`Series` and :class:`DataFrame` reductions with methods ``any`` and ``all`` not returning Boolean results for object data (:issue:`12863`, :issue:`35450`, :issue:`27709`)
- Bug in :meth:`Series.clip` would fail if the Series contains NA values and has nullable int or float as a data type (:issue:`40851`)
- Bug in :meth:`UInt64Index.where` and :meth:`UInt64Index.putmask` with an ``np.int64`` dtype ``other`` incorrectly raising ``TypeError`` (:issue:`41974`)
- Bug in :meth:`DataFrame.agg()` not sorting the aggregated axis in the order of the provided aggregation functions when one or more aggregation function fails to produce results (:issue:`33634`)
- Bug in :meth:`DataFrame.clip` not interpreting missing values as no threshold (:issue:`40420`)

Conversion
^^^^^^^^^^
- Bug in :meth:`Series.to_dict` with ``orient='records'`` now returns Python native types (:issue:`25969`)
- Bug in :meth:`Series.view` and :meth:`Index.view` when converting between datetime-like (``datetime64[ns]``, ``datetime64[ns, tz]``, ``timedelta64``, ``period``) dtypes (:issue:`39788`)
- Bug in creating a :class:`DataFrame` from an empty ``np.recarray`` not retaining the original dtypes (:issue:`40121`)
- Bug in :class:`DataFrame` failing to raise a ``TypeError`` when constructing from a ``frozenset`` (:issue:`40163`)
- Bug in :class:`Index` construction silently ignoring a passed ``dtype`` when the data cannot be cast to that dtype (:issue:`21311`)
- Bug in :meth:`StringArray.astype` falling back to NumPy and raising when converting to ``dtype='categorical'`` (:issue:`40450`)
- Bug in :func:`factorize` where, when given an array with a numeric NumPy dtype lower than int64, uint64 and float64, the unique values did not keep their original dtype (:issue:`41132`)
- Bug in :class:`DataFrame` construction with a dictionary containing an array-like with ``ExtensionDtype`` and ``copy=True`` failing to make a copy (:issue:`38939`)
- Bug in :meth:`qcut` raising error when taking ``Float64DType`` as input (:issue:`40730`)
- Bug in :class:`DataFrame` and :class:`Series` construction with ``datetime64[ns]`` data and ``dtype=object`` resulting in ``datetime`` objects instead of :class:`Timestamp` objects (:issue:`41599`)
- Bug in :class:`DataFrame` and :class:`Series` construction with ``timedelta64[ns]`` data and ``dtype=object`` resulting in ``np.timedelta64`` objects instead of :class:`Timedelta` objects (:issue:`41599`)
- Bug in :class:`DataFrame` construction when given a two-dimensional object-dtype ``np.ndarray`` of :class:`Period` or :class:`Interval` objects failing to cast to :class:`PeriodDtype` or :class:`IntervalDtype`, respectively (:issue:`41812`)
- Bug in constructing a :class:`Series` from a list and a :class:`PandasDtype` (:issue:`39357`)
- Bug in creating a :class:`Series` from a ``range`` object that does not fit in the bounds of ``int64`` dtype (:issue:`30173`)
- Bug in creating a :class:`Series` from a ``dict`` with all-tuple keys and an :class:`Index` that requires reindexing (:issue:`41707`)
- Bug in :func:`.infer_dtype` not recognizing Series, Index, or array with a Period dtype (:issue:`23553`)
- Bug in :func:`.infer_dtype` raising an error for general :class:`.ExtensionArray` objects. It will now return ``"unknown-array"`` instead of raising (:issue:`37367`)
- Bug in :meth:`DataFrame.convert_dtypes` incorrectly raised a ``ValueError`` when called on an empty DataFrame (:issue:`40393`)

Strings
^^^^^^^
- Bug in the conversion from ``pyarrow.ChunkedArray`` to :class:`~arrays.StringArray` when the original had zero chunks (:issue:`41040`)
- Bug in :meth:`Series.replace` and :meth:`DataFrame.replace` ignoring replacements with ``regex=True`` for ``StringDType`` data (:issue:`41333`, :issue:`35977`)
- Bug in :meth:`Series.str.extract` with :class:`~arrays.StringArray` returning object dtype for an empty :class:`DataFrame` (:issue:`41441`)
- Bug in :meth:`Series.str.replace` where the ``case`` argument was ignored when ``regex=False`` (:issue:`41602`)

Interval
^^^^^^^^
- Bug in :meth:`IntervalIndex.intersection` and :meth:`IntervalIndex.symmetric_difference` always returning object-dtype when operating with :class:`CategoricalIndex` (:issue:`38653`, :issue:`38741`)
- Bug in :meth:`IntervalIndex.intersection` returning duplicates when at least one of the :class:`Index` objects have duplicates which are present in the other (:issue:`38743`)
- :meth:`IntervalIndex.union`, :meth:`IntervalIndex.intersection`, :meth:`IntervalIndex.difference`, and :meth:`IntervalIndex.symmetric_difference` now cast to the appropriate dtype instead of raising a ``TypeError`` when operating with another :class:`IntervalIndex` with incompatible dtype (:issue:`39267`)
- :meth:`PeriodIndex.union`, :meth:`PeriodIndex.intersection`, :meth:`PeriodIndex.symmetric_difference`, :meth:`PeriodIndex.difference` now cast to object dtype instead of raising ``IncompatibleFrequency`` when operating with another :class:`PeriodIndex` with incompatible dtype (:issue:`39306`)
- Bug in :meth:`IntervalIndex.is_monotonic`, :meth:`IntervalIndex.get_loc`, :meth:`IntervalIndex.get_indexer_for`, and :meth:`IntervalIndex.__contains__` when NA values are present (:issue:`41831`)

Indexing
^^^^^^^^
- Bug in :meth:`Index.union` and :meth:`MultiIndex.union` dropping duplicate ``Index`` values when ``Index`` was not monotonic or ``sort`` was set to ``False`` (:issue:`36289`, :issue:`31326`, :issue:`40862`)
- Bug in :meth:`CategoricalIndex.get_indexer` failing to raise ``InvalidIndexError`` when non-unique (:issue:`38372`)
- Bug in :meth:`IntervalIndex.get_indexer` when ``target`` has ``CategoricalDtype`` and both the index and the target contain NA values (:issue:`41934`)
- Bug in :meth:`Series.loc` raising a ``ValueError`` when input was filtered with a Boolean list and values to set were a list with lower dimension (:issue:`20438`)
- Bug in inserting many new columns into a :class:`DataFrame` causing incorrect subsequent indexing behavior (:issue:`38380`)
- Bug in :meth:`DataFrame.__setitem__` raising a ``ValueError`` when setting multiple values to duplicate columns (:issue:`15695`)
- Bug in :meth:`DataFrame.loc`, :meth:`Series.loc`, :meth:`DataFrame.__getitem__` and :meth:`Series.__getitem__` returning incorrect elements for non-monotonic :class:`DatetimeIndex` for string slices (:issue:`33146`)
- Bug in :meth:`DataFrame.reindex` and :meth:`Series.reindex` with timezone aware indexes raising a ``TypeError`` for ``method="ffill"`` and ``method="bfill"`` and specified ``tolerance`` (:issue:`38566`)
- Bug in :meth:`DataFrame.reindex` with ``datetime64[ns]`` or ``timedelta64[ns]`` incorrectly casting to integers when the ``fill_value`` requires casting to object dtype (:issue:`39755`)
- Bug in :meth:`DataFrame.__setitem__` raising a ``ValueError`` when setting on an empty :class:`DataFrame` using specified columns and a nonempty :class:`DataFrame` value (:issue:`38831`)
- Bug in :meth:`DataFrame.loc.__setitem__` raising a ``ValueError`` when operating on a unique column when the :class:`DataFrame` has duplicate columns (:issue:`38521`)
- Bug in :meth:`DataFrame.iloc.__setitem__` and :meth:`DataFrame.loc.__setitem__` with mixed dtypes when setting with a dictionary value (:issue:`38335`)
- Bug in :meth:`Series.loc.__setitem__` and :meth:`DataFrame.loc.__setitem__` raising ``KeyError`` when provided a Boolean generator (:issue:`39614`)
- Bug in :meth:`Series.iloc` and :meth:`DataFrame.iloc` raising a ``KeyError`` when provided a generator (:issue:`39614`)
- Bug in :meth:`DataFrame.__setitem__` not raising a ``ValueError`` when the right hand side is a :class:`DataFrame` with wrong number of columns (:issue:`38604`)
- Bug in :meth:`Series.__setitem__` raising a ``ValueError`` when setting a :class:`Series` with a scalar indexer (:issue:`38303`)
- Bug in :meth:`DataFrame.loc` dropping levels of a :class:`MultiIndex` when the :class:`DataFrame` used as input has only one row (:issue:`10521`)
- Bug in :meth:`DataFrame.__getitem__` and :meth:`Series.__getitem__` always raising ``KeyError`` when slicing with existing strings where the :class:`Index` has milliseconds (:issue:`33589`)
- Bug in setting ``timedelta64`` or ``datetime64`` values into numeric :class:`Series` failing to cast to object dtype (:issue:`39086`, :issue:`39619`)
- Bug in setting :class:`Interval` values into a :class:`Series` or :class:`DataFrame` with mismatched :class:`IntervalDtype` incorrectly casting the new values to the existing dtype (:issue:`39120`)
- Bug in setting ``datetime64`` values into a :class:`Series` with integer-dtype incorrectly casting the datetime64 values to integers (:issue:`39266`)
- Bug in setting ``np.datetime64("NaT")`` into a :class:`Series` with :class:`Datetime64TZDtype` incorrectly treating the timezone-naive value as timezone-aware (:issue:`39769`)
- Bug in :meth:`Index.get_loc` not raising ``KeyError`` when ``key=NaN`` and ``method`` is specified but ``NaN`` is not in the :class:`Index` (:issue:`39382`)
- Bug in :meth:`DatetimeIndex.insert` when inserting ``np.datetime64("NaT")`` into a timezone-aware index incorrectly treating the timezone-naive value as timezone-aware (:issue:`39769`)
- Bug in incorrectly raising in :meth:`Index.insert`, when setting a new column that cannot be held in the existing ``frame.columns``, or in :meth:`Series.reset_index` or :meth:`DataFrame.reset_index` instead of casting to a compatible dtype (:issue:`39068`)
- Bug in :meth:`RangeIndex.append` where a single object of length 1 was concatenated incorrectly (:issue:`39401`)
- Bug in :meth:`RangeIndex.astype` where when converting to :class:`CategoricalIndex`, the categories became a :class:`Int64Index` instead of a :class:`RangeIndex` (:issue:`41263`)
- Bug in setting ``numpy.timedelta64`` values into an object-dtype :class:`Series` using a Boolean indexer (:issue:`39488`)
- Bug in setting numeric values into a into a boolean-dtypes :class:`Series` using ``at`` or ``iat`` failing to cast to object-dtype (:issue:`39582`)
- Bug in :meth:`DataFrame.__setitem__` and :meth:`DataFrame.iloc.__setitem__` raising ``ValueError`` when trying to index with a row-slice and setting a list as values (:issue:`40440`)
- Bug in :meth:`DataFrame.loc` not raising ``KeyError`` when the key was not found in :class:`MultiIndex` and the levels were not fully specified (:issue:`41170`)
- Bug in :meth:`DataFrame.loc.__setitem__` when setting-with-expansion incorrectly raising when the index in the expanding axis contained duplicates (:issue:`40096`)
- Bug in :meth:`DataFrame.loc.__getitem__` with :class:`MultiIndex` casting to float when at least one index column has float dtype and we retrieve a scalar (:issue:`41369`)
- Bug in :meth:`DataFrame.loc` incorrectly matching non-Boolean index elements (:issue:`20432`)
- Bug in indexing with ``np.nan`` on a :class:`Series` or :class:`DataFrame` with a :class:`CategoricalIndex` incorrectly raising ``KeyError`` when ``np.nan`` keys are present (:issue:`41933`)
- Bug in :meth:`Series.__delitem__` with ``ExtensionDtype`` incorrectly casting to ``ndarray`` (:issue:`40386`)
- Bug in :meth:`DataFrame.at` with a :class:`CategoricalIndex` returning incorrect results when passed integer keys (:issue:`41846`)
- Bug in :meth:`DataFrame.loc` returning a :class:`MultiIndex` in the wrong order if an indexer has duplicates (:issue:`40978`)
- Bug in :meth:`DataFrame.__setitem__` raising a ``TypeError`` when using a ``str`` subclass as the column name with a :class:`DatetimeIndex` (:issue:`37366`)
- Bug in :meth:`PeriodIndex.get_loc` failing to raise a ``KeyError`` when given a :class:`Period` with a mismatched ``freq`` (:issue:`41670`)
- Bug ``.loc.__getitem__`` with a :class:`UInt64Index` and negative-integer keys raising ``OverflowError`` instead of ``KeyError`` in some cases, wrapping around to positive integers in others (:issue:`41777`)
- Bug in :meth:`Index.get_indexer` failing to raise ``ValueError`` in some cases with invalid ``method``, ``limit``, or ``tolerance`` arguments (:issue:`41918`)
- Bug when slicing a :class:`Series` or :class:`DataFrame` with a :class:`TimedeltaIndex` when passing an invalid string raising ``ValueError`` instead of a ``TypeError`` (:issue:`41821`)
- Bug in :class:`Index` constructor sometimes silently ignoring a specified ``dtype`` (:issue:`38879`)
- :meth:`Index.where` behavior now mirrors :meth:`Index.putmask` behavior, i.e. ``index.where(mask, other)`` matches ``index.putmask(~mask, other)`` (:issue:`39412`)

Missing
^^^^^^^
- Bug in :class:`Grouper` did not correctly propagate the ``dropna`` argument; :meth:`.DataFrameGroupBy.transform` now correctly handles missing values for ``dropna=True`` (:issue:`35612`)
- Bug in :func:`isna`, :meth:`Series.isna`, :meth:`Index.isna`, :meth:`DataFrame.isna`, and the corresponding ``notna`` functions not recognizing ``Decimal("NaN")`` objects (:issue:`39409`)
- Bug in :meth:`DataFrame.fillna` not accepting a dictionary for the ``downcast`` keyword (:issue:`40809`)
- Bug in :func:`isna` not returning a copy of the mask for nullable types, causing any subsequent mask modification to change the original array (:issue:`40935`)
- Bug in :class:`DataFrame` construction with float data containing ``NaN`` and an integer ``dtype`` casting instead of retaining the ``NaN`` (:issue:`26919`)
- Bug in :meth:`Series.isin` and :meth:`MultiIndex.isin` didn't treat all nans as equivalent if they were in tuples (:issue:`41836`)

MultiIndex
^^^^^^^^^^
- Bug in :meth:`DataFrame.drop` raising a ``TypeError`` when the :class:`MultiIndex` is non-unique and ``level`` is not provided (:issue:`36293`)
- Bug in :meth:`MultiIndex.intersection` duplicating ``NaN`` in the result (:issue:`38623`)
- Bug in :meth:`MultiIndex.equals` incorrectly returning ``True`` when the :class:`MultiIndex` contained ``NaN`` even when they are differently ordered (:issue:`38439`)
- Bug in :meth:`MultiIndex.intersection` always returning an empty result when intersecting with :class:`CategoricalIndex` (:issue:`38653`)
- Bug in :meth:`MultiIndex.difference` incorrectly raising ``TypeError`` when indexes contain non-sortable entries (:issue:`41915`)
- Bug in :meth:`MultiIndex.reindex` raising a ``ValueError`` when used on an empty :class:`MultiIndex` and indexing only a specific level (:issue:`41170`)
- Bug in :meth:`MultiIndex.reindex` raising ``TypeError`` when reindexing against a flat :class:`Index` (:issue:`41707`)

I/O
^^^
- Bug in :meth:`Index.__repr__` when ``display.max_seq_items=1`` (:issue:`38415`)
- Bug in :func:`read_csv` not recognizing scientific notation if the argument ``decimal`` is set and ``engine="python"`` (:issue:`31920`)
- Bug in :func:`read_csv` interpreting ``NA`` value as comment, when ``NA`` does contain the comment string fixed for ``engine="python"`` (:issue:`34002`)
- Bug in :func:`read_csv` raising an ``IndexError`` with multiple header columns and ``index_col`` is specified when the file has no data rows (:issue:`38292`)
- Bug in :func:`read_csv` not accepting ``usecols`` with a different length than ``names`` for ``engine="python"`` (:issue:`16469`)
- Bug in :meth:`read_csv` returning object dtype when ``delimiter=","`` with ``usecols`` and ``parse_dates`` specified for ``engine="python"`` (:issue:`35873`)
- Bug in :func:`read_csv` raising a ``TypeError`` when ``names`` and ``parse_dates`` is specified for ``engine="c"`` (:issue:`33699`)
- Bug in :func:`read_clipboard` and :func:`DataFrame.to_clipboard` not working in WSL (:issue:`38527`)
- Allow custom error values for the ``parse_dates`` argument of :func:`read_sql`, :func:`read_sql_query` and :func:`read_sql_table` (:issue:`35185`)
- Bug in :meth:`DataFrame.to_hdf` and :meth:`Series.to_hdf` raising a ``KeyError`` when trying to apply for subclasses of ``DataFrame`` or ``Series`` (:issue:`33748`)
- Bug in :meth:`.HDFStore.put` raising a wrong ``TypeError`` when saving a DataFrame with non-string dtype (:issue:`34274`)
- Bug in :func:`json_normalize` resulting in the first element of a generator object not being included in the returned DataFrame (:issue:`35923`)
- Bug in :func:`read_csv` applying the thousands separator to date columns when the column should be parsed for dates and ``usecols`` is specified for ``engine="python"`` (:issue:`39365`)
- Bug in :func:`read_excel` forward filling :class:`MultiIndex` names when multiple header and index columns are specified (:issue:`34673`)
- Bug in :func:`read_excel` not respecting :func:`set_option` (:issue:`34252`)
- Bug in :func:`read_csv` not switching ``true_values`` and ``false_values`` for nullable Boolean dtype (:issue:`34655`)
- Bug in :func:`read_json` when ``orient="split"`` not maintaining a numeric string index (:issue:`28556`)
- :meth:`read_sql` returned an empty generator if ``chunksize`` was non-zero and the query returned no results. Now returns a generator with a single empty DataFrame (:issue:`34411`)
- Bug in :func:`read_hdf` returning unexpected records when filtering on categorical string columns using the ``where`` parameter (:issue:`39189`)
- Bug in :func:`read_sas` raising a ``ValueError`` when ``datetimes`` were null (:issue:`39725`)
- Bug in :func:`read_excel` dropping empty values from single-column spreadsheets (:issue:`39808`)
- Bug in :func:`read_excel` loading trailing empty rows/columns for some filetypes (:issue:`41167`)
- Bug in :func:`read_excel` raising an ``AttributeError`` when the excel file had a ``MultiIndex`` header followed by two empty rows and no index (:issue:`40442`)
- Bug in :func:`read_excel`, :func:`read_csv`, :func:`read_table`, :func:`read_fwf`, and :func:`read_clipboard` where one blank row after a ``MultiIndex`` header with no index would be dropped (:issue:`40442`)
- Bug in :meth:`DataFrame.to_string` misplacing the truncation column when ``index=False`` (:issue:`40904`)
- Bug in :meth:`DataFrame.to_string` adding an extra dot and misaligning the truncation row when ``index=False`` (:issue:`40904`)
- Bug in :func:`read_orc` always raising an ``AttributeError`` (:issue:`40918`)
- Bug in :func:`read_csv` and :func:`read_table` silently ignoring ``prefix`` if ``names`` and ``prefix`` are defined, now raising a ``ValueError`` (:issue:`39123`)
- Bug in :func:`read_csv` and :func:`read_excel` not respecting the dtype for a duplicated column name when ``mangle_dupe_cols`` is set to ``True`` (:issue:`35211`)
- Bug in :func:`read_csv` silently ignoring ``sep`` if ``delimiter`` and ``sep`` are defined, now raising a ``ValueError`` (:issue:`39823`)
- Bug in :func:`read_csv` and :func:`read_table` misinterpreting arguments when ``sys.setprofile`` had been previously called (:issue:`41069`)
- Bug in the conversion from PyArrow to pandas (e.g. for reading Parquet) with nullable dtypes and a PyArrow array whose data buffer size is not a multiple of the dtype size (:issue:`40896`)
- Bug in :func:`read_excel` would raise an error when pandas could not determine the file type even though the user specified the ``engine`` argument (:issue:`41225`)
- Bug in :func:`read_clipboard` copying from an excel file shifts values into the wrong column if there are null values in first column (:issue:`41108`)
- Bug in :meth:`DataFrame.to_hdf` and :meth:`Series.to_hdf` raising a ``TypeError`` when trying to append a string column to an incompatible column (:issue:`41897`)

Period
^^^^^^
- Comparisons of :class:`Period` objects or :class:`Index`, :class:`Series`, or :class:`DataFrame` with mismatched ``PeriodDtype`` now behave like other mismatched-type comparisons, returning ``False`` for equals, ``True`` for not-equal, and raising ``TypeError`` for inequality checks (:issue:`39274`)

Plotting
^^^^^^^^
- Bug in :func:`plotting.scatter_matrix` raising when 2d ``ax`` argument passed (:issue:`16253`)
- Prevent warnings when Matplotlib's ``constrained_layout`` is enabled (:issue:`25261`)
- Bug in :func:`DataFrame.plot` was showing the wrong colors in the legend if the function was called repeatedly and some calls used ``yerr`` while others didn't (:issue:`39522`)
- Bug in :func:`DataFrame.plot` was showing the wrong colors in the legend if the function was called repeatedly and some calls used ``secondary_y`` and others use ``legend=False`` (:issue:`40044`)
- Bug in :meth:`DataFrame.plot.box` when ``dark_background`` theme was selected, caps or min/max markers for the plot were not visible (:issue:`40769`)

Groupby/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^
- Bug in :meth:`.GroupBy.agg` with :class:`PeriodDtype` columns incorrectly casting results too aggressively (:issue:`38254`)
- Bug in :meth:`.SeriesGroupBy.value_counts` where unobserved categories in a grouped categorical Series were not tallied (:issue:`38672`)
- Bug in :meth:`.SeriesGroupBy.value_counts` where an error was raised on an empty Series (:issue:`39172`)
- Bug in :meth:`.GroupBy.indices` would contain non-existent indices when null values were present in the groupby keys (:issue:`9304`)
- Fixed bug in :meth:`.GroupBy.sum` causing a loss of precision by now using Kahan summation (:issue:`38778`)
- Fixed bug in :meth:`.GroupBy.cumsum` and :meth:`.GroupBy.mean` causing loss of precision through using Kahan summation (:issue:`38934`)
- Bug in :meth:`.Resampler.aggregate` and :meth:`DataFrame.transform` raising a ``TypeError`` instead of ``SpecificationError`` when missing keys had mixed dtypes (:issue:`39025`)
- Bug in :meth:`.DataFrameGroupBy.idxmin` and :meth:`.DataFrameGroupBy.idxmax` with ``ExtensionDtype`` columns (:issue:`38733`)
- Bug in :meth:`Series.resample` would raise when the index was a :class:`PeriodIndex` consisting of ``NaT`` (:issue:`39227`)
- Bug in :meth:`.RollingGroupby.corr` and :meth:`.ExpandingGroupby.corr` where the groupby column would return ``0`` instead of ``np.nan`` when providing ``other`` that was longer than each group (:issue:`39591`)
- Bug in :meth:`.ExpandingGroupby.corr` and :meth:`.ExpandingGroupby.cov` where ``1`` would be returned instead of ``np.nan`` when providing ``other`` that was longer than each group (:issue:`39591`)
- Bug in :meth:`.GroupBy.mean`, :meth:`.GroupBy.median` and :meth:`DataFrame.pivot_table` not propagating metadata (:issue:`28283`)
- Bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` not calculating window bounds correctly when window is an offset and dates are in descending order (:issue:`40002`)
- Bug in :meth:`Series.groupby` and :meth:`DataFrame.groupby` on an empty ``Series`` or ``DataFrame`` would lose index, columns, and/or data types when directly using the methods ``idxmax``, ``idxmin``, ``mad``, ``min``, ``max``, ``sum``, ``prod``, and ``skew`` or using them through ``apply``, ``aggregate``, or ``resample`` (:issue:`26411`)
- Bug in :meth:`.GroupBy.apply` where a :class:`MultiIndex` would be created instead of an :class:`Index` when used on a :class:`.RollingGroupby` object (:issue:`39732`)
- Bug in :meth:`.DataFrameGroupBy.sample` where an error was raised when ``weights`` was specified and the index was an :class:`Int64Index` (:issue:`39927`)
- Bug in :meth:`.DataFrameGroupBy.aggregate` and :meth:`.Resampler.aggregate` would sometimes raise a ``SpecificationError`` when passed a dictionary and columns were missing; will now always raise a ``KeyError`` instead (:issue:`40004`)
- Bug in :meth:`.DataFrameGroupBy.sample` where column selection was not applied before computing the result (:issue:`39928`)
- Bug in :class:`.ExponentialMovingWindow` when calling ``__getitem__`` would incorrectly raise a ``ValueError`` when providing ``times`` (:issue:`40164`)
- Bug in :class:`.ExponentialMovingWindow` when calling ``__getitem__`` would not retain ``com``, ``span``, ``alpha`` or ``halflife`` attributes  (:issue:`40164`)
- :class:`.ExponentialMovingWindow` now raises a ``NotImplementedError`` when specifying ``times`` with ``adjust=False`` due to an incorrect calculation (:issue:`40098`)
- Bug in :meth:`.ExponentialMovingWindowGroupby.mean` where the ``times`` argument was ignored when ``engine='numba'`` (:issue:`40951`)
- Bug in :meth:`.ExponentialMovingWindowGroupby.mean` where the wrong times were used the in case of multiple groups (:issue:`40951`)
- Bug in :class:`.ExponentialMovingWindowGroupby` where the times vector and values became out of sync for non-trivial groups (:issue:`40951`)
- Bug in :meth:`Series.asfreq` and :meth:`DataFrame.asfreq` dropping rows when the index was not sorted (:issue:`39805`)
- Bug in aggregation functions for :class:`DataFrame` not respecting ``numeric_only`` argument when ``level`` keyword was given (:issue:`40660`)
- Bug in :meth:`.SeriesGroupBy.aggregate` where using a user-defined function to aggregate a Series with an object-typed :class:`Index` causes an incorrect :class:`Index` shape (:issue:`40014`)
- Bug in :class:`.RollingGroupby` where ``as_index=False`` argument in ``groupby`` was ignored (:issue:`39433`)
- Bug in :meth:`.GroupBy.any` and :meth:`.GroupBy.all` raising a ``ValueError`` when using with nullable type columns holding ``NA`` even with ``skipna=True`` (:issue:`40585`)
- Bug in :meth:`.GroupBy.cummin` and :meth:`.GroupBy.cummax` incorrectly rounding integer values near the ``int64`` implementations bounds (:issue:`40767`)
- Bug in :meth:`.GroupBy.rank` with nullable dtypes incorrectly raising a ``TypeError`` (:issue:`41010`)
- Bug in :meth:`.GroupBy.cummin` and :meth:`.GroupBy.cummax` computing wrong result with nullable data types too large to roundtrip when casting to float (:issue:`37493`)
- Bug in :meth:`DataFrame.rolling` returning mean zero for all ``NaN`` window with ``min_periods=0`` if calculation is not numerical stable (:issue:`41053`)
- Bug in :meth:`DataFrame.rolling` returning sum not zero for all ``NaN`` window with ``min_periods=0`` if calculation is not numerical stable (:issue:`41053`)
- Bug in :meth:`.SeriesGroupBy.agg` failing to retain ordered :class:`CategoricalDtype` on order-preserving aggregations (:issue:`41147`)
- Bug in :meth:`.GroupBy.min` and :meth:`.GroupBy.max` with multiple object-dtype columns and ``numeric_only=False`` incorrectly raising a ``ValueError`` (:issue:`41111`)
- Bug in :meth:`.DataFrameGroupBy.rank` with the GroupBy object's ``axis=0`` and the ``rank`` method's keyword ``axis=1`` (:issue:`41320`)
- Bug in :meth:`DataFrameGroupBy.__getitem__` with non-unique columns incorrectly returning a malformed :class:`SeriesGroupBy` instead of :class:`DataFrameGroupBy` (:issue:`41427`)
- Bug in :meth:`.DataFrameGroupBy.transform` with non-unique columns incorrectly raising an ``AttributeError`` (:issue:`41427`)
- Bug in :meth:`.Resampler.apply` with non-unique columns incorrectly dropping duplicated columns (:issue:`41445`)
- Bug in :meth:`Series.groupby` aggregations incorrectly returning empty :class:`Series` instead of raising ``TypeError`` on aggregations that are invalid for its dtype, e.g. ``.prod`` with ``datetime64[ns]`` dtype (:issue:`41342`)
- Bug in :class:`DataFrameGroupBy` aggregations incorrectly failing to drop columns with invalid dtypes for that aggregation when there are no valid columns (:issue:`41291`)
- Bug in :meth:`DataFrame.rolling.__iter__` where ``on`` was not assigned to the index of the resulting objects (:issue:`40373`)
- Bug in :meth:`.DataFrameGroupBy.transform` and :meth:`.DataFrameGroupBy.agg` with ``engine="numba"`` where ``*args`` were being cached with the user passed function (:issue:`41647`)
- Bug in :class:`DataFrameGroupBy` methods ``agg``, ``transform``, ``sum``, ``bfill``, ``ffill``, ``pad``, ``pct_change``, ``shift``, ``ohlc`` dropping ``.columns.names`` (:issue:`41497`)


Reshaping
^^^^^^^^^
- Bug in :func:`merge` raising error when performing an inner join with partial index and ``right_index=True`` when there was no overlap between indices (:issue:`33814`)
- Bug in :meth:`DataFrame.unstack` with missing levels led to incorrect index names (:issue:`37510`)
- Bug in :func:`merge_asof` propagating the right Index with ``left_index=True`` and ``right_on`` specification instead of left Index (:issue:`33463`)
- Bug in :meth:`DataFrame.join` on a DataFrame with a :class:`MultiIndex` returned the wrong result when one of both indexes had only one level (:issue:`36909`)
- :func:`merge_asof` now raises a ``ValueError`` instead of a cryptic ``TypeError`` in case of non-numerical merge columns (:issue:`29130`)
- Bug in :meth:`DataFrame.join` not assigning values correctly when the DataFrame had a :class:`MultiIndex` where at least one dimension had dtype ``Categorical`` with non-alphabetically sorted categories (:issue:`38502`)
- :meth:`Series.value_counts` and :meth:`Series.mode` now return consistent keys in original order (:issue:`12679`, :issue:`11227` and :issue:`39007`)
- Bug in :meth:`DataFrame.stack` not handling ``NaN`` in :class:`MultiIndex` columns correctly (:issue:`39481`)
- Bug in :meth:`DataFrame.apply` would give incorrect results when the argument ``func`` was a string, ``axis=1``, and the axis argument was not supported; now raises a ``ValueError`` instead (:issue:`39211`)
- Bug in :meth:`DataFrame.sort_values` not reshaping the index correctly after sorting on columns when ``ignore_index=True`` (:issue:`39464`)
- Bug in :meth:`DataFrame.append` returning incorrect dtypes with combinations of ``ExtensionDtype`` dtypes (:issue:`39454`)
- Bug in :meth:`DataFrame.append` returning incorrect dtypes when used with combinations of ``datetime64`` and ``timedelta64`` dtypes (:issue:`39574`)
- Bug in :meth:`DataFrame.append` with a :class:`DataFrame` with a :class:`MultiIndex` and appending a :class:`Series` whose :class:`Index` is not a :class:`MultiIndex` (:issue:`41707`)
- Bug in :meth:`DataFrame.pivot_table` returning a :class:`MultiIndex` for a single value when operating on an empty DataFrame (:issue:`13483`)
- :class:`Index` can now be passed to the :func:`numpy.all` function (:issue:`40180`)
- Bug in :meth:`DataFrame.stack` not preserving ``CategoricalDtype`` in a :class:`MultiIndex` (:issue:`36991`)
- Bug in :func:`to_datetime` raising an error when the input sequence contained unhashable items (:issue:`39756`)
- Bug in :meth:`Series.explode` preserving the index when ``ignore_index`` was ``True`` and values were scalars (:issue:`40487`)
- Bug in :func:`to_datetime` raising a ``ValueError`` when :class:`Series` contains ``None`` and ``NaT`` and has more than 50 elements (:issue:`39882`)
- Bug in :meth:`Series.unstack` and :meth:`DataFrame.unstack` with object-dtype values containing timezone-aware datetime objects incorrectly raising ``TypeError`` (:issue:`41875`)
- Bug in :meth:`DataFrame.melt` raising ``InvalidIndexError`` when :class:`DataFrame` has duplicate columns used as ``value_vars`` (:issue:`41951`)

Sparse
^^^^^^
- Bug in :meth:`DataFrame.sparse.to_coo` raising a ``KeyError`` with columns that are a numeric :class:`Index` without a ``0`` (:issue:`18414`)
- Bug in :meth:`SparseArray.astype` with ``copy=False`` producing incorrect results when going from integer dtype to floating dtype (:issue:`34456`)
- Bug in :meth:`SparseArray.max` and :meth:`SparseArray.min` would always return an empty result (:issue:`40921`)

ExtensionArray
^^^^^^^^^^^^^^
- Bug in :meth:`DataFrame.where` when ``other`` is a Series with an :class:`ExtensionDtype` (:issue:`38729`)
- Fixed bug where :meth:`Series.idxmax`, :meth:`Series.idxmin`, :meth:`Series.argmax`, and :meth:`Series.argmin` would fail when the underlying data is an :class:`ExtensionArray` (:issue:`32749`, :issue:`33719`, :issue:`36566`)
- Fixed bug where some properties of subclasses of :class:`PandasExtensionDtype` where improperly cached (:issue:`40329`)
- Bug in :meth:`DataFrame.mask` where masking a DataFrame with an :class:`ExtensionDtype` raises a ``ValueError`` (:issue:`40941`)

Styler
^^^^^^
- Bug in :class:`.Styler` where the ``subset`` argument in methods raised an error for some valid MultiIndex slices (:issue:`33562`)
- :class:`.Styler` rendered HTML output has seen minor alterations to support w3 good code standards (:issue:`39626`)
- Bug in :class:`.Styler` where rendered HTML was missing a column class identifier for certain header cells (:issue:`39716`)
- Bug in :meth:`.Styler.background_gradient` where text-color was not determined correctly (:issue:`39888`)
- Bug in :meth:`.Styler.set_table_styles` where multiple elements in CSS-selectors of the ``table_styles`` argument were not correctly added (:issue:`34061`)
- Bug in :class:`.Styler` where copying from Jupyter dropped the top left cell and misaligned headers (:issue:`12147`)
- Bug in :class:`Styler.where` where ``kwargs`` were not passed to the applicable callable (:issue:`40845`)
- Bug in :class:`.Styler` causing CSS to duplicate on multiple renders (:issue:`39395`, :issue:`40334`)

Other
^^^^^
- ``inspect.getmembers(Series)`` no longer raises an ``AbstractMethodError`` (:issue:`38782`)
- Bug in :meth:`Series.where` with numeric dtype and ``other=None`` not casting to ``nan`` (:issue:`39761`)
- Bug in :func:`.assert_series_equal`, :func:`.assert_frame_equal`, :func:`.assert_index_equal` and :func:`.assert_extension_array_equal` incorrectly raising when an attribute has an unrecognized NA type (:issue:`39461`)
- Bug in :func:`.assert_index_equal` with ``exact=True`` not raising when comparing :class:`CategoricalIndex` instances with ``Int64Index`` and ``RangeIndex`` categories (:issue:`41263`)
- Bug in :meth:`DataFrame.equals`, :meth:`Series.equals`, and :meth:`Index.equals` with object-dtype containing ``np.datetime64("NaT")`` or ``np.timedelta64("NaT")`` (:issue:`39650`)
- Bug in :func:`show_versions` where console JSON output was not proper JSON (:issue:`39701`)
- pandas can now compile on z/OS when using `xlc <https://www.ibm.com/products/xl-cpp-compiler-zos>`_ (:issue:`35826`)
- Bug in :func:`pandas.util.hash_pandas_object` not recognizing ``hash_key``, ``encoding`` and ``categorize`` when the input object type is a :class:`DataFrame` (:issue:`41404`)

.. ---------------------------------------------------------------------------

.. _whatsnew_130.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.5..v1.3.0
.. _whatsnew_0170:

Version 0.17.0 (October 9, 2015)
--------------------------------

{{ header }}


This is a major release from 0.16.2 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

.. warning::

   pandas >= 0.17.0 will no longer support compatibility with Python version 3.2 (:issue:`9118`)

.. warning::

   The ``pandas.io.data`` package is deprecated and will be replaced by the
   `pandas-datareader package <https://github.com/pydata/pandas-datareader>`_.
   This will allow the data modules to be independently updated to your pandas
   installation. The API for ``pandas-datareader v0.1.1`` is exactly the same
   as in ``pandas v0.17.0`` (:issue:`8961`, :issue:`10861`).

   After installing pandas-datareader, you can easily change your imports:

   .. code-block:: python

     from pandas.io import data, wb

   becomes

   .. code-block:: python

     from pandas_datareader import data, wb

Highlights include:

- Release the Global Interpreter Lock (GIL) on some cython operations, see :ref:`here <whatsnew_0170.gil>`
- Plotting methods are now available as attributes of the ``.plot`` accessor, see :ref:`here <whatsnew_0170.plot>`
- The sorting API has been revamped to remove some long-time inconsistencies, see :ref:`here <whatsnew_0170.api_breaking.sorting>`
- Support for a ``datetime64[ns]`` with timezones as a first-class dtype, see :ref:`here <whatsnew_0170.tz>`
- The default for ``to_datetime`` will now be to ``raise`` when presented with unparsable formats,
  previously this would return the original input. Also, date parse
  functions now return consistent results. See :ref:`here <whatsnew_0170.api_breaking.to_datetime>`
- The default for ``dropna`` in ``HDFStore`` has changed to ``False``, to store by default all rows even
  if they are all ``NaN``, see :ref:`here <whatsnew_0170.api_breaking.hdf_dropna>`
- Datetime accessor (``dt``) now supports ``Series.dt.strftime`` to generate formatted strings for datetime-likes, and ``Series.dt.total_seconds`` to generate each duration of the timedelta in seconds. See :ref:`here <whatsnew_0170.strftime>`
- ``Period`` and ``PeriodIndex`` can handle multiplied freq like ``3D``, which corresponding to 3 days span. See :ref:`here <whatsnew_0170.periodfreq>`
- Development installed versions of pandas will now have ``PEP440`` compliant version strings (:issue:`9518`)
- Development support for benchmarking with the `Air Speed Velocity library <https://github.com/spacetelescope/asv/>`_ (:issue:`8361`)
- Support for reading SAS xport files, see :ref:`here <whatsnew_0170.enhancements.sas_xport>`
- Documentation comparing SAS to *pandas*, see :ref:`here <compare_with_sas>`
- Removal of the automatic TimeSeries broadcasting, deprecated since 0.8.0, see :ref:`here <whatsnew_0170.prior_deprecations>`
- Display format with plain text can optionally align with Unicode East Asian Width, see :ref:`here <whatsnew_0170.east_asian_width>`
- Compatibility with Python 3.5 (:issue:`11097`)
- Compatibility with matplotlib 1.5.0 (:issue:`11111`)

Check the :ref:`API Changes <whatsnew_0170.api>` and :ref:`deprecations <whatsnew_0170.deprecations>` before updating.

.. contents:: What's new in v0.17.0
    :local:
    :backlinks: none

.. _whatsnew_0170.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0170.tz:

Datetime with TZ
^^^^^^^^^^^^^^^^

We are adding an implementation that natively supports datetime with timezones. A ``Series`` or a ``DataFrame`` column previously
*could* be assigned a datetime with timezones, and would work as an ``object`` dtype. This had performance issues with a large
number rows. See the :ref:`docs <timeseries.timezone_series>` for more details. (:issue:`8260`, :issue:`10763`, :issue:`11034`).

The new implementation allows for having a single-timezone across all rows, with operations in a performant manner.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": pd.date_range("20130101", periods=3),
           "B": pd.date_range("20130101", periods=3, tz="US/Eastern"),
           "C": pd.date_range("20130101", periods=3, tz="CET"),
       }
   )
   df
   df.dtypes

.. ipython:: python

   df.B
   df.B.dt.tz_localize(None)

This uses a new-dtype representation as well, that is very similar in look-and-feel to its numpy cousin ``datetime64[ns]``

.. ipython:: python

   df["B"].dtype
   type(df["B"].dtype)

.. note::

   There is a slightly different string repr for the underlying ``DatetimeIndex`` as a result of the dtype changes, but
   functionally these are the same.

   Previous behavior:

   .. code-block:: ipython

      In [1]: pd.date_range('20130101', periods=3, tz='US/Eastern')
      Out[1]: DatetimeIndex(['2013-01-01 00:00:00-05:00', '2013-01-02 00:00:00-05:00',
                             '2013-01-03 00:00:00-05:00'],
                            dtype='datetime64[ns]', freq='D', tz='US/Eastern')

      In [2]: pd.date_range('20130101', periods=3, tz='US/Eastern').dtype
      Out[2]: dtype('<M8[ns]')

   New behavior:

   .. ipython:: python

      pd.date_range("20130101", periods=3, tz="US/Eastern")
      pd.date_range("20130101", periods=3, tz="US/Eastern").dtype

.. _whatsnew_0170.gil:

Releasing the GIL
^^^^^^^^^^^^^^^^^

We are releasing the global-interpreter-lock (GIL) on some cython operations.
This will allow other threads to run simultaneously during computation, potentially allowing performance improvements
from multi-threading. Notably ``groupby``, ``nsmallest``, ``value_counts`` and some indexing operations benefit from this. (:issue:`8882`)

For example the groupby expression in the following code will have the GIL released during the factorization step, e.g. ``df.groupby('key')``
as well as the ``.sum()`` operation.

.. code-block:: python

   N = 1000000
   ngroups = 10
   df = DataFrame(
       {"key": np.random.randint(0, ngroups, size=N), "data": np.random.randn(N)}
   )
   df.groupby("key")["data"].sum()

Releasing of the GIL could benefit an application that uses threads for user interactions (e.g. QT_), or performing multi-threaded computations. A nice example of a library that can handle these types of computation-in-parallel is the dask_ library.

.. _dask: https://dask.readthedocs.io/en/latest/
.. _QT: https://wiki.python.org/moin/PyQt

.. _whatsnew_0170.plot:

Plot submethods
^^^^^^^^^^^^^^^

The Series and DataFrame ``.plot()`` method allows for customizing :ref:`plot types<visualization.other>` by supplying the ``kind`` keyword arguments. Unfortunately, many of these kinds of plots use different required and optional keyword arguments, which makes it difficult to discover what any given plot kind uses out of the dozens of possible arguments.

To alleviate this issue, we have added a new, optional plotting interface, which exposes each kind of plot as a method of the ``.plot`` attribute. Instead of writing ``series.plot(kind=<kind>, ...)``, you can now also use ``series.plot.<kind>(...)``:

.. ipython::
    :verbatim:

    In [13]: df = pd.DataFrame(np.random.rand(10, 2), columns=['a', 'b'])

    In [14]: df.plot.bar()

.. image:: ../_static/whatsnew_plot_submethods.png

As a result of this change, these methods are now all discoverable via tab-completion:

.. ipython::
    :verbatim:

    In [15]: df.plot.<TAB>  # noqa: E225, E999
    df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter
    df.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie

Each method signature only includes relevant arguments. Currently, these are limited to required arguments, but in the future these will include optional arguments, as well. For an overview, see the new :ref:`api.dataframe.plotting` API documentation.

.. _whatsnew_0170.strftime:

Additional methods for ``dt`` accessor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Series.dt.strftime
""""""""""""""""""

We are now supporting a ``Series.dt.strftime`` method for datetime-likes to generate a formatted string (:issue:`10110`). Examples:

.. ipython:: python

   # DatetimeIndex
   s = pd.Series(pd.date_range("20130101", periods=4))
   s
   s.dt.strftime("%Y/%m/%d")

.. ipython:: python

   # PeriodIndex
   s = pd.Series(pd.period_range("20130101", periods=4))
   s
   s.dt.strftime("%Y/%m/%d")

The string format is as the python standard library and details can be found `here <https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior>`_

Series.dt.total_seconds
"""""""""""""""""""""""

``pd.Series`` of type ``timedelta64`` has new method ``.dt.total_seconds()`` returning the duration of the timedelta in seconds (:issue:`10817`)

.. ipython:: python

   # TimedeltaIndex
   s = pd.Series(pd.timedelta_range("1 minutes", periods=4))
   s
   s.dt.total_seconds()

.. _whatsnew_0170.periodfreq:

Period frequency enhancement
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Period``, ``PeriodIndex`` and ``period_range`` can now accept multiplied freq. Also, ``Period.freq`` and ``PeriodIndex.freq`` are now stored as a ``DateOffset`` instance like ``DatetimeIndex``, and not as ``str`` (:issue:`7811`)

A multiplied freq represents a span of corresponding length. The example below creates a period of 3 days. Addition and subtraction will shift the period by its span.

.. ipython:: python

   p = pd.Period("2015-08-01", freq="3D")
   p
   p + 1
   p - 2
   p.to_timestamp()
   p.to_timestamp(how="E")

You can use the multiplied freq in ``PeriodIndex`` and ``period_range``.

.. ipython:: python

   idx = pd.period_range("2015-08-01", periods=4, freq="2D")
   idx
   idx + 1

.. _whatsnew_0170.enhancements.sas_xport:

Support for SAS XPORT files
^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`~pandas.io.read_sas` provides support for reading *SAS XPORT* format files. (:issue:`4052`).

.. code-block:: python

    df = pd.read_sas("sas_xport.xpt")

It is also possible to obtain an iterator and read an XPORT file
incrementally.

.. code-block:: python

    for df in pd.read_sas("sas_xport.xpt", chunksize=10000):
        do_something(df)

See the :ref:`docs <io.sas>` for more details.

.. _whatsnew_0170.matheval:

Support for math functions in .eval()
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`~pandas.eval` now supports calling math functions (:issue:`4893`)

.. code-block:: python

    df = pd.DataFrame({"a": np.random.randn(10)})
    df.eval("b = sin(a)")

The support math functions are ``sin``, ``cos``, ``exp``, ``log``, ``expm1``, ``log1p``,
``sqrt``, ``sinh``, ``cosh``, ``tanh``, ``arcsin``, ``arccos``, ``arctan``, ``arccosh``,
``arcsinh``, ``arctanh``, ``abs`` and ``arctan2``.

These functions map to the intrinsics for the ``NumExpr`` engine.  For the Python
engine, they are mapped to ``NumPy`` calls.

Changes to Excel with ``MultiIndex``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In version 0.16.2 a ``DataFrame`` with ``MultiIndex`` columns could not be written to Excel via ``to_excel``.
That functionality has been added (:issue:`10564`), along with updating  ``read_excel`` so that the data can
be read back with, no loss of information, by specifying which columns/rows make up the ``MultiIndex``
in the ``header`` and ``index_col`` parameters (:issue:`4679`)

See the :ref:`documentation <io.excel>` for more details.

.. ipython:: python

   df = pd.DataFrame(
       [[1, 2, 3, 4], [5, 6, 7, 8]],
       columns=pd.MultiIndex.from_product(
           [["foo", "bar"], ["a", "b"]], names=["col1", "col2"]
       ),
       index=pd.MultiIndex.from_product([["j"], ["l", "k"]], names=["i1", "i2"]),
   )

   df
   df.to_excel("test.xlsx")

   df = pd.read_excel("test.xlsx", header=[0, 1], index_col=[0, 1])
   df

.. ipython:: python
   :suppress:

   import os

   os.remove("test.xlsx")

Previously, it was necessary to specify the ``has_index_names`` argument in ``read_excel``,
if the serialized data had index names.  For version 0.17.0 the output format of ``to_excel``
has been changed to make this keyword unnecessary - the change is shown below.

**Old**

.. image:: ../_static/old-excel-index.png

**New**

.. image:: ../_static/new-excel-index.png

.. warning::

   Excel files saved in version 0.16.2 or prior that had index names will still able to be read in,
   but the ``has_index_names`` argument must specified to ``True``.

.. _whatsnew_0170.gbq:

Google BigQuery enhancements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
- Added ability to automatically create a table/dataset using the :func:`pandas.io.gbq.to_gbq` function if the destination table/dataset does not exist. (:issue:`8325`, :issue:`11121`).
- Added ability to replace an existing table and schema when calling the :func:`pandas.io.gbq.to_gbq` function via the ``if_exists`` argument. See the `docs <https://pandas-gbq.readthedocs.io/en/latest/writing.html>`__ for more details (:issue:`8325`).
- ``InvalidColumnOrder`` and ``InvalidPageToken`` in the gbq module will raise ``ValueError`` instead of ``IOError``.
- The ``generate_bq_schema()`` function is now deprecated and will be removed in a future version (:issue:`11121`)
- The gbq module will now support Python 3 (:issue:`11094`).

.. _whatsnew_0170.east_asian_width:

Display alignment with Unicode East Asian width
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning::

   Enabling this option will affect the performance for printing of ``DataFrame`` and ``Series`` (about 2 times slower).
   Use only when it is actually required.

Some East Asian countries use Unicode characters its width is corresponding to 2 alphabets. If a ``DataFrame`` or ``Series`` contains these characters, the default output cannot be aligned properly. The following options are added to enable precise handling for these characters.

- ``display.unicode.east_asian_width``: Whether to use the Unicode East Asian Width to calculate the display text width. (:issue:`2612`)
- ``display.unicode.ambiguous_as_wide``: Whether to handle Unicode characters belong to Ambiguous as Wide. (:issue:`11102`)

.. ipython:: python

   df = pd.DataFrame({u"国籍": ["UK", u"日本"], u"名前": ["Alice", u"しのぶ"]})
   df;

.. image:: ../_static/option_unicode01.png

.. ipython:: python

   pd.set_option("display.unicode.east_asian_width", True)
   df;

.. image:: ../_static/option_unicode02.png

For further details, see :ref:`here <options.east_asian_width>`

.. ipython:: python
   :suppress:

   pd.set_option("display.unicode.east_asian_width", False)

.. _whatsnew_0170.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Support for ``openpyxl`` >= 2.2. The API for style support is now stable (:issue:`10125`)
- ``merge`` now accepts the argument ``indicator`` which adds a Categorical-type column (by default called ``_merge``) to the output object that takes on the values (:issue:`8790`)

  ===================================   ================
  Observation Origin                    ``_merge`` value
  ===================================   ================
  Merge key only in ``'left'`` frame    ``left_only``
  Merge key only in ``'right'`` frame   ``right_only``
  Merge key in both frames              ``both``
  ===================================   ================

  .. ipython:: python

    df1 = pd.DataFrame({"col1": [0, 1], "col_left": ["a", "b"]})
    df2 = pd.DataFrame({"col1": [1, 2, 2], "col_right": [2, 2, 2]})
    pd.merge(df1, df2, on="col1", how="outer", indicator=True)

  For more, see the :ref:`updated docs <merging.indicator>`

- ``pd.to_numeric`` is a new function to coerce strings to numbers (possibly with coercion) (:issue:`11133`)

- ``pd.merge`` will now allow duplicate column names if they are not merged upon (:issue:`10639`).

- ``pd.pivot`` will now allow passing index as ``None`` (:issue:`3962`).

- ``pd.concat`` will now use existing Series names if provided (:issue:`10698`).

  .. ipython:: python

     foo = pd.Series([1, 2], name="foo")
     bar = pd.Series([1, 2])
     baz = pd.Series([4, 5])

  Previous behavior:

  .. code-block:: ipython

     In [1]: pd.concat([foo, bar, baz], axis=1)
     Out[1]:
           0  1  2
        0  1  1  4
        1  2  2  5

  New behavior:

  .. ipython:: python

    pd.concat([foo, bar, baz], axis=1)

- ``DataFrame`` has gained the ``nlargest`` and ``nsmallest`` methods (:issue:`10393`)

- Add a ``limit_direction`` keyword argument that works with ``limit`` to enable ``interpolate`` to fill ``NaN`` values forward, backward, or both (:issue:`9218`, :issue:`10420`, :issue:`11115`)

  .. ipython:: python

     ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13])
     ser.interpolate(limit=1, limit_direction="both")

- Added a ``DataFrame.round`` method to round the values to a variable number of decimal places (:issue:`10568`).

  .. ipython:: python

     df = pd.DataFrame(
         np.random.random([3, 3]),
         columns=["A", "B", "C"],
         index=["first", "second", "third"],
     )
     df
     df.round(2)
     df.round({"A": 0, "C": 2})

- ``drop_duplicates`` and ``duplicated`` now accept a ``keep`` keyword to target first, last, and all duplicates. The ``take_last`` keyword is deprecated, see :ref:`here <whatsnew_0170.deprecations>` (:issue:`6511`, :issue:`8505`)

  .. ipython:: python

     s = pd.Series(["A", "B", "C", "A", "B", "D"])
     s.drop_duplicates()
     s.drop_duplicates(keep="last")
     s.drop_duplicates(keep=False)

- Reindex now has a ``tolerance`` argument that allows for finer control of :ref:`basics.limits_on_reindex_fill` (:issue:`10411`):

  .. ipython:: python

     df = pd.DataFrame({"x": range(5), "t": pd.date_range("2000-01-01", periods=5)})
     df.reindex([0.1, 1.9, 3.5], method="nearest", tolerance=0.2)

  When used on a ``DatetimeIndex``, ``TimedeltaIndex`` or ``PeriodIndex``, ``tolerance`` will coerced into a ``Timedelta`` if possible. This allows you to specify tolerance with a string:

  .. ipython:: python

     df = df.set_index("t")
     df.reindex(pd.to_datetime(["1999-12-31"]), method="nearest", tolerance="1 day")

  ``tolerance`` is also exposed by the lower level ``Index.get_indexer`` and ``Index.get_loc`` methods.

- Added functionality to use the ``base`` argument when resampling a ``TimeDeltaIndex`` (:issue:`10530`)

- ``DatetimeIndex`` can be instantiated using strings contains ``NaT`` (:issue:`7599`)

- ``to_datetime`` can now accept the ``yearfirst`` keyword (:issue:`7599`)

- ``pandas.tseries.offsets`` larger than the ``Day`` offset can now be used with a ``Series`` for addition/subtraction (:issue:`10699`).  See the :ref:`docs <timeseries.offsetseries>` for more details.

- ``pd.Timedelta.total_seconds()`` now returns Timedelta duration to ns precision (previously microsecond precision) (:issue:`10939`)

- ``PeriodIndex`` now supports arithmetic with ``np.ndarray`` (:issue:`10638`)

- Support pickling of ``Period`` objects (:issue:`10439`)

- ``.as_blocks`` will now take a ``copy`` optional argument to return a copy of the data, default is to copy (no change in behavior from prior versions), (:issue:`9607`)

- ``regex`` argument to ``DataFrame.filter`` now handles numeric column names instead of raising ``ValueError`` (:issue:`10384`).

- Enable reading gzip compressed files via URL, either by explicitly setting the compression parameter or by inferring from the presence of the HTTP Content-Encoding header in the response (:issue:`8685`)

- Enable writing Excel files in :ref:`memory <io.excel_writing_buffer>` using StringIO/BytesIO (:issue:`7074`)

- Enable serialization of lists and dicts to strings in ``ExcelWriter`` (:issue:`8188`)

- SQL io functions now accept a SQLAlchemy connectable. (:issue:`7877`)

- ``pd.read_sql`` and ``to_sql`` can accept database URI as ``con`` parameter (:issue:`10214`)

- ``read_sql_table`` will now allow reading from views (:issue:`10750`).

- Enable writing complex values to ``HDFStores`` when using the ``table`` format (:issue:`10447`)

- Enable ``pd.read_hdf`` to be used without specifying a key when the HDF file contains a single dataset (:issue:`10443`)

- ``pd.read_stata`` will now read Stata 118 type files. (:issue:`9882`)

- ``msgpack`` submodule has been updated to 0.4.6 with backward compatibility (:issue:`10581`)

- ``DataFrame.to_dict`` now accepts ``orient='index'`` keyword argument (:issue:`10844`).

- ``DataFrame.apply`` will return a Series of dicts if the passed function returns a dict and ``reduce=True`` (:issue:`8735`).

- Allow passing ``kwargs`` to the interpolation methods (:issue:`10378`).

- Improved error message when concatenating an empty iterable of ``Dataframe`` objects (:issue:`9157`)

- ``pd.read_csv`` can now read bz2-compressed files incrementally, and the C parser can read bz2-compressed files from AWS S3 (:issue:`11070`, :issue:`11072`).

- In ``pd.read_csv``, recognize ``s3n://`` and ``s3a://`` URLs as designating S3 file storage (:issue:`11070`, :issue:`11071`).

- Read CSV files from AWS S3 incrementally, instead of first downloading the entire file. (Full file download still required for compressed files in Python 2.)  (:issue:`11070`, :issue:`11073`)

- ``pd.read_csv`` is now able to infer compression type for files read from AWS S3 storage (:issue:`11070`, :issue:`11074`).


.. _whatsnew_0170.api:

.. _whatsnew_0170.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0170.api_breaking.sorting:

Changes to sorting API
^^^^^^^^^^^^^^^^^^^^^^

The sorting API has had some longtime inconsistencies. (:issue:`9816`, :issue:`8239`).

Here is a summary of the API **PRIOR** to 0.17.0:

- ``Series.sort`` is **INPLACE** while ``DataFrame.sort`` returns a new object.
- ``Series.order`` returns a new object
- It was possible to use ``Series/DataFrame.sort_index`` to sort by **values** by passing the ``by`` keyword.
- ``Series/DataFrame.sortlevel`` worked only on a ``MultiIndex`` for sorting by index.

To address these issues, we have revamped the API:

- We have introduced a new method, :meth:`DataFrame.sort_values`, which is the merger of ``DataFrame.sort()``, ``Series.sort()``,
  and ``Series.order()``, to handle sorting of **values**.
- The existing methods ``Series.sort()``, ``Series.order()``, and ``DataFrame.sort()`` have been deprecated and will be removed in a
  future version.
- The ``by`` argument of ``DataFrame.sort_index()`` has been deprecated and will be removed in a future version.
- The existing method ``.sort_index()`` will gain the ``level`` keyword to enable level sorting.

We now have two distinct and non-overlapping methods of sorting. A ``*`` marks items that
will show a ``FutureWarning``.

To sort by the **values**:

==================================    ====================================
Previous                              Replacement
==================================    ====================================
\* ``Series.order()``                 ``Series.sort_values()``
\* ``Series.sort()``                  ``Series.sort_values(inplace=True)``
\* ``DataFrame.sort(columns=...)``    ``DataFrame.sort_values(by=...)``
==================================    ====================================

To sort by the **index**:

==================================    ====================================
Previous                              Replacement
==================================    ====================================
``Series.sort_index()``               ``Series.sort_index()``
``Series.sortlevel(level=...)``       ``Series.sort_index(level=...``)
``DataFrame.sort_index()``            ``DataFrame.sort_index()``
``DataFrame.sortlevel(level=...)``    ``DataFrame.sort_index(level=...)``
\* ``DataFrame.sort()``                 ``DataFrame.sort_index()``
==================================    ====================================

We have also deprecated and changed similar methods in two Series-like classes, ``Index`` and ``Categorical``.

==================================    ====================================
Previous                              Replacement
==================================    ====================================
\* ``Index.order()``                  ``Index.sort_values()``
\* ``Categorical.order()``            ``Categorical.sort_values()``
==================================    ====================================

.. _whatsnew_0170.api_breaking.to_datetime:

Changes to to_datetime and to_timedelta
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Error handling
""""""""""""""

The default for ``pd.to_datetime`` error handling has changed to ``errors='raise'``.
In prior versions it was ``errors='ignore'``. Furthermore, the ``coerce`` argument
has been deprecated in favor of ``errors='coerce'``. This means that invalid parsing
will raise rather that return the original input as in previous versions. (:issue:`10636`)

Previous behavior:

.. code-block:: ipython

   In [2]: pd.to_datetime(['2009-07-31', 'asd'])
   Out[2]: array(['2009-07-31', 'asd'], dtype=object)

New behavior:

.. code-block:: ipython

   In [3]: pd.to_datetime(['2009-07-31', 'asd'])
   ValueError: Unknown string format

Of course you can coerce this as well.

.. ipython:: python

   pd.to_datetime(["2009-07-31", "asd"], errors="coerce")

To keep the previous behavior, you can use ``errors='ignore'``:

.. ipython:: python

   pd.to_datetime(["2009-07-31", "asd"], errors="ignore")

Furthermore, ``pd.to_timedelta`` has gained a similar API, of ``errors='raise'|'ignore'|'coerce'``, and the ``coerce`` keyword
has been deprecated in favor of ``errors='coerce'``.

Consistent parsing
""""""""""""""""""

The string parsing of ``to_datetime``, ``Timestamp`` and ``DatetimeIndex`` has
been made consistent. (:issue:`7599`)

Prior to v0.17.0, ``Timestamp`` and ``to_datetime`` may parse year-only datetime-string incorrectly using today's date, otherwise ``DatetimeIndex``
uses the beginning of the year. ``Timestamp`` and ``to_datetime`` may raise ``ValueError`` in some types of datetime-string which ``DatetimeIndex``
can parse, such as a quarterly string.

Previous behavior:

.. code-block:: ipython

   In [1]: pd.Timestamp('2012Q2')
   Traceback
      ...
   ValueError: Unable to parse 2012Q2

   # Results in today's date.
   In [2]: pd.Timestamp('2014')
   Out [2]: 2014-08-12 00:00:00

v0.17.0 can parse them as below. It works on ``DatetimeIndex`` also.

New behavior:

.. ipython:: python

   pd.Timestamp("2012Q2")
   pd.Timestamp("2014")
   pd.DatetimeIndex(["2012Q2", "2014"])

.. note::

   If you want to perform calculations based on today's date, use ``Timestamp.now()`` and ``pandas.tseries.offsets``.

   .. ipython:: python

      import pandas.tseries.offsets as offsets

      pd.Timestamp.now()
      pd.Timestamp.now() + offsets.DateOffset(years=1)

Changes to Index comparisons
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Operator equal on ``Index`` should behavior similarly to ``Series`` (:issue:`9947`, :issue:`10637`)

Starting in v0.17.0, comparing ``Index`` objects of different lengths will raise
a ``ValueError``. This is to be consistent with the behavior of ``Series``.

Previous behavior:

.. code-block:: ipython

   In [2]: pd.Index([1, 2, 3]) == pd.Index([1, 4, 5])
   Out[2]: array([ True, False, False], dtype=bool)

   In [3]: pd.Index([1, 2, 3]) == pd.Index([2])
   Out[3]: array([False,  True, False], dtype=bool)

   In [4]: pd.Index([1, 2, 3]) == pd.Index([1, 2])
   Out[4]: False

New behavior:

.. code-block:: ipython

   In [8]: pd.Index([1, 2, 3]) == pd.Index([1, 4, 5])
   Out[8]: array([ True, False, False], dtype=bool)

   In [9]: pd.Index([1, 2, 3]) == pd.Index([2])
   ValueError: Lengths must match to compare

   In [10]: pd.Index([1, 2, 3]) == pd.Index([1, 2])
   ValueError: Lengths must match to compare

Note that this is different from the ``numpy`` behavior where a comparison can
be broadcast:

.. ipython:: python

   np.array([1, 2, 3]) == np.array([1])

or it can return False if broadcasting can not be done:

.. ipython:: python
   :okwarning:

   np.array([1, 2, 3]) == np.array([1, 2])

Changes to boolean comparisons vs. None
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Boolean comparisons of a ``Series`` vs ``None`` will now be equivalent to comparing with ``np.nan``, rather than raise ``TypeError``. (:issue:`1079`).

.. ipython:: python

   s = pd.Series(range(3))
   s.iloc[1] = None
   s

Previous behavior:

.. code-block:: ipython

   In [5]: s == None
   TypeError: Could not compare <type 'NoneType'> type with Series

New behavior:

.. ipython:: python

   s == None

Usually you simply want to know which values are null.

.. ipython:: python

   s.isnull()

.. warning::

   You generally will want to use ``isnull/notnull`` for these types of comparisons, as ``isnull/notnull`` tells you which elements are null. One has to be
   mindful that ``nan's`` don't compare equal, but ``None's`` do. Note that pandas/numpy uses the fact that ``np.nan != np.nan``, and treats ``None`` like ``np.nan``.

   .. ipython:: python

      None == None
      np.nan == np.nan

.. _whatsnew_0170.api_breaking.hdf_dropna:

HDFStore dropna behavior
^^^^^^^^^^^^^^^^^^^^^^^^

The default behavior for HDFStore write functions with ``format='table'`` is now to keep rows that are all missing. Previously, the behavior was to drop rows that were all missing save the index. The previous behavior can be replicated using the ``dropna=True`` option. (:issue:`9382`)

Previous behavior:

.. ipython:: python

   df_with_missing = pd.DataFrame(
       {"col1": [0, np.nan, 2], "col2": [1, np.nan, np.nan]}
   )

   df_with_missing


.. code-block:: ipython

   In [27]:
   df_with_missing.to_hdf('file.h5',
                          'df_with_missing',
                          format='table',
                          mode='w')

   In [28]: pd.read_hdf('file.h5', 'df_with_missing')

   Out [28]:
         col1  col2
     0     0     1
     2     2   NaN


New behavior:

.. ipython:: python

   df_with_missing.to_hdf("file.h5", "df_with_missing", format="table", mode="w")

   pd.read_hdf("file.h5", "df_with_missing")

.. ipython:: python
   :suppress:

   import os

   os.remove("file.h5")

See the :ref:`docs <io.hdf5>` for more details.

.. _whatsnew_0170.api_breaking.display_precision:

Changes to ``display.precision`` option
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``display.precision`` option has been clarified to refer to decimal places (:issue:`10451`).

Earlier versions of pandas would format floating point numbers to have one less decimal place than the value in
``display.precision``.

.. code-block:: ipython

  In [1]: pd.set_option('display.precision', 2)

  In [2]: pd.DataFrame({'x': [123.456789]})
  Out[2]:
         x
  0  123.5

If interpreting precision as "significant figures" this did work for scientific notation but that same interpretation
did not work for values with standard formatting. It was also out of step with how numpy handles formatting.

Going forward the value of ``display.precision`` will directly control the number of places after the decimal, for
regular formatting as well as scientific notation, similar to how numpy's ``precision`` print option works.

.. ipython:: python

  pd.set_option("display.precision", 2)
  pd.DataFrame({"x": [123.456789]})

To preserve output behavior with prior versions the default value of ``display.precision`` has been reduced to ``6``
from ``7``.

.. ipython:: python
  :suppress:

  pd.set_option("display.precision", 6)

.. _whatsnew_0170.api_breaking.categorical_unique:

Changes to ``Categorical.unique``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Categorical.unique`` now returns new ``Categoricals`` with ``categories`` and ``codes`` that are unique, rather than returning ``np.array`` (:issue:`10508`)

- unordered category: values and categories are sorted by appearance order.
- ordered category: values are sorted by appearance order, categories keep existing order.

.. ipython:: python

   cat = pd.Categorical(["C", "A", "B", "C"], categories=["A", "B", "C"], ordered=True)
   cat
   cat.unique()

   cat = pd.Categorical(["C", "A", "B", "C"], categories=["A", "B", "C"])
   cat
   cat.unique()

Changes to ``bool`` passed as ``header`` in parsers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In earlier versions of pandas, if a bool was passed the ``header`` argument of
``read_csv``, ``read_excel``, or ``read_html`` it was implicitly converted to
an integer, resulting in ``header=0`` for ``False`` and ``header=1`` for ``True``
(:issue:`6113`)

A ``bool`` input to ``header`` will now raise a ``TypeError``

.. code-block:: ipython

   In [29]: df = pd.read_csv('data.csv', header=False)
   TypeError: Passing a bool to header is invalid. Use header=None for no header or
   header=int or list-like of ints to specify the row(s) making up the column names


.. _whatsnew_0170.api_breaking.other:

Other API changes
^^^^^^^^^^^^^^^^^

- Line and kde plot with ``subplots=True`` now uses default colors, not all black. Specify ``color='k'`` to draw all lines in black (:issue:`9894`)
- Calling the ``.value_counts()`` method on a Series with a ``categorical`` dtype now returns a Series with a ``CategoricalIndex`` (:issue:`10704`)
- The metadata properties of subclasses of pandas objects will now be serialized (:issue:`10553`).
- ``groupby`` using ``Categorical`` follows the same rule as ``Categorical.unique`` described above  (:issue:`10508`)
- When constructing ``DataFrame`` with an array of ``complex64`` dtype previously meant the corresponding column
  was automatically promoted to the ``complex128`` dtype. pandas will now preserve the itemsize of the input for complex data (:issue:`10952`)
- some numeric reduction operators would return ``ValueError``, rather than ``TypeError`` on object types that includes strings and numbers (:issue:`11131`)
- Passing currently unsupported ``chunksize`` argument to ``read_excel`` or ``ExcelFile.parse`` will now raise ``NotImplementedError`` (:issue:`8011`)
- Allow an ``ExcelFile`` object to be passed into ``read_excel`` (:issue:`11198`)
- ``DatetimeIndex.union`` does not infer ``freq`` if ``self`` and the input have ``None`` as ``freq`` (:issue:`11086`)
- ``NaT``'s methods now either raise ``ValueError``, or return ``np.nan`` or ``NaT`` (:issue:`9513`)

  ===============================     ===============================================================
  Behavior                            Methods
  ===============================     ===============================================================
  return ``np.nan``                   ``weekday``, ``isoweekday``
  return ``NaT``                      ``date``, ``now``, ``replace``, ``to_datetime``, ``today``
  return ``np.datetime64('NaT')``     ``to_datetime64`` (unchanged)
  raise ``ValueError``                All other public methods (names not beginning with underscores)
  ===============================     ===============================================================

.. _whatsnew_0170.deprecations:

Deprecations
^^^^^^^^^^^^

- For ``Series`` the following indexing functions are deprecated (:issue:`10177`).

  =====================  =================================
  Deprecated Function    Replacement
  =====================  =================================
  ``.irow(i)``           ``.iloc[i]`` or ``.iat[i]``
  ``.iget(i)``           ``.iloc[i]`` or ``.iat[i]``
  ``.iget_value(i)``     ``.iloc[i]`` or ``.iat[i]``
  =====================  =================================

- For ``DataFrame`` the following indexing functions are deprecated (:issue:`10177`).

  =====================  =================================
  Deprecated Function    Replacement
  =====================  =================================
  ``.irow(i)``           ``.iloc[i]``
  ``.iget_value(i, j)``  ``.iloc[i, j]`` or ``.iat[i, j]``
  ``.icol(j)``           ``.iloc[:, j]``
  =====================  =================================

.. note:: These indexing function have been deprecated in the documentation since 0.11.0.

- ``Categorical.name`` was deprecated to make ``Categorical`` more ``numpy.ndarray`` like. Use ``Series(cat, name="whatever")`` instead (:issue:`10482`).
- Setting missing values (NaN) in a ``Categorical``'s ``categories`` will issue a warning (:issue:`10748`). You can still have missing values in the ``values``.
- ``drop_duplicates`` and ``duplicated``'s ``take_last`` keyword was deprecated in favor of ``keep``. (:issue:`6511`, :issue:`8505`)
- ``Series.nsmallest`` and ``nlargest``'s ``take_last`` keyword was deprecated in favor of ``keep``. (:issue:`10792`)
- ``DataFrame.combineAdd`` and ``DataFrame.combineMult`` are deprecated. They
  can easily be replaced by using the ``add`` and ``mul`` methods:
  ``DataFrame.add(other, fill_value=0)`` and ``DataFrame.mul(other, fill_value=1.)``
  (:issue:`10735`).
- ``TimeSeries`` deprecated in favor of ``Series`` (note that this has been an alias since 0.13.0), (:issue:`10890`)
- ``SparsePanel`` deprecated and will be removed in a future version (:issue:`11157`).
- ``Series.is_time_series`` deprecated in favor of ``Series.index.is_all_dates`` (:issue:`11135`)
- Legacy offsets (like ``'A@JAN'``) are deprecated (note that this has been alias since 0.8.0) (:issue:`10878`)
- ``WidePanel`` deprecated in favor of ``Panel``, ``LongPanel`` in favor of ``DataFrame`` (note these have been aliases since < 0.11.0), (:issue:`10892`)
- ``DataFrame.convert_objects`` has been deprecated in favor of type-specific functions ``pd.to_datetime``, ``pd.to_timestamp`` and ``pd.to_numeric`` (new in 0.17.0) (:issue:`11133`).

.. _whatsnew_0170.prior_deprecations:

Removal of prior version deprecations/changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Removal of ``na_last`` parameters from ``Series.order()`` and ``Series.sort()``, in favor of ``na_position``. (:issue:`5231`)
- Remove of ``percentile_width`` from ``.describe()``, in favor of ``percentiles``. (:issue:`7088`)
- Removal of ``colSpace`` parameter from ``DataFrame.to_string()``, in favor of ``col_space``, circa 0.8.0 version.
- Removal of automatic time-series broadcasting (:issue:`2304`)

  .. ipython:: python

     np.random.seed(1234)
     df = pd.DataFrame(
         np.random.randn(5, 2),
         columns=list("AB"),
         index=pd.date_range("2013-01-01", periods=5),
     )
     df

  Previously

  .. code-block:: ipython

     In [3]: df + df.A
     FutureWarning: TimeSeries broadcasting along DataFrame index by default is deprecated.
     Please use DataFrame.<op> to explicitly broadcast arithmetic operations along the index

     Out[3]:
                         A         B
     2013-01-01  0.942870 -0.719541
     2013-01-02  2.865414  1.120055
     2013-01-03 -1.441177  0.166574
     2013-01-04  1.719177  0.223065
     2013-01-05  0.031393 -2.226989

  Current

  .. ipython:: python

     df.add(df.A, axis="index")


- Remove ``table`` keyword in ``HDFStore.put/append``, in favor of using ``format=`` (:issue:`4645`)
- Remove ``kind`` in ``read_excel/ExcelFile`` as its unused (:issue:`4712`)
- Remove ``infer_type`` keyword from ``pd.read_html`` as its unused (:issue:`4770`, :issue:`7032`)
- Remove ``offset`` and ``timeRule`` keywords from ``Series.tshift/shift``, in favor of ``freq`` (:issue:`4853`, :issue:`4864`)
- Remove ``pd.load/pd.save`` aliases in favor of ``pd.to_pickle/pd.read_pickle`` (:issue:`3787`)

.. _whatsnew_0170.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Development support for benchmarking with the `Air Speed Velocity library <https://github.com/spacetelescope/asv/>`_ (:issue:`8361`)
- Added vbench benchmarks for alternative ExcelWriter engines and reading Excel files (:issue:`7171`)
- Performance improvements in ``Categorical.value_counts`` (:issue:`10804`)
- Performance improvements in ``SeriesGroupBy.nunique`` and ``SeriesGroupBy.value_counts`` and ``SeriesGroupby.transform`` (:issue:`10820`, :issue:`11077`)
- Performance improvements in ``DataFrame.drop_duplicates`` with integer dtypes (:issue:`10917`)
- Performance improvements in ``DataFrame.duplicated`` with wide frames. (:issue:`10161`, :issue:`11180`)
- 4x improvement in ``timedelta`` string parsing (:issue:`6755`, :issue:`10426`)
- 8x improvement in ``timedelta64`` and ``datetime64`` ops (:issue:`6755`)
- Significantly improved performance of indexing ``MultiIndex`` with slicers (:issue:`10287`)
- 8x improvement in ``iloc`` using list-like input (:issue:`10791`)
- Improved performance of ``Series.isin`` for datetimelike/integer Series (:issue:`10287`)
- 20x improvement in ``concat`` of Categoricals when categories are identical (:issue:`10587`)
- Improved performance of ``to_datetime`` when specified format string is ISO8601 (:issue:`10178`)
- 2x improvement of ``Series.value_counts`` for float dtype (:issue:`10821`)
- Enable ``infer_datetime_format`` in ``to_datetime`` when date components do not have 0 padding (:issue:`11142`)
- Regression from 0.16.1 in constructing ``DataFrame`` from nested dictionary (:issue:`11084`)
- Performance improvements in addition/subtraction operations for ``DateOffset`` with ``Series`` or ``DatetimeIndex``  (:issue:`10744`, :issue:`11205`)

.. _whatsnew_0170.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in incorrect computation of ``.mean()`` on ``timedelta64[ns]`` because of overflow (:issue:`9442`)
- Bug in  ``.isin`` on older numpies (:issue:`11232`)
- Bug in ``DataFrame.to_html(index=False)`` renders unnecessary ``name`` row (:issue:`10344`)
- Bug in ``DataFrame.to_latex()`` the ``column_format`` argument could not be passed (:issue:`9402`)
- Bug in ``DatetimeIndex`` when localizing with ``NaT`` (:issue:`10477`)
- Bug in ``Series.dt`` ops in preserving meta-data (:issue:`10477`)
- Bug in preserving ``NaT`` when passed in an otherwise invalid ``to_datetime`` construction (:issue:`10477`)
- Bug in ``DataFrame.apply`` when function returns categorical series. (:issue:`9573`)
- Bug in ``to_datetime`` with invalid dates and formats supplied (:issue:`10154`)
- Bug in ``Index.drop_duplicates`` dropping name(s) (:issue:`10115`)
- Bug in ``Series.quantile`` dropping name (:issue:`10881`)
- Bug in ``pd.Series`` when setting a value on an empty ``Series`` whose index has a frequency. (:issue:`10193`)
- Bug in ``pd.Series.interpolate`` with invalid ``order`` keyword values. (:issue:`10633`)
- Bug in ``DataFrame.plot`` raises ``ValueError`` when color name is specified by multiple characters (:issue:`10387`)
- Bug in ``Index`` construction with a mixed list of tuples (:issue:`10697`)
- Bug in ``DataFrame.reset_index`` when index contains ``NaT``. (:issue:`10388`)
- Bug in ``ExcelReader`` when worksheet is empty (:issue:`6403`)
- Bug in ``BinGrouper.group_info`` where returned values are not compatible with base class (:issue:`10914`)
- Bug in clearing the cache on ``DataFrame.pop`` and a subsequent inplace op (:issue:`10912`)
- Bug in indexing with a mixed-integer ``Index`` causing an ``ImportError`` (:issue:`10610`)
- Bug in ``Series.count`` when index has nulls (:issue:`10946`)
- Bug in pickling of a non-regular freq ``DatetimeIndex`` (:issue:`11002`)
- Bug causing ``DataFrame.where`` to not respect the ``axis`` parameter when the frame has a symmetric shape. (:issue:`9736`)
- Bug in ``Table.select_column`` where name is not preserved (:issue:`10392`)
- Bug in ``offsets.generate_range`` where ``start`` and ``end`` have finer precision than ``offset`` (:issue:`9907`)
- Bug in ``pd.rolling_*`` where ``Series.name`` would be lost in the output (:issue:`10565`)
- Bug in ``stack`` when index or columns are not unique. (:issue:`10417`)
- Bug in setting a ``Panel`` when an axis has a MultiIndex (:issue:`10360`)
- Bug in ``USFederalHolidayCalendar`` where ``USMemorialDay`` and ``USMartinLutherKingJr`` were incorrect (:issue:`10278` and :issue:`9760` )
- Bug in ``.sample()`` where returned object, if set, gives unnecessary ``SettingWithCopyWarning`` (:issue:`10738`)
- Bug in ``.sample()`` where weights passed as ``Series`` were not aligned along axis before being treated positionally, potentially causing problems if weight indices were not aligned with sampled object. (:issue:`10738`)

- Regression fixed in (:issue:`9311`, :issue:`6620`, :issue:`9345`), where groupby with a datetime-like converting to float with certain aggregators (:issue:`10979`)

- Bug in ``DataFrame.interpolate`` with ``axis=1`` and ``inplace=True`` (:issue:`10395`)
- Bug in ``io.sql.get_schema`` when specifying multiple columns as primary
  key (:issue:`10385`).

- Bug in ``groupby(sort=False)`` with datetime-like ``Categorical`` raises ``ValueError`` (:issue:`10505`)
- Bug in ``groupby(axis=1)`` with ``filter()`` throws ``IndexError`` (:issue:`11041`)
- Bug in ``test_categorical`` on big-endian builds (:issue:`10425`)
- Bug in ``Series.shift`` and ``DataFrame.shift`` not supporting categorical data (:issue:`9416`)
- Bug in ``Series.map`` using categorical ``Series`` raises ``AttributeError`` (:issue:`10324`)
- Bug in ``MultiIndex.get_level_values`` including ``Categorical`` raises ``AttributeError`` (:issue:`10460`)
- Bug in ``pd.get_dummies`` with ``sparse=True`` not returning ``SparseDataFrame`` (:issue:`10531`)
- Bug in ``Index`` subtypes (such as ``PeriodIndex``) not returning their own type for ``.drop`` and ``.insert`` methods (:issue:`10620`)
- Bug in ``algos.outer_join_indexer`` when ``right`` array is empty (:issue:`10618`)

- Bug in ``filter`` (regression from 0.16.0) and ``transform`` when grouping on multiple keys, one of which is datetime-like (:issue:`10114`)


- Bug in ``to_datetime`` and ``to_timedelta`` causing ``Index`` name to be lost (:issue:`10875`)
- Bug in ``len(DataFrame.groupby)`` causing ``IndexError`` when there's a column containing only NaNs (:issue:`11016`)

- Bug that caused segfault when resampling an empty Series (:issue:`10228`)
- Bug in ``DatetimeIndex`` and ``PeriodIndex.value_counts`` resets name from its result, but retains in result's ``Index``. (:issue:`10150`)
- Bug in ``pd.eval`` using ``numexpr`` engine coerces 1 element numpy array to scalar (:issue:`10546`)
- Bug in ``pd.concat`` with ``axis=0`` when column is of dtype ``category`` (:issue:`10177`)
- Bug in ``read_msgpack`` where input type is not always checked (:issue:`10369`, :issue:`10630`)
- Bug in ``pd.read_csv`` with kwargs ``index_col=False``, ``index_col=['a', 'b']`` or ``dtype``
  (:issue:`10413`, :issue:`10467`, :issue:`10577`)
- Bug in ``Series.from_csv`` with ``header`` kwarg not setting the ``Series.name`` or the ``Series.index.name`` (:issue:`10483`)
- Bug in ``groupby.var`` which caused variance to be inaccurate for small float values (:issue:`10448`)
- Bug in ``Series.plot(kind='hist')`` Y Label not informative (:issue:`10485`)
- Bug in ``read_csv`` when using a converter which generates a ``uint8`` type (:issue:`9266`)

- Bug causes memory leak in time-series line and area plot (:issue:`9003`)

- Bug when setting a ``Panel`` sliced along the major or minor axes when the right-hand side is a ``DataFrame`` (:issue:`11014`)
- Bug that returns ``None`` and does not raise ``NotImplementedError`` when operator functions (e.g. ``.add``) of ``Panel`` are not implemented (:issue:`7692`)

- Bug in line and kde plot cannot accept multiple colors when ``subplots=True`` (:issue:`9894`)
- Bug in ``DataFrame.plot`` raises ``ValueError`` when color name is specified by multiple characters (:issue:`10387`)

- Bug in left and right ``align`` of ``Series`` with ``MultiIndex`` may be inverted (:issue:`10665`)
- Bug in left and right ``join`` of with ``MultiIndex`` may be inverted (:issue:`10741`)

- Bug in ``read_stata`` when reading a file with a different order set in ``columns`` (:issue:`10757`)
- Bug in ``Categorical`` may not representing properly when category contains ``tz`` or ``Period`` (:issue:`10713`)
- Bug in ``Categorical.__iter__`` may not returning correct ``datetime`` and ``Period`` (:issue:`10713`)
- Bug in indexing with a ``PeriodIndex`` on an object with a ``PeriodIndex`` (:issue:`4125`)
- Bug in ``read_csv`` with ``engine='c'``: EOF preceded by a comment, blank line, etc. was not handled correctly (:issue:`10728`, :issue:`10548`)

- Reading "famafrench" data via ``DataReader`` results in HTTP 404 error because of the website url is changed (:issue:`10591`).
- Bug in ``read_msgpack`` where DataFrame to decode has duplicate column names (:issue:`9618`)
- Bug in ``io.common.get_filepath_or_buffer`` which caused reading of valid S3 files to fail if the bucket also contained keys for which the user does not have read permission (:issue:`10604`)
- Bug in vectorised setting of timestamp columns with python ``datetime.date`` and numpy ``datetime64`` (:issue:`10408`, :issue:`10412`)
- Bug in ``Index.take`` may add unnecessary ``freq`` attribute (:issue:`10791`)
- Bug in ``merge`` with empty ``DataFrame`` may raise ``IndexError`` (:issue:`10824`)
- Bug in ``to_latex`` where unexpected keyword argument for some documented arguments (:issue:`10888`)
- Bug in indexing of large ``DataFrame`` where ``IndexError`` is uncaught (:issue:`10645` and :issue:`10692`)
- Bug in ``read_csv`` when using the ``nrows`` or ``chunksize`` parameters if file contains only a header line (:issue:`9535`)
- Bug in serialization of ``category`` types in HDF5 in presence of alternate encodings. (:issue:`10366`)
- Bug in ``pd.DataFrame`` when constructing an empty DataFrame with a string dtype (:issue:`9428`)
- Bug in ``pd.DataFrame.diff`` when DataFrame is not consolidated (:issue:`10907`)
- Bug in ``pd.unique`` for arrays with the ``datetime64`` or ``timedelta64`` dtype that meant an array with object dtype was returned instead the original dtype (:issue:`9431`)
- Bug in ``Timedelta`` raising error when slicing from 0s (:issue:`10583`)
- Bug in ``DatetimeIndex.take`` and ``TimedeltaIndex.take`` may not raise ``IndexError`` against invalid index (:issue:`10295`)
- Bug in ``Series([np.nan]).astype('M8[ms]')``, which now returns ``Series([pd.NaT])`` (:issue:`10747`)
- Bug in ``PeriodIndex.order`` reset freq (:issue:`10295`)
- Bug in ``date_range`` when ``freq`` divides ``end`` as nanos (:issue:`10885`)
- Bug in ``iloc`` allowing memory outside bounds of a Series to be accessed with negative integers (:issue:`10779`)
- Bug in ``read_msgpack`` where encoding is not respected (:issue:`10581`)
- Bug preventing access to the first index when using ``iloc`` with a list containing the appropriate negative integer (:issue:`10547`, :issue:`10779`)
- Bug in ``TimedeltaIndex`` formatter causing error while trying to save ``DataFrame`` with ``TimedeltaIndex`` using ``to_csv`` (:issue:`10833`)
- Bug in ``DataFrame.where`` when handling Series slicing (:issue:`10218`, :issue:`9558`)
- Bug where ``pd.read_gbq`` throws ``ValueError`` when Bigquery returns zero rows (:issue:`10273`)
- Bug in ``to_json`` which was causing segmentation fault when serializing 0-rank ndarray (:issue:`9576`)
- Bug in plotting functions may raise ``IndexError`` when plotted on ``GridSpec`` (:issue:`10819`)
- Bug in plot result may show unnecessary minor ticklabels (:issue:`10657`)
- Bug in ``groupby`` incorrect computation for aggregation on ``DataFrame`` with ``NaT`` (E.g ``first``, ``last``, ``min``). (:issue:`10590`, :issue:`11010`)
- Bug when constructing ``DataFrame`` where passing a dictionary with only scalar values and specifying columns did not raise an error (:issue:`10856`)
- Bug in ``.var()`` causing roundoff errors for highly similar values (:issue:`10242`)
- Bug in ``DataFrame.plot(subplots=True)`` with duplicated columns outputs incorrect result (:issue:`10962`)
- Bug in ``Index`` arithmetic may result in incorrect class (:issue:`10638`)
- Bug in ``date_range`` results in empty if freq is negative annually, quarterly and monthly (:issue:`11018`)
- Bug in ``DatetimeIndex`` cannot infer negative freq (:issue:`11018`)
- Remove use of some deprecated numpy comparison operations, mainly in tests. (:issue:`10569`)
- Bug in ``Index`` dtype may not applied properly (:issue:`11017`)
- Bug in ``io.gbq`` when testing for minimum google api client version (:issue:`10652`)
- Bug in ``DataFrame`` construction from nested ``dict`` with ``timedelta`` keys (:issue:`11129`)
- Bug in ``.fillna`` against may raise ``TypeError`` when data contains datetime dtype (:issue:`7095`, :issue:`11153`)
- Bug in ``.groupby`` when number of keys to group by is same as length of index (:issue:`11185`)
- Bug in ``convert_objects`` where converted values might not be returned if all null and ``coerce`` (:issue:`9589`)
- Bug in ``convert_objects`` where ``copy`` keyword was not respected (:issue:`9589`)


.. _whatsnew_0.17.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.16.2..v0.17.0
.. _whatsnew_0151:

Version 0.15.1 (November 9, 2014)
---------------------------------

{{ header }}


This is a minor bug-fix release from 0.15.0 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

- :ref:`Enhancements <whatsnew_0151.enhancements>`
- :ref:`API Changes <whatsnew_0151.api>`
- :ref:`Bug Fixes <whatsnew_0151.bug_fixes>`

.. _whatsnew_0151.api:

API changes
~~~~~~~~~~~

- ``s.dt.hour`` and other ``.dt`` accessors will now return ``np.nan`` for missing values (rather than previously -1), (:issue:`8689`)

  .. ipython:: python

     s = pd.Series(pd.date_range("20130101", periods=5, freq="D"))
     s.iloc[2] = np.nan
     s

  previous behavior:

  .. code-block:: ipython

     In [6]: s.dt.hour
     Out[6]:
     0    0
     1    0
     2   -1
     3    0
     4    0
     dtype: int64

  current behavior:

  .. ipython:: python

     s.dt.hour

- ``groupby`` with ``as_index=False`` will not add erroneous extra columns to
  result (:issue:`8582`):

  .. ipython:: python

     np.random.seed(2718281)
     df = pd.DataFrame(np.random.randint(0, 100, (10, 2)), columns=["jim", "joe"])
     df.head()

     ts = pd.Series(5 * np.random.randint(0, 3, 10))

  previous behavior:

  .. code-block:: ipython

     In [4]: df.groupby(ts, as_index=False).max()
     Out[4]:
        NaN  jim  joe
     0    0   72   83
     1    5   77   84
     2   10   96   65

  current behavior:

  .. ipython:: python

    df.groupby(ts, as_index=False).max()

- ``groupby`` will not erroneously exclude columns if the column name conflicts
  with the grouper name (:issue:`8112`):

  .. ipython:: python

     df = pd.DataFrame({"jim": range(5), "joe": range(5, 10)})
     df
     gr = df.groupby(df["jim"] < 2)

  previous behavior (excludes 1st column from output):

  .. code-block:: ipython

     In [4]: gr.apply(sum)
     Out[4]:
            joe
     jim
     False   24
     True    11

  current behavior:

  .. ipython:: python

     gr.apply(sum)

- Support for slicing with monotonic decreasing indexes, even if ``start`` or ``stop`` is
  not found in the index (:issue:`7860`):

  .. ipython:: python

    s = pd.Series(["a", "b", "c", "d"], [4, 3, 2, 1])
    s

  previous behavior:

  .. code-block:: ipython

     In [8]: s.loc[3.5:1.5]
     KeyError: 3.5

  current behavior:

  .. ipython:: python

     s.loc[3.5:1.5]

- ``io.data.Options`` has been fixed for a change in the format of the Yahoo Options page (:issue:`8612`), (:issue:`8741`)

  .. note::

    As a result of a change in Yahoo's option page layout, when an expiry date is given,
    ``Options`` methods now return data for a single expiry date.  Previously, methods returned all
    data for the selected month.

  The ``month`` and ``year`` parameters have been undeprecated and can be used to get all
  options data for a given month.

  If an expiry date that is not valid is given, data for the next expiry after the given
  date is returned.

  Option data frames are now saved on the instance as ``callsYYMMDD`` or ``putsYYMMDD``.  Previously
  they were saved as ``callsMMYY`` and ``putsMMYY``.  The next expiry is saved as ``calls`` and ``puts``.

  New features:

  - The expiry parameter can now be a single date or a list-like object containing dates.

  - A new property ``expiry_dates`` was added, which returns all available expiry dates.

  Current behavior:

  .. code-block:: ipython

      In [17]: from pandas.io.data import Options

      In [18]: aapl = Options('aapl', 'yahoo')

      In [19]: aapl.get_call_data().iloc[0:5, 0:1]
      Out[19]:
                                                   Last
      Strike Expiry     Type Symbol
      80     2014-11-14 call AAPL141114C00080000  29.05
      84     2014-11-14 call AAPL141114C00084000  24.80
      85     2014-11-14 call AAPL141114C00085000  24.05
      86     2014-11-14 call AAPL141114C00086000  22.76
      87     2014-11-14 call AAPL141114C00087000  21.74

      In [20]: aapl.expiry_dates
      Out[20]:
      [datetime.date(2014, 11, 14),
       datetime.date(2014, 11, 22),
       datetime.date(2014, 11, 28),
       datetime.date(2014, 12, 5),
       datetime.date(2014, 12, 12),
       datetime.date(2014, 12, 20),
       datetime.date(2015, 1, 17),
       datetime.date(2015, 2, 20),
       datetime.date(2015, 4, 17),
       datetime.date(2015, 7, 17),
       datetime.date(2016, 1, 15),
       datetime.date(2017, 1, 20)]

      In [21]: aapl.get_near_stock_price(expiry=aapl.expiry_dates[0:3]).iloc[0:5, 0:1]
      Out[21]:
                                                  Last
      Strike Expiry     Type Symbol
      109    2014-11-22 call AAPL141122C00109000  1.48
             2014-11-28 call AAPL141128C00109000  1.79
      110    2014-11-14 call AAPL141114C00110000  0.55
             2014-11-22 call AAPL141122C00110000  1.02
             2014-11-28 call AAPL141128C00110000  1.32

.. _whatsnew_0151.datetime64_plotting:

- pandas now also registers the ``datetime64`` dtype in matplotlib's units registry
  to plot such values as datetimes. This is activated once pandas is imported. In
  previous versions, plotting an array of ``datetime64`` values will have resulted
  in plotted integer values. To keep the previous behaviour, you can do
  ``del matplotlib.units.registry[np.datetime64]`` (:issue:`8614`).


.. _whatsnew_0151.enhancements:

Enhancements
~~~~~~~~~~~~

- ``concat`` permits a wider variety of iterables of pandas objects to be
  passed as the first parameter (:issue:`8645`):

  .. ipython:: python

     from collections import deque

     df1 = pd.DataFrame([1, 2, 3])
     df2 = pd.DataFrame([4, 5, 6])

  previous behavior:

  .. code-block:: ipython

     In [7]: pd.concat(deque((df1, df2)))
     TypeError: first argument must be a list-like of pandas objects, you passed an object of type "deque"

  current behavior:

  .. ipython:: python

     pd.concat(deque((df1, df2)))

- Represent ``MultiIndex`` labels with a dtype that utilizes memory based on the level size. In prior versions, the memory usage was a constant 8 bytes per element in each level. In addition, in prior versions, the *reported* memory usage was incorrect as it didn't show the usage for the memory occupied by the underling data array. (:issue:`8456`)

  .. ipython:: python

     dfi = pd.DataFrame(
         1, index=pd.MultiIndex.from_product([["a"], range(1000)]), columns=["A"]
     )

  previous behavior:

  .. code-block:: ipython

     # this was underreported in prior versions
     In [1]: dfi.memory_usage(index=True)
     Out[1]:
     Index    8000 # took about 24008 bytes in < 0.15.1
     A        8000
     dtype: int64


  current behavior:

  .. ipython:: python

     dfi.memory_usage(index=True)

- Added Index properties ``is_monotonic_increasing`` and ``is_monotonic_decreasing`` (:issue:`8680`).

- Added option to select columns when importing Stata files (:issue:`7935`)

- Qualify memory usage in ``DataFrame.info()`` by adding ``+`` if it is a lower bound (:issue:`8578`)

- Raise errors in certain aggregation cases where an argument such as ``numeric_only`` is not handled (:issue:`8592`).

- Added support for 3-character ISO and non-standard country codes in :func:`io.wb.download()` (:issue:`8482`)

- World Bank data requests now will warn/raise based
  on an ``errors`` argument, as well as a list of hard-coded country codes and
  the World Bank's JSON response.  In prior versions, the error messages
  didn't look at the World Bank's JSON response.  Problem-inducing input were
  simply dropped prior to the request. The issue was that many good countries
  were cropped in the hard-coded approach.  All countries will work now, but
  some bad countries will raise exceptions because some edge cases break the
  entire response. (:issue:`8482`)

- Added option to ``Series.str.split()`` to return a ``DataFrame`` rather than a ``Series`` (:issue:`8428`)

- Added option to ``df.info(null_counts=None|True|False)`` to override the default display options and force showing of the null-counts (:issue:`8701`)


.. _whatsnew_0151.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in unpickling of a ``CustomBusinessDay`` object (:issue:`8591`)
- Bug in coercing ``Categorical`` to a records array, e.g. ``df.to_records()`` (:issue:`8626`)
- Bug in ``Categorical`` not created properly with ``Series.to_frame()`` (:issue:`8626`)
- Bug in coercing in astype of a ``Categorical`` of a passed ``pd.Categorical`` (this now raises ``TypeError`` correctly), (:issue:`8626`)
- Bug in ``cut``/``qcut`` when using ``Series`` and ``retbins=True`` (:issue:`8589`)
- Bug in writing Categorical columns to an SQL database with ``to_sql`` (:issue:`8624`).
- Bug in comparing ``Categorical`` of datetime raising when being compared to a scalar datetime (:issue:`8687`)
- Bug in selecting from a ``Categorical`` with ``.iloc`` (:issue:`8623`)
- Bug in groupby-transform with a Categorical (:issue:`8623`)
- Bug in duplicated/drop_duplicates with a Categorical (:issue:`8623`)
- Bug in ``Categorical`` reflected comparison operator raising if the first argument was a numpy array scalar (e.g. np.int64) (:issue:`8658`)
- Bug in Panel indexing with a list-like (:issue:`8710`)
- Compat issue is ``DataFrame.dtypes`` when ``options.mode.use_inf_as_null`` is True (:issue:`8722`)
- Bug in ``read_csv``, ``dialect`` parameter would not take a string (:issue:`8703`)
- Bug in slicing a MultiIndex level with an empty-list (:issue:`8737`)
- Bug in numeric index operations of add/sub with Float/Index Index with numpy arrays (:issue:`8608`)
- Bug in setitem with empty indexer and unwanted coercion of dtypes (:issue:`8669`)
- Bug in ix/loc block splitting on setitem (manifests with integer-like dtypes, e.g. datetime64) (:issue:`8607`)
- Bug when doing label based indexing with integers not found in the index for
  non-unique but monotonic indexes (:issue:`8680`).
- Bug when indexing a Float64Index with ``np.nan`` on numpy 1.7 (:issue:`8980`).
- Fix ``shape`` attribute for ``MultiIndex`` (:issue:`8609`)
- Bug in ``GroupBy`` where a name conflict between the grouper and columns
  would break ``groupby`` operations (:issue:`7115`, :issue:`8112`)
- Fixed a bug where plotting a column ``y`` and specifying a label would mutate the index name of the original DataFrame (:issue:`8494`)
- Fix regression in plotting of a DatetimeIndex directly with matplotlib (:issue:`8614`).
- Bug in ``date_range`` where partially-specified dates would incorporate current date (:issue:`6961`)
- Bug in Setting by indexer to a scalar value with a mixed-dtype ``Panel4d`` was failing (:issue:`8702`)
- Bug where ``DataReader``'s would fail if one of the symbols passed was invalid.  Now returns data for valid symbols and np.nan for invalid (:issue:`8494`)
- Bug in ``get_quote_yahoo`` that wouldn't allow non-float return values (:issue:`5229`).


.. _whatsnew_0.15.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.15.0..v0.15.1

.. _whatsnew_104:

What's new in 1.0.4 (May 28, 2020)
------------------------------------

These are the changes in pandas 1.0.4. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_104.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fix regression where :meth:`Series.isna` and :meth:`DataFrame.isna` would raise for categorical dtype when ``pandas.options.mode.use_inf_as_na`` was set to ``True`` (:issue:`33594`)
- Fix regression in :meth:`GroupBy.first` and :meth:`GroupBy.last` where None is not preserved in object dtype (:issue:`32800`)
- Fix regression in DataFrame reductions using ``numeric_only=True`` and ExtensionArrays (:issue:`33256`).
- Fix performance regression in ``memory_usage(deep=True)`` for object dtype (:issue:`33012`)
- Fix regression where :meth:`Categorical.replace` would replace with ``NaN`` whenever the new value and replacement value were equal (:issue:`33288`)
- Fix regression where an ordered :class:`Categorical` containing only ``NaN`` values would raise rather than returning ``NaN`` when taking the minimum or maximum  (:issue:`33450`)
- Fix regression in :meth:`DataFrameGroupBy.agg` with dictionary input losing ``ExtensionArray`` dtypes (:issue:`32194`)
- Fix to preserve the ability to index with the "nearest" method with xarray's CFTimeIndex, an :class:`Index` subclass (`pydata/xarray#3751 <https://github.com/pydata/xarray/issues/3751>`_, :issue:`32905`).
- Fix regression in :meth:`DataFrame.describe` raising ``TypeError: unhashable type: 'dict'`` (:issue:`32409`)
- Fix regression in :meth:`DataFrame.replace` casts columns to ``object`` dtype if items in ``to_replace`` not in values (:issue:`32988`)
- Fix regression in :meth:`Series.groupby` would raise ``ValueError`` when grouping by :class:`PeriodIndex` level (:issue:`34010`)
- Fix regression in :meth:`GroupBy.rolling.apply` ignores args and kwargs parameters (:issue:`33433`)
- Fix regression in error message with ``np.min`` or ``np.max`` on unordered :class:`Categorical` (:issue:`33115`)
- Fix regression in :meth:`DataFrame.loc` and :meth:`Series.loc` throwing an error when a ``datetime64[ns, tz]`` value is provided (:issue:`32395`)

.. _whatsnew_104.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in :meth:`SeriesGroupBy.first`, :meth:`SeriesGroupBy.last`, :meth:`SeriesGroupBy.min`, and :meth:`SeriesGroupBy.max` returning floats when applied to nullable Booleans (:issue:`33071`)
- Bug in :meth:`Rolling.min` and :meth:`Rolling.max`: Growing memory usage after multiple calls when using a fixed window (:issue:`30726`)
- Bug in :meth:`~DataFrame.to_parquet` was not raising ``PermissionError`` when writing to a private s3 bucket with invalid creds. (:issue:`27679`)
- Bug in :meth:`~DataFrame.to_csv` was silently failing when writing to an invalid s3 bucket. (:issue:`32486`)
- Bug in :meth:`read_parquet` was raising a ``FileNotFoundError`` when passed an s3 directory path. (:issue:`26388`)
- Bug in :meth:`~DataFrame.to_parquet` was throwing an ``AttributeError`` when writing a partitioned parquet file to s3 (:issue:`27596`)
- Bug in :meth:`GroupBy.quantile` causes the quantiles to be shifted when the ``by`` axis contains ``NaN`` (:issue:`33200`, :issue:`33569`)

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.3..v1.0.4
.. _whatsnew_0703:

Version 0.7.3 (April 12, 2012)
------------------------------

{{ header }}


This is a minor release from 0.7.2 and fixes many minor bugs and adds a number
of nice new features. There are also a couple of API changes to note; these
should not affect very many users, and we are inclined to call them "bug fixes"
even though they do constitute a change in behavior. See the :ref:`full release
notes <release>` or issue
tracker on GitHub for a complete list.

New features
~~~~~~~~~~~~

- New :ref:`fixed width file reader <io.fwf>`, ``read_fwf``
- New :ref:`scatter_matrix <visualization.scatter_matrix>` function for making
  a scatter plot matrix

.. code-block:: python

   from pandas.tools.plotting import scatter_matrix

   scatter_matrix(df, alpha=0.2)  # noqa F821


- Add ``stacked`` argument to Series and DataFrame's ``plot`` method for
  :ref:`stacked bar plots <visualization.barplot>`.

.. code-block:: python

   df.plot(kind="bar", stacked=True)  # noqa F821


.. code-block:: python

   df.plot(kind="barh", stacked=True)  # noqa F821


- Add log x and y :ref:`scaling options <visualization.basic>` to
  ``DataFrame.plot`` and ``Series.plot``
- Add ``kurt`` methods to Series and DataFrame for computing kurtosis


NA boolean comparison API change
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Reverted some changes to how NA values (represented typically as ``NaN`` or
``None``) are handled in non-numeric Series:

.. code-block:: ipython

   In [1]: series = pd.Series(["Steve", np.nan, "Joe"])

   In [2]: series == "Steve"
   Out[2]:
   0     True
   1    False
   2    False
   Length: 3, dtype: bool

   In [3]: series != "Steve"
   Out[3]:
   0    False
   1     True
   2     True
   Length: 3, dtype: bool

In comparisons, NA / NaN will always come through as ``False`` except with
``!=`` which is ``True``. *Be very careful* with boolean arithmetic, especially
negation, in the presence of NA data. You may wish to add an explicit NA
filter into boolean array operations if you are worried about this:

.. code-block:: ipython

   In [4]: mask = series == "Steve"

   In [5]: series[mask & series.notnull()]
   Out[5]:
   0    Steve
   Length: 1, dtype: object

While propagating NA in comparisons may seem like the right behavior to some
users (and you could argue on purely technical grounds that this is the right
thing to do), the evaluation was made that propagating NA everywhere, including
in numerical arrays, would cause a large amount of problems for users. Thus, a
"practicality beats purity" approach was taken. This issue may be revisited at
some point in the future.

Other API changes
~~~~~~~~~~~~~~~~~

When calling ``apply`` on a grouped Series, the return value will also be a
Series, to be more consistent with the ``groupby`` behavior with DataFrame:

.. code-block:: ipython

      In [6]: df = pd.DataFrame(
         ...:     {
         ...:         "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
         ...:         "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
         ...:         "C": np.random.randn(8),
         ...:         "D": np.random.randn(8),
         ...:     }
         ...: )
         ...:

      In [7]: df
      Out[7]:
         A      B         C         D
      0  foo    one  0.469112 -0.861849
      1  bar    one -0.282863 -2.104569
      2  foo    two -1.509059 -0.494929
      3  bar  three -1.135632  1.071804
      4  foo    two  1.212112  0.721555
      5  bar    two -0.173215 -0.706771
      6  foo    one  0.119209 -1.039575
      7  foo  three -1.044236  0.271860

      [8 rows x 4 columns]

      In [8]: grouped = df.groupby("A")["C"]

      In [9]: grouped.describe()
      Out[9]:
         count      mean       std       min       25%       50%       75%       max
      A
      bar    3.0 -0.530570  0.526860 -1.135632 -0.709248 -0.282863 -0.228039 -0.173215
      foo    5.0 -0.150572  1.113308 -1.509059 -1.044236  0.119209  0.469112  1.212112

      [2 rows x 8 columns]

      In [10]: grouped.apply(lambda x: x.sort_values()[-2:])  # top 2 values
      Out[10]:
      A
      bar  1   -0.282863
           5   -0.173215
      foo  0    0.469112
           4    1.212112
      Name: C, Length: 4, dtype: float64


.. _whatsnew_0.7.3.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.7.2..v0.7.3
.. _whatsnew_0240:

What's new in 0.24.0 (January 25, 2019)
---------------------------------------

.. warning::

   The 0.24.x series of releases will be the last to support Python 2. Future feature
   releases will support Python 3 only. See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more
   details.

{{ header }}

This is a major release from 0.23.4 and includes a number of API changes, new
features, enhancements, and performance improvements along with a large number
of bug fixes.

Highlights include:

* :ref:`Optional Integer NA Support <whatsnew_0240.enhancements.intna>`
* :ref:`New APIs for accessing the array backing a Series or Index <whatsnew_0240.values_api>`
* :ref:`A new top-level method for creating arrays <whatsnew_0240.enhancements.array>`
* :ref:`Store Interval and Period data in a Series or DataFrame <whatsnew_0240.enhancements.interval>`
* :ref:`Support for joining on two MultiIndexes <whatsnew_0240.enhancements.join_with_two_multiindexes>`


Check the :ref:`API Changes <whatsnew_0240.api_breaking>` and :ref:`deprecations <whatsnew_0240.deprecations>` before updating.

These are the changes in pandas 0.24.0. See :ref:`release` for a full changelog
including other versions of pandas.


Enhancements
~~~~~~~~~~~~

.. _whatsnew_0240.enhancements.intna:

Optional integer NA support
^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas has gained the ability to hold integer dtypes with missing values. This long requested feature is enabled through the use of :ref:`extension types <extending.extension-types>`.

.. note::

   IntegerArray is currently experimental. Its API or implementation may
   change without warning.

We can construct a ``Series`` with the specified dtype. The dtype string ``Int64`` is a pandas ``ExtensionDtype``. Specifying a list or array using the traditional missing value
marker of ``np.nan`` will infer to integer dtype. The display of the ``Series`` will also use the ``NaN`` to indicate missing values in string outputs. (:issue:`20700`, :issue:`20747`, :issue:`22441`, :issue:`21789`, :issue:`22346`)

.. ipython:: python

   s = pd.Series([1, 2, np.nan], dtype='Int64')
   s


Operations on these dtypes will propagate ``NaN`` as other pandas operations.

.. ipython:: python

   # arithmetic
   s + 1

   # comparison
   s == 1

   # indexing
   s.iloc[1:3]

   # operate with other dtypes
   s + s.iloc[1:3].astype('Int8')

   # coerce when needed
   s + 0.01

These dtypes can operate as part of a ``DataFrame``.

.. ipython:: python

   df = pd.DataFrame({'A': s, 'B': [1, 1, 3], 'C': list('aab')})
   df
   df.dtypes


These dtypes can be merged, reshaped, and casted.

.. ipython:: python

   pd.concat([df[['A']], df[['B', 'C']]], axis=1).dtypes
   df['A'].astype(float)

Reduction and groupby operations such as ``sum`` work.

.. ipython:: python

   df.sum()
   df.groupby('B').A.sum()

.. warning::

   The Integer NA support currently uses the capitalized dtype version, e.g. ``Int8`` as compared to the traditional ``int8``. This may be changed at a future date.

See :ref:`integer_na` for more.


.. _whatsnew_0240.values_api:

Accessing the values in a Series or Index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:attr:`Series.array` and :attr:`Index.array` have been added for extracting the array backing a
``Series`` or ``Index``. (:issue:`19954`, :issue:`23623`)

.. ipython:: python

   idx = pd.period_range('2000', periods=4)
   idx.array
   pd.Series(idx).array

Historically, this would have been done with ``series.values``, but with
``.values`` it was unclear whether the returned value would be the actual array,
some transformation of it, or one of pandas custom arrays (like
``Categorical``). For example, with :class:`PeriodIndex`, ``.values`` generates
a new ndarray of period objects each time.

.. ipython:: python

   idx.values
   id(idx.values)
   id(idx.values)

If you need an actual NumPy array, use :meth:`Series.to_numpy` or :meth:`Index.to_numpy`.

.. ipython:: python

   idx.to_numpy()
   pd.Series(idx).to_numpy()

For Series and Indexes backed by normal NumPy arrays, :attr:`Series.array` will return a
new :class:`arrays.PandasArray`, which is a thin (no-copy) wrapper around a
:class:`numpy.ndarray`. :class:`~arrays.PandasArray` isn't especially useful on its own,
but it does provide the same interface as any extension array defined in pandas or by
a third-party library.

.. ipython:: python

   ser = pd.Series([1, 2, 3])
   ser.array
   ser.to_numpy()

We haven't removed or deprecated :attr:`Series.values` or :attr:`DataFrame.values`, but we
highly recommend and using ``.array`` or ``.to_numpy()`` instead.

See :ref:`Dtypes <basics.dtypes>` and :ref:`Attributes and Underlying Data <basics.attrs>` for more.


.. _whatsnew_0240.enhancements.array:

``pandas.array``: a new top-level method for creating arrays
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A new top-level method :func:`array` has been added for creating 1-dimensional arrays (:issue:`22860`).
This can be used to create any :ref:`extension array <extending.extension-types>`, including
extension arrays registered by :ref:`3rd party libraries <ecosystem.extensions>`.
See the :ref:`dtypes docs <basics.dtypes>` for more on extension arrays.

.. ipython:: python

   pd.array([1, 2, np.nan], dtype='Int64')
   pd.array(['a', 'b', 'c'], dtype='category')

Passing data for which there isn't dedicated extension type (e.g. float, integer, etc.)
will return a new :class:`arrays.PandasArray`, which is just a thin (no-copy)
wrapper around a :class:`numpy.ndarray` that satisfies the pandas extension array interface.

.. ipython:: python

   pd.array([1, 2, 3])

On their own, a :class:`~arrays.PandasArray` isn't a very useful object.
But if you need write low-level code that works generically for any
:class:`~pandas.api.extensions.ExtensionArray`, :class:`~arrays.PandasArray`
satisfies that need.

Notice that by default, if no ``dtype`` is specified, the dtype of the returned
array is inferred from the data. In particular, note that the first example of
``[1, 2, np.nan]`` would have returned a floating-point array, since ``NaN``
is a float.

.. ipython:: python

   pd.array([1, 2, np.nan])


.. _whatsnew_0240.enhancements.interval:

Storing Interval and Period data in Series and DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Interval` and :class:`Period` data may now be stored in a :class:`Series` or :class:`DataFrame`, in addition to an
:class:`IntervalIndex` and :class:`PeriodIndex` like previously (:issue:`19453`, :issue:`22862`).

.. ipython:: python

   ser = pd.Series(pd.interval_range(0, 5))
   ser
   ser.dtype

For periods:

.. ipython:: python

   pser = pd.Series(pd.period_range("2000", freq="D", periods=5))
   pser
   pser.dtype

Previously, these would be cast to a NumPy array with object dtype. In general,
this should result in better performance when storing an array of intervals or periods
in a :class:`Series` or column of a :class:`DataFrame`.

Use :attr:`Series.array` to extract the underlying array of intervals or periods
from the ``Series``:

.. ipython:: python

   ser.array
   pser.array

These return an instance of :class:`arrays.IntervalArray` or :class:`arrays.PeriodArray`,
the new extension arrays that back interval and period data.

.. warning::

   For backwards compatibility, :attr:`Series.values` continues to return
   a NumPy array of objects for Interval and Period data. We recommend
   using :attr:`Series.array` when you need the array of data stored in the
   ``Series``, and :meth:`Series.to_numpy` when you know you need a NumPy array.

   See :ref:`Dtypes <basics.dtypes>` and :ref:`Attributes and Underlying Data <basics.attrs>`
   for more.


.. _whatsnew_0240.enhancements.join_with_two_multiindexes:

Joining with two multi-indexes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`DataFrame.merge` and :func:`DataFrame.join` can now be used to join multi-indexed ``Dataframe`` instances on the overlapping index levels (:issue:`6360`)

See the :ref:`Merge, join, and concatenate
<merging.Join_with_two_multi_indexes>` documentation section.

.. ipython:: python

   index_left = pd.MultiIndex.from_tuples([('K0', 'X0'), ('K0', 'X1'),
                                          ('K1', 'X2')],
                                          names=['key', 'X'])

   left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],
                        'B': ['B0', 'B1', 'B2']}, index=index_left)

   index_right = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'),
                                           ('K2', 'Y2'), ('K2', 'Y3')],
                                           names=['key', 'Y'])

   right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],
                         'D': ['D0', 'D1', 'D2', 'D3']}, index=index_right)

   left.join(right)

For earlier versions this can be done using the following.

.. ipython:: python

   pd.merge(left.reset_index(), right.reset_index(),
            on=['key'], how='inner').set_index(['key', 'X', 'Y'])

.. _whatsnew_0240.enhancements.read_html:

Function ``read_html`` enhancements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`read_html` previously ignored ``colspan`` and ``rowspan`` attributes.
Now it understands them, treating them as sequences of cells with the same
value. (:issue:`17054`)

.. ipython:: python

    result = pd.read_html("""
      <table>
        <thead>
          <tr>
            <th>A</th><th>B</th><th>C</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="2">1</td><td>2</td>
          </tr>
        </tbody>
      </table>""")

*Previous behavior*:

.. code-block:: ipython

    In [13]: result
    Out [13]:
    [   A  B   C
     0  1  2 NaN]

*New behavior*:

.. ipython:: python

    result


New ``Styler.pipe()`` method
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
The :class:`~pandas.io.formats.style.Styler` class has gained a
:meth:`~pandas.io.formats.style.Styler.pipe` method.  This provides a
convenient way to apply users' predefined styling functions, and can help reduce
"boilerplate" when using DataFrame styling functionality repeatedly within a notebook. (:issue:`23229`)

.. ipython:: python

    df = pd.DataFrame({'N': [1250, 1500, 1750], 'X': [0.25, 0.35, 0.50]})

    def format_and_align(styler):
        return (styler.format({'N': '{:,}', 'X': '{:.1%}'})
                      .set_properties(**{'text-align': 'right'}))

    df.style.pipe(format_and_align).set_caption('Summary of results.')

Similar methods already exist for other classes in pandas, including :meth:`DataFrame.pipe`,
:meth:`GroupBy.pipe() <pandas.core.groupby.GroupBy.pipe>`, and :meth:`Resampler.pipe() <pandas.core.resample.Resampler.pipe>`.

.. _whatsnew_0240.enhancements.rename_axis:

Renaming names in a MultiIndex
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`DataFrame.rename_axis` now supports ``index`` and ``columns`` arguments
and :func:`Series.rename_axis` supports ``index`` argument (:issue:`19978`).

This change allows a dictionary to be passed so that some of the names
of a ``MultiIndex`` can be changed.

Example:

.. ipython:: python

    mi = pd.MultiIndex.from_product([list('AB'), list('CD'), list('EF')],
                                    names=['AB', 'CD', 'EF'])
    df = pd.DataFrame(list(range(len(mi))), index=mi, columns=['N'])
    df
    df.rename_axis(index={'CD': 'New'})

See the :ref:`Advanced documentation on renaming<advanced.index_names>` for more details.

.. _whatsnew_0240.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- :func:`merge` now directly allows merge between objects of type ``DataFrame`` and named ``Series``, without the need to convert the ``Series`` object into a ``DataFrame`` beforehand (:issue:`21220`)
- ``ExcelWriter`` now accepts ``mode`` as a keyword argument, enabling append to existing workbooks when using the ``openpyxl`` engine (:issue:`3441`)
- ``FrozenList`` has gained the ``.union()`` and ``.difference()`` methods. This functionality greatly simplifies groupby's that rely on explicitly excluding certain columns. See :ref:`Splitting an object into groups <groupby.split>` for more information (:issue:`15475`, :issue:`15506`).
- :func:`DataFrame.to_parquet` now accepts ``index`` as an argument, allowing
  the user to override the engine's default behavior to include or omit the
  dataframe's indexes from the resulting Parquet file. (:issue:`20768`)
- :func:`read_feather` now accepts ``columns`` as an argument, allowing the user to specify which columns should be read. (:issue:`24025`)
- :meth:`DataFrame.corr` and :meth:`Series.corr` now accept a callable for generic calculation methods of correlation, e.g. histogram intersection (:issue:`22684`)
- :func:`DataFrame.to_string` now accepts ``decimal`` as an argument, allowing the user to specify which decimal separator should be used in the output. (:issue:`23614`)
- :func:`DataFrame.to_html` now accepts ``render_links`` as an argument, allowing the user to generate HTML with links to any URLs that appear in the DataFrame.
  See the :ref:`section on writing HTML <io.html>` in the IO docs for example usage. (:issue:`2679`)
- :func:`pandas.read_csv` now supports pandas extension types as an argument to ``dtype``, allowing the user to use pandas extension types when reading CSVs. (:issue:`23228`)
- The :meth:`~DataFrame.shift` method now accepts ``fill_value`` as an argument, allowing the user to specify a value which will be used instead of NA/NaT in the empty periods. (:issue:`15486`)
- :func:`to_datetime` now supports the ``%Z`` and ``%z`` directive when passed into ``format`` (:issue:`13486`)
- :func:`Series.mode` and :func:`DataFrame.mode` now support the ``dropna`` parameter which can be used to specify whether ``NaN``/``NaT`` values should be considered (:issue:`17534`)
- :func:`DataFrame.to_csv` and :func:`Series.to_csv` now support the ``compression`` keyword when a file handle is passed. (:issue:`21227`)
- :meth:`Index.droplevel` is now implemented also for flat indexes, for compatibility with :class:`MultiIndex` (:issue:`21115`)
- :meth:`Series.droplevel` and :meth:`DataFrame.droplevel` are now implemented (:issue:`20342`)
- Added support for reading from/writing to Google Cloud Storage via the ``gcsfs`` library (:issue:`19454`, :issue:`23094`)
- :func:`DataFrame.to_gbq` and :func:`read_gbq` signature and documentation updated to
  reflect changes from the `pandas-gbq library version 0.8.0
  <https://pandas-gbq.readthedocs.io/en/latest/changelog.html#changelog-0-8-0>`__.
  Adds a ``credentials`` argument, which enables the use of any kind of
  `google-auth credentials
  <https://google-auth.readthedocs.io/en/latest/>`__. (:issue:`21627`,
  :issue:`22557`, :issue:`23662`)
- New method :meth:`HDFStore.walk` will recursively walk the group hierarchy of an HDF5 file (:issue:`10932`)
- :func:`read_html` copies cell data across ``colspan`` and ``rowspan``, and it treats all-``th`` table rows as headers if ``header`` kwarg is not given and there is no ``thead`` (:issue:`17054`)
- :meth:`Series.nlargest`, :meth:`Series.nsmallest`, :meth:`DataFrame.nlargest`, and :meth:`DataFrame.nsmallest` now accept the value ``"all"`` for the ``keep`` argument. This keeps all ties for the nth largest/smallest value (:issue:`16818`)
- :class:`IntervalIndex` has gained the :meth:`~IntervalIndex.set_closed` method to change the existing ``closed`` value (:issue:`21670`)
- :func:`~DataFrame.to_csv`, :func:`~Series.to_csv`, :func:`~DataFrame.to_json`, and :func:`~Series.to_json` now support ``compression='infer'`` to infer compression based on filename extension (:issue:`15008`).
  The default compression for ``to_csv``, ``to_json``, and ``to_pickle`` methods has been updated to ``'infer'`` (:issue:`22004`).
- :meth:`DataFrame.to_sql` now supports writing ``TIMESTAMP WITH TIME ZONE`` types for supported databases. For databases that don't support timezones, datetime data will be stored as timezone unaware local timestamps. See the :ref:`io.sql_datetime_data` for implications (:issue:`9086`).
- :func:`to_timedelta` now supports iso-formatted timedelta strings (:issue:`21877`)
- :class:`Series` and :class:`DataFrame` now support :class:`Iterable` objects in the constructor (:issue:`2193`)
- :class:`DatetimeIndex` has gained the :attr:`DatetimeIndex.timetz` attribute. This returns the local time with timezone information. (:issue:`21358`)
- :meth:`~Timestamp.round`, :meth:`~Timestamp.ceil`, and :meth:`~Timestamp.floor` for :class:`DatetimeIndex` and :class:`Timestamp`
  now support an ``ambiguous`` argument for handling datetimes that are rounded to ambiguous times (:issue:`18946`)
  and a ``nonexistent`` argument for handling datetimes that are rounded to nonexistent times. See :ref:`timeseries.timezone_nonexistent` (:issue:`22647`)
- The result of :meth:`~DataFrame.resample` is now iterable similar to ``groupby()`` (:issue:`15314`).
- :meth:`Series.resample` and :meth:`DataFrame.resample` have gained the :meth:`pandas.core.resample.Resampler.quantile` (:issue:`15023`).
- :meth:`DataFrame.resample` and :meth:`Series.resample` with a :class:`PeriodIndex` will now respect the ``base`` argument in the same fashion as with a :class:`DatetimeIndex`. (:issue:`23882`)
- :meth:`pandas.api.types.is_list_like` has gained a keyword ``allow_sets`` which is ``True`` by default; if ``False``,
  all instances of ``set`` will not be considered "list-like" anymore (:issue:`23061`)
- :meth:`Index.to_frame` now supports overriding column name(s) (:issue:`22580`).
- :meth:`Categorical.from_codes` now can take a ``dtype`` parameter as an alternative to passing ``categories`` and ``ordered`` (:issue:`24398`).
- New attribute ``__git_version__`` will return git commit sha of current build (:issue:`21295`).
- Compatibility with Matplotlib 3.0 (:issue:`22790`).
- Added :meth:`Interval.overlaps`, :meth:`arrays.IntervalArray.overlaps`, and :meth:`IntervalIndex.overlaps` for determining overlaps between interval-like objects (:issue:`21998`)
- :func:`read_fwf` now accepts keyword ``infer_nrows`` (:issue:`15138`).
- :func:`~DataFrame.to_parquet` now supports writing a ``DataFrame`` as a directory of parquet files partitioned by a subset of the columns when ``engine = 'pyarrow'`` (:issue:`23283`)
- :meth:`Timestamp.tz_localize`, :meth:`DatetimeIndex.tz_localize`, and :meth:`Series.tz_localize` have gained the ``nonexistent`` argument for alternative handling of nonexistent times. See :ref:`timeseries.timezone_nonexistent` (:issue:`8917`, :issue:`24466`)
- :meth:`Index.difference`, :meth:`Index.intersection`, :meth:`Index.union`, and :meth:`Index.symmetric_difference` now have an optional ``sort`` parameter to control whether the results should be sorted if possible (:issue:`17839`, :issue:`24471`)
- :meth:`read_excel()` now accepts ``usecols`` as a list of column names or callable (:issue:`18273`)
- :meth:`MultiIndex.to_flat_index` has been added to flatten multiple levels into a single-level :class:`Index` object.
- :meth:`DataFrame.to_stata` and :class:`pandas.io.stata.StataWriter117` can write mixed string columns to Stata strl format (:issue:`23633`)
- :meth:`DataFrame.between_time` and :meth:`DataFrame.at_time` have gained the ``axis`` parameter (:issue:`8839`)
- :meth:`DataFrame.to_records` now accepts ``index_dtypes`` and ``column_dtypes`` parameters to allow different data types in stored column and index records (:issue:`18146`)
- :class:`IntervalIndex` has gained the :attr:`~IntervalIndex.is_overlapping` attribute to indicate if the ``IntervalIndex`` contains any overlapping intervals (:issue:`23309`)
- :func:`pandas.DataFrame.to_sql` has gained the ``method`` argument to control SQL insertion clause. See the :ref:`insertion method <io.sql.method>` section in the documentation. (:issue:`8953`)
- :meth:`DataFrame.corrwith` now supports Spearman's rank correlation, Kendall's tau as well as callable correlation methods. (:issue:`21925`)
- :meth:`DataFrame.to_json`, :meth:`DataFrame.to_csv`, :meth:`DataFrame.to_pickle`, and other export methods now support tilde(~) in path argument. (:issue:`23473`)

.. _whatsnew_0240.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

pandas 0.24.0 includes a number of API breaking changes.


.. _whatsnew_0240.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We have updated our minimum supported versions of dependencies (:issue:`21242`, :issue:`18742`, :issue:`23774`, :issue:`24767`).
If installed, we now require:

+-----------------+-----------------+----------+
| Package         | Minimum Version | Required |
+=================+=================+==========+
| numpy           | 1.12.0          |    X     |
+-----------------+-----------------+----------+
| bottleneck      | 1.2.0           |          |
+-----------------+-----------------+----------+
| fastparquet     | 0.2.1           |          |
+-----------------+-----------------+----------+
| matplotlib      | 2.0.0           |          |
+-----------------+-----------------+----------+
| numexpr         | 2.6.1           |          |
+-----------------+-----------------+----------+
| pandas-gbq      | 0.8.0           |          |
+-----------------+-----------------+----------+
| pyarrow         | 0.9.0           |          |
+-----------------+-----------------+----------+
| pytables        | 3.4.2           |          |
+-----------------+-----------------+----------+
| scipy           | 0.18.1          |          |
+-----------------+-----------------+----------+
| xlrd            | 1.0.0           |          |
+-----------------+-----------------+----------+
| pytest (dev)    | 3.6             |          |
+-----------------+-----------------+----------+

Additionally we no longer depend on ``feather-format`` for feather based storage
and replaced it with references to ``pyarrow`` (:issue:`21639` and :issue:`23053`).

.. _whatsnew_0240.api_breaking.csv_line_terminator:

``os.linesep`` is used for ``line_terminator`` of ``DataFrame.to_csv``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`DataFrame.to_csv` now uses :func:`os.linesep` rather than ``'\n'``
for the default line terminator (:issue:`20353`).
This change only affects when running on Windows, where ``'\r\n'`` was used for line terminator
even when ``'\n'`` was passed in ``line_terminator``.

*Previous behavior* on Windows:

.. code-block:: ipython

    In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],
       ...:                      "string_with_crlf": ["a\r\nbc"]})

    In [2]: # When passing file PATH to to_csv,
       ...: # line_terminator does not work, and csv is saved with '\r\n'.
       ...: # Also, this converts all '\n's in the data to '\r\n'.
       ...: data.to_csv("test.csv", index=False, line_terminator='\n')

    In [3]: with open("test.csv", mode='rb') as f:
       ...:     print(f.read())
    Out[3]: b'string_with_lf,string_with_crlf\r\n"a\r\nbc","a\r\r\nbc"\r\n'

    In [4]: # When passing file OBJECT with newline option to
       ...: # to_csv, line_terminator works.
       ...: with open("test2.csv", mode='w', newline='\n') as f:
       ...:     data.to_csv(f, index=False, line_terminator='\n')

    In [5]: with open("test2.csv", mode='rb') as f:
       ...:     print(f.read())
    Out[5]: b'string_with_lf,string_with_crlf\n"a\nbc","a\r\nbc"\n'


*New behavior* on Windows:

Passing ``line_terminator`` explicitly, set the ``line terminator`` to that character.

.. code-block:: ipython

   In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],
      ...:                      "string_with_crlf": ["a\r\nbc"]})

   In [2]: data.to_csv("test.csv", index=False, line_terminator='\n')

   In [3]: with open("test.csv", mode='rb') as f:
      ...:     print(f.read())
   Out[3]: b'string_with_lf,string_with_crlf\n"a\nbc","a\r\nbc"\n'


On Windows, the value of ``os.linesep`` is ``'\r\n'``, so if ``line_terminator`` is not
set, ``'\r\n'`` is used for line terminator.

.. code-block:: ipython

   In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],
      ...:                      "string_with_crlf": ["a\r\nbc"]})

   In [2]: data.to_csv("test.csv", index=False)

   In [3]: with open("test.csv", mode='rb') as f:
      ...:     print(f.read())
   Out[3]: b'string_with_lf,string_with_crlf\r\n"a\nbc","a\r\nbc"\r\n'


For file objects, specifying ``newline`` is not sufficient to set the line terminator.
You must pass in the ``line_terminator`` explicitly, even in this case.

.. code-block:: ipython

   In [1]: data = pd.DataFrame({"string_with_lf": ["a\nbc"],
      ...:                      "string_with_crlf": ["a\r\nbc"]})

   In [2]: with open("test2.csv", mode='w', newline='\n') as f:
      ...:     data.to_csv(f, index=False)

   In [3]: with open("test2.csv", mode='rb') as f:
      ...:     print(f.read())
   Out[3]: b'string_with_lf,string_with_crlf\r\n"a\nbc","a\r\nbc"\r\n'

.. _whatsnew_0240.bug_fixes.nan_with_str_dtype:

Proper handling of ``np.NaN`` in a string data-typed column with the Python engine
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

There was bug in :func:`read_excel` and :func:`read_csv` with the Python
engine, where missing values turned to ``'nan'`` with ``dtype=str`` and
``na_filter=True``. Now, these missing values are converted to the string
missing indicator, ``np.nan``. (:issue:`20377`)

.. ipython:: python
   :suppress:

   from io import StringIO

*Previous behavior*:

.. code-block:: ipython

   In [5]: data = 'a,b,c\n1,,3\n4,5,6'
   In [6]: df = pd.read_csv(StringIO(data), engine='python', dtype=str, na_filter=True)
   In [7]: df.loc[0, 'b']
   Out[7]:
   'nan'

*New behavior*:

.. ipython:: python

   data = 'a,b,c\n1,,3\n4,5,6'
   df = pd.read_csv(StringIO(data), engine='python', dtype=str, na_filter=True)
   df.loc[0, 'b']

Notice how we now instead output ``np.nan`` itself instead of a stringified form of it.

.. _whatsnew_0240.api.timezone_offset_parsing:

Parsing datetime strings with timezone offsets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, parsing datetime strings with UTC offsets with :func:`to_datetime`
or :class:`DatetimeIndex` would automatically convert the datetime to UTC
without timezone localization. This is inconsistent from parsing the same
datetime string with :class:`Timestamp` which would preserve the UTC
offset in the ``tz`` attribute. Now, :func:`to_datetime` preserves the UTC
offset in the ``tz`` attribute when all the datetime strings have the same
UTC offset (:issue:`17697`, :issue:`11736`, :issue:`22457`)

*Previous behavior*:

.. code-block:: ipython

    In [2]: pd.to_datetime("2015-11-18 15:30:00+05:30")
    Out[2]: Timestamp('2015-11-18 10:00:00')

    In [3]: pd.Timestamp("2015-11-18 15:30:00+05:30")
    Out[3]: Timestamp('2015-11-18 15:30:00+0530', tz='pytz.FixedOffset(330)')

    # Different UTC offsets would automatically convert the datetimes to UTC (without a UTC timezone)
    In [4]: pd.to_datetime(["2015-11-18 15:30:00+05:30", "2015-11-18 16:30:00+06:30"])
    Out[4]: DatetimeIndex(['2015-11-18 10:00:00', '2015-11-18 10:00:00'], dtype='datetime64[ns]', freq=None)

*New behavior*:

.. ipython:: python

    pd.to_datetime("2015-11-18 15:30:00+05:30")
    pd.Timestamp("2015-11-18 15:30:00+05:30")

Parsing datetime strings with the same UTC offset will preserve the UTC offset in the ``tz``

.. ipython:: python

    pd.to_datetime(["2015-11-18 15:30:00+05:30"] * 2)

Parsing datetime strings with different UTC offsets will now create an Index of
``datetime.datetime`` objects with different UTC offsets

.. ipython:: python

    idx = pd.to_datetime(["2015-11-18 15:30:00+05:30",
                          "2015-11-18 16:30:00+06:30"])
    idx
    idx[0]
    idx[1]

Passing ``utc=True`` will mimic the previous behavior but will correctly indicate
that the dates have been converted to UTC

.. ipython:: python

    pd.to_datetime(["2015-11-18 15:30:00+05:30",
                    "2015-11-18 16:30:00+06:30"], utc=True)


.. _whatsnew_0240.api_breaking.read_csv_mixed_tz:

Parsing mixed-timezones with :func:`read_csv`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`read_csv` no longer silently converts mixed-timezone columns to UTC (:issue:`24987`).

*Previous behavior*

.. code-block:: python

   >>> import io
   >>> content = """\
   ... a
   ... 2000-01-01T00:00:00+05:00
   ... 2000-01-01T00:00:00+06:00"""
   >>> df = pd.read_csv(io.StringIO(content), parse_dates=['a'])
   >>> df.a
   0   1999-12-31 19:00:00
   1   1999-12-31 18:00:00
   Name: a, dtype: datetime64[ns]

*New behavior*

.. ipython:: python

   import io
   content = """\
   a
   2000-01-01T00:00:00+05:00
   2000-01-01T00:00:00+06:00"""
   df = pd.read_csv(io.StringIO(content), parse_dates=['a'])
   df.a

As can be seen, the ``dtype`` is object; each value in the column is a string.
To convert the strings to an array of datetimes, the ``date_parser`` argument

.. ipython:: python

   df = pd.read_csv(io.StringIO(content), parse_dates=['a'],
                    date_parser=lambda col: pd.to_datetime(col, utc=True))
   df.a

See :ref:`whatsnew_0240.api.timezone_offset_parsing` for more.

.. _whatsnew_0240.api_breaking.period_end_time:

Time values in ``dt.end_time`` and ``to_timestamp(how='end')``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The time values in :class:`Period` and :class:`PeriodIndex` objects are now set
to '23:59:59.999999999' when calling :attr:`Series.dt.end_time`, :attr:`Period.end_time`,
:attr:`PeriodIndex.end_time`, :func:`Period.to_timestamp()` with ``how='end'``,
or :func:`PeriodIndex.to_timestamp()` with ``how='end'`` (:issue:`17157`)

*Previous behavior*:

.. code-block:: ipython

   In [2]: p = pd.Period('2017-01-01', 'D')
   In [3]: pi = pd.PeriodIndex([p])

   In [4]: pd.Series(pi).dt.end_time[0]
   Out[4]: Timestamp(2017-01-01 00:00:00)

   In [5]: p.end_time
   Out[5]: Timestamp(2017-01-01 23:59:59.999999999)

*New behavior*:

Calling :attr:`Series.dt.end_time` will now result in a time of '23:59:59.999999999' as
is the case with :attr:`Period.end_time`, for example

.. ipython:: python

   p = pd.Period('2017-01-01', 'D')
   pi = pd.PeriodIndex([p])

   pd.Series(pi).dt.end_time[0]

   p.end_time

.. _whatsnew_0240.api_breaking.datetime_unique:

Series.unique for timezone-aware data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The return type of :meth:`Series.unique` for datetime with timezone values has changed
from an :class:`numpy.ndarray` of :class:`Timestamp` objects to a :class:`arrays.DatetimeArray` (:issue:`24024`).

.. ipython:: python

   ser = pd.Series([pd.Timestamp('2000', tz='UTC'),
                    pd.Timestamp('2000', tz='UTC')])

*Previous behavior*:

.. code-block:: ipython

   In [3]: ser.unique()
   Out[3]: array([Timestamp('2000-01-01 00:00:00+0000', tz='UTC')], dtype=object)


*New behavior*:

.. ipython:: python

   ser.unique()


.. _whatsnew_0240.api_breaking.sparse_values:

Sparse data structure refactor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``SparseArray``, the array backing ``SparseSeries`` and the columns in a ``SparseDataFrame``,
is now an extension array (:issue:`21978`, :issue:`19056`, :issue:`22835`).
To conform to this interface and for consistency with the rest of pandas, some API breaking
changes were made:

- ``SparseArray`` is no longer a subclass of :class:`numpy.ndarray`. To convert a ``SparseArray`` to a NumPy array, use :func:`numpy.asarray`.
- ``SparseArray.dtype`` and ``SparseSeries.dtype`` are now instances of :class:`SparseDtype`, rather than ``np.dtype``. Access the underlying dtype with ``SparseDtype.subtype``.
- ``numpy.asarray(sparse_array)`` now returns a dense array with all the values, not just the non-fill-value values (:issue:`14167`)
- ``SparseArray.take`` now matches the API of :meth:`pandas.api.extensions.ExtensionArray.take` (:issue:`19506`):

  * The default value of ``allow_fill`` has changed from ``False`` to ``True``.
  * The ``out`` and ``mode`` parameters are now longer accepted (previously, this raised if they were specified).
  * Passing a scalar for ``indices`` is no longer allowed.

- The result of :func:`concat` with a mix of sparse and dense Series is a Series with sparse values, rather than a ``SparseSeries``.
- ``SparseDataFrame.combine`` and ``DataFrame.combine_first`` no longer supports combining a sparse column with a dense column while preserving the sparse subtype. The result will be an object-dtype SparseArray.
- Setting :attr:`SparseArray.fill_value` to a fill value with a different dtype is now allowed.
- ``DataFrame[column]`` is now a :class:`Series` with sparse values, rather than a :class:`SparseSeries`, when slicing a single column with sparse values (:issue:`23559`).
- The result of :meth:`Series.where` is now a ``Series`` with sparse values, like with other extension arrays (:issue:`24077`)

Some new warnings are issued for operations that require or are likely to materialize a large dense array:

- A :class:`errors.PerformanceWarning` is issued when using fillna with a ``method``, as a dense array is constructed to create the filled array. Filling with a ``value`` is the efficient way to fill a sparse array.
- A :class:`errors.PerformanceWarning` is now issued when concatenating sparse Series with differing fill values. The fill value from the first sparse array continues to be used.

In addition to these API breaking changes, many :ref:`Performance Improvements and Bug Fixes have been made <whatsnew_0240.bug_fixes.sparse>`.

Finally, a ``Series.sparse`` accessor was added to provide sparse-specific methods like :meth:`Series.sparse.from_coo`.

.. ipython:: python

   s = pd.Series([0, 0, 1, 1, 1], dtype='Sparse[int]')
   s.sparse.density

.. _whatsnew_0240.api_breaking.get_dummies:

:meth:`get_dummies` always returns a DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, when ``sparse=True`` was passed to :func:`get_dummies`, the return value could be either
a :class:`DataFrame` or a :class:`SparseDataFrame`, depending on whether all or a just a subset
of the columns were dummy-encoded. Now, a :class:`DataFrame` is always returned (:issue:`24284`).

*Previous behavior*

The first :func:`get_dummies` returns a :class:`DataFrame` because the column ``A``
is not dummy encoded. When just ``["B", "C"]`` are passed to ``get_dummies``,
then all the columns are dummy-encoded, and a :class:`SparseDataFrame` was returned.

.. code-block:: ipython

   In [2]: df = pd.DataFrame({"A": [1, 2], "B": ['a', 'b'], "C": ['a', 'a']})

   In [3]: type(pd.get_dummies(df, sparse=True))
   Out[3]: pandas.core.frame.DataFrame

   In [4]: type(pd.get_dummies(df[['B', 'C']], sparse=True))
   Out[4]: pandas.core.sparse.frame.SparseDataFrame

.. ipython:: python
   :suppress:

   df = pd.DataFrame({"A": [1, 2], "B": ['a', 'b'], "C": ['a', 'a']})

*New behavior*

Now, the return type is consistently a :class:`DataFrame`.

.. ipython:: python

   type(pd.get_dummies(df, sparse=True))
   type(pd.get_dummies(df[['B', 'C']], sparse=True))

.. note::

   There's no difference in memory usage between a :class:`SparseDataFrame`
   and a :class:`DataFrame` with sparse values. The memory usage will
   be the same as in the previous version of pandas.

.. _whatsnew_0240.api_breaking.frame_to_dict_index_orient:

Raise ValueError in ``DataFrame.to_dict(orient='index')``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Bug in :func:`DataFrame.to_dict` raises ``ValueError`` when used with
``orient='index'`` and a non-unique index instead of losing data (:issue:`22801`)

.. ipython:: python
    :okexcept:

    df = pd.DataFrame({'a': [1, 2], 'b': [0.5, 0.75]}, index=['A', 'A'])
    df

    df.to_dict(orient='index')

.. _whatsnew_0240.api.datetimelike.normalize:

Tick DateOffset normalize restrictions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Creating a ``Tick`` object (:class:`Day`, :class:`Hour`, :class:`Minute`,
:class:`Second`, :class:`Milli`, :class:`Micro`, :class:`Nano`) with
``normalize=True`` is no longer supported.  This prevents unexpected behavior
where addition could fail to be monotone or associative.  (:issue:`21427`)

*Previous behavior*:

.. code-block:: ipython


   In [2]: ts = pd.Timestamp('2018-06-11 18:01:14')

   In [3]: ts
   Out[3]: Timestamp('2018-06-11 18:01:14')

   In [4]: tic = pd.offsets.Hour(n=2, normalize=True)
      ...:

   In [5]: tic
   Out[5]: <2 * Hours>

   In [6]: ts + tic
   Out[6]: Timestamp('2018-06-11 00:00:00')

   In [7]: ts + tic + tic + tic == ts + (tic + tic + tic)
   Out[7]: False

*New behavior*:

.. ipython:: python

    ts = pd.Timestamp('2018-06-11 18:01:14')
    tic = pd.offsets.Hour(n=2)
    ts + tic + tic + tic == ts + (tic + tic + tic)


.. _whatsnew_0240.api.datetimelike:


.. _whatsnew_0240.api.period_subtraction:

Period subtraction
^^^^^^^^^^^^^^^^^^

Subtraction of a ``Period`` from another ``Period`` will give a ``DateOffset``.
instead of an integer (:issue:`21314`)

*Previous behavior*:

.. code-block:: ipython

    In [2]: june = pd.Period('June 2018')

    In [3]: april = pd.Period('April 2018')

    In [4]: june - april
    Out [4]: 2

*New behavior*:

.. ipython:: python

    june = pd.Period('June 2018')
    april = pd.Period('April 2018')
    june - april

Similarly, subtraction of a ``Period`` from a ``PeriodIndex`` will now return
an ``Index`` of ``DateOffset`` objects instead of an ``Int64Index``

*Previous behavior*:

.. code-block:: ipython

    In [2]: pi = pd.period_range('June 2018', freq='M', periods=3)

    In [3]: pi - pi[0]
    Out[3]: Int64Index([0, 1, 2], dtype='int64')

*New behavior*:

.. ipython:: python

    pi = pd.period_range('June 2018', freq='M', periods=3)
    pi - pi[0]


.. _whatsnew_0240.api.timedelta64_subtract_nan:

Addition/subtraction of ``NaN`` from :class:`DataFrame`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Adding or subtracting ``NaN`` from a :class:`DataFrame` column with
``timedelta64[ns]`` dtype will now raise a ``TypeError`` instead of returning
all-``NaT``.  This is for compatibility with ``TimedeltaIndex`` and
``Series`` behavior (:issue:`22163`)

.. ipython:: python

   df = pd.DataFrame([pd.Timedelta(days=1)])
   df

*Previous behavior*:

.. code-block:: ipython

    In [4]: df = pd.DataFrame([pd.Timedelta(days=1)])

    In [5]: df - np.nan
    Out[5]:
        0
    0 NaT

*New behavior*:

.. code-block:: ipython

    In [2]: df - np.nan
    ...
    TypeError: unsupported operand type(s) for -: 'TimedeltaIndex' and 'float'

.. _whatsnew_0240.api.dataframe_cmp_broadcasting:

DataFrame comparison operations broadcasting changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Previously, the broadcasting behavior of :class:`DataFrame` comparison
operations (``==``, ``!=``, ...) was inconsistent with the behavior of
arithmetic operations (``+``, ``-``, ...).  The behavior of the comparison
operations has been changed to match the arithmetic operations in these cases.
(:issue:`22880`)

The affected cases are:

- operating against a 2-dimensional ``np.ndarray`` with either 1 row or 1 column will now broadcast the same way a ``np.ndarray`` would (:issue:`23000`).
- a list or tuple with length matching the number of rows in the :class:`DataFrame` will now raise ``ValueError`` instead of operating column-by-column (:issue:`22880`.
- a list or tuple with length matching the number of columns in the :class:`DataFrame` will now operate row-by-row instead of raising ``ValueError`` (:issue:`22880`).

.. ipython:: python

   arr = np.arange(6).reshape(3, 2)
   df = pd.DataFrame(arr)
   df

*Previous behavior*:

.. code-block:: ipython

   In [5]: df == arr[[0], :]
       ...: # comparison previously broadcast where arithmetic would raise
   Out[5]:
          0      1
   0   True   True
   1  False  False
   2  False  False
   In [6]: df + arr[[0], :]
   ...
   ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (1, 2)

   In [7]: df == (1, 2)
       ...: # length matches number of columns;
       ...: # comparison previously raised where arithmetic would broadcast
   ...
   ValueError: Invalid broadcasting comparison [(1, 2)] with block values
   In [8]: df + (1, 2)
   Out[8]:
      0  1
   0  1  3
   1  3  5
   2  5  7

   In [9]: df == (1, 2, 3)
       ...:  # length matches number of rows
       ...:  # comparison previously broadcast where arithmetic would raise
   Out[9]:
          0      1
   0  False   True
   1   True  False
   2  False  False
   In [10]: df + (1, 2, 3)
   ...
   ValueError: Unable to coerce to Series, length must be 2: given 3

*New behavior*:

.. ipython:: python

   # Comparison operations and arithmetic operations both broadcast.
   df == arr[[0], :]
   df + arr[[0], :]

.. ipython:: python

   # Comparison operations and arithmetic operations both broadcast.
   df == (1, 2)
   df + (1, 2)

.. code-block:: ipython

   # Comparison operations and arithmetic operations both raise ValueError.
   In [6]: df == (1, 2, 3)
   ...
   ValueError: Unable to coerce to Series, length must be 2: given 3

   In [7]: df + (1, 2, 3)
   ...
   ValueError: Unable to coerce to Series, length must be 2: given 3

.. _whatsnew_0240.api.dataframe_arithmetic_broadcasting:

DataFrame arithmetic operations broadcasting changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`DataFrame` arithmetic operations when operating with 2-dimensional
``np.ndarray`` objects now broadcast in the same way as ``np.ndarray``
broadcast.  (:issue:`23000`)

.. ipython:: python

   arr = np.arange(6).reshape(3, 2)
   df = pd.DataFrame(arr)
   df

*Previous behavior*:

.. code-block:: ipython

   In [5]: df + arr[[0], :]   # 1 row, 2 columns
   ...
   ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (1, 2)
   In [6]: df + arr[:, [1]]   # 1 column, 3 rows
   ...
   ValueError: Unable to coerce to DataFrame, shape must be (3, 2): given (3, 1)

*New behavior*:

.. ipython:: python

   df + arr[[0], :]   # 1 row, 2 columns
   df + arr[:, [1]]   # 1 column, 3 rows

.. _whatsnew_0240.api.incompatibilities:

Series and Index data-dtype incompatibilities
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Series`` and ``Index`` constructors now raise when the
data is incompatible with a passed ``dtype=`` (:issue:`15832`)

*Previous behavior*:

.. code-block:: ipython

    In [4]: pd.Series([-1], dtype="uint64")
    Out [4]:
    0    18446744073709551615
    dtype: uint64

*New behavior*:

.. code-block:: ipython

    In [4]: pd.Series([-1], dtype="uint64")
    Out [4]:
    ...
    OverflowError: Trying to coerce negative values to unsigned integers

.. _whatsnew_0240.api.concat_categorical:

Concatenation changes
^^^^^^^^^^^^^^^^^^^^^

Calling :func:`pandas.concat` on a ``Categorical`` of ints with NA values now
causes them to be processed as objects when concatenating with anything
other than another ``Categorical`` of ints (:issue:`19214`)

.. ipython:: python

    s = pd.Series([0, 1, np.nan])
    c = pd.Series([0, 1, np.nan], dtype="category")

*Previous behavior*

.. code-block:: ipython

    In [3]: pd.concat([s, c])
    Out[3]:
    0    0.0
    1    1.0
    2    NaN
    0    0.0
    1    1.0
    2    NaN
    dtype: float64

*New behavior*

.. ipython:: python

    pd.concat([s, c])

Datetimelike API changes
^^^^^^^^^^^^^^^^^^^^^^^^

- For :class:`DatetimeIndex` and :class:`TimedeltaIndex` with non-``None`` ``freq`` attribute, addition or subtraction of integer-dtyped array or ``Index`` will return an object of the same class (:issue:`19959`)
- :class:`DateOffset` objects are now immutable. Attempting to alter one of these will now raise ``AttributeError`` (:issue:`21341`)
- :class:`PeriodIndex` subtraction of another ``PeriodIndex`` will now return an object-dtype :class:`Index` of :class:`DateOffset` objects instead of raising a ``TypeError`` (:issue:`20049`)
- :func:`cut` and :func:`qcut` now returns a :class:`DatetimeIndex` or :class:`TimedeltaIndex` bins when the input is datetime or timedelta dtype respectively and ``retbins=True`` (:issue:`19891`)
- :meth:`DatetimeIndex.to_period` and :meth:`Timestamp.to_period` will issue a warning when timezone information will be lost (:issue:`21333`)
- :meth:`PeriodIndex.tz_convert` and :meth:`PeriodIndex.tz_localize` have been removed (:issue:`21781`)

.. _whatsnew_0240.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- A newly constructed empty :class:`DataFrame` with integer as the ``dtype`` will now only be cast to ``float64`` if ``index`` is specified (:issue:`22858`)
- :meth:`Series.str.cat` will now raise if ``others`` is a ``set`` (:issue:`23009`)
- Passing scalar values to :class:`DatetimeIndex` or :class:`TimedeltaIndex` will now raise ``TypeError`` instead of ``ValueError`` (:issue:`23539`)
- ``max_rows`` and ``max_cols`` parameters removed from :class:`HTMLFormatter` since truncation is handled by :class:`DataFrameFormatter` (:issue:`23818`)
- :func:`read_csv` will now raise a ``ValueError`` if a column with missing values is declared as having dtype ``bool`` (:issue:`20591`)
- The column order of the resultant :class:`DataFrame` from :meth:`MultiIndex.to_frame` is now guaranteed to match the :attr:`MultiIndex.names` order. (:issue:`22420`)
- Incorrectly passing a :class:`DatetimeIndex` to :meth:`MultiIndex.from_tuples`, rather than a sequence of tuples, now raises a ``TypeError`` rather than a ``ValueError`` (:issue:`24024`)
- :func:`pd.offsets.generate_range` argument ``time_rule`` has been removed; use ``offset`` instead (:issue:`24157`)
- In 0.23.x, pandas would raise a ``ValueError`` on a merge of a numeric column (e.g. ``int`` dtyped column) and an ``object`` dtyped column (:issue:`9780`). We have re-enabled the ability to merge ``object`` and other dtypes; pandas will still raise on a merge between a numeric and an ``object`` dtyped column that is composed only of strings (:issue:`21681`)
- Accessing a level of a ``MultiIndex`` with a duplicate name (e.g. in
  :meth:`~MultiIndex.get_level_values`) now raises a ``ValueError`` instead of a ``KeyError`` (:issue:`21678`).
- Invalid construction of ``IntervalDtype`` will now always raise a ``TypeError`` rather than a ``ValueError`` if the subdtype is invalid (:issue:`21185`)
- Trying to reindex a ``DataFrame`` with a non unique ``MultiIndex`` now raises a ``ValueError`` instead of an ``Exception`` (:issue:`21770`)
- :class:`Index` subtraction will attempt to operate element-wise instead of raising ``TypeError`` (:issue:`19369`)
- :class:`pandas.io.formats.style.Styler` supports a ``number-format`` property when using :meth:`~pandas.io.formats.style.Styler.to_excel` (:issue:`22015`)
- :meth:`DataFrame.corr` and :meth:`Series.corr` now raise a ``ValueError`` along with a helpful error message instead of a ``KeyError`` when supplied with an invalid method (:issue:`22298`)
- :meth:`shift` will now always return a copy, instead of the previous behaviour of returning self when shifting by 0 (:issue:`22397`)
- :meth:`DataFrame.set_index` now gives a better (and less frequent) KeyError, raises a ``ValueError`` for incorrect types,
  and will not fail on duplicate column names with ``drop=True``. (:issue:`22484`)
- Slicing a single row of a DataFrame with multiple ExtensionArrays of the same type now preserves the dtype, rather than coercing to object (:issue:`22784`)
- :class:`DateOffset` attribute ``_cacheable`` and method ``_should_cache`` have been removed (:issue:`23118`)
- :meth:`Series.searchsorted`, when supplied a scalar value to search for, now returns a scalar instead of an array (:issue:`23801`).
- :meth:`Categorical.searchsorted`, when supplied a scalar value to search for, now returns a scalar instead of an array (:issue:`23466`).
- :meth:`Categorical.searchsorted` now raises a ``KeyError`` rather that a ``ValueError``, if a searched for key is not found in its categories (:issue:`23466`).
- :meth:`Index.hasnans` and :meth:`Series.hasnans` now always return a python boolean. Previously, a python or a numpy boolean could be returned, depending on circumstances (:issue:`23294`).
- The order of the arguments of :func:`DataFrame.to_html` and :func:`DataFrame.to_string` is rearranged to be consistent with each other. (:issue:`23614`)
- :meth:`CategoricalIndex.reindex` now raises a ``ValueError`` if the target index is non-unique and not equal to the current index. It previously only raised if the target index was not of a categorical dtype (:issue:`23963`).
- :func:`Series.to_list` and :func:`Index.to_list` are now aliases of ``Series.tolist`` respectively ``Index.tolist`` (:issue:`8826`)
- The result of ``SparseSeries.unstack`` is now a :class:`DataFrame` with sparse values, rather than a :class:`SparseDataFrame` (:issue:`24372`).
- :class:`DatetimeIndex` and :class:`TimedeltaIndex` no longer ignore the dtype precision. Passing a non-nanosecond resolution dtype will raise a ``ValueError`` (:issue:`24753`)


.. _whatsnew_0240.api.extension:

Extension type changes
~~~~~~~~~~~~~~~~~~~~~~

**Equality and hashability**

pandas now requires that extension dtypes be hashable (i.e. the respective
``ExtensionDtype`` objects; hashability is not a requirement for the values
of the corresponding ``ExtensionArray``). The base class implements
a default ``__eq__`` and ``__hash__``. If you have a parametrized dtype, you should
update the ``ExtensionDtype._metadata`` tuple to match the signature of your
``__init__`` method. See :class:`pandas.api.extensions.ExtensionDtype` for more (:issue:`22476`).

**New and changed methods**

- :meth:`~pandas.api.types.ExtensionArray.dropna` has been added (:issue:`21185`)
- :meth:`~pandas.api.types.ExtensionArray.repeat` has been added (:issue:`24349`)
- The ``ExtensionArray`` constructor, ``_from_sequence`` now take the keyword arg ``copy=False`` (:issue:`21185`)
- :meth:`pandas.api.extensions.ExtensionArray.shift` added as part of the basic ``ExtensionArray`` interface (:issue:`22387`).
- :meth:`~pandas.api.types.ExtensionArray.searchsorted` has been added (:issue:`24350`)
- Support for reduction operations such as ``sum``, ``mean`` via opt-in base class method override (:issue:`22762`)
- :func:`ExtensionArray.isna` is allowed to return an ``ExtensionArray`` (:issue:`22325`).

**Dtype changes**

- ``ExtensionDtype`` has gained the ability to instantiate from string dtypes, e.g. ``decimal`` would instantiate a registered ``DecimalDtype``; furthermore
  the ``ExtensionDtype`` has gained the method ``construct_array_type`` (:issue:`21185`)
- Added ``ExtensionDtype._is_numeric`` for controlling whether an extension dtype is considered numeric (:issue:`22290`).
- Added :meth:`pandas.api.types.register_extension_dtype` to register an extension type with pandas (:issue:`22664`)
- Updated the ``.type`` attribute for ``PeriodDtype``, ``DatetimeTZDtype``, and ``IntervalDtype`` to be instances of the dtype (``Period``, ``Timestamp``, and ``Interval`` respectively) (:issue:`22938`)

.. _whatsnew_0240.enhancements.extension_array_operators:

**Operator support**

A ``Series`` based on an ``ExtensionArray`` now supports arithmetic and comparison
operators (:issue:`19577`). There are two approaches for providing operator support for an ``ExtensionArray``:

1. Define each of the operators on your ``ExtensionArray`` subclass.
2. Use an operator implementation from pandas that depends on operators that are already defined
   on the underlying elements (scalars) of the ``ExtensionArray``.

See the :ref:`ExtensionArray Operator Support
<extending.extension.operator>` documentation section for details on both
ways of adding operator support.

**Other changes**

- A default repr for :class:`pandas.api.extensions.ExtensionArray` is now provided (:issue:`23601`).
- :meth:`ExtensionArray._formatting_values` is deprecated. Use :attr:`ExtensionArray._formatter` instead. (:issue:`23601`)
- An ``ExtensionArray`` with a boolean dtype now works correctly as a boolean indexer. :meth:`pandas.api.types.is_bool_dtype` now properly considers them boolean (:issue:`22326`)

**Bug fixes**

- Bug in :meth:`Series.get` for ``Series`` using ``ExtensionArray`` and integer index (:issue:`21257`)
- :meth:`~Series.shift` now dispatches to :meth:`ExtensionArray.shift` (:issue:`22386`)
- :meth:`Series.combine()` works correctly with :class:`~pandas.api.extensions.ExtensionArray` inside of :class:`Series` (:issue:`20825`)
- :meth:`Series.combine()` with scalar argument now works for any function type (:issue:`21248`)
- :meth:`Series.astype` and :meth:`DataFrame.astype` now dispatch to :meth:`ExtensionArray.astype` (:issue:`21185`).
- Slicing a single row of a ``DataFrame`` with multiple ExtensionArrays of the same type now preserves the dtype, rather than coercing to object (:issue:`22784`)
- Bug when concatenating multiple ``Series`` with different extension dtypes not casting to object dtype (:issue:`22994`)
- Series backed by an ``ExtensionArray`` now work with :func:`util.hash_pandas_object` (:issue:`23066`)
- :meth:`DataFrame.stack` no longer converts to object dtype for DataFrames where each column has the same extension dtype. The output Series will have the same dtype as the columns (:issue:`23077`).
- :meth:`Series.unstack` and :meth:`DataFrame.unstack` no longer convert extension arrays to object-dtype ndarrays. Each column in the output ``DataFrame`` will now have the same dtype as the input (:issue:`23077`).
- Bug when grouping :meth:`Dataframe.groupby()` and aggregating on ``ExtensionArray`` it was not returning the actual ``ExtensionArray`` dtype (:issue:`23227`).
- Bug in :func:`pandas.merge` when merging on an extension array-backed column (:issue:`23020`).


.. _whatsnew_0240.deprecations:

Deprecations
~~~~~~~~~~~~

- :attr:`MultiIndex.labels` has been deprecated and replaced by :attr:`MultiIndex.codes`.
  The functionality is unchanged. The new name better reflects the natures of
  these codes and makes the ``MultiIndex`` API more similar to the API for :class:`CategoricalIndex` (:issue:`13443`).
  As a consequence, other uses of the name ``labels`` in ``MultiIndex`` have also been deprecated and replaced with ``codes``:

  - You should initialize a ``MultiIndex`` instance using a parameter named ``codes`` rather than ``labels``.
  - ``MultiIndex.set_labels`` has been deprecated in favor of :meth:`MultiIndex.set_codes`.
  - For method :meth:`MultiIndex.copy`, the ``labels`` parameter has been deprecated and replaced by a ``codes`` parameter.
- :meth:`DataFrame.to_stata`, :meth:`read_stata`, :class:`StataReader` and :class:`StataWriter` have deprecated the ``encoding`` argument. The encoding of a Stata dta file is determined by the file type and cannot be changed (:issue:`21244`)
- :meth:`MultiIndex.to_hierarchical` is deprecated and will be removed in a future version (:issue:`21613`)
- :meth:`Series.ptp` is deprecated. Use ``numpy.ptp`` instead (:issue:`21614`)
- :meth:`Series.compress` is deprecated. Use ``Series[condition]`` instead (:issue:`18262`)
- The signature of :meth:`Series.to_csv` has been uniformed to that of :meth:`DataFrame.to_csv`: the name of the first argument is now ``path_or_buf``, the order of subsequent arguments has changed, the ``header`` argument now defaults to ``True``. (:issue:`19715`)
- :meth:`Categorical.from_codes` has deprecated providing float values for the ``codes`` argument. (:issue:`21767`)
- :func:`pandas.read_table` is deprecated. Instead, use :func:`read_csv` passing ``sep='\t'`` if necessary. This deprecation has been removed in 0.25.0. (:issue:`21948`)
- :meth:`Series.str.cat` has deprecated using arbitrary list-likes *within* list-likes. A list-like container may still contain
  many ``Series``, ``Index`` or 1-dimensional ``np.ndarray``, or alternatively, only scalar values. (:issue:`21950`)
- :meth:`FrozenNDArray.searchsorted` has deprecated the ``v`` parameter in favor of ``value`` (:issue:`14645`)
- :func:`DatetimeIndex.shift` and :func:`PeriodIndex.shift` now accept ``periods`` argument instead of ``n`` for consistency with :func:`Index.shift` and :func:`Series.shift`. Using ``n`` throws a deprecation warning (:issue:`22458`, :issue:`22912`)
- The ``fastpath`` keyword of the different Index constructors is deprecated (:issue:`23110`).
- :meth:`Timestamp.tz_localize`, :meth:`DatetimeIndex.tz_localize`, and :meth:`Series.tz_localize` have deprecated the ``errors`` argument in favor of the ``nonexistent`` argument (:issue:`8917`)
- The class ``FrozenNDArray`` has been deprecated. When unpickling, ``FrozenNDArray`` will be unpickled to ``np.ndarray`` once this class is removed (:issue:`9031`)
- The methods :meth:`DataFrame.update` and :meth:`Panel.update` have deprecated the ``raise_conflict=False|True`` keyword in favor of ``errors='ignore'|'raise'`` (:issue:`23585`)
- The methods :meth:`Series.str.partition` and :meth:`Series.str.rpartition` have deprecated the ``pat`` keyword in favor of ``sep`` (:issue:`22676`)
- Deprecated the ``nthreads`` keyword of :func:`pandas.read_feather` in favor of ``use_threads`` to reflect the changes in ``pyarrow>=0.11.0``. (:issue:`23053`)
- :func:`pandas.read_excel` has deprecated accepting ``usecols`` as an integer. Please pass in a list of ints from 0 to ``usecols`` inclusive instead (:issue:`23527`)
- Constructing a :class:`TimedeltaIndex` from data with ``datetime64``-dtyped data is deprecated, will raise ``TypeError`` in a future version (:issue:`23539`)
- Constructing a :class:`DatetimeIndex` from data with ``timedelta64``-dtyped data is deprecated, will raise ``TypeError`` in a future version (:issue:`23675`)
- The ``keep_tz=False`` option (the default) of the ``keep_tz`` keyword of
  :meth:`DatetimeIndex.to_series` is deprecated (:issue:`17832`).
- Timezone converting a tz-aware ``datetime.datetime`` or :class:`Timestamp` with :class:`Timestamp` and the ``tz`` argument is now deprecated. Instead, use :meth:`Timestamp.tz_convert` (:issue:`23579`)
- :func:`pandas.api.types.is_period` is deprecated in favor of ``pandas.api.types.is_period_dtype`` (:issue:`23917`)
- :func:`pandas.api.types.is_datetimetz` is deprecated in favor of ``pandas.api.types.is_datetime64tz`` (:issue:`23917`)
- Creating a :class:`TimedeltaIndex`, :class:`DatetimeIndex`, or :class:`PeriodIndex` by passing range arguments ``start``, ``end``, and ``periods`` is deprecated in favor of :func:`timedelta_range`, :func:`date_range`, or :func:`period_range` (:issue:`23919`)
- Passing a string alias like ``'datetime64[ns, UTC]'`` as the ``unit`` parameter to :class:`DatetimeTZDtype` is deprecated. Use :class:`DatetimeTZDtype.construct_from_string` instead (:issue:`23990`).
- The ``skipna`` parameter of :meth:`~pandas.api.types.infer_dtype` will switch to ``True`` by default in a future version of pandas (:issue:`17066`, :issue:`24050`)
- In :meth:`Series.where` with Categorical data, providing an ``other`` that is not present in the categories is deprecated. Convert the categorical to a different dtype or add the ``other`` to the categories first (:issue:`24077`).
- :meth:`Series.clip_lower`, :meth:`Series.clip_upper`, :meth:`DataFrame.clip_lower` and :meth:`DataFrame.clip_upper` are deprecated and will be removed in a future version. Use ``Series.clip(lower=threshold)``, ``Series.clip(upper=threshold)`` and the equivalent ``DataFrame`` methods (:issue:`24203`)
- :meth:`Series.nonzero` is deprecated and will be removed in a future version (:issue:`18262`)
- Passing an integer to :meth:`Series.fillna` and :meth:`DataFrame.fillna` with ``timedelta64[ns]`` dtypes is deprecated, will raise ``TypeError`` in a future version.  Use ``obj.fillna(pd.Timedelta(...))`` instead (:issue:`24694`)
- ``Series.cat.categorical``, ``Series.cat.name`` and ``Series.cat.index`` have been deprecated. Use the attributes on ``Series.cat`` or ``Series`` directly. (:issue:`24751`).
- Passing a dtype without a precision like ``np.dtype('datetime64')`` or ``timedelta64`` to :class:`Index`, :class:`DatetimeIndex` and :class:`TimedeltaIndex` is now deprecated. Use the nanosecond-precision dtype instead (:issue:`24753`).

.. _whatsnew_0240.deprecations.datetimelike_int_ops:

Integer addition/subtraction with datetimes and timedeltas is deprecated
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the past, users could—in some cases—add or subtract integers or integer-dtype
arrays from :class:`Timestamp`, :class:`DatetimeIndex` and :class:`TimedeltaIndex`.

This usage is now deprecated.  Instead add or subtract integer multiples of
the object's ``freq`` attribute (:issue:`21939`, :issue:`23878`).

*Previous behavior*:

.. code-block:: ipython

    In [5]: ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())
    In [6]: ts + 2
    Out[6]: Timestamp('1994-05-06 14:15:16', freq='H')

    In [7]: tdi = pd.timedelta_range('1D', periods=2)
    In [8]: tdi - np.array([2, 1])
    Out[8]: TimedeltaIndex(['-1 days', '1 days'], dtype='timedelta64[ns]', freq=None)

    In [9]: dti = pd.date_range('2001-01-01', periods=2, freq='7D')
    In [10]: dti + pd.Index([1, 2])
    Out[10]: DatetimeIndex(['2001-01-08', '2001-01-22'], dtype='datetime64[ns]', freq=None)

*New behavior*:

.. ipython:: python
    :okwarning:

    ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())
    ts + 2 * ts.freq

    tdi = pd.timedelta_range('1D', periods=2)
    tdi - np.array([2 * tdi.freq, 1 * tdi.freq])

    dti = pd.date_range('2001-01-01', periods=2, freq='7D')
    dti + pd.Index([1 * dti.freq, 2 * dti.freq])


.. _whatsnew_0240.deprecations.integer_tz:

Passing integer data and a timezone to DatetimeIndex
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The behavior of :class:`DatetimeIndex` when passed integer data and
a timezone is changing in a future version of pandas. Previously, these
were interpreted as wall times in the desired timezone. In the future,
these will be interpreted as wall times in UTC, which are then converted
to the desired timezone (:issue:`24559`).

The default behavior remains the same, but issues a warning:

.. code-block:: ipython

   In [3]: pd.DatetimeIndex([946684800000000000], tz="US/Central")
   /bin/ipython:1: FutureWarning:
       Passing integer-dtype data and a timezone to DatetimeIndex. Integer values
       will be interpreted differently in a future version of pandas. Previously,
       these were viewed as datetime64[ns] values representing the wall time
       *in the specified timezone*. In the future, these will be viewed as
       datetime64[ns] values representing the wall time *in UTC*. This is similar
       to a nanosecond-precision UNIX epoch. To accept the future behavior, use

           pd.to_datetime(integer_data, utc=True).tz_convert(tz)

       To keep the previous behavior, use

           pd.to_datetime(integer_data).tz_localize(tz)

    #!/bin/python3
    Out[3]: DatetimeIndex(['2000-01-01 00:00:00-06:00'], dtype='datetime64[ns, US/Central]', freq=None)

As the warning message explains, opt in to the future behavior by specifying that
the integer values are UTC, and then converting to the final timezone:

.. ipython:: python

   pd.to_datetime([946684800000000000], utc=True).tz_convert('US/Central')

The old behavior can be retained with by localizing directly to the final timezone:

.. ipython:: python

   pd.to_datetime([946684800000000000]).tz_localize('US/Central')

.. _whatsnew_0240.deprecations.tz_aware_array:

Converting timezone-aware Series and Index to NumPy arrays
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The conversion from a :class:`Series` or :class:`Index` with timezone-aware
datetime data will change to preserve timezones by default (:issue:`23569`).

NumPy doesn't have a dedicated dtype for timezone-aware datetimes.
In the past, converting a :class:`Series` or :class:`DatetimeIndex` with
timezone-aware datatimes would convert to a NumPy array by

1. converting the tz-aware data to UTC
2. dropping the timezone-info
3. returning a :class:`numpy.ndarray` with ``datetime64[ns]`` dtype

Future versions of pandas will preserve the timezone information by returning an
object-dtype NumPy array where each value is a :class:`Timestamp` with the correct
timezone attached

.. ipython:: python

   ser = pd.Series(pd.date_range('2000', periods=2, tz="CET"))
   ser

The default behavior remains the same, but issues a warning

.. code-block:: python

   In [8]: np.asarray(ser)
   /bin/ipython:1: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive
         ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray
         with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.

           To accept the future behavior, pass 'dtype=object'.
           To keep the old behavior, pass 'dtype="datetime64[ns]"'.
     #!/bin/python3
   Out[8]:
   array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],
         dtype='datetime64[ns]')

The previous or future behavior can be obtained, without any warnings, by specifying
the ``dtype``

*Previous behavior*

.. ipython:: python

   np.asarray(ser, dtype='datetime64[ns]')

*Future behavior*

.. ipython:: python

   # New behavior
   np.asarray(ser, dtype=object)


Or by using :meth:`Series.to_numpy`

.. ipython:: python

   ser.to_numpy()
   ser.to_numpy(dtype="datetime64[ns]")

All the above applies to a :class:`DatetimeIndex` with tz-aware values as well.

.. _whatsnew_0240.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- The ``LongPanel`` and ``WidePanel`` classes have been removed (:issue:`10892`)
- :meth:`Series.repeat` has renamed the ``reps`` argument to ``repeats`` (:issue:`14645`)
- Several private functions were removed from the (non-public) module ``pandas.core.common`` (:issue:`22001`)
- Removal of the previously deprecated module ``pandas.core.datetools`` (:issue:`14105`, :issue:`14094`)
- Strings passed into :meth:`DataFrame.groupby` that refer to both column and index levels will raise a ``ValueError`` (:issue:`14432`)
- :meth:`Index.repeat` and :meth:`MultiIndex.repeat` have renamed the ``n`` argument to ``repeats`` (:issue:`14645`)
- The ``Series`` constructor and ``.astype`` method will now raise a ``ValueError`` if timestamp dtypes are passed in without a unit (e.g. ``np.datetime64``) for the ``dtype`` parameter (:issue:`15987`)
- Removal of the previously deprecated ``as_indexer`` keyword completely from ``str.match()`` (:issue:`22356`, :issue:`6581`)
- The modules ``pandas.types``, ``pandas.computation``, and ``pandas.util.decorators`` have been removed (:issue:`16157`, :issue:`16250`)
- Removed the ``pandas.formats.style`` shim for :class:`pandas.io.formats.style.Styler` (:issue:`16059`)
- ``pandas.pnow``, ``pandas.match``, ``pandas.groupby``, ``pd.get_store``, ``pd.Expr``, and ``pd.Term`` have been removed (:issue:`15538`, :issue:`15940`)
- :meth:`Categorical.searchsorted` and :meth:`Series.searchsorted` have renamed the ``v`` argument to ``value`` (:issue:`14645`)
- ``pandas.parser``, ``pandas.lib``, and ``pandas.tslib`` have been removed (:issue:`15537`)
- :meth:`Index.searchsorted` have renamed the ``key`` argument to ``value`` (:issue:`14645`)
- ``DataFrame.consolidate`` and ``Series.consolidate`` have been removed (:issue:`15501`)
- Removal of the previously deprecated module ``pandas.json`` (:issue:`19944`)
- The module ``pandas.tools`` has been removed (:issue:`15358`, :issue:`16005`)
- :meth:`SparseArray.get_values` and :meth:`SparseArray.to_dense` have dropped the ``fill`` parameter (:issue:`14686`)
- ``DataFrame.sortlevel`` and ``Series.sortlevel`` have been removed (:issue:`15099`)
- :meth:`SparseSeries.to_dense` has dropped the ``sparse_only`` parameter (:issue:`14686`)
- :meth:`DataFrame.astype` and :meth:`Series.astype` have renamed the ``raise_on_error`` argument to ``errors`` (:issue:`14967`)
- ``is_sequence``, ``is_any_int_dtype``, and ``is_floating_dtype`` have been removed from ``pandas.api.types`` (:issue:`16163`, :issue:`16189`)

.. _whatsnew_0240.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Slicing Series and DataFrames with an monotonically increasing :class:`CategoricalIndex`
  is now very fast and has speed comparable to slicing with an ``Int64Index``.
  The speed increase is both when indexing by label (using .loc) and position(.iloc) (:issue:`20395`)
  Slicing a monotonically increasing :class:`CategoricalIndex` itself (i.e. ``ci[1000:2000]``)
  shows similar speed improvements as above (:issue:`21659`)
- Improved performance of :meth:`CategoricalIndex.equals` when comparing to another :class:`CategoricalIndex` (:issue:`24023`)
- Improved performance of :func:`Series.describe` in case of numeric dtpyes (:issue:`21274`)
- Improved performance of :func:`pandas.core.groupby.GroupBy.rank` when dealing with tied rankings (:issue:`21237`)
- Improved performance of :func:`DataFrame.set_index` with columns consisting of :class:`Period` objects (:issue:`21582`, :issue:`21606`)
- Improved performance of :meth:`Series.at` and :meth:`Index.get_value` for Extension Arrays values (e.g. :class:`Categorical`) (:issue:`24204`)
- Improved performance of membership checks in :class:`Categorical` and :class:`CategoricalIndex`
  (i.e. ``x in cat``-style checks are much faster). :meth:`CategoricalIndex.contains`
  is likewise much faster (:issue:`21369`, :issue:`21508`)
- Improved performance of :meth:`HDFStore.groups` (and dependent functions like
  :meth:`HDFStore.keys`.  (i.e. ``x in store`` checks are much faster)
  (:issue:`21372`)
- Improved the performance of :func:`pandas.get_dummies` with ``sparse=True`` (:issue:`21997`)
- Improved performance of :func:`IndexEngine.get_indexer_non_unique` for sorted, non-unique indexes (:issue:`9466`)
- Improved performance of :func:`PeriodIndex.unique` (:issue:`23083`)
- Improved performance of :func:`concat` for ``Series`` objects (:issue:`23404`)
- Improved performance of :meth:`DatetimeIndex.normalize` and :meth:`Timestamp.normalize` for timezone naive or UTC datetimes (:issue:`23634`)
- Improved performance of :meth:`DatetimeIndex.tz_localize` and various ``DatetimeIndex`` attributes with dateutil UTC timezone (:issue:`23772`)
- Fixed a performance regression on Windows with Python 3.7 of :func:`read_csv` (:issue:`23516`)
- Improved performance of :class:`Categorical` constructor for ``Series`` objects (:issue:`23814`)
- Improved performance of :meth:`~DataFrame.where` for Categorical data (:issue:`24077`)
- Improved performance of iterating over a :class:`Series`. Using :meth:`DataFrame.itertuples` now creates iterators
  without internally allocating lists of all elements (:issue:`20783`)
- Improved performance of :class:`Period` constructor, additionally benefitting ``PeriodArray`` and ``PeriodIndex`` creation (:issue:`24084`, :issue:`24118`)
- Improved performance of tz-aware :class:`DatetimeArray` binary operations (:issue:`24491`)

.. _whatsnew_0240.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^

- Bug in :meth:`Categorical.from_codes` where ``NaN`` values in ``codes`` were silently converted to ``0`` (:issue:`21767`). In the future this will raise a ``ValueError``. Also changes the behavior of ``.from_codes([1.1, 2.0])``.
- Bug in :meth:`Categorical.sort_values` where ``NaN`` values were always positioned in front regardless of ``na_position`` value. (:issue:`22556`).
- Bug when indexing with a boolean-valued ``Categorical``. Now a boolean-valued ``Categorical`` is treated as a boolean mask (:issue:`22665`)
- Constructing a :class:`CategoricalIndex` with empty values and boolean categories was raising a ``ValueError`` after a change to dtype coercion (:issue:`22702`).
- Bug in :meth:`Categorical.take` with a user-provided ``fill_value`` not encoding the ``fill_value``, which could result in a ``ValueError``, incorrect results, or a segmentation fault (:issue:`23296`).
- In :meth:`Series.unstack`, specifying a ``fill_value`` not present in the categories now raises a ``TypeError`` rather than ignoring the ``fill_value`` (:issue:`23284`)
- Bug when resampling :meth:`DataFrame.resample()` and aggregating on categorical data, the categorical dtype was getting lost. (:issue:`23227`)
- Bug in many methods of the ``.str``-accessor, which always failed on calling the ``CategoricalIndex.str`` constructor (:issue:`23555`, :issue:`23556`)
- Bug in :meth:`Series.where` losing the categorical dtype for categorical data (:issue:`24077`)
- Bug in :meth:`Categorical.apply` where ``NaN`` values could be handled unpredictably. They now remain unchanged (:issue:`24241`)
- Bug in :class:`Categorical` comparison methods incorrectly raising ``ValueError`` when operating against a :class:`DataFrame` (:issue:`24630`)
- Bug in :meth:`Categorical.set_categories` where setting fewer new categories with ``rename=True`` caused a segmentation fault (:issue:`24675`)

Datetimelike
^^^^^^^^^^^^

- Fixed bug where two :class:`DateOffset` objects with different ``normalize`` attributes could evaluate as equal (:issue:`21404`)
- Fixed bug where :meth:`Timestamp.resolution` incorrectly returned 1-microsecond ``timedelta`` instead of 1-nanosecond :class:`Timedelta` (:issue:`21336`, :issue:`21365`)
- Bug in :func:`to_datetime` that did not consistently return an :class:`Index` when ``box=True`` was specified (:issue:`21864`)
- Bug in :class:`DatetimeIndex` comparisons where string comparisons incorrectly raises ``TypeError`` (:issue:`22074`)
- Bug in :class:`DatetimeIndex` comparisons when comparing against ``timedelta64[ns]`` dtyped arrays; in some cases ``TypeError`` was incorrectly raised, in others it incorrectly failed to raise (:issue:`22074`)
- Bug in :class:`DatetimeIndex` comparisons when comparing against object-dtyped arrays (:issue:`22074`)
- Bug in :class:`DataFrame` with ``datetime64[ns]`` dtype addition and subtraction with ``Timedelta``-like objects (:issue:`22005`, :issue:`22163`)
- Bug in :class:`DataFrame` with ``datetime64[ns]`` dtype addition and subtraction with ``DateOffset`` objects returning an ``object`` dtype instead of ``datetime64[ns]`` dtype (:issue:`21610`, :issue:`22163`)
- Bug in :class:`DataFrame` with ``datetime64[ns]`` dtype comparing against ``NaT`` incorrectly (:issue:`22242`, :issue:`22163`)
- Bug in :class:`DataFrame` with ``datetime64[ns]`` dtype subtracting ``Timestamp``-like object incorrectly returned ``datetime64[ns]`` dtype instead of ``timedelta64[ns]`` dtype (:issue:`8554`, :issue:`22163`)
- Bug in :class:`DataFrame` with ``datetime64[ns]`` dtype subtracting ``np.datetime64`` object with non-nanosecond unit failing to convert to nanoseconds (:issue:`18874`, :issue:`22163`)
- Bug in :class:`DataFrame` comparisons against ``Timestamp``-like objects failing to raise ``TypeError`` for inequality checks with mismatched types (:issue:`8932`, :issue:`22163`)
- Bug in :class:`DataFrame` with mixed dtypes including ``datetime64[ns]`` incorrectly raising ``TypeError`` on equality comparisons (:issue:`13128`, :issue:`22163`)
- Bug in :attr:`DataFrame.values` returning a :class:`DatetimeIndex` for a single-column ``DataFrame`` with tz-aware datetime values. Now a 2-D :class:`numpy.ndarray` of :class:`Timestamp` objects is returned (:issue:`24024`)
- Bug in :meth:`DataFrame.eq` comparison against ``NaT`` incorrectly returning ``True`` or ``NaN`` (:issue:`15697`, :issue:`22163`)
- Bug in :class:`DatetimeIndex` subtraction that incorrectly failed to raise ``OverflowError`` (:issue:`22492`, :issue:`22508`)
- Bug in :class:`DatetimeIndex` incorrectly allowing indexing with ``Timedelta`` object (:issue:`20464`)
- Bug in :class:`DatetimeIndex` where frequency was being set if original frequency was ``None`` (:issue:`22150`)
- Bug in rounding methods of :class:`DatetimeIndex` (:meth:`~DatetimeIndex.round`, :meth:`~DatetimeIndex.ceil`, :meth:`~DatetimeIndex.floor`) and :class:`Timestamp` (:meth:`~Timestamp.round`, :meth:`~Timestamp.ceil`, :meth:`~Timestamp.floor`) could give rise to loss of precision (:issue:`22591`)
- Bug in :func:`to_datetime` with an :class:`Index` argument that would drop the ``name`` from the result (:issue:`21697`)
- Bug in :class:`PeriodIndex` where adding or subtracting a :class:`timedelta` or :class:`Tick` object produced incorrect results (:issue:`22988`)
- Bug in the :class:`Series` repr with period-dtype data missing a space before the data (:issue:`23601`)
- Bug in :func:`date_range` when decrementing a start date to a past end date by a negative frequency (:issue:`23270`)
- Bug in :meth:`Series.min` which would return ``NaN`` instead of ``NaT`` when called on a series of ``NaT`` (:issue:`23282`)
- Bug in :meth:`Series.combine_first` not properly aligning categoricals, so that missing values in ``self`` where not filled by valid values from ``other`` (:issue:`24147`)
- Bug in :func:`DataFrame.combine` with datetimelike values raising a TypeError (:issue:`23079`)
- Bug in :func:`date_range` with frequency of ``Day`` or higher where dates sufficiently far in the future could wrap around to the past instead of raising ``OutOfBoundsDatetime`` (:issue:`14187`)
- Bug in :func:`period_range` ignoring the frequency of ``start`` and ``end`` when those are provided as :class:`Period` objects (:issue:`20535`).
- Bug in :class:`PeriodIndex` with attribute ``freq.n`` greater than 1 where adding a :class:`DateOffset` object would return incorrect results (:issue:`23215`)
- Bug in :class:`Series` that interpreted string indices as lists of characters when setting datetimelike values (:issue:`23451`)
- Bug in :class:`DataFrame` when creating a new column from an ndarray of :class:`Timestamp` objects with timezones creating an object-dtype column, rather than datetime with timezone (:issue:`23932`)
- Bug in :class:`Timestamp` constructor which would drop the frequency of an input :class:`Timestamp` (:issue:`22311`)
- Bug in :class:`DatetimeIndex` where calling ``np.array(dtindex, dtype=object)`` would incorrectly return an array of ``long`` objects (:issue:`23524`)
- Bug in :class:`Index` where passing a timezone-aware :class:`DatetimeIndex` and ``dtype=object`` would incorrectly raise a ``ValueError`` (:issue:`23524`)
- Bug in :class:`Index` where calling ``np.array(dtindex, dtype=object)`` on a timezone-naive :class:`DatetimeIndex` would return an array of ``datetime`` objects instead of :class:`Timestamp` objects, potentially losing nanosecond portions of the timestamps (:issue:`23524`)
- Bug in :class:`Categorical.__setitem__` not allowing setting with another ``Categorical`` when both are unordered and have the same categories, but in a different order (:issue:`24142`)
- Bug in :func:`date_range` where using dates with millisecond resolution or higher could return incorrect values or the wrong number of values in the index (:issue:`24110`)
- Bug in :class:`DatetimeIndex` where constructing a :class:`DatetimeIndex` from a :class:`Categorical` or :class:`CategoricalIndex` would incorrectly drop timezone information (:issue:`18664`)
- Bug in :class:`DatetimeIndex` and :class:`TimedeltaIndex` where indexing with ``Ellipsis`` would incorrectly lose the index's ``freq`` attribute (:issue:`21282`)
- Clarified error message produced when passing an incorrect ``freq`` argument to :class:`DatetimeIndex` with ``NaT`` as the first entry in the passed data (:issue:`11587`)
- Bug in :func:`to_datetime` where ``box`` and ``utc`` arguments were ignored when passing a :class:`DataFrame` or ``dict`` of unit mappings (:issue:`23760`)
- Bug in :attr:`Series.dt` where the cache would not update properly after an in-place operation (:issue:`24408`)
- Bug in :class:`PeriodIndex` where comparisons against an array-like object with length 1 failed to raise ``ValueError`` (:issue:`23078`)
- Bug in :meth:`DatetimeIndex.astype`, :meth:`PeriodIndex.astype` and :meth:`TimedeltaIndex.astype` ignoring the sign of the ``dtype`` for unsigned integer dtypes (:issue:`24405`).
- Fixed bug in :meth:`Series.max` with ``datetime64[ns]``-dtype failing to return ``NaT`` when nulls are present and ``skipna=False`` is passed (:issue:`24265`)
- Bug in :func:`to_datetime` where arrays of ``datetime`` objects containing both timezone-aware and timezone-naive ``datetimes`` would fail to raise ``ValueError`` (:issue:`24569`)
- Bug in :func:`to_datetime` with invalid datetime format doesn't coerce input to ``NaT`` even if ``errors='coerce'`` (:issue:`24763`)

Timedelta
^^^^^^^^^
- Bug in :class:`DataFrame` with ``timedelta64[ns]`` dtype division by ``Timedelta``-like scalar incorrectly returning ``timedelta64[ns]`` dtype instead of ``float64`` dtype (:issue:`20088`, :issue:`22163`)
- Bug in adding a :class:`Index` with object dtype to a :class:`Series` with ``timedelta64[ns]`` dtype incorrectly raising (:issue:`22390`)
- Bug in multiplying a :class:`Series` with numeric dtype against a ``timedelta`` object (:issue:`22390`)
- Bug in :class:`Series` with numeric dtype when adding or subtracting an array or ``Series`` with ``timedelta64`` dtype (:issue:`22390`)
- Bug in :class:`Index` with numeric dtype when multiplying or dividing an array with dtype ``timedelta64`` (:issue:`22390`)
- Bug in :class:`TimedeltaIndex` incorrectly allowing indexing with ``Timestamp`` object (:issue:`20464`)
- Fixed bug where subtracting :class:`Timedelta` from an object-dtyped array would raise ``TypeError`` (:issue:`21980`)
- Fixed bug in adding a :class:`DataFrame` with all-`timedelta64[ns]` dtypes to a :class:`DataFrame` with all-integer dtypes returning incorrect results instead of raising ``TypeError`` (:issue:`22696`)
- Bug in :class:`TimedeltaIndex` where adding a timezone-aware datetime scalar incorrectly returned a timezone-naive :class:`DatetimeIndex` (:issue:`23215`)
- Bug in :class:`TimedeltaIndex` where adding ``np.timedelta64('NaT')`` incorrectly returned an all-``NaT`` :class:`DatetimeIndex` instead of an all-``NaT`` :class:`TimedeltaIndex` (:issue:`23215`)
- Bug in :class:`Timedelta` and :func:`to_timedelta()` have inconsistencies in supported unit string (:issue:`21762`)
- Bug in :class:`TimedeltaIndex` division where dividing by another :class:`TimedeltaIndex` raised ``TypeError`` instead of returning a :class:`Float64Index` (:issue:`23829`, :issue:`22631`)
- Bug in :class:`TimedeltaIndex` comparison operations where comparing against non-``Timedelta``-like objects would raise ``TypeError`` instead of returning all-``False`` for ``__eq__`` and all-``True`` for ``__ne__`` (:issue:`24056`)
- Bug in :class:`Timedelta` comparisons when comparing with a ``Tick`` object incorrectly raising ``TypeError`` (:issue:`24710`)

Timezones
^^^^^^^^^

- Bug in :meth:`Index.shift` where an ``AssertionError`` would raise when shifting across DST (:issue:`8616`)
- Bug in :class:`Timestamp` constructor where passing an invalid timezone offset designator (``Z``) would not raise a ``ValueError`` (:issue:`8910`)
- Bug in :meth:`Timestamp.replace` where replacing at a DST boundary would retain an incorrect offset (:issue:`7825`)
- Bug in :meth:`Series.replace` with ``datetime64[ns, tz]`` data when replacing ``NaT`` (:issue:`11792`)
- Bug in :class:`Timestamp` when passing different string date formats with a timezone offset would produce different timezone offsets (:issue:`12064`)
- Bug when comparing a tz-naive :class:`Timestamp` to a tz-aware :class:`DatetimeIndex` which would coerce the :class:`DatetimeIndex` to tz-naive (:issue:`12601`)
- Bug in :meth:`Series.truncate` with a tz-aware :class:`DatetimeIndex` which would cause a core dump (:issue:`9243`)
- Bug in :class:`Series` constructor which would coerce tz-aware and tz-naive :class:`Timestamp` to tz-aware (:issue:`13051`)
- Bug in :class:`Index` with ``datetime64[ns, tz]`` dtype that did not localize integer data correctly (:issue:`20964`)
- Bug in :class:`DatetimeIndex` where constructing with an integer and tz would not localize correctly (:issue:`12619`)
- Fixed bug where :meth:`DataFrame.describe` and :meth:`Series.describe` on tz-aware datetimes did not show ``first`` and ``last`` result (:issue:`21328`)
- Bug in :class:`DatetimeIndex` comparisons failing to raise ``TypeError`` when comparing timezone-aware ``DatetimeIndex`` against ``np.datetime64`` (:issue:`22074`)
- Bug in ``DataFrame`` assignment with a timezone-aware scalar (:issue:`19843`)
- Bug in :func:`DataFrame.asof` that raised a ``TypeError`` when attempting to compare tz-naive and tz-aware timestamps (:issue:`21194`)
- Bug when constructing a :class:`DatetimeIndex` with :class:`Timestamp` constructed with the ``replace`` method across DST (:issue:`18785`)
- Bug when setting a new value with :meth:`DataFrame.loc` with a :class:`DatetimeIndex` with a DST transition (:issue:`18308`, :issue:`20724`)
- Bug in :meth:`Index.unique` that did not re-localize tz-aware dates correctly (:issue:`21737`)
- Bug when indexing a :class:`Series` with a DST transition (:issue:`21846`)
- Bug in :meth:`DataFrame.resample` and :meth:`Series.resample` where an ``AmbiguousTimeError`` or ``NonExistentTimeError`` would raise if a timezone aware timeseries ended on a DST transition (:issue:`19375`, :issue:`10117`)
- Bug in :meth:`DataFrame.drop` and :meth:`Series.drop` when specifying a tz-aware Timestamp key to drop from a :class:`DatetimeIndex` with a DST transition (:issue:`21761`)
- Bug in :class:`DatetimeIndex` constructor where ``NaT`` and ``dateutil.tz.tzlocal`` would raise an ``OutOfBoundsDatetime`` error (:issue:`23807`)
- Bug in :meth:`DatetimeIndex.tz_localize` and :meth:`Timestamp.tz_localize` with ``dateutil.tz.tzlocal`` near a DST transition that would return an incorrectly localized datetime (:issue:`23807`)
- Bug in :class:`Timestamp` constructor where a ``dateutil.tz.tzutc`` timezone passed with a ``datetime.datetime`` argument would be converted to a ``pytz.UTC`` timezone (:issue:`23807`)
- Bug in :func:`to_datetime` where ``utc=True`` was not respected when specifying a ``unit`` and ``errors='ignore'`` (:issue:`23758`)
- Bug in :func:`to_datetime` where ``utc=True`` was not respected when passing a :class:`Timestamp` (:issue:`24415`)
- Bug in :meth:`DataFrame.any` returns wrong value when ``axis=1`` and the data is of datetimelike type (:issue:`23070`)
- Bug in :meth:`DatetimeIndex.to_period` where a timezone aware index was converted to UTC first before creating :class:`PeriodIndex` (:issue:`22905`)
- Bug in :meth:`DataFrame.tz_localize`, :meth:`DataFrame.tz_convert`, :meth:`Series.tz_localize`, and :meth:`Series.tz_convert` where ``copy=False`` would mutate the original argument inplace (:issue:`6326`)
- Bug in :meth:`DataFrame.max` and :meth:`DataFrame.min` with ``axis=1`` where a :class:`Series` with ``NaN`` would be returned when all columns contained the same timezone (:issue:`10390`)

Offsets
^^^^^^^

- Bug in :class:`FY5253` where date offsets could incorrectly raise an ``AssertionError`` in arithmetic operations (:issue:`14774`)
- Bug in :class:`DateOffset` where keyword arguments ``week`` and ``milliseconds`` were accepted and ignored.  Passing these will now raise ``ValueError`` (:issue:`19398`)
- Bug in adding :class:`DateOffset` with :class:`DataFrame` or :class:`PeriodIndex` incorrectly raising ``TypeError`` (:issue:`23215`)
- Bug in comparing :class:`DateOffset` objects with non-DateOffset objects, particularly strings, raising ``ValueError`` instead of returning ``False`` for equality checks and ``True`` for not-equal checks (:issue:`23524`)

Numeric
^^^^^^^

- Bug in :class:`Series` ``__rmatmul__`` doesn't support matrix vector multiplication (:issue:`21530`)
- Bug in :func:`factorize` fails with read-only array (:issue:`12813`)
- Fixed bug in :func:`unique` handled signed zeros inconsistently: for some inputs 0.0 and -0.0 were treated as equal and for some inputs as different. Now they are treated as equal for all inputs (:issue:`21866`)
- Bug in :meth:`DataFrame.agg`, :meth:`DataFrame.transform` and :meth:`DataFrame.apply` where,
  when supplied with a list of functions and ``axis=1`` (e.g. ``df.apply(['sum', 'mean'], axis=1)``),
  a ``TypeError`` was wrongly raised. For all three methods such calculation are now done correctly. (:issue:`16679`).
- Bug in :class:`Series` comparison against datetime-like scalars and arrays (:issue:`22074`)
- Bug in :class:`DataFrame` multiplication between boolean dtype and integer returning ``object`` dtype instead of integer dtype (:issue:`22047`, :issue:`22163`)
- Bug in :meth:`DataFrame.apply` where, when supplied with a string argument and additional positional or keyword arguments (e.g. ``df.apply('sum', min_count=1)``), a ``TypeError`` was wrongly raised (:issue:`22376`)
- Bug in :meth:`DataFrame.astype` to extension dtype may raise ``AttributeError`` (:issue:`22578`)
- Bug in :class:`DataFrame` with ``timedelta64[ns]`` dtype arithmetic operations with ``ndarray`` with integer dtype incorrectly treating the narray as ``timedelta64[ns]`` dtype (:issue:`23114`)
- Bug in :meth:`Series.rpow` with object dtype ``NaN`` for ``1 ** NA`` instead of ``1`` (:issue:`22922`).
- :meth:`Series.agg` can now handle numpy NaN-aware methods like :func:`numpy.nansum` (:issue:`19629`)
- Bug in :meth:`Series.rank` and :meth:`DataFrame.rank` when ``pct=True`` and more than 2\ :sup:`24` rows are present resulted in percentages greater than 1.0 (:issue:`18271`)
- Calls such as :meth:`DataFrame.round` with a non-unique :meth:`CategoricalIndex` now return expected data. Previously, data would be improperly duplicated (:issue:`21809`).
- Added ``log10``, ``floor`` and ``ceil`` to the list of supported functions in :meth:`DataFrame.eval` (:issue:`24139`, :issue:`24353`)
- Logical operations ``&, |, ^`` between :class:`Series` and :class:`Index` will no longer raise ``ValueError`` (:issue:`22092`)
- Checking PEP 3141 numbers in :func:`~pandas.api.types.is_scalar` function returns ``True`` (:issue:`22903`)
- Reduction methods like :meth:`Series.sum` now accept the default value of ``keepdims=False`` when called from a NumPy ufunc, rather than raising a ``TypeError``. Full support for ``keepdims`` has not been implemented (:issue:`24356`).

Conversion
^^^^^^^^^^

- Bug in :meth:`DataFrame.combine_first` in which column types were unexpectedly converted to float (:issue:`20699`)
- Bug in :meth:`DataFrame.clip` in which column types are not preserved and casted to float (:issue:`24162`)
- Bug in :meth:`DataFrame.clip` when order of columns of dataframes doesn't match, result observed is wrong in numeric values (:issue:`20911`)
- Bug in :meth:`DataFrame.astype` where converting to an extension dtype when duplicate column names are present causes a ``RecursionError`` (:issue:`24704`)

Strings
^^^^^^^

- Bug in :meth:`Index.str.partition` was not nan-safe (:issue:`23558`).
- Bug in :meth:`Index.str.split` was not nan-safe (:issue:`23677`).
- Bug :func:`Series.str.contains` not respecting the ``na`` argument for a ``Categorical`` dtype ``Series`` (:issue:`22158`)
- Bug in :meth:`Index.str.cat` when the result contained only ``NaN`` (:issue:`24044`)

Interval
^^^^^^^^

- Bug in the :class:`IntervalIndex` constructor where the ``closed`` parameter did not always override the inferred ``closed`` (:issue:`19370`)
- Bug in the ``IntervalIndex`` repr where a trailing comma was missing after the list of intervals (:issue:`20611`)
- Bug in :class:`Interval` where scalar arithmetic operations did not retain the ``closed`` value (:issue:`22313`)
- Bug in :class:`IntervalIndex` where indexing with datetime-like values raised a ``KeyError`` (:issue:`20636`)
- Bug in ``IntervalTree`` where data containing ``NaN`` triggered a warning and resulted in incorrect indexing queries with :class:`IntervalIndex` (:issue:`23352`)

Indexing
^^^^^^^^

- Bug in :meth:`DataFrame.ne` fails if columns contain column name "dtype" (:issue:`22383`)
- The traceback from a ``KeyError`` when asking ``.loc`` for a single missing label is now shorter and more clear (:issue:`21557`)
- :class:`PeriodIndex` now emits a ``KeyError`` when a malformed string is looked up, which is consistent with the behavior of :class:`DatetimeIndex` (:issue:`22803`)
- When ``.ix`` is asked for a missing integer label in a :class:`MultiIndex` with a first level of integer type, it now raises a ``KeyError``, consistently with the case of a flat :class:`Int64Index`, rather than falling back to positional indexing (:issue:`21593`)
- Bug in :meth:`Index.reindex` when reindexing a tz-naive and tz-aware :class:`DatetimeIndex` (:issue:`8306`)
- Bug in :meth:`Series.reindex` when reindexing an empty series with a ``datetime64[ns, tz]`` dtype (:issue:`20869`)
- Bug in :class:`DataFrame` when setting values with ``.loc`` and a timezone aware :class:`DatetimeIndex` (:issue:`11365`)
- ``DataFrame.__getitem__`` now accepts dictionaries and dictionary keys as list-likes of labels, consistently with ``Series.__getitem__`` (:issue:`21294`)
- Fixed ``DataFrame[np.nan]`` when columns are non-unique (:issue:`21428`)
- Bug when indexing :class:`DatetimeIndex` with nanosecond resolution dates and timezones (:issue:`11679`)
- Bug where indexing with a Numpy array containing negative values would mutate the indexer (:issue:`21867`)
- Bug where mixed indexes wouldn't allow integers for ``.at`` (:issue:`19860`)
- ``Float64Index.get_loc`` now raises ``KeyError`` when boolean key passed. (:issue:`19087`)
- Bug in :meth:`DataFrame.loc` when indexing with an :class:`IntervalIndex` (:issue:`19977`)
- :class:`Index` no longer mangles ``None``, ``NaN`` and ``NaT``, i.e. they are treated as three different keys. However, for numeric Index all three are still coerced to a ``NaN`` (:issue:`22332`)
- Bug in ``scalar in Index`` if scalar is a float while the ``Index`` is of integer dtype (:issue:`22085`)
- Bug in :func:`MultiIndex.set_levels` when levels value is not subscriptable (:issue:`23273`)
- Bug where setting a timedelta column by ``Index`` causes it to be casted to double, and therefore lose precision (:issue:`23511`)
- Bug in :func:`Index.union` and :func:`Index.intersection` where name of the ``Index`` of the result was not computed correctly for certain cases (:issue:`9943`, :issue:`9862`)
- Bug in :class:`Index` slicing with boolean :class:`Index` may raise ``TypeError`` (:issue:`22533`)
- Bug in ``PeriodArray.__setitem__`` when accepting slice and list-like value (:issue:`23978`)
- Bug in :class:`DatetimeIndex`, :class:`TimedeltaIndex` where indexing with ``Ellipsis`` would lose their ``freq`` attribute (:issue:`21282`)
- Bug in ``iat`` where using it to assign an incompatible value would create a new column (:issue:`23236`)

Missing
^^^^^^^

- Bug in :func:`DataFrame.fillna` where a ``ValueError`` would raise when one column contained a ``datetime64[ns, tz]`` dtype (:issue:`15522`)
- Bug in :func:`Series.hasnans` that could be incorrectly cached and return incorrect answers if null elements are introduced after an initial call (:issue:`19700`)
- :func:`Series.isin` now treats all NaN-floats as equal also for ``np.object_``-dtype. This behavior is consistent with the behavior for float64 (:issue:`22119`)
- :func:`unique` no longer mangles NaN-floats and the ``NaT``-object for ``np.object_``-dtype, i.e. ``NaT`` is no longer coerced to a NaN-value and is treated as a different entity. (:issue:`22295`)
- :class:`DataFrame` and :class:`Series` now properly handle numpy masked arrays with hardened masks. Previously, constructing a DataFrame or Series from a masked array with a hard mask would create a pandas object containing the underlying value, rather than the expected NaN. (:issue:`24574`)
- Bug in :class:`DataFrame` constructor where ``dtype`` argument was not honored when handling numpy masked record arrays. (:issue:`24874`)

MultiIndex
^^^^^^^^^^

- Bug in :func:`io.formats.style.Styler.applymap` where ``subset=`` with :class:`MultiIndex` slice would reduce to :class:`Series` (:issue:`19861`)
- Removed compatibility for :class:`MultiIndex` pickles prior to version 0.8.0; compatibility with :class:`MultiIndex` pickles from version 0.13 forward is maintained (:issue:`21654`)
- :meth:`MultiIndex.get_loc_level` (and as a consequence, ``.loc`` on a ``Series`` or ``DataFrame`` with a :class:`MultiIndex` index) will now raise a ``KeyError``, rather than returning an empty ``slice``, if asked a label which is present in the ``levels`` but is unused (:issue:`22221`)
- :class:`MultiIndex` has gained the :meth:`MultiIndex.from_frame`, it allows constructing a :class:`MultiIndex` object from a :class:`DataFrame` (:issue:`22420`)
- Fix ``TypeError`` in Python 3 when creating :class:`MultiIndex` in which some levels have mixed types, e.g. when some labels are tuples (:issue:`15457`)

IO
^^

- Bug in :func:`read_csv` in which a column specified with ``CategoricalDtype`` of boolean categories was not being correctly coerced from string values to booleans (:issue:`20498`)
- Bug in :func:`read_csv` in which unicode column names were not being properly recognized with Python 2.x (:issue:`13253`)
- Bug in :meth:`DataFrame.to_sql` when writing timezone aware data (``datetime64[ns, tz]`` dtype) would raise a ``TypeError`` (:issue:`9086`)
- Bug in :meth:`DataFrame.to_sql` where a naive :class:`DatetimeIndex` would be written as ``TIMESTAMP WITH TIMEZONE`` type in supported databases, e.g. PostgreSQL (:issue:`23510`)
- Bug in :meth:`read_excel()` when ``parse_cols`` is specified with an empty dataset (:issue:`9208`)
- :func:`read_html()` no longer ignores all-whitespace ``<tr>`` within ``<thead>`` when considering the ``skiprows`` and ``header`` arguments. Previously, users had to decrease their ``header`` and ``skiprows`` values on such tables to work around the issue. (:issue:`21641`)
- :func:`read_excel()` will correctly show the deprecation warning for previously deprecated ``sheetname`` (:issue:`17994`)
- :func:`read_csv()` and :func:`read_table()` will throw ``UnicodeError`` and not coredump on badly encoded strings (:issue:`22748`)
- :func:`read_csv()` will correctly parse timezone-aware datetimes (:issue:`22256`)
- Bug in :func:`read_csv()` in which memory management was prematurely optimized for the C engine when the data was being read in chunks (:issue:`23509`)
- Bug in :func:`read_csv()` in unnamed columns were being improperly identified when extracting a multi-index (:issue:`23687`)
- :func:`read_sas()` will parse numbers in sas7bdat-files that have width less than 8 bytes correctly. (:issue:`21616`)
- :func:`read_sas()` will correctly parse sas7bdat files with many columns (:issue:`22628`)
- :func:`read_sas()` will correctly parse sas7bdat files with data page types having also bit 7 set (so page type is 128 + 256 = 384) (:issue:`16615`)
- Bug in :func:`read_sas()` in which an incorrect error was raised on an invalid file format. (:issue:`24548`)
- Bug in :meth:`detect_client_encoding` where potential ``IOError`` goes unhandled when importing in a mod_wsgi process due to restricted access to stdout. (:issue:`21552`)
- Bug in :func:`DataFrame.to_html()` with ``index=False`` misses truncation indicators (...) on truncated DataFrame (:issue:`15019`, :issue:`22783`)
- Bug in :func:`DataFrame.to_html()` with ``index=False`` when both columns and row index are ``MultiIndex`` (:issue:`22579`)
- Bug in :func:`DataFrame.to_html()` with ``index_names=False`` displaying index name (:issue:`22747`)
- Bug in :func:`DataFrame.to_html()` with ``header=False`` not displaying row index names (:issue:`23788`)
- Bug in :func:`DataFrame.to_html()` with ``sparsify=False`` that caused it to raise ``TypeError`` (:issue:`22887`)
- Bug in :func:`DataFrame.to_string()` that broke column alignment when ``index=False`` and width of first column's values is greater than the width of first column's header (:issue:`16839`, :issue:`13032`)
- Bug in :func:`DataFrame.to_string()` that caused representations of :class:`DataFrame` to not take up the whole window (:issue:`22984`)
- Bug in :func:`DataFrame.to_csv` where a single level MultiIndex incorrectly wrote a tuple. Now just the value of the index is written (:issue:`19589`).
- :class:`HDFStore` will raise ``ValueError`` when the ``format`` kwarg is passed to the constructor (:issue:`13291`)
- Bug in :meth:`HDFStore.append` when appending a :class:`DataFrame` with an empty string column and ``min_itemsize`` < 8 (:issue:`12242`)
- Bug in :func:`read_csv()` in which memory leaks occurred in the C engine when parsing ``NaN`` values due to insufficient cleanup on completion or error (:issue:`21353`)
- Bug in :func:`read_csv()` in which incorrect error messages were being raised when ``skipfooter`` was passed in along with ``nrows``, ``iterator``, or ``chunksize`` (:issue:`23711`)
- Bug in :func:`read_csv()` in which :class:`MultiIndex` index names were being improperly handled in the cases when they were not provided (:issue:`23484`)
- Bug in :func:`read_csv()` in which unnecessary warnings were being raised when the dialect's values conflicted with the default arguments (:issue:`23761`)
- Bug in :func:`read_html()` in which the error message was not displaying the valid flavors when an invalid one was provided (:issue:`23549`)
- Bug in :meth:`read_excel()` in which extraneous header names were extracted, even though none were specified (:issue:`11733`)
- Bug in :meth:`read_excel()` in which column names were not being properly converted to string sometimes in Python 2.x (:issue:`23874`)
- Bug in :meth:`read_excel()` in which ``index_col=None`` was not being respected and parsing index columns anyway (:issue:`18792`, :issue:`20480`)
- Bug in :meth:`read_excel()` in which ``usecols`` was not being validated for proper column names when passed in as a string (:issue:`20480`)
- Bug in :meth:`DataFrame.to_dict` when the resulting dict contains non-Python scalars in the case of numeric data (:issue:`23753`)
- :func:`DataFrame.to_string()`, :func:`DataFrame.to_html()`, :func:`DataFrame.to_latex()` will correctly format output when a string is passed as the ``float_format`` argument (:issue:`21625`, :issue:`22270`)
- Bug in :func:`read_csv` that caused it to raise ``OverflowError`` when trying to use 'inf' as ``na_value`` with integer index column (:issue:`17128`)
- Bug in :func:`read_csv` that caused the C engine on Python 3.6+ on Windows to improperly read CSV filenames with accented or special characters (:issue:`15086`)
- Bug in :func:`read_fwf` in which the compression type of a file was not being properly inferred (:issue:`22199`)
- Bug in :func:`pandas.io.json.json_normalize` that caused it to raise ``TypeError`` when two consecutive elements of ``record_path`` are dicts (:issue:`22706`)
- Bug in :meth:`DataFrame.to_stata`, :class:`pandas.io.stata.StataWriter` and :class:`pandas.io.stata.StataWriter117` where a exception would leave a partially written and invalid dta file (:issue:`23573`)
- Bug in :meth:`DataFrame.to_stata` and :class:`pandas.io.stata.StataWriter117` that produced invalid files when using strLs with non-ASCII characters (:issue:`23573`)
- Bug in :class:`HDFStore` that caused it to raise ``ValueError`` when reading a Dataframe in Python 3 from fixed format written in Python 2 (:issue:`24510`)
- Bug in :func:`DataFrame.to_string()` and more generally in the floating ``repr`` formatter. Zeros were not trimmed if ``inf`` was present in a columns while it was the case with NA values. Zeros are now trimmed as in the presence of NA (:issue:`24861`).
- Bug in the ``repr`` when truncating the number of columns and having a wide last column (:issue:`24849`).

Plotting
^^^^^^^^

- Bug in :func:`DataFrame.plot.scatter` and :func:`DataFrame.plot.hexbin` caused x-axis label and ticklabels to disappear when colorbar was on in IPython inline backend (:issue:`10611`, :issue:`10678`, and :issue:`20455`)
- Bug in plotting a Series with datetimes using :func:`matplotlib.axes.Axes.scatter` (:issue:`22039`)
- Bug in :func:`DataFrame.plot.bar` caused bars to use multiple colors instead of a single one (:issue:`20585`)
- Bug in validating color parameter caused extra color to be appended to the given color array. This happened to multiple plotting functions using matplotlib. (:issue:`20726`)

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in :func:`pandas.core.window.Rolling.min` and :func:`pandas.core.window.Rolling.max` with ``closed='left'``, a datetime-like index and only one entry in the series leading to segfault (:issue:`24718`)
- Bug in :func:`pandas.core.groupby.GroupBy.first` and :func:`pandas.core.groupby.GroupBy.last` with ``as_index=False`` leading to the loss of timezone information (:issue:`15884`)
- Bug in :meth:`DateFrame.resample` when downsampling across a DST boundary (:issue:`8531`)
- Bug in date anchoring for :meth:`DateFrame.resample` with offset :class:`Day` when n > 1 (:issue:`24127`)
- Bug where ``ValueError`` is wrongly raised when calling :func:`~pandas.core.groupby.SeriesGroupBy.count` method of a
  ``SeriesGroupBy`` when the grouping variable only contains NaNs and numpy version < 1.13 (:issue:`21956`).
- Multiple bugs in :func:`pandas.core.window.Rolling.min` with ``closed='left'`` and a
  datetime-like index leading to incorrect results and also segfault. (:issue:`21704`)
- Bug in :meth:`pandas.core.resample.Resampler.apply` when passing positional arguments to applied func (:issue:`14615`).
- Bug in :meth:`Series.resample` when passing ``numpy.timedelta64`` to ``loffset`` kwarg (:issue:`7687`).
- Bug in :meth:`pandas.core.resample.Resampler.asfreq` when frequency of ``TimedeltaIndex`` is a subperiod of a new frequency (:issue:`13022`).
- Bug in :meth:`pandas.core.groupby.SeriesGroupBy.mean` when values were integral but could not fit inside of int64, overflowing instead. (:issue:`22487`)
- :func:`pandas.core.groupby.RollingGroupby.agg` and :func:`pandas.core.groupby.ExpandingGroupby.agg` now support multiple aggregation functions as parameters (:issue:`15072`)
- Bug in :meth:`DataFrame.resample` and :meth:`Series.resample` when resampling by a weekly offset (``'W'``) across a DST transition (:issue:`9119`, :issue:`21459`)
- Bug in :meth:`DataFrame.expanding` in which the ``axis`` argument was not being respected during aggregations (:issue:`23372`)
- Bug in :meth:`pandas.core.groupby.GroupBy.transform` which caused missing values when the input function can accept a :class:`DataFrame` but renames it (:issue:`23455`).
- Bug in :func:`pandas.core.groupby.GroupBy.nth` where column order was not always preserved (:issue:`20760`)
- Bug in :meth:`pandas.core.groupby.GroupBy.rank` with ``method='dense'`` and ``pct=True`` when a group has only one member would raise a ``ZeroDivisionError`` (:issue:`23666`).
- Calling :meth:`pandas.core.groupby.GroupBy.rank` with empty groups and ``pct=True`` was raising a ``ZeroDivisionError`` (:issue:`22519`)
- Bug in :meth:`DataFrame.resample` when resampling ``NaT`` in ``TimeDeltaIndex`` (:issue:`13223`).
- Bug in :meth:`DataFrame.groupby` did not respect the ``observed`` argument when selecting a column and instead always used ``observed=False`` (:issue:`23970`)
- Bug in :func:`pandas.core.groupby.SeriesGroupBy.pct_change` or :func:`pandas.core.groupby.DataFrameGroupBy.pct_change` would previously work across groups when calculating the percent change, where it now correctly works per group (:issue:`21200`, :issue:`21235`).
- Bug preventing hash table creation with very large number (2^32) of rows (:issue:`22805`)
- Bug in groupby when grouping on categorical causes ``ValueError`` and incorrect grouping if ``observed=True`` and ``nan`` is present in categorical column (:issue:`24740`, :issue:`21151`).

Reshaping
^^^^^^^^^

- Bug in :func:`pandas.concat` when joining resampled DataFrames with timezone aware index (:issue:`13783`)
- Bug in :func:`pandas.concat` when joining only ``Series`` the ``names`` argument of ``concat`` is no longer ignored (:issue:`23490`)
- Bug in :meth:`Series.combine_first` with ``datetime64[ns, tz]`` dtype which would return tz-naive result (:issue:`21469`)
- Bug in :meth:`Series.where` and :meth:`DataFrame.where` with ``datetime64[ns, tz]`` dtype (:issue:`21546`)
- Bug in :meth:`DataFrame.where` with an empty DataFrame and empty ``cond`` having non-bool dtype (:issue:`21947`)
- Bug in :meth:`Series.mask` and :meth:`DataFrame.mask` with ``list`` conditionals (:issue:`21891`)
- Bug in :meth:`DataFrame.replace` raises RecursionError when converting OutOfBounds ``datetime64[ns, tz]`` (:issue:`20380`)
- :func:`pandas.core.groupby.GroupBy.rank` now raises a ``ValueError`` when an invalid value is passed for argument ``na_option`` (:issue:`22124`)
- Bug in :func:`get_dummies` with Unicode attributes in Python 2 (:issue:`22084`)
- Bug in :meth:`DataFrame.replace` raises ``RecursionError`` when replacing empty lists (:issue:`22083`)
- Bug in :meth:`Series.replace` and :meth:`DataFrame.replace` when dict is used as the ``to_replace`` value and one key in the dict is another key's value, the results were inconsistent between using integer key and using string key (:issue:`20656`)
- Bug in :meth:`DataFrame.drop_duplicates` for empty ``DataFrame`` which incorrectly raises an error (:issue:`20516`)
- Bug in :func:`pandas.wide_to_long` when a string is passed to the stubnames argument and a column name is a substring of that stubname (:issue:`22468`)
- Bug in :func:`merge` when merging ``datetime64[ns, tz]`` data that contained a DST transition (:issue:`18885`)
- Bug in :func:`merge_asof` when merging on float values within defined tolerance (:issue:`22981`)
- Bug in :func:`pandas.concat` when concatenating a multicolumn DataFrame with tz-aware data against a DataFrame with a different number of columns (:issue:`22796`)
- Bug in :func:`merge_asof` where confusing error message raised when attempting to merge with missing values (:issue:`23189`)
- Bug in :meth:`DataFrame.nsmallest` and :meth:`DataFrame.nlargest` for dataframes that have a :class:`MultiIndex` for columns (:issue:`23033`).
- Bug in :func:`pandas.melt` when passing column names that are not present in ``DataFrame`` (:issue:`23575`)
- Bug in :meth:`DataFrame.append` with a :class:`Series` with a dateutil timezone would raise a ``TypeError`` (:issue:`23682`)
- Bug in :class:`Series` construction when passing no data and ``dtype=str`` (:issue:`22477`)
- Bug in :func:`cut` with ``bins`` as an overlapping ``IntervalIndex`` where multiple bins were returned per item instead of raising a ``ValueError`` (:issue:`23980`)
- Bug in :func:`pandas.concat` when joining ``Series`` datetimetz with ``Series`` category would lose timezone (:issue:`23816`)
- Bug in :meth:`DataFrame.join` when joining on partial MultiIndex would drop names (:issue:`20452`).
- :meth:`DataFrame.nlargest` and :meth:`DataFrame.nsmallest` now returns the correct n values when keep != 'all' also when tied on the first columns (:issue:`22752`)
- Constructing a DataFrame with an index argument that wasn't already an instance of :class:`~pandas.core.Index` was broken (:issue:`22227`).
- Bug in :class:`DataFrame` prevented list subclasses to be used to construction (:issue:`21226`)
- Bug in :func:`DataFrame.unstack` and :func:`DataFrame.pivot_table` returning a misleading error message when the resulting DataFrame has more elements than int32 can handle. Now, the error message is improved, pointing towards the actual problem (:issue:`20601`)
- Bug in :func:`DataFrame.unstack` where a ``ValueError`` was raised when unstacking timezone aware values (:issue:`18338`)
- Bug in :func:`DataFrame.stack` where timezone aware values were converted to timezone naive values (:issue:`19420`)
- Bug in :func:`merge_asof` where a ``TypeError`` was raised when ``by_col`` were timezone aware values (:issue:`21184`)
- Bug showing an incorrect shape when throwing error during ``DataFrame`` construction. (:issue:`20742`)

.. _whatsnew_0240.bug_fixes.sparse:

Sparse
^^^^^^

- Updating a boolean, datetime, or timedelta column to be Sparse now works (:issue:`22367`)
- Bug in :meth:`Series.to_sparse` with Series already holding sparse data not constructing properly (:issue:`22389`)
- Providing a ``sparse_index`` to the SparseArray constructor no longer defaults the na-value to ``np.nan`` for all dtypes. The correct na_value for ``data.dtype`` is now used.
- Bug in ``SparseArray.nbytes`` under-reporting its memory usage by not including the size of its sparse index.
- Improved performance of :meth:`Series.shift` for non-NA ``fill_value``, as values are no longer converted to a dense array.
- Bug in ``DataFrame.groupby`` not including ``fill_value`` in the groups for non-NA ``fill_value`` when grouping by a sparse column (:issue:`5078`)
- Bug in unary inversion operator (``~``) on a ``SparseSeries`` with boolean values. The performance of this has also been improved (:issue:`22835`)
- Bug in :meth:`SparseArary.unique` not returning the unique values (:issue:`19595`)
- Bug in :meth:`SparseArray.nonzero` and :meth:`SparseDataFrame.dropna` returning shifted/incorrect results (:issue:`21172`)
- Bug in :meth:`DataFrame.apply` where dtypes would lose sparseness (:issue:`23744`)
- Bug in :func:`concat` when concatenating a list of :class:`Series` with all-sparse values changing the ``fill_value`` and converting to a dense Series (:issue:`24371`)

Style
^^^^^

- :meth:`~pandas.io.formats.style.Styler.background_gradient` now takes a ``text_color_threshold`` parameter to automatically lighten the text color based on the luminance of the background color. This improves readability with dark background colors without the need to limit the background colormap range. (:issue:`21258`)
- :meth:`~pandas.io.formats.style.Styler.background_gradient` now also supports tablewise application (in addition to rowwise and columnwise) with ``axis=None`` (:issue:`15204`)
- :meth:`~pandas.io.formats.style.Styler.bar` now also supports tablewise application (in addition to rowwise and columnwise) with ``axis=None`` and setting clipping range with ``vmin`` and ``vmax`` (:issue:`21548` and :issue:`21526`). ``NaN`` values are also handled properly.

Build changes
^^^^^^^^^^^^^

- Building pandas for development now requires ``cython >= 0.28.2`` (:issue:`21688`)
- Testing pandas now requires ``hypothesis>=3.58``.  You can find `the Hypothesis docs here <https://hypothesis.readthedocs.io/en/latest/index.html>`_, and a pandas-specific introduction :ref:`in the contributing guide <using-hypothesis>`. (:issue:`22280`)
- Building pandas on macOS now targets minimum macOS 10.9 if run on macOS 10.9 or above (:issue:`23424`)

Other
^^^^^

- Bug where C variables were declared with external linkage causing import errors if certain other C libraries were imported before pandas. (:issue:`24113`)


.. _whatsnew_0.24.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.23.4..v0.24.0
.. _whatsnew_133:

What's new in 1.3.3 (September 12, 2021)
----------------------------------------

These are the changes in pandas 1.3.3. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_133.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :class:`DataFrame` constructor failing to broadcast for defined :class:`Index` and len one list of :class:`Timestamp` (:issue:`42810`)
- Fixed regression in :meth:`.GroupBy.agg` incorrectly raising in some cases (:issue:`42390`)
- Fixed regression in :meth:`.GroupBy.apply` where ``nan`` values were dropped even with ``dropna=False`` (:issue:`43205`)
- Fixed regression in :meth:`.GroupBy.quantile` which was failing with ``pandas.NA`` (:issue:`42849`)
- Fixed regression in :meth:`merge` where ``on`` columns with ``ExtensionDtype`` or ``bool`` data types were cast to ``object`` in ``right`` and ``outer`` merge (:issue:`40073`)
- Fixed regression in :meth:`RangeIndex.where` and :meth:`RangeIndex.putmask` raising ``AssertionError`` when result did not represent a :class:`RangeIndex` (:issue:`43240`)
- Fixed regression in :meth:`read_parquet` where the ``fastparquet`` engine would not work properly with fastparquet 0.7.0 (:issue:`43075`)
- Fixed regression in :meth:`DataFrame.loc.__setitem__` raising ``ValueError`` when setting array as cell value (:issue:`43422`)
- Fixed regression in :func:`is_list_like` where objects with ``__iter__`` set to ``None`` would be identified as iterable (:issue:`43373`)
- Fixed regression in :meth:`DataFrame.__getitem__` raising error for slice of :class:`DatetimeIndex` when index is non monotonic (:issue:`43223`)
- Fixed regression in :meth:`.Resampler.aggregate` when used after column selection would raise if ``func`` is a list of aggregation functions (:issue:`42905`)
- Fixed regression in :meth:`DataFrame.corr` where Kendall correlation would produce incorrect results for columns with repeated values (:issue:`43401`)
- Fixed regression in :meth:`DataFrame.groupby` where aggregation on columns with object types dropped results on those columns (:issue:`42395`, :issue:`43108`)
- Fixed regression in :meth:`Series.fillna` raising ``TypeError`` when filling ``float`` ``Series`` with list-like fill value having a dtype which couldn't cast lostlessly (like ``float32`` filled with ``float64``) (:issue:`43424`)
- Fixed regression in :func:`read_csv` raising ``AttributeError`` when the file handle is an ``tempfile.SpooledTemporaryFile`` object (:issue:`43439`)
- Fixed performance regression in :meth:`core.window.ewm.ExponentialMovingWindow.mean` (:issue:`42333`)

.. ---------------------------------------------------------------------------

.. _whatsnew_133.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~
- Performance improvement for :meth:`DataFrame.__setitem__` when the key or value is not a :class:`DataFrame`, or key is not list-like (:issue:`43274`)

.. ---------------------------------------------------------------------------

.. _whatsnew_133.bug_fixes:

Bug fixes
~~~~~~~~~
- Fixed bug in :meth:`.DataFrameGroupBy.agg` and :meth:`.DataFrameGroupBy.transform` with ``engine="numba"`` where ``index`` data was not being correctly passed into ``func`` (:issue:`43133`)

.. ---------------------------------------------------------------------------

.. _whatsnew_133.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.2..v1.3.3
.. _whatsnew_0234:

What's new in 0.23.4 (August 3, 2018)
-------------------------------------

{{ header }}


This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes
and bug fixes. We recommend that all users upgrade to this version.

.. warning::

   Starting January 1, 2019, pandas feature releases will support Python 3 only.
   See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

.. contents:: What's new in v0.23.4
    :local:
    :backlinks: none

.. _whatsnew_0234.fixed_regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Python 3.7 with Windows gave all missing values for rolling variance calculations (:issue:`21813`)

.. _whatsnew_0234.bug_fixes:

Bug fixes
~~~~~~~~~

**Groupby/resample/rolling**

- Bug where calling :func:`DataFrameGroupBy.agg` with a list of functions including ``ohlc`` as the non-initial element would raise a ``ValueError`` (:issue:`21716`)
- Bug in ``roll_quantile`` caused a memory leak when calling ``.rolling(...).quantile(q)`` with ``q`` in (0,1) (:issue:`21965`)

**Missing**

- Bug in :func:`Series.clip` and :func:`DataFrame.clip` cannot accept list-like threshold containing ``NaN`` (:issue:`19992`)

.. _whatsnew_0.23.4.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.23.3..v0.23.4
.. _whatsnew_0141:

Version 0.14.1 (July 11, 2014)
------------------------------

{{ header }}


This is a minor release from 0.14.0 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

- Highlights include:

  - New methods :meth:`~pandas.DataFrame.select_dtypes` to select columns
    based on the dtype and :meth:`~pandas.Series.sem` to calculate the
    standard error of the mean.
  - Support for dateutil timezones (see :ref:`docs <timeseries.timezone>`).
  - Support for ignoring full line comments in the :func:`~pandas.read_csv`
    text parser.
  - New documentation section on :ref:`Options and Settings <options>`.
  - Lots of bug fixes.

- :ref:`Enhancements <whatsnew_0141.enhancements>`
- :ref:`API Changes <whatsnew_0141.api>`
- :ref:`Performance Improvements <whatsnew_0141.performance>`
- :ref:`Experimental Changes <whatsnew_0141.experimental>`
- :ref:`Bug Fixes <whatsnew_0141.bug_fixes>`

.. _whatsnew_0141.api:

API changes
~~~~~~~~~~~

- Openpyxl now raises a ValueError on construction of the openpyxl writer
  instead of warning on pandas import (:issue:`7284`).

- For ``StringMethods.extract``, when no match is found, the result - only
  containing ``NaN`` values - now also has ``dtype=object`` instead of
  ``float`` (:issue:`7242`)

- ``Period`` objects no longer raise a ``TypeError`` when compared using ``==``
  with another object that *isn't* a ``Period``. Instead
  when comparing a ``Period`` with another object using ``==`` if the other
  object isn't a ``Period`` ``False`` is returned. (:issue:`7376`)

- Previously, the behaviour on resetting the time or not in
  ``offsets.apply``, ``rollforward`` and ``rollback`` operations differed
  between offsets. With the support of the ``normalize`` keyword for all offsets(see
  below) with a default value of False (preserve time), the behaviour changed for certain
  offsets (BusinessMonthBegin, MonthEnd, BusinessMonthEnd, CustomBusinessMonthEnd,
  BusinessYearBegin, LastWeekOfMonth, FY5253Quarter, LastWeekOfMonth, Easter):

  .. code-block:: ipython

    In [6]: from pandas.tseries import offsets

    In [7]: d = pd.Timestamp('2014-01-01 09:00')

    # old behaviour < 0.14.1
    In [8]: d + offsets.MonthEnd()
    Out[8]: pd.Timestamp('2014-01-31 00:00:00')

  Starting from 0.14.1 all offsets preserve time by default. The old
  behaviour can be obtained with ``normalize=True``

  .. ipython:: python
     :suppress:

     import pandas.tseries.offsets as offsets

     d = pd.Timestamp("2014-01-01 09:00")

  .. ipython:: python

     # new behaviour
     d + offsets.MonthEnd()
     d + offsets.MonthEnd(normalize=True)

  Note that for the other offsets the default behaviour did not change.

- Add back ``#N/A N/A`` as a default NA value in text parsing, (regression from 0.12) (:issue:`5521`)
- Raise a ``TypeError`` on inplace-setting with a ``.where`` and a non ``np.nan`` value as this is inconsistent
  with a set-item expression like ``df[mask] = None`` (:issue:`7656`)


.. _whatsnew_0141.enhancements:

Enhancements
~~~~~~~~~~~~

- Add ``dropna`` argument to ``value_counts`` and ``nunique`` (:issue:`5569`).
- Add :meth:`~pandas.DataFrame.select_dtypes` method to allow selection of
  columns based on dtype (:issue:`7316`). See :ref:`the docs <basics.selectdtypes>`.
- All ``offsets`` supports the ``normalize`` keyword to specify whether
  ``offsets.apply``, ``rollforward`` and ``rollback`` resets the time (hour,
  minute, etc) or not (default ``False``, preserves time) (:issue:`7156`):

  .. code-block:: python

     import pandas.tseries.offsets as offsets

     day = offsets.Day()
     day.apply(pd.Timestamp("2014-01-01 09:00"))

     day = offsets.Day(normalize=True)
     day.apply(pd.Timestamp("2014-01-01 09:00"))

- ``PeriodIndex`` is represented as the same format as ``DatetimeIndex`` (:issue:`7601`)
- ``StringMethods`` now work on empty Series (:issue:`7242`)
- The file parsers ``read_csv`` and ``read_table`` now ignore line comments provided by
  the parameter ``comment``, which accepts only a single character for the C reader.
  In particular, they allow for comments before file data begins (:issue:`2685`)
- Add ``NotImplementedError`` for simultaneous use of ``chunksize`` and ``nrows``
  for read_csv() (:issue:`6774`).
- Tests for basic reading of public S3 buckets now exist (:issue:`7281`).
- ``read_html`` now sports an ``encoding`` argument that is passed to the
  underlying parser library. You can use this to read non-ascii encoded web
  pages (:issue:`7323`).
- ``read_excel`` now supports reading from URLs in the same way
  that ``read_csv`` does.  (:issue:`6809`)
- Support for dateutil timezones, which can now be used in the same way as
  pytz timezones across pandas. (:issue:`4688`)

  .. ipython:: python

     rng = pd.date_range(
         "3/6/2012 00:00", periods=10, freq="D", tz="dateutil/Europe/London"
     )
     rng.tz

  See :ref:`the docs <timeseries.timezone>`.

- Implemented ``sem`` (standard error of the mean) operation for ``Series``,
  ``DataFrame``, ``Panel``, and ``Groupby`` (:issue:`6897`)
- Add ``nlargest`` and ``nsmallest`` to the ``Series`` ``groupby`` allowlist,
  which means you can now use these methods on a ``SeriesGroupBy`` object
  (:issue:`7053`).
- All offsets ``apply``, ``rollforward`` and ``rollback`` can now handle ``np.datetime64``, previously results in ``ApplyTypeError`` (:issue:`7452`)
- ``Period`` and ``PeriodIndex`` can contain ``NaT`` in its values (:issue:`7485`)
- Support pickling ``Series``, ``DataFrame`` and ``Panel`` objects with
  non-unique labels along *item* axis (``index``, ``columns`` and ``items``
  respectively) (:issue:`7370`).
- Improved inference of datetime/timedelta with mixed null objects. Regression from 0.13.1 in interpretation of an object Index
  with all null elements (:issue:`7431`)

.. _whatsnew_0141.performance:

Performance
~~~~~~~~~~~
- Improvements in dtype inference for numeric operations involving yielding performance gains for dtypes: ``int64``, ``timedelta64``, ``datetime64`` (:issue:`7223`)
- Improvements in Series.transform for significant performance gains (:issue:`6496`)
- Improvements in DataFrame.transform with ufuncs and built-in grouper functions for significant performance gains (:issue:`7383`)
- Regression in groupby aggregation of datetime64 dtypes (:issue:`7555`)
- Improvements in ``MultiIndex.from_product`` for large iterables (:issue:`7627`)


.. _whatsnew_0141.experimental:

Experimental
~~~~~~~~~~~~

- ``pandas.io.data.Options`` has a new method, ``get_all_data`` method, and now consistently returns a
  MultiIndexed ``DataFrame`` (:issue:`5602`)
- ``io.gbq.read_gbq`` and ``io.gbq.to_gbq`` were refactored to remove the
  dependency on the Google ``bq.py`` command line client. This submodule
  now uses ``httplib2`` and the Google ``apiclient`` and ``oauth2client`` API client
  libraries which should be more stable and, therefore, reliable than
  ``bq.py``. See :ref:`the docs <io.bigquery>`. (:issue:`6937`).


.. _whatsnew_0141.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in ``DataFrame.where`` with a symmetric shaped frame and a passed other of a DataFrame (:issue:`7506`)
- Bug in Panel indexing with a MultiIndex axis (:issue:`7516`)
- Regression in datetimelike slice indexing with a duplicated index and non-exact end-points (:issue:`7523`)
- Bug in setitem with list-of-lists and single vs mixed types (:issue:`7551`:)
- Bug in time ops with non-aligned Series (:issue:`7500`)
- Bug in timedelta inference when assigning an incomplete Series (:issue:`7592`)
- Bug in groupby ``.nth`` with a Series and integer-like column name (:issue:`7559`)
- Bug in ``Series.get`` with a boolean accessor (:issue:`7407`)
- Bug in ``value_counts`` where ``NaT`` did not qualify as missing (``NaN``) (:issue:`7423`)
- Bug in ``to_timedelta`` that accepted invalid units and misinterpreted 'm/h' (:issue:`7611`, :issue:`6423`)
- Bug in line plot doesn't set correct ``xlim`` if ``secondary_y=True`` (:issue:`7459`)
- Bug in grouped ``hist`` and ``scatter`` plots use old ``figsize`` default (:issue:`7394`)
- Bug in plotting subplots with ``DataFrame.plot``, ``hist`` clears passed ``ax`` even if the number of subplots is one (:issue:`7391`).
- Bug in plotting subplots with ``DataFrame.boxplot`` with ``by`` kw raises ``ValueError`` if the number of subplots exceeds 1 (:issue:`7391`).
- Bug in subplots displays ``ticklabels`` and ``labels`` in different rule (:issue:`5897`)
- Bug in ``Panel.apply`` with a MultiIndex as an axis (:issue:`7469`)
- Bug in ``DatetimeIndex.insert`` doesn't preserve ``name`` and ``tz`` (:issue:`7299`)
- Bug in ``DatetimeIndex.asobject`` doesn't preserve ``name`` (:issue:`7299`)
- Bug in MultiIndex slicing with datetimelike ranges (strings and Timestamps), (:issue:`7429`)
- Bug in ``Index.min`` and ``max`` doesn't handle ``nan`` and ``NaT`` properly (:issue:`7261`)
- Bug in ``PeriodIndex.min/max`` results in ``int`` (:issue:`7609`)
- Bug in ``resample`` where ``fill_method`` was ignored if you passed ``how`` (:issue:`2073`)
- Bug in ``TimeGrouper`` doesn't exclude column specified by ``key`` (:issue:`7227`)
- Bug in ``DataFrame`` and ``Series`` bar and barh plot raises ``TypeError`` when ``bottom``
  and ``left`` keyword is specified (:issue:`7226`)
- Bug in ``DataFrame.hist`` raises ``TypeError`` when it contains non numeric column (:issue:`7277`)
- Bug in ``Index.delete`` does not preserve ``name`` and ``freq`` attributes (:issue:`7302`)
- Bug in ``DataFrame.query()``/``eval`` where local string variables with the @
  sign were being treated as temporaries attempting to be deleted
  (:issue:`7300`).
- Bug in ``Float64Index`` which didn't allow duplicates (:issue:`7149`).
- Bug in ``DataFrame.replace()`` where truthy values were being replaced
  (:issue:`7140`).
- Bug in ``StringMethods.extract()`` where a single match group Series
  would use the matcher's name instead of the group name (:issue:`7313`).
- Bug in ``isnull()`` when ``mode.use_inf_as_null == True`` where isnull
  wouldn't test ``True`` when it encountered an ``inf``/``-inf``
  (:issue:`7315`).
- Bug in inferred_freq results in None for eastern hemisphere timezones (:issue:`7310`)
- Bug in ``Easter`` returns incorrect date when offset is negative (:issue:`7195`)
- Bug in broadcasting with ``.div``, integer dtypes and divide-by-zero (:issue:`7325`)
- Bug in ``CustomBusinessDay.apply`` raises ``NameError`` when ``np.datetime64`` object is passed (:issue:`7196`)
- Bug in ``MultiIndex.append``, ``concat`` and ``pivot_table`` don't preserve timezone (:issue:`6606`)
- Bug in ``.loc`` with a list of indexers on a single-multi index level (that is not nested) (:issue:`7349`)
- Bug in ``Series.map`` when mapping a dict with tuple keys of different lengths (:issue:`7333`)
- Bug all ``StringMethods`` now work on empty Series (:issue:`7242`)
- Fix delegation of ``read_sql`` to ``read_sql_query`` when query does not contain 'select' (:issue:`7324`).
- Bug where a string column name assignment to a ``DataFrame`` with a
  ``Float64Index`` raised a ``TypeError`` during a call to ``np.isnan``
  (:issue:`7366`).
- Bug where ``NDFrame.replace()`` didn't correctly replace objects with
  ``Period`` values (:issue:`7379`).
- Bug in ``.ix`` getitem should always return a Series (:issue:`7150`)
- Bug in MultiIndex slicing with incomplete indexers (:issue:`7399`)
- Bug in MultiIndex slicing with a step in a sliced level (:issue:`7400`)
- Bug where negative indexers in ``DatetimeIndex`` were not correctly sliced
  (:issue:`7408`)
- Bug where ``NaT`` wasn't repr'd correctly in a ``MultiIndex`` (:issue:`7406`,
  :issue:`7409`).
- Bug where bool objects were converted to ``nan`` in ``convert_objects``
  (:issue:`7416`).
- Bug in ``quantile`` ignoring the axis keyword argument (:issue:`7306`)
- Bug where ``nanops._maybe_null_out`` doesn't work with complex numbers
  (:issue:`7353`)
- Bug in several ``nanops`` functions when ``axis==0`` for
  1-dimensional ``nan`` arrays (:issue:`7354`)
- Bug where ``nanops.nanmedian`` doesn't work when ``axis==None``
  (:issue:`7352`)
- Bug where ``nanops._has_infs`` doesn't work with many dtypes
  (:issue:`7357`)
- Bug in ``StataReader.data`` where reading a 0-observation dta failed (:issue:`7369`)
- Bug in ``StataReader`` when reading Stata 13 (117) files containing fixed width strings (:issue:`7360`)
- Bug in ``StataWriter`` where encoding was ignored (:issue:`7286`)
- Bug in ``DatetimeIndex`` comparison doesn't handle ``NaT`` properly (:issue:`7529`)
- Bug in passing input with ``tzinfo`` to some offsets ``apply``, ``rollforward`` or ``rollback`` resets ``tzinfo`` or raises ``ValueError`` (:issue:`7465`)
- Bug in ``DatetimeIndex.to_period``, ``PeriodIndex.asobject``, ``PeriodIndex.to_timestamp`` doesn't preserve ``name`` (:issue:`7485`)
- Bug in ``DatetimeIndex.to_period`` and ``PeriodIndex.to_timestamp`` handle ``NaT`` incorrectly (:issue:`7228`)
- Bug in ``offsets.apply``, ``rollforward`` and ``rollback`` may return normal ``datetime`` (:issue:`7502`)
- Bug in ``resample`` raises ``ValueError`` when target contains ``NaT`` (:issue:`7227`)
- Bug in ``Timestamp.tz_localize`` resets ``nanosecond`` info (:issue:`7534`)
- Bug in ``DatetimeIndex.asobject`` raises ``ValueError`` when it contains ``NaT`` (:issue:`7539`)
- Bug in ``Timestamp.__new__`` doesn't preserve nanosecond properly (:issue:`7610`)
- Bug in ``Index.astype(float)`` where it would return an ``object`` dtype
  ``Index`` (:issue:`7464`).
- Bug in ``DataFrame.reset_index`` loses ``tz`` (:issue:`3950`)
- Bug in ``DatetimeIndex.freqstr`` raises ``AttributeError`` when ``freq`` is ``None`` (:issue:`7606`)
- Bug in ``GroupBy.size`` created by ``TimeGrouper`` raises ``AttributeError`` (:issue:`7453`)
- Bug in single column bar plot is misaligned (:issue:`7498`).
- Bug in area plot with tz-aware time series raises ``ValueError`` (:issue:`7471`)
- Bug in non-monotonic ``Index.union`` may preserve ``name`` incorrectly (:issue:`7458`)
- Bug in ``DatetimeIndex.intersection`` doesn't preserve timezone (:issue:`4690`)
- Bug in ``rolling_var`` where a window larger than the array would raise an error(:issue:`7297`)
- Bug with last plotted timeseries dictating ``xlim`` (:issue:`2960`)
- Bug with ``secondary_y`` axis not being considered for timeseries ``xlim`` (:issue:`3490`)
- Bug in ``Float64Index`` assignment with a non scalar indexer (:issue:`7586`)
- Bug in ``pandas.core.strings.str_contains`` does not properly match in a case insensitive fashion when ``regex=False`` and ``case=False`` (:issue:`7505`)
- Bug in ``expanding_cov``, ``expanding_corr``, ``rolling_cov``, and ``rolling_corr`` for two arguments with mismatched index  (:issue:`7512`)
- Bug in ``to_sql`` taking the boolean column as text column (:issue:`7678`)
- Bug in grouped ``hist`` doesn't handle ``rot`` kw and ``sharex`` kw properly (:issue:`7234`)
- Bug in ``.loc`` performing fallback integer indexing with ``object`` dtype indices (:issue:`7496`)
- Bug (regression) in ``PeriodIndex`` constructor when passed ``Series`` objects (:issue:`7701`).


.. _whatsnew_0.14.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.14.0..v0.14.1
.. _whatsnew_0152:

Version 0.15.2 (December 12, 2014)
----------------------------------

{{ header }}


This is a minor release from 0.15.1 and includes a large number of bug fixes
along with several new features, enhancements, and performance improvements.
A small number of API changes were necessary to fix existing bugs.
We recommend that all users upgrade to this version.

- :ref:`Enhancements <whatsnew_0152.enhancements>`
- :ref:`API Changes <whatsnew_0152.api>`
- :ref:`Performance Improvements <whatsnew_0152.performance>`
- :ref:`Bug Fixes <whatsnew_0152.bug_fixes>`

.. _whatsnew_0152.api:

API changes
~~~~~~~~~~~

- Indexing in ``MultiIndex`` beyond lex-sort depth is now supported, though
  a lexically sorted index will have a better performance. (:issue:`2646`)

  .. ipython:: python
    :okwarning:

    df = pd.DataFrame({'jim':[0, 0, 1, 1],
                       'joe':['x', 'x', 'z', 'y'],
                       'jolie':np.random.rand(4)}).set_index(['jim', 'joe'])
    df
    df.index.lexsort_depth

    # in prior versions this would raise a KeyError
    # will now show a PerformanceWarning
    df.loc[(1, 'z')]

    # lexically sorting
    df2 = df.sort_index()
    df2
    df2.index.lexsort_depth
    df2.loc[(1,'z')]

- Bug in unique of Series with ``category`` dtype, which returned all categories regardless
  whether they were "used" or not (see :issue:`8559` for the discussion).
  Previous behaviour was to return all categories:

  .. code-block:: ipython

    In [3]: cat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])

    In [4]: cat
    Out[4]:
    [a, b, a]
    Categories (3, object): [a < b < c]

    In [5]: cat.unique()
    Out[5]: array(['a', 'b', 'c'], dtype=object)

  Now, only the categories that do effectively occur in the array are returned:

  .. ipython:: python

    cat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])
    cat.unique()

- ``Series.all`` and ``Series.any`` now support the ``level`` and ``skipna`` parameters. ``Series.all``, ``Series.any``, ``Index.all``, and ``Index.any`` no longer support the ``out`` and ``keepdims`` parameters, which existed for compatibility with ndarray. Various index types no longer support the ``all`` and ``any`` aggregation functions and will now raise ``TypeError``. (:issue:`8302`).

- Allow equality comparisons of Series with a categorical dtype and object dtype; previously these would raise ``TypeError`` (:issue:`8938`)

- Bug in ``NDFrame``: conflicting attribute/column names now behave consistently between getting and setting. Previously, when both a column and attribute named ``y`` existed, ``data.y`` would return the attribute, while ``data.y = z`` would update the column (:issue:`8994`)

  .. ipython:: python

     data = pd.DataFrame({'x': [1, 2, 3]})
     data.y = 2
     data['y'] = [2, 4, 6]
     data

     # this assignment was inconsistent
     data.y = 5

  Old behavior:

  .. code-block:: ipython

     In [6]: data.y
     Out[6]: 2

     In [7]: data['y'].values
     Out[7]: array([5, 5, 5])

  New behavior:

  .. ipython:: python

     data.y
     data['y'].values

- ``Timestamp('now')`` is now equivalent to ``Timestamp.now()`` in that it returns the local time rather than UTC. Also, ``Timestamp('today')`` is now equivalent to ``Timestamp.today()`` and both have ``tz`` as a possible argument. (:issue:`9000`)

- Fix negative step support for label-based slices (:issue:`8753`)

  Old behavior:

  .. code-block:: ipython

     In [1]: s = pd.Series(np.arange(3), ['a', 'b', 'c'])
     Out[1]:
     a    0
     b    1
     c    2
     dtype: int64

     In [2]: s.loc['c':'a':-1]
     Out[2]:
     c    2
     dtype: int64

  New behavior:

  .. ipython:: python

     s = pd.Series(np.arange(3), ['a', 'b', 'c'])
     s.loc['c':'a':-1]


.. _whatsnew_0152.enhancements:

Enhancements
~~~~~~~~~~~~

``Categorical`` enhancements:

- Added ability to export Categorical data to Stata (:issue:`8633`).  See :ref:`here <io.stata-categorical>` for limitations of categorical variables exported to Stata data files.
- Added flag ``order_categoricals`` to ``StataReader`` and ``read_stata`` to select whether to order imported categorical data (:issue:`8836`).  See :ref:`here <io.stata-categorical>` for more information on importing categorical variables from Stata data files.
- Added ability to export Categorical data to/from HDF5 (:issue:`7621`). Queries work the same as if it was an object array. However, the ``category`` dtyped data is stored in a more efficient manner. See :ref:`here <io.hdf5-categorical>` for an example and caveats w.r.t. prior versions of pandas.
- Added support for ``searchsorted()`` on ``Categorical`` class (:issue:`8420`).

Other enhancements:

- Added the ability to specify the SQL type of columns when writing a DataFrame
  to a database (:issue:`8778`).
  For example, specifying to use the sqlalchemy ``String`` type instead of the
  default ``Text`` type for string columns:

  .. code-block:: python

     from sqlalchemy.types import String
     data.to_sql('data_dtype', engine, dtype={'Col_1': String})  # noqa F821

- ``Series.all`` and ``Series.any`` now support the ``level`` and ``skipna`` parameters (:issue:`8302`):

  .. ipython:: python
     :okwarning:

     s = pd.Series([False, True, False], index=[0, 0, 1])
     s.any(level=0)

- ``Panel`` now supports the ``all`` and ``any`` aggregation functions. (:issue:`8302`):

  .. code-block:: python

     >>> p = pd.Panel(np.random.rand(2, 5, 4) > 0.1)
     >>> p.all()
            0      1      2     3
     0   True   True   True  True
     1   True  False   True  True
     2   True   True   True  True
     3  False   True  False  True
     4   True   True   True  True

- Added support for ``utcfromtimestamp()``, ``fromtimestamp()``, and ``combine()`` on ``Timestamp`` class (:issue:`5351`).
- Added Google Analytics (`pandas.io.ga`) basic documentation (:issue:`8835`). See `here <https://pandas.pydata.org/pandas-docs/version/0.15.2/remote_data.html#remote-data-ga>`__.
- ``Timedelta`` arithmetic returns ``NotImplemented`` in unknown cases, allowing extensions by custom classes (:issue:`8813`).
- ``Timedelta`` now supports arithmetic with ``numpy.ndarray`` objects of the appropriate dtype (numpy 1.8 or newer only) (:issue:`8884`).
- Added ``Timedelta.to_timedelta64()`` method to the public API (:issue:`8884`).
- Added ``gbq.generate_bq_schema()`` function to the gbq module (:issue:`8325`).
- ``Series`` now works with map objects the same way as generators (:issue:`8909`).
- Added context manager to ``HDFStore`` for automatic closing (:issue:`8791`).
- ``to_datetime`` gains an ``exact`` keyword to allow for a format to not require an exact match for a provided format string (if its ``False``). ``exact`` defaults to ``True`` (meaning that exact matching is still the default)  (:issue:`8904`)
- Added ``axvlines`` boolean option to parallel_coordinates plot function, determines whether vertical lines will be printed, default is True
- Added ability to read table footers to read_html (:issue:`8552`)
- ``to_sql`` now infers data types of non-NA values for columns that contain NA values and have dtype ``object`` (:issue:`8778`).


.. _whatsnew_0152.performance:

Performance
~~~~~~~~~~~

- Reduce memory usage when skiprows is an integer in read_csv (:issue:`8681`)
- Performance boost for ``to_datetime`` conversions with a passed ``format=``, and the ``exact=False`` (:issue:`8904`)


.. _whatsnew_0152.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in concat of Series with ``category`` dtype which were coercing to ``object``. (:issue:`8641`)
- Bug in Timestamp-Timestamp not returning a Timedelta type and datelike-datelike ops with timezones (:issue:`8865`)
- Made consistent a timezone mismatch exception (either tz operated with None or incompatible timezone), will now return ``TypeError`` rather than ``ValueError`` (a couple of edge cases only), (:issue:`8865`)
- Bug in using a ``pd.Grouper(key=...)`` with no level/axis or level only (:issue:`8795`, :issue:`8866`)
- Report a ``TypeError`` when invalid/no parameters are passed in a groupby (:issue:`8015`)
- Bug in packaging pandas with ``py2app/cx_Freeze`` (:issue:`8602`, :issue:`8831`)
- Bug in ``groupby`` signatures that didn't include \*args or \*\*kwargs (:issue:`8733`).
- ``io.data.Options`` now raises ``RemoteDataError`` when no expiry dates are available from Yahoo and when it receives no data from Yahoo (:issue:`8761`), (:issue:`8783`).
- Unclear error message in csv parsing when passing dtype and names and the parsed data is a different data type (:issue:`8833`)
- Bug in slicing a MultiIndex with an empty list and at least one boolean indexer (:issue:`8781`)
- ``io.data.Options`` now raises ``RemoteDataError`` when no expiry dates are available from Yahoo (:issue:`8761`).
- ``Timedelta`` kwargs may now be numpy ints and floats (:issue:`8757`).
- Fixed several outstanding bugs for ``Timedelta`` arithmetic and comparisons (:issue:`8813`, :issue:`5963`, :issue:`5436`).
- ``sql_schema`` now generates dialect appropriate ``CREATE TABLE`` statements (:issue:`8697`)
- ``slice`` string method now takes step into account (:issue:`8754`)
- Bug in ``BlockManager`` where setting values with different type would break block integrity (:issue:`8850`)
- Bug in ``DatetimeIndex`` when using ``time`` object as key (:issue:`8667`)
- Bug in ``merge`` where ``how='left'`` and ``sort=False`` would not preserve left frame order (:issue:`7331`)
- Bug in ``MultiIndex.reindex`` where reindexing at level would not reorder labels (:issue:`4088`)
- Bug in certain operations with dateutil timezones, manifesting with dateutil 2.3 (:issue:`8639`)
- Regression in DatetimeIndex iteration with a Fixed/Local offset timezone (:issue:`8890`)
- Bug in ``to_datetime`` when parsing a nanoseconds using the ``%f`` format (:issue:`8989`)
- ``io.data.Options`` now raises ``RemoteDataError`` when no expiry dates are available from Yahoo and when it receives no data from Yahoo (:issue:`8761`), (:issue:`8783`).
- Fix: The font size was only set on x axis if vertical or the y axis if horizontal. (:issue:`8765`)
- Fixed division by 0 when reading big csv files in python 3 (:issue:`8621`)
- Bug in outputting a MultiIndex with ``to_html,index=False`` which would add an extra column (:issue:`8452`)
- Imported categorical variables from Stata files retain the ordinal information in the underlying data (:issue:`8836`).
- Defined ``.size`` attribute across ``NDFrame`` objects to provide compat with numpy >= 1.9.1; buggy with ``np.array_split`` (:issue:`8846`)
- Skip testing of histogram plots for matplotlib <= 1.2 (:issue:`8648`).
- Bug where ``get_data_google`` returned object dtypes (:issue:`3995`)
- Bug in ``DataFrame.stack(..., dropna=False)`` when the DataFrame's ``columns`` is a ``MultiIndex``
  whose ``labels`` do not reference all its ``levels``. (:issue:`8844`)
- Bug in that Option context applied on ``__enter__`` (:issue:`8514`)
- Bug in resample that causes a ValueError when resampling across multiple days
  and the last offset is not calculated from the start of the range (:issue:`8683`)
- Bug where ``DataFrame.plot(kind='scatter')`` fails when checking if an np.array is in the DataFrame (:issue:`8852`)
- Bug in ``pd.infer_freq/DataFrame.inferred_freq`` that prevented proper sub-daily frequency inference when the index contained DST days (:issue:`8772`).
- Bug where index name was still used when plotting a series with ``use_index=False`` (:issue:`8558`).
- Bugs when trying to stack multiple columns, when some (or all) of the level names are numbers (:issue:`8584`).
- Bug in ``MultiIndex`` where ``__contains__`` returns wrong result if index is not lexically sorted or unique (:issue:`7724`)
- BUG CSV: fix problem with trailing white space in skipped rows, (:issue:`8679`), (:issue:`8661`), (:issue:`8983`)
- Regression in ``Timestamp`` does not parse 'Z' zone designator for UTC (:issue:`8771`)
- Bug in ``StataWriter`` the produces writes strings with 244 characters irrespective of actual size (:issue:`8969`)
- Fixed ValueError raised by cummin/cummax when datetime64 Series contains NaT. (:issue:`8965`)
- Bug in DataReader returns object dtype if there are missing values (:issue:`8980`)
- Bug in plotting if sharex was enabled and index was a timeseries, would show labels on multiple axes (:issue:`3964`).
- Bug where passing a unit to the TimedeltaIndex constructor applied the to nano-second conversion twice. (:issue:`9011`).
- Bug in plotting of a period-like array (:issue:`9012`)


.. _whatsnew_0.15.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.15.1..v0.15.2
.. _whatsnew_0100:

Version 0.10.0 (December 17, 2012)
----------------------------------

{{ header }}


This is a major release from 0.9.1 and includes many new features and
enhancements along with a large number of bug fixes. There are also a number of
important API changes that long-time pandas users should pay close attention
to.

File parsing new features
~~~~~~~~~~~~~~~~~~~~~~~~~

The delimited file parsing engine (the guts of ``read_csv`` and ``read_table``)
has been rewritten from the ground up and now uses a fraction the amount of
memory while parsing, while being 40% or more faster in most use cases (in some
cases much faster).

There are also many new features:

- Much-improved Unicode handling via the ``encoding`` option.
- Column filtering (``usecols``)
- Dtype specification (``dtype`` argument)
- Ability to specify strings to be recognized as True/False
- Ability to yield NumPy record arrays (``as_recarray``)
- High performance ``delim_whitespace`` option
- Decimal format (e.g. European format) specification
- Easier CSV dialect options: ``escapechar``, ``lineterminator``,
  ``quotechar``, etc.
- More robust handling of many exceptional kinds of files observed in the wild

API changes
~~~~~~~~~~~

**Deprecated DataFrame BINOP TimeSeries special case behavior**

The default behavior of binary operations between a DataFrame and a Series has
always been to align on the DataFrame's columns and broadcast down the rows,
**except** in the special case that the DataFrame contains time series. Since
there are now method for each binary operator enabling you to specify how you
want to broadcast, we are phasing out this special case (Zen of Python:
*Special cases aren't special enough to break the rules*). Here's what I'm
talking about:

.. ipython:: python
   :okwarning:

   import pandas as pd

   df = pd.DataFrame(np.random.randn(6, 4), index=pd.date_range("1/1/2000", periods=6))
   df
   # deprecated now
   df - df[0]
   # Change your code to
   df.sub(df[0], axis=0)  # align on axis 0 (rows)

You will get a deprecation warning in the 0.10.x series, and the deprecated
functionality will be removed in 0.11 or later.

**Altered resample default behavior**

The default time series ``resample`` binning behavior of daily ``D`` and
*higher* frequencies has been changed to ``closed='left', label='left'``. Lower
nfrequencies are unaffected. The prior defaults were causing a great deal of
confusion for users, especially resampling data to daily frequency (which
labeled the aggregated group with the end of the interval: the next day).

.. code-block:: ipython

   In [1]: dates = pd.date_range('1/1/2000', '1/5/2000', freq='4h')

   In [2]: series = pd.Series(np.arange(len(dates)), index=dates)

   In [3]: series
   Out[3]:
   2000-01-01 00:00:00     0
   2000-01-01 04:00:00     1
   2000-01-01 08:00:00     2
   2000-01-01 12:00:00     3
   2000-01-01 16:00:00     4
   2000-01-01 20:00:00     5
   2000-01-02 00:00:00     6
   2000-01-02 04:00:00     7
   2000-01-02 08:00:00     8
   2000-01-02 12:00:00     9
   2000-01-02 16:00:00    10
   2000-01-02 20:00:00    11
   2000-01-03 00:00:00    12
   2000-01-03 04:00:00    13
   2000-01-03 08:00:00    14
   2000-01-03 12:00:00    15
   2000-01-03 16:00:00    16
   2000-01-03 20:00:00    17
   2000-01-04 00:00:00    18
   2000-01-04 04:00:00    19
   2000-01-04 08:00:00    20
   2000-01-04 12:00:00    21
   2000-01-04 16:00:00    22
   2000-01-04 20:00:00    23
   2000-01-05 00:00:00    24
   Freq: 4H, dtype: int64

   In [4]: series.resample('D', how='sum')
   Out[4]:
   2000-01-01     15
   2000-01-02     51
   2000-01-03     87
   2000-01-04    123
   2000-01-05     24
   Freq: D, dtype: int64

   In [5]: # old behavior
   In [6]: series.resample('D', how='sum', closed='right', label='right')
   Out[6]:
   2000-01-01      0
   2000-01-02     21
   2000-01-03     57
   2000-01-04     93
   2000-01-05    129
   Freq: D, dtype: int64

- Infinity and negative infinity are no longer treated as NA by ``isnull`` and
  ``notnull``. That they ever were was a relic of early pandas. This behavior
  can be re-enabled globally by the ``mode.use_inf_as_null`` option:

.. code-block:: ipython

    In [6]: s = pd.Series([1.5, np.inf, 3.4, -np.inf])

    In [7]: pd.isnull(s)
    Out[7]:
    0    False
    1    False
    2    False
    3    False
    Length: 4, dtype: bool

    In [8]: s.fillna(0)
    Out[8]:
    0    1.500000
    1         inf
    2    3.400000
    3        -inf
    Length: 4, dtype: float64

    In [9]: pd.set_option('use_inf_as_null', True)

    In [10]: pd.isnull(s)
    Out[10]:
    0    False
    1     True
    2    False
    3     True
    Length: 4, dtype: bool

    In [11]: s.fillna(0)
    Out[11]:
    0    1.5
    1    0.0
    2    3.4
    3    0.0
    Length: 4, dtype: float64

    In [12]: pd.reset_option('use_inf_as_null')

- Methods with the ``inplace`` option now all return ``None`` instead of the
  calling object. E.g. code written like ``df = df.fillna(0, inplace=True)``
  may stop working. To fix, simply delete the unnecessary variable assignment.

- ``pandas.merge`` no longer sorts the group keys (``sort=False``) by
  default. This was done for performance reasons: the group-key sorting is
  often one of the more expensive parts of the computation and is often
  unnecessary.

- The default column names for a file with no header have been changed to the
  integers ``0`` through ``N - 1``. This is to create consistency with the
  DataFrame constructor with no columns specified. The v0.9.0 behavior (names
  ``X0``, ``X1``, ...) can be reproduced by specifying ``prefix='X'``:

.. ipython:: python
   :okwarning:

    import io

    data = """
    a,b,c
    1,Yes,2
    3,No,4
    """
    print(data)
    pd.read_csv(io.StringIO(data), header=None)
    pd.read_csv(io.StringIO(data), header=None, prefix="X")

- Values like ``'Yes'`` and ``'No'`` are not interpreted as boolean by default,
  though this can be controlled by new ``true_values`` and ``false_values``
  arguments:

.. code-block:: ipython

    In [4]: print(data)

        a,b,c
        1,Yes,2
        3,No,4

    In [5]: pd.read_csv(io.StringIO(data))
    Out[5]:
           a    b  c
    0      1  Yes  2
    1      3   No  4

    In [6]: pd.read_csv(io.StringIO(data), true_values=["Yes"], false_values=["No"])
    Out[6]:
           a      b  c
    0      1   True  2
    1      3  False  4

- The file parsers will not recognize non-string values arising from a
  converter function as NA if passed in the ``na_values`` argument. It's better
  to do post-processing using the ``replace`` function instead.

- Calling ``fillna`` on Series or DataFrame with no arguments is no longer
  valid code. You must either specify a fill value or an interpolation method:

.. ipython:: python

   s = pd.Series([np.nan, 1.0, 2.0, np.nan, 4])
   s
   s.fillna(0)
   s.fillna(method="pad")

Convenience methods ``ffill`` and  ``bfill`` have been added:

.. ipython:: python

   s.ffill()


- ``Series.apply`` will now operate on a returned value from the applied
  function, that is itself a series, and possibly upcast the result to a
  DataFrame

  .. ipython:: python

      def f(x):
          return pd.Series([x, x ** 2], index=["x", "x^2"])


      s = pd.Series(np.random.rand(5))
      s
      s.apply(f)

- New API functions for working with pandas options (:issue:`2097`):

  - ``get_option`` / ``set_option`` - get/set the value of an option. Partial
    names are accepted.  - ``reset_option`` - reset one or more options to
    their default value. Partial names are accepted.  - ``describe_option`` -
    print a description of one or more options. When called with no
    arguments. print all registered options.

  Note: ``set_printoptions``/ ``reset_printoptions`` are now deprecated (but
  functioning), the print options now live under "display.XYZ". For example:

  .. ipython:: python

     pd.get_option("display.max_rows")

- to_string() methods now always return unicode strings  (:issue:`2224`).

New features
~~~~~~~~~~~~

Wide DataFrame printing
~~~~~~~~~~~~~~~~~~~~~~~

Instead of printing the summary information, pandas now splits the string
representation across multiple rows by default:

.. ipython:: python

   wide_frame = pd.DataFrame(np.random.randn(5, 16))

   wide_frame

The old behavior of printing out summary information can be achieved via the
'expand_frame_repr' print option:

.. ipython:: python

   pd.set_option("expand_frame_repr", False)

   wide_frame

.. ipython:: python
   :suppress:

   pd.reset_option("expand_frame_repr")

The width of each line can be changed via 'line_width' (80 by default):

.. code-block:: python

   pd.set_option("line_width", 40)

   wide_frame


Updated PyTables support
~~~~~~~~~~~~~~~~~~~~~~~~

:ref:`Docs <io.hdf5>` for PyTables ``Table`` format & several enhancements to the api. Here is a taste of what to expect.

.. code-block:: ipython

    In [41]: store = pd.HDFStore('store.h5')

    In [42]: df = pd.DataFrame(np.random.randn(8, 3),
       ....:                   index=pd.date_range('1/1/2000', periods=8),
       ....:                   columns=['A', 'B', 'C'])

    In [43]: df
    Out[43]:
                       A         B         C
    2000-01-01 -2.036047  0.000830 -0.955697
    2000-01-02 -0.898872 -0.725411  0.059904
    2000-01-03 -0.449644  1.082900 -1.221265
    2000-01-04  0.361078  1.330704  0.855932
    2000-01-05 -1.216718  1.488887  0.018993
    2000-01-06 -0.877046  0.045976  0.437274
    2000-01-07 -0.567182 -0.888657 -0.556383
    2000-01-08  0.655457  1.117949 -2.782376

    [8 rows x 3 columns]

    # appending data frames
    In [44]: df1 = df[0:4]

    In [45]: df2 = df[4:]

    In [46]: store.append('df', df1)

    In [47]: store.append('df', df2)

    In [48]: store
    Out[48]:
    <class 'pandas.io.pytables.HDFStore'>
    File path: store.h5
    /df            frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])

    # selecting the entire store
    In [49]: store.select('df')
    Out[49]:
                       A         B         C
    2000-01-01 -2.036047  0.000830 -0.955697
    2000-01-02 -0.898872 -0.725411  0.059904
    2000-01-03 -0.449644  1.082900 -1.221265
    2000-01-04  0.361078  1.330704  0.855932
    2000-01-05 -1.216718  1.488887  0.018993
    2000-01-06 -0.877046  0.045976  0.437274
    2000-01-07 -0.567182 -0.888657 -0.556383
    2000-01-08  0.655457  1.117949 -2.782376

    [8 rows x 3 columns]

.. code-block:: ipython

    In [50]: wp = pd.Panel(np.random.randn(2, 5, 4), items=['Item1', 'Item2'],
       ....:               major_axis=pd.date_range('1/1/2000', periods=5),
       ....:               minor_axis=['A', 'B', 'C', 'D'])

    In [51]: wp
    Out[51]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 2 (items) x 5 (major_axis) x 4 (minor_axis)
    Items axis: Item1 to Item2
    Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00
    Minor_axis axis: A to D

    # storing a panel
    In [52]: store.append('wp', wp)

    # selecting via A QUERY
    In [53]: store.select('wp', [pd.Term('major_axis>20000102'),
       ....:                     pd.Term('minor_axis', '=', ['A', 'B'])])
       ....:
    Out[53]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)
    Items axis: Item1 to Item2
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00
    Minor_axis axis: A to B

    # removing data from tables
    In [54]: store.remove('wp', pd.Term('major_axis>20000103'))
    Out[54]: 8

    In [55]: store.select('wp')
    Out[55]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 2 (items) x 3 (major_axis) x 4 (minor_axis)
    Items axis: Item1 to Item2
    Major_axis axis: 2000-01-01 00:00:00 to 2000-01-03 00:00:00
    Minor_axis axis: A to D

    # deleting a store
    In [56]: del store['df']

    In [57]: store
    Out[57]:
    <class 'pandas.io.pytables.HDFStore'>
    File path: store.h5
    /wp            wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])


**Enhancements**

- added ability to hierarchical keys

   .. code-block:: ipython

        In [58]: store.put('foo/bar/bah', df)

        In [59]: store.append('food/orange', df)

        In [60]: store.append('food/apple', df)

        In [61]: store
        Out[61]:
        <class 'pandas.io.pytables.HDFStore'>
        File path: store.h5
        /foo/bar/bah            frame        (shape->[8,3])
        /food/apple             frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])
        /food/orange            frame_table  (typ->appendable,nrows->8,ncols->3,indexers->[index])
        /wp                     wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])

        # remove all nodes under this level
        In [62]: store.remove('food')

        In [63]: store
        Out[63]:
        <class 'pandas.io.pytables.HDFStore'>
        File path: store.h5
        /foo/bar/bah            frame        (shape->[8,3])
        /wp                     wide_table   (typ->appendable,nrows->12,ncols->2,indexers->[major_axis,minor_axis])

- added mixed-dtype support!

   .. code-block:: ipython

        In [64]: df['string'] = 'string'

        In [65]: df['int'] = 1

        In [66]: store.append('df', df)

        In [67]: df1 = store.select('df')

        In [68]: df1
        Out[68]:
                           A         B         C  string  int
        2000-01-01 -2.036047  0.000830 -0.955697  string    1
        2000-01-02 -0.898872 -0.725411  0.059904  string    1
        2000-01-03 -0.449644  1.082900 -1.221265  string    1
        2000-01-04  0.361078  1.330704  0.855932  string    1
        2000-01-05 -1.216718  1.488887  0.018993  string    1
        2000-01-06 -0.877046  0.045976  0.437274  string    1
        2000-01-07 -0.567182 -0.888657 -0.556383  string    1
        2000-01-08  0.655457  1.117949 -2.782376  string    1

        [8 rows x 5 columns]

        In [69]: df1.get_dtype_counts()
        Out[69]:
        float64    3
        int64      1
        object     1
        dtype: int64

- performance improvements on table writing
- support for arbitrarily indexed dimensions
- ``SparseSeries`` now has a ``density`` property (:issue:`2384`)
- enable ``Series.str.strip/lstrip/rstrip`` methods to take an input argument
  to strip arbitrary characters (:issue:`2411`)
- implement ``value_vars`` in ``melt`` to limit values to certain columns
  and add ``melt`` to pandas namespace (:issue:`2412`)

**Bug Fixes**

- added ``Term`` method of specifying where conditions (:issue:`1996`).
- ``del store['df']`` now call ``store.remove('df')`` for store deletion
- deleting of consecutive rows is much faster than before
- ``min_itemsize`` parameter can be specified in table creation to force a
  minimum size for indexing columns (the previous implementation would set the
  column size based on the first append)
- indexing support via ``create_table_index`` (requires PyTables >= 2.3)
  (:issue:`698`).
- appending on a store would fail if the table was not first created via ``put``
- fixed issue with missing attributes after loading a pickled dataframe (GH2431)
- minor change to select and remove: require a table ONLY if where is also
  provided (and not None)

**Compatibility**

0.10 of ``HDFStore`` is backwards compatible for reading tables created in a prior version of pandas,
however, query terms using the prior (undocumented) methodology are unsupported. You must read in the entire
file and write it out using the new format to take advantage of the updates.

N dimensional panels (experimental)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Adding experimental support for Panel4D and factory functions to create n-dimensional named panels.
Here is a taste of what to expect.

.. code-block:: ipython

  In [58]: p4d = Panel4D(np.random.randn(2, 2, 5, 4),
    ....:       labels=['Label1','Label2'],
    ....:       items=['Item1', 'Item2'],
    ....:       major_axis=date_range('1/1/2000', periods=5),
    ....:       minor_axis=['A', 'B', 'C', 'D'])
    ....:

  In [59]: p4d
  Out[59]:
  <class 'pandas.core.panelnd.Panel4D'>
  Dimensions: 2 (labels) x 2 (items) x 5 (major_axis) x 4 (minor_axis)
  Labels axis: Label1 to Label2
  Items axis: Item1 to Item2
  Major_axis axis: 2000-01-01 00:00:00 to 2000-01-05 00:00:00
  Minor_axis axis: A to D





See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.


.. _whatsnew_0.10.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.9.0..v0.10.0
.. _whatsnew_0251:

What's new in 0.25.1 (August 21, 2019)
--------------------------------------

These are the changes in pandas 0.25.1. See :ref:`release` for a full changelog
including other versions of pandas.

IO and LZMA
~~~~~~~~~~~

Some users may unknowingly have an incomplete Python installation lacking the ``lzma`` module from the standard library. In this case, ``import pandas`` failed due to an ``ImportError`` (:issue:`27575`).
pandas will now warn, rather than raising an ``ImportError`` if the ``lzma`` module is not present. Any subsequent attempt to use ``lzma`` methods will raise a ``RuntimeError``.
A possible fix for the lack of the ``lzma`` module is to ensure you have the necessary libraries and then re-install Python.
For example, on MacOS installing Python with ``pyenv`` may lead to an incomplete Python installation due to unmet system dependencies at compilation time (like ``xz``). Compilation will succeed, but Python might fail at run time. The issue can be solved by installing the necessary dependencies and then re-installing Python.

.. _whatsnew_0251.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^

- Bug in :meth:`Categorical.fillna` that would replace all values, not just those that are ``NaN`` (:issue:`26215`)

Datetimelike
^^^^^^^^^^^^

- Bug in :func:`to_datetime` where passing a timezone-naive :class:`DatetimeArray` or :class:`DatetimeIndex` and ``utc=True`` would incorrectly return a timezone-naive result (:issue:`27733`)
- Bug in :meth:`Period.to_timestamp` where a :class:`Period` outside the :class:`Timestamp` implementation bounds (roughly 1677-09-21 to 2262-04-11) would return an incorrect :class:`Timestamp` instead of raising ``OutOfBoundsDatetime`` (:issue:`19643`)
- Bug in iterating over :class:`DatetimeIndex` when the underlying data is read-only (:issue:`28055`)

Timezones
^^^^^^^^^

- Bug in :class:`Index` where a numpy object array with a timezone aware :class:`Timestamp` and ``np.nan`` would not return a :class:`DatetimeIndex` (:issue:`27011`)

Numeric
^^^^^^^

- Bug in :meth:`Series.interpolate` when using a timezone aware :class:`DatetimeIndex` (:issue:`27548`)
- Bug when printing negative floating point complex numbers would raise an ``IndexError`` (:issue:`27484`)
- Bug where :class:`DataFrame` arithmetic operators such as :meth:`DataFrame.mul` with a :class:`Series` with axis=1 would raise an ``AttributeError`` on :class:`DataFrame` larger than the minimum threshold to invoke numexpr (:issue:`27636`)
- Bug in :class:`DataFrame` arithmetic where missing values in results were incorrectly masked with ``NaN`` instead of ``Inf`` (:issue:`27464`)

Conversion
^^^^^^^^^^

- Improved the warnings for the deprecated methods :meth:`Series.real` and :meth:`Series.imag` (:issue:`27610`)

Interval
^^^^^^^^

- Bug in :class:`IntervalIndex` where ``dir(obj)`` would raise ``ValueError`` (:issue:`27571`)

Indexing
^^^^^^^^

- Bug in partial-string indexing returning a NumPy array rather than a ``Series`` when indexing with a scalar like ``.loc['2015']`` (:issue:`27516`)
- Break reference cycle involving :class:`Index` and other index classes to allow garbage collection of index objects without running the GC. (:issue:`27585`, :issue:`27840`)
- Fix regression in assigning values to a single column of a DataFrame with a ``MultiIndex`` columns (:issue:`27841`).
- Fix regression in ``.ix`` fallback with an ``IntervalIndex`` (:issue:`27865`).

Missing
^^^^^^^

- Bug in :func:`pandas.isnull` or :func:`pandas.isna` when the input is a type e.g. ``type(pandas.Series())`` (:issue:`27482`)

IO
^^

- Avoid calling ``S3File.s3`` when reading parquet, as this was removed in s3fs version 0.3.0 (:issue:`27756`)
- Better error message when a negative header is passed in :func:`pandas.read_csv` (:issue:`27779`)
- Follow the ``min_rows`` display option (introduced in v0.25.0) correctly in the HTML repr in the notebook (:issue:`27991`).

Plotting
^^^^^^^^

- Added a ``pandas_plotting_backends`` entrypoint group for registering plot backends. See :ref:`extending.plotting-backends` for more (:issue:`26747`).
- Fixed the re-instatement of Matplotlib datetime converters after calling
  :meth:`pandas.plotting.deregister_matplotlib_converters` (:issue:`27481`).
- Fix compatibility issue with matplotlib when passing a pandas ``Index`` to a plot call (:issue:`27775`).

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Fixed regression in :meth:`pands.core.groupby.DataFrameGroupBy.quantile` raising when multiple quantiles are given (:issue:`27526`)
- Bug in :meth:`pandas.core.groupby.DataFrameGroupBy.transform` where applying a timezone conversion lambda function would drop timezone information (:issue:`27496`)
- Bug in :meth:`pandas.core.groupby.GroupBy.nth` where ``observed=False`` was being ignored for Categorical groupers (:issue:`26385`)
- Bug in windowing over read-only arrays (:issue:`27766`)
- Fixed segfault in ``pandas.core.groupby.DataFrameGroupBy.quantile`` when an invalid quantile was passed (:issue:`27470`)

Reshaping
^^^^^^^^^

- A ``KeyError`` is now raised if ``.unstack()`` is called on a :class:`Series` or :class:`DataFrame` with a flat :class:`Index` passing a name which is not the correct one (:issue:`18303`)
- Bug :meth:`merge_asof` could not merge :class:`Timedelta` objects when passing ``tolerance`` kwarg (:issue:`27642`)
- Bug in :meth:`DataFrame.crosstab` when ``margins`` set to ``True`` and ``normalize`` is not ``False``, an error is raised. (:issue:`27500`)
- :meth:`DataFrame.join` now suppresses the ``FutureWarning`` when the sort parameter is specified (:issue:`21952`)
- Bug in :meth:`DataFrame.join` raising with readonly arrays (:issue:`27943`)

Sparse
^^^^^^

- Bug in reductions for :class:`Series` with Sparse dtypes (:issue:`27080`)

Other
^^^^^

- Bug in :meth:`Series.replace` and :meth:`DataFrame.replace` when replacing timezone-aware timestamps using a dict-like replacer (:issue:`27720`)
- Bug in :meth:`Series.rename` when using a custom type indexer. Now any value that isn't callable or dict-like is treated as a scalar. (:issue:`27814`)

.. _whatsnew_0.251.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.25.0..v0.25.1
.. _whatsnew_141:

What's new in 1.4.1 (February ??, 2022)
---------------------------------------

These are the changes in pandas 1.4.1. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_141.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Regression in :meth:`Series.mask` with ``inplace=True`` and ``PeriodDtype`` and an incompatible ``other`` coercing to a common dtype instead of raising (:issue:`45546`)
- Regression in :func:`.assert_frame_equal` not respecting ``check_flags=False`` (:issue:`45554`)
- Regression in :meth:`Series.fillna` with ``downcast=False`` incorrectly downcasting ``object`` dtype (:issue:`45603`)
- Regression in :meth:`DataFrame.loc.__setitem__` losing :class:`Index` name if :class:`DataFrame` was empty before (:issue:`45621`)
- Regression in :func:`join` with overlapping :class:`IntervalIndex` raising an ``InvalidIndexError`` (:issue:`45661`)
-

.. ---------------------------------------------------------------------------

.. _whatsnew_141.bug_fixes:

Bug fixes
~~~~~~~~~
- Fixed segfault in :meth:``DataFrame.to_json`` when dumping tz-aware datetimes in Python 3.10 (:issue:`42130`)
- Fixed window aggregations in :meth:`DataFrame.rolling` and :meth:`Series.rolling` to skip over unused elements (:issue:`45647`)
-

.. ---------------------------------------------------------------------------

.. _whatsnew_141.other:

Other
~~~~~
- Reverted performance speedup of :meth:`DataFrame.corr` for ``method=pearson`` to fix precision regression (:issue:`45640`, :issue:`42761`)
-

.. ---------------------------------------------------------------------------

.. _whatsnew_141.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.4.0..v1.4.1|HEAD
.. _whatsnew_0162:

Version 0.16.2 (June 12, 2015)
------------------------------

{{ header }}


This is a minor bug-fix release from 0.16.1 and includes a large number of
bug fixes along some new features (:meth:`~DataFrame.pipe` method), enhancements, and performance improvements.

We recommend that all users upgrade to this version.

Highlights include:

- A new ``pipe`` method, see :ref:`here <whatsnew_0162.enhancements.pipe>`
- Documentation on how to use numba_ with *pandas*, see :ref:`here <enhancingperf.numba>`


.. contents:: What's new in v0.16.2
    :local:
    :backlinks: none

.. _numba: http://numba.pydata.org

.. _whatsnew_0162.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0162.enhancements.pipe:

Pipe
^^^^

We've introduced a new method :meth:`DataFrame.pipe`. As suggested by the name, ``pipe``
should be used to pipe data through a chain of function calls.
The goal is to avoid confusing nested function calls like

.. code-block:: python

   # df is a DataFrame
   # f, g, and h are functions that take and return DataFrames
   f(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821

The logic flows from inside out, and function names are separated from their keyword arguments.
This can be rewritten as

.. code-block:: python

   (
       df.pipe(h)  # noqa F821
       .pipe(g, arg1=1)  # noqa F821
       .pipe(f, arg2=2, arg3=3)  # noqa F821
   )

Now both the code and the logic flow from top to bottom. Keyword arguments are next to
their functions. Overall the code is much more readable.

In the example above, the functions ``f``, ``g``, and ``h`` each expected the DataFrame as the first positional argument.
When the function you wish to apply takes its data anywhere other than the first argument, pass a tuple
of ``(function, keyword)`` indicating where the DataFrame should flow. For example:

.. ipython:: python
   :okwarning:

   import statsmodels.formula.api as sm

   bb = pd.read_csv("data/baseball.csv", index_col="id")

   # sm.ols takes (formula, data)
   (
       bb.query("h > 0")
       .assign(ln_h=lambda df: np.log(df.h))
       .pipe((sm.ols, "data"), "hr ~ ln_h + year + g + C(lg)")
       .fit()
       .summary()
   )

The pipe method is inspired by unix pipes, which stream text through
processes. More recently dplyr_ and magrittr_ have introduced the
popular ``(%>%)`` pipe operator for R_.

See the :ref:`documentation <basics.pipe>` for more. (:issue:`10129`)

.. _dplyr: https://github.com/tidyverse/dplyr
.. _magrittr: https://github.com/smbache/magrittr
.. _R: http://www.r-project.org

.. _whatsnew_0162.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Added ``rsplit`` to Index/Series StringMethods (:issue:`10303`)

- Removed the hard-coded size limits on the ``DataFrame`` HTML representation
  in the IPython notebook, and leave this to IPython itself (only for IPython
  v3.0 or greater). This eliminates the duplicate scroll bars that appeared in
  the notebook with large frames (:issue:`10231`).

  Note that the notebook has a ``toggle output scrolling`` feature to limit the
  display of very large frames (by clicking left of the output).
  You can also configure the way DataFrames are displayed using the pandas
  options, see here :ref:`here <options.frequently_used>`.

- ``axis`` parameter of ``DataFrame.quantile`` now accepts also ``index`` and ``column``. (:issue:`9543`)

.. _whatsnew_0162.api:

API changes
~~~~~~~~~~~

- ``Holiday`` now raises ``NotImplementedError`` if both ``offset`` and ``observance`` are used in the constructor instead of returning an incorrect result (:issue:`10217`).


.. _whatsnew_0162.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved ``Series.resample`` performance with ``dtype=datetime64[ns]`` (:issue:`7754`)
- Increase performance of ``str.split`` when ``expand=True`` (:issue:`10081`)

.. _whatsnew_0162.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in ``Series.hist`` raises an error when a one row ``Series`` was given (:issue:`10214`)
- Bug where ``HDFStore.select`` modifies the passed columns list (:issue:`7212`)
- Bug in ``Categorical`` repr with ``display.width`` of ``None`` in Python 3 (:issue:`10087`)
- Bug in ``to_json`` with certain orients and a ``CategoricalIndex`` would segfault (:issue:`10317`)
- Bug where some of the nan functions do not have consistent return dtypes (:issue:`10251`)
- Bug in ``DataFrame.quantile`` on checking that a valid axis was passed (:issue:`9543`)
- Bug in ``groupby.apply`` aggregation for ``Categorical`` not preserving categories (:issue:`10138`)
- Bug in ``to_csv`` where ``date_format`` is ignored if the ``datetime`` is fractional (:issue:`10209`)
- Bug in ``DataFrame.to_json`` with mixed data types (:issue:`10289`)
- Bug in cache updating when consolidating (:issue:`10264`)
- Bug in ``mean()`` where integer dtypes can overflow (:issue:`10172`)
- Bug where ``Panel.from_dict`` does not set dtype when specified (:issue:`10058`)
- Bug in ``Index.union`` raises ``AttributeError`` when passing array-likes. (:issue:`10149`)
- Bug in ``Timestamp``'s' ``microsecond``, ``quarter``, ``dayofyear``, ``week`` and ``daysinmonth`` properties return ``np.int`` type, not built-in ``int``. (:issue:`10050`)
- Bug in ``NaT`` raises ``AttributeError`` when accessing to ``daysinmonth``, ``dayofweek`` properties. (:issue:`10096`)
- Bug in Index repr when using the ``max_seq_items=None`` setting (:issue:`10182`).
- Bug in getting timezone data with ``dateutil`` on various platforms ( :issue:`9059`, :issue:`8639`, :issue:`9663`, :issue:`10121`)
- Bug in displaying datetimes with mixed frequencies; display 'ms' datetimes to the proper precision. (:issue:`10170`)
- Bug in ``setitem`` where type promotion is applied to the entire block (:issue:`10280`)
- Bug in ``Series`` arithmetic methods may incorrectly hold names (:issue:`10068`)
- Bug in ``GroupBy.get_group`` when grouping on multiple keys, one of which is categorical. (:issue:`10132`)
- Bug in ``DatetimeIndex`` and ``TimedeltaIndex`` names are lost after timedelta arithmetic ( :issue:`9926`)
- Bug in ``DataFrame`` construction from nested ``dict`` with ``datetime64`` (:issue:`10160`)
- Bug in ``Series`` construction from ``dict`` with ``datetime64`` keys (:issue:`9456`)
- Bug in ``Series.plot(label="LABEL")`` not correctly setting the label (:issue:`10119`)
- Bug in ``plot`` not defaulting to matplotlib ``axes.grid`` setting (:issue:`9792`)
- Bug causing strings containing an exponent, but no decimal to be parsed as ``int`` instead of ``float`` in ``engine='python'`` for the ``read_csv`` parser (:issue:`9565`)
- Bug in ``Series.align`` resets ``name`` when ``fill_value`` is specified (:issue:`10067`)
- Bug in ``read_csv`` causing index name not to be set on an empty DataFrame (:issue:`10184`)
- Bug in ``SparseSeries.abs`` resets ``name`` (:issue:`10241`)
- Bug in ``TimedeltaIndex`` slicing may reset freq (:issue:`10292`)
- Bug in ``GroupBy.get_group`` raises ``ValueError`` when group key contains ``NaT`` (:issue:`6992`)
- Bug in ``SparseSeries`` constructor ignores input data name (:issue:`10258`)
- Bug in ``Categorical.remove_categories`` causing a ``ValueError`` when removing the ``NaN`` category if underlying dtype is floating-point (:issue:`10156`)
- Bug where infer_freq infers time rule (WOM-5XXX) unsupported by to_offset (:issue:`9425`)
- Bug in ``DataFrame.to_hdf()`` where table format would raise a seemingly unrelated error for invalid (non-string) column names. This is now explicitly forbidden. (:issue:`9057`)
- Bug to handle masking empty ``DataFrame`` (:issue:`10126`).
- Bug where MySQL interface could not handle numeric table/column names (:issue:`10255`)
- Bug in ``read_csv`` with a ``date_parser`` that returned a ``datetime64`` array of other time resolution than ``[ns]`` (:issue:`10245`)
- Bug in ``Panel.apply`` when the result has ndim=0 (:issue:`10332`)
- Bug in ``read_hdf`` where ``auto_close`` could not be passed (:issue:`9327`).
- Bug in ``read_hdf`` where open stores could not be used (:issue:`10330`).
- Bug in adding empty ``DataFrames``, now results in a ``DataFrame`` that ``.equals`` an empty ``DataFrame`` (:issue:`10181`).
- Bug in ``to_hdf`` and ``HDFStore`` which did not check that complib choices were valid (:issue:`4582`, :issue:`8874`).


.. _whatsnew_0.16.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.16.1..v0.16.2
.. _whatsnew_0253:

What's new in 0.25.3 (October 31, 2019)
---------------------------------------

These are the changes in pandas 0.25.3. See :ref:`release` for a full changelog
including other versions of pandas.

.. _whatsnew_0253.bug_fixes:

Bug fixes
~~~~~~~~~

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in :meth:`DataFrameGroupBy.quantile` where NA values in the grouping could cause segfaults or incorrect results (:issue:`28882`)

Contributors
~~~~~~~~~~~~

.. contributors:: v0.25.2..v0.25.3
.. _whatsnew_123:

What's new in 1.2.3 (March 02, 2021)
------------------------------------

These are the changes in pandas 1.2.3. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_123.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`~DataFrame.to_excel` raising ``KeyError`` when giving duplicate columns with ``columns`` attribute (:issue:`39695`)
- Fixed regression in nullable integer unary ops propagating mask on assignment (:issue:`39943`)
- Fixed regression in :meth:`DataFrame.__setitem__` not aligning :class:`DataFrame` on right-hand side for boolean indexer (:issue:`39931`)
- Fixed regression in :meth:`~DataFrame.to_json` failing to use ``compression`` with URL-like paths that are internally opened in binary mode or with user-provided file objects that are opened in binary mode (:issue:`39985`)
- Fixed regression in :meth:`Series.sort_index` and :meth:`DataFrame.sort_index`, which exited with an ungraceful error when having kwarg ``ascending=None`` passed. Passing ``ascending=None`` is still considered invalid, and the improved error message suggests a proper usage (``ascending`` must be a boolean or a list-like of boolean) (:issue:`39434`)
- Fixed regression in :meth:`DataFrame.transform` and :meth:`Series.transform` giving incorrect column labels when passed a dictionary with a mix of list and non-list values (:issue:`40018`)

.. ---------------------------------------------------------------------------

.. _whatsnew_123.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.2..v1.2.3
.. _whatsnew_0220:

Version 0.22.0 (December 29, 2017)
----------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a major release from 0.21.1 and includes a single, API-breaking change.
We recommend that all users upgrade to this version after carefully reading the
release note (singular!).

.. _whatsnew_0220.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

pandas 0.22.0 changes the handling of empty and all-*NA* sums and products. The
summary is that

* The sum of an empty or all-*NA* ``Series`` is now ``0``
* The product of an empty or all-*NA* ``Series`` is now ``1``
* We've added a ``min_count`` parameter to ``.sum()`` and ``.prod()`` controlling
  the minimum number of valid values for the result to be valid. If fewer than
  ``min_count`` non-*NA* values are present, the result is *NA*. The default is
  ``0``. To return ``NaN``, the 0.21 behavior, use ``min_count=1``.

Some background: In pandas 0.21, we fixed a long-standing inconsistency
in the return value of all-*NA* series depending on whether or not bottleneck
was installed. See :ref:`whatsnew_0210.api_breaking.bottleneck`. At the same
time, we changed the sum and prod of an empty ``Series`` to also be ``NaN``.

Based on feedback, we've partially reverted those changes.

Arithmetic operations
^^^^^^^^^^^^^^^^^^^^^

The default sum for empty or all-*NA* ``Series`` is now ``0``.

*pandas 0.21.x*

.. code-block:: ipython

   In [1]: pd.Series([]).sum()
   Out[1]: nan

   In [2]: pd.Series([np.nan]).sum()
   Out[2]: nan

*pandas 0.22.0*

.. ipython:: python
   :okwarning:

   pd.Series([]).sum()
   pd.Series([np.nan]).sum()

The default behavior is the same as pandas 0.20.3 with bottleneck installed. It
also matches the behavior of NumPy's ``np.nansum`` on empty and all-*NA* arrays.

To have the sum of an empty series return ``NaN`` (the default behavior of
pandas 0.20.3 without bottleneck, or pandas 0.21.x), use the ``min_count``
keyword.

.. ipython:: python
   :okwarning:

   pd.Series([]).sum(min_count=1)

Thanks to the ``skipna`` parameter, the ``.sum`` on an all-*NA*
series is conceptually the same as the ``.sum`` of an empty one with
``skipna=True`` (the default).

.. ipython:: python

   pd.Series([np.nan]).sum(min_count=1)  # skipna=True by default

The ``min_count`` parameter refers to the minimum number of *non-null* values
required for a non-NA sum or product.

:meth:`Series.prod` has been updated to behave the same as :meth:`Series.sum`,
returning ``1`` instead.

.. ipython:: python
   :okwarning:

   pd.Series([]).prod()
   pd.Series([np.nan]).prod()
   pd.Series([]).prod(min_count=1)

These changes affect :meth:`DataFrame.sum` and :meth:`DataFrame.prod` as well.
Finally, a few less obvious places in pandas are affected by this change.

Grouping by a Categorical
^^^^^^^^^^^^^^^^^^^^^^^^^

Grouping by a ``Categorical`` and summing now returns ``0`` instead of
``NaN`` for categories with no observations. The product now returns ``1``
instead of ``NaN``.

*pandas 0.21.x*

.. code-block:: ipython

   In [8]: grouper = pd.Categorical(['a', 'a'], categories=['a', 'b'])

   In [9]: pd.Series([1, 2]).groupby(grouper).sum()
   Out[9]:
   a    3.0
   b    NaN
   dtype: float64

*pandas 0.22*

.. ipython:: python

   grouper = pd.Categorical(["a", "a"], categories=["a", "b"])
   pd.Series([1, 2]).groupby(grouper).sum()

To restore the 0.21 behavior of returning ``NaN`` for unobserved groups,
use ``min_count>=1``.

.. ipython:: python

   pd.Series([1, 2]).groupby(grouper).sum(min_count=1)

Resample
^^^^^^^^

The sum and product of all-*NA* bins has changed from ``NaN`` to ``0`` for
sum and ``1`` for product.

*pandas 0.21.x*

.. code-block:: ipython

   In [11]: s = pd.Series([1, 1, np.nan, np.nan],
      ....:               index=pd.date_range('2017', periods=4))
      ....: s
   Out[11]:
   2017-01-01    1.0
   2017-01-02    1.0
   2017-01-03    NaN
   2017-01-04    NaN
   Freq: D, dtype: float64

   In [12]: s.resample('2d').sum()
   Out[12]:
   2017-01-01    2.0
   2017-01-03    NaN
   Freq: 2D, dtype: float64

*pandas 0.22.0*

.. ipython:: python

   s = pd.Series([1, 1, np.nan, np.nan], index=pd.date_range("2017", periods=4))
   s.resample("2d").sum()

To restore the 0.21 behavior of returning ``NaN``, use ``min_count>=1``.

.. ipython:: python

   s.resample("2d").sum(min_count=1)

In particular, upsampling and taking the sum or product is affected, as
upsampling introduces missing values even if the original series was
entirely valid.

*pandas 0.21.x*

.. code-block:: ipython

   In [14]: idx = pd.DatetimeIndex(['2017-01-01', '2017-01-02'])

   In [15]: pd.Series([1, 2], index=idx).resample('12H').sum()
   Out[15]:
   2017-01-01 00:00:00    1.0
   2017-01-01 12:00:00    NaN
   2017-01-02 00:00:00    2.0
   Freq: 12H, dtype: float64

*pandas 0.22.0*

.. ipython:: python

   idx = pd.DatetimeIndex(["2017-01-01", "2017-01-02"])
   pd.Series([1, 2], index=idx).resample("12H").sum()

Once again, the ``min_count`` keyword is available to restore the 0.21 behavior.

.. ipython:: python

   pd.Series([1, 2], index=idx).resample("12H").sum(min_count=1)

Rolling and expanding
^^^^^^^^^^^^^^^^^^^^^

Rolling and expanding already have a ``min_periods`` keyword that behaves
similar to ``min_count``. The only case that changes is when doing a rolling
or expanding sum with ``min_periods=0``. Previously this returned ``NaN``,
when fewer than ``min_periods`` non-*NA* values were in the window. Now it
returns ``0``.

*pandas 0.21.1*

.. code-block:: ipython

   In [17]: s = pd.Series([np.nan, np.nan])

   In [18]: s.rolling(2, min_periods=0).sum()
   Out[18]:
   0   NaN
   1   NaN
   dtype: float64

*pandas 0.22.0*

.. ipython:: python

   s = pd.Series([np.nan, np.nan])
   s.rolling(2, min_periods=0).sum()

The default behavior of ``min_periods=None``, implying that ``min_periods``
equals the window size, is unchanged.

Compatibility
~~~~~~~~~~~~~

If you maintain a library that should work across pandas versions, it
may be easiest to exclude pandas 0.21 from your requirements. Otherwise, all your
``sum()`` calls would need to check if the ``Series`` is empty before summing.

With setuptools, in your ``setup.py`` use::

    install_requires=['pandas!=0.21.*', ...]

With conda, use

.. code-block:: yaml

    requirements:
      run:
        - pandas !=0.21.0,!=0.21.1

Note that the inconsistency in the return value for all-*NA* series is still
there for pandas 0.20.3 and earlier. Avoiding pandas 0.21 will only help with
the empty case.


.. _whatsnew_0.22.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.21.1..v0.22.0
.. _whatsnew_0171:

Version 0.17.1 (November 21, 2015)
----------------------------------

{{ header }}


.. note::

   We are proud to announce that *pandas* has become a sponsored project of the (`NumFOCUS organization`_). This will help ensure the success of development of *pandas* as a world-class open-source project.

.. _numfocus organization: http://www.numfocus.org/blog/numfocus-announces-new-fiscally-sponsored-project-pandas

This is a minor bug-fix release from 0.17.0 and includes a large number of
bug fixes along several new features, enhancements, and performance improvements.
We recommend that all users upgrade to this version.

Highlights include:

- Support for Conditional HTML Formatting, see :ref:`here <whatsnew_0171.style>`
- Releasing the GIL on the csv reader & other ops, see :ref:`here <whatsnew_0171.performance>`
- Fixed regression in ``DataFrame.drop_duplicates`` from 0.16.2, causing incorrect results on integer values (:issue:`11376`)

.. contents:: What's new in v0.17.1
    :local:
    :backlinks: none

New features
~~~~~~~~~~~~

.. _whatsnew_0171.style:

Conditional HTML formatting
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning::
    This is a new feature and is under active development.
    We'll be adding features an  possibly making breaking changes in future
    releases. Feedback is welcome in :issue:`11610`

We've added *experimental* support for conditional HTML formatting:
the visual styling of a DataFrame based on the data.
The styling is accomplished with HTML and CSS.
Accesses the styler class with the :attr:`pandas.DataFrame.style`, attribute,
an instance of :class:`~pandas.core.style.Styler` with your data attached.

Here's a quick example:

  .. ipython:: python

    np.random.seed(123)
    df = pd.DataFrame(np.random.randn(10, 5), columns=list("abcde"))
    html = df.style.background_gradient(cmap="viridis", low=0.5)

We can render the HTML to get the following table.

.. raw:: html
   :file: whatsnew_0171_html_table.html

:class:`~pandas.core.style.Styler` interacts nicely with the Jupyter Notebook.
See the :ref:`documentation </user_guide/style.ipynb>` for more.

.. _whatsnew_0171.enhancements:

Enhancements
~~~~~~~~~~~~

- ``DatetimeIndex`` now supports conversion to strings with ``astype(str)`` (:issue:`10442`)
- Support for ``compression`` (gzip/bz2) in :meth:`pandas.DataFrame.to_csv` (:issue:`7615`)
- ``pd.read_*`` functions can now also accept :class:`python:pathlib.Path`, or :class:`py:py._path.local.LocalPath`
  objects for the ``filepath_or_buffer`` argument. (:issue:`11033`)
  - The ``DataFrame`` and ``Series`` functions ``.to_csv()``, ``.to_html()`` and ``.to_latex()`` can now handle paths beginning with tildes (e.g. ``~/Documents/``) (:issue:`11438`)
- ``DataFrame`` now uses the fields of a ``namedtuple`` as columns, if columns are not supplied (:issue:`11181`)
- ``DataFrame.itertuples()`` now returns ``namedtuple`` objects, when possible. (:issue:`11269`, :issue:`11625`)
- Added ``axvlines_kwds`` to parallel coordinates plot (:issue:`10709`)
- Option to ``.info()`` and ``.memory_usage()`` to provide for deep introspection of memory consumption. Note that this can be expensive to compute and therefore is an optional parameter. (:issue:`11595`)

  .. ipython:: python

     df = pd.DataFrame({"A": ["foo"] * 1000})  # noqa: F821
     df["B"] = df["A"].astype("category")

     # shows the '+' as we have object dtypes
     df.info()

     # we have an accurate memory assessment (but can be expensive to compute this)
     df.info(memory_usage="deep")

- ``Index`` now has a ``fillna`` method (:issue:`10089`)

  .. ipython:: python

     pd.Index([1, np.nan, 3]).fillna(2)

- Series of type ``category`` now make ``.str.<...>`` and ``.dt.<...>`` accessor methods / properties available, if the categories are of that type. (:issue:`10661`)

  .. ipython:: python

     s = pd.Series(list("aabb")).astype("category")
     s
     s.str.contains("a")

     date = pd.Series(pd.date_range("1/1/2015", periods=5)).astype("category")
     date
     date.dt.day

- ``pivot_table`` now has a ``margins_name`` argument so you can use something other than the default of 'All' (:issue:`3335`)
- Implement export of ``datetime64[ns, tz]`` dtypes with a fixed HDF5 store (:issue:`11411`)
- Pretty printing sets (e.g. in DataFrame cells) now uses set literal syntax (``{x, y}``) instead of
  Legacy Python syntax (``set([x, y])``) (:issue:`11215`)
- Improve the error message in :func:`pandas.io.gbq.to_gbq` when a streaming insert fails (:issue:`11285`)
  and when the DataFrame does not match the schema of the destination table (:issue:`11359`)

.. _whatsnew_0171.api:

API changes
~~~~~~~~~~~

- raise ``NotImplementedError`` in ``Index.shift`` for non-supported index types (:issue:`8038`)
- ``min`` and ``max`` reductions on ``datetime64`` and ``timedelta64`` dtyped series now
  result in ``NaT`` and not ``nan`` (:issue:`11245`).
- Indexing with a null key will raise a ``TypeError``, instead of a ``ValueError`` (:issue:`11356`)
- ``Series.ptp`` will now ignore missing values by default (:issue:`11163`)

.. _whatsnew_0171.deprecations:

Deprecations
^^^^^^^^^^^^

- The ``pandas.io.ga`` module which implements ``google-analytics`` support is deprecated and will be removed in a future version (:issue:`11308`)
- Deprecate the ``engine`` keyword in ``.to_csv()``, which will be removed in a future version (:issue:`11274`)

.. _whatsnew_0171.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Checking monotonic-ness before sorting on an index (:issue:`11080`)
- ``Series.dropna`` performance improvement when its dtype can't contain ``NaN`` (:issue:`11159`)
- Release the GIL on most datetime field operations (e.g. ``DatetimeIndex.year``, ``Series.dt.year``), normalization, and conversion to and from ``Period``, ``DatetimeIndex.to_period`` and ``PeriodIndex.to_timestamp`` (:issue:`11263`)
- Release the GIL on some rolling algos: ``rolling_median``, ``rolling_mean``, ``rolling_max``, ``rolling_min``, ``rolling_var``, ``rolling_kurt``, ``rolling_skew`` (:issue:`11450`)
- Release the GIL when reading and parsing text files in ``read_csv``, ``read_table`` (:issue:`11272`)
- Improved performance of ``rolling_median`` (:issue:`11450`)
- Improved performance of ``to_excel`` (:issue:`11352`)
- Performance bug in repr of ``Categorical`` categories, which was rendering the strings before chopping them for display (:issue:`11305`)
- Performance improvement in ``Categorical.remove_unused_categories``, (:issue:`11643`).
- Improved performance of ``Series`` constructor with no data and ``DatetimeIndex`` (:issue:`11433`)
- Improved performance of ``shift``, ``cumprod``, and ``cumsum`` with groupby (:issue:`4095`)

.. _whatsnew_0171.bug_fixes:

Bug fixes
~~~~~~~~~

- ``SparseArray.__iter__()`` now does not cause ``PendingDeprecationWarning`` in Python 3.5 (:issue:`11622`)
- Regression from 0.16.2 for output formatting of long floats/nan, restored in (:issue:`11302`)
- ``Series.sort_index()`` now correctly handles the ``inplace`` option (:issue:`11402`)
- Incorrectly distributed .c file in the build on ``PyPi`` when reading a csv of floats and passing ``na_values=<a scalar>`` would show an exception (:issue:`11374`)
- Bug in ``.to_latex()`` output broken when the index has a name (:issue:`10660`)
- Bug in ``HDFStore.append`` with strings whose encoded length exceeded the max unencoded length (:issue:`11234`)
- Bug in merging ``datetime64[ns, tz]`` dtypes (:issue:`11405`)
- Bug in ``HDFStore.select`` when comparing with a numpy scalar in a where clause (:issue:`11283`)
- Bug in using ``DataFrame.ix`` with a MultiIndex indexer (:issue:`11372`)
- Bug in ``date_range`` with ambiguous endpoints (:issue:`11626`)
- Prevent adding new attributes to the accessors ``.str``, ``.dt`` and ``.cat``. Retrieving such
  a value was not possible, so error out on setting it. (:issue:`10673`)
- Bug in tz-conversions with an ambiguous time and ``.dt`` accessors (:issue:`11295`)
- Bug in output formatting when using an index of ambiguous times (:issue:`11619`)
- Bug in comparisons of Series vs list-likes (:issue:`11339`)
- Bug in ``DataFrame.replace`` with a ``datetime64[ns, tz]`` and a non-compat to_replace (:issue:`11326`, :issue:`11153`)
- Bug in ``isnull`` where ``numpy.datetime64('NaT')`` in a ``numpy.array`` was not determined to be null(:issue:`11206`)
- Bug in list-like indexing with a mixed-integer Index (:issue:`11320`)
- Bug in ``pivot_table`` with ``margins=True`` when indexes are of ``Categorical`` dtype (:issue:`10993`)
- Bug in ``DataFrame.plot`` cannot use hex strings colors (:issue:`10299`)
- Regression in ``DataFrame.drop_duplicates`` from 0.16.2, causing incorrect results on integer values (:issue:`11376`)
- Bug in ``pd.eval`` where unary ops in a list error (:issue:`11235`)
- Bug in ``squeeze()`` with zero length arrays (:issue:`11230`, :issue:`8999`)
- Bug in ``describe()`` dropping column names for hierarchical indexes (:issue:`11517`)
- Bug in ``DataFrame.pct_change()`` not propagating ``axis`` keyword on ``.fillna`` method (:issue:`11150`)
- Bug in ``.to_csv()`` when a mix of integer and string column names are passed as the ``columns`` parameter (:issue:`11637`)
- Bug in indexing with a ``range``, (:issue:`11652`)
- Bug in inference of numpy scalars and preserving dtype when setting columns (:issue:`11638`)
- Bug in ``to_sql`` using unicode column names giving UnicodeEncodeError with (:issue:`11431`).
- Fix regression in setting of ``xticks`` in ``plot`` (:issue:`11529`).
- Bug in ``holiday.dates`` where observance rules could not be applied to holiday and doc enhancement (:issue:`11477`, :issue:`11533`)
- Fix plotting issues when having plain ``Axes`` instances instead of ``SubplotAxes`` (:issue:`11520`, :issue:`11556`).
- Bug in ``DataFrame.to_latex()`` produces an extra rule when ``header=False`` (:issue:`7124`)
- Bug in ``df.groupby(...).apply(func)`` when a func returns a ``Series`` containing a new datetimelike column (:issue:`11324`)
- Bug in ``pandas.json`` when file to load is big (:issue:`11344`)
- Bugs in ``to_excel`` with duplicate columns (:issue:`11007`, :issue:`10982`, :issue:`10970`)
- Fixed a bug that prevented the construction of an empty series of dtype ``datetime64[ns, tz]`` (:issue:`11245`).
- Bug in ``read_excel`` with MultiIndex containing integers (:issue:`11317`)
- Bug in ``to_excel`` with openpyxl 2.2+ and merging (:issue:`11408`)
- Bug in ``DataFrame.to_dict()`` produces a ``np.datetime64`` object instead of ``Timestamp`` when only datetime is present in data (:issue:`11327`)
- Bug in ``DataFrame.corr()`` raises exception when computes Kendall correlation for DataFrames with boolean and not boolean columns (:issue:`11560`)
- Bug in the link-time error caused by C ``inline`` functions on FreeBSD 10+ (with ``clang``) (:issue:`10510`)
- Bug in ``DataFrame.to_csv`` in passing through arguments for formatting ``MultiIndexes``, including ``date_format`` (:issue:`7791`)
- Bug in ``DataFrame.join()`` with ``how='right'`` producing a ``TypeError`` (:issue:`11519`)
- Bug in ``Series.quantile`` with empty list results has ``Index`` with ``object`` dtype (:issue:`11588`)
- Bug in ``pd.merge`` results in empty ``Int64Index`` rather than ``Index(dtype=object)`` when the merge result is empty (:issue:`11588`)
- Bug in ``Categorical.remove_unused_categories`` when having ``NaN`` values (:issue:`11599`)
- Bug in ``DataFrame.to_sparse()`` loses column names for MultiIndexes (:issue:`11600`)
- Bug in ``DataFrame.round()`` with non-unique column index producing a Fatal Python error (:issue:`11611`)
- Bug in ``DataFrame.round()`` with ``decimals`` being a non-unique indexed Series producing extra columns (:issue:`11618`)


.. _whatsnew_0.17.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.17.0..v0.17.1
.. _whatsnew_110:

What's new in 1.1.0 (July 28, 2020)
-----------------------------------

These are the changes in pandas 1.1.0. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

Enhancements
~~~~~~~~~~~~

.. _whatsnew_110.specify_missing_labels:

KeyErrors raised by loc specify missing labels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Previously, if labels were missing for a ``.loc`` call, a KeyError was raised stating that this was no longer supported.

Now the error message also includes a list of the missing labels (max 10 items, display width 80 characters). See :issue:`34272`.


.. _whatsnew_110.astype_string:

All dtypes can now be converted to ``StringDtype``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, declaring or converting to :class:`StringDtype` was in general only possible if the data was already only ``str`` or nan-like (:issue:`31204`).
:class:`StringDtype` now works in all situations where ``astype(str)`` or ``dtype=str`` work:

For example, the below now works:

.. ipython:: python

   ser = pd.Series([1, "abc", np.nan], dtype="string")
   ser
   ser[0]
   pd.Series([1, 2, np.nan], dtype="Int64").astype("string")


.. _whatsnew_110.period_index_partial_string_slicing:

Non-monotonic PeriodIndex partial string slicing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`PeriodIndex` now supports partial string slicing for non-monotonic indexes, mirroring :class:`DatetimeIndex` behavior (:issue:`31096`)

For example:

.. ipython:: python

   dti = pd.date_range("2014-01-01", periods=30, freq="30D")
   pi = dti.to_period("D")
   ser_monotonic = pd.Series(np.arange(30), index=pi)
   shuffler = list(range(0, 30, 2)) + list(range(1, 31, 2))
   ser = ser_monotonic[shuffler]
   ser

.. ipython:: python

   ser["2014"]
   ser.loc["May 2015"]


.. _whatsnew_110.dataframe_or_series_comparing:

Comparing two ``DataFrame`` or two ``Series`` and summarizing the differences
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added :meth:`DataFrame.compare` and :meth:`Series.compare` for comparing two ``DataFrame`` or two ``Series`` (:issue:`30429`)

.. ipython:: python

   df = pd.DataFrame(
       {
           "col1": ["a", "a", "b", "b", "a"],
           "col2": [1.0, 2.0, 3.0, np.nan, 5.0],
           "col3": [1.0, 2.0, 3.0, 4.0, 5.0]
       },
       columns=["col1", "col2", "col3"],
   )
   df

.. ipython:: python

   df2 = df.copy()
   df2.loc[0, 'col1'] = 'c'
   df2.loc[2, 'col3'] = 4.0
   df2

.. ipython:: python

   df.compare(df2)

See :ref:`User Guide <merging.compare>` for more details.


.. _whatsnew_110.groupby_key:

Allow NA in groupby key
^^^^^^^^^^^^^^^^^^^^^^^^

With :ref:`groupby <groupby.dropna>` , we've added a ``dropna`` keyword to :meth:`DataFrame.groupby` and :meth:`Series.groupby` in order to
allow ``NA`` values in group keys. Users can define ``dropna`` to ``False`` if they want to include
``NA`` values in groupby keys. The default is set to ``True`` for ``dropna`` to keep backwards
compatibility (:issue:`3729`)

.. ipython:: python

    df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]
    df_dropna = pd.DataFrame(df_list, columns=["a", "b", "c"])

    df_dropna

.. ipython:: python

    # Default ``dropna`` is set to True, which will exclude NaNs in keys
    df_dropna.groupby(by=["b"], dropna=True).sum()

    # In order to allow NaN in keys, set ``dropna`` to False
    df_dropna.groupby(by=["b"], dropna=False).sum()

The default setting of ``dropna`` argument is ``True`` which means ``NA`` are not included in group keys.


.. _whatsnew_110.key_sorting:

Sorting with keys
^^^^^^^^^^^^^^^^^

We've added a ``key`` argument to the :class:`DataFrame` and :class:`Series` sorting methods, including
:meth:`DataFrame.sort_values`, :meth:`DataFrame.sort_index`, :meth:`Series.sort_values`,
and :meth:`Series.sort_index`. The ``key`` can be any callable function which is applied
column-by-column to each column used for sorting, before sorting is performed (:issue:`27237`).
See :ref:`sort_values with keys <basics.sort_value_key>` and :ref:`sort_index with keys
<basics.sort_index_key>` for more information.

.. ipython:: python

   s = pd.Series(['C', 'a', 'B'])
   s

.. ipython:: python

   s.sort_values()


Note how this is sorted with capital letters first. If we apply the :meth:`Series.str.lower`
method, we get

.. ipython:: python

   s.sort_values(key=lambda x: x.str.lower())


When applied to a ``DataFrame``, they key is applied per-column to all columns or a subset if
``by`` is specified, e.g.

.. ipython:: python

   df = pd.DataFrame({'a': ['C', 'C', 'a', 'a', 'B', 'B'],
                      'b': [1, 2, 3, 4, 5, 6]})
   df

.. ipython:: python

   df.sort_values(by=['a'], key=lambda col: col.str.lower())


For more details, see examples and documentation in :meth:`DataFrame.sort_values`,
:meth:`Series.sort_values`, and :meth:`~DataFrame.sort_index`.

.. _whatsnew_110.timestamp_fold_support:

Fold argument support in Timestamp constructor
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Timestamp:` now supports the keyword-only fold argument according to `PEP 495 <https://www.python.org/dev/peps/pep-0495/#the-fold-attribute>`_ similar to parent ``datetime.datetime`` class. It supports both accepting fold as an initialization argument and inferring fold from other constructor arguments (:issue:`25057`, :issue:`31338`). Support is limited to ``dateutil`` timezones as ``pytz`` doesn't support fold.

For example:

.. ipython:: python

    ts = pd.Timestamp("2019-10-27 01:30:00+00:00")
    ts.fold

.. ipython:: python

    ts = pd.Timestamp(year=2019, month=10, day=27, hour=1, minute=30,
                      tz="dateutil/Europe/London", fold=1)
    ts

For more on working with fold, see :ref:`Fold subsection <timeseries.fold>` in the user guide.

.. _whatsnew_110.to_datetime_multiple_tzname_tzoffset_support:

Parsing timezone-aware format with different timezones in to_datetime
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`to_datetime` now supports parsing formats containing timezone names (``%Z``) and UTC offsets (``%z``) from different timezones then converting them to UTC by setting ``utc=True``. This would return a :class:`DatetimeIndex` with timezone at UTC as opposed to an :class:`Index` with ``object`` dtype if ``utc=True`` is not set (:issue:`32792`).

For example:

.. ipython:: python

    tz_strs = ["2010-01-01 12:00:00 +0100", "2010-01-01 12:00:00 -0100",
               "2010-01-01 12:00:00 +0300", "2010-01-01 12:00:00 +0400"]
    pd.to_datetime(tz_strs, format='%Y-%m-%d %H:%M:%S %z', utc=True)
    pd.to_datetime(tz_strs, format='%Y-%m-%d %H:%M:%S %z')

.. _whatsnew_110.grouper_resample_origin:

Grouper and resample now supports the arguments origin and offset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Grouper` and :meth:`DataFrame.resample` now supports the arguments ``origin`` and ``offset``. It let the user control the timestamp on which to adjust the grouping. (:issue:`31809`)

The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like ``30D``) or that divides a day (like ``90s`` or ``1min``). But it can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can now specify a fixed timestamp with the argument ``origin``.

Two arguments are now deprecated (more information in the documentation of :meth:`DataFrame.resample`):

- ``base`` should be replaced by ``offset``.
- ``loffset`` should be replaced by directly adding an offset to the index :class:`DataFrame` after being resampled.

Small example of the use of ``origin``:

.. ipython:: python

    start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'
    middle = '2000-10-02 00:00:00'
    rng = pd.date_range(start, end, freq='7min')
    ts = pd.Series(np.arange(len(rng)) * 3, index=rng)
    ts

Resample with the default behavior ``'start_day'`` (origin is ``2000-10-01 00:00:00``):

.. ipython:: python

    ts.resample('17min').sum()
    ts.resample('17min', origin='start_day').sum()

Resample using a fixed origin:

.. ipython:: python

    ts.resample('17min', origin='epoch').sum()
    ts.resample('17min', origin='2000-01-01').sum()

If needed you can adjust the bins with the argument ``offset`` (a :class:`Timedelta`) that would be added to the default ``origin``.

For a full example, see: :ref:`timeseries.adjust-the-start-of-the-bins`.

fsspec now used for filesystem handling
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For reading and writing to filesystems other than local and reading from HTTP(S),
the optional dependency ``fsspec`` will be used to dispatch operations (:issue:`33452`).
This will give unchanged
functionality for S3 and GCS storage, which were already supported, but also add
support for several other storage implementations such as `Azure Datalake and Blob`_,
SSH, FTP, dropbox and github. For docs and capabilities, see the `fsspec docs`_.

The existing capability to interface with S3 and GCS will be unaffected by this
change, as ``fsspec`` will still bring in the same packages as before.

.. _Azure Datalake and Blob: https://github.com/fsspec/adlfs

.. _fsspec docs: https://filesystem-spec.readthedocs.io/en/latest/

.. _whatsnew_110.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Compatibility with matplotlib 3.3.0 (:issue:`34850`)
- :meth:`IntegerArray.astype` now supports ``datetime64`` dtype (:issue:`32538`)
- :class:`IntegerArray` now implements the ``sum`` operation (:issue:`33172`)
- Added :class:`pandas.errors.InvalidIndexError` (:issue:`34570`).
- Added :meth:`DataFrame.value_counts` (:issue:`5377`)
- Added a :func:`pandas.api.indexers.FixedForwardWindowIndexer` class to support forward-looking windows during ``rolling`` operations.
- Added a :func:`pandas.api.indexers.VariableOffsetWindowIndexer` class to support ``rolling`` operations with non-fixed offsets (:issue:`34994`)
- :meth:`~DataFrame.describe` now includes a ``datetime_is_numeric`` keyword to control how datetime columns are summarized (:issue:`30164`, :issue:`34798`)
- :class:`~pandas.io.formats.style.Styler` may now render CSS more efficiently where multiple cells have the same styling (:issue:`30876`)
- :meth:`~pandas.io.formats.style.Styler.highlight_null` now accepts ``subset`` argument (:issue:`31345`)
- When writing directly to a sqlite connection :meth:`DataFrame.to_sql` now supports the ``multi`` method (:issue:`29921`)
- :class:`pandas.errors.OptionError` is now exposed in ``pandas.errors`` (:issue:`27553`)
- Added :meth:`api.extensions.ExtensionArray.argmax` and :meth:`api.extensions.ExtensionArray.argmin` (:issue:`24382`)
- :func:`timedelta_range` will now infer a frequency when passed ``start``, ``stop``, and ``periods`` (:issue:`32377`)
- Positional slicing on a :class:`IntervalIndex` now supports slices with ``step > 1`` (:issue:`31658`)
- :class:`Series.str` now has a ``fullmatch`` method that matches a regular expression against the entire string in each row of the :class:`Series`, similar to ``re.fullmatch`` (:issue:`32806`).
- :meth:`DataFrame.sample` will now also allow array-like and BitGenerator objects to be passed to ``random_state`` as seeds (:issue:`32503`)
- :meth:`Index.union` will now raise ``RuntimeWarning`` for :class:`MultiIndex` objects if the object inside are unsortable. Pass ``sort=False`` to suppress this warning (:issue:`33015`)
- Added :meth:`Series.dt.isocalendar` and :meth:`DatetimeIndex.isocalendar` that returns a :class:`DataFrame` with year, week, and day calculated according to the ISO 8601 calendar (:issue:`33206`, :issue:`34392`).
- The :meth:`DataFrame.to_feather` method now supports additional keyword
  arguments (e.g. to set the compression) that are added in pyarrow 0.17
  (:issue:`33422`).
- The :func:`cut` will now accept parameter ``ordered`` with default ``ordered=True``. If ``ordered=False`` and no labels are provided, an error will be raised (:issue:`33141`)
- :meth:`DataFrame.to_csv`, :meth:`DataFrame.to_pickle`,
  and :meth:`DataFrame.to_json` now support passing a dict of
  compression arguments when using the ``gzip`` and ``bz2`` protocols.
  This can be used to set a custom compression level, e.g.,
  ``df.to_csv(path, compression={'method': 'gzip', 'compresslevel': 1}``
  (:issue:`33196`)
- :meth:`melt` has gained an ``ignore_index`` (default ``True``) argument that, if set to ``False``, prevents the method from dropping the index (:issue:`17440`).
- :meth:`Series.update` now accepts objects that can be coerced to a :class:`Series`,
  such as ``dict`` and ``list``, mirroring the behavior of :meth:`DataFrame.update` (:issue:`33215`)
- :meth:`~pandas.core.groupby.DataFrameGroupBy.transform` and :meth:`~pandas.core.groupby.DataFrameGroupBy.aggregate` have gained ``engine`` and ``engine_kwargs`` arguments that support executing functions with ``Numba`` (:issue:`32854`, :issue:`33388`)
- :meth:`~pandas.core.resample.Resampler.interpolate` now supports SciPy interpolation method :class:`scipy.interpolate.CubicSpline` as method ``cubicspline`` (:issue:`33670`)
- :class:`~pandas.core.groupby.DataFrameGroupBy` and :class:`~pandas.core.groupby.SeriesGroupBy` now implement the ``sample`` method for doing random sampling within groups (:issue:`31775`)
- :meth:`DataFrame.to_numpy` now supports the ``na_value`` keyword to control the NA sentinel in the output array (:issue:`33820`)
- Added :class:`api.extension.ExtensionArray.equals` to the extension array interface, similar to :meth:`Series.equals` (:issue:`27081`)
- The minimum supported dta version has increased to 105 in :func:`read_stata` and :class:`~pandas.io.stata.StataReader`  (:issue:`26667`).
- :meth:`~DataFrame.to_stata` supports compression using the ``compression``
  keyword argument. Compression can either be inferred or explicitly set using a string or a
  dictionary containing both the method and any additional arguments that are passed to the
  compression library. Compression was also added to the low-level Stata-file writers
  :class:`~pandas.io.stata.StataWriter`, :class:`~pandas.io.stata.StataWriter117`,
  and :class:`~pandas.io.stata.StataWriterUTF8` (:issue:`26599`).
- :meth:`HDFStore.put` now accepts a ``track_times`` parameter. This parameter is passed to the ``create_table`` method of ``PyTables`` (:issue:`32682`).
- :meth:`Series.plot` and :meth:`DataFrame.plot` now accepts ``xlabel`` and ``ylabel`` parameters to present labels on x and y axis (:issue:`9093`).
- Made :class:`pandas.core.window.rolling.Rolling` and :class:`pandas.core.window.expanding.Expanding` iterable（:issue:`11704`)
- Made ``option_context`` a :class:`contextlib.ContextDecorator`, which allows it to be used as a decorator over an entire function (:issue:`34253`).
- :meth:`DataFrame.to_csv` and :meth:`Series.to_csv` now accept an ``errors`` argument (:issue:`22610`)
- :meth:`~pandas.core.groupby.DataFrameGroupBy.groupby.transform` now allows ``func`` to be ``pad``, ``backfill`` and ``cumcount`` (:issue:`31269`).
- :func:`read_json` now accepts an ``nrows`` parameter. (:issue:`33916`).
- :meth:`DataFrame.hist`, :meth:`Series.hist`, :meth:`core.groupby.DataFrameGroupBy.hist`, and :meth:`core.groupby.SeriesGroupBy.hist` have gained the ``legend`` argument. Set to True to show a legend in the histogram. (:issue:`6279`)
- :func:`concat` and :meth:`~DataFrame.append` now preserve extension dtypes, for example
  combining a nullable integer column with a numpy integer column will no longer
  result in object dtype but preserve the integer dtype (:issue:`33607`, :issue:`34339`, :issue:`34095`).
- :func:`read_gbq` now allows to disable progress bar (:issue:`33360`).
- :func:`read_gbq` now supports the ``max_results`` kwarg from ``pandas-gbq`` (:issue:`34639`).
- :meth:`DataFrame.cov` and :meth:`Series.cov` now support a new parameter ``ddof`` to support delta degrees of freedom as in the corresponding numpy methods (:issue:`34611`).
- :meth:`DataFrame.to_html` and :meth:`DataFrame.to_string`'s ``col_space`` parameter now accepts a list or dict to change only some specific columns' width (:issue:`28917`).
- :meth:`DataFrame.to_excel` can now also write OpenOffice spreadsheet (.ods) files (:issue:`27222`)
- :meth:`~Series.explode` now accepts ``ignore_index`` to reset the index, similar to :meth:`pd.concat` or :meth:`DataFrame.sort_values` (:issue:`34932`).
- :meth:`DataFrame.to_markdown` and :meth:`Series.to_markdown` now accept ``index`` argument as an alias for tabulate's ``showindex`` (:issue:`32667`)
- :meth:`read_csv` now accepts string values like "0", "0.0", "1", "1.0" as convertible to the nullable Boolean dtype (:issue:`34859`)
- :class:`pandas.core.window.ExponentialMovingWindow` now supports a ``times`` argument that allows ``mean`` to be calculated with observations spaced by the timestamps in ``times`` (:issue:`34839`)
- :meth:`DataFrame.agg` and :meth:`Series.agg` now accept named aggregation for renaming the output columns/indexes. (:issue:`26513`)
- ``compute.use_numba`` now exists as a configuration option that utilizes the numba engine when available (:issue:`33966`, :issue:`35374`)
- :meth:`Series.plot` now supports asymmetric error bars. Previously, if :meth:`Series.plot` received a "2xN" array with error values for ``yerr`` and/or ``xerr``, the left/lower values (first row) were mirrored, while the right/upper values (second row) were ignored. Now, the first row represents the left/lower error values and the second row the right/upper error values. (:issue:`9536`)

.. ---------------------------------------------------------------------------

.. _whatsnew_110.notable_bug_fixes:

Notable bug fixes
~~~~~~~~~~~~~~~~~

These are bug fixes that might have notable behavior changes.

``MultiIndex.get_indexer`` interprets ``method`` argument correctly
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This restores the behavior of :meth:`MultiIndex.get_indexer` with ``method='backfill'`` or ``method='pad'`` to the behavior before pandas 0.23.0. In particular, MultiIndexes are treated as a list of tuples and padding or backfilling is done with respect to the ordering of these lists of tuples (:issue:`29896`).

As an example of this, given:

.. ipython:: python

        df = pd.DataFrame({
            'a': [0, 0, 0, 0],
            'b': [0, 2, 3, 4],
            'c': ['A', 'B', 'C', 'D'],
        }).set_index(['a', 'b'])
        mi_2 = pd.MultiIndex.from_product([[0], [-1, 0, 1, 3, 4, 5]])

The differences in reindexing ``df`` with ``mi_2`` and using ``method='backfill'`` can be seen here:

*pandas >= 0.23, < 1.1.0*:

.. code-block:: ipython

    In [1]: df.reindex(mi_2, method='backfill')
    Out[1]:
          c
    0 -1  A
       0  A
       1  D
       3  A
       4  A
       5  C

*pandas <0.23, >= 1.1.0*

.. ipython:: python

        df.reindex(mi_2, method='backfill')

And the differences in reindexing ``df`` with ``mi_2`` and using ``method='pad'`` can be seen here:

*pandas >= 0.23, < 1.1.0*

.. code-block:: ipython

    In [1]: df.reindex(mi_2, method='pad')
    Out[1]:
            c
    0 -1  NaN
       0  NaN
       1    D
       3  NaN
       4    A
       5    C

*pandas < 0.23, >= 1.1.0*

.. ipython:: python

        df.reindex(mi_2, method='pad')

.. _whatsnew_110.notable_bug_fixes.indexing_raises_key_errors:

Failed label-based lookups always raise KeyError
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Label lookups ``series[key]``, ``series.loc[key]`` and ``frame.loc[key]``
used to raise either ``KeyError`` or ``TypeError`` depending on the type of
key and type of :class:`Index`.  These now consistently raise ``KeyError`` (:issue:`31867`)

.. ipython:: python

    ser1 = pd.Series(range(3), index=[0, 1, 2])
    ser2 = pd.Series(range(3), index=pd.date_range("2020-02-01", periods=3))

*Previous behavior*:

.. code-block:: ipython

    In [3]: ser1[1.5]
    ...
    TypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float

    In [4] ser1["foo"]
    ...
    KeyError: 'foo'

    In [5]: ser1.loc[1.5]
    ...
    TypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float

    In [6]: ser1.loc["foo"]
    ...
    KeyError: 'foo'

    In [7]: ser2.loc[1]
    ...
    TypeError: cannot do label indexing on DatetimeIndex with these indexers [1] of type int

    In [8]: ser2.loc[pd.Timestamp(0)]
    ...
    KeyError: Timestamp('1970-01-01 00:00:00')

*New behavior*:

.. code-block:: ipython

    In [3]: ser1[1.5]
    ...
    KeyError: 1.5

    In [4] ser1["foo"]
    ...
    KeyError: 'foo'

    In [5]: ser1.loc[1.5]
    ...
    KeyError: 1.5

    In [6]: ser1.loc["foo"]
    ...
    KeyError: 'foo'

    In [7]: ser2.loc[1]
    ...
    KeyError: 1

    In [8]: ser2.loc[pd.Timestamp(0)]
    ...
    KeyError: Timestamp('1970-01-01 00:00:00')


Similarly, :meth:`DataFrame.at` and :meth:`Series.at` will raise a ``TypeError`` instead of a ``ValueError`` if an incompatible key is passed, and ``KeyError`` if a missing key is passed, matching the behavior of ``.loc[]`` (:issue:`31722`)

.. _whatsnew_110.notable_bug_fixes.indexing_int_multiindex_raises_key_errors:

Failed Integer Lookups on MultiIndex Raise KeyError
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Indexing with integers with a :class:`MultiIndex` that has an integer-dtype
first level incorrectly failed to raise ``KeyError`` when one or more of
those integer keys is not present in the first level of the index (:issue:`33539`)

.. ipython:: python

    idx = pd.Index(range(4))
    dti = pd.date_range("2000-01-03", periods=3)
    mi = pd.MultiIndex.from_product([idx, dti])
    ser = pd.Series(range(len(mi)), index=mi)

*Previous behavior*:

.. code-block:: ipython

    In [5]: ser[[5]]
    Out[5]: Series([], dtype: int64)

*New behavior*:

.. code-block:: ipython

    In [5]: ser[[5]]
    ...
    KeyError: '[5] not in index'

:meth:`DataFrame.merge` preserves right frame's row order
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
:meth:`DataFrame.merge` now preserves the right frame's row order when executing a right merge (:issue:`27453`)

.. ipython:: python

    left_df = pd.DataFrame({'animal': ['dog', 'pig'],
                           'max_speed': [40, 11]})
    right_df = pd.DataFrame({'animal': ['quetzal', 'pig'],
                            'max_speed': [80, 11]})
    left_df
    right_df

*Previous behavior*:

.. code-block:: python

    >>> left_df.merge(right_df, on=['animal', 'max_speed'], how="right")
        animal  max_speed
    0      pig         11
    1  quetzal         80

*New behavior*:

.. ipython:: python

    left_df.merge(right_df, on=['animal', 'max_speed'], how="right")

.. ---------------------------------------------------------------------------

.. _whatsnew_110.notable_bug_fixes.assignment_to_multiple_columns:

Assignment to multiple columns of a DataFrame when some columns do not exist
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Assignment to multiple columns of a :class:`DataFrame` when some of the columns do not exist would previously assign the values to the last column. Now, new columns will be constructed with the right values. (:issue:`13658`)

.. ipython:: python

   df = pd.DataFrame({'a': [0, 1, 2], 'b': [3, 4, 5]})
   df

*Previous behavior*:

.. code-block:: ipython

   In [3]: df[['a', 'c']] = 1
   In [4]: df
   Out[4]:
      a  b
   0  1  1
   1  1  1
   2  1  1

*New behavior*:

.. ipython:: python

   df[['a', 'c']] = 1
   df

.. _whatsnew_110.notable_bug_fixes.groupby_consistency:

Consistency across groupby reductions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using :meth:`DataFrame.groupby` with ``as_index=True`` and the aggregation ``nunique`` would include the grouping column(s) in the columns of the result. Now the grouping column(s) only appear in the index, consistent with other reductions. (:issue:`32579`)

.. ipython:: python

   df = pd.DataFrame({"a": ["x", "x", "y", "y"], "b": [1, 1, 2, 3]})
   df

*Previous behavior*:

.. code-block:: ipython

   In [3]: df.groupby("a", as_index=True).nunique()
   Out[4]:
      a  b
   a
   x  1  1
   y  1  2

*New behavior*:

.. ipython:: python

   df.groupby("a", as_index=True).nunique()

Using :meth:`DataFrame.groupby` with ``as_index=False`` and the function ``idxmax``, ``idxmin``, ``mad``, ``nunique``, ``sem``, ``skew``, or ``std`` would modify the grouping column. Now the grouping column remains unchanged, consistent with other reductions. (:issue:`21090`, :issue:`10355`)

*Previous behavior*:

.. code-block:: ipython

   In [3]: df.groupby("a", as_index=False).nunique()
   Out[4]:
      a  b
   0  1  1
   1  1  2

*New behavior*:

.. ipython:: python

   df.groupby("a", as_index=False).nunique()

The method :meth:`~pandas.core.groupby.DataFrameGroupBy.size` would previously ignore ``as_index=False``. Now the grouping columns are returned as columns, making the result a :class:`DataFrame` instead of a :class:`Series`. (:issue:`32599`)

*Previous behavior*:

.. code-block:: ipython

   In [3]: df.groupby("a", as_index=False).size()
   Out[4]:
   a
   x    2
   y    2
   dtype: int64

*New behavior*:

.. ipython:: python

   df.groupby("a", as_index=False).size()

.. _whatsnew_110.api_breaking.groupby_results_lost_as_index_false:

:meth:`~pandas.core.groupby.DataFrameGroupby.agg` lost results with ``as_index=False`` when relabeling columns
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously :meth:`~pandas.core.groupby.DataFrameGroupby.agg` lost the result columns, when the ``as_index`` option was
set to ``False`` and the result columns were relabeled. In this case the result values were replaced with
the previous index (:issue:`32240`).

.. ipython:: python

   df = pd.DataFrame({"key": ["x", "y", "z", "x", "y", "z"],
                      "val": [1.0, 0.8, 2.0, 3.0, 3.6, 0.75]})
   df

*Previous behavior*:

.. code-block:: ipython

   In [2]: grouped = df.groupby("key", as_index=False)
   In [3]: result = grouped.agg(min_val=pd.NamedAgg(column="val", aggfunc="min"))
   In [4]: result
   Out[4]:
        min_val
    0 	x
    1 	y
    2 	z

*New behavior*:

.. ipython:: python

   grouped = df.groupby("key", as_index=False)
   result = grouped.agg(min_val=pd.NamedAgg(column="val", aggfunc="min"))
   result


.. _whatsnew_110.notable_bug_fixes.apply_applymap_first_once:

apply and applymap on ``DataFrame`` evaluates first row/column only once
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. ipython:: python

    df = pd.DataFrame({'a': [1, 2], 'b': [3, 6]})

    def func(row):
        print(row)
        return row

*Previous behavior*:

.. code-block:: ipython

    In [4]: df.apply(func, axis=1)
    a    1
    b    3
    Name: 0, dtype: int64
    a    1
    b    3
    Name: 0, dtype: int64
    a    2
    b    6
    Name: 1, dtype: int64
    Out[4]:
       a  b
    0  1  3
    1  2  6

*New behavior*:

.. ipython:: python

    df.apply(func, axis=1)

.. _whatsnew_110.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_110.api_breaking.testing.check_freq:

Added ``check_freq`` argument to ``testing.assert_frame_equal`` and ``testing.assert_series_equal``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``check_freq`` argument was added to :func:`testing.assert_frame_equal` and :func:`testing.assert_series_equal` in pandas 1.1.0 and defaults to ``True``. :func:`testing.assert_frame_equal` and :func:`testing.assert_series_equal` now raise ``AssertionError`` if the indexes do not have the same frequency. Before pandas 1.1.0, the index frequency was not checked.


Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some minimum supported versions of dependencies were updated (:issue:`33718`, :issue:`29766`, :issue:`29723`, pytables >= 3.4.3).
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| numpy           | 1.15.4          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| pytz            | 2015.4          |    X     |         |
+-----------------+-----------------+----------+---------+
| python-dateutil | 2.7.3           |    X     |    X    |
+-----------------+-----------------+----------+---------+
| bottleneck      | 1.2.1           |          |         |
+-----------------+-----------------+----------+---------+
| numexpr         | 2.6.2           |          |         |
+-----------------+-----------------+----------+---------+
| pytest (dev)    | 4.0.2           |          |         |
+-----------------+-----------------+----------+---------+

For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
| beautifulsoup4  | 4.6.0           |         |
+-----------------+-----------------+---------+
| fastparquet     | 0.3.2           |         |
+-----------------+-----------------+---------+
| fsspec          | 0.7.4           |         |
+-----------------+-----------------+---------+
| gcsfs           | 0.6.0           |    X    |
+-----------------+-----------------+---------+
| lxml            | 3.8.0           |         |
+-----------------+-----------------+---------+
| matplotlib      | 2.2.2           |         |
+-----------------+-----------------+---------+
| numba           | 0.46.0          |         |
+-----------------+-----------------+---------+
| openpyxl        | 2.5.7           |         |
+-----------------+-----------------+---------+
| pyarrow         | 0.13.0          |         |
+-----------------+-----------------+---------+
| pymysql         | 0.7.1           |         |
+-----------------+-----------------+---------+
| pytables        | 3.4.3           |    X    |
+-----------------+-----------------+---------+
| s3fs            | 0.4.0           |    X    |
+-----------------+-----------------+---------+
| scipy           | 1.2.0           |    X    |
+-----------------+-----------------+---------+
| sqlalchemy      | 1.1.4           |         |
+-----------------+-----------------+---------+
| xarray          | 0.8.2           |         |
+-----------------+-----------------+---------+
| xlrd            | 1.1.0           |         |
+-----------------+-----------------+---------+
| xlsxwriter      | 0.9.8           |         |
+-----------------+-----------------+---------+
| xlwt            | 1.2.0           |         |
+-----------------+-----------------+---------+
| pandas-gbq      | 1.2.0           |    X    |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

Development changes
^^^^^^^^^^^^^^^^^^^

- The minimum version of Cython is now the most recent bug-fix version (0.29.16) (:issue:`33334`).


.. _whatsnew_110.deprecations:

Deprecations
~~~~~~~~~~~~

- Lookups on a :class:`Series` with a single-item list containing a slice (e.g. ``ser[[slice(0, 4)]]``) are deprecated and will raise in a future version.  Either convert the list to a tuple, or pass the slice directly instead (:issue:`31333`)

- :meth:`DataFrame.mean` and :meth:`DataFrame.median` with ``numeric_only=None`` will include ``datetime64`` and ``datetime64tz`` columns in a future version (:issue:`29941`)
- Setting values with ``.loc`` using a positional slice is deprecated and will raise in a future version.  Use ``.loc`` with labels or ``.iloc`` with positions instead (:issue:`31840`)
- :meth:`DataFrame.to_dict` has deprecated accepting short names for ``orient`` and will raise in a future version (:issue:`32515`)
- :meth:`Categorical.to_dense` is deprecated and will be removed in a future version, use ``np.asarray(cat)`` instead (:issue:`32639`)
- The ``fastpath`` keyword in the ``SingleBlockManager`` constructor is deprecated and will be removed in a future version (:issue:`33092`)
- Providing ``suffixes`` as a ``set`` in :func:`pandas.merge` is deprecated. Provide a tuple instead (:issue:`33740`, :issue:`34741`).
- Indexing a :class:`Series` with a multi-dimensional indexer like ``[:, None]`` to return an ``ndarray`` now raises a ``FutureWarning``. Convert to a NumPy array before indexing instead (:issue:`27837`)
- :meth:`Index.is_mixed` is deprecated and will be removed in a future version, check ``index.inferred_type`` directly instead (:issue:`32922`)

- Passing any arguments but the first one to :func:`read_html` as
  positional arguments is deprecated. All other
  arguments should be given as keyword arguments (:issue:`27573`).

- Passing any arguments but ``path_or_buf`` (the first one) to
  :func:`read_json` as positional arguments is deprecated. All
  other arguments should be given as keyword arguments (:issue:`27573`).

- Passing any arguments but the first two to :func:`read_excel` as
  positional arguments is deprecated. All other
  arguments should be given as keyword arguments (:issue:`27573`).

- :func:`pandas.api.types.is_categorical` is deprecated and will be removed in a future version; use :func:`pandas.api.types.is_categorical_dtype` instead (:issue:`33385`)
- :meth:`Index.get_value` is deprecated and will be removed in a future version (:issue:`19728`)
- :meth:`Series.dt.week` and :meth:`Series.dt.weekofyear` are deprecated and will be removed in a future version, use :meth:`Series.dt.isocalendar().week` instead (:issue:`33595`)
- :meth:`DatetimeIndex.week` and ``DatetimeIndex.weekofyear`` are deprecated and will be removed in a future version, use ``DatetimeIndex.isocalendar().week`` instead (:issue:`33595`)
- :meth:`DatetimeArray.week` and ``DatetimeArray.weekofyear`` are deprecated and will be removed in a future version, use ``DatetimeArray.isocalendar().week`` instead (:issue:`33595`)
- :meth:`DateOffset.__call__` is deprecated and will be removed in a future version, use ``offset + other`` instead (:issue:`34171`)
- :meth:`~pandas.tseries.offsets.BusinessDay.apply_index` is deprecated and will be removed in a future version. Use ``offset + other`` instead (:issue:`34580`)
- :meth:`DataFrame.tshift` and :meth:`Series.tshift` are deprecated and will be removed in a future version, use :meth:`DataFrame.shift` and :meth:`Series.shift` instead (:issue:`11631`)
- Indexing an :class:`Index` object with a float key is deprecated, and will
  raise an ``IndexError`` in the future. You can manually convert to an integer key
  instead (:issue:`34191`).
- The ``squeeze`` keyword in :meth:`~DataFrame.groupby` is deprecated and will be removed in a future version (:issue:`32380`)
- The ``tz`` keyword in :meth:`Period.to_timestamp` is deprecated and will be removed in a future version; use ``per.to_timestamp(...).tz_localize(tz)`` instead (:issue:`34522`)
- :meth:`DatetimeIndex.to_perioddelta` is deprecated and will be removed in a future version.  Use ``index - index.to_period(freq).to_timestamp()`` instead (:issue:`34853`)
- :meth:`DataFrame.melt` accepting a ``value_name`` that already exists is deprecated, and will be removed in a future version (:issue:`34731`)
- The ``center`` keyword in the :meth:`DataFrame.expanding` function is deprecated and will be removed in a future version (:issue:`20647`)



.. ---------------------------------------------------------------------------


.. _whatsnew_110.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Performance improvement in :class:`Timedelta` constructor (:issue:`30543`)
- Performance improvement in :class:`Timestamp` constructor (:issue:`30543`)
- Performance improvement in flex arithmetic ops between :class:`DataFrame` and :class:`Series` with ``axis=0`` (:issue:`31296`)
- Performance improvement in arithmetic ops between :class:`DataFrame` and :class:`Series` with ``axis=1`` (:issue:`33600`)
- The internal index method :meth:`~Index._shallow_copy` now copies cached attributes over to the new index,
  avoiding creating these again on the new index. This can speed up many operations that depend on creating copies of
  existing indexes (:issue:`28584`, :issue:`32640`, :issue:`32669`)
- Significant performance improvement when creating a :class:`DataFrame` with
  sparse values from ``scipy.sparse`` matrices using the
  :meth:`DataFrame.sparse.from_spmatrix` constructor (:issue:`32821`,
  :issue:`32825`,  :issue:`32826`, :issue:`32856`, :issue:`32858`).
- Performance improvement for groupby methods :meth:`~pandas.core.groupby.groupby.Groupby.first`
  and :meth:`~pandas.core.groupby.groupby.Groupby.last` (:issue:`34178`)
- Performance improvement in :func:`factorize` for nullable (integer and Boolean) dtypes (:issue:`33064`).
- Performance improvement when constructing :class:`Categorical` objects (:issue:`33921`)
- Fixed performance regression in :func:`pandas.qcut` and :func:`pandas.cut` (:issue:`33921`)
- Performance improvement in reductions (``sum``, ``prod``, ``min``, ``max``) for nullable (integer and Boolean) dtypes (:issue:`30982`, :issue:`33261`, :issue:`33442`).
- Performance improvement in arithmetic operations between two :class:`DataFrame` objects (:issue:`32779`)
- Performance improvement in :class:`pandas.core.groupby.RollingGroupby` (:issue:`34052`)
- Performance improvement in arithmetic operations (``sub``, ``add``, ``mul``, ``div``) for :class:`MultiIndex` (:issue:`34297`)
- Performance improvement in ``DataFrame[bool_indexer]`` when ``bool_indexer`` is a ``list`` (:issue:`33924`)
- Significant performance improvement of :meth:`io.formats.style.Styler.render` with styles added with various ways such as :meth:`io.formats.style.Styler.apply`, :meth:`io.formats.style.Styler.applymap` or :meth:`io.formats.style.Styler.bar` (:issue:`19917`)

.. ---------------------------------------------------------------------------

.. _whatsnew_110.bug_fixes:

Bug fixes
~~~~~~~~~


Categorical
^^^^^^^^^^^

- Passing an invalid ``fill_value`` to :meth:`Categorical.take` raises a ``ValueError`` instead of ``TypeError`` (:issue:`33660`)
- Combining a :class:`Categorical` with integer categories and which contains missing values with a float dtype column in operations such as :func:`concat` or :meth:`~DataFrame.append` will now result in a float column instead of an object dtype column (:issue:`33607`)
- Bug where :func:`merge` was unable to join on non-unique categorical indices (:issue:`28189`)
- Bug when passing categorical data to :class:`Index` constructor along with ``dtype=object`` incorrectly returning a :class:`CategoricalIndex` instead of object-dtype :class:`Index` (:issue:`32167`)
- Bug where :class:`Categorical` comparison operator ``__ne__`` would incorrectly evaluate to ``False`` when either element was missing (:issue:`32276`)
- :meth:`Categorical.fillna` now accepts :class:`Categorical` ``other`` argument (:issue:`32420`)
- Repr of :class:`Categorical` was not distinguishing between ``int`` and ``str`` (:issue:`33676`)

Datetimelike
^^^^^^^^^^^^

- Passing an integer dtype other than ``int64`` to ``np.array(period_index, dtype=...)`` will now raise ``TypeError`` instead of incorrectly using ``int64`` (:issue:`32255`)
- :meth:`Series.to_timestamp` now raises a ``TypeError`` if the axis is not a :class:`PeriodIndex`. Previously an ``AttributeError`` was raised (:issue:`33327`)
- :meth:`Series.to_period` now raises a ``TypeError`` if the axis is not a :class:`DatetimeIndex`. Previously an ``AttributeError`` was raised (:issue:`33327`)
- :class:`Period` no longer accepts tuples for the ``freq`` argument (:issue:`34658`)
- Bug in :class:`Timestamp` where constructing a :class:`Timestamp` from ambiguous epoch time and calling constructor again changed the :meth:`Timestamp.value` property (:issue:`24329`)
- :meth:`DatetimeArray.searchsorted`, :meth:`TimedeltaArray.searchsorted`, :meth:`PeriodArray.searchsorted` not recognizing non-pandas scalars and incorrectly raising ``ValueError`` instead of ``TypeError`` (:issue:`30950`)
- Bug in :class:`Timestamp` where constructing :class:`Timestamp` with dateutil timezone less than 128 nanoseconds before daylight saving time switch from winter to summer would result in nonexistent time (:issue:`31043`)
- Bug in :meth:`Period.to_timestamp`, :meth:`Period.start_time` with microsecond frequency returning a timestamp one nanosecond earlier than the correct time (:issue:`31475`)
- :class:`Timestamp` raised a confusing error message when year, month or day is missing (:issue:`31200`)
- Bug in :class:`DatetimeIndex` constructor incorrectly accepting ``bool``-dtype inputs (:issue:`32668`)
- Bug in :meth:`DatetimeIndex.searchsorted` not accepting a ``list`` or :class:`Series` as its argument (:issue:`32762`)
- Bug where :meth:`PeriodIndex` raised when passed a :class:`Series` of strings (:issue:`26109`)
- Bug in :class:`Timestamp` arithmetic when adding or subtracting an ``np.ndarray`` with ``timedelta64`` dtype (:issue:`33296`)
- Bug in :meth:`DatetimeIndex.to_period` not inferring the frequency when called with no arguments (:issue:`33358`)
- Bug in :meth:`DatetimeIndex.tz_localize` incorrectly retaining ``freq`` in some cases where the original ``freq`` is no longer valid (:issue:`30511`)
- Bug in :meth:`DatetimeIndex.intersection` losing ``freq`` and timezone in some cases (:issue:`33604`)
- Bug in :meth:`DatetimeIndex.get_indexer` where incorrect output would be returned for mixed datetime-like targets (:issue:`33741`)
- Bug in :class:`DatetimeIndex` addition and subtraction with some types of :class:`DateOffset` objects incorrectly retaining an invalid ``freq`` attribute (:issue:`33779`)
- Bug in :class:`DatetimeIndex` where setting the ``freq`` attribute on an index could silently change the ``freq`` attribute on another index viewing the same data (:issue:`33552`)
- :meth:`DataFrame.min` and :meth:`DataFrame.max` were not returning consistent results with :meth:`Series.min` and :meth:`Series.max` when called on objects initialized with empty :func:`pd.to_datetime`
- Bug in :meth:`DatetimeIndex.intersection` and :meth:`TimedeltaIndex.intersection` with results not having the correct ``name`` attribute (:issue:`33904`)
- Bug in :meth:`DatetimeArray.__setitem__`, :meth:`TimedeltaArray.__setitem__`, :meth:`PeriodArray.__setitem__` incorrectly allowing values with ``int64`` dtype to be silently cast (:issue:`33717`)
- Bug in subtracting :class:`TimedeltaIndex` from :class:`Period` incorrectly raising ``TypeError`` in some cases where it should succeed and ``IncompatibleFrequency`` in some cases where it should raise ``TypeError`` (:issue:`33883`)
- Bug in constructing a :class:`Series` or :class:`Index` from a read-only NumPy array with non-ns
  resolution which converted to object dtype instead of coercing to ``datetime64[ns]``
  dtype when within the timestamp bounds (:issue:`34843`).
- The ``freq`` keyword in :class:`Period`, :func:`date_range`, :func:`period_range`, :func:`pd.tseries.frequencies.to_offset` no longer allows tuples, pass as string instead (:issue:`34703`)
- Bug in :meth:`DataFrame.append` when appending a :class:`Series` containing a scalar tz-aware :class:`Timestamp` to an empty :class:`DataFrame` resulted in an object column instead of ``datetime64[ns, tz]`` dtype (:issue:`35038`)
- ``OutOfBoundsDatetime`` issues an improved error message when timestamp is out of implementation bounds. (:issue:`32967`)
- Bug in :meth:`AbstractHolidayCalendar.holidays` when no rules were defined (:issue:`31415`)
- Bug in :class:`Tick` comparisons raising ``TypeError`` when comparing against timedelta-like objects (:issue:`34088`)
- Bug in :class:`Tick` multiplication raising ``TypeError`` when multiplying by a float (:issue:`34486`)

Timedelta
^^^^^^^^^

- Bug in constructing a :class:`Timedelta` with a high precision integer that would round the :class:`Timedelta` components (:issue:`31354`)
- Bug in dividing ``np.nan`` or ``None`` by :class:`Timedelta` incorrectly returning ``NaT`` (:issue:`31869`)
- :class:`Timedelta` now understands ``µs`` as an identifier for microsecond (:issue:`32899`)
- :class:`Timedelta` string representation now includes nanoseconds, when nanoseconds are non-zero (:issue:`9309`)
- Bug in comparing a :class:`Timedelta` object against an ``np.ndarray`` with ``timedelta64`` dtype incorrectly viewing all entries as unequal (:issue:`33441`)
- Bug in :func:`timedelta_range` that produced an extra point on a edge case (:issue:`30353`, :issue:`33498`)
- Bug in :meth:`DataFrame.resample` that produced an extra point on a edge case (:issue:`30353`, :issue:`13022`, :issue:`33498`)
- Bug in :meth:`DataFrame.resample` that ignored the ``loffset`` argument when dealing with timedelta (:issue:`7687`, :issue:`33498`)
- Bug in :class:`Timedelta` and :func:`pandas.to_timedelta` that ignored the ``unit`` argument for string input (:issue:`12136`)

Timezones
^^^^^^^^^

- Bug in :func:`to_datetime` with ``infer_datetime_format=True`` where timezone names (e.g. ``UTC``) would not be parsed correctly (:issue:`33133`)


Numeric
^^^^^^^
- Bug in :meth:`DataFrame.floordiv` with ``axis=0`` not treating division-by-zero like :meth:`Series.floordiv` (:issue:`31271`)
- Bug in :func:`to_numeric` with string argument ``"uint64"`` and ``errors="coerce"`` silently fails (:issue:`32394`)
- Bug in :func:`to_numeric` with ``downcast="unsigned"`` fails for empty data (:issue:`32493`)
- Bug in :meth:`DataFrame.mean` with ``numeric_only=False`` and either ``datetime64`` dtype or ``PeriodDtype`` column incorrectly raising ``TypeError`` (:issue:`32426`)
- Bug in :meth:`DataFrame.count` with ``level="foo"`` and index level ``"foo"`` containing NaNs causes segmentation fault (:issue:`21824`)
- Bug in :meth:`DataFrame.diff` with ``axis=1`` returning incorrect results with mixed dtypes (:issue:`32995`)
- Bug in :meth:`DataFrame.corr` and :meth:`DataFrame.cov` raising when handling nullable integer columns with ``pandas.NA`` (:issue:`33803`)
- Bug in arithmetic operations between :class:`DataFrame` objects with non-overlapping columns with duplicate labels causing an infinite loop (:issue:`35194`)
- Bug in :class:`DataFrame` and :class:`Series` addition and subtraction between object-dtype objects and ``datetime64`` dtype objects (:issue:`33824`)
- Bug in :meth:`Index.difference` giving incorrect results when comparing a :class:`Float64Index` and object :class:`Index` (:issue:`35217`)
- Bug in :class:`DataFrame` reductions (e.g. ``df.min()``, ``df.max()``) with ``ExtensionArray`` dtypes (:issue:`34520`, :issue:`32651`)
- :meth:`Series.interpolate` and :meth:`DataFrame.interpolate` now raise a ValueError if ``limit_direction`` is ``'forward'`` or ``'both'`` and ``method`` is ``'backfill'`` or ``'bfill'`` or ``limit_direction`` is ``'backward'`` or ``'both'`` and ``method`` is ``'pad'`` or ``'ffill'`` (:issue:`34746`)

Conversion
^^^^^^^^^^
- Bug in :class:`Series` construction from NumPy array with big-endian ``datetime64`` dtype (:issue:`29684`)
- Bug in :class:`Timedelta` construction with large nanoseconds keyword value (:issue:`32402`)
- Bug in :class:`DataFrame` construction where sets would be duplicated rather than raising (:issue:`32582`)
- The :class:`DataFrame` constructor no longer accepts a list of :class:`DataFrame` objects. Because of changes to NumPy, :class:`DataFrame` objects are now consistently treated as 2D objects, so a list of :class:`DataFrame` objects is considered 3D, and no longer acceptable for the :class:`DataFrame` constructor (:issue:`32289`).
- Bug in :class:`DataFrame` when initiating a frame with lists and assign ``columns`` with nested list for ``MultiIndex`` (:issue:`32173`)
- Improved error message for invalid construction of list when creating a new index (:issue:`35190`)


Strings
^^^^^^^

- Bug in the :meth:`~Series.astype` method when converting "string" dtype data to nullable integer dtype (:issue:`32450`).
- Fixed issue where taking ``min`` or ``max`` of a ``StringArray`` or ``Series`` with ``StringDtype`` type would raise. (:issue:`31746`)
- Bug in :meth:`Series.str.cat` returning ``NaN`` output when other had :class:`Index` type (:issue:`33425`)
- :func:`pandas.api.dtypes.is_string_dtype` no longer incorrectly identifies categorical series as string.

Interval
^^^^^^^^
- Bug in :class:`IntervalArray` incorrectly allowing the underlying data to be changed when setting values (:issue:`32782`)

Indexing
^^^^^^^^

- :meth:`DataFrame.xs` now raises a  ``TypeError`` if a ``level`` keyword is supplied and the axis is not a :class:`MultiIndex`. Previously an ``AttributeError`` was raised (:issue:`33610`)
- Bug in slicing on a :class:`DatetimeIndex` with a partial-timestamp dropping high-resolution indices near the end of a year, quarter, or month (:issue:`31064`)
- Bug in :meth:`PeriodIndex.get_loc` treating higher-resolution strings differently from :meth:`PeriodIndex.get_value` (:issue:`31172`)
- Bug in :meth:`Series.at` and :meth:`DataFrame.at` not matching ``.loc`` behavior when looking up an integer in a :class:`Float64Index` (:issue:`31329`)
- Bug in :meth:`PeriodIndex.is_monotonic` incorrectly returning ``True`` when containing leading ``NaT`` entries (:issue:`31437`)
- Bug in :meth:`DatetimeIndex.get_loc` raising ``KeyError`` with converted-integer key instead of the user-passed key (:issue:`31425`)
- Bug in :meth:`Series.xs` incorrectly returning ``Timestamp`` instead of ``datetime64`` in some object-dtype cases (:issue:`31630`)
- Bug in :meth:`DataFrame.iat` incorrectly returning ``Timestamp`` instead of ``datetime`` in some object-dtype cases (:issue:`32809`)
- Bug in :meth:`DataFrame.at` when either columns or index is non-unique (:issue:`33041`)
- Bug in :meth:`Series.loc` and :meth:`DataFrame.loc` when indexing with an integer key on a object-dtype :class:`Index` that is not all-integers (:issue:`31905`)
- Bug in :meth:`DataFrame.iloc.__setitem__` on a :class:`DataFrame` with duplicate columns incorrectly setting values for all matching columns (:issue:`15686`, :issue:`22036`)
- Bug in :meth:`DataFrame.loc` and :meth:`Series.loc` with a :class:`DatetimeIndex`, :class:`TimedeltaIndex`, or :class:`PeriodIndex` incorrectly allowing lookups of non-matching datetime-like dtypes (:issue:`32650`)
- Bug in :meth:`Series.__getitem__` indexing with non-standard scalars, e.g. ``np.dtype`` (:issue:`32684`)
- Bug in :class:`Index` constructor where an unhelpful error message was raised for NumPy scalars (:issue:`33017`)
- Bug in :meth:`DataFrame.lookup` incorrectly raising an ``AttributeError`` when ``frame.index`` or ``frame.columns`` is not unique; this will now raise a ``ValueError`` with a helpful error message (:issue:`33041`)
- Bug in :class:`Interval` where a :class:`Timedelta` could not be added or subtracted from a :class:`Timestamp` interval (:issue:`32023`)
- Bug in :meth:`DataFrame.copy` not invalidating _item_cache after copy caused post-copy value updates to not be reflected (:issue:`31784`)
- Fixed regression in :meth:`DataFrame.loc` and :meth:`Series.loc` throwing an error when a ``datetime64[ns, tz]`` value is provided (:issue:`32395`)
- Bug in :meth:`Series.__getitem__` with an integer key and a :class:`MultiIndex` with leading integer level failing to raise ``KeyError`` if the key is not present in the first level (:issue:`33355`)
- Bug in :meth:`DataFrame.iloc` when slicing a single column :class:`DataFrame` with ``ExtensionDtype`` (e.g. ``df.iloc[:, :1]``) returning an invalid result (:issue:`32957`)
- Bug in :meth:`DatetimeIndex.insert` and :meth:`TimedeltaIndex.insert` causing index ``freq`` to be lost when setting an element into an empty :class:`Series` (:issue:`33573`)
- Bug in :meth:`Series.__setitem__` with an :class:`IntervalIndex` and a list-like key of integers (:issue:`33473`)
- Bug in :meth:`Series.__getitem__` allowing missing labels with ``np.ndarray``, :class:`Index`, :class:`Series` indexers but not ``list``, these now all raise ``KeyError`` (:issue:`33646`)
- Bug in :meth:`DataFrame.truncate` and :meth:`Series.truncate` where index was assumed to be monotone increasing (:issue:`33756`)
- Indexing with a list of strings representing datetimes failed on :class:`DatetimeIndex` or :class:`PeriodIndex` (:issue:`11278`)
- Bug in :meth:`Series.at` when used with a :class:`MultiIndex` would raise an exception on valid inputs (:issue:`26989`)
- Bug in :meth:`DataFrame.loc` with dictionary of values changes columns with dtype of ``int`` to ``float`` (:issue:`34573`)
- Bug in :meth:`Series.loc` when used with a :class:`MultiIndex` would raise an ``IndexingError`` when accessing a ``None`` value (:issue:`34318`)
- Bug in :meth:`DataFrame.reset_index` and :meth:`Series.reset_index` would not preserve data types on an empty :class:`DataFrame` or :class:`Series` with a :class:`MultiIndex` (:issue:`19602`)
- Bug in :class:`Series` and :class:`DataFrame` indexing with a ``time`` key on a :class:`DatetimeIndex` with ``NaT`` entries (:issue:`35114`)

Missing
^^^^^^^
- Calling :meth:`fillna` on an empty :class:`Series` now correctly returns a shallow copied object. The behaviour is now consistent with :class:`Index`, :class:`DataFrame` and a non-empty :class:`Series` (:issue:`32543`).
- Bug in :meth:`Series.replace` when argument ``to_replace`` is of type dict/list and is used on a :class:`Series` containing ``<NA>`` was raising a ``TypeError``. The method now handles this by ignoring ``<NA>`` values when doing the comparison for the replacement (:issue:`32621`)
- Bug in :meth:`~Series.any` and :meth:`~Series.all` incorrectly returning ``<NA>`` for all ``False`` or all ``True`` values using the nulllable Boolean dtype and with ``skipna=False`` (:issue:`33253`)
- Clarified documentation on interpolate with ``method=akima``. The ``der`` parameter must be scalar or ``None`` (:issue:`33426`)
- :meth:`DataFrame.interpolate` uses the correct axis convention now. Previously interpolating along columns lead to interpolation along indices and vice versa. Furthermore interpolating with methods ``pad``, ``ffill``, ``bfill`` and ``backfill`` are identical to using these methods with :meth:`DataFrame.fillna` (:issue:`12918`, :issue:`29146`)
- Bug in :meth:`DataFrame.interpolate` when called on a :class:`DataFrame` with column names of string type was throwing a ValueError. The method is now independent of the type of the column names (:issue:`33956`)
- Passing :class:`NA` into a format string using format specs will now work. For example ``"{:.1f}".format(pd.NA)`` would previously raise a ``ValueError``, but will now return the string ``"<NA>"`` (:issue:`34740`)
- Bug in :meth:`Series.map` not raising on invalid ``na_action`` (:issue:`32815`)

MultiIndex
^^^^^^^^^^

- :meth:`DataFrame.swaplevels` now raises a ``TypeError`` if the axis is not a :class:`MultiIndex`. Previously an ``AttributeError`` was raised (:issue:`31126`)
- Bug in :meth:`Dataframe.loc` when used with a :class:`MultiIndex`. The returned values were not in the same order as the given inputs (:issue:`22797`)

.. ipython:: python

        df = pd.DataFrame(np.arange(4),
                          index=[["a", "a", "b", "b"], [1, 2, 1, 2]])
        # Rows are now ordered as the requested keys
        df.loc[(['b', 'a'], [2, 1]), :]

- Bug in :meth:`MultiIndex.intersection` was not guaranteed to preserve order when ``sort=False``. (:issue:`31325`)
- Bug in :meth:`DataFrame.truncate` was dropping :class:`MultiIndex` names. (:issue:`34564`)

.. ipython:: python

        left = pd.MultiIndex.from_arrays([["b", "a"], [2, 1]])
        right = pd.MultiIndex.from_arrays([["a", "b", "c"], [1, 2, 3]])
        # Common elements are now guaranteed to be ordered by the left side
        left.intersection(right, sort=False)

- Bug when joining two :class:`MultiIndex` without specifying level with different columns. Return-indexers parameter was ignored. (:issue:`34074`)

IO
^^
- Passing a ``set`` as ``names`` argument to :func:`pandas.read_csv`, :func:`pandas.read_table`, or :func:`pandas.read_fwf` will raise ``ValueError: Names should be an ordered collection.`` (:issue:`34946`)
- Bug in print-out when ``display.precision`` is zero. (:issue:`20359`)
- Bug in :func:`read_json` where integer overflow was occurring when json contains big number strings. (:issue:`30320`)
- :func:`read_csv` will now raise a ``ValueError`` when the arguments ``header`` and ``prefix`` both are not ``None``. (:issue:`27394`)
- Bug in :meth:`DataFrame.to_json` was raising ``NotFoundError`` when ``path_or_buf`` was an S3 URI (:issue:`28375`)
- Bug in :meth:`DataFrame.to_parquet` overwriting pyarrow's default for
  ``coerce_timestamps``; following pyarrow's default allows writing nanosecond
  timestamps with ``version="2.0"`` (:issue:`31652`).
- Bug in :func:`read_csv` was raising ``TypeError`` when ``sep=None`` was used in combination with ``comment`` keyword (:issue:`31396`)
- Bug in :class:`HDFStore` that caused it to set to ``int64`` the dtype of a ``datetime64`` column when reading a :class:`DataFrame` in Python 3 from fixed format written in Python 2 (:issue:`31750`)
- :func:`read_sas()` now handles dates and datetimes larger than :attr:`Timestamp.max` returning them as :class:`datetime.datetime` objects (:issue:`20927`)
- Bug in :meth:`DataFrame.to_json` where ``Timedelta`` objects would not be serialized correctly with ``date_format="iso"`` (:issue:`28256`)
- :func:`read_csv` will raise a ``ValueError`` when the column names passed in ``parse_dates`` are missing in the :class:`Dataframe` (:issue:`31251`)
- Bug in :func:`read_excel` where a UTF-8 string with a high surrogate would cause a segmentation violation (:issue:`23809`)
- Bug in :func:`read_csv` was causing a file descriptor leak on an empty file (:issue:`31488`)
- Bug in :func:`read_csv` was causing a segfault when there were blank lines between the header and data rows (:issue:`28071`)
- Bug in :func:`read_csv` was raising a misleading exception on a permissions issue (:issue:`23784`)
- Bug in :func:`read_csv` was raising an ``IndexError`` when ``header=None`` and two extra data columns
- Bug in :func:`read_sas` was raising an ``AttributeError`` when reading files from Google Cloud Storage (:issue:`33069`)
- Bug in :meth:`DataFrame.to_sql` where an ``AttributeError`` was raised when saving an out of bounds date (:issue:`26761`)
- Bug in :func:`read_excel` did not correctly handle multiple embedded spaces in OpenDocument text cells. (:issue:`32207`)
- Bug in :func:`read_json` was raising ``TypeError`` when reading a ``list`` of Booleans into a :class:`Series`. (:issue:`31464`)
- Bug in :func:`pandas.io.json.json_normalize` where location specified by ``record_path`` doesn't point to an array. (:issue:`26284`)
- :func:`pandas.read_hdf` has a more explicit error message when loading an
  unsupported HDF file (:issue:`9539`)
- Bug in :meth:`~DataFrame.read_feather` was raising an ``ArrowIOError`` when reading an s3 or http file path (:issue:`29055`)
- Bug in :meth:`~DataFrame.to_excel` could not handle the column name ``render`` and was raising an ``KeyError`` (:issue:`34331`)
- Bug in :meth:`~SQLDatabase.execute` was raising a ``ProgrammingError`` for some DB-API drivers when the SQL statement contained the ``%`` character and no parameters were present (:issue:`34211`)
- Bug in :meth:`~pandas.io.stata.StataReader` which resulted in categorical variables with different dtypes when reading data using an iterator. (:issue:`31544`)
- :meth:`HDFStore.keys` has now an optional ``include`` parameter that allows the retrieval of all native HDF5 table names (:issue:`29916`)
- ``TypeError`` exceptions raised by :func:`read_csv` and :func:`read_table` were showing as ``parser_f`` when an unexpected keyword argument was passed (:issue:`25648`)
- Bug in :func:`read_excel` for ODS files removes 0.0 values (:issue:`27222`)
- Bug in :func:`ujson.encode` was raising an ``OverflowError`` with numbers larger than ``sys.maxsize`` (:issue:`34395`)
- Bug in :meth:`HDFStore.append_to_multiple` was raising a ``ValueError`` when the ``min_itemsize`` parameter is set (:issue:`11238`)
- Bug in :meth:`~HDFStore.create_table` now raises an error when ``column`` argument was not specified in ``data_columns`` on input (:issue:`28156`)
- :func:`read_json` now could read line-delimited json file from a file url while ``lines`` and ``chunksize`` are set.
- Bug in :meth:`DataFrame.to_sql` when reading DataFrames with ``-np.inf`` entries with MySQL now has a more explicit ``ValueError`` (:issue:`34431`)
- Bug where capitalised files extensions were not decompressed by read_* functions (:issue:`35164`)
- Bug in :meth:`read_excel` that was raising a ``TypeError`` when ``header=None`` and ``index_col`` is given as a ``list`` (:issue:`31783`)
- Bug in :func:`read_excel` where datetime values are used in the header in a :class:`MultiIndex` (:issue:`34748`)
- :func:`read_excel` no longer takes ``**kwds`` arguments. This means that passing in the keyword argument ``chunksize`` now raises a ``TypeError`` (previously raised a ``NotImplementedError``), while passing in the keyword argument ``encoding`` now raises a ``TypeError`` (:issue:`34464`)
- Bug in :meth:`DataFrame.to_records` was incorrectly losing timezone information in timezone-aware ``datetime64`` columns (:issue:`32535`)

Plotting
^^^^^^^^

- :meth:`DataFrame.plot` for line/bar now accepts color by dictionary (:issue:`8193`).
- Bug in :meth:`DataFrame.plot.hist` where weights are not working for multiple columns (:issue:`33173`)
- Bug in :meth:`DataFrame.boxplot` and :meth:`DataFrame.plot.boxplot` lost color attributes of ``medianprops``, ``whiskerprops``, ``capprops`` and ``boxprops`` (:issue:`30346`)
- Bug in :meth:`DataFrame.hist` where the order of ``column`` argument was ignored (:issue:`29235`)
- Bug in :meth:`DataFrame.plot.scatter` that when adding multiple plots with different ``cmap``, colorbars always use the first ``cmap`` (:issue:`33389`)
- Bug in :meth:`DataFrame.plot.scatter` was adding a colorbar to the plot even if the argument ``c`` was assigned to a column containing color names (:issue:`34316`)
- Bug in :meth:`pandas.plotting.bootstrap_plot` was causing cluttered axes and overlapping labels (:issue:`34905`)
- Bug in :meth:`DataFrame.plot.scatter` caused an error when plotting variable marker sizes (:issue:`32904`)

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Using a :class:`pandas.api.indexers.BaseIndexer` with ``count``, ``min``, ``max``, ``median``, ``skew``,  ``cov``, ``corr`` will now return correct results for any monotonic :class:`pandas.api.indexers.BaseIndexer` descendant (:issue:`32865`)
- :meth:`DataFrameGroupby.mean` and :meth:`SeriesGroupby.mean` (and similarly for :meth:`~DataFrameGroupby.median`, :meth:`~DataFrameGroupby.std` and :meth:`~DataFrameGroupby.var`) now raise a ``TypeError`` if a non-accepted keyword argument is passed into it. Previously an ``UnsupportedFunctionCall`` was raised (``AssertionError`` if ``min_count`` passed into :meth:`~DataFrameGroupby.median`) (:issue:`31485`)
- Bug in :meth:`GroupBy.apply` raises ``ValueError`` when the ``by`` axis is not sorted, has duplicates, and the applied ``func`` does not mutate passed in objects (:issue:`30667`)
- Bug in :meth:`DataFrameGroupBy.transform` produces an incorrect result with transformation functions (:issue:`30918`)
- Bug in :meth:`Groupby.transform` was returning the wrong result when grouping by multiple keys of which some were categorical and others not (:issue:`32494`)
- Bug in :meth:`GroupBy.count` causes segmentation fault when grouped-by columns contain NaNs (:issue:`32841`)
- Bug in :meth:`DataFrame.groupby` and :meth:`Series.groupby` produces inconsistent type when aggregating Boolean :class:`Series` (:issue:`32894`)
- Bug in :meth:`DataFrameGroupBy.sum` and :meth:`SeriesGroupBy.sum` where a large negative number would be returned when the number of non-null values was below ``min_count`` for nullable integer dtypes (:issue:`32861`)
- Bug in :meth:`SeriesGroupBy.quantile` was raising on nullable integers (:issue:`33136`)
- Bug in :meth:`DataFrame.resample` where an ``AmbiguousTimeError`` would be raised when the resulting timezone aware :class:`DatetimeIndex` had a DST transition at midnight (:issue:`25758`)
- Bug in :meth:`DataFrame.groupby` where a ``ValueError`` would be raised when grouping by a categorical column with read-only categories and ``sort=False`` (:issue:`33410`)
- Bug in :meth:`GroupBy.agg`, :meth:`GroupBy.transform`, and :meth:`GroupBy.resample` where subclasses are not preserved (:issue:`28330`)
- Bug in :meth:`SeriesGroupBy.agg` where any column name was accepted in the named aggregation of :class:`SeriesGroupBy` previously. The behaviour now allows only ``str`` and callables else would raise ``TypeError``. (:issue:`34422`)
- Bug in :meth:`DataFrame.groupby` lost the name of the :class:`Index` when one of the ``agg`` keys referenced an empty list (:issue:`32580`)
- Bug in :meth:`Rolling.apply` where ``center=True`` was ignored when ``engine='numba'`` was specified (:issue:`34784`)
- Bug in :meth:`DataFrame.ewm.cov` was throwing ``AssertionError`` for :class:`MultiIndex` inputs (:issue:`34440`)
- Bug in :meth:`core.groupby.DataFrameGroupBy.quantile` raised ``TypeError`` for non-numeric types rather than dropping the columns (:issue:`27892`)
- Bug in :meth:`core.groupby.DataFrameGroupBy.transform` when ``func='nunique'`` and columns are of type ``datetime64``, the result would also be of type ``datetime64`` instead of ``int64`` (:issue:`35109`)
- Bug in :meth:`DataFrame.groupby` raising an ``AttributeError`` when selecting a column and aggregating with ``as_index=False`` (:issue:`35246`).
- Bug in :meth:`DataFrameGroupBy.first` and :meth:`DataFrameGroupBy.last` that would raise an unnecessary ``ValueError`` when grouping on multiple ``Categoricals`` (:issue:`34951`)

Reshaping
^^^^^^^^^

- Bug effecting all numeric and Boolean reduction methods not returning subclassed data type. (:issue:`25596`)
- Bug in :meth:`DataFrame.pivot_table` when only :class:`MultiIndexed` columns is set (:issue:`17038`)
- Bug in :meth:`DataFrame.unstack` and :meth:`Series.unstack` can take tuple names in :class:`MultiIndexed` data (:issue:`19966`)
- Bug in :meth:`DataFrame.pivot_table` when ``margin`` is ``True`` and only ``column`` is defined (:issue:`31016`)
- Fixed incorrect error message in :meth:`DataFrame.pivot` when ``columns`` is set to ``None``. (:issue:`30924`)
- Bug in :func:`crosstab` when inputs are two :class:`Series` and have tuple names, the output will keep a dummy :class:`MultiIndex` as columns. (:issue:`18321`)
- :meth:`DataFrame.pivot` can now take lists for ``index`` and ``columns`` arguments (:issue:`21425`)
- Bug in :func:`concat` where the resulting indices are not copied when ``copy=True`` (:issue:`29879`)
- Bug in :meth:`SeriesGroupBy.aggregate` was resulting in aggregations being overwritten when they shared the same name (:issue:`30880`)
- Bug where :meth:`Index.astype` would lose the :attr:`name` attribute when converting from ``Float64Index`` to ``Int64Index``, or when casting to an ``ExtensionArray`` dtype (:issue:`32013`)
- :meth:`Series.append` will now raise a ``TypeError`` when passed a :class:`DataFrame` or a sequence containing :class:`DataFrame` (:issue:`31413`)
- :meth:`DataFrame.replace` and :meth:`Series.replace` will raise a ``TypeError`` if ``to_replace`` is not an expected type. Previously the ``replace`` would fail silently (:issue:`18634`)
- Bug on inplace operation of a :class:`Series` that was adding a column to the :class:`DataFrame` from where it was originally dropped from (using ``inplace=True``) (:issue:`30484`)
- Bug in :meth:`DataFrame.apply` where callback was called with :class:`Series` parameter even though ``raw=True`` requested. (:issue:`32423`)
- Bug in :meth:`DataFrame.pivot_table` losing timezone information when creating a :class:`MultiIndex` level from a column with timezone-aware dtype (:issue:`32558`)
- Bug in :func:`concat` where when passing a non-dict mapping as ``objs`` would raise a ``TypeError`` (:issue:`32863`)
- :meth:`DataFrame.agg` now provides more descriptive ``SpecificationError`` message when attempting to aggregate a non-existent column (:issue:`32755`)
- Bug in :meth:`DataFrame.unstack` when :class:`MultiIndex` columns and :class:`MultiIndex` rows were used (:issue:`32624`, :issue:`24729` and :issue:`28306`)
- Appending a dictionary to a :class:`DataFrame` without passing ``ignore_index=True`` will raise ``TypeError: Can only append a dict if ignore_index=True`` instead of ``TypeError: Can only append a :class:`Series` if ignore_index=True or if the :class:`Series` has a name`` (:issue:`30871`)
- Bug in :meth:`DataFrame.corrwith()`, :meth:`DataFrame.memory_usage()`, :meth:`DataFrame.dot()`,
  :meth:`DataFrame.idxmin()`, :meth:`DataFrame.idxmax()`, :meth:`DataFrame.duplicated()`, :meth:`DataFrame.isin()`,
  :meth:`DataFrame.count()`, :meth:`Series.explode()`, :meth:`Series.asof()` and :meth:`DataFrame.asof()` not
  returning subclassed types. (:issue:`31331`)
- Bug in :func:`concat` was not allowing for concatenation of :class:`DataFrame` and :class:`Series` with duplicate keys (:issue:`33654`)
- Bug in :func:`cut` raised an error when the argument ``labels`` contains duplicates (:issue:`33141`)
- Ensure only named functions can be used in :func:`eval()` (:issue:`32460`)
- Bug in :meth:`Dataframe.aggregate` and :meth:`Series.aggregate` was causing a recursive loop in some cases (:issue:`34224`)
- Fixed bug in :func:`melt` where melting :class:`MultiIndex` columns with ``col_level > 0`` would raise a ``KeyError`` on ``id_vars`` (:issue:`34129`)
- Bug in :meth:`Series.where` with an empty :class:`Series` and empty ``cond`` having non-bool dtype (:issue:`34592`)
- Fixed regression where :meth:`DataFrame.apply` would raise ``ValueError`` for elements with ``S`` dtype (:issue:`34529`)

Sparse
^^^^^^
- Creating a :class:`SparseArray` from timezone-aware dtype will issue a warning before dropping timezone information, instead of doing so silently (:issue:`32501`)
- Bug in :meth:`arrays.SparseArray.from_spmatrix` wrongly read scipy sparse matrix (:issue:`31991`)
- Bug in :meth:`Series.sum` with ``SparseArray`` raised a ``TypeError`` (:issue:`25777`)
- Bug where :class:`DataFrame` containing an all-sparse :class:`SparseArray` filled with ``NaN`` when indexed by a list-like (:issue:`27781`, :issue:`29563`)
- The repr of :class:`SparseDtype` now includes the repr of its ``fill_value`` attribute. Previously it used ``fill_value``'s  string representation (:issue:`34352`)
- Bug where empty :class:`DataFrame` could not be cast to :class:`SparseDtype` (:issue:`33113`)
- Bug in :meth:`arrays.SparseArray` was returning the incorrect type when indexing a sparse dataframe with an iterable (:issue:`34526`, :issue:`34540`)

ExtensionArray
^^^^^^^^^^^^^^

- Fixed bug where :meth:`Series.value_counts` would raise on empty input of ``Int64`` dtype (:issue:`33317`)
- Fixed bug in :func:`concat` when concatenating :class:`DataFrame` objects with non-overlapping columns resulting in object-dtype columns rather than preserving the extension dtype (:issue:`27692`, :issue:`33027`)
- Fixed bug where :meth:`StringArray.isna` would return ``False`` for NA values when ``pandas.options.mode.use_inf_as_na`` was set to ``True`` (:issue:`33655`)
- Fixed bug in :class:`Series` construction with EA dtype and index but no data or scalar data fails (:issue:`26469`)
- Fixed bug that caused :meth:`Series.__repr__()` to crash for extension types whose elements are multidimensional arrays (:issue:`33770`).
- Fixed bug where :meth:`Series.update` would raise a ``ValueError`` for ``ExtensionArray`` dtypes with missing values (:issue:`33980`)
- Fixed bug where :meth:`StringArray.memory_usage` was not implemented (:issue:`33963`)
- Fixed bug where :meth:`DataFrameGroupBy` would ignore the ``min_count`` argument for aggregations on nullable Boolean dtypes (:issue:`34051`)
- Fixed bug where the constructor of :class:`DataFrame` with ``dtype='string'`` would fail (:issue:`27953`, :issue:`33623`)
- Bug where :class:`DataFrame` column set to scalar extension type was considered an object type rather than the extension type (:issue:`34832`)
- Fixed bug in :meth:`IntegerArray.astype` to correctly copy the mask as well (:issue:`34931`).

Other
^^^^^

- Set operations on an object-dtype :class:`Index` now always return object-dtype results (:issue:`31401`)
- Fixed :func:`pandas.testing.assert_series_equal` to correctly raise if the ``left`` argument is a different subclass with ``check_series_type=True`` (:issue:`32670`).
- Getting a missing attribute in a :meth:`DataFrame.query` or :meth:`DataFrame.eval` string raises the correct ``AttributeError`` (:issue:`32408`)
- Fixed bug in :func:`pandas.testing.assert_series_equal` where dtypes were checked for ``Interval`` and ``ExtensionArray`` operands when ``check_dtype`` was ``False`` (:issue:`32747`)
- Bug in :meth:`DataFrame.__dir__` caused a segfault when using unicode surrogates in a column name (:issue:`25509`)
- Bug in :meth:`DataFrame.equals` and :meth:`Series.equals` in allowing subclasses to be equal (:issue:`34402`).

.. ---------------------------------------------------------------------------

.. _whatsnew_110.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.5..v1.1.0|HEAD
.. _whatsnew_0161:

Version 0.16.1 (May 11, 2015)
-----------------------------

{{ header }}


This is a minor bug-fix release from 0.16.0 and includes a large number of
bug fixes along several new features, enhancements, and performance improvements.
We recommend that all users upgrade to this version.

Highlights include:

- Support for a ``CategoricalIndex``, a category based index, see :ref:`here <whatsnew_0161.enhancements.categoricalindex>`
- New section on how-to-contribute to *pandas*, see :ref:`here <contributing>`
- Revised "Merge, join, and concatenate" documentation, including graphical examples to make it easier to understand each operations, see :ref:`here <merging>`
- New method ``sample`` for drawing random samples from Series, DataFrames and Panels. See :ref:`here <whatsnew_0161.enhancements.sample>`
- The default ``Index`` printing has changed to a more uniform format, see :ref:`here <whatsnew_0161.index_repr>`
- ``BusinessHour`` datetime-offset is now supported, see :ref:`here <timeseries.businesshour>`

-  Further enhancement to the ``.str`` accessor to make string operations easier, see :ref:`here <whatsnew_0161.enhancements.string>`

.. contents:: What's new in v0.16.1
    :local:
    :backlinks: none

.. _whatsnew_0161.enhancements:

.. warning::

   In pandas 0.17.0, the sub-package ``pandas.io.data`` will be removed in favor of a separately installable package (:issue:`8961`).

Enhancements
~~~~~~~~~~~~

.. _whatsnew_0161.enhancements.categoricalindex:

CategoricalIndex
^^^^^^^^^^^^^^^^

We introduce a ``CategoricalIndex``, a new type of index object that is useful for supporting
indexing with duplicates. This is a container around a ``Categorical`` (introduced in v0.15.0)
and allows efficient indexing and storage of an index with a large number of duplicated elements. Prior to 0.16.1,
setting the index of a ``DataFrame/Series`` with a ``category`` dtype would convert this to regular object-based ``Index``.

.. code-block:: ipython

    In [1]: df = pd.DataFrame({'A': np.arange(6),
       ...:                    'B': pd.Series(list('aabbca'))
       ...:                           .astype('category', categories=list('cab'))
       ...:                    })
       ...:

    In [2]: df
    Out[2]:
       A  B
    0  0  a
    1  1  a
    2  2  b
    3  3  b
    4  4  c
    5  5  a

    In [3]: df.dtypes
    Out[3]:
    A       int64
    B    category
    dtype: object

    In [4]: df.B.cat.categories
    Out[4]: Index(['c', 'a', 'b'], dtype='object')


setting the index, will create a ``CategoricalIndex``

.. code-block:: ipython

    In [5]: df2 = df.set_index('B')

    In [6]: df2.index
    Out[6]: CategoricalIndex(['a', 'a', 'b', 'b', 'c', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')

indexing with ``__getitem__/.iloc/.loc/.ix`` works similarly to an Index with duplicates.
The indexers MUST be in the category or the operation will raise.

.. code-block:: ipython

    In [7]: df2.loc['a']
    Out[7]:
       A
    B
    a  0
    a  1
    a  5

and preserves the ``CategoricalIndex``

.. code-block:: ipython

    In [8]: df2.loc['a'].index
    Out[8]: CategoricalIndex(['a', 'a', 'a'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')


sorting will order by the order of the categories

.. code-block:: ipython

    In [9]: df2.sort_index()
    Out[9]:
       A
    B
    c  4
    a  0
    a  1
    a  5
    b  2
    b  3

groupby operations on the index will preserve the index nature as well

.. code-block:: ipython

    In [10]: df2.groupby(level=0).sum()
    Out[10]:
       A
    B
    c  4
    a  6
    b  5

    In [11]: df2.groupby(level=0).sum().index
    Out[11]: CategoricalIndex(['c', 'a', 'b'], categories=['c', 'a', 'b'], ordered=False, name='B', dtype='category')


reindexing operations, will return a resulting index based on the type of the passed
indexer, meaning that passing a list will return a plain-old-``Index``; indexing with
a ``Categorical`` will return a ``CategoricalIndex``, indexed according to the categories
of the PASSED ``Categorical`` dtype. This allows one to arbitrarily index these even with
values NOT in the categories, similarly to how you can reindex ANY pandas index.

.. code-block:: ipython

    In [12]: df2.reindex(['a', 'e'])
    Out[12]:
         A
    B
    a  0.0
    a  1.0
    a  5.0
    e  NaN

    In [13]: df2.reindex(['a', 'e']).index
    Out[13]: pd.Index(['a', 'a', 'a', 'e'], dtype='object', name='B')

    In [14]: df2.reindex(pd.Categorical(['a', 'e'], categories=list('abcde')))
    Out[14]:
         A
    B
    a  0.0
    a  1.0
    a  5.0
    e  NaN

    In [15]: df2.reindex(pd.Categorical(['a', 'e'], categories=list('abcde'))).index
    Out[15]: pd.CategoricalIndex(['a', 'a', 'a', 'e'],
                                 categories=['a', 'b', 'c', 'd', 'e'],
                                 ordered=False, name='B',
                                 dtype='category')

See the :ref:`documentation <advanced.categoricalindex>` for more. (:issue:`7629`, :issue:`10038`, :issue:`10039`)

.. _whatsnew_0161.enhancements.sample:

Sample
^^^^^^

Series, DataFrames, and Panels now have a new method: :meth:`~pandas.DataFrame.sample`.
The method accepts a specific number of rows or columns to return, or a fraction of the
total number or rows or columns. It also has options for sampling with or without replacement,
for passing in a column for weights for non-uniform sampling, and for setting seed values to
facilitate replication. (:issue:`2419`)

.. ipython:: python

   example_series = pd.Series([0, 1, 2, 3, 4, 5])

   # When no arguments are passed, returns 1
   example_series.sample()

   # One may specify either a number of rows:
   example_series.sample(n=3)

   # Or a fraction of the rows:
   example_series.sample(frac=0.5)

   # weights are accepted.
   example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]
   example_series.sample(n=3, weights=example_weights)

   # weights will also be normalized if they do not sum to one,
   # and missing values will be treated as zeros.
   example_weights2 = [0.5, 0, 0, 0, None, np.nan]
   example_series.sample(n=1, weights=example_weights2)


When applied to a DataFrame, one may pass the name of a column to specify sampling weights
when sampling from rows.

.. ipython:: python

   df = pd.DataFrame({"col1": [9, 8, 7, 6], "weight_column": [0.5, 0.4, 0.1, 0]})
   df.sample(n=3, weights="weight_column")


.. _whatsnew_0161.enhancements.string:

String methods enhancements
^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`Continuing from v0.16.0 <whatsnew_0160.enhancements.string>`, the following
enhancements make string operations easier and more consistent with standard python string operations.


- Added ``StringMethods`` (``.str`` accessor) to ``Index`` (:issue:`9068`)

  The ``.str`` accessor is now available for both ``Series`` and ``Index``.

  .. ipython:: python

     idx = pd.Index([" jack", "jill ", " jesse ", "frank"])
     idx.str.strip()

  One special case for the ``.str`` accessor on ``Index`` is that if a string method returns ``bool``, the ``.str`` accessor
  will return a ``np.array`` instead of a boolean ``Index`` (:issue:`8875`). This enables the following expression
  to work naturally:

  .. ipython:: python

     idx = pd.Index(["a1", "a2", "b1", "b2"])
     s = pd.Series(range(4), index=idx)
     s
     idx.str.startswith("a")
     s[s.index.str.startswith("a")]

- The following new methods are accessible via ``.str`` accessor to apply the function to each values. (:issue:`9766`, :issue:`9773`, :issue:`10031`, :issue:`10045`, :issue:`10052`)

  ================  ===============  ===============  ===============  ================
  ..                ..               Methods          ..               ..
  ================  ===============  ===============  ===============  ================
  ``capitalize()``  ``swapcase()``   ``normalize()``  ``partition()``  ``rpartition()``
  ``index()``       ``rindex()``     ``translate()``
  ================  ===============  ===============  ===============  ================

- ``split`` now takes ``expand`` keyword to specify whether to expand dimensionality. ``return_type`` is deprecated. (:issue:`9847`)

  .. ipython:: python

     s = pd.Series(["a,b", "a,c", "b,c"])

     # return Series
     s.str.split(",")

     # return DataFrame
     s.str.split(",", expand=True)

     idx = pd.Index(["a,b", "a,c", "b,c"])

     # return Index
     idx.str.split(",")

     # return MultiIndex
     idx.str.split(",", expand=True)


- Improved ``extract`` and ``get_dummies`` methods for ``Index.str`` (:issue:`9980`)


.. _whatsnew_0161.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- ``BusinessHour`` offset is now supported, which represents business hours starting from 09:00 - 17:00 on ``BusinessDay`` by default. See :ref:`Here <timeseries.businesshour>` for details. (:issue:`7905`)

  .. ipython:: python

     pd.Timestamp("2014-08-01 09:00") + pd.tseries.offsets.BusinessHour()
     pd.Timestamp("2014-08-01 07:00") + pd.tseries.offsets.BusinessHour()
     pd.Timestamp("2014-08-01 16:30") + pd.tseries.offsets.BusinessHour()

- ``DataFrame.diff`` now takes an ``axis`` parameter that determines the direction of differencing (:issue:`9727`)

- Allow ``clip``, ``clip_lower``, and ``clip_upper`` to accept array-like arguments as thresholds (This is a regression from 0.11.0). These methods now have an ``axis`` parameter which determines how the Series or DataFrame will be aligned with the threshold(s). (:issue:`6966`)

- ``DataFrame.mask()`` and ``Series.mask()`` now support same keywords as ``where`` (:issue:`8801`)

- ``drop`` function can now accept ``errors`` keyword to suppress ``ValueError`` raised when any of label does not exist in the target data. (:issue:`6736`)

  .. ipython:: python

    df = pd.DataFrame(np.random.randn(3, 3), columns=["A", "B", "C"])
    df.drop(["A", "X"], axis=1, errors="ignore")

- Add support for separating years and quarters using dashes, for
  example 2014-Q1.  (:issue:`9688`)

- Allow conversion of values with dtype ``datetime64`` or ``timedelta64`` to strings using ``astype(str)`` (:issue:`9757`)
- ``get_dummies`` function now accepts ``sparse`` keyword.  If set to ``True``, the return ``DataFrame`` is sparse, e.g. ``SparseDataFrame``. (:issue:`8823`)
- ``Period`` now accepts ``datetime64`` as value input. (:issue:`9054`)

- Allow timedelta string conversion when leading zero is missing from time definition, ie ``0:00:00`` vs ``00:00:00``. (:issue:`9570`)
- Allow ``Panel.shift`` with ``axis='items'`` (:issue:`9890`)

- Trying to write an excel file now raises ``NotImplementedError`` if the ``DataFrame`` has a ``MultiIndex`` instead of writing a broken Excel file. (:issue:`9794`)
- Allow ``Categorical.add_categories`` to accept ``Series`` or ``np.array``. (:issue:`9927`)

- Add/delete ``str/dt/cat`` accessors dynamically from ``__dir__``. (:issue:`9910`)
- Add ``normalize`` as a ``dt`` accessor method. (:issue:`10047`)

- ``DataFrame`` and ``Series`` now have ``_constructor_expanddim`` property as overridable constructor for one higher dimensionality data. This should be used only when it is really needed, see :ref:`here <extending.subclassing-pandas>`

- ``pd.lib.infer_dtype`` now returns ``'bytes'`` in Python 3 where appropriate. (:issue:`10032`)


.. _whatsnew_0161.api:

API changes
~~~~~~~~~~~

- When passing in an ax to ``df.plot( ..., ax=ax)``, the ``sharex`` kwarg will now default to ``False``.
  The result is that the visibility of xlabels and xticklabels will not anymore be changed. You
  have to do that by yourself for the right axes in your figure or set ``sharex=True`` explicitly
  (but this changes the visible for all axes in the figure, not only the one which is passed in!).
  If pandas creates the subplots itself (e.g. no passed in ``ax`` kwarg), then the
  default is still ``sharex=True`` and the visibility changes are applied.

- :meth:`~pandas.DataFrame.assign` now inserts new columns in alphabetical order. Previously
  the order was arbitrary. (:issue:`9777`)

- By default, ``read_csv`` and ``read_table`` will now try to infer the compression type based on the file extension. Set ``compression=None`` to restore the previous behavior (no decompression). (:issue:`9770`)

.. _whatsnew_0161.deprecations:

Deprecations
^^^^^^^^^^^^

- ``Series.str.split``'s ``return_type`` keyword was removed in favor of ``expand`` (:issue:`9847`)


.. _whatsnew_0161.index_repr:

Index representation
~~~~~~~~~~~~~~~~~~~~

The string representation of ``Index`` and its sub-classes have now been unified. These will show a single-line display if there are few values; a wrapped multi-line display for a lot of values (but less than ``display.max_seq_items``; if lots of items (> ``display.max_seq_items``) will show a truncated display (the head and tail of the data). The formatting for ``MultiIndex`` is unchanged (a multi-line wrapped display). The display width responds to the option ``display.max_seq_items``, which is defaulted to 100. (:issue:`6482`)

Previous behavior

.. code-block:: ipython

   In [2]: pd.Index(range(4), name='foo')
   Out[2]: Int64Index([0, 1, 2, 3], dtype='int64')

   In [3]: pd.Index(range(104), name='foo')
   Out[3]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...], dtype='int64')

   In [4]: pd.date_range('20130101', periods=4, name='foo', tz='US/Eastern')
   Out[4]:
   <class 'pandas.tseries.index.DatetimeIndex'>
   [2013-01-01 00:00:00-05:00, ..., 2013-01-04 00:00:00-05:00]
   Length: 4, Freq: D, Timezone: US/Eastern

   In [5]: pd.date_range('20130101', periods=104, name='foo', tz='US/Eastern')
   Out[5]:
   <class 'pandas.tseries.index.DatetimeIndex'>
   [2013-01-01 00:00:00-05:00, ..., 2013-04-14 00:00:00-04:00]
   Length: 104, Freq: D, Timezone: US/Eastern

New behavior

.. ipython:: python

   pd.set_option("display.width", 80)
   pd.Index(range(4), name="foo")
   pd.Index(range(30), name="foo")
   pd.Index(range(104), name="foo")
   pd.CategoricalIndex(["a", "bb", "ccc", "dddd"], ordered=True, name="foobar")
   pd.CategoricalIndex(["a", "bb", "ccc", "dddd"] * 10, ordered=True, name="foobar")
   pd.CategoricalIndex(["a", "bb", "ccc", "dddd"] * 100, ordered=True, name="foobar")
   pd.date_range("20130101", periods=4, name="foo", tz="US/Eastern")
   pd.date_range("20130101", periods=25, freq="D")
   pd.date_range("20130101", periods=104, name="foo", tz="US/Eastern")


.. _whatsnew_0161.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved csv write performance with mixed dtypes, including datetimes by up to 5x (:issue:`9940`)
- Improved csv write performance generally by 2x (:issue:`9940`)
- Improved the performance of ``pd.lib.max_len_string_array`` by 5-7x (:issue:`10024`)


.. _whatsnew_0161.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug where labels did not appear properly in the legend of ``DataFrame.plot()``, passing ``label=`` arguments works, and Series indices are no longer mutated. (:issue:`9542`)
- Bug in json serialization causing a segfault when a frame had zero length. (:issue:`9805`)
- Bug in ``read_csv`` where missing trailing delimiters would cause segfault. (:issue:`5664`)
- Bug in retaining index name on appending (:issue:`9862`)
- Bug in ``scatter_matrix`` draws unexpected axis ticklabels (:issue:`5662`)
- Fixed bug in ``StataWriter`` resulting in changes to input ``DataFrame`` upon save (:issue:`9795`).
- Bug in ``transform`` causing length mismatch when null entries were present and a fast aggregator was being used (:issue:`9697`)
- Bug in ``equals`` causing false negatives when block order differed (:issue:`9330`)
- Bug in grouping with multiple ``pd.Grouper`` where one is non-time based (:issue:`10063`)
- Bug in ``read_sql_table`` error when reading postgres table with timezone (:issue:`7139`)
- Bug in ``DataFrame`` slicing may not retain metadata (:issue:`9776`)
- Bug where ``TimdeltaIndex`` were not properly serialized in fixed ``HDFStore`` (:issue:`9635`)
- Bug with ``TimedeltaIndex`` constructor ignoring ``name`` when given another ``TimedeltaIndex`` as data (:issue:`10025`).
- Bug in ``DataFrameFormatter._get_formatted_index`` with not applying ``max_colwidth`` to the ``DataFrame`` index (:issue:`7856`)
- Bug in ``.loc`` with a read-only ndarray data source (:issue:`10043`)
- Bug in ``groupby.apply()`` that would raise if a passed user defined function either returned only ``None`` (for all input). (:issue:`9685`)
- Always use temporary files in pytables tests (:issue:`9992`)
- Bug in plotting continuously using ``secondary_y`` may not show legend properly. (:issue:`9610`, :issue:`9779`)
- Bug in ``DataFrame.plot(kind="hist")`` results in ``TypeError`` when ``DataFrame`` contains non-numeric columns  (:issue:`9853`)
- Bug where repeated plotting of ``DataFrame`` with a ``DatetimeIndex`` may raise ``TypeError`` (:issue:`9852`)
- Bug in ``setup.py`` that would allow an incompat cython version to build (:issue:`9827`)
- Bug in plotting ``secondary_y`` incorrectly attaches ``right_ax`` property to secondary axes specifying itself recursively. (:issue:`9861`)
- Bug in ``Series.quantile`` on empty Series of type ``Datetime`` or ``Timedelta`` (:issue:`9675`)
- Bug in ``where`` causing incorrect results when upcasting was required (:issue:`9731`)
- Bug in ``FloatArrayFormatter`` where decision boundary for displaying "small" floats in decimal format is off by one order of magnitude for a given display.precision (:issue:`9764`)
- Fixed bug where ``DataFrame.plot()`` raised an error when both ``color`` and ``style`` keywords were passed and there was no color symbol in the style strings (:issue:`9671`)
- Not showing a ``DeprecationWarning`` on combining list-likes with an ``Index`` (:issue:`10083`)
- Bug in ``read_csv`` and ``read_table`` when using ``skip_rows`` parameter if blank lines are present. (:issue:`9832`)
- Bug in ``read_csv()`` interprets ``index_col=True`` as ``1`` (:issue:`9798`)
- Bug in index equality comparisons using ``==`` failing on Index/MultiIndex type incompatibility (:issue:`9785`)
- Bug in which ``SparseDataFrame`` could not take ``nan`` as a column name (:issue:`8822`)
- Bug in ``to_msgpack`` and ``read_msgpack`` zlib and blosc compression support (:issue:`9783`)
- Bug ``GroupBy.size`` doesn't attach index name properly if grouped by ``TimeGrouper`` (:issue:`9925`)
- Bug causing an exception in slice assignments because ``length_of_indexer`` returns wrong results (:issue:`9995`)
- Bug in csv parser causing lines with initial white space plus one non-space character to be skipped. (:issue:`9710`)
- Bug in C csv parser causing spurious NaNs when data started with newline followed by white space. (:issue:`10022`)
- Bug causing elements with a null group to spill into the final group when grouping by a ``Categorical`` (:issue:`9603`)
- Bug where .iloc and .loc behavior is not consistent on empty dataframes (:issue:`9964`)
- Bug in invalid attribute access on a ``TimedeltaIndex`` incorrectly raised ``ValueError`` instead of ``AttributeError`` (:issue:`9680`)
- Bug in unequal comparisons between categorical data and a scalar, which was not in the categories (e.g. ``Series(Categorical(list("abc"), ordered=True)) > "d"``. This returned ``False`` for all elements, but now raises a ``TypeError``. Equality comparisons also now return ``False`` for ``==`` and ``True`` for ``!=``. (:issue:`9848`)
- Bug in DataFrame ``__setitem__`` when right hand side is a dictionary (:issue:`9874`)
- Bug in ``where`` when dtype is ``datetime64/timedelta64``, but dtype of other is not (:issue:`9804`)
- Bug in ``MultiIndex.sortlevel()`` results in unicode level name breaks (:issue:`9856`)
- Bug in which ``groupby.transform`` incorrectly enforced output dtypes to match input dtypes. (:issue:`9807`)
- Bug in ``DataFrame`` constructor when ``columns`` parameter is set, and ``data`` is an empty list (:issue:`9939`)
- Bug in bar plot with ``log=True`` raises ``TypeError`` if all values are less than 1 (:issue:`9905`)
- Bug in horizontal bar plot ignores ``log=True`` (:issue:`9905`)
- Bug in PyTables queries that did not return proper results using the index (:issue:`8265`, :issue:`9676`)
- Bug where dividing a dataframe containing values of type ``Decimal`` by another ``Decimal`` would raise. (:issue:`9787`)
- Bug where using DataFrames asfreq would remove the name of the index. (:issue:`9885`)
- Bug causing extra index point when resample BM/BQ (:issue:`9756`)
- Changed caching in ``AbstractHolidayCalendar`` to be at the instance level rather than at the class level as the latter can result in unexpected behaviour. (:issue:`9552`)
- Fixed latex output for MultiIndexed dataframes (:issue:`9778`)
- Bug causing an exception when setting an empty range using ``DataFrame.loc`` (:issue:`9596`)
- Bug in hiding ticklabels with subplots and shared axes when adding a new plot to an existing grid of axes (:issue:`9158`)
- Bug in ``transform`` and ``filter`` when grouping on a categorical variable (:issue:`9921`)
- Bug in ``transform`` when groups are equal in number and dtype to the input index (:issue:`9700`)
- Google BigQuery connector now imports dependencies on a per-method basis.(:issue:`9713`)
- Updated BigQuery connector to no longer use deprecated ``oauth2client.tools.run()`` (:issue:`8327`)
- Bug in subclassed ``DataFrame``. It may not return the correct class, when slicing or subsetting it. (:issue:`9632`)
- Bug in ``.median()`` where non-float null values are not handled correctly (:issue:`10040`)
- Bug in Series.fillna() where it raises if a numerically convertible string is given (:issue:`10092`)


.. _whatsnew_0.16.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.16.0..v0.16.1
.. _whatsnew_0702:

Version 0.7.2 (March 16, 2012)
------------------------------

{{ header }}


This release targets bugs in 0.7.1, and adds a few minor features.

New features
~~~~~~~~~~~~

  - Add additional tie-breaking methods in DataFrame.rank (:issue:`874`)
  - Add ascending parameter to rank in Series, DataFrame (:issue:`875`)
  - Add coerce_float option to DataFrame.from_records (:issue:`893`)
  - Add sort_columns parameter to allow unsorted plots (:issue:`918`)
  - Enable column access via attributes on GroupBy (:issue:`882`)
  - Can pass dict of values to DataFrame.fillna (:issue:`661`)
  - Can select multiple hierarchical groups by passing list of values in .ix
    (:issue:`134`)
  - Add ``axis`` option to DataFrame.fillna (:issue:`174`)
  - Add level keyword to ``drop`` for dropping values from a level (:issue:`159`)

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

  - Use khash for Series.value_counts, add raw function to algorithms.py (:issue:`861`)
  - Intercept __builtin__.sum in groupby (:issue:`885`)



.. _whatsnew_0.7.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.7.1..v0.7.2
.. _whatsnew_114:

What's new in 1.1.4 (October 30, 2020)
--------------------------------------

These are the changes in pandas 1.1.4. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_114.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :func:`read_csv` raising a ``ValueError`` when ``names`` was of type ``dict_keys`` (:issue:`36928`)
- Fixed regression in :func:`read_csv` with more than 1M rows and specifying a ``index_col`` argument (:issue:`37094`)
- Fixed regression where attempting to mutate a :class:`DateOffset` object would no longer raise an ``AttributeError`` (:issue:`36940`)
- Fixed regression where :meth:`DataFrame.agg` would fail with :exc:`TypeError` when passed positional arguments to be passed on to the aggregation function (:issue:`36948`).
- Fixed regression in :class:`RollingGroupby` with ``sort=False`` not being respected (:issue:`36889`)
- Fixed regression in :meth:`Series.astype` converting ``None`` to ``"nan"`` when casting to string (:issue:`36904`)
- Fixed regression in :meth:`Series.rank` method failing for read-only data (:issue:`37290`)
- Fixed regression in :class:`RollingGroupby` causing a segmentation fault with Index of dtype object (:issue:`36727`)
- Fixed regression in :meth:`DataFrame.resample(...).apply(...)` raised ``AttributeError`` when input was a :class:`DataFrame` and only a :class:`Series` was evaluated (:issue:`36951`)
- Fixed regression in ``DataFrame.groupby(..).std()`` with nullable integer dtype (:issue:`37415`)
- Fixed regression in :class:`PeriodDtype` comparing both equal and unequal to its string representation (:issue:`37265`)
- Fixed regression where slicing :class:`DatetimeIndex` raised :exc:`AssertionError` on irregular time series with ``pd.NaT`` or on unsorted indices (:issue:`36953` and :issue:`35509`)
- Fixed regression in certain offsets (:meth:`pd.offsets.Day() <pandas.tseries.offsets.Day>` and below) no longer being hashable (:issue:`37267`)
- Fixed regression in :class:`StataReader` which required ``chunksize`` to be manually set when using an iterator to read a dataset (:issue:`37280`)
- Fixed regression in setitem with :meth:`DataFrame.iloc` which raised error when trying to set a value while filtering with a boolean list (:issue:`36741`)
- Fixed regression in setitem with a Series getting aligned before setting the values (:issue:`37427`)
- Fixed regression in :attr:`MultiIndex.is_monotonic_increasing` returning wrong results with ``NaN`` in at least one of the levels (:issue:`37220`)
- Fixed regression in inplace arithmetic operation on a Series not updating the parent DataFrame (:issue:`36373`)

.. ---------------------------------------------------------------------------

.. _whatsnew_114.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug causing ``groupby(...).sum()`` and similar to not preserve metadata (:issue:`29442`)
- Bug in :meth:`Series.isin` and :meth:`DataFrame.isin` raising a ``ValueError`` when the target was read-only (:issue:`37174`)
- Bug in :meth:`GroupBy.fillna` that introduced a performance regression after 1.0.5 (:issue:`36757`)
- Bug in :meth:`DataFrame.info` was raising a ``KeyError`` when the DataFrame has integer column names (:issue:`37245`)
- Bug in :meth:`DataFrameGroupby.apply` would drop a :class:`CategoricalIndex` when grouped on (:issue:`35792`)

.. ---------------------------------------------------------------------------

.. _whatsnew_114.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.3..v1.1.4
.. _whatsnew_0202:

Version 0.20.2 (June 4, 2017)
-----------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a minor bug-fix release in the 0.20.x series and includes some small regression fixes,
bug fixes and performance improvements.
We recommend that all users upgrade to this version.

.. contents:: What's new in v0.20.2
    :local:
    :backlinks: none


.. _whatsnew_0202.enhancements:

Enhancements
~~~~~~~~~~~~

- Unblocked access to additional compression types supported in pytables: 'blosc:blosclz, 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd' (:issue:`14478`)
- ``Series`` provides a ``to_latex`` method (:issue:`16180`)

- A new groupby method :meth:`~pandas.core.groupby.GroupBy.ngroup`,
  parallel to the existing :meth:`~pandas.core.groupby.GroupBy.cumcount`,
  has been added to return the group order (:issue:`11642`); see
  :ref:`here <groupby.ngroup>`.

.. _whatsnew_0202.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Performance regression fix when indexing with a list-like (:issue:`16285`)
- Performance regression fix for MultiIndexes (:issue:`16319`, :issue:`16346`)
- Improved performance of ``.clip()`` with scalar arguments (:issue:`15400`)
- Improved performance of groupby with categorical groupers (:issue:`16413`)
- Improved performance of ``MultiIndex.remove_unused_levels()`` (:issue:`16556`)

.. _whatsnew_0202.bug_fixes:

Bug fixes
~~~~~~~~~

- Silenced a warning on some Windows environments about "tput: terminal attributes: No such device or address" when
  detecting the terminal size. This fix only applies to python 3 (:issue:`16496`)
- Bug in using ``pathlib.Path`` or ``py.path.local`` objects with io functions (:issue:`16291`)
- Bug in ``Index.symmetric_difference()`` on two equal MultiIndex's, results in a ``TypeError`` (:issue:`13490`)
- Bug in ``DataFrame.update()`` with ``overwrite=False`` and ``NaN values`` (:issue:`15593`)
- Passing an invalid engine to :func:`read_csv` now raises an informative
  ``ValueError`` rather than ``UnboundLocalError``. (:issue:`16511`)
- Bug in :func:`unique` on an array of tuples (:issue:`16519`)
- Bug in :func:`cut` when ``labels`` are set, resulting in incorrect label ordering (:issue:`16459`)
- Fixed a compatibility issue with IPython 6.0's tab completion showing deprecation warnings on ``Categoricals`` (:issue:`16409`)

Conversion
^^^^^^^^^^

- Bug in :func:`to_numeric` in which empty data inputs were causing a segfault of the interpreter (:issue:`16302`)
- Silence numpy warnings when broadcasting ``DataFrame`` to ``Series`` with comparison ops (:issue:`16378`, :issue:`16306`)


Indexing
^^^^^^^^

- Bug in ``DataFrame.reset_index(level=)`` with single level index (:issue:`16263`)
- Bug in partial string indexing with a monotonic, but not strictly-monotonic, index incorrectly reversing the slice bounds (:issue:`16515`)
- Bug in ``MultiIndex.remove_unused_levels()`` that would not return a ``MultiIndex`` equal to the original. (:issue:`16556`)

IO
^^

- Bug in :func:`read_csv` when ``comment`` is passed in a space delimited text file (:issue:`16472`)
- Bug in :func:`read_csv` not raising an exception with nonexistent columns in ``usecols`` when it had the correct length (:issue:`14671`)
- Bug that would force importing of the clipboard routines unnecessarily, potentially causing an import error on startup (:issue:`16288`)
- Bug that raised ``IndexError`` when HTML-rendering an empty ``DataFrame`` (:issue:`15953`)
- Bug in :func:`read_csv` in which tarfile object inputs were raising an error in Python 2.x for the C engine (:issue:`16530`)
- Bug where ``DataFrame.to_html()`` ignored the ``index_names`` parameter (:issue:`16493`)
- Bug where ``pd.read_hdf()`` returns numpy strings for index names (:issue:`13492`)

- Bug in ``HDFStore.select_as_multiple()`` where start/stop arguments were not respected (:issue:`16209`)

Plotting
^^^^^^^^

- Bug in ``DataFrame.plot`` with a single column and a list-like ``color`` (:issue:`3486`)
- Bug in ``plot`` where ``NaT`` in ``DatetimeIndex`` results in ``Timestamp.min`` (:issue:`12405`)
- Bug in ``DataFrame.boxplot`` where ``figsize`` keyword was not respected for non-grouped boxplots (:issue:`11959`)




GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in creating a time-based rolling window on an empty ``DataFrame`` (:issue:`15819`)
- Bug in ``rolling.cov()`` with offset window (:issue:`16058`)
- Bug in ``.resample()`` and ``.groupby()`` when aggregating on integers (:issue:`16361`)


Sparse
^^^^^^

- Bug in construction of ``SparseDataFrame`` from ``scipy.sparse.dok_matrix`` (:issue:`16179`)

Reshaping
^^^^^^^^^

- Bug in ``DataFrame.stack`` with unsorted levels in ``MultiIndex`` columns (:issue:`16323`)
- Bug in ``pd.wide_to_long()`` where no error was raised when ``i`` was not a unique identifier (:issue:`16382`)
- Bug in ``Series.isin(..)`` with a list of tuples (:issue:`16394`)
- Bug in construction of a ``DataFrame`` with mixed dtypes including an all-NaT column. (:issue:`16395`)
- Bug in ``DataFrame.agg()`` and ``Series.agg()`` with aggregating on non-callable attributes (:issue:`16405`)


Numeric
^^^^^^^
- Bug in ``.interpolate()``, where ``limit_direction`` was not respected when ``limit=None`` (default) was passed (:issue:`16282`)

Categorical
^^^^^^^^^^^

- Fixed comparison operations considering the order of the categories when both categoricals are unordered (:issue:`16014`)

Other
^^^^^

- Bug in ``DataFrame.drop()`` with an empty-list with non-unique indices (:issue:`16270`)


.. _whatsnew_0.20.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.20.0..v0.20.2
.. _whatsnew_0231:

What's new in 0.23.1 (June 12, 2018)
------------------------------------

{{ header }}


This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes
and bug fixes. We recommend that all users upgrade to this version.

.. warning::

   Starting January 1, 2019, pandas feature releases will support Python 3 only.
   See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

.. contents:: What's new in v0.23.1
    :local:
    :backlinks: none

.. _whatsnew_0231.fixed_regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

**Comparing Series with datetime.date**

We've reverted a 0.23.0 change to comparing a :class:`Series` holding datetimes and a ``datetime.date`` object (:issue:`21152`).
In pandas 0.22 and earlier, comparing a Series holding datetimes and ``datetime.date`` objects would coerce the ``datetime.date`` to a datetime before comparing.
This was inconsistent with Python, NumPy, and :class:`DatetimeIndex`, which never consider a datetime and ``datetime.date`` equal.

In 0.23.0, we unified operations between DatetimeIndex and Series, and in the process changed comparisons between a Series of datetimes and ``datetime.date`` without warning.

We've temporarily restored the 0.22.0 behavior, so datetimes and dates may again compare equal, but restore the 0.23.0 behavior in a future release.

To summarize, here's the behavior in 0.22.0, 0.23.0, 0.23.1:

.. code-block:: python

   # 0.22.0... Silently coerce the datetime.date
   >>> import datetime
   >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)
   0     True
   1    False
   dtype: bool

   # 0.23.0... Do not coerce the datetime.date
   >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)
   0    False
   1    False
   dtype: bool

   # 0.23.1... Coerce the datetime.date with a warning
   >>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)
   /bin/python:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the
   'datetime.date' is coerced to a datetime. In the future pandas will
   not coerce, and the values not compare equal to the 'datetime.date'.
   To retain the current behavior, convert the 'datetime.date' to a
   datetime with 'pd.Timestamp'.
     #!/bin/python3
   0     True
   1    False
   dtype: bool

In addition, ordering comparisons will raise a ``TypeError`` in the future.

**Other fixes**

- Reverted the ability of :func:`~DataFrame.to_sql` to perform multivalue
  inserts as this caused regression in certain cases (:issue:`21103`).
  In the future this will be made configurable.
- Fixed regression in the :attr:`DatetimeIndex.date` and :attr:`DatetimeIndex.time`
  attributes in case of timezone-aware data: :attr:`DatetimeIndex.time` returned
  a tz-aware time instead of tz-naive (:issue:`21267`) and :attr:`DatetimeIndex.date`
  returned incorrect date when the input date has a non-UTC timezone (:issue:`21230`).
- Fixed regression in :meth:`pandas.io.json.json_normalize` when called with ``None`` values
  in nested levels in JSON, and to not drop keys with value as ``None`` (:issue:`21158`, :issue:`21356`).
- Bug in :meth:`~DataFrame.to_csv` causes encoding error when compression and encoding are specified (:issue:`21241`, :issue:`21118`)
- Bug preventing pandas from being importable with -OO optimization (:issue:`21071`)
- Bug in :meth:`Categorical.fillna` incorrectly raising a ``TypeError`` when ``value`` the individual categories are iterable and ``value`` is an iterable (:issue:`21097`, :issue:`19788`)
- Fixed regression in constructors coercing NA values like ``None`` to strings when passing ``dtype=str`` (:issue:`21083`)
- Regression in :func:`pivot_table` where an ordered ``Categorical`` with missing
  values for the pivot's ``index`` would give a mis-aligned result (:issue:`21133`)
- Fixed regression in merging on boolean index/columns (:issue:`21119`).

.. _whatsnew_0231.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of :meth:`CategoricalIndex.is_monotonic_increasing`, :meth:`CategoricalIndex.is_monotonic_decreasing` and :meth:`CategoricalIndex.is_monotonic` (:issue:`21025`)
- Improved performance of :meth:`CategoricalIndex.is_unique` (:issue:`21107`)


.. _whatsnew_0231.bug_fixes:

Bug fixes
~~~~~~~~~

**Groupby/resample/rolling**

- Bug in :func:`DataFrame.agg` where applying multiple aggregation functions to a :class:`DataFrame` with duplicated column names would cause a stack overflow (:issue:`21063`)
- Bug in :func:`pandas.core.groupby.GroupBy.ffill` and :func:`pandas.core.groupby.GroupBy.bfill` where the fill within a grouping would not always be applied as intended due to the implementations' use of a non-stable sort (:issue:`21207`)
- Bug in :func:`pandas.core.groupby.GroupBy.rank` where results did not scale to 100% when specifying ``method='dense'`` and ``pct=True``
- Bug in :func:`pandas.DataFrame.rolling` and :func:`pandas.Series.rolling` which incorrectly accepted a 0 window size rather than raising (:issue:`21286`)

**Data-type specific**

- Bug in :meth:`Series.str.replace()` where the method throws ``TypeError`` on Python 3.5.2 (:issue:`21078`)
- Bug in :class:`Timedelta` where passing a float with a unit would prematurely round the float precision (:issue:`14156`)
- Bug in :func:`pandas.testing.assert_index_equal` which raised ``AssertionError`` incorrectly, when comparing two :class:`CategoricalIndex` objects with param ``check_categorical=False`` (:issue:`19776`)

**Sparse**

- Bug in :attr:`SparseArray.shape` which previously only returned the shape :attr:`SparseArray.sp_values` (:issue:`21126`)

**Indexing**

- Bug in :meth:`Series.reset_index` where appropriate error was not raised with an invalid level name (:issue:`20925`)
- Bug in :func:`interval_range` when ``start``/``periods`` or ``end``/``periods`` are specified with float ``start`` or ``end`` (:issue:`21161`)
- Bug in :meth:`MultiIndex.set_names` where error raised for a ``MultiIndex`` with ``nlevels == 1`` (:issue:`21149`)
- Bug in :class:`IntervalIndex` constructors where creating an ``IntervalIndex`` from categorical data was not fully supported (:issue:`21243`, :issue:`21253`)
- Bug in :meth:`MultiIndex.sort_index` which was not guaranteed to sort correctly with ``level=1``; this was also causing data misalignment in particular :meth:`DataFrame.stack` operations (:issue:`20994`, :issue:`20945`, :issue:`21052`)

**Plotting**

- New keywords (sharex, sharey) to turn on/off sharing of x/y-axis by subplots generated with pandas.DataFrame().groupby().boxplot() (:issue:`20968`)

**I/O**

- Bug in IO methods specifying ``compression='zip'`` which produced uncompressed zip archives (:issue:`17778`, :issue:`21144`)
- Bug in :meth:`DataFrame.to_stata` which prevented exporting DataFrames to buffers and most file-like objects (:issue:`21041`)
- Bug in :meth:`read_stata` and :class:`StataReader` which did not correctly decode utf-8 strings on Python 3 from Stata 14 files (dta version 118) (:issue:`21244`)
- Bug in IO JSON :func:`read_json` reading empty JSON schema with ``orient='table'`` back to :class:`DataFrame` caused an error (:issue:`21287`)

**Reshaping**

- Bug in :func:`concat` where error was raised in concatenating :class:`Series` with numpy scalar and tuple names (:issue:`21015`)
- Bug in :func:`concat` warning message providing the wrong guidance for future behavior (:issue:`21101`)

**Other**

- Tab completion on :class:`Index` in IPython no longer outputs deprecation warnings (:issue:`21125`)
- Bug preventing pandas being used on Windows without C++ redistributable installed (:issue:`21106`)

.. _whatsnew_0.23.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.23.0..v0.23.1
.. _whatsnew_0110:

Version 0.11.0 (April 22, 2013)
-------------------------------

{{ header }}


This is a major release from 0.10.1 and includes many new features and
enhancements along with a large number of bug fixes. The methods of Selecting
Data have had quite a number of additions, and Dtype support is now full-fledged.
There are also a number of important API changes that long-time pandas users should
pay close attention to.

There is a new section in the documentation, :ref:`10 Minutes to Pandas <10min>`,
primarily geared to new users.

There is a new section in the documentation, :ref:`Cookbook <cookbook>`, a collection
of useful recipes in pandas (and that we want contributions!).

There are several libraries that are now :ref:`Recommended Dependencies <install.recommended_dependencies>`

Selection choices
~~~~~~~~~~~~~~~~~

Starting in 0.11.0, object selection has had a number of user-requested additions in
order to support more explicit location based indexing. pandas now supports
three types of multi-axis indexing.

- ``.loc`` is strictly label based, will raise ``KeyError`` when the items are not found, allowed inputs are:

  - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is interpreted as a *label* of the index. This use is **not** an integer position along the index)
  - A list or array of labels ``['a', 'b', 'c']``
  - A slice object with labels ``'a':'f'``, (note that contrary to usual python slices, **both** the start and the stop are included!)
  - A boolean array

  See more at :ref:`Selection by Label <indexing.label>`

- ``.iloc`` is strictly integer position based (from ``0`` to ``length-1`` of the axis), will raise ``IndexError`` when the requested indices are out of bounds. Allowed inputs are:

  - An integer e.g. ``5``
  - A list or array of integers ``[4, 3, 0]``
  - A slice object with ints ``1:7``
  - A boolean array

  See more at :ref:`Selection by Position <indexing.integer>`

- ``.ix`` supports mixed integer and label based access. It is primarily label based, but will fallback to integer positional access. ``.ix`` is the most general and will support
  any of the inputs to ``.loc`` and ``.iloc``, as well as support for floating point label schemes. ``.ix`` is especially useful when dealing with mixed positional and label
  based hierarchical indexes.

  As using integer slices with ``.ix`` have different behavior depending on whether the slice
  is interpreted as position based or label based, it's usually better to be
  explicit and use ``.iloc`` or ``.loc``.

  See more at :ref:`Advanced Indexing <advanced>` and :ref:`Advanced Hierarchical <advanced.advanced_hierarchical>`.


Selection deprecations
~~~~~~~~~~~~~~~~~~~~~~

Starting in version 0.11.0, these methods *may* be deprecated in future versions.

- ``irow``
- ``icol``
- ``iget_value``

See the section :ref:`Selection by Position <indexing.integer>` for substitutes.

Dtypes
~~~~~~

Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the ``dtype`` keyword, a passed ``ndarray``, or a passed ``Series``, then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will **NOT** be combined. The following example will give you a taste.

.. ipython:: python

   df1 = pd.DataFrame(np.random.randn(8, 1), columns=['A'], dtype='float32')
   df1
   df1.dtypes
   df2 = pd.DataFrame({'A': pd.Series(np.random.randn(8), dtype='float16'),
                       'B': pd.Series(np.random.randn(8)),
                       'C': pd.Series(range(8), dtype='uint8')})
   df2
   df2.dtypes

   # here you get some upcasting
   df3 = df1.reindex_like(df2).fillna(value=0.0) + df2
   df3
   df3.dtypes

Dtype conversion
~~~~~~~~~~~~~~~~

This is lower-common-denominator upcasting, meaning you get the dtype which can accommodate all of the types

.. ipython:: python

   df3.values.dtype

Conversion

.. ipython:: python

   df3.astype('float32').dtypes

Mixed conversion

.. code-block:: ipython

    In [12]: df3['D'] = '1.'

    In [13]: df3['E'] = '1'

    In [14]: df3.convert_objects(convert_numeric=True).dtypes
    Out[14]:
    A    float32
    B    float64
    C    float64
    D    float64
    E      int64
    dtype: object

    # same, but specific dtype conversion
    In [15]: df3['D'] = df3['D'].astype('float16')

    In [16]: df3['E'] = df3['E'].astype('int32')

    In [17]: df3.dtypes
    Out[17]:
    A    float32
    B    float64
    C    float64
    D    float16
    E      int32
    dtype: object

Forcing date coercion (and setting ``NaT`` when not datelike)

.. code-block:: ipython

    In [18]: import datetime

    In [19]: s = pd.Series([datetime.datetime(2001, 1, 1, 0, 0), 'foo', 1.0, 1,
       ....:                pd.Timestamp('20010104'), '20010105'], dtype='O')
       ....:

    In [20]: s.convert_objects(convert_dates='coerce')
    Out[20]:
    0   2001-01-01
    1          NaT
    2          NaT
    3          NaT
    4   2001-01-04
    5   2001-01-05
    dtype: datetime64[ns]

Dtype gotchas
~~~~~~~~~~~~~

**Platform gotchas**

Starting in 0.11.0, construction of DataFrame/Series will use default dtypes of ``int64`` and ``float64``,
*regardless of platform*. This is not an apparent change from earlier versions of pandas. If you specify
dtypes, they *WILL* be respected, however (:issue:`2837`)

The following will all result in ``int64`` dtypes

.. code-block:: ipython

    In [21]: pd.DataFrame([1, 2], columns=['a']).dtypes
    Out[21]:
    a    int64
    dtype: object

    In [22]: pd.DataFrame({'a': [1, 2]}).dtypes
    Out[22]:
    a    int64
    dtype: object

    In [23]: pd.DataFrame({'a': 1}, index=range(2)).dtypes
    Out[23]:
    a    int64
    dtype: object

Keep in mind that ``DataFrame(np.array([1,2]))`` **WILL** result in ``int32`` on 32-bit platforms!


**Upcasting gotchas**

Performing indexing operations on integer type data can easily upcast the data.
The dtype of the input data will be preserved in cases where ``nans`` are not introduced.

.. code-block:: ipython

    In [24]: dfi = df3.astype('int32')

    In [25]: dfi['D'] = dfi['D'].astype('int64')

    In [26]: dfi
    Out[26]:
      A  B  C  D  E
    0  0  0  0  1  1
    1 -2  0  1  1  1
    2 -2  0  2  1  1
    3  0 -1  3  1  1
    4  1  0  4  1  1
    5  0  0  5  1  1
    6  0 -1  6  1  1
    7  0  0  7  1  1

    In [27]: dfi.dtypes
    Out[27]:
    A    int32
    B    int32
    C    int32
    D    int64
    E    int32
    dtype: object

    In [28]: casted = dfi[dfi > 0]

    In [29]: casted
    Out[29]:
        A   B    C  D  E
    0  NaN NaN  NaN  1  1
    1  NaN NaN  1.0  1  1
    2  NaN NaN  2.0  1  1
    3  NaN NaN  3.0  1  1
    4  1.0 NaN  4.0  1  1
    5  NaN NaN  5.0  1  1
    6  NaN NaN  6.0  1  1
    7  NaN NaN  7.0  1  1

    In [30]: casted.dtypes
    Out[30]:
    A    float64
    B    float64
    C    float64
    D      int64
    E      int32
    dtype: object

While float dtypes are unchanged.

.. code-block:: ipython

    In [31]: df4 = df3.copy()

    In [32]: df4['A'] = df4['A'].astype('float32')

    In [33]: df4.dtypes
    Out[33]:
    A    float32
    B    float64
    C    float64
    D    float16
    E      int32
    dtype: object

    In [34]: casted = df4[df4 > 0]

    In [35]: casted
    Out[35]:
              A         B    C    D  E
    0       NaN       NaN  NaN  1.0  1
    1       NaN  0.567020  1.0  1.0  1
    2       NaN  0.276232  2.0  1.0  1
    3       NaN       NaN  3.0  1.0  1
    4  1.933792       NaN  4.0  1.0  1
    5       NaN  0.113648  5.0  1.0  1
    6       NaN       NaN  6.0  1.0  1
    7       NaN  0.524988  7.0  1.0  1

    In [36]: casted.dtypes
    Out[36]:
    A    float32
    B    float64
    C    float64
    D    float16
    E      int32
    dtype: object

Datetimes conversion
~~~~~~~~~~~~~~~~~~~~

Datetime64[ns] columns in a DataFrame (or a Series) allow the use of ``np.nan`` to indicate a nan value,
in addition to the traditional ``NaT``, or not-a-time. This allows convenient nan setting in a generic way.
Furthermore ``datetime64[ns]`` columns are created by default, when passed datetimelike objects (*this change was introduced in 0.10.1*)
(:issue:`2809`, :issue:`2810`)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(6, 2), pd.date_range('20010102', periods=6),
                     columns=['A', ' B'])
   df['timestamp'] = pd.Timestamp('20010103')
   df

   # datetime64[ns] out of the box
   df.dtypes.value_counts()

   # use the traditional nan, which is mapped to NaT internally
   df.loc[df.index[2:4], ['A', 'timestamp']] = np.nan
   df

Astype conversion on ``datetime64[ns]`` to ``object``, implicitly converts ``NaT`` to ``np.nan``

.. ipython:: python

   import datetime
   s = pd.Series([datetime.datetime(2001, 1, 2, 0, 0) for i in range(3)])
   s.dtype
   s[1] = np.nan
   s
   s.dtype
   s = s.astype('O')
   s
   s.dtype


API changes
~~~~~~~~~~~

  - Added to_series() method to indices, to facilitate the creation of indexers
    (:issue:`3275`)

  - ``HDFStore``

    - added the method ``select_column`` to select a single column from a table as a Series.
    - deprecated the ``unique`` method, can be replicated by ``select_column(key,column).unique()``
    - ``min_itemsize`` parameter to ``append`` will now automatically create data_columns for passed keys

Enhancements
~~~~~~~~~~~~

  - Improved performance of df.to_csv() by up to 10x in some cases. (:issue:`3059`)

  - Numexpr is now a :ref:`Recommended Dependencies <install.recommended_dependencies>`, to accelerate certain
    types of numerical and boolean operations

  - Bottleneck is now a :ref:`Recommended Dependencies <install.recommended_dependencies>`, to accelerate certain
    types of ``nan`` operations

  - ``HDFStore``

    - support ``read_hdf/to_hdf`` API similar to ``read_csv/to_csv``

      .. ipython:: python

          df = pd.DataFrame({'A': range(5), 'B': range(5)})
          df.to_hdf('store.h5', 'table', append=True)
          pd.read_hdf('store.h5', 'table', where=['index > 2'])

      .. ipython:: python
          :suppress:
          :okexcept:

          import os

          os.remove('store.h5')

    - provide dotted attribute access to ``get`` from stores, e.g. ``store.df == store['df']``

    - new keywords ``iterator=boolean``, and ``chunksize=number_in_a_chunk`` are
      provided to support iteration on ``select`` and ``select_as_multiple`` (:issue:`3076`)

  - You can now select timestamps from an *unordered* timeseries similarly to an *ordered* timeseries (:issue:`2437`)

  - You can now select with a string from a DataFrame with a datelike index, in a similar way to a Series (:issue:`3070`)

    .. ipython:: python
       :okwarning:

        idx = pd.date_range("2001-10-1", periods=5, freq='M')
        ts = pd.Series(np.random.rand(len(idx)), index=idx)
        ts['2001']

        df = pd.DataFrame({'A': ts})
        df['2001']

  - ``Squeeze`` to possibly remove length 1 dimensions from an object.

    .. code-block:: python

       >>> p = pd.Panel(np.random.randn(3, 4, 4), items=['ItemA', 'ItemB', 'ItemC'],
       ...              major_axis=pd.date_range('20010102', periods=4),
       ...              minor_axis=['A', 'B', 'C', 'D'])
       >>> p
       <class 'pandas.core.panel.Panel'>
       Dimensions: 3 (items) x 4 (major_axis) x 4 (minor_axis)
       Items axis: ItemA to ItemC
       Major_axis axis: 2001-01-02 00:00:00 to 2001-01-05 00:00:00
       Minor_axis axis: A to D

       >>> p.reindex(items=['ItemA']).squeeze()
                          A         B         C         D
       2001-01-02  0.926089 -2.026458  0.501277 -0.204683
       2001-01-03 -0.076524  1.081161  1.141361  0.479243
       2001-01-04  0.641817 -0.185352  1.824568  0.809152
       2001-01-05  0.575237  0.669934  1.398014 -0.399338

       >>> p.reindex(items=['ItemA'], minor=['B']).squeeze()
       2001-01-02   -2.026458
       2001-01-03    1.081161
       2001-01-04   -0.185352
       2001-01-05    0.669934
       Freq: D, Name: B, dtype: float64

  - In ``pd.io.data.Options``,

    + Fix bug when trying to fetch data for the current month when already
      past expiry.
    + Now using lxml to scrape html instead of BeautifulSoup (lxml was faster).
    + New instance variables for calls and puts are automatically created
      when a method that creates them is called. This works for current month
      where the instance variables are simply ``calls`` and ``puts``. Also
      works for future expiry months and save the instance variable as
      ``callsMMYY`` or ``putsMMYY``, where ``MMYY`` are, respectively, the
      month and year of the option's expiry.
    + ``Options.get_near_stock_price`` now allows the user to specify the
      month for which to get relevant options data.
    + ``Options.get_forward_data`` now has optional kwargs ``near`` and
      ``above_below``. This allows the user to specify if they would like to
      only return forward looking data for options near the current stock
      price. This just obtains the data from Options.get_near_stock_price
      instead of Options.get_xxx_data() (:issue:`2758`).

  - Cursor coordinate information is now displayed in time-series plots.

  - added option ``display.max_seq_items`` to control the number of
    elements printed per sequence pprinting it.  (:issue:`2979`)

  - added option ``display.chop_threshold`` to control display of small numerical
    values. (:issue:`2739`)

  - added option ``display.max_info_rows`` to prevent verbose_info from being
    calculated for frames above 1M rows (configurable). (:issue:`2807`, :issue:`2918`)

  - value_counts() now accepts a "normalize" argument, for normalized
    histograms. (:issue:`2710`).

  - DataFrame.from_records now accepts not only dicts but any instance of
    the collections.Mapping ABC.

  - added option ``display.mpl_style`` providing a sleeker visual style
    for plots. Based on https://gist.github.com/huyng/816622 (:issue:`3075`).

  - Treat boolean values as integers (values 1 and 0) for numeric
    operations. (:issue:`2641`)

  - to_html() now accepts an optional "escape" argument to control reserved
    HTML character escaping (enabled by default) and escapes ``&``, in addition
    to ``<`` and ``>``.  (:issue:`2919`)

See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.


.. _whatsnew_0.11.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.10.1..v0.11.0
.. _whatsnew_0252:

What's new in 0.25.2 (October 15, 2019)
---------------------------------------

These are the changes in pandas 0.25.2. See :ref:`release` for a full changelog
including other versions of pandas.

.. note::

    pandas 0.25.2 adds compatibility for Python 3.8 (:issue:`28147`).

.. _whatsnew_0252.bug_fixes:

Bug fixes
~~~~~~~~~

Indexing
^^^^^^^^

- Fix regression in :meth:`DataFrame.reindex` not following the ``limit`` argument (:issue:`28631`).
- Fix regression in :meth:`RangeIndex.get_indexer` for decreasing :class:`RangeIndex` where target values may be improperly identified as missing/present (:issue:`28678`)

IO
^^

- Fix regression in notebook display where ``<th>`` tags were missing for :attr:`DataFrame.index` values (:issue:`28204`).
- Regression in :meth:`~DataFrame.to_csv` where writing a :class:`Series` or :class:`DataFrame` indexed by an :class:`IntervalIndex` would incorrectly raise a ``TypeError`` (:issue:`28210`)
- Fix :meth:`~DataFrame.to_csv` with ``ExtensionArray`` with list-like values (:issue:`28840`).

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug incorrectly raising an ``IndexError`` when passing a list of quantiles to :meth:`pandas.core.groupby.DataFrameGroupBy.quantile` (:issue:`28113`).
- Bug in :meth:`pandas.core.groupby.GroupBy.shift`, :meth:`pandas.core.groupby.GroupBy.bfill` and :meth:`pandas.core.groupby.GroupBy.ffill` where timezone information would be dropped (:issue:`19995`, :issue:`27992`)

Other
^^^^^

- Compatibility with Python 3.8 in :meth:`DataFrame.query` (:issue:`27261`)
- Fix to ensure that tab-completion in an IPython console does not raise
  warnings for deprecated attributes (:issue:`27900`).

.. _whatsnew_0.252.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.25.1..v0.25.2
.. _whatsnew_101:

What's new in 1.0.1 (February 5, 2020)
--------------------------------------

These are the changes in pandas 1.0.1. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_101.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :class:`DataFrame` setting values with a slice (e.g. ``df[-4:] = 1``) indexing by label instead of position (:issue:`31469`)
- Fixed regression when indexing a ``Series`` or ``DataFrame`` indexed by ``DatetimeIndex`` with a slice containing a :class:`datetime.date` (:issue:`31501`)
- Fixed regression in ``DataFrame.__setitem__`` raising an ``AttributeError`` with a :class:`MultiIndex` and a non-monotonic indexer (:issue:`31449`)
- Fixed regression in :class:`Series` multiplication when multiplying a numeric :class:`Series` with >10000 elements with a timedelta-like scalar (:issue:`31457`)
- Fixed regression in ``.groupby().agg()`` raising an ``AssertionError`` for some reductions like ``min`` on object-dtype columns (:issue:`31522`)
- Fixed regression in ``.groupby()`` aggregations with categorical dtype using Cythonized reduction functions (e.g. ``first``) (:issue:`31450`)
- Fixed regression in :meth:`GroupBy.apply` if called with a function which returned a non-pandas non-scalar object (e.g. a list or numpy array) (:issue:`31441`)
- Fixed regression in :meth:`DataFrame.groupby` whereby taking the minimum or maximum of a column with period dtype would raise a ``TypeError``. (:issue:`31471`)
- Fixed regression in :meth:`DataFrame.groupby` with an empty DataFrame grouping by a level of a MultiIndex (:issue:`31670`).
- Fixed regression in :meth:`DataFrame.apply` with object dtype and non-reducing function (:issue:`31505`)
- Fixed regression in :meth:`to_datetime` when parsing non-nanosecond resolution datetimes (:issue:`31491`)
- Fixed regression in :meth:`~DataFrame.to_csv` where specifying an ``na_rep`` might truncate the values written (:issue:`31447`)
- Fixed regression in :class:`Categorical` construction with ``numpy.str_`` categories (:issue:`31499`)
- Fixed regression in :meth:`DataFrame.loc` and :meth:`DataFrame.iloc` when selecting a row containing a single ``datetime64`` or ``timedelta64`` column (:issue:`31649`)
- Fixed regression where setting :attr:`pd.options.display.max_colwidth` was not accepting negative integer. In addition, this behavior has been deprecated in favor of using ``None`` (:issue:`31532`)
- Fixed regression in objTOJSON.c fix return-type warning (:issue:`31463`)
- Fixed regression in :meth:`qcut` when passed a nullable integer. (:issue:`31389`)
- Fixed regression in assigning to a :class:`Series` using a nullable integer dtype (:issue:`31446`)
- Fixed performance regression when indexing a ``DataFrame`` or ``Series`` with a :class:`MultiIndex` for the index using a list of labels (:issue:`31648`)
- Fixed regression in :meth:`read_csv` used in file like object ``RawIOBase`` is not recognize ``encoding`` option (:issue:`31575`)

.. ---------------------------------------------------------------------------

.. _whatsnew_101.deprecations:

Deprecations
~~~~~~~~~~~~

- Support for negative integer for :attr:`pd.options.display.max_colwidth` is deprecated in favor of using ``None`` (:issue:`31532`)

.. ---------------------------------------------------------------------------

.. _whatsnew_101.bug_fixes:

Bug fixes
~~~~~~~~~

**Datetimelike**

- Fixed bug in :meth:`to_datetime` raising when ``cache=True`` and out-of-bound values are present (:issue:`31491`)

**Numeric**

- Bug in dtypes being lost in ``DataFrame.__invert__`` (``~`` operator) with mixed dtypes (:issue:`31183`)
  and for extension-array backed ``Series`` and ``DataFrame`` (:issue:`23087`)

**Plotting**

- Plotting tz-aware timeseries no longer gives UserWarning (:issue:`31205`)

**Interval**

- Bug in :meth:`Series.shift` with ``interval`` dtype raising a ``TypeError`` when shifting an interval array of integers or datetimes (:issue:`34195`)

.. ---------------------------------------------------------------------------

.. _whatsnew_101.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.0..v1.0.1|HEAD
.. _whatsnew_100:

What's new in 1.0.0 (January 29, 2020)
--------------------------------------

These are the changes in pandas 1.0.0. See :ref:`release` for a full changelog
including other versions of pandas.

.. note::

    The pandas 1.0 release removed a lot of functionality that was deprecated
    in previous releases (see :ref:`below <whatsnew_100.prior_deprecations>`
    for an overview). It is recommended to first upgrade to pandas 0.25 and to
    ensure your code is working without warnings, before upgrading to pandas
    1.0.


New deprecation policy
~~~~~~~~~~~~~~~~~~~~~~

Starting with pandas 1.0.0, pandas will adopt a variant of `SemVer`_ to
version releases. Briefly,

* Deprecations will be introduced in minor releases (e.g. 1.1.0, 1.2.0, 2.1.0, ...)
* Deprecations will be enforced in major releases (e.g. 1.0.0, 2.0.0, 3.0.0, ...)
* API-breaking changes will be made only in major releases (except for experimental features)

See :ref:`policies.version` for more.

.. _2019 Pandas User Survey: https://pandas.pydata.org/community/blog/2019-user-survey.html
.. _SemVer: https://semver.org

{{ header }}

.. ---------------------------------------------------------------------------

Enhancements
~~~~~~~~~~~~

.. _whatsnew_100.numba_rolling_apply:

Using Numba in ``rolling.apply`` and ``expanding.apply``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added an ``engine`` keyword to :meth:`~core.window.rolling.Rolling.apply` and :meth:`~core.window.expanding.Expanding.apply`
that allows the user to execute the routine using `Numba <https://numba.pydata.org/>`__ instead of Cython.
Using the Numba engine can yield significant performance gains if the apply function can operate on numpy arrays and
the data set is larger (1 million rows or greater). For more details, see
:ref:`rolling apply documentation <window.numba_engine>` (:issue:`28987`, :issue:`30936`)

.. _whatsnew_100.custom_window:

Defining custom windows for rolling operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added a :func:`pandas.api.indexers.BaseIndexer` class that allows users to define how
window bounds are created during ``rolling`` operations. Users can define their own ``get_window_bounds``
method on a :func:`pandas.api.indexers.BaseIndexer` subclass that will generate the start and end
indices used for each window during the rolling aggregation. For more details and example usage, see
the :ref:`custom window rolling documentation <window.custom_rolling_window>`

.. _whatsnew_100.to_markdown:

Converting to markdown
^^^^^^^^^^^^^^^^^^^^^^

We've added :meth:`~DataFrame.to_markdown` for creating a markdown table (:issue:`11052`)

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2, 3], "B": [1, 2, 3]}, index=['a', 'a', 'b'])
   print(df.to_markdown())

Experimental new features
~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_100.NA:

Experimental ``NA`` scalar to denote missing values
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A new ``pd.NA`` value (singleton) is introduced to represent scalar missing
values. Up to now, pandas used several values to represent missing data: ``np.nan`` is used for this for float data, ``np.nan`` or
``None`` for object-dtype data and ``pd.NaT`` for datetime-like data. The
goal of ``pd.NA`` is to provide a "missing" indicator that can be used
consistently across data types. ``pd.NA`` is currently used by the nullable integer and boolean
data types and the new string data type (:issue:`28095`).

.. warning::

   Experimental: the behaviour of ``pd.NA`` can still change without warning.

For example, creating a Series using the nullable integer dtype:

.. ipython:: python

    s = pd.Series([1, 2, None], dtype="Int64")
    s
    s[2]

Compared to ``np.nan``, ``pd.NA`` behaves differently in certain operations.
In addition to arithmetic operations, ``pd.NA`` also propagates as "missing"
or "unknown" in comparison operations:

.. ipython:: python

    np.nan > 1
    pd.NA > 1

For logical operations, ``pd.NA`` follows the rules of the
`three-valued logic <https://en.wikipedia.org/wiki/Three-valued_logic>`__ (or
*Kleene logic*). For example:

.. ipython:: python

    pd.NA | True

For more, see :ref:`NA section <missing_data.NA>` in the user guide on missing
data.


.. _whatsnew_100.string:

Dedicated string data type
^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added :class:`StringDtype`, an extension type dedicated to string data.
Previously, strings were typically stored in object-dtype NumPy arrays. (:issue:`29975`)

.. warning::

   ``StringDtype`` is currently considered experimental. The implementation
   and parts of the API may change without warning.

The ``'string'`` extension type solves several issues with object-dtype NumPy arrays:

1. You can accidentally store a *mixture* of strings and non-strings in an
   ``object`` dtype array. A ``StringArray`` can only store strings.
2. ``object`` dtype breaks dtype-specific operations like :meth:`DataFrame.select_dtypes`.
   There isn't a clear way to select *just* text while excluding non-text,
   but still object-dtype columns.
3. When reading code, the contents of an ``object`` dtype array is less clear
   than ``string``.


.. ipython:: python

   pd.Series(['abc', None, 'def'], dtype=pd.StringDtype())

You can use the alias ``"string"`` as well.

.. ipython:: python

   s = pd.Series(['abc', None, 'def'], dtype="string")
   s

The usual string accessor methods work. Where appropriate, the return type
of the Series or columns of a DataFrame will also have string dtype.

.. ipython:: python

   s.str.upper()
   s.str.split('b', expand=True).dtypes

String accessor methods returning integers will return a value with :class:`Int64Dtype`

.. ipython:: python

   s.str.count("a")

We recommend explicitly using the ``string`` data type when working with strings.
See :ref:`text.types` for more.

.. _whatsnew_100.boolean:

Boolean data type with missing values support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added :class:`BooleanDtype` / :class:`~arrays.BooleanArray`, an extension
type dedicated to boolean data that can hold missing values. The default
``bool`` data type based on a bool-dtype NumPy array, the column can only hold
``True`` or ``False``, and not missing values. This new :class:`~arrays.BooleanArray`
can store missing values as well by keeping track of this in a separate mask.
(:issue:`29555`, :issue:`30095`, :issue:`31131`)

.. ipython:: python

   pd.Series([True, False, None], dtype=pd.BooleanDtype())

You can use the alias ``"boolean"`` as well.

.. ipython:: python

   s = pd.Series([True, False, None], dtype="boolean")
   s

.. _whatsnew_100.convert_dtypes:

Method ``convert_dtypes`` to ease use of supported extension dtypes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order to encourage use of the extension dtypes ``StringDtype``,
``BooleanDtype``, ``Int64Dtype``, ``Int32Dtype``, etc., that support ``pd.NA``, the
methods :meth:`DataFrame.convert_dtypes` and :meth:`Series.convert_dtypes`
have been introduced. (:issue:`29752`) (:issue:`30929`)

Example:

.. ipython:: python

   df = pd.DataFrame({'x': ['abc', None, 'def'],
                      'y': [1, 2, np.nan],
                      'z': [True, False, True]})
   df
   df.dtypes

.. ipython:: python

   converted = df.convert_dtypes()
   converted
   converted.dtypes

This is especially useful after reading in data using readers such as :func:`read_csv`
and :func:`read_excel`.
See :ref:`here <missing_data.NA.conversion>` for a description.


.. _whatsnew_100.enhancements.other:

Other enhancements
~~~~~~~~~~~~~~~~~~

- :meth:`DataFrame.to_string` added the ``max_colwidth`` parameter to control when wide columns are truncated (:issue:`9784`)
- Added the ``na_value`` argument to :meth:`Series.to_numpy`, :meth:`Index.to_numpy` and :meth:`DataFrame.to_numpy` to control the value used for missing data (:issue:`30322`)
- :meth:`MultiIndex.from_product` infers level names from inputs if not explicitly provided (:issue:`27292`)
- :meth:`DataFrame.to_latex` now accepts ``caption`` and ``label`` arguments (:issue:`25436`)
- DataFrames with :ref:`nullable integer <integer_na>`, the :ref:`new string dtype <text.types>`
  and period data type can now be converted to ``pyarrow`` (>=0.15.0), which means that it is
  supported in writing to the Parquet file format when using the ``pyarrow`` engine (:issue:`28368`).
  Full roundtrip to parquet (writing and reading back in with :meth:`~DataFrame.to_parquet` / :func:`read_parquet`)
  is supported starting with pyarrow >= 0.16 (:issue:`20612`).
- :func:`to_parquet` now appropriately handles the ``schema`` argument for user defined schemas in the pyarrow engine. (:issue:`30270`)
- :meth:`DataFrame.to_json` now accepts an ``indent`` integer argument to enable pretty printing of JSON output (:issue:`12004`)
- :meth:`read_stata` can read Stata 119 dta files. (:issue:`28250`)
- Implemented :meth:`pandas.core.window.Window.var` and :meth:`pandas.core.window.Window.std` functions (:issue:`26597`)
- Added ``encoding`` argument to :meth:`DataFrame.to_string` for non-ascii text (:issue:`28766`)
- Added ``encoding`` argument to :func:`DataFrame.to_html` for non-ascii text (:issue:`28663`)
- :meth:`Styler.background_gradient` now accepts ``vmin`` and ``vmax`` arguments (:issue:`12145`)
- :meth:`Styler.format` added the ``na_rep`` parameter to help format the missing values (:issue:`21527`, :issue:`28358`)
- :func:`read_excel` now can read binary Excel (``.xlsb``) files by passing ``engine='pyxlsb'``. For more details and example usage, see the :ref:`Binary Excel files documentation <io.xlsb>`. Closes :issue:`8540`.
- The ``partition_cols`` argument in :meth:`DataFrame.to_parquet` now accepts a string (:issue:`27117`)
- :func:`pandas.read_json` now parses ``NaN``, ``Infinity`` and ``-Infinity`` (:issue:`12213`)
- DataFrame constructor preserve ``ExtensionArray`` dtype with ``ExtensionArray`` (:issue:`11363`)
- :meth:`DataFrame.sort_values` and :meth:`Series.sort_values` have gained ``ignore_index`` keyword to be able to reset index after sorting (:issue:`30114`)
- :meth:`DataFrame.sort_index` and :meth:`Series.sort_index` have gained ``ignore_index`` keyword to reset index (:issue:`30114`)
- :meth:`DataFrame.drop_duplicates` has gained ``ignore_index`` keyword to reset index (:issue:`30114`)
- Added new writer for exporting Stata dta files in versions 118 and 119, ``StataWriterUTF8``.  These files formats support exporting strings containing Unicode characters. Format 119 supports data sets with more than 32,767 variables (:issue:`23573`, :issue:`30959`)
- :meth:`Series.map` now accepts ``collections.abc.Mapping`` subclasses as a mapper (:issue:`29733`)
- Added an experimental :attr:`~DataFrame.attrs` for storing global metadata about a dataset (:issue:`29062`)
- :meth:`Timestamp.fromisocalendar` is now compatible with python 3.8 and above (:issue:`28115`)
- :meth:`DataFrame.to_pickle` and :func:`read_pickle` now accept URL (:issue:`30163`)



.. ---------------------------------------------------------------------------

.. _whatsnew_100.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_100.api_breaking.MultiIndex._names:

Avoid using names from ``MultiIndex.levels``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As part of a larger refactor to :class:`MultiIndex` the level names are now
stored separately from the levels (:issue:`27242`). We recommend using
:attr:`MultiIndex.names` to access the names, and :meth:`Index.set_names`
to update the names.

For backwards compatibility, you can still *access* the names via the levels.

.. ipython:: python

   mi = pd.MultiIndex.from_product([[1, 2], ['a', 'b']], names=['x', 'y'])
   mi.levels[0].name

However, it is no longer possible to *update* the names of the ``MultiIndex``
via the level.

.. ipython:: python
   :okexcept:

   mi.levels[0].name = "new name"
   mi.names

To update, use ``MultiIndex.set_names``, which returns a new ``MultiIndex``.

.. ipython:: python

   mi2 = mi.set_names("new name", level=0)
   mi2.names

New repr for :class:`~pandas.arrays.IntervalArray`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`pandas.arrays.IntervalArray` adopts a new ``__repr__`` in accordance with other array classes (:issue:`25022`)

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: pd.arrays.IntervalArray.from_tuples([(0, 1), (2, 3)])
   Out[2]:
   IntervalArray([(0, 1], (2, 3]],
                 closed='right',
                 dtype='interval[int64]')

*pandas 1.0.0*

.. ipython:: python

   pd.arrays.IntervalArray.from_tuples([(0, 1), (2, 3)])

``DataFrame.rename`` now only accepts one positional argument
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.rename` would previously accept positional arguments that would lead
to ambiguous or undefined behavior. From pandas 1.0, only the very first argument, which
maps labels to their new names along the default axis, is allowed to be passed by position
(:issue:`29136`).

.. ipython:: python
   :suppress:

   df = pd.DataFrame([[1]])

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: df = pd.DataFrame([[1]])
   In [2]: df.rename({0: 1}, {0: 2})
   Out[2]:
   FutureWarning: ...Use named arguments to resolve ambiguity...
      2
   1  1

*pandas 1.0.0*

.. code-block:: ipython

   In [3]: df.rename({0: 1}, {0: 2})
   Traceback (most recent call last):
   ...
   TypeError: rename() takes from 1 to 2 positional arguments but 3 were given

Note that errors will now be raised when conflicting or potentially ambiguous arguments are provided.

*pandas 0.25.x*

.. code-block:: ipython

   In [4]: df.rename({0: 1}, index={0: 2})
   Out[4]:
      0
   1  1

   In [5]: df.rename(mapper={0: 1}, index={0: 2})
   Out[5]:
      0
   2  1

*pandas 1.0.0*

.. code-block:: ipython

   In [6]: df.rename({0: 1}, index={0: 2})
   Traceback (most recent call last):
   ...
   TypeError: Cannot specify both 'mapper' and any of 'index' or 'columns'

   In [7]: df.rename(mapper={0: 1}, index={0: 2})
   Traceback (most recent call last):
   ...
   TypeError: Cannot specify both 'mapper' and any of 'index' or 'columns'

You can still change the axis along which the first positional argument is applied by
supplying the ``axis`` keyword argument.

.. ipython:: python

   df.rename({0: 1})
   df.rename({0: 1}, axis=1)

If you would like to update both the index and column labels, be sure to use the respective
keywords.

.. ipython:: python

   df.rename(index={0: 1}, columns={0: 2})

Extended verbose info output for :class:`~pandas.DataFrame`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.info` now shows line numbers for the columns summary (:issue:`17304`)

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: df = pd.DataFrame({"int_col": [1, 2, 3],
   ...                    "text_col": ["a", "b", "c"],
   ...                    "float_col": [0.0, 0.1, 0.2]})
   In [2]: df.info(verbose=True)
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 3 entries, 0 to 2
   Data columns (total 3 columns):
   int_col      3 non-null int64
   text_col     3 non-null object
   float_col    3 non-null float64
   dtypes: float64(1), int64(1), object(1)
   memory usage: 152.0+ bytes

*pandas 1.0.0*

.. ipython:: python

   df = pd.DataFrame({"int_col": [1, 2, 3],
                      "text_col": ["a", "b", "c"],
                      "float_col": [0.0, 0.1, 0.2]})
   df.info(verbose=True)

:meth:`pandas.array` inference changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`pandas.array` now infers pandas' new extension types in several cases (:issue:`29791`):

1. String data (including missing values) now returns a :class:`arrays.StringArray`.
2. Integer data (including missing values) now returns a :class:`arrays.IntegerArray`.
3. Boolean data (including missing values) now returns the new :class:`arrays.BooleanArray`

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: pd.array(["a", None])
   Out[1]:
   <PandasArray>
   ['a', None]
   Length: 2, dtype: object

   In [2]: pd.array([1, None])
   Out[2]:
   <PandasArray>
   [1, None]
   Length: 2, dtype: object


*pandas 1.0.0*

.. ipython:: python

   pd.array(["a", None])
   pd.array([1, None])

As a reminder, you can specify the ``dtype`` to disable all inference.

:class:`arrays.IntegerArray` now uses :attr:`pandas.NA`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`arrays.IntegerArray` now uses :attr:`pandas.NA` rather than
:attr:`numpy.nan` as its missing value marker (:issue:`29964`).

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: a = pd.array([1, 2, None], dtype="Int64")
   In [2]: a
   Out[2]:
   <IntegerArray>
   [1, 2, NaN]
   Length: 3, dtype: Int64

   In [3]: a[2]
   Out[3]:
   nan

*pandas 1.0.0*

.. ipython:: python

   a = pd.array([1, 2, None], dtype="Int64")
   a
   a[2]

This has a few API-breaking consequences.

**Converting to a NumPy ndarray**

When converting to a NumPy array missing values will be ``pd.NA``, which cannot
be converted to a float. So calling ``np.asarray(integer_array, dtype="float")``
will now raise.

*pandas 0.25.x*

.. code-block:: ipython

    In [1]: np.asarray(a, dtype="float")
    Out[1]:
    array([ 1.,  2., nan])

*pandas 1.0.0*

.. ipython:: python
   :okexcept:

   np.asarray(a, dtype="float")

Use :meth:`arrays.IntegerArray.to_numpy` with an explicit ``na_value`` instead.

.. ipython:: python

   a.to_numpy(dtype="float", na_value=np.nan)

**Reductions can return ``pd.NA``**

When performing a reduction such as a sum with ``skipna=False``, the result
will now be ``pd.NA`` instead of ``np.nan`` in presence of missing values
(:issue:`30958`).

*pandas 0.25.x*

.. code-block:: ipython

    In [1]: pd.Series(a).sum(skipna=False)
    Out[1]:
    nan

*pandas 1.0.0*

.. ipython:: python

   pd.Series(a).sum(skipna=False)

**value_counts returns a nullable integer dtype**

:meth:`Series.value_counts` with a nullable integer dtype now returns a nullable
integer dtype for the values.

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: pd.Series([2, 1, 1, None], dtype="Int64").value_counts().dtype
   Out[1]:
   dtype('int64')

*pandas 1.0.0*

.. ipython:: python

   pd.Series([2, 1, 1, None], dtype="Int64").value_counts().dtype

See :ref:`missing_data.NA` for more on the differences between :attr:`pandas.NA`
and :attr:`numpy.nan`.

:class:`arrays.IntegerArray` comparisons return :class:`arrays.BooleanArray`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Comparison operations on a :class:`arrays.IntegerArray` now returns a
:class:`arrays.BooleanArray` rather than a NumPy array (:issue:`29964`).

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: a = pd.array([1, 2, None], dtype="Int64")
   In [2]: a
   Out[2]:
   <IntegerArray>
   [1, 2, NaN]
   Length: 3, dtype: Int64

   In [3]: a > 1
   Out[3]:
   array([False,  True, False])

*pandas 1.0.0*

.. ipython:: python

   a = pd.array([1, 2, None], dtype="Int64")
   a > 1

Note that missing values now propagate, rather than always comparing unequal
like :attr:`numpy.nan`. See :ref:`missing_data.NA` for more.

By default :meth:`Categorical.min` now returns the minimum instead of np.nan
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When :class:`Categorical` contains ``np.nan``,
:meth:`Categorical.min` no longer return ``np.nan`` by default (skipna=True) (:issue:`25303`)

*pandas 0.25.x*

.. code-block:: ipython

   In [1]: pd.Categorical([1, 2, np.nan], ordered=True).min()
   Out[1]: nan


*pandas 1.0.0*

.. ipython:: python

   pd.Categorical([1, 2, np.nan], ordered=True).min()


Default dtype of empty :class:`pandas.Series`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Initialising an empty :class:`pandas.Series` without specifying a dtype will raise a ``DeprecationWarning`` now
(:issue:`17261`). The default dtype will change from ``float64`` to ``object`` in future releases so that it is
consistent with the behaviour of :class:`DataFrame` and :class:`Index`.

*pandas 1.0.0*

.. code-block:: ipython

   In [1]: pd.Series()
   Out[2]:
   DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.
   Series([], dtype: float64)

Result dtype inference changes for resample operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The rules for the result dtype in :meth:`DataFrame.resample` aggregations have changed for extension types (:issue:`31359`).
Previously, pandas would attempt to convert the result back to the original dtype, falling back to the usual
inference rules if that was not possible. Now, pandas will only return a result of the original dtype if the
scalar values in the result are instances of the extension dtype's scalar type.

.. ipython:: python

   df = pd.DataFrame({"A": ['a', 'b']}, dtype='category',
                     index=pd.date_range('2000', periods=2))
   df


*pandas 0.25.x*

.. code-block:: ipython

   In [1]> df.resample("2D").agg(lambda x: 'a').A.dtype
   Out[1]:
   CategoricalDtype(categories=['a', 'b'], ordered=False)

*pandas 1.0.0*

.. ipython:: python

   df.resample("2D").agg(lambda x: 'a').A.dtype

This fixes an inconsistency between ``resample`` and ``groupby``.
This also fixes a potential bug, where the **values** of the result might change
depending on how the results are cast back to the original dtype.

*pandas 0.25.x*

.. code-block:: ipython

   In [1] df.resample("2D").agg(lambda x: 'c')
   Out[1]:

        A
   0  NaN

*pandas 1.0.0*

.. ipython:: python

   df.resample("2D").agg(lambda x: 'c')


.. _whatsnew_100.api_breaking.python:

Increased minimum version for Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.0.0 supports Python 3.6.1 and higher (:issue:`29212`).

.. _whatsnew_100.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some minimum supported versions of dependencies were updated (:issue:`29766`, :issue:`29723`).
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| numpy           | 1.13.3          |    X     |         |
+-----------------+-----------------+----------+---------+
| pytz            | 2015.4          |    X     |         |
+-----------------+-----------------+----------+---------+
| python-dateutil | 2.6.1           |    X     |         |
+-----------------+-----------------+----------+---------+
| bottleneck      | 1.2.1           |          |         |
+-----------------+-----------------+----------+---------+
| numexpr         | 2.6.2           |          |         |
+-----------------+-----------------+----------+---------+
| pytest (dev)    | 4.0.2           |          |         |
+-----------------+-----------------+----------+---------+

For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
| beautifulsoup4  | 4.6.0           |         |
+-----------------+-----------------+---------+
| fastparquet     | 0.3.2           |    X    |
+-----------------+-----------------+---------+
| gcsfs           | 0.2.2           |         |
+-----------------+-----------------+---------+
| lxml            | 3.8.0           |         |
+-----------------+-----------------+---------+
| matplotlib      | 2.2.2           |         |
+-----------------+-----------------+---------+
| numba           | 0.46.0          |    X    |
+-----------------+-----------------+---------+
| openpyxl        | 2.5.7           |    X    |
+-----------------+-----------------+---------+
| pyarrow         | 0.13.0          |    X    |
+-----------------+-----------------+---------+
| pymysql         | 0.7.1           |         |
+-----------------+-----------------+---------+
| pytables        | 3.4.2           |         |
+-----------------+-----------------+---------+
| s3fs            | 0.3.0           |    X    |
+-----------------+-----------------+---------+
| scipy           | 0.19.0          |         |
+-----------------+-----------------+---------+
| sqlalchemy      | 1.1.4           |         |
+-----------------+-----------------+---------+
| xarray          | 0.8.2           |         |
+-----------------+-----------------+---------+
| xlrd            | 1.1.0           |         |
+-----------------+-----------------+---------+
| xlsxwriter      | 0.9.8           |         |
+-----------------+-----------------+---------+
| xlwt            | 1.2.0           |         |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

Build changes
^^^^^^^^^^^^^

pandas has added a `pyproject.toml <https://www.python.org/dev/peps/pep-0517/>`_ file and will no longer include
cythonized files in the source distribution uploaded to PyPI (:issue:`28341`, :issue:`20775`). If you're installing
a built distribution (wheel) or via conda, this shouldn't have any effect on you. If you're building pandas from
source, you should no longer need to install Cython into your build environment before calling ``pip install pandas``.


.. _whatsnew_100.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- :class:`core.groupby.GroupBy.transform` now raises on invalid operation names (:issue:`27489`)
- :meth:`pandas.api.types.infer_dtype` will now return "integer-na" for integer and ``np.nan`` mix (:issue:`27283`)
- :meth:`MultiIndex.from_arrays` will no longer infer names from arrays if ``names=None`` is explicitly provided (:issue:`27292`)
- In order to improve tab-completion, pandas does not include most deprecated attributes when introspecting a pandas object using ``dir`` (e.g. ``dir(df)``).
  To see which attributes are excluded, see an object's ``_deprecations`` attribute, for example ``pd.DataFrame._deprecations`` (:issue:`28805`).
- The returned dtype of :func:`unique` now matches the input dtype. (:issue:`27874`)
- Changed the default configuration value for ``options.matplotlib.register_converters`` from ``True`` to ``"auto"`` (:issue:`18720`).
  Now, pandas custom formatters will only be applied to plots created by pandas, through :meth:`~DataFrame.plot`.
  Previously, pandas' formatters would be applied to all plots created *after* a :meth:`~DataFrame.plot`.
  See :ref:`units registration <whatsnew_100.matplotlib_units>` for more.
- :meth:`Series.dropna` has dropped its ``**kwargs`` argument in favor of a single ``how`` parameter.
  Supplying anything else than ``how`` to ``**kwargs`` raised a ``TypeError`` previously (:issue:`29388`)
- When testing pandas, the new minimum required version of pytest is 5.0.1 (:issue:`29664`)
- :meth:`Series.str.__iter__` was deprecated and will be removed in future releases (:issue:`28277`).
- Added ``<NA>`` to the list of default NA values for :meth:`read_csv` (:issue:`30821`)

.. _whatsnew_100.api.documentation:

Documentation improvements
^^^^^^^^^^^^^^^^^^^^^^^^^^

- Added new section on :ref:`scale` (:issue:`28315`).
- Added sub-section on :ref:`io.query_multi` for HDF5 datasets (:issue:`28791`).

.. ---------------------------------------------------------------------------

.. _whatsnew_100.deprecations:

Deprecations
~~~~~~~~~~~~

- :meth:`Series.item` and :meth:`Index.item` have been _undeprecated_ (:issue:`29250`)
- ``Index.set_value`` has been deprecated. For a given index ``idx``, array ``arr``,
  value in ``idx`` of ``idx_val`` and a new value of ``val``, ``idx.set_value(arr, idx_val, val)``
  is equivalent to ``arr[idx.get_loc(idx_val)] = val``, which should be used instead (:issue:`28621`).
- :func:`is_extension_type` is deprecated, :func:`is_extension_array_dtype` should be used instead (:issue:`29457`)
- :func:`eval` keyword argument "truediv" is deprecated and will be removed in a future version (:issue:`29812`)
- :meth:`DateOffset.isAnchored` and :meth:`DatetOffset.onOffset` are deprecated and will be removed in a future version, use :meth:`DateOffset.is_anchored` and :meth:`DateOffset.is_on_offset` instead (:issue:`30340`)
- ``pandas.tseries.frequencies.get_offset`` is deprecated and will be removed in a future version, use ``pandas.tseries.frequencies.to_offset`` instead (:issue:`4205`)
- :meth:`Categorical.take_nd` and :meth:`CategoricalIndex.take_nd` are deprecated, use :meth:`Categorical.take` and :meth:`CategoricalIndex.take` instead (:issue:`27745`)
- The parameter ``numeric_only`` of :meth:`Categorical.min` and :meth:`Categorical.max` is deprecated and replaced with ``skipna`` (:issue:`25303`)
- The parameter ``label`` in :func:`lreshape` has been deprecated and will be removed in a future version (:issue:`29742`)
- ``pandas.core.index`` has been deprecated and will be removed in a future version, the public classes are available in the top-level namespace (:issue:`19711`)
- :func:`pandas.json_normalize` is now exposed in the top-level namespace.
  Usage of ``json_normalize`` as ``pandas.io.json.json_normalize`` is now deprecated and
  it is recommended to use ``json_normalize`` as :func:`pandas.json_normalize` instead (:issue:`27586`).
- The ``numpy`` argument of :meth:`pandas.read_json` is deprecated (:issue:`28512`).
- :meth:`DataFrame.to_stata`, :meth:`DataFrame.to_feather`, and :meth:`DataFrame.to_parquet` argument "fname" is deprecated, use "path" instead (:issue:`23574`)
- The deprecated internal attributes ``_start``, ``_stop`` and ``_step`` of :class:`RangeIndex` now raise a ``FutureWarning`` instead of a ``DeprecationWarning`` (:issue:`26581`)
- The ``pandas.util.testing`` module has been deprecated. Use the public API in ``pandas.testing`` documented at :ref:`api.general.testing` (:issue:`16232`).
- ``pandas.SparseArray`` has been deprecated.  Use ``pandas.arrays.SparseArray`` (:class:`arrays.SparseArray`) instead. (:issue:`30642`)
- The parameter ``is_copy`` of :meth:`Series.take` and :meth:`DataFrame.take` has been deprecated and will be removed in a future version. (:issue:`27357`)
- Support for multi-dimensional indexing (e.g. ``index[:, None]``) on a :class:`Index` is deprecated and will be removed in a future version, convert to a numpy array before indexing instead (:issue:`30588`)
- The ``pandas.np`` submodule is now deprecated. Import numpy directly instead (:issue:`30296`)
- The ``pandas.datetime`` class is now deprecated. Import from ``datetime`` instead (:issue:`30610`)
- :class:`~DataFrame.diff` will raise a ``TypeError`` rather than implicitly losing the dtype of extension types in the future. Convert to the correct dtype before calling ``diff`` instead (:issue:`31025`)

**Selecting Columns from a Grouped DataFrame**

When selecting columns from a :class:`DataFrameGroupBy` object, passing individual keys (or a tuple of keys) inside single brackets is deprecated,
a list of items should be used instead. (:issue:`23566`) For example:

.. code-block:: ipython

    df = pd.DataFrame({
        "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
        "B": np.random.randn(8),
        "C": np.random.randn(8),
    })
    g = df.groupby('A')

    # single key, returns SeriesGroupBy
    g['B']

    # tuple of single key, returns SeriesGroupBy
    g[('B',)]

    # tuple of multiple keys, returns DataFrameGroupBy, raises FutureWarning
    g[('B', 'C')]

    # multiple keys passed directly, returns DataFrameGroupBy, raises FutureWarning
    # (implicitly converts the passed strings into a single tuple)
    g['B', 'C']

    # proper way, returns DataFrameGroupBy
    g[['B', 'C']]

.. ---------------------------------------------------------------------------

.. _whatsnew_100.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Removed SparseSeries and SparseDataFrame**

``SparseSeries``, ``SparseDataFrame`` and the ``DataFrame.to_sparse`` method
have been removed (:issue:`28425`). We recommend using a ``Series`` or
``DataFrame`` with sparse values instead. See :ref:`sparse.migration` for help
with migrating existing code.

.. _whatsnew_100.matplotlib_units:

**Matplotlib unit registration**

Previously, pandas would register converters with matplotlib as a side effect of importing pandas (:issue:`18720`).
This changed the output of plots made via matplotlib plots after pandas was imported, even if you were using
matplotlib directly rather than :meth:`~DataFrame.plot`.

To use pandas formatters with a matplotlib plot, specify

.. code-block:: ipython

   In [1]: import pandas as pd
   In [2]: pd.options.plotting.matplotlib.register_converters = True

Note that plots created by :meth:`DataFrame.plot` and :meth:`Series.plot` *do* register the converters
automatically. The only behavior change is when plotting a date-like object via ``matplotlib.pyplot.plot``
or ``matplotlib.Axes.plot``. See :ref:`plotting.formatters` for more.

**Other removals**

- Removed the previously deprecated keyword "index" from :func:`read_stata`, :class:`StataReader`, and :meth:`StataReader.read`, use "index_col" instead (:issue:`17328`)
- Removed ``StataReader.data`` method, use :meth:`StataReader.read` instead (:issue:`9493`)
- Removed ``pandas.plotting._matplotlib.tsplot``, use :meth:`Series.plot` instead (:issue:`19980`)
- ``pandas.tseries.converter.register`` has been moved to :func:`pandas.plotting.register_matplotlib_converters` (:issue:`18307`)
- :meth:`Series.plot` no longer accepts positional arguments, pass keyword arguments instead (:issue:`30003`)
- :meth:`DataFrame.hist` and :meth:`Series.hist` no longer allows ``figsize="default"``, specify figure size by passinig a tuple instead (:issue:`30003`)
- Floordiv of integer-dtyped array by :class:`Timedelta` now raises ``TypeError`` (:issue:`21036`)
- :class:`TimedeltaIndex` and :class:`DatetimeIndex` no longer accept non-nanosecond dtype strings like "timedelta64" or "datetime64", use "timedelta64[ns]" and "datetime64[ns]" instead (:issue:`24806`)
- Changed the default "skipna" argument in :func:`pandas.api.types.infer_dtype` from ``False`` to ``True`` (:issue:`24050`)
- Removed ``Series.ix`` and ``DataFrame.ix`` (:issue:`26438`)
- Removed ``Index.summary`` (:issue:`18217`)
- Removed the previously deprecated keyword "fastpath" from the :class:`Index` constructor (:issue:`23110`)
- Removed ``Series.get_value``, ``Series.set_value``, ``DataFrame.get_value``, ``DataFrame.set_value`` (:issue:`17739`)
- Removed ``Series.compound`` and ``DataFrame.compound`` (:issue:`26405`)
- Changed the default "inplace" argument in :meth:`DataFrame.set_index` and :meth:`Series.set_axis` from ``None`` to ``False`` (:issue:`27600`)
- Removed ``Series.cat.categorical``, ``Series.cat.index``, ``Series.cat.name`` (:issue:`24751`)
- Removed the previously deprecated keyword "box" from :func:`to_datetime` and :func:`to_timedelta`; in addition these now always returns :class:`DatetimeIndex`, :class:`TimedeltaIndex`, :class:`Index`, :class:`Series`, or :class:`DataFrame` (:issue:`24486`)
- :func:`to_timedelta`, :class:`Timedelta`, and :class:`TimedeltaIndex` no longer allow "M", "y", or "Y" for the "unit" argument (:issue:`23264`)
- Removed the previously deprecated keyword "time_rule" from (non-public) ``offsets.generate_range``, which has been moved to :func:`core.arrays._ranges.generate_range` (:issue:`24157`)
- :meth:`DataFrame.loc` or :meth:`Series.loc` with listlike indexers and missing labels will no longer reindex (:issue:`17295`)
- :meth:`DataFrame.to_excel` and :meth:`Series.to_excel` with non-existent columns will no longer reindex (:issue:`17295`)
- Removed the previously deprecated keyword "join_axes" from :func:`concat`; use ``reindex_like`` on the result instead (:issue:`22318`)
- Removed the previously deprecated keyword "by" from :meth:`DataFrame.sort_index`, use :meth:`DataFrame.sort_values` instead (:issue:`10726`)
- Removed support for nested renaming in :meth:`DataFrame.aggregate`, :meth:`Series.aggregate`, :meth:`core.groupby.DataFrameGroupBy.aggregate`, :meth:`core.groupby.SeriesGroupBy.aggregate`, :meth:`core.window.rolling.Rolling.aggregate` (:issue:`18529`)
- Passing ``datetime64`` data to :class:`TimedeltaIndex` or ``timedelta64`` data to ``DatetimeIndex`` now raises ``TypeError`` (:issue:`23539`, :issue:`23937`)
- Passing ``int64`` values to :class:`DatetimeIndex` and a timezone now interprets the values as nanosecond timestamps in UTC, not wall times in the given timezone (:issue:`24559`)
- A tuple passed to :meth:`DataFrame.groupby` is now exclusively treated as a single key (:issue:`18314`)
- Removed ``Index.contains``, use ``key in index`` instead (:issue:`30103`)
- Addition and subtraction of ``int`` or integer-arrays is no longer allowed in :class:`Timestamp`, :class:`DatetimeIndex`, :class:`TimedeltaIndex`, use ``obj + n * obj.freq`` instead of ``obj + n`` (:issue:`22535`)
- Removed ``Series.ptp`` (:issue:`21614`)
- Removed ``Series.from_array`` (:issue:`18258`)
- Removed ``DataFrame.from_items`` (:issue:`18458`)
- Removed ``DataFrame.as_matrix``, ``Series.as_matrix`` (:issue:`18458`)
- Removed ``Series.asobject`` (:issue:`18477`)
- Removed ``DataFrame.as_blocks``, ``Series.as_blocks``, ``DataFrame.blocks``, ``Series.blocks`` (:issue:`17656`)
- :meth:`pandas.Series.str.cat` now defaults to aligning ``others``, using ``join='left'`` (:issue:`27611`)
- :meth:`pandas.Series.str.cat` does not accept list-likes *within* list-likes anymore (:issue:`27611`)
- :meth:`Series.where` with ``Categorical`` dtype (or :meth:`DataFrame.where` with ``Categorical`` column) no longer allows setting new categories (:issue:`24114`)
- Removed the previously deprecated keywords "start", "end", and "periods" from the :class:`DatetimeIndex`, :class:`TimedeltaIndex`, and :class:`PeriodIndex` constructors; use :func:`date_range`, :func:`timedelta_range`, and :func:`period_range` instead (:issue:`23919`)
- Removed the previously deprecated keyword "verify_integrity" from the :class:`DatetimeIndex` and :class:`TimedeltaIndex` constructors (:issue:`23919`)
- Removed the previously deprecated keyword "fastpath" from ``pandas.core.internals.blocks.make_block`` (:issue:`19265`)
- Removed the previously deprecated keyword "dtype" from :meth:`Block.make_block_same_class` (:issue:`19434`)
- Removed ``ExtensionArray._formatting_values``. Use :attr:`ExtensionArray._formatter` instead. (:issue:`23601`)
- Removed ``MultiIndex.to_hierarchical`` (:issue:`21613`)
- Removed ``MultiIndex.labels``, use :attr:`MultiIndex.codes` instead (:issue:`23752`)
- Removed the previously deprecated keyword "labels" from the :class:`MultiIndex` constructor, use "codes" instead (:issue:`23752`)
- Removed ``MultiIndex.set_labels``, use :meth:`MultiIndex.set_codes` instead (:issue:`23752`)
- Removed the previously deprecated keyword "labels" from :meth:`MultiIndex.set_codes`, :meth:`MultiIndex.copy`, :meth:`MultiIndex.drop`, use "codes" instead (:issue:`23752`)
- Removed support for legacy HDF5 formats (:issue:`29787`)
- Passing a dtype alias (e.g. 'datetime64[ns, UTC]') to :class:`DatetimeTZDtype` is no longer allowed, use :meth:`DatetimeTZDtype.construct_from_string` instead (:issue:`23990`)
- Removed the previously deprecated keyword "skip_footer" from :func:`read_excel`; use "skipfooter" instead (:issue:`18836`)
- :func:`read_excel` no longer allows an integer value for the parameter ``usecols``, instead pass a list of integers from 0 to ``usecols`` inclusive (:issue:`23635`)
- Removed the previously deprecated keyword "convert_datetime64" from :meth:`DataFrame.to_records` (:issue:`18902`)
- Removed ``IntervalIndex.from_intervals`` in favor of the :class:`IntervalIndex` constructor (:issue:`19263`)
- Changed the default "keep_tz" argument in :meth:`DatetimeIndex.to_series` from ``None`` to ``True`` (:issue:`23739`)
- Removed ``api.types.is_period`` and ``api.types.is_datetimetz`` (:issue:`23917`)
- Ability to read pickles containing :class:`Categorical` instances created with pre-0.16 version of pandas has been removed (:issue:`27538`)
- Removed ``pandas.tseries.plotting.tsplot`` (:issue:`18627`)
- Removed the previously deprecated keywords "reduce" and "broadcast" from :meth:`DataFrame.apply` (:issue:`18577`)
- Removed the previously deprecated ``assert_raises_regex`` function in ``pandas._testing`` (:issue:`29174`)
- Removed the previously deprecated ``FrozenNDArray`` class in ``pandas.core.indexes.frozen`` (:issue:`29335`)
- Removed the previously deprecated keyword "nthreads" from :func:`read_feather`, use "use_threads" instead (:issue:`23053`)
- Removed ``Index.is_lexsorted_for_tuple`` (:issue:`29305`)
- Removed support for nested renaming in :meth:`DataFrame.aggregate`, :meth:`Series.aggregate`, :meth:`core.groupby.DataFrameGroupBy.aggregate`, :meth:`core.groupby.SeriesGroupBy.aggregate`, :meth:`core.window.rolling.Rolling.aggregate` (:issue:`29608`)
- Removed ``Series.valid``; use :meth:`Series.dropna` instead (:issue:`18800`)
- Removed ``DataFrame.is_copy``, ``Series.is_copy`` (:issue:`18812`)
- Removed ``DataFrame.get_ftype_counts``, ``Series.get_ftype_counts`` (:issue:`18243`)
- Removed ``DataFrame.ftypes``, ``Series.ftypes``, ``Series.ftype`` (:issue:`26744`)
- Removed ``Index.get_duplicates``, use ``idx[idx.duplicated()].unique()`` instead (:issue:`20239`)
- Removed ``Series.clip_upper``, ``Series.clip_lower``, ``DataFrame.clip_upper``, ``DataFrame.clip_lower`` (:issue:`24203`)
- Removed the ability to alter :attr:`DatetimeIndex.freq`, :attr:`TimedeltaIndex.freq`, or :attr:`PeriodIndex.freq` (:issue:`20772`)
- Removed ``DatetimeIndex.offset`` (:issue:`20730`)
- Removed ``DatetimeIndex.asobject``, ``TimedeltaIndex.asobject``, ``PeriodIndex.asobject``, use ``astype(object)`` instead (:issue:`29801`)
- Removed the previously deprecated keyword "order" from :func:`factorize` (:issue:`19751`)
- Removed the previously deprecated keyword "encoding" from :func:`read_stata` and :meth:`DataFrame.to_stata` (:issue:`21400`)
- Changed the default "sort" argument in :func:`concat` from ``None`` to ``False`` (:issue:`20613`)
- Removed the previously deprecated keyword "raise_conflict" from :meth:`DataFrame.update`, use "errors" instead (:issue:`23585`)
- Removed the previously deprecated keyword "n" from :meth:`DatetimeIndex.shift`, :meth:`TimedeltaIndex.shift`, :meth:`PeriodIndex.shift`, use "periods" instead (:issue:`22458`)
- Removed the previously deprecated keywords "how", "fill_method", and "limit" from :meth:`DataFrame.resample` (:issue:`30139`)
- Passing an integer to :meth:`Series.fillna` or :meth:`DataFrame.fillna` with ``timedelta64[ns]`` dtype now raises ``TypeError`` (:issue:`24694`)
- Passing multiple axes to :meth:`DataFrame.dropna` is no longer supported (:issue:`20995`)
- Removed ``Series.nonzero``, use ``to_numpy().nonzero()`` instead (:issue:`24048`)
- Passing floating dtype ``codes`` to :meth:`Categorical.from_codes` is no longer supported, pass ``codes.astype(np.int64)`` instead (:issue:`21775`)
- Removed the previously deprecated keyword "pat" from :meth:`Series.str.partition` and :meth:`Series.str.rpartition`, use "sep" instead (:issue:`23767`)
- Removed ``Series.put`` (:issue:`27106`)
- Removed ``Series.real``, ``Series.imag`` (:issue:`27106`)
- Removed ``Series.to_dense``, ``DataFrame.to_dense`` (:issue:`26684`)
- Removed ``Index.dtype_str``, use ``str(index.dtype)`` instead (:issue:`27106`)
- :meth:`Categorical.ravel` returns a :class:`Categorical` instead of a ``ndarray`` (:issue:`27199`)
- The 'outer' method on Numpy ufuncs, e.g. ``np.subtract.outer`` operating on :class:`Series` objects is no longer supported, and will raise ``NotImplementedError`` (:issue:`27198`)
- Removed ``Series.get_dtype_counts`` and ``DataFrame.get_dtype_counts`` (:issue:`27145`)
- Changed the default "fill_value" argument in :meth:`Categorical.take` from ``True`` to ``False`` (:issue:`20841`)
- Changed the default value for the ``raw`` argument in :func:`Series.rolling().apply() <pandas.core.window.Rolling.apply>`, :func:`DataFrame.rolling().apply() <pandas.core.window.Rolling.apply>`, :func:`Series.expanding().apply() <pandas.core.window.Expanding.apply>`, and :func:`DataFrame.expanding().apply() <pandas.core.window.Expanding.apply>` from ``None`` to ``False`` (:issue:`20584`)
- Removed deprecated behavior of :meth:`Series.argmin` and :meth:`Series.argmax`, use :meth:`Series.idxmin` and :meth:`Series.idxmax` for the old behavior (:issue:`16955`)
- Passing a tz-aware ``datetime.datetime`` or :class:`Timestamp` into the :class:`Timestamp` constructor with the ``tz`` argument now raises a ``ValueError`` (:issue:`23621`)
- Removed ``Series.base``, ``Index.base``, ``Categorical.base``, ``Series.flags``, ``Index.flags``, ``PeriodArray.flags``, ``Series.strides``, ``Index.strides``, ``Series.itemsize``, ``Index.itemsize``, ``Series.data``, ``Index.data`` (:issue:`20721`)
- Changed :meth:`Timedelta.resolution` to match the behavior of the standard library ``datetime.timedelta.resolution``, for the old behavior, use :meth:`Timedelta.resolution_string` (:issue:`26839`)
- Removed ``Timestamp.weekday_name``, ``DatetimeIndex.weekday_name``, and ``Series.dt.weekday_name`` (:issue:`18164`)
- Removed the previously deprecated keyword "errors" in :meth:`Timestamp.tz_localize`, :meth:`DatetimeIndex.tz_localize`, and :meth:`Series.tz_localize` (:issue:`22644`)
- Changed the default "ordered" argument in :class:`CategoricalDtype` from ``None`` to ``False`` (:issue:`26336`)
- :meth:`Series.set_axis` and :meth:`DataFrame.set_axis` now require "labels" as the first argument and "axis" as an optional named parameter (:issue:`30089`)
- Removed ``to_msgpack``, ``read_msgpack``, ``DataFrame.to_msgpack``, ``Series.to_msgpack`` (:issue:`27103`)
- Removed ``Series.compress`` (:issue:`21930`)
- Removed the previously deprecated keyword "fill_value" from :meth:`Categorical.fillna`, use "value" instead (:issue:`19269`)
- Removed the previously deprecated keyword "data" from :func:`andrews_curves`, use "frame" instead (:issue:`6956`)
- Removed the previously deprecated keyword "data" from :func:`parallel_coordinates`, use "frame" instead (:issue:`6956`)
- Removed the previously deprecated keyword "colors" from :func:`parallel_coordinates`, use "color" instead (:issue:`6956`)
- Removed the previously deprecated keywords "verbose" and "private_key" from :func:`read_gbq` (:issue:`30200`)
- Calling ``np.array`` and ``np.asarray`` on tz-aware :class:`Series` and :class:`DatetimeIndex` will now return an object array of tz-aware :class:`Timestamp` (:issue:`24596`)
-

.. ---------------------------------------------------------------------------

.. _whatsnew_100.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Performance improvement in :class:`DataFrame` arithmetic and comparison operations with scalars (:issue:`24990`, :issue:`29853`)
- Performance improvement in indexing with a non-unique :class:`IntervalIndex` (:issue:`27489`)
- Performance improvement in :attr:`MultiIndex.is_monotonic` (:issue:`27495`)
- Performance improvement in :func:`cut` when ``bins`` is an :class:`IntervalIndex` (:issue:`27668`)
- Performance improvement when initializing a :class:`DataFrame` using a ``range`` (:issue:`30171`)
- Performance improvement in :meth:`DataFrame.corr` when ``method`` is ``"spearman"`` (:issue:`28139`)
- Performance improvement in :meth:`DataFrame.replace` when provided a list of values to replace (:issue:`28099`)
- Performance improvement in :meth:`DataFrame.select_dtypes` by using vectorization instead of iterating over a loop (:issue:`28317`)
- Performance improvement in :meth:`Categorical.searchsorted` and  :meth:`CategoricalIndex.searchsorted` (:issue:`28795`)
- Performance improvement when comparing a :class:`Categorical` with a scalar and the scalar is not found in the categories (:issue:`29750`)
- Performance improvement when checking if values in a :class:`Categorical` are equal, equal or larger or larger than a given scalar.
  The improvement is not present if checking if the :class:`Categorical` is less than or less than or equal than the scalar (:issue:`29820`)
- Performance improvement in :meth:`Index.equals` and  :meth:`MultiIndex.equals` (:issue:`29134`)
- Performance improvement in :func:`~pandas.api.types.infer_dtype` when ``skipna`` is ``True`` (:issue:`28814`)

.. ---------------------------------------------------------------------------

.. _whatsnew_100.bug_fixes:

Bug fixes
~~~~~~~~~


Categorical
^^^^^^^^^^^

- Added test to assert the :func:`fillna` raises the correct ``ValueError`` message when the value isn't a value from categories (:issue:`13628`)
- Bug in :meth:`Categorical.astype` where ``NaN`` values were handled incorrectly when casting to int (:issue:`28406`)
- :meth:`DataFrame.reindex` with a :class:`CategoricalIndex` would fail when the targets contained duplicates, and wouldn't fail if the source contained duplicates (:issue:`28107`)
- Bug in :meth:`Categorical.astype` not allowing for casting to extension dtypes (:issue:`28668`)
- Bug where :func:`merge` was unable to join on categorical and extension dtype columns (:issue:`28668`)
- :meth:`Categorical.searchsorted` and :meth:`CategoricalIndex.searchsorted` now work on unordered categoricals also (:issue:`21667`)
- Added test to assert roundtripping to parquet with :func:`DataFrame.to_parquet` or :func:`read_parquet` will preserve Categorical dtypes for string types (:issue:`27955`)
- Changed the error message in :meth:`Categorical.remove_categories` to always show the invalid removals as a set (:issue:`28669`)
- Using date accessors on a categorical dtyped :class:`Series` of datetimes was not returning an object of the
  same type as if one used the :meth:`.str.` / :meth:`.dt.` on a :class:`Series` of that type. E.g. when accessing :meth:`Series.dt.tz_localize` on a
  :class:`Categorical` with duplicate entries, the accessor was skipping duplicates (:issue:`27952`)
- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` that would give incorrect results on categorical data (:issue:`26988`)
- Bug where calling :meth:`Categorical.min` or :meth:`Categorical.max` on an empty Categorical would raise a numpy exception (:issue:`30227`)
- The following methods now also correctly output values for unobserved categories when called through ``groupby(..., observed=False)`` (:issue:`17605`)
  * :meth:`core.groupby.SeriesGroupBy.count`
  * :meth:`core.groupby.SeriesGroupBy.size`
  * :meth:`core.groupby.SeriesGroupBy.nunique`
  * :meth:`core.groupby.SeriesGroupBy.nth`


Datetimelike
^^^^^^^^^^^^
- Bug in :meth:`Series.__setitem__` incorrectly casting ``np.timedelta64("NaT")`` to ``np.datetime64("NaT")`` when inserting into a :class:`Series` with datetime64 dtype (:issue:`27311`)
- Bug in :meth:`Series.dt` property lookups when the underlying data is read-only (:issue:`27529`)
- Bug in ``HDFStore.__getitem__`` incorrectly reading tz attribute created in Python 2 (:issue:`26443`)
- Bug in :func:`to_datetime` where passing arrays of malformed ``str`` with errors="coerce" could incorrectly lead to raising ``ValueError`` (:issue:`28299`)
- Bug in :meth:`core.groupby.SeriesGroupBy.nunique` where ``NaT`` values were interfering with the count of unique values (:issue:`27951`)
- Bug in :class:`Timestamp` subtraction when subtracting a :class:`Timestamp` from a ``np.datetime64`` object incorrectly raising ``TypeError`` (:issue:`28286`)
- Addition and subtraction of integer or integer-dtype arrays with :class:`Timestamp` will now raise ``NullFrequencyError`` instead of ``ValueError`` (:issue:`28268`)
- Bug in :class:`Series` and :class:`DataFrame` with integer dtype failing to raise ``TypeError`` when adding or subtracting a ``np.datetime64`` object (:issue:`28080`)
- Bug in :meth:`Series.astype`, :meth:`Index.astype`, and :meth:`DataFrame.astype` failing to handle ``NaT`` when casting to an integer dtype (:issue:`28492`)
- Bug in :class:`Week` with ``weekday`` incorrectly raising ``AttributeError`` instead of ``TypeError`` when adding or subtracting an invalid type (:issue:`28530`)
- Bug in :class:`DataFrame` arithmetic operations when operating with a :class:`Series` with dtype ``'timedelta64[ns]'`` (:issue:`28049`)
- Bug in :func:`core.groupby.generic.SeriesGroupBy.apply` raising ``ValueError`` when a column in the original DataFrame is a datetime and the column labels are not standard integers (:issue:`28247`)
- Bug in :func:`pandas._config.localization.get_locales` where the ``locales -a`` encodes the locales list as windows-1252 (:issue:`23638`, :issue:`24760`, :issue:`27368`)
- Bug in :meth:`Series.var` failing to raise ``TypeError`` when called with ``timedelta64[ns]`` dtype (:issue:`28289`)
- Bug in :meth:`DatetimeIndex.strftime` and :meth:`Series.dt.strftime` where ``NaT`` was converted to the string ``'NaT'`` instead of ``np.nan`` (:issue:`29578`)
- Bug in masking datetime-like arrays with a boolean mask of an incorrect length not raising an ``IndexError`` (:issue:`30308`)
- Bug in :attr:`Timestamp.resolution` being a property instead of a class attribute (:issue:`29910`)
- Bug in :func:`pandas.to_datetime` when called with ``None`` raising ``TypeError`` instead of returning ``NaT`` (:issue:`30011`)
- Bug in :func:`pandas.to_datetime` failing for ``deques`` when using ``cache=True`` (the default) (:issue:`29403`)
- Bug in :meth:`Series.item` with ``datetime64`` or ``timedelta64`` dtype, :meth:`DatetimeIndex.item`, and :meth:`TimedeltaIndex.item` returning an integer instead of a :class:`Timestamp` or :class:`Timedelta` (:issue:`30175`)
- Bug in :class:`DatetimeIndex` addition when adding a non-optimized :class:`DateOffset` incorrectly dropping timezone information (:issue:`30336`)
- Bug in :meth:`DataFrame.drop` where attempting to drop non-existent values from a DatetimeIndex would yield a confusing error message (:issue:`30399`)
- Bug in :meth:`DataFrame.append` would remove the timezone-awareness of new data (:issue:`30238`)
- Bug in :meth:`Series.cummin` and :meth:`Series.cummax` with timezone-aware dtype incorrectly dropping its timezone (:issue:`15553`)
- Bug in :class:`DatetimeArray`, :class:`TimedeltaArray`, and :class:`PeriodArray` where inplace addition and subtraction did not actually operate inplace (:issue:`24115`)
- Bug in :func:`pandas.to_datetime` when called with ``Series`` storing ``IntegerArray`` raising ``TypeError`` instead of returning ``Series`` (:issue:`30050`)
- Bug in :func:`date_range` with custom business hours as ``freq`` and given number of ``periods`` (:issue:`30593`)
- Bug in :class:`PeriodIndex` comparisons with incorrectly casting integers to :class:`Period` objects, inconsistent with the :class:`Period` comparison behavior (:issue:`30722`)
- Bug in :meth:`DatetimeIndex.insert` raising a ``ValueError`` instead of a ``TypeError`` when trying to insert a timezone-aware :class:`Timestamp` into a timezone-naive :class:`DatetimeIndex`, or vice-versa (:issue:`30806`)

Timedelta
^^^^^^^^^
- Bug in subtracting a :class:`TimedeltaIndex` or :class:`TimedeltaArray` from a ``np.datetime64`` object (:issue:`29558`)
-

Timezones
^^^^^^^^^

-


Numeric
^^^^^^^
- Bug in :meth:`DataFrame.quantile` with zero-column :class:`DataFrame` incorrectly raising (:issue:`23925`)
- :class:`DataFrame` flex inequality comparisons methods (:meth:`DataFrame.lt`, :meth:`DataFrame.le`, :meth:`DataFrame.gt`, :meth:`DataFrame.ge`) with object-dtype and ``complex`` entries failing to raise ``TypeError`` like their :class:`Series` counterparts (:issue:`28079`)
- Bug in :class:`DataFrame` logical operations (``&``, ``|``, ``^``) not matching :class:`Series` behavior by filling NA values (:issue:`28741`)
- Bug in :meth:`DataFrame.interpolate` where specifying axis by name references variable before it is assigned (:issue:`29142`)
- Bug in :meth:`Series.var` not computing the right value with a nullable integer dtype series not passing through ddof argument (:issue:`29128`)
- Improved error message when using ``frac`` > 1 and ``replace`` = False (:issue:`27451`)
- Bug in numeric indexes resulted in it being possible to instantiate an :class:`Int64Index`, :class:`UInt64Index`, or :class:`Float64Index` with an invalid dtype (e.g. datetime-like) (:issue:`29539`)
- Bug in :class:`UInt64Index` precision loss while constructing from a list with values in the ``np.uint64`` range (:issue:`29526`)
- Bug in :class:`NumericIndex` construction that caused indexing to fail when integers in the ``np.uint64`` range were used (:issue:`28023`)
- Bug in :class:`NumericIndex` construction that caused :class:`UInt64Index` to be casted to :class:`Float64Index` when integers in the ``np.uint64`` range were used to index a :class:`DataFrame` (:issue:`28279`)
- Bug in :meth:`Series.interpolate` when using method=`index` with an unsorted index, would previously return incorrect results. (:issue:`21037`)
- Bug in :meth:`DataFrame.round` where a :class:`DataFrame` with a :class:`CategoricalIndex` of :class:`IntervalIndex` columns would incorrectly raise a ``TypeError`` (:issue:`30063`)
- Bug in :meth:`Series.pct_change` and :meth:`DataFrame.pct_change` when there are duplicated indices (:issue:`30463`)
- Bug in :class:`DataFrame` cumulative operations (e.g. cumsum, cummax) incorrect casting to object-dtype (:issue:`19296`)
- Bug in :class:`~DataFrame.diff` losing the dtype for extension types (:issue:`30889`)
- Bug in :class:`DataFrame.diff` raising an ``IndexError`` when one of the columns was a nullable integer dtype (:issue:`30967`)

Conversion
^^^^^^^^^^

-

Strings
^^^^^^^

- Calling :meth:`Series.str.isalnum` (and other "ismethods") on an empty ``Series`` would return an ``object`` dtype instead of ``bool`` (:issue:`29624`)
-


Interval
^^^^^^^^

- Bug in :meth:`IntervalIndex.get_indexer` where a :class:`Categorical` or :class:`CategoricalIndex` ``target`` would incorrectly raise a ``TypeError`` (:issue:`30063`)
- Bug in ``pandas.core.dtypes.cast.infer_dtype_from_scalar`` where passing ``pandas_dtype=True`` did not infer :class:`IntervalDtype` (:issue:`30337`)
- Bug in :class:`Series` constructor where constructing a ``Series`` from a ``list`` of :class:`Interval` objects resulted in ``object`` dtype instead of :class:`IntervalDtype` (:issue:`23563`)
- Bug in :class:`IntervalDtype` where the ``kind`` attribute was incorrectly set as ``None`` instead of ``"O"`` (:issue:`30568`)
- Bug in :class:`IntervalIndex`, :class:`~arrays.IntervalArray`, and :class:`Series` with interval data where equality comparisons were incorrect (:issue:`24112`)

Indexing
^^^^^^^^

- Bug in assignment using a reverse slicer (:issue:`26939`)
- Bug in :meth:`DataFrame.explode` would duplicate frame in the presence of duplicates in the index (:issue:`28010`)
- Bug in reindexing a :meth:`PeriodIndex` with another type of index that contained a ``Period`` (:issue:`28323`) (:issue:`28337`)
- Fix assignment of column via ``.loc`` with numpy non-ns datetime type (:issue:`27395`)
- Bug in :meth:`Float64Index.astype` where ``np.inf`` was not handled properly when casting to an integer dtype (:issue:`28475`)
- :meth:`Index.union` could fail when the left contained duplicates (:issue:`28257`)
- Bug when indexing with ``.loc`` where the index was a :class:`CategoricalIndex` with non-string categories didn't work (:issue:`17569`, :issue:`30225`)
- :meth:`Index.get_indexer_non_unique` could fail with ``TypeError`` in some cases, such as when searching for ints in a string index (:issue:`28257`)
- Bug in :meth:`Float64Index.get_loc` incorrectly raising ``TypeError`` instead of ``KeyError`` (:issue:`29189`)
- Bug in :meth:`DataFrame.loc` with incorrect dtype when setting Categorical value in 1-row DataFrame (:issue:`25495`)
- :meth:`MultiIndex.get_loc` can't find missing values when input includes missing values (:issue:`19132`)
- Bug in :meth:`Series.__setitem__` incorrectly assigning values with boolean indexer when the length of new data matches the number of ``True`` values and new data is not a ``Series`` or an ``np.array`` (:issue:`30567`)
- Bug in indexing with a :class:`PeriodIndex` incorrectly accepting integers representing years, use e.g. ``ser.loc["2007"]`` instead of ``ser.loc[2007]`` (:issue:`30763`)

Missing
^^^^^^^

-

MultiIndex
^^^^^^^^^^

- Constructor for :class:`MultiIndex` verifies that the given ``sortorder`` is compatible with the actual ``lexsort_depth``  if ``verify_integrity`` parameter is ``True`` (the default) (:issue:`28735`)
- Series and MultiIndex ``.drop`` with ``MultiIndex`` raise exception if labels not in given in level (:issue:`8594`)
-

IO
^^

- :meth:`read_csv` now accepts binary mode file buffers when using the Python csv engine (:issue:`23779`)
- Bug in :meth:`DataFrame.to_json` where using a Tuple as a column or index value and using ``orient="columns"`` or ``orient="index"`` would produce invalid JSON (:issue:`20500`)
- Improve infinity parsing. :meth:`read_csv` now interprets ``Infinity``, ``+Infinity``, ``-Infinity`` as floating point values (:issue:`10065`)
- Bug in :meth:`DataFrame.to_csv` where values were truncated when the length of ``na_rep`` was shorter than the text input data. (:issue:`25099`)
- Bug in :func:`DataFrame.to_string` where values were truncated using display options instead of outputting the full content (:issue:`9784`)
- Bug in :meth:`DataFrame.to_json` where a datetime column label would not be written out in ISO format with ``orient="table"`` (:issue:`28130`)
- Bug in :func:`DataFrame.to_parquet` where writing to GCS would fail with ``engine='fastparquet'`` if the file did not already exist (:issue:`28326`)
- Bug in :func:`read_hdf` closing stores that it didn't open when Exceptions are raised (:issue:`28699`)
- Bug in :meth:`DataFrame.read_json` where using ``orient="index"`` would not maintain the order (:issue:`28557`)
- Bug in :meth:`DataFrame.to_html` where the length of the ``formatters`` argument was not verified (:issue:`28469`)
- Bug in :meth:`DataFrame.read_excel` with ``engine='ods'`` when ``sheet_name`` argument references a non-existent sheet (:issue:`27676`)
- Bug in :meth:`pandas.io.formats.style.Styler` formatting for floating values not displaying decimals correctly (:issue:`13257`)
- Bug in :meth:`DataFrame.to_html` when using ``formatters=<list>`` and ``max_cols`` together. (:issue:`25955`)
- Bug in :meth:`Styler.background_gradient` not able to work with dtype ``Int64`` (:issue:`28869`)
- Bug in :meth:`DataFrame.to_clipboard` which did not work reliably in ipython (:issue:`22707`)
- Bug in :func:`read_json` where default encoding was not set to ``utf-8`` (:issue:`29565`)
- Bug in :class:`PythonParser` where str and bytes were being mixed when dealing with the decimal field (:issue:`29650`)
- :meth:`read_gbq` now accepts ``progress_bar_type`` to display progress bar while the data downloads. (:issue:`29857`)
- Bug in :func:`pandas.io.json.json_normalize` where a missing value in the location specified by ``record_path`` would raise a ``TypeError`` (:issue:`30148`)
- :func:`read_excel` now accepts binary data (:issue:`15914`)
- Bug in :meth:`read_csv` in which encoding handling was limited to just the string ``utf-16`` for the C engine (:issue:`24130`)

Plotting
^^^^^^^^

- Bug in :meth:`Series.plot` not able to plot boolean values (:issue:`23719`)
- Bug in :meth:`DataFrame.plot` not able to plot when no rows (:issue:`27758`)
- Bug in :meth:`DataFrame.plot` producing incorrect legend markers when plotting multiple series on the same axis (:issue:`18222`)
- Bug in :meth:`DataFrame.plot` when ``kind='box'`` and data contains datetime or timedelta data. These types are now automatically dropped (:issue:`22799`)
- Bug in :meth:`DataFrame.plot.line` and :meth:`DataFrame.plot.area` produce wrong xlim in x-axis (:issue:`27686`, :issue:`25160`, :issue:`24784`)
- Bug where :meth:`DataFrame.boxplot` would not accept a ``color`` parameter like :meth:`DataFrame.plot.box` (:issue:`26214`)
- Bug in the ``xticks`` argument being ignored for :meth:`DataFrame.plot.bar` (:issue:`14119`)
- :func:`set_option` now validates that the plot backend provided to ``'plotting.backend'`` implements the backend when the option is set, rather than when a plot is created (:issue:`28163`)
- :meth:`DataFrame.plot` now allow a ``backend`` keyword argument to allow changing between backends in one session (:issue:`28619`).
- Bug in color validation incorrectly raising for non-color styles (:issue:`29122`).
- Allow :meth:`DataFrame.plot.scatter` to plot ``objects`` and ``datetime`` type data (:issue:`18755`, :issue:`30391`)
- Bug in :meth:`DataFrame.hist`, ``xrot=0`` does not work with ``by`` and subplots (:issue:`30288`).

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in :meth:`core.groupby.DataFrameGroupBy.apply` only showing output from a single group when function returns an :class:`Index` (:issue:`28652`)
- Bug in :meth:`DataFrame.groupby` with multiple groups where an ``IndexError`` would be raised if any group contained all NA values (:issue:`20519`)
- Bug in :meth:`pandas.core.resample.Resampler.size` and :meth:`pandas.core.resample.Resampler.count` returning wrong dtype when used with an empty :class:`Series` or :class:`DataFrame` (:issue:`28427`)
- Bug in :meth:`DataFrame.rolling` not allowing for rolling over datetimes when ``axis=1`` (:issue:`28192`)
- Bug in :meth:`DataFrame.rolling` not allowing rolling over multi-index levels (:issue:`15584`).
- Bug in :meth:`DataFrame.rolling` not allowing rolling on monotonic decreasing time indexes (:issue:`19248`).
- Bug in :meth:`DataFrame.groupby` not offering selection by column name when ``axis=1`` (:issue:`27614`)
- Bug in :meth:`core.groupby.DataFrameGroupby.agg` not able to use lambda function with named aggregation (:issue:`27519`)
- Bug in :meth:`DataFrame.groupby` losing column name information when grouping by a categorical column (:issue:`28787`)
- Remove error raised due to duplicated input functions in named aggregation in :meth:`DataFrame.groupby` and :meth:`Series.groupby`. Previously error will be raised if the same function is applied on the same column and now it is allowed if new assigned names are different. (:issue:`28426`)
- :meth:`core.groupby.SeriesGroupBy.value_counts` will be able to handle the case even when the :class:`Grouper` makes empty groups (:issue:`28479`)
- Bug in :meth:`core.window.rolling.Rolling.quantile` ignoring ``interpolation`` keyword argument when used within a groupby (:issue:`28779`)
- Bug in :meth:`DataFrame.groupby` where ``any``, ``all``, ``nunique`` and transform functions would incorrectly handle duplicate column labels (:issue:`21668`)
- Bug in :meth:`core.groupby.DataFrameGroupBy.agg` with timezone-aware datetime64 column incorrectly casting results to the original dtype (:issue:`29641`)
- Bug in :meth:`DataFrame.groupby` when using axis=1 and having a single level columns index (:issue:`30208`)
- Bug in :meth:`DataFrame.groupby` when using nunique on axis=1 (:issue:`30253`)
- Bug in :meth:`GroupBy.quantile` with multiple list-like q value and integer column names (:issue:`30289`)
- Bug in :meth:`GroupBy.pct_change` and :meth:`core.groupby.SeriesGroupBy.pct_change` causes ``TypeError`` when ``fill_method`` is ``None`` (:issue:`30463`)
- Bug in :meth:`Rolling.count` and :meth:`Expanding.count` argument where ``min_periods`` was ignored (:issue:`26996`)

Reshaping
^^^^^^^^^

- Bug in :meth:`DataFrame.apply` that caused incorrect output with empty :class:`DataFrame` (:issue:`28202`, :issue:`21959`)
- Bug in :meth:`DataFrame.stack` not handling non-unique indexes correctly when creating MultiIndex (:issue:`28301`)
- Bug in :meth:`pivot_table` not returning correct type ``float`` when ``margins=True`` and ``aggfunc='mean'`` (:issue:`24893`)
- Bug :func:`merge_asof` could not use :class:`datetime.timedelta` for ``tolerance`` kwarg (:issue:`28098`)
- Bug in :func:`merge`, did not append suffixes correctly with MultiIndex (:issue:`28518`)
- :func:`qcut` and :func:`cut` now handle boolean input (:issue:`20303`)
- Fix to ensure all int dtypes can be used in :func:`merge_asof` when using a tolerance value. Previously every non-int64 type would raise an erroneous ``MergeError`` (:issue:`28870`).
- Better error message in :func:`get_dummies` when ``columns`` isn't a list-like value (:issue:`28383`)
- Bug in :meth:`Index.join` that caused infinite recursion error for mismatched ``MultiIndex`` name orders. (:issue:`25760`, :issue:`28956`)
- Bug :meth:`Series.pct_change` where supplying an anchored frequency would throw a ``ValueError`` (:issue:`28664`)
- Bug where :meth:`DataFrame.equals` returned True incorrectly in some cases when two DataFrames had the same columns in different orders (:issue:`28839`)
- Bug in :meth:`DataFrame.replace` that caused non-numeric replacer's dtype not respected (:issue:`26632`)
- Bug in :func:`melt` where supplying mixed strings and numeric values for ``id_vars`` or ``value_vars`` would incorrectly raise a ``ValueError`` (:issue:`29718`)
- Dtypes are now preserved when transposing a ``DataFrame`` where each column is the same extension dtype (:issue:`30091`)
- Bug in :func:`merge_asof` merging on a tz-aware ``left_index`` and ``right_on`` a tz-aware column (:issue:`29864`)
- Improved error message and docstring in :func:`cut` and :func:`qcut` when ``labels=True`` (:issue:`13318`)
- Bug in missing ``fill_na`` parameter to :meth:`DataFrame.unstack` with list of levels (:issue:`30740`)

Sparse
^^^^^^
- Bug in :class:`SparseDataFrame` arithmetic operations incorrectly casting inputs to float (:issue:`28107`)
- Bug in ``DataFrame.sparse`` returning a ``Series`` when there was a column named ``sparse`` rather than the accessor (:issue:`30758`)
- Fixed :meth:`operator.xor` with a boolean-dtype ``SparseArray``. Now returns a sparse result, rather than object dtype (:issue:`31025`)

ExtensionArray
^^^^^^^^^^^^^^

- Bug in :class:`arrays.PandasArray` when setting a scalar string (:issue:`28118`, :issue:`28150`).
- Bug where nullable integers could not be compared to strings (:issue:`28930`)
- Bug where :class:`DataFrame` constructor raised ``ValueError`` with list-like data and ``dtype`` specified (:issue:`30280`)

Other
^^^^^
- Trying to set the ``display.precision``, ``display.max_rows`` or ``display.max_columns`` using :meth:`set_option` to anything but a ``None`` or a positive int will raise a ``ValueError`` (:issue:`23348`)
- Using :meth:`DataFrame.replace` with overlapping keys in a nested dictionary will no longer raise, now matching the behavior of a flat dictionary (:issue:`27660`)
- :meth:`DataFrame.to_csv` and :meth:`Series.to_csv` now support dicts as ``compression`` argument with key ``'method'`` being the compression method and others as additional compression options when the compression method is ``'zip'``. (:issue:`26023`)
- Bug in :meth:`Series.diff` where a boolean series would incorrectly raise a ``TypeError`` (:issue:`17294`)
- :meth:`Series.append` will no longer raise a ``TypeError`` when passed a tuple of ``Series`` (:issue:`28410`)
- Fix corrupted error message when calling ``pandas.libs._json.encode()`` on a 0d array (:issue:`18878`)
- Backtick quoting in :meth:`DataFrame.query` and :meth:`DataFrame.eval` can now also be used to use invalid identifiers like names that start with a digit, are python keywords, or are using single character operators. (:issue:`27017`)
- Bug in ``pd.core.util.hashing.hash_pandas_object`` where arrays containing tuples were incorrectly treated as non-hashable (:issue:`28969`)
- Bug in :meth:`DataFrame.append` that raised ``IndexError`` when appending with empty list (:issue:`28769`)
- Fix :class:`AbstractHolidayCalendar` to return correct results for
  years after 2030 (now goes up to 2200) (:issue:`27790`)
- Fixed :class:`~arrays.IntegerArray` returning ``inf`` rather than ``NaN`` for operations dividing by ``0`` (:issue:`27398`)
- Fixed ``pow`` operations for :class:`~arrays.IntegerArray` when the other value is ``0`` or ``1`` (:issue:`29997`)
- Bug in :meth:`Series.count` raises if use_inf_as_na is enabled (:issue:`29478`)
- Bug in :class:`Index` where a non-hashable name could be set without raising ``TypeError`` (:issue:`29069`)
- Bug in :class:`DataFrame` constructor when passing a 2D ``ndarray`` and an extension dtype (:issue:`12513`)
- Bug in :meth:`DataFrame.to_csv` when supplied a series with a ``dtype="string"`` and a ``na_rep``, the ``na_rep`` was being truncated to 2 characters. (:issue:`29975`)
- Bug where :meth:`DataFrame.itertuples` would incorrectly determine whether or not namedtuples could be used for dataframes of 255 columns (:issue:`28282`)
- Handle nested NumPy ``object`` arrays in :func:`testing.assert_series_equal` for ExtensionArray implementations (:issue:`30841`)
- Bug in :class:`Index` constructor incorrectly allowing 2-dimensional input arrays (:issue:`13601`, :issue:`27125`)

.. ---------------------------------------------------------------------------

.. _whatsnew_100.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.25.3..v1.0.0
.. _whatsnew_125:

What's new in 1.2.5 (June 22, 2021)
-----------------------------------

These are the changes in pandas 1.2.5. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_125.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :func:`concat` between two :class:`DataFrame` where one has an :class:`Index` that is all-None and the other is :class:`DatetimeIndex` incorrectly raising (:issue:`40841`)
- Fixed regression in :meth:`DataFrame.sum` and :meth:`DataFrame.prod` when ``min_count`` and ``numeric_only`` are both given (:issue:`41074`)
- Fixed regression in :func:`read_csv` when using ``memory_map=True`` with an non-UTF8 encoding (:issue:`40986`)
- Fixed regression in :meth:`DataFrame.replace` and :meth:`Series.replace` when the values to replace is a NumPy float array (:issue:`40371`)
- Fixed regression in :func:`ExcelFile` when a corrupt file is opened but not closed (:issue:`41778`)
- Fixed regression in :meth:`DataFrame.astype` with ``dtype=str`` failing to convert ``NaN`` in categorical columns (:issue:`41797`)

.. ---------------------------------------------------------------------------

.. _whatsnew_125.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.4..v1.2.5|HEAD
.. _whatsnew_0180:

Version 0.18.0 (March 13, 2016)
-------------------------------

{{ header }}


This is a major release from 0.17.1 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

.. warning::

   pandas >= 0.18.0 no longer supports compatibility with Python version 2.6
   and 3.3 (:issue:`7718`, :issue:`11273`)

.. warning::

   ``numexpr`` version 2.4.4 will now show a warning and not be used as a computation back-end for pandas because of some buggy behavior. This does not affect other versions (>= 2.1 and >= 2.4.6). (:issue:`12489`)

Highlights include:

- Moving and expanding window functions are now methods on Series and DataFrame,
  similar to ``.groupby``, see :ref:`here <whatsnew_0180.enhancements.moments>`.
- Adding support for a ``RangeIndex`` as a specialized form of the ``Int64Index``
  for memory savings, see :ref:`here <whatsnew_0180.enhancements.rangeindex>`.
- API breaking change to the ``.resample`` method to make it more ``.groupby``
  like, see :ref:`here <whatsnew_0180.breaking.resample>`.
- Removal of support for positional indexing with floats, which was deprecated
  since 0.14.0. This will now raise a ``TypeError``, see :ref:`here <whatsnew_0180.float_indexers>`.
- The ``.to_xarray()`` function has been added for compatibility with the
  `xarray package <http://xarray.pydata.org/en/stable/>`__, see :ref:`here <whatsnew_0180.enhancements.xarray>`.
- The ``read_sas`` function has been enhanced to read ``sas7bdat`` files, see :ref:`here <whatsnew_0180.enhancements.sas>`.
- Addition of the :ref:`.str.extractall() method <whatsnew_0180.enhancements.extract>`,
  and API changes to the :ref:`.str.extract() method <whatsnew_0180.enhancements.extract>`
  and :ref:`.str.cat() method <whatsnew_0180.enhancements.strcat>`.
- ``pd.test()`` top-level nose test runner is available (:issue:`4327`).

Check the :ref:`API Changes <whatsnew_0180.api_breaking>` and :ref:`deprecations <whatsnew_0180.deprecations>` before updating.

.. contents:: What's new in v0.18.0
    :local:
    :backlinks: none

.. _whatsnew_0180.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0180.enhancements.moments:

Window functions are now methods
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Window functions have been refactored to be methods on ``Series/DataFrame`` objects, rather than top-level functions, which are now deprecated. This allows these window-type functions, to have a similar API to that of ``.groupby``. See the full documentation :ref:`here <window.overview>` (:issue:`11603`, :issue:`12373`)


.. ipython:: python

   np.random.seed(1234)
   df = pd.DataFrame({'A': range(10), 'B': np.random.randn(10)})
   df

Previous behavior:

.. code-block:: ipython

   In [8]: pd.rolling_mean(df, window=3)
           FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with
                          DataFrame.rolling(window=3,center=False).mean()
   Out[8]:
       A         B
   0 NaN       NaN
   1 NaN       NaN
   2   1  0.237722
   3   2 -0.023640
   4   3  0.133155
   5   4 -0.048693
   6   5  0.342054
   7   6  0.370076
   8   7  0.079587
   9   8 -0.954504

New behavior:

.. ipython:: python

   r = df.rolling(window=3)

These show a descriptive repr

.. ipython:: python

   r
with tab-completion of available methods and properties.

.. code-block:: ipython

   In [9]: r.<TAB>  # noqa E225, E999
   r.A           r.agg         r.apply       r.count       r.exclusions  r.max         r.median      r.name        r.skew        r.sum
   r.B           r.aggregate   r.corr        r.cov         r.kurt        r.mean        r.min         r.quantile    r.std         r.var

The methods operate on the ``Rolling`` object itself

.. ipython:: python

   r.mean()

They provide getitem accessors

.. ipython:: python

   r['A'].mean()

And multiple aggregations

.. ipython:: python

   r.agg({'A': ['mean', 'std'],
          'B': ['mean', 'std']})

.. _whatsnew_0180.enhancements.rename:

Changes to rename
^^^^^^^^^^^^^^^^^

``Series.rename`` and ``NDFrame.rename_axis`` can now take a scalar or list-like
argument for altering the Series or axis *name*, in addition to their old behaviors of altering labels. (:issue:`9494`, :issue:`11965`)

.. ipython:: python

   s = pd.Series(np.random.randn(5))
   s.rename('newname')

.. ipython:: python

   df = pd.DataFrame(np.random.randn(5, 2))
   (df.rename_axis("indexname")
      .rename_axis("columns_name", axis="columns"))

The new functionality works well in method chains. Previously these methods only accepted functions or dicts mapping a *label* to a new label.
This continues to work as before for function or dict-like values.


.. _whatsnew_0180.enhancements.rangeindex:

Range Index
^^^^^^^^^^^

A ``RangeIndex`` has been added to the ``Int64Index`` sub-classes to support a memory saving alternative for common use cases. This has a similar implementation to the python ``range`` object (``xrange`` in python 2), in that it only stores the start, stop, and step values for the index. It will transparently interact with the user API, converting to ``Int64Index`` if needed.

This will now be the default constructed index for ``NDFrame`` objects, rather than previous an ``Int64Index``. (:issue:`939`, :issue:`12070`, :issue:`12071`, :issue:`12109`, :issue:`12888`)

Previous behavior:

.. code-block:: ipython

   In [3]: s = pd.Series(range(1000))

   In [4]: s.index
   Out[4]:
   Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
               ...
               990, 991, 992, 993, 994, 995, 996, 997, 998, 999], dtype='int64', length=1000)

   In [6]: s.index.nbytes
   Out[6]: 8000


New behavior:

.. ipython:: python

   s = pd.Series(range(1000))
   s.index
   s.index.nbytes

.. _whatsnew_0180.enhancements.extract:

Changes to str.extract
^^^^^^^^^^^^^^^^^^^^^^

The :ref:`.str.extract <text.extract>` method takes a regular
expression with capture groups, finds the first match in each subject
string, and returns the contents of the capture groups
(:issue:`11386`).

In v0.18.0, the ``expand`` argument was added to
``extract``.

- ``expand=False``: it returns a ``Series``, ``Index``, or ``DataFrame``, depending on the subject and regular expression pattern (same behavior as pre-0.18.0).
- ``expand=True``: it always returns a ``DataFrame``, which is more consistent and less confusing from the perspective of a user.

Currently the default is ``expand=None`` which gives a ``FutureWarning`` and uses ``expand=False``. To avoid this warning, please explicitly specify ``expand``.

.. code-block:: ipython

   In [1]: pd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\d)', expand=None)
   FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame)
   but in a future version of pandas this will be changed to expand=True (return DataFrame)

   Out[1]:
   0      1
   1      2
   2    NaN
   dtype: object

Extracting a regular expression with one group returns a Series if
``expand=False``.

.. ipython:: python

   pd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\d)', expand=False)

It returns a ``DataFrame`` with one column if ``expand=True``.

.. ipython:: python

   pd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\d)', expand=True)

Calling on an ``Index`` with a regex with exactly one capture group
returns an ``Index`` if ``expand=False``.

.. ipython:: python

   s = pd.Series(["a1", "b2", "c3"], ["A11", "B22", "C33"])
   s.index
   s.index.str.extract("(?P<letter>[a-zA-Z])", expand=False)

It returns a ``DataFrame`` with one column if ``expand=True``.

.. ipython:: python

   s.index.str.extract("(?P<letter>[a-zA-Z])", expand=True)

Calling on an ``Index`` with a regex with more than one capture group
raises ``ValueError`` if ``expand=False``.

.. code-block:: python

    >>> s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=False)
    ValueError: only one regex group is supported with Index

It returns a ``DataFrame`` if ``expand=True``.

.. ipython:: python

   s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=True)

In summary, ``extract(expand=True)`` always returns a ``DataFrame``
with a row for every subject string, and a column for every capture
group.

.. _whatsnew_0180.enhancements.extractall:

Addition of str.extractall
^^^^^^^^^^^^^^^^^^^^^^^^^^

The :ref:`.str.extractall <text.extractall>` method was added
(:issue:`11386`).  Unlike ``extract``, which returns only the first
match.

.. ipython:: python

   s = pd.Series(["a1a2", "b1", "c1"], ["A", "B", "C"])
   s
   s.str.extract(r"(?P<letter>[ab])(?P<digit>\d)", expand=False)

The ``extractall`` method returns all matches.

.. ipython:: python

   s.str.extractall(r"(?P<letter>[ab])(?P<digit>\d)")

.. _whatsnew_0180.enhancements.strcat:

Changes to str.cat
^^^^^^^^^^^^^^^^^^

The method ``.str.cat()`` concatenates the members of a ``Series``. Before, if ``NaN`` values were present in the Series, calling ``.str.cat()`` on it would return ``NaN``, unlike the rest of the ``Series.str.*`` API. This behavior has been amended to ignore ``NaN`` values by default. (:issue:`11435`).

A new, friendlier ``ValueError`` is added to protect against the mistake of supplying the ``sep`` as an arg, rather than as a kwarg. (:issue:`11334`).

.. ipython:: python

    pd.Series(['a', 'b', np.nan, 'c']).str.cat(sep=' ')
    pd.Series(['a', 'b', np.nan, 'c']).str.cat(sep=' ', na_rep='?')

.. code-block:: ipython

    In [2]: pd.Series(['a', 'b', np.nan, 'c']).str.cat(' ')
    ValueError: Did you mean to supply a ``sep`` keyword?


.. _whatsnew_0180.enhancements.rounding:

Datetimelike rounding
^^^^^^^^^^^^^^^^^^^^^

``DatetimeIndex``, ``Timestamp``, ``TimedeltaIndex``, ``Timedelta`` have gained the ``.round()``, ``.floor()`` and ``.ceil()`` method for datetimelike rounding, flooring and ceiling. (:issue:`4314`, :issue:`11963`)

Naive datetimes

.. ipython:: python

   dr = pd.date_range('20130101 09:12:56.1234', periods=3)
   dr
   dr.round('s')

   # Timestamp scalar
   dr[0]
   dr[0].round('10s')

Tz-aware are rounded, floored and ceiled in local times

.. ipython:: python

   dr = dr.tz_localize('US/Eastern')
   dr
   dr.round('s')

Timedeltas

.. ipython:: python

   t = pd.timedelta_range('1 days 2 hr 13 min 45 us', periods=3, freq='d')
   t
   t.round('10min')

   # Timedelta scalar
   t[0]
   t[0].round('2h')


In addition, ``.round()``, ``.floor()`` and ``.ceil()`` will be available through the ``.dt`` accessor of ``Series``.

.. ipython:: python

   s = pd.Series(dr)
   s
   s.dt.round('D')

Formatting of integers in FloatIndex
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Integers in ``FloatIndex``, e.g. 1., are now formatted with a decimal point and a ``0`` digit, e.g. ``1.0`` (:issue:`11713`)
This change not only affects the display to the console, but also the output of IO methods like ``.to_csv`` or ``.to_html``.

Previous behavior:

.. code-block:: ipython

   In [2]: s = pd.Series([1, 2, 3], index=np.arange(3.))

   In [3]: s
   Out[3]:
   0    1
   1    2
   2    3
   dtype: int64

   In [4]: s.index
   Out[4]: Float64Index([0.0, 1.0, 2.0], dtype='float64')

   In [5]: print(s.to_csv(path=None))
   0,1
   1,2
   2,3


New behavior:

.. ipython:: python

   s = pd.Series([1, 2, 3], index=np.arange(3.))
   s
   s.index
   print(s.to_csv(path_or_buf=None, header=False))

Changes to dtype assignment behaviors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When a DataFrame's slice is updated with a new slice of the same dtype, the dtype of the DataFrame will now remain the same. (:issue:`10503`)

Previous behavior:

.. code-block:: ipython

   In [5]: df = pd.DataFrame({'a': [0, 1, 1],
                              'b': pd.Series([100, 200, 300], dtype='uint32')})

   In [7]: df.dtypes
   Out[7]:
   a     int64
   b    uint32
   dtype: object

   In [8]: ix = df['a'] == 1

   In [9]: df.loc[ix, 'b'] = df.loc[ix, 'b']

   In [11]: df.dtypes
   Out[11]:
   a    int64
   b    int64
   dtype: object

New behavior:

.. ipython:: python

   df = pd.DataFrame({'a': [0, 1, 1],
                      'b': pd.Series([100, 200, 300], dtype='uint32')})
   df.dtypes
   ix = df['a'] == 1
   df.loc[ix, 'b'] = df.loc[ix, 'b']
   df.dtypes

When a DataFrame's integer slice is partially updated with a new slice of floats that could potentially be down-casted to integer without losing precision, the dtype of the slice will be set to float instead of integer.

Previous behavior:

.. code-block:: ipython

   In [4]: df = pd.DataFrame(np.array(range(1,10)).reshape(3,3),
                             columns=list('abc'),
                             index=[[4,4,8], [8,10,12]])

   In [5]: df
   Out[5]:
         a  b  c
   4 8   1  2  3
     10  4  5  6
   8 12  7  8  9

   In [7]: df.ix[4, 'c'] = np.array([0., 1.])

   In [8]: df
   Out[8]:
         a  b  c
   4 8   1  2  0
     10  4  5  1
   8 12  7  8  9

New behavior:

.. ipython:: python

   df = pd.DataFrame(np.array(range(1,10)).reshape(3,3),
                     columns=list('abc'),
                     index=[[4,4,8], [8,10,12]])
   df
   df.loc[4, 'c'] = np.array([0., 1.])
   df

.. _whatsnew_0180.enhancements.xarray:

Method to_xarray
^^^^^^^^^^^^^^^^

In a future version of pandas, we will be deprecating ``Panel`` and other > 2 ndim objects. In order to provide for continuity,
all ``NDFrame`` objects have gained the ``.to_xarray()`` method in order to convert to ``xarray`` objects, which has
a pandas-like interface for > 2 ndim. (:issue:`11972`)

See the `xarray full-documentation here <http://xarray.pydata.org/en/stable/>`__.

.. code-block:: ipython

   In [1]: p = Panel(np.arange(2*3*4).reshape(2,3,4))

   In [2]: p.to_xarray()
   Out[2]:
   <xarray.DataArray (items: 2, major_axis: 3, minor_axis: 4)>
   array([[[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11]],

          [[12, 13, 14, 15],
           [16, 17, 18, 19],
           [20, 21, 22, 23]]])
   Coordinates:
     * items       (items) int64 0 1
     * major_axis  (major_axis) int64 0 1 2
     * minor_axis  (minor_axis) int64 0 1 2 3

Latex representation
^^^^^^^^^^^^^^^^^^^^

``DataFrame`` has gained a ``._repr_latex_()`` method in order to allow for conversion to latex in a ipython/jupyter notebook using nbconvert. (:issue:`11778`)

Note that this must be activated by setting the option ``pd.display.latex.repr=True`` (:issue:`12182`)

For example, if you have a jupyter notebook you plan to convert to latex using nbconvert, place the statement ``pd.display.latex.repr=True`` in the first cell to have the contained DataFrame output also stored as latex.

The options ``display.latex.escape`` and ``display.latex.longtable`` have also been added to the configuration and are used automatically by the ``to_latex``
method. See the :ref:`available options docs <options.available>` for more info.

.. _whatsnew_0180.enhancements.sas:

``pd.read_sas()`` changes
^^^^^^^^^^^^^^^^^^^^^^^^^

``read_sas`` has gained the ability to read SAS7BDAT files, including compressed files.  The files can be read in entirety, or incrementally.  For full details see :ref:`here <io.sas>`. (:issue:`4052`)

.. _whatsnew_0180.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Handle truncated floats in SAS xport files (:issue:`11713`)
- Added option to hide index in ``Series.to_string`` (:issue:`11729`)
- ``read_excel`` now supports s3 urls of the format ``s3://bucketname/filename`` (:issue:`11447`)
- add support for ``AWS_S3_HOST`` env variable when reading from s3 (:issue:`12198`)
- A simple version of ``Panel.round()`` is now implemented (:issue:`11763`)
- For Python 3.x, ``round(DataFrame)``, ``round(Series)``, ``round(Panel)`` will work (:issue:`11763`)
- ``sys.getsizeof(obj)`` returns the memory usage of a pandas object, including the
  values it contains (:issue:`11597`)
- ``Series`` gained an ``is_unique`` attribute (:issue:`11946`)
- ``DataFrame.quantile`` and ``Series.quantile`` now accept ``interpolation`` keyword (:issue:`10174`).
- Added ``DataFrame.style.format`` for more flexible formatting of cell values (:issue:`11692`)
- ``DataFrame.select_dtypes`` now allows the ``np.float16`` type code (:issue:`11990`)
- ``pivot_table()`` now accepts most iterables for the ``values`` parameter (:issue:`12017`)
- Added Google ``BigQuery`` service account authentication support, which enables authentication on remote servers. (:issue:`11881`, :issue:`12572`). For further details see `here <https://pandas-gbq.readthedocs.io/en/latest/intro.html>`__
- ``HDFStore`` is now iterable: ``for k in store`` is equivalent to ``for k in store.keys()`` (:issue:`12221`).
- Add missing methods/fields to ``.dt`` for ``Period`` (:issue:`8848`)
- The entire code base has been ``PEP``-ified (:issue:`12096`)

.. _whatsnew_0180.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- the leading white spaces have been removed from the output of ``.to_string(index=False)`` method (:issue:`11833`)
- the ``out`` parameter has been removed from the ``Series.round()`` method. (:issue:`11763`)
- ``DataFrame.round()`` leaves non-numeric columns unchanged in its return, rather than raises. (:issue:`11885`)
- ``DataFrame.head(0)`` and ``DataFrame.tail(0)`` return empty frames, rather than ``self``.  (:issue:`11937`)
- ``Series.head(0)`` and ``Series.tail(0)`` return empty series, rather than ``self``.  (:issue:`11937`)
- ``to_msgpack`` and ``read_msgpack`` encoding now defaults to ``'utf-8'``. (:issue:`12170`)
- the order of keyword arguments to text file parsing functions (``.read_csv()``, ``.read_table()``, ``.read_fwf()``) changed to group related arguments. (:issue:`11555`)
- ``NaTType.isoformat`` now returns the string ``'NaT`` to allow the result to
  be passed to the constructor of ``Timestamp``. (:issue:`12300`)

NaT and Timedelta operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``NaT`` and ``Timedelta`` have expanded arithmetic operations, which are extended to ``Series``
arithmetic where applicable.  Operations defined for ``datetime64[ns]`` or ``timedelta64[ns]``
are now also defined for ``NaT`` (:issue:`11564`).

``NaT`` now supports arithmetic operations with integers and floats.

.. ipython:: python

   pd.NaT * 1
   pd.NaT * 1.5
   pd.NaT / 2
   pd.NaT * np.nan

``NaT`` defines more arithmetic operations with ``datetime64[ns]`` and ``timedelta64[ns]``.

.. ipython:: python

   pd.NaT / pd.NaT
   pd.Timedelta('1s') / pd.NaT

``NaT`` may represent either a ``datetime64[ns]`` null or a ``timedelta64[ns]`` null.
Given the ambiguity, it is treated as a ``timedelta64[ns]``, which allows more operations
to succeed.

.. ipython:: python

   pd.NaT + pd.NaT

   # same as
   pd.Timedelta('1s') + pd.Timedelta('1s')

as opposed to

.. code-block:: ipython

   In [3]: pd.Timestamp('19900315') + pd.Timestamp('19900315')
   TypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'

However, when wrapped in a ``Series`` whose ``dtype`` is ``datetime64[ns]`` or ``timedelta64[ns]``,
the ``dtype`` information is respected.

.. code-block:: ipython

   In [1]: pd.Series([pd.NaT], dtype='<M8[ns]') + pd.Series([pd.NaT], dtype='<M8[ns]')
   TypeError: can only operate on a datetimes for subtraction,
              but the operator [__add__] was passed

.. ipython:: python

   pd.Series([pd.NaT], dtype='<m8[ns]') + pd.Series([pd.NaT], dtype='<m8[ns]')

``Timedelta`` division by ``floats`` now works.

.. ipython:: python

   pd.Timedelta('1s') / 2.0

Subtraction by ``Timedelta`` in a ``Series`` by a ``Timestamp`` works (:issue:`11925`)

.. ipython:: python

   ser = pd.Series(pd.timedelta_range('1 day', periods=3))
   ser
   pd.Timestamp('2012-01-01') - ser


``NaT.isoformat()`` now returns ``'NaT'``. This change allows
``pd.Timestamp`` to rehydrate any timestamp like object from its isoformat
(:issue:`12300`).

Changes to msgpack
^^^^^^^^^^^^^^^^^^

Forward incompatible changes in ``msgpack`` writing format were made over 0.17.0 and 0.18.0; older versions of pandas cannot read files packed by newer versions (:issue:`12129`, :issue:`10527`)

Bugs in ``to_msgpack`` and ``read_msgpack`` introduced in 0.17.0 and fixed in 0.18.0, caused files packed in Python 2 unreadable by Python 3 (:issue:`12142`). The following table describes the backward and forward compat of msgpacks.

.. warning::

   +----------------------+------------------------+
   | Packed with          | Can be unpacked with   |
   +======================+========================+
   | pre-0.17 / Python 2  | any                    |
   +----------------------+------------------------+
   | pre-0.17 / Python 3  | any                    |
   +----------------------+------------------------+
   | 0.17 / Python 2      | - ==0.17 / Python 2    |
   |                      | - >=0.18 / any Python  |
   +----------------------+------------------------+
   | 0.17 / Python 3      | >=0.18 / any Python    |
   +----------------------+------------------------+
   | 0.18                 | >= 0.18                |
   +----------------------+------------------------+


   0.18.0 is backward-compatible for reading files packed by older versions, except for files packed with 0.17 in Python 2, in which case only they can only be unpacked in Python 2.

Signature change for .rank
^^^^^^^^^^^^^^^^^^^^^^^^^^

``Series.rank`` and ``DataFrame.rank`` now have the same signature (:issue:`11759`)

Previous signature

.. code-block:: ipython

   In [3]: pd.Series([0,1]).rank(method='average', na_option='keep',
                                 ascending=True, pct=False)
   Out[3]:
   0    1
   1    2
   dtype: float64

   In [4]: pd.DataFrame([0,1]).rank(axis=0, numeric_only=None,
                                    method='average', na_option='keep',
                                    ascending=True, pct=False)
   Out[4]:
      0
   0  1
   1  2

New signature

.. ipython:: python

   pd.Series([0,1]).rank(axis=0, method='average', numeric_only=False,
                         na_option='keep', ascending=True, pct=False)
   pd.DataFrame([0,1]).rank(axis=0, method='average', numeric_only=False,
                            na_option='keep', ascending=True, pct=False)


Bug in QuarterBegin with n=0
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, the behavior of the QuarterBegin offset was inconsistent
depending on the date when the ``n`` parameter was 0. (:issue:`11406`)

The general semantics of anchored offsets for ``n=0`` is to not move the date
when it is an anchor point (e.g., a quarter start date), and otherwise roll
forward to the next anchor point.

.. ipython:: python

   d = pd.Timestamp('2014-02-01')
   d
   d + pd.offsets.QuarterBegin(n=0, startingMonth=2)
   d + pd.offsets.QuarterBegin(n=0, startingMonth=1)

For the ``QuarterBegin`` offset in previous versions, the date would be rolled
*backwards* if date was in the same month as the quarter start date.

.. code-block:: ipython

   In [3]: d = pd.Timestamp('2014-02-15')

   In [4]: d + pd.offsets.QuarterBegin(n=0, startingMonth=2)
   Out[4]: Timestamp('2014-02-01 00:00:00')

This behavior has been corrected in version 0.18.0, which is consistent with
other anchored offsets like ``MonthBegin`` and ``YearBegin``.

.. ipython:: python

   d = pd.Timestamp('2014-02-15')
   d + pd.offsets.QuarterBegin(n=0, startingMonth=2)

.. _whatsnew_0180.breaking.resample:

Resample API
^^^^^^^^^^^^

Like the change in the window functions API :ref:`above <whatsnew_0180.enhancements.moments>`, ``.resample(...)`` is changing to have a more groupby-like API. (:issue:`11732`, :issue:`12702`, :issue:`12202`, :issue:`12332`, :issue:`12334`, :issue:`12348`, :issue:`12448`).

.. ipython:: python

   np.random.seed(1234)
   df = pd.DataFrame(np.random.rand(10,4),
                     columns=list('ABCD'),
                     index=pd.date_range('2010-01-01 09:00:00',
                                         periods=10, freq='s'))
   df


**Previous API**:

You would write a resampling operation that immediately evaluates. If a ``how`` parameter was not provided, it
would default to ``how='mean'``.

.. code-block:: ipython

   In [6]: df.resample('2s')
   Out[6]:
                            A         B         C         D
   2010-01-01 09:00:00  0.485748  0.447351  0.357096  0.793615
   2010-01-01 09:00:02  0.820801  0.794317  0.364034  0.531096
   2010-01-01 09:00:04  0.433985  0.314582  0.424104  0.625733
   2010-01-01 09:00:06  0.624988  0.609738  0.633165  0.612452
   2010-01-01 09:00:08  0.510470  0.534317  0.573201  0.806949

You could also specify a ``how`` directly

.. code-block:: ipython

   In [7]: df.resample('2s', how='sum')
   Out[7]:
                            A         B         C         D
   2010-01-01 09:00:00  0.971495  0.894701  0.714192  1.587231
   2010-01-01 09:00:02  1.641602  1.588635  0.728068  1.062191
   2010-01-01 09:00:04  0.867969  0.629165  0.848208  1.251465
   2010-01-01 09:00:06  1.249976  1.219477  1.266330  1.224904
   2010-01-01 09:00:08  1.020940  1.068634  1.146402  1.613897

**New API**:

Now, you can write ``.resample(..)`` as a 2-stage operation like ``.groupby(...)``, which
yields a ``Resampler``.

.. ipython:: python
   :okwarning:

   r = df.resample('2s')
   r

Downsampling
""""""""""""

You can then use this object to perform operations.
These are downsampling operations (going from a higher frequency to a lower one).

.. ipython:: python

   r.mean()

.. ipython:: python

   r.sum()

Furthermore, resample now supports ``getitem`` operations to perform the resample on specific columns.

.. ipython:: python

   r[['A','C']].mean()

and ``.aggregate`` type operations.

.. ipython:: python

   r.agg({'A' : 'mean', 'B' : 'sum'})

These accessors can of course, be combined

.. ipython:: python

   r[['A','B']].agg(['mean','sum'])

Upsampling
""""""""""

.. currentmodule:: pandas.tseries.resample

Upsampling operations take you from a lower frequency to a higher frequency. These are now
performed with the ``Resampler`` objects with :meth:`~Resampler.backfill`,
:meth:`~Resampler.ffill`, :meth:`~Resampler.fillna` and :meth:`~Resampler.asfreq` methods.

.. ipython:: python

   s = pd.Series(np.arange(5, dtype='int64'),
                 index=pd.date_range('2010-01-01', periods=5, freq='Q'))
   s

Previously

.. code-block:: ipython

   In [6]: s.resample('M', fill_method='ffill')
   Out[6]:
   2010-03-31    0
   2010-04-30    0
   2010-05-31    0
   2010-06-30    1
   2010-07-31    1
   2010-08-31    1
   2010-09-30    2
   2010-10-31    2
   2010-11-30    2
   2010-12-31    3
   2011-01-31    3
   2011-02-28    3
   2011-03-31    4
   Freq: M, dtype: int64

New API

.. ipython:: python

   s.resample('M').ffill()

.. note::

   In the new API, you can either downsample OR upsample. The prior implementation would allow you to pass an aggregator function (like ``mean``) even though you were upsampling, providing a bit of confusion.

Previous API will work but with deprecations
""""""""""""""""""""""""""""""""""""""""""""

.. warning::

   This new API for resample includes some internal changes for the prior-to-0.18.0 API, to work with a deprecation warning in most cases, as the resample operation returns a deferred object. We can intercept operations and just do what the (pre 0.18.0) API did (with a warning). Here is a typical use case:

   .. code-block:: ipython

      In [4]: r = df.resample('2s')

      In [6]: r*10
      pandas/tseries/resample.py:80: FutureWarning: .resample() is now a deferred operation
      use .resample(...).mean() instead of .resample(...)

      Out[6]:
                            A         B         C         D
      2010-01-01 09:00:00  4.857476  4.473507  3.570960  7.936154
      2010-01-01 09:00:02  8.208011  7.943173  3.640340  5.310957
      2010-01-01 09:00:04  4.339846  3.145823  4.241039  6.257326
      2010-01-01 09:00:06  6.249881  6.097384  6.331650  6.124518
      2010-01-01 09:00:08  5.104699  5.343172  5.732009  8.069486

   However, getting and assignment operations directly on a ``Resampler`` will raise a ``ValueError``:

   .. code-block:: ipython

      In [7]: r.iloc[0] = 5
      ValueError: .resample() is now a deferred operation
      use .resample(...).mean() instead of .resample(...)

   There is a situation where the new API can not perform all the operations when using original code.
   This code is intending to resample every 2s, take the ``mean`` AND then take the ``min`` of those results.

   .. code-block:: ipython

      In [4]: df.resample('2s').min()
      Out[4]:
      A    0.433985
      B    0.314582
      C    0.357096
      D    0.531096
      dtype: float64

   The new API will:

   .. ipython:: python

      df.resample('2s').min()

   The good news is the return dimensions will differ between the new API and the old API, so this should loudly raise
   an exception.

   To replicate the original operation

   .. ipython:: python

      df.resample('2s').mean().min()

Changes to eval
^^^^^^^^^^^^^^^

In prior versions, new columns assignments in an ``eval`` expression resulted
in an inplace change to the ``DataFrame``. (:issue:`9297`, :issue:`8664`, :issue:`10486`)

.. ipython:: python

   df = pd.DataFrame({'a': np.linspace(0, 10, 5), 'b': range(5)})
   df

.. ipython:: python
   :suppress:

   df.eval('c = a + b', inplace=True)

.. code-block:: ipython

   In [12]: df.eval('c = a + b')
   FutureWarning: eval expressions containing an assignment currentlydefault to operating inplace.
   This will change in a future version of pandas, use inplace=True to avoid this warning.

   In [13]: df
   Out[13]:
         a  b     c
   0   0.0  0   0.0
   1   2.5  1   3.5
   2   5.0  2   7.0
   3   7.5  3  10.5
   4  10.0  4  14.0

In version 0.18.0, a new ``inplace`` keyword was added to choose whether the
assignment should be done inplace or return a copy.

.. ipython:: python

   df
   df.eval('d = c - b', inplace=False)
   df
   df.eval('d = c - b', inplace=True)
   df

.. warning::

   For backwards compatibility, ``inplace`` defaults to ``True`` if not specified.
   This will change in a future version of pandas. If your code depends on an
   inplace assignment you should update to explicitly set ``inplace=True``

The ``inplace`` keyword parameter was also added the ``query`` method.

.. ipython:: python

   df.query('a > 5')
   df.query('a > 5', inplace=True)
   df

.. warning::

   Note that the default value for ``inplace`` in a ``query``
   is ``False``, which is consistent with prior versions.

``eval`` has also been updated to allow multi-line expressions for multiple
assignments.  These expressions will be evaluated one at a time in order.  Only
assignments are valid for multi-line expressions.

.. ipython:: python

   df
   df.eval("""
   e = d + a
   f = e - 22
   g = f / 2.0""", inplace=True)
   df


.. _whatsnew_0180.api:

Other API changes
^^^^^^^^^^^^^^^^^
- ``DataFrame.between_time`` and ``Series.between_time`` now only parse a fixed set of time strings. Parsing of date strings is no longer supported and raises a ``ValueError``. (:issue:`11818`)

  .. ipython:: python

     s = pd.Series(range(10), pd.date_range('2015-01-01', freq='H', periods=10))
     s.between_time("7:00am", "9:00am")

  This will now raise.

  .. code-block:: ipython

     In [2]: s.between_time('20150101 07:00:00','20150101 09:00:00')
     ValueError: Cannot convert arg ['20150101 07:00:00'] to a time.

- ``.memory_usage()`` now includes values in the index, as does memory_usage in ``.info()`` (:issue:`11597`)
- ``DataFrame.to_latex()`` now supports non-ascii encodings (eg ``utf-8``) in Python 2 with the parameter ``encoding`` (:issue:`7061`)
- ``pandas.merge()`` and ``DataFrame.merge()`` will show a specific error message when trying to merge with an object that is not of type ``DataFrame`` or a subclass (:issue:`12081`)
- ``DataFrame.unstack`` and ``Series.unstack`` now take ``fill_value`` keyword to allow direct replacement of missing values when an unstack results in missing values in the resulting ``DataFrame``. As an added benefit, specifying ``fill_value`` will preserve the data type of the original stacked data.  (:issue:`9746`)
- As part of the new API for :ref:`window functions <whatsnew_0180.enhancements.moments>` and :ref:`resampling <whatsnew_0180.breaking.resample>`, aggregation functions have been clarified, raising more informative error messages on invalid aggregations. (:issue:`9052`). A full set of examples are presented in :ref:`groupby <groupby.aggregate>`.
- Statistical functions for ``NDFrame`` objects (like ``sum(), mean(), min()``) will now raise if non-numpy-compatible arguments are passed in for ``**kwargs`` (:issue:`12301`)
- ``.to_latex`` and ``.to_html`` gain a ``decimal`` parameter like ``.to_csv``; the default is ``'.'`` (:issue:`12031`)
- More helpful error message when constructing a ``DataFrame`` with empty data but with indices (:issue:`8020`)
- ``.describe()`` will now properly handle bool dtype as a categorical (:issue:`6625`)
- More helpful error message with an invalid ``.transform`` with user defined input (:issue:`10165`)
- Exponentially weighted functions now allow specifying alpha directly (:issue:`10789`) and raise ``ValueError`` if parameters violate ``0 < alpha <= 1`` (:issue:`12492`)

.. _whatsnew_0180.deprecations:

Deprecations
^^^^^^^^^^^^

.. _whatsnew_0180.window_deprecations:

- The functions ``pd.rolling_*``, ``pd.expanding_*``, and ``pd.ewm*`` are deprecated and replaced by the corresponding method call. Note that
  the new suggested syntax includes all of the arguments (even if default) (:issue:`11603`)

  .. code-block:: ipython

     In [1]: s = pd.Series(range(3))

     In [2]: pd.rolling_mean(s,window=2,min_periods=1)
             FutureWarning: pd.rolling_mean is deprecated for Series and
                  will be removed in a future version, replace with
                  Series.rolling(min_periods=1,window=2,center=False).mean()
     Out[2]:
             0    0.0
             1    0.5
             2    1.5
             dtype: float64

     In [3]: pd.rolling_cov(s, s, window=2)
             FutureWarning: pd.rolling_cov is deprecated for Series and
                  will be removed in a future version, replace with
                  Series.rolling(window=2).cov(other=<Series>)
     Out[3]:
             0    NaN
             1    0.5
             2    0.5
             dtype: float64

- The ``freq`` and ``how`` arguments to the ``.rolling``, ``.expanding``, and ``.ewm`` (new) functions are deprecated, and will be removed in a future version. You can simply resample the input prior to creating a window function. (:issue:`11603`).

  For example, instead of ``s.rolling(window=5,freq='D').max()`` to get the max value on a rolling 5 Day window, one could use ``s.resample('D').mean().rolling(window=5).max()``, which first resamples the data to daily data, then provides a rolling 5 day window.

- ``pd.tseries.frequencies.get_offset_name`` function is deprecated. Use offset's ``.freqstr`` property as alternative (:issue:`11192`)
- ``pandas.stats.fama_macbeth`` routines are deprecated and will be removed in a future version (:issue:`6077`)
- ``pandas.stats.ols``, ``pandas.stats.plm`` and ``pandas.stats.var`` routines are deprecated and will be removed in a future version (:issue:`6077`)
- show a ``FutureWarning`` rather than a ``DeprecationWarning`` on using long-time deprecated syntax in ``HDFStore.select``, where the ``where`` clause is not a string-like (:issue:`12027`)

- The ``pandas.options.display.mpl_style`` configuration has been deprecated
  and will be removed in a future version of pandas. This functionality
  is better handled by matplotlib's `style sheets`_ (:issue:`11783`).


.. _style sheets: http://matplotlib.org/users/style_sheets.html

.. _whatsnew_0180.float_indexers:

Removal of deprecated float indexers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In :issue:`4892` indexing with floating point numbers on a non-``Float64Index`` was deprecated (in version 0.14.0).
In 0.18.0, this deprecation warning is removed and these will now raise a ``TypeError``. (:issue:`12165`, :issue:`12333`)

.. ipython:: python

   s = pd.Series([1, 2, 3], index=[4, 5, 6])
   s
   s2 = pd.Series([1, 2, 3], index=list('abc'))
   s2

Previous behavior:

.. code-block:: ipython

   # this is label indexing
   In [2]: s[5.0]
   FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
   Out[2]: 2

   # this is positional indexing
   In [3]: s.iloc[1.0]
   FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
   Out[3]: 2

   # this is label indexing
   In [4]: s.loc[5.0]
   FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
   Out[4]: 2

   # .ix would coerce 1.0 to the positional 1, and index
   In [5]: s2.ix[1.0] = 10
   FutureWarning: scalar indexers for index type Index should be integers and not floating point

   In [6]: s2
   Out[6]:
   a     1
   b    10
   c     3
   dtype: int64

New behavior:

For iloc, getting & setting via a float scalar will always raise.

.. code-block:: ipython

   In [3]: s.iloc[2.0]
   TypeError: cannot do label indexing on <class 'pandas.indexes.numeric.Int64Index'> with these indexers [2.0] of <type 'float'>

Other indexers will coerce to a like integer for both getting and setting. The ``FutureWarning`` has been dropped for ``.loc``, ``.ix`` and ``[]``.

.. ipython:: python

   s[5.0]
   s.loc[5.0]

and setting

.. ipython:: python

   s_copy = s.copy()
   s_copy[5.0] = 10
   s_copy
   s_copy = s.copy()
   s_copy.loc[5.0] = 10
   s_copy

Positional setting with ``.ix`` and a float indexer will ADD this value to the index, rather than previously setting the value by position.

.. code-block:: ipython

   In [3]: s2.ix[1.0] = 10
   In [4]: s2
   Out[4]:
   a       1
   b       2
   c       3
   1.0    10
   dtype: int64

Slicing will also coerce integer-like floats to integers for a non-``Float64Index``.

.. ipython:: python

   s.loc[5.0:6]

Note that for floats that are NOT coercible to ints, the label based bounds will be excluded

.. ipython:: python

   s.loc[5.1:6]

Float indexing on a ``Float64Index`` is unchanged.

.. ipython:: python

   s = pd.Series([1, 2, 3], index=np.arange(3.))
   s[1.0]
   s[1.0:2.5]

.. _whatsnew_0180.prior_deprecations:

Removal of prior version deprecations/changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Removal of ``rolling_corr_pairwise`` in favor of ``.rolling().corr(pairwise=True)`` (:issue:`4950`)
- Removal of ``expanding_corr_pairwise`` in favor of ``.expanding().corr(pairwise=True)`` (:issue:`4950`)
- Removal of ``DataMatrix`` module. This was not imported into the pandas namespace in any event (:issue:`12111`)
- Removal of ``cols`` keyword in favor of ``subset`` in ``DataFrame.duplicated()`` and ``DataFrame.drop_duplicates()`` (:issue:`6680`)
- Removal of the ``read_frame`` and ``frame_query`` (both aliases for ``pd.read_sql``)
  and ``write_frame`` (alias of ``to_sql``) functions in the ``pd.io.sql`` namespace,
  deprecated since 0.14.0 (:issue:`6292`).
- Removal of the ``order`` keyword from ``.factorize()`` (:issue:`6930`)

.. _whatsnew_0180.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of ``andrews_curves`` (:issue:`11534`)
- Improved huge ``DatetimeIndex``, ``PeriodIndex`` and ``TimedeltaIndex``'s ops performance including ``NaT`` (:issue:`10277`)
- Improved performance of ``pandas.concat`` (:issue:`11958`)
- Improved performance of ``StataReader`` (:issue:`11591`)
- Improved performance in construction of ``Categoricals`` with ``Series`` of datetimes containing ``NaT`` (:issue:`12077`)


- Improved performance of ISO 8601 date parsing for dates without separators (:issue:`11899`), leading zeros (:issue:`11871`) and with white space preceding the time zone (:issue:`9714`)




.. _whatsnew_0180.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in ``GroupBy.size`` when data-frame is empty. (:issue:`11699`)
- Bug in ``Period.end_time`` when a multiple of time period is requested (:issue:`11738`)
- Regression in ``.clip`` with tz-aware datetimes (:issue:`11838`)
- Bug in ``date_range`` when the boundaries fell on the frequency (:issue:`11804`, :issue:`12409`)
- Bug in consistency of passing nested dicts to ``.groupby(...).agg(...)`` (:issue:`9052`)
- Accept unicode in ``Timedelta`` constructor (:issue:`11995`)
- Bug in value label reading for ``StataReader`` when reading incrementally (:issue:`12014`)
- Bug in vectorized ``DateOffset`` when ``n`` parameter is ``0`` (:issue:`11370`)
- Compat for numpy 1.11 w.r.t. ``NaT`` comparison changes (:issue:`12049`)
- Bug in ``read_csv`` when reading from a ``StringIO`` in threads (:issue:`11790`)
- Bug in not treating ``NaT`` as a missing value in datetimelikes when factorizing & with ``Categoricals`` (:issue:`12077`)
- Bug in getitem when the values of a ``Series`` were tz-aware (:issue:`12089`)
- Bug in ``Series.str.get_dummies`` when one of the variables was 'name' (:issue:`12180`)
- Bug in ``pd.concat`` while concatenating tz-aware NaT series. (:issue:`11693`, :issue:`11755`, :issue:`12217`)
- Bug in ``pd.read_stata`` with version <= 108 files (:issue:`12232`)
- Bug in ``Series.resample`` using a frequency of ``Nano`` when the index is a ``DatetimeIndex`` and contains non-zero nanosecond parts (:issue:`12037`)
- Bug in resampling with ``.nunique`` and a sparse index (:issue:`12352`)
- Removed some compiler warnings (:issue:`12471`)
- Work around compat issues with ``boto`` in python 3.5 (:issue:`11915`)
- Bug in ``NaT`` subtraction from ``Timestamp`` or ``DatetimeIndex`` with timezones (:issue:`11718`)
- Bug in subtraction of ``Series`` of a single tz-aware ``Timestamp`` (:issue:`12290`)
- Use compat iterators in PY2 to support ``.next()`` (:issue:`12299`)
- Bug in ``Timedelta.round`` with negative values (:issue:`11690`)
- Bug in ``.loc`` against ``CategoricalIndex`` may result in normal ``Index`` (:issue:`11586`)
- Bug in ``DataFrame.info`` when duplicated column names exist (:issue:`11761`)
- Bug in ``.copy`` of datetime tz-aware objects (:issue:`11794`)
- Bug in ``Series.apply`` and ``Series.map`` where ``timedelta64`` was not boxed (:issue:`11349`)
- Bug in ``DataFrame.set_index()`` with tz-aware ``Series`` (:issue:`12358`)



- Bug in subclasses of ``DataFrame`` where ``AttributeError`` did not propagate (:issue:`11808`)
- Bug groupby on tz-aware data where selection not returning ``Timestamp`` (:issue:`11616`)
- Bug in ``pd.read_clipboard`` and ``pd.to_clipboard`` functions not supporting Unicode; upgrade included ``pyperclip`` to v1.5.15 (:issue:`9263`)
- Bug in ``DataFrame.query`` containing an assignment (:issue:`8664`)

- Bug in ``from_msgpack`` where ``__contains__()`` fails for columns of the unpacked ``DataFrame``, if the ``DataFrame`` has object columns. (:issue:`11880`)
- Bug in ``.resample`` on categorical data with ``TimedeltaIndex`` (:issue:`12169`)


- Bug in timezone info lost when broadcasting scalar datetime to ``DataFrame`` (:issue:`11682`)
- Bug in ``Index`` creation from ``Timestamp`` with mixed tz coerces to UTC (:issue:`11488`)
- Bug in ``to_numeric`` where it does not raise if input is more than one dimension (:issue:`11776`)
- Bug in parsing timezone offset strings with non-zero minutes (:issue:`11708`)
- Bug in ``df.plot`` using incorrect colors for bar plots under matplotlib 1.5+ (:issue:`11614`)
- Bug in the ``groupby`` ``plot`` method when using keyword arguments (:issue:`11805`).
- Bug in ``DataFrame.duplicated`` and ``drop_duplicates`` causing spurious matches when setting ``keep=False`` (:issue:`11864`)
- Bug in ``.loc`` result with duplicated key may have ``Index`` with incorrect dtype (:issue:`11497`)
- Bug in ``pd.rolling_median`` where memory allocation failed even with sufficient memory (:issue:`11696`)
- Bug in ``DataFrame.style`` with spurious zeros (:issue:`12134`)
- Bug in ``DataFrame.style`` with integer columns not starting at 0 (:issue:`12125`)
- Bug in ``.style.bar`` may not rendered properly using specific browser (:issue:`11678`)
- Bug in rich comparison of ``Timedelta`` with a ``numpy.array`` of ``Timedelta`` that caused an infinite recursion (:issue:`11835`)
- Bug in ``DataFrame.round`` dropping column index name (:issue:`11986`)
- Bug in ``df.replace`` while replacing value in mixed dtype ``Dataframe`` (:issue:`11698`)
- Bug in ``Index`` prevents copying name of passed ``Index``, when a new name is not provided (:issue:`11193`)
- Bug in ``read_excel`` failing to read any non-empty sheets when empty sheets exist and ``sheetname=None`` (:issue:`11711`)
- Bug in ``read_excel`` failing to raise ``NotImplemented`` error when keywords ``parse_dates`` and ``date_parser`` are provided (:issue:`11544`)
- Bug in ``read_sql`` with ``pymysql`` connections failing to return chunked data (:issue:`11522`)
- Bug in ``.to_csv`` ignoring formatting parameters ``decimal``, ``na_rep``, ``float_format`` for float indexes (:issue:`11553`)
- Bug in ``Int64Index`` and ``Float64Index`` preventing the use of the modulo operator (:issue:`9244`)
- Bug in ``MultiIndex.drop`` for not lexsorted MultiIndexes (:issue:`12078`)

- Bug in ``DataFrame`` when masking an empty ``DataFrame`` (:issue:`11859`)


- Bug in ``.plot`` potentially modifying the ``colors`` input when the number of columns didn't match the number of series provided (:issue:`12039`).
- Bug in ``Series.plot`` failing when index has a ``CustomBusinessDay`` frequency (:issue:`7222`).
- Bug in ``.to_sql`` for ``datetime.time`` values with sqlite fallback (:issue:`8341`)
- Bug in ``read_excel`` failing to read data with one column when ``squeeze=True`` (:issue:`12157`)
- Bug in ``read_excel`` failing to read one empty column (:issue:`12292`, :issue:`9002`)
- Bug in ``.groupby`` where a ``KeyError`` was not raised for a wrong column if there was only one row in the dataframe (:issue:`11741`)
- Bug in ``.read_csv`` with dtype specified on empty data producing an error (:issue:`12048`)
- Bug in ``.read_csv`` where strings like ``'2E'`` are treated as valid floats (:issue:`12237`)
- Bug in building *pandas* with debugging symbols (:issue:`12123`)


- Removed ``millisecond`` property of ``DatetimeIndex``. This would always raise a ``ValueError`` (:issue:`12019`).
- Bug in ``Series`` constructor with read-only data (:issue:`11502`)
- Removed ``pandas._testing.choice()``.  Should use ``np.random.choice()``, instead. (:issue:`12386`)
- Bug in ``.loc`` setitem indexer preventing the use of a TZ-aware DatetimeIndex (:issue:`12050`)
- Bug in ``.style`` indexes and MultiIndexes not appearing (:issue:`11655`)
- Bug in ``to_msgpack`` and ``from_msgpack`` which did not correctly serialize or deserialize ``NaT`` (:issue:`12307`).
- Bug in ``.skew`` and ``.kurt`` due to roundoff error for highly similar values (:issue:`11974`)
- Bug in ``Timestamp`` constructor where microsecond resolution was lost if HHMMSS were not separated with ':' (:issue:`10041`)
- Bug in ``buffer_rd_bytes`` src->buffer could be freed more than once if reading failed, causing a segfault (:issue:`12098`)

- Bug in ``crosstab`` where arguments with non-overlapping indexes would return a ``KeyError`` (:issue:`10291`)

- Bug in ``DataFrame.apply`` in which reduction was not being prevented for cases in which ``dtype`` was not a numpy dtype (:issue:`12244`)
- Bug when initializing categorical series with a scalar value. (:issue:`12336`)
- Bug when specifying a UTC ``DatetimeIndex`` by setting ``utc=True`` in ``.to_datetime`` (:issue:`11934`)
- Bug when increasing the buffer size of CSV reader in ``read_csv`` (:issue:`12494`)
- Bug when setting columns of a ``DataFrame`` with duplicate column names (:issue:`12344`)


.. _whatsnew_0.18.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.17.1..v0.18.0
.. _whatsnew_0192:

Version 0.19.2 (December 24, 2016)
----------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a minor bug-fix release in the 0.19.x series and includes some small regression fixes,
bug fixes and performance improvements.
We recommend that all users upgrade to this version.

Highlights include:

- Compatibility with Python 3.6
- Added a `Pandas Cheat Sheet <https://github.com/pandas-dev/pandas/tree/main/doc/cheatsheet/Pandas_Cheat_Sheet.pdf>`__. (:issue:`13202`).


.. contents:: What's new in v0.19.2
    :local:
    :backlinks: none


.. _whatsnew_0192.enhancements:

Enhancements
~~~~~~~~~~~~

The ``pd.merge_asof()``, added in 0.19.0, gained some improvements:

- ``pd.merge_asof()`` gained ``left_index``/``right_index`` and ``left_by``/``right_by`` arguments (:issue:`14253`)
- ``pd.merge_asof()`` can take multiple columns in ``by`` parameter and has specialized dtypes for better performance (:issue:`13936`)


.. _whatsnew_0192.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Performance regression with ``PeriodIndex`` (:issue:`14822`)
- Performance regression in indexing with getitem (:issue:`14930`)
- Improved performance of ``.replace()`` (:issue:`12745`)
- Improved performance ``Series`` creation with a datetime index and dictionary data (:issue:`14894`)


.. _whatsnew_0192.bug_fixes:

Bug fixes
~~~~~~~~~
- Compat with python 3.6 for pickling of some offsets (:issue:`14685`)
- Compat with python 3.6 for some indexing exception types (:issue:`14684`, :issue:`14689`)
- Compat with python 3.6 for deprecation warnings in the test suite (:issue:`14681`)
- Compat with python 3.6 for Timestamp pickles (:issue:`14689`)
- Compat with ``dateutil==2.6.0``; segfault reported in the testing suite (:issue:`14621`)
- Allow ``nanoseconds`` in ``Timestamp.replace`` as a kwarg (:issue:`14621`)
- Bug in ``pd.read_csv`` in which aliasing was being done for ``na_values`` when passed in as a dictionary (:issue:`14203`)
- Bug in ``pd.read_csv`` in which column indices for a dict-like ``na_values`` were not being respected (:issue:`14203`)
- Bug in ``pd.read_csv`` where reading files fails, if the number of headers is equal to the number of lines in the file (:issue:`14515`)
- Bug in ``pd.read_csv`` for the Python engine in which an unhelpful error message was being raised when multi-char delimiters were not being respected with quotes (:issue:`14582`)
- Fix bugs (:issue:`14734`, :issue:`13654`) in ``pd.read_sas`` and ``pandas.io.sas.sas7bdat.SAS7BDATReader`` that caused problems when reading a SAS file incrementally.
- Bug in ``pd.read_csv`` for the Python engine in which an unhelpful error message was being raised when ``skipfooter`` was not being respected by Python's CSV library (:issue:`13879`)
- Bug in ``.fillna()`` in which timezone aware datetime64 values were incorrectly rounded (:issue:`14872`)
- Bug in ``.groupby(..., sort=True)`` of a non-lexsorted MultiIndex when grouping with multiple levels (:issue:`14776`)
- Bug in ``pd.cut`` with negative values and a single bin (:issue:`14652`)
- Bug in ``pd.to_numeric`` where a 0 was not unsigned on a ``downcast='unsigned'`` argument (:issue:`14401`)
- Bug in plotting regular and irregular timeseries using shared axes
  (``sharex=True`` or ``ax.twinx()``) (:issue:`13341`, :issue:`14322`).
- Bug in not propagating exceptions in parsing invalid datetimes, noted in python 3.6 (:issue:`14561`)
- Bug in resampling a ``DatetimeIndex`` in local TZ, covering a DST change, which would raise ``AmbiguousTimeError`` (:issue:`14682`)
- Bug in indexing that transformed ``RecursionError`` into ``KeyError`` or ``IndexingError`` (:issue:`14554`)
- Bug in ``HDFStore`` when writing a ``MultiIndex`` when using ``data_columns=True`` (:issue:`14435`)
- Bug in ``HDFStore.append()`` when writing a ``Series`` and passing a ``min_itemsize`` argument containing a value for the ``index`` (:issue:`11412`)
- Bug when writing to a ``HDFStore`` in ``table`` format with a ``min_itemsize`` value for the ``index`` and without asking to append (:issue:`10381`)
- Bug in ``Series.groupby.nunique()`` raising an ``IndexError`` for an empty ``Series`` (:issue:`12553`)
- Bug in ``DataFrame.nlargest`` and ``DataFrame.nsmallest`` when the index had duplicate values (:issue:`13412`)
- Bug in clipboard functions on linux with python2 with unicode and separators (:issue:`13747`)
- Bug in clipboard functions on Windows 10 and python 3 (:issue:`14362`, :issue:`12807`)
- Bug in ``.to_clipboard()`` and Excel compat (:issue:`12529`)
- Bug in ``DataFrame.combine_first()`` for integer columns (:issue:`14687`).
- Bug in ``pd.read_csv()`` in which the ``dtype`` parameter was not being respected for empty data (:issue:`14712`)
- Bug in ``pd.read_csv()`` in which the ``nrows`` parameter was not being respected for large input when using the C engine for parsing (:issue:`7626`)
- Bug in ``pd.merge_asof()`` could not handle timezone-aware DatetimeIndex when a tolerance was specified (:issue:`14844`)
- Explicit check in ``to_stata`` and ``StataWriter`` for out-of-range values when writing doubles (:issue:`14618`)
- Bug in ``.plot(kind='kde')`` which did not drop missing values to generate the KDE Plot, instead generating an empty plot. (:issue:`14821`)
- Bug in ``unstack()`` if called with a list of column(s) as an argument, regardless of the dtypes of all columns, they get coerced to ``object`` (:issue:`11847`)


.. _whatsnew_0.19.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.19.1..v0.19.2
.. _whatsnew_115:

What's new in 1.1.5 (December 07, 2020)
---------------------------------------

These are the changes in pandas 1.1.5. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_115.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in addition of a timedelta-like scalar to a :class:`DatetimeIndex` raising incorrectly (:issue:`37295`)
- Fixed regression in :meth:`Series.groupby` raising when the :class:`Index` of the :class:`Series` had a tuple as its name (:issue:`37755`)
- Fixed regression in :meth:`DataFrame.loc` and :meth:`Series.loc` for ``__setitem__`` when one-dimensional tuple was given to select from :class:`MultiIndex` (:issue:`37711`)
- Fixed regression in inplace operations on :class:`Series` with ``ExtensionDtype`` with NumPy dtyped operand (:issue:`37910`)
- Fixed regression in metadata propagation for ``groupby`` iterator (:issue:`37343`)
- Fixed regression in :class:`MultiIndex` constructed from a :class:`DatetimeIndex` not retaining frequency (:issue:`35563`)
- Fixed regression in :class:`Index` constructor raising a ``AttributeError`` when passed a :class:`SparseArray` with datetime64 values (:issue:`35843`)
- Fixed regression in :meth:`DataFrame.unstack` with columns with integer dtype (:issue:`37115`)
- Fixed regression in indexing on a :class:`Series` with ``CategoricalDtype`` after unpickling (:issue:`37631`)
- Fixed regression in :meth:`DataFrame.groupby` aggregation with out-of-bounds datetime objects in an object-dtype column (:issue:`36003`)
- Fixed regression in ``df.groupby(..).rolling(..)`` with the resulting :class:`MultiIndex` when grouping by a label that is in the index (:issue:`37641`)
- Fixed regression in :meth:`DataFrame.fillna` not filling ``NaN`` after other operations such as :meth:`DataFrame.pivot` (:issue:`36495`).
- Fixed performance regression in ``df.groupby(..).rolling(..)`` (:issue:`38038`)
- Fixed regression in :meth:`MultiIndex.intersection` returning duplicates when at least one of the indexes had duplicates (:issue:`36915`)
- Fixed regression in :meth:`.GroupBy.first` and :meth:`.GroupBy.last` where ``None`` was considered a non-NA value (:issue:`38286`)

.. ---------------------------------------------------------------------------

.. _whatsnew_115.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in pytables methods in python 3.9 (:issue:`38041`)

.. ---------------------------------------------------------------------------

.. _whatsnew_115.other:

Other
~~~~~
- Only set ``-Werror`` as a compiler flag in the CI jobs (:issue:`33315`, :issue:`33314`)

.. ---------------------------------------------------------------------------

.. _whatsnew_115.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.4..v1.1.5|HEAD
.. _whatsnew_121:

What's new in 1.2.1 (January 20, 2021)
--------------------------------------

These are the changes in pandas 1.2.1. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_121.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :meth:`~DataFrame.to_csv` that created corrupted zip files when there were more rows than ``chunksize`` (:issue:`38714`)
- Fixed regression in :meth:`~DataFrame.to_csv` opening ``codecs.StreamReaderWriter`` in binary mode instead of in text mode (:issue:`39247`)
- Fixed regression in :meth:`read_csv` and other read functions were the encoding error policy (``errors``) did not default to ``"replace"`` when no encoding was specified (:issue:`38989`)
- Fixed regression in :func:`read_excel` with non-rawbyte file handles (:issue:`38788`)
- Fixed regression in :meth:`DataFrame.to_stata` not removing the created file when an error occurred (:issue:`39202`)
- Fixed regression in ``DataFrame.__setitem__`` raising ``ValueError`` when expanding :class:`DataFrame` and new column is from type ``"0 - name"`` (:issue:`39010`)
- Fixed regression in setting with :meth:`DataFrame.loc`  raising ``ValueError`` when :class:`DataFrame` has unsorted :class:`MultiIndex` columns and indexer is a scalar (:issue:`38601`)
- Fixed regression in setting with :meth:`DataFrame.loc` raising ``KeyError`` with :class:`MultiIndex` and list-like columns indexer enlarging :class:`DataFrame` (:issue:`39147`)
- Fixed regression in :meth:`~DataFrame.groupby()` with :class:`Categorical` grouping column not showing unused categories for ``grouped.indices`` (:issue:`38642`)
- Fixed regression in :meth:`.GroupBy.sem` where the presence of non-numeric columns would cause an error instead of being dropped (:issue:`38774`)
- Fixed regression in :meth:`.DataFrameGroupBy.diff` raising for ``int8`` and ``int16`` columns (:issue:`39050`)
- Fixed regression in :meth:`DataFrame.groupby` when aggregating an ``ExtensionDType`` that could fail for non-numeric values (:issue:`38980`)
- Fixed regression in :meth:`.Rolling.skew` and :meth:`.Rolling.kurt` modifying the object inplace (:issue:`38908`)
- Fixed regression in :meth:`DataFrame.any` and :meth:`DataFrame.all` not returning a result for tz-aware ``datetime64`` columns (:issue:`38723`)
- Fixed regression in :meth:`DataFrame.apply` with ``axis=1`` using str accessor in apply function (:issue:`38979`)
- Fixed regression in :meth:`DataFrame.replace` raising ``ValueError`` when :class:`DataFrame` has dtype ``bytes`` (:issue:`38900`)
- Fixed regression in :meth:`Series.fillna` that raised ``RecursionError`` with ``datetime64[ns, UTC]`` dtype (:issue:`38851`)
- Fixed regression in comparisons between ``NaT`` and ``datetime.date`` objects incorrectly returning ``True`` (:issue:`39151`)
- Fixed regression in calling NumPy :func:`~numpy.ufunc.accumulate` ufuncs on DataFrames, e.g. ``np.maximum.accumulate(df)`` (:issue:`39259`)
- Fixed regression in repr of float-like strings of an ``object`` dtype having trailing 0's truncated after the decimal (:issue:`38708`)
- Fixed regression that raised ``AttributeError`` with PyArrow versions [0.16.0, 1.0.0) (:issue:`38801`)
- Fixed regression in :func:`pandas.testing.assert_frame_equal` raising ``TypeError`` with ``check_like=True`` when :class:`Index` or columns have mixed dtype (:issue:`39168`)

We have reverted a commit that resulted in several plotting related regressions in pandas 1.2.0 (:issue:`38969`, :issue:`38736`, :issue:`38865`, :issue:`38947` and :issue:`39126`).
As a result, bugs reported as fixed in pandas 1.2.0 related to inconsistent tick labeling in bar plots are again present (:issue:`26186` and :issue:`11465`)

.. ---------------------------------------------------------------------------

.. _whatsnew_121.ufunc_deprecation:

Calling NumPy ufuncs on non-aligned DataFrames
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before pandas 1.2.0, calling a NumPy ufunc on non-aligned DataFrames (or
DataFrame / Series combination) would ignore the indices, only match
the inputs by shape, and use the index/columns of the first DataFrame for
the result:

.. code-block:: ipython

    In [1]: df1 = pd.DataFrame({"a": [1, 2], "b": [3, 4]}, index=[0, 1])
    In [2]: df2 = pd.DataFrame({"a": [1, 2], "b": [3, 4]}, index=[1, 2])
    In [3]: df1
    Out[3]:
       a  b
    0  1  3
    1  2  4
    In [4]: df2
    Out[4]:
       a  b
    1  1  3
    2  2  4

    In [5]: np.add(df1, df2)
    Out[5]:
       a  b
    0  2  6
    1  4  8

This contrasts with how other pandas operations work, which first align
the inputs:

.. code-block:: ipython

    In [6]: df1 + df2
    Out[6]:
         a    b
    0  NaN  NaN
    1  3.0  7.0
    2  NaN  NaN

In pandas 1.2.0, we refactored how NumPy ufuncs are called on DataFrames, and
this started to align the inputs first (:issue:`39184`), as happens in other
pandas operations and as it happens for ufuncs called on Series objects.

For pandas 1.2.1, we restored the previous behaviour to avoid a breaking
change, but the above example of ``np.add(df1, df2)`` with non-aligned inputs
will now to raise a warning, and a future pandas 2.0 release will start
aligning the inputs first (:issue:`39184`). Calling a NumPy ufunc on Series
objects (eg ``np.add(s1, s2)``) already aligns and continues to do so.

To avoid the warning and keep the current behaviour of ignoring the indices,
convert one of the arguments to a NumPy array:

.. code-block:: ipython

    In [7]: np.add(df1, np.asarray(df2))
    Out[7]:
       a  b
    0  2  6
    1  4  8

To obtain the future behaviour and silence the warning, you can align manually
before passing the arguments to the ufunc:

.. code-block:: ipython

    In [8]: df1, df2 = df1.align(df2)
    In [9]: np.add(df1, df2)
    Out[9]:
         a    b
    0  NaN  NaN
    1  3.0  7.0
    2  NaN  NaN

.. ---------------------------------------------------------------------------

.. _whatsnew_121.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in :meth:`read_csv` with ``float_precision="high"`` caused segfault or wrong parsing of long exponent strings. This resulted in a regression in some cases as the default for ``float_precision`` was changed in pandas 1.2.0 (:issue:`38753`)
- Bug in :func:`read_csv` not closing an opened file handle when a ``csv.Error`` or ``UnicodeDecodeError`` occurred while initializing (:issue:`39024`)
- Bug in :func:`pandas.testing.assert_index_equal` raising ``TypeError`` with ``check_order=False`` when :class:`Index` has mixed dtype (:issue:`39168`)

.. ---------------------------------------------------------------------------

.. _whatsnew_121.other:

Other
~~~~~

- The deprecated attributes ``_AXIS_NAMES`` and ``_AXIS_NUMBERS`` of :class:`DataFrame` and :class:`Series` will no longer show up in ``dir`` or ``inspect.getmembers`` calls (:issue:`38740`)
- Bumped minimum fastparquet version to 0.4.0 to avoid ``AttributeError`` from numba (:issue:`38344`)
- Bumped minimum pymysql version to 0.8.1 to avoid test failures (:issue:`38344`)
- Fixed build failure on MacOS 11 in Python 3.9.1 (:issue:`38766`)
- Added reference to backwards incompatible ``check_freq`` arg of :func:`testing.assert_frame_equal` and :func:`testing.assert_series_equal` in :ref:`pandas 1.1.0 what's new <whatsnew_110.api_breaking.testing.check_freq>` (:issue:`34050`)

.. ---------------------------------------------------------------------------

.. _whatsnew_121.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.0..v1.2.1
.. _whatsnew_111:

What's new in 1.1.1 (August 20, 2020)
-------------------------------------

These are the changes in pandas 1.1.1. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_111.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`CategoricalIndex.format` where, when stringified scalars had different lengths, the shorter string would be right-filled with spaces, so it had the same length as the longest string (:issue:`35439`)
- Fixed regression in :meth:`Series.truncate` when trying to truncate a single-element series (:issue:`35544`)
- Fixed regression where :meth:`DataFrame.to_numpy` would raise a ``RuntimeError`` for mixed dtypes when converting to ``str`` (:issue:`35455`)
- Fixed regression where :func:`read_csv` would raise a ``ValueError`` when ``pandas.options.mode.use_inf_as_na`` was set to ``True`` (:issue:`35493`)
- Fixed regression where :func:`pandas.testing.assert_series_equal` would raise an error when non-numeric dtypes were passed with ``check_exact=True`` (:issue:`35446`)
- Fixed regression in ``.groupby(..).rolling(..)`` where column selection was ignored (:issue:`35486`)
- Fixed regression where :meth:`DataFrame.interpolate` would raise a ``TypeError`` when the :class:`DataFrame` was empty (:issue:`35598`)
- Fixed regression in :meth:`DataFrame.shift` with ``axis=1`` and heterogeneous dtypes (:issue:`35488`)
- Fixed regression in :meth:`DataFrame.diff` with read-only data (:issue:`35559`)
- Fixed regression in ``.groupby(..).rolling(..)`` where a segfault would occur with ``center=True`` and an odd number of values (:issue:`35552`)
- Fixed regression in :meth:`DataFrame.apply` where functions that altered the input in-place only operated on a single row (:issue:`35462`)
- Fixed regression in :meth:`DataFrame.reset_index` would raise a ``ValueError`` on empty :class:`DataFrame` with a :class:`MultiIndex` with a ``datetime64`` dtype level (:issue:`35606`, :issue:`35657`)
- Fixed regression where :func:`pandas.merge_asof` would raise a ``UnboundLocalError`` when ``left_index``, ``right_index`` and ``tolerance`` were set (:issue:`35558`)
- Fixed regression in ``.groupby(..).rolling(..)`` where a custom ``BaseIndexer`` would be ignored (:issue:`35557`)
- Fixed regression in :meth:`DataFrame.replace` and :meth:`Series.replace` where compiled regular expressions would be ignored during replacement (:issue:`35680`)
- Fixed regression in :meth:`~pandas.core.groupby.DataFrameGroupBy.aggregate` where a list of functions would produce the wrong results if at least one of the functions did not aggregate (:issue:`35490`)
- Fixed memory usage issue when instantiating large :class:`pandas.arrays.StringArray` (:issue:`35499`)

.. ---------------------------------------------------------------------------

.. _whatsnew_111.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in :class:`~pandas.io.formats.style.Styler` whereby ``cell_ids`` argument had no effect due to other recent changes (:issue:`35588`) (:issue:`35663`)
- Bug in :func:`pandas.testing.assert_series_equal` and :func:`pandas.testing.assert_frame_equal` where extension dtypes were not ignored when ``check_dtypes`` was set to ``False`` (:issue:`35715`)
- Bug in :meth:`to_timedelta` fails when ``arg`` is a :class:`Series` with ``Int64`` dtype containing null values (:issue:`35574`)
- Bug in ``.groupby(..).rolling(..)`` where passing ``closed`` with column selection would raise a ``ValueError`` (:issue:`35549`)
- Bug in :class:`DataFrame` constructor failing to raise ``ValueError`` in some cases when ``data`` and ``index`` have mismatched lengths (:issue:`33437`)

.. ---------------------------------------------------------------------------

.. _whatsnew_111.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.0..v1.1.1
.. _whatsnew_0200:

Version 0.20.1 (May 5, 2017)
----------------------------

{{ header }}

This is a major release from 0.19.2 and includes a number of API changes, deprecations, new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

Highlights include:

- New ``.agg()`` API for Series/DataFrame similar to the groupby-rolling-resample API's, see :ref:`here <whatsnew_0200.enhancements.agg>`
- Integration with the ``feather-format``, including a new top-level ``pd.read_feather()`` and ``DataFrame.to_feather()`` method, see :ref:`here <io.feather>`.
- The ``.ix`` indexer has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_ix>`
- ``Panel`` has been deprecated, see :ref:`here <whatsnew_0200.api_breaking.deprecate_panel>`
- Addition of an ``IntervalIndex`` and ``Interval`` scalar type, see :ref:`here <whatsnew_0200.enhancements.intervalindex>`
- Improved user API when grouping by index levels in ``.groupby()``, see :ref:`here <whatsnew_0200.enhancements.groupby_access>`
- Improved support for ``UInt64`` dtypes, see :ref:`here <whatsnew_0200.enhancements.uint64_support>`
- A new orient for JSON serialization, ``orient='table'``, that uses the Table Schema spec and that gives the possibility for a more interactive repr in the Jupyter Notebook, see :ref:`here <whatsnew_0200.enhancements.table_schema>`
- Experimental support for exporting styled DataFrames (``DataFrame.style``) to Excel, see :ref:`here <whatsnew_0200.enhancements.style_excel>`
- Window binary corr/cov operations now return a MultiIndexed ``DataFrame`` rather than a ``Panel``, as ``Panel`` is now deprecated, see :ref:`here <whatsnew_0200.api_breaking.rolling_pairwise>`
- Support for S3 handling now uses ``s3fs``, see :ref:`here <whatsnew_0200.api_breaking.s3>`
- Google BigQuery support now uses the ``pandas-gbq`` library, see :ref:`here <whatsnew_0200.api_breaking.gbq>`

.. warning::

  pandas has changed the internal structure and layout of the code base.
  This can affect imports that are not from the top-level ``pandas.*`` namespace, please see the changes :ref:`here <whatsnew_0200.privacy>`.

Check the :ref:`API Changes <whatsnew_0200.api_breaking>` and :ref:`deprecations <whatsnew_0200.deprecations>` before updating.

.. note::

   This is a combined release for 0.20.0 and 0.20.1.
   Version 0.20.1 contains one additional change for backwards-compatibility with downstream projects using pandas' ``utils`` routines. (:issue:`16250`)

.. contents:: What's new in v0.20.0
    :local:
    :backlinks: none

.. _whatsnew_0200.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0200.enhancements.agg:

Method ``agg`` API for DataFrame/Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Series & DataFrame have been enhanced to support the aggregation API. This is a familiar API
from groupby, window operations, and resampling. This allows aggregation operations in a concise way
by using :meth:`~DataFrame.agg` and :meth:`~DataFrame.transform`. The full documentation
is :ref:`here <basics.aggregate>` (:issue:`1623`).

Here is a sample

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                     index=pd.date_range('1/1/2000', periods=10))
   df.iloc[3:7] = np.nan
   df

One can operate using string function names, callables, lists, or dictionaries of these.

Using a single function is equivalent to ``.apply``.

.. ipython:: python

   df.agg('sum')

Multiple aggregations with a list of functions.

.. ipython:: python

   df.agg(['sum', 'min'])

Using a dict provides the ability to apply specific aggregations per column.
You will get a matrix-like output of all of the aggregators. The output has one column
per unique function. Those functions applied to a particular column will be ``NaN``:

.. ipython:: python

   df.agg({'A': ['sum', 'min'], 'B': ['min', 'max']})

The API also supports a ``.transform()`` function for broadcasting results.

.. ipython:: python
   :okwarning:

   df.transform(['abs', lambda x: x - x.min()])

When presented with mixed dtypes that cannot be aggregated, ``.agg()`` will only take the valid
aggregations. This is similar to how groupby ``.agg()`` works. (:issue:`15015`)

.. ipython:: python

   df = pd.DataFrame({'A': [1, 2, 3],
                      'B': [1., 2., 3.],
                      'C': ['foo', 'bar', 'baz'],
                      'D': pd.date_range('20130101', periods=3)})
   df.dtypes

.. ipython:: python
   :okwarning:

   df.agg(['min', 'sum'])

.. _whatsnew_0200.enhancements.dataio_dtype:

Keyword argument ``dtype`` for data IO
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``'python'`` engine for :func:`read_csv`, as well as the :func:`read_fwf` function for parsing
fixed-width text files and :func:`read_excel` for parsing Excel files, now accept the ``dtype`` keyword argument for specifying the types of specific columns (:issue:`14295`). See the :ref:`io docs <io.dtypes>` for more information.

.. ipython:: python
   :suppress:

   from io import StringIO

.. ipython:: python

   data = "a  b\n1  2\n3  4"
   pd.read_fwf(StringIO(data)).dtypes
   pd.read_fwf(StringIO(data), dtype={'a': 'float64', 'b': 'object'}).dtypes

.. _whatsnew_0120.enhancements.datetime_origin:

Method ``.to_datetime()`` has gained an ``origin`` parameter
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`to_datetime` has gained a new parameter, ``origin``, to define a reference date
from where to compute the resulting timestamps when parsing numerical values with a specific ``unit`` specified. (:issue:`11276`, :issue:`11745`)

For example, with 1960-01-01 as the starting date:

.. ipython:: python

   pd.to_datetime([1, 2, 3], unit='D', origin=pd.Timestamp('1960-01-01'))

The default is set at ``origin='unix'``, which defaults to ``1970-01-01 00:00:00``, which is
commonly called 'unix epoch' or POSIX time. This was the previous default, so this is a backward compatible change.

.. ipython:: python

   pd.to_datetime([1, 2, 3], unit='D')


.. _whatsnew_0200.enhancements.groupby_access:

GroupBy enhancements
^^^^^^^^^^^^^^^^^^^^

Strings passed to ``DataFrame.groupby()`` as the ``by`` parameter may now reference either column names or index level names. Previously, only column names could be referenced. This allows to easily group by a column and index level at the same time. (:issue:`5677`)

.. ipython:: python

   arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
             ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]

   index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])

   df = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 3, 3],
                      'B': np.arange(8)},
                     index=index)
   df

   df.groupby(['second', 'A']).sum()


.. _whatsnew_0200.enhancements.compressed_urls:

Better support for compressed URLs in ``read_csv``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The compression code was refactored (:issue:`12688`). As a result, reading
dataframes from URLs in :func:`read_csv` or :func:`read_table` now supports
additional compression methods: ``xz``, ``bz2``, and ``zip`` (:issue:`14570`).
Previously, only ``gzip`` compression was supported. By default, compression of
URLs and paths are now inferred using their file extensions. Additionally,
support for bz2 compression in the python 2 C-engine improved (:issue:`14874`).

.. ipython:: python

   url = ('https://github.com/{repo}/raw/{branch}/{path}'
          .format(repo='pandas-dev/pandas',
                  branch='main',
                  path='pandas/tests/io/parser/data/salaries.csv.bz2'))
   # default, infer compression
   df = pd.read_csv(url, sep='\t', compression='infer')
   # explicitly specify compression
   df = pd.read_csv(url, sep='\t', compression='bz2')
   df.head(2)

.. _whatsnew_0200.enhancements.pickle_compression:

Pickle file IO now supports compression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`read_pickle`, :meth:`DataFrame.to_pickle` and :meth:`Series.to_pickle`
can now read from and write to compressed pickle files. Compression methods
can be an explicit parameter or be inferred from the file extension.
See :ref:`the docs here. <io.pickle.compression>`

.. ipython:: python

   df = pd.DataFrame({'A': np.random.randn(1000),
                      'B': 'foo',
                      'C': pd.date_range('20130101', periods=1000, freq='s')})

Using an explicit compression type

.. ipython:: python

   df.to_pickle("data.pkl.compress", compression="gzip")
   rt = pd.read_pickle("data.pkl.compress", compression="gzip")
   rt.head()

The default is to infer the compression type from the extension (``compression='infer'``):

.. ipython:: python

   df.to_pickle("data.pkl.gz")
   rt = pd.read_pickle("data.pkl.gz")
   rt.head()
   df["A"].to_pickle("s1.pkl.bz2")
   rt = pd.read_pickle("s1.pkl.bz2")
   rt.head()

.. ipython:: python
   :suppress:

   import os
   os.remove("data.pkl.compress")
   os.remove("data.pkl.gz")
   os.remove("s1.pkl.bz2")

.. _whatsnew_0200.enhancements.uint64_support:

UInt64 support improved
^^^^^^^^^^^^^^^^^^^^^^^

pandas has significantly improved support for operations involving unsigned,
or purely non-negative, integers. Previously, handling these integers would
result in improper rounding or data-type casting, leading to incorrect results.
Notably, a new numerical index, ``UInt64Index``, has been created (:issue:`14937`)

.. code-block:: ipython

   In [1]: idx = pd.UInt64Index([1, 2, 3])
   In [2]: df = pd.DataFrame({'A': ['a', 'b', 'c']}, index=idx)
   In [3]: df.index
   Out[3]: UInt64Index([1, 2, 3], dtype='uint64')

- Bug in converting object elements of array-like objects to unsigned 64-bit integers (:issue:`4471`, :issue:`14982`)
- Bug in ``Series.unique()`` in which unsigned 64-bit integers were causing overflow (:issue:`14721`)
- Bug in ``DataFrame`` construction in which unsigned 64-bit integer elements were being converted to objects (:issue:`14881`)
- Bug in ``pd.read_csv()`` in which unsigned 64-bit integer elements were being improperly converted to the wrong data types (:issue:`14983`)
- Bug in ``pd.unique()`` in which unsigned 64-bit integers were causing overflow (:issue:`14915`)
- Bug in ``pd.value_counts()`` in which unsigned 64-bit integers were being erroneously truncated in the output (:issue:`14934`)

.. _whatsnew_0200.enhancements.groupy_categorical:

GroupBy on categoricals
^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, ``.groupby(..., sort=False)`` would fail with a ``ValueError`` when grouping on a categorical series with some categories not appearing in the data. (:issue:`13179`)

.. ipython:: python

   chromosomes = np.r_[np.arange(1, 23).astype(str), ['X', 'Y']]
   df = pd.DataFrame({
       'A': np.random.randint(100),
       'B': np.random.randint(100),
       'C': np.random.randint(100),
       'chromosomes': pd.Categorical(np.random.choice(chromosomes, 100),
                                     categories=chromosomes,
                                     ordered=True)})
   df

**Previous behavior**:

.. code-block:: ipython

   In [3]: df[df.chromosomes != '1'].groupby('chromosomes', sort=False).sum()
   ---------------------------------------------------------------------------
   ValueError: items in new_categories are not the same as in old categories

**New behavior**:

.. ipython:: python

   df[df.chromosomes != '1'].groupby('chromosomes', sort=False).sum()

.. _whatsnew_0200.enhancements.table_schema:

Table schema output
^^^^^^^^^^^^^^^^^^^

The new orient ``'table'`` for :meth:`DataFrame.to_json`
will generate a `Table Schema`_ compatible string representation of
the data.

.. ipython:: python

   df = pd.DataFrame(
       {'A': [1, 2, 3],
        'B': ['a', 'b', 'c'],
        'C': pd.date_range('2016-01-01', freq='d', periods=3)},
       index=pd.Index(range(3), name='idx'))
   df
   df.to_json(orient='table')


See :ref:`IO: Table Schema for more information <io.table_schema>`.

Additionally, the repr for ``DataFrame`` and ``Series`` can now publish
this JSON Table schema representation of the Series or DataFrame if you are
using IPython (or another frontend like `nteract`_ using the Jupyter messaging
protocol).
This gives frontends like the Jupyter notebook and `nteract`_
more flexibility in how they display pandas objects, since they have
more information about the data.
You must enable this by setting the ``display.html.table_schema`` option to ``True``.

.. _Table Schema: http://specs.frictionlessdata.io/json-table-schema/
.. _nteract: https://nteract.io/

.. _whatsnew_0200.enhancements.scipy_sparse:

SciPy sparse matrix from/to SparseDataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas now supports creating sparse dataframes directly from ``scipy.sparse.spmatrix`` instances.
See the :ref:`documentation <sparse.scipysparse>` for more information. (:issue:`4343`)

All sparse formats are supported, but matrices that are not in :mod:`COOrdinate <scipy.sparse>` format will be converted, copying data as needed.

.. code-block:: python

   from scipy.sparse import csr_matrix
   arr = np.random.random(size=(1000, 5))
   arr[arr < .9] = 0
   sp_arr = csr_matrix(arr)
   sp_arr
   sdf = pd.SparseDataFrame(sp_arr)
   sdf

To convert a ``SparseDataFrame`` back to sparse SciPy matrix in COO format, you can use:

.. code-block:: python

   sdf.to_coo()

.. _whatsnew_0200.enhancements.style_excel:

Excel output for styled DataFrames
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Experimental support has been added to export ``DataFrame.style`` formats to Excel using the ``openpyxl`` engine. (:issue:`15530`)

For example, after running the following, ``styled.xlsx`` renders as below:

.. ipython:: python
   :okwarning:

   np.random.seed(24)
   df = pd.DataFrame({'A': np.linspace(1, 10, 10)})
   df = pd.concat([df, pd.DataFrame(np.random.RandomState(24).randn(10, 4),
                                    columns=list('BCDE'))],
                  axis=1)
   df.iloc[0, 2] = np.nan
   df
   styled = (df.style
             .applymap(lambda val: 'color:red;' if val < 0 else 'color:black;')
             .highlight_max())
   styled.to_excel('styled.xlsx', engine='openpyxl')

.. image:: ../_static/style-excel.png

.. ipython:: python
   :suppress:

   import os
   os.remove('styled.xlsx')

See the :ref:`Style documentation </user_guide/style.ipynb#Export-to-Excel>` for more detail.

.. _whatsnew_0200.enhancements.intervalindex:

IntervalIndex
^^^^^^^^^^^^^

pandas has gained an ``IntervalIndex`` with its own dtype, ``interval`` as well as the ``Interval`` scalar type. These allow first-class support for interval
notation, specifically as a return type for the categories in :func:`cut` and :func:`qcut`. The ``IntervalIndex`` allows some unique indexing, see the
:ref:`docs <advanced.intervalindex>`. (:issue:`7640`, :issue:`8625`)

.. warning::

   These indexing behaviors of the IntervalIndex are provisional and may change in a future version of pandas. Feedback on usage is welcome.


Previous behavior:

The returned categories were strings, representing Intervals

.. code-block:: ipython

   In [1]: c = pd.cut(range(4), bins=2)

   In [2]: c
   Out[2]:
   [(-0.003, 1.5], (-0.003, 1.5], (1.5, 3], (1.5, 3]]
   Categories (2, object): [(-0.003, 1.5] < (1.5, 3]]

   In [3]: c.categories
   Out[3]: Index(['(-0.003, 1.5]', '(1.5, 3]'], dtype='object')

New behavior:

.. ipython:: python

   c = pd.cut(range(4), bins=2)
   c
   c.categories

Furthermore, this allows one to bin *other* data with these same bins, with ``NaN`` representing a missing
value similar to other dtypes.

.. ipython:: python

   pd.cut([0, 3, 5, 1], bins=c.categories)

An ``IntervalIndex`` can also be used in ``Series`` and ``DataFrame`` as the index.

.. ipython:: python

   df = pd.DataFrame({'A': range(4),
                      'B': pd.cut([0, 3, 1, 1], bins=c.categories)
                      }).set_index('B')
   df

Selecting via a specific interval:

.. ipython:: python

   df.loc[pd.Interval(1.5, 3.0)]

Selecting via a scalar value that is contained *in* the intervals.

.. ipython:: python

   df.loc[0]

.. _whatsnew_0200.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- ``DataFrame.rolling()`` now accepts the parameter ``closed='right'|'left'|'both'|'neither'`` to choose the rolling window-endpoint closedness. See the :ref:`documentation <window.endpoints>` (:issue:`13965`)
- Integration with the ``feather-format``, including a new top-level ``pd.read_feather()`` and ``DataFrame.to_feather()`` method, see :ref:`here <io.feather>`.
- ``Series.str.replace()`` now accepts a callable, as replacement, which is passed to ``re.sub`` (:issue:`15055`)
- ``Series.str.replace()`` now accepts a compiled regular expression as a pattern (:issue:`15446`)
- ``Series.sort_index`` accepts parameters ``kind`` and ``na_position`` (:issue:`13589`, :issue:`14444`)
- ``DataFrame`` and ``DataFrame.groupby()``  have gained a ``nunique()`` method to count the distinct values over an axis (:issue:`14336`, :issue:`15197`).
- ``DataFrame`` has gained a ``melt()`` method, equivalent to ``pd.melt()``, for unpivoting from a wide to long format (:issue:`12640`).
- ``pd.read_excel()`` now preserves sheet order when using ``sheetname=None`` (:issue:`9930`)
- Multiple offset aliases with decimal points are now supported (e.g. ``0.5min`` is parsed as ``30s``) (:issue:`8419`)
- ``.isnull()`` and ``.notnull()`` have been added to ``Index`` object to make them more consistent with the ``Series`` API (:issue:`15300`)
- New ``UnsortedIndexError`` (subclass of ``KeyError``) raised when indexing/slicing into an
  unsorted MultiIndex (:issue:`11897`). This allows differentiation between errors due to lack
  of sorting or an incorrect key. See :ref:`here <advanced.unsorted>`
- ``MultiIndex`` has gained a ``.to_frame()`` method to convert to a ``DataFrame`` (:issue:`12397`)
- ``pd.cut`` and ``pd.qcut`` now support datetime64 and timedelta64 dtypes (:issue:`14714`, :issue:`14798`)
- ``pd.qcut`` has gained the ``duplicates='raise'|'drop'`` option to control whether to raise on duplicated edges (:issue:`7751`)
- ``Series`` provides a ``to_excel`` method to output Excel files (:issue:`8825`)
- The ``usecols`` argument in ``pd.read_csv()`` now accepts a callable function as a value  (:issue:`14154`)
- The ``skiprows`` argument in ``pd.read_csv()`` now accepts a callable function as a value  (:issue:`10882`)
- The ``nrows`` and ``chunksize`` arguments in ``pd.read_csv()`` are supported if both are passed (:issue:`6774`, :issue:`15755`)
- ``DataFrame.plot`` now prints a title above each subplot if ``suplots=True`` and ``title`` is a list of strings (:issue:`14753`)
- ``DataFrame.plot`` can pass the matplotlib 2.0 default color cycle as a single string as color parameter, see `here <http://matplotlib.org/2.0.0/users/colors.html#cn-color-selection>`__. (:issue:`15516`)
- ``Series.interpolate()`` now supports timedelta as an index type with ``method='time'`` (:issue:`6424`)
- Addition of a ``level`` keyword to ``DataFrame/Series.rename`` to rename
  labels in the specified level of a MultiIndex (:issue:`4160`).
- ``DataFrame.reset_index()`` will now interpret a tuple ``index.name`` as a key spanning across levels of ``columns``, if this is a ``MultiIndex`` (:issue:`16164`)
- ``Timedelta.isoformat`` method added for formatting Timedeltas as an `ISO 8601 duration`_. See the :ref:`Timedelta docs <timedeltas.isoformat>` (:issue:`15136`)
- ``.select_dtypes()`` now allows the string ``datetimetz`` to generically select datetimes with tz (:issue:`14910`)
- The ``.to_latex()`` method will now accept ``multicolumn`` and ``multirow`` arguments to use the accompanying LaTeX enhancements
- ``pd.merge_asof()`` gained the option ``direction='backward'|'forward'|'nearest'`` (:issue:`14887`)
- ``Series/DataFrame.asfreq()`` have gained a ``fill_value`` parameter, to fill missing values (:issue:`3715`).
- ``Series/DataFrame.resample.asfreq`` have gained a ``fill_value`` parameter, to fill missing values during resampling (:issue:`3715`).
- :func:`pandas.util.hash_pandas_object` has gained the ability to hash a ``MultiIndex`` (:issue:`15224`)
- ``Series/DataFrame.squeeze()`` have gained the ``axis`` parameter. (:issue:`15339`)
- ``DataFrame.to_excel()`` has a new ``freeze_panes`` parameter to turn on Freeze Panes when exporting to Excel (:issue:`15160`)
- ``pd.read_html()`` will parse multiple header rows, creating a MultiIndex header. (:issue:`13434`).
- HTML table output skips ``colspan`` or ``rowspan`` attribute if equal to 1. (:issue:`15403`)
- :class:`pandas.io.formats.style.Styler` template now has blocks for easier extension, see the :ref:`example notebook </user_guide/style.ipynb#Subclassing>` (:issue:`15649`)
- :meth:`Styler.render() <pandas.io.formats.style.Styler.render>` now accepts ``**kwargs`` to allow user-defined variables in the template (:issue:`15649`)
- Compatibility with Jupyter notebook 5.0; MultiIndex column labels are left-aligned and MultiIndex row-labels are top-aligned (:issue:`15379`)
- ``TimedeltaIndex`` now has a custom date-tick formatter specifically designed for nanosecond level precision (:issue:`8711`)
- ``pd.api.types.union_categoricals`` gained the ``ignore_ordered`` argument to allow ignoring the ordered attribute of unioned categoricals (:issue:`13410`). See the :ref:`categorical union docs <categorical.union>` for more information.
- ``DataFrame.to_latex()`` and ``DataFrame.to_string()`` now allow optional header aliases. (:issue:`15536`)
- Re-enable the ``parse_dates`` keyword of ``pd.read_excel()`` to parse string columns as dates (:issue:`14326`)
- Added ``.empty`` property to subclasses of ``Index``. (:issue:`15270`)
- Enabled floor division for ``Timedelta`` and ``TimedeltaIndex`` (:issue:`15828`)
- ``pandas.io.json.json_normalize()`` gained the option ``errors='ignore'|'raise'``; the default is ``errors='raise'`` which is backward compatible. (:issue:`14583`)
- ``pandas.io.json.json_normalize()`` with an empty ``list`` will return an empty ``DataFrame`` (:issue:`15534`)
- ``pandas.io.json.json_normalize()`` has gained a ``sep`` option that accepts ``str`` to separate joined fields; the default is ".", which is backward compatible. (:issue:`14883`)
- :meth:`MultiIndex.remove_unused_levels` has been added to facilitate :ref:`removing unused levels <advanced.shown_levels>`. (:issue:`15694`)
- ``pd.read_csv()`` will now raise a ``ParserError`` error whenever any parsing error occurs (:issue:`15913`, :issue:`15925`)
- ``pd.read_csv()`` now supports the ``error_bad_lines`` and ``warn_bad_lines`` arguments for the Python parser (:issue:`15925`)
- The ``display.show_dimensions`` option can now also be used to specify
  whether the length of a ``Series`` should be shown in its repr (:issue:`7117`).
- ``parallel_coordinates()`` has gained a ``sort_labels`` keyword argument that sorts class labels and the colors assigned to them (:issue:`15908`)
- Options added to allow one to turn on/off using ``bottleneck`` and ``numexpr``, see :ref:`here <basics.accelerate>` (:issue:`16157`)
- ``DataFrame.style.bar()`` now accepts two more options to further customize the bar chart. Bar alignment is set with ``align='left'|'mid'|'zero'``, the default is "left", which is backward compatible; You can now pass a list of ``color=[color_negative, color_positive]``. (:issue:`14757`)

.. _ISO 8601 duration: https://en.wikipedia.org/wiki/ISO_8601#Durations


.. _whatsnew_0200.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew.api_breaking.io_compat:

Possible incompatibility for HDF5 formats created with pandas < 0.13.0
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``pd.TimeSeries`` was deprecated officially in 0.17.0, though has already been an alias since 0.13.0. It has
been dropped in favor of ``pd.Series``. (:issue:`15098`).

This *may* cause HDF5 files that were created in prior versions to become unreadable if ``pd.TimeSeries``
was used. This is most likely to be for pandas < 0.13.0. If you find yourself in this situation.
You can use a recent prior version of pandas to read in your HDF5 files,
then write them out again after applying the procedure below.

.. code-block:: ipython

   In [2]: s = pd.TimeSeries([1, 2, 3], index=pd.date_range('20130101', periods=3))

   In [3]: s
   Out[3]:
   2013-01-01    1
   2013-01-02    2
   2013-01-03    3
   Freq: D, dtype: int64

   In [4]: type(s)
   Out[4]: pandas.core.series.TimeSeries

   In [5]: s = pd.Series(s)

   In [6]: s
   Out[6]:
   2013-01-01    1
   2013-01-02    2
   2013-01-03    3
   Freq: D, dtype: int64

   In [7]: type(s)
   Out[7]: pandas.core.series.Series


.. _whatsnew_0200.api_breaking.index_map:

Map on Index types now return other Index types
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``map`` on an ``Index`` now returns an ``Index``, not a numpy array (:issue:`12766`)

.. ipython:: python

   idx = pd.Index([1, 2])
   idx
   mi = pd.MultiIndex.from_tuples([(1, 2), (2, 4)])
   mi

Previous behavior:

.. code-block:: ipython

   In [5]: idx.map(lambda x: x * 2)
   Out[5]: array([2, 4])

   In [6]: idx.map(lambda x: (x, x * 2))
   Out[6]: array([(1, 2), (2, 4)], dtype=object)

   In [7]: mi.map(lambda x: x)
   Out[7]: array([(1, 2), (2, 4)], dtype=object)

   In [8]: mi.map(lambda x: x[0])
   Out[8]: array([1, 2])

New behavior:

.. ipython:: python

   idx.map(lambda x: x * 2)
   idx.map(lambda x: (x, x * 2))

   mi.map(lambda x: x)

   mi.map(lambda x: x[0])


``map`` on a ``Series`` with ``datetime64`` values may return ``int64`` dtypes rather than ``int32``

.. ipython:: python

   s = pd.Series(pd.date_range('2011-01-02T00:00', '2011-01-02T02:00', freq='H')
                 .tz_localize('Asia/Tokyo'))
   s

Previous behavior:

.. code-block:: ipython

   In [9]: s.map(lambda x: x.hour)
   Out[9]:
   0    0
   1    1
   2    2
   dtype: int32

New behavior:

.. ipython:: python

   s.map(lambda x: x.hour)


.. _whatsnew_0200.api_breaking.index_dt_field:

Accessing datetime fields of Index now return Index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The datetime-related attributes (see :ref:`here <timeseries.components>`
for an overview) of ``DatetimeIndex``, ``PeriodIndex`` and ``TimedeltaIndex`` previously
returned numpy arrays. They will now return a new ``Index`` object, except
in the case of a boolean field, where the result will still be a boolean ndarray. (:issue:`15022`)

Previous behaviour:

.. code-block:: ipython

   In [1]: idx = pd.date_range("2015-01-01", periods=5, freq='10H')

   In [2]: idx.hour
   Out[2]: array([ 0, 10, 20,  6, 16], dtype=int32)

New behavior:

.. ipython:: python

   idx = pd.date_range("2015-01-01", periods=5, freq='10H')
   idx.hour

This has the advantage that specific ``Index`` methods are still available on the
result. On the other hand, this might have backward incompatibilities: e.g.
compared to numpy arrays, ``Index`` objects are not mutable. To get the original
ndarray, you can always convert explicitly using ``np.asarray(idx.hour)``.

.. _whatsnew_0200.api_breaking.unique:

pd.unique will now be consistent with extension types
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In prior versions, using :meth:`Series.unique` and :func:`pandas.unique` on ``Categorical`` and tz-aware
data-types would yield different return types. These are now made consistent. (:issue:`15903`)

- Datetime tz-aware

  Previous behaviour:

  .. code-block:: ipython

     # Series
     In [5]: pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),
        ...:            pd.Timestamp('20160101', tz='US/Eastern')]).unique()
     Out[5]: array([Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')], dtype=object)

     In [6]: pd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),
        ...:                      pd.Timestamp('20160101', tz='US/Eastern')]))
     Out[6]: array(['2016-01-01T05:00:00.000000000'], dtype='datetime64[ns]')

     # Index
     In [7]: pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),
        ...:           pd.Timestamp('20160101', tz='US/Eastern')]).unique()
     Out[7]: DatetimeIndex(['2016-01-01 00:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None)

     In [8]: pd.unique([pd.Timestamp('20160101', tz='US/Eastern'),
        ...:            pd.Timestamp('20160101', tz='US/Eastern')])
     Out[8]: array(['2016-01-01T05:00:00.000000000'], dtype='datetime64[ns]')

  New behavior:

  .. ipython:: python

     # Series, returns an array of Timestamp tz-aware
     pd.Series([pd.Timestamp(r'20160101', tz=r'US/Eastern'),
                pd.Timestamp(r'20160101', tz=r'US/Eastern')]).unique()
     pd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),
               pd.Timestamp('20160101', tz='US/Eastern')]))

     # Index, returns a DatetimeIndex
     pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),
               pd.Timestamp('20160101', tz='US/Eastern')]).unique()
     pd.unique(pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),
                         pd.Timestamp('20160101', tz='US/Eastern')]))

- Categoricals

  Previous behaviour:

  .. code-block:: ipython

     In [1]: pd.Series(list('baabc'), dtype='category').unique()
     Out[1]:
     [b, a, c]
     Categories (3, object): [b, a, c]

     In [2]: pd.unique(pd.Series(list('baabc'), dtype='category'))
     Out[2]: array(['b', 'a', 'c'], dtype=object)

  New behavior:

  .. ipython:: python

     # returns a Categorical
     pd.Series(list('baabc'), dtype='category').unique()
     pd.unique(pd.Series(list('baabc'), dtype='category'))

.. _whatsnew_0200.api_breaking.s3:

S3 file handling
^^^^^^^^^^^^^^^^

pandas now uses `s3fs <http://s3fs.readthedocs.io/>`_ for handling S3 connections. This shouldn't break
any code. However, since ``s3fs`` is not a required dependency, you will need to install it separately, like ``boto``
in prior versions of pandas. (:issue:`11915`).

.. _whatsnew_0200.api_breaking.partial_string_indexing:

Partial string indexing changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:ref:`DatetimeIndex Partial String Indexing <timeseries.partialindexing>` now works as an exact match, provided that string resolution coincides with index resolution, including a case when both are seconds (:issue:`14826`). See :ref:`Slice vs. Exact Match <timeseries.slice_vs_exact_match>` for details.

.. ipython:: python

   df = pd.DataFrame({'a': [1, 2, 3]}, pd.DatetimeIndex(['2011-12-31 23:59:59',
                                                         '2012-01-01 00:00:00',
                                                         '2012-01-01 00:00:01']))
Previous behavior:

.. code-block:: ipython

   In [4]: df['2011-12-31 23:59:59']
   Out[4]:
                          a
   2011-12-31 23:59:59  1

   In [5]: df['a']['2011-12-31 23:59:59']
   Out[5]:
   2011-12-31 23:59:59    1
   Name: a, dtype: int64


New behavior:

.. code-block:: ipython

   In [4]: df['2011-12-31 23:59:59']
   KeyError: '2011-12-31 23:59:59'

   In [5]: df['a']['2011-12-31 23:59:59']
   Out[5]: 1

.. _whatsnew_0200.api_breaking.concat_dtypes:

Concat of different float dtypes will not automatically upcast
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, ``concat`` of multiple objects with different ``float`` dtypes would automatically upcast results to a dtype of ``float64``.
Now the smallest acceptable dtype will be used (:issue:`13247`)

.. ipython:: python

   df1 = pd.DataFrame(np.array([1.0], dtype=np.float32, ndmin=2))
   df1.dtypes

   df2 = pd.DataFrame(np.array([np.nan], dtype=np.float32, ndmin=2))
   df2.dtypes

Previous behavior:

.. code-block:: ipython

   In [7]: pd.concat([df1, df2]).dtypes
   Out[7]:
   0    float64
   dtype: object

New behavior:

.. ipython:: python

   pd.concat([df1, df2]).dtypes

.. _whatsnew_0200.api_breaking.gbq:

pandas Google BigQuery support has moved
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas has split off Google BigQuery support into a separate package ``pandas-gbq``. You can ``conda install pandas-gbq -c conda-forge`` or
``pip install pandas-gbq`` to get it. The functionality of :func:`read_gbq` and :meth:`DataFrame.to_gbq` remain the same with the
currently released version of ``pandas-gbq=0.1.4``. Documentation is now hosted `here <https://pandas-gbq.readthedocs.io/>`__  (:issue:`15347`)

.. _whatsnew_0200.api_breaking.memory_usage:

Memory usage for Index is more accurate
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, showing ``.memory_usage()`` on a pandas structure that has an index, would only include actual index values and not include structures that facilitated fast indexing. This will generally be different for ``Index`` and ``MultiIndex`` and less-so for other index types. (:issue:`15237`)

Previous behavior:

.. code-block:: ipython

   In [8]: index = pd.Index(['foo', 'bar', 'baz'])

   In [9]: index.memory_usage(deep=True)
   Out[9]: 180

   In [10]: index.get_loc('foo')
   Out[10]: 0

   In [11]: index.memory_usage(deep=True)
   Out[11]: 180

New behavior:

.. code-block:: ipython

   In [8]: index = pd.Index(['foo', 'bar', 'baz'])

   In [9]: index.memory_usage(deep=True)
   Out[9]: 180

   In [10]: index.get_loc('foo')
   Out[10]: 0

   In [11]: index.memory_usage(deep=True)
   Out[11]: 260

.. _whatsnew_0200.api_breaking.sort_index:

DataFrame.sort_index changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In certain cases, calling ``.sort_index()`` on a MultiIndexed DataFrame would return the *same* DataFrame without seeming to sort.
This would happen with a ``lexsorted``, but non-monotonic levels. (:issue:`15622`, :issue:`15687`, :issue:`14015`, :issue:`13431`, :issue:`15797`)

This is *unchanged* from prior versions, but shown for illustration purposes:

.. ipython:: python

   df = pd.DataFrame(np.arange(6), columns=['value'],
                     index=pd.MultiIndex.from_product([list('BA'), range(3)]))
   df

.. code-block:: python

   In [87]: df.index.is_lexsorted()
   Out[87]: False

   In [88]: df.index.is_monotonic
   Out[88]: False

Sorting works as expected

.. ipython:: python

   df.sort_index()

.. code-block:: python

   In [90]: df.sort_index().index.is_lexsorted()
   Out[90]: True

   In [91]: df.sort_index().index.is_monotonic
   Out[91]: True

However, this example, which has a non-monotonic 2nd level,
doesn't behave as desired.

.. ipython:: python

   df = pd.DataFrame({'value': [1, 2, 3, 4]},
                     index=pd.MultiIndex([['a', 'b'], ['bb', 'aa']],
                                         [[0, 0, 1, 1], [0, 1, 0, 1]]))
   df

Previous behavior:

.. code-block:: python

   In [11]: df.sort_index()
   Out[11]:
         value
   a bb      1
     aa      2
   b bb      3
     aa      4

   In [14]: df.sort_index().index.is_lexsorted()
   Out[14]: True

   In [15]: df.sort_index().index.is_monotonic
   Out[15]: False

New behavior:

.. code-block:: python

   In [94]: df.sort_index()
   Out[94]:
         value
   a aa      2
     bb      1
   b aa      4
     bb      3

   [4 rows x 1 columns]

   In [95]: df.sort_index().index.is_lexsorted()
   Out[95]: True

   In [96]: df.sort_index().index.is_monotonic
   Out[96]: True


.. _whatsnew_0200.api_breaking.groupby_describe:

GroupBy describe formatting
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The output formatting of ``groupby.describe()`` now labels the ``describe()`` metrics in the columns instead of the index.
This format is consistent with ``groupby.agg()`` when applying multiple functions at once. (:issue:`4792`)

Previous behavior:

.. code-block:: ipython

   In [1]: df = pd.DataFrame({'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4]})

   In [2]: df.groupby('A').describe()
   Out[2]:
                   B
   A
   1 count  2.000000
     mean   1.500000
     std    0.707107
     min    1.000000
     25%    1.250000
     50%    1.500000
     75%    1.750000
     max    2.000000
   2 count  2.000000
     mean   3.500000
     std    0.707107
     min    3.000000
     25%    3.250000
     50%    3.500000
     75%    3.750000
     max    4.000000

   In [3]: df.groupby('A').agg([np.mean, np.std, np.min, np.max])
   Out[3]:
        B
     mean       std amin amax
   A
   1  1.5  0.707107    1    2
   2  3.5  0.707107    3    4

New behavior:

.. ipython:: python

   df = pd.DataFrame({'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4]})

   df.groupby('A').describe()

   df.groupby('A').agg([np.mean, np.std, np.min, np.max])

.. _whatsnew_0200.api_breaking.rolling_pairwise:

Window binary corr/cov operations return a MultiIndex DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A binary window operation, like ``.corr()`` or ``.cov()``, when operating on a ``.rolling(..)``, ``.expanding(..)``, or ``.ewm(..)`` object,
will now return a 2-level ``MultiIndexed DataFrame`` rather than a ``Panel``, as ``Panel`` is now deprecated,
see :ref:`here <whatsnew_0200.api_breaking.deprecate_panel>`. These are equivalent in function,
but a MultiIndexed ``DataFrame`` enjoys more support in pandas.
See the section on :ref:`Windowed Binary Operations <window.cov_corr>` for more information. (:issue:`15677`)

.. ipython:: python

   np.random.seed(1234)
   df = pd.DataFrame(np.random.rand(100, 2),
                     columns=pd.Index(['A', 'B'], name='bar'),
                     index=pd.date_range('20160101',
                                         periods=100, freq='D', name='foo'))
   df.tail()

Previous behavior:

.. code-block:: ipython

   In [2]: df.rolling(12).corr()
   Out[2]:
   <class 'pandas.core.panel.Panel'>
   Dimensions: 100 (items) x 2 (major_axis) x 2 (minor_axis)
   Items axis: 2016-01-01 00:00:00 to 2016-04-09 00:00:00
   Major_axis axis: A to B
   Minor_axis axis: A to B

New behavior:

.. ipython:: python

   res = df.rolling(12).corr()
   res.tail()

Retrieving a correlation matrix for a cross-section

.. ipython:: python

   df.rolling(12).corr().loc['2016-04-07']

.. _whatsnew_0200.api_breaking.hdfstore_where:

HDFStore where string comparison
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions most types could be compared to string column in a ``HDFStore``
usually resulting in an invalid comparison, returning an empty result frame. These comparisons will now raise a
``TypeError`` (:issue:`15492`)

.. ipython:: python

   df = pd.DataFrame({'unparsed_date': ['2014-01-01', '2014-01-01']})
   df.to_hdf('store.h5', 'key', format='table', data_columns=True)
   df.dtypes

Previous behavior:

.. code-block:: ipython

   In [4]: pd.read_hdf('store.h5', 'key', where='unparsed_date > ts')
   File "<string>", line 1
     (unparsed_date > 1970-01-01 00:00:01.388552400)
                           ^
   SyntaxError: invalid token

New behavior:

.. code-block:: ipython

   In [18]: ts = pd.Timestamp('2014-01-01')

   In [19]: pd.read_hdf('store.h5', 'key', where='unparsed_date > ts')
   TypeError: Cannot compare 2014-01-01 00:00:00 of
   type <class 'pandas.tslib.Timestamp'> to string column

.. ipython:: python
   :suppress:

   import os
   os.remove('store.h5')

.. _whatsnew_0200.api_breaking.index_order:

Index.intersection and inner join now preserve the order of the left Index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`Index.intersection` now preserves the order of the calling ``Index`` (left)
instead of the other ``Index`` (right) (:issue:`15582`). This affects inner
joins, :meth:`DataFrame.join` and :func:`merge`, and the ``.align`` method.

- ``Index.intersection``

  .. ipython:: python

     left = pd.Index([2, 1, 0])
     left
     right = pd.Index([1, 2, 3])
     right

  Previous behavior:

  .. code-block:: ipython

     In [4]: left.intersection(right)
     Out[4]: Int64Index([1, 2], dtype='int64')

  New behavior:

  .. ipython:: python

     left.intersection(right)

- ``DataFrame.join`` and ``pd.merge``

  .. ipython:: python

     left = pd.DataFrame({'a': [20, 10, 0]}, index=[2, 1, 0])
     left
     right = pd.DataFrame({'b': [100, 200, 300]}, index=[1, 2, 3])
     right

  Previous behavior:

  .. code-block:: ipython

     In [4]: left.join(right, how='inner')
     Out[4]:
        a    b
     1  10  100
     2  20  200

  New behavior:

  .. ipython:: python

     left.join(right, how='inner')

.. _whatsnew_0200.api_breaking.pivot_table:

Pivot table always returns a DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The documentation for :meth:`pivot_table` states that a ``DataFrame`` is *always* returned. Here a bug
is fixed that allowed this to return a ``Series`` under certain circumstance. (:issue:`4386`)

.. ipython:: python

   df = pd.DataFrame({'col1': [3, 4, 5],
                      'col2': ['C', 'D', 'E'],
                      'col3': [1, 3, 9]})
   df

Previous behavior:

.. code-block:: ipython

   In [2]: df.pivot_table('col1', index=['col3', 'col2'], aggfunc=np.sum)
   Out[2]:
   col3  col2
   1     C       3
   3     D       4
   9     E       5
   Name: col1, dtype: int64

New behavior:

.. ipython:: python

   df.pivot_table('col1', index=['col3', 'col2'], aggfunc=np.sum)

.. _whatsnew_0200.api:

Other API changes
^^^^^^^^^^^^^^^^^

- ``numexpr`` version is now required to be >= 2.4.6 and it will not be used at all if this requisite is not fulfilled (:issue:`15213`).
- ``CParserError`` has been renamed to ``ParserError`` in ``pd.read_csv()`` and will be removed in the future (:issue:`12665`)
- ``SparseArray.cumsum()`` and ``SparseSeries.cumsum()`` will now always return ``SparseArray`` and ``SparseSeries`` respectively (:issue:`12855`)
- ``DataFrame.applymap()`` with an empty ``DataFrame`` will return a copy of the empty ``DataFrame`` instead of a ``Series`` (:issue:`8222`)
- ``Series.map()`` now respects default values of dictionary subclasses with a ``__missing__`` method, such as ``collections.Counter`` (:issue:`15999`)
- ``.loc`` has compat with ``.ix`` for accepting iterators, and NamedTuples (:issue:`15120`)
- ``interpolate()`` and ``fillna()`` will raise a ``ValueError`` if the ``limit`` keyword argument is not greater than 0. (:issue:`9217`)
- ``pd.read_csv()`` will now issue a ``ParserWarning`` whenever there are conflicting values provided by the ``dialect`` parameter and the user (:issue:`14898`)
- ``pd.read_csv()`` will now raise a ``ValueError`` for the C engine if the quote character is larger than one byte (:issue:`11592`)
- ``inplace`` arguments now require a boolean value, else a ``ValueError`` is thrown (:issue:`14189`)
- ``pandas.api.types.is_datetime64_ns_dtype`` will now report ``True`` on a tz-aware dtype, similar to ``pandas.api.types.is_datetime64_any_dtype``
- ``DataFrame.asof()`` will return a null filled ``Series`` instead the scalar ``NaN`` if a match is not found (:issue:`15118`)
- Specific support for ``copy.copy()`` and ``copy.deepcopy()`` functions on NDFrame objects (:issue:`15444`)
- ``Series.sort_values()`` accepts a one element list of bool for consistency with the behavior of ``DataFrame.sort_values()`` (:issue:`15604`)
- ``.merge()`` and ``.join()`` on ``category`` dtype columns will now preserve the category dtype when possible (:issue:`10409`)
- ``SparseDataFrame.default_fill_value`` will be 0, previously was ``nan`` in the return from ``pd.get_dummies(..., sparse=True)`` (:issue:`15594`)
- The default behaviour of ``Series.str.match`` has changed from extracting
  groups to matching the pattern. The extracting behaviour was deprecated
  since pandas version 0.13.0 and can be done with the ``Series.str.extract``
  method (:issue:`5224`). As a consequence, the ``as_indexer`` keyword is
  ignored (no longer needed to specify the new behaviour) and is deprecated.
- ``NaT`` will now correctly report ``False`` for datetimelike boolean operations such as ``is_month_start`` (:issue:`15781`)
- ``NaT`` will now correctly return ``np.nan`` for ``Timedelta`` and ``Period`` accessors such as ``days`` and ``quarter`` (:issue:`15782`)
- ``NaT`` will now returns ``NaT`` for ``tz_localize`` and ``tz_convert``
  methods (:issue:`15830`)
- ``DataFrame`` and ``Panel`` constructors with invalid input will now raise ``ValueError`` rather than ``PandasError``, if called with scalar inputs and not axes (:issue:`15541`)
- ``DataFrame`` and ``Panel`` constructors with invalid input will now raise ``ValueError`` rather than ``pandas.core.common.PandasError``, if called with scalar inputs and not axes; The exception ``PandasError`` is removed as well. (:issue:`15541`)
- The exception ``pandas.core.common.AmbiguousIndexError`` is removed as it is not referenced (:issue:`15541`)


.. _whatsnew_0200.privacy:

Reorganization of the library: privacy changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0200.privacy.extensions:

Modules privacy has changed
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some formerly public python/c/c++/cython extension modules have been moved and/or renamed. These are all removed from the public API.
Furthermore, the ``pandas.core``, ``pandas.compat``, and ``pandas.util`` top-level modules are now considered to be PRIVATE.
If indicated, a deprecation warning will be issued if you reference these modules. (:issue:`12588`)

.. csv-table::
    :header: "Previous Location", "New Location", "Deprecated"
    :widths: 30, 30, 4

    "pandas.lib", "pandas._libs.lib", "X"
    "pandas.tslib", "pandas._libs.tslib", "X"
    "pandas.computation", "pandas.core.computation", "X"
    "pandas.msgpack", "pandas.io.msgpack", ""
    "pandas.index", "pandas._libs.index", ""
    "pandas.algos", "pandas._libs.algos", ""
    "pandas.hashtable", "pandas._libs.hashtable", ""
    "pandas.indexes", "pandas.core.indexes", ""
    "pandas.json", "pandas._libs.json / pandas.io.json", "X"
    "pandas.parser", "pandas._libs.parsers", "X"
    "pandas.formats", "pandas.io.formats", ""
    "pandas.sparse", "pandas.core.sparse", ""
    "pandas.tools", "pandas.core.reshape", "X"
    "pandas.types", "pandas.core.dtypes", "X"
    "pandas.io.sas.saslib", "pandas.io.sas._sas", ""
    "pandas._join", "pandas._libs.join", ""
    "pandas._hash", "pandas._libs.hashing", ""
    "pandas._period", "pandas._libs.period", ""
    "pandas._sparse", "pandas._libs.sparse", ""
    "pandas._testing", "pandas._libs.testing", ""
    "pandas._window", "pandas._libs.window", ""


Some new subpackages are created with public functionality that is not directly
exposed in the top-level namespace: ``pandas.errors``, ``pandas.plotting`` and
``pandas.testing`` (more details below). Together with ``pandas.api.types`` and
certain functions in the ``pandas.io`` and ``pandas.tseries`` submodules,
these are now the public subpackages.

Further changes:

- The function :func:`~pandas.api.types.union_categoricals` is now importable from ``pandas.api.types``, formerly from ``pandas.types.concat`` (:issue:`15998`)
- The type import ``pandas.tslib.NaTType`` is deprecated and can be replaced by using ``type(pandas.NaT)`` (:issue:`16146`)
- The public functions in ``pandas.tools.hashing`` deprecated from that locations, but are now importable from ``pandas.util`` (:issue:`16223`)
- The modules in ``pandas.util``: ``decorators``, ``print_versions``, ``doctools``, ``validators``, ``depr_module`` are now private. Only the functions exposed in ``pandas.util`` itself are public (:issue:`16223`)

.. _whatsnew_0200.privacy.errors:

``pandas.errors``
^^^^^^^^^^^^^^^^^

We are adding a standard public module for all pandas exceptions & warnings ``pandas.errors``. (:issue:`14800`). Previously
these exceptions & warnings could be imported from ``pandas.core.common`` or ``pandas.io.common``. These exceptions and warnings
will be removed from the ``*.common`` locations in a future release. (:issue:`15541`)

The following are now part of this API:

.. code-block:: python

   ['DtypeWarning',
    'EmptyDataError',
    'OutOfBoundsDatetime',
    'ParserError',
    'ParserWarning',
    'PerformanceWarning',
    'UnsortedIndexError',
    'UnsupportedFunctionCall']


.. _whatsnew_0200.privacy.testing:

``pandas.testing``
^^^^^^^^^^^^^^^^^^

We are adding a standard module that exposes the public testing functions in ``pandas.testing`` (:issue:`9895`). Those functions can be used when writing tests for functionality using pandas objects.

The following testing functions are now part of this API:

- :func:`testing.assert_frame_equal`
- :func:`testing.assert_series_equal`
- :func:`testing.assert_index_equal`


.. _whatsnew_0200.privacy.plotting:

``pandas.plotting``
^^^^^^^^^^^^^^^^^^^

A new public ``pandas.plotting`` module has been added that holds plotting functionality that was previously in either ``pandas.tools.plotting`` or in the top-level namespace. See the :ref:`deprecations sections <whatsnew_0200.privacy.deprecate_plotting>` for more details.

.. _whatsnew_0200.privacy.development:

Other development changes
^^^^^^^^^^^^^^^^^^^^^^^^^

- Building pandas for development now requires ``cython >= 0.23`` (:issue:`14831`)
- Require at least 0.23 version of cython to avoid problems with character encodings (:issue:`14699`)
- Switched the test framework to use `pytest <http://doc.pytest.org/en/latest>`__ (:issue:`13097`)
- Reorganization of tests directory layout (:issue:`14854`, :issue:`15707`).


.. _whatsnew_0200.deprecations:

Deprecations
~~~~~~~~~~~~

.. _whatsnew_0200.api_breaking.deprecate_ix:

Deprecate ``.ix``
^^^^^^^^^^^^^^^^^

The ``.ix`` indexer is deprecated, in favor of the more strict ``.iloc`` and ``.loc`` indexers. ``.ix`` offers a lot of magic on the inference of what the user wants to do. More specifically, ``.ix`` can decide to index *positionally* OR via *labels*, depending on the data type of the index. This has caused quite a bit of user confusion over the years. The full indexing documentation is :ref:`here <indexing>`. (:issue:`14218`)

The recommended methods of indexing are:

- ``.loc`` if you want to *label* index
- ``.iloc`` if you want to *positionally* index.

Using ``.ix`` will now show a ``DeprecationWarning`` with a link to some examples of how to convert code `here <https://pandas.pydata.org/pandas-docs/version/1.0/user_guide/indexing.html#ix-indexer-is-deprecated>`__.


.. ipython:: python

   df = pd.DataFrame({'A': [1, 2, 3],
                      'B': [4, 5, 6]},
                     index=list('abc'))

   df

Previous behavior, where you wish to get the 0th and the 2nd elements from the index in the 'A' column.

.. code-block:: ipython

   In [3]: df.ix[[0, 2], 'A']
   Out[3]:
   a    1
   c    3
   Name: A, dtype: int64

Using ``.loc``. Here we will select the appropriate indexes from the index, then use *label* indexing.

.. ipython:: python

   df.loc[df.index[[0, 2]], 'A']

Using ``.iloc``. Here we will get the location of the 'A' column, then use *positional* indexing to select things.

.. ipython:: python

   df.iloc[[0, 2], df.columns.get_loc('A')]


.. _whatsnew_0200.api_breaking.deprecate_panel:

Deprecate Panel
^^^^^^^^^^^^^^^

``Panel`` is deprecated and will be removed in a future version. The recommended way to represent 3-D data are
with a ``MultiIndex`` on a ``DataFrame`` via the :meth:`~Panel.to_frame` or with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas
provides a :meth:`~Panel.to_xarray` method to automate this conversion (:issue:`13563`).

.. code-block:: ipython

    In [133]: import pandas._testing as tm

    In [134]: p = tm.makePanel()

    In [135]: p
    Out[135]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 3 (items) x 3 (major_axis) x 4 (minor_axis)
    Items axis: ItemA to ItemC
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00
    Minor_axis axis: A to D

Convert to a MultiIndex DataFrame

.. code-block:: ipython

    In [136]: p.to_frame()
    Out[136]:
                         ItemA     ItemB     ItemC
    major      minor
    2000-01-03 A      0.628776 -1.409432  0.209395
               B      0.988138 -1.347533 -0.896581
               C     -0.938153  1.272395 -0.161137
               D     -0.223019 -0.591863 -1.051539
    2000-01-04 A      0.186494  1.422986 -0.592886
               B     -0.072608  0.363565  1.104352
               C     -1.239072 -1.449567  0.889157
               D      2.123692 -0.414505 -0.319561
    2000-01-05 A      0.952478 -2.147855 -1.473116
               B     -0.550603 -0.014752 -0.431550
               C      0.139683 -1.195524  0.288377
               D      0.122273 -1.425795 -0.619993

    [12 rows x 3 columns]

Convert to an xarray DataArray

.. code-block:: ipython

    In [137]: p.to_xarray()
    Out[137]:
    <xarray.DataArray (items: 3, major_axis: 3, minor_axis: 4)>
    array([[[ 0.628776,  0.988138, -0.938153, -0.223019],
            [ 0.186494, -0.072608, -1.239072,  2.123692],
            [ 0.952478, -0.550603,  0.139683,  0.122273]],

           [[-1.409432, -1.347533,  1.272395, -0.591863],
            [ 1.422986,  0.363565, -1.449567, -0.414505],
            [-2.147855, -0.014752, -1.195524, -1.425795]],

           [[ 0.209395, -0.896581, -0.161137, -1.051539],
            [-0.592886,  1.104352,  0.889157, -0.319561],
            [-1.473116, -0.43155 ,  0.288377, -0.619993]]])
    Coordinates:
      * items       (items) object 'ItemA' 'ItemB' 'ItemC'
      * major_axis  (major_axis) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05
      * minor_axis  (minor_axis) object 'A' 'B' 'C' 'D'

.. _whatsnew_0200.api_breaking.deprecate_group_agg_dict:

Deprecate groupby.agg() with a dictionary when renaming
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``.groupby(..).agg(..)``, ``.rolling(..).agg(..)``, and ``.resample(..).agg(..)``  syntax can accept a variable of inputs, including scalars,
list, and a dict of column names to scalars or lists. This provides a useful syntax for constructing multiple
(potentially different) aggregations.

However, ``.agg(..)`` can *also* accept a dict that allows 'renaming' of the result columns. This is a complicated and confusing syntax, as well as not consistent
between ``Series`` and ``DataFrame``. We are deprecating this 'renaming' functionality.

- We are deprecating passing a dict to a grouped/rolled/resampled ``Series``. This allowed
  one to ``rename`` the resulting aggregation, but this had a completely different
  meaning than passing a dictionary to a grouped ``DataFrame``, which accepts column-to-aggregations.
- We are deprecating passing a dict-of-dicts to a grouped/rolled/resampled ``DataFrame`` in a similar manner.

This is an illustrative example:

.. ipython:: python

   df = pd.DataFrame({'A': [1, 1, 1, 2, 2],
                      'B': range(5),
                      'C': range(5)})
   df

Here is a typical useful syntax for computing different aggregations for different columns. This
is a natural, and useful syntax. We aggregate from the dict-to-list by taking the specified
columns and applying the list of functions. This returns a ``MultiIndex`` for the columns (this is *not* deprecated).

.. ipython:: python

   df.groupby('A').agg({'B': 'sum', 'C': 'min'})

Here's an example of the first deprecation, passing a dict to a grouped ``Series``. This
is a combination aggregation & renaming:

.. code-block:: ipython

   In [6]: df.groupby('A').B.agg({'foo': 'count'})
   FutureWarning: using a dict on a Series for aggregation
   is deprecated and will be removed in a future version

   Out[6]:
      foo
   A
   1    3
   2    2

You can accomplish the same operation, more idiomatically by:

.. ipython:: python

   df.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})


Here's an example of the second deprecation, passing a dict-of-dict to a grouped ``DataFrame``:

.. code-block:: python

   In [23]: (df.groupby('A')
       ...:    .agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})
       ...:  )
   FutureWarning: using a dict with renaming is deprecated and
   will be removed in a future version

   Out[23]:
        B   C
      foo bar
   A
   1   3   0
   2   7   3


You can accomplish nearly the same by:

.. ipython:: python

   (df.groupby('A')
      .agg({'B': 'sum', 'C': 'min'})
      .rename(columns={'B': 'foo', 'C': 'bar'})
    )



.. _whatsnew_0200.privacy.deprecate_plotting:

Deprecate .plotting
^^^^^^^^^^^^^^^^^^^

The ``pandas.tools.plotting`` module has been deprecated,  in favor of the top level ``pandas.plotting`` module. All the public plotting functions are now available
from ``pandas.plotting`` (:issue:`12548`).

Furthermore, the top-level ``pandas.scatter_matrix`` and ``pandas.plot_params`` are deprecated.
Users can import these from ``pandas.plotting`` as well.

Previous script:

.. code-block:: python

   pd.tools.plotting.scatter_matrix(df)
   pd.scatter_matrix(df)

Should be changed to:

.. code-block:: python

   pd.plotting.scatter_matrix(df)



.. _whatsnew_0200.deprecations.other:

Other deprecations
^^^^^^^^^^^^^^^^^^

- ``SparseArray.to_dense()`` has deprecated the ``fill`` parameter, as that parameter was not being respected (:issue:`14647`)
- ``SparseSeries.to_dense()`` has deprecated the ``sparse_only`` parameter (:issue:`14647`)
- ``Series.repeat()`` has deprecated the ``reps`` parameter in favor of ``repeats`` (:issue:`12662`)
- The ``Series`` constructor and ``.astype`` method have deprecated accepting timestamp dtypes without a frequency (e.g. ``np.datetime64``) for the ``dtype`` parameter (:issue:`15524`)
- ``Index.repeat()`` and ``MultiIndex.repeat()`` have deprecated the ``n`` parameter in favor of ``repeats`` (:issue:`12662`)
- ``Categorical.searchsorted()`` and ``Series.searchsorted()`` have deprecated the ``v`` parameter in favor of ``value`` (:issue:`12662`)
- ``TimedeltaIndex.searchsorted()``, ``DatetimeIndex.searchsorted()``, and ``PeriodIndex.searchsorted()`` have deprecated the ``key`` parameter in favor of ``value`` (:issue:`12662`)
- ``DataFrame.astype()`` has deprecated the ``raise_on_error`` parameter in favor of ``errors`` (:issue:`14878`)
- ``Series.sortlevel`` and ``DataFrame.sortlevel`` have been deprecated in favor of ``Series.sort_index`` and ``DataFrame.sort_index`` (:issue:`15099`)
- importing ``concat`` from ``pandas.tools.merge`` has been deprecated in favor of imports from the ``pandas`` namespace. This should only affect explicit imports (:issue:`15358`)
- ``Series/DataFrame/Panel.consolidate()`` been deprecated as a public method. (:issue:`15483`)
- The ``as_indexer`` keyword of ``Series.str.match()`` has been deprecated (ignored keyword) (:issue:`15257`).
- The following top-level pandas functions have been deprecated and will be removed in a future version (:issue:`13790`, :issue:`15940`)

  * ``pd.pnow()``, replaced by ``Period.now()``
  * ``pd.Term``, is removed, as it is not applicable to user code. Instead use in-line string expressions in the where clause when searching in HDFStore
  * ``pd.Expr``, is removed, as it is not applicable to user code.
  * ``pd.match()``, is removed.
  * ``pd.groupby()``, replaced by using the ``.groupby()`` method directly on a ``Series/DataFrame``
  * ``pd.get_store()``, replaced by a direct call to ``pd.HDFStore(...)``
- ``is_any_int_dtype``, ``is_floating_dtype``, and ``is_sequence`` are deprecated from ``pandas.api.types`` (:issue:`16042`)

.. _whatsnew_0200.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- The ``pandas.rpy`` module is removed. Similar functionality can be accessed
  through the `rpy2 <https://rpy2.readthedocs.io/>`__ project.
  See the `R interfacing docs <https://pandas.pydata.org/pandas-docs/version/0.20/r_interface.html>`__ for more details.
- The ``pandas.io.ga`` module with a ``google-analytics`` interface is removed (:issue:`11308`).
  Similar functionality can be found in the `Google2Pandas <https://github.com/panalysis/Google2Pandas>`__ package.
- ``pd.to_datetime`` and ``pd.to_timedelta`` have dropped the ``coerce`` parameter in favor of ``errors`` (:issue:`13602`)
- ``pandas.stats.fama_macbeth``, ``pandas.stats.ols``, ``pandas.stats.plm`` and ``pandas.stats.var``, as well as the top-level ``pandas.fama_macbeth`` and ``pandas.ols`` routines are removed. Similar functionality can be found in the `statsmodels <https://www.statsmodels.org/dev/>`__ package. (:issue:`11898`)
- The ``TimeSeries`` and ``SparseTimeSeries`` classes, aliases of ``Series``
  and ``SparseSeries``, are removed (:issue:`10890`, :issue:`15098`).
- ``Series.is_time_series`` is dropped in favor of ``Series.index.is_all_dates`` (:issue:`15098`)
- The deprecated ``irow``, ``icol``, ``iget`` and ``iget_value`` methods are removed
  in favor of ``iloc`` and ``iat`` as explained :ref:`here <whatsnew_0170.deprecations>` (:issue:`10711`).
- The deprecated ``DataFrame.iterkv()`` has been removed in favor of ``DataFrame.iteritems()`` (:issue:`10711`)
- The ``Categorical`` constructor has dropped the ``name`` parameter (:issue:`10632`)
- ``Categorical`` has dropped support for ``NaN`` categories (:issue:`10748`)
- The ``take_last`` parameter has been dropped from ``duplicated()``, ``drop_duplicates()``, ``nlargest()``, and ``nsmallest()`` methods (:issue:`10236`, :issue:`10792`, :issue:`10920`)
- ``Series``, ``Index``, and ``DataFrame`` have dropped the ``sort`` and ``order`` methods (:issue:`10726`)
- Where clauses in ``pytables`` are only accepted as strings and expressions types and not other data-types (:issue:`12027`)
- ``DataFrame`` has dropped the ``combineAdd`` and ``combineMult`` methods in favor of ``add`` and ``mul`` respectively (:issue:`10735`)

.. _whatsnew_0200.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of ``pd.wide_to_long()`` (:issue:`14779`)
- Improved performance of ``pd.factorize()`` by releasing the GIL with ``object`` dtype when inferred as strings (:issue:`14859`, :issue:`16057`)
- Improved performance of timeseries plotting with an irregular DatetimeIndex
  (or with ``compat_x=True``) (:issue:`15073`).
- Improved performance of ``groupby().cummin()`` and ``groupby().cummax()`` (:issue:`15048`, :issue:`15109`, :issue:`15561`, :issue:`15635`)
- Improved performance and reduced memory when indexing with a ``MultiIndex`` (:issue:`15245`)
- When reading buffer object in ``read_sas()`` method without specified format, filepath string is inferred rather than buffer object. (:issue:`14947`)
- Improved performance of ``.rank()`` for categorical data (:issue:`15498`)
- Improved performance when using ``.unstack()`` (:issue:`15503`)
- Improved performance of merge/join on ``category`` columns (:issue:`10409`)
- Improved performance of ``drop_duplicates()`` on ``bool`` columns (:issue:`12963`)
- Improve performance of ``pd.core.groupby.GroupBy.apply`` when the applied
  function used the ``.name`` attribute of the group DataFrame (:issue:`15062`).
- Improved performance of ``iloc`` indexing with a list or array (:issue:`15504`).
- Improved performance of ``Series.sort_index()`` with a monotonic index (:issue:`15694`)
- Improved performance in ``pd.read_csv()`` on some platforms with buffered reads (:issue:`16039`)

.. _whatsnew_0200.bug_fixes:

Bug fixes
~~~~~~~~~

Conversion
^^^^^^^^^^

- Bug in ``Timestamp.replace`` now raises ``TypeError`` when incorrect argument names are given; previously this raised ``ValueError`` (:issue:`15240`)
- Bug in ``Timestamp.replace`` with compat for passing long integers (:issue:`15030`)
- Bug in ``Timestamp`` returning UTC based time/date attributes when a timezone was provided (:issue:`13303`, :issue:`6538`)
- Bug in ``Timestamp`` incorrectly localizing timezones during construction (:issue:`11481`, :issue:`15777`)
- Bug in ``TimedeltaIndex`` addition where overflow was being allowed without error (:issue:`14816`)
- Bug in ``TimedeltaIndex`` raising a ``ValueError`` when boolean indexing with ``loc`` (:issue:`14946`)
- Bug in catching an overflow in ``Timestamp`` + ``Timedelta/Offset`` operations (:issue:`15126`)
- Bug in ``DatetimeIndex.round()`` and ``Timestamp.round()`` floating point accuracy when rounding by milliseconds or less (:issue:`14440`, :issue:`15578`)
- Bug in ``astype()`` where ``inf`` values were incorrectly converted to integers. Now raises error now with ``astype()`` for Series and DataFrames (:issue:`14265`)
- Bug in ``DataFrame(..).apply(to_numeric)`` when values are of type decimal.Decimal. (:issue:`14827`)
- Bug in ``describe()`` when passing a numpy array which does not contain the median to the ``percentiles`` keyword argument (:issue:`14908`)
- Cleaned up ``PeriodIndex`` constructor, including raising on floats more consistently (:issue:`13277`)
- Bug in using ``__deepcopy__`` on empty NDFrame objects (:issue:`15370`)
- Bug in ``.replace()`` may result in incorrect dtypes. (:issue:`12747`, :issue:`15765`)
- Bug in ``Series.replace`` and ``DataFrame.replace`` which failed on empty replacement dicts (:issue:`15289`)
- Bug in ``Series.replace`` which replaced a numeric by string (:issue:`15743`)
- Bug in ``Index`` construction with ``NaN`` elements and integer dtype specified (:issue:`15187`)
- Bug in ``Series`` construction with a datetimetz (:issue:`14928`)
- Bug in ``Series.dt.round()`` inconsistent behaviour on ``NaT`` 's with different arguments (:issue:`14940`)
- Bug in ``Series`` constructor when both ``copy=True`` and ``dtype`` arguments are provided (:issue:`15125`)
- Incorrect dtyped ``Series`` was returned by comparison methods (e.g., ``lt``, ``gt``, ...) against a constant for an empty ``DataFrame`` (:issue:`15077`)
- Bug in ``Series.ffill()`` with mixed dtypes containing tz-aware datetimes. (:issue:`14956`)
- Bug in ``DataFrame.fillna()`` where the argument ``downcast`` was ignored when fillna value was of type ``dict`` (:issue:`15277`)
- Bug in ``.asfreq()``, where frequency was not set for empty ``Series`` (:issue:`14320`)
- Bug in ``DataFrame`` construction with nulls and datetimes in a list-like (:issue:`15869`)
- Bug in ``DataFrame.fillna()`` with tz-aware datetimes (:issue:`15855`)
- Bug in ``is_string_dtype``, ``is_timedelta64_ns_dtype``, and ``is_string_like_dtype`` in which an error was raised when ``None`` was passed in (:issue:`15941`)
- Bug in the return type of ``pd.unique`` on a ``Categorical``, which was returning an ndarray and not a ``Categorical`` (:issue:`15903`)
- Bug in ``Index.to_series()`` where the index was not copied (and so mutating later would change the original), (:issue:`15949`)
- Bug in indexing with partial string indexing with a len-1 DataFrame (:issue:`16071`)
- Bug in ``Series`` construction where passing invalid dtype didn't raise an error. (:issue:`15520`)

Indexing
^^^^^^^^

- Bug in ``Index`` power operations with reversed operands (:issue:`14973`)
- Bug in ``DataFrame.sort_values()`` when sorting by multiple columns where one column is of type ``int64`` and contains ``NaT`` (:issue:`14922`)
- Bug in ``DataFrame.reindex()`` in which ``method`` was ignored when passing ``columns`` (:issue:`14992`)
- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`, :issue:`15424`)
- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a numpy array (:issue:`15434`)
- Bug in ``Series.asof`` which raised if the series contained all ``np.nan`` (:issue:`15713`)
- Bug in ``.at`` when selecting from a tz-aware column (:issue:`15822`)
- Bug in ``Series.where()`` and ``DataFrame.where()`` where array-like conditionals were being rejected (:issue:`15414`)
- Bug in ``Series.where()`` where TZ-aware data was converted to float representation (:issue:`15701`)
- Bug in ``.loc`` that would not return the correct dtype for scalar access for a DataFrame (:issue:`11617`)
- Bug in output formatting of a ``MultiIndex`` when names are integers (:issue:`12223`, :issue:`15262`)
- Bug in ``Categorical.searchsorted()`` where alphabetical instead of the provided categorical order was used (:issue:`14522`)
- Bug in ``Series.iloc`` where a ``Categorical`` object for list-like indexes input was returned, where a ``Series`` was expected. (:issue:`14580`)
- Bug in ``DataFrame.isin`` comparing datetimelike to empty frame (:issue:`15473`)
- Bug in ``.reset_index()`` when an all ``NaN`` level of a ``MultiIndex`` would fail (:issue:`6322`)
- Bug in ``.reset_index()`` when raising error for index name already present in ``MultiIndex`` columns (:issue:`16120`)
- Bug in creating a ``MultiIndex`` with tuples and not passing a list of names; this will now raise ``ValueError`` (:issue:`15110`)
- Bug in the HTML display with a ``MultiIndex`` and truncation (:issue:`14882`)
- Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)
- Bug in ``pd.concat()`` where the names of ``MultiIndex`` of resulting ``DataFrame`` are not handled correctly when ``None`` is presented in the names of ``MultiIndex`` of input ``DataFrame`` (:issue:`15787`)
- Bug in ``DataFrame.sort_index()`` and ``Series.sort_index()`` where ``na_position`` doesn't work with a ``MultiIndex`` (:issue:`14784`, :issue:`16604`)
- Bug in ``pd.concat()`` when combining objects with a ``CategoricalIndex`` (:issue:`16111`)
- Bug in indexing with a scalar and a ``CategoricalIndex`` (:issue:`16123`)

IO
^^

- Bug in ``pd.to_numeric()`` in which float and unsigned integer elements were being improperly casted (:issue:`14941`, :issue:`15005`)
- Bug in ``pd.read_fwf()`` where the skiprows parameter was not being respected during column width inference (:issue:`11256`)
- Bug in ``pd.read_csv()`` in which the ``dialect`` parameter was not being verified before processing (:issue:`14898`)
- Bug in ``pd.read_csv()`` in which missing data was being improperly handled with ``usecols`` (:issue:`6710`)
- Bug in ``pd.read_csv()`` in which a file containing a row with many columns followed by rows with fewer columns would cause a crash (:issue:`14125`)
- Bug in ``pd.read_csv()`` for the C engine where ``usecols`` were being indexed incorrectly with ``parse_dates`` (:issue:`14792`)
- Bug in ``pd.read_csv()`` with ``parse_dates`` when multi-line headers are specified (:issue:`15376`)
- Bug in ``pd.read_csv()`` with ``float_precision='round_trip'`` which caused a segfault when a text entry is parsed (:issue:`15140`)
- Bug in ``pd.read_csv()`` when an index was specified and no values were specified as null values (:issue:`15835`)
- Bug in ``pd.read_csv()`` in which certain invalid file objects caused the Python interpreter to crash (:issue:`15337`)
- Bug in ``pd.read_csv()`` in which invalid values for ``nrows`` and ``chunksize`` were allowed (:issue:`15767`)
- Bug in ``pd.read_csv()`` for the Python engine in which unhelpful error messages were being raised when parsing errors occurred (:issue:`15910`)
- Bug in ``pd.read_csv()`` in which the ``skipfooter`` parameter was not being properly validated (:issue:`15925`)
- Bug in ``pd.to_csv()`` in which there was numeric overflow when a timestamp index was being written (:issue:`15982`)
- Bug in ``pd.util.hashing.hash_pandas_object()`` in which hashing of categoricals depended on the ordering of categories, instead of just their values. (:issue:`15143`)
- Bug in ``.to_json()`` where ``lines=True`` and contents (keys or values) contain escaped characters (:issue:`15096`)
- Bug in ``.to_json()`` causing single byte ascii characters to be expanded to four byte unicode (:issue:`15344`)
- Bug in ``.to_json()`` for the C engine where rollover was not correctly handled for case where frac is odd and diff is exactly 0.5 (:issue:`15716`, :issue:`15864`)
- Bug in ``pd.read_json()`` for Python 2 where ``lines=True`` and contents contain non-ascii unicode characters (:issue:`15132`)
- Bug in ``pd.read_msgpack()`` in which ``Series`` categoricals were being improperly processed (:issue:`14901`)
- Bug in ``pd.read_msgpack()`` which did not allow loading of a dataframe with an index of type ``CategoricalIndex`` (:issue:`15487`)
- Bug in ``pd.read_msgpack()`` when deserializing a ``CategoricalIndex`` (:issue:`15487`)
- Bug in ``DataFrame.to_records()`` with converting a ``DatetimeIndex`` with a timezone (:issue:`13937`)
- Bug in ``DataFrame.to_records()`` which failed with unicode characters in column names (:issue:`11879`)
- Bug in ``.to_sql()`` when writing a DataFrame with numeric index names (:issue:`15404`).
- Bug in ``DataFrame.to_html()`` with ``index=False`` and ``max_rows`` raising in ``IndexError`` (:issue:`14998`)
- Bug in ``pd.read_hdf()`` passing a ``Timestamp`` to the ``where`` parameter with a non date column (:issue:`15492`)
- Bug in ``DataFrame.to_stata()`` and ``StataWriter`` which produces incorrectly formatted files to be produced for some locales (:issue:`13856`)
- Bug in ``StataReader`` and ``StataWriter`` which allows invalid encodings (:issue:`15723`)
- Bug in the ``Series`` repr not showing the length when the output was truncated (:issue:`15962`).

Plotting
^^^^^^^^

- Bug in ``DataFrame.hist`` where ``plt.tight_layout`` caused an ``AttributeError``  (use ``matplotlib >= 2.0.1``) (:issue:`9351`)
- Bug in ``DataFrame.boxplot`` where ``fontsize`` was not applied to the tick labels on both axes (:issue:`15108`)
- Bug in the date and time converters pandas registers with matplotlib not handling multiple dimensions (:issue:`16026`)
- Bug in ``pd.scatter_matrix()`` could accept either ``color`` or ``c``, but not both (:issue:`14855`)

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in ``.groupby(..).resample()`` when passed the ``on=`` kwarg. (:issue:`15021`)
- Properly set ``__name__`` and ``__qualname__`` for ``Groupby.*`` functions (:issue:`14620`)
- Bug in ``GroupBy.get_group()`` failing with a categorical grouper (:issue:`15155`)
- Bug in ``.groupby(...).rolling(...)`` when ``on`` is specified and using a ``DatetimeIndex`` (:issue:`15130`, :issue:`13966`)
- Bug in groupby operations with ``timedelta64`` when passing ``numeric_only=False`` (:issue:`5724`)
- Bug in ``groupby.apply()`` coercing ``object`` dtypes to numeric types, when not all values were numeric (:issue:`14423`, :issue:`15421`, :issue:`15670`)
- Bug in ``resample``, where a non-string ``loffset`` argument would not be applied when resampling a timeseries (:issue:`13218`)
- Bug in ``DataFrame.groupby().describe()`` when grouping on ``Index`` containing tuples (:issue:`14848`)
- Bug in ``groupby().nunique()`` with a datetimelike-grouper where bins counts were incorrect (:issue:`13453`)
- Bug in ``groupby.transform()`` that would coerce the resultant dtypes back to the original (:issue:`10972`, :issue:`11444`)
- Bug in ``groupby.agg()`` incorrectly localizing timezone on ``datetime`` (:issue:`15426`, :issue:`10668`, :issue:`13046`)
- Bug in ``.rolling/expanding()`` functions where ``count()`` was not counting ``np.Inf``, nor handling ``object`` dtypes (:issue:`12541`)
- Bug in ``.rolling()`` where ``pd.Timedelta`` or ``datetime.timedelta`` was not accepted as a ``window`` argument (:issue:`15440`)
- Bug in ``Rolling.quantile`` function that caused a segmentation fault when called with a quantile value outside of the range [0, 1] (:issue:`15463`)
- Bug in ``DataFrame.resample().median()`` if duplicate column names are present (:issue:`14233`)

Sparse
^^^^^^

- Bug in ``SparseSeries.reindex`` on single level with list of length 1 (:issue:`15447`)
- Bug in repr-formatting a ``SparseDataFrame`` after a value was set on (a copy of) one of its series (:issue:`15488`)
- Bug in ``SparseDataFrame`` construction with lists not coercing to dtype (:issue:`15682`)
- Bug in sparse array indexing in which indices were not being validated (:issue:`15863`)

Reshaping
^^^^^^^^^

- Bug in ``pd.merge_asof()`` where ``left_index`` or ``right_index`` caused a failure when multiple ``by`` was specified (:issue:`15676`)
- Bug in ``pd.merge_asof()`` where ``left_index``/``right_index`` together caused a failure when ``tolerance`` was specified (:issue:`15135`)
- Bug in ``DataFrame.pivot_table()`` where ``dropna=True`` would not drop all-NaN columns when the columns was a ``category`` dtype (:issue:`15193`)
- Bug in ``pd.melt()`` where passing a tuple value for ``value_vars`` caused a ``TypeError`` (:issue:`15348`)
- Bug in ``pd.pivot_table()`` where no error was raised when values argument was not in the columns (:issue:`14938`)
- Bug in ``pd.concat()`` in which concatenating with an empty dataframe with ``join='inner'`` was being improperly handled (:issue:`15328`)
- Bug with ``sort=True`` in ``DataFrame.join`` and ``pd.merge`` when joining on indexes (:issue:`15582`)
- Bug in ``DataFrame.nsmallest`` and ``DataFrame.nlargest`` where identical values resulted in duplicated rows (:issue:`15297`)
- Bug in :func:`pandas.pivot_table` incorrectly raising ``UnicodeError`` when passing unicode input for ``margins`` keyword (:issue:`13292`)

Numeric
^^^^^^^

- Bug in ``.rank()`` which incorrectly ranks ordered categories (:issue:`15420`)
- Bug in ``.corr()`` and ``.cov()`` where the column and index were the same object (:issue:`14617`)
- Bug in ``.mode()`` where ``mode`` was not returned if was only a single value (:issue:`15714`)
- Bug in ``pd.cut()`` with a single bin on an all 0s array (:issue:`15428`)
- Bug in ``pd.qcut()`` with a single quantile and an array with identical values (:issue:`15431`)
- Bug in ``pandas.tools.utils.cartesian_product()`` with large input can cause overflow on windows (:issue:`15265`)
- Bug in ``.eval()`` which caused multi-line evals to fail with local variables not on the first line (:issue:`15342`)

Other
^^^^^

- Compat with SciPy 0.19.0 for testing on ``.interpolate()`` (:issue:`15662`)
- Compat for 32-bit platforms for ``.qcut/cut``; bins will now be ``int64`` dtype (:issue:`14866`)
- Bug in interactions with ``Qt`` when a ``QtApplication`` already exists (:issue:`14372`)
- Avoid use of ``np.finfo()`` during ``import pandas`` removed to mitigate deadlock on Python GIL misuse (:issue:`14641`)


.. _whatsnew_0.20.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.19.2..v0.20.0
.. _whatsnew_04x:

Versions 0.4.1 through 0.4.3 (September 25 - October 9, 2011)
-------------------------------------------------------------

{{ header }}

New features
~~~~~~~~~~~~

- Added Python 3 support using 2to3 (:issue:`200`)
- :ref:`Added <dsintro.name_attribute>` ``name`` attribute to ``Series``, now
  prints as part of ``Series.__repr__``
- :ref:`Added <missing.isna>` instance methods ``isnull`` and ``notnull`` to
  Series (:issue:`209`, :issue:`203`)
- :ref:`Added <basics.align>` ``Series.align`` method for aligning two series
  with choice of join method (ENH56_)
- :ref:`Added <advanced.get_level_values>` method ``get_level_values`` to
  ``MultiIndex`` (:issue:`188`)
- Set values in mixed-type ``DataFrame`` objects via ``.ix`` indexing attribute (:issue:`135`)
- Added new ``DataFrame`` :ref:`methods <basics.dtypes>`
  ``get_dtype_counts`` and property ``dtypes`` (ENHdc_)
- Added :ref:`ignore_index <merging.ignore_index>` option to
  ``DataFrame.append`` to stack DataFrames (ENH1b_)
- ``read_csv`` tries to :ref:`sniff <io.sniff>` delimiters using
  ``csv.Sniffer`` (:issue:`146`)
- ``read_csv`` can :ref:`read <io.csv_multiindex>` multiple columns into a
  ``MultiIndex``; DataFrame's ``to_csv`` method writes out a corresponding
  ``MultiIndex`` (:issue:`151`)
- ``DataFrame.rename`` has a new ``copy`` parameter to :ref:`rename
  <basics.rename>` a DataFrame in place (ENHed_)
- :ref:`Enable <reshaping.unstack_by_name>` unstacking by name (:issue:`142`)
- :ref:`Enable <advanced.sortlevel_byname>` ``sortlevel`` to work by level (:issue:`141`)

Performance enhancements
~~~~~~~~~~~~~~~~~~~~~~~~

- Altered binary operations on differently-indexed SparseSeries objects
  to use the integer-based (dense) alignment logic which is faster with a
  larger number of blocks (:issue:`205`)
- Wrote faster Cython data alignment / merging routines resulting in
  substantial speed increases
- Improved performance of ``isnull`` and ``notnull``, a regression from v0.3.0
  (:issue:`187`)
- Refactored code related to ``DataFrame.join`` so that intermediate aligned
  copies of the data in each ``DataFrame`` argument do not need to be created.
  Substantial performance increases result (:issue:`176`)
- Substantially improved performance of generic ``Index.intersection`` and
  ``Index.union``
- Implemented ``BlockManager.take`` resulting in significantly faster ``take``
  performance on mixed-type ``DataFrame`` objects (:issue:`104`)
- Improved performance of ``Series.sort_index``
- Significant groupby performance enhancement: removed unnecessary integrity
  checks in DataFrame internals that were slowing down slicing operations to
  retrieve groups
- Optimized ``_ensure_index`` function resulting in performance savings in
  type-checking Index objects
- Wrote fast time series merging / joining methods in Cython. Will be
  integrated later into DataFrame.join and related functions

.. _ENH1b: https://github.com/pandas-dev/pandas/commit/1ba56251f0013ff7cd8834e9486cef2b10098371
.. _ENHdc: https://github.com/pandas-dev/pandas/commit/dca3c5c5a6a3769ee01465baca04cfdfa66a4f76
.. _ENHed: https://github.com/pandas-dev/pandas/commit/edd9f1945fc010a57fa0ae3b3444d1fffe592591
.. _ENH56: https://github.com/pandas-dev/pandas/commit/56e0c9ffafac79ce262b55a6a13e1b10a88fbe93

Contributors
~~~~~~~~~~~~

.. contributors:: v0.4.1..v0.4.3
.. _whatsnew_0900:

{{ header }}


Version 0.9.0 (October 7, 2012)
-------------------------------

This is a major release from 0.8.1 and includes several new features and
enhancements along with a large number of bug fixes. New features include
vectorized unicode encoding/decoding for ``Series.str``, ``to_latex`` method to
DataFrame, more flexible parsing of boolean values, and enabling the download of
options data from Yahoo! Finance.

New features
~~~~~~~~~~~~

  - Add ``encode`` and ``decode`` for unicode handling to :ref:`vectorized
    string processing methods <text.string_methods>` in Series.str  (:issue:`1706`)
  - Add ``DataFrame.to_latex`` method (:issue:`1735`)
  - Add convenient expanding window equivalents of all rolling_* ops (:issue:`1785`)
  - Add Options class to pandas.io.data for fetching options data from Yahoo!
    Finance (:issue:`1748`, :issue:`1739`)
  - More flexible parsing of boolean values (Yes, No, TRUE, FALSE, etc)
    (:issue:`1691`, :issue:`1295`)
  - Add ``level`` parameter to ``Series.reset_index``
  - ``TimeSeries.between_time`` can now select times across midnight (:issue:`1871`)
  - Series constructor can now handle generator as input (:issue:`1679`)
  - ``DataFrame.dropna`` can now take multiple axes (tuple/list) as input
    (:issue:`924`)
  - Enable ``skip_footer`` parameter in ``ExcelFile.parse`` (:issue:`1843`)

API changes
~~~~~~~~~~~

  - The default column names when ``header=None`` and no columns names passed to
    functions like ``read_csv`` has changed to be more Pythonic and amenable to
    attribute access:

.. ipython:: python

   import io

   data = """
   0,0,1
   1,1,0
   0,1,0
   """
   df = pd.read_csv(io.StringIO(data), header=None)
   df


- Creating a Series from another Series, passing an index, will cause reindexing
  to happen inside rather than treating the Series like an ndarray. Technically
  improper usages like ``Series(df[col1], index=df[col2])`` that worked before
  "by accident" (this was never intended) will lead to all NA Series in some
  cases. To be perfectly clear:

.. ipython:: python

   s1 = pd.Series([1, 2, 3])
   s1

   s2 = pd.Series(s1, index=["foo", "bar", "baz"])
   s2

- Deprecated ``day_of_year`` API removed from PeriodIndex, use ``dayofyear``
  (:issue:`1723`)

- Don't modify NumPy suppress printoption to True at import time

- The internal HDF5 data arrangement for DataFrames has been transposed.  Legacy
  files will still be readable by HDFStore (:issue:`1834`, :issue:`1824`)

- Legacy cruft removed: pandas.stats.misc.quantileTS

- Use ISO8601 format for Period repr: monthly, daily, and on down (:issue:`1776`)

- Empty DataFrame columns are now created as object dtype. This will prevent a
  class of TypeErrors that was occurring in code where the dtype of a column
  would depend on the presence of data or not (e.g. a SQL query having results)
  (:issue:`1783`)

- Setting parts of DataFrame/Panel using ix now aligns input Series/DataFrame
  (:issue:`1630`)

- ``first`` and ``last`` methods in ``GroupBy`` no longer drop non-numeric
  columns (:issue:`1809`)

- Resolved inconsistencies in specifying custom NA values in text parser.
  ``na_values`` of type dict no longer override default NAs unless
  ``keep_default_na`` is set to false explicitly (:issue:`1657`)

- ``DataFrame.dot`` will not do data alignment, and also work with Series
  (:issue:`1915`)


See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.



.. _whatsnew_0.9.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.8.1..v0.9.0
.. _whatsnew_0130:

Version 0.13.0 (January 3, 2014)
--------------------------------

{{ header }}



This is a major release from 0.12.0 and includes a number of API changes, several new features and
enhancements along with a large number of bug fixes.

Highlights include:

- support for a new index type ``Float64Index``, and other Indexing enhancements
- ``HDFStore`` has a new string based syntax for query specification
- support for new methods of interpolation
- updated ``timedelta`` operations
- a new string manipulation method ``extract``
- Nanosecond support for Offsets
- ``isin`` for DataFrames

Several experimental features are added, including:

- new ``eval/query`` methods for expression evaluation
- support for ``msgpack`` serialization
- an i/o interface to Google's ``BigQuery``

Their are several new or updated docs sections including:

- :ref:`Comparison with SQL<compare_with_sql>`, which should be useful for those familiar with SQL but still learning pandas.
- :ref:`Comparison with R<compare_with_r>`, idiom translations from R to pandas.
- :ref:`Enhancing Performance<enhancingperf>`, ways to enhance pandas performance with ``eval/query``.

.. warning::

   In 0.13.0 ``Series`` has internally been refactored to no longer sub-class ``ndarray``
   but instead subclass ``NDFrame``, similar to the rest of the pandas containers. This should be
   a transparent change with only very limited API implications. See :ref:`Internal Refactoring<whatsnew_0130.refactoring>`

API changes
~~~~~~~~~~~

- ``read_excel`` now supports an integer in its ``sheetname`` argument giving
  the index of the sheet to read in (:issue:`4301`).
- Text parser now treats anything that reads like inf ("inf", "Inf", "-Inf",
  "iNf", etc.) as infinity. (:issue:`4220`, :issue:`4219`), affecting
  ``read_table``, ``read_csv``, etc.
- ``pandas`` now is Python 2/3 compatible without the need for 2to3 thanks to
  @jtratner. As a result, pandas now uses iterators more extensively. This
  also led to the introduction of substantive parts of the Benjamin
  Peterson's ``six`` library into compat. (:issue:`4384`, :issue:`4375`,
  :issue:`4372`)
- ``pandas.util.compat`` and ``pandas.util.py3compat`` have been merged into
  ``pandas.compat``. ``pandas.compat`` now includes many functions allowing
  2/3 compatibility. It contains both list and iterator versions of range,
  filter, map and zip, plus other necessary elements for Python 3
  compatibility. ``lmap``, ``lzip``, ``lrange`` and ``lfilter`` all produce
  lists instead of iterators, for compatibility with ``numpy``, subscripting
  and ``pandas`` constructors.(:issue:`4384`, :issue:`4375`, :issue:`4372`)
- ``Series.get`` with negative indexers now returns the same as ``[]`` (:issue:`4390`)
- Changes to how ``Index`` and ``MultiIndex`` handle metadata (``levels``,
  ``labels``, and ``names``) (:issue:`4039`):

  .. code-block:: python

     # previously, you would have set levels or labels directly
     >>> pd.index.levels = [[1, 2, 3, 4], [1, 2, 4, 4]]

     # now, you use the set_levels or set_labels methods
     >>> index = pd.index.set_levels([[1, 2, 3, 4], [1, 2, 4, 4]])

     # similarly, for names, you can rename the object
     # but setting names is not deprecated
     >>> index = pd.index.set_names(["bob", "cranberry"])

     # and all methods take an inplace kwarg - but return None
     >>> pd.index.set_names(["bob", "cranberry"], inplace=True)

- **All** division with ``NDFrame`` objects is now *truedivision*, regardless
  of the future import. This means that operating on pandas objects will by default
  use *floating point* division, and return a floating point dtype.
  You can use ``//`` and ``floordiv`` to do integer division.

  Integer division

  .. code-block:: ipython

     In [3]: arr = np.array([1, 2, 3, 4])

     In [4]: arr2 = np.array([5, 3, 2, 1])

     In [5]: arr / arr2
     Out[5]: array([0, 0, 1, 4])

     In [6]: pd.Series(arr) // pd.Series(arr2)
     Out[6]:
     0    0
     1    0
     2    1
     3    4
     dtype: int64

  True Division

  .. code-block:: ipython

      In [7]: pd.Series(arr) / pd.Series(arr2)  # no future import required
      Out[7]:
      0    0.200000
      1    0.666667
      2    1.500000
      3    4.000000
      dtype: float64

- Infer and downcast dtype if ``downcast='infer'`` is passed to ``fillna/ffill/bfill`` (:issue:`4604`)
- ``__nonzero__`` for all NDFrame objects, will now raise a ``ValueError``, this reverts back to (:issue:`1073`, :issue:`4633`)
  behavior. See :ref:`gotchas<gotchas.truth>` for a more detailed discussion.

  This prevents doing boolean comparison on *entire* pandas objects, which is inherently ambiguous. These all will raise a ``ValueError``.

  .. code-block:: python

     >>> df = pd.DataFrame({'A': np.random.randn(10),
     ...                    'B': np.random.randn(10),
     ...                    'C': pd.date_range('20130101', periods=10)
     ...                    })
     ...
     >>> if df:
     ...     pass
     ...
     Traceback (most recent call last):
         ...
     ValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,
     a.bool(), a.item(), a.any() or a.all().

     >>> df1 = df
     >>> df2 = df
     >>> df1 and df2
     Traceback (most recent call last):
         ...
     ValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,
     a.bool(), a.item(), a.any() or a.all().

     >>> d = [1, 2, 3]
     >>> s1 = pd.Series(d)
     >>> s2 = pd.Series(d)
     >>> s1 and s2
     Traceback (most recent call last):
         ...
     ValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,
     a.bool(), a.item(), a.any() or a.all().

  Added the ``.bool()`` method to ``NDFrame`` objects to facilitate evaluating of single-element boolean Series:

  .. ipython:: python

     pd.Series([True]).bool()
     pd.Series([False]).bool()
     pd.DataFrame([[True]]).bool()
     pd.DataFrame([[False]]).bool()

- All non-Index NDFrames (``Series``, ``DataFrame``, ``Panel``, ``Panel4D``,
  ``SparsePanel``, etc.), now support the entire set of arithmetic operators
  and arithmetic flex methods (add, sub, mul, etc.). ``SparsePanel`` does not
  support ``pow`` or ``mod`` with non-scalars. (:issue:`3765`)
- ``Series`` and ``DataFrame`` now have a ``mode()`` method to calculate the
  statistical mode(s) by axis/Series. (:issue:`5367`)

- Chained assignment will now by default warn if the user is assigning to a copy. This can be changed
  with the option ``mode.chained_assignment``, allowed options are ``raise/warn/None``. See :ref:`the docs<indexing.view_versus_copy>`.

  .. ipython:: python

     dfc = pd.DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})
     pd.set_option('chained_assignment', 'warn')

  The following warning / exception will show if this is attempted.

  .. ipython:: python
     :okwarning:

     dfc.loc[0]['A'] = 1111

  ::

     Traceback (most recent call last)
        ...
     SettingWithCopyWarning:
        A value is trying to be set on a copy of a slice from a DataFrame.
        Try using .loc[row_index,col_indexer] = value instead

  Here is the correct method of assignment.

  .. ipython:: python

     dfc.loc[0, 'A'] = 11
     dfc

- ``Panel.reindex`` has the following call signature ``Panel.reindex(items=None, major_axis=None, minor_axis=None, **kwargs)``
   to conform with other ``NDFrame`` objects. See :ref:`Internal Refactoring<whatsnew_0130.refactoring>` for more information.

- ``Series.argmin`` and ``Series.argmax`` are now aliased to ``Series.idxmin`` and ``Series.idxmax``. These return the *index* of the
   min or max element respectively. Prior to 0.13.0 these would return the position of the min / max element. (:issue:`6214`)

Prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

These were announced changes in 0.12 or prior that are taking effect as of 0.13.0

- Remove deprecated ``Factor`` (:issue:`3650`)
- Remove deprecated ``set_printoptions/reset_printoptions`` (:issue:`3046`)
- Remove deprecated ``_verbose_info`` (:issue:`3215`)
- Remove deprecated ``read_clipboard/to_clipboard/ExcelFile/ExcelWriter`` from ``pandas.io.parsers`` (:issue:`3717`)
  These are available as functions in the main pandas namespace (e.g. ``pd.read_clipboard``)
- default for ``tupleize_cols`` is now ``False`` for both ``to_csv`` and ``read_csv``. Fair warning in 0.12 (:issue:`3604`)
- default for ``display.max_seq_len`` is now 100 rather than ``None``. This activates
  truncated display ("...") of long sequences in various places. (:issue:`3391`)

Deprecations
~~~~~~~~~~~~

Deprecated in 0.13.0

- deprecated ``iterkv``, which will be removed in a future release (this was
  an alias of iteritems used to bypass ``2to3``'s changes).
  (:issue:`4384`, :issue:`4375`, :issue:`4372`)
- deprecated the string method ``match``, whose role is now performed more
  idiomatically by ``extract``. In a future release, the default behavior
  of ``match`` will change to become analogous to ``contains``, which returns
  a boolean indexer. (Their
  distinction is strictness: ``match`` relies on ``re.match`` while
  ``contains`` relies on ``re.search``.) In this release, the deprecated
  behavior is the default, but the new behavior is available through the
  keyword argument ``as_indexer=True``.

Indexing API changes
~~~~~~~~~~~~~~~~~~~~

Prior to 0.13, it was impossible to use a label indexer (``.loc/.ix``) to set a value that
was not contained in the index of a particular axis. (:issue:`2578`). See :ref:`the docs<indexing.basics.partial_setting>`

In the ``Series`` case this is effectively an appending operation

.. ipython:: python

   s = pd.Series([1, 2, 3])
   s
   s[5] = 5.
   s

.. ipython:: python

   dfi = pd.DataFrame(np.arange(6).reshape(3, 2),
                      columns=['A', 'B'])
   dfi

This would previously ``KeyError``

.. ipython:: python

   dfi.loc[:, 'C'] = dfi.loc[:, 'A']
   dfi

This is like an ``append`` operation.

.. ipython:: python

   dfi.loc[3] = 5
   dfi

A Panel setting operation on an arbitrary axis aligns the input to the Panel

.. code-block:: ipython

   In [20]: p = pd.Panel(np.arange(16).reshape(2, 4, 2),
      ....:              items=['Item1', 'Item2'],
      ....:              major_axis=pd.date_range('2001/1/12', periods=4),
      ....:              minor_axis=['A', 'B'], dtype='float64')
      ....:

   In [21]: p
   Out[21]:
   <class 'pandas.core.panel.Panel'>
   Dimensions: 2 (items) x 4 (major_axis) x 2 (minor_axis)
   Items axis: Item1 to Item2
   Major_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00
   Minor_axis axis: A to B

   In [22]: p.loc[:, :, 'C'] = pd.Series([30, 32], index=p.items)

   In [23]: p
   Out[23]:
   <class 'pandas.core.panel.Panel'>
   Dimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)
   Items axis: Item1 to Item2
   Major_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00
   Minor_axis axis: A to C

   In [24]: p.loc[:, :, 'C']
   Out[24]:
               Item1  Item2
   2001-01-12   30.0   32.0
   2001-01-13   30.0   32.0
   2001-01-14   30.0   32.0
   2001-01-15   30.0   32.0

Float64Index API change
~~~~~~~~~~~~~~~~~~~~~~~

- Added a new index type, ``Float64Index``. This will be automatically created when passing floating values in index creation.
  This enables a pure label-based slicing paradigm that makes ``[],ix,loc`` for scalar indexing and slicing work exactly the
  same. See :ref:`the docs<advanced.float64index>`, (:issue:`263`)

  Construction is by default for floating type values.

  .. ipython:: python

     index = pd.Index([1.5, 2, 3, 4.5, 5])
     index
     s = pd.Series(range(5), index=index)
     s

  Scalar selection for ``[],.ix,.loc`` will always be label based. An integer will match an equal float index (e.g. ``3`` is equivalent to ``3.0``)

  .. ipython:: python

     s[3]
     s.loc[3]

  The only positional indexing is via ``iloc``

  .. ipython:: python

     s.iloc[3]

  A scalar index that is not found will raise ``KeyError``

  Slicing is ALWAYS on the values of the index, for ``[],ix,loc`` and ALWAYS positional with ``iloc``

  .. ipython:: python

     s[2:4]
     s.loc[2:4]
     s.iloc[2:4]

  In float indexes, slicing using floats are allowed

  .. ipython:: python

     s[2.1:4.6]
     s.loc[2.1:4.6]

- Indexing on other index types are preserved (and positional fallback for ``[],ix``), with the exception, that floating point slicing
  on indexes on non ``Float64Index`` will now raise a ``TypeError``.

  .. code-block:: ipython

     In [1]: pd.Series(range(5))[3.5]
     TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)

     In [1]: pd.Series(range(5))[3.5:4.5]
     TypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)

  Using a scalar float indexer will be deprecated in a future version, but is allowed for now.

  .. code-block:: ipython

     In [3]: pd.Series(range(5))[3.0]
     Out[3]: 3

HDFStore API changes
~~~~~~~~~~~~~~~~~~~~

- Query Format Changes. A much more string-like query format is now supported. See :ref:`the docs<io.hdf5-query>`.

  .. ipython:: python

     path = 'test.h5'
     dfq = pd.DataFrame(np.random.randn(10, 4),
                        columns=list('ABCD'),
                        index=pd.date_range('20130101', periods=10))
     dfq.to_hdf(path, 'dfq', format='table', data_columns=True)

  Use boolean expressions, with in-line function evaluation.

  .. ipython:: python

     pd.read_hdf(path, 'dfq',
                 where="index>Timestamp('20130104') & columns=['A', 'B']")

  Use an inline column reference

  .. ipython:: python

     pd.read_hdf(path, 'dfq',
                 where="A>0 or C>0")

  .. ipython:: python
     :suppress:

     import os
     os.remove(path)

- the ``format`` keyword now replaces the ``table`` keyword; allowed values are ``fixed(f)`` or ``table(t)``
  the same defaults as prior < 0.13.0 remain, e.g. ``put`` implies ``fixed`` format and ``append`` implies
  ``table`` format. This default format can be set as an option by setting ``io.hdf.default_format``.

  .. ipython:: python

     path = 'test.h5'
     df = pd.DataFrame(np.random.randn(10, 2))
     df.to_hdf(path, 'df_table', format='table')
     df.to_hdf(path, 'df_table2', append=True)
     df.to_hdf(path, 'df_fixed')
     with pd.HDFStore(path) as store:
         print(store)

  .. ipython:: python
     :suppress:

     import os
     os.remove(path)

- Significant table writing performance improvements
- handle a passed ``Series`` in table format (:issue:`4330`)
- can now serialize a ``timedelta64[ns]`` dtype in a table (:issue:`3577`), See :ref:`the docs<io.hdf5-timedelta>`.
- added an ``is_open`` property to indicate if the underlying file handle is_open;
  a closed store will now report 'CLOSED' when viewing the store (rather than raising an error)
  (:issue:`4409`)
- a close of a ``HDFStore`` now will close that instance of the ``HDFStore``
  but will only close the actual file if the ref count (by ``PyTables``) w.r.t. all of the open handles
  are 0. Essentially you have a local instance of ``HDFStore`` referenced by a variable. Once you
  close it, it will report closed. Other references (to the same file) will continue to operate
  until they themselves are closed. Performing an action on a closed file will raise
  ``ClosedFileError``

  .. ipython:: python

     path = 'test.h5'
     df = pd.DataFrame(np.random.randn(10, 2))
     store1 = pd.HDFStore(path)
     store2 = pd.HDFStore(path)
     store1.append('df', df)
     store2.append('df2', df)

     store1
     store2
     store1.close()
     store2
     store2.close()
     store2

  .. ipython:: python
     :suppress:

     import os
     os.remove(path)

- removed the ``_quiet`` attribute, replace by a ``DuplicateWarning`` if retrieving
  duplicate rows from a table (:issue:`4367`)
- removed the ``warn`` argument from ``open``. Instead a ``PossibleDataLossError`` exception will
  be raised if you try to use ``mode='w'`` with an OPEN file handle (:issue:`4367`)
- allow a passed locations array or mask as a ``where`` condition (:issue:`4467`).
  See :ref:`the docs<io.hdf5-where_mask>` for an example.
- add the keyword ``dropna=True`` to ``append`` to change whether ALL nan rows are not written
  to the store (default is ``True``, ALL nan rows are NOT written), also settable
  via the option ``io.hdf.dropna_table`` (:issue:`4625`)
- pass through store creation arguments; can be used to support in-memory stores

DataFrame repr changes
~~~~~~~~~~~~~~~~~~~~~~

The HTML and plain text representations of :class:`DataFrame` now show
a truncated view of the table once it exceeds a certain size, rather
than switching to the short info view (:issue:`4886`, :issue:`5550`).
This makes the representation more consistent as small DataFrames get
larger.

.. image:: ../_static/df_repr_truncated.png
   :alt: Truncated HTML representation of a DataFrame

To get the info view, call :meth:`DataFrame.info`. If you prefer the
info view as the repr for large DataFrames, you can set this by running
``set_option('display.large_repr', 'info')``.

Enhancements
~~~~~~~~~~~~

- ``df.to_clipboard()`` learned a new ``excel`` keyword that let's you
  paste df data directly into excel (enabled by default). (:issue:`5070`).
- ``read_html`` now raises a ``URLError`` instead of catching and raising a
  ``ValueError`` (:issue:`4303`, :issue:`4305`)
- Added a test for ``read_clipboard()`` and ``to_clipboard()`` (:issue:`4282`)
- Clipboard functionality now works with PySide (:issue:`4282`)
- Added a more informative error message when plot arguments contain
  overlapping color and style arguments (:issue:`4402`)
- ``to_dict`` now takes ``records`` as a possible out type.  Returns an array
  of column-keyed dictionaries. (:issue:`4936`)

- ``NaN`` handing in get_dummies (:issue:`4446`) with ``dummy_na``

  .. ipython:: python

     # previously, nan was erroneously counted as 2 here
     # now it is not counted at all
     pd.get_dummies([1, 2, np.nan])

     # unless requested
     pd.get_dummies([1, 2, np.nan], dummy_na=True)


- ``timedelta64[ns]`` operations. See :ref:`the docs<timedeltas.timedeltas_convert>`.

  .. warning::

     Most of these operations require ``numpy >= 1.7``

  Using the new top-level ``to_timedelta``, you can convert a scalar or array from the standard
  timedelta format (produced by ``to_csv``) into a timedelta type (``np.timedelta64`` in ``nanoseconds``).

  .. ipython:: python

     pd.to_timedelta('1 days 06:05:01.00003')
     pd.to_timedelta('15.5us')
     pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])
     pd.to_timedelta(np.arange(5), unit='s')
     pd.to_timedelta(np.arange(5), unit='d')

  A Series of dtype ``timedelta64[ns]`` can now be divided by another
  ``timedelta64[ns]`` object, or astyped to yield a ``float64`` dtyped Series. This
  is frequency conversion. See :ref:`the docs<timedeltas.timedeltas_convert>` for the docs.

  .. ipython:: python

     import datetime
     td = pd.Series(pd.date_range('20130101', periods=4)) - pd.Series(
         pd.date_range('20121201', periods=4))
     td[2] += np.timedelta64(datetime.timedelta(minutes=5, seconds=3))
     td[3] = np.nan
     td

     # to days
     td / np.timedelta64(1, 'D')
     td.astype('timedelta64[D]')

     # to seconds
     td / np.timedelta64(1, 's')
     td.astype('timedelta64[s]')

  Dividing or multiplying a ``timedelta64[ns]`` Series by an integer or integer Series

  .. ipython:: python

     td * -1
     td * pd.Series([1, 2, 3, 4])

  Absolute ``DateOffset`` objects can act equivalently to ``timedeltas``

  .. ipython:: python

     from pandas import offsets
     td + offsets.Minute(5) + offsets.Milli(5)

  Fillna is now supported for timedeltas

  .. ipython:: python

     td.fillna(pd.Timedelta(0))
     td.fillna(datetime.timedelta(days=1, seconds=5))

  You can do numeric reduction operations on timedeltas.

  .. ipython:: python

     td.mean()
     td.quantile(.1)

- ``plot(kind='kde')`` now accepts the optional parameters ``bw_method`` and
  ``ind``, passed to scipy.stats.gaussian_kde() (for scipy >= 0.11.0) to set
  the bandwidth, and to gkde.evaluate() to specify the indices at which it
  is evaluated, respectively. See scipy docs. (:issue:`4298`)

- DataFrame constructor now accepts a numpy masked record array (:issue:`3478`)

- The new vectorized string method ``extract`` return regular expression
  matches more conveniently.

  .. ipython:: python
     :okwarning:

     pd.Series(['a1', 'b2', 'c3']).str.extract('[ab](\\d)')

  Elements that do not match return ``NaN``. Extracting a regular expression
  with more than one group returns a DataFrame with one column per group.


  .. ipython:: python
     :okwarning:

     pd.Series(['a1', 'b2', 'c3']).str.extract('([ab])(\\d)')

  Elements that do not match return a row of ``NaN``.
  Thus, a Series of messy strings can be *converted* into a
  like-indexed Series or DataFrame of cleaned-up or more useful strings,
  without necessitating ``get()`` to access tuples or ``re.match`` objects.

  Named groups like

  .. ipython:: python
     :okwarning:

     pd.Series(['a1', 'b2', 'c3']).str.extract(
         '(?P<letter>[ab])(?P<digit>\\d)')

  and optional groups can also be used.

  .. ipython:: python
     :okwarning:

      pd.Series(['a1', 'b2', '3']).str.extract(
          '(?P<letter>[ab])?(?P<digit>\\d)')

- ``read_stata`` now accepts Stata 13 format (:issue:`4291`)

- ``read_fwf`` now infers the column specifications from the first 100 rows of
  the file if the data has correctly separated and properly aligned columns
  using the delimiter provided to the function (:issue:`4488`).

- support for nanosecond times as an offset

  .. warning::

     These operations require ``numpy >= 1.7``

  Period conversions in the range of seconds and below were reworked and extended
  up to nanoseconds. Periods in the nanosecond range are now available.

  .. ipython:: python

     pd.date_range('2013-01-01', periods=5, freq='5N')

  or with frequency as offset

  .. ipython:: python

     pd.date_range('2013-01-01', periods=5, freq=pd.offsets.Nano(5))

  Timestamps can be modified in the nanosecond range

  .. ipython:: python

     t = pd.Timestamp('20130101 09:01:02')
     t + pd.tseries.offsets.Nano(123)

- A new method, ``isin`` for DataFrames, which plays nicely with boolean indexing. The argument to ``isin``, what we're comparing the DataFrame to, can be a DataFrame, Series, dict, or array of values. See :ref:`the docs<indexing.basics.indexing_isin>` for more.

  To get the rows where any of the conditions are met:

  .. ipython:: python

     dfi = pd.DataFrame({'A': [1, 2, 3, 4], 'B': ['a', 'b', 'f', 'n']})
     dfi
     other = pd.DataFrame({'A': [1, 3, 3, 7], 'B': ['e', 'f', 'f', 'e']})
     mask = dfi.isin(other)
     mask
     dfi[mask.any(1)]

- ``Series`` now supports a ``to_frame`` method to convert it to a single-column DataFrame (:issue:`5164`)

- All R datasets listed here http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html can now be loaded into pandas objects

  .. code-block:: python

     # note that pandas.rpy was deprecated in v0.16.0
     import pandas.rpy.common as com
     com.load_data('Titanic')

- ``tz_localize`` can infer a fall daylight savings transition based on the structure
  of the unlocalized data (:issue:`4230`), see :ref:`the docs<timeseries.timezone>`

- ``DatetimeIndex`` is now in the API documentation, see :ref:`the docs<api.datetimeindex>`

- :meth:`~pandas.io.json.json_normalize` is a new method to allow you to create a flat table
  from semi-structured JSON data. See :ref:`the docs<io.json_normalize>` (:issue:`1067`)

- Added PySide support for the qtpandas DataFrameModel and DataFrameWidget.

- Python csv parser now supports usecols (:issue:`4335`)

- Frequencies gained several new offsets:

  * ``LastWeekOfMonth`` (:issue:`4637`)
  * ``FY5253``, and ``FY5253Quarter`` (:issue:`4511`)


- DataFrame has a new ``interpolate`` method, similar to Series (:issue:`4434`, :issue:`1892`)

  .. ipython:: python

      df = pd.DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
                        'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]})
      df.interpolate()

  Additionally, the ``method`` argument to ``interpolate`` has been expanded
  to include ``'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
  'barycentric', 'krogh', 'piecewise_polynomial', 'pchip', 'polynomial', 'spline'``
  The new methods require scipy_. Consult the Scipy reference guide_ and documentation_ for more information
  about when the various methods are appropriate. See :ref:`the docs<missing_data.interpolate>`.

  Interpolate now also accepts a ``limit`` keyword argument.
  This works similar to ``fillna``'s limit:

  .. ipython:: python

    ser = pd.Series([1, 3, np.nan, np.nan, np.nan, 11])
    ser.interpolate(limit=2)

- Added ``wide_to_long`` panel data convenience function. See :ref:`the docs<reshaping.melt>`.

  .. ipython:: python

    np.random.seed(123)
    df = pd.DataFrame({"A1970" : {0 : "a", 1 : "b", 2 : "c"},
                       "A1980" : {0 : "d", 1 : "e", 2 : "f"},
                       "B1970" : {0 : 2.5, 1 : 1.2, 2 : .7},
                       "B1980" : {0 : 3.2, 1 : 1.3, 2 : .1},
                       "X"     : dict(zip(range(3), np.random.randn(3)))
                      })
    df["id"] = df.index
    df
    pd.wide_to_long(df, ["A", "B"], i="id", j="year")

.. _scipy: http://www.scipy.org
.. _documentation: http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation
.. _guide: http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html

- ``to_csv`` now takes a ``date_format`` keyword argument that specifies how
  output datetime objects should be formatted. Datetimes encountered in the
  index, columns, and values will all have this formatting applied. (:issue:`4313`)
- ``DataFrame.plot`` will scatter plot x versus y by passing ``kind='scatter'`` (:issue:`2215`)
- Added support for Google Analytics v3 API segment IDs that also supports v2 IDs. (:issue:`5271`)

.. _whatsnew_0130.experimental:

Experimental
~~~~~~~~~~~~

- The new :func:`~pandas.eval` function implements expression evaluation using
  ``numexpr`` behind the scenes. This results in large speedups for
  complicated expressions involving large DataFrames/Series. For example,

  .. ipython:: python

     nrows, ncols = 20000, 100
     df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols))
                           for _ in range(4)]

  .. ipython:: python

     # eval with NumExpr backend
     %timeit pd.eval('df1 + df2 + df3 + df4')

  .. ipython:: python

     # pure Python evaluation
     %timeit df1 + df2 + df3 + df4

  For more details, see the :ref:`the docs<enhancingperf.eval>`

- Similar to ``pandas.eval``, :class:`~pandas.DataFrame` has a new
  ``DataFrame.eval`` method that evaluates an expression in the context of
  the ``DataFrame``. For example,

  .. ipython:: python
     :suppress:

     try:
         del a  # noqa: F821
     except NameError:
         pass

     try:
         del b  # noqa: F821
     except NameError:
         pass

  .. ipython:: python

     df = pd.DataFrame(np.random.randn(10, 2), columns=['a', 'b'])
     df.eval('a + b')

- :meth:`~pandas.DataFrame.query` method has been added that allows
  you to select elements of a ``DataFrame`` using a natural query syntax
  nearly identical to Python syntax. For example,

  .. ipython:: python
     :suppress:

     try:
         del a  # noqa: F821
     except NameError:
         pass

     try:
         del b  # noqa: F821
     except NameError:
         pass

     try:
         del c  # noqa: F821
     except NameError:
         pass

  .. ipython:: python

     n = 20
     df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=['a', 'b', 'c'])
     df.query('a < b < c')

  selects all the rows of ``df`` where ``a < b < c`` evaluates to ``True``.
  For more details see the :ref:`the docs<indexing.query>`.

- ``pd.read_msgpack()`` and ``pd.to_msgpack()`` are now a supported method of serialization
  of arbitrary pandas (and python objects) in a lightweight portable binary format. See :ref:`the docs<io.msgpack>`

  .. warning::

     Since this is an EXPERIMENTAL LIBRARY, the storage format may not be stable until a future release.

  .. code-block:: python

     df = pd.DataFrame(np.random.rand(5, 2), columns=list('AB'))
     df.to_msgpack('foo.msg')
     pd.read_msgpack('foo.msg')

     s = pd.Series(np.random.rand(5), index=pd.date_range('20130101', periods=5))
     pd.to_msgpack('foo.msg', df, s)
     pd.read_msgpack('foo.msg')

  You can pass ``iterator=True`` to iterator over the unpacked results

  .. code-block:: python

     for o in pd.read_msgpack('foo.msg', iterator=True):
         print(o)

  .. ipython:: python
     :suppress:
     :okexcept:

     os.remove('foo.msg')

- ``pandas.io.gbq`` provides a simple way to extract from, and load data into,
  Google's BigQuery Data Sets by way of pandas DataFrames. BigQuery is a high
  performance SQL-like database service, useful for performing ad-hoc queries
  against extremely large datasets. :ref:`See the docs <io.bigquery>`

  .. code-block:: python

     from pandas.io import gbq

     # A query to select the average monthly temperatures in the
     # in the year 2000 across the USA. The dataset,
     # publicata:samples.gsod, is available on all BigQuery accounts,
     # and is based on NOAA gsod data.

     query = """SELECT station_number as STATION,
     month as MONTH, AVG(mean_temp) as MEAN_TEMP
     FROM publicdata:samples.gsod
     WHERE YEAR = 2000
     GROUP BY STATION, MONTH
     ORDER BY STATION, MONTH ASC"""

     # Fetch the result set for this query

     # Your Google BigQuery Project ID
     # To find this, see your dashboard:
     # https://console.developers.google.com/iam-admin/projects?authuser=0
     projectid = 'xxxxxxxxx'
     df = gbq.read_gbq(query, project_id=projectid)

     # Use pandas to process and reshape the dataset

     df2 = df.pivot(index='STATION', columns='MONTH', values='MEAN_TEMP')
     df3 = pd.concat([df2.min(), df2.mean(), df2.max()],
                     axis=1, keys=["Min Tem", "Mean Temp", "Max Temp"])

  The resulting DataFrame is::

     > df3
                 Min Tem  Mean Temp    Max Temp
      MONTH
      1     -53.336667  39.827892   89.770968
      2     -49.837500  43.685219   93.437932
      3     -77.926087  48.708355   96.099998
      4     -82.892858  55.070087   97.317240
      5     -92.378261  61.428117  102.042856
      6     -77.703334  65.858888  102.900000
      7     -87.821428  68.169663  106.510714
      8     -89.431999  68.614215  105.500000
      9     -86.611112  63.436935  107.142856
      10    -78.209677  56.880838   92.103333
      11    -50.125000  48.861228   94.996428
      12    -50.332258  42.286879   94.396774

  .. warning::

     To use this module, you will need a BigQuery account. See
     <https://cloud.google.com/products/big-query> for details.

     As of 10/10/13, there is a bug in Google's API preventing result sets
     from being larger than 100,000 rows. A patch is scheduled for the week of
     10/14/13.

.. _whatsnew_0130.refactoring:

Internal refactoring
~~~~~~~~~~~~~~~~~~~~

In 0.13.0 there is a major refactor primarily to subclass ``Series`` from
``NDFrame``, which is the base class currently for ``DataFrame`` and ``Panel``,
to unify methods and behaviors. Series formerly subclassed directly from
``ndarray``. (:issue:`4080`, :issue:`3862`, :issue:`816`)

.. warning::

   There are two potential incompatibilities from < 0.13.0

   - Using certain numpy functions would previously return a ``Series`` if passed a ``Series``
     as an argument. This seems only to affect ``np.ones_like``, ``np.empty_like``,
     ``np.diff`` and ``np.where``. These now return ``ndarrays``.

     .. ipython:: python

        s = pd.Series([1, 2, 3, 4])

     Numpy Usage

     .. ipython:: python

        np.ones_like(s)
        np.diff(s)
        np.where(s > 1, s, np.nan)

     Pandonic Usage

     .. ipython:: python

        pd.Series(1, index=s.index)
        s.diff()
        s.where(s > 1)

   - Passing a ``Series`` directly to a cython function expecting an ``ndarray`` type will no
     long work directly, you must pass ``Series.values``, See :ref:`Enhancing Performance<enhancingperf.ndarray>`

   - ``Series(0.5)`` would previously return the scalar ``0.5``, instead this will return a 1-element ``Series``

   - This change breaks ``rpy2<=2.3.8``. an Issue has been opened against rpy2 and a workaround
     is detailed in :issue:`5698`. Thanks @JanSchulz.

- Pickle compatibility is preserved for pickles created prior to 0.13. These must be unpickled with ``pd.read_pickle``, see :ref:`Pickling<io.pickle>`.

- Refactor of series.py/frame.py/panel.py to move common code to generic.py

  - added ``_setup_axes`` to created generic NDFrame structures
  - moved methods

    - ``from_axes,_wrap_array,axes,ix,loc,iloc,shape,empty,swapaxes,transpose,pop``
    - ``__iter__,keys,__contains__,__len__,__neg__,__invert__``
    - ``convert_objects,as_blocks,as_matrix,values``
    - ``__getstate__,__setstate__`` (compat remains in frame/panel)
    - ``__getattr__,__setattr__``
    - ``_indexed_same,reindex_like,align,where,mask``
    - ``fillna,replace`` (``Series`` replace is now consistent with ``DataFrame``)
    - ``filter`` (also added axis argument to selectively filter on a different axis)
    - ``reindex,reindex_axis,take``
    - ``truncate`` (moved to become part of ``NDFrame``)

- These are API changes which make ``Panel`` more consistent with ``DataFrame``

  - ``swapaxes`` on a ``Panel`` with the same axes specified now return a copy
  - support attribute access for setting
  - filter supports the same API as the original ``DataFrame`` filter

- Reindex called with no arguments will now return a copy of the input object

- ``TimeSeries`` is now an alias for ``Series``. the property ``is_time_series``
  can be used to distinguish (if desired)

- Refactor of Sparse objects to use BlockManager

  - Created a new block type in internals, ``SparseBlock``, which can hold multi-dtypes
    and is non-consolidatable. ``SparseSeries`` and ``SparseDataFrame`` now inherit
    more methods from there hierarchy (Series/DataFrame), and no longer inherit
    from ``SparseArray`` (which instead is the object of the ``SparseBlock``)
  - Sparse suite now supports integration with non-sparse data. Non-float sparse
    data is supportable (partially implemented)
  - Operations on sparse structures within DataFrames should preserve sparseness,
    merging type operations will convert to dense (and back to sparse), so might
    be somewhat inefficient
  - enable setitem on ``SparseSeries`` for boolean/integer/slices
  - ``SparsePanels`` implementation is unchanged (e.g. not using BlockManager, needs work)

- added ``ftypes`` method to Series/DataFrame, similar to ``dtypes``, but indicates
  if the underlying is sparse/dense (as well as the dtype)
- All ``NDFrame`` objects can now use ``__finalize__()`` to specify various
  values to propagate to new objects from an existing one (e.g. ``name`` in ``Series`` will
  follow more automatically now)
- Internal type checking is now done via a suite of generated classes, allowing ``isinstance(value, klass)``
  without having to directly import the klass, courtesy of @jtratner
- Bug in Series update where the parent frame is not updating its cache based on
  changes (:issue:`4080`) or types (:issue:`3217`), fillna (:issue:`3386`)
- Indexing with dtype conversions fixed (:issue:`4463`, :issue:`4204`)
- Refactor ``Series.reindex`` to core/generic.py (:issue:`4604`, :issue:`4618`), allow ``method=`` in reindexing
  on a Series to work
- ``Series.copy`` no longer accepts the ``order`` parameter and is now consistent with ``NDFrame`` copy
- Refactor ``rename`` methods to core/generic.py; fixes ``Series.rename`` for (:issue:`4605`), and adds ``rename``
  with the same signature for ``Panel``
- Refactor ``clip`` methods to core/generic.py (:issue:`4798`)
- Refactor of ``_get_numeric_data/_get_bool_data`` to core/generic.py, allowing Series/Panel functionality
- ``Series`` (for index) / ``Panel`` (for items) now allow attribute access to its elements  (:issue:`1903`)

  .. ipython:: python

     s = pd.Series([1, 2, 3], index=list('abc'))
     s.b
     s.a = 5
     s

.. _release.bug_fixes-0.13.0:

Bug fixes
~~~~~~~~~

- ``HDFStore``

  - raising an invalid ``TypeError`` rather than ``ValueError`` when
    appending with a different block ordering (:issue:`4096`)
  - ``read_hdf`` was not respecting as passed ``mode`` (:issue:`4504`)
  - appending a 0-len table will work correctly (:issue:`4273`)
  - ``to_hdf`` was raising when passing both arguments ``append`` and
    ``table`` (:issue:`4584`)
  - reading from a store with duplicate columns across dtypes would raise
    (:issue:`4767`)
  - Fixed a bug where ``ValueError`` wasn't correctly raised when column
    names weren't strings (:issue:`4956`)
  - A zero length series written in Fixed format not deserializing properly.
    (:issue:`4708`)
  - Fixed decoding perf issue on pyt3 (:issue:`5441`)
  - Validate levels in a MultiIndex before storing (:issue:`5527`)
  - Correctly handle ``data_columns`` with a Panel (:issue:`5717`)
- Fixed bug in tslib.tz_convert(vals, tz1, tz2): it could raise IndexError
  exception while trying to access trans[pos + 1] (:issue:`4496`)
- The ``by`` argument now works correctly with the ``layout`` argument
  (:issue:`4102`, :issue:`4014`) in ``*.hist`` plotting methods
- Fixed bug in ``PeriodIndex.map`` where using ``str`` would return the str
  representation of the index (:issue:`4136`)
- Fixed test failure ``test_time_series_plot_color_with_empty_kwargs`` when
  using custom matplotlib default colors (:issue:`4345`)
- Fix running of stata IO tests. Now uses temporary files to write
  (:issue:`4353`)
- Fixed an issue where ``DataFrame.sum`` was slower than ``DataFrame.mean``
  for integer valued frames (:issue:`4365`)
- ``read_html`` tests now work with Python 2.6 (:issue:`4351`)
- Fixed bug where ``network`` testing was throwing ``NameError`` because a
  local variable was undefined (:issue:`4381`)
- In ``to_json``, raise if a passed ``orient`` would cause loss of data
  because of a duplicate index (:issue:`4359`)
- In ``to_json``, fix date handling so milliseconds are the default timestamp
  as the docstring says (:issue:`4362`).
- ``as_index`` is no longer ignored when doing groupby apply (:issue:`4648`,
  :issue:`3417`)
- JSON NaT handling fixed, NaTs are now serialized to ``null`` (:issue:`4498`)
- Fixed JSON handling of escapable characters in JSON object keys
  (:issue:`4593`)
- Fixed passing ``keep_default_na=False`` when ``na_values=None``
  (:issue:`4318`)
- Fixed bug with ``values`` raising an error on a DataFrame with duplicate
  columns and mixed dtypes, surfaced in (:issue:`4377`)
- Fixed bug with duplicate columns and type conversion in ``read_json`` when
  ``orient='split'`` (:issue:`4377`)
- Fixed JSON bug where locales with decimal separators other than '.' threw
  exceptions when encoding / decoding certain values. (:issue:`4918`)
- Fix ``.iat`` indexing with a ``PeriodIndex`` (:issue:`4390`)
- Fixed an issue where ``PeriodIndex`` joining with self was returning a new
  instance rather than the same instance (:issue:`4379`); also adds a test
  for this for the other index types
- Fixed a bug with all the dtypes being converted to object when using the
  CSV cparser with the usecols parameter (:issue:`3192`)
- Fix an issue in merging blocks where the resulting DataFrame had partially
  set _ref_locs (:issue:`4403`)
- Fixed an issue where hist subplots were being overwritten when they were
  called using the top level matplotlib API (:issue:`4408`)
- Fixed a bug where calling ``Series.astype(str)`` would truncate the string
  (:issue:`4405`, :issue:`4437`)
- Fixed a py3 compat issue where bytes were being repr'd as tuples
  (:issue:`4455`)
- Fixed Panel attribute naming conflict if item is named 'a'
  (:issue:`3440`)
- Fixed an issue where duplicate indexes were raising when plotting
  (:issue:`4486`)
- Fixed an issue where cumsum and cumprod didn't work with bool dtypes
  (:issue:`4170`, :issue:`4440`)
- Fixed Panel slicing issued in ``xs`` that was returning an incorrect dimmed
  object (:issue:`4016`)
- Fix resampling bug where custom reduce function not used if only one group
  (:issue:`3849`, :issue:`4494`)
- Fixed Panel assignment with a transposed frame (:issue:`3830`)
- Raise on set indexing with a Panel and a Panel as a value which needs
  alignment (:issue:`3777`)
- frozenset objects now raise in the ``Series`` constructor (:issue:`4482`,
  :issue:`4480`)
- Fixed issue with sorting a duplicate MultiIndex that has multiple dtypes
  (:issue:`4516`)
- Fixed bug in ``DataFrame.set_values`` which was causing name attributes to
  be lost when expanding the index. (:issue:`3742`, :issue:`4039`)
- Fixed issue where individual ``names``, ``levels`` and ``labels`` could be
  set on ``MultiIndex`` without validation (:issue:`3714`, :issue:`4039`)
- Fixed (:issue:`3334`) in pivot_table. Margins did not compute if values is
  the index.
- Fix bug in having a rhs of ``np.timedelta64`` or ``np.offsets.DateOffset``
  when operating with datetimes (:issue:`4532`)
- Fix arithmetic with series/datetimeindex and ``np.timedelta64`` not working
  the same (:issue:`4134`) and buggy timedelta in NumPy 1.6 (:issue:`4135`)
- Fix bug in ``pd.read_clipboard`` on windows with PY3 (:issue:`4561`); not
  decoding properly
- ``tslib.get_period_field()`` and ``tslib.get_period_field_arr()`` now raise
  if code argument out of range (:issue:`4519`, :issue:`4520`)
- Fix boolean indexing on an empty series loses index names (:issue:`4235`),
  infer_dtype works with empty arrays.
- Fix reindexing with multiple axes; if an axes match was not replacing the
  current axes, leading to a possible lazy frequency inference issue
  (:issue:`3317`)
- Fixed issue where ``DataFrame.apply`` was reraising exceptions incorrectly
  (causing the original stack trace to be truncated).
- Fix selection with ``ix/loc`` and non_unique selectors (:issue:`4619`)
- Fix assignment with iloc/loc involving a dtype change in an existing column
  (:issue:`4312`, :issue:`5702`) have internal setitem_with_indexer in core/indexing
  to use Block.setitem
- Fixed bug where thousands operator was not handled correctly for floating
  point numbers in csv_import (:issue:`4322`)
- Fix an issue with CacheableOffset not properly being used by many
  DateOffset; this prevented the DateOffset from being cached (:issue:`4609`)
- Fix boolean comparison with a DataFrame on the lhs, and a list/tuple on the
  rhs (:issue:`4576`)
- Fix error/dtype conversion with setitem of ``None`` on ``Series/DataFrame``
  (:issue:`4667`)
- Fix decoding based on a passed in non-default encoding in ``pd.read_stata``
  (:issue:`4626`)
- Fix ``DataFrame.from_records`` with a plain-vanilla ``ndarray``.
  (:issue:`4727`)
- Fix some inconsistencies with ``Index.rename`` and ``MultiIndex.rename``,
  etc. (:issue:`4718`, :issue:`4628`)
- Bug in using ``iloc/loc`` with a cross-sectional and duplicate indices
  (:issue:`4726`)
- Bug with using ``QUOTE_NONE`` with ``to_csv`` causing ``Exception``.
  (:issue:`4328`)
- Bug with Series indexing not raising an error when the right-hand-side has
  an incorrect length (:issue:`2702`)
- Bug in MultiIndexing with a partial string selection as one part of a
  MultIndex (:issue:`4758`)
- Bug with reindexing on the index with a non-unique index will now raise
  ``ValueError`` (:issue:`4746`)
- Bug in setting with ``loc/ix`` a single indexer with a MultiIndex axis and
  a NumPy array, related to (:issue:`3777`)
- Bug in concatenation with duplicate columns across dtypes not merging with
  axis=0 (:issue:`4771`, :issue:`4975`)
- Bug in ``iloc`` with a slice index failing (:issue:`4771`)
- Incorrect error message with no colspecs or width in ``read_fwf``.
  (:issue:`4774`)
- Fix bugs in indexing in a Series with a duplicate index (:issue:`4548`,
  :issue:`4550`)
- Fixed bug with reading compressed files with ``read_fwf`` in Python 3.
  (:issue:`3963`)
- Fixed an issue with a duplicate index and assignment with a dtype change
  (:issue:`4686`)
- Fixed bug with reading compressed files in as ``bytes`` rather than ``str``
  in Python 3. Simplifies bytes-producing file-handling in Python 3
  (:issue:`3963`, :issue:`4785`).
- Fixed an issue related to ticklocs/ticklabels with log scale bar plots
  across different versions of matplotlib (:issue:`4789`)
- Suppressed DeprecationWarning associated with internal calls issued by
  repr() (:issue:`4391`)
- Fixed an issue with a duplicate index and duplicate selector with ``.loc``
  (:issue:`4825`)
- Fixed an issue with ``DataFrame.sort_index`` where, when sorting by a
  single column and passing a list for ``ascending``, the argument for
  ``ascending`` was being interpreted as ``True`` (:issue:`4839`,
  :issue:`4846`)
- Fixed ``Panel.tshift`` not working. Added ``freq`` support to ``Panel.shift``
  (:issue:`4853`)
- Fix an issue in TextFileReader w/ Python engine (i.e. PythonParser)
  with thousands != "," (:issue:`4596`)
- Bug in getitem with a duplicate index when using where (:issue:`4879`)
- Fix Type inference code coerces float column into datetime (:issue:`4601`)
- Fixed ``_ensure_numeric`` does not check for complex numbers
  (:issue:`4902`)
- Fixed a bug in ``Series.hist`` where two figures were being created when
  the ``by`` argument was passed (:issue:`4112`, :issue:`4113`).
- Fixed a bug in ``convert_objects`` for > 2 ndims (:issue:`4937`)
- Fixed a bug in DataFrame/Panel cache insertion and subsequent indexing
  (:issue:`4939`, :issue:`5424`)
- Fixed string methods for ``FrozenNDArray`` and ``FrozenList``
  (:issue:`4929`)
- Fixed a bug with setting invalid or out-of-range values in indexing
  enlargement scenarios (:issue:`4940`)
- Tests for fillna on empty Series (:issue:`4346`), thanks @immerrr
- Fixed ``copy()`` to shallow copy axes/indices as well and thereby keep
  separate metadata. (:issue:`4202`, :issue:`4830`)
- Fixed skiprows option in Python parser for read_csv (:issue:`4382`)
- Fixed bug preventing ``cut`` from working with ``np.inf`` levels without
  explicitly passing labels (:issue:`3415`)
- Fixed wrong check for overlapping in ``DatetimeIndex.union``
  (:issue:`4564`)
- Fixed conflict between thousands separator and date parser in csv_parser
  (:issue:`4678`)
- Fix appending when dtypes are not the same (error showing mixing
  float/np.datetime64) (:issue:`4993`)
- Fix repr for DateOffset. No longer show duplicate entries in kwds.
  Removed unused offset fields. (:issue:`4638`)
- Fixed wrong index name during read_csv if using usecols. Applies to c
  parser only. (:issue:`4201`)
- ``Timestamp`` objects can now appear in the left hand side of a comparison
  operation with a ``Series`` or ``DataFrame`` object (:issue:`4982`).
- Fix a bug when indexing with ``np.nan`` via ``iloc/loc`` (:issue:`5016`)
- Fixed a bug where low memory c parser could create different types in
  different chunks of the same file. Now coerces to numerical type or raises
  warning. (:issue:`3866`)
- Fix a bug where reshaping a ``Series`` to its own shape raised
  ``TypeError`` (:issue:`4554`) and other reshaping issues.
- Bug in setting with ``ix/loc`` and a mixed int/string index (:issue:`4544`)
- Make sure series-series boolean comparisons are label based (:issue:`4947`)
- Bug in multi-level indexing with a Timestamp partial indexer
  (:issue:`4294`)
- Tests/fix for MultiIndex construction of an all-nan frame (:issue:`4078`)
- Fixed a bug where :func:`~pandas.read_html` wasn't correctly inferring
  values of tables with commas (:issue:`5029`)
- Fixed a bug where :func:`~pandas.read_html` wasn't providing a stable
  ordering of returned tables (:issue:`4770`, :issue:`5029`).
- Fixed a bug where :func:`~pandas.read_html` was incorrectly parsing when
  passed ``index_col=0`` (:issue:`5066`).
- Fixed a bug where :func:`~pandas.read_html` was incorrectly inferring the
  type of headers (:issue:`5048`).
- Fixed a bug where ``DatetimeIndex`` joins with ``PeriodIndex`` caused a
  stack overflow (:issue:`3899`).
- Fixed a bug where ``groupby`` objects didn't allow plots (:issue:`5102`).
- Fixed a bug where ``groupby`` objects weren't tab-completing column names
  (:issue:`5102`).
- Fixed a bug where ``groupby.plot()`` and friends were duplicating figures
  multiple times (:issue:`5102`).
- Provide automatic conversion of ``object`` dtypes on fillna, related
  (:issue:`5103`)
- Fixed a bug where default options were being overwritten in the option
  parser cleaning (:issue:`5121`).
- Treat a list/ndarray identically for ``iloc`` indexing with list-like
  (:issue:`5006`)
- Fix ``MultiIndex.get_level_values()`` with missing values (:issue:`5074`)
- Fix bound checking for Timestamp() with datetime64 input (:issue:`4065`)
- Fix a bug where ``TestReadHtml`` wasn't calling the correct ``read_html()``
  function (:issue:`5150`).
- Fix a bug with ``NDFrame.replace()`` which made replacement appear as
  though it was (incorrectly) using regular expressions (:issue:`5143`).
- Fix better error message for to_datetime (:issue:`4928`)
- Made sure different locales are tested on travis-ci (:issue:`4918`). Also
  adds a couple of utilities for getting locales and setting locales with a
  context manager.
- Fixed segfault on ``isnull(MultiIndex)`` (now raises an error instead)
  (:issue:`5123`, :issue:`5125`)
- Allow duplicate indices when performing operations that align
  (:issue:`5185`, :issue:`5639`)
- Compound dtypes in a constructor raise ``NotImplementedError``
  (:issue:`5191`)
- Bug in comparing duplicate frames (:issue:`4421`) related
- Bug in describe on duplicate frames
- Bug in ``to_datetime`` with a format and ``coerce=True`` not raising
  (:issue:`5195`)
- Bug in ``loc`` setting with multiple indexers and a rhs of a Series that
  needs broadcasting (:issue:`5206`)
- Fixed bug where inplace setting of levels or labels on ``MultiIndex`` would
  not clear cached ``values`` property and therefore return wrong ``values``.
  (:issue:`5215`)
- Fixed bug where filtering a grouped DataFrame or Series did not maintain
  the original ordering (:issue:`4621`).
- Fixed ``Period`` with a business date freq to always roll-forward if on a
  non-business date. (:issue:`5203`)
- Fixed bug in Excel writers where frames with duplicate column names weren't
  written correctly. (:issue:`5235`)
- Fixed issue with ``drop`` and a non-unique index on Series (:issue:`5248`)
- Fixed segfault in C parser caused by passing more names than columns in
  the file. (:issue:`5156`)
- Fix ``Series.isin`` with date/time-like dtypes (:issue:`5021`)
- C and Python Parser can now handle the more common MultiIndex column
  format which doesn't have a row for index names (:issue:`4702`)
- Bug when trying to use an out-of-bounds date as an object dtype
  (:issue:`5312`)
- Bug when trying to display an embedded PandasObject (:issue:`5324`)
- Allows operating of Timestamps to return a datetime if the result is out-of-bounds
  related (:issue:`5312`)
- Fix return value/type signature of ``initObjToJSON()`` to be compatible
  with numpy's ``import_array()`` (:issue:`5334`, :issue:`5326`)
- Bug when renaming then set_index on a DataFrame (:issue:`5344`)
- Test suite no longer leaves around temporary files when testing graphics. (:issue:`5347`)
  (thanks for catching this @yarikoptic!)
- Fixed html tests on win32. (:issue:`4580`)
- Make sure that ``head/tail`` are ``iloc`` based, (:issue:`5370`)
- Fixed bug for ``PeriodIndex`` string representation if there are 1 or 2
  elements. (:issue:`5372`)
- The GroupBy methods ``transform`` and ``filter`` can be used on Series
  and DataFrames that have repeated (non-unique) indices. (:issue:`4620`)
- Fix empty series not printing name in repr (:issue:`4651`)
- Make tests create temp files in temp directory by default. (:issue:`5419`)
- ``pd.to_timedelta`` of a scalar returns a scalar (:issue:`5410`)
- ``pd.to_timedelta`` accepts ``NaN`` and ``NaT``, returning ``NaT`` instead of raising (:issue:`5437`)
- performance improvements in ``isnull`` on larger size pandas objects
- Fixed various setitem with 1d ndarray that does not have a matching
  length to the indexer (:issue:`5508`)
- Bug in getitem with a MultiIndex and ``iloc`` (:issue:`5528`)
- Bug in delitem on a Series (:issue:`5542`)
- Bug fix in apply when using custom function and objects are not mutated (:issue:`5545`)
- Bug in selecting from a non-unique index with ``loc`` (:issue:`5553`)
- Bug in groupby returning non-consistent types when user function returns a ``None``, (:issue:`5592`)
- Work around regression in numpy 1.7.0 which erroneously raises IndexError from ``ndarray.item`` (:issue:`5666`)
- Bug in repeated indexing of object with resultant non-unique index (:issue:`5678`)
- Bug in fillna with Series and a passed series/dict (:issue:`5703`)
- Bug in groupby transform with a datetime-like grouper (:issue:`5712`)
- Bug in MultiIndex selection in PY3 when using certain keys (:issue:`5725`)
- Row-wise concat of differing dtypes failing in certain cases (:issue:`5754`)

.. _whatsnew_0.13.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.12.0..v0.13.0
.. _whatsnew_120:

What's new in 1.2.0 (December 26, 2020)
---------------------------------------

These are the changes in pandas 1.2.0. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. warning::

   The `xlwt <https://xlwt.readthedocs.io/en/latest/>`_ package for writing old-style ``.xls``
   excel files is no longer maintained.
   The `xlrd <https://xlrd.readthedocs.io/en/latest/>`_ package is now only for reading
   old-style ``.xls`` files.

   Previously, the default argument ``engine=None`` to :func:`~pandas.read_excel`
   would result in using the ``xlrd`` engine in many cases, including new
   Excel 2007+ (``.xlsx``) files.
   If `openpyxl <https://openpyxl.readthedocs.io/en/stable/>`_  is installed,
   many of these cases will now default to using the ``openpyxl`` engine.
   See the :func:`read_excel` documentation for more details.

   Thus, it is strongly encouraged to install ``openpyxl`` to read Excel 2007+
   (``.xlsx``) files.
   **Please do not report issues when using ``xlrd`` to read ``.xlsx`` files.**
   This is no longer supported, switch to using ``openpyxl`` instead.

   Attempting to use the ``xlwt`` engine will raise a ``FutureWarning``
   unless the option :attr:`io.excel.xls.writer` is set to ``"xlwt"``.
   While this option is now deprecated and will also raise a ``FutureWarning``,
   it can be globally set and the warning suppressed. Users are recommended to
   write ``.xlsx`` files using the ``openpyxl`` engine instead.

.. ---------------------------------------------------------------------------

Enhancements
~~~~~~~~~~~~

.. _whatsnew_120.duplicate_labels:

Optionally disallow duplicate labels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Series` and :class:`DataFrame` can now be created with ``allows_duplicate_labels=False`` flag to
control whether the index or columns can contain duplicate labels (:issue:`28394`). This can be used to
prevent accidental introduction of duplicate labels, which can affect downstream operations.

By default, duplicates continue to be allowed.

.. code-block:: ipython

    In [1]: pd.Series([1, 2], index=['a', 'a'])
    Out[1]:
    a    1
    a    2
    Length: 2, dtype: int64

    In [2]: pd.Series([1, 2], index=['a', 'a']).set_flags(allows_duplicate_labels=False)
    ...
    DuplicateLabelError: Index has duplicates.
          positions
    label
    a        [0, 1]

pandas will propagate the ``allows_duplicate_labels`` property through many operations.

.. code-block:: ipython

    In [3]: a = (
       ...:     pd.Series([1, 2], index=['a', 'b'])
       ...:       .set_flags(allows_duplicate_labels=False)
       ...: )

    In [4]: a
    Out[4]:
    a    1
    b    2
    Length: 2, dtype: int64

    # An operation introducing duplicates
    In [5]: a.reindex(['a', 'b', 'a'])
    ...
    DuplicateLabelError: Index has duplicates.
          positions
    label
    a        [0, 2]

    [1 rows x 1 columns]

.. warning::

   This is an experimental feature. Currently, many methods fail to
   propagate the ``allows_duplicate_labels`` value. In future versions
   it is expected that every method taking or returning one or more
   DataFrame or Series objects will propagate ``allows_duplicate_labels``.

See :ref:`duplicates` for more.

The ``allows_duplicate_labels`` flag is stored in the new :attr:`DataFrame.flags`
attribute. This stores global attributes that apply to the *pandas object*. This
differs from :attr:`DataFrame.attrs`, which stores information that applies to
the dataset.

Passing arguments to fsspec backends
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Many read/write functions have acquired the ``storage_options`` optional argument,
to pass a dictionary of parameters to the storage backend. This allows, for
example, for passing credentials to S3 and GCS storage. The details of what
parameters can be passed to which backends can be found in the documentation
of the individual storage backends (detailed from the fsspec docs for
`builtin implementations`_ and linked to `external ones`_). See
Section :ref:`io.remote`.

:issue:`35655` added fsspec support (including ``storage_options``)
for reading excel files.

.. _builtin implementations: https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations
.. _external ones: https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations

.. _whatsnew_120.binary_handle_to_csv:

Support for binary file handles in ``to_csv``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`to_csv` supports file handles in binary mode (:issue:`19827` and :issue:`35058`)
with ``encoding`` (:issue:`13068` and :issue:`23854`) and ``compression`` (:issue:`22555`).
If pandas does not automatically detect whether the file handle is opened in binary or text mode,
it is necessary to provide ``mode="wb"``.

For example:

.. ipython:: python

   import io

   data = pd.DataFrame([0, 1, 2])
   buffer = io.BytesIO()
   data.to_csv(buffer, encoding="utf-8", compression="gzip")

Support for short caption and table position in ``to_latex``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.to_latex` now allows one to specify
a floating table position (:issue:`35281`)
and a short caption (:issue:`36267`).

The keyword ``position`` has been added to set the position.

.. ipython:: python
   :okwarning:

   data = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
   table = data.to_latex(position='ht')
   print(table)

Usage of the keyword ``caption`` has been extended.
Besides taking a single string as an argument,
one can optionally provide a tuple ``(full_caption, short_caption)``
to add a short caption macro.

.. ipython:: python
   :okwarning:

   data = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
   table = data.to_latex(caption=('the full long caption', 'short caption'))
   print(table)

.. _whatsnew_120.read_csv_table_precision_default:

Change in default floating precision for ``read_csv`` and ``read_table``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For the C parsing engine, the methods :meth:`read_csv` and :meth:`read_table` previously defaulted to a parser that
could read floating point numbers slightly incorrectly with respect to the last bit in precision.
The option ``floating_precision="high"`` has always been available to avoid this issue.
Beginning with this version, the default is now to use the more accurate parser by making
``floating_precision=None`` correspond to the high precision parser, and the new option
``floating_precision="legacy"`` to use the legacy parser. The change to using the higher precision
parser by default should have no impact on performance. (:issue:`17154`)

.. _whatsnew_120.floating:

Experimental nullable data types for float data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We've added :class:`Float32Dtype` / :class:`Float64Dtype` and :class:`~arrays.FloatingArray`.
These are extension data types dedicated to floating point data that can hold the
``pd.NA`` missing value indicator (:issue:`32265`, :issue:`34307`).

While the default float data type already supports missing values using ``np.nan``,
these new data types use ``pd.NA`` (and its corresponding behavior) as the missing
value indicator, in line with the already existing nullable :ref:`integer <integer_na>`
and :ref:`boolean <boolean>` data types.

One example where the behavior of ``np.nan`` and ``pd.NA`` is different is
comparison operations:

.. ipython:: python

  # the default NumPy float64 dtype
  s1 = pd.Series([1.5, None])
  s1
  s1 > 1

.. ipython:: python

  # the new nullable float64 dtype
  s2 = pd.Series([1.5, None], dtype="Float64")
  s2
  s2 > 1

See the :ref:`missing_data.NA` doc section for more details on the behavior
when using the ``pd.NA`` missing value indicator.

As shown above, the dtype can be specified using the "Float64" or "Float32"
string (capitalized to distinguish it from the default "float64" data type).
Alternatively, you can also use the dtype object:

.. ipython:: python

   pd.Series([1.5, None], dtype=pd.Float32Dtype())

Operations with the existing integer or boolean nullable data types that
give float results will now also use the nullable floating data types (:issue:`38178`).

.. warning::

   Experimental: the new floating data types are currently experimental, and their
   behavior or API may still change without warning. Especially the behavior
   regarding NaN (distinct from NA missing values) is subject to change.

.. _whatsnew_120.index_name_preservation:

Index/column name preservation when aggregating
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When aggregating using :meth:`concat` or the :class:`DataFrame` constructor, pandas
will now attempt to preserve index and column names whenever possible (:issue:`35847`).
In the case where all inputs share a common name, this name will be assigned to the
result. When the input names do not all agree, the result will be unnamed. Here is an
example where the index name is preserved:

.. ipython:: python

    idx = pd.Index(range(5), name='abc')
    ser = pd.Series(range(5, 10), index=idx)
    pd.concat({'x': ser[1:], 'y': ser[:-1]}, axis=1)

The same is true for :class:`MultiIndex`, but the logic is applied separately on a
level-by-level basis.

.. _whatsnew_120.groupby_ewm:

GroupBy supports EWM operations directly
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`.DataFrameGroupBy` now supports exponentially weighted window operations directly (:issue:`16037`).

.. ipython:: python

    df = pd.DataFrame({'A': ['a', 'b', 'a', 'b'], 'B': range(4)})
    df
    df.groupby('A').ewm(com=1.0).mean()

Additionally ``mean`` supports execution via `Numba <https://numba.pydata.org/>`__ with
the  ``engine`` and ``engine_kwargs`` arguments. Numba must be installed as an optional dependency
to use this feature.

.. _whatsnew_120.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^
- Added ``day_of_week`` (compatibility alias ``dayofweek``) property to :class:`Timestamp`, :class:`.DatetimeIndex`, :class:`Period`, :class:`PeriodIndex` (:issue:`9605`)
- Added ``day_of_year`` (compatibility alias ``dayofyear``) property to :class:`Timestamp`, :class:`.DatetimeIndex`, :class:`Period`, :class:`PeriodIndex` (:issue:`9605`)
- Added :meth:`~DataFrame.set_flags` for setting table-wide flags on a Series or DataFrame (:issue:`28394`)
- :meth:`DataFrame.applymap` now supports ``na_action`` (:issue:`23803`)
- :class:`Index` with object dtype supports division and multiplication (:issue:`34160`)
- :meth:`io.sql.get_schema` now supports a ``schema`` keyword argument that will add a schema into the create table statement (:issue:`28486`)
- :meth:`DataFrame.explode` and :meth:`Series.explode` now support exploding of sets (:issue:`35614`)
- :meth:`DataFrame.hist` now supports time series (datetime) data (:issue:`32590`)
- :meth:`.Styler.set_table_styles` now allows the direct styling of rows and columns and can be chained (:issue:`35607`)
- :class:`.Styler` now allows direct CSS class name addition to individual data cells (:issue:`36159`)
- :meth:`.Rolling.mean` and :meth:`.Rolling.sum` use Kahan summation to calculate the mean to avoid numerical problems (:issue:`10319`, :issue:`11645`, :issue:`13254`, :issue:`32761`, :issue:`36031`)
- :meth:`.DatetimeIndex.searchsorted`, :meth:`.TimedeltaIndex.searchsorted`, :meth:`PeriodIndex.searchsorted`, and :meth:`Series.searchsorted` with datetime-like dtypes will now try to cast string arguments (list-like and scalar) to the matching datetime-like type (:issue:`36346`)
- Added methods :meth:`IntegerArray.prod`, :meth:`IntegerArray.min`, and :meth:`IntegerArray.max` (:issue:`33790`)
- Calling a NumPy ufunc on a ``DataFrame`` with extension types now preserves the extension types when possible (:issue:`23743`)
- Calling a binary-input NumPy ufunc on multiple ``DataFrame`` objects now aligns, matching the behavior of binary operations and ufuncs on ``Series`` (:issue:`23743`).
  This change has been reverted in pandas 1.2.1, and the behaviour to not align DataFrames
  is deprecated instead, see the :ref:`the 1.2.1 release notes <whatsnew_121.ufunc_deprecation>`.
- Where possible :meth:`RangeIndex.difference` and :meth:`RangeIndex.symmetric_difference` will return :class:`RangeIndex` instead of :class:`Int64Index` (:issue:`36564`)
- :meth:`DataFrame.to_parquet` now supports :class:`MultiIndex` for columns in parquet format (:issue:`34777`)
- :func:`read_parquet` gained a ``use_nullable_dtypes=True`` option to use nullable dtypes that use ``pd.NA`` as missing value indicator where possible for the resulting DataFrame (default is ``False``, and only applicable for ``engine="pyarrow"``) (:issue:`31242`)
- Added :meth:`.Rolling.sem` and :meth:`Expanding.sem` to compute the standard error of the mean (:issue:`26476`)
- :meth:`.Rolling.var` and :meth:`.Rolling.std` use Kahan summation and Welford's Method to avoid numerical issues (:issue:`37051`)
- :meth:`DataFrame.corr` and :meth:`DataFrame.cov` use Welford's Method to avoid numerical issues (:issue:`37448`)
- :meth:`DataFrame.plot` now recognizes ``xlabel`` and ``ylabel`` arguments for plots of type ``scatter`` and ``hexbin`` (:issue:`37001`)
- :class:`DataFrame` now supports the ``divmod`` operation (:issue:`37165`)
- :meth:`DataFrame.to_parquet` now returns a ``bytes`` object when no ``path`` argument is passed (:issue:`37105`)
- :class:`.Rolling` now supports the ``closed`` argument for fixed windows (:issue:`34315`)
- :class:`.DatetimeIndex` and :class:`Series` with ``datetime64`` or ``datetime64tz`` dtypes now support ``std`` (:issue:`37436`)
- :class:`Window` now supports all Scipy window types in ``win_type`` with flexible keyword argument support (:issue:`34556`)
- :meth:`testing.assert_index_equal` now has a ``check_order`` parameter that allows indexes to be checked in an order-insensitive manner (:issue:`37478`)
- :func:`read_csv` supports memory-mapping for compressed files (:issue:`37621`)
- Add support for ``min_count`` keyword for :meth:`DataFrame.groupby` and :meth:`DataFrame.resample` for functions ``min``, ``max``, ``first`` and ``last`` (:issue:`37821`, :issue:`37768`)
- Improve error reporting for :meth:`DataFrame.merge` when invalid merge column definitions were given (:issue:`16228`)
- Improve numerical stability for :meth:`.Rolling.skew`, :meth:`.Rolling.kurt`, :meth:`Expanding.skew` and :meth:`Expanding.kurt` through implementation of Kahan summation (:issue:`6929`)
- Improved error reporting for subsetting columns of a :class:`.DataFrameGroupBy` with ``axis=1`` (:issue:`37725`)
- Implement method ``cross`` for :meth:`DataFrame.merge` and :meth:`DataFrame.join` (:issue:`5401`)
- When :func:`read_csv`, :func:`read_sas` and :func:`read_json` are called with ``chunksize``/``iterator`` they can be used in a ``with`` statement as they return context-managers (:issue:`38225`)
- Augmented the list of named colors available for styling Excel exports, enabling all of CSS4 colors (:issue:`38247`)

.. ---------------------------------------------------------------------------

.. _whatsnew_120.notable_bug_fixes:

Notable bug fixes
~~~~~~~~~~~~~~~~~

These are bug fixes that might have notable behavior changes.

Consistency of DataFrame Reductions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
:meth:`DataFrame.any` and :meth:`DataFrame.all` with ``bool_only=True`` now
determines whether to exclude object-dtype columns on a column-by-column basis,
instead of checking if *all* object-dtype columns can be considered boolean.

This prevents pathological behavior where applying the reduction on a subset
of columns could result in a larger Series result. See (:issue:`37799`).

.. ipython:: python

    df = pd.DataFrame({"A": ["foo", "bar"], "B": [True, False]}, dtype=object)
    df["C"] = pd.Series([True, True])


*Previous behavior*:

.. code-block:: ipython

    In [5]: df.all(bool_only=True)
    Out[5]:
    C    True
    dtype: bool

    In [6]: df[["B", "C"]].all(bool_only=True)
    Out[6]:
    B    False
    C    True
    dtype: bool

*New behavior*:

.. ipython:: python

    In [5]: df.all(bool_only=True)

    In [6]: df[["B", "C"]].all(bool_only=True)


Other DataFrame reductions with ``numeric_only=None`` will also avoid
this pathological behavior (:issue:`37827`):

.. ipython:: python

    df = pd.DataFrame({"A": [0, 1, 2], "B": ["a", "b", "c"]}, dtype=object)


*Previous behavior*:

.. code-block:: ipython

    In [3]: df.mean()
    Out[3]: Series([], dtype: float64)

    In [4]: df[["A"]].mean()
    Out[4]:
    A    1.0
    dtype: float64

*New behavior*:

.. ipython:: python
   :okwarning:

    df.mean()

    df[["A"]].mean()

Moreover, DataFrame reductions with ``numeric_only=None`` will now be
consistent with their Series counterparts.  In particular, for
reductions where the Series method raises ``TypeError``, the
DataFrame reduction will now consider that column non-numeric
instead of casting to a NumPy array which may have different semantics (:issue:`36076`,
:issue:`28949`, :issue:`21020`).

.. ipython:: python
   :okwarning:

    ser = pd.Series([0, 1], dtype="category", name="A")
    df = ser.to_frame()


*Previous behavior*:

.. code-block:: ipython

    In [5]: df.any()
    Out[5]:
    A    True
    dtype: bool

*New behavior*:

.. ipython:: python
   :okwarning:

    df.any()


.. _whatsnew_120.api_breaking.python:

Increased minimum version for Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.2.0 supports Python 3.7.1 and higher (:issue:`35214`).

.. _whatsnew_120.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some minimum supported versions of dependencies were updated (:issue:`35214`).
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| numpy           | 1.16.5          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| pytz            | 2017.3          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| python-dateutil | 2.7.3           |    X     |         |
+-----------------+-----------------+----------+---------+
| bottleneck      | 1.2.1           |          |         |
+-----------------+-----------------+----------+---------+
| numexpr         | 2.6.8           |          |    X    |
+-----------------+-----------------+----------+---------+
| pytest (dev)    | 5.0.1           |          |    X    |
+-----------------+-----------------+----------+---------+
| mypy (dev)      | 0.782           |          |    X    |
+-----------------+-----------------+----------+---------+

For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
| beautifulsoup4  | 4.6.0           |         |
+-----------------+-----------------+---------+
| fastparquet     | 0.3.2           |         |
+-----------------+-----------------+---------+
| fsspec          | 0.7.4           |         |
+-----------------+-----------------+---------+
| gcsfs           | 0.6.0           |         |
+-----------------+-----------------+---------+
| lxml            | 4.3.0           |    X    |
+-----------------+-----------------+---------+
| matplotlib      | 2.2.3           |    X    |
+-----------------+-----------------+---------+
| numba           | 0.46.0          |         |
+-----------------+-----------------+---------+
| openpyxl        | 2.6.0           |    X    |
+-----------------+-----------------+---------+
| pyarrow         | 0.15.0          |    X    |
+-----------------+-----------------+---------+
| pymysql         | 0.7.11          |    X    |
+-----------------+-----------------+---------+
| pytables        | 3.5.1           |    X    |
+-----------------+-----------------+---------+
| s3fs            | 0.4.0           |         |
+-----------------+-----------------+---------+
| scipy           | 1.2.0           |         |
+-----------------+-----------------+---------+
| sqlalchemy      | 1.2.8           |    X    |
+-----------------+-----------------+---------+
| xarray          | 0.12.3          |    X    |
+-----------------+-----------------+---------+
| xlrd            | 1.2.0           |    X    |
+-----------------+-----------------+---------+
| xlsxwriter      | 1.0.2           |    X    |
+-----------------+-----------------+---------+
| xlwt            | 1.3.0           |    X    |
+-----------------+-----------------+---------+
| pandas-gbq      | 0.12.0          |         |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

.. _whatsnew_120.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- Sorting in descending order is now stable for :meth:`Series.sort_values` and :meth:`Index.sort_values` for Datetime-like :class:`Index` subclasses. This will affect sort order when sorting a DataFrame on multiple columns, sorting with a key function that produces duplicates, or requesting the sorting index when using :meth:`Index.sort_values`. When using :meth:`Series.value_counts`, the count of missing values is no longer necessarily last in the list of duplicate counts. Instead, its position corresponds to the position in the original Series. When using :meth:`Index.sort_values` for Datetime-like :class:`Index` subclasses, NaTs ignored the ``na_position`` argument and were sorted to the beginning. Now they respect ``na_position``, the default being ``last``, same as other :class:`Index` subclasses (:issue:`35992`)
- Passing an invalid ``fill_value`` to :meth:`Categorical.take`, :meth:`.DatetimeArray.take`, :meth:`TimedeltaArray.take`, or :meth:`PeriodArray.take` now raises a ``TypeError`` instead of a ``ValueError`` (:issue:`37733`)
- Passing an invalid ``fill_value`` to :meth:`Series.shift` with a ``CategoricalDtype`` now raises a ``TypeError`` instead of a ``ValueError`` (:issue:`37733`)
- Passing an invalid value to :meth:`IntervalIndex.insert` or :meth:`CategoricalIndex.insert` now raises a ``TypeError`` instead of a ``ValueError`` (:issue:`37733`)
- Attempting to reindex a Series with a :class:`CategoricalIndex` with an invalid ``fill_value`` now raises a ``TypeError`` instead of a ``ValueError`` (:issue:`37733`)
- :meth:`CategoricalIndex.append` with an index that contains non-category values will now cast instead of raising ``TypeError`` (:issue:`38098`)

.. ---------------------------------------------------------------------------

.. _whatsnew_120.deprecations:

Deprecations
~~~~~~~~~~~~
- Deprecated parameter ``inplace`` in :meth:`MultiIndex.set_codes` and :meth:`MultiIndex.set_levels` (:issue:`35626`)
- Deprecated parameter ``dtype`` of method :meth:`~Index.copy` for all :class:`Index` subclasses. Use the :meth:`~Index.astype` method instead for changing dtype (:issue:`35853`)
- Deprecated parameters ``levels`` and ``codes`` in :meth:`MultiIndex.copy`. Use the :meth:`~MultiIndex.set_levels` and :meth:`~MultiIndex.set_codes` methods instead (:issue:`36685`)
- Date parser functions :func:`~pandas.io.date_converters.parse_date_time`, :func:`~pandas.io.date_converters.parse_date_fields`, :func:`~pandas.io.date_converters.parse_all_fields` and :func:`~pandas.io.date_converters.generic_parser` from ``pandas.io.date_converters`` are deprecated and will be removed in a future version; use :func:`to_datetime` instead (:issue:`35741`)
- :meth:`DataFrame.lookup` is deprecated and will be removed in a future version, use :meth:`DataFrame.melt` and :meth:`DataFrame.loc` instead (:issue:`35224`)
- The method :meth:`Index.to_native_types` is deprecated. Use ``.astype(str)`` instead (:issue:`28867`)
- Deprecated indexing :class:`DataFrame` rows with a single datetime-like string as ``df[string]`` (given the ambiguity whether it is indexing the rows or selecting a column), use ``df.loc[string]`` instead (:issue:`36179`)
- Deprecated :meth:`Index.is_all_dates` (:issue:`27744`)
- The default value of ``regex`` for :meth:`Series.str.replace` will change from ``True`` to ``False`` in a future release. In addition, single character regular expressions will *not* be treated as literal strings when ``regex=True`` is set (:issue:`24804`)
- Deprecated automatic alignment on comparison operations between :class:`DataFrame` and :class:`Series`, do ``frame, ser = frame.align(ser, axis=1, copy=False)`` before e.g. ``frame == ser`` (:issue:`28759`)
- :meth:`Rolling.count` with ``min_periods=None`` will default to the size of the window in a future version (:issue:`31302`)
- Using "outer" ufuncs on DataFrames to return 4d ndarray is now deprecated. Convert to an ndarray first (:issue:`23743`)
- Deprecated slice-indexing on tz-aware :class:`DatetimeIndex` with naive ``datetime`` objects, to match scalar indexing behavior (:issue:`36148`)
- :meth:`Index.ravel` returning a ``np.ndarray`` is deprecated, in the future this will return a view on the same index (:issue:`19956`)
- Deprecate use of strings denoting units with 'M', 'Y' or 'y' in :func:`~pandas.to_timedelta` (:issue:`36666`)
- :class:`Index` methods ``&``, ``|``, and ``^`` behaving as the set operations :meth:`Index.intersection`, :meth:`Index.union`, and :meth:`Index.symmetric_difference`, respectively, are deprecated and in the future will behave as pointwise boolean operations matching :class:`Series` behavior.  Use the named set methods instead (:issue:`36758`)
- :meth:`Categorical.is_dtype_equal` and :meth:`CategoricalIndex.is_dtype_equal` are deprecated, will be removed in a future version (:issue:`37545`)
- :meth:`Series.slice_shift` and :meth:`DataFrame.slice_shift` are deprecated, use :meth:`Series.shift` or :meth:`DataFrame.shift` instead (:issue:`37601`)
- Partial slicing on unordered :class:`.DatetimeIndex` objects with keys that are not in the index is deprecated and will be removed in a future version (:issue:`18531`)
- The ``how`` keyword in :meth:`PeriodIndex.astype` is deprecated and will be removed in a future version, use ``index.to_timestamp(how=how)`` instead (:issue:`37982`)
- Deprecated :meth:`Index.asi8` for :class:`Index` subclasses other than :class:`.DatetimeIndex`, :class:`.TimedeltaIndex`, and :class:`PeriodIndex` (:issue:`37877`)
- The ``inplace`` parameter of :meth:`Categorical.remove_unused_categories` is deprecated and will be removed in a future version (:issue:`37643`)
- The ``null_counts`` parameter of :meth:`DataFrame.info` is deprecated and replaced by ``show_counts``. It will be removed in a future version (:issue:`37999`)

**Calling NumPy ufuncs on non-aligned DataFrames**

Calling NumPy ufuncs on non-aligned DataFrames changed behaviour in pandas
1.2.0 (to align the inputs before calling the ufunc), but this change is
reverted in pandas 1.2.1. The behaviour to not align is now deprecated instead,
see the :ref:`the 1.2.1 release notes <whatsnew_121.ufunc_deprecation>` for
more details.

.. ---------------------------------------------------------------------------


.. _whatsnew_120.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Performance improvements when creating DataFrame or Series with dtype ``str`` or :class:`StringDtype` from array with many string elements (:issue:`36304`, :issue:`36317`, :issue:`36325`, :issue:`36432`, :issue:`37371`)
- Performance improvement in :meth:`.GroupBy.agg` with the ``numba`` engine (:issue:`35759`)
- Performance improvements when creating :meth:`Series.map` from a huge dictionary (:issue:`34717`)
- Performance improvement in :meth:`.GroupBy.transform` with the ``numba`` engine (:issue:`36240`)
- :class:`.Styler` uuid method altered to compress data transmission over web whilst maintaining reasonably low table collision probability (:issue:`36345`)
- Performance improvement in :func:`to_datetime` with non-ns time unit for ``float`` ``dtype`` columns (:issue:`20445`)
- Performance improvement in setting values on an :class:`IntervalArray` (:issue:`36310`)
- The internal index method :meth:`~Index._shallow_copy` now makes the new index and original index share cached attributes, avoiding creating these again, if created on either. This can speed up operations that depend on creating copies of existing indexes (:issue:`36840`)
- Performance improvement in :meth:`.RollingGroupby.count` (:issue:`35625`)
- Small performance decrease to :meth:`.Rolling.min` and :meth:`.Rolling.max` for fixed windows (:issue:`36567`)
- Reduced peak memory usage in :meth:`DataFrame.to_pickle` when using ``protocol=5`` in python 3.8+ (:issue:`34244`)
- Faster ``dir`` calls when the object has many index labels, e.g. ``dir(ser)`` (:issue:`37450`)
- Performance improvement in :class:`ExpandingGroupby` (:issue:`37064`)
- Performance improvement in :meth:`Series.astype` and :meth:`DataFrame.astype` for :class:`Categorical` (:issue:`8628`)
- Performance improvement in :meth:`DataFrame.groupby` for ``float`` ``dtype`` (:issue:`28303`), changes of the underlying hash-function can lead to changes in float based indexes sort ordering for ties (e.g. :meth:`Index.value_counts`)
- Performance improvement in :meth:`pd.isin` for inputs with more than 1e6 elements (:issue:`36611`)
- Performance improvement for :meth:`DataFrame.__setitem__` with list-like indexers (:issue:`37954`)
- :meth:`read_json` now avoids reading entire file into memory when chunksize is specified (:issue:`34548`)

.. ---------------------------------------------------------------------------

.. _whatsnew_120.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^
- :meth:`Categorical.fillna` will always return a copy, validate a passed fill value regardless of whether there are any NAs to fill, and disallow an ``NaT`` as a fill value for numeric categories (:issue:`36530`)
- Bug in :meth:`Categorical.__setitem__` that incorrectly raised when trying to set a tuple value (:issue:`20439`)
- Bug in :meth:`CategoricalIndex.equals` incorrectly casting non-category entries to ``np.nan`` (:issue:`37667`)
- Bug in :meth:`CategoricalIndex.where` incorrectly setting non-category entries to ``np.nan`` instead of raising ``TypeError`` (:issue:`37977`)
- Bug in :meth:`Categorical.to_numpy` and ``np.array(categorical)`` with tz-aware ``datetime64`` categories incorrectly dropping the time zone information instead of casting to object dtype (:issue:`38136`)

Datetime-like
^^^^^^^^^^^^^
- Bug in :meth:`DataFrame.combine_first` that would convert datetime-like column on other :class:`DataFrame` to integer when the column is not present in original :class:`DataFrame` (:issue:`28481`)
- Bug in :attr:`.DatetimeArray.date` where a ``ValueError`` would be raised with a read-only backing array (:issue:`33530`)
- Bug in ``NaT`` comparisons failing to raise ``TypeError`` on invalid inequality comparisons (:issue:`35046`)
- Bug in :class:`.DateOffset` where attributes reconstructed from pickle files differ from original objects when input values exceed normal ranges (e.g. months=12) (:issue:`34511`)
- Bug in :meth:`.DatetimeIndex.get_slice_bound` where ``datetime.date`` objects were not accepted or naive :class:`Timestamp` with a tz-aware :class:`.DatetimeIndex` (:issue:`35690`)
- Bug in :meth:`.DatetimeIndex.slice_locs` where ``datetime.date`` objects were not accepted (:issue:`34077`)
- Bug in :meth:`.DatetimeIndex.searchsorted`, :meth:`.TimedeltaIndex.searchsorted`, :meth:`PeriodIndex.searchsorted`, and :meth:`Series.searchsorted` with ``datetime64``, ``timedelta64`` or :class:`Period` dtype placement of ``NaT`` values being inconsistent with NumPy (:issue:`36176`, :issue:`36254`)
- Inconsistency in :class:`.DatetimeArray`, :class:`.TimedeltaArray`, and :class:`.PeriodArray` method ``__setitem__`` casting arrays of strings to datetime-like scalars but not scalar strings (:issue:`36261`)
- Bug in :meth:`.DatetimeArray.take` incorrectly allowing ``fill_value`` with a mismatched time zone (:issue:`37356`)
- Bug in :class:`.DatetimeIndex.shift` incorrectly raising when shifting empty indexes (:issue:`14811`)
- :class:`Timestamp` and :class:`.DatetimeIndex` comparisons between tz-aware and tz-naive objects now follow the standard library ``datetime`` behavior, returning ``True``/``False`` for ``!=``/``==`` and raising for inequality comparisons (:issue:`28507`)
- Bug in :meth:`.DatetimeIndex.equals` and :meth:`.TimedeltaIndex.equals` incorrectly considering ``int64`` indexes as equal (:issue:`36744`)
- :meth:`Series.to_json`, :meth:`DataFrame.to_json`, and :meth:`read_json` now implement time zone parsing when orient structure is ``table`` (:issue:`35973`)
- :meth:`astype` now attempts to convert to ``datetime64[ns, tz]`` directly from ``object`` with inferred time zone from string (:issue:`35973`)
- Bug in :meth:`.TimedeltaIndex.sum` and :meth:`Series.sum` with ``timedelta64`` dtype on an empty index or series returning ``NaT`` instead of ``Timedelta(0)`` (:issue:`31751`)
- Bug in :meth:`.DatetimeArray.shift` incorrectly allowing ``fill_value`` with a mismatched time zone (:issue:`37299`)
- Bug in adding a :class:`.BusinessDay` with nonzero ``offset`` to a non-scalar other (:issue:`37457`)
- Bug in :func:`to_datetime` with a read-only array incorrectly raising (:issue:`34857`)
- Bug in :meth:`Series.isin` with ``datetime64[ns]`` dtype and :meth:`.DatetimeIndex.isin` incorrectly casting integers to datetimes (:issue:`36621`)
- Bug in :meth:`Series.isin` with ``datetime64[ns]`` dtype and :meth:`.DatetimeIndex.isin` failing to consider tz-aware and tz-naive datetimes as always different (:issue:`35728`)
- Bug in :meth:`Series.isin` with ``PeriodDtype`` dtype and :meth:`PeriodIndex.isin` failing to consider arguments with different ``PeriodDtype`` as always different (:issue:`37528`)
- Bug in :class:`Period` constructor now correctly handles nanoseconds in the ``value`` argument (:issue:`34621` and :issue:`17053`)

Timedelta
^^^^^^^^^
- Bug in :class:`.TimedeltaIndex`, :class:`Series`, and :class:`DataFrame` floor-division with ``timedelta64`` dtypes and ``NaT`` in the denominator (:issue:`35529`)
- Bug in parsing of ISO 8601 durations in :class:`Timedelta` and :func:`to_datetime` (:issue:`29773`, :issue:`36204`)
- Bug in :func:`to_timedelta` with a read-only array incorrectly raising (:issue:`34857`)
- Bug in :class:`Timedelta` incorrectly truncating to sub-second portion of a string input when it has precision higher than nanoseconds (:issue:`36738`)

Timezones
^^^^^^^^^

- Bug in :func:`date_range` was raising ``AmbiguousTimeError`` for valid input with ``ambiguous=False`` (:issue:`35297`)
- Bug in :meth:`Timestamp.replace` was losing fold information (:issue:`37610`)


Numeric
^^^^^^^
- Bug in :func:`to_numeric` where float precision was incorrect (:issue:`31364`)
- Bug in :meth:`DataFrame.any` with ``axis=1`` and ``bool_only=True`` ignoring the ``bool_only`` keyword (:issue:`32432`)
- Bug in :meth:`Series.equals` where a ``ValueError`` was raised when NumPy arrays were compared to scalars (:issue:`35267`)
- Bug in :class:`Series` where two Series each have a :class:`.DatetimeIndex` with different time zones having those indexes incorrectly changed when performing arithmetic operations (:issue:`33671`)
- Bug in :mod:`pandas.testing` module functions when used with ``check_exact=False`` on complex numeric types (:issue:`28235`)
- Bug in :meth:`DataFrame.__rmatmul__` error handling reporting transposed shapes (:issue:`21581`)
- Bug in :class:`Series` flex arithmetic methods where the result when operating with a ``list``, ``tuple`` or ``np.ndarray`` would have an incorrect name (:issue:`36760`)
- Bug in :class:`.IntegerArray` multiplication with ``timedelta`` and ``np.timedelta64`` objects (:issue:`36870`)
- Bug in :class:`MultiIndex` comparison with tuple incorrectly treating tuple as array-like (:issue:`21517`)
- Bug in :meth:`DataFrame.diff` with ``datetime64`` dtypes including ``NaT`` values failing to fill ``NaT`` results correctly (:issue:`32441`)
- Bug in :class:`DataFrame` arithmetic ops incorrectly accepting keyword arguments (:issue:`36843`)
- Bug in :class:`.IntervalArray` comparisons with :class:`Series` not returning Series (:issue:`36908`)
- Bug in :class:`DataFrame` allowing arithmetic operations with list of array-likes with undefined results. Behavior changed to raising ``ValueError`` (:issue:`36702`)
- Bug in :meth:`DataFrame.std` with ``timedelta64`` dtype and ``skipna=False`` (:issue:`37392`)
- Bug in :meth:`DataFrame.min` and :meth:`DataFrame.max` with ``datetime64`` dtype and ``skipna=False`` (:issue:`36907`)
- Bug in :meth:`DataFrame.idxmax` and :meth:`DataFrame.idxmin` with mixed dtypes incorrectly raising ``TypeError`` (:issue:`38195`)

Conversion
^^^^^^^^^^

- Bug in :meth:`DataFrame.to_dict` with ``orient='records'`` now returns python native datetime objects for datetime-like columns (:issue:`21256`)
- Bug in :meth:`Series.astype` conversion from ``string`` to ``float`` raised in presence of ``pd.NA`` values (:issue:`37626`)

Strings
^^^^^^^
- Bug in :meth:`Series.to_string`, :meth:`DataFrame.to_string`, and :meth:`DataFrame.to_latex` adding a leading space when ``index=False`` (:issue:`24980`)
- Bug in :func:`to_numeric` raising a ``TypeError`` when attempting to convert a string dtype Series containing only numeric strings and ``NA`` (:issue:`37262`)

Interval
^^^^^^^^

- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` where :class:`Interval` dtypes would be converted to object dtypes (:issue:`34871`)
- Bug in :meth:`IntervalIndex.take` with negative indices and ``fill_value=None`` (:issue:`37330`)
- Bug in :meth:`IntervalIndex.putmask` with datetime-like dtype incorrectly casting to object dtype (:issue:`37968`)
- Bug in :meth:`IntervalArray.astype` incorrectly dropping dtype information with a :class:`CategoricalDtype` object (:issue:`37984`)

Indexing
^^^^^^^^

- Bug in :meth:`PeriodIndex.get_loc` incorrectly raising ``ValueError`` on non-datelike strings instead of ``KeyError``, causing similar errors in :meth:`Series.__getitem__`, :meth:`Series.__contains__`, and :meth:`Series.loc.__getitem__` (:issue:`34240`)
- Bug in :meth:`Index.sort_values` where, when empty values were passed, the method would break by trying to compare missing values instead of pushing them to the end of the sort order (:issue:`35584`)
- Bug in :meth:`Index.get_indexer` and :meth:`Index.get_indexer_non_unique` where ``int64`` arrays are returned instead of ``intp`` (:issue:`36359`)
- Bug in :meth:`DataFrame.sort_index` where parameter ascending passed as a list on a single level index gives wrong result (:issue:`32334`)
- Bug in :meth:`DataFrame.reset_index` was incorrectly raising a ``ValueError`` for input with a :class:`MultiIndex` with missing values in a level with ``Categorical`` dtype (:issue:`24206`)
- Bug in indexing with boolean masks on datetime-like values sometimes returning a view instead of a copy (:issue:`36210`)
- Bug in :meth:`DataFrame.__getitem__` and :meth:`DataFrame.loc.__getitem__` with :class:`IntervalIndex` columns and a numeric indexer (:issue:`26490`)
- Bug in :meth:`Series.loc.__getitem__` with a non-unique :class:`MultiIndex` and an empty-list indexer (:issue:`13691`)
- Bug in indexing on a :class:`Series` or :class:`DataFrame` with a :class:`MultiIndex` and a level named ``"0"`` (:issue:`37194`)
- Bug in :meth:`Series.__getitem__` when using an unsigned integer array as an indexer giving incorrect results or segfaulting instead of raising ``KeyError`` (:issue:`37218`)
- Bug in :meth:`Index.where` incorrectly casting numeric values to strings (:issue:`37591`)
- Bug in :meth:`DataFrame.loc` returning empty result when indexer is a slice with negative step size (:issue:`38071`)
- Bug in :meth:`Series.loc` and :meth:`DataFrame.loc` raises when the index was of ``object`` dtype and the given numeric label was in the index (:issue:`26491`)
- Bug in :meth:`DataFrame.loc` returned requested key plus missing values when ``loc`` was applied to single level from a :class:`MultiIndex` (:issue:`27104`)
- Bug in indexing on a :class:`Series` or :class:`DataFrame` with a :class:`CategoricalIndex` using a list-like indexer containing NA values (:issue:`37722`)
- Bug in :meth:`DataFrame.loc.__setitem__` expanding an empty :class:`DataFrame` with mixed dtypes (:issue:`37932`)
- Bug in :meth:`DataFrame.xs` ignored ``droplevel=False`` for columns (:issue:`19056`)
- Bug in :meth:`DataFrame.reindex` raising ``IndexingError`` wrongly for empty DataFrame with ``tolerance`` not ``None`` or ``method="nearest"`` (:issue:`27315`)
- Bug in indexing on a :class:`Series` or :class:`DataFrame` with a :class:`CategoricalIndex` using list-like indexer that contains elements that are in the index's ``categories`` but not in the index itself failing to raise ``KeyError`` (:issue:`37901`)
- Bug on inserting a boolean label into a :class:`DataFrame` with a numeric :class:`Index` columns incorrectly casting to integer (:issue:`36319`)
- Bug in :meth:`DataFrame.iloc` and :meth:`Series.iloc` aligning objects in ``__setitem__`` (:issue:`22046`)
- Bug in :meth:`MultiIndex.drop` does not raise if labels are partially found (:issue:`37820`)
- Bug in :meth:`DataFrame.loc` did not raise ``KeyError`` when missing combination was given with ``slice(None)`` for remaining levels (:issue:`19556`)
- Bug in :meth:`DataFrame.loc` raising ``TypeError`` when non-integer slice was given to select values from :class:`MultiIndex` (:issue:`25165`, :issue:`24263`)
- Bug in :meth:`Series.at` returning :class:`Series` with one element instead of scalar when index is a :class:`MultiIndex` with one level (:issue:`38053`)
- Bug in :meth:`DataFrame.loc` returning and assigning elements in wrong order when indexer is differently ordered than the :class:`MultiIndex` to filter (:issue:`31330`, :issue:`34603`)
- Bug in :meth:`DataFrame.loc` and :meth:`DataFrame.__getitem__`  raising ``KeyError`` when columns were :class:`MultiIndex` with only one level (:issue:`29749`)
- Bug in :meth:`Series.__getitem__` and :meth:`DataFrame.__getitem__` raising blank ``KeyError`` without missing keys for :class:`IntervalIndex` (:issue:`27365`)
- Bug in setting a new label on a :class:`DataFrame` or :class:`Series` with a :class:`CategoricalIndex` incorrectly raising ``TypeError`` when the new label is not among the index's categories (:issue:`38098`)
- Bug in :meth:`Series.loc` and :meth:`Series.iloc` raising ``ValueError`` when inserting a list-like ``np.array``, ``list`` or ``tuple`` in an ``object`` Series of equal length (:issue:`37748`, :issue:`37486`)
- Bug in :meth:`Series.loc` and :meth:`Series.iloc` setting all the values of an ``object`` Series with those of a list-like ``ExtensionArray`` instead of inserting it (:issue:`38271`)

Missing
^^^^^^^

- Bug in :meth:`.SeriesGroupBy.transform` now correctly handles missing values for ``dropna=False`` (:issue:`35014`)
- Bug in :meth:`Series.nunique` with ``dropna=True`` was returning incorrect results when both ``NA`` and ``None`` missing values were present (:issue:`37566`)
- Bug in :meth:`Series.interpolate` where kwarg ``limit_area`` and ``limit_direction`` had no effect when using methods ``pad`` and ``backfill`` (:issue:`31048`)

MultiIndex
^^^^^^^^^^

- Bug in :meth:`DataFrame.xs` when used with :class:`IndexSlice` raises ``TypeError`` with message ``"Expected label or tuple of labels"`` (:issue:`35301`)
- Bug in :meth:`DataFrame.reset_index` with ``NaT`` values in index raises ``ValueError`` with message ``"cannot convert float NaN to integer"`` (:issue:`36541`)
- Bug in :meth:`DataFrame.combine_first` when used with :class:`MultiIndex` containing string and ``NaN`` values raises ``TypeError`` (:issue:`36562`)
- Bug in :meth:`MultiIndex.drop` dropped ``NaN`` values when non existing key was given as input (:issue:`18853`)
- Bug in :meth:`MultiIndex.drop` dropping more values than expected when index has duplicates and is not sorted (:issue:`33494`)

I/O
^^^

- :func:`read_sas` no longer leaks resources on failure (:issue:`35566`)
- Bug in :meth:`DataFrame.to_csv` and :meth:`Series.to_csv` caused a ``ValueError`` when it was called with a filename in combination with ``mode`` containing a ``b`` (:issue:`35058`)
- Bug in :meth:`read_csv` with ``float_precision='round_trip'`` did not handle ``decimal`` and ``thousands`` parameters (:issue:`35365`)
- :meth:`to_pickle` and :meth:`read_pickle` were closing user-provided file objects (:issue:`35679`)
- :meth:`to_csv` passes compression arguments for ``'gzip'`` always to ``gzip.GzipFile`` (:issue:`28103`)
- :meth:`to_csv` did not support zip compression for binary file object not having a filename (:issue:`35058`)
- :meth:`to_csv` and :meth:`read_csv` did not honor ``compression`` and ``encoding`` for path-like objects that are internally converted to file-like objects (:issue:`35677`, :issue:`26124`, :issue:`32392`)
- :meth:`DataFrame.to_pickle`, :meth:`Series.to_pickle`, and :meth:`read_pickle` did not support compression for file-objects (:issue:`26237`, :issue:`29054`, :issue:`29570`)
- Bug in :func:`LongTableBuilder.middle_separator` was duplicating LaTeX longtable entries in the List of Tables of a LaTeX document (:issue:`34360`)
- Bug in :meth:`read_csv` with ``engine='python'`` truncating data if multiple items present in first row and first element started with BOM (:issue:`36343`)
- Removed ``private_key`` and ``verbose`` from :func:`read_gbq` as they are no longer supported in ``pandas-gbq`` (:issue:`34654`, :issue:`30200`)
- Bumped minimum pytables version to 3.5.1 to avoid a ``ValueError`` in :meth:`read_hdf` (:issue:`24839`)
- Bug in :func:`read_table` and :func:`read_csv` when ``delim_whitespace=True`` and ``sep=default`` (:issue:`36583`)
- Bug in :meth:`DataFrame.to_json` and :meth:`Series.to_json` when used with ``lines=True`` and ``orient='records'`` the last line of the record is not appended with 'new line character' (:issue:`36888`)
- Bug in :meth:`read_parquet` with fixed offset time zones. String representation of time zones was not recognized (:issue:`35997`, :issue:`36004`)
- Bug in :meth:`DataFrame.to_html`, :meth:`DataFrame.to_string`, and :meth:`DataFrame.to_latex` ignoring the ``na_rep`` argument when ``float_format`` was also specified (:issue:`9046`, :issue:`13828`)
- Bug in output rendering of complex numbers showing too many trailing zeros (:issue:`36799`)
- Bug in :class:`HDFStore` threw a ``TypeError`` when exporting an empty DataFrame with ``datetime64[ns, tz]`` dtypes with a fixed HDF5 store (:issue:`20594`)
- Bug in :class:`HDFStore` was dropping time zone information when exporting a Series with ``datetime64[ns, tz]`` dtypes with a fixed HDF5 store (:issue:`20594`)
- :func:`read_csv` was closing user-provided binary file handles when ``engine="c"`` and an ``encoding`` was requested (:issue:`36980`)
- Bug in :meth:`DataFrame.to_hdf` was not dropping missing rows with ``dropna=True`` (:issue:`35719`)
- Bug in :func:`read_html` was raising a ``TypeError`` when supplying a ``pathlib.Path`` argument to the ``io`` parameter (:issue:`37705`)
- :meth:`DataFrame.to_excel`, :meth:`Series.to_excel`, :meth:`DataFrame.to_markdown`, and :meth:`Series.to_markdown` now support writing to fsspec URLs such as S3 and Google Cloud Storage (:issue:`33987`)
- Bug in :func:`read_fwf` with ``skip_blank_lines=True`` was not skipping blank lines (:issue:`37758`)
- Parse missing values using :func:`read_json` with ``dtype=False`` to ``NaN`` instead of ``None`` (:issue:`28501`)
- :meth:`read_fwf` was inferring compression with ``compression=None`` which was not consistent with the other ``read_*`` functions (:issue:`37909`)
- :meth:`DataFrame.to_html` was ignoring ``formatters`` argument for ``ExtensionDtype`` columns (:issue:`36525`)
- Bumped minimum xarray version to 0.12.3 to avoid reference to the removed ``Panel`` class (:issue:`27101`, :issue:`37983`)
- :meth:`DataFrame.to_csv` was re-opening file-like handles that also implement ``os.PathLike`` (:issue:`38125`)
- Bug in the conversion of a sliced ``pyarrow.Table`` with missing values to a DataFrame (:issue:`38525`)
- Bug in :func:`read_sql_table` raising a ``sqlalchemy.exc.OperationalError`` when column names contained a percentage sign (:issue:`37517`)

Period
^^^^^^

- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` where :class:`Period` dtypes would be converted to object dtypes (:issue:`34871`)

Plotting
^^^^^^^^

- Bug in :meth:`DataFrame.plot` was rotating xticklabels when ``subplots=True``, even if the x-axis wasn't an irregular time series (:issue:`29460`)
- Bug in :meth:`DataFrame.plot` where a marker letter in the ``style`` keyword sometimes caused a ``ValueError`` (:issue:`21003`)
- Bug in :meth:`DataFrame.plot.bar` and :meth:`Series.plot.bar` where ticks positions were assigned by value order instead of using the actual value for numeric or a smart ordering for string (:issue:`26186`, :issue:`11465`). This fix has been reverted in pandas 1.2.1, see :doc:`v1.2.1`
- Twinned axes were losing their tick labels which should only happen to all but the last row or column of 'externally' shared axes (:issue:`33819`)
- Bug in :meth:`Series.plot` and :meth:`DataFrame.plot` was throwing a :exc:`ValueError` when the Series or DataFrame was
  indexed by a :class:`.TimedeltaIndex` with a fixed frequency and the x-axis lower limit was greater than the upper limit (:issue:`37454`)
- Bug in :meth:`.DataFrameGroupBy.boxplot` when ``subplots=False`` would raise a ``KeyError`` (:issue:`16748`)
- Bug in :meth:`DataFrame.plot` and :meth:`Series.plot` was overwriting matplotlib's shared y axes behavior when no ``sharey`` parameter was passed (:issue:`37942`)
- Bug in :meth:`DataFrame.plot` was raising a ``TypeError`` with ``ExtensionDtype`` columns (:issue:`32073`)

Styler
^^^^^^

- Bug in :meth:`Styler.render` HTML was generated incorrectly because of formatting error in ``rowspan`` attribute, it now matches with w3 syntax (:issue:`38234`)

Groupby/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in :meth:`.DataFrameGroupBy.count` and :meth:`SeriesGroupBy.sum` returning ``NaN`` for missing categories when grouped on multiple ``Categoricals``. Now returning ``0`` (:issue:`35028`)
- Bug in :meth:`.DataFrameGroupBy.apply` that would sometimes throw an erroneous ``ValueError`` if the grouping axis had duplicate entries (:issue:`16646`)
- Bug in :meth:`DataFrame.resample` that would throw a ``ValueError`` when resampling from ``"D"`` to ``"24H"`` over a transition into daylight savings time (DST) (:issue:`35219`)
- Bug when combining methods :meth:`DataFrame.groupby` with :meth:`DataFrame.resample` and :meth:`DataFrame.interpolate` raising a ``TypeError`` (:issue:`35325`)
- Bug in :meth:`.DataFrameGroupBy.apply` where a non-nuisance grouping column would be dropped from the output columns if another groupby method was called before ``.apply`` (:issue:`34656`)
- Bug when subsetting columns on a :class:`~pandas.core.groupby.DataFrameGroupBy` (e.g. ``df.groupby('a')[['b']])``) would reset the attributes ``axis``, ``dropna``, ``group_keys``, ``level``, ``mutated``, ``sort``, and ``squeeze`` to their default values (:issue:`9959`)
- Bug in :meth:`.DataFrameGroupBy.tshift` failing to raise ``ValueError`` when a frequency cannot be inferred for the index of a group (:issue:`35937`)
- Bug in :meth:`DataFrame.groupby` does not always maintain column index name for ``any``, ``all``, ``bfill``, ``ffill``, ``shift`` (:issue:`29764`)
- Bug in :meth:`.DataFrameGroupBy.apply` raising error with ``np.nan`` group(s) when ``dropna=False`` (:issue:`35889`)
- Bug in :meth:`.Rolling.sum` returned wrong values when dtypes where mixed between float and integer and ``axis=1`` (:issue:`20649`, :issue:`35596`)
- Bug in :meth:`.Rolling.count` returned ``np.nan`` with :class:`~pandas.api.indexers.FixedForwardWindowIndexer` as window, ``min_periods=0`` and only missing values in the window (:issue:`35579`)
- Bug where :class:`pandas.core.window.Rolling` produces incorrect window sizes when using a ``PeriodIndex`` (:issue:`34225`)
- Bug in :meth:`.DataFrameGroupBy.ffill` and :meth:`.DataFrameGroupBy.bfill` where a ``NaN`` group would return filled values instead of ``NaN`` when ``dropna=True`` (:issue:`34725`)
- Bug in :meth:`.RollingGroupby.count` where a ``ValueError`` was raised when specifying the ``closed`` parameter (:issue:`35869`)
- Bug in :meth:`.DataFrameGroupBy.rolling` returning wrong values with partial centered window (:issue:`36040`)
- Bug in :meth:`.DataFrameGroupBy.rolling` returned wrong values with time aware window containing ``NaN``. Raises ``ValueError`` because windows are not monotonic now (:issue:`34617`)
- Bug in :meth:`.Rolling.__iter__` where a ``ValueError`` was not raised when ``min_periods`` was larger than ``window`` (:issue:`37156`)
- Using :meth:`.Rolling.var` instead of :meth:`.Rolling.std` avoids numerical issues for :meth:`.Rolling.corr` when :meth:`.Rolling.var` is still within floating point precision while :meth:`.Rolling.std` is not (:issue:`31286`)
- Bug in :meth:`.DataFrameGroupBy.quantile` and :meth:`.Resampler.quantile` raised ``TypeError`` when values were of type ``Timedelta`` (:issue:`29485`)
- Bug in :meth:`.Rolling.median` and :meth:`.Rolling.quantile` returned wrong values for :class:`.BaseIndexer` subclasses with non-monotonic starting or ending points for windows (:issue:`37153`)
- Bug in :meth:`DataFrame.groupby` dropped ``nan`` groups from result with ``dropna=False`` when grouping over a single column (:issue:`35646`, :issue:`35542`)
- Bug in :meth:`.DataFrameGroupBy.head`, :meth:`DataFrameGroupBy.tail`, :meth:`SeriesGroupBy.head`, and :meth:`SeriesGroupBy.tail` would raise when used with ``axis=1`` (:issue:`9772`)
- Bug in :meth:`.DataFrameGroupBy.transform` would raise when used with ``axis=1`` and a transformation kernel (e.g. "shift") (:issue:`36308`)
- Bug in :meth:`.DataFrameGroupBy.resample` using ``.agg`` with sum produced different result than just calling ``.sum`` (:issue:`33548`)
- Bug in :meth:`.DataFrameGroupBy.apply` dropped values on ``nan`` group when returning the same axes with the original frame (:issue:`38227`)
- Bug in :meth:`.DataFrameGroupBy.quantile` couldn't handle with arraylike ``q`` when grouping by columns (:issue:`33795`)
- Bug in :meth:`DataFrameGroupBy.rank` with ``datetime64tz`` or period dtype incorrectly casting results to those dtypes instead of returning ``float64`` dtype (:issue:`38187`)

Reshaping
^^^^^^^^^

- Bug in :meth:`DataFrame.crosstab` was returning incorrect results on inputs with duplicate row names, duplicate column names or duplicate names between row and column labels (:issue:`22529`)
- Bug in :meth:`DataFrame.pivot_table` with ``aggfunc='count'`` or ``aggfunc='sum'`` returning ``NaN`` for missing categories when pivoted on a ``Categorical``. Now returning ``0`` (:issue:`31422`)
- Bug in :func:`concat` and :class:`DataFrame` constructor where input index names are not preserved in some cases (:issue:`13475`)
- Bug in func :meth:`crosstab` when using multiple columns with ``margins=True`` and ``normalize=True`` (:issue:`35144`)
- Bug in :meth:`DataFrame.stack` where an empty DataFrame.stack would raise an error (:issue:`36113`). Now returning an empty Series with empty MultiIndex.
- Bug in :meth:`Series.unstack`. Now a Series with single level of Index trying to unstack would raise a ``ValueError`` (:issue:`36113`)
- Bug in :meth:`DataFrame.agg` with ``func={'name':<FUNC>}`` incorrectly raising ``TypeError`` when ``DataFrame.columns==['Name']`` (:issue:`36212`)
- Bug in :meth:`Series.transform` would give incorrect results or raise when the argument ``func`` was a dictionary (:issue:`35811`)
- Bug in :meth:`DataFrame.pivot` did not preserve :class:`MultiIndex` level names for columns when rows and columns are both multiindexed (:issue:`36360`)
- Bug in :meth:`DataFrame.pivot` modified ``index`` argument when ``columns`` was passed but ``values`` was not (:issue:`37635`)
- Bug in :meth:`DataFrame.join` returned a non deterministic level-order for the resulting :class:`MultiIndex` (:issue:`36910`)
- Bug in :meth:`DataFrame.combine_first` caused wrong alignment with dtype ``string`` and one level of ``MultiIndex`` containing only ``NA`` (:issue:`37591`)
- Fixed regression in :func:`merge` on merging :class:`.DatetimeIndex` with empty DataFrame (:issue:`36895`)
- Bug in :meth:`DataFrame.apply` not setting index of return value when ``func`` return type is ``dict`` (:issue:`37544`)
- Bug in :meth:`DataFrame.merge` and :meth:`pandas.merge` returning inconsistent ordering in result for ``how=right`` and ``how=left`` (:issue:`35382`)
- Bug in :func:`merge_ordered` couldn't handle list-like ``left_by`` or ``right_by`` (:issue:`35269`)
- Bug in :func:`merge_ordered` returned wrong join result when length of ``left_by`` or ``right_by`` equals to the rows of ``left`` or ``right`` (:issue:`38166`)
- Bug in :func:`merge_ordered` didn't raise when elements in ``left_by`` or ``right_by`` not exist in ``left`` columns or ``right`` columns (:issue:`38167`)
- Bug in :func:`DataFrame.drop_duplicates` not validating bool dtype for ``ignore_index`` keyword (:issue:`38274`)

ExtensionArray
^^^^^^^^^^^^^^

- Fixed bug where :class:`DataFrame` column set to scalar extension type via a dict instantiation was considered an object type rather than the extension type (:issue:`35965`)
- Fixed bug where ``astype()`` with equal dtype and ``copy=False`` would return a new object (:issue:`28488`)
- Fixed bug when applying a NumPy ufunc with multiple outputs to an :class:`.IntegerArray` returning ``None`` (:issue:`36913`)
- Fixed an inconsistency in :class:`.PeriodArray`'s ``__init__`` signature to those of :class:`.DatetimeArray` and :class:`.TimedeltaArray` (:issue:`37289`)
- Reductions for :class:`.BooleanArray`, :class:`.Categorical`, :class:`.DatetimeArray`, :class:`.FloatingArray`, :class:`.IntegerArray`, :class:`.PeriodArray`, :class:`.TimedeltaArray`, and :class:`.PandasArray` are now keyword-only methods (:issue:`37541`)
- Fixed a bug where a  ``TypeError`` was wrongly raised if a membership check was made on an ``ExtensionArray`` containing nan-like values (:issue:`37867`)

Other
^^^^^

- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` incorrectly raising an ``AssertionError`` instead of a ``ValueError`` when invalid parameter combinations are passed (:issue:`36045`)
- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` with numeric values and string ``to_replace`` (:issue:`34789`)
- Fixed metadata propagation in :meth:`Series.abs` and ufuncs called on Series and DataFrames (:issue:`28283`)
- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` incorrectly casting from ``PeriodDtype`` to object dtype (:issue:`34871`)
- Fixed bug in metadata propagation incorrectly copying DataFrame columns as metadata when the column name overlaps with the metadata name (:issue:`37037`)
- Fixed metadata propagation in the :class:`Series.dt`, :class:`Series.str` accessors, :class:`DataFrame.duplicated`, :class:`DataFrame.stack`, :class:`DataFrame.unstack`, :class:`DataFrame.pivot`, :class:`DataFrame.append`, :class:`DataFrame.diff`, :class:`DataFrame.applymap` and :class:`DataFrame.update` methods (:issue:`28283`, :issue:`37381`)
- Fixed metadata propagation when selecting columns with ``DataFrame.__getitem__`` (:issue:`28283`)
- Bug in :meth:`Index.intersection` with non-:class:`Index` failing to set the correct name on the returned :class:`Index` (:issue:`38111`)
- Bug in :meth:`RangeIndex.intersection` failing to set the correct name on the returned :class:`Index` in some corner cases (:issue:`38197`)
- Bug in :meth:`Index.difference` failing to set the correct name on the returned :class:`Index` in some corner cases (:issue:`38268`)
- Bug in :meth:`Index.union` behaving differently depending on whether operand is an :class:`Index` or other list-like (:issue:`36384`)
- Bug in :meth:`Index.intersection` with non-matching numeric dtypes casting to ``object`` dtype instead of minimal common dtype (:issue:`38122`)
- Bug in :meth:`IntervalIndex.union` returning an incorrectly-typed :class:`Index` when empty (:issue:`38282`)
- Passing an array with 2 or more dimensions to the :class:`Series` constructor now raises the more specific ``ValueError`` rather than a bare ``Exception`` (:issue:`35744`)
- Bug in ``dir`` where ``dir(obj)`` wouldn't show attributes defined on the instance for pandas objects (:issue:`37173`)
- Bug in :meth:`Index.drop` raising ``InvalidIndexError`` when index has duplicates (:issue:`38051`)
- Bug in :meth:`RangeIndex.difference` returning :class:`Int64Index` in some cases where it should return :class:`RangeIndex` (:issue:`38028`)
- Fixed bug in :func:`assert_series_equal` when comparing a datetime-like array with an equivalent non extension dtype array (:issue:`37609`)
- Bug in :func:`.is_bool_dtype` would raise when passed a valid string such as ``"boolean"`` (:issue:`38386`)
- Fixed regression in logical operators raising ``ValueError`` when columns of :class:`DataFrame` are a :class:`CategoricalIndex` with unused categories (:issue:`38367`)

.. ---------------------------------------------------------------------------

.. _whatsnew_120.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.5..v1.2.0
.. _whatsnew_150:

What's new in 1.5.0 (??)
------------------------

These are the changes in pandas 1.5.0. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------
.. _whatsnew_150.enhancements:

Enhancements
~~~~~~~~~~~~

.. _whatsnew_150.enhancements.styler:

Styler
^^^^^^

  - New method :meth:`.Styler.to_string` for alternative customisable output methods (:issue:`44502`)
  - Various bug fixes, see below.

.. _whatsnew_150.enhancements.enhancement2:

enhancement2
^^^^^^^^^^^^

.. _whatsnew_150.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^
- :meth:`MultiIndex.to_frame` now supports the argument ``allow_duplicates`` and raises on duplicate labels if it is missing or False (:issue:`45245`)
- :class:`StringArray` now accepts array-likes containing nan-likes (``None``, ``np.nan``) for the ``values`` parameter in its constructor in addition to strings and :attr:`pandas.NA`. (:issue:`40839`)
- Improved the rendering of ``categories`` in :class:`CategoricalIndex` (:issue:`45218`)
- :meth:`to_numeric` now preserves float64 arrays when downcasting would generate values not representable in float32 (:issue:`43693`)
- :meth:`Series.reset_index` and :meth:`DataFrame.reset_index` now support the argument ``allow_duplicates`` (:issue:`44410`)
- :meth:`.GroupBy.min` and :meth:`.GroupBy.max` now supports `Numba <https://numba.pydata.org/>`_ execution with the ``engine`` keyword (:issue:`45428`)
-

.. ---------------------------------------------------------------------------
.. _whatsnew_150.notable_bug_fixes:

Notable bug fixes
~~~~~~~~~~~~~~~~~

These are bug fixes that might have notable behavior changes.

.. _whatsnew_150.notable_bug_fixes.notable_bug_fix1:

notable_bug_fix1
^^^^^^^^^^^^^^^^

.. _whatsnew_150.notable_bug_fixes.notable_bug_fix2:

notable_bug_fix2
^^^^^^^^^^^^^^^^

.. ---------------------------------------------------------------------------
.. _whatsnew_150.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_150.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Some minimum supported versions of dependencies were updated.
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| mypy (dev)      | 0.931           |          |    X    |
+-----------------+-----------------+----------+---------+


For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
|                 |                 |    X    |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.


.. _whatsnew_150.read_xml_dtypes:

read_xml now supports ``dtype``, ``converters``, and ``parse_dates``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Similar to other IO methods, :func:`pandas.read_xml` now supports assigning specific dtypes to columns,
apply converter methods, and parse dates (:issue:`43567`).

.. ipython:: python

    xml_dates = """<?xml version='1.0' encoding='utf-8'?>
    <data>
      <row>
        <shape>square</shape>
        <degrees>00360</degrees>
        <sides>4.0</sides>
        <date>2020-01-01</date>
       </row>
      <row>
        <shape>circle</shape>
        <degrees>00360</degrees>
        <sides/>
        <date>2021-01-01</date>
      </row>
      <row>
        <shape>triangle</shape>
        <degrees>00180</degrees>
        <sides>3.0</sides>
        <date>2022-01-01</date>
      </row>
    </data>"""

    df = pd.read_xml(
        xml_dates,
        dtype={'sides': 'Int64'},
        converters={'degrees': str},
        parse_dates=['date']
    )
    df
    df.dtypes

.. _whatsnew_150.api_breaking.other:

Other API changes
^^^^^^^^^^^^^^^^^
-
-

.. ---------------------------------------------------------------------------
.. _whatsnew_150.deprecations:

Deprecations
~~~~~~~~~~~~

.. _whatsnew_150.deprecations.int_slicing_series:

In a future version, integer slicing on a :class:`Series` with a :class:`Int64Index` or :class:`RangeIndex` will be treated as *label-based*, not positional. This will make the behavior consistent with other :meth:`Series.__getitem__` and :meth:`Series.__setitem__` behaviors (:issue:`45162`).

For example:

.. ipython:: python

   ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])

In the old behavior, ``ser[2:4]`` treats the slice as positional:

*Old behavior*:

.. code-block:: ipython

    In [3]: ser[2:4]
    Out[3]:
    5    3
    7    4
    dtype: int64

In a future version, this will be treated as label-based:

*Future behavior*:

.. code-block:: ipython

    In [4]: ser.loc[2:4]
    Out[4]:
    2    1
    3    2
    dtype: int64

To retain the old behavior, use ``series.iloc[i:j]``. To get the future behavior,
use ``series.loc[i:j]``.

Slicing on a :class:`DataFrame` will not be affected.

.. _whatsnew_150.deprecations.other:

Other Deprecations
^^^^^^^^^^^^^^^^^^
- Deprecated the keyword ``line_terminator`` in :meth:`DataFrame.to_csv` and :meth:`Series.to_csv`, use ``lineterminator`` instead; this is for consistency with :func:`read_csv` and the standard library 'csv' module (:issue:`9568`)
- Deprecated behavior of :meth:`SparseArray.astype`, :meth:`Series.astype`, and :meth:`DataFrame.astype` with :class:`SparseDtype` when passing a non-sparse ``dtype``. In a future version, this will cast to that non-sparse dtype instead of wrapping it in a :class:`SparseDtype` (:issue:`34457`)
- Deprecated behavior of :meth:`DatetimeIndex.intersection` and :meth:`DatetimeIndex.symmetric_difference` (``union`` behavior was already deprecated in version 1.3.0) with mixed timezones; in a future version both will be cast to UTC instead of object dtype (:issue:`39328`, :issue:`45357`)
- Deprecated :meth:`DataFrame.iteritems`, :meth:`Series.iteritems`, :meth:`HDFStore.iteritems` in favor of :meth:`DataFrame.items`, :meth:`Series.items`, :meth:`HDFStore.items`  (:issue:`45321`)
- Deprecated :meth:`Series.is_monotonic` and :meth:`Index.is_monotonic` in favor of :meth:`Series.is_monotonic_increasing` and :meth:`Index.is_monotonic_increasing` (:issue:`45422`, :issue:`21335`)
- Deprecated the ``__array_wrap__`` method of DataFrame and Series, rely on standard numpy ufuncs instead (:issue:`45451`)
-


.. ---------------------------------------------------------------------------
.. _whatsnew_150.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~
- Performance improvement in :meth:`.GroupBy.transform` for some user-defined DataFrame -> Series functions (:issue:`45387`)
- Performance improvement in :meth:`DataFrame.duplicated` when subset consists of only one column (:issue:`45236`)
- Performance improvement in :meth:`.GroupBy.transform` when broadcasting values for user-defined functions (:issue:`45708`)
-

.. ---------------------------------------------------------------------------
.. _whatsnew_150.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^
- Bug in :meth:`CategoricalIndex.union` when the index's categories are integer-dtype and the index contains ``NaN`` values incorrectly raising instead of casting to ``float64`` (:issue:`45362`)
-

Datetimelike
^^^^^^^^^^^^
- Bug in :meth:`DataFrame.quantile` with datetime-like dtypes and no rows incorrectly returning ``float64`` dtype instead of retaining datetime-like dtype (:issue:`41544`)
- Bug in :func:`to_datetime` with sequences of ``np.str_`` objects incorrectly raising (:issue:`32264`)
- Bug in :class:`Timestamp` construction when passing datetime components as positional arguments and ``tzinfo`` as a keyword argument incorrectly raising (:issue:`31929`)
-

Timedelta
^^^^^^^^^
-

Timezones
^^^^^^^^^
-
-

Numeric
^^^^^^^
- Bug in operations with array-likes with ``dtype="boolean"`` and :attr:`NA` incorrectly altering the array in-place (:issue:`45421`)
- Bug in multiplying a :class:`Series` with ``IntegerDtype`` or ``FloatingDtype`` by an arraylike with ``timedelta64[ns]`` dtype incorrectly raising (:issue:`45622`)
-

Conversion
^^^^^^^^^^
- Bug in :meth:`DataFrame.astype` not preserving subclasses (:issue:`40810`)
- Bug in constructing a :class:`Series` from a float-containing list or a floating-dtype ndarray-like (e.g. ``dask.Array``) and an integer dtype raising instead of casting like we would with an ``np.ndarray`` (:issue:`40110`)
- Bug in :meth:`Float64Index.astype` to unsigned integer dtype incorrectly casting to ``np.int64`` dtype (:issue:`45309`)
- Bug in :meth:`Series.astype` and :meth:`DataFrame.astype` from floating dtype to unsigned integer dtype failing to raise in the presence of negative values (:issue:`45151`)
- Bug in :func:`array` with ``FloatingDtype`` and values containing float-castable strings incorrectly raising (:issue:`45424`)
-

Strings
^^^^^^^
-
-

Interval
^^^^^^^^
- Bug in :meth:`IntervalArray.__setitem__` when setting ``np.nan`` into an integer-backed array raising ``ValueError`` instead of ``TypeError`` (:issue:`45484`)
-

Indexing
^^^^^^^^
- Bug in :meth:`loc.__getitem__` with a list of keys causing an internal inconsistency that could lead to a disconnect between ``frame.at[x, y]`` vs ``frame[y].loc[x]`` (:issue:`22372`)
- Bug in :meth:`DataFrame.iloc` where indexing a single row on a :class:`DataFrame` with a single ExtensionDtype column gave a copy instead of a view on the underlying data (:issue:`45241`)
- Bug in setting a NA value (``None`` or ``np.nan``) into a :class:`Series` with int-based :class:`IntervalDtype` incorrectly casting to object dtype instead of a float-based :class:`IntervalDtype` (:issue:`45568`)
- Bug in :meth:`Series.__setitem__` with a non-integer :class:`Index` when using an integer key to set a value that cannot be set inplace where a ``ValueError`` was raised insead of casting to a common dtype (:issue:`45070`)
- Bug when setting a value too large for a :class:`Series` dtype failing to coerce to a common type (:issue:`26049`, :issue:`32878`)
- Bug in :meth:`loc.__setitem__` treating ``range`` keys as positional instead of label-based (:issue:`45479`)
- Bug in :meth:`Series.__setitem__` when setting ``boolean`` dtype values containing ``NA`` incorrectly raising instead of casting to ``boolean`` dtype (:issue:`45462`)
- Bug in :meth:`Series.__setitem__` where setting :attr:`NA` into a numeric-dtpye :class:`Series` would incorrectly upcast to object-dtype rather than treating the value as ``np.nan`` (:issue:`44199`)
- Bug in :meth:`DataFrame.mask` with ``inplace=True`` and ``ExtensionDtype`` columns incorrectly raising (:issue:`45577`)
- Bug in getting a column from a DataFrame with an object-dtype row index with datetime-like values: the resulting Series now preserves the exact object-dtype Index from the parent DataFrame (:issue:`42950`)
- Bug in indexing on a :class:`DatetimeIndex` with a ``np.str_`` key incorrectly raising (:issue:`45580`)
- Bug in :meth:`CategoricalIndex.get_indexer` when index contains ``NaN`` values, resulting in elements that are in target but not present in the index to be mapped to the index of the NaN element, instead of -1 (:issue:`45361`)
-

Missing
^^^^^^^
-
-

MultiIndex
^^^^^^^^^^
-
-

I/O
^^^
- Bug in :meth:`DataFrame.to_stata` where no error is raised if the :class:`DataFrame` contains ``-np.inf`` (:issue:`45350`)
- Bug in :meth:`DataFrame.info` where a new line at the end of the output is omitted when called on an empty :class:`DataFrame` (:issue:`45494`)
- Bug in :func:`read_csv` not recognizing line break for ``on_bad_lines="warn"`` for ``engine="c"`` (:issue:`41710`)
- Bug in :func:`read_parquet` when ``engine="pyarrow"`` which caused partial write to disk when column of unsupported datatype was passed (:issue:`44914`)
-

Period
^^^^^^
-
-

Plotting
^^^^^^^^
- Bug in :meth:`DataFrame.plot.barh` that prevented labeling the x-axis and ``xlabel`` updating the y-axis label (:issue:`45144`)
- Bug in :meth:`DataFrame.plot.box` that prevented labeling the x-axis (:issue:`45463`)
- Bug in :meth:`DataFrame.boxplot` that prevented passing in ``xlabel`` and ``ylabel`` (:issue:`45463`)
- Bug in :meth:`DataFrame.boxplot` that prevented specifying ``vert=False`` (:issue:`36918`)
-

Groupby/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^
- Bug in :meth:`DataFrame.resample` ignoring ``closed="right"`` on :class:`TimedeltaIndex` (:issue:`45414`)
-

Reshaping
^^^^^^^^^
- Bug in :func:`concat` between a :class:`Series` with integer dtype and another with :class:`CategoricalDtype` with integer categories and containing ``NaN`` values casting to object dtype instead of ``float64`` (:issue:`45359`)
- Bug in :func:`get_dummies` that selected object and categorical dtypes but not string (:issue:`44965`)
-

Sparse
^^^^^^
- Bug in :meth:`Series.where` and :meth:`DataFrame.where` with ``SparseDtype`` failing to retain the array's ``fill_value`` (:issue:`45691`)
-

ExtensionArray
^^^^^^^^^^^^^^
- Bug in :meth:`IntegerArray.searchsorted` and :meth:`FloatingArray.searchsorted` returning inconsistent results when acting on ``np.nan`` (:issue:`45255`)
-

Styler
^^^^^^
- Minor bug when attempting to apply styling functions to an empty DataFrame subset (:issue:`45313`)
-

Other
^^^^^
- Bug in :meth:`Series.asof` and :meth:`DataFrame.asof` incorrectly casting bool-dtype results to ``float64`` dtype (:issue:`16063`)
-

.. ***DO NOT USE THIS SECTION***

-
-

.. ---------------------------------------------------------------------------
.. _whatsnew_150.contributors:

Contributors
~~~~~~~~~~~~
.. _whatsnew_080:

Version 0.8.0 (June 29, 2012)
-----------------------------

{{ header }}


This is a major release from 0.7.3 and includes extensive work on the time
series handling and processing infrastructure as well as a great deal of new
functionality throughout the library. It includes over 700 commits from more
than 20 distinct authors. Most pandas 0.7.3 and earlier users should not
experience any issues upgrading, but due to the migration to the NumPy
datetime64 dtype, there may be a number of bugs and incompatibilities
lurking. Lingering incompatibilities will be fixed ASAP in a 0.8.1 release if
necessary. See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.

Support for non-unique indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All objects can now work with non-unique indexes. Data alignment / join
operations work according to SQL join semantics (including, if application,
index duplication in many-to-many joins)

NumPy datetime64 dtype and 1.6 dependency
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Time series data are now represented using NumPy's datetime64 dtype; thus,
pandas 0.8.0 now requires at least NumPy 1.6. It has been tested and verified
to work with the development version (1.7+) of NumPy as well which includes
some significant user-facing API changes. NumPy 1.6 also has a number of bugs
having to do with nanosecond resolution data, so I recommend that you steer
clear of NumPy 1.6's datetime64 API functions (though limited as they are) and
only interact with this data using the interface that pandas provides.

See the end of the 0.8.0 section for a "porting" guide listing potential issues
for users migrating legacy code bases from pandas 0.7 or earlier to 0.8.0.

Bug fixes to the 0.7.x series for legacy NumPy < 1.6 users will be provided as
they arise. There will be no more further development in 0.7.x beyond bug
fixes.

Time Series changes and improvements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

    With this release, legacy scikits.timeseries users should be able to port
    their code to use pandas.

.. note::

    See :ref:`documentation <timeseries>` for overview of pandas timeseries API.

- New datetime64 representation **speeds up join operations and data
  alignment**, **reduces memory usage**, and improve serialization /
  deserialization performance significantly over datetime.datetime
- High performance and flexible **resample** method for converting from
  high-to-low and low-to-high frequency. Supports interpolation, user-defined
  aggregation functions, and control over how the intervals and result labeling
  are defined. A suite of high performance Cython/C-based resampling functions
  (including Open-High-Low-Close) have also been implemented.
- Revamp of :ref:`frequency aliases <timeseries.offset_aliases>` and support for
  **frequency shortcuts** like '15min', or '1h30min'
- New :ref:`DatetimeIndex class <timeseries.datetimeindex>` supports both fixed
  frequency and irregular time
  series. Replaces now deprecated DateRange class
- New ``PeriodIndex`` and ``Period`` classes for representing
  :ref:`time spans <timeseries.periods>` and performing **calendar logic**,
  including the ``12 fiscal quarterly frequencies <timeseries.quarterly>``.
  This is a partial port of, and a substantial enhancement to,
  elements of the scikits.timeseries code base. Support for conversion between
  PeriodIndex and DatetimeIndex
- New Timestamp data type subclasses ``datetime.datetime``, providing the same
  interface while enabling working with nanosecond-resolution data. Also
  provides :ref:`easy time zone conversions <timeseries.timezone>`.
- Enhanced support for :ref:`time zones <timeseries.timezone>`. Add
  ``tz_convert`` and ``tz_localize`` methods to TimeSeries and DataFrame. All
  timestamps are stored as UTC; Timestamps from DatetimeIndex objects with time
  zone set will be localized to local time. Time zone conversions are therefore
  essentially free. User needs to know very little about pytz library now; only
  time zone names as strings are required. Time zone-aware timestamps are
  equal if and only if their UTC timestamps match. Operations between time
  zone-aware time series with different time zones will result in a UTC-indexed
  time series.
- Time series **string indexing conveniences** / shortcuts: slice years, year
  and month, and index values with strings
- Enhanced time series **plotting**; adaptation of scikits.timeseries
  matplotlib-based plotting code
- New ``date_range``, ``bdate_range``, and ``period_range`` :ref:`factory
  functions <timeseries.daterange>`
- Robust **frequency inference** function ``infer_freq`` and ``inferred_freq``
  property of DatetimeIndex, with option to infer frequency on construction of
  DatetimeIndex
- to_datetime function efficiently **parses array of strings** to
  DatetimeIndex. DatetimeIndex will parse array or list of strings to
  datetime64
- **Optimized** support for datetime64-dtype data in Series and DataFrame
  columns
- New NaT (Not-a-Time) type to represent **NA** in timestamp arrays
- Optimize Series.asof for looking up **"as of" values** for arrays of
  timestamps
- Milli, Micro, Nano date offset objects
- Can index time series with datetime.time objects to select all data at
  particular **time of day** (``TimeSeries.at_time``) or **between two times**
  (``TimeSeries.between_time``)
- Add :ref:`tshift <timeseries.advanced_datetime>` method for leading/lagging
  using the frequency (if any) of the index, as opposed to a naive lead/lag
  using shift

Other new features
~~~~~~~~~~~~~~~~~~

- New :ref:`cut <reshaping.tile.cut>` and ``qcut`` functions (like R's cut
  function) for computing a categorical variable from a continuous variable by
  binning values either into value-based (``cut``) or quantile-based (``qcut``)
  bins
- Rename ``Factor`` to ``Categorical`` and add a number of usability features
- Add :ref:`limit <missing_data.fillna.limit>` argument to fillna/reindex
- More flexible multiple function application in GroupBy, and can pass list
  (name, function) tuples to get result in particular order with given names
- Add flexible :ref:`replace <missing_data.replace>` method for efficiently
  substituting values
- Enhanced :ref:`read_csv/read_table <io.parse_dates>` for reading time series
  data and converting multiple columns to dates
- Add :ref:`comments <io.comments>` option to parser functions: read_csv, etc.
- Add :ref:`dayfirst <io.dayfirst>` option to parser functions for parsing
  international DD/MM/YYYY dates
- Allow the user to specify the CSV reader :ref:`dialect <io.dialect>` to
  control quoting etc.
- Handling :ref:`thousands <io.thousands>` separators in read_csv to improve
  integer parsing.
- Enable unstacking of multiple levels in one shot. Alleviate ``pivot_table``
  bugs (empty columns being introduced)
- Move to klib-based hash tables for indexing; better performance and less
  memory usage than Python's dict
- Add first, last, min, max, and prod optimized GroupBy functions
- New :ref:`ordered_merge <merging.merge_ordered>` function
- Add flexible :ref:`comparison <basics.binop>` instance methods eq, ne, lt,
  gt, etc. to DataFrame, Series
- Improve :ref:`scatter_matrix <visualization.scatter_matrix>` plotting
  function and add histogram or kernel density estimates to diagonal
- Add :ref:`'kde' <visualization.kde>` plot option for density plots
- Support for converting DataFrame to R data.frame through rpy2
- Improved support for complex numbers in Series and DataFrame
- Add :ref:`pct_change <computation.pct_change>` method to all data structures
- Add max_colwidth configuration option for DataFrame console output
- :ref:`Interpolate <missing_data.interpolate>` Series values using index values
- Can select multiple columns from GroupBy
- Add :ref:`update <merging.combine_first.update>` methods to Series/DataFrame
  for updating values in place
- Add ``any`` and ``all`` method to DataFrame

New plotting methods
~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import pandas as pd

   fx = pd.read_pickle("data/fx_prices")
   import matplotlib.pyplot as plt

``Series.plot`` now supports a ``secondary_y`` option:

.. code-block:: python

   plt.figure()

   fx["FR"].plot(style="g")

   fx["IT"].plot(style="k--", secondary_y=True)

Vytautas Jancauskas, the 2012 GSOC participant, has added many new plot
types. For example, ``'kde'`` is a new option:

.. code-block:: python

   s = pd.Series(
       np.concatenate((np.random.randn(1000), np.random.randn(1000) * 0.5 + 3))
   )
   plt.figure()
   s.hist(density=True, alpha=0.2)
   s.plot(kind="kde")

See :ref:`the plotting page <visualization.other>` for much more.

Other API changes
~~~~~~~~~~~~~~~~~

- Deprecation of ``offset``, ``time_rule``, and ``timeRule`` arguments names in
  time series functions. Warnings will be printed until pandas 0.9 or 1.0.

Potential porting issues for pandas <= 0.7.3 users
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The major change that may affect you in pandas 0.8.0 is that time series
indexes use NumPy's ``datetime64`` data type instead of ``dtype=object`` arrays
of Python's built-in ``datetime.datetime`` objects. ``DateRange`` has been
replaced by ``DatetimeIndex`` but otherwise behaved identically. But, if you
have code that converts ``DateRange`` or ``Index`` objects that used to contain
``datetime.datetime`` values to plain NumPy arrays, you may have bugs lurking
with code using scalar values because you are handing control over to NumPy:

.. ipython:: python

   import datetime

   rng = pd.date_range("1/1/2000", periods=10)
   rng[5]
   isinstance(rng[5], datetime.datetime)
   rng_asarray = np.asarray(rng)
   scalar_val = rng_asarray[5]
   type(scalar_val)

pandas's ``Timestamp`` object is a subclass of ``datetime.datetime`` that has
nanosecond support (the ``nanosecond`` field store the nanosecond value between
0 and 999). It should substitute directly into any code that used
``datetime.datetime`` values before. Thus, I recommend not casting
``DatetimeIndex`` to regular NumPy arrays.

If you have code that requires an array of ``datetime.datetime`` objects, you
have a couple of options. First, the ``astype(object)`` method of ``DatetimeIndex``
produces an array of ``Timestamp`` objects:

.. ipython:: python

   stamp_array = rng.astype(object)
   stamp_array
   stamp_array[5]

To get an array of proper ``datetime.datetime`` objects, use the
``to_pydatetime`` method:

.. ipython:: python

   dt_array = rng.to_pydatetime()
   dt_array
   dt_array[5]

matplotlib knows how to handle ``datetime.datetime`` but not Timestamp
objects. While I recommend that you plot time series using ``TimeSeries.plot``,
you can either use ``to_pydatetime`` or register a converter for the Timestamp
type. See `matplotlib documentation
<http://matplotlib.org/api/units_api.html>`__ for more on this.

.. warning::

    There are bugs in the user-facing API with the nanosecond datetime64 unit
    in NumPy 1.6. In particular, the string version of the array shows garbage
    values, and conversion to ``dtype=object`` is similarly broken.

    .. ipython:: python

       rng = pd.date_range("1/1/2000", periods=10)
       rng
       np.asarray(rng)
       converted = np.asarray(rng, dtype=object)
       converted[5]

    **Trust me: don't panic**. If you are using NumPy 1.6 and restrict your
    interaction with ``datetime64`` values to pandas's API you will be just
    fine. There is nothing wrong with the data-type (a 64-bit integer
    internally); all of the important data processing happens in pandas and is
    heavily tested. I strongly recommend that you **do not work directly with
    datetime64 arrays in NumPy 1.6** and only use the pandas API.


**Support for non-unique indexes**: In the latter case, you may have code
inside a ``try:... catch:`` block that failed due to the index not being
unique. In many cases it will no longer fail (some method like ``append`` still
check for uniqueness unless disabled). However, all is not lost: you can
inspect ``index.is_unique`` and raise an exception explicitly if it is
``False`` or go to a different code branch.


.. _whatsnew_0.8.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.7.3..v0.8.0
.. _whatsnew_122:

What's new in 1.2.2 (February 09, 2021)
---------------------------------------

These are the changes in pandas 1.2.2. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_122.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :func:`read_excel` that caused it to raise ``AttributeError`` when checking version of older xlrd versions (:issue:`38955`)
- Fixed regression in :class:`DataFrame` constructor reordering element when construction from datetime ndarray with dtype not ``"datetime64[ns]"`` (:issue:`39422`)
- Fixed regression in :meth:`DataFrame.astype` and :meth:`Series.astype` not casting to bytes dtype (:issue:`39474`)
- Fixed regression in :meth:`~DataFrame.to_pickle` failing to create bz2/xz compressed pickle files with ``protocol=5`` (:issue:`39002`)
- Fixed regression in :func:`pandas.testing.assert_series_equal` and :func:`pandas.testing.assert_frame_equal` always raising ``AssertionError`` when comparing extension dtypes (:issue:`39410`)
- Fixed regression in :meth:`~DataFrame.to_csv` opening ``codecs.StreamWriter`` in binary mode instead of in text mode and ignoring user-provided ``mode`` (:issue:`39247`)
- Fixed regression in :meth:`Categorical.astype` casting to incorrect dtype when ``np.int32`` is passed to dtype argument (:issue:`39402`)
- Fixed regression in :meth:`~DataFrame.to_excel` creating corrupt files when appending (``mode="a"``) to an existing file (:issue:`39576`)
- Fixed regression in :meth:`DataFrame.transform` failing in case of an empty DataFrame or Series (:issue:`39636`)
- Fixed regression in :meth:`~DataFrame.groupby` or :meth:`~DataFrame.resample` when aggregating an all-NaN or numeric object dtype column (:issue:`39329`)
- Fixed regression in :meth:`.Rolling.count` where the ``min_periods`` argument would be set to ``0`` after the operation (:issue:`39554`)
- Fixed regression in :func:`read_excel` that incorrectly raised when the argument ``io`` was a non-path and non-buffer and the ``engine`` argument was specified (:issue:`39528`)

.. ---------------------------------------------------------------------------

.. _whatsnew_122.bug_fixes:

Bug fixes
~~~~~~~~~

- :func:`pandas.read_excel` error message when a specified ``sheetname`` does not exist is now uniform across engines (:issue:`39250`)
- Fixed bug in :func:`pandas.read_excel` producing incorrect results when the engine ``openpyxl`` is used and the excel file is missing or has incorrect dimension information; the fix requires ``openpyxl`` >= 3.0.0, prior versions may still fail (:issue:`38956`, :issue:`39001`)
- Fixed bug in :func:`pandas.read_excel` sometimes producing a ``DataFrame`` with trailing rows of ``np.nan`` when the engine ``openpyxl`` is used (:issue:`39181`)

.. ---------------------------------------------------------------------------

.. _whatsnew_122.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.2.1..v1.2.2
.. _whatsnew_0150:

Version 0.15.0 (October 18, 2014)
---------------------------------

{{ header }}


This is a major release from 0.14.1 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

.. warning::

   pandas >= 0.15.0 will no longer support compatibility with NumPy versions <
   1.7.0. If you want to use the latest versions of pandas, please upgrade to
   NumPy >= 1.7.0 (:issue:`7711`)

- Highlights include:

  - The ``Categorical`` type was integrated as a first-class pandas type, see :ref:`here <whatsnew_0150.cat>`
  - New scalar type ``Timedelta``, and a new index type ``TimedeltaIndex``, see :ref:`here <whatsnew_0150.timedeltaindex>`
  - New datetimelike properties accessor ``.dt`` for Series, see :ref:`Datetimelike Properties <whatsnew_0150.dt>`
  - New DataFrame default display for ``df.info()`` to include memory usage, see :ref:`Memory Usage <whatsnew_0150.memory>`
  - ``read_csv`` will now by default ignore blank lines when parsing, see :ref:`here <whatsnew_0150.blanklines>`
  - API change in using Indexes in set operations, see :ref:`here <whatsnew_0150.index_set_ops>`
  - Enhancements in the handling of timezones, see :ref:`here <whatsnew_0150.tz>`
  - A lot of improvements to the rolling and expanding moment functions, see :ref:`here <whatsnew_0150.roll>`
  - Internal refactoring of the ``Index`` class to no longer sub-class ``ndarray``, see :ref:`Internal Refactoring <whatsnew_0150.refactoring>`
  - dropping support for ``PyTables`` less than version 3.0.0, and ``numexpr`` less than version 2.1 (:issue:`7990`)
  - Split indexing documentation into :ref:`Indexing and Selecting Data <indexing>` and :ref:`MultiIndex / Advanced Indexing <advanced>`
  - Split out string methods documentation into :ref:`Working with Text Data <text>`

- Check the :ref:`API Changes <whatsnew_0150.api>` and :ref:`deprecations <whatsnew_0150.deprecations>` before updating

- :ref:`Other Enhancements <whatsnew_0150.enhancements>`

- :ref:`Performance Improvements <whatsnew_0150.performance>`

- :ref:`Bug Fixes <whatsnew_0150.bug_fixes>`

.. warning::

   In 0.15.0 ``Index`` has internally been refactored to no longer sub-class ``ndarray``
   but instead subclass ``PandasObject``, similarly to the rest of the pandas objects. This change allows very easy sub-classing and creation of new index types. This should be
   a transparent change with only very limited API implications (See the :ref:`Internal Refactoring <whatsnew_0150.refactoring>`)

.. warning::

   The refactoring in :class:`~pandas.Categorical` changed the two argument constructor from
   "codes/labels and levels" to "values and levels (now called 'categories')". This can lead to subtle bugs. If you use
   :class:`~pandas.Categorical` directly, please audit your code before updating to this pandas
   version and change it to use the :meth:`~pandas.Categorical.from_codes` constructor. See more on ``Categorical`` :ref:`here <whatsnew_0150.cat>`


New features
~~~~~~~~~~~~

.. _whatsnew_0150.cat:

Categoricals in Series/DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`~pandas.Categorical` can now be included in ``Series`` and ``DataFrames`` and gained new
methods to manipulate. Thanks to Jan Schulz for much of this API/implementation. (:issue:`3943`, :issue:`5313`, :issue:`5314`,
:issue:`7444`, :issue:`7839`, :issue:`7848`, :issue:`7864`, :issue:`7914`, :issue:`7768`, :issue:`8006`, :issue:`3678`,
:issue:`8075`, :issue:`8076`, :issue:`8143`, :issue:`8453`, :issue:`8518`).

For full docs, see the :ref:`categorical introduction <categorical>` and the
:ref:`API documentation <api.arrays.categorical>`.

.. ipython:: python
    :okwarning:

    df = pd.DataFrame({"id": [1, 2, 3, 4, 5, 6],
                       "raw_grade": ['a', 'b', 'b', 'a', 'a', 'e']})

    df["grade"] = df["raw_grade"].astype("category")
    df["grade"]

    # Rename the categories
    df["grade"].cat.categories = ["very good", "good", "very bad"]

    # Reorder the categories and simultaneously add the missing categories
    df["grade"] = df["grade"].cat.set_categories(["very bad", "bad",
                                                  "medium", "good", "very good"])
    df["grade"]
    df.sort_values("grade")
    df.groupby("grade").size()

- ``pandas.core.group_agg`` and ``pandas.core.factor_agg`` were removed. As an alternative, construct
  a dataframe and use ``df.groupby(<group>).agg(<func>)``.

- Supplying "codes/labels and levels" to the :class:`~pandas.Categorical` constructor is not
  supported anymore. Supplying two arguments to the constructor is now interpreted as
  "values and levels (now called 'categories')". Please change your code to use the :meth:`~pandas.Categorical.from_codes`
  constructor.

- The ``Categorical.labels`` attribute was renamed to ``Categorical.codes`` and is read
  only. If you want to manipulate codes, please use one of the
  :ref:`API methods on Categoricals <api.arrays.categorical>`.

- The ``Categorical.levels`` attribute is renamed to ``Categorical.categories``.


.. _whatsnew_0150.timedeltaindex:

TimedeltaIndex/scalar
^^^^^^^^^^^^^^^^^^^^^

We introduce a new scalar type ``Timedelta``, which is a subclass of ``datetime.timedelta``, and behaves in a similar manner,
but allows compatibility with ``np.timedelta64`` types as well as a host of custom representation, parsing, and attributes.
This type is very similar to how ``Timestamp`` works for ``datetimes``. It is a nice-API box for the type. See the :ref:`docs <timedeltas.timedeltas>`.
(:issue:`3009`, :issue:`4533`, :issue:`8209`, :issue:`8187`, :issue:`8190`, :issue:`7869`, :issue:`7661`, :issue:`8345`, :issue:`8471`)

.. warning::

   ``Timedelta`` scalars (and ``TimedeltaIndex``) component fields are *not the same* as the component fields on a ``datetime.timedelta`` object. For example, ``.seconds`` on a ``datetime.timedelta`` object returns the total number of seconds combined between ``hours``, ``minutes`` and ``seconds``. In contrast, the pandas ``Timedelta`` breaks out hours, minutes, microseconds and nanoseconds separately.

   .. code-block:: ipython

      # Timedelta accessor
      In [9]: tds = pd.Timedelta('31 days 5 min 3 sec')

      In [10]: tds.minutes
      Out[10]: 5L

      In [11]: tds.seconds
      Out[11]: 3L

      # datetime.timedelta accessor
      # this is 5 minutes * 60 + 3 seconds
      In [12]: tds.to_pytimedelta().seconds
      Out[12]: 303

   **Note**: this is no longer true starting from v0.16.0, where full
   compatibility with ``datetime.timedelta`` is introduced. See the
   :ref:`0.16.0 whatsnew entry <whatsnew_0160.api_breaking.timedelta>`

.. warning::

       Prior to 0.15.0 ``pd.to_timedelta`` would return a ``Series`` for list-like/Series input, and a ``np.timedelta64`` for scalar input.
       It will now return a ``TimedeltaIndex`` for list-like input, ``Series`` for Series input, and ``Timedelta`` for scalar input.

       The arguments to ``pd.to_timedelta`` are now ``(arg,unit='ns',box=True,coerce=False)``, previously were ``(arg,box=True,unit='ns')`` as these are more logical.

Construct a scalar

.. ipython:: python

   pd.Timedelta('1 days 06:05:01.00003')
   pd.Timedelta('15.5us')
   pd.Timedelta('1 hour 15.5us')

   # negative Timedeltas have this string repr
   # to be more consistent with datetime.timedelta conventions
   pd.Timedelta('-1us')

   # a NaT
   pd.Timedelta('nan')

Access fields for a ``Timedelta``

.. ipython:: python

   td = pd.Timedelta('1 hour 3m 15.5us')
   td.seconds
   td.microseconds
   td.nanoseconds

Construct a ``TimedeltaIndex``

.. ipython:: python
   :suppress:

   import datetime

.. ipython:: python

   pd.TimedeltaIndex(['1 days', '1 days, 00:00:05',
                      np.timedelta64(2, 'D'),
                      datetime.timedelta(days=2, seconds=2)])

Constructing a ``TimedeltaIndex`` with a regular range

.. ipython:: python

   pd.timedelta_range('1 days', periods=5, freq='D')
   pd.timedelta_range(start='1 days', end='2 days', freq='30T')

You can now use a ``TimedeltaIndex`` as the index of a pandas object

.. ipython:: python

   s = pd.Series(np.arange(5),
                 index=pd.timedelta_range('1 days', periods=5, freq='s'))
   s

You can select with partial string selections

.. ipython:: python

   s['1 day 00:00:02']
   s['1 day':'1 day 00:00:02']

Finally, the combination of ``TimedeltaIndex`` with ``DatetimeIndex`` allow certain combination operations that are ``NaT`` preserving:

.. ipython:: python

   tdi = pd.TimedeltaIndex(['1 days', pd.NaT, '2 days'])
   tdi.tolist()
   dti = pd.date_range('20130101', periods=3)
   dti.tolist()

   (dti + tdi).tolist()
   (dti - tdi).tolist()

- iteration of a ``Series`` e.g. ``list(Series(...))`` of ``timedelta64[ns]`` would prior to v0.15.0 return ``np.timedelta64`` for each element. These will now be wrapped in ``Timedelta``.


.. _whatsnew_0150.memory:

Memory usage
^^^^^^^^^^^^

Implemented methods to find memory usage of a DataFrame. See the :ref:`FAQ <df-memory-usage>` for more. (:issue:`6852`).

A new display option ``display.memory_usage`` (see :ref:`options`) sets the default behavior of the ``memory_usage`` argument in the ``df.info()`` method. By default ``display.memory_usage`` is ``True``.

.. ipython:: python

    dtypes = ['int64', 'float64', 'datetime64[ns]', 'timedelta64[ns]',
              'complex128', 'object', 'bool']
    n = 5000
    data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}
    df = pd.DataFrame(data)
    df['categorical'] = df['object'].astype('category')

    df.info()

Additionally :meth:`~pandas.DataFrame.memory_usage` is an available method for a dataframe object which returns the memory usage of each column.

.. ipython:: python

    df.memory_usage(index=True)


.. _whatsnew_0150.dt:

Series.dt accessor
^^^^^^^^^^^^^^^^^^

``Series`` has gained an accessor to succinctly return datetime like properties for the *values* of the Series, if its a datetime/period like Series. (:issue:`7207`)
This will return a Series, indexed like the existing Series. See the :ref:`docs <basics.dt_accessors>`

.. ipython:: python

   # datetime
   s = pd.Series(pd.date_range('20130101 09:10:12', periods=4))
   s
   s.dt.hour
   s.dt.second
   s.dt.day
   s.dt.freq

This enables nice expressions like this:

.. ipython:: python

   s[s.dt.day == 2]

You can easily produce tz aware transformations:

.. ipython:: python

   stz = s.dt.tz_localize('US/Eastern')
   stz
   stz.dt.tz

You can also chain these types of operations:

.. ipython:: python

   s.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')

The ``.dt`` accessor works for period and timedelta dtypes.

.. ipython:: python

   # period
   s = pd.Series(pd.period_range('20130101', periods=4, freq='D'))
   s
   s.dt.year
   s.dt.day

.. ipython:: python

   # timedelta
   s = pd.Series(pd.timedelta_range('1 day 00:00:05', periods=4, freq='s'))
   s
   s.dt.days
   s.dt.seconds
   s.dt.components


.. _whatsnew_0150.tz:

Timezone handling improvements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- ``tz_localize(None)`` for tz-aware ``Timestamp`` and ``DatetimeIndex`` now removes timezone holding local time,
  previously this resulted in ``Exception`` or ``TypeError`` (:issue:`7812`)

  .. ipython:: python

     ts = pd.Timestamp('2014-08-01 09:00', tz='US/Eastern')
     ts
     ts.tz_localize(None)

     didx = pd.date_range(start='2014-08-01 09:00', freq='H',
                          periods=10, tz='US/Eastern')
     didx
     didx.tz_localize(None)

- ``tz_localize`` now accepts the ``ambiguous`` keyword which allows for passing an array of bools
  indicating whether the date belongs in DST or not, 'NaT' for setting transition times to NaT,
  'infer' for inferring DST/non-DST, and 'raise' (default) for an ``AmbiguousTimeError`` to be raised. See :ref:`the docs<timeseries.timezone_ambiguous>` for more details (:issue:`7943`)

- ``DataFrame.tz_localize`` and ``DataFrame.tz_convert`` now accepts an optional ``level`` argument
  for localizing a specific level of a MultiIndex (:issue:`7846`)

- ``Timestamp.tz_localize`` and ``Timestamp.tz_convert`` now raise ``TypeError`` in error cases, rather than ``Exception`` (:issue:`8025`)

- a timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone (rather than being a naive ``datetime64[ns]``) as ``object`` dtype (:issue:`8411`)

- ``Timestamp.__repr__`` displays ``dateutil.tz.tzoffset`` info (:issue:`7907`)


.. _whatsnew_0150.roll:

Rolling/expanding moments improvements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- :func:`rolling_min`, :func:`rolling_max`, :func:`rolling_cov`, and :func:`rolling_corr`
  now return objects with all ``NaN`` when ``len(arg) < min_periods <= window`` rather
  than raising. (This makes all rolling functions consistent in this behavior). (:issue:`7766`)

  Prior to 0.15.0

  .. ipython:: python

     s = pd.Series([10, 11, 12, 13])

  .. code-block:: ipython

     In [15]: pd.rolling_min(s, window=10, min_periods=5)
     ValueError: min_periods (5) must be <= window (4)

  New behavior

  .. code-block:: ipython

     In [4]: pd.rolling_min(s, window=10, min_periods=5)
     Out[4]:
     0   NaN
     1   NaN
     2   NaN
     3   NaN
     dtype: float64

- :func:`rolling_max`, :func:`rolling_min`, :func:`rolling_sum`, :func:`rolling_mean`, :func:`rolling_median`,
  :func:`rolling_std`, :func:`rolling_var`, :func:`rolling_skew`, :func:`rolling_kurt`, :func:`rolling_quantile`,
  :func:`rolling_cov`, :func:`rolling_corr`, :func:`rolling_corr_pairwise`,
  :func:`rolling_window`, and :func:`rolling_apply` with ``center=True`` previously would return a result of the same
  structure as the input ``arg`` with ``NaN`` in the final ``(window-1)/2`` entries.

  Now the final ``(window-1)/2`` entries of the result are calculated as if the input ``arg`` were followed
  by ``(window-1)/2`` ``NaN`` values (or with shrinking windows, in the case of :func:`rolling_apply`).
  (:issue:`7925`, :issue:`8269`)

  Prior behavior (note final value is ``NaN``):

  .. code-block:: ipython

    In [7]: pd.rolling_sum(Series(range(4)), window=3, min_periods=0, center=True)
    Out[7]:
    0     1
    1     3
    2     6
    3   NaN
    dtype: float64

  New behavior (note final value is ``5 = sum([2, 3, NaN])``):

  .. code-block:: ipython

     In [7]: pd.rolling_sum(pd.Series(range(4)), window=3,
       ....:                min_periods=0, center=True)
     Out[7]:
     0    1
     1    3
     2    6
     3    5
     dtype: float64

- :func:`rolling_window` now normalizes the weights properly in rolling mean mode (`mean=True`) so that
  the calculated weighted means (e.g. 'triang', 'gaussian') are distributed about the same means as those
  calculated without weighting (i.e. 'boxcar'). See :ref:`the note on normalization <window.weighted>` for further details. (:issue:`7618`)

  .. ipython:: python

    s = pd.Series([10.5, 8.8, 11.4, 9.7, 9.3])

  Behavior prior to 0.15.0:

  .. code-block:: ipython

     In [39]: pd.rolling_window(s, window=3, win_type='triang', center=True)
     Out[39]:
     0         NaN
     1    6.583333
     2    6.883333
     3    6.683333
     4         NaN
     dtype: float64

  New behavior

  .. code-block:: ipython

     In [10]: pd.rolling_window(s, window=3, win_type='triang', center=True)
     Out[10]:
     0       NaN
     1     9.875
     2    10.325
     3    10.025
     4       NaN
     dtype: float64

- Removed ``center`` argument from all :func:`expanding_ <expanding_apply>` functions (see :ref:`list <api.functions_expanding>`),
  as the results produced when ``center=True`` did not make much sense. (:issue:`7925`)

- Added optional ``ddof`` argument to :func:`expanding_cov` and :func:`rolling_cov`.
  The default value of ``1`` is backwards-compatible. (:issue:`8279`)

- Documented the ``ddof`` argument to :func:`expanding_var`, :func:`expanding_std`,
  :func:`rolling_var`, and :func:`rolling_std`. These functions' support of a
  ``ddof`` argument (with a default value of ``1``) was previously undocumented. (:issue:`8064`)

- :func:`ewma`, :func:`ewmstd`, :func:`ewmvol`, :func:`ewmvar`, :func:`ewmcov`, and :func:`ewmcorr`
  now interpret ``min_periods`` in the same manner that the :func:`rolling_*()` and :func:`expanding_*()` functions do:
  a given result entry will be ``NaN`` if the (expanding, in this case) window does not contain
  at least ``min_periods`` values. The previous behavior was to set to ``NaN`` the ``min_periods`` entries
  starting with the first non- ``NaN`` value. (:issue:`7977`)

  Prior behavior (note values start at index ``2``, which is ``min_periods`` after index ``0``
  (the index of the first non-empty value)):

  .. ipython:: python

    s  = pd.Series([1, None, None, None, 2, 3])

  .. code-block:: ipython

	In [51]: ewma(s, com=3., min_periods=2)
	Out[51]:
	0         NaN
	1         NaN
	2    1.000000
	3    1.000000
	4    1.571429
	5    2.189189
	dtype: float64

  New behavior (note values start at index ``4``, the location of the 2nd (since ``min_periods=2``) non-empty value):

  .. code-block:: ipython

     In [2]: pd.ewma(s, com=3., min_periods=2)
     Out[2]:
     0         NaN
     1         NaN
     2         NaN
     3         NaN
     4    1.759644
     5    2.383784
     dtype: float64

- :func:`ewmstd`, :func:`ewmvol`, :func:`ewmvar`, :func:`ewmcov`, and :func:`ewmcorr`
  now have an optional ``adjust`` argument, just like :func:`ewma` does,
  affecting how the weights are calculated.
  The default value of ``adjust`` is ``True``, which is backwards-compatible.
  See :ref:`Exponentially weighted moment functions <window.exponentially_weighted>` for details. (:issue:`7911`)

- :func:`ewma`, :func:`ewmstd`, :func:`ewmvol`, :func:`ewmvar`, :func:`ewmcov`, and :func:`ewmcorr`
  now have an optional ``ignore_na`` argument.
  When ``ignore_na=False`` (the default), missing values are taken into account in the weights calculation.
  When ``ignore_na=True`` (which reproduces the pre-0.15.0 behavior), missing values are ignored in the weights calculation.
  (:issue:`7543`)

  .. code-block:: ipython

     In [7]: pd.ewma(pd.Series([None, 1., 8.]), com=2.)
     Out[7]:
     0    NaN
     1    1.0
     2    5.2
     dtype: float64

     In [8]: pd.ewma(pd.Series([1., None, 8.]), com=2.,
       ....:         ignore_na=True)  # pre-0.15.0 behavior
     Out[8]:
     0    1.0
     1    1.0
     2    5.2
     dtype: float64

     In [9]: pd.ewma(pd.Series([1., None, 8.]), com=2.,
       ....:         ignore_na=False)  # new default
     Out[9]:
     0    1.000000
     1    1.000000
     2    5.846154
     dtype: float64

  .. warning::

     By default (``ignore_na=False``) the :func:`ewm*()` functions' weights calculation
     in the presence of missing values is different than in pre-0.15.0 versions.
     To reproduce the pre-0.15.0 calculation of weights in the presence of missing values
     one must specify explicitly ``ignore_na=True``.

- Bug in :func:`expanding_cov`, :func:`expanding_corr`, :func:`rolling_cov`, :func:`rolling_cor`, :func:`ewmcov`, and :func:`ewmcorr`
  returning results with columns sorted by name and producing an error for non-unique columns;
  now handles non-unique columns and returns columns in original order
  (except for the case of two DataFrames with ``pairwise=False``, where behavior is unchanged) (:issue:`7542`)
- Bug in :func:`rolling_count` and :func:`expanding_*()` functions unnecessarily producing error message for zero-length data (:issue:`8056`)
- Bug in :func:`rolling_apply` and :func:`expanding_apply` interpreting ``min_periods=0`` as ``min_periods=1`` (:issue:`8080`)
- Bug in :func:`expanding_std` and :func:`expanding_var` for a single value producing a confusing error message (:issue:`7900`)
- Bug in :func:`rolling_std` and :func:`rolling_var` for a single value producing ``0`` rather than ``NaN`` (:issue:`7900`)

- Bug in :func:`ewmstd`, :func:`ewmvol`, :func:`ewmvar`, and :func:`ewmcov`
  calculation of de-biasing factors when ``bias=False`` (the default).
  Previously an incorrect constant factor was used, based on ``adjust=True``, ``ignore_na=True``,
  and an infinite number of observations.
  Now a different factor is used for each entry, based on the actual weights
  (analogous to the usual ``N/(N-1)`` factor).
  In particular, for a single point a value of ``NaN`` is returned when ``bias=False``,
  whereas previously a value of (approximately) ``0`` was returned.

  For example, consider the following pre-0.15.0 results for ``ewmvar(..., bias=False)``,
  and the corresponding debiasing factors:

  .. ipython:: python

     s = pd.Series([1., 2., 0., 4.])

  .. code-block:: ipython

	 In [89]: ewmvar(s, com=2., bias=False)
	 Out[89]:
	 0   -2.775558e-16
	 1    3.000000e-01
	 2    9.556787e-01
	 3    3.585799e+00
	 dtype: float64

	 In [90]: ewmvar(s, com=2., bias=False) / ewmvar(s, com=2., bias=True)
	 Out[90]:
	 0    1.25
	 1    1.25
	 2    1.25
	 3    1.25
	 dtype: float64

  Note that entry ``0`` is approximately 0, and the debiasing factors are a constant 1.25.
  By comparison, the following 0.15.0 results have a ``NaN`` for entry ``0``,
  and the debiasing factors are decreasing (towards 1.25):

  .. code-block:: ipython

     In [14]: pd.ewmvar(s, com=2., bias=False)
     Out[14]:
     0         NaN
     1    0.500000
     2    1.210526
     3    4.089069
     dtype: float64

     In [15]: pd.ewmvar(s, com=2., bias=False) / pd.ewmvar(s, com=2., bias=True)
     Out[15]:
     0         NaN
     1    2.083333
     2    1.583333
     3    1.425439
     dtype: float64

  See :ref:`Exponentially weighted moment functions <window.exponentially_weighted>` for details. (:issue:`7912`)


.. _whatsnew_0150.sql:

Improvements in the SQL IO module
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Added support for a ``chunksize`` parameter to ``to_sql`` function. This allows DataFrame to be written in chunks and avoid packet-size overflow errors (:issue:`8062`).
- Added support for a ``chunksize`` parameter to ``read_sql`` function. Specifying this argument will return an iterator through chunks of the query result (:issue:`2908`).
- Added support for writing ``datetime.date`` and ``datetime.time`` object columns with ``to_sql`` (:issue:`6932`).
- Added support for specifying a ``schema`` to read from/write to with ``read_sql_table`` and ``to_sql`` (:issue:`7441`, :issue:`7952`).
  For example:

  .. code-block:: python

         df.to_sql('table', engine, schema='other_schema')  # noqa F821
         pd.read_sql_table('table', engine, schema='other_schema')  # noqa F821

- Added support for writing ``NaN`` values with ``to_sql`` (:issue:`2754`).
- Added support for writing datetime64 columns with ``to_sql`` for all database flavors (:issue:`7103`).


.. _whatsnew_0150.api:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0150.api_breaking:

Breaking changes
^^^^^^^^^^^^^^^^

API changes related to ``Categorical`` (see :ref:`here <whatsnew_0150.cat>`
for more details):

- The ``Categorical`` constructor with two arguments changed from
  "codes/labels and levels" to "values and levels (now called 'categories')".
  This can lead to subtle bugs. If you use :class:`~pandas.Categorical` directly,
  please audit your code by changing it to use the :meth:`~pandas.Categorical.from_codes`
  constructor.

  An old function call like (prior to 0.15.0):

  .. code-block:: python

    pd.Categorical([0,1,0,2,1], levels=['a', 'b', 'c'])

  will have to adapted to the following to keep the same behaviour:

  .. code-block:: ipython

    In [2]: pd.Categorical.from_codes([0,1,0,2,1], categories=['a', 'b', 'c'])
    Out[2]:
    [a, b, a, c, b]
    Categories (3, object): [a, b, c]

API changes related to the introduction of the ``Timedelta`` scalar (see
:ref:`above <whatsnew_0150.timedeltaindex>` for more details):

- Prior to 0.15.0 :func:`to_timedelta` would return a ``Series`` for list-like/Series input,
  and a ``np.timedelta64`` for scalar input. It will now return a ``TimedeltaIndex`` for
  list-like input, ``Series`` for Series input, and ``Timedelta`` for scalar input.

For API changes related to the rolling and expanding functions, see detailed overview :ref:`above <whatsnew_0150.roll>`.

Other notable API changes:

- Consistency when indexing with ``.loc`` and a list-like indexer when no values are found.

  .. ipython:: python

     df = pd.DataFrame([['a'], ['b']], index=[1, 2])
     df

  In prior versions there was a difference in these two constructs:

  - ``df.loc[[3]]`` would return a frame reindexed by 3 (with all ``np.nan`` values)
  - ``df.loc[[3],:]`` would raise ``KeyError``.

  Both will now raise a ``KeyError``. The rule is that *at least 1* indexer must be found when using a list-like and ``.loc`` (:issue:`7999`)

  Furthermore in prior versions these were also different:

  - ``df.loc[[1,3]]`` would return a frame reindexed by [1,3]
  - ``df.loc[[1,3],:]`` would raise ``KeyError``.

  Both will now return a frame reindex by [1,3]. E.g.

  .. code-block:: ipython

     In [3]: df.loc[[1, 3]]
     Out[3]:
          0
     1    a
     3  NaN

     In [4]: df.loc[[1, 3], :]
     Out[4]:
          0
     1    a
     3  NaN

  This can also be seen in multi-axis indexing with a ``Panel``.

  .. code-block:: python

     >>> p = pd.Panel(np.arange(2 * 3 * 4).reshape(2, 3, 4),
     ...              items=['ItemA', 'ItemB'],
     ...              major_axis=[1, 2, 3],
     ...              minor_axis=['A', 'B', 'C', 'D'])
     >>> p
     <class 'pandas.core.panel.Panel'>
     Dimensions: 2 (items) x 3 (major_axis) x 4 (minor_axis)
     Items axis: ItemA to ItemB
     Major_axis axis: 1 to 3
     Minor_axis axis: A to D


  The following would raise ``KeyError`` prior to 0.15.0:

  .. code-block:: ipython

     In [5]:
     Out[5]:
        ItemA  ItemD
     1      3    NaN
     2      7    NaN
     3     11    NaN

  Furthermore, ``.loc`` will raise If no values are found in a MultiIndex with a list-like indexer:

  .. ipython:: python
     :okexcept:

     s = pd.Series(np.arange(3, dtype='int64'),
                   index=pd.MultiIndex.from_product([['A'],
                                                    ['foo', 'bar', 'baz']],
                                                    names=['one', 'two'])
                   ).sort_index()
     s
     try:
         s.loc[['D']]
     except KeyError as e:
         print("KeyError: " + str(e))

- Assigning values to ``None`` now considers the dtype when choosing an 'empty' value (:issue:`7941`).

  Previously, assigning to ``None`` in numeric containers changed the
  dtype to object (or errored, depending on the call). It now uses
  ``NaN``:

  .. ipython:: python

     s = pd.Series([1, 2, 3])
     s.loc[0] = None
     s

  ``NaT`` is now used similarly for datetime containers.

  For object containers, we now preserve ``None`` values (previously these
  were converted to ``NaN`` values).

  .. ipython:: python

     s = pd.Series(["a", "b", "c"])
     s.loc[0] = None
     s

  To insert a ``NaN``, you must explicitly use ``np.nan``. See the :ref:`docs <missing.inserting>`.

- In prior versions, updating a pandas object inplace would not reflect in other python references to this object. (:issue:`8511`, :issue:`5104`)

  .. ipython:: python

     s = pd.Series([1, 2, 3])
     s2 = s
     s += 1.5

  Behavior prior to v0.15.0

  .. code-block:: ipython


     # the original object
     In [5]: s
     Out[5]:
     0    2.5
     1    3.5
     2    4.5
     dtype: float64


     # a reference to the original object
     In [7]: s2
     Out[7]:
     0    1
     1    2
     2    3
     dtype: int64

  This is now the correct behavior

  .. ipython:: python

     # the original object
     s

     # a reference to the original object
     s2

.. _whatsnew_0150.blanklines:

- Made both the C-based and Python engines for ``read_csv`` and ``read_table`` ignore empty lines in input as well as
  white space-filled lines, as long as ``sep`` is not white space. This is an API change
  that can be controlled by the keyword parameter ``skip_blank_lines``.  See :ref:`the docs <io.skiplines>` (:issue:`4466`)

- A timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone
  and inserted as ``object`` dtype rather than being converted to a naive ``datetime64[ns]`` (:issue:`8411`).

- Bug in passing a ``DatetimeIndex`` with a timezone that was not being retained in DataFrame construction from a dict (:issue:`7822`)

  In prior versions this would drop the timezone, now it retains the timezone,
  but gives a column of ``object`` dtype:

  .. ipython:: python

        i = pd.date_range('1/1/2011', periods=3, freq='10s', tz='US/Eastern')
        i
        df = pd.DataFrame({'a': i})
        df
        df.dtypes

  Previously this would have yielded a column of ``datetime64`` dtype, but without timezone info.

  The behaviour of assigning a column to an existing dataframe as ``df['a'] = i``
  remains unchanged (this already returned an  ``object`` column with a timezone).

- When passing multiple levels to :meth:`~pandas.DataFrame.stack()`, it will now raise a ``ValueError`` when the
  levels aren't all level names or all level numbers (:issue:`7660`). See
  :ref:`Reshaping by stacking and unstacking <reshaping.stack_multiple>`.

- Raise a ``ValueError`` in ``df.to_hdf`` with 'fixed' format, if ``df`` has non-unique columns as the resulting file will be broken (:issue:`7761`)

- ``SettingWithCopy`` raise/warnings (according to the option ``mode.chained_assignment``) will now be issued when setting a value on a sliced mixed-dtype DataFrame using chained-assignment. (:issue:`7845`, :issue:`7950`)

  .. code-block:: python

     In [1]: df = pd.DataFrame(np.arange(0, 9), columns=['count'])

     In [2]: df['group'] = 'b'

     In [3]: df.iloc[0:5]['group'] = 'a'
     /usr/local/bin/ipython:1: SettingWithCopyWarning:
     A value is trying to be set on a copy of a slice from a DataFrame.
     Try using .loc[row_indexer,col_indexer] = value instead

     See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy

- ``merge``, ``DataFrame.merge``, and ``ordered_merge`` now return the same type
  as the ``left`` argument (:issue:`7737`).

- Previously an enlargement with a mixed-dtype frame would act unlike ``.append`` which will preserve dtypes (related :issue:`2578`, :issue:`8176`):

  .. ipython:: python

     df = pd.DataFrame([[True, 1], [False, 2]],
                       columns=["female", "fitness"])
     df
     df.dtypes

     # dtypes are now preserved
     df.loc[2] = df.loc[1]
     df
     df.dtypes

- ``Series.to_csv()`` now returns a string when ``path=None``, matching the behaviour of ``DataFrame.to_csv()`` (:issue:`8215`).

- ``read_hdf`` now raises ``IOError`` when a file that doesn't exist is passed in. Previously, a new, empty file was created, and a ``KeyError`` raised (:issue:`7715`).

- ``DataFrame.info()`` now ends its output with a newline character (:issue:`8114`)
- Concatenating no objects will now raise a ``ValueError`` rather than a bare ``Exception``.
- Merge errors will now be sub-classes of ``ValueError`` rather than raw ``Exception`` (:issue:`8501`)
- ``DataFrame.plot`` and ``Series.plot`` keywords are now have consistent orders (:issue:`8037`)


.. _whatsnew_0150.refactoring:

Internal refactoring
^^^^^^^^^^^^^^^^^^^^

In 0.15.0 ``Index`` has internally been refactored to no longer sub-class ``ndarray``
but instead subclass ``PandasObject``, similarly to the rest of the pandas objects. This
change allows very easy sub-classing and creation of new index types. This should be
a transparent change with only very limited API implications (:issue:`5080`, :issue:`7439`, :issue:`7796`, :issue:`8024`, :issue:`8367`, :issue:`7997`, :issue:`8522`):

- you may need to unpickle pandas version < 0.15.0 pickles using ``pd.read_pickle`` rather than ``pickle.load``. See :ref:`pickle docs <io.pickle>`
- when plotting with a ``PeriodIndex``, the matplotlib internal axes will now be arrays of ``Period`` rather than a ``PeriodIndex`` (this is similar to how a ``DatetimeIndex`` passes arrays of ``datetimes`` now)
- MultiIndexes will now raise similarly to other pandas objects w.r.t. truth testing, see :ref:`here <gotchas.truth>` (:issue:`7897`).
- When plotting a DatetimeIndex directly with matplotlib's ``plot`` function,
  the axis labels will no longer be formatted as dates but as integers (the
  internal representation of a ``datetime64``). **UPDATE** This is fixed
  in 0.15.1, see :ref:`here <whatsnew_0151.datetime64_plotting>`.

.. _whatsnew_0150.deprecations:

Deprecations
^^^^^^^^^^^^

- The attributes ``Categorical`` ``labels`` and ``levels`` attributes are
  deprecated and renamed to ``codes`` and ``categories``.
- The ``outtype`` argument to ``pd.DataFrame.to_dict`` has been deprecated in favor of ``orient``. (:issue:`7840`)
- The ``convert_dummies`` method has been deprecated in favor of
  ``get_dummies`` (:issue:`8140`)
- The ``infer_dst`` argument in ``tz_localize`` will be deprecated in favor of
  ``ambiguous`` to allow for more flexibility in dealing with DST transitions.
  Replace ``infer_dst=True`` with ``ambiguous='infer'`` for the same behavior (:issue:`7943`).
  See :ref:`the docs<timeseries.timezone_ambiguous>` for more details.
- The top-level ``pd.value_range`` has been deprecated and can be replaced by ``.describe()`` (:issue:`8481`)

.. _whatsnew_0150.index_set_ops:

- The ``Index`` set operations ``+`` and ``-`` were deprecated in order to provide these for numeric type operations on certain index types. ``+`` can be replaced by ``.union()`` or ``|``, and ``-`` by ``.difference()``. Further the method name ``Index.diff()`` is deprecated and can be replaced by ``Index.difference()`` (:issue:`8226`)

  .. code-block:: python

     # +
     pd.Index(['a', 'b', 'c']) + pd.Index(['b', 'c', 'd'])

     # should be replaced by
     pd.Index(['a', 'b', 'c']).union(pd.Index(['b', 'c', 'd']))

  .. code-block:: python

     # -
     pd.Index(['a', 'b', 'c']) - pd.Index(['b', 'c', 'd'])

     # should be replaced by
     pd.Index(['a', 'b', 'c']).difference(pd.Index(['b', 'c', 'd']))

- The ``infer_types`` argument to :func:`~pandas.read_html` now has no
  effect and is deprecated (:issue:`7762`, :issue:`7032`).


.. _whatsnew_0150.prior_deprecations:

Removal of prior version deprecations/changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Remove ``DataFrame.delevel`` method in favor of ``DataFrame.reset_index``



.. _whatsnew_0150.enhancements:

Enhancements
~~~~~~~~~~~~

Enhancements in the importing/exporting of Stata files:

- Added support for bool, uint8, uint16 and uint32 data types in ``to_stata`` (:issue:`7097`, :issue:`7365`)
- Added conversion option when importing Stata files (:issue:`8527`)
- ``DataFrame.to_stata`` and ``StataWriter`` check string length for
  compatibility with limitations imposed in dta files where fixed-width
  strings must contain 244 or fewer characters.  Attempting to write Stata
  dta files with strings longer than 244 characters raises a ``ValueError``. (:issue:`7858`)
- ``read_stata`` and ``StataReader`` can import missing data information into a
  ``DataFrame`` by setting the argument ``convert_missing`` to ``True``. When
  using this options, missing values are returned as ``StataMissingValue``
  objects and columns containing missing values have ``object`` data type. (:issue:`8045`)

Enhancements in the plotting functions:

- Added ``layout`` keyword to ``DataFrame.plot``. You can pass a tuple of ``(rows, columns)``, one of which can be ``-1`` to automatically infer (:issue:`6667`, :issue:`8071`).
- Allow to pass multiple axes to ``DataFrame.plot``, ``hist`` and ``boxplot`` (:issue:`5353`, :issue:`6970`, :issue:`7069`)
- Added support for ``c``, ``colormap`` and ``colorbar`` arguments for ``DataFrame.plot`` with ``kind='scatter'`` (:issue:`7780`)
- Histogram from ``DataFrame.plot`` with ``kind='hist'`` (:issue:`7809`), See :ref:`the docs<visualization.hist>`.
- Boxplot from ``DataFrame.plot`` with ``kind='box'`` (:issue:`7998`), See :ref:`the docs<visualization.box>`.

Other:

- ``read_csv`` now has a keyword parameter ``float_precision`` which specifies which floating-point converter the C engine should use during parsing, see :ref:`here <io.float_precision>` (:issue:`8002`, :issue:`8044`)

- Added ``searchsorted`` method to ``Series`` objects (:issue:`7447`)

- :func:`describe` on mixed-types DataFrames is more flexible. Type-based column filtering is now possible via the ``include``/``exclude`` arguments.
  See the :ref:`docs <basics.describe>` (:issue:`8164`).

  .. ipython:: python

    df = pd.DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,
                       'catB': ['a', 'b', 'c', 'd'] * 6,
                       'numC': np.arange(24),
                       'numD': np.arange(24.) + .5})
    df.describe(include=["object"])
    df.describe(include=["number", "object"], exclude=["float"])

  Requesting all columns is possible with the shorthand 'all'

  .. ipython:: python

    df.describe(include='all')

  Without those arguments, ``describe`` will behave as before, including only numerical columns or, if none are, only categorical columns. See also the :ref:`docs <basics.describe>`

- Added ``split`` as an option to the ``orient`` argument in ``pd.DataFrame.to_dict``. (:issue:`7840`)

- The ``get_dummies`` method can now be used on DataFrames. By default only
  categorical columns are encoded as 0's and 1's, while other columns are
  left untouched.

  .. ipython:: python

    df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],
                    'C': [1, 2, 3]})
    pd.get_dummies(df)

- ``PeriodIndex`` supports ``resolution`` as the same as ``DatetimeIndex`` (:issue:`7708`)
- ``pandas.tseries.holiday`` has added support for additional holidays and ways to observe holidays (:issue:`7070`)
- ``pandas.tseries.holiday.Holiday`` now supports a list of offsets in Python3 (:issue:`7070`)
- ``pandas.tseries.holiday.Holiday`` now supports a days_of_week parameter (:issue:`7070`)
- ``GroupBy.nth()`` now supports selecting multiple nth values (:issue:`7910`)

  .. ipython:: python

    business_dates = pd.date_range(start='4/1/2014', end='6/30/2014', freq='B')
    df = pd.DataFrame(1, index=business_dates, columns=['a', 'b'])
    # get the first, 4th, and last date index for each month
    df.groupby([df.index.year, df.index.month]).nth([0, 3, -1])

- ``Period`` and ``PeriodIndex`` supports addition/subtraction with ``timedelta``-likes (:issue:`7966`)

  If ``Period`` freq is ``D``, ``H``, ``T``, ``S``, ``L``, ``U``, ``N``, ``Timedelta``-like can be added if the result can have same freq. Otherwise, only the same ``offsets`` can be added.

  .. ipython:: python

     idx = pd.period_range('2014-07-01 09:00', periods=5, freq='H')
     idx
     idx + pd.offsets.Hour(2)
     idx + pd.Timedelta('120m')

     idx = pd.period_range('2014-07', periods=5, freq='M')
     idx
     idx + pd.offsets.MonthEnd(3)

- Added experimental compatibility with ``openpyxl`` for versions >= 2.0. The ``DataFrame.to_excel``
  method ``engine`` keyword now recognizes ``openpyxl1`` and ``openpyxl2``
  which will explicitly require openpyxl v1 and v2 respectively, failing if
  the requested version is not available. The ``openpyxl`` engine is a now a
  meta-engine that automatically uses whichever version of openpyxl is
  installed. (:issue:`7177`)

- ``DataFrame.fillna`` can now accept a ``DataFrame`` as a fill value (:issue:`8377`)

- Passing multiple levels to :meth:`~pandas.DataFrame.stack()` will now work when multiple level
  numbers are passed (:issue:`7660`). See
  :ref:`Reshaping by stacking and unstacking <reshaping.stack_multiple>`.

- :func:`set_names`, :func:`set_labels`, and :func:`set_levels` methods now take an optional ``level`` keyword argument to all modification of specific level(s) of a MultiIndex. Additionally :func:`set_names` now accepts a scalar string value when operating on an ``Index`` or on a specific level of a ``MultiIndex`` (:issue:`7792`)

  .. ipython:: python

      idx = pd.MultiIndex.from_product([['a'], range(3), list("pqr")],
                                       names=['foo', 'bar', 'baz'])
      idx.set_names('qux', level=0)
      idx.set_names(['qux', 'corge'], level=[0, 1])
      idx.set_levels(['a', 'b', 'c'], level='bar')
      idx.set_levels([['a', 'b', 'c'], [1, 2, 3]], level=[1, 2])

- ``Index.isin`` now supports a ``level`` argument to specify which index level
  to use for membership tests (:issue:`7892`, :issue:`7890`)

  .. code-block:: ipython

     In [1]: idx = pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']])

     In [2]: idx.values
     Out[2]: array([(0, 'a'), (0, 'b'), (0, 'c'), (1, 'a'), (1, 'b'), (1, 'c')], dtype=object)

     In [3]: idx.isin(['a', 'c', 'e'], level=1)
     Out[3]: array([ True, False,  True,  True, False,  True], dtype=bool)

- ``Index`` now supports ``duplicated`` and ``drop_duplicates``. (:issue:`4060`)

  .. ipython:: python

     idx = pd.Index([1, 2, 3, 4, 1, 2])
     idx
     idx.duplicated()
     idx.drop_duplicates()

- add ``copy=True`` argument to ``pd.concat`` to enable pass through of complete blocks (:issue:`8252`)

- Added support for numpy 1.8+ data types (``bool_``, ``int_``, ``float_``, ``string_``) for conversion to R dataframe  (:issue:`8400`)



.. _whatsnew_0150.performance:

Performance
~~~~~~~~~~~

- Performance improvements in ``DatetimeIndex.__iter__`` to allow faster iteration (:issue:`7683`)
- Performance improvements in ``Period`` creation (and ``PeriodIndex`` setitem) (:issue:`5155`)
- Improvements in Series.transform for significant performance gains (revised) (:issue:`6496`)
- Performance improvements in ``StataReader`` when reading large files (:issue:`8040`, :issue:`8073`)
- Performance improvements in ``StataWriter`` when writing large files (:issue:`8079`)
- Performance and memory usage improvements in multi-key ``groupby`` (:issue:`8128`)
- Performance improvements in groupby ``.agg`` and ``.apply`` where builtins max/min were not mapped to numpy/cythonized versions (:issue:`7722`)
- Performance improvement in writing to sql (``to_sql``) of up to 50% (:issue:`8208`).
- Performance benchmarking of groupby for large value of ngroups (:issue:`6787`)
- Performance improvement in ``CustomBusinessDay``, ``CustomBusinessMonth`` (:issue:`8236`)
- Performance improvement for ``MultiIndex.values`` for multi-level indexes containing datetimes (:issue:`8543`)



.. _whatsnew_0150.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in pivot_table, when using margins and a dict aggfunc (:issue:`8349`)
- Bug in ``read_csv`` where ``squeeze=True`` would return a view (:issue:`8217`)
- Bug in checking of table name in ``read_sql`` in certain cases (:issue:`7826`).
- Bug in ``DataFrame.groupby`` where ``Grouper`` does not recognize level when frequency is specified (:issue:`7885`)
- Bug in multiindexes dtypes getting mixed up when DataFrame is saved to SQL table (:issue:`8021`)
- Bug in ``Series`` 0-division with a float and integer operand dtypes  (:issue:`7785`)
- Bug in ``Series.astype("unicode")`` not calling ``unicode`` on the values correctly (:issue:`7758`)
- Bug in ``DataFrame.as_matrix()`` with mixed ``datetime64[ns]`` and ``timedelta64[ns]`` dtypes (:issue:`7778`)
- Bug in ``HDFStore.select_column()`` not preserving UTC timezone info when selecting a ``DatetimeIndex`` (:issue:`7777`)
- Bug in ``to_datetime`` when ``format='%Y%m%d'`` and ``coerce=True`` are specified, where previously an object array was returned (rather than
  a coerced time-series with ``NaT``), (:issue:`7930`)
- Bug in ``DatetimeIndex`` and ``PeriodIndex`` in-place addition and subtraction cause different result from normal one (:issue:`6527`)
- Bug in adding and subtracting ``PeriodIndex`` with ``PeriodIndex`` raise ``TypeError`` (:issue:`7741`)
- Bug in ``combine_first`` with ``PeriodIndex`` data raises ``TypeError`` (:issue:`3367`)
- Bug in MultiIndex slicing with missing indexers (:issue:`7866`)
- Bug in MultiIndex slicing with various edge cases (:issue:`8132`)
- Regression in MultiIndex indexing with a non-scalar type object (:issue:`7914`)
- Bug in ``Timestamp`` comparisons with ``==`` and ``int64`` dtype (:issue:`8058`)
- Bug in pickles contains ``DateOffset`` may raise ``AttributeError`` when ``normalize`` attribute is referred internally (:issue:`7748`)
- Bug in ``Panel`` when using ``major_xs`` and ``copy=False`` is passed (deprecation warning fails because of missing ``warnings``) (:issue:`8152`).
- Bug in pickle deserialization that failed for pre-0.14.1 containers with dup items trying to avoid ambiguity
  when matching block and manager items, when there's only one block there's no ambiguity (:issue:`7794`)
- Bug in putting a ``PeriodIndex`` into a ``Series`` would convert to ``int64`` dtype, rather than ``object`` of ``Periods`` (:issue:`7932`)
- Bug in ``HDFStore`` iteration when passing a where (:issue:`8014`)
- Bug in ``DataFrameGroupby.transform`` when transforming with a passed non-sorted key (:issue:`8046`, :issue:`8430`)
- Bug in repeated timeseries line and area plot may result in ``ValueError`` or incorrect kind (:issue:`7733`)
- Bug in inference in a ``MultiIndex`` with ``datetime.date`` inputs (:issue:`7888`)
- Bug in ``get`` where an ``IndexError`` would not cause the default value to be returned (:issue:`7725`)
- Bug in ``offsets.apply``, ``rollforward`` and ``rollback`` may reset nanosecond (:issue:`7697`)
- Bug in ``offsets.apply``, ``rollforward`` and ``rollback`` may raise ``AttributeError`` if ``Timestamp`` has ``dateutil`` tzinfo (:issue:`7697`)
- Bug in sorting a MultiIndex frame with a ``Float64Index`` (:issue:`8017`)
- Bug in inconsistent panel setitem with a rhs of a ``DataFrame`` for alignment (:issue:`7763`)
- Bug in ``is_superperiod`` and ``is_subperiod`` cannot handle higher frequencies than ``S`` (:issue:`7760`, :issue:`7772`, :issue:`7803`)
- Bug in 32-bit platforms with ``Series.shift`` (:issue:`8129`)
- Bug in ``PeriodIndex.unique`` returns int64 ``np.ndarray`` (:issue:`7540`)
- Bug in ``groupby.apply`` with a non-affecting mutation in the function (:issue:`8467`)
- Bug in ``DataFrame.reset_index`` which has ``MultiIndex`` contains ``PeriodIndex`` or ``DatetimeIndex`` with tz raises ``ValueError`` (:issue:`7746`, :issue:`7793`)
- Bug in ``DataFrame.plot`` with ``subplots=True`` may draw unnecessary minor xticks and yticks (:issue:`7801`)
- Bug in ``StataReader`` which did not read variable labels in 117 files due to difference between Stata documentation and implementation (:issue:`7816`)
- Bug in ``StataReader`` where strings were always converted to 244 characters-fixed width irrespective of underlying string size (:issue:`7858`)
- Bug in ``DataFrame.plot`` and ``Series.plot`` may ignore ``rot`` and ``fontsize`` keywords (:issue:`7844`)
- Bug in ``DatetimeIndex.value_counts`` doesn't preserve tz  (:issue:`7735`)
- Bug in ``PeriodIndex.value_counts`` results in ``Int64Index`` (:issue:`7735`)
- Bug in ``DataFrame.join`` when doing left join on index and there are multiple matches (:issue:`5391`)
- Bug in ``GroupBy.transform()`` where int groups with a transform that
  didn't preserve the index were incorrectly truncated (:issue:`7972`).
- Bug in ``groupby`` where callable objects without name attributes would take the wrong path,
  and produce a ``DataFrame`` instead of a ``Series`` (:issue:`7929`)
- Bug in ``groupby`` error message when a DataFrame grouping column is duplicated (:issue:`7511`)
- Bug in ``read_html`` where the ``infer_types`` argument forced coercion of
  date-likes incorrectly (:issue:`7762`, :issue:`7032`).
- Bug in ``Series.str.cat`` with an index which was filtered as to not include the first item (:issue:`7857`)
- Bug in ``Timestamp`` cannot parse ``nanosecond`` from string (:issue:`7878`)
- Bug in ``Timestamp`` with string offset and ``tz`` results incorrect (:issue:`7833`)
- Bug in ``tslib.tz_convert`` and ``tslib.tz_convert_single`` may return different results (:issue:`7798`)
- Bug in ``DatetimeIndex.intersection`` of non-overlapping timestamps with tz raises ``IndexError`` (:issue:`7880`)
- Bug in alignment with TimeOps and non-unique indexes (:issue:`8363`)
- Bug in ``GroupBy.filter()`` where fast path vs. slow path made the filter
  return a non scalar value that appeared valid but wasn't (:issue:`7870`).
- Bug in ``date_range()``/``DatetimeIndex()`` when the timezone was inferred from input dates yet incorrect
  times were returned when crossing DST boundaries (:issue:`7835`, :issue:`7901`).
- Bug in ``to_excel()`` where a negative sign was being prepended to positive infinity and was absent for negative infinity (:issue:`7949`)
- Bug in area plot draws legend with incorrect ``alpha`` when ``stacked=True`` (:issue:`8027`)
- ``Period`` and ``PeriodIndex`` addition/subtraction with ``np.timedelta64`` results in incorrect internal representations (:issue:`7740`)
- Bug in ``Holiday`` with no offset or observance (:issue:`7987`)
- Bug in ``DataFrame.to_latex`` formatting when columns or index is a ``MultiIndex`` (:issue:`7982`).
- Bug in ``DateOffset`` around Daylight Savings Time produces unexpected results (:issue:`5175`).
- Bug in ``DataFrame.shift`` where empty columns would throw ``ZeroDivisionError`` on numpy 1.7 (:issue:`8019`)
- Bug in installation where ``html_encoding/*.html`` wasn't installed and
  therefore some tests were not running correctly (:issue:`7927`).
- Bug in ``read_html`` where ``bytes`` objects were not tested for in
  ``_read`` (:issue:`7927`).
- Bug in ``DataFrame.stack()`` when one of the column levels was a datelike (:issue:`8039`)
- Bug in broadcasting numpy scalars with ``DataFrame`` (:issue:`8116`)
- Bug in ``pivot_table`` performed with nameless ``index`` and ``columns`` raises ``KeyError`` (:issue:`8103`)
- Bug in ``DataFrame.plot(kind='scatter')`` draws points and errorbars with different colors when the color is specified by ``c`` keyword (:issue:`8081`)
- Bug in ``Float64Index`` where ``iat`` and ``at`` were not testing and were
  failing (:issue:`8092`).
- Bug in ``DataFrame.boxplot()`` where y-limits were not set correctly when
  producing multiple axes (:issue:`7528`, :issue:`5517`).
- Bug in ``read_csv`` where line comments were not handled correctly given
  a custom line terminator or ``delim_whitespace=True`` (:issue:`8122`).
- Bug in ``read_html`` where empty tables caused a ``StopIteration`` (:issue:`7575`)
- Bug in casting when setting a column in a same-dtype block (:issue:`7704`)
- Bug in accessing groups from a ``GroupBy`` when the original grouper
  was a tuple (:issue:`8121`).
- Bug in ``.at`` that would accept integer indexers on a non-integer index and do fallback (:issue:`7814`)
- Bug with kde plot and NaNs (:issue:`8182`)
- Bug in ``GroupBy.count`` with float32 data type were nan values were not excluded (:issue:`8169`).
- Bug with stacked barplots and NaNs (:issue:`8175`).
- Bug in resample with non evenly divisible offsets (e.g. '7s') (:issue:`8371`)
- Bug in interpolation methods with the ``limit`` keyword when no values needed interpolating (:issue:`7173`).
- Bug where ``col_space`` was ignored in ``DataFrame.to_string()`` when ``header=False`` (:issue:`8230`).
- Bug with ``DatetimeIndex.asof`` incorrectly matching partial strings and returning the wrong date (:issue:`8245`).
- Bug in plotting methods modifying the global matplotlib rcParams (:issue:`8242`).
- Bug in ``DataFrame.__setitem__`` that caused errors when setting a dataframe column to a sparse array (:issue:`8131`)
- Bug where ``Dataframe.boxplot()`` failed when entire column was empty (:issue:`8181`).
- Bug with messed variables in ``radviz`` visualization (:issue:`8199`).
- Bug in interpolation methods with the ``limit`` keyword when no values needed interpolating (:issue:`7173`).
- Bug where ``col_space`` was ignored in ``DataFrame.to_string()`` when ``header=False`` (:issue:`8230`).
- Bug in ``to_clipboard`` that would clip long column data (:issue:`8305`)
- Bug in ``DataFrame`` terminal display: Setting max_column/max_rows to zero did not trigger auto-resizing of dfs to fit terminal width/height (:issue:`7180`).
- Bug in OLS where running with "cluster" and "nw_lags" parameters did not work correctly, but also did not throw an error
  (:issue:`5884`).
- Bug in ``DataFrame.dropna`` that interpreted non-existent columns in the subset argument as the 'last column' (:issue:`8303`)
- Bug in ``Index.intersection`` on non-monotonic non-unique indexes (:issue:`8362`).
- Bug in masked series assignment where mismatching types would break alignment (:issue:`8387`)
- Bug in ``NDFrame.equals`` gives false negatives with dtype=object (:issue:`8437`)
- Bug in assignment with indexer where type diversity would break alignment (:issue:`8258`)
- Bug in ``NDFrame.loc`` indexing when row/column names were lost when target was a list/ndarray (:issue:`6552`)
- Regression in ``NDFrame.loc`` indexing when rows/columns were converted to Float64Index if target was an empty list/ndarray (:issue:`7774`)
- Bug in ``Series`` that allows it to be indexed by a ``DataFrame`` which has unexpected results.  Such indexing is no longer permitted (:issue:`8444`)
- Bug in item assignment of a ``DataFrame`` with MultiIndex columns where right-hand-side columns were not aligned (:issue:`7655`)
- Suppress FutureWarning generated by NumPy when comparing object arrays containing NaN for equality (:issue:`7065`)
- Bug in ``DataFrame.eval()`` where the dtype of the ``not`` operator (``~``)
  was not correctly inferred as ``bool``.


.. _whatsnew_0.15.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.14.1..v0.15.0
.. _whatsnew_0700:

Version 0.7.0 (February 9, 2012)
--------------------------------

{{ header }}


New features
~~~~~~~~~~~~

- New unified :ref:`merge function <merging.join>` for efficiently performing
  full gamut of database / relational-algebra operations. Refactored existing
  join methods to use the new infrastructure, resulting in substantial
  performance gains (:issue:`220`, :issue:`249`, :issue:`267`)

- New :ref:`unified concatenation function <merging.concat>` for concatenating
  Series, DataFrame or Panel objects along an axis. Can form union or
  intersection of the other axes. Improves performance of ``Series.append`` and
  ``DataFrame.append`` (:issue:`468`, :issue:`479`, :issue:`273`)

- Can pass multiple DataFrames to
  ``DataFrame.append`` to concatenate (stack) and multiple Series to
  ``Series.append`` too

- :ref:`Can<basics.dataframe.from_list_of_dicts>` pass list of dicts (e.g., a
  list of JSON objects) to DataFrame constructor (:issue:`526`)

- You can now :ref:`set multiple columns <indexing.columns.multiple>` in a
  DataFrame via ``__getitem__``, useful for transformation (:issue:`342`)

- Handle differently-indexed output values in ``DataFrame.apply`` (:issue:`498`)

.. code-block:: ipython

   In [1]: df = pd.DataFrame(np.random.randn(10, 4))
   In [2]: df.apply(lambda x: x.describe())
   Out[2]:
                  0          1          2          3
   count  10.000000  10.000000  10.000000  10.000000
   mean    0.190912  -0.395125  -0.731920  -0.403130
   std     0.730951   0.813266   1.112016   0.961912
   min    -0.861849  -2.104569  -1.776904  -1.469388
   25%    -0.411391  -0.698728  -1.501401  -1.076610
   50%     0.380863  -0.228039  -1.191943  -1.004091
   75%     0.658444   0.057974  -0.034326   0.461706
   max     1.212112   0.577046   1.643563   1.071804

   [8 rows x 4 columns]

- :ref:`Add<advanced.reorderlevels>` ``reorder_levels`` method to Series and
  DataFrame (:issue:`534`)

- :ref:`Add<indexing.dictionarylike>` dict-like ``get`` function to DataFrame
  and Panel (:issue:`521`)

- :ref:`Add<basics.iterrows>` ``DataFrame.iterrows`` method for efficiently
  iterating through the rows of a DataFrame

- Add ``DataFrame.to_panel`` with code adapted from
  ``LongPanel.to_long``

- :ref:`Add <basics.reindexing>` ``reindex_axis`` method added to DataFrame

- :ref:`Add <basics.stats>` ``level`` option to binary arithmetic functions on
  ``DataFrame`` and ``Series``

- :ref:`Add <advanced.advanced_reindex>` ``level`` option to the ``reindex``
  and ``align`` methods on Series and DataFrame for broadcasting values across
  a level (:issue:`542`, :issue:`552`, others)

- Add attribute-based item access to
  ``Panel`` and add IPython completion (:issue:`563`)

- :ref:`Add <visualization.basic>` ``logy`` option to ``Series.plot`` for
  log-scaling on the Y axis

- :ref:`Add <io.formatting>` ``index`` and ``header`` options to
  ``DataFrame.to_string``

- :ref:`Can <merging.multiple_join>` pass multiple DataFrames to
  ``DataFrame.join`` to join on index (:issue:`115`)

- :ref:`Can <merging.multiple_join>` pass multiple Panels to ``Panel.join``
  (:issue:`115`)

- :ref:`Added <io.formatting>` ``justify`` argument to ``DataFrame.to_string``
  to allow different alignment of column headers

- :ref:`Add <groupby.attributes>` ``sort`` option to GroupBy to allow disabling
  sorting of the group keys for potential speedups (:issue:`595`)

- :ref:`Can <basics.dataframe.from_series>` pass MaskedArray to Series
  constructor (:issue:`563`)

- Add Panel item access via attributes
  and IPython completion (:issue:`554`)

- Implement ``DataFrame.lookup``, fancy-indexing analogue for retrieving values
  given a sequence of row and column labels (:issue:`338`)

- Can pass a :ref:`list of functions <groupby.aggregate.multifunc>` to
  aggregate with groupby on a DataFrame, yielding an aggregated result with
  hierarchical columns (:issue:`166`)

- Can call ``cummin`` and ``cummax`` on Series and DataFrame to get cumulative
  minimum and maximum, respectively (:issue:`647`)

- ``value_range`` added as utility function to get min and max of a dataframe
  (:issue:`288`)

- Added ``encoding`` argument to ``read_csv``, ``read_table``, ``to_csv`` and
  ``from_csv`` for non-ascii text (:issue:`717`)

- :ref:`Added <basics.stats>` ``abs`` method to pandas objects

- :ref:`Added <reshaping.pivot>` ``crosstab`` function for easily computing frequency tables

- :ref:`Added <indexing.set_ops>` ``isin`` method to index objects

- :ref:`Added <advanced.xs>` ``level`` argument to ``xs`` method of DataFrame.


API changes to integer indexing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One of the potentially riskiest API changes in 0.7.0, but also one of the most
important, was a complete review of how **integer indexes** are handled with
regard to label-based indexing. Here is an example:

.. code-block:: ipython

    In [3]: s = pd.Series(np.random.randn(10), index=range(0, 20, 2))
    In [4]: s
    Out[4]:
    0    -1.294524
    2     0.413738
    4     0.276662
    6    -0.472035
    8    -0.013960
    10   -0.362543
    12   -0.006154
    14   -0.923061
    16    0.895717
    18    0.805244
    Length: 10, dtype: float64

    In [5]: s[0]
    Out[5]: -1.2945235902555294

    In [6]: s[2]
    Out[6]: 0.41373810535784006

    In [7]: s[4]
    Out[7]: 0.2766617129497566

This is all exactly identical to the behavior before. However, if you ask for a
key **not** contained in the Series, in versions 0.6.1 and prior, Series would
*fall back* on a location-based lookup. This now raises a ``KeyError``:

.. code-block:: ipython

   In [2]: s[1]
   KeyError: 1

This change also has the same impact on DataFrame:

.. code-block:: ipython

   In [3]: df = pd.DataFrame(np.random.randn(8, 4), index=range(0, 16, 2))

   In [4]: df
       0        1       2       3
   0   0.88427  0.3363 -0.1787  0.03162
   2   0.14451 -0.1415  0.2504  0.58374
   4  -1.44779 -0.9186 -1.4996  0.27163
   6  -0.26598 -2.4184 -0.2658  0.11503
   8  -0.58776  0.3144 -0.8566  0.61941
   10  0.10940 -0.7175 -1.0108  0.47990
   12 -1.16919 -0.3087 -0.6049 -0.43544
   14 -0.07337  0.3410  0.0424 -0.16037

   In [5]: df.ix[3]
   KeyError: 3

In order to support purely integer-based indexing, the following methods have
been added:

.. csv-table::
    :header: "Method","Description"
    :widths: 40,60

	``Series.iget_value(i)``, Retrieve value stored at location ``i``
	``Series.iget(i)``, Alias for ``iget_value``
	``DataFrame.irow(i)``, Retrieve the ``i``-th row
	``DataFrame.icol(j)``, Retrieve the ``j``-th column
	"``DataFrame.iget_value(i, j)``", Retrieve the value at row ``i`` and column ``j``

API tweaks regarding label-based slicing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Label-based slicing using ``ix`` now requires that the index be sorted
(monotonic) **unless** both the start and endpoint are contained in the index:

.. code-block:: python

   In [1]: s = pd.Series(np.random.randn(6), index=list('gmkaec'))

   In [2]: s
   Out[2]:
   g   -1.182230
   m   -0.276183
   k   -0.243550
   a    1.628992
   e    0.073308
   c   -0.539890
   dtype: float64

Then this is OK:

.. code-block:: python

   In [3]: s.ix['k':'e']
   Out[3]:
   k   -0.243550
   a    1.628992
   e    0.073308
   dtype: float64

But this is not:

.. code-block:: ipython

   In [12]: s.ix['b':'h']
   KeyError 'b'

If the index had been sorted, the "range selection" would have been possible:

.. code-block:: python

   In [4]: s2 = s.sort_index()

   In [5]: s2
   Out[5]:
   a    1.628992
   c   -0.539890
   e    0.073308
   g   -1.182230
   k   -0.243550
   m   -0.276183
   dtype: float64

   In [6]: s2.ix['b':'h']
   Out[6]:
   c   -0.539890
   e    0.073308
   g   -1.182230
   dtype: float64

Changes to Series ``[]`` operator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As as notational convenience, you can pass a sequence of labels or a label
slice to a Series when getting and setting values via ``[]`` (i.e. the
``__getitem__`` and ``__setitem__`` methods). The behavior will be the same as
passing similar input to ``ix`` **except in the case of integer indexing**:

.. code-block:: ipython

  In [8]: s = pd.Series(np.random.randn(6), index=list('acegkm'))

  In [9]: s
  Out[9]:
  a   -1.206412
  c    2.565646
  e    1.431256
  g    1.340309
  k   -1.170299
  m   -0.226169
  Length: 6, dtype: float64

  In [10]: s[['m', 'a', 'c', 'e']]
  Out[10]:
  m   -0.226169
  a   -1.206412
  c    2.565646
  e    1.431256
  Length: 4, dtype: float64

  In [11]: s['b':'l']
  Out[11]:
  c    2.565646
  e    1.431256
  g    1.340309
  k   -1.170299
  Length: 4, dtype: float64

  In [12]: s['c':'k']
  Out[12]:
  c    2.565646
  e    1.431256
  g    1.340309
  k   -1.170299
  Length: 4, dtype: float64

In the case of integer indexes, the behavior will be exactly as before
(shadowing ``ndarray``):

.. code-block:: ipython

  In [13]: s = pd.Series(np.random.randn(6), index=range(0, 12, 2))

  In [14]: s[[4, 0, 2]]
  Out[14]:
  4    0.132003
  0    0.410835
  2    0.813850
  Length: 3, dtype: float64

  In [15]: s[1:5]
  Out[15]:
  2    0.813850
  4    0.132003
  6   -0.827317
  8   -0.076467
  Length: 4, dtype: float64

If you wish to do indexing with sequences and slicing on an integer index with
label semantics, use ``ix``.

Other API changes
~~~~~~~~~~~~~~~~~

- The deprecated ``LongPanel`` class has been completely removed

- If ``Series.sort`` is called on a column of a DataFrame, an exception will
  now be raised. Before it was possible to accidentally mutate a DataFrame's
  column by doing ``df[col].sort()`` instead of the side-effect free method
  ``df[col].order()`` (:issue:`316`)

- Miscellaneous renames and deprecations which will (harmlessly) raise
  ``FutureWarning``

- ``drop`` added as an optional parameter to ``DataFrame.reset_index`` (:issue:`699`)

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- :ref:`Cythonized GroupBy aggregations <groupby.aggregate.cython>` no longer
  presort the data, thus achieving a significant speedup (:issue:`93`).  GroupBy
  aggregations with Python functions significantly sped up by clever
  manipulation of the ndarray data type in Cython (:issue:`496`).
- Better error message in DataFrame constructor when passed column labels
  don't match data (:issue:`497`)
- Substantially improve performance of multi-GroupBy aggregation when a
  Python function is passed, reuse ndarray object in Cython (:issue:`496`)
- Can store objects indexed by tuples and floats in HDFStore (:issue:`492`)
- Don't print length by default in Series.to_string, add ``length`` option (:issue:`489`)
- Improve Cython code for multi-groupby to aggregate without having to sort
  the data (:issue:`93`)
- Improve MultiIndex reindexing speed by storing tuples in the MultiIndex,
  test for backwards unpickling compatibility
- Improve column reindexing performance by using specialized Cython take
  function
- Further performance tweaking of Series.__getitem__ for standard use cases
- Avoid Index dict creation in some cases (i.e. when getting slices, etc.),
  regression from prior versions
- Friendlier error message in setup.py if NumPy not installed
- Use common set of NA-handling operations (sum, mean, etc.) in Panel class
  also (:issue:`536`)
- Default name assignment when calling ``reset_index`` on DataFrame with a
  regular (non-hierarchical) index (:issue:`476`)
- Use Cythonized groupers when possible in Series/DataFrame stat ops with
  ``level`` parameter passed (:issue:`545`)
- Ported skiplist data structure to C to speed up ``rolling_median`` by about
  5-10x in most typical use cases (:issue:`374`)


.. _whatsnew_0.7.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.6.1..v0.7.0
.. _whatsnew_135:

What's new in 1.3.5 (December 12, 2021)
---------------------------------------

These are the changes in pandas 1.3.5. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_135.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :meth:`Series.equals` when comparing floats with dtype object to None (:issue:`44190`)
- Fixed regression in :func:`merge_asof` raising error when array was supplied as join key (:issue:`42844`)
- Fixed regression when resampling :class:`DataFrame` with :class:`DateTimeIndex` with empty groups and ``uint8``, ``uint16`` or ``uint32`` columns incorrectly raising ``RuntimeError`` (:issue:`43329`)
- Fixed regression in creating a :class:`DataFrame` from a timezone-aware :class:`Timestamp` scalar near a Daylight Savings Time transition (:issue:`42505`)
- Fixed performance regression in :func:`read_csv` (:issue:`44106`)
- Fixed regression in :meth:`Series.duplicated` and :meth:`Series.drop_duplicates` when Series has :class:`Categorical` dtype with boolean categories (:issue:`44351`)
- Fixed regression in :meth:`.GroupBy.sum` with ``timedelta64[ns]`` dtype containing ``NaT`` failing to treat that value as NA (:issue:`42659`)
- Fixed regression in :meth:`.RollingGroupby.cov` and :meth:`.RollingGroupby.corr` when ``other`` had the same shape as each group would incorrectly return superfluous groups in the result (:issue:`42915`)


.. ---------------------------------------------------------------------------

.. _whatsnew_135.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.4..v1.3.5|HEAD
.. _whatsnew_0210:

Version 0.21.0 (October 27, 2017)
---------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import * # noqa F401, F403


This is a major release from 0.20.3 and includes a number of API changes, deprecations, new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

Highlights include:

- Integration with `Apache Parquet <https://parquet.apache.org/>`__, including a new top-level :func:`read_parquet` function and :meth:`DataFrame.to_parquet` method, see :ref:`here <whatsnew_0210.enhancements.parquet>`.
- New user-facing :class:`pandas.api.types.CategoricalDtype` for specifying
  categoricals independent of the data, see :ref:`here <whatsnew_0210.enhancements.categorical_dtype>`.
- The behavior of ``sum`` and ``prod`` on all-NaN Series/DataFrames is now consistent and no longer depends on whether `bottleneck <https://bottleneck.readthedocs.io>`__ is installed, and ``sum`` and ``prod`` on empty Series now return NaN instead of 0, see :ref:`here <whatsnew_0210.api_breaking.bottleneck>`.
- Compatibility fixes for pypy, see :ref:`here <whatsnew_0210.pypy>`.
- Additions to the ``drop``, ``reindex`` and ``rename`` API to make them more consistent, see :ref:`here <whatsnew_0210.enhancements.drop_api>`.
- Addition of the new methods ``DataFrame.infer_objects`` (see :ref:`here <whatsnew_0210.enhancements.infer_objects>`) and ``GroupBy.pipe`` (see :ref:`here <whatsnew_0210.enhancements.GroupBy_pipe>`).
- Indexing with a list of labels, where one or more of the labels is missing, is deprecated and will raise a KeyError in a future version, see :ref:`here <whatsnew_0210.api_breaking.loc>`.

Check the :ref:`API Changes <whatsnew_0210.api_breaking>` and :ref:`deprecations <whatsnew_0210.deprecations>` before updating.

.. contents:: What's new in v0.21.0
    :local:
    :backlinks: none
    :depth: 2

.. _whatsnew_0210.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0210.enhancements.parquet:

Integration with Apache Parquet file format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Integration with `Apache Parquet <https://parquet.apache.org/>`__, including a new top-level :func:`read_parquet` and :func:`DataFrame.to_parquet` method, see :ref:`here <io.parquet>` (:issue:`15838`, :issue:`17438`).

`Apache Parquet <https://parquet.apache.org/>`__ provides a cross-language, binary file format for reading and writing data frames efficiently.
Parquet is designed to faithfully serialize and de-serialize ``DataFrame`` s, supporting all of the pandas
dtypes, including extension dtypes such as datetime with timezones.

This functionality depends on either the `pyarrow <http://arrow.apache.org/docs/python/>`__ or `fastparquet <https://fastparquet.readthedocs.io/en/latest/>`__ library.
For more details, see :ref:`the IO docs on Parquet <io.parquet>`.


.. _whatsnew_0210.enhancements.infer_objects:

Method ``infer_objects`` type conversion
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :meth:`DataFrame.infer_objects` and :meth:`Series.infer_objects`
methods have been added to perform dtype inference on object columns, replacing
some of the functionality of the deprecated ``convert_objects``
method. See the documentation :ref:`here <basics.object_conversion>`
for more details. (:issue:`11221`)

This method only performs soft conversions on object columns, converting Python objects
to native types, but not any coercive conversions. For example:

.. ipython:: python

   df = pd.DataFrame({'A': [1, 2, 3],
                      'B': np.array([1, 2, 3], dtype='object'),
                      'C': ['1', '2', '3']})
   df.dtypes
   df.infer_objects().dtypes

Note that column ``'C'`` was not converted - only scalar numeric types
will be converted to a new type.  Other types of conversion should be accomplished
using the :func:`to_numeric` function (or :func:`to_datetime`, :func:`to_timedelta`).

.. ipython:: python

   df = df.infer_objects()
   df['C'] = pd.to_numeric(df['C'], errors='coerce')
   df.dtypes

.. _whatsnew_0210.enhancements.attribute_access:

Improved warnings when attempting to create columns
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

New users are often puzzled by the relationship between column operations and
attribute access on ``DataFrame`` instances (:issue:`7175`). One specific
instance of this confusion is attempting to create a new column by setting an
attribute on the ``DataFrame``:

.. code-block:: ipython

   In [1]: df = pd.DataFrame({'one': [1., 2., 3.]})
   In [2]: df.two = [4, 5, 6]

This does not raise any obvious exceptions, but also does not create a new column:

.. code-block:: ipython

   In [3]: df
   Out[3]:
       one
   0  1.0
   1  2.0
   2  3.0

Setting a list-like data structure into a new attribute now raises a ``UserWarning`` about the potential for unexpected behavior. See :ref:`Attribute Access <indexing.attribute_access>`.

.. _whatsnew_0210.enhancements.drop_api:

Method ``drop`` now also accepts index/columns keywords
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :meth:`~DataFrame.drop` method has gained ``index``/``columns`` keywords as an
alternative to specifying the ``axis``. This is similar to the behavior of ``reindex``
(:issue:`12392`).

For example:

.. ipython:: python

    df = pd.DataFrame(np.arange(8).reshape(2, 4),
                      columns=['A', 'B', 'C', 'D'])
    df
    df.drop(['B', 'C'], axis=1)
    # the following is now equivalent
    df.drop(columns=['B', 'C'])

.. _whatsnew_0210.enhancements.rename_reindex_axis:

Methods ``rename``, ``reindex`` now also accept axis keyword
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :meth:`DataFrame.rename` and :meth:`DataFrame.reindex` methods have gained
the ``axis`` keyword to specify the axis to target with the operation
(:issue:`12392`).

Here's ``rename``:

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
   df.rename(str.lower, axis='columns')
   df.rename(id, axis='index')

And ``reindex``:

.. ipython:: python

   df.reindex(['A', 'B', 'C'], axis='columns')
   df.reindex([0, 1, 3], axis='index')

The "index, columns" style continues to work as before.

.. ipython:: python

   df.rename(index=id, columns=str.lower)
   df.reindex(index=[0, 1, 3], columns=['A', 'B', 'C'])

We *highly* encourage using named arguments to avoid confusion when using either
style.

.. _whatsnew_0210.enhancements.categorical_dtype:

``CategoricalDtype`` for specifying categoricals
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`pandas.api.types.CategoricalDtype` has been added to the public API and
expanded to include the ``categories`` and ``ordered`` attributes. A
``CategoricalDtype`` can be used to specify the set of categories and
orderedness of an array, independent of the data. This can be useful for example,
when converting string data to a ``Categorical`` (:issue:`14711`,
:issue:`15078`, :issue:`16015`, :issue:`17643`):

.. ipython:: python

   from pandas.api.types import CategoricalDtype

   s = pd.Series(['a', 'b', 'c', 'a'])  # strings
   dtype = CategoricalDtype(categories=['a', 'b', 'c', 'd'], ordered=True)
   s.astype(dtype)

One place that deserves special mention is in :meth:`read_csv`. Previously, with
``dtype={'col': 'category'}``, the returned values and categories would always
be strings.

.. ipython:: python
   :suppress:

   from io import StringIO

.. ipython:: python

   data = 'A,B\na,1\nb,2\nc,3'
   pd.read_csv(StringIO(data), dtype={'B': 'category'}).B.cat.categories

Notice the "object" dtype.

With a ``CategoricalDtype`` of all numerics, datetimes, or
timedeltas, we can automatically convert to the correct type

.. ipython:: python

   dtype = {'B': CategoricalDtype([1, 2, 3])}
   pd.read_csv(StringIO(data), dtype=dtype).B.cat.categories

The values have been correctly interpreted as integers.

The ``.dtype`` property of a ``Categorical``, ``CategoricalIndex`` or a
``Series`` with categorical type will now return an instance of
``CategoricalDtype``. While the repr has changed, ``str(CategoricalDtype())`` is
still the string ``'category'``. We'll take this moment to remind users that the
*preferred* way to detect categorical data is to use
:func:`pandas.api.types.is_categorical_dtype`, and not ``str(dtype) == 'category'``.

See the :ref:`CategoricalDtype docs <categorical.categoricaldtype>` for more.

.. _whatsnew_0210.enhancements.GroupBy_pipe:

``GroupBy`` objects now have a ``pipe`` method
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``GroupBy`` objects now have a ``pipe`` method, similar to the one on
``DataFrame`` and ``Series``, that allow for functions that take a
``GroupBy`` to be composed in a clean, readable syntax. (:issue:`17871`)

For a concrete example on combining ``.groupby`` and ``.pipe`` , imagine having a
DataFrame with columns for stores, products, revenue and sold quantity. We'd like to
do a groupwise calculation of *prices* (i.e. revenue/quantity) per store and per product.
We could do this in a multi-step operation, but expressing it in terms of piping can make the
code more readable.

First we set the data:

.. ipython:: python

   import numpy as np
   n = 1000
   df = pd.DataFrame({'Store': np.random.choice(['Store_1', 'Store_2'], n),
                      'Product': np.random.choice(['Product_1',
                                                   'Product_2',
                                                   'Product_3'
                                                   ], n),
                      'Revenue': (np.random.random(n) * 50 + 10).round(2),
                      'Quantity': np.random.randint(1, 10, size=n)})
   df.head(2)

Now, to find prices per store/product, we can simply do:

.. ipython:: python

   (df.groupby(['Store', 'Product'])
      .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum())
      .unstack().round(2))

See the :ref:`documentation <groupby.pipe>` for more.


.. _whatsnew_0210.enhancements.rename_categories:

``Categorical.rename_categories`` accepts a dict-like
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`~Series.cat.rename_categories` now accepts a dict-like argument for
``new_categories``. The previous categories are looked up in the dictionary's
keys and replaced if found. The behavior of missing and extra keys is the same
as in :meth:`DataFrame.rename`.

.. ipython:: python

   c = pd.Categorical(['a', 'a', 'b'])
   c.rename_categories({"a": "eh", "b": "bee"})

.. warning::

    To assist with upgrading pandas, ``rename_categories`` treats ``Series`` as
    list-like. Typically, Series are considered to be dict-like (e.g. in
    ``.rename``, ``.map``). In a future version of pandas ``rename_categories``
    will change to treat them as dict-like. Follow the warning message's
    recommendations for writing future-proof code.

    .. code-block:: ipython

        In [33]: c.rename_categories(pd.Series([0, 1], index=['a', 'c']))
        FutureWarning: Treating Series 'new_categories' as a list-like and using the values.
        In a future version, 'rename_categories' will treat Series like a dictionary.
        For dict-like, use 'new_categories.to_dict()'
        For list-like, use 'new_categories.values'.
        Out[33]:
        [0, 0, 1]
        Categories (2, int64): [0, 1]


.. _whatsnew_0210.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

New functions or methods
""""""""""""""""""""""""

- :meth:`~pandas.core.resample.Resampler.nearest` is added to support nearest-neighbor upsampling (:issue:`17496`).
- :class:`~pandas.Index` has added support for a ``to_frame`` method (:issue:`15230`).

New keywords
""""""""""""

- Added a ``skipna`` parameter to :func:`~pandas.api.types.infer_dtype` to
  support type inference in the presence of missing values (:issue:`17059`).
- :func:`Series.to_dict` and :func:`DataFrame.to_dict` now support an ``into`` keyword which allows you to specify the ``collections.Mapping`` subclass that you would like returned.  The default is ``dict``, which is backwards compatible. (:issue:`16122`)
- :func:`Series.set_axis` and :func:`DataFrame.set_axis` now support the ``inplace`` parameter. (:issue:`14636`)
- :func:`Series.to_pickle` and :func:`DataFrame.to_pickle` have gained a ``protocol`` parameter (:issue:`16252`). By default, this parameter is set to `HIGHEST_PROTOCOL <https://docs.python.org/3/library/pickle.html#data-stream-format>`__
- :func:`read_feather` has gained the ``nthreads`` parameter for multi-threaded operations (:issue:`16359`)
- :func:`DataFrame.clip()` and :func:`Series.clip()` have gained an ``inplace`` argument. (:issue:`15388`)
- :func:`crosstab` has gained a ``margins_name`` parameter to define the name of the row / column that will contain the totals when ``margins=True``. (:issue:`15972`)
- :func:`read_json` now accepts a ``chunksize`` parameter that can be used when ``lines=True``. If ``chunksize`` is passed, read_json now returns an iterator which reads in ``chunksize`` lines with each iteration. (:issue:`17048`)
- :func:`read_json` and :func:`~DataFrame.to_json` now accept a ``compression`` argument which allows them to transparently handle compressed files. (:issue:`17798`)

Various enhancements
""""""""""""""""""""

- Improved the import time of pandas by about 2.25x.  (:issue:`16764`)
- Support for `PEP 519 -- Adding a file system path protocol
  <https://www.python.org/dev/peps/pep-0519/>`_ on most readers (e.g.
  :func:`read_csv`) and writers (e.g. :meth:`DataFrame.to_csv`) (:issue:`13823`).
- Added a ``__fspath__`` method to ``pd.HDFStore``, ``pd.ExcelFile``,
  and ``pd.ExcelWriter`` to work properly with the file system path protocol (:issue:`13823`).
- The ``validate`` argument for :func:`merge` now checks whether a merge is one-to-one, one-to-many, many-to-one, or many-to-many. If a merge is found to not be an example of specified merge type, an exception of type ``MergeError`` will be raised. For more, see :ref:`here <merging.validation>` (:issue:`16270`)
- Added support for `PEP 518 <https://www.python.org/dev/peps/pep-0518/>`_ (``pyproject.toml``) to the build system (:issue:`16745`)
- :func:`RangeIndex.append` now returns a ``RangeIndex`` object when possible (:issue:`16212`)
- :func:`Series.rename_axis` and :func:`DataFrame.rename_axis` with ``inplace=True`` now return ``None`` while renaming the axis inplace. (:issue:`15704`)
- :func:`api.types.infer_dtype` now infers decimals. (:issue:`15690`)
- :func:`DataFrame.select_dtypes` now accepts scalar values for include/exclude as well as list-like. (:issue:`16855`)
- :func:`date_range` now accepts 'YS' in addition to 'AS' as an alias for start of year. (:issue:`9313`)
- :func:`date_range` now accepts 'Y' in addition to 'A' as an alias for end of year. (:issue:`9313`)
- :func:`DataFrame.add_prefix` and :func:`DataFrame.add_suffix` now accept strings containing the '%' character. (:issue:`17151`)
- Read/write methods that infer compression (:func:`read_csv`, :func:`read_table`, :func:`read_pickle`, and :meth:`~DataFrame.to_pickle`) can now infer from path-like objects, such as ``pathlib.Path``. (:issue:`17206`)
- :func:`read_sas` now recognizes much more of the most frequently used date (datetime) formats in SAS7BDAT files. (:issue:`15871`)
- :func:`DataFrame.items` and :func:`Series.items` are now present in both Python 2 and 3 and is lazy in all cases. (:issue:`13918`, :issue:`17213`)
- :meth:`pandas.io.formats.style.Styler.where` has been implemented as a convenience for :meth:`pandas.io.formats.style.Styler.applymap`. (:issue:`17474`)
- :func:`MultiIndex.is_monotonic_decreasing` has been implemented.  Previously returned ``False`` in all cases. (:issue:`16554`)
- :func:`read_excel` raises ``ImportError`` with a better message if ``xlrd`` is not installed. (:issue:`17613`)
- :meth:`DataFrame.assign` will preserve the original order of ``**kwargs`` for Python 3.6+ users instead of sorting the column names. (:issue:`14207`)
- :func:`Series.reindex`, :func:`DataFrame.reindex`, :func:`Index.get_indexer` now support list-like argument for ``tolerance``. (:issue:`17367`)

.. _whatsnew_0210.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0210.api_breaking.deps:

Dependencies have increased minimum versions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We have updated our minimum supported versions of dependencies (:issue:`15206`, :issue:`15543`, :issue:`15214`).
If installed, we now require:

   +--------------+-----------------+----------+
   | Package      | Minimum Version | Required |
   +==============+=================+==========+
   | Numpy        | 1.9.0           |    X     |
   +--------------+-----------------+----------+
   | Matplotlib   | 1.4.3           |          |
   +--------------+-----------------+----------+
   | Scipy        | 0.14.0          |          |
   +--------------+-----------------+----------+
   | Bottleneck   | 1.0.0           |          |
   +--------------+-----------------+----------+

Additionally, support has been dropped for Python 3.4 (:issue:`15251`).


.. _whatsnew_0210.api_breaking.bottleneck:

Sum/prod of all-NaN or empty Series/DataFrames is now consistently NaN
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note::

   The changes described here have been partially reverted. See
   the :ref:`v0.22.0 Whatsnew <whatsnew_0220>` for more.


The behavior of ``sum`` and ``prod`` on all-NaN Series/DataFrames no longer depends on
whether `bottleneck <https://bottleneck.readthedocs.io>`__ is installed, and return value of ``sum`` and ``prod`` on an empty Series has changed (:issue:`9422`, :issue:`15507`).

Calling ``sum`` or ``prod`` on an empty or all-``NaN`` ``Series``, or columns of a ``DataFrame``, will result in ``NaN``. See the :ref:`docs <missing_data.numeric_sum>`.

.. ipython:: python

   s = pd.Series([np.nan])

Previously WITHOUT ``bottleneck`` installed:

.. code-block:: ipython

   In [2]: s.sum()
   Out[2]: np.nan

Previously WITH ``bottleneck``:

.. code-block:: ipython

   In [2]: s.sum()
   Out[2]: 0.0

New behavior, without regard to the bottleneck installation:

.. ipython:: python

   s.sum()

Note that this also changes the sum of an empty ``Series``. Previously this always returned 0 regardless of a ``bottleneck`` installation:

.. code-block:: ipython

   In [1]: pd.Series([]).sum()
   Out[1]: 0

but for consistency with the all-NaN case, this was changed to return NaN as well:

.. ipython:: python
   :okwarning:

   pd.Series([]).sum()


.. _whatsnew_0210.api_breaking.loc:

Indexing with a list with missing labels is deprecated
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, selecting with a list of labels, where one or more labels were missing would always succeed, returning ``NaN`` for missing labels.
This will now show a ``FutureWarning``. In the future this will raise a ``KeyError`` (:issue:`15747`).
This warning will trigger on a ``DataFrame`` or a ``Series`` for using ``.loc[]``  or ``[[]]`` when passing a list-of-labels with at least 1 missing label.
See the :ref:`deprecation docs <indexing.deprecate_loc_reindex_listlike>`.


.. ipython:: python

   s = pd.Series([1, 2, 3])
   s

Previous behavior

.. code-block:: ipython

   In [4]: s.loc[[1, 2, 3]]
   Out[4]:
   1    2.0
   2    3.0
   3    NaN
   dtype: float64


Current behavior

.. code-block:: ipython

   In [4]: s.loc[[1, 2, 3]]
   Passing list-likes to .loc or [] with any missing label will raise
   KeyError in the future, you can use .reindex() as an alternative.

   See the documentation here:
   https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike

   Out[4]:
   1    2.0
   2    3.0
   3    NaN
   dtype: float64

The idiomatic way to achieve selecting potentially not-found elements is via ``.reindex()``

.. ipython:: python

   s.reindex([1, 2, 3])

Selection with all keys found is unchanged.

.. ipython:: python

   s.loc[[1, 2]]


.. _whatsnew_0210.api.na_changes:

NA naming changes
^^^^^^^^^^^^^^^^^

In order to promote more consistency among the pandas API, we have added additional top-level
functions :func:`isna` and :func:`notna` that are aliases for :func:`isnull` and :func:`notnull`.
The naming scheme is now more consistent with methods like ``.dropna()`` and ``.fillna()``. Furthermore
in all cases where ``.isnull()`` and ``.notnull()`` methods are defined, these have additional methods
named ``.isna()`` and ``.notna()``, these are included for classes ``Categorical``,
``Index``, ``Series``, and ``DataFrame``. (:issue:`15001`).

The configuration option ``pd.options.mode.use_inf_as_null`` is deprecated, and ``pd.options.mode.use_inf_as_na`` is added as a replacement.


.. _whatsnew_0210.api_breaking.iteration_scalars:

Iteration of Series/Index will now return Python scalars
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, when using certain iteration methods for a ``Series`` with dtype ``int`` or ``float``, you would receive a ``numpy`` scalar, e.g. a ``np.int64``, rather than a Python ``int``. Issue (:issue:`10904`) corrected this for ``Series.tolist()`` and ``list(Series)``. This change makes all iteration methods consistent, in particular, for ``__iter__()`` and ``.map()``; note that this only affects int/float dtypes. (:issue:`13236`, :issue:`13258`, :issue:`14216`).

.. ipython:: python

   s = pd.Series([1, 2, 3])
   s

Previously:

.. code-block:: ipython

   In [2]: type(list(s)[0])
   Out[2]: numpy.int64

New behavior:

.. ipython:: python

   type(list(s)[0])

Furthermore this will now correctly box the results of iteration for :func:`DataFrame.to_dict` as well.

.. ipython:: python

   d = {'a': [1], 'b': ['b']}
   df = pd.DataFrame(d)

Previously:

.. code-block:: ipython

   In [8]: type(df.to_dict()['a'][0])
   Out[8]: numpy.int64

New behavior:

.. ipython:: python

   type(df.to_dict()['a'][0])


.. _whatsnew_0210.api_breaking.loc_with_index:

Indexing with a Boolean Index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously when passing a boolean ``Index`` to ``.loc``, if the index of the ``Series/DataFrame`` had ``boolean`` labels,
you would get a label based selection, potentially duplicating result labels, rather than a boolean indexing selection
(where ``True`` selects elements), this was inconsistent how a boolean numpy array indexed. The new behavior is to
act like a boolean numpy array indexer. (:issue:`17738`)

Previous behavior:

.. ipython:: python

   s = pd.Series([1, 2, 3], index=[False, True, False])
   s

.. code-block:: ipython

   In [59]: s.loc[pd.Index([True, False, True])]
   Out[59]:
   True     2
   False    1
   False    3
   True     2
   dtype: int64

Current behavior

.. ipython:: python

   s.loc[pd.Index([True, False, True])]


Furthermore, previously if you had an index that was non-numeric (e.g. strings), then a boolean Index would raise a ``KeyError``.
This will now be treated as a boolean indexer.

Previously behavior:

.. ipython:: python

   s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
   s

.. code-block:: ipython

   In [39]: s.loc[pd.Index([True, False, True])]
   KeyError: "None of [Index([True, False, True], dtype='object')] are in the [index]"

Current behavior

.. ipython:: python

   s.loc[pd.Index([True, False, True])]


.. _whatsnew_0210.api_breaking.period_index_resampling:

``PeriodIndex`` resampling
^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions of pandas, resampling a ``Series``/``DataFrame`` indexed by a ``PeriodIndex`` returned a ``DatetimeIndex`` in some cases (:issue:`12884`). Resampling to a multiplied frequency now returns a ``PeriodIndex`` (:issue:`15944`). As a minor enhancement, resampling a ``PeriodIndex`` can now handle ``NaT`` values (:issue:`13224`)

Previous behavior:

.. code-block:: ipython

   In [1]: pi = pd.period_range('2017-01', periods=12, freq='M')

   In [2]: s = pd.Series(np.arange(12), index=pi)

   In [3]: resampled = s.resample('2Q').mean()

   In [4]: resampled
   Out[4]:
   2017-03-31     1.0
   2017-09-30     5.5
   2018-03-31    10.0
   Freq: 2Q-DEC, dtype: float64

   In [5]: resampled.index
   Out[5]: DatetimeIndex(['2017-03-31', '2017-09-30', '2018-03-31'], dtype='datetime64[ns]', freq='2Q-DEC')

New behavior:

.. ipython:: python

   pi = pd.period_range('2017-01', periods=12, freq='M')

   s = pd.Series(np.arange(12), index=pi)

   resampled = s.resample('2Q').mean()

   resampled

   resampled.index

Upsampling and calling ``.ohlc()`` previously returned a ``Series``, basically identical to calling ``.asfreq()``. OHLC upsampling now returns a DataFrame with columns ``open``, ``high``, ``low`` and ``close`` (:issue:`13083`). This is consistent with downsampling and ``DatetimeIndex`` behavior.

Previous behavior:

.. code-block:: ipython

   In [1]: pi = pd.period_range(start='2000-01-01', freq='D', periods=10)

   In [2]: s = pd.Series(np.arange(10), index=pi)

   In [3]: s.resample('H').ohlc()
   Out[3]:
   2000-01-01 00:00    0.0
                   ...
   2000-01-10 23:00    NaN
   Freq: H, Length: 240, dtype: float64

   In [4]: s.resample('M').ohlc()
   Out[4]:
            open  high  low  close
   2000-01     0     9    0      9

New behavior:

.. ipython:: python

   pi = pd.period_range(start='2000-01-01', freq='D', periods=10)

   s = pd.Series(np.arange(10), index=pi)

   s.resample('H').ohlc()

   s.resample('M').ohlc()


.. _whatsnew_0210.api_breaking.pandas_eval:

Improved error handling during item assignment in pd.eval
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`eval` will now raise a ``ValueError`` when item assignment malfunctions, or
inplace operations are specified, but there is no item assignment in the expression (:issue:`16732`)

.. ipython:: python

   arr = np.array([1, 2, 3])

Previously, if you attempted the following expression, you would get a not very helpful error message:

.. code-block:: ipython

   In [3]: pd.eval("a = 1 + 2", target=arr, inplace=True)
   ...
   IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`)
   and integer or boolean arrays are valid indices

This is a very long way of saying numpy arrays don't support string-item indexing. With this
change, the error message is now this:

.. code-block:: python

   In [3]: pd.eval("a = 1 + 2", target=arr, inplace=True)
   ...
   ValueError: Cannot assign expression output to target

It also used to be possible to evaluate expressions inplace, even if there was no item assignment:

.. code-block:: ipython

   In [4]: pd.eval("1 + 2", target=arr, inplace=True)
   Out[4]: 3

However, this input does not make much sense because the output is not being assigned to
the target. Now, a ``ValueError`` will be raised when such an input is passed in:

.. code-block:: ipython

   In [4]: pd.eval("1 + 2", target=arr, inplace=True)
   ...
   ValueError: Cannot operate inplace if there is no assignment


.. _whatsnew_0210.api_breaking.dtype_conversions:

Dtype conversions
^^^^^^^^^^^^^^^^^

Previously assignments, ``.where()`` and ``.fillna()`` with a ``bool`` assignment, would coerce to same the type (e.g. int / float), or raise for datetimelikes. These will now preserve the bools with ``object`` dtypes. (:issue:`16821`).

.. ipython:: python

   s = pd.Series([1, 2, 3])

.. code-block:: python

   In [5]: s[1] = True

   In [6]: s
   Out[6]:
   0    1
   1    1
   2    3
   dtype: int64

New behavior

.. ipython:: python

   s[1] = True
   s

Previously, as assignment to a datetimelike with a non-datetimelike would coerce the
non-datetime-like item being assigned (:issue:`14145`).

.. ipython:: python

   s = pd.Series([pd.Timestamp('2011-01-01'), pd.Timestamp('2012-01-01')])

.. code-block:: python

   In [1]: s[1] = 1

   In [2]: s
   Out[2]:
   0   2011-01-01 00:00:00.000000000
   1   1970-01-01 00:00:00.000000001
   dtype: datetime64[ns]

These now coerce to ``object`` dtype.

.. ipython:: python

   s[1] = 1
   s

- Inconsistent behavior in ``.where()`` with datetimelikes which would raise rather than coerce to ``object`` (:issue:`16402`)
- Bug in assignment against ``int64`` data with ``np.ndarray`` with ``float64`` dtype may keep ``int64`` dtype (:issue:`14001`)


.. _whatsnew_210.api.multiindex_single:

MultiIndex constructor with a single level
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``MultiIndex`` constructors no longer squeezes a MultiIndex with all
length-one levels down to a regular ``Index``. This affects all the
``MultiIndex`` constructors. (:issue:`17178`)

Previous behavior:

.. code-block:: ipython

   In [2]: pd.MultiIndex.from_tuples([('a',), ('b',)])
   Out[2]: Index(['a', 'b'], dtype='object')

Length 1 levels are no longer special-cased. They behave exactly as if you had
length 2+ levels, so a :class:`MultiIndex` is always returned from all of the
``MultiIndex`` constructors:

.. ipython:: python

   pd.MultiIndex.from_tuples([('a',), ('b',)])

.. _whatsnew_0210.api.utc_localization_with_series:

UTC localization with Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, :func:`to_datetime` did not localize datetime ``Series`` data when ``utc=True`` was passed. Now, :func:`to_datetime` will correctly localize ``Series`` with a ``datetime64[ns, UTC]`` dtype to be consistent with how list-like and ``Index`` data are handled. (:issue:`6415`).

Previous behavior

.. ipython:: python

   s = pd.Series(['20130101 00:00:00'] * 3)

.. code-block:: ipython

   In [12]: pd.to_datetime(s, utc=True)
   Out[12]:
   0   2013-01-01
   1   2013-01-01
   2   2013-01-01
   dtype: datetime64[ns]

New behavior

.. ipython:: python

   pd.to_datetime(s, utc=True)

Additionally, DataFrames with datetime columns that were parsed by :func:`read_sql_table` and :func:`read_sql_query` will also be localized to UTC only if the original SQL columns were timezone aware datetime columns.

.. _whatsnew_0210.api.consistency_of_range_functions:

Consistency of range functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, there were some inconsistencies between the various range functions: :func:`date_range`, :func:`bdate_range`, :func:`period_range`, :func:`timedelta_range`, and :func:`interval_range`. (:issue:`17471`).

One of the inconsistent behaviors occurred when the ``start``, ``end`` and ``period`` parameters were all specified, potentially leading to ambiguous ranges.  When all three parameters were passed, ``interval_range`` ignored the ``period`` parameter, ``period_range`` ignored the ``end`` parameter, and the other range functions raised.  To promote consistency among the range functions, and avoid potentially ambiguous ranges, ``interval_range`` and ``period_range`` will now raise when all three parameters are passed.

Previous behavior:

.. code-block:: ipython

   In [2]: pd.interval_range(start=0, end=4, periods=6)
   Out[2]:
   IntervalIndex([(0, 1], (1, 2], (2, 3]]
                 closed='right',
                 dtype='interval[int64]')

  In [3]: pd.period_range(start='2017Q1', end='2017Q4', periods=6, freq='Q')
  Out[3]: PeriodIndex(['2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1', '2018Q2'], dtype='period[Q-DEC]', freq='Q-DEC')

New behavior:

.. code-block:: ipython

  In [2]: pd.interval_range(start=0, end=4, periods=6)
  ---------------------------------------------------------------------------
  ValueError: Of the three parameters: start, end, and periods, exactly two must be specified

  In [3]: pd.period_range(start='2017Q1', end='2017Q4', periods=6, freq='Q')
  ---------------------------------------------------------------------------
  ValueError: Of the three parameters: start, end, and periods, exactly two must be specified

Additionally, the endpoint parameter ``end`` was not included in the intervals produced by ``interval_range``.  However, all other range functions include ``end`` in their output.  To promote consistency among the range functions, ``interval_range`` will now include ``end`` as the right endpoint of the final interval, except if ``freq`` is specified in a way which skips ``end``.

Previous behavior:

.. code-block:: ipython

   In [4]: pd.interval_range(start=0, end=4)
   Out[4]:
   IntervalIndex([(0, 1], (1, 2], (2, 3]]
                 closed='right',
                 dtype='interval[int64]')


New behavior:

.. ipython:: python

   pd.interval_range(start=0, end=4)

.. _whatsnew_0210.api.mpl_converters:

No automatic Matplotlib converters
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas no longer registers our ``date``, ``time``, ``datetime``,
``datetime64``, and ``Period`` converters with matplotlib when pandas is
imported. Matplotlib plot methods (``plt.plot``, ``ax.plot``, ...), will not
nicely format the x-axis for ``DatetimeIndex`` or ``PeriodIndex`` values. You
must explicitly register these methods:

pandas built-in ``Series.plot`` and ``DataFrame.plot`` *will* register these
converters on first-use (:issue:`17710`).

.. note::

  This change has been temporarily reverted in pandas 0.21.1,
  for more details see :ref:`here <whatsnew_0211.converters>`.

.. _whatsnew_0210.api:

Other API changes
^^^^^^^^^^^^^^^^^

- The Categorical constructor no longer accepts a scalar for the ``categories`` keyword. (:issue:`16022`)
- Accessing a non-existent attribute on a closed :class:`~pandas.HDFStore` will now
  raise an ``AttributeError`` rather than a ``ClosedFileError`` (:issue:`16301`)
- :func:`read_csv` now issues a ``UserWarning`` if the ``names`` parameter contains duplicates (:issue:`17095`)
- :func:`read_csv` now treats ``'null'`` and ``'n/a'`` strings as missing values by default (:issue:`16471`, :issue:`16078`)
- :class:`pandas.HDFStore`'s string representation is now faster and less detailed. For the previous behavior, use ``pandas.HDFStore.info()``. (:issue:`16503`).
- Compression defaults in HDF stores now follow pytables standards. Default is no compression and if ``complib`` is missing and ``complevel`` > 0 ``zlib`` is used (:issue:`15943`)
- ``Index.get_indexer_non_unique()`` now returns a ndarray indexer rather than an ``Index``; this is consistent with ``Index.get_indexer()`` (:issue:`16819`)
- Removed the ``@slow`` decorator from ``pandas._testing``, which caused issues for some downstream packages' test suites. Use ``@pytest.mark.slow`` instead, which achieves the same thing (:issue:`16850`)
- Moved definition of ``MergeError`` to the ``pandas.errors`` module.
- The signature of :func:`Series.set_axis` and :func:`DataFrame.set_axis` has been changed from ``set_axis(axis, labels)`` to ``set_axis(labels, axis=0)``, for consistency with the rest of the API. The old signature is deprecated and will show a ``FutureWarning`` (:issue:`14636`)
- :func:`Series.argmin` and :func:`Series.argmax` will now raise a ``TypeError`` when used with ``object`` dtypes, instead of a ``ValueError`` (:issue:`13595`)
- :class:`Period` is now immutable, and will now raise an ``AttributeError`` when a user tries to assign a new value to the ``ordinal`` or ``freq`` attributes (:issue:`17116`).
- :func:`to_datetime` when passed a tz-aware ``origin=`` kwarg will now raise a more informative ``ValueError`` rather than a ``TypeError`` (:issue:`16842`)
- :func:`to_datetime` now raises a ``ValueError`` when format includes ``%W`` or ``%U`` without also including day of the week and calendar year (:issue:`16774`)
- Renamed non-functional ``index`` to ``index_col`` in :func:`read_stata` to improve API consistency (:issue:`16342`)
- Bug in :func:`DataFrame.drop` caused boolean labels ``False`` and ``True`` to be treated as labels 0 and 1 respectively when dropping indices from a numeric index. This will now raise a ValueError (:issue:`16877`)
- Restricted DateOffset keyword arguments.  Previously, ``DateOffset`` subclasses allowed arbitrary keyword arguments which could lead to unexpected behavior.  Now, only valid arguments will be accepted. (:issue:`17176`).

.. _whatsnew_0210.deprecations:

Deprecations
~~~~~~~~~~~~

- :meth:`DataFrame.from_csv` and :meth:`Series.from_csv` have been deprecated in favor of :func:`read_csv()` (:issue:`4191`)
- :func:`read_excel()` has deprecated ``sheetname`` in favor of ``sheet_name`` for consistency with ``.to_excel()`` (:issue:`10559`).
- :func:`read_excel()` has deprecated ``parse_cols`` in favor of ``usecols`` for consistency with :func:`read_csv` (:issue:`4988`)
- :func:`read_csv()` has deprecated the ``tupleize_cols`` argument. Column tuples will always be converted to a ``MultiIndex`` (:issue:`17060`)
- :meth:`DataFrame.to_csv` has deprecated the ``tupleize_cols`` argument. MultiIndex columns will be always written as rows in the CSV file (:issue:`17060`)
- The ``convert`` parameter has been deprecated in the ``.take()`` method, as it was not being respected (:issue:`16948`)
- ``pd.options.html.border`` has been deprecated in favor of ``pd.options.display.html.border`` (:issue:`15793`).
- :func:`SeriesGroupBy.nth` has deprecated ``True`` in favor of ``'all'`` for its kwarg ``dropna`` (:issue:`11038`).
- :func:`DataFrame.as_blocks` is deprecated, as this is exposing the internal implementation (:issue:`17302`)
- ``pd.TimeGrouper`` is deprecated in favor of :class:`pandas.Grouper` (:issue:`16747`)
- ``cdate_range`` has been deprecated in favor of :func:`bdate_range`, which has gained ``weekmask`` and ``holidays`` parameters for building custom frequency date ranges. See the :ref:`documentation <timeseries.custom-freq-ranges>` for more details (:issue:`17596`)
- passing ``categories`` or ``ordered`` kwargs to :func:`Series.astype` is deprecated, in favor of passing a :ref:`CategoricalDtype <whatsnew_0210.enhancements.categorical_dtype>` (:issue:`17636`)
- ``.get_value`` and ``.set_value`` on ``Series``, ``DataFrame``, ``Panel``, ``SparseSeries``, and ``SparseDataFrame`` are deprecated in favor of using ``.iat[]`` or ``.at[]`` accessors (:issue:`15269`)
- Passing a non-existent column in ``.to_excel(..., columns=)`` is deprecated and will raise a ``KeyError`` in the future (:issue:`17295`)
- ``raise_on_error`` parameter to :func:`Series.where`, :func:`Series.mask`, :func:`DataFrame.where`, :func:`DataFrame.mask` is deprecated, in favor of ``errors=`` (:issue:`14968`)
- Using :meth:`DataFrame.rename_axis` and :meth:`Series.rename_axis` to alter index or column *labels* is now deprecated in favor of using ``.rename``. ``rename_axis`` may still be used to alter the name of the index or columns (:issue:`17833`).
- :meth:`~DataFrame.reindex_axis` has been deprecated in favor of :meth:`~DataFrame.reindex`. See :ref:`here <whatsnew_0210.enhancements.rename_reindex_axis>` for more (:issue:`17833`).

.. _whatsnew_0210.deprecations.select:

Series.select and DataFrame.select
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :meth:`Series.select` and :meth:`DataFrame.select` methods are deprecated in favor of using ``df.loc[labels.map(crit)]`` (:issue:`12401`)

.. ipython:: python

   df = pd.DataFrame({'A': [1, 2, 3]}, index=['foo', 'bar', 'baz'])

.. code-block:: ipython

   In [3]: df.select(lambda x: x in ['bar', 'baz'])
   FutureWarning: select is deprecated and will be removed in a future release. You can use .loc[crit] as a replacement
   Out[3]:
        A
   bar  2
   baz  3

.. ipython:: python

   df.loc[df.index.map(lambda x: x in ['bar', 'baz'])]


.. _whatsnew_0210.deprecations.argmin_min:

Series.argmax and Series.argmin
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The behavior of :func:`Series.argmax` and :func:`Series.argmin` have been deprecated in favor of :func:`Series.idxmax` and :func:`Series.idxmin`, respectively (:issue:`16830`).

For compatibility with NumPy arrays, ``pd.Series`` implements ``argmax`` and
``argmin``. Since pandas 0.13.0, ``argmax`` has been an alias for
:meth:`pandas.Series.idxmax`, and ``argmin`` has been an alias for
:meth:`pandas.Series.idxmin`. They return the *label* of the maximum or minimum,
rather than the *position*.

We've deprecated the current behavior of ``Series.argmax`` and
``Series.argmin``. Using either of these will emit a ``FutureWarning``. Use
:meth:`Series.idxmax` if you want the label of the maximum. Use
``Series.values.argmax()`` if you want the position of the maximum. Likewise for
the minimum. In a future release ``Series.argmax`` and ``Series.argmin`` will
return the position of the maximum or minimum.

.. _whatsnew_0210.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- :func:`read_excel()` has dropped the ``has_index_names`` parameter (:issue:`10967`)
- The ``pd.options.display.height`` configuration has been dropped (:issue:`3663`)
- The ``pd.options.display.line_width`` configuration has been dropped (:issue:`2881`)
- The ``pd.options.display.mpl_style`` configuration has been dropped (:issue:`12190`)
- ``Index`` has dropped the ``.sym_diff()`` method in favor of ``.symmetric_difference()`` (:issue:`12591`)
- ``Categorical`` has dropped the ``.order()`` and ``.sort()`` methods in favor of ``.sort_values()`` (:issue:`12882`)
- :func:`eval` and :func:`DataFrame.eval` have changed the default of ``inplace`` from ``None`` to ``False`` (:issue:`11149`)
- The function ``get_offset_name`` has been dropped in favor of the ``.freqstr`` attribute for an offset (:issue:`11834`)
- pandas no longer tests for compatibility with hdf5-files created with pandas < 0.11 (:issue:`17404`).



.. _whatsnew_0210.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of instantiating :class:`SparseDataFrame` (:issue:`16773`)
- :attr:`Series.dt` no longer performs frequency inference, yielding a large speedup when accessing the attribute (:issue:`17210`)
- Improved performance of :meth:`~Series.cat.set_categories` by not materializing the values (:issue:`17508`)
- :attr:`Timestamp.microsecond` no longer re-computes on attribute access (:issue:`17331`)
- Improved performance of the :class:`CategoricalIndex` for data that is already categorical dtype (:issue:`17513`)
- Improved performance of :meth:`RangeIndex.min` and :meth:`RangeIndex.max` by using ``RangeIndex`` properties to perform the computations (:issue:`17607`)

.. _whatsnew_0210.docs:

Documentation changes
~~~~~~~~~~~~~~~~~~~~~

- Several ``NaT`` method docstrings (e.g. :func:`NaT.ctime`) were incorrect (:issue:`17327`)
- The documentation has had references to versions < v0.17 removed and cleaned up (:issue:`17442`, :issue:`17442`, :issue:`17404` & :issue:`17504`)

.. _whatsnew_0210.bug_fixes:

Bug fixes
~~~~~~~~~

Conversion
^^^^^^^^^^

- Bug in assignment against datetime-like data with ``int`` may incorrectly convert to datetime-like (:issue:`14145`)
- Bug in assignment against ``int64`` data with ``np.ndarray`` with ``float64`` dtype may keep ``int64`` dtype (:issue:`14001`)
- Fixed the return type of ``IntervalIndex.is_non_overlapping_monotonic`` to be a Python ``bool`` for consistency with similar attributes/methods.  Previously returned a ``numpy.bool_``. (:issue:`17237`)
- Bug in ``IntervalIndex.is_non_overlapping_monotonic`` when intervals are closed on both sides and overlap at a point (:issue:`16560`)
- Bug in :func:`Series.fillna` returns frame when ``inplace=True`` and ``value`` is dict (:issue:`16156`)
- Bug in :attr:`Timestamp.weekday_name` returning a UTC-based weekday name when localized to a timezone (:issue:`17354`)
- Bug in ``Timestamp.replace`` when replacing ``tzinfo`` around DST changes (:issue:`15683`)
- Bug in ``Timedelta`` construction and arithmetic that would not propagate the ``Overflow`` exception (:issue:`17367`)
- Bug in :meth:`~DataFrame.astype` converting to object dtype when passed extension type classes (``DatetimeTZDtype``, ``CategoricalDtype``) rather than instances. Now a ``TypeError`` is raised when a class is passed (:issue:`17780`).
- Bug in :meth:`to_numeric` in which elements were not always being coerced to numeric when ``errors='coerce'`` (:issue:`17007`, :issue:`17125`)
- Bug in ``DataFrame`` and ``Series`` constructors where ``range`` objects are converted to ``int32`` dtype on Windows instead of ``int64`` (:issue:`16804`)

Indexing
^^^^^^^^

- When called with a null slice (e.g. ``df.iloc[:]``), the ``.iloc`` and ``.loc`` indexers return a shallow copy of the original object. Previously they returned the original object. (:issue:`13873`).
- When called on an unsorted ``MultiIndex``, the ``loc`` indexer now will raise ``UnsortedIndexError`` only if proper slicing is used on non-sorted levels (:issue:`16734`).
- Fixes regression in 0.20.3 when indexing with a string on a ``TimedeltaIndex`` (:issue:`16896`).
- Fixed :func:`TimedeltaIndex.get_loc` handling of ``np.timedelta64`` inputs (:issue:`16909`).
- Fix :func:`MultiIndex.sort_index` ordering when ``ascending`` argument is a list, but not all levels are specified, or are in a different order (:issue:`16934`).
- Fixes bug where indexing with ``np.inf`` caused an ``OverflowError`` to be raised (:issue:`16957`)
- Bug in reindexing on an empty ``CategoricalIndex`` (:issue:`16770`)
- Fixes ``DataFrame.loc`` for setting with alignment and tz-aware ``DatetimeIndex`` (:issue:`16889`)
- Avoids ``IndexError`` when passing an Index or Series to ``.iloc`` with older numpy (:issue:`17193`)
- Allow unicode empty strings as placeholders in multilevel columns in Python 2 (:issue:`17099`)
- Bug in ``.iloc`` when used with inplace addition or assignment and an int indexer on a ``MultiIndex`` causing the wrong indexes to be read from and written to (:issue:`17148`)
- Bug in ``.isin()`` in which checking membership in empty ``Series`` objects raised an error (:issue:`16991`)
- Bug in ``CategoricalIndex`` reindexing in which specified indices containing duplicates were not being respected (:issue:`17323`)
- Bug in intersection of ``RangeIndex`` with negative step (:issue:`17296`)
- Bug in ``IntervalIndex`` where performing a scalar lookup fails for included right endpoints of non-overlapping monotonic decreasing indexes (:issue:`16417`, :issue:`17271`)
- Bug in :meth:`DataFrame.first_valid_index` and :meth:`DataFrame.last_valid_index` when no valid entry (:issue:`17400`)
- Bug in :func:`Series.rename` when called with a callable, incorrectly alters the name of the ``Series``, rather than the name of the ``Index``. (:issue:`17407`)
- Bug in :func:`String.str_get` raises ``IndexError`` instead of inserting NaNs when using a negative index. (:issue:`17704`)

IO
^^

- Bug in :func:`read_hdf` when reading a timezone aware index from ``fixed`` format HDFStore (:issue:`17618`)
- Bug in :func:`read_csv` in which columns were not being thoroughly de-duplicated (:issue:`17060`)
- Bug in :func:`read_csv` in which specified column names were not being thoroughly de-duplicated (:issue:`17095`)
- Bug in :func:`read_csv` in which non integer values for the header argument generated an unhelpful / unrelated error message (:issue:`16338`)
- Bug in :func:`read_csv` in which memory management issues in exception handling, under certain conditions, would cause the interpreter to segfault (:issue:`14696`, :issue:`16798`).
- Bug in :func:`read_csv` when called with ``low_memory=False`` in which a CSV with at least one column > 2GB in size would incorrectly raise a ``MemoryError`` (:issue:`16798`).
- Bug in :func:`read_csv` when called with a single-element list ``header`` would return a ``DataFrame`` of all NaN values (:issue:`7757`)
- Bug in :meth:`DataFrame.to_csv` defaulting to 'ascii' encoding in Python 3, instead of 'utf-8' (:issue:`17097`)
- Bug in :func:`read_stata` where value labels could not be read when using an iterator (:issue:`16923`)
- Bug in :func:`read_stata` where the index was not set (:issue:`16342`)
- Bug in :func:`read_html` where import check fails when run in multiple threads (:issue:`16928`)
- Bug in :func:`read_csv` where automatic delimiter detection caused a ``TypeError`` to be thrown when a bad line was encountered rather than the correct error message (:issue:`13374`)
- Bug in :meth:`DataFrame.to_html` with ``notebook=True`` where DataFrames with named indices or non-MultiIndex indices had undesired horizontal or vertical alignment for column or row labels, respectively (:issue:`16792`)
- Bug in :meth:`DataFrame.to_html` in which there was no validation of the ``justify`` parameter (:issue:`17527`)
- Bug in :func:`HDFStore.select` when reading a contiguous mixed-data table featuring VLArray (:issue:`17021`)
- Bug in :func:`to_json` where several conditions (including objects with unprintable symbols, objects with deep recursion, overlong labels) caused segfaults instead of raising the appropriate exception (:issue:`14256`)

Plotting
^^^^^^^^
- Bug in plotting methods using ``secondary_y`` and ``fontsize`` not setting secondary axis font size (:issue:`12565`)
- Bug when plotting ``timedelta`` and ``datetime`` dtypes on y-axis (:issue:`16953`)
- Line plots no longer assume monotonic x data when calculating xlims, they show the entire lines now even for unsorted x data. (:issue:`11310`, :issue:`11471`)
- With matplotlib 2.0.0 and above, calculation of x limits for line plots is left to matplotlib, so that its new default settings are applied. (:issue:`15495`)
- Bug in ``Series.plot.bar`` or ``DataFrame.plot.bar`` with ``y`` not respecting user-passed ``color`` (:issue:`16822`)
- Bug causing ``plotting.parallel_coordinates`` to reset the random seed when using random colors (:issue:`17525`)


GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in ``DataFrame.resample(...).size()`` where an empty ``DataFrame`` did not return a ``Series`` (:issue:`14962`)
- Bug in :func:`infer_freq` causing indices with 2-day gaps during the working week to be wrongly inferred as business daily (:issue:`16624`)
- Bug in ``.rolling(...).quantile()`` which incorrectly used different defaults than :func:`Series.quantile()` and :func:`DataFrame.quantile()` (:issue:`9413`, :issue:`16211`)
- Bug in ``groupby.transform()`` that would coerce boolean dtypes back to float (:issue:`16875`)
- Bug in ``Series.resample(...).apply()`` where an empty ``Series`` modified the source index and did not return the name of a ``Series`` (:issue:`14313`)
- Bug in ``.rolling(...).apply(...)`` with a ``DataFrame`` with a ``DatetimeIndex``, a ``window`` of a timedelta-convertible and ``min_periods >= 1`` (:issue:`15305`)
- Bug in ``DataFrame.groupby`` where index and column keys were not recognized correctly when the number of keys equaled the number of elements on the groupby axis (:issue:`16859`)
- Bug in ``groupby.nunique()`` with ``TimeGrouper`` which cannot handle ``NaT`` correctly (:issue:`17575`)
- Bug in ``DataFrame.groupby`` where a single level selection from a ``MultiIndex`` unexpectedly sorts (:issue:`17537`)
- Bug in ``DataFrame.groupby`` where spurious warning is raised when ``Grouper`` object is used to override ambiguous column name (:issue:`17383`)
- Bug in ``TimeGrouper`` differs when passes as a list and as a scalar (:issue:`17530`)

Sparse
^^^^^^

- Bug in ``SparseSeries`` raises ``AttributeError`` when a dictionary is passed in as data (:issue:`16905`)
- Bug in :func:`SparseDataFrame.fillna` not filling all NaNs when frame was instantiated from SciPy sparse matrix (:issue:`16112`)
- Bug in :func:`SparseSeries.unstack` and :func:`SparseDataFrame.stack` (:issue:`16614`, :issue:`15045`)
- Bug in :func:`make_sparse` treating two numeric/boolean data, which have same bits, as same when array ``dtype`` is ``object`` (:issue:`17574`)
- :func:`SparseArray.all` and :func:`SparseArray.any` are now implemented to handle ``SparseArray``, these were used but not implemented (:issue:`17570`)

Reshaping
^^^^^^^^^
- Joining/Merging with a non unique ``PeriodIndex`` raised a ``TypeError`` (:issue:`16871`)
- Bug in :func:`crosstab` where non-aligned series of integers were casted to float (:issue:`17005`)
- Bug in merging with categorical dtypes with datetimelikes incorrectly raised a ``TypeError`` (:issue:`16900`)
- Bug when using :func:`isin` on a large object series and large comparison array (:issue:`16012`)
- Fixes regression from 0.20, :func:`Series.aggregate` and :func:`DataFrame.aggregate` allow dictionaries as return values again (:issue:`16741`)
- Fixes dtype of result with integer dtype input, from :func:`pivot_table` when called with ``margins=True`` (:issue:`17013`)
- Bug in :func:`crosstab` where passing two ``Series`` with the same name raised a ``KeyError`` (:issue:`13279`)
- :func:`Series.argmin`, :func:`Series.argmax`, and their counterparts on ``DataFrame`` and groupby objects work correctly with floating point data that contains infinite values (:issue:`13595`).
- Bug in :func:`unique` where checking a tuple of strings raised a ``TypeError`` (:issue:`17108`)
- Bug in :func:`concat` where order of result index was unpredictable if it contained non-comparable elements (:issue:`17344`)
- Fixes regression when sorting by multiple columns on a ``datetime64`` dtype ``Series`` with ``NaT`` values (:issue:`16836`)
- Bug in :func:`pivot_table` where the result's columns did not preserve the categorical dtype of ``columns`` when ``dropna`` was ``False`` (:issue:`17842`)
- Bug in ``DataFrame.drop_duplicates`` where dropping with non-unique column names raised a ``ValueError`` (:issue:`17836`)
- Bug in :func:`unstack` which, when called on a list of levels, would discard the ``fillna`` argument (:issue:`13971`)
- Bug in the alignment of ``range`` objects and other list-likes with ``DataFrame`` leading to operations being performed row-wise instead of column-wise (:issue:`17901`)

Numeric
^^^^^^^
- Bug in ``.clip()`` with ``axis=1`` and a list-like for ``threshold`` is passed; previously this raised ``ValueError`` (:issue:`15390`)
- :func:`Series.clip()` and :func:`DataFrame.clip()` now treat NA values for upper and lower arguments as ``None`` instead of raising ``ValueError`` (:issue:`17276`).


Categorical
^^^^^^^^^^^
- Bug in :func:`Series.isin` when called with a categorical (:issue:`16639`)
- Bug in the categorical constructor with empty values and categories causing the ``.categories`` to be an empty ``Float64Index`` rather than an empty ``Index`` with object dtype (:issue:`17248`)
- Bug in categorical operations with :ref:`Series.cat <categorical.cat>` not preserving the original Series' name (:issue:`17509`)
- Bug in :func:`DataFrame.merge` failing for categorical columns with boolean/int data types (:issue:`17187`)
- Bug in constructing a ``Categorical``/``CategoricalDtype`` when the specified ``categories`` are of categorical type (:issue:`17884`).

.. _whatsnew_0210.pypy:

PyPy
^^^^

- Compatibility with PyPy in :func:`read_csv` with ``usecols=[<unsorted ints>]`` and
  :func:`read_json` (:issue:`17351`)
- Split tests into cases for CPython and PyPy where needed, which highlights the fragility
  of index matching with ``float('nan')``, ``np.nan`` and ``NAT`` (:issue:`17351`)
- Fix :func:`DataFrame.memory_usage` to support PyPy. Objects on PyPy do not have a fixed size,
  so an approximation is used instead (:issue:`17228`)

Other
^^^^^
- Bug where some inplace operators were not being wrapped and produced a copy when invoked (:issue:`12962`)
- Bug in :func:`eval` where the ``inplace`` parameter was being incorrectly handled (:issue:`16732`)



.. _whatsnew_0.21.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.20.3..v0.21.0
.. _whatsnew_0211:

Version 0.21.1 (December 12, 2017)
----------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a minor bug-fix release in the 0.21.x series and includes some small regression fixes,
bug fixes and performance improvements.
We recommend that all users upgrade to this version.

Highlights include:

- Temporarily restore matplotlib datetime plotting functionality. This should
  resolve issues for users who implicitly relied on pandas to plot datetimes
  with matplotlib. See :ref:`here <whatsnew_0211.converters>`.
- Improvements to the Parquet IO functions introduced in 0.21.0. See
  :ref:`here <whatsnew_0211.enhancements.parquet>`.


.. contents:: What's new in v0.21.1
    :local:
    :backlinks: none


.. _whatsnew_0211.converters:

Restore Matplotlib datetime converter registration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

pandas implements some matplotlib converters for nicely formatting the axis
labels on plots with ``datetime`` or ``Period`` values. Prior to pandas 0.21.0,
these were implicitly registered with matplotlib, as a side effect of ``import
pandas``.

In pandas 0.21.0, we required users to explicitly register the
converter. This caused problems for some users who relied on those converters
being present for regular ``matplotlib.pyplot`` plotting methods, so we're
temporarily reverting that change; pandas 0.21.1 again registers the converters on
import, just like before 0.21.0.

We've added a new option to control the converters:
``pd.options.plotting.matplotlib.register_converters``. By default, they are
registered. Toggling this to ``False`` removes pandas' formatters and restore
any converters we overwrote when registering them (:issue:`18301`).

We're working with the matplotlib developers to make this easier. We're trying
to balance user convenience (automatically registering the converters) with
import performance and best practices (importing pandas shouldn't have the side
effect of overwriting any custom converters you've already set). In the future
we hope to have most of the datetime formatting functionality in matplotlib,
with just the pandas-specific converters in pandas. We'll then gracefully
deprecate the automatic registration of converters in favor of users explicitly
registering them when they want them.

.. _whatsnew_0211.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0211.enhancements.parquet:

Improvements to the Parquet IO functionality
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- :func:`DataFrame.to_parquet` will now write non-default indexes when the
  underlying engine supports it. The indexes will be preserved when reading
  back in with :func:`read_parquet` (:issue:`18581`).
- :func:`read_parquet` now allows to specify the columns to read from a parquet file (:issue:`18154`)
- :func:`read_parquet` now allows to specify kwargs which are passed to the respective engine (:issue:`18216`)

.. _whatsnew_0211.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- :meth:`Timestamp.timestamp` is now available in Python 2.7. (:issue:`17329`)
- :class:`Grouper` and :class:`TimeGrouper` now have a friendly repr output (:issue:`18203`).

.. _whatsnew_0211.deprecations:

Deprecations
~~~~~~~~~~~~

- ``pandas.tseries.register`` has been renamed to
  :func:`pandas.plotting.register_matplotlib_converters` (:issue:`18301`)

.. _whatsnew_0211.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of plotting large series/dataframes (:issue:`18236`).

.. _whatsnew_0211.bug_fixes:

Bug fixes
~~~~~~~~~

Conversion
^^^^^^^^^^

- Bug in :class:`TimedeltaIndex` subtraction could incorrectly overflow when ``NaT`` is present (:issue:`17791`)
- Bug in :class:`DatetimeIndex` subtracting datetimelike from DatetimeIndex could fail to overflow (:issue:`18020`)
- Bug in :meth:`IntervalIndex.copy` when copying and ``IntervalIndex`` with non-default ``closed`` (:issue:`18339`)
- Bug in :func:`DataFrame.to_dict` where columns of datetime that are tz-aware were not converted to required arrays when used with ``orient='records'``, raising ``TypeError`` (:issue:`18372`)
- Bug in :class:`DateTimeIndex` and :meth:`date_range` where mismatching tz-aware ``start`` and ``end`` timezones would not raise an err if ``end.tzinfo`` is None (:issue:`18431`)
- Bug in :meth:`Series.fillna` which raised when passed a long integer on Python 2 (:issue:`18159`).

Indexing
^^^^^^^^

- Bug in a boolean comparison of a ``datetime.datetime`` and a ``datetime64[ns]`` dtype Series (:issue:`17965`)
- Bug where a ``MultiIndex`` with more than a million records was not raising ``AttributeError`` when trying to access a missing attribute (:issue:`18165`)
- Bug in :class:`IntervalIndex` constructor when a list of intervals is passed with non-default ``closed`` (:issue:`18334`)
- Bug in ``Index.putmask`` when an invalid mask passed (:issue:`18368`)
- Bug in masked assignment of a ``timedelta64[ns]`` dtype ``Series``, incorrectly coerced to float (:issue:`18493`)

IO
^^

- Bug in class:`~pandas.io.stata.StataReader` not converting date/time columns with display formatting addressed (:issue:`17990`). Previously columns with display formatting were normally left as ordinal numbers and not converted to datetime objects.
- Bug in :func:`read_csv` when reading a compressed UTF-16 encoded file (:issue:`18071`)
- Bug in :func:`read_csv` for handling null values in index columns when specifying ``na_filter=False`` (:issue:`5239`)
- Bug in :func:`read_csv` when reading numeric category fields with high cardinality (:issue:`18186`)
- Bug in :meth:`DataFrame.to_csv` when the table had ``MultiIndex`` columns, and a list of strings was passed in for ``header`` (:issue:`5539`)
- Bug in parsing integer datetime-like columns with specified format in ``read_sql`` (:issue:`17855`).
- Bug in :meth:`DataFrame.to_msgpack` when serializing data of the ``numpy.bool_`` datatype (:issue:`18390`)
- Bug in :func:`read_json` not decoding when reading line delimited JSON from S3 (:issue:`17200`)
- Bug in :func:`pandas.io.json.json_normalize` to avoid modification of ``meta`` (:issue:`18610`)
- Bug in :func:`to_latex` where repeated MultiIndex values were not printed even though a higher level index differed from the previous row (:issue:`14484`)
- Bug when reading NaN-only categorical columns in :class:`HDFStore` (:issue:`18413`)
- Bug in :meth:`DataFrame.to_latex` with ``longtable=True`` where a latex multicolumn always spanned over three columns (:issue:`17959`)

Plotting
^^^^^^^^

- Bug in ``DataFrame.plot()`` and ``Series.plot()`` with :class:`DatetimeIndex` where a figure generated by them is not pickleable in Python 3 (:issue:`18439`)

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in ``DataFrame.resample(...).apply(...)`` when there is a callable that returns different columns (:issue:`15169`)
- Bug in ``DataFrame.resample(...)`` when there is a time change (DST) and resampling frequency is 12h or higher (:issue:`15549`)
- Bug in ``pd.DataFrameGroupBy.count()`` when counting over a datetimelike column (:issue:`13393`)
- Bug in ``rolling.var`` where calculation is inaccurate with a zero-valued array (:issue:`18430`)

Reshaping
^^^^^^^^^

- Error message in ``pd.merge_asof()`` for key datatype mismatch now includes datatype of left and right key (:issue:`18068`)
- Bug in ``pd.concat`` when empty and non-empty DataFrames or Series are concatenated (:issue:`18178` :issue:`18187`)
- Bug in ``DataFrame.filter(...)`` when :class:`unicode` is passed as a condition in Python 2 (:issue:`13101`)
- Bug when merging empty DataFrames when ``np.seterr(divide='raise')`` is set (:issue:`17776`)

Numeric
^^^^^^^

- Bug in ``pd.Series.rolling.skew()`` and ``rolling.kurt()`` with all equal values has floating issue (:issue:`18044`)

Categorical
^^^^^^^^^^^

- Bug in :meth:`DataFrame.astype` where casting to 'category' on an empty ``DataFrame`` causes a segmentation fault (:issue:`18004`)
- Error messages in the testing module have been improved when items have different ``CategoricalDtype`` (:issue:`18069`)
- ``CategoricalIndex`` can now correctly take a ``pd.api.types.CategoricalDtype`` as its dtype (:issue:`18116`)
- Bug in ``Categorical.unique()`` returning read-only ``codes``  array when all categories were ``NaN`` (:issue:`18051`)
- Bug in ``DataFrame.groupby(axis=1)`` with a ``CategoricalIndex`` (:issue:`18432`)

String
^^^^^^

- :meth:`Series.str.split()` will now propagate ``NaN`` values across all expanded columns instead of ``None`` (:issue:`18450`)


.. _whatsnew_0.21.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.21.0..v0.21.1
.. _whatsnew_0203:

Version 0.20.3 (July 7, 2017)
-----------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a minor bug-fix release in the 0.20.x series and includes some small regression fixes
and bug fixes. We recommend that all users upgrade to this version.

.. contents:: What's new in v0.20.3
    :local:
    :backlinks: none

.. _whatsnew_0203.bug_fixes:

Bug fixes
~~~~~~~~~

- Fixed a bug in failing to compute rolling computations of a column-MultiIndexed ``DataFrame`` (:issue:`16789`, :issue:`16825`)
- Fixed a pytest marker failing downstream packages' tests suites (:issue:`16680`)

Conversion
^^^^^^^^^^

- Bug in pickle compat prior to the v0.20.x series, when ``UTC`` is a timezone in a Series/DataFrame/Index (:issue:`16608`)
- Bug in ``Series`` construction when passing a ``Series`` with ``dtype='category'`` (:issue:`16524`).
- Bug in :meth:`DataFrame.astype` when passing a ``Series`` as the ``dtype`` kwarg. (:issue:`16717`).

Indexing
^^^^^^^^

- Bug in ``Float64Index`` causing an empty array instead of ``None`` to be returned from ``.get(np.nan)`` on a Series whose index did not contain any ``NaN`` s (:issue:`8569`)
- Bug in ``MultiIndex.isin`` causing an error when passing an empty iterable (:issue:`16777`)
- Fixed a bug in a slicing DataFrame/Series that have a  ``TimedeltaIndex`` (:issue:`16637`)

IO
^^

- Bug in :func:`read_csv` in which files weren't opened as binary files by the C engine on Windows, causing EOF characters mid-field, which would fail (:issue:`16039`, :issue:`16559`, :issue:`16675`)
- Bug in :func:`read_hdf` in which reading a ``Series`` saved to an HDF file in 'fixed' format fails when an explicit ``mode='r'`` argument is supplied (:issue:`16583`)
- Bug in :meth:`DataFrame.to_latex` where ``bold_rows`` was wrongly specified to be ``True`` by default, whereas in reality row labels remained non-bold whatever parameter provided. (:issue:`16707`)
- Fixed an issue with :meth:`DataFrame.style` where generated element ids were not unique (:issue:`16780`)
- Fixed loading a ``DataFrame`` with a ``PeriodIndex``, from a ``format='fixed'`` HDFStore, in Python 3, that was written in Python 2 (:issue:`16781`)

Plotting
^^^^^^^^

- Fixed regression that prevented RGB and RGBA tuples from being used as color arguments (:issue:`16233`)
- Fixed an issue with :meth:`DataFrame.plot.scatter` that incorrectly raised a ``KeyError`` when categorical data is used for plotting (:issue:`16199`)

Reshaping
^^^^^^^^^

- ``PeriodIndex`` / ``TimedeltaIndex.join`` was missing the ``sort=`` kwarg (:issue:`16541`)
- Bug in joining on a ``MultiIndex`` with a ``category`` dtype for a level (:issue:`16627`).
- Bug in :func:`merge` when merging/joining with multiple categorical columns (:issue:`16767`)

Categorical
^^^^^^^^^^^

- Bug in ``DataFrame.sort_values`` not respecting the ``kind`` parameter with categorical data (:issue:`16793`)


.. _whatsnew_0.20.3.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.20.2..v0.20.3
.. _whatsnew_0250:

What's new in 0.25.0 (July 18, 2019)
------------------------------------

.. warning::

   Starting with the 0.25.x series of releases, pandas only supports Python 3.5.3 and higher.
   See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more details.

.. warning::

   The minimum supported Python version will be bumped to 3.6 in a future release.

.. warning::

   ``Panel`` has been fully removed. For N-D labeled data structures, please
   use `xarray <https://xarray.pydata.org/en/stable/>`_

.. warning::

   :func:`read_pickle` and :func:`read_msgpack` are only guaranteed backwards compatible back to
   pandas version 0.20.3 (:issue:`27082`)

{{ header }}

These are the changes in pandas 0.25.0. See :ref:`release` for a full changelog
including other versions of pandas.


Enhancements
~~~~~~~~~~~~

.. _whatsnew_0250.enhancements.agg_relabel:

GroupBy aggregation with relabeling
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas has added special groupby behavior, known as "named aggregation", for naming the
output columns when applying multiple aggregation functions to specific columns (:issue:`18366`, :issue:`26512`).

.. ipython:: python

   animals = pd.DataFrame({'kind': ['cat', 'dog', 'cat', 'dog'],
                           'height': [9.1, 6.0, 9.5, 34.0],
                           'weight': [7.9, 7.5, 9.9, 198.0]})
   animals
   animals.groupby("kind").agg(
       min_height=pd.NamedAgg(column='height', aggfunc='min'),
       max_height=pd.NamedAgg(column='height', aggfunc='max'),
       average_weight=pd.NamedAgg(column='weight', aggfunc=np.mean),
   )

Pass the desired columns names as the ``**kwargs`` to ``.agg``. The values of ``**kwargs``
should be tuples where the first element is the column selection, and the second element is the
aggregation function to apply. pandas provides the ``pandas.NamedAgg`` namedtuple to make it clearer
what the arguments to the function are, but plain tuples are accepted as well.

.. ipython:: python

   animals.groupby("kind").agg(
       min_height=('height', 'min'),
       max_height=('height', 'max'),
       average_weight=('weight', np.mean),
   )

Named aggregation is the recommended replacement for the deprecated "dict-of-dicts"
approach to naming the output of column-specific aggregations (:ref:`whatsnew_0200.api_breaking.deprecate_group_agg_dict`).

A similar approach is now available for Series groupby objects as well. Because there's no need for
column selection, the values can just be the functions to apply

.. ipython:: python

   animals.groupby("kind").height.agg(
       min_height="min",
       max_height="max",
   )


This type of aggregation is the recommended alternative to the deprecated behavior when passing
a dict to a Series groupby aggregation (:ref:`whatsnew_0200.api_breaking.deprecate_group_agg_dict`).

See :ref:`groupby.aggregate.named` for more.

.. _whatsnew_0250.enhancements.multiple_lambdas:

GroupBy aggregation with multiple lambdas
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can now provide multiple lambda functions to a list-like aggregation in
:class:`pandas.core.groupby.GroupBy.agg` (:issue:`26430`).

.. ipython:: python

   animals.groupby('kind').height.agg([
       lambda x: x.iloc[0], lambda x: x.iloc[-1]
   ])

   animals.groupby('kind').agg([
       lambda x: x.iloc[0] - x.iloc[1],
       lambda x: x.iloc[0] + x.iloc[1]
   ])

Previously, these raised a ``SpecificationError``.

.. _whatsnew_0250.enhancements.multi_index_repr:

Better repr for MultiIndex
^^^^^^^^^^^^^^^^^^^^^^^^^^

Printing of :class:`MultiIndex` instances now shows tuples of each row and ensures
that the tuple items are vertically aligned, so it's now easier to understand
the structure of the ``MultiIndex``. (:issue:`13480`):

The repr now looks like this:

.. ipython:: python

   pd.MultiIndex.from_product([['a', 'abc'], range(500)])

Previously, outputting a :class:`MultiIndex` printed all the ``levels`` and
``codes`` of the ``MultiIndex``, which was visually unappealing and made
the output more difficult to navigate. For example (limiting the range to 5):

.. code-block:: ipython

   In [1]: pd.MultiIndex.from_product([['a', 'abc'], range(5)])
   Out[1]: MultiIndex(levels=[['a', 'abc'], [0, 1, 2, 3]],
      ...:            codes=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 2, 3, 0, 1, 2, 3]])

In the new repr, all values will be shown, if the number of rows is smaller
than :attr:`options.display.max_seq_items` (default: 100 items). Horizontally,
the output will truncate, if it's wider than :attr:`options.display.width`
(default: 80 characters).

.. _whatsnew_0250.enhancements.shorter_truncated_repr:

Shorter truncated repr for Series and DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Currently, the default display options of pandas ensure that when a Series
or DataFrame has more than 60 rows, its repr gets truncated to this maximum
of 60 rows (the ``display.max_rows`` option). However, this still gives
a repr that takes up a large part of the vertical screen estate. Therefore,
a new option ``display.min_rows`` is introduced with a default of 10 which
determines the number of rows showed in the truncated repr:

- For small Series or DataFrames, up to ``max_rows`` number of rows is shown
  (default: 60).
- For larger Series of DataFrame with a length above ``max_rows``, only
  ``min_rows`` number of rows is shown (default: 10, i.e. the first and last
  5 rows).

This dual option allows to still see the full content of relatively small
objects (e.g. ``df.head(20)`` shows all 20 rows), while giving a brief repr
for large objects.

To restore the previous behaviour of a single threshold, set
``pd.options.display.min_rows = None``.

.. _whatsnew_0250.enhancements.json_normalize_with_max_level:

JSON normalize with max_level param support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`json_normalize` normalizes the provided input dict to all
nested levels. The new max_level parameter provides more control over
which level to end normalization (:issue:`23843`):

The repr now looks like this:

.. code-block:: ipython

    from pandas.io.json import json_normalize
    data = [{
        'CreatedBy': {'Name': 'User001'},
        'Lookup': {'TextField': 'Some text',
                   'UserField': {'Id': 'ID001', 'Name': 'Name001'}},
        'Image': {'a': 'b'}
    }]
    json_normalize(data, max_level=1)


.. _whatsnew_0250.enhancements.explode:

Series.explode to split list-like values to rows
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Series` and :class:`DataFrame` have gained the :meth:`DataFrame.explode` methods to transform list-likes to individual rows. See :ref:`section on Exploding list-like column <reshaping.explode>` in docs for more information (:issue:`16538`, :issue:`10511`)


Here is a typical usecase. You have comma separated string in a column.

.. ipython:: python

    df = pd.DataFrame([{'var1': 'a,b,c', 'var2': 1},
                       {'var1': 'd,e,f', 'var2': 2}])
    df

Creating a long form ``DataFrame`` is now straightforward using chained operations

.. ipython:: python

    df.assign(var1=df.var1.str.split(',')).explode('var1')

.. _whatsnew_0250.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^
- :func:`DataFrame.plot` keywords ``logy``, ``logx`` and ``loglog`` can now accept the value ``'sym'`` for symlog scaling. (:issue:`24867`)
- Added support for ISO week year format ('%G-%V-%u') when parsing datetimes using :meth:`to_datetime` (:issue:`16607`)
- Indexing of ``DataFrame`` and ``Series`` now accepts zerodim ``np.ndarray`` (:issue:`24919`)
- :meth:`Timestamp.replace` now supports the ``fold`` argument to disambiguate DST transition times (:issue:`25017`)
- :meth:`DataFrame.at_time` and :meth:`Series.at_time` now support :class:`datetime.time` objects with timezones (:issue:`24043`)
- :meth:`DataFrame.pivot_table` now accepts an ``observed`` parameter which is passed to underlying calls to :meth:`DataFrame.groupby` to speed up grouping categorical data. (:issue:`24923`)
- ``Series.str`` has gained :meth:`Series.str.casefold` method to removes all case distinctions present in a string (:issue:`25405`)
- :meth:`DataFrame.set_index` now works for instances of ``abc.Iterator``, provided their output is of the same length as the calling frame (:issue:`22484`, :issue:`24984`)
- :meth:`DatetimeIndex.union` now supports the ``sort`` argument. The behavior of the sort parameter matches that of :meth:`Index.union` (:issue:`24994`)
- :meth:`RangeIndex.union` now supports the ``sort`` argument. If ``sort=False`` an unsorted ``Int64Index`` is always returned. ``sort=None`` is the default and returns a monotonically increasing ``RangeIndex`` if possible or a sorted ``Int64Index`` if not (:issue:`24471`)
- :meth:`TimedeltaIndex.intersection` now also supports the ``sort`` keyword (:issue:`24471`)
- :meth:`DataFrame.rename` now supports the ``errors`` argument to raise errors when attempting to rename nonexistent keys (:issue:`13473`)
- Added :ref:`api.frame.sparse` for working with a ``DataFrame`` whose values are sparse (:issue:`25681`)
- :class:`RangeIndex` has gained :attr:`~RangeIndex.start`, :attr:`~RangeIndex.stop`, and :attr:`~RangeIndex.step` attributes (:issue:`25710`)
- :class:`datetime.timezone` objects are now supported as arguments to timezone methods and constructors (:issue:`25065`)
- :meth:`DataFrame.query` and :meth:`DataFrame.eval` now supports quoting column names with backticks to refer to names with spaces (:issue:`6508`)
- :func:`merge_asof` now gives a more clear error message when merge keys are categoricals that are not equal (:issue:`26136`)
- :meth:`pandas.core.window.Rolling` supports exponential (or Poisson) window type (:issue:`21303`)
- Error message for missing required imports now includes the original import error's text (:issue:`23868`)
- :class:`DatetimeIndex` and :class:`TimedeltaIndex` now have a ``mean`` method (:issue:`24757`)
- :meth:`DataFrame.describe` now formats integer percentiles without decimal point (:issue:`26660`)
- Added support for reading SPSS .sav files using :func:`read_spss` (:issue:`26537`)
- Added new option ``plotting.backend`` to be able to select a plotting backend different than the existing ``matplotlib`` one. Use ``pandas.set_option('plotting.backend', '<backend-module>')`` where ``<backend-module`` is a library implementing the pandas plotting API (:issue:`14130`)
- :class:`pandas.offsets.BusinessHour` supports multiple opening hours intervals (:issue:`15481`)
- :func:`read_excel` can now use ``openpyxl`` to read Excel files via the ``engine='openpyxl'`` argument. This will become the default in a future release (:issue:`11499`)
- :func:`pandas.io.excel.read_excel` supports reading OpenDocument tables. Specify ``engine='odf'`` to enable. Consult the :ref:`IO User Guide <io.ods>` for more details (:issue:`9070`)
- :class:`Interval`, :class:`IntervalIndex`, and :class:`~arrays.IntervalArray` have gained an :attr:`~Interval.is_empty` attribute denoting if the given interval(s) are empty (:issue:`27219`)

.. _whatsnew_0250.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0250.api_breaking.utc_offset_indexing:


Indexing with date strings with UTC offsets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Indexing a :class:`DataFrame` or :class:`Series` with a :class:`DatetimeIndex` with a
date string with a UTC offset would previously ignore the UTC offset. Now, the UTC offset
is respected in indexing. (:issue:`24076`, :issue:`16785`)

.. ipython:: python

    df = pd.DataFrame([0], index=pd.DatetimeIndex(['2019-01-01'], tz='US/Pacific'))
    df

*Previous behavior*:

.. code-block:: ipython

    In [3]: df['2019-01-01 00:00:00+04:00':'2019-01-01 01:00:00+04:00']
    Out[3]:
                               0
    2019-01-01 00:00:00-08:00  0

*New behavior*:

.. ipython:: python

    df['2019-01-01 12:00:00+04:00':'2019-01-01 13:00:00+04:00']


.. _whatsnew_0250.api_breaking.multi_indexing:


``MultiIndex`` constructed from levels and codes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Constructing a :class:`MultiIndex` with ``NaN`` levels or codes value < -1 was allowed previously.
Now, construction with codes value < -1 is not allowed and ``NaN`` levels' corresponding codes
would be reassigned as -1. (:issue:`19387`)

*Previous behavior*:

.. code-block:: ipython

    In [1]: pd.MultiIndex(levels=[[np.nan, None, pd.NaT, 128, 2]],
       ...:               codes=[[0, -1, 1, 2, 3, 4]])
       ...:
    Out[1]: MultiIndex(levels=[[nan, None, NaT, 128, 2]],
                       codes=[[0, -1, 1, 2, 3, 4]])

    In [2]: pd.MultiIndex(levels=[[1, 2]], codes=[[0, -2]])
    Out[2]: MultiIndex(levels=[[1, 2]],
                       codes=[[0, -2]])

*New behavior*:

.. ipython:: python
    :okexcept:

    pd.MultiIndex(levels=[[np.nan, None, pd.NaT, 128, 2]],
                  codes=[[0, -1, 1, 2, 3, 4]])
    pd.MultiIndex(levels=[[1, 2]], codes=[[0, -2]])


.. _whatsnew_0250.api_breaking.groupby_apply_first_group_once:

``GroupBy.apply`` on ``DataFrame`` evaluates first group only once
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The implementation of :meth:`DataFrameGroupBy.apply() <pandas.core.groupby.DataFrameGroupBy.apply>`
previously evaluated the supplied function consistently twice on the first group
to infer if it is safe to use a fast code path. Particularly for functions with
side effects, this was an undesired behavior and may have led to surprises. (:issue:`2936`, :issue:`2656`, :issue:`7739`, :issue:`10519`, :issue:`12155`, :issue:`20084`, :issue:`21417`)

Now every group is evaluated only a single time.

.. ipython:: python

    df = pd.DataFrame({"a": ["x", "y"], "b": [1, 2]})
    df

    def func(group):
        print(group.name)
        return group

*Previous behavior*:

.. code-block:: python

   In [3]: df.groupby('a').apply(func)
   x
   x
   y
   Out[3]:
      a  b
   0  x  1
   1  y  2

*New behavior*:

.. ipython:: python

    df.groupby("a").apply(func)


Concatenating sparse values
^^^^^^^^^^^^^^^^^^^^^^^^^^^

When passed DataFrames whose values are sparse, :func:`concat` will now return a
:class:`Series` or :class:`DataFrame` with sparse values, rather than a :class:`SparseDataFrame` (:issue:`25702`).

.. ipython:: python
   :okwarning:

   df = pd.DataFrame({"A": pd.SparseArray([0, 1])})

*Previous behavior*:

.. code-block:: ipython

   In [2]: type(pd.concat([df, df]))
   pandas.core.sparse.frame.SparseDataFrame

*New behavior*:

.. ipython:: python

   type(pd.concat([df, df]))


This now matches the existing behavior of :class:`concat` on ``Series`` with sparse values.
:func:`concat` will continue to return a ``SparseDataFrame`` when all the values
are instances of ``SparseDataFrame``.

This change also affects routines using :func:`concat` internally, like :func:`get_dummies`,
which now returns a :class:`DataFrame` in all cases (previously a ``SparseDataFrame`` was
returned if all the columns were dummy encoded, and a :class:`DataFrame` otherwise).

Providing any ``SparseSeries`` or ``SparseDataFrame`` to :func:`concat` will
cause a ``SparseSeries`` or ``SparseDataFrame`` to be returned, as before.

The ``.str``-accessor performs stricter type checks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Due to the lack of more fine-grained dtypes, :attr:`Series.str` so far only checked whether the data was
of ``object`` dtype. :attr:`Series.str` will now infer the dtype data *within* the Series; in particular,
``'bytes'``-only data will raise an exception (except for :meth:`Series.str.decode`, :meth:`Series.str.get`,
:meth:`Series.str.len`, :meth:`Series.str.slice`), see :issue:`23163`, :issue:`23011`, :issue:`23551`.

*Previous behavior*:

.. code-block:: python

    In [1]: s = pd.Series(np.array(['a', 'ba', 'cba'], 'S'), dtype=object)

    In [2]: s
    Out[2]:
    0      b'a'
    1     b'ba'
    2    b'cba'
    dtype: object

    In [3]: s.str.startswith(b'a')
    Out[3]:
    0     True
    1    False
    2    False
    dtype: bool

*New behavior*:

.. ipython:: python
    :okexcept:

    s = pd.Series(np.array(['a', 'ba', 'cba'], 'S'), dtype=object)
    s
    s.str.startswith(b'a')

.. _whatsnew_0250.api_breaking.groupby_categorical:

Categorical dtypes are preserved during GroupBy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, columns that were categorical, but not the groupby key(s) would be converted to ``object`` dtype during groupby operations. pandas now will preserve these dtypes. (:issue:`18502`)

.. ipython:: python

   cat = pd.Categorical(["foo", "bar", "bar", "qux"], ordered=True)
   df = pd.DataFrame({'payload': [-1, -2, -1, -2], 'col': cat})
   df
   df.dtypes

*Previous Behavior*:

.. code-block:: python

   In [5]: df.groupby('payload').first().col.dtype
   Out[5]: dtype('O')

*New Behavior*:

.. ipython:: python

   df.groupby('payload').first().col.dtype


.. _whatsnew_0250.api_breaking.incompatible_index_unions:

Incompatible Index type unions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When performing :func:`Index.union` operations between objects of incompatible dtypes,
the result will be a base :class:`Index` of dtype ``object``. This behavior holds true for
unions between :class:`Index` objects that previously would have been prohibited. The dtype
of empty :class:`Index` objects will now be evaluated before performing union operations
rather than simply returning the other :class:`Index` object. :func:`Index.union` can now be
considered commutative, such that ``A.union(B) == B.union(A)`` (:issue:`23525`).

*Previous behavior*:

.. code-block:: python

    In [1]: pd.period_range('19910905', periods=2).union(pd.Int64Index([1, 2, 3]))
    ...
    ValueError: can only call with other PeriodIndex-ed objects

    In [2]: pd.Index([], dtype=object).union(pd.Index([1, 2, 3]))
    Out[2]: Int64Index([1, 2, 3], dtype='int64')

*New behavior*:

.. code-block:: python

    In [3]: pd.period_range('19910905', periods=2).union(pd.Int64Index([1, 2, 3]))
    Out[3]: Index([1991-09-05, 1991-09-06, 1, 2, 3], dtype='object')
    In [4]: pd.Index([], dtype=object).union(pd.Index([1, 2, 3]))
    Out[4]: Index([1, 2, 3], dtype='object')

Note that integer- and floating-dtype indexes are considered "compatible". The integer
values are coerced to floating point, which may result in loss of precision. See
:ref:`indexing.set_ops` for more.


``DataFrame`` GroupBy ffill/bfill no longer return group labels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The methods ``ffill``, ``bfill``, ``pad`` and ``backfill`` of
:class:`DataFrameGroupBy <pandas.core.groupby.DataFrameGroupBy>`
previously included the group labels in the return value, which was
inconsistent with other groupby transforms. Now only the filled values
are returned. (:issue:`21521`)

.. ipython:: python

    df = pd.DataFrame({"a": ["x", "y"], "b": [1, 2]})
    df

*Previous behavior*:

.. code-block:: python

   In [3]: df.groupby("a").ffill()
   Out[3]:
      a  b
   0  x  1
   1  y  2

*New behavior*:

.. ipython:: python

    df.groupby("a").ffill()

``DataFrame`` describe on an empty Categorical / object column will return top and freq
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When calling :meth:`DataFrame.describe` with an empty categorical / object
column, the 'top' and 'freq' columns were previously omitted, which was inconsistent with
the output for non-empty columns. Now the 'top' and 'freq' columns will always be included,
with :attr:`numpy.nan` in the case of an empty :class:`DataFrame` (:issue:`26397`)

.. ipython:: python

   df = pd.DataFrame({"empty_col": pd.Categorical([])})
   df

*Previous behavior*:

.. code-block:: python

   In [3]: df.describe()
   Out[3]:
           empty_col
   count           0
   unique          0

*New behavior*:

.. ipython:: python

   df.describe()

``__str__`` methods now call ``__repr__`` rather than vice versa
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas has until now mostly defined string representations in a pandas objects'
``__str__``/``__unicode__``/``__bytes__`` methods, and called ``__str__`` from the ``__repr__``
method, if a specific ``__repr__`` method is not found. This is not needed for Python3.
In pandas 0.25, the string representations of pandas objects are now generally
defined in ``__repr__``, and calls to ``__str__`` in general now pass the call on to
the ``__repr__``, if a specific ``__str__`` method doesn't exist, as is standard for Python.
This change is backward compatible for direct usage of pandas, but if you subclass
pandas objects *and* give your subclasses specific ``__str__``/``__repr__`` methods,
you may have to adjust your ``__str__``/``__repr__`` methods (:issue:`26495`).

.. _whatsnew_0250.api_breaking.interval_indexing:


Indexing an ``IntervalIndex`` with ``Interval`` objects
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Indexing methods for :class:`IntervalIndex` have been modified to require exact matches only for :class:`Interval` queries.
``IntervalIndex`` methods previously matched on any overlapping ``Interval``.  Behavior with scalar points, e.g. querying
with an integer, is unchanged (:issue:`16316`).

.. ipython:: python

   ii = pd.IntervalIndex.from_tuples([(0, 4), (1, 5), (5, 8)])
   ii

The ``in`` operator (``__contains__``) now only returns ``True`` for exact matches to ``Intervals`` in the ``IntervalIndex``, whereas
this would previously return ``True`` for any ``Interval`` overlapping an ``Interval`` in the ``IntervalIndex``.

*Previous behavior*:

.. code-block:: python

   In [4]: pd.Interval(1, 2, closed='neither') in ii
   Out[4]: True

   In [5]: pd.Interval(-10, 10, closed='both') in ii
   Out[5]: True

*New behavior*:

.. ipython:: python

   pd.Interval(1, 2, closed='neither') in ii
   pd.Interval(-10, 10, closed='both') in ii

The :meth:`~IntervalIndex.get_loc` method now only returns locations for exact matches to ``Interval`` queries, as opposed to the previous behavior of
returning locations for overlapping matches.  A ``KeyError`` will be raised if an exact match is not found.

*Previous behavior*:

.. code-block:: python

   In [6]: ii.get_loc(pd.Interval(1, 5))
   Out[6]: array([0, 1])

   In [7]: ii.get_loc(pd.Interval(2, 6))
   Out[7]: array([0, 1, 2])

*New behavior*:

.. code-block:: python

   In [6]: ii.get_loc(pd.Interval(1, 5))
   Out[6]: 1

   In [7]: ii.get_loc(pd.Interval(2, 6))
   ---------------------------------------------------------------------------
   KeyError: Interval(2, 6, closed='right')

Likewise, :meth:`~IntervalIndex.get_indexer` and :meth:`~IntervalIndex.get_indexer_non_unique` will also only return locations for exact matches
to ``Interval`` queries, with ``-1`` denoting that an exact match was not found.

These indexing changes extend to querying a :class:`Series` or :class:`DataFrame` with an ``IntervalIndex`` index.

.. ipython:: python

   s = pd.Series(list('abc'), index=ii)
   s

Selecting from a ``Series`` or ``DataFrame`` using ``[]`` (``__getitem__``) or ``loc`` now only returns exact matches for ``Interval`` queries.

*Previous behavior*:

.. code-block:: python

   In [8]: s[pd.Interval(1, 5)]
   Out[8]:
   (0, 4]    a
   (1, 5]    b
   dtype: object

   In [9]: s.loc[pd.Interval(1, 5)]
   Out[9]:
   (0, 4]    a
   (1, 5]    b
   dtype: object

*New behavior*:

.. ipython:: python

   s[pd.Interval(1, 5)]
   s.loc[pd.Interval(1, 5)]

Similarly, a ``KeyError`` will be raised for non-exact matches instead of returning overlapping matches.

*Previous behavior*:

.. code-block:: python

   In [9]: s[pd.Interval(2, 3)]
   Out[9]:
   (0, 4]    a
   (1, 5]    b
   dtype: object

   In [10]: s.loc[pd.Interval(2, 3)]
   Out[10]:
   (0, 4]    a
   (1, 5]    b
   dtype: object

*New behavior*:

.. code-block:: python

   In [6]: s[pd.Interval(2, 3)]
   ---------------------------------------------------------------------------
   KeyError: Interval(2, 3, closed='right')

   In [7]: s.loc[pd.Interval(2, 3)]
   ---------------------------------------------------------------------------
   KeyError: Interval(2, 3, closed='right')

The :meth:`~IntervalIndex.overlaps` method can be used to create a boolean indexer that replicates the
previous behavior of returning overlapping matches.

*New behavior*:

.. ipython:: python

   idxr = s.index.overlaps(pd.Interval(2, 3))
   idxr
   s[idxr]
   s.loc[idxr]


.. _whatsnew_0250.api_breaking.ufunc:

Binary ufuncs on Series now align
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Applying a binary ufunc like :func:`numpy.power` now aligns the inputs
when both are :class:`Series` (:issue:`23293`).

.. ipython:: python

   s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
   s2 = pd.Series([3, 4, 5], index=['d', 'c', 'b'])
   s1
   s2

*Previous behavior*

.. code-block:: ipython

   In [5]: np.power(s1, s2)
   Out[5]:
   a      1
   b     16
   c    243
   dtype: int64

*New behavior*

.. ipython:: python

   np.power(s1, s2)

This matches the behavior of other binary operations in pandas, like :meth:`Series.add`.
To retain the previous behavior, convert the other ``Series`` to an array before
applying the ufunc.

.. ipython:: python

   np.power(s1, s2.array)

Categorical.argsort now places missing values at the end
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`Categorical.argsort` now places missing values at the end of the array, making it
consistent with NumPy and the rest of pandas (:issue:`21801`).

.. ipython:: python

   cat = pd.Categorical(['b', None, 'a'], categories=['a', 'b'], ordered=True)

*Previous behavior*

.. code-block:: ipython

   In [2]: cat = pd.Categorical(['b', None, 'a'], categories=['a', 'b'], ordered=True)

   In [3]: cat.argsort()
   Out[3]: array([1, 2, 0])

   In [4]: cat[cat.argsort()]
   Out[4]:
   [NaN, a, b]
   categories (2, object): [a < b]

*New behavior*

.. ipython:: python

   cat.argsort()
   cat[cat.argsort()]

.. _whatsnew_0250.api_breaking.list_of_dict:

Column order is preserved when passing a list of dicts to DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Starting with Python 3.7 the key-order of ``dict`` is `guaranteed <https://mail.python.org/pipermail/python-dev/2017-December/151283.html>`_. In practice, this has been true since
Python 3.6. The :class:`DataFrame` constructor now treats a list of dicts in the same way as
it does a list of ``OrderedDict``, i.e. preserving the order of the dicts.
This change applies only when pandas is running on Python>=3.6 (:issue:`27309`).

.. ipython:: python

   data = [
       {'name': 'Joe', 'state': 'NY', 'age': 18},
       {'name': 'Jane', 'state': 'KY', 'age': 19, 'hobby': 'Minecraft'},
       {'name': 'Jean', 'state': 'OK', 'age': 20, 'finances': 'good'}
   ]

*Previous Behavior*:

The columns were lexicographically sorted previously,

.. code-block:: python

   In [1]: pd.DataFrame(data)
   Out[1]:
      age finances      hobby  name state
   0   18      NaN        NaN   Joe    NY
   1   19      NaN  Minecraft  Jane    KY
   2   20     good        NaN  Jean    OK

*New Behavior*:

The column order now matches the insertion-order of the keys in the ``dict``,
considering all the records from top to bottom. As a consequence, the column
order of the resulting DataFrame has changed compared to previous pandas versions.

.. ipython:: python

   pd.DataFrame(data)

.. _whatsnew_0250.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Due to dropping support for Python 2.7, a number of optional dependencies have updated minimum versions (:issue:`25725`, :issue:`24942`, :issue:`25752`).
Independently, some minimum supported versions of dependencies were updated (:issue:`23519`, :issue:`25554`).
If installed, we now require:

+-----------------+-----------------+----------+
| Package         | Minimum Version | Required |
+=================+=================+==========+
| numpy           | 1.13.3          |    X     |
+-----------------+-----------------+----------+
| pytz            | 2015.4          |    X     |
+-----------------+-----------------+----------+
| python-dateutil | 2.6.1           |    X     |
+-----------------+-----------------+----------+
| bottleneck      | 1.2.1           |          |
+-----------------+-----------------+----------+
| numexpr         | 2.6.2           |          |
+-----------------+-----------------+----------+
| pytest (dev)    | 4.0.2           |          |
+-----------------+-----------------+----------+

For `optional libraries <https://pandas.pydata.org/docs/getting_started/install.html>`_ the general recommendation is to use the latest version.
The following table lists the lowest version per library that is currently being tested throughout the development of pandas.
Optional libraries below the lowest tested version may still work, but are not considered supported.

+-----------------+-----------------+
| Package         | Minimum Version |
+=================+=================+
| beautifulsoup4  | 4.6.0           |
+-----------------+-----------------+
| fastparquet     | 0.2.1           |
+-----------------+-----------------+
| gcsfs           | 0.2.2           |
+-----------------+-----------------+
| lxml            | 3.8.0           |
+-----------------+-----------------+
| matplotlib      | 2.2.2           |
+-----------------+-----------------+
| openpyxl        | 2.4.8           |
+-----------------+-----------------+
| pyarrow         | 0.9.0           |
+-----------------+-----------------+
| pymysql         | 0.7.1           |
+-----------------+-----------------+
| pytables        | 3.4.2           |
+-----------------+-----------------+
| scipy           | 0.19.0          |
+-----------------+-----------------+
| sqlalchemy      | 1.1.4           |
+-----------------+-----------------+
| xarray          | 0.8.2           |
+-----------------+-----------------+
| xlrd            | 1.1.0           |
+-----------------+-----------------+
| xlsxwriter      | 0.9.8           |
+-----------------+-----------------+
| xlwt            | 1.2.0           |
+-----------------+-----------------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

.. _whatsnew_0250.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- :class:`DatetimeTZDtype` will now standardize pytz timezones to a common timezone instance (:issue:`24713`)
- :class:`Timestamp` and :class:`Timedelta` scalars now implement the :meth:`to_numpy` method as aliases to :meth:`Timestamp.to_datetime64` and :meth:`Timedelta.to_timedelta64`, respectively. (:issue:`24653`)
- :meth:`Timestamp.strptime` will now rise a ``NotImplementedError`` (:issue:`25016`)
- Comparing :class:`Timestamp` with unsupported objects now returns :py:obj:`NotImplemented` instead of raising ``TypeError``. This implies that unsupported rich comparisons are delegated to the other object, and are now consistent with Python 3 behavior for ``datetime`` objects (:issue:`24011`)
- Bug in :meth:`DatetimeIndex.snap` which didn't preserving the ``name`` of the input :class:`Index` (:issue:`25575`)
- The ``arg`` argument in :meth:`pandas.core.groupby.DataFrameGroupBy.agg` has been renamed to ``func`` (:issue:`26089`)
- The ``arg`` argument in :meth:`pandas.core.window._Window.aggregate` has been renamed to ``func`` (:issue:`26372`)
- Most pandas classes had a ``__bytes__`` method, which was used for getting a python2-style bytestring representation of the object. This method has been removed as a part of dropping Python2 (:issue:`26447`)
- The ``.str``-accessor has been disabled for 1-level :class:`MultiIndex`, use :meth:`MultiIndex.to_flat_index` if necessary (:issue:`23679`)
- Removed support of gtk package for clipboards (:issue:`26563`)
- Using an unsupported version of Beautiful Soup 4 will now raise an ``ImportError`` instead of a ``ValueError`` (:issue:`27063`)
- :meth:`Series.to_excel` and :meth:`DataFrame.to_excel` will now raise a ``ValueError`` when saving timezone aware data. (:issue:`27008`, :issue:`7056`)
- :meth:`ExtensionArray.argsort` places NA values at the end of the sorted array. (:issue:`21801`)
- :meth:`DataFrame.to_hdf` and :meth:`Series.to_hdf` will now raise a ``NotImplementedError`` when saving a :class:`MultiIndex` with extension data types for a ``fixed`` format. (:issue:`7775`)
- Passing duplicate ``names`` in :meth:`read_csv` will now raise a ``ValueError`` (:issue:`17346`)

.. _whatsnew_0250.deprecations:

Deprecations
~~~~~~~~~~~~

Sparse subclasses
^^^^^^^^^^^^^^^^^

The ``SparseSeries`` and ``SparseDataFrame`` subclasses are deprecated. Their functionality is better-provided
by a ``Series`` or ``DataFrame`` with sparse values.

**Previous way**

.. code-block:: python

   df = pd.SparseDataFrame({"A": [0, 0, 1, 2]})
   df.dtypes

**New way**

.. ipython:: python
   :okwarning:

   df = pd.DataFrame({"A": pd.SparseArray([0, 0, 1, 2])})
   df.dtypes

The memory usage of the two approaches is identical. See :ref:`sparse.migration` for more (:issue:`19239`).

msgpack format
^^^^^^^^^^^^^^

The msgpack format is deprecated as of 0.25 and will be removed in a future version. It is recommended to use pyarrow for on-the-wire transmission of pandas objects. (:issue:`27084`)


Other deprecations
^^^^^^^^^^^^^^^^^^

- The deprecated ``.ix[]`` indexer now raises a more visible ``FutureWarning`` instead of ``DeprecationWarning`` (:issue:`26438`).
- Deprecated the ``units=M`` (months) and ``units=Y`` (year) parameters for ``units`` of :func:`pandas.to_timedelta`, :func:`pandas.Timedelta` and :func:`pandas.TimedeltaIndex` (:issue:`16344`)
- :meth:`pandas.concat` has deprecated the ``join_axes``-keyword. Instead, use :meth:`DataFrame.reindex` or :meth:`DataFrame.reindex_like` on the result or on the inputs (:issue:`21951`)
- The :attr:`SparseArray.values` attribute is deprecated. You can use ``np.asarray(...)`` or
  the :meth:`SparseArray.to_dense` method instead (:issue:`26421`).
- The functions :func:`pandas.to_datetime` and :func:`pandas.to_timedelta` have deprecated the ``box`` keyword. Instead, use :meth:`to_numpy` or :meth:`Timestamp.to_datetime64` or :meth:`Timedelta.to_timedelta64`. (:issue:`24416`)
- The :meth:`DataFrame.compound` and :meth:`Series.compound` methods are deprecated and will be removed in a future version (:issue:`26405`).
- The internal attributes ``_start``, ``_stop`` and ``_step`` attributes of :class:`RangeIndex` have been deprecated.
  Use the public attributes :attr:`~RangeIndex.start`, :attr:`~RangeIndex.stop` and :attr:`~RangeIndex.step` instead (:issue:`26581`).
- The :meth:`Series.ftype`, :meth:`Series.ftypes` and :meth:`DataFrame.ftypes` methods are deprecated and will be removed in a future version.
  Instead, use :meth:`Series.dtype` and :meth:`DataFrame.dtypes` (:issue:`26705`).
- The :meth:`Series.get_values`, :meth:`DataFrame.get_values`, :meth:`Index.get_values`,
  :meth:`SparseArray.get_values` and :meth:`Categorical.get_values` methods are deprecated.
  One of ``np.asarray(..)`` or :meth:`~Series.to_numpy` can be used instead (:issue:`19617`).
- The 'outer' method on NumPy ufuncs, e.g. ``np.subtract.outer`` has been deprecated on :class:`Series` objects. Convert the input to an array with :attr:`Series.array` first (:issue:`27186`)
- :meth:`Timedelta.resolution` is deprecated and replaced with :meth:`Timedelta.resolution_string`.  In a future version, :meth:`Timedelta.resolution` will be changed to behave like the standard library :attr:`datetime.timedelta.resolution` (:issue:`21344`)
- :func:`read_table` has been undeprecated. (:issue:`25220`)
- :attr:`Index.dtype_str` is deprecated. (:issue:`18262`)
- :attr:`Series.imag` and :attr:`Series.real` are deprecated. (:issue:`18262`)
- :meth:`Series.put` is deprecated. (:issue:`18262`)
- :meth:`Index.item` and :meth:`Series.item` is deprecated. (:issue:`18262`)
- The default value ``ordered=None`` in :class:`~pandas.api.types.CategoricalDtype` has been deprecated in favor of ``ordered=False``. When converting between categorical types ``ordered=True`` must be explicitly passed in order to be preserved. (:issue:`26336`)
- :meth:`Index.contains` is deprecated. Use ``key in index`` (``__contains__``) instead (:issue:`17753`).
- :meth:`DataFrame.get_dtype_counts` is deprecated. (:issue:`18262`)
- :meth:`Categorical.ravel` will return a :class:`Categorical` instead of a ``np.ndarray`` (:issue:`27199`)


.. _whatsnew_0250.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Removed ``Panel`` (:issue:`25047`, :issue:`25191`, :issue:`25231`)
- Removed the previously deprecated ``sheetname`` keyword in :func:`read_excel` (:issue:`16442`, :issue:`20938`)
- Removed the previously deprecated ``TimeGrouper`` (:issue:`16942`)
- Removed the previously deprecated ``parse_cols`` keyword in :func:`read_excel` (:issue:`16488`)
- Removed the previously deprecated ``pd.options.html.border`` (:issue:`16970`)
- Removed the previously deprecated ``convert_objects`` (:issue:`11221`)
- Removed the previously deprecated ``select`` method of ``DataFrame`` and ``Series`` (:issue:`17633`)
- Removed the previously deprecated behavior of :class:`Series` treated as list-like in :meth:`~Series.cat.rename_categories` (:issue:`17982`)
- Removed the previously deprecated ``DataFrame.reindex_axis`` and ``Series.reindex_axis`` (:issue:`17842`)
- Removed the previously deprecated behavior of altering column or index labels with :meth:`Series.rename_axis` or :meth:`DataFrame.rename_axis` (:issue:`17842`)
- Removed the previously deprecated ``tupleize_cols`` keyword argument in :meth:`read_html`, :meth:`read_csv`, and :meth:`DataFrame.to_csv` (:issue:`17877`, :issue:`17820`)
- Removed the previously deprecated ``DataFrame.from.csv`` and ``Series.from_csv`` (:issue:`17812`)
- Removed the previously deprecated ``raise_on_error`` keyword argument in :meth:`DataFrame.where` and :meth:`DataFrame.mask` (:issue:`17744`)
- Removed the previously deprecated ``ordered`` and ``categories`` keyword arguments in ``astype`` (:issue:`17742`)
- Removed the previously deprecated ``cdate_range`` (:issue:`17691`)
- Removed the previously deprecated ``True`` option for the ``dropna`` keyword argument in :func:`SeriesGroupBy.nth` (:issue:`17493`)
- Removed the previously deprecated ``convert`` keyword argument in :meth:`Series.take` and :meth:`DataFrame.take` (:issue:`17352`)
- Removed the previously deprecated behavior of arithmetic operations with ``datetime.date`` objects (:issue:`21152`)

.. _whatsnew_0250.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Significant speedup in :class:`SparseArray` initialization that benefits most operations, fixing performance regression introduced in v0.20.0 (:issue:`24985`)
- :meth:`DataFrame.to_stata()` is now faster when outputting data with any string or non-native endian columns (:issue:`25045`)
- Improved performance of :meth:`Series.searchsorted`. The speedup is especially large when the dtype is
  int8/int16/int32 and the searched key is within the integer bounds for the dtype (:issue:`22034`)
- Improved performance of :meth:`pandas.core.groupby.GroupBy.quantile` (:issue:`20405`)
- Improved performance of slicing and other selected operation on a :class:`RangeIndex` (:issue:`26565`, :issue:`26617`, :issue:`26722`)
- :class:`RangeIndex` now performs standard lookup without instantiating an actual hashtable, hence saving memory (:issue:`16685`)
- Improved performance of :meth:`read_csv` by faster tokenizing and faster parsing of small float numbers (:issue:`25784`)
- Improved performance of :meth:`read_csv` by faster parsing of N/A and boolean values (:issue:`25804`)
- Improved performance of :attr:`IntervalIndex.is_monotonic`, :attr:`IntervalIndex.is_monotonic_increasing` and :attr:`IntervalIndex.is_monotonic_decreasing` by removing conversion to :class:`MultiIndex` (:issue:`24813`)
- Improved performance of :meth:`DataFrame.to_csv` when writing datetime dtypes (:issue:`25708`)
- Improved performance of :meth:`read_csv` by much faster parsing of ``MM/YYYY`` and ``DD/MM/YYYY`` datetime formats (:issue:`25922`)
- Improved performance of nanops for dtypes that cannot store NaNs. Speedup is particularly prominent for :meth:`Series.all` and :meth:`Series.any` (:issue:`25070`)
- Improved performance of :meth:`Series.map` for dictionary mappers on categorical series by mapping the categories instead of mapping all values (:issue:`23785`)
- Improved performance of :meth:`IntervalIndex.intersection` (:issue:`24813`)
- Improved performance of :meth:`read_csv` by faster concatenating date columns without extra conversion to string for integer/float zero and float ``NaN``; by faster checking the string for the possibility of being a date (:issue:`25754`)
- Improved performance of :attr:`IntervalIndex.is_unique` by removing conversion to ``MultiIndex`` (:issue:`24813`)
- Restored performance of :meth:`DatetimeIndex.__iter__` by re-enabling specialized code path (:issue:`26702`)
- Improved performance when building :class:`MultiIndex` with at least one :class:`CategoricalIndex` level (:issue:`22044`)
- Improved performance by removing the need for a garbage collect when checking for ``SettingWithCopyWarning`` (:issue:`27031`)
- For :meth:`to_datetime` changed default value of cache parameter to ``True`` (:issue:`26043`)
- Improved performance of :class:`DatetimeIndex` and :class:`PeriodIndex` slicing given non-unique, monotonic data (:issue:`27136`).
- Improved performance of :meth:`pd.read_json` for index-oriented data. (:issue:`26773`)
- Improved performance of :meth:`MultiIndex.shape` (:issue:`27384`).

.. _whatsnew_0250.bug_fixes:

Bug fixes
~~~~~~~~~


Categorical
^^^^^^^^^^^

- Bug in :func:`DataFrame.at` and :func:`Series.at` that would raise exception if the index was a :class:`CategoricalIndex` (:issue:`20629`)
- Fixed bug in comparison of ordered :class:`Categorical` that contained missing values with a scalar which sometimes incorrectly resulted in ``True`` (:issue:`26504`)
- Bug in :meth:`DataFrame.dropna` when the :class:`DataFrame` has a :class:`CategoricalIndex` containing :class:`Interval` objects incorrectly raised a ``TypeError`` (:issue:`25087`)

Datetimelike
^^^^^^^^^^^^

- Bug in :func:`to_datetime` which would raise an (incorrect) ``ValueError`` when called with a date far into the future and the ``format`` argument specified instead of raising ``OutOfBoundsDatetime`` (:issue:`23830`)
- Bug in :func:`to_datetime` which would raise ``InvalidIndexError: Reindexing only valid with uniquely valued Index objects`` when called with ``cache=True``, with ``arg`` including at least two different elements from the set ``{None, numpy.nan, pandas.NaT}`` (:issue:`22305`)
- Bug in :class:`DataFrame` and :class:`Series` where timezone aware data with ``dtype='datetime64[ns]`` was not cast to naive (:issue:`25843`)
- Improved :class:`Timestamp` type checking in various datetime functions to prevent exceptions when using a subclassed ``datetime`` (:issue:`25851`)
- Bug in :class:`Series` and :class:`DataFrame` repr where ``np.datetime64('NaT')`` and ``np.timedelta64('NaT')`` with ``dtype=object`` would be represented as ``NaN`` (:issue:`25445`)
- Bug in :func:`to_datetime` which does not replace the invalid argument with ``NaT`` when error is set to coerce (:issue:`26122`)
- Bug in adding :class:`DateOffset` with nonzero month to :class:`DatetimeIndex` would raise ``ValueError`` (:issue:`26258`)
- Bug in :func:`to_datetime` which raises unhandled ``OverflowError`` when called with mix of invalid dates and ``NaN`` values with ``format='%Y%m%d'`` and ``error='coerce'`` (:issue:`25512`)
- Bug in :meth:`isin` for datetimelike indexes; :class:`DatetimeIndex`, :class:`TimedeltaIndex` and :class:`PeriodIndex` where the ``levels`` parameter was ignored. (:issue:`26675`)
- Bug in :func:`to_datetime` which raises ``TypeError`` for ``format='%Y%m%d'`` when called for invalid integer dates with length >= 6 digits with ``errors='ignore'``
- Bug when comparing a :class:`PeriodIndex` against a zero-dimensional numpy array (:issue:`26689`)
- Bug in constructing a ``Series`` or ``DataFrame`` from a numpy ``datetime64`` array with a non-ns unit and out-of-bound timestamps generating rubbish data, which will now correctly raise an ``OutOfBoundsDatetime`` error (:issue:`26206`).
- Bug in :func:`date_range` with unnecessary ``OverflowError`` being raised for very large or very small dates (:issue:`26651`)
- Bug where adding :class:`Timestamp` to a ``np.timedelta64`` object would raise instead of returning a :class:`Timestamp` (:issue:`24775`)
- Bug where comparing a zero-dimensional numpy array containing a ``np.datetime64`` object to a :class:`Timestamp` would incorrect raise ``TypeError`` (:issue:`26916`)
- Bug in :func:`to_datetime` which would raise ``ValueError: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True`` when called with ``cache=True``, with ``arg`` including datetime strings with different offset (:issue:`26097`)
-

Timedelta
^^^^^^^^^

- Bug in :func:`TimedeltaIndex.intersection` where for non-monotonic indices in some cases an empty ``Index`` was returned when in fact an intersection existed (:issue:`25913`)
- Bug with comparisons between :class:`Timedelta` and ``NaT`` raising ``TypeError`` (:issue:`26039`)
- Bug when adding or subtracting a :class:`BusinessHour` to a :class:`Timestamp` with the resulting time landing in a following or prior day respectively (:issue:`26381`)
- Bug when comparing a :class:`TimedeltaIndex` against a zero-dimensional numpy array (:issue:`26689`)

Timezones
^^^^^^^^^

- Bug in :func:`DatetimeIndex.to_frame` where timezone aware data would be converted to timezone naive data (:issue:`25809`)
- Bug in :func:`to_datetime` with ``utc=True`` and datetime strings that would apply previously parsed UTC offsets to subsequent arguments (:issue:`24992`)
- Bug in :func:`Timestamp.tz_localize` and :func:`Timestamp.tz_convert` does not propagate ``freq`` (:issue:`25241`)
- Bug in :func:`Series.at` where setting :class:`Timestamp` with timezone raises ``TypeError`` (:issue:`25506`)
- Bug in :func:`DataFrame.update` when updating with timezone aware data would return timezone naive data (:issue:`25807`)
- Bug in :func:`to_datetime` where an uninformative ``RuntimeError`` was raised when passing a naive :class:`Timestamp` with datetime strings with mixed UTC offsets (:issue:`25978`)
- Bug in :func:`to_datetime` with ``unit='ns'`` would drop timezone information from the parsed argument (:issue:`26168`)
- Bug in :func:`DataFrame.join` where joining a timezone aware index with a timezone aware column would result in a column of ``NaN`` (:issue:`26335`)
- Bug in :func:`date_range` where ambiguous or nonexistent start or end times were not handled by the ``ambiguous`` or ``nonexistent`` keywords respectively (:issue:`27088`)
- Bug in :meth:`DatetimeIndex.union` when combining a timezone aware and timezone unaware :class:`DatetimeIndex` (:issue:`21671`)
- Bug when applying a numpy reduction function (e.g. :meth:`numpy.minimum`) to a timezone aware :class:`Series` (:issue:`15552`)

Numeric
^^^^^^^

- Bug in :meth:`to_numeric` in which large negative numbers were being improperly handled (:issue:`24910`)
- Bug in :meth:`to_numeric` in which numbers were being coerced to float, even though ``errors`` was not ``coerce`` (:issue:`24910`)
- Bug in :meth:`to_numeric` in which invalid values for ``errors`` were being allowed (:issue:`26466`)
- Bug in :class:`format` in which floating point complex numbers were not being formatted to proper display precision and trimming (:issue:`25514`)
- Bug in error messages in :meth:`DataFrame.corr` and :meth:`Series.corr`. Added the possibility of using a callable. (:issue:`25729`)
- Bug in :meth:`Series.divmod` and :meth:`Series.rdivmod` which would raise an (incorrect) ``ValueError`` rather than return a pair of :class:`Series` objects as result (:issue:`25557`)
- Raises a helpful exception when a non-numeric index is sent to :meth:`interpolate` with methods which require numeric index. (:issue:`21662`)
- Bug in :meth:`~pandas.eval` when comparing floats with scalar operators, for example: ``x < -0.1`` (:issue:`25928`)
- Fixed bug where casting all-boolean array to integer extension array failed (:issue:`25211`)
- Bug in ``divmod`` with a :class:`Series` object containing zeros incorrectly raising ``AttributeError`` (:issue:`26987`)
- Inconsistency in :class:`Series` floor-division (`//`) and ``divmod`` filling positive//zero with ``NaN`` instead of ``Inf`` (:issue:`27321`)
-

Conversion
^^^^^^^^^^

- Bug in :func:`DataFrame.astype()` when passing a dict of columns and types the ``errors`` parameter was ignored. (:issue:`25905`)
-

Strings
^^^^^^^

- Bug in the ``__name__`` attribute of several methods of :class:`Series.str`, which were set incorrectly (:issue:`23551`)
- Improved error message when passing :class:`Series` of wrong dtype to :meth:`Series.str.cat` (:issue:`22722`)
-


Interval
^^^^^^^^

- Construction of :class:`Interval` is restricted to numeric, :class:`Timestamp` and :class:`Timedelta` endpoints (:issue:`23013`)
- Fixed bug in :class:`Series`/:class:`DataFrame` not displaying ``NaN`` in :class:`IntervalIndex` with missing values (:issue:`25984`)
- Bug in :meth:`IntervalIndex.get_loc` where a ``KeyError`` would be incorrectly raised for a decreasing :class:`IntervalIndex` (:issue:`25860`)
- Bug in :class:`Index` constructor where passing mixed closed :class:`Interval` objects would result in a ``ValueError`` instead of an ``object`` dtype ``Index`` (:issue:`27172`)

Indexing
^^^^^^^^

- Improved exception message when calling :meth:`DataFrame.iloc` with a list of non-numeric objects (:issue:`25753`).
- Improved exception message when calling ``.iloc`` or ``.loc`` with a boolean indexer with different length (:issue:`26658`).
- Bug in ``KeyError`` exception message when indexing a :class:`MultiIndex` with a non-existent key not displaying the original key (:issue:`27250`).
- Bug in ``.iloc`` and ``.loc`` with a boolean indexer not raising an ``IndexError`` when too few items are passed (:issue:`26658`).
- Bug in :meth:`DataFrame.loc` and :meth:`Series.loc` where ``KeyError`` was not raised for a ``MultiIndex`` when the key was less than or equal to the number of levels in the :class:`MultiIndex` (:issue:`14885`).
- Bug in which :meth:`DataFrame.append` produced an erroneous warning indicating that a ``KeyError`` will be thrown in the future when the data to be appended contains new columns (:issue:`22252`).
- Bug in which :meth:`DataFrame.to_csv` caused a segfault for a reindexed data frame, when the indices were single-level :class:`MultiIndex` (:issue:`26303`).
- Fixed bug where assigning a :class:`arrays.PandasArray` to a :class:`pandas.core.frame.DataFrame` would raise error (:issue:`26390`)
- Allow keyword arguments for callable local reference used in the :meth:`DataFrame.query` string (:issue:`26426`)
- Fixed a ``KeyError`` when indexing a :class:`MultiIndex`` level with a list containing exactly one label, which is missing (:issue:`27148`)
- Bug which produced ``AttributeError`` on partial matching :class:`Timestamp` in a :class:`MultiIndex`  (:issue:`26944`)
- Bug in :class:`Categorical` and  :class:`CategoricalIndex` with :class:`Interval` values when using the ``in`` operator (``__contains``) with objects that are not comparable to the values in the ``Interval`` (:issue:`23705`)
- Bug in :meth:`DataFrame.loc` and :meth:`DataFrame.iloc` on a :class:`DataFrame` with a single timezone-aware datetime64[ns] column incorrectly returning a scalar instead of a :class:`Series` (:issue:`27110`)
- Bug in :class:`CategoricalIndex` and :class:`Categorical` incorrectly raising ``ValueError`` instead of ``TypeError`` when a list is passed using the ``in`` operator (``__contains__``) (:issue:`21729`)
- Bug in setting a new value in a :class:`Series` with a :class:`Timedelta` object incorrectly casting the value to an integer (:issue:`22717`)
- Bug in :class:`Series` setting a new key (``__setitem__``) with a timezone-aware datetime incorrectly raising ``ValueError`` (:issue:`12862`)
- Bug in :meth:`DataFrame.iloc` when indexing with a read-only indexer (:issue:`17192`)
- Bug in :class:`Series` setting an existing tuple key (``__setitem__``) with timezone-aware datetime values incorrectly raising ``TypeError`` (:issue:`20441`)

Missing
^^^^^^^

- Fixed misleading exception message in :meth:`Series.interpolate` if argument ``order`` is required, but omitted (:issue:`10633`, :issue:`24014`).
- Fixed class type displayed in exception message in :meth:`DataFrame.dropna` if invalid ``axis`` parameter passed (:issue:`25555`)
- A ``ValueError`` will now be thrown by :meth:`DataFrame.fillna` when ``limit`` is not a positive integer (:issue:`27042`)
-

MultiIndex
^^^^^^^^^^

- Bug in which incorrect exception raised by :class:`Timedelta` when testing the membership of :class:`MultiIndex` (:issue:`24570`)
-

IO
^^

- Bug in :func:`DataFrame.to_html()` where values were truncated using display options instead of outputting the full content (:issue:`17004`)
- Fixed bug in missing text when using :meth:`to_clipboard` if copying utf-16 characters in Python 3 on Windows (:issue:`25040`)
- Bug in :func:`read_json` for ``orient='table'`` when it tries to infer dtypes by default, which is not applicable as dtypes are already defined in the JSON schema (:issue:`21345`)
- Bug in :func:`read_json` for ``orient='table'`` and float index, as it infers index dtype by default, which is not applicable because index dtype is already defined in the JSON schema (:issue:`25433`)
- Bug in :func:`read_json` for ``orient='table'`` and string of float column names, as it makes a column name type conversion to :class:`Timestamp`, which is not applicable because column names are already defined in the JSON schema (:issue:`25435`)
- Bug in :func:`json_normalize` for ``errors='ignore'`` where missing values in the input data, were filled in resulting ``DataFrame`` with the string ``"nan"`` instead of ``numpy.nan`` (:issue:`25468`)
- :meth:`DataFrame.to_html` now raises ``TypeError`` when using an invalid type for the ``classes`` parameter instead of ``AssertionError`` (:issue:`25608`)
- Bug in :meth:`DataFrame.to_string` and :meth:`DataFrame.to_latex` that would lead to incorrect output when the ``header`` keyword is used (:issue:`16718`)
- Bug in :func:`read_csv` not properly interpreting the UTF8 encoded filenames on Windows on Python 3.6+ (:issue:`15086`)
- Improved performance in :meth:`pandas.read_stata` and :class:`pandas.io.stata.StataReader` when converting columns that have missing values (:issue:`25772`)
- Bug in :meth:`DataFrame.to_html` where header numbers would ignore display options when rounding (:issue:`17280`)
- Bug in :func:`read_hdf` where reading a table from an HDF5 file written directly with PyTables fails with a ``ValueError`` when using a sub-selection via the ``start`` or ``stop`` arguments (:issue:`11188`)
- Bug in :func:`read_hdf` not properly closing store after a ``KeyError`` is raised (:issue:`25766`)
- Improved the explanation for the failure when value labels are repeated in Stata dta files and suggested work-arounds (:issue:`25772`)
- Improved :meth:`pandas.read_stata` and :class:`pandas.io.stata.StataReader` to read incorrectly formatted 118 format files saved by Stata (:issue:`25960`)
- Improved the ``col_space`` parameter in :meth:`DataFrame.to_html` to accept a string so CSS length values can be set correctly (:issue:`25941`)
- Fixed bug in loading objects from S3 that contain ``#`` characters in the URL (:issue:`25945`)
- Adds ``use_bqstorage_api`` parameter to :func:`read_gbq` to speed up downloads of large data frames. This feature requires version 0.10.0 of the ``pandas-gbq`` library as well as the ``google-cloud-bigquery-storage`` and ``fastavro`` libraries. (:issue:`26104`)
- Fixed memory leak in :meth:`DataFrame.to_json` when dealing with numeric data (:issue:`24889`)
- Bug in :func:`read_json` where date strings with ``Z`` were not converted to a UTC timezone (:issue:`26168`)
- Added ``cache_dates=True`` parameter to :meth:`read_csv`, which allows to cache unique dates when they are parsed (:issue:`25990`)
- :meth:`DataFrame.to_excel` now raises a ``ValueError`` when the caller's dimensions exceed the limitations of Excel (:issue:`26051`)
- Fixed bug in :func:`pandas.read_csv` where a BOM would result in incorrect parsing using engine='python' (:issue:`26545`)
- :func:`read_excel` now raises a ``ValueError`` when input is of type :class:`pandas.io.excel.ExcelFile` and ``engine`` param is passed since :class:`pandas.io.excel.ExcelFile` has an engine defined (:issue:`26566`)
- Bug while selecting from :class:`HDFStore` with ``where=''`` specified (:issue:`26610`).
- Fixed bug in :func:`DataFrame.to_excel()` where custom objects (i.e. ``PeriodIndex``) inside merged cells were not being converted into types safe for the Excel writer (:issue:`27006`)
- Bug in :meth:`read_hdf` where reading a timezone aware :class:`DatetimeIndex` would raise a ``TypeError`` (:issue:`11926`)
- Bug in :meth:`to_msgpack` and :meth:`read_msgpack` which would raise a ``ValueError`` rather than a ``FileNotFoundError`` for an invalid path (:issue:`27160`)
- Fixed bug in :meth:`DataFrame.to_parquet` which would raise a ``ValueError`` when the dataframe had no columns (:issue:`27339`)
- Allow parsing of :class:`PeriodDtype` columns when using :func:`read_csv` (:issue:`26934`)

Plotting
^^^^^^^^

- Fixed bug where :class:`api.extensions.ExtensionArray` could not be used in matplotlib plotting (:issue:`25587`)
- Bug in an error message in :meth:`DataFrame.plot`. Improved the error message if non-numerics are passed to :meth:`DataFrame.plot` (:issue:`25481`)
- Bug in incorrect ticklabel positions when plotting an index that are non-numeric / non-datetime (:issue:`7612`, :issue:`15912`, :issue:`22334`)
- Fixed bug causing plots of :class:`PeriodIndex` timeseries to fail if the frequency is a multiple of the frequency rule code (:issue:`14763`)
- Fixed bug when plotting a :class:`DatetimeIndex` with ``datetime.timezone.utc`` timezone (:issue:`17173`)
-

GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug in :meth:`pandas.core.resample.Resampler.agg` with a timezone aware index where ``OverflowError`` would raise when passing a list of functions (:issue:`22660`)
- Bug in :meth:`pandas.core.groupby.DataFrameGroupBy.nunique` in which the names of column levels were lost (:issue:`23222`)
- Bug in :func:`pandas.core.groupby.GroupBy.agg` when applying an aggregation function to timezone aware data (:issue:`23683`)
- Bug in :func:`pandas.core.groupby.GroupBy.first` and :func:`pandas.core.groupby.GroupBy.last` where timezone information would be dropped (:issue:`21603`)
- Bug in :func:`pandas.core.groupby.GroupBy.size` when grouping only NA values (:issue:`23050`)
- Bug in :func:`Series.groupby` where ``observed`` kwarg was previously ignored (:issue:`24880`)
- Bug in :func:`Series.groupby` where using ``groupby`` with a :class:`MultiIndex` Series with a list of labels equal to the length of the series caused incorrect grouping (:issue:`25704`)
- Ensured that ordering of outputs in ``groupby`` aggregation functions is consistent across all versions of Python (:issue:`25692`)
- Ensured that result group order is correct when grouping on an ordered ``Categorical`` and specifying ``observed=True`` (:issue:`25871`, :issue:`25167`)
- Bug in :meth:`pandas.core.window.Rolling.min` and :meth:`pandas.core.window.Rolling.max` that caused a memory leak (:issue:`25893`)
- Bug in :meth:`pandas.core.window.Rolling.count` and ``pandas.core.window.Expanding.count`` was previously ignoring the ``axis`` keyword (:issue:`13503`)
- Bug in :meth:`pandas.core.groupby.GroupBy.idxmax` and :meth:`pandas.core.groupby.GroupBy.idxmin` with datetime column would return incorrect dtype (:issue:`25444`, :issue:`15306`)
- Bug in :meth:`pandas.core.groupby.GroupBy.cumsum`, :meth:`pandas.core.groupby.GroupBy.cumprod`, :meth:`pandas.core.groupby.GroupBy.cummin` and :meth:`pandas.core.groupby.GroupBy.cummax` with categorical column having absent categories, would return incorrect result or segfault (:issue:`16771`)
- Bug in :meth:`pandas.core.groupby.GroupBy.nth` where NA values in the grouping would return incorrect results (:issue:`26011`)
- Bug in :meth:`pandas.core.groupby.SeriesGroupBy.transform` where transforming an empty group would raise a ``ValueError`` (:issue:`26208`)
- Bug in :meth:`pandas.core.frame.DataFrame.groupby` where passing a :class:`pandas.core.groupby.grouper.Grouper` would return incorrect groups when using the ``.groups`` accessor (:issue:`26326`)
- Bug in :meth:`pandas.core.groupby.GroupBy.agg` where incorrect results are returned for uint64 columns. (:issue:`26310`)
- Bug in :meth:`pandas.core.window.Rolling.median` and :meth:`pandas.core.window.Rolling.quantile` where MemoryError is raised with empty window (:issue:`26005`)
- Bug in :meth:`pandas.core.window.Rolling.median` and :meth:`pandas.core.window.Rolling.quantile` where incorrect results are returned with ``closed='left'`` and ``closed='neither'`` (:issue:`26005`)
- Improved :class:`pandas.core.window.Rolling`, :class:`pandas.core.window.Window` and :class:`pandas.core.window.ExponentialMovingWindow` functions to exclude nuisance columns from results instead of raising errors and raise a ``DataError`` only if all columns are nuisance (:issue:`12537`)
- Bug in :meth:`pandas.core.window.Rolling.max` and :meth:`pandas.core.window.Rolling.min` where incorrect results are returned with an empty variable window (:issue:`26005`)
- Raise a helpful exception when an unsupported weighted window function is used as an argument of :meth:`pandas.core.window.Window.aggregate` (:issue:`26597`)

Reshaping
^^^^^^^^^

- Bug in :func:`pandas.merge` adds a string of ``None``, if ``None`` is assigned in suffixes instead of remain the column name as-is (:issue:`24782`).
- Bug in :func:`merge` when merging by index name would sometimes result in an incorrectly numbered index (missing index values are now assigned NA) (:issue:`24212`, :issue:`25009`)
- :func:`to_records` now accepts dtypes to its ``column_dtypes`` parameter (:issue:`24895`)
- Bug in :func:`concat` where order of ``OrderedDict`` (and ``dict`` in Python 3.6+) is not respected, when passed in as  ``objs`` argument (:issue:`21510`)
- Bug in :func:`pivot_table` where columns with ``NaN`` values are dropped even if ``dropna`` argument is ``False``, when the ``aggfunc`` argument contains a ``list`` (:issue:`22159`)
- Bug in :func:`concat` where the resulting ``freq`` of two :class:`DatetimeIndex` with the same ``freq`` would be dropped (:issue:`3232`).
- Bug in :func:`merge` where merging with equivalent Categorical dtypes was raising an error (:issue:`22501`)
- bug in :class:`DataFrame` instantiating with a dict of iterators or generators (e.g. ``pd.DataFrame({'A': reversed(range(3))})``) raised an error (:issue:`26349`).
- Bug in :class:`DataFrame` instantiating with a ``range`` (e.g. ``pd.DataFrame(range(3))``) raised an error (:issue:`26342`).
- Bug in :class:`DataFrame` constructor when passing non-empty tuples would cause a segmentation fault (:issue:`25691`)
- Bug in :func:`Series.apply` failed when the series is a timezone aware :class:`DatetimeIndex` (:issue:`25959`)
- Bug in :func:`pandas.cut` where large bins could incorrectly raise an error due to an integer overflow (:issue:`26045`)
- Bug in :func:`DataFrame.sort_index` where an error is thrown when a multi-indexed ``DataFrame`` is sorted on all levels with the initial level sorted last (:issue:`26053`)
- Bug in :meth:`Series.nlargest` treats ``True`` as smaller than ``False`` (:issue:`26154`)
- Bug in :func:`DataFrame.pivot_table` with a :class:`IntervalIndex` as pivot index would raise ``TypeError`` (:issue:`25814`)
- Bug in which :meth:`DataFrame.from_dict` ignored order of ``OrderedDict`` when ``orient='index'`` (:issue:`8425`).
- Bug in :meth:`DataFrame.transpose` where transposing a DataFrame with a timezone-aware datetime column would incorrectly raise ``ValueError`` (:issue:`26825`)
- Bug in :func:`pivot_table` when pivoting a timezone aware column as the ``values`` would remove timezone information (:issue:`14948`)
- Bug in :func:`merge_asof` when specifying multiple ``by`` columns where one is ``datetime64[ns, tz]`` dtype (:issue:`26649`)

Sparse
^^^^^^

- Significant speedup in :class:`SparseArray` initialization that benefits most operations, fixing performance regression introduced in v0.20.0 (:issue:`24985`)
- Bug in :class:`SparseFrame` constructor where passing ``None`` as the data would cause ``default_fill_value`` to be ignored (:issue:`16807`)
- Bug in :class:`SparseDataFrame` when adding a column in which the length of values does not match length of index, ``AssertionError`` is raised instead of raising ``ValueError`` (:issue:`25484`)
- Introduce a better error message in :meth:`Series.sparse.from_coo` so it returns a ``TypeError`` for inputs that are not coo matrices (:issue:`26554`)
- Bug in :func:`numpy.modf` on a :class:`SparseArray`. Now a tuple of :class:`SparseArray` is returned (:issue:`26946`).


Build changes
^^^^^^^^^^^^^

- Fix install error with PyPy on macOS (:issue:`26536`)

ExtensionArray
^^^^^^^^^^^^^^

- Bug in :func:`factorize` when passing an ``ExtensionArray`` with a custom ``na_sentinel`` (:issue:`25696`).
- :meth:`Series.count` miscounts NA values in ExtensionArrays (:issue:`26835`)
- Added ``Series.__array_ufunc__`` to better handle NumPy ufuncs applied to Series backed by extension arrays (:issue:`23293`).
- Keyword argument ``deep`` has been removed from :meth:`ExtensionArray.copy` (:issue:`27083`)

Other
^^^^^

- Removed unused C functions from vendored UltraJSON implementation (:issue:`26198`)
- Allow :class:`Index` and :class:`RangeIndex` to be passed to numpy ``min`` and ``max`` functions (:issue:`26125`)
- Use actual class name in repr of empty objects of a ``Series`` subclass (:issue:`27001`).
- Bug in :class:`DataFrame` where passing an object array of timezone-aware ``datetime`` objects would incorrectly raise ``ValueError`` (:issue:`13287`)

.. _whatsnew_0.250.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.24.2..v0.25.0
.. _whatsnew_0160:

Version 0.16.0 (March 22, 2015)
-------------------------------

{{ header }}


This is a major release from 0.15.2 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

Highlights include:

- ``DataFrame.assign`` method, see :ref:`here <whatsnew_0160.enhancements.assign>`
- ``Series.to_coo/from_coo`` methods to interact with ``scipy.sparse``, see :ref:`here <whatsnew_0160.enhancements.sparse>`
- Backwards incompatible change to ``Timedelta`` to conform the ``.seconds`` attribute with ``datetime.timedelta``, see :ref:`here <whatsnew_0160.api_breaking.timedelta>`
- Changes to the ``.loc`` slicing API to conform with the behavior of ``.ix`` see :ref:`here <whatsnew_0160.api_breaking.indexing>`
- Changes to the default for ordering in the ``Categorical`` constructor, see :ref:`here <whatsnew_0160.api_breaking.categorical>`
-  Enhancement to the ``.str`` accessor to make string operations easier, see :ref:`here <whatsnew_0160.enhancements.string>`
- The ``pandas.tools.rplot``, ``pandas.sandbox.qtpandas`` and ``pandas.rpy``
  modules are deprecated. We refer users to external packages like
  `seaborn <http://stanford.edu/~mwaskom/software/seaborn/>`_,
  `pandas-qt <https://github.com/datalyze-solutions/pandas-qt>`_ and
  `rpy2 <http://rpy2.bitbucket.org/>`_ for similar or equivalent
  functionality, see :ref:`here <whatsnew_0160.deprecations>`

Check the :ref:`API Changes <whatsnew_0160.api>` and :ref:`deprecations <whatsnew_0160.deprecations>` before updating.

.. contents:: What's new in v0.16.0
    :local:
    :backlinks: none


.. _whatsnew_0160.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0160.enhancements.assign:

DataFrame assign
^^^^^^^^^^^^^^^^

Inspired by `dplyr's
<https://dplyr.tidyverse.org/articles/dplyr.html#mutating-operations>`__ ``mutate`` verb, DataFrame has a new
:meth:`~pandas.DataFrame.assign` method.
The function signature for ``assign`` is simply ``**kwargs``. The keys
are the column names for the new fields, and the values are either a value
to be inserted (for example, a ``Series`` or NumPy array), or a function
of one argument to be called on the ``DataFrame``. The new values are inserted,
and the entire DataFrame (with all original and new columns) is returned.

.. ipython:: python

   iris = pd.read_csv('data/iris.data')
   iris.head()

   iris.assign(sepal_ratio=iris['SepalWidth'] / iris['SepalLength']).head()

Above was an example of inserting a precomputed value. We can also pass in
a function to be evaluated.

.. ipython:: python

    iris.assign(sepal_ratio=lambda x: (x['SepalWidth']
                                       / x['SepalLength'])).head()

The power of ``assign`` comes when used in chains of operations. For example,
we can limit the DataFrame to just those with a Sepal Length greater than 5,
calculate the ratio, and plot

.. ipython:: python

   iris = pd.read_csv('data/iris.data')
   (iris.query('SepalLength > 5')
        .assign(SepalRatio=lambda x: x.SepalWidth / x.SepalLength,
                PetalRatio=lambda x: x.PetalWidth / x.PetalLength)
        .plot(kind='scatter', x='SepalRatio', y='PetalRatio'))

.. image:: ../_static/whatsnew_assign.png
  :scale: 50 %

See the :ref:`documentation <dsintro.chained_assignment>` for more. (:issue:`9229`)


.. _whatsnew_0160.enhancements.sparse:

Interaction with scipy.sparse
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Added :meth:`SparseSeries.to_coo` and :meth:`SparseSeries.from_coo` methods (:issue:`8048`) for converting to and from ``scipy.sparse.coo_matrix`` instances (see :ref:`here <sparse.scipysparse>`). For example, given a SparseSeries with MultiIndex we can convert to a ``scipy.sparse.coo_matrix`` by specifying the row and column labels as index levels:

.. code-block:: python

   s = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])
   s.index = pd.MultiIndex.from_tuples([(1, 2, 'a', 0),
                                        (1, 2, 'a', 1),
                                        (1, 1, 'b', 0),
                                        (1, 1, 'b', 1),
                                        (2, 1, 'b', 0),
                                        (2, 1, 'b', 1)],
                                       names=['A', 'B', 'C', 'D'])

   s

   # SparseSeries
   ss = s.to_sparse()
   ss

   A, rows, columns = ss.to_coo(row_levels=['A', 'B'],
                                column_levels=['C', 'D'],
                                sort_labels=False)

   A
   A.todense()
   rows
   columns

The from_coo method is a convenience method for creating a ``SparseSeries``
from a ``scipy.sparse.coo_matrix``:

.. code-block:: python

   from scipy import sparse
   A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])),
                         shape=(3, 4))
   A
   A.todense()

   ss = pd.SparseSeries.from_coo(A)
   ss

.. _whatsnew_0160.enhancements.string:

String methods enhancements
^^^^^^^^^^^^^^^^^^^^^^^^^^^

- Following new methods are accessible via ``.str`` accessor to apply the function to each values. This is intended to make it more consistent with standard methods on strings. (:issue:`9282`, :issue:`9352`, :issue:`9386`, :issue:`9387`, :issue:`9439`)

  =============  =============  =============  ===============    ===============
  ..             ..             Methods        ..                 ..
  =============  =============  =============  ===============    ===============
  ``isalnum()``  ``isalpha()``  ``isdigit()``  ``isdigit()``      ``isspace()``
  ``islower()``  ``isupper()``  ``istitle()``  ``isnumeric()``    ``isdecimal()``
  ``find()``     ``rfind()``    ``ljust()``    ``rjust()``        ``zfill()``
  =============  =============  =============  ===============    ===============

  .. ipython:: python

     s = pd.Series(['abcd', '3456', 'EFGH'])
     s.str.isalpha()
     s.str.find('ab')

- :meth:`Series.str.pad` and :meth:`Series.str.center` now accept ``fillchar`` option to specify filling character (:issue:`9352`)

  .. ipython:: python

     s = pd.Series(['12', '300', '25'])
     s.str.pad(5, fillchar='_')

- Added :meth:`Series.str.slice_replace`, which previously raised ``NotImplementedError`` (:issue:`8888`)

  .. ipython:: python

     s = pd.Series(['ABCD', 'EFGH', 'IJK'])
     s.str.slice_replace(1, 3, 'X')
     # replaced with empty char
     s.str.slice_replace(0, 1)

.. _whatsnew_0160.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Reindex now supports ``method='nearest'`` for frames or series with a monotonic increasing or decreasing index (:issue:`9258`):

  .. ipython:: python

     df = pd.DataFrame({'x': range(5)})
     df.reindex([0.2, 1.8, 3.5], method='nearest')

  This method is also exposed by the lower level ``Index.get_indexer`` and ``Index.get_loc`` methods.

- The ``read_excel()`` function's :ref:`sheetname <io.excel.specifying_sheets>` argument now accepts a list and ``None``, to get multiple or all sheets respectively.  If more than one sheet is specified, a dictionary is returned. (:issue:`9450`)

  .. code-block:: python

     # Returns the 1st and 4th sheet, as a dictionary of DataFrames.
     pd.read_excel('path_to_file.xls', sheetname=['Sheet1', 3])


- Allow Stata files to be read incrementally with an iterator; support for long strings in Stata files. See the docs :ref:`here<io.stata_reader>` (:issue:`9493`:).
- Paths beginning with ~ will now be expanded to begin with the user's home directory (:issue:`9066`)
- Added time interval selection in ``get_data_yahoo`` (:issue:`9071`)
- Added ``Timestamp.to_datetime64()`` to complement ``Timedelta.to_timedelta64()`` (:issue:`9255`)
- ``tseries.frequencies.to_offset()`` now accepts ``Timedelta`` as input (:issue:`9064`)
- Lag parameter was added to the autocorrelation method of ``Series``, defaults to lag-1 autocorrelation (:issue:`9192`)
- ``Timedelta`` will now accept ``nanoseconds`` keyword in constructor (:issue:`9273`)
- SQL code now safely escapes table and column names (:issue:`8986`)
- Added auto-complete for ``Series.str.<tab>``, ``Series.dt.<tab>`` and ``Series.cat.<tab>`` (:issue:`9322`)
- ``Index.get_indexer`` now supports ``method='pad'`` and ``method='backfill'`` even for any target array, not just monotonic targets. These methods also work for monotonic decreasing as well as monotonic increasing indexes (:issue:`9258`).
- ``Index.asof`` now works on all index types (:issue:`9258`).
- A ``verbose`` argument has been augmented in ``io.read_excel()``, defaults to False. Set to True to print sheet names as they are parsed. (:issue:`9450`)
- Added ``days_in_month`` (compatibility alias ``daysinmonth``) property to ``Timestamp``, ``DatetimeIndex``, ``Period``, ``PeriodIndex``, and ``Series.dt`` (:issue:`9572`)
- Added ``decimal`` option in ``to_csv`` to provide formatting for non-'.' decimal separators (:issue:`781`)
- Added ``normalize`` option for ``Timestamp`` to normalized to midnight (:issue:`8794`)
- Added example for ``DataFrame`` import to R using HDF5 file and ``rhdf5``
  library. See the :ref:`documentation <io.external_compatibility>` for more
  (:issue:`9636`).

.. _whatsnew_0160.api:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0160.api_breaking:

.. _whatsnew_0160.api_breaking.timedelta:

Changes in timedelta
^^^^^^^^^^^^^^^^^^^^

In v0.15.0 a new scalar type ``Timedelta`` was introduced, that is a
sub-class of ``datetime.timedelta``. Mentioned :ref:`here <whatsnew_0150.timedeltaindex>` was a notice of an API change w.r.t. the ``.seconds`` accessor. The intent was to provide a user-friendly set of accessors that give the 'natural' value for that unit, e.g. if you had a ``Timedelta('1 day, 10:11:12')``, then ``.seconds`` would return 12. However, this is at odds with the definition of ``datetime.timedelta``, which defines ``.seconds`` as ``10 * 3600 + 11 * 60 + 12 == 36672``.

So in v0.16.0, we are restoring the API to match that of ``datetime.timedelta``. Further, the component values are still available through the ``.components`` accessor. This affects the ``.seconds`` and ``.microseconds`` accessors, and removes the ``.hours``, ``.minutes``, ``.milliseconds`` accessors. These changes affect ``TimedeltaIndex`` and the Series ``.dt`` accessor as well. (:issue:`9185`, :issue:`9139`)

Previous behavior

.. code-block:: ipython

   In [2]: t = pd.Timedelta('1 day, 10:11:12.100123')

   In [3]: t.days
   Out[3]: 1

   In [4]: t.seconds
   Out[4]: 12

   In [5]: t.microseconds
   Out[5]: 123

New behavior

.. ipython:: python

   t = pd.Timedelta('1 day, 10:11:12.100123')
   t.days
   t.seconds
   t.microseconds

Using ``.components`` allows the full component access

.. ipython:: python

   t.components
   t.components.seconds

.. _whatsnew_0160.api_breaking.indexing:

Indexing changes
^^^^^^^^^^^^^^^^

The behavior of a small sub-set of edge cases for using ``.loc`` have changed (:issue:`8613`). Furthermore we have improved the content of the error messages that are raised:

- Slicing with ``.loc`` where the start and/or stop bound is not found in the index is now allowed; this previously would raise a ``KeyError``. This makes the behavior the same as ``.ix`` in this case. This change is only for slicing, not when indexing with a single label.

  .. ipython:: python

     df = pd.DataFrame(np.random.randn(5, 4),
                       columns=list('ABCD'),
                       index=pd.date_range('20130101', periods=5))
     df
     s = pd.Series(range(5), [-2, -1, 1, 2, 3])
     s

  Previous behavior

  .. code-block:: ipython

     In [4]: df.loc['2013-01-02':'2013-01-10']
     KeyError: 'stop bound [2013-01-10] is not in the [index]'

     In [6]: s.loc[-10:3]
     KeyError: 'start bound [-10] is not the [index]'

  New behavior

  .. ipython:: python

     df.loc['2013-01-02':'2013-01-10']
     s.loc[-10:3]

- Allow slicing with float-like values on an integer index for ``.ix``. Previously this was only enabled for ``.loc``:

  Previous behavior

  .. code-block:: ipython

     In [8]: s.ix[-1.0:2]
     TypeError: the slice start value [-1.0] is not a proper indexer for this index type (Int64Index)

  New behavior

  .. code-block:: python

     In [2]: s.ix[-1.0:2]
     Out[2]:
     -1    1
      1    2
      2    3
     dtype: int64

- Provide a useful exception for indexing with an invalid type for that index when using ``.loc``. For example trying to use ``.loc`` on an index of type ``DatetimeIndex`` or ``PeriodIndex`` or ``TimedeltaIndex``, with an integer (or a float).

  Previous behavior

  .. code-block:: python

     In [4]: df.loc[2:3]
     KeyError: 'start bound [2] is not the [index]'

  New behavior

  .. code-block:: ipython

     In [4]: df.loc[2:3]
     TypeError: Cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with <type 'int'> keys


.. _whatsnew_0160.api_breaking.categorical:

Categorical changes
^^^^^^^^^^^^^^^^^^^

In prior versions, ``Categoricals`` that had an unspecified ordering (meaning no ``ordered`` keyword was passed) were defaulted as ``ordered`` Categoricals. Going forward, the ``ordered`` keyword in the ``Categorical`` constructor will default to ``False``. Ordering must now be explicit.

Furthermore, previously you *could* change the ``ordered`` attribute of a Categorical by just setting the attribute, e.g. ``cat.ordered=True``; This is now deprecated and you should use ``cat.as_ordered()`` or ``cat.as_unordered()``. These will by default return a **new** object and not modify the existing object. (:issue:`9347`, :issue:`9190`)

Previous behavior

.. code-block:: ipython

   In [3]: s = pd.Series([0, 1, 2], dtype='category')

   In [4]: s
   Out[4]:
   0    0
   1    1
   2    2
   dtype: category
   Categories (3, int64): [0 < 1 < 2]

   In [5]: s.cat.ordered
   Out[5]: True

   In [6]: s.cat.ordered = False

   In [7]: s
   Out[7]:
   0    0
   1    1
   2    2
   dtype: category
   Categories (3, int64): [0, 1, 2]

New behavior

.. ipython:: python

   s = pd.Series([0, 1, 2], dtype='category')
   s
   s.cat.ordered
   s = s.cat.as_ordered()
   s
   s.cat.ordered

   # you can set in the constructor of the Categorical
   s = pd.Series(pd.Categorical([0, 1, 2], ordered=True))
   s
   s.cat.ordered

For ease of creation of series of categorical data, we have added the ability to pass keywords when calling ``.astype()``. These are passed directly to the constructor.

.. code-block:: python

    In [54]: s = pd.Series(["a", "b", "c", "a"]).astype('category', ordered=True)

    In [55]: s
    Out[55]:
    0    a
    1    b
    2    c
    3    a
    dtype: category
    Categories (3, object): [a < b < c]

    In [56]: s = (pd.Series(["a", "b", "c", "a"])
       ....:        .astype('category', categories=list('abcdef'), ordered=False))

    In [57]: s
    Out[57]:
    0    a
    1    b
    2    c
    3    a
    dtype: category
    Categories (6, object): [a, b, c, d, e, f]


.. _whatsnew_0160.api_breaking.other:

Other API changes
^^^^^^^^^^^^^^^^^

- ``Index.duplicated`` now returns ``np.array(dtype=bool)`` rather than ``Index(dtype=object)`` containing ``bool`` values. (:issue:`8875`)
- ``DataFrame.to_json`` now returns accurate type serialisation for each column for frames of mixed dtype (:issue:`9037`)

  Previously data was coerced to a common dtype before serialisation, which for
  example resulted in integers being serialised to floats:

  .. code-block:: ipython

    In [2]: pd.DataFrame({'i': [1,2], 'f': [3.0, 4.2]}).to_json()
    Out[2]: '{"f":{"0":3.0,"1":4.2},"i":{"0":1.0,"1":2.0}}'

  Now each column is serialised using its correct dtype:

  .. code-block:: ipython

    In [2]:  pd.DataFrame({'i': [1,2], 'f': [3.0, 4.2]}).to_json()
    Out[2]: '{"f":{"0":3.0,"1":4.2},"i":{"0":1,"1":2}}'

- ``DatetimeIndex``, ``PeriodIndex`` and ``TimedeltaIndex.summary`` now output the same format. (:issue:`9116`)
- ``TimedeltaIndex.freqstr`` now output the same string format as ``DatetimeIndex``. (:issue:`9116`)

- Bar and horizontal bar plots no longer add a dashed line along the info axis. The prior style can be achieved with matplotlib's ``axhline`` or ``axvline`` methods (:issue:`9088`).

- ``Series`` accessors ``.dt``, ``.cat`` and ``.str`` now raise ``AttributeError`` instead of ``TypeError`` if the series does not contain the appropriate type of data (:issue:`9617`). This follows Python's built-in exception hierarchy more closely and ensures that tests like ``hasattr(s, 'cat')`` are consistent on both Python 2 and 3.

- ``Series`` now supports bitwise operation for integral types (:issue:`9016`). Previously even if the input dtypes were integral, the output dtype was coerced to ``bool``.

  Previous behavior

  .. code-block:: ipython

     In [2]: pd.Series([0, 1, 2, 3], list('abcd')) | pd.Series([4, 4, 4, 4], list('abcd'))
     Out[2]:
     a    True
     b    True
     c    True
     d    True
     dtype: bool

  New behavior. If the input dtypes are integral, the output dtype is also integral and the output
  values are the result of the bitwise operation.

  .. code-block:: ipython

     In [2]: pd.Series([0, 1, 2, 3], list('abcd')) | pd.Series([4, 4, 4, 4], list('abcd'))
     Out[2]:
     a    4
     b    5
     c    6
     d    7
     dtype: int64


- During division involving a ``Series`` or ``DataFrame``, ``0/0`` and ``0//0`` now give ``np.nan`` instead of ``np.inf``. (:issue:`9144`, :issue:`8445`)

  Previous behavior

  .. code-block:: ipython

        In [2]: p = pd.Series([0, 1])

        In [3]: p / 0
        Out[3]:
        0    inf
        1    inf
        dtype: float64

        In [4]: p // 0
        Out[4]:
        0    inf
        1    inf
        dtype: float64



  New behavior

  .. ipython:: python

     p = pd.Series([0, 1])
     p / 0
     p // 0

- ``Series.values_counts`` and ``Series.describe`` for categorical data will now put ``NaN`` entries at the end. (:issue:`9443`)
- ``Series.describe`` for categorical data will now give counts and frequencies of 0, not ``NaN``, for unused categories (:issue:`9443`)

- Due to a bug fix, looking up a partial string label with ``DatetimeIndex.asof`` now includes values that match the string, even if they are after the start of the partial string label (:issue:`9258`).

  Old behavior:

  .. code-block:: ipython

    In [4]: pd.to_datetime(['2000-01-31', '2000-02-28']).asof('2000-02')
    Out[4]: Timestamp('2000-01-31 00:00:00')

  Fixed behavior:

  .. ipython:: python

    pd.to_datetime(['2000-01-31', '2000-02-28']).asof('2000-02')

  To reproduce the old behavior, simply add more precision to the label (e.g., use ``2000-02-01`` instead of ``2000-02``).


.. _whatsnew_0160.deprecations:

Deprecations
^^^^^^^^^^^^

- The ``rplot`` trellis plotting interface is deprecated and will be removed
  in a future version. We refer to external packages like
  `seaborn <http://stanford.edu/~mwaskom/software/seaborn/>`_ for similar
  but more refined functionality (:issue:`3445`).
  The documentation includes some examples how to convert your existing code
  from ``rplot`` to seaborn `here <https://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html#trellis-plotting-interface>`__.

- The ``pandas.sandbox.qtpandas`` interface is deprecated and will be removed in a future version.
  We refer users to the external package `pandas-qt <https://github.com/datalyze-solutions/pandas-qt>`_. (:issue:`9615`)

- The ``pandas.rpy`` interface is deprecated and will be removed in a future version.
  Similar functionality can be accessed through the `rpy2 <http://rpy2.bitbucket.org/>`_ project (:issue:`9602`)

- Adding ``DatetimeIndex/PeriodIndex`` to another ``DatetimeIndex/PeriodIndex`` is being deprecated as a set-operation. This will be changed to a ``TypeError`` in a future version. ``.union()`` should be used for the union set operation. (:issue:`9094`)
- Subtracting ``DatetimeIndex/PeriodIndex`` from another ``DatetimeIndex/PeriodIndex`` is being deprecated as a set-operation. This will be changed to an actual numeric subtraction yielding a ``TimeDeltaIndex`` in a future version. ``.difference()`` should be used for the differencing set operation. (:issue:`9094`)


.. _whatsnew_0160.prior_deprecations:

Removal of prior version deprecations/changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- ``DataFrame.pivot_table`` and ``crosstab``'s ``rows`` and ``cols`` keyword arguments were removed in favor
  of ``index`` and ``columns`` (:issue:`6581`)
- ``DataFrame.to_excel`` and ``DataFrame.to_csv`` ``cols`` keyword argument was removed in favor of ``columns`` (:issue:`6581`)
- Removed ``convert_dummies`` in favor of ``get_dummies`` (:issue:`6581`)
- Removed ``value_range`` in favor of ``describe`` (:issue:`6581`)

.. _whatsnew_0160.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Fixed a performance regression for ``.loc`` indexing with an array or list-like (:issue:`9126`:).
- ``DataFrame.to_json`` 30x performance improvement for mixed dtype frames. (:issue:`9037`)
- Performance improvements in ``MultiIndex.duplicated`` by working with labels instead of values (:issue:`9125`)
- Improved the speed of ``nunique`` by calling ``unique`` instead of ``value_counts`` (:issue:`9129`, :issue:`7771`)
- Performance improvement of up to 10x in ``DataFrame.count`` and ``DataFrame.dropna`` by taking advantage of homogeneous/heterogeneous dtypes appropriately (:issue:`9136`)
- Performance improvement of up to 20x in ``DataFrame.count`` when using a ``MultiIndex`` and the ``level`` keyword argument  (:issue:`9163`)
- Performance and memory usage improvements in ``merge`` when key space exceeds ``int64`` bounds (:issue:`9151`)
- Performance improvements in multi-key ``groupby`` (:issue:`9429`)
- Performance improvements in ``MultiIndex.sortlevel`` (:issue:`9445`)
- Performance and memory usage improvements in ``DataFrame.duplicated`` (:issue:`9398`)
- Cythonized ``Period`` (:issue:`9440`)
- Decreased memory usage on ``to_hdf`` (:issue:`9648`)

.. _whatsnew_0160.bug_fixes:

Bug fixes
~~~~~~~~~

- Changed ``.to_html`` to remove leading/trailing spaces in table body (:issue:`4987`)
- Fixed issue using ``read_csv`` on s3 with Python 3 (:issue:`9452`)
- Fixed compatibility issue in ``DatetimeIndex`` affecting architectures where ``numpy.int_`` defaults to ``numpy.int32`` (:issue:`8943`)
- Bug in Panel indexing with an object-like (:issue:`9140`)
- Bug in the returned ``Series.dt.components`` index was reset to the default index (:issue:`9247`)
- Bug in ``Categorical.__getitem__/__setitem__`` with listlike input getting incorrect results from indexer coercion (:issue:`9469`)
- Bug in partial setting with a DatetimeIndex (:issue:`9478`)
- Bug in groupby for integer and datetime64 columns when applying an aggregator that caused the value to be
  changed when the number was sufficiently large (:issue:`9311`, :issue:`6620`)
- Fixed bug in ``to_sql`` when mapping a ``Timestamp`` object column (datetime
  column with timezone info) to the appropriate sqlalchemy type (:issue:`9085`).
- Fixed bug in ``to_sql`` ``dtype`` argument not accepting an instantiated
  SQLAlchemy type  (:issue:`9083`).
- Bug in ``.loc`` partial setting with a ``np.datetime64`` (:issue:`9516`)
- Incorrect dtypes inferred on datetimelike looking ``Series`` & on ``.xs`` slices (:issue:`9477`)
- Items in ``Categorical.unique()`` (and ``s.unique()`` if ``s`` is of dtype ``category``) now appear in the order in which they are originally found, not in sorted order (:issue:`9331`). This is now consistent with the behavior for other dtypes in pandas.
- Fixed bug on big endian platforms which produced incorrect results in ``StataReader`` (:issue:`8688`).
- Bug in ``MultiIndex.has_duplicates`` when having many levels causes an indexer overflow (:issue:`9075`, :issue:`5873`)
- Bug in ``pivot`` and ``unstack`` where ``nan`` values would break index alignment (:issue:`4862`, :issue:`7401`, :issue:`7403`, :issue:`7405`, :issue:`7466`, :issue:`9497`)
- Bug in left ``join`` on MultiIndex with ``sort=True`` or null values (:issue:`9210`).
- Bug in ``MultiIndex`` where inserting new keys would fail (:issue:`9250`).
- Bug in ``groupby`` when key space exceeds ``int64`` bounds (:issue:`9096`).
- Bug in ``unstack`` with ``TimedeltaIndex`` or ``DatetimeIndex`` and nulls (:issue:`9491`).
- Bug in ``rank`` where comparing floats with tolerance will cause inconsistent behaviour (:issue:`8365`).
- Fixed character encoding bug in ``read_stata`` and ``StataReader`` when loading data from a URL (:issue:`9231`).
- Bug in adding ``offsets.Nano`` to other offsets raises ``TypeError`` (:issue:`9284`)
- Bug in ``DatetimeIndex`` iteration, related to (:issue:`8890`), fixed in (:issue:`9100`)
- Bugs in ``resample`` around DST transitions. This required fixing offset classes so they behave correctly on DST transitions. (:issue:`5172`, :issue:`8744`, :issue:`8653`, :issue:`9173`, :issue:`9468`).
- Bug in binary operator method (eg ``.mul()``) alignment with integer levels (:issue:`9463`).
- Bug in boxplot, scatter and hexbin plot may show an unnecessary warning (:issue:`8877`)
- Bug in subplot with ``layout`` kw may show unnecessary warning (:issue:`9464`)
- Bug in using grouper functions that need passed through arguments (e.g. axis), when using wrapped function (e.g. ``fillna``), (:issue:`9221`)
- ``DataFrame`` now properly supports simultaneous ``copy`` and ``dtype`` arguments in constructor (:issue:`9099`)
- Bug in ``read_csv`` when using skiprows on a file with CR line endings with the c engine. (:issue:`9079`)
- ``isnull`` now detects ``NaT`` in ``PeriodIndex`` (:issue:`9129`)
- Bug in groupby ``.nth()`` with a multiple column groupby (:issue:`8979`)
- Bug in ``DataFrame.where`` and ``Series.where`` coerce numerics to string incorrectly (:issue:`9280`)
- Bug in ``DataFrame.where`` and ``Series.where`` raise ``ValueError`` when string list-like is passed. (:issue:`9280`)
- Accessing ``Series.str`` methods on with non-string values now raises ``TypeError`` instead of producing incorrect results (:issue:`9184`)
- Bug in ``DatetimeIndex.__contains__`` when index has duplicates and is not monotonic increasing (:issue:`9512`)
- Fixed division by zero error for ``Series.kurt()`` when all values are equal (:issue:`9197`)
- Fixed issue in the ``xlsxwriter`` engine where it added a default 'General' format to cells if no other format was applied. This prevented other row or column formatting being applied. (:issue:`9167`)
- Fixes issue with ``index_col=False`` when ``usecols`` is also specified in ``read_csv``. (:issue:`9082`)
- Bug where ``wide_to_long`` would modify the input stub names list (:issue:`9204`)
- Bug in ``to_sql`` not storing float64 values using double precision. (:issue:`9009`)
- ``SparseSeries`` and ``SparsePanel`` now accept zero argument constructors (same as their non-sparse counterparts) (:issue:`9272`).
- Regression in merging ``Categorical`` and ``object`` dtypes (:issue:`9426`)
- Bug in ``read_csv`` with buffer overflows with certain malformed input files (:issue:`9205`)
- Bug in groupby MultiIndex with missing pair (:issue:`9049`, :issue:`9344`)
- Fixed bug in ``Series.groupby`` where grouping on ``MultiIndex`` levels would ignore the sort argument (:issue:`9444`)
- Fix bug in ``DataFrame.Groupby`` where ``sort=False`` is ignored in the case of Categorical columns. (:issue:`8868`)
- Fixed bug with reading CSV files from Amazon S3 on python 3 raising a TypeError (:issue:`9452`)
- Bug in the Google BigQuery reader where the 'jobComplete' key may be present but False in the query results (:issue:`8728`)
- Bug in ``Series.values_counts`` with excluding ``NaN`` for categorical type ``Series`` with ``dropna=True`` (:issue:`9443`)
- Fixed missing numeric_only option for ``DataFrame.std/var/sem`` (:issue:`9201`)
- Support constructing ``Panel`` or ``Panel4D`` with scalar data (:issue:`8285`)
- ``Series`` text representation disconnected from ``max_rows``/``max_columns`` (:issue:`7508`).

\

- ``Series`` number formatting inconsistent when truncated (:issue:`8532`).

  Previous behavior

  .. code-block:: python

    In [2]: pd.options.display.max_rows = 10
    In [3]: s = pd.Series([1,1,1,1,1,1,1,1,1,1,0.9999,1,1]*10)
    In [4]: s
    Out[4]:
    0    1
    1    1
    2    1
    ...
    127    0.9999
    128    1.0000
    129    1.0000
    Length: 130, dtype: float64

  New behavior

  .. code-block:: python

    0      1.0000
    1      1.0000
    2      1.0000
    3      1.0000
    4      1.0000
    ...
    125    1.0000
    126    1.0000
    127    0.9999
    128    1.0000
    129    1.0000
    dtype: float64

- A Spurious ``SettingWithCopy`` Warning was generated when setting a new item in a frame in some cases (:issue:`8730`)

  The following would previously report a ``SettingWithCopy`` Warning.

  .. ipython:: python

     df1 = pd.DataFrame({'x': pd.Series(['a', 'b', 'c']),
                         'y': pd.Series(['d', 'e', 'f'])})
     df2 = df1[['x']]
     df2['y'] = ['g', 'h', 'i']


.. _whatsnew_0.16.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.15.2..v0.16.0
.. _whatsnew_060:

Version 0.6.0 (November 25, 2011)
---------------------------------

{{ header }}

New features
~~~~~~~~~~~~
- :ref:`Added <reshaping.melt>` ``melt`` function to ``pandas.core.reshape``
- :ref:`Added <groupby.multiindex>` ``level`` parameter to group by level in Series and DataFrame descriptive statistics (:issue:`313`)
- :ref:`Added <basics.head_tail>` ``head`` and ``tail`` methods to Series, analogous to DataFrame (:issue:`296`)
- :ref:`Added <indexing.boolean>` ``Series.isin`` function which checks if each value is contained in a passed sequence (:issue:`289`)
- :ref:`Added <io.formatting>` ``float_format`` option to ``Series.to_string``
- :ref:`Added <io.parse_dates>` ``skip_footer`` (:issue:`291`) and ``converters`` (:issue:`343`) options to ``read_csv`` and ``read_table``
- :ref:`Added <indexing.duplicate>` ``drop_duplicates`` and ``duplicated`` functions for removing duplicate DataFrame rows and checking for duplicate rows, respectively (:issue:`319`)
- :ref:`Implemented <dsintro.boolean>` operators '&', '|', '^', '-' on DataFrame (:issue:`347`)
- :ref:`Added <basics.stats>` ``Series.mad``, mean absolute deviation
- :ref:`Added <timeseries.offsets>` ``QuarterEnd`` DateOffset (:issue:`321`)
- :ref:`Added <dsintro.numpy_interop>` ``dot`` to DataFrame (:issue:`65`)
- Added ``orient`` option to ``Panel.from_dict`` (:issue:`359`, :issue:`301`)
- :ref:`Added <basics.dataframe.from_dict>` ``orient`` option to ``DataFrame.from_dict``
- :ref:`Added <basics.dataframe.from_records>` passing list of tuples or list of lists to ``DataFrame.from_records`` (:issue:`357`)
- :ref:`Added <groupby.multiindex>` multiple levels to groupby (:issue:`103`)
- :ref:`Allow <basics.sorting>` multiple columns in ``by`` argument of ``DataFrame.sort_index`` (:issue:`92`, :issue:`362`)
- :ref:`Added <indexing.basics.get_value>` fast ``get_value`` and ``put_value`` methods to DataFrame (:issue:`360`)
- :ref:`Added <computation.covariance>` ``cov`` instance methods to Series and DataFrame (:issue:`194`, :issue:`362`)
- :ref:`Added <visualization.barplot>` ``kind='bar'`` option to ``DataFrame.plot`` (:issue:`348`)
- :ref:`Added <basics.idxmin>` ``idxmin`` and ``idxmax`` to Series and DataFrame (:issue:`286`)
- :ref:`Added <io.clipboard>` ``read_clipboard`` function to parse DataFrame from clipboard (:issue:`300`)
- :ref:`Added <basics.stats>` ``nunique`` function to Series for counting unique elements (:issue:`297`)
- :ref:`Made <basics.dataframe>` DataFrame constructor use Series name if no columns passed (:issue:`373`)
- :ref:`Support <io.parse_dates>` regular expressions in read_table/read_csv (:issue:`364`)
- :ref:`Added <io.html>` ``DataFrame.to_html`` for writing DataFrame to HTML (:issue:`387`)
- :ref:`Added <basics.dataframe>` support for MaskedArray data in DataFrame, masked values converted to NaN (:issue:`396`)
- :ref:`Added <visualization.box>` ``DataFrame.boxplot`` function (:issue:`368`)
- :ref:`Can <basics.apply>` pass extra args, kwds to DataFrame.apply (:issue:`376`)
- :ref:`Implement <merging.multikey_join>` ``DataFrame.join`` with vector ``on`` argument (:issue:`312`)
- :ref:`Added <visualization.basic>` ``legend`` boolean flag to ``DataFrame.plot`` (:issue:`324`)
- :ref:`Can <reshaping.stacking>` pass multiple levels to ``stack`` and ``unstack`` (:issue:`370`)
- :ref:`Can <reshaping.pivot>` pass multiple values columns to ``pivot_table`` (:issue:`381`)
- :ref:`Use <groupby.multiindex>` Series name in GroupBy for result index (:issue:`363`)
- :ref:`Added <basics.apply>` ``raw`` option to ``DataFrame.apply`` for performance if only need ndarray (:issue:`309`)
- Added proper, tested weighted least squares to standard and panel OLS (:issue:`303`)

Performance enhancements
~~~~~~~~~~~~~~~~~~~~~~~~
- VBENCH Cythonized ``cache_readonly``, resulting in substantial micro-performance enhancements throughout the code base (:issue:`361`)
- VBENCH Special Cython matrix iterator for applying arbitrary reduction operations with 3-5x better performance than ``np.apply_along_axis`` (:issue:`309`)
- VBENCH Improved performance of ``MultiIndex.from_tuples``
- VBENCH Special Cython matrix iterator for applying arbitrary reduction operations
- VBENCH + DOCUMENT Add ``raw`` option to ``DataFrame.apply`` for getting better performance when
- VBENCH Faster cythonized count by level in Series and DataFrame (:issue:`341`)
- VBENCH? Significant GroupBy performance enhancement with multiple keys with many "empty" combinations
- VBENCH New Cython vectorized function ``map_infer`` speeds up ``Series.apply`` and ``Series.map`` significantly when passed elementwise Python function, motivated by (:issue:`355`)
- VBENCH Significantly improved performance of ``Series.order``, which also makes np.unique called on a Series faster (:issue:`327`)
- VBENCH Vastly improved performance of GroupBy on axes with a MultiIndex (:issue:`299`)



.. _whatsnew_0.6.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.5.0..v0.6.0
.. _whatsnew_113:

What's new in 1.1.3 (October 5, 2020)
-------------------------------------

These are the changes in pandas 1.1.3. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

Enhancements
~~~~~~~~~~~~

Added support for new Python version
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.1.3 now supports Python 3.9 (:issue:`36296`).

Development Changes
^^^^^^^^^^^^^^^^^^^

- The minimum version of Cython is now the most recent bug-fix version (0.29.21) (:issue:`36296`).

.. ---------------------------------------------------------------------------

.. _whatsnew_113.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :meth:`DataFrame.agg`, :meth:`DataFrame.apply`, :meth:`Series.agg`, and :meth:`Series.apply` where internal suffix is exposed to the users when no relabelling is applied (:issue:`36189`)
- Fixed regression in :class:`IntegerArray` unary plus and minus operations raising a ``TypeError`` (:issue:`36063`)
- Fixed regression when adding a :meth:`timedelta_range` to a :class:`Timestamp` raised a ``ValueError`` (:issue:`35897`)
- Fixed regression in :meth:`Series.__getitem__` incorrectly raising when the input was a tuple (:issue:`35534`)
- Fixed regression in :meth:`Series.__getitem__` incorrectly raising when the input was a frozenset (:issue:`35747`)
- Fixed regression in modulo of :class:`Index`, :class:`Series` and :class:`DataFrame` using ``numexpr`` using C not Python semantics (:issue:`36047`, :issue:`36526`)
- Fixed regression in :meth:`read_excel` with ``engine="odf"`` caused ``UnboundLocalError`` in some cases where cells had nested child nodes (:issue:`36122`, :issue:`35802`)
- Fixed regression in :meth:`DataFrame.replace` inconsistent replace when using a float in the replace method (:issue:`35376`)
- Fixed regression in :meth:`Series.loc` on a :class:`Series` with a :class:`MultiIndex` containing :class:`Timestamp` raising ``InvalidIndexError`` (:issue:`35858`)
- Fixed regression in :class:`DataFrame` and :class:`Series` comparisons between numeric arrays and strings (:issue:`35700`, :issue:`36377`)
- Fixed regression in :meth:`DataFrame.apply` with ``raw=True`` and user-function returning string (:issue:`35940`)
- Fixed regression when setting empty :class:`DataFrame` column to a :class:`Series` in preserving name of index in frame (:issue:`36527`)
- Fixed regression in :class:`Period` incorrect value for ordinal over the maximum timestamp (:issue:`36430`)
- Fixed regression in :func:`read_table` raised ``ValueError`` when ``delim_whitespace`` was set to ``True`` (:issue:`35958`)
- Fixed regression in :meth:`Series.dt.normalize` when normalizing pre-epoch dates the result was shifted one day (:issue:`36294`)

.. ---------------------------------------------------------------------------

.. _whatsnew_113.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in :func:`read_spss` where passing a ``pathlib.Path`` as ``path`` would raise a ``TypeError`` (:issue:`33666`)
- Bug in :meth:`Series.str.startswith` and :meth:`Series.str.endswith` with ``category`` dtype not propagating ``na`` parameter (:issue:`36241`)
- Bug in :class:`Series` constructor where integer overflow would occur for sufficiently large scalar inputs when an index was provided (:issue:`36291`)
- Bug in :meth:`DataFrame.sort_values` raising an ``AttributeError`` when sorting on a key that casts column to categorical dtype (:issue:`36383`)
- Bug in :meth:`DataFrame.stack` raising a ``ValueError`` when stacking :class:`MultiIndex` columns based on position when the levels had duplicate names (:issue:`36353`)
- Bug in :meth:`Series.astype` showing too much precision when casting from ``np.float32`` to string dtype (:issue:`36451`)
- Bug in :meth:`Series.isin` and :meth:`DataFrame.isin` when using ``NaN`` and a row length above 1,000,000 (:issue:`22205`)
- Bug in :func:`cut` raising a ``ValueError`` when passed a :class:`Series` of labels with ``ordered=False`` (:issue:`36603`)

.. ---------------------------------------------------------------------------

.. _whatsnew_113.other:

Other
~~~~~
- Reverted enhancement added in pandas-1.1.0 where :func:`timedelta_range` infers a frequency when passed ``start``, ``stop``, and ``periods`` (:issue:`32377`)

.. ---------------------------------------------------------------------------

.. _whatsnew_113.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.2..v1.1.3
.. _whatsnew_140:

What's new in 1.4.0 (January 22, 2022)
--------------------------------------

These are the changes in pandas 1.4.0. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_140.enhancements:

Enhancements
~~~~~~~~~~~~

.. _whatsnew_140.enhancements.warning_lineno:

Improved warning messages
^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, warning messages may have pointed to lines within the pandas
library. Running the script ``setting_with_copy_warning.py``

.. code-block:: python

    import pandas as pd

    df = pd.DataFrame({'a': [1, 2, 3]})
    df[:2].loc[:, 'a'] = 5

with pandas 1.3 resulted in::

    .../site-packages/pandas/core/indexing.py:1951: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame.

This made it difficult to determine where the warning was being generated from.
Now pandas will inspect the call stack, reporting the first line outside of the
pandas library that gave rise to the warning. The output of the above script is
now::

    setting_with_copy_warning.py:4: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame.




.. _whatsnew_140.enhancements.ExtensionIndex:

Index can hold arbitrary ExtensionArrays
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Until now, passing a custom :class:`ExtensionArray` to ``pd.Index`` would cast
the array to ``object`` dtype. Now :class:`Index` can directly hold arbitrary
ExtensionArrays (:issue:`43930`).

*Previous behavior*:

.. ipython:: python

   arr = pd.array([1, 2, pd.NA])
   idx = pd.Index(arr)

In the old behavior, ``idx`` would be object-dtype:

*Previous behavior*:

.. code-block:: ipython

   In [1]: idx
   Out[1]: Index([1, 2, <NA>], dtype='object')

With the new behavior, we keep the original dtype:

*New behavior*:

.. ipython:: python

   idx

One exception to this is ``SparseArray``, which will continue to cast to numpy
dtype until pandas 2.0. At that point it will retain its dtype like other
ExtensionArrays.

.. _whatsnew_140.enhancements.styler:

Styler
^^^^^^

:class:`.Styler` has been further developed in 1.4.0. The following general enhancements have been made:

  - Styling and formatting of indexes has been added, with :meth:`.Styler.apply_index`, :meth:`.Styler.applymap_index` and :meth:`.Styler.format_index`. These mirror the signature of the methods already used to style and format data values, and work with both HTML, LaTeX and Excel format (:issue:`41893`, :issue:`43101`, :issue:`41993`, :issue:`41995`)
  - The new method :meth:`.Styler.hide` deprecates :meth:`.Styler.hide_index` and :meth:`.Styler.hide_columns` (:issue:`43758`)
  - The keyword arguments ``level`` and ``names`` have been added to :meth:`.Styler.hide` (and implicitly to the deprecated methods :meth:`.Styler.hide_index` and :meth:`.Styler.hide_columns`) for additional control of visibility of MultiIndexes and of Index names (:issue:`25475`, :issue:`43404`, :issue:`43346`)
  - The :meth:`.Styler.export` and :meth:`.Styler.use` have been updated to address all of the added functionality from v1.2.0 and v1.3.0 (:issue:`40675`)
  - Global options under the category ``pd.options.styler`` have been extended to configure default ``Styler`` properties which address formatting, encoding, and HTML and LaTeX rendering. Note that formerly ``Styler`` relied on ``display.html.use_mathjax``, which has now been replaced by ``styler.html.mathjax`` (:issue:`41395`)
  - Validation of certain keyword arguments, e.g. ``caption`` (:issue:`43368`)
  - Various bug fixes as recorded below

Additionally there are specific enhancements to the HTML specific rendering:

  - :meth:`.Styler.bar` introduces additional arguments to control alignment and display (:issue:`26070`, :issue:`36419`), and it also validates the input arguments ``width`` and ``height`` (:issue:`42511`)
  - :meth:`.Styler.to_html` introduces keyword arguments ``sparse_index``, ``sparse_columns``, ``bold_headers``, ``caption``, ``max_rows`` and ``max_columns`` (:issue:`41946`, :issue:`43149`, :issue:`42972`)
  - :meth:`.Styler.to_html` omits CSSStyle rules for hidden table elements as a performance enhancement (:issue:`43619`)
  - Custom CSS classes can now be directly specified without string replacement (:issue:`43686`)
  - Ability to render hyperlinks automatically via a new ``hyperlinks`` formatting keyword argument (:issue:`45058`)

There are also some LaTeX specific enhancements:

  - :meth:`.Styler.to_latex` introduces keyword argument ``environment``, which also allows a specific "longtable" entry through a separate jinja2 template (:issue:`41866`)
  - Naive sparsification is now possible for LaTeX without the necessity of including the multirow package (:issue:`43369`)
  - *cline* support has been added for :class:`MultiIndex` row sparsification through a keyword argument (:issue:`45138`)

.. _whatsnew_140.enhancements.pyarrow_csv_engine:

Multi-threaded CSV reading with a new CSV Engine based on pyarrow
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`pandas.read_csv` now accepts ``engine="pyarrow"`` (requires at least
``pyarrow`` 1.0.1) as an argument, allowing for faster csv parsing on multicore
machines with pyarrow installed. See the :doc:`I/O docs </user_guide/io>` for
more info. (:issue:`23697`, :issue:`43706`)

.. _whatsnew_140.enhancements.window_rank:

Rank function for rolling and expanding windows
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Added ``rank`` function to :class:`Rolling` and :class:`Expanding`. The new
function supports the ``method``, ``ascending``, and ``pct`` flags of
:meth:`DataFrame.rank`. The ``method`` argument supports ``min``, ``max``, and
``average`` ranking methods.
Example:

.. ipython:: python

    s = pd.Series([1, 4, 2, 3, 5, 3])
    s.rolling(3).rank()

    s.rolling(3).rank(method="max")

.. _whatsnew_140.enhancements.groupby_indexing:

Groupby positional indexing
^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is now possible to specify positional ranges relative to the ends of each
group.

Negative arguments for :meth:`.GroupBy.head` and :meth:`.GroupBy.tail` now work
correctly and result in ranges relative to the end and start of each group,
respectively. Previously, negative arguments returned empty frames.

.. ipython:: python

    df = pd.DataFrame([["g", "g0"], ["g", "g1"], ["g", "g2"], ["g", "g3"],
                       ["h", "h0"], ["h", "h1"]], columns=["A", "B"])
    df.groupby("A").head(-1)


:meth:`.GroupBy.nth` now accepts a slice or list of integers and slices.

.. ipython:: python

    df.groupby("A").nth(slice(1, -1))
    df.groupby("A").nth([slice(None, 1), slice(-1, None)])

:meth:`.GroupBy.nth` now accepts index notation.

.. ipython:: python

    df.groupby("A").nth[1, -1]
    df.groupby("A").nth[1:-1]
    df.groupby("A").nth[:1, -1:]

.. _whatsnew_140.dict_tight:

DataFrame.from_dict and DataFrame.to_dict have new ``'tight'`` option
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A new ``'tight'`` dictionary format that preserves :class:`MultiIndex` entries
and names is now available with the :meth:`DataFrame.from_dict` and
:meth:`DataFrame.to_dict` methods and can be used with the standard ``json``
library to produce a tight representation of :class:`DataFrame` objects
(:issue:`4889`).

.. ipython:: python

    df = pd.DataFrame.from_records(
        [[1, 3], [2, 4]],
        index=pd.MultiIndex.from_tuples([("a", "b"), ("a", "c")],
                                        names=["n1", "n2"]),
        columns=pd.MultiIndex.from_tuples([("x", 1), ("y", 2)],
                                          names=["z1", "z2"]),
    )
    df
    df.to_dict(orient='tight')

.. _whatsnew_140.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^
- :meth:`concat` will preserve the ``attrs`` when it is the same for all objects and discard the ``attrs`` when they are different (:issue:`41828`)
- :class:`DataFrameGroupBy` operations with ``as_index=False`` now correctly retain ``ExtensionDtype`` dtypes for columns being grouped on (:issue:`41373`)
- Add support for assigning values to ``by`` argument in :meth:`DataFrame.plot.hist` and :meth:`DataFrame.plot.box` (:issue:`15079`)
- :meth:`Series.sample`, :meth:`DataFrame.sample`, and :meth:`.GroupBy.sample` now accept a ``np.random.Generator`` as input to ``random_state``. A generator will be more performant, especially with ``replace=False`` (:issue:`38100`)
- :meth:`Series.ewm` and :meth:`DataFrame.ewm` now support a ``method`` argument with a ``'table'`` option that performs the windowing operation over an entire :class:`DataFrame`. See :ref:`Window Overview <window.overview>` for performance and functional benefits (:issue:`42273`)
- :meth:`.GroupBy.cummin` and :meth:`.GroupBy.cummax` now support the argument ``skipna`` (:issue:`34047`)
- :meth:`read_table` now supports the argument ``storage_options`` (:issue:`39167`)
- :meth:`DataFrame.to_stata` and :meth:`StataWriter` now accept the keyword only argument ``value_labels`` to save labels for non-categorical columns (:issue:`38454`)
- Methods that relied on hashmap based algos such as :meth:`DataFrameGroupBy.value_counts`, :meth:`DataFrameGroupBy.count` and :func:`factorize` ignored imaginary component for complex numbers (:issue:`17927`)
- Add :meth:`Series.str.removeprefix` and :meth:`Series.str.removesuffix` introduced in Python 3.9 to remove pre-/suffixes from string-type :class:`Series` (:issue:`36944`)
- Attempting to write into a file in missing parent directory with :meth:`DataFrame.to_csv`, :meth:`DataFrame.to_html`, :meth:`DataFrame.to_excel`, :meth:`DataFrame.to_feather`, :meth:`DataFrame.to_parquet`, :meth:`DataFrame.to_stata`, :meth:`DataFrame.to_json`, :meth:`DataFrame.to_pickle`, and :meth:`DataFrame.to_xml` now explicitly mentions missing parent directory, the same is true for :class:`Series` counterparts (:issue:`24306`)
- Indexing with ``.loc`` and ``.iloc`` now supports ``Ellipsis`` (:issue:`37750`)
- :meth:`IntegerArray.all` , :meth:`IntegerArray.any`, :meth:`FloatingArray.any`, and :meth:`FloatingArray.all` use Kleene logic (:issue:`41967`)
- Added support for nullable boolean and integer types in :meth:`DataFrame.to_stata`, :class:`~pandas.io.stata.StataWriter`, :class:`~pandas.io.stata.StataWriter117`, and :class:`~pandas.io.stata.StataWriterUTF8` (:issue:`40855`)
- :meth:`DataFrame.__pos__` and :meth:`DataFrame.__neg__` now retain ``ExtensionDtype`` dtypes (:issue:`43883`)
- The error raised when an optional dependency can't be imported now includes the original exception, for easier investigation (:issue:`43882`)
- Added :meth:`.ExponentialMovingWindow.sum` (:issue:`13297`)
- :meth:`Series.str.split` now supports a ``regex`` argument that explicitly specifies whether the pattern is a regular expression. Default is ``None`` (:issue:`43563`, :issue:`32835`, :issue:`25549`)
- :meth:`DataFrame.dropna` now accepts a single label as ``subset`` along with array-like (:issue:`41021`)
- Added :meth:`DataFrameGroupBy.value_counts` (:issue:`43564`)
- :func:`read_csv` now accepts a ``callable`` function in ``on_bad_lines`` when ``engine="python"`` for custom handling of bad lines (:issue:`5686`)
- :class:`ExcelWriter` argument ``if_sheet_exists="overlay"`` option added (:issue:`40231`)
- :meth:`read_excel` now accepts a ``decimal`` argument that allow the user to specify the decimal point when parsing string columns to numeric (:issue:`14403`)
- :meth:`.GroupBy.mean`, :meth:`.GroupBy.std`, :meth:`.GroupBy.var`, and :meth:`.GroupBy.sum` now support `Numba <http://numba.pydata.org/>`_ execution with the ``engine`` keyword (:issue:`43731`, :issue:`44862`, :issue:`44939`)
- :meth:`Timestamp.isoformat` now handles the ``timespec`` argument from the base ``datetime`` class (:issue:`26131`)
- :meth:`NaT.to_numpy` ``dtype`` argument is now respected, so ``np.timedelta64`` can be returned (:issue:`44460`)
- New option ``display.max_dir_items`` customizes the number of columns added to :meth:`Dataframe.__dir__` and suggested for tab completion (:issue:`37996`)
- Added "Juneteenth National Independence Day" to ``USFederalHolidayCalendar`` (:issue:`44574`)
- :meth:`.Rolling.var`, :meth:`.Expanding.var`, :meth:`.Rolling.std`, and :meth:`.Expanding.std` now support `Numba <http://numba.pydata.org/>`_ execution with the ``engine`` keyword (:issue:`44461`)
- :meth:`Series.info` has been added, for compatibility with :meth:`DataFrame.info` (:issue:`5167`)
- Implemented :meth:`IntervalArray.min` and :meth:`IntervalArray.max`, as a result of which ``min`` and ``max`` now work for :class:`IntervalIndex`, :class:`Series` and :class:`DataFrame` with ``IntervalDtype`` (:issue:`44746`)
- :meth:`UInt64Index.map` now retains ``dtype`` where possible (:issue:`44609`)
- :meth:`read_json` can now parse unsigned long long integers (:issue:`26068`)
- :meth:`DataFrame.take` now raises a ``TypeError`` when passed a scalar for the indexer (:issue:`42875`)
- :meth:`is_list_like` now identifies duck-arrays as list-like unless ``.ndim == 0`` (:issue:`35131`)
- :class:`ExtensionDtype` and :class:`ExtensionArray` are now (de)serialized when exporting a :class:`DataFrame` with :meth:`DataFrame.to_json` using ``orient='table'`` (:issue:`20612`, :issue:`44705`)
- Add support for `Zstandard <http://facebook.github.io/zstd/>`_ compression to :meth:`DataFrame.to_pickle`/:meth:`read_pickle` and friends (:issue:`43925`)
- :meth:`DataFrame.to_sql` now returns an ``int`` of the number of written rows (:issue:`23998`)

.. ---------------------------------------------------------------------------

.. _whatsnew_140.notable_bug_fixes:

Notable bug fixes
~~~~~~~~~~~~~~~~~

These are bug fixes that might have notable behavior changes.

.. _whatsnew_140.notable_bug_fixes.inconsistent_date_string_parsing:

Inconsistent date string parsing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``dayfirst`` option of :func:`to_datetime` isn't strict, and this can lead
to surprising behavior:

.. ipython:: python
    :okwarning:

    pd.to_datetime(["31-12-2021"], dayfirst=False)

Now, a warning will be raised if a date string cannot be parsed accordance to
the given ``dayfirst`` value when the value is a delimited date string (e.g.
``31-12-2012``).

.. _whatsnew_140.notable_bug_fixes.concat_with_empty_or_all_na:

Ignoring dtypes in concat with empty or all-NA columns
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When using :func:`concat` to concatenate two or more :class:`DataFrame` objects,
if one of the DataFrames was empty or had all-NA values, its dtype was
*sometimes* ignored when finding the concatenated dtype.  These are now
consistently *not* ignored (:issue:`43507`).

.. ipython:: python

    df1 = pd.DataFrame({"bar": [pd.Timestamp("2013-01-01")]}, index=range(1))
    df2 = pd.DataFrame({"bar": np.nan}, index=range(1, 2))
    res = pd.concat([df1, df2])

Previously, the float-dtype in ``df2`` would be ignored so the result dtype
would be ``datetime64[ns]``. As a result, the ``np.nan`` would be cast to
``NaT``.

*Previous behavior*:

.. code-block:: ipython

    In [4]: res
    Out[4]:
             bar
    0 2013-01-01
    1        NaT

Now the float-dtype is respected. Since the common dtype for these DataFrames is
object, the ``np.nan`` is retained.

*New behavior*:

.. ipython:: python

    res

.. _whatsnew_140.notable_bug_fixes.value_counts_and_mode_do_not_coerce_to_nan:

Null-values are no longer coerced to NaN-value in value_counts and mode
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`Series.value_counts` and :meth:`Series.mode` no longer coerce ``None``,
``NaT`` and other null-values to a NaN-value for ``np.object``-dtype. This
behavior is now consistent with ``unique``, ``isin`` and others
(:issue:`42688`).

.. ipython:: python

    s = pd.Series([True, None, pd.NaT, None, pd.NaT, None])
    res = s.value_counts(dropna=False)

Previously, all null-values were replaced by a NaN-value.

*Previous behavior*:

.. code-block:: ipython

    In [3]: res
    Out[3]:
    NaN     5
    True    1
    dtype: int64

Now null-values are no longer mangled.

*New behavior*:

.. ipython:: python

    res

.. _whatsnew_140.notable_bug_fixes.read_csv_mangle_dup_cols:

mangle_dupe_cols in read_csv no longer renames unique columns conflicting with target names
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`read_csv` no longer renames unique column labels which conflict with the target
names of duplicated columns. Already existing columns are skipped, i.e. the next
available index is used for the target column name (:issue:`14704`).

.. ipython:: python

    import io

    data = "a,a,a.1\n1,2,3"
    res = pd.read_csv(io.StringIO(data))

Previously, the second column was called ``a.1``, while the third column was
also renamed to ``a.1.1``.

*Previous behavior*:

.. code-block:: ipython

    In [3]: res
    Out[3]:
        a  a.1  a.1.1
    0   1    2      3

Now the renaming checks if ``a.1`` already exists when changing the name of the
second column and jumps this index. The second column is instead renamed to
``a.2``.

*New behavior*:

.. ipython:: python

    res

.. _whatsnew_140.notable_bug_fixes.unstack_pivot_int32_limit:

unstack and pivot_table no longer raises ValueError for result that would exceed int32 limit
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously :meth:`DataFrame.pivot_table` and :meth:`DataFrame.unstack` would
raise a ``ValueError`` if the operation could produce a result with more than
``2**31 - 1`` elements. This operation now raises a
:class:`errors.PerformanceWarning` instead (:issue:`26314`).

*Previous behavior*:

.. code-block:: ipython

    In [3]: df = DataFrame({"ind1": np.arange(2 ** 16), "ind2": np.arange(2 ** 16), "count": 0})
    In [4]: df.pivot_table(index="ind1", columns="ind2", values="count", aggfunc="count")
    ValueError: Unstacked DataFrame is too big, causing int32 overflow

*New behavior*:

.. code-block:: python

    In [4]: df.pivot_table(index="ind1", columns="ind2", values="count", aggfunc="count")
    PerformanceWarning: The following operation may generate 4294967296 cells in the resulting pandas object.

.. ---------------------------------------------------------------------------

.. _whatsnew_140.notable_bug_fixes.groupby_apply_mutation:

groupby.apply consistent transform detection
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`.GroupBy.apply` is designed to be flexible, allowing users to perform
aggregations, transformations, filters, and use it with user-defined functions
that might not fall into any of these categories. As part of this, apply will
attempt to detect when an operation is a transform, and in such a case, the
result will have the same index as the input. In order to determine if the
operation is a transform, pandas compares the input's index to the result's and
determines if it has been mutated. Previously in pandas 1.3, different code
paths used different definitions of "mutated": some would use Python's ``is``
whereas others would test only up to equality.

This inconsistency has been removed, pandas now tests up to equality.

.. ipython:: python

    def func(x):
        return x.copy()

    df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})
    df

*Previous behavior*:

.. code-block:: ipython

    In [3]: df.groupby(['a']).apply(func)
    Out[3]:
         a  b  c
    a
    1 0  1  3  5
    2 1  2  4  6

    In [4]: df.set_index(['a', 'b']).groupby(['a']).apply(func)
    Out[4]:
         c
    a b
    1 3  5
    2 4  6

In the examples above, the first uses a code path where pandas uses ``is`` and
determines that ``func`` is not a transform whereas the second tests up to
equality and determines that ``func`` is a transform. In the first case, the
result's index is not the same as the input's.

*New behavior*:

.. ipython:: python

    df.groupby(['a']).apply(func)
    df.set_index(['a', 'b']).groupby(['a']).apply(func)

Now in both cases it is determined that ``func`` is a transform. In each case,
the result has the same index as the input.

.. _whatsnew_140.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_140.api_breaking.python:

Increased minimum version for Python
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas 1.4.0 supports Python 3.8 and higher.

.. _whatsnew_140.api_breaking.deps:

Increased minimum versions for dependencies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Some minimum supported versions of dependencies were updated.
If installed, we now require:

+-----------------+-----------------+----------+---------+
| Package         | Minimum Version | Required | Changed |
+=================+=================+==========+=========+
| numpy           | 1.18.5          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| pytz            | 2020.1          |    X     |    X    |
+-----------------+-----------------+----------+---------+
| python-dateutil | 2.8.1           |    X     |    X    |
+-----------------+-----------------+----------+---------+
| bottleneck      | 1.3.1           |          |    X    |
+-----------------+-----------------+----------+---------+
| numexpr         | 2.7.1           |          |    X    |
+-----------------+-----------------+----------+---------+
| pytest (dev)    | 6.0             |          |         |
+-----------------+-----------------+----------+---------+
| mypy (dev)      | 0.930           |          |    X    |
+-----------------+-----------------+----------+---------+

For `optional libraries
<https://pandas.pydata.org/docs/getting_started/install.html>`_ the general
recommendation is to use the latest version. The following table lists the
lowest version per library that is currently being tested throughout the
development of pandas. Optional libraries below the lowest tested version may
still work, but are not considered supported.

+-----------------+-----------------+---------+
| Package         | Minimum Version | Changed |
+=================+=================+=========+
| beautifulsoup4  | 4.8.2           |    X    |
+-----------------+-----------------+---------+
| fastparquet     | 0.4.0           |         |
+-----------------+-----------------+---------+
| fsspec          | 0.7.4           |         |
+-----------------+-----------------+---------+
| gcsfs           | 0.6.0           |         |
+-----------------+-----------------+---------+
| lxml            | 4.5.0           |    X    |
+-----------------+-----------------+---------+
| matplotlib      | 3.3.2           |    X    |
+-----------------+-----------------+---------+
| numba           | 0.50.1          |    X    |
+-----------------+-----------------+---------+
| openpyxl        | 3.0.3           |    X    |
+-----------------+-----------------+---------+
| pandas-gbq      | 0.14.0          |    X    |
+-----------------+-----------------+---------+
| pyarrow         | 1.0.1           |    X    |
+-----------------+-----------------+---------+
| pymysql         | 0.10.1          |    X    |
+-----------------+-----------------+---------+
| pytables        | 3.6.1           |    X    |
+-----------------+-----------------+---------+
| s3fs            | 0.4.0           |         |
+-----------------+-----------------+---------+
| scipy           | 1.4.1           |    X    |
+-----------------+-----------------+---------+
| sqlalchemy      | 1.4.0           |    X    |
+-----------------+-----------------+---------+
| tabulate        | 0.8.7           |         |
+-----------------+-----------------+---------+
| xarray          | 0.15.1          |    X    |
+-----------------+-----------------+---------+
| xlrd            | 2.0.1           |    X    |
+-----------------+-----------------+---------+
| xlsxwriter      | 1.2.2           |    X    |
+-----------------+-----------------+---------+
| xlwt            | 1.3.0           |         |
+-----------------+-----------------+---------+

See :ref:`install.dependencies` and :ref:`install.optional_dependencies` for more.

.. _whatsnew_140.api_breaking.other:

Other API changes
^^^^^^^^^^^^^^^^^
- :meth:`Index.get_indexer_for` no longer accepts keyword arguments (other than ``target``); in the past these would be silently ignored if the index was not unique (:issue:`42310`)
- Change in the position of the ``min_rows`` argument in :meth:`DataFrame.to_string` due to change in the docstring (:issue:`44304`)
- Reduction operations for :class:`DataFrame` or :class:`Series` now raising a ``ValueError`` when ``None`` is passed for ``skipna`` (:issue:`44178`)
- :func:`read_csv` and :func:`read_html` no longer raising an error when one of the header rows consists only of ``Unnamed:`` columns (:issue:`13054`)
- Changed the ``name`` attribute of several holidays in
  ``USFederalHolidayCalendar`` to match `official federal holiday
  names <https://www.opm.gov/policy-data-oversight/pay-leave/federal-holidays/>`_
  specifically:

   - "New Year's Day" gains the possessive apostrophe
   - "Presidents Day" becomes "Washington's Birthday"
   - "Martin Luther King Jr. Day" is now "Birthday of Martin Luther King, Jr."
   - "July 4th" is now "Independence Day"
   - "Thanksgiving" is now "Thanksgiving Day"
   - "Christmas" is now "Christmas Day"
   - Added "Juneteenth National Independence Day"

.. ---------------------------------------------------------------------------

.. _whatsnew_140.deprecations:

Deprecations
~~~~~~~~~~~~

.. _whatsnew_140.deprecations.int64_uint64_float64index:

Deprecated Int64Index, UInt64Index & Float64Index
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Int64Index`, :class:`UInt64Index` and :class:`Float64Index` have been
deprecated in favor of the base :class:`Index` class and will be removed in
Pandas 2.0 (:issue:`43028`).

For constructing a numeric index, you can use the base :class:`Index` class
instead specifying the data type (which will also work on older pandas
releases):

.. code-block:: python

    # replace
    pd.Int64Index([1, 2, 3])
    # with
    pd.Index([1, 2, 3], dtype="int64")

For checking the data type of an index object, you can replace ``isinstance``
checks with checking the ``dtype``:

.. code-block:: python

    # replace
    isinstance(idx, pd.Int64Index)
    # with
    idx.dtype == "int64"

Currently, in order to maintain backward compatibility, calls to :class:`Index`
will continue to return :class:`Int64Index`, :class:`UInt64Index` and
:class:`Float64Index` when given numeric data, but in the future, an
:class:`Index` will be returned.

*Current behavior*:

.. code-block:: ipython

    In [1]: pd.Index([1, 2, 3], dtype="int32")
    Out [1]: Int64Index([1, 2, 3], dtype='int64')
    In [1]: pd.Index([1, 2, 3], dtype="uint64")
    Out [1]: UInt64Index([1, 2, 3], dtype='uint64')

*Future behavior*:

.. code-block:: ipython

    In [3]: pd.Index([1, 2, 3], dtype="int32")
    Out [3]: Index([1, 2, 3], dtype='int32')
    In [4]: pd.Index([1, 2, 3], dtype="uint64")
    Out [4]: Index([1, 2, 3], dtype='uint64')


.. _whatsnew_140.deprecations.frame_series_append:

Deprecated Frame.append and Series.append
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.append` and :meth:`Series.append` have been deprecated and will
be removed in Pandas 2.0. Use :func:`pandas.concat` instead (:issue:`35407`).

*Deprecated syntax*

.. code-block:: ipython

    In [1]: pd.Series([1, 2]).append(pd.Series([3, 4])
    Out [1]:
    <stdin>:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
    0    1
    1    2
    0    3
    1    4
    dtype: int64

    In [2]: df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
    In [3]: df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
    In [4]: df1.append(df2)
    Out [4]:
    <stdin>:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
       A  B
    0  1  2
    1  3  4
    0  5  6
    1  7  8

*Recommended syntax*

.. ipython:: python

    pd.concat([pd.Series([1, 2]), pd.Series([3, 4])])

    df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))
    df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))
    pd.concat([df1, df2])


.. _whatsnew_140.deprecations.other:

Other Deprecations
^^^^^^^^^^^^^^^^^^
- Deprecated :meth:`Index.is_type_compatible` (:issue:`42113`)
- Deprecated ``method`` argument in :meth:`Index.get_loc`, use ``index.get_indexer([label], method=...)`` instead (:issue:`42269`)
- Deprecated treating integer keys in :meth:`Series.__setitem__` as positional when the index is a :class:`Float64Index` not containing the key, a :class:`IntervalIndex` with no entries containing the key, or a :class:`MultiIndex` with leading :class:`Float64Index` level not containing the key (:issue:`33469`)
- Deprecated treating ``numpy.datetime64`` objects as UTC times when passed to the :class:`Timestamp` constructor along with a timezone. In a future version, these will be treated as wall-times. To retain the old behavior, use ``Timestamp(dt64).tz_localize("UTC").tz_convert(tz)`` (:issue:`24559`)
- Deprecated ignoring missing labels when indexing with a sequence of labels on a level of a :class:`MultiIndex` (:issue:`42351`)
- Creating an empty :class:`Series` without a ``dtype`` will now raise a more visible ``FutureWarning`` instead of a ``DeprecationWarning`` (:issue:`30017`)
- Deprecated the ``kind`` argument in :meth:`Index.get_slice_bound`, :meth:`Index.slice_indexer`, and :meth:`Index.slice_locs`; in a future version passing ``kind`` will raise (:issue:`42857`)
- Deprecated dropping of nuisance columns in :class:`Rolling`, :class:`Expanding`, and :class:`EWM` aggregations (:issue:`42738`)
- Deprecated :meth:`Index.reindex` with a non-unique :class:`Index` (:issue:`42568`)
- Deprecated :meth:`.Styler.render` in favor of :meth:`.Styler.to_html` (:issue:`42140`)
- Deprecated :meth:`.Styler.hide_index` and :meth:`.Styler.hide_columns` in favor of :meth:`.Styler.hide` (:issue:`43758`)
- Deprecated passing in a string column label into ``times`` in :meth:`DataFrame.ewm` (:issue:`43265`)
- Deprecated the ``include_start`` and ``include_end`` arguments in :meth:`DataFrame.between_time`; in a future version passing ``include_start`` or ``include_end`` will raise (:issue:`40245`)
- Deprecated the ``squeeze`` argument to :meth:`read_csv`, :meth:`read_table`, and :meth:`read_excel`. Users should squeeze the :class:`DataFrame` afterwards with ``.squeeze("columns")`` instead (:issue:`43242`)
- Deprecated the ``index`` argument to :class:`SparseArray` construction (:issue:`23089`)
- Deprecated the ``closed`` argument in :meth:`date_range` and :meth:`bdate_range` in favor of ``inclusive`` argument; In a future version passing ``closed`` will raise (:issue:`40245`)
- Deprecated :meth:`.Rolling.validate`, :meth:`.Expanding.validate`, and :meth:`.ExponentialMovingWindow.validate` (:issue:`43665`)
- Deprecated silent dropping of columns that raised a ``TypeError`` in :class:`Series.transform` and :class:`DataFrame.transform` when used with a dictionary (:issue:`43740`)
- Deprecated silent dropping of columns that raised a ``TypeError``, ``DataError``, and some cases of ``ValueError`` in :meth:`Series.aggregate`, :meth:`DataFrame.aggregate`, :meth:`Series.groupby.aggregate`, and :meth:`DataFrame.groupby.aggregate` when used with a list (:issue:`43740`)
- Deprecated casting behavior when setting timezone-aware value(s) into a timezone-aware :class:`Series` or :class:`DataFrame` column when the timezones do not match. Previously this cast to object dtype. In a future version, the values being inserted will be converted to the series or column's existing timezone (:issue:`37605`)
- Deprecated casting behavior when passing an item with mismatched-timezone to :meth:`DatetimeIndex.insert`, :meth:`DatetimeIndex.putmask`, :meth:`DatetimeIndex.where` :meth:`DatetimeIndex.fillna`, :meth:`Series.mask`, :meth:`Series.where`, :meth:`Series.fillna`, :meth:`Series.shift`, :meth:`Series.replace`, :meth:`Series.reindex` (and :class:`DataFrame` column analogues). In the past this has cast to object ``dtype``. In a future version, these will cast the passed item to the index or series's timezone (:issue:`37605`, :issue:`44940`)
- Deprecated the ``prefix`` keyword argument in :func:`read_csv` and :func:`read_table`, in a future version the argument will be removed (:issue:`43396`)
- Deprecated passing non boolean argument to ``sort`` in :func:`concat` (:issue:`41518`)
- Deprecated passing arguments as positional for :func:`read_fwf` other than ``filepath_or_buffer`` (:issue:`41485`)
- Deprecated passing arguments as positional for :func:`read_xml` other than ``path_or_buffer`` (:issue:`45133`)
- Deprecated passing ``skipna=None`` for :meth:`DataFrame.mad` and :meth:`Series.mad`, pass ``skipna=True`` instead (:issue:`44580`)
- Deprecated the behavior of :func:`to_datetime` with the string "now" with ``utc=False``; in a future version this will match ``Timestamp("now")``, which in turn matches :meth:`Timestamp.now` returning the local time (:issue:`18705`)
- Deprecated :meth:`DateOffset.apply`, use ``offset + other`` instead (:issue:`44522`)
- Deprecated parameter ``names`` in :meth:`Index.copy` (:issue:`44916`)
- A deprecation warning is now shown for :meth:`DataFrame.to_latex` indicating the arguments signature may change and emulate more the arguments to :meth:`.Styler.to_latex` in future versions (:issue:`44411`)
- Deprecated behavior of :func:`concat` between objects with bool-dtype and numeric-dtypes; in a future version these will cast to object dtype instead of coercing bools to numeric values (:issue:`39817`)
- Deprecated :meth:`Categorical.replace`, use :meth:`Series.replace` instead (:issue:`44929`)
- Deprecated passing ``set`` or ``dict`` as indexer for :meth:`DataFrame.loc.__setitem__`, :meth:`DataFrame.loc.__getitem__`, :meth:`Series.loc.__setitem__`, :meth:`Series.loc.__getitem__`, :meth:`DataFrame.__getitem__`, :meth:`Series.__getitem__` and :meth:`Series.__setitem__` (:issue:`42825`)
- Deprecated :meth:`Index.__getitem__` with a bool key; use ``index.values[key]`` to get the old behavior (:issue:`44051`)
- Deprecated downcasting column-by-column in :meth:`DataFrame.where` with integer-dtypes (:issue:`44597`)
- Deprecated :meth:`DatetimeIndex.union_many`, use :meth:`DatetimeIndex.union` instead (:issue:`44091`)
- Deprecated :meth:`.Groupby.pad` in favor of :meth:`.Groupby.ffill` (:issue:`33396`)
- Deprecated :meth:`.Groupby.backfill` in favor of :meth:`.Groupby.bfill` (:issue:`33396`)
- Deprecated :meth:`.Resample.pad` in favor of :meth:`.Resample.ffill` (:issue:`33396`)
- Deprecated :meth:`.Resample.backfill` in favor of :meth:`.Resample.bfill` (:issue:`33396`)
- Deprecated ``numeric_only=None`` in :meth:`DataFrame.rank`; in a future version ``numeric_only`` must be either ``True`` or ``False`` (the default) (:issue:`45036`)
- Deprecated the behavior of :meth:`Timestamp.utcfromtimestamp`, in the future it will return a timezone-aware UTC :class:`Timestamp` (:issue:`22451`)
- Deprecated :meth:`NaT.freq` (:issue:`45071`)
- Deprecated behavior of :class:`Series` and :class:`DataFrame` construction when passed float-dtype data containing ``NaN`` and an integer dtype ignoring the dtype argument; in a future version this will raise (:issue:`40110`)
- Deprecated the behaviour of :meth:`Series.to_frame` and :meth:`Index.to_frame` to ignore the ``name`` argument when ``name=None``. Currently, this means to preserve the existing name, but in the future explicitly passing ``name=None`` will set ``None`` as the name of the column in the resulting DataFrame (:issue:`44212`)

.. ---------------------------------------------------------------------------

.. _whatsnew_140.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~
- Performance improvement in :meth:`.GroupBy.sample`, especially when ``weights`` argument provided (:issue:`34483`)
- Performance improvement when converting non-string arrays to string arrays (:issue:`34483`)
- Performance improvement in :meth:`.GroupBy.transform` for user-defined functions (:issue:`41598`)
- Performance improvement in constructing :class:`DataFrame` objects (:issue:`42631`, :issue:`43142`, :issue:`43147`, :issue:`43307`, :issue:`43144`, :issue:`44826`)
- Performance improvement in :meth:`GroupBy.shift` when ``fill_value`` argument is provided (:issue:`26615`)
- Performance improvement in :meth:`DataFrame.corr` for ``method=pearson`` on data without missing values (:issue:`40956`)
- Performance improvement in some :meth:`GroupBy.apply` operations (:issue:`42992`, :issue:`43578`)
- Performance improvement in :func:`read_stata` (:issue:`43059`, :issue:`43227`)
- Performance improvement in :func:`read_sas` (:issue:`43333`)
- Performance improvement in :meth:`to_datetime` with ``uint`` dtypes (:issue:`42606`)
- Performance improvement in :meth:`to_datetime` with ``infer_datetime_format`` set to ``True`` (:issue:`43901`)
- Performance improvement in :meth:`Series.sparse.to_coo` (:issue:`42880`)
- Performance improvement in indexing with a :class:`UInt64Index` (:issue:`43862`)
- Performance improvement in indexing with a :class:`Float64Index` (:issue:`43705`)
- Performance improvement in indexing with a non-unique :class:`Index` (:issue:`43792`)
- Performance improvement in indexing with a listlike indexer on a :class:`MultiIndex` (:issue:`43370`)
- Performance improvement in indexing with a :class:`MultiIndex` indexer on another :class:`MultiIndex` (:issue:`43370`)
- Performance improvement in :meth:`GroupBy.quantile` (:issue:`43469`, :issue:`43725`)
- Performance improvement in :meth:`GroupBy.count` (:issue:`43730`, :issue:`43694`)
- Performance improvement in :meth:`GroupBy.any` and :meth:`GroupBy.all` (:issue:`43675`, :issue:`42841`)
- Performance improvement in :meth:`GroupBy.std` (:issue:`43115`, :issue:`43576`)
- Performance improvement in :meth:`GroupBy.cumsum` (:issue:`43309`)
- :meth:`SparseArray.min` and :meth:`SparseArray.max` no longer require converting to a dense array (:issue:`43526`)
- Indexing into a :class:`SparseArray` with a ``slice`` with ``step=1`` no longer requires converting to a dense array (:issue:`43777`)
- Performance improvement in :meth:`SparseArray.take` with ``allow_fill=False`` (:issue:`43654`)
- Performance improvement in :meth:`.Rolling.mean`, :meth:`.Expanding.mean`, :meth:`.Rolling.sum`, :meth:`.Expanding.sum`, :meth:`.Rolling.max`, :meth:`.Expanding.max`, :meth:`.Rolling.min` and :meth:`.Expanding.min` with ``engine="numba"`` (:issue:`43612`, :issue:`44176`, :issue:`45170`)
- Improved performance of :meth:`pandas.read_csv` with ``memory_map=True`` when file encoding is UTF-8 (:issue:`43787`)
- Performance improvement in :meth:`RangeIndex.sort_values` overriding :meth:`Index.sort_values` (:issue:`43666`)
- Performance improvement in :meth:`RangeIndex.insert` (:issue:`43988`)
- Performance improvement in :meth:`Index.insert` (:issue:`43953`)
- Performance improvement in :meth:`DatetimeIndex.tolist` (:issue:`43823`)
- Performance improvement in :meth:`DatetimeIndex.union` (:issue:`42353`)
- Performance improvement in :meth:`Series.nsmallest` (:issue:`43696`)
- Performance improvement in :meth:`DataFrame.insert` (:issue:`42998`)
- Performance improvement in :meth:`DataFrame.dropna` (:issue:`43683`)
- Performance improvement in :meth:`DataFrame.fillna` (:issue:`43316`)
- Performance improvement in :meth:`DataFrame.values` (:issue:`43160`)
- Performance improvement in :meth:`DataFrame.select_dtypes` (:issue:`42611`)
- Performance improvement in :class:`DataFrame` reductions (:issue:`43185`, :issue:`43243`, :issue:`43311`, :issue:`43609`)
- Performance improvement in :meth:`Series.unstack` and :meth:`DataFrame.unstack` (:issue:`43335`, :issue:`43352`, :issue:`42704`, :issue:`43025`)
- Performance improvement in :meth:`Series.to_frame` (:issue:`43558`)
- Performance improvement in :meth:`Series.mad` (:issue:`43010`)
- Performance improvement in :func:`merge` (:issue:`43332`)
- Performance improvement in :func:`to_csv` when index column is a datetime and is formatted (:issue:`39413`)
- Performance improvement in :func:`to_csv` when :class:`MultiIndex` contains a lot of unused levels (:issue:`37484`)
- Performance improvement in :func:`read_csv` when ``index_col`` was set with a numeric column (:issue:`44158`)
- Performance improvement in :func:`concat` (:issue:`43354`)
- Performance improvement in :meth:`SparseArray.__getitem__` (:issue:`23122`)
- Performance improvement in constructing a :class:`DataFrame` from array-like objects like a ``Pytorch`` tensor (:issue:`44616`)

.. ---------------------------------------------------------------------------

.. _whatsnew_140.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^
- Bug in setting dtype-incompatible values into a :class:`Categorical` (or ``Series`` or ``DataFrame`` backed by ``Categorical``) raising ``ValueError`` instead of ``TypeError`` (:issue:`41919`)
- Bug in :meth:`Categorical.searchsorted` when passing a dtype-incompatible value raising ``KeyError`` instead of ``TypeError`` (:issue:`41919`)
- Bug in :meth:`Categorical.astype` casting datetimes and :class:`Timestamp` to int for dtype ``object`` (:issue:`44930`)
- Bug in :meth:`Series.where` with ``CategoricalDtype`` when passing a dtype-incompatible value raising ``ValueError`` instead of ``TypeError`` (:issue:`41919`)
- Bug in :meth:`Categorical.fillna` when passing a dtype-incompatible value raising ``ValueError`` instead of ``TypeError`` (:issue:`41919`)
- Bug in :meth:`Categorical.fillna` with a tuple-like category raising ``ValueError`` instead of ``TypeError`` when filling with a non-category tuple (:issue:`41919`)

Datetimelike
^^^^^^^^^^^^
- Bug in :class:`DataFrame` constructor unnecessarily copying non-datetimelike 2D object arrays (:issue:`39272`)
- Bug in :func:`to_datetime` with ``format`` and ``pandas.NA`` was raising ``ValueError`` (:issue:`42957`)
- :func:`to_datetime` would silently swap ``MM/DD/YYYY`` and ``DD/MM/YYYY`` formats if the given ``dayfirst`` option could not be respected - now, a warning is raised in the case of delimited date strings (e.g. ``31-12-2012``) (:issue:`12585`)
- Bug in :meth:`date_range` and :meth:`bdate_range` do not return right bound when ``start`` = ``end`` and set is closed on one side (:issue:`43394`)
- Bug in inplace addition and subtraction of :class:`DatetimeIndex` or :class:`TimedeltaIndex` with :class:`DatetimeArray` or :class:`TimedeltaArray` (:issue:`43904`)
- Bug in calling ``np.isnan``, ``np.isfinite``, or ``np.isinf`` on a timezone-aware :class:`DatetimeIndex` incorrectly raising ``TypeError`` (:issue:`43917`)
- Bug in constructing a :class:`Series` from datetime-like strings with mixed timezones incorrectly partially-inferring datetime values (:issue:`40111`)
- Bug in addition of a :class:`Tick` object and a ``np.timedelta64`` object incorrectly raising instead of returning :class:`Timedelta` (:issue:`44474`)
- ``np.maximum.reduce`` and ``np.minimum.reduce`` now correctly return :class:`Timestamp` and :class:`Timedelta` objects when operating on :class:`Series`, :class:`DataFrame`, or :class:`Index` with ``datetime64[ns]`` or ``timedelta64[ns]`` dtype (:issue:`43923`)
- Bug in adding a ``np.timedelta64`` object to a :class:`BusinessDay` or :class:`CustomBusinessDay` object incorrectly raising (:issue:`44532`)
- Bug in :meth:`Index.insert` for inserting ``np.datetime64``, ``np.timedelta64`` or ``tuple`` into :class:`Index` with ``dtype='object'`` with negative loc adding ``None`` and replacing existing value (:issue:`44509`)
- Bug in :meth:`Timestamp.to_pydatetime` failing to retain the ``fold`` attribute (:issue:`45087`)
- Bug in :meth:`Series.mode` with ``DatetimeTZDtype`` incorrectly returning timezone-naive and ``PeriodDtype`` incorrectly raising (:issue:`41927`)
- Fixed regression in :meth:`~Series.reindex` raising an error when using an incompatible fill value with a datetime-like dtype (or not raising a deprecation warning for using a ``datetime.date`` as fill value) (:issue:`42921`)
- Bug in :class:`DateOffset`` addition with :class:`Timestamp` where ``offset.nanoseconds`` would not be included in the result (:issue:`43968`, :issue:`36589`)
- Bug in :meth:`Timestamp.fromtimestamp` not supporting the ``tz`` argument (:issue:`45083`)
- Bug in :class:`DataFrame` construction from dict of :class:`Series` with mismatched index dtypes sometimes raising depending on the ordering of the passed dict (:issue:`44091`)
- Bug in :class:`Timestamp` hashing during some DST transitions caused a segmentation fault (:issue:`33931` and :issue:`40817`)

Timedelta
^^^^^^^^^
- Bug in division of all-``NaT`` :class:`TimeDeltaIndex`, :class:`Series` or :class:`DataFrame` column with object-dtype array like of numbers failing to infer the result as timedelta64-dtype (:issue:`39750`)
- Bug in floor division of ``timedelta64[ns]`` data with a scalar returning garbage values (:issue:`44466`)
- Bug in :class:`Timedelta` now properly taking into account any nanoseconds contribution of any kwarg (:issue:`43764`, :issue:`45227`)

Time Zones
^^^^^^^^^^
- Bug in :func:`to_datetime` with ``infer_datetime_format=True`` failing to parse zero UTC offset (``Z``) correctly (:issue:`41047`)
- Bug in :meth:`Series.dt.tz_convert` resetting index in a :class:`Series` with :class:`CategoricalIndex` (:issue:`43080`)
- Bug in ``Timestamp`` and ``DatetimeIndex`` incorrectly raising a ``TypeError`` when subtracting two timezone-aware objects with mismatched timezones (:issue:`31793`)

Numeric
^^^^^^^
- Bug in floor-dividing a list or tuple of integers by a :class:`Series` incorrectly raising (:issue:`44674`)
- Bug in :meth:`DataFrame.rank` raising ``ValueError`` with ``object`` columns and ``method="first"`` (:issue:`41931`)
- Bug in :meth:`DataFrame.rank` treating missing values and extreme values as equal (for example ``np.nan`` and ``np.inf``), causing incorrect results when ``na_option="bottom"`` or ``na_option="top`` used (:issue:`41931`)
- Bug in ``numexpr`` engine still being used when the option ``compute.use_numexpr`` is set to ``False`` (:issue:`32556`)
- Bug in :class:`DataFrame` arithmetic ops with a subclass whose :meth:`_constructor` attribute is a callable other than the subclass itself (:issue:`43201`)
- Bug in arithmetic operations involving :class:`RangeIndex` where the result would have the incorrect ``name`` (:issue:`43962`)
- Bug in arithmetic operations involving :class:`Series` where the result could have the incorrect ``name`` when the operands having matching NA or matching tuple names (:issue:`44459`)
- Bug in division with ``IntegerDtype`` or ``BooleanDtype`` array and NA scalar incorrectly raising (:issue:`44685`)
- Bug in multiplying a :class:`Series` with ``FloatingDtype`` with a timedelta-like scalar incorrectly raising (:issue:`44772`)

Conversion
^^^^^^^^^^
- Bug in :class:`UInt64Index` constructor when passing a list containing both positive integers small enough to cast to int64 and integers too large to hold in int64 (:issue:`42201`)
- Bug in :class:`Series` constructor returning 0 for missing values with dtype ``int64`` and ``False`` for dtype ``bool`` (:issue:`43017`, :issue:`43018`)
- Bug in constructing a :class:`DataFrame` from a :class:`PandasArray` containing :class:`Series` objects behaving differently than an equivalent ``np.ndarray`` (:issue:`43986`)
- Bug in :class:`IntegerDtype` not allowing coercion from string dtype (:issue:`25472`)
- Bug in :func:`to_datetime` with ``arg:xr.DataArray`` and ``unit="ns"`` specified raises ``TypeError`` (:issue:`44053`)
- Bug in :meth:`DataFrame.convert_dtypes` not returning the correct type when a subclass does not overload :meth:`_constructor_sliced` (:issue:`43201`)
- Bug in :meth:`DataFrame.astype` not propagating ``attrs`` from the original :class:`DataFrame` (:issue:`44414`)
- Bug in :meth:`DataFrame.convert_dtypes` result losing ``columns.names`` (:issue:`41435`)
- Bug in constructing a ``IntegerArray`` from pyarrow data failing to validate dtypes (:issue:`44891`)
- Bug in :meth:`Series.astype` not allowing converting from a ``PeriodDtype`` to ``datetime64`` dtype, inconsistent with the :class:`PeriodIndex` behavior (:issue:`45038`)

Strings
^^^^^^^
- Bug in checking for ``string[pyarrow]`` dtype incorrectly raising an ``ImportError`` when pyarrow is not installed (:issue:`44276`)

Interval
^^^^^^^^
- Bug in :meth:`Series.where` with ``IntervalDtype`` incorrectly raising when the ``where`` call should not replace anything (:issue:`44181`)

Indexing
^^^^^^^^
- Bug in :meth:`Series.rename` with :class:`MultiIndex` and ``level`` is provided (:issue:`43659`)
- Bug in :meth:`DataFrame.truncate` and :meth:`Series.truncate` when the object's :class:`Index` has a length greater than one but only one unique value (:issue:`42365`)
- Bug in :meth:`Series.loc` and :meth:`DataFrame.loc` with a :class:`MultiIndex` when indexing with a tuple in which one of the levels is also a tuple (:issue:`27591`)
- Bug in :meth:`Series.loc` with a :class:`MultiIndex` whose first level contains only ``np.nan`` values (:issue:`42055`)
- Bug in indexing on a :class:`Series` or :class:`DataFrame` with a :class:`DatetimeIndex` when passing a string, the return type depended on whether the index was monotonic (:issue:`24892`)
- Bug in indexing on a :class:`MultiIndex` failing to drop scalar levels when the indexer is a tuple containing a datetime-like string (:issue:`42476`)
- Bug in :meth:`DataFrame.sort_values` and :meth:`Series.sort_values` when passing an ascending value, failed to raise or incorrectly raising ``ValueError`` (:issue:`41634`)
- Bug in updating values of :class:`pandas.Series` using boolean index, created by using :meth:`pandas.DataFrame.pop` (:issue:`42530`)
- Bug in :meth:`Index.get_indexer_non_unique` when index contains multiple ``np.nan`` (:issue:`35392`)
- Bug in :meth:`DataFrame.query` did not handle the degree sign in a backticked column name, such as \`Temp(°C)\`, used in an expression to query a :class:`DataFrame` (:issue:`42826`)
- Bug in :meth:`DataFrame.drop` where the error message did not show missing labels with commas when raising ``KeyError`` (:issue:`42881`)
- Bug in :meth:`DataFrame.query` where method calls in query strings led to errors when the ``numexpr`` package was installed (:issue:`22435`)
- Bug in :meth:`DataFrame.nlargest` and :meth:`Series.nlargest` where sorted result did not count indexes containing ``np.nan`` (:issue:`28984`)
- Bug in indexing on a non-unique object-dtype :class:`Index` with an NA scalar (e.g. ``np.nan``) (:issue:`43711`)
- Bug in :meth:`DataFrame.__setitem__` incorrectly writing into an existing column's array rather than setting a new array when the new dtype and the old dtype match (:issue:`43406`)
- Bug in setting floating-dtype values into a :class:`Series` with integer dtype failing to set inplace when those values can be losslessly converted to integers (:issue:`44316`)
- Bug in :meth:`Series.__setitem__` with object dtype when setting an array with matching size and dtype='datetime64[ns]' or dtype='timedelta64[ns]' incorrectly converting the datetime/timedeltas to integers (:issue:`43868`)
- Bug in :meth:`DataFrame.sort_index` where ``ignore_index=True`` was not being respected when the index was already sorted (:issue:`43591`)
- Bug in :meth:`Index.get_indexer_non_unique` when index contains multiple ``np.datetime64("NaT")`` and ``np.timedelta64("NaT")`` (:issue:`43869`)
- Bug in setting a scalar :class:`Interval` value into a :class:`Series` with ``IntervalDtype`` when the scalar's sides are floats and the values' sides are integers (:issue:`44201`)
- Bug when setting string-backed :class:`Categorical` values that can be parsed to datetimes into a :class:`DatetimeArray` or :class:`Series` or :class:`DataFrame` column backed by :class:`DatetimeArray` failing to parse these strings (:issue:`44236`)
- Bug in :meth:`Series.__setitem__` with an integer dtype other than ``int64`` setting with a ``range`` object unnecessarily upcasting to ``int64`` (:issue:`44261`)
- Bug in :meth:`Series.__setitem__` with a boolean mask indexer setting a listlike value of length 1 incorrectly broadcasting that value (:issue:`44265`)
- Bug in :meth:`Series.reset_index` not ignoring ``name`` argument when ``drop`` and ``inplace`` are set to ``True`` (:issue:`44575`)
- Bug in :meth:`DataFrame.loc.__setitem__` and :meth:`DataFrame.iloc.__setitem__` with mixed dtypes sometimes failing to operate in-place (:issue:`44345`)
- Bug in :meth:`DataFrame.loc.__getitem__` incorrectly raising ``KeyError`` when selecting a single column with a boolean key (:issue:`44322`).
- Bug in setting :meth:`DataFrame.iloc` with a single ``ExtensionDtype`` column and setting 2D values e.g. ``df.iloc[:] = df.values`` incorrectly raising (:issue:`44514`)
- Bug in setting values with :meth:`DataFrame.iloc` with a single ``ExtensionDtype`` column and a tuple of arrays as the indexer (:issue:`44703`)
- Bug in indexing on columns with ``loc`` or ``iloc`` using a slice with a negative step with ``ExtensionDtype`` columns incorrectly raising (:issue:`44551`)
- Bug in :meth:`DataFrame.loc.__setitem__` changing dtype when indexer was completely ``False`` (:issue:`37550`)
- Bug in :meth:`IntervalIndex.get_indexer_non_unique` returning boolean mask instead of array of integers for a non unique and non monotonic index (:issue:`44084`)
- Bug in :meth:`IntervalIndex.get_indexer_non_unique` not handling targets of ``dtype`` 'object' with NaNs correctly (:issue:`44482`)
- Fixed regression where a single column ``np.matrix`` was no longer coerced to a 1d ``np.ndarray`` when added to a :class:`DataFrame` (:issue:`42376`)
- Bug in :meth:`Series.__getitem__` with a :class:`CategoricalIndex` of integers treating lists of integers as positional indexers, inconsistent with the behavior with a single scalar integer (:issue:`15470`, :issue:`14865`)
- Bug in :meth:`Series.__setitem__` when setting floats or integers into integer-dtype :class:`Series` failing to upcast when necessary to retain precision (:issue:`45121`)
- Bug in :meth:`DataFrame.iloc.__setitem__` ignores axis argument (:issue:`45032`)

Missing
^^^^^^^
- Bug in :meth:`DataFrame.fillna` with ``limit`` and no ``method`` ignores ``axis='columns'`` or ``axis = 1`` (:issue:`40989`, :issue:`17399`)
- Bug in :meth:`DataFrame.fillna` not replacing missing values when using a dict-like ``value`` and duplicate column names (:issue:`43476`)
- Bug in constructing a :class:`DataFrame` with a dictionary ``np.datetime64`` as a value and ``dtype='timedelta64[ns]'``, or vice-versa, incorrectly casting instead of raising (:issue:`44428`)
- Bug in :meth:`Series.interpolate` and :meth:`DataFrame.interpolate` with ``inplace=True`` not writing to the underlying array(s) in-place (:issue:`44749`)
- Bug in :meth:`Index.fillna` incorrectly returning an unfilled :class:`Index` when NA values are present and ``downcast`` argument is specified. This now raises ``NotImplementedError`` instead; do not pass ``downcast`` argument (:issue:`44873`)
- Bug in :meth:`DataFrame.dropna` changing :class:`Index` even if no entries were dropped (:issue:`41965`)
- Bug in :meth:`Series.fillna` with an object-dtype incorrectly ignoring ``downcast="infer"`` (:issue:`44241`)

MultiIndex
^^^^^^^^^^
- Bug in :meth:`MultiIndex.get_loc` where the first level is a :class:`DatetimeIndex` and a string key is passed (:issue:`42465`)
- Bug in :meth:`MultiIndex.reindex` when passing a ``level`` that corresponds to an ``ExtensionDtype`` level (:issue:`42043`)
- Bug in :meth:`MultiIndex.get_loc` raising ``TypeError`` instead of ``KeyError`` on nested tuple (:issue:`42440`)
- Bug in :meth:`MultiIndex.union` setting wrong ``sortorder`` causing errors in subsequent indexing operations with slices (:issue:`44752`)
- Bug in :meth:`MultiIndex.putmask` where the other value was also a :class:`MultiIndex` (:issue:`43212`)
- Bug in :meth:`MultiIndex.dtypes` duplicate level names returned only one dtype per name (:issue:`45174`)

I/O
^^^
- Bug in :func:`read_excel` attempting to read chart sheets from .xlsx files (:issue:`41448`)
- Bug in :func:`json_normalize` where ``errors=ignore`` could fail to ignore missing values of ``meta`` when ``record_path`` has a length greater than one (:issue:`41876`)
- Bug in :func:`read_csv` with multi-header input and arguments referencing column names as tuples (:issue:`42446`)
- Bug in :func:`read_fwf`, where difference in lengths of ``colspecs`` and ``names`` was not raising ``ValueError`` (:issue:`40830`)
- Bug in :func:`Series.to_json` and :func:`DataFrame.to_json` where some attributes were skipped when serializing plain Python objects to JSON (:issue:`42768`, :issue:`33043`)
- Column headers are dropped when constructing a :class:`DataFrame` from a sqlalchemy's ``Row`` object (:issue:`40682`)
- Bug in unpickling an :class:`Index` with object dtype incorrectly inferring numeric dtypes (:issue:`43188`)
- Bug in :func:`read_csv` where reading multi-header input with unequal lengths incorrectly raised ``IndexError`` (:issue:`43102`)
- Bug in :func:`read_csv` raising ``ParserError`` when reading file in chunks and some chunk blocks have fewer columns than header for ``engine="c"`` (:issue:`21211`)
- Bug in :func:`read_csv`, changed exception class when expecting a file path name or file-like object from ``OSError`` to ``TypeError`` (:issue:`43366`)
- Bug in :func:`read_csv` and :func:`read_fwf` ignoring all ``skiprows`` except first when ``nrows`` is specified for ``engine='python'`` (:issue:`44021`, :issue:`10261`)
- Bug in :func:`read_csv` keeping the original column in object format when ``keep_date_col=True`` is set (:issue:`13378`)
- Bug in :func:`read_json` not handling non-numpy dtypes correctly (especially ``category``) (:issue:`21892`, :issue:`33205`)
- Bug in :func:`json_normalize` where multi-character ``sep`` parameter is incorrectly prefixed to every key (:issue:`43831`)
- Bug in :func:`json_normalize` where reading data with missing multi-level metadata would not respect ``errors="ignore"`` (:issue:`44312`)
- Bug in :func:`read_csv` used second row to guess implicit index if ``header`` was set to ``None`` for ``engine="python"`` (:issue:`22144`)
- Bug in :func:`read_csv` not recognizing bad lines when ``names`` were given for ``engine="c"`` (:issue:`22144`)
- Bug in :func:`read_csv` with :code:`float_precision="round_trip"` which did not skip initial/trailing whitespace (:issue:`43713`)
- Bug when Python is built without the lzma module: a warning was raised at the pandas import time, even if the lzma capability isn't used (:issue:`43495`)
- Bug in :func:`read_csv` not applying dtype for ``index_col`` (:issue:`9435`)
- Bug in dumping/loading a :class:`DataFrame` with ``yaml.dump(frame)`` (:issue:`42748`)
- Bug in :func:`read_csv` raising ``ValueError`` when ``names`` was longer than ``header`` but equal to data rows for ``engine="python"`` (:issue:`38453`)
- Bug in :class:`ExcelWriter`, where ``engine_kwargs`` were not passed through to all engines (:issue:`43442`)
- Bug in :func:`read_csv` raising ``ValueError`` when ``parse_dates`` was used with :class:`MultiIndex` columns (:issue:`8991`)
- Bug in :func:`read_csv` not raising an ``ValueError`` when ``\n`` was specified as ``delimiter`` or ``sep`` which conflicts with ``lineterminator`` (:issue:`43528`)
- Bug in :func:`to_csv` converting datetimes in categorical :class:`Series` to integers (:issue:`40754`)
- Bug in :func:`read_csv` converting columns to numeric after date parsing failed (:issue:`11019`)
- Bug in :func:`read_csv` not replacing ``NaN`` values with ``np.nan`` before attempting date conversion (:issue:`26203`)
- Bug in :func:`read_csv` raising ``AttributeError`` when attempting to read a .csv file and infer index column dtype from an nullable integer type (:issue:`44079`)
- Bug in :func:`to_csv` always coercing datetime columns with different formats to the same format (:issue:`21734`)
- :meth:`DataFrame.to_csv` and :meth:`Series.to_csv` with ``compression`` set to ``'zip'`` no longer create a zip file containing a file ending with ".zip". Instead, they try to infer the inner file name more smartly (:issue:`39465`)
- Bug in :func:`read_csv` where reading a mixed column of booleans and missing values to a float type results in the missing values becoming 1.0 rather than NaN (:issue:`42808`, :issue:`34120`)
- Bug in :func:`to_xml` raising error for ``pd.NA`` with extension array dtype (:issue:`43903`)
- Bug in :func:`read_csv` when passing simultaneously a parser in ``date_parser`` and ``parse_dates=False``, the parsing was still called (:issue:`44366`)
- Bug in :func:`read_csv` not setting name of :class:`MultiIndex` columns correctly when ``index_col`` is not the first column (:issue:`38549`)
- Bug in :func:`read_csv` silently ignoring errors when failing to create a memory-mapped file (:issue:`44766`)
- Bug in :func:`read_csv` when passing a ``tempfile.SpooledTemporaryFile`` opened in binary mode (:issue:`44748`)
- Bug in :func:`read_json` raising ``ValueError`` when attempting to parse json strings containing "://" (:issue:`36271`)
- Bug in :func:`read_csv` when ``engine="c"`` and ``encoding_errors=None`` which caused a segfault (:issue:`45180`)
- Bug in :func:`read_csv` an invalid value of ``usecols`` leading to an unclosed file handle (:issue:`45384`)
- Bug in :meth:`DataFrame.to_json` fix memory leak (:issue:`43877`)

Period
^^^^^^
- Bug in adding a :class:`Period` object to a ``np.timedelta64`` object incorrectly raising ``TypeError`` (:issue:`44182`)
- Bug in :meth:`PeriodIndex.to_timestamp` when the index has ``freq="B"`` inferring ``freq="D"`` for its result instead of ``freq="B"`` (:issue:`44105`)
- Bug in :class:`Period` constructor incorrectly allowing ``np.timedelta64("NaT")`` (:issue:`44507`)
- Bug in :meth:`PeriodIndex.to_timestamp` giving incorrect values for indexes with non-contiguous data (:issue:`44100`)
- Bug in :meth:`Series.where` with ``PeriodDtype`` incorrectly raising when the ``where`` call should not replace anything (:issue:`45135`)

Plotting
^^^^^^^^
- When given non-numeric data, :meth:`DataFrame.boxplot` now raises a ``ValueError`` rather than a cryptic ``KeyError`` or ``ZeroDivisionError``, in line with other plotting functions like :meth:`DataFrame.hist` (:issue:`43480`)

Groupby/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^
- Bug in :meth:`SeriesGroupBy.apply` where passing an unrecognized string argument failed to raise ``TypeError`` when the underlying ``Series`` is empty (:issue:`42021`)
- Bug in :meth:`Series.rolling.apply`, :meth:`DataFrame.rolling.apply`, :meth:`Series.expanding.apply` and :meth:`DataFrame.expanding.apply` with ``engine="numba"`` where ``*args`` were being cached with the user passed function (:issue:`42287`)
- Bug in :meth:`GroupBy.max` and :meth:`GroupBy.min` with nullable integer dtypes losing precision (:issue:`41743`)
- Bug in :meth:`DataFrame.groupby.rolling.var` would calculate the rolling variance only on the first group (:issue:`42442`)
- Bug in :meth:`GroupBy.shift` that would return the grouping columns if ``fill_value`` was not ``None`` (:issue:`41556`)
- Bug in :meth:`SeriesGroupBy.nlargest` and :meth:`SeriesGroupBy.nsmallest` would have an inconsistent index when the input :class:`Series` was sorted and ``n`` was greater than or equal to all group sizes (:issue:`15272`, :issue:`16345`, :issue:`29129`)
- Bug in :meth:`pandas.DataFrame.ewm`, where non-float64 dtypes were silently failing (:issue:`42452`)
- Bug in :meth:`pandas.DataFrame.rolling` operation along rows (``axis=1``) incorrectly omits columns containing ``float16`` and ``float32`` (:issue:`41779`)
- Bug in :meth:`Resampler.aggregate` did not allow the use of Named Aggregation (:issue:`32803`)
- Bug in :meth:`Series.rolling` when the :class:`Series` ``dtype`` was ``Int64`` (:issue:`43016`)
- Bug in :meth:`DataFrame.rolling.corr` when the :class:`DataFrame` columns was a :class:`MultiIndex` (:issue:`21157`)
- Bug in :meth:`DataFrame.groupby.rolling` when specifying ``on`` and calling ``__getitem__`` would subsequently return incorrect results (:issue:`43355`)
- Bug in :meth:`GroupBy.apply` with time-based :class:`Grouper` objects incorrectly raising ``ValueError`` in corner cases where the grouping vector contains a ``NaT`` (:issue:`43500`, :issue:`43515`)
- Bug in :meth:`GroupBy.mean` failing with ``complex`` dtype (:issue:`43701`)
- Bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` not calculating window bounds correctly for the first row when ``center=True`` and index is decreasing (:issue:`43927`)
- Bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` for centered datetimelike windows with uneven nanosecond (:issue:`43997`)
- Bug in :meth:`GroupBy.mean` raising ``KeyError`` when column was selected at least twice (:issue:`44924`)
- Bug in :meth:`GroupBy.nth` failing on ``axis=1`` (:issue:`43926`)
- Bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` not respecting right bound on centered datetime-like windows, if the index contain duplicates (:issue:`3944`)
- Bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` when using a :class:`pandas.api.indexers.BaseIndexer` subclass that returned unequal start and end arrays would segfault instead of raising a ``ValueError`` (:issue:`44470`)
- Bug in :meth:`Groupby.nunique` not respecting ``observed=True`` for ``categorical`` grouping columns (:issue:`45128`)
- Bug in :meth:`GroupBy.head` and :meth:`GroupBy.tail` not dropping groups with ``NaN`` when ``dropna=True`` (:issue:`45089`)
- Bug in :meth:`GroupBy.__iter__` after selecting a subset of columns in a :class:`GroupBy` object, which returned all columns instead of the chosen subset (:issue:`44821`)
- Bug in :meth:`Groupby.rolling` when non-monotonic data passed, fails to correctly raise ``ValueError`` (:issue:`43909`)
- Bug where grouping by a :class:`Series` that has a ``categorical`` data type and length unequal to the axis of grouping raised ``ValueError`` (:issue:`44179`)

Reshaping
^^^^^^^^^
- Improved error message when creating a :class:`DataFrame` column from a multi-dimensional :class:`numpy.ndarray` (:issue:`42463`)
- Bug in :func:`concat` creating :class:`MultiIndex` with duplicate level entries when concatenating a :class:`DataFrame` with duplicates in :class:`Index` and multiple keys (:issue:`42651`)
- Bug in :meth:`pandas.cut` on :class:`Series` with duplicate indices and non-exact :meth:`pandas.CategoricalIndex` (:issue:`42185`, :issue:`42425`)
- Bug in :meth:`DataFrame.append` failing to retain dtypes when appended columns do not match (:issue:`43392`)
- Bug in :func:`concat` of ``bool`` and ``boolean`` dtypes resulting in ``object`` dtype instead of ``boolean`` dtype (:issue:`42800`)
- Bug in :func:`crosstab` when inputs are categorical :class:`Series`, there are categories that are not present in one or both of the :class:`Series`, and ``margins=True``. Previously the margin value for missing categories was ``NaN``. It is now correctly reported as 0 (:issue:`43505`)
- Bug in :func:`concat` would fail when the ``objs`` argument all had the same index and the ``keys`` argument contained duplicates (:issue:`43595`)
- Bug in :func:`concat` which ignored the ``sort`` parameter (:issue:`43375`)
- Bug in :func:`merge` with :class:`MultiIndex` as column index for the ``on`` argument returning an error when assigning a column internally (:issue:`43734`)
- Bug in :func:`crosstab` would fail when inputs are lists or tuples (:issue:`44076`)
- Bug in :meth:`DataFrame.append` failing to retain ``index.name`` when appending a list of :class:`Series` objects (:issue:`44109`)
- Fixed metadata propagation in :meth:`Dataframe.apply` method, consequently fixing the same issue for :meth:`Dataframe.transform`, :meth:`Dataframe.nunique` and :meth:`Dataframe.mode` (:issue:`28283`)
- Bug in :func:`concat` casting levels of :class:`MultiIndex` to float if all levels only consist of missing values (:issue:`44900`)
- Bug in :meth:`DataFrame.stack` with ``ExtensionDtype`` columns incorrectly raising (:issue:`43561`)
- Bug in :func:`merge` raising ``KeyError`` when joining over differently named indexes with on keywords (:issue:`45094`)
- Bug in :meth:`Series.unstack` with object doing unwanted type inference on resulting columns (:issue:`44595`)
- Bug in :meth:`MultiIndex.join()` with overlapping ``IntervalIndex`` levels (:issue:`44096`)
- Bug in :meth:`DataFrame.replace` and :meth:`Series.replace` results is different ``dtype`` based on ``regex`` parameter (:issue:`44864`)
- Bug in :meth:`DataFrame.pivot` with ``index=None`` when the :class:`DataFrame` index was a :class:`MultiIndex` (:issue:`23955`)

Sparse
^^^^^^
- Bug in :meth:`DataFrame.sparse.to_coo` raising ``AttributeError`` when column names are not unique (:issue:`29564`)
- Bug in :meth:`SparseArray.max` and :meth:`SparseArray.min` raising ``ValueError`` for arrays with 0 non-null elements (:issue:`43527`)
- Bug in :meth:`DataFrame.sparse.to_coo` silently converting non-zero fill values to zero (:issue:`24817`)
- Bug in :class:`SparseArray` comparison methods with an array-like operand of mismatched length raising ``AssertionError`` or unclear ``ValueError`` depending on the input (:issue:`43863`)
- Bug in :class:`SparseArray` arithmetic methods ``floordiv`` and ``mod`` behaviors when dividing by zero not matching the non-sparse :class:`Series` behavior (:issue:`38172`)
- Bug in :class:`SparseArray` unary methods as well as :meth:`SparseArray.isna` doesn't recalculate indexes (:issue:`44955`)

ExtensionArray
^^^^^^^^^^^^^^
- Bug in :func:`array` failing to preserve :class:`PandasArray` (:issue:`43887`)
- NumPy ufuncs ``np.abs``, ``np.positive``, ``np.negative`` now correctly preserve dtype when called on ExtensionArrays that implement ``__abs__, __pos__, __neg__``, respectively. In particular this is fixed for :class:`TimedeltaArray` (:issue:`43899`, :issue:`23316`)
- NumPy ufuncs ``np.minimum.reduce`` ``np.maximum.reduce``, ``np.add.reduce``, and ``np.prod.reduce`` now work correctly instead of raising ``NotImplementedError`` on :class:`Series` with ``IntegerDtype`` or ``FloatDtype`` (:issue:`43923`, :issue:`44793`)
- NumPy ufuncs with ``out`` keyword are now supported by arrays with ``IntegerDtype`` and ``FloatingDtype`` (:issue:`45122`)
- Avoid raising ``PerformanceWarning`` about fragmented :class:`DataFrame` when using many columns with an extension dtype (:issue:`44098`)
- Bug in :class:`IntegerArray` and :class:`FloatingArray` construction incorrectly coercing mismatched NA values (e.g. ``np.timedelta64("NaT")``) to numeric NA (:issue:`44514`)
- Bug in :meth:`BooleanArray.__eq__` and :meth:`BooleanArray.__ne__` raising ``TypeError`` on comparison with an incompatible type (like a string). This caused :meth:`DataFrame.replace` to sometimes raise a ``TypeError`` if a nullable boolean column was included (:issue:`44499`)
- Bug in :func:`array` incorrectly raising when passed a ``ndarray`` with ``float16`` dtype (:issue:`44715`)
- Bug in calling ``np.sqrt`` on :class:`BooleanArray` returning a malformed :class:`FloatingArray` (:issue:`44715`)
- Bug in :meth:`Series.where` with ``ExtensionDtype`` when ``other`` is a NA scalar incompatible with the :class:`Series` dtype (e.g. ``NaT`` with a numeric dtype) incorrectly casting to a compatible NA value (:issue:`44697`)
- Bug in :meth:`Series.replace` where explicitly passing ``value=None`` is treated as if no ``value`` was passed, and ``None`` not being in the result (:issue:`36984`, :issue:`19998`)
- Bug in :meth:`Series.replace` with unwanted downcasting being done in no-op replacements (:issue:`44498`)
- Bug in :meth:`Series.replace` with ``FloatDtype``, ``string[python]``, or ``string[pyarrow]`` dtype not being preserved when possible (:issue:`33484`, :issue:`40732`, :issue:`31644`, :issue:`41215`, :issue:`25438`)

Styler
^^^^^^
- Bug in :class:`.Styler` where the ``uuid`` at initialization maintained a floating underscore (:issue:`43037`)
- Bug in :meth:`.Styler.to_html` where the ``Styler`` object was updated if the ``to_html`` method was called with some args (:issue:`43034`)
- Bug in :meth:`.Styler.copy` where ``uuid`` was not previously copied (:issue:`40675`)
- Bug in :meth:`Styler.apply` where functions which returned :class:`Series` objects were not correctly handled in terms of aligning their index labels (:issue:`13657`, :issue:`42014`)
- Bug when rendering an empty :class:`DataFrame` with a named :class:`Index` (:issue:`43305`)
- Bug when rendering a single level :class:`MultiIndex` (:issue:`43383`)
- Bug when combining non-sparse rendering and :meth:`.Styler.hide_columns` or :meth:`.Styler.hide_index` (:issue:`43464`)
- Bug setting a table style when using multiple selectors in :class:`.Styler` (:issue:`44011`)
- Bugs where row trimming and column trimming failed to reflect hidden rows (:issue:`43703`, :issue:`44247`)

Other
^^^^^
- Bug in :meth:`DataFrame.astype` with non-unique columns and a :class:`Series` ``dtype`` argument (:issue:`44417`)
- Bug in :meth:`CustomBusinessMonthBegin.__add__` (:meth:`CustomBusinessMonthEnd.__add__`) not applying the extra ``offset`` parameter when beginning (end) of the target month is already a business day (:issue:`41356`)
- Bug in :meth:`RangeIndex.union` with another ``RangeIndex`` with matching (even) ``step`` and starts differing by strictly less than ``step / 2`` (:issue:`44019`)
- Bug in :meth:`RangeIndex.difference` with ``sort=None`` and ``step<0`` failing to sort (:issue:`44085`)
- Bug in :meth:`Series.replace` and :meth:`DataFrame.replace` with ``value=None`` and ExtensionDtypes (:issue:`44270`, :issue:`37899`)
- Bug in :meth:`FloatingArray.equals` failing to consider two arrays equal if they contain ``np.nan`` values (:issue:`44382`)
- Bug in :meth:`DataFrame.shift` with ``axis=1`` and ``ExtensionDtype`` columns incorrectly raising when an incompatible ``fill_value`` is passed (:issue:`44564`)
- Bug in :meth:`DataFrame.shift` with ``axis=1`` and ``periods`` larger than ``len(frame.columns)`` producing an invalid :class:`DataFrame` (:issue:`44978`)
- Bug in :meth:`DataFrame.diff` when passing a NumPy integer object instead of an ``int`` object (:issue:`44572`)
- Bug in :meth:`Series.replace` raising ``ValueError`` when using ``regex=True`` with a :class:`Series` containing ``np.nan`` values (:issue:`43344`)
- Bug in :meth:`DataFrame.to_records` where an incorrect ``n`` was used when missing names were replaced by ``level_n`` (:issue:`44818`)
- Bug in :meth:`DataFrame.eval` where ``resolvers`` argument was overriding the default resolvers (:issue:`34966`)
- :meth:`Series.__repr__` and :meth:`DataFrame.__repr__` no longer replace all null-values in indexes with "NaN" but use their real string-representations. "NaN" is used only for ``float("nan")`` (:issue:`45263`)

.. ---------------------------------------------------------------------------

.. _whatsnew_140.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.5..v1.4.0

.. _whatsnew_050:

Version 0.5.0 (October 24, 2011)
--------------------------------

{{ header }}

New features
~~~~~~~~~~~~

- :ref:`Added <basics.df_join>` ``DataFrame.align`` method with standard join options
- :ref:`Added <io.parse_dates>` ``parse_dates`` option to ``read_csv`` and ``read_table`` methods to optionally try to parse dates in the index columns
- :ref:`Added <io.parse_dates>` ``nrows``, ``chunksize``, and ``iterator`` arguments to ``read_csv`` and ``read_table``. The last two return a new ``TextParser`` class capable of lazily iterating through chunks of a flat file (:issue:`242`)
- :ref:`Added <merging.multikey_join>` ability to join on multiple columns in ``DataFrame.join`` (:issue:`214`)
- Added private ``_get_duplicates`` function to ``Index`` for identifying duplicate values more easily (ENH5c_)
- :ref:`Added <indexing.df_cols>` column attribute access to DataFrame.
- :ref:`Added <indexing.df_cols>` Python tab completion hook for DataFrame columns. (:issue:`233`, :issue:`230`)
- :ref:`Implemented <basics.describe>` ``Series.describe`` for Series containing objects (:issue:`241`)
- :ref:`Added <merging.df_inner_join>` inner join option to ``DataFrame.join`` when joining on key(s) (:issue:`248`)
- :ref:`Implemented <indexing.df_cols>` selecting DataFrame columns by passing a list to ``__getitem__`` (:issue:`253`)
- :ref:`Implemented <indexing.set_ops>` & and | to intersect / union Index objects, respectively (:issue:`261`)
- :ref:`Added<reshaping.pivot>` ``pivot_table`` convenience function to pandas namespace (:issue:`234`)
- :ref:`Implemented <basics.rename_axis>` ``Panel.rename_axis`` function (:issue:`243`)
- DataFrame will show index level names in console output (:issue:`334`)
- :ref:`Implemented <advanced.take>` ``Panel.take``
- :ref:`Added<basics.console_output>` ``set_eng_float_format`` for alternate DataFrame floating point string formatting (ENH61_)
- :ref:`Added <indexing.set_index>` convenience ``set_index`` function for creating a DataFrame index from its existing columns
- :ref:`Implemented <groupby.multiindex>` ``groupby`` hierarchical index level name  (:issue:`223`)
- :ref:`Added <io.store_in_csv>` support for different delimiters in ``DataFrame.to_csv`` (:issue:`244`)

Performance enhancements
~~~~~~~~~~~~~~~~~~~~~~~~

- VBENCH Major performance improvements in file parsing functions ``read_csv`` and ``read_table``
- VBENCH Added Cython function for converting tuples to ndarray very fast. Speeds up many MultiIndex-related operations
- VBENCH Refactored merging / joining code into a tidy class and disabled unnecessary computations in the float/object case, thus getting about 10% better performance (:issue:`211`)
- VBENCH Improved speed of ``DataFrame.xs`` on mixed-type DataFrame objects by about 5x, regression from 0.3.0 (:issue:`215`)
- VBENCH With new ``DataFrame.align`` method, speeding up binary operations between differently-indexed DataFrame objects by 10-25%.
- VBENCH Significantly sped up conversion of nested dict into DataFrame (:issue:`212`)
- VBENCH Significantly speed up DataFrame ``__repr__`` and ``count`` on large mixed-type DataFrame objects

.. _ENH61: https://github.com/pandas-dev/pandas/commit/6141961
.. _ENH5c: https://github.com/pandas-dev/pandas/commit/5ca6ff5d822ee4ddef1ec0d87b6d83d8b4bbd3eb


.. _whatsnew_0.5.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.4.0..v0.5.0
.. _whatsnew_0181:

Version 0.18.1 (May 3, 2016)
----------------------------

{{ header }}


This is a minor bug-fix release from 0.18.0 and includes a large number of
bug fixes along with several new features, enhancements, and performance improvements.
We recommend that all users upgrade to this version.

Highlights include:

- ``.groupby(...)`` has been enhanced to provide convenient syntax when working with ``.rolling(..)``, ``.expanding(..)`` and ``.resample(..)`` per group, see :ref:`here <whatsnew_0181.deferred_ops>`
- ``pd.to_datetime()`` has gained the ability to assemble dates from a ``DataFrame``, see :ref:`here <whatsnew_0181.enhancements.assembling>`
- Method chaining improvements, see :ref:`here <whatsnew_0181.enhancements.method_chain>`.
- Custom business hour offset, see :ref:`here <whatsnew_0181.enhancements.custombusinesshour>`.
- Many bug fixes in the handling of ``sparse``, see :ref:`here <whatsnew_0181.sparse>`
- Expanded the :ref:`Tutorials section <tutorial-modern>` with a feature on modern pandas, courtesy of `@TomAugsburger <https://twitter.com/TomAugspurger>`__. (:issue:`13045`).


.. contents:: What's new in v0.18.1
    :local:
    :backlinks: none

.. _whatsnew_0181.new_features:

New features
~~~~~~~~~~~~

.. _whatsnew_0181.enhancements.custombusinesshour:

Custom business hour
^^^^^^^^^^^^^^^^^^^^

The ``CustomBusinessHour`` is a mixture of ``BusinessHour`` and ``CustomBusinessDay`` which
allows you to specify arbitrary holidays. For details,
see :ref:`Custom Business Hour <timeseries.custombusinesshour>` (:issue:`11514`)

.. ipython:: python

    from pandas.tseries.offsets import CustomBusinessHour
    from pandas.tseries.holiday import USFederalHolidayCalendar

    bhour_us = CustomBusinessHour(calendar=USFederalHolidayCalendar())

Friday before MLK Day

.. ipython:: python

    import datetime

    dt = datetime.datetime(2014, 1, 17, 15)

    dt + bhour_us

Tuesday after MLK Day (Monday is skipped because it's a holiday)

.. ipython:: python

    dt + bhour_us * 2

.. _whatsnew_0181.deferred_ops:

Method ``.groupby(..)`` syntax with window and resample operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``.groupby(...)`` has been enhanced to provide convenient syntax when working with ``.rolling(..)``, ``.expanding(..)`` and ``.resample(..)`` per group, see (:issue:`12486`, :issue:`12738`).

You can now use ``.rolling(..)`` and ``.expanding(..)`` as methods on groupbys. These return another deferred object (similar to what ``.rolling()`` and ``.expanding()`` do on ungrouped pandas objects). You can then operate on these ``RollingGroupby`` objects in a similar manner.

Previously you would have to do this to get a rolling window mean per-group:

.. ipython:: python

   df = pd.DataFrame({"A": [1] * 20 + [2] * 12 + [3] * 8, "B": np.arange(40)})
   df

.. ipython:: python

   df.groupby("A").apply(lambda x: x.rolling(4).B.mean())

Now you can do:

.. ipython:: python

   df.groupby("A").rolling(4).B.mean()

For ``.resample(..)`` type of operations, previously you would have to:

.. ipython:: python

   df = pd.DataFrame(
       {
           "date": pd.date_range(start="2016-01-01", periods=4, freq="W"),
           "group": [1, 1, 2, 2],
           "val": [5, 6, 7, 8],
       }
   ).set_index("date")

   df

.. ipython:: python

   df.groupby("group").apply(lambda x: x.resample("1D").ffill())

Now you can do:

.. ipython:: python

   df.groupby("group").resample("1D").ffill()

.. _whatsnew_0181.enhancements.method_chain:

Method chaining improvements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following methods / indexers now accept a ``callable``. It is intended to make
these more useful in method chains, see the :ref:`documentation <indexing.callable>`.
(:issue:`11485`, :issue:`12533`)

- ``.where()`` and ``.mask()``
- ``.loc[]``, ``iloc[]`` and ``.ix[]``
- ``[]`` indexing

Methods ``.where()`` and ``.mask()``
""""""""""""""""""""""""""""""""""""

These can accept a callable for the condition and ``other``
arguments.

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6], "C": [7, 8, 9]})
   df.where(lambda x: x > 4, lambda x: x + 10)

Methods ``.loc[]``, ``.iloc[]``, ``.ix[]``
""""""""""""""""""""""""""""""""""""""""""

These can accept a callable, and a tuple of callable as a slicer. The callable
can return a valid boolean indexer or anything which is valid for these indexer's input.

.. ipython:: python

   # callable returns bool indexer
   df.loc[lambda x: x.A >= 2, lambda x: x.sum() > 10]

   # callable returns list of labels
   df.loc[lambda x: [1, 2], lambda x: ["A", "B"]]

Indexing with``[]``
"""""""""""""""""""

Finally, you can use a callable in ``[]`` indexing of Series, DataFrame and Panel.
The callable must return a valid input for ``[]`` indexing depending on its
class and index type.

.. ipython:: python

   df[lambda x: "A"]

Using these methods / indexers, you can chain data selection operations
without using temporary variable.

.. ipython:: python

   bb = pd.read_csv("data/baseball.csv", index_col="id")
   (bb.groupby(["year", "team"]).sum().loc[lambda df: df.r > 100])

.. _whatsnew_0181.partial_string_indexing:

Partial string indexing on ``DatetimeIndex`` when part of a ``MultiIndex``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Partial string indexing now matches on ``DateTimeIndex`` when part of a ``MultiIndex`` (:issue:`10331`)

.. ipython:: python

   dft2 = pd.DataFrame(
       np.random.randn(20, 1),
       columns=["A"],
       index=pd.MultiIndex.from_product(
           [pd.date_range("20130101", periods=10, freq="12H"), ["a", "b"]]
       ),
   )
   dft2
   dft2.loc["2013-01-05"]

On other levels

.. ipython:: python

   idx = pd.IndexSlice
   dft2 = dft2.swaplevel(0, 1).sort_index()
   dft2
   dft2.loc[idx[:, "2013-01-05"], :]

.. _whatsnew_0181.enhancements.assembling:

Assembling datetimes
^^^^^^^^^^^^^^^^^^^^

``pd.to_datetime()`` has gained the ability to assemble datetimes from a passed in ``DataFrame`` or a dict. (:issue:`8158`).

.. ipython:: python

   df = pd.DataFrame(
       {"year": [2015, 2016], "month": [2, 3], "day": [4, 5], "hour": [2, 3]}
   )
   df

Assembling using the passed frame.

.. ipython:: python

   pd.to_datetime(df)

You can pass only the columns that you need to assemble.

.. ipython:: python

   pd.to_datetime(df[["year", "month", "day"]])

.. _whatsnew_0181.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- ``pd.read_csv()`` now supports ``delim_whitespace=True`` for the Python engine (:issue:`12958`)
- ``pd.read_csv()`` now supports opening ZIP files that contains a single CSV, via extension inference or explicit ``compression='zip'`` (:issue:`12175`)
- ``pd.read_csv()`` now supports opening files using xz compression, via extension inference or explicit ``compression='xz'`` is specified; ``xz`` compressions is also supported by ``DataFrame.to_csv`` in the same way (:issue:`11852`)
- ``pd.read_msgpack()`` now always gives writeable ndarrays even when compression is used (:issue:`12359`).
- ``pd.read_msgpack()`` now supports serializing and de-serializing categoricals with msgpack (:issue:`12573`)
- ``.to_json()`` now supports ``NDFrames`` that contain categorical and sparse data (:issue:`10778`)
- ``interpolate()`` now supports ``method='akima'`` (:issue:`7588`).
- ``pd.read_excel()`` now accepts path objects (e.g. ``pathlib.Path``, ``py.path.local``) for the file path, in line with other ``read_*`` functions (:issue:`12655`)
- Added ``.weekday_name`` property as a component to ``DatetimeIndex`` and the ``.dt`` accessor. (:issue:`11128`)

- ``Index.take`` now handles ``allow_fill`` and ``fill_value`` consistently (:issue:`12631`)

  .. ipython:: python

     idx = pd.Index([1.0, 2.0, 3.0, 4.0], dtype="float")

     # default, allow_fill=True, fill_value=None
     idx.take([2, -1])
     idx.take([2, -1], fill_value=True)

- ``Index`` now supports ``.str.get_dummies()`` which returns ``MultiIndex``, see :ref:`Creating Indicator Variables <text.indicator>` (:issue:`10008`, :issue:`10103`)

  .. ipython:: python

     idx = pd.Index(["a|b", "a|c", "b|c"])
     idx.str.get_dummies("|")


- ``pd.crosstab()`` has gained a ``normalize`` argument for normalizing frequency tables (:issue:`12569`). Examples in the updated docs :ref:`here <reshaping.crosstabulations>`.
- ``.resample(..).interpolate()`` is now supported (:issue:`12925`)
- ``.isin()`` now accepts passed ``sets`` (:issue:`12988`)

.. _whatsnew_0181.sparse:

Sparse changes
~~~~~~~~~~~~~~

These changes conform sparse handling to return the correct types and work to make a smoother experience with indexing.

``SparseArray.take`` now returns a scalar for scalar input, ``SparseArray`` for others. Furthermore, it handles a negative indexer with the same rule as ``Index`` (:issue:`10560`, :issue:`12796`)

.. code-block:: python

   s = pd.SparseArray([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])
   s.take(0)
   s.take([1, 2, 3])

- Bug in ``SparseSeries[]`` indexing with ``Ellipsis`` raises ``KeyError`` (:issue:`9467`)
- Bug in ``SparseArray[]`` indexing with tuples are not handled properly (:issue:`12966`)
- Bug in ``SparseSeries.loc[]`` with list-like input raises ``TypeError`` (:issue:`10560`)
- Bug in ``SparseSeries.iloc[]`` with scalar input may raise ``IndexError`` (:issue:`10560`)
- Bug in ``SparseSeries.loc[]``, ``.iloc[]`` with ``slice`` returns ``SparseArray``, rather than ``SparseSeries`` (:issue:`10560`)
- Bug in ``SparseDataFrame.loc[]``, ``.iloc[]`` may results in dense ``Series``, rather than ``SparseSeries`` (:issue:`12787`)
- Bug in ``SparseArray`` addition ignores ``fill_value`` of right hand side (:issue:`12910`)
- Bug in ``SparseArray`` mod raises ``AttributeError`` (:issue:`12910`)
- Bug in ``SparseArray`` pow calculates ``1 ** np.nan`` as ``np.nan`` which must be 1 (:issue:`12910`)
- Bug in ``SparseArray`` comparison output may incorrect result or raise ``ValueError`` (:issue:`12971`)
- Bug in ``SparseSeries.__repr__`` raises ``TypeError`` when it is longer than ``max_rows`` (:issue:`10560`)
- Bug in ``SparseSeries.shape`` ignores ``fill_value`` (:issue:`10452`)
- Bug in ``SparseSeries`` and ``SparseArray`` may have different ``dtype`` from its dense values (:issue:`12908`)
- Bug in ``SparseSeries.reindex`` incorrectly handle ``fill_value`` (:issue:`12797`)
- Bug in ``SparseArray.to_frame()`` results in ``DataFrame``, rather than ``SparseDataFrame`` (:issue:`9850`)
- Bug in ``SparseSeries.value_counts()`` does not count ``fill_value`` (:issue:`6749`)
- Bug in ``SparseArray.to_dense()`` does not preserve ``dtype`` (:issue:`10648`)
- Bug in ``SparseArray.to_dense()`` incorrectly handle ``fill_value`` (:issue:`12797`)
- Bug in ``pd.concat()`` of ``SparseSeries`` results in dense (:issue:`10536`)
- Bug in ``pd.concat()`` of ``SparseDataFrame`` incorrectly handle ``fill_value`` (:issue:`9765`)
- Bug in ``pd.concat()`` of ``SparseDataFrame`` may raise ``AttributeError`` (:issue:`12174`)
- Bug in ``SparseArray.shift()`` may raise ``NameError`` or ``TypeError`` (:issue:`12908`)

.. _whatsnew_0181.api:

API changes
~~~~~~~~~~~

.. _whatsnew_0181.api.groubynth:

Method ``.groupby(..).nth()`` changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The index in ``.groupby(..).nth()`` output is now more consistent when the ``as_index`` argument is passed (:issue:`11039`):

.. ipython:: python

   df = pd.DataFrame({"A": ["a", "b", "a"], "B": [1, 2, 3]})
   df

Previous behavior:

.. code-block:: ipython

   In [3]: df.groupby('A', as_index=True)['B'].nth(0)
   Out[3]:
   0    1
   1    2
   Name: B, dtype: int64

   In [4]: df.groupby('A', as_index=False)['B'].nth(0)
   Out[4]:
   0    1
   1    2
   Name: B, dtype: int64

New behavior:

.. ipython:: python

    df.groupby("A", as_index=True)["B"].nth(0)
    df.groupby("A", as_index=False)["B"].nth(0)

Furthermore, previously, a ``.groupby`` would always sort, regardless if ``sort=False`` was passed with ``.nth()``.

.. ipython:: python

   np.random.seed(1234)
   df = pd.DataFrame(np.random.randn(100, 2), columns=["a", "b"])
   df["c"] = np.random.randint(0, 4, 100)

Previous behavior:

.. code-block:: ipython

   In [4]: df.groupby('c', sort=True).nth(1)
   Out[4]:
             a         b
   c
   0 -0.334077  0.002118
   1  0.036142 -2.074978
   2 -0.720589  0.887163
   3  0.859588 -0.636524

   In [5]: df.groupby('c', sort=False).nth(1)
   Out[5]:
             a         b
   c
   0 -0.334077  0.002118
   1  0.036142 -2.074978
   2 -0.720589  0.887163
   3  0.859588 -0.636524

New behavior:

.. ipython:: python

   df.groupby("c", sort=True).nth(1)
   df.groupby("c", sort=False).nth(1)


.. _whatsnew_0181.numpy_compatibility:

NumPy function compatibility
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Compatibility between pandas array-like methods (e.g. ``sum`` and ``take``) and their ``numpy``
counterparts has been greatly increased by augmenting the signatures of the ``pandas`` methods so
as to accept arguments that can be passed in from ``numpy``, even if they are not necessarily
used in the ``pandas`` implementation (:issue:`12644`, :issue:`12638`, :issue:`12687`)

- ``.searchsorted()`` for ``Index`` and ``TimedeltaIndex`` now accept a ``sorter`` argument to maintain compatibility with numpy's ``searchsorted`` function (:issue:`12238`)
- Bug in numpy compatibility of ``np.round()`` on a ``Series`` (:issue:`12600`)

An example of this signature augmentation is illustrated below:

.. code-block:: python

   sp = pd.SparseDataFrame([1, 2, 3])
   sp

Previous behaviour:

.. code-block:: ipython

   In [2]: np.cumsum(sp, axis=0)
   ...
   TypeError: cumsum() takes at most 2 arguments (4 given)

New behaviour:

.. code-block:: python

   np.cumsum(sp, axis=0)

.. _whatsnew_0181.apply_resample:

Using ``.apply`` on GroupBy resampling
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using ``apply`` on resampling groupby operations (using a ``pd.TimeGrouper``) now has the same output types as similar ``apply`` calls on other groupby operations. (:issue:`11742`).

.. ipython:: python

    df = pd.DataFrame(
        {"date": pd.to_datetime(["10/10/2000", "11/10/2000"]), "value": [10, 13]}
    )
    df

Previous behavior:

.. code-block:: ipython

    In [1]: df.groupby(pd.TimeGrouper(key='date',
       ...:                           freq='M')).apply(lambda x: x.value.sum())
    Out[1]:
    ...
    TypeError: cannot concatenate a non-NDFrame object

    # Output is a Series
    In [2]: df.groupby(pd.TimeGrouper(key='date',
       ...:                           freq='M')).apply(lambda x: x[['value']].sum())
    Out[2]:
    date
    2000-10-31  value    10
    2000-11-30  value    13
    dtype: int64

New behavior:

.. code-block:: ipython

    # Output is a Series
    In [55]: df.groupby(pd.TimeGrouper(key='date',
        ...:                           freq='M')).apply(lambda x: x.value.sum())
    Out[55]:
    date
    2000-10-31    10
    2000-11-30    13
    Freq: M, dtype: int64

    # Output is a DataFrame
    In [56]: df.groupby(pd.TimeGrouper(key='date',
        ...:                           freq='M')).apply(lambda x: x[['value']].sum())
    Out[56]:
                value
    date
    2000-10-31     10
    2000-11-30     13

.. _whatsnew_0181.read_csv_exceptions:

Changes in ``read_csv`` exceptions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


In order to standardize the ``read_csv`` API for both the ``c`` and ``python`` engines, both will now raise an
``EmptyDataError``, a subclass of ``ValueError``, in response to empty columns or header (:issue:`12493`, :issue:`12506`)

Previous behaviour:

.. code-block:: ipython

   In [1]: import io

   In [2]: df = pd.read_csv(io.StringIO(''), engine='c')
   ...
   ValueError: No columns to parse from file

   In [3]: df = pd.read_csv(io.StringIO(''), engine='python')
   ...
   StopIteration

New behaviour:

.. code-block:: ipython

   In [1]: df = pd.read_csv(io.StringIO(''), engine='c')
   ...
   pandas.io.common.EmptyDataError: No columns to parse from file

   In [2]: df = pd.read_csv(io.StringIO(''), engine='python')
   ...
   pandas.io.common.EmptyDataError: No columns to parse from file

In addition to this error change, several others have been made as well:

- ``CParserError`` now sub-classes ``ValueError`` instead of just a ``Exception`` (:issue:`12551`)
- A ``CParserError`` is now raised instead of a generic ``Exception`` in ``read_csv`` when the ``c`` engine cannot parse a column (:issue:`12506`)
- A ``ValueError`` is now raised instead of a generic ``Exception`` in ``read_csv`` when the ``c`` engine encounters a ``NaN`` value in an integer column (:issue:`12506`)
- A ``ValueError`` is now raised instead of a generic ``Exception`` in ``read_csv`` when ``true_values`` is specified, and the ``c`` engine encounters an element in a column containing unencodable bytes (:issue:`12506`)
- ``pandas.parser.OverflowError`` exception has been removed and has been replaced with Python's built-in ``OverflowError`` exception (:issue:`12506`)
- ``pd.read_csv()`` no longer allows a combination of strings and integers for the ``usecols`` parameter (:issue:`12678`)


.. _whatsnew_0181.api.to_datetime:

Method ``to_datetime`` error changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Bugs in ``pd.to_datetime()`` when passing a ``unit`` with convertible entries and ``errors='coerce'`` or non-convertible with ``errors='ignore'``. Furthermore, an ``OutOfBoundsDateime`` exception will be raised when an out-of-range value is encountered for that unit when ``errors='raise'``. (:issue:`11758`, :issue:`13052`, :issue:`13059`)

Previous behaviour:

.. code-block:: ipython

   In [27]: pd.to_datetime(1420043460, unit='s', errors='coerce')
   Out[27]: NaT

   In [28]: pd.to_datetime(11111111, unit='D', errors='ignore')
   OverflowError: Python int too large to convert to C long

   In [29]: pd.to_datetime(11111111, unit='D', errors='raise')
   OverflowError: Python int too large to convert to C long

New behaviour:

.. code-block:: ipython

   In [2]: pd.to_datetime(1420043460, unit='s', errors='coerce')
   Out[2]: Timestamp('2014-12-31 16:31:00')

   In [3]: pd.to_datetime(11111111, unit='D', errors='ignore')
   Out[3]: 11111111

   In [4]: pd.to_datetime(11111111, unit='D', errors='raise')
   OutOfBoundsDatetime: cannot convert input with unit 'D'

.. _whatsnew_0181.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- ``.swaplevel()`` for ``Series``, ``DataFrame``, ``Panel``, and ``MultiIndex`` now features defaults for its first two parameters ``i`` and ``j`` that swap the two innermost levels of the index. (:issue:`12934`)
- ``.searchsorted()`` for ``Index`` and ``TimedeltaIndex`` now accept a ``sorter`` argument to maintain compatibility with numpy's ``searchsorted`` function (:issue:`12238`)
- ``Period`` and ``PeriodIndex`` now raises ``IncompatibleFrequency`` error which inherits ``ValueError`` rather than raw ``ValueError`` (:issue:`12615`)
- ``Series.apply`` for category dtype now applies the passed function to each of the ``.categories`` (and not the ``.codes``), and returns a ``category`` dtype if possible (:issue:`12473`)
- ``read_csv`` will now raise a ``TypeError`` if ``parse_dates`` is neither a boolean, list, or dictionary (matches the doc-string) (:issue:`5636`)
- The default for ``.query()/.eval()`` is now ``engine=None``, which will use ``numexpr`` if it's installed; otherwise it will fallback to the ``python`` engine. This mimics the pre-0.18.1 behavior if ``numexpr`` is installed (and which, previously, if numexpr was not installed, ``.query()/.eval()`` would raise). (:issue:`12749`)
- ``pd.show_versions()`` now includes ``pandas_datareader`` version (:issue:`12740`)
- Provide a proper ``__name__`` and ``__qualname__`` attributes for generic functions (:issue:`12021`)
- ``pd.concat(ignore_index=True)`` now uses ``RangeIndex`` as default (:issue:`12695`)
- ``pd.merge()`` and ``DataFrame.join()`` will show a ``UserWarning`` when merging/joining a single- with a multi-leveled dataframe (:issue:`9455`, :issue:`12219`)
- Compat with ``scipy`` > 0.17 for deprecated ``piecewise_polynomial`` interpolation method; support for the replacement ``from_derivatives`` method (:issue:`12887`)

.. _whatsnew_0181.deprecations:

Deprecations
^^^^^^^^^^^^

- The method name ``Index.sym_diff()`` is deprecated and can be replaced by ``Index.symmetric_difference()`` (:issue:`12591`)
- The method name ``Categorical.sort()`` is deprecated in favor of ``Categorical.sort_values()`` (:issue:`12882`)








.. _whatsnew_0181.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved speed of SAS reader (:issue:`12656`, :issue:`12961`)
- Performance improvements in ``.groupby(..).cumcount()`` (:issue:`11039`)
- Improved memory usage in ``pd.read_csv()`` when using ``skiprows=an_integer`` (:issue:`13005`)
- Improved performance of ``DataFrame.to_sql`` when checking case sensitivity for tables. Now only checks if table has been created correctly when table name is not lower case. (:issue:`12876`)
- Improved performance of ``Period`` construction and time series plotting (:issue:`12903`, :issue:`11831`).
- Improved performance of ``.str.encode()`` and ``.str.decode()`` methods (:issue:`13008`)
- Improved performance of ``to_numeric`` if input is numeric dtype (:issue:`12777`)
- Improved performance of sparse arithmetic with ``IntIndex`` (:issue:`13036`)








.. _whatsnew_0181.bug_fixes:

Bug fixes
~~~~~~~~~
- ``usecols`` parameter in ``pd.read_csv`` is now respected even when the lines of a CSV file are not even (:issue:`12203`)
- Bug in ``groupby.transform(..)`` when ``axis=1`` is specified with a non-monotonic ordered index (:issue:`12713`)
- Bug in ``Period`` and ``PeriodIndex`` creation raises ``KeyError`` if ``freq="Minute"`` is specified. Note that "Minute" freq is deprecated in v0.17.0, and recommended to use ``freq="T"`` instead (:issue:`11854`)
- Bug in ``.resample(...).count()`` with a ``PeriodIndex`` always raising a ``TypeError`` (:issue:`12774`)
- Bug in ``.resample(...)`` with a ``PeriodIndex`` casting to a ``DatetimeIndex`` when empty (:issue:`12868`)
- Bug in ``.resample(...)`` with a ``PeriodIndex`` when resampling to an existing frequency (:issue:`12770`)
- Bug in printing data which contains ``Period`` with different ``freq`` raises ``ValueError`` (:issue:`12615`)
- Bug in ``Series`` construction with ``Categorical`` and ``dtype='category'`` is specified (:issue:`12574`)
- Bugs in concatenation with a coercible dtype was too aggressive, resulting in different dtypes in output formatting when an object was longer than ``display.max_rows`` (:issue:`12411`, :issue:`12045`, :issue:`11594`, :issue:`10571`, :issue:`12211`)
- Bug in ``float_format`` option with option not being validated as a callable. (:issue:`12706`)
- Bug in ``GroupBy.filter`` when ``dropna=False`` and no groups fulfilled the criteria (:issue:`12768`)
- Bug in ``__name__`` of ``.cum*`` functions (:issue:`12021`)
- Bug in ``.astype()`` of a ``Float64Inde/Int64Index`` to an ``Int64Index`` (:issue:`12881`)
- Bug in round tripping an integer based index in ``.to_json()/.read_json()`` when ``orient='index'`` (the default) (:issue:`12866`)
- Bug in plotting ``Categorical`` dtypes cause error when attempting stacked bar plot (:issue:`13019`)
- Compat with >= ``numpy`` 1.11 for ``NaT`` comparisons (:issue:`12969`)
- Bug in ``.drop()`` with a non-unique ``MultiIndex``. (:issue:`12701`)
- Bug in ``.concat`` of datetime tz-aware and naive DataFrames (:issue:`12467`)
- Bug in correctly raising a ``ValueError`` in ``.resample(..).fillna(..)`` when passing a non-string (:issue:`12952`)
- Bug fixes in various encoding and header processing issues in ``pd.read_sas()`` (:issue:`12659`, :issue:`12654`, :issue:`12647`, :issue:`12809`)
- Bug in ``pd.crosstab()`` where would silently ignore ``aggfunc`` if ``values=None`` (:issue:`12569`).
- Potential segfault in ``DataFrame.to_json`` when serialising ``datetime.time`` (:issue:`11473`).
- Potential segfault in ``DataFrame.to_json`` when attempting to serialise 0d array (:issue:`11299`).
- Segfault in ``to_json`` when attempting to serialise a ``DataFrame`` or ``Series`` with non-ndarray values; now supports serialization of ``category``, ``sparse``, and ``datetime64[ns, tz]`` dtypes (:issue:`10778`).
- Bug in ``DataFrame.to_json`` with unsupported dtype not passed to default handler (:issue:`12554`).
- Bug in ``.align`` not returning the sub-class (:issue:`12983`)
- Bug in aligning a ``Series`` with a ``DataFrame`` (:issue:`13037`)
- Bug in ``ABCPanel`` in which ``Panel4D`` was not being considered as a valid instance of this generic type (:issue:`12810`)


- Bug in consistency of ``.name`` on ``.groupby(..).apply(..)`` cases (:issue:`12363`)

- Bug in ``Timestamp.__repr__`` that caused ``pprint`` to fail in nested structures (:issue:`12622`)
- Bug in ``Timedelta.min`` and ``Timedelta.max``, the properties now report the true minimum/maximum ``timedeltas`` as recognized by pandas. See the :ref:`documentation <timedeltas.limitations>`. (:issue:`12727`)
- Bug in ``.quantile()`` with interpolation may coerce to ``float`` unexpectedly (:issue:`12772`)
- Bug in ``.quantile()`` with empty ``Series`` may return scalar rather than empty ``Series`` (:issue:`12772`)


- Bug in ``.loc`` with out-of-bounds in a large indexer would raise ``IndexError`` rather than ``KeyError`` (:issue:`12527`)
- Bug in resampling when using a ``TimedeltaIndex`` and ``.asfreq()``, would previously not include the final fencepost (:issue:`12926`)

- Bug in equality testing with a ``Categorical`` in a ``DataFrame`` (:issue:`12564`)
- Bug in ``GroupBy.first()``, ``.last()`` returns incorrect row when ``TimeGrouper`` is used (:issue:`7453`)



- Bug in ``pd.read_csv()`` with the ``c`` engine when specifying ``skiprows`` with newlines in quoted items (:issue:`10911`, :issue:`12775`)
- Bug in ``DataFrame`` timezone lost when assigning tz-aware datetime ``Series`` with alignment (:issue:`12981`)




- Bug in ``.value_counts()`` when ``normalize=True`` and ``dropna=True`` where nulls still contributed to the normalized count (:issue:`12558`)
- Bug in ``Series.value_counts()`` loses name if its dtype is ``category`` (:issue:`12835`)
- Bug in ``Series.value_counts()`` loses timezone info (:issue:`12835`)
- Bug in ``Series.value_counts(normalize=True)`` with ``Categorical`` raises ``UnboundLocalError`` (:issue:`12835`)
- Bug in ``Panel.fillna()`` ignoring ``inplace=True`` (:issue:`12633`)
- Bug in ``pd.read_csv()`` when specifying ``names``, ``usecols``, and ``parse_dates`` simultaneously with the ``c`` engine (:issue:`9755`)
- Bug in ``pd.read_csv()`` when specifying ``delim_whitespace=True`` and ``lineterminator`` simultaneously with the ``c`` engine (:issue:`12912`)
- Bug in ``Series.rename``, ``DataFrame.rename`` and ``DataFrame.rename_axis`` not treating ``Series`` as mappings to relabel (:issue:`12623`).
- Clean in ``.rolling.min`` and ``.rolling.max`` to enhance dtype handling (:issue:`12373`)
- Bug in ``groupby`` where complex types are coerced to float (:issue:`12902`)
- Bug in ``Series.map`` raises ``TypeError`` if its dtype is ``category`` or tz-aware ``datetime`` (:issue:`12473`)

- Bugs on 32bit platforms for some test comparisons (:issue:`12972`)
- Bug in index coercion when falling back from ``RangeIndex`` construction (:issue:`12893`)
- Better error message in window functions when invalid argument (e.g. a float window) is passed (:issue:`12669`)

- Bug in slicing subclassed ``DataFrame`` defined to return subclassed ``Series`` may return normal ``Series`` (:issue:`11559`)


- Bug in ``.str`` accessor methods may raise ``ValueError`` if input has ``name`` and the result is ``DataFrame`` or ``MultiIndex`` (:issue:`12617`)
- Bug in ``DataFrame.last_valid_index()`` and ``DataFrame.first_valid_index()`` on empty frames (:issue:`12800`)


- Bug in ``CategoricalIndex.get_loc`` returns different result from regular ``Index`` (:issue:`12531`)
- Bug in ``PeriodIndex.resample`` where name not propagated (:issue:`12769`)

- Bug in ``date_range`` ``closed`` keyword and timezones (:issue:`12684`).

- Bug in ``pd.concat`` raises ``AttributeError`` when input data contains tz-aware datetime and timedelta (:issue:`12620`)
- Bug in ``pd.concat`` did not handle empty ``Series`` properly (:issue:`11082`)

- Bug in ``.plot.bar`` alignment when ``width`` is specified with ``int`` (:issue:`12979`)


- Bug in ``fill_value`` is ignored if the argument to a binary operator is a constant (:issue:`12723`)

- Bug in ``pd.read_html()`` when using bs4 flavor and parsing table with a header and only one column (:issue:`9178`)

- Bug in ``.pivot_table`` when ``margins=True`` and ``dropna=True`` where nulls still contributed to margin count (:issue:`12577`)
- Bug in ``.pivot_table`` when ``dropna=False`` where table index/column names disappear (:issue:`12133`)
- Bug in ``pd.crosstab()`` when ``margins=True`` and ``dropna=False`` which raised (:issue:`12642`)

- Bug in ``Series.name`` when ``name`` attribute can be a hashable type (:issue:`12610`)

- Bug in ``.describe()`` resets categorical columns information (:issue:`11558`)
- Bug where ``loffset`` argument was not applied when calling ``resample().count()`` on a timeseries (:issue:`12725`)
- ``pd.read_excel()`` now accepts column names associated with keyword argument ``names`` (:issue:`12870`)
- Bug in ``pd.to_numeric()`` with ``Index`` returns ``np.ndarray``, rather than ``Index`` (:issue:`12777`)
- Bug in ``pd.to_numeric()`` with datetime-like may raise ``TypeError`` (:issue:`12777`)
- Bug in ``pd.to_numeric()`` with scalar raises ``ValueError`` (:issue:`12777`)


.. _whatsnew_0.18.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.18.0..v0.18.1
.. _whatsnew_0801:

Version 0.8.1 (July 22, 2012)
-----------------------------

{{ header }}


This release includes a few new features, performance enhancements, and over 30
bug fixes from 0.8.0.  New features include notably NA friendly string
processing functionality and a series of new plot types and options.

New features
~~~~~~~~~~~~

  - Add :ref:`vectorized string processing methods <text.string_methods>`
    accessible via Series.str (:issue:`620`)
  - Add option to disable adjustment in EWMA (:issue:`1584`)
  - :ref:`Radviz plot <visualization.radviz>` (:issue:`1566`)
  - :ref:`Parallel coordinates plot <visualization.parallel_coordinates>`
  - :ref:`Bootstrap plot <visualization.bootstrap>`
  - Per column styles and secondary y-axis plotting (:issue:`1559`)
  - New datetime converters millisecond plotting  (:issue:`1599`)
  - Add option to disable "sparse" display of hierarchical indexes (:issue:`1538`)
  - Series/DataFrame's ``set_index`` method can :ref:`append levels
    <indexing.set_index>` to an existing Index/MultiIndex (:issue:`1569`, :issue:`1577`)

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

  - Improved implementation of rolling min and max (thanks to `Bottleneck
    <https://bottleneck.readthedocs.io>`__ !)
  - Add accelerated ``'median'`` GroupBy option (:issue:`1358`)
  - Significantly improve the performance of parsing ISO8601-format date
    strings with ``DatetimeIndex`` or ``to_datetime`` (:issue:`1571`)
  - Improve the performance of GroupBy on single-key aggregations and use with
    Categorical types
  - Significant datetime parsing performance improvements



.. _whatsnew_0.8.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.8.0..v0.8.1
.. _release:

{{ header }}

*************
Release notes
*************

This is the list of changes to pandas between each release. For full details,
see the `commit logs <https://github.com/pandas-dev/pandas/commits/>`_. For install and
upgrade instructions, see :ref:`install`.

Version 1.5
-----------

.. toctree::
   :maxdepth: 2

   v1.5.0

Version 1.4
-----------

.. toctree::
   :maxdepth: 2

   v1.4.1
   v1.4.0

Version 1.3
-----------

.. toctree::
   :maxdepth: 2

   v1.3.5
   v1.3.4
   v1.3.3
   v1.3.2
   v1.3.1
   v1.3.0

Version 1.2
-----------

.. toctree::
   :maxdepth: 2

   v1.2.5
   v1.2.4
   v1.2.3
   v1.2.2
   v1.2.1
   v1.2.0

Version 1.1
-----------

.. toctree::
   :maxdepth: 2

   v1.1.5
   v1.1.4
   v1.1.3
   v1.1.2
   v1.1.1
   v1.1.0

Version 1.0
-----------

.. toctree::
   :maxdepth: 2

   v1.0.5
   v1.0.4
   v1.0.3
   v1.0.2
   v1.0.1
   v1.0.0

Version 0.25
------------

.. toctree::
   :maxdepth: 2

   v0.25.3
   v0.25.2
   v0.25.1
   v0.25.0

Version 0.24
------------

.. toctree::
   :maxdepth: 2

   v0.24.2
   v0.24.1
   v0.24.0

Version 0.23
------------

.. toctree::
   :maxdepth: 2

   v0.23.4
   v0.23.3
   v0.23.2
   v0.23.1
   v0.23.0

Version 0.22
------------

.. toctree::
   :maxdepth: 2

   v0.22.0

Version 0.21
------------

.. toctree::
   :maxdepth: 2

   v0.21.1
   v0.21.0

Version 0.20
------------

.. toctree::
   :maxdepth: 2

   v0.20.3
   v0.20.2
   v0.20.0

Version 0.19
------------

.. toctree::
   :maxdepth: 2

   v0.19.2
   v0.19.1
   v0.19.0

Version 0.18
------------

.. toctree::
   :maxdepth: 2

   v0.18.1
   v0.18.0

Version 0.17
------------

.. toctree::
   :maxdepth: 2

   v0.17.1
   v0.17.0

Version 0.16
------------

.. toctree::
   :maxdepth: 2

   v0.16.2
   v0.16.1
   v0.16.0

Version 0.15
------------

.. toctree::
   :maxdepth: 2

   v0.15.2
   v0.15.1
   v0.15.0

Version 0.14
------------

.. toctree::
   :maxdepth: 2

   v0.14.1
   v0.14.0

Version 0.13
------------

.. toctree::
   :maxdepth: 2

   v0.13.1
   v0.13.0

Version 0.12
------------

.. toctree::
   :maxdepth: 2

   v0.12.0

Version 0.11
------------

.. toctree::
   :maxdepth: 2

   v0.11.0

Version 0.10
------------

.. toctree::
   :maxdepth: 2

   v0.10.1
   v0.10.0

Version 0.9
-----------

.. toctree::
   :maxdepth: 2

   v0.9.1
   v0.9.0

Version 0.8
------------

.. toctree::
   :maxdepth: 2

   v0.8.1
   v0.8.0

Version 0.7
-----------

.. toctree::
   :maxdepth: 2

   v0.7.3
   v0.7.2
   v0.7.1
   v0.7.0

Version 0.6
-----------

.. toctree::
   :maxdepth: 2

   v0.6.1
   v0.6.0

Version 0.5
-----------

.. toctree::
   :maxdepth: 2

   v0.5.0

Version 0.4
-----------

.. toctree::
   :maxdepth: 2

   v0.4.x
.. _whatsnew_0101:

Version 0.10.1 (January 22, 2013)
---------------------------------

{{ header }}


This is a minor release from 0.10.0 and includes new features, enhancements,
and bug fixes. In particular, there is substantial new HDFStore functionality
contributed by Jeff Reback.

An undesired API breakage with functions taking the ``inplace`` option has been
reverted and deprecation warnings added.

API changes
~~~~~~~~~~~

- Functions taking an ``inplace`` option return the calling object as before. A
  deprecation message has been added
- Groupby aggregations Max/Min no longer exclude non-numeric data (:issue:`2700`)
- Resampling an empty DataFrame now returns an empty DataFrame instead of
  raising an exception (:issue:`2640`)
- The file reader will now raise an exception when NA values are found in an
  explicitly specified integer column instead of converting the column to float
  (:issue:`2631`)
- DatetimeIndex.unique now returns a DatetimeIndex with the same name and
- timezone instead of an array (:issue:`2563`)

New features
~~~~~~~~~~~~

- MySQL support for database (contribution from Dan Allan)

HDFStore
~~~~~~~~

You may need to upgrade your existing data files. Please visit the
**compatibility** section in the main docs.


.. ipython:: python
   :suppress:
   :okexcept:

   import os

   os.remove("store.h5")

You can designate (and index) certain columns that you want to be able to
perform queries on a table, by passing a list to ``data_columns``

.. ipython:: python

   store = pd.HDFStore("store.h5")
   df = pd.DataFrame(
       np.random.randn(8, 3),
       index=pd.date_range("1/1/2000", periods=8),
       columns=["A", "B", "C"],
   )
   df["string"] = "foo"
   df.loc[df.index[4:6], "string"] = np.nan
   df.loc[df.index[7:9], "string"] = "bar"
   df["string2"] = "cool"
   df

   # on-disk operations
   store.append("df", df, data_columns=["B", "C", "string", "string2"])
   store.select("df", "B>0 and string=='foo'")

   # this is in-memory version of this type of selection
   df[(df.B > 0) & (df.string == "foo")]

Retrieving unique values in an indexable or data column.

.. code-block:: python

   # note that this is deprecated as of 0.14.0
   # can be replicated by: store.select_column('df','index').unique()
   store.unique("df", "index")
   store.unique("df", "string")

You can now store ``datetime64`` in data columns

.. ipython:: python

    df_mixed = df.copy()
    df_mixed["datetime64"] = pd.Timestamp("20010102")
    df_mixed.loc[df_mixed.index[3:4], ["A", "B"]] = np.nan

    store.append("df_mixed", df_mixed)
    df_mixed1 = store.select("df_mixed")
    df_mixed1
    df_mixed1.dtypes.value_counts()

You can pass ``columns`` keyword to select to filter a list of the return
columns, this is equivalent to passing a
``Term('columns',list_of_columns_to_filter)``

.. ipython:: python

   store.select("df", columns=["A", "B"])

``HDFStore`` now serializes MultiIndex dataframes when appending tables.

.. code-block:: ipython

    In [19]: index = pd.MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'],
       ....:                               ['one', 'two', 'three']],
       ....:                       labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],
       ....:                               [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],
       ....:                       names=['foo', 'bar'])
       ....:

    In [20]: df = pd.DataFrame(np.random.randn(10, 3), index=index,
       ....:                   columns=['A', 'B', 'C'])
       ....:

    In [21]: df
    Out[21]:
                      A         B         C
    foo bar
    foo one   -0.116619  0.295575 -1.047704
        two    1.640556  1.905836  2.772115
        three  0.088787 -1.144197 -0.633372
    bar one    0.925372 -0.006438 -0.820408
        two   -0.600874 -1.039266  0.824758
    baz two   -0.824095 -0.337730 -0.927764
        three -0.840123  0.248505 -0.109250
    qux one    0.431977 -0.460710  0.336505
        two   -3.207595 -1.535854  0.409769
        three -0.673145 -0.741113 -0.110891

    In [22]: store.append('mi', df)

    In [23]: store.select('mi')
    Out[23]:
                      A         B         C
    foo bar
    foo one   -0.116619  0.295575 -1.047704
        two    1.640556  1.905836  2.772115
        three  0.088787 -1.144197 -0.633372
    bar one    0.925372 -0.006438 -0.820408
        two   -0.600874 -1.039266  0.824758
    baz two   -0.824095 -0.337730 -0.927764
        three -0.840123  0.248505 -0.109250
    qux one    0.431977 -0.460710  0.336505
        two   -3.207595 -1.535854  0.409769
        three -0.673145 -0.741113 -0.110891

    # the levels are automatically included as data columns
    In [24]: store.select('mi', "foo='bar'")
    Out[24]:
                    A         B         C
    foo bar
    bar one  0.925372 -0.006438 -0.820408
        two -0.600874 -1.039266  0.824758

Multi-table creation via ``append_to_multiple`` and selection via
``select_as_multiple`` can create/select from multiple tables and return a
combined result, by using ``where`` on a selector table.

.. ipython:: python

   df_mt = pd.DataFrame(
       np.random.randn(8, 6),
       index=pd.date_range("1/1/2000", periods=8),
       columns=["A", "B", "C", "D", "E", "F"],
   )
   df_mt["foo"] = "bar"

   # you can also create the tables individually
   store.append_to_multiple(
       {"df1_mt": ["A", "B"], "df2_mt": None}, df_mt, selector="df1_mt"
   )
   store

   # individual tables were created
   store.select("df1_mt")
   store.select("df2_mt")

   # as a multiple
   store.select_as_multiple(
       ["df1_mt", "df2_mt"], where=["A>0", "B>0"], selector="df1_mt"
   )

.. ipython:: python
   :suppress:

   store.close()
   os.remove("store.h5")

**Enhancements**

- ``HDFStore`` now can read native PyTables table format tables

- You can pass ``nan_rep = 'my_nan_rep'`` to append, to change the default nan
  representation on disk (which converts to/from ``np.nan``), this defaults to
  ``nan``.

- You can pass ``index`` to ``append``. This defaults to ``True``. This will
  automagically create indices on the *indexables* and *data columns* of the
  table

- You can pass ``chunksize=an integer`` to ``append``, to change the writing
  chunksize (default is 50000). This will significantly lower your memory usage
  on writing.

- You can pass ``expectedrows=an integer`` to the first ``append``, to set the
  TOTAL number of expected rows that ``PyTables`` will expected. This will
  optimize read/write performance.

- ``Select`` now supports passing ``start`` and ``stop`` to provide selection
  space limiting in selection.

- Greatly improved ISO8601 (e.g., yyyy-mm-dd) date parsing for file parsers (:issue:`2698`)
- Allow ``DataFrame.merge`` to handle combinatorial sizes too large for 64-bit
  integer (:issue:`2690`)
- Series now has unary negation (-series) and inversion (~series) operators (:issue:`2686`)
- DataFrame.plot now includes a ``logx`` parameter to change the x-axis to log scale (:issue:`2327`)
- Series arithmetic operators can now handle constant and ndarray input (:issue:`2574`)
- ExcelFile now takes a ``kind`` argument to specify the file type (:issue:`2613`)
- A faster implementation for Series.str methods (:issue:`2602`)

**Bug Fixes**

- ``HDFStore`` tables can now store ``float32`` types correctly (cannot be
  mixed with ``float64`` however)
- Fixed Google Analytics prefix when specifying request segment (:issue:`2713`).
- Function to reset Google Analytics token store so users can recover from
  improperly setup client secrets (:issue:`2687`).
- Fixed groupby bug resulting in segfault when passing in MultiIndex (:issue:`2706`)
- Fixed bug where passing a Series with datetime64 values into ``to_datetime``
  results in bogus output values (:issue:`2699`)
- Fixed bug in ``pattern in HDFStore`` expressions when pattern is not a valid
  regex (:issue:`2694`)
- Fixed performance issues while aggregating boolean data (:issue:`2692`)
- When given a boolean mask key and a Series of new values, Series __setitem__
  will now align the incoming values with the original Series (:issue:`2686`)
- Fixed MemoryError caused by performing counting sort on sorting MultiIndex
  levels with a very large number of combinatorial values (:issue:`2684`)
- Fixed bug that causes plotting to fail when the index is a DatetimeIndex with
  a fixed-offset timezone (:issue:`2683`)
- Corrected business day subtraction logic when the offset is more than 5 bdays
  and the starting date is on a weekend (:issue:`2680`)
- Fixed C file parser behavior when the file has more columns than data
  (:issue:`2668`)
- Fixed file reader bug that misaligned columns with data in the presence of an
  implicit column and a specified ``usecols`` value
- DataFrames with numerical or datetime indices are now sorted prior to
  plotting (:issue:`2609`)
- Fixed DataFrame.from_records error when passed columns, index, but empty
  records (:issue:`2633`)
- Several bug fixed for Series operations when dtype is datetime64 (:issue:`2689`,
  :issue:`2629`, :issue:`2626`)


See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.


.. _whatsnew_0.10.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.10.0..v0.10.1
.. _whatsnew_0191:

Version 0.19.1 (November 3, 2016)
---------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import *  # noqa F401, F403


This is a minor bug-fix release from 0.19.0 and includes some small regression fixes,
bug fixes and performance improvements.
We recommend that all users upgrade to this version.

.. contents:: What's new in v0.19.1
    :local:
    :backlinks: none


.. _whatsnew_0191.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Fixed performance regression in factorization of ``Period`` data (:issue:`14338`)
- Fixed performance regression in ``Series.asof(where)`` when ``where`` is a scalar (:issue:`14461`)
- Improved performance in ``DataFrame.asof(where)`` when ``where`` is a scalar (:issue:`14461`)
- Improved performance in ``.to_json()`` when ``lines=True`` (:issue:`14408`)
- Improved performance in certain types of ``loc`` indexing with a MultiIndex (:issue:`14551`).


.. _whatsnew_0191.bug_fixes:

Bug fixes
~~~~~~~~~

- Source installs from PyPI will now again work without ``cython`` installed, as in previous versions (:issue:`14204`)
- Compat with Cython 0.25 for building (:issue:`14496`)
- Fixed regression where user-provided file handles were closed in ``read_csv`` (c engine) (:issue:`14418`).
- Fixed regression in ``DataFrame.quantile`` when missing values where present in some columns (:issue:`14357`).
- Fixed regression in ``Index.difference`` where the ``freq`` of a ``DatetimeIndex`` was incorrectly set (:issue:`14323`)
- Added back ``pandas.core.common.array_equivalent`` with a deprecation warning (:issue:`14555`).
- Bug in ``pd.read_csv`` for the C engine in which quotation marks were improperly parsed in skipped rows (:issue:`14459`)
- Bug in ``pd.read_csv`` for Python 2.x in which Unicode quote characters were no longer being respected (:issue:`14477`)
- Fixed regression in ``Index.append`` when categorical indices were appended (:issue:`14545`).
- Fixed regression in ``pd.DataFrame`` where constructor fails when given dict with ``None`` value (:issue:`14381`)
- Fixed regression in ``DatetimeIndex._maybe_cast_slice_bound`` when index is empty (:issue:`14354`).
- Bug in localizing an ambiguous timezone when a boolean is passed (:issue:`14402`)
- Bug in ``TimedeltaIndex`` addition with a Datetime-like object where addition overflow in the negative direction was not being caught (:issue:`14068`, :issue:`14453`)
- Bug in string indexing against data with ``object`` ``Index`` may raise ``AttributeError`` (:issue:`14424`)
- Correctly raise ``ValueError`` on empty input to ``pd.eval()`` and ``df.query()`` (:issue:`13139`)
- Bug in ``RangeIndex.intersection`` when result is a empty set (:issue:`14364`).
- Bug in groupby-transform broadcasting that could cause incorrect dtype coercion (:issue:`14457`)
- Bug in ``Series.__setitem__`` which allowed mutating read-only arrays (:issue:`14359`).
- Bug in ``DataFrame.insert`` where multiple calls with duplicate columns can fail (:issue:`14291`)
- ``pd.merge()`` will raise ``ValueError`` with non-boolean parameters in passed boolean type arguments (:issue:`14434`)
- Bug in ``Timestamp`` where dates very near the minimum (1677-09) could underflow on creation (:issue:`14415`)
- Bug in ``pd.concat`` where names of the ``keys`` were not propagated to the resulting ``MultiIndex`` (:issue:`14252`)
- Bug in ``pd.concat`` where ``axis`` cannot take string parameters ``'rows'`` or ``'columns'`` (:issue:`14369`)
- Bug in ``pd.concat`` with dataframes heterogeneous in length and tuple ``keys`` (:issue:`14438`)
- Bug in ``MultiIndex.set_levels`` where illegal level values were still set after raising an error (:issue:`13754`)
- Bug in ``DataFrame.to_json`` where ``lines=True`` and a value contained a ``}`` character (:issue:`14391`)
- Bug in ``df.groupby`` causing an ``AttributeError`` when grouping a single index frame by a column and the index level (:issue:`14327`)
- Bug in ``df.groupby`` where ``TypeError`` raised when ``pd.Grouper(key=...)`` is passed in a list (:issue:`14334`)
- Bug in ``pd.pivot_table`` may raise ``TypeError`` or ``ValueError`` when ``index`` or ``columns``
  is not scalar and ``values`` is not specified (:issue:`14380`)


.. _whatsnew_0.19.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.19.0..v0.19.1
.. _whatsnew_0701:

Version 0.7.1 (February 29, 2012)
---------------------------------

{{ header }}


This release includes a few new features and addresses over a dozen bugs in
0.7.0.

New features
~~~~~~~~~~~~

  - Add ``to_clipboard`` function to pandas namespace for writing objects to
    the system clipboard (:issue:`774`)
  - Add ``itertuples`` method to DataFrame for iterating through the rows of a
    dataframe as tuples (:issue:`818`)
  - Add ability to pass fill_value and method to DataFrame and Series align
    method (:issue:`806`, :issue:`807`)
  - Add fill_value option to reindex, align methods (:issue:`784`)
  - Enable concat to produce DataFrame from Series (:issue:`787`)
  - Add ``between`` method to Series (:issue:`802`)
  - Add HTML representation hook to DataFrame for the IPython HTML notebook
    (:issue:`773`)
  - Support for reading Excel 2007 XML documents using openpyxl

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

  - Improve performance and memory usage of fillna on DataFrame
  - Can concatenate a list of Series along axis=1 to obtain a DataFrame (:issue:`787`)



.. _whatsnew_0.7.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.7.0..v0.7.1
.. _whatsnew_0901:

Version 0.9.1 (November 14, 2012)
---------------------------------

{{ header }}


This is a bug fix release from 0.9.0 and includes several new features and
enhancements along with a large number of bug fixes. The new features include
by-column sort order for DataFrame and Series, improved NA handling for the rank
method, masking functions for DataFrame, and intraday time-series filtering for
DataFrame.

New features
~~~~~~~~~~~~

  - ``Series.sort``, ``DataFrame.sort``, and ``DataFrame.sort_index`` can now be
    specified in a per-column manner to support multiple sort orders (:issue:`928`)

    .. code-block:: ipython

       In [2]: df = pd.DataFrame(np.random.randint(0, 2, (6, 3)),
          ...:                   columns=['A', 'B', 'C'])

       In [3]: df.sort(['A', 'B'], ascending=[1, 0])

       Out[3]:
          A  B  C
       3  0  1  1
       4  0  1  1
       2  0  0  1
       0  1  0  0
       1  1  0  0
       5  1  0  0

  - ``DataFrame.rank`` now supports additional argument values for the
    ``na_option`` parameter so missing values can be assigned either the largest
    or the smallest rank (:issue:`1508`, :issue:`2159`)

    .. ipython:: python

        df = pd.DataFrame(np.random.randn(6, 3), columns=['A', 'B', 'C'])

        df.loc[2:4] = np.nan

        df.rank()

        df.rank(na_option='top')

        df.rank(na_option='bottom')


  - DataFrame has new ``where`` and ``mask`` methods to select values according to a
    given boolean mask (:issue:`2109`, :issue:`2151`)

	DataFrame currently supports slicing via a boolean vector the same length as the DataFrame (inside the ``[]``).
	The returned DataFrame has the same number of columns as the original, but is sliced on its index.

        .. ipython:: python

    	    df = DataFrame(np.random.randn(5, 3), columns = ['A','B','C'])

	    df

	    df[df['A'] > 0]

	If a DataFrame is sliced with a DataFrame based boolean condition (with the same size as the original DataFrame),
	then a DataFrame the same size (index and columns) as the original is returned, with
	elements that do not meet the boolean condition as ``NaN``. This is accomplished via
	the new method ``DataFrame.where``. In addition, ``where`` takes an optional ``other`` argument for replacement.

	.. ipython:: python

	   df[df>0]

	   df.where(df>0)

	   df.where(df>0,-df)

	Furthermore, ``where`` now aligns the input boolean condition (ndarray or DataFrame), such that partial selection
	with setting is possible. This is analogous to partial setting via ``.ix`` (but on the contents rather than the axis labels)

	.. ipython:: python

	   df2 = df.copy()
   	   df2[ df2[1:4] > 0 ] = 3
	   df2

	``DataFrame.mask`` is the inverse boolean operation of ``where``.

	.. ipython:: python

	   df.mask(df<=0)

  - Enable referencing of Excel columns by their column names (:issue:`1936`)

    .. ipython:: python

        xl = pd.ExcelFile('data/test.xls')
        xl.parse('Sheet1', index_col=0, parse_dates=True,
                 parse_cols='A:D')


  - Added option to disable pandas-style tick locators and formatters
    using ``series.plot(x_compat=True)`` or ``pandas.plot_params['x_compat'] =
    True`` (:issue:`2205`)
  - Existing TimeSeries methods ``at_time`` and ``between_time`` were added to
    DataFrame (:issue:`2149`)
  - DataFrame.dot can now accept ndarrays (:issue:`2042`)
  - DataFrame.drop now supports non-unique indexes (:issue:`2101`)
  - Panel.shift now supports negative periods (:issue:`2164`)
  - DataFrame now support unary ~ operator (:issue:`2110`)

API changes
~~~~~~~~~~~

  - Upsampling data with a PeriodIndex will result in a higher frequency
    TimeSeries that spans the original time window

    .. code-block:: ipython

       In [1]: prng = pd.period_range('2012Q1', periods=2, freq='Q')

       In [2]: s = pd.Series(np.random.randn(len(prng)), prng)

       In [4]: s.resample('M')
       Out[4]:
       2012-01   -1.471992
       2012-02         NaN
       2012-03         NaN
       2012-04   -0.493593
       2012-05         NaN
       2012-06         NaN
       Freq: M, dtype: float64

  - Period.end_time now returns the last nanosecond in the time interval
    (:issue:`2124`, :issue:`2125`, :issue:`1764`)

    .. ipython:: python

        p = pd.Period('2012')

        p.end_time


  - File parsers no longer coerce to float or bool for columns that have custom
    converters specified (:issue:`2184`)

    .. ipython:: python

        import io

        data = ('A,B,C\n'
                '00001,001,5\n'
                '00002,002,6')
        pd.read_csv(io.StringIO(data), converters={'A': lambda x: x.strip()})


See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.


.. _whatsnew_0.9.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.9.0..v0.9.1
.. _whatsnew_0120:

Version 0.12.0 (July 24, 2013)
------------------------------

{{ header }}


This is a major release from 0.11.0 and includes several new features and
enhancements along with a large number of bug fixes.

Highlights include a consistent I/O API naming scheme, routines to read html,
write MultiIndexes to csv files, read & write STATA data files, read & write JSON format
files, Python 3 support for ``HDFStore``, filtering of groupby expressions via ``filter``, and a
revamped ``replace`` routine that accepts regular expressions.

API changes
~~~~~~~~~~~

  - The I/O API is now much more consistent with a set of top level ``reader`` functions
    accessed like ``pd.read_csv()`` that generally return a ``pandas`` object.

    * ``read_csv``
    * ``read_excel``
    * ``read_hdf``
    * ``read_sql``
    * ``read_json``
    * ``read_html``
    * ``read_stata``
    * ``read_clipboard``

    The corresponding ``writer`` functions are object methods that are accessed like ``df.to_csv()``

    * ``to_csv``
    * ``to_excel``
    * ``to_hdf``
    * ``to_sql``
    * ``to_json``
    * ``to_html``
    * ``to_stata``
    * ``to_clipboard``


  - Fix modulo and integer division on Series,DataFrames to act similarly to ``float`` dtypes to return
    ``np.nan`` or ``np.inf`` as appropriate (:issue:`3590`). This correct a numpy bug that treats ``integer``
    and ``float`` dtypes differently.

    .. ipython:: python

        p = pd.DataFrame({"first": [4, 5, 8], "second": [0, 0, 3]})
        p % 0
        p % p
        p / p
        p / 0

  - Add ``squeeze`` keyword to ``groupby`` to allow reduction from
    DataFrame -> Series if groups are unique. This is a Regression from 0.10.1.
    We are reverting back to the prior behavior. This means groupby will return the
    same shaped objects whether the groups are unique or not. Revert this issue (:issue:`2893`)
    with (:issue:`3596`).

    .. code-block:: ipython

        In [2]: df2 = pd.DataFrame([{"val1": 1, "val2": 20},
           ...:                     {"val1": 1, "val2": 19},
           ...:                     {"val1": 1, "val2": 27},
           ...:                     {"val1": 1, "val2": 12}])

        In [3]: def func(dataf):
           ...:     return dataf["val2"] - dataf["val2"].mean()
           ...:

        In [4]: # squeezing the result frame to a series (because we have unique groups)
           ...: df2.groupby("val1", squeeze=True).apply(func)
        Out[4]:
        0    0.5
        1   -0.5
        2    7.5
        3   -7.5
        Name: 1, dtype: float64

        In [5]: # no squeezing (the default, and behavior in 0.10.1)
           ...: df2.groupby("val1").apply(func)
        Out[5]:
        val2    0    1    2    3
        val1
        1     0.5 -0.5  7.5 -7.5

  - Raise on ``iloc`` when boolean indexing with a label based indexer mask
    e.g. a boolean Series, even with integer labels, will raise. Since ``iloc``
    is purely positional based, the labels on the Series are not alignable (:issue:`3631`)

    This case is rarely used, and there are plenty of alternatives. This preserves the
    ``iloc`` API to be *purely* positional based.

    .. ipython:: python

       df = pd.DataFrame(range(5), index=list("ABCDE"), columns=["a"])
       mask = df.a % 2 == 0
       mask

       # this is what you should use
       df.loc[mask]

       # this will work as well
       df.iloc[mask.values]

    ``df.iloc[mask]`` will raise a ``ValueError``

  - The ``raise_on_error`` argument to plotting functions is removed. Instead,
    plotting functions raise a ``TypeError`` when the ``dtype`` of the object
    is ``object`` to remind you to avoid ``object`` arrays whenever possible
    and thus you should cast to an appropriate numeric dtype if you need to
    plot something.

  - Add ``colormap`` keyword to DataFrame plotting methods. Accepts either a
    matplotlib colormap object (ie, matplotlib.cm.jet) or a string name of such
    an object (ie, 'jet'). The colormap is sampled to select the color for each
    column. Please see :ref:`visualization.colormaps` for more information.
    (:issue:`3860`)

  - ``DataFrame.interpolate()`` is now deprecated. Please use
    ``DataFrame.fillna()`` and ``DataFrame.replace()`` instead. (:issue:`3582`,
    :issue:`3675`, :issue:`3676`)

  - the ``method`` and ``axis`` arguments of ``DataFrame.replace()`` are
    deprecated

  - ``DataFrame.replace`` 's ``infer_types`` parameter is removed and now
    performs conversion by default. (:issue:`3907`)

  - Add the keyword ``allow_duplicates`` to ``DataFrame.insert`` to allow a duplicate column
    to be inserted if ``True``, default is ``False`` (same as prior to 0.12) (:issue:`3679`)
  - Implement ``__nonzero__`` for ``NDFrame`` objects (:issue:`3691`, :issue:`3696`)

  - IO api

    - added top-level function ``read_excel`` to replace the following,
      The original API is deprecated and will be removed in a future version

      .. code-block:: python

         from pandas.io.parsers import ExcelFile

         xls = ExcelFile("path_to_file.xls")
         xls.parse("Sheet1", index_col=None, na_values=["NA"])

      With

      .. code-block:: python

         import pandas as pd

         pd.read_excel("path_to_file.xls", "Sheet1", index_col=None, na_values=["NA"])

    - added top-level function ``read_sql`` that is equivalent to the following

      .. code-block:: python

         from pandas.io.sql import read_frame

         read_frame(...)

  - ``DataFrame.to_html`` and ``DataFrame.to_latex`` now accept a path for
    their first argument (:issue:`3702`)

  - Do not allow astypes on ``datetime64[ns]`` except to ``object``, and
    ``timedelta64[ns]`` to ``object/int`` (:issue:`3425`)

  - The behavior of ``datetime64`` dtypes has changed with respect to certain
    so-called reduction operations (:issue:`3726`). The following operations now
    raise a ``TypeError`` when performed on a ``Series`` and return an *empty*
    ``Series`` when performed on a ``DataFrame`` similar to performing these
    operations on, for example, a ``DataFrame`` of ``slice`` objects:

    - sum, prod, mean, std, var, skew, kurt, corr, and cov

  - ``read_html`` now defaults to ``None`` when reading, and falls back on
    ``bs4`` + ``html5lib`` when lxml fails to parse. a list of parsers to try
    until success is also valid

  - The internal ``pandas`` class hierarchy has changed (slightly). The
    previous ``PandasObject`` now is called ``PandasContainer`` and a new
    ``PandasObject`` has become the base class for ``PandasContainer`` as well
    as ``Index``, ``Categorical``, ``GroupBy``, ``SparseList``, and
    ``SparseArray`` (+ their base classes). Currently, ``PandasObject``
    provides string methods (from ``StringMixin``). (:issue:`4090`, :issue:`4092`)

  - New ``StringMixin`` that, given a ``__unicode__`` method, gets python 2 and
    python 3 compatible string methods (``__str__``, ``__bytes__``, and
    ``__repr__``). Plus string safety throughout. Now employed in many places
    throughout the pandas library. (:issue:`4090`, :issue:`4092`)

IO enhancements
~~~~~~~~~~~~~~~

  - ``pd.read_html()`` can now parse HTML strings, files or urls and return
    DataFrames, courtesy of @cpcloud. (:issue:`3477`, :issue:`3605`, :issue:`3606`, :issue:`3616`).
    It works with a *single* parser backend: BeautifulSoup4 + html5lib :ref:`See the docs<io.html>`

    You can use ``pd.read_html()`` to read the output from ``DataFrame.to_html()`` like so

    .. ipython:: python
       :okwarning:

        df = pd.DataFrame({"a": range(3), "b": list("abc")})
        print(df)
        html = df.to_html()
        alist = pd.read_html(html, index_col=0)
        print(df == alist[0])

    Note that ``alist`` here is a Python ``list`` so ``pd.read_html()`` and
    ``DataFrame.to_html()`` are not inverses.

    - ``pd.read_html()`` no longer performs hard conversion of date strings
      (:issue:`3656`).

    .. warning::

      You may have to install an older version of BeautifulSoup4,
      :ref:`See the installation docs<install.optional_dependencies>`

  - Added module for reading and writing Stata files: ``pandas.io.stata`` (:issue:`1512`)
    accessible via ``read_stata`` top-level function for reading,
    and ``to_stata`` DataFrame method for writing, :ref:`See the docs<io.stata>`

  - Added module for reading and writing json format files: ``pandas.io.json``
    accessible via ``read_json`` top-level function for reading,
    and ``to_json`` DataFrame method for writing, :ref:`See the docs<io.json>`
    various issues (:issue:`1226`, :issue:`3804`, :issue:`3876`, :issue:`3867`, :issue:`1305`)

  - ``MultiIndex`` column support for reading and writing csv format files

    - The ``header`` option in ``read_csv`` now accepts a
      list of the rows from which to read the index.

    - The option, ``tupleize_cols`` can now be specified in both ``to_csv`` and
      ``read_csv``, to provide compatibility for the pre 0.12 behavior of
      writing and reading ``MultIndex`` columns via a list of tuples. The default in
      0.12 is to write lists of tuples and *not* interpret list of tuples as a
      ``MultiIndex`` column.

      Note: The default behavior in 0.12 remains unchanged from prior versions, but starting with 0.13,
      the default *to* write and read ``MultiIndex`` columns will be in the new
      format. (:issue:`3571`, :issue:`1651`, :issue:`3141`)

    - If an ``index_col`` is not specified (e.g. you don't have an index, or wrote it
      with ``df.to_csv(..., index=False``), then any ``names`` on the columns index will
      be *lost*.

      .. ipython:: python

         from pandas._testing import makeCustomDataframe as mkdf

         df = mkdf(5, 3, r_idx_nlevels=2, c_idx_nlevels=4)
         df.to_csv("mi.csv")
         print(open("mi.csv").read())
         pd.read_csv("mi.csv", header=[0, 1, 2, 3], index_col=[0, 1])

      .. ipython:: python
         :suppress:

         import os

         os.remove("mi.csv")

  - Support for ``HDFStore`` (via ``PyTables 3.0.0``) on Python3

  - Iterator support via ``read_hdf`` that automatically opens and closes the
    store when iteration is finished. This is only for *tables*

    .. code-block:: ipython

        In [25]: path = 'store_iterator.h5'

        In [26]: pd.DataFrame(np.random.randn(10, 2)).to_hdf(path, 'df', table=True)

        In [27]: for df in pd.read_hdf(path, 'df', chunksize=3):
           ....:     print(df)
           ....:
                  0         1
        0  0.713216 -0.778461
        1 -0.661062  0.862877
        2  0.344342  0.149565
                  0         1
        3 -0.626968 -0.875772
        4 -0.930687 -0.218983
        5  0.949965 -0.442354
                  0         1
        6 -0.402985  1.111358
        7 -0.241527 -0.670477
        8  0.049355  0.632633
                  0         1
        9 -1.502767 -1.225492



  - ``read_csv`` will now throw a more informative error message when a file
    contains no columns, e.g., all newline characters

Other enhancements
~~~~~~~~~~~~~~~~~~

  - ``DataFrame.replace()`` now allows regular expressions on contained
    ``Series`` with object dtype. See the examples section in the regular docs
    :ref:`Replacing via String Expression <missing_data.replace_expression>`

    For example you can do

    .. ipython:: python

        df = pd.DataFrame({"a": list("ab.."), "b": [1, 2, 3, 4]})
        df.replace(regex=r"\s*\.\s*", value=np.nan)

    to replace all occurrences of the string ``'.'`` with zero or more
    instances of surrounding white space with ``NaN``.

    Regular string replacement still works as expected. For example, you can do

    .. ipython:: python

        df.replace(".", np.nan)

    to replace all occurrences of the string ``'.'`` with ``NaN``.

  - ``pd.melt()`` now accepts the optional parameters ``var_name`` and ``value_name``
    to specify custom column names of the returned DataFrame.

  - ``pd.set_option()`` now allows N option, value pairs (:issue:`3667`).

    Let's say that we had an option ``'a.b'`` and another option ``'b.c'``.
    We can set them at the same time:

    .. code-block:: ipython

        In [31]: pd.get_option('a.b')
        Out[31]: 2

        In [32]: pd.get_option('b.c')
        Out[32]: 3

        In [33]: pd.set_option('a.b', 1, 'b.c', 4)

        In [34]: pd.get_option('a.b')
        Out[34]: 1

        In [35]: pd.get_option('b.c')
        Out[35]: 4

  - The ``filter`` method for group objects returns a subset of the original
    object. Suppose we want to take only elements that belong to groups with a
    group sum greater than 2.

    .. ipython:: python

       sf = pd.Series([1, 1, 2, 3, 3, 3])
       sf.groupby(sf).filter(lambda x: x.sum() > 2)

    The argument of ``filter`` must a function that, applied to the group as a
    whole, returns ``True`` or ``False``.

    Another useful operation is filtering out elements that belong to groups
    with only a couple members.

    .. ipython:: python

       dff = pd.DataFrame({"A": np.arange(8), "B": list("aabbbbcc")})
       dff.groupby("B").filter(lambda x: len(x) > 2)

    Alternatively, instead of dropping the offending groups, we can return a
    like-indexed objects where the groups that do not pass the filter are
    filled with NaNs.

    .. ipython:: python

       dff.groupby("B").filter(lambda x: len(x) > 2, dropna=False)

  - Series and DataFrame hist methods now take a ``figsize`` argument (:issue:`3834`)

  - DatetimeIndexes no longer try to convert mixed-integer indexes during join
    operations (:issue:`3877`)

  - Timestamp.min and Timestamp.max now represent valid Timestamp instances instead
    of the default datetime.min and datetime.max (respectively), thanks @SleepingPills

  - ``read_html`` now raises when no tables are found and BeautifulSoup==4.2.0
    is detected (:issue:`4214`)


Experimental features
~~~~~~~~~~~~~~~~~~~~~

  - Added experimental ``CustomBusinessDay`` class to support ``DateOffsets``
    with custom holiday calendars and custom weekmasks. (:issue:`2301`)

    .. note::

       This uses the ``numpy.busdaycalendar`` API introduced in Numpy 1.7 and
       therefore requires Numpy 1.7.0 or newer.

    .. ipython:: python

      from pandas.tseries.offsets import CustomBusinessDay
      from datetime import datetime

      # As an interesting example, let's look at Egypt where
      # a Friday-Saturday weekend is observed.
      weekmask_egypt = "Sun Mon Tue Wed Thu"
      # They also observe International Workers' Day so let's
      # add that for a couple of years
      holidays = ["2012-05-01", datetime(2013, 5, 1), np.datetime64("2014-05-01")]
      bday_egypt = CustomBusinessDay(holidays=holidays, weekmask=weekmask_egypt)
      dt = datetime(2013, 4, 30)
      print(dt + 2 * bday_egypt)
      dts = pd.date_range(dt, periods=5, freq=bday_egypt)
      print(pd.Series(dts.weekday, dts).map(pd.Series("Mon Tue Wed Thu Fri Sat Sun".split())))

Bug fixes
~~~~~~~~~

  - Plotting functions now raise a ``TypeError`` before trying to plot anything
    if the associated objects have a dtype of ``object`` (:issue:`1818`,
    :issue:`3572`, :issue:`3911`, :issue:`3912`), but they will try to convert object arrays to
    numeric arrays if possible so that you can still plot, for example, an
    object array with floats. This happens before any drawing takes place which
    eliminates any spurious plots from showing up.

  - ``fillna`` methods now raise a ``TypeError`` if the ``value`` parameter is
    a list or tuple.

  - ``Series.str`` now supports iteration (:issue:`3638`). You can iterate over the
    individual elements of each string in the ``Series``. Each iteration yields
    a ``Series`` with either a single character at each index of the original
    ``Series`` or ``NaN``. For example,

    .. ipython:: python
        :okwarning:

        strs = "go", "bow", "joe", "slow"
        ds = pd.Series(strs)

        for s in ds.str:
            print(s)

        s
        s.dropna().values.item() == "w"

    The last element yielded by the iterator will be a ``Series`` containing
    the last element of the longest string in the ``Series`` with all other
    elements being ``NaN``. Here since ``'slow'`` is the longest string
    and there are no other strings with the same length ``'w'`` is the only
    non-null string in the yielded ``Series``.

  - ``HDFStore``

    - will retain index attributes (freq,tz,name) on recreation (:issue:`3499`)
    - will warn with a ``AttributeConflictWarning`` if you are attempting to append
      an index with a different frequency than the existing, or attempting
      to append an index with a different name than the existing
    - support datelike columns with a timezone as data_columns (:issue:`2852`)

  - Non-unique index support clarified (:issue:`3468`).

    - Fix assigning a new index to a duplicate index in a DataFrame would fail (:issue:`3468`)
    - Fix construction of a DataFrame with a duplicate index
    - ref_locs support to allow duplicative indices across dtypes,
      allows iget support to always find the index (even across dtypes) (:issue:`2194`)
    - applymap on a DataFrame with a non-unique index now works
      (removed warning) (:issue:`2786`), and fix (:issue:`3230`)
    - Fix to_csv to handle non-unique columns (:issue:`3495`)
    - Duplicate indexes with getitem will return items in the correct order (:issue:`3455`, :issue:`3457`)
      and handle missing elements like unique indices (:issue:`3561`)
    - Duplicate indexes with and empty DataFrame.from_records will return a correct frame (:issue:`3562`)
    - Concat to produce a non-unique columns when duplicates are across dtypes is fixed (:issue:`3602`)
    - Allow insert/delete to non-unique columns (:issue:`3679`)
    - Non-unique indexing with a slice via ``loc`` and friends fixed (:issue:`3659`)
    - Allow insert/delete to non-unique columns (:issue:`3679`)
    - Extend ``reindex`` to correctly deal with non-unique indices (:issue:`3679`)
    - ``DataFrame.itertuples()`` now works with frames with duplicate column
      names (:issue:`3873`)
    - Bug in non-unique indexing via ``iloc`` (:issue:`4017`); added ``takeable`` argument to
      ``reindex`` for location-based taking
    - Allow non-unique indexing in series via ``.ix/.loc`` and ``__getitem__`` (:issue:`4246`)
    - Fixed non-unique indexing memory allocation issue with ``.ix/.loc`` (:issue:`4280`)

  - ``DataFrame.from_records`` did not accept empty recarrays (:issue:`3682`)
  - ``read_html`` now correctly skips tests (:issue:`3741`)
  - Fixed a bug where ``DataFrame.replace`` with a compiled regular expression
    in the ``to_replace`` argument wasn't working (:issue:`3907`)
  - Improved ``network`` test decorator to catch ``IOError`` (and therefore
    ``URLError`` as well). Added ``with_connectivity_check`` decorator to allow
    explicitly checking a website as a proxy for seeing if there is network
    connectivity. Plus, new ``optional_args`` decorator factory for decorators.
    (:issue:`3910`, :issue:`3914`)
  - Fixed testing issue where too many sockets where open thus leading to a
    connection reset issue (:issue:`3982`, :issue:`3985`, :issue:`4028`,
    :issue:`4054`)
  - Fixed failing tests in test_yahoo, test_google where symbols were not
    retrieved but were being accessed (:issue:`3982`, :issue:`3985`,
    :issue:`4028`, :issue:`4054`)
  - ``Series.hist`` will now take the figure from the current environment if
    one is not passed
  - Fixed bug where a 1xN DataFrame would barf on a 1xN mask (:issue:`4071`)
  - Fixed running of ``tox`` under python3 where the pickle import was getting
    rewritten in an incompatible way (:issue:`4062`, :issue:`4063`)
  - Fixed bug where sharex and sharey were not being passed to grouped_hist
    (:issue:`4089`)
  - Fixed bug in ``DataFrame.replace`` where a nested dict wasn't being
    iterated over when regex=False (:issue:`4115`)
  - Fixed bug in the parsing of microseconds when using the ``format``
    argument in ``to_datetime`` (:issue:`4152`)
  - Fixed bug in ``PandasAutoDateLocator`` where ``invert_xaxis`` triggered
    incorrectly ``MilliSecondLocator``  (:issue:`3990`)
  - Fixed bug in plotting that wasn't raising on invalid colormap for
    matplotlib 1.1.1 (:issue:`4215`)
  - Fixed the legend displaying in ``DataFrame.plot(kind='kde')`` (:issue:`4216`)
  - Fixed bug where Index slices weren't carrying the name attribute
    (:issue:`4226`)
  - Fixed bug in initializing ``DatetimeIndex`` with an array of strings
    in a certain time zone (:issue:`4229`)
  - Fixed bug where html5lib wasn't being properly skipped (:issue:`4265`)
  - Fixed bug where get_data_famafrench wasn't using the correct file edges
    (:issue:`4281`)

See the :ref:`full release notes
<release>` or issue tracker
on GitHub for a complete list.


.. _whatsnew_0.12.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.11.0..v0.12.0
.. _whatsnew_0131:

Version 0.13.1 (February 3, 2014)
---------------------------------

{{ header }}



This is a minor release from 0.13.0 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

Highlights include:

- Added ``infer_datetime_format`` keyword to ``read_csv/to_datetime`` to allow speedups for homogeneously formatted datetimes.
- Will intelligently limit display precision for datetime/timedelta formats.
- Enhanced Panel :meth:`~pandas.Panel.apply` method.
- Suggested tutorials in new :ref:`Tutorials<tutorials>` section.
- Our pandas ecosystem is growing, We now feature related projects in a new :ref:`Pandas Ecosystem<ecosystem>` section.
- Much work has been taking place on improving the docs, and a new :ref:`Contributing<contributing>` section has been added.
- Even though it may only be of interest to devs, we <3 our new CI status page: `ScatterCI <http://scatterci.github.io/pydata/pandas>`__.

.. warning::

   0.13.1 fixes a bug that was caused by a combination of having numpy < 1.8, and doing
   chained assignment on a string-like array. Please review :ref:`the docs<indexing.view_versus_copy>`,
   chained indexing can have unexpected results and should generally be avoided.

   This would previously segfault:

   .. ipython:: python

      df = pd.DataFrame({"A": np.array(["foo", "bar", "bah", "foo", "bar"])})
      df["A"].iloc[0] = np.nan
      df

   The recommended way to do this type of assignment is:

   .. ipython:: python

      df = pd.DataFrame({"A": np.array(["foo", "bar", "bah", "foo", "bar"])})
      df.loc[0, "A"] = np.nan
      df

Output formatting enhancements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- df.info() view now display dtype info per column (:issue:`5682`)

- df.info() now honors the option ``max_info_rows``, to disable null counts for large frames (:issue:`5974`)

  .. ipython:: python

     max_info_rows = pd.get_option("max_info_rows")

     df = pd.DataFrame(
         {
             "A": np.random.randn(10),
             "B": np.random.randn(10),
             "C": pd.date_range("20130101", periods=10),
         }
     )
     df.iloc[3:6, [0, 2]] = np.nan

  .. ipython:: python

     # set to not display the null counts
     pd.set_option("max_info_rows", 0)
     df.info()

  .. ipython:: python

     # this is the default (same as in 0.13.0)
     pd.set_option("max_info_rows", max_info_rows)
     df.info()

- Add ``show_dimensions`` display option for the new DataFrame repr to control whether the dimensions print.

  .. ipython:: python

      df = pd.DataFrame([[1, 2], [3, 4]])
      pd.set_option("show_dimensions", False)
      df

      pd.set_option("show_dimensions", True)
      df

- The ``ArrayFormatter`` for ``datetime`` and ``timedelta64`` now intelligently
  limit precision based on the values in the array (:issue:`3401`)

  Previously output might look like:

  ..   code-block:: text

        age                 today               diff
      0 2001-01-01 00:00:00 2013-04-19 00:00:00 4491 days, 00:00:00
      1 2004-06-01 00:00:00 2013-04-19 00:00:00 3244 days, 00:00:00

  Now the output looks like:

  .. ipython:: python

     df = pd.DataFrame(
         [pd.Timestamp("20010101"), pd.Timestamp("20040601")], columns=["age"]
     )
     df["today"] = pd.Timestamp("20130419")
     df["diff"] = df["today"] - df["age"]
     df

API changes
~~~~~~~~~~~

- Add ``-NaN`` and ``-nan`` to the default set of NA values (:issue:`5952`).
  See :ref:`NA Values <io.na_values>`.

- Added ``Series.str.get_dummies`` vectorized string method (:issue:`6021`), to extract
  dummy/indicator variables for separated string columns:

  .. ipython:: python

      s = pd.Series(["a", "a|b", np.nan, "a|c"])
      s.str.get_dummies(sep="|")

- Added the ``NDFrame.equals()`` method to compare if two NDFrames are
  equal have equal axes, dtypes, and values. Added the
  ``array_equivalent`` function to compare if two ndarrays are
  equal. NaNs in identical locations are treated as
  equal. (:issue:`5283`) See also :ref:`the docs<basics.equals>` for a motivating example.

  .. code-block:: python

      df = pd.DataFrame({"col": ["foo", 0, np.nan]})
      df2 = pd.DataFrame({"col": [np.nan, 0, "foo"]}, index=[2, 1, 0])
      df.equals(df2)
      df.equals(df2.sort_index())

- ``DataFrame.apply`` will use the ``reduce`` argument to determine whether a
  ``Series`` or a ``DataFrame`` should be returned when the ``DataFrame`` is
  empty (:issue:`6007`).

  Previously, calling ``DataFrame.apply`` an empty ``DataFrame`` would return
  either a ``DataFrame`` if there were no columns, or the function being
  applied would be called with an empty ``Series`` to guess whether a
  ``Series`` or ``DataFrame`` should be returned:

  .. code-block:: ipython

    In [32]: def applied_func(col):
      ....:    print("Apply function being called with: ", col)
      ....:    return col.sum()
      ....:

    In [33]: empty = DataFrame(columns=['a', 'b'])

    In [34]: empty.apply(applied_func)
    Apply function being called with:  Series([], Length: 0, dtype: float64)
    Out[34]:
    a   NaN
    b   NaN
    Length: 2, dtype: float64

  Now, when ``apply`` is called on an empty ``DataFrame``: if the ``reduce``
  argument is ``True`` a ``Series`` will returned, if it is ``False`` a
  ``DataFrame`` will be returned, and if it is ``None`` (the default) the
  function being applied will be called with an empty series to try and guess
  the return type.

  .. code-block:: ipython

    In [35]: empty.apply(applied_func, reduce=True)
    Out[35]:
    a   NaN
    b   NaN
    Length: 2, dtype: float64

    In [36]: empty.apply(applied_func, reduce=False)
    Out[36]:
    Empty DataFrame
    Columns: [a, b]
    Index: []

    [0 rows x 2 columns]


Prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are no announced changes in 0.13 or prior that are taking effect as of 0.13.1

Deprecations
~~~~~~~~~~~~

There are no deprecations of prior behavior in 0.13.1

Enhancements
~~~~~~~~~~~~

- ``pd.read_csv`` and ``pd.to_datetime`` learned a new ``infer_datetime_format`` keyword which greatly
  improves parsing perf in many cases. Thanks to @lexual for suggesting and @danbirken
  for rapidly implementing. (:issue:`5490`, :issue:`6021`)

  If ``parse_dates`` is enabled and this flag is set, pandas will attempt to
  infer the format of the datetime strings in the columns, and if it can
  be inferred, switch to a faster method of parsing them.  In some cases
  this can increase the parsing speed by ~5-10x.

  .. code-block:: python

      # Try to infer the format for the index column
      df = pd.read_csv(
          "foo.csv", index_col=0, parse_dates=True, infer_datetime_format=True
      )

- ``date_format`` and ``datetime_format`` keywords can now be specified when writing to ``excel``
  files (:issue:`4133`)

- ``MultiIndex.from_product`` convenience function for creating a MultiIndex from
  the cartesian product of a set of iterables (:issue:`6055`):

  .. ipython:: python

      shades = ["light", "dark"]
      colors = ["red", "green", "blue"]

      pd.MultiIndex.from_product([shades, colors], names=["shade", "color"])

- Panel :meth:`~pandas.Panel.apply` will work on non-ufuncs. See :ref:`the docs<basics.apply>`.

  .. code-block:: ipython

      In [28]: import pandas._testing as tm

      In [29]: panel = tm.makePanel(5)

      In [30]: panel
      Out[30]:
      <class 'pandas.core.panel.Panel'>
      Dimensions: 3 (items) x 5 (major_axis) x 4 (minor_axis)
      Items axis: ItemA to ItemC
      Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
      Minor_axis axis: A to D

      In [31]: panel['ItemA']
      Out[31]:
                         A         B         C         D
      2000-01-03 -0.673690  0.577046 -1.344312 -1.469388
      2000-01-04  0.113648 -1.715002  0.844885  0.357021
      2000-01-05 -1.478427 -1.039268  1.075770 -0.674600
      2000-01-06  0.524988 -0.370647 -0.109050 -1.776904
      2000-01-07  0.404705 -1.157892  1.643563 -0.968914

      [5 rows x 4 columns]

  Specifying an ``apply`` that operates on a Series (to return a single element)

  .. code-block:: ipython

      In [32]: panel.apply(lambda x: x.dtype, axis='items')
      Out[32]:
                        A        B        C        D
      2000-01-03  float64  float64  float64  float64
      2000-01-04  float64  float64  float64  float64
      2000-01-05  float64  float64  float64  float64
      2000-01-06  float64  float64  float64  float64
      2000-01-07  float64  float64  float64  float64

      [5 rows x 4 columns]

  A similar reduction type operation

  .. code-block:: ipython

      In [33]: panel.apply(lambda x: x.sum(), axis='major_axis')
      Out[33]:
            ItemA     ItemB     ItemC
      A -1.108775 -1.090118 -2.984435
      B -3.705764  0.409204  1.866240
      C  2.110856  2.960500 -0.974967
      D -4.532785  0.303202 -3.685193

      [4 rows x 3 columns]

  This is equivalent to

  .. code-block:: ipython

      In [34]: panel.sum('major_axis')
      Out[34]:
            ItemA     ItemB     ItemC
      A -1.108775 -1.090118 -2.984435
      B -3.705764  0.409204  1.866240
      C  2.110856  2.960500 -0.974967
      D -4.532785  0.303202 -3.685193

      [4 rows x 3 columns]

  A transformation operation that returns a Panel, but is computing
  the z-score across the major_axis

  .. code-block:: ipython

      In [35]: result = panel.apply(lambda x: (x - x.mean()) / x.std(),
        ....:                      axis='major_axis')
        ....:

      In [36]: result
      Out[36]:
      <class 'pandas.core.panel.Panel'>
      Dimensions: 3 (items) x 5 (major_axis) x 4 (minor_axis)
      Items axis: ItemA to ItemC
      Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
      Minor_axis axis: A to D

      In [37]: result['ItemA']                           # noqa E999
      Out[37]:
                        A         B         C         D
      2000-01-03 -0.535778  1.500802 -1.506416 -0.681456
      2000-01-04  0.397628 -1.108752  0.360481  1.529895
      2000-01-05 -1.489811 -0.339412  0.557374  0.280845
      2000-01-06  0.885279  0.421830 -0.453013 -1.053785
      2000-01-07  0.742682 -0.474468  1.041575 -0.075499

      [5 rows x 4 columns]

- Panel :meth:`~pandas.Panel.apply` operating on cross-sectional slabs. (:issue:`1148`)

  .. code-block:: ipython

      In [38]: def f(x):
         ....:     return ((x.T - x.mean(1)) / x.std(1)).T
         ....:

      In [39]: result = panel.apply(f, axis=['items', 'major_axis'])

      In [40]: result
      Out[40]:
      <class 'pandas.core.panel.Panel'>
      Dimensions: 4 (items) x 5 (major_axis) x 3 (minor_axis)
      Items axis: A to D
      Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
      Minor_axis axis: ItemA to ItemC

      In [41]: result.loc[:, :, 'ItemA']
      Out[41]:
                         A         B         C         D
      2000-01-03  0.012922 -0.030874 -0.629546 -0.757034
      2000-01-04  0.392053 -1.071665  0.163228  0.548188
      2000-01-05 -1.093650 -0.640898  0.385734 -1.154310
      2000-01-06  1.005446 -1.154593 -0.595615 -0.809185
      2000-01-07  0.783051 -0.198053  0.919339 -1.052721

      [5 rows x 4 columns]

  This is equivalent to the following

  .. code-block:: ipython

      In [42]: result = pd.Panel({ax: f(panel.loc[:, :, ax]) for ax in panel.minor_axis})

      In [43]: result
      Out[43]:
      <class 'pandas.core.panel.Panel'>
      Dimensions: 4 (items) x 5 (major_axis) x 3 (minor_axis)
      Items axis: A to D
      Major_axis axis: 2000-01-03 00:00:00 to 2000-01-07 00:00:00
      Minor_axis axis: ItemA to ItemC

      In [44]: result.loc[:, :, 'ItemA']
      Out[44]:
                         A         B         C         D
      2000-01-03  0.012922 -0.030874 -0.629546 -0.757034
      2000-01-04  0.392053 -1.071665  0.163228  0.548188
      2000-01-05 -1.093650 -0.640898  0.385734 -1.154310
      2000-01-06  1.005446 -1.154593 -0.595615 -0.809185
      2000-01-07  0.783051 -0.198053  0.919339 -1.052721

      [5 rows x 4 columns]

Performance
~~~~~~~~~~~

Performance improvements for 0.13.1

- Series datetime/timedelta binary operations (:issue:`5801`)
- DataFrame ``count/dropna`` for ``axis=1``
- Series.str.contains now has a ``regex=False`` keyword which can be faster for plain (non-regex) string patterns. (:issue:`5879`)
- Series.str.extract (:issue:`5944`)
- ``dtypes/ftypes`` methods (:issue:`5968`)
- indexing with object dtypes (:issue:`5968`)
- ``DataFrame.apply`` (:issue:`6013`)
- Regression in JSON IO (:issue:`5765`)
- Index construction from Series (:issue:`6150`)

Experimental
~~~~~~~~~~~~

There are no experimental changes in 0.13.1

.. _release.bug_fixes-0.13.1:

Bug fixes
~~~~~~~~~

- Bug in ``io.wb.get_countries`` not including all countries (:issue:`6008`)
- Bug in Series replace with timestamp dict (:issue:`5797`)
- read_csv/read_table now respects the ``prefix`` kwarg (:issue:`5732`).
- Bug in selection with missing values via ``.ix`` from a duplicate indexed DataFrame failing (:issue:`5835`)
- Fix issue of boolean comparison on empty DataFrames (:issue:`5808`)
- Bug in isnull handling ``NaT`` in an object array (:issue:`5443`)
- Bug in ``to_datetime`` when passed a ``np.nan`` or integer datelike and a format string (:issue:`5863`)
- Bug in groupby dtype conversion with datetimelike (:issue:`5869`)
- Regression in handling of empty Series as indexers to Series  (:issue:`5877`)
- Bug in internal caching, related to (:issue:`5727`)
- Testing bug in reading JSON/msgpack from a non-filepath on windows under py3 (:issue:`5874`)
- Bug when assigning to .ix[tuple(...)] (:issue:`5896`)
- Bug in fully reindexing a Panel (:issue:`5905`)
- Bug in idxmin/max with object dtypes (:issue:`5914`)
- Bug in ``BusinessDay`` when adding n days to a date not on offset when n>5 and n%5==0 (:issue:`5890`)
- Bug in assigning to chained series with a series via ix (:issue:`5928`)
- Bug in creating an empty DataFrame, copying, then assigning (:issue:`5932`)
- Bug in DataFrame.tail with empty frame (:issue:`5846`)
- Bug in propagating metadata on ``resample`` (:issue:`5862`)
- Fixed string-representation of ``NaT`` to be "NaT" (:issue:`5708`)
- Fixed string-representation for Timestamp to show nanoseconds if present (:issue:`5912`)
- ``pd.match`` not returning passed sentinel
- ``Panel.to_frame()`` no longer fails when ``major_axis`` is a
  ``MultiIndex`` (:issue:`5402`).
- Bug in ``pd.read_msgpack`` with inferring a ``DateTimeIndex`` frequency
  incorrectly (:issue:`5947`)
- Fixed ``to_datetime`` for array with both Tz-aware datetimes and ``NaT``'s  (:issue:`5961`)
- Bug in rolling skew/kurtosis when passed a Series with bad data (:issue:`5749`)
- Bug in scipy ``interpolate`` methods with a datetime index (:issue:`5975`)
- Bug in NaT comparison if a mixed datetime/np.datetime64 with NaT were passed (:issue:`5968`)
- Fixed bug with ``pd.concat`` losing dtype information if all inputs are empty (:issue:`5742`)
- Recent changes in IPython cause warnings to be emitted when using previous versions
  of pandas in QTConsole, now fixed. If you're using an older version and
  need to suppress the warnings, see (:issue:`5922`).
- Bug in merging ``timedelta`` dtypes (:issue:`5695`)
- Bug in plotting.scatter_matrix function. Wrong alignment among diagonal
  and off-diagonal plots, see (:issue:`5497`).
- Regression in Series with a MultiIndex via ix (:issue:`6018`)
- Bug in Series.xs with a MultiIndex (:issue:`6018`)
- Bug in Series construction of mixed type with datelike and an integer (which should result in
  object type and not automatic conversion) (:issue:`6028`)
- Possible segfault when chained indexing with an object array under NumPy 1.7.1 (:issue:`6026`, :issue:`6056`)
- Bug in setting using fancy indexing a single element with a non-scalar (e.g. a list),
  (:issue:`6043`)
- ``to_sql`` did not respect ``if_exists`` (:issue:`4110` :issue:`4304`)
- Regression in ``.get(None)`` indexing from 0.12 (:issue:`5652`)
- Subtle ``iloc`` indexing bug, surfaced in (:issue:`6059`)
- Bug with insert of strings into DatetimeIndex (:issue:`5818`)
- Fixed unicode bug in to_html/HTML repr (:issue:`6098`)
- Fixed missing arg validation in get_options_data (:issue:`6105`)
- Bug in assignment with duplicate columns in a frame where the locations
  are a slice (e.g. next to each other) (:issue:`6120`)
- Bug in propagating _ref_locs during construction of a DataFrame with dups
  index/columns (:issue:`6121`)
- Bug in ``DataFrame.apply`` when using mixed datelike reductions (:issue:`6125`)
- Bug in ``DataFrame.append`` when appending a row with different columns (:issue:`6129`)
- Bug in DataFrame construction with recarray and non-ns datetime dtype (:issue:`6140`)
- Bug in ``.loc`` setitem indexing with a dataframe on rhs, multiple item setting, and
  a datetimelike (:issue:`6152`)
- Fixed a bug in ``query``/``eval`` during lexicographic string comparisons (:issue:`6155`).
- Fixed a bug in ``query`` where the index of a single-element ``Series`` was
  being thrown away (:issue:`6148`).
- Bug in ``HDFStore`` on appending a dataframe with MultiIndexed columns to
  an existing table (:issue:`6167`)
- Consistency with dtypes in setting an empty DataFrame (:issue:`6171`)
- Bug in selecting on a MultiIndex ``HDFStore`` even in the presence of under
  specified column spec (:issue:`6169`)
- Bug in ``nanops.var`` with ``ddof=1`` and 1 elements would sometimes return ``inf``
  rather than ``nan`` on some platforms (:issue:`6136`)
- Bug in Series and DataFrame bar plots ignoring the ``use_index`` keyword (:issue:`6209`)
- Bug in groupby with mixed str/int under python3 fixed; ``argsort`` was failing (:issue:`6212`)

.. _whatsnew_0.13.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.13.0..v0.13.1
.. _whatsnew_0190:

Version 0.19.0 (October 2, 2016)
--------------------------------

{{ header }}

This is a major release from 0.18.1 and includes number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

Highlights include:

- :func:`merge_asof` for asof-style time-series joining, see :ref:`here <whatsnew_0190.enhancements.asof_merge>`
- ``.rolling()`` is now time-series aware, see :ref:`here <whatsnew_0190.enhancements.rolling_ts>`
- :func:`read_csv` now supports parsing ``Categorical`` data, see :ref:`here <whatsnew_0190.enhancements.read_csv_categorical>`
- A function :func:`union_categorical` has been added for combining categoricals, see :ref:`here <whatsnew_0190.enhancements.union_categoricals>`
- ``PeriodIndex`` now has its own ``period`` dtype, and changed to be more consistent with other ``Index`` classes. See :ref:`here <whatsnew_0190.api.period>`
- Sparse data structures gained enhanced support of ``int`` and ``bool`` dtypes, see :ref:`here <whatsnew_0190.sparse>`
- Comparison operations with ``Series`` no longer ignores the index, see :ref:`here <whatsnew_0190.api.series_ops>` for an overview of the API changes.
- Introduction of a pandas development API for utility functions, see :ref:`here <whatsnew_0190.dev_api>`.
- Deprecation of ``Panel4D`` and ``PanelND``. We recommend to represent these types of n-dimensional data with the `xarray package <http://xarray.pydata.org/en/stable/>`__.
- Removal of the previously deprecated modules ``pandas.io.data``, ``pandas.io.wb``, ``pandas.tools.rplot``.

.. warning::

    pandas >= 0.19.0 will no longer silence numpy ufunc warnings upon import, see :ref:`here <whatsnew_0190.errstate>`.

.. contents:: What's new in v0.19.0
    :local:
    :backlinks: none

.. _whatsnew_0190.new_features:

New features
~~~~~~~~~~~~

.. _whatsnew_0190.enhancements.asof_merge:

Function ``merge_asof`` for asof-style time-series joining
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A long-time requested feature has been added through the :func:`merge_asof` function, to
support asof style joining of time-series (:issue:`1870`, :issue:`13695`, :issue:`13709`, :issue:`13902`). Full documentation is
:ref:`here <merging.merge_asof>`.

The :func:`merge_asof` performs an asof merge, which is similar to a left-join
except that we match on nearest key rather than equal keys.

.. ipython:: python

   left = pd.DataFrame({"a": [1, 5, 10], "left_val": ["a", "b", "c"]})
   right = pd.DataFrame({"a": [1, 2, 3, 6, 7], "right_val": [1, 2, 3, 6, 7]})

   left
   right

We typically want to match exactly when possible, and use the most
recent value otherwise.

.. ipython:: python

   pd.merge_asof(left, right, on="a")

We can also match rows ONLY with prior data, and not an exact match.

.. ipython:: python

   pd.merge_asof(left, right, on="a", allow_exact_matches=False)


In a typical time-series example, we have ``trades`` and ``quotes`` and we want to ``asof-join`` them.
This also illustrates using the ``by`` parameter to group data before merging.

.. ipython:: python

   trades = pd.DataFrame(
       {
           "time": pd.to_datetime(
               [
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.038",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.048",
               ]
           ),
           "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],
           "price": [51.95, 51.95, 720.77, 720.92, 98.00],
           "quantity": [75, 155, 100, 100, 100],
       },
       columns=["time", "ticker", "price", "quantity"],
   )

   quotes = pd.DataFrame(
       {
           "time": pd.to_datetime(
               [
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.030",
                   "20160525 13:30:00.041",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.049",
                   "20160525 13:30:00.072",
                   "20160525 13:30:00.075",
               ]
           ),
           "ticker": ["GOOG", "MSFT", "MSFT", "MSFT", "GOOG", "AAPL", "GOOG", "MSFT"],
           "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
           "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],
       },
       columns=["time", "ticker", "bid", "ask"],
   )

.. ipython:: python

   trades
   quotes

An asof merge joins on the ``on``, typically a datetimelike field, which is ordered, and
in this case we are using a grouper in the ``by`` field. This is like a left-outer join, except
that forward filling happens automatically taking the most recent non-NaN value.

.. ipython:: python

   pd.merge_asof(trades, quotes, on="time", by="ticker")

This returns a merged DataFrame with the entries in the same order as the original left
passed DataFrame (``trades`` in this case), with the fields of the ``quotes`` merged.

.. _whatsnew_0190.enhancements.rolling_ts:

Method ``.rolling()`` is now time-series aware
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``.rolling()`` objects are now time-series aware and can accept a time-series offset (or convertible) for the ``window`` argument (:issue:`13327`, :issue:`12995`).
See the full documentation :ref:`here <window.generic>`.

.. ipython:: python

   dft = pd.DataFrame(
       {"B": [0, 1, 2, np.nan, 4]},
       index=pd.date_range("20130101 09:00:00", periods=5, freq="s"),
   )
   dft

This is a regular frequency index. Using an integer window parameter works to roll along the window frequency.

.. ipython:: python

   dft.rolling(2).sum()
   dft.rolling(2, min_periods=1).sum()

Specifying an offset allows a more intuitive specification of the rolling frequency.

.. ipython:: python

   dft.rolling("2s").sum()

Using a non-regular, but still monotonic index, rolling with an integer window does not impart any special calculation.

.. ipython:: python


   dft = pd.DataFrame(
       {"B": [0, 1, 2, np.nan, 4]},
       index=pd.Index(
           [
               pd.Timestamp("20130101 09:00:00"),
               pd.Timestamp("20130101 09:00:02"),
               pd.Timestamp("20130101 09:00:03"),
               pd.Timestamp("20130101 09:00:05"),
               pd.Timestamp("20130101 09:00:06"),
           ],
           name="foo",
       ),
   )

   dft
   dft.rolling(2).sum()

Using the time-specification generates variable windows for this sparse data.

.. ipython:: python

   dft.rolling("2s").sum()

Furthermore, we now allow an optional ``on`` parameter to specify a column (rather than the
default of the index) in a DataFrame.

.. ipython:: python

   dft = dft.reset_index()
   dft
   dft.rolling("2s", on="foo").sum()

.. _whatsnew_0190.enhancements.read_csv_dupe_col_names_support:

Method ``read_csv`` has improved support for duplicate column names
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. ipython:: python
   :suppress:

   from io import StringIO

:ref:`Duplicate column names <io.dupe_names>` are now supported in :func:`read_csv` whether
they are in the file or passed in as the ``names`` parameter (:issue:`7160`, :issue:`9424`)

.. ipython:: python

   data = "0,1,2\n3,4,5"
   names = ["a", "b", "a"]

**Previous behavior**:

.. code-block:: ipython

   In [2]: pd.read_csv(StringIO(data), names=names)
   Out[2]:
      a  b  a
   0  2  1  2
   1  5  4  5

The first ``a`` column contained the same data as the second ``a`` column, when it should have
contained the values ``[0, 3]``.

**New behavior**:

.. ipython:: python
   :okexcept:

   pd.read_csv(StringIO(data), names=names)


.. _whatsnew_0190.enhancements.read_csv_categorical:

Method ``read_csv`` supports parsing ``Categorical`` directly
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :func:`read_csv` function now supports parsing a ``Categorical`` column when
specified as a dtype (:issue:`10153`).  Depending on the structure of the data,
this can result in a faster parse time and lower memory usage compared to
converting to ``Categorical`` after parsing.  See the io :ref:`docs here <io.categorical>`.

.. ipython:: python

   data = """
   col1,col2,col3
   a,b,1
   a,b,2
   c,d,3
   """

   pd.read_csv(StringIO(data))
   pd.read_csv(StringIO(data)).dtypes
   pd.read_csv(StringIO(data), dtype="category").dtypes

Individual columns can be parsed as a ``Categorical`` using a dict specification

.. ipython:: python

   pd.read_csv(StringIO(data), dtype={"col1": "category"}).dtypes

.. note::

   The resulting categories will always be parsed as strings (object dtype).
   If the categories are numeric they can be converted using the
   :func:`to_numeric` function, or as appropriate, another converter
   such as :func:`to_datetime`.

   .. ipython:: python

      df = pd.read_csv(StringIO(data), dtype="category")
      df.dtypes
      df["col3"]
      df["col3"].cat.categories = pd.to_numeric(df["col3"].cat.categories)
      df["col3"]

.. _whatsnew_0190.enhancements.union_categoricals:

Categorical concatenation
^^^^^^^^^^^^^^^^^^^^^^^^^

- A function :func:`union_categoricals` has been added for combining categoricals, see :ref:`Unioning Categoricals<categorical.union>` (:issue:`13361`, :issue:`13763`, :issue:`13846`, :issue:`14173`)

  .. ipython:: python

     from pandas.api.types import union_categoricals

     a = pd.Categorical(["b", "c"])
     b = pd.Categorical(["a", "b"])
     union_categoricals([a, b])

- ``concat`` and ``append`` now can concat ``category`` dtypes with different ``categories`` as ``object`` dtype (:issue:`13524`)

  .. ipython:: python

     s1 = pd.Series(["a", "b"], dtype="category")
     s2 = pd.Series(["b", "c"], dtype="category")

**Previous behavior**:

.. code-block:: ipython

   In [1]: pd.concat([s1, s2])
   ValueError: incompatible categories in categorical concat

**New behavior**:

.. ipython:: python

   pd.concat([s1, s2])

.. _whatsnew_0190.enhancements.semi_month_offsets:

Semi-month offsets
^^^^^^^^^^^^^^^^^^

pandas has gained new frequency offsets, ``SemiMonthEnd`` ('SM') and ``SemiMonthBegin`` ('SMS').
These provide date offsets anchored (by default) to the 15th and end of month, and 15th and 1st of month respectively.
(:issue:`1543`)

.. ipython:: python

   from pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin

**SemiMonthEnd**:

.. ipython:: python

   pd.Timestamp("2016-01-01") + SemiMonthEnd()

   pd.date_range("2015-01-01", freq="SM", periods=4)

**SemiMonthBegin**:

.. ipython:: python

   pd.Timestamp("2016-01-01") + SemiMonthBegin()

   pd.date_range("2015-01-01", freq="SMS", periods=4)

Using the anchoring suffix, you can also specify the day of month to use instead of the 15th.

.. ipython:: python

   pd.date_range("2015-01-01", freq="SMS-16", periods=4)

   pd.date_range("2015-01-01", freq="SM-14", periods=4)

.. _whatsnew_0190.enhancements.index:

New Index methods
^^^^^^^^^^^^^^^^^

The following methods and options are added to ``Index``, to be more consistent with the ``Series`` and ``DataFrame`` API.

``Index`` now supports the ``.where()`` function for same shape indexing (:issue:`13170`)

.. ipython:: python

   idx = pd.Index(["a", "b", "c"])
   idx.where([True, False, True])


``Index`` now supports ``.dropna()`` to exclude missing values (:issue:`6194`)

.. ipython:: python

   idx = pd.Index([1, 2, np.nan, 4])
   idx.dropna()

For ``MultiIndex``, values are dropped if any level is missing by default. Specifying
``how='all'`` only drops values where all levels are missing.

.. ipython:: python

   midx = pd.MultiIndex.from_arrays([[1, 2, np.nan, 4], [1, 2, np.nan, np.nan]])
   midx
   midx.dropna()
   midx.dropna(how="all")

``Index`` now supports ``.str.extractall()`` which returns a ``DataFrame``, see the :ref:`docs here <text.extractall>` (:issue:`10008`, :issue:`13156`)

.. ipython:: python

   idx = pd.Index(["a1a2", "b1", "c1"])
   idx.str.extractall(r"[ab](?P<digit>\d)")

``Index.astype()`` now accepts an optional boolean argument ``copy``, which allows optional copying if the requirements on dtype are satisfied (:issue:`13209`)

.. _whatsnew_0190.gbq:

Google BigQuery enhancements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- The :func:`read_gbq` method has gained the ``dialect`` argument to allow users to specify whether to use BigQuery's legacy SQL or BigQuery's standard SQL. See the `docs <https://pandas-gbq.readthedocs.io/en/latest/reading.html>`__ for more details (:issue:`13615`).
- The :func:`~DataFrame.to_gbq` method now allows the DataFrame column order to differ from the destination table schema (:issue:`11359`).

.. _whatsnew_0190.errstate:

Fine-grained NumPy errstate
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previous versions of pandas would permanently silence numpy's ufunc error handling when ``pandas`` was imported. pandas did this in order to silence the warnings that would arise from using numpy ufuncs on missing data, which are usually represented as ``NaN`` s. Unfortunately, this silenced legitimate warnings arising in non-pandas code in the application. Starting with 0.19.0, pandas will use the ``numpy.errstate`` context manager to silence these warnings in a more fine-grained manner, only around where these operations are actually used in the pandas code base. (:issue:`13109`, :issue:`13145`)

After upgrading pandas, you may see *new* ``RuntimeWarnings`` being issued from your code. These are likely legitimate, and the underlying cause likely existed in the code when using previous versions of pandas that simply silenced the warning. Use `numpy.errstate <https://numpy.org/doc/stable/reference/generated/numpy.errstate.html>`__ around the source of the ``RuntimeWarning`` to control how these conditions are handled.

.. _whatsnew_0190.get_dummies_dtypes:

Method ``get_dummies`` now returns integer dtypes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``pd.get_dummies`` function now returns dummy-encoded columns as small integers, rather than floats (:issue:`8725`). This should provide an improved memory footprint.

**Previous behavior**:

.. code-block:: ipython

   In [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes

   Out[1]:
   a    float64
   b    float64
   c    float64
   dtype: object

**New behavior**:

.. ipython:: python

   pd.get_dummies(["a", "b", "a", "c"]).dtypes


.. _whatsnew_0190.enhancements.to_numeric_downcast:

Downcast values to smallest possible dtype in ``to_numeric``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``pd.to_numeric()`` now accepts a ``downcast`` parameter, which will downcast the data if possible to smallest specified numerical dtype (:issue:`13352`)

.. ipython:: python

   s = ["1", 2, 3]
   pd.to_numeric(s, downcast="unsigned")
   pd.to_numeric(s, downcast="integer")

.. _whatsnew_0190.dev_api:

pandas development API
^^^^^^^^^^^^^^^^^^^^^^

As part of making pandas API more uniform and accessible in the future, we have created a standard
sub-package of pandas, ``pandas.api`` to hold public API's. We are starting by exposing type
introspection functions in ``pandas.api.types``. More sub-packages and officially sanctioned API's
will be published in future versions of pandas (:issue:`13147`, :issue:`13634`)

The following are now part of this API:

.. ipython:: python

   import pprint
   from pandas.api import types

   funcs = [f for f in dir(types) if not f.startswith("_")]
   pprint.pprint(funcs)

.. note::

   Calling these functions from the internal module ``pandas.core.common`` will now show a ``DeprecationWarning`` (:issue:`13990`)


.. _whatsnew_0190.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- ``Timestamp`` can now accept positional and keyword parameters similar to :func:`datetime.datetime` (:issue:`10758`, :issue:`11630`)

  .. ipython:: python

     pd.Timestamp(2012, 1, 1)

     pd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)

- The ``.resample()`` function now accepts a ``on=`` or ``level=`` parameter for resampling on a datetimelike column or ``MultiIndex`` level (:issue:`13500`)

  .. ipython:: python

     df = pd.DataFrame(
         {"date": pd.date_range("2015-01-01", freq="W", periods=5), "a": np.arange(5)},
         index=pd.MultiIndex.from_arrays(
             [[1, 2, 3, 4, 5], pd.date_range("2015-01-01", freq="W", periods=5)],
             names=["v", "d"],
         ),
     )
     df
     df.resample("M", on="date").sum()
     df.resample("M", level="d").sum()

- The ``.get_credentials()`` method of ``GbqConnector`` can now first try to fetch `the application default credentials <https://developers.google.com/identity/protocols/application-default-credentials>`__. See the docs for more details (:issue:`13577`).
- The ``.tz_localize()`` method of ``DatetimeIndex`` and ``Timestamp`` has gained the ``errors`` keyword, so you can potentially coerce nonexistent timestamps to ``NaT``. The default behavior remains to raising a ``NonExistentTimeError`` (:issue:`13057`)
- ``.to_hdf/read_hdf()`` now accept path objects (e.g. ``pathlib.Path``, ``py.path.local``) for the file path (:issue:`11773`)
- The ``pd.read_csv()`` with ``engine='python'`` has gained support for the
  ``decimal`` (:issue:`12933`), ``na_filter`` (:issue:`13321`) and the ``memory_map`` option (:issue:`13381`).
- Consistent with the Python API, ``pd.read_csv()`` will now interpret ``+inf`` as positive infinity (:issue:`13274`)
- The ``pd.read_html()`` has gained support for the ``na_values``, ``converters``, ``keep_default_na``  options (:issue:`13461`)
- ``Categorical.astype()`` now accepts an optional boolean argument ``copy``, effective when dtype is categorical (:issue:`13209`)
- ``DataFrame`` has gained the ``.asof()`` method to return the last non-NaN values according to the selected subset (:issue:`13358`)
- The ``DataFrame`` constructor will now respect key ordering if a list of ``OrderedDict`` objects are passed in (:issue:`13304`)
- ``pd.read_html()`` has gained support for the ``decimal`` option (:issue:`12907`)
- ``Series`` has gained the properties ``.is_monotonic``, ``.is_monotonic_increasing``, ``.is_monotonic_decreasing``, similar to ``Index`` (:issue:`13336`)
- ``DataFrame.to_sql()`` now allows a single value as the SQL type for all columns (:issue:`11886`).
- ``Series.append`` now supports the ``ignore_index`` option (:issue:`13677`)
- ``.to_stata()`` and ``StataWriter`` can now write variable labels to Stata dta files using a dictionary to make column names to labels (:issue:`13535`, :issue:`13536`)
- ``.to_stata()`` and ``StataWriter`` will automatically convert ``datetime64[ns]`` columns to Stata format ``%tc``, rather than raising a ``ValueError`` (:issue:`12259`)
- ``read_stata()`` and ``StataReader`` raise with a more explicit error message when reading Stata files with repeated value labels when ``convert_categoricals=True`` (:issue:`13923`)
- ``DataFrame.style`` will now render sparsified MultiIndexes (:issue:`11655`)
- ``DataFrame.style`` will now show column level names (e.g. ``DataFrame.columns.names``) (:issue:`13775`)
- ``DataFrame`` has gained support to re-order the columns based on the values
  in a row using ``df.sort_values(by='...', axis=1)`` (:issue:`10806`)

  .. ipython:: python

     df = pd.DataFrame({"A": [2, 7], "B": [3, 5], "C": [4, 8]}, index=["row1", "row2"])
     df
     df.sort_values(by="row2", axis=1)

- Added documentation to :ref:`I/O<io.dtypes>` regarding the perils of reading in columns with mixed dtypes and how to handle it (:issue:`13746`)
- :meth:`~DataFrame.to_html` now has a ``border`` argument to control the value in the opening ``<table>`` tag. The default is the value of the ``html.border`` option, which defaults to 1. This also affects the notebook HTML repr, but since Jupyter's CSS includes a border-width attribute, the visual effect is the same. (:issue:`11563`).
- Raise ``ImportError`` in the sql functions when ``sqlalchemy`` is not installed and a connection string is used (:issue:`11920`).
- Compatibility with matplotlib 2.0. Older versions of pandas should also work with matplotlib 2.0 (:issue:`13333`)
- ``Timestamp``, ``Period``, ``DatetimeIndex``, ``PeriodIndex`` and ``.dt`` accessor have gained a ``.is_leap_year`` property to check whether the date belongs to a leap year. (:issue:`13727`)
- ``astype()`` will now accept a dict of column name to data types mapping as the ``dtype`` argument. (:issue:`12086`)
- The ``pd.read_json`` and ``DataFrame.to_json`` has gained support for reading and writing json lines with ``lines`` option see :ref:`Line delimited json <io.jsonl>` (:issue:`9180`)
- :func:`read_excel` now supports the true_values and false_values keyword arguments (:issue:`13347`)
- ``groupby()`` will now accept a scalar and a single-element list for specifying ``level`` on a non-``MultiIndex`` grouper. (:issue:`13907`)
- Non-convertible dates in an excel date column will be returned without conversion and the column will be ``object`` dtype, rather than raising an exception (:issue:`10001`).
- ``pd.Timedelta(None)`` is now accepted and will return ``NaT``, mirroring ``pd.Timestamp`` (:issue:`13687`)
- ``pd.read_stata()`` can now handle some format 111 files, which are produced by SAS when generating Stata dta files (:issue:`11526`)
- ``Series`` and ``Index`` now support ``divmod`` which will return a tuple of
  series or indices. This behaves like a standard binary operator with regards
  to broadcasting rules (:issue:`14208`).


.. _whatsnew_0190.api:

API changes
~~~~~~~~~~~

``Series.tolist()`` will now return Python types
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Series.tolist()`` will now return Python types in the output, mimicking NumPy ``.tolist()`` behavior (:issue:`10904`)


.. ipython:: python

   s = pd.Series([1, 2, 3])

**Previous behavior**:

.. code-block:: ipython

   In [7]: type(s.tolist()[0])
   Out[7]:
    <class 'numpy.int64'>

**New behavior**:

.. ipython:: python

   type(s.tolist()[0])

.. _whatsnew_0190.api.series_ops:

``Series`` operators for different indexes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Following ``Series`` operators have been changed to make all operators consistent,
including ``DataFrame`` (:issue:`1134`, :issue:`4581`, :issue:`13538`)

- ``Series`` comparison operators now raise ``ValueError`` when ``index`` are different.
- ``Series`` logical operators align both ``index`` of left and right hand side.

.. warning::
   Until 0.18.1, comparing ``Series`` with the same length, would succeed even if
   the ``.index`` are different (the result ignores ``.index``). As of 0.19.0, this will raises ``ValueError`` to be more strict. This section also describes how to keep previous behavior or align different indexes, using the flexible comparison methods like ``.eq``.


As a result, ``Series`` and ``DataFrame`` operators behave as below:

Arithmetic operators
""""""""""""""""""""

Arithmetic operators align both ``index`` (no changes).

.. ipython:: python

   s1 = pd.Series([1, 2, 3], index=list("ABC"))
   s2 = pd.Series([2, 2, 2], index=list("ABD"))
   s1 + s2

   df1 = pd.DataFrame([1, 2, 3], index=list("ABC"))
   df2 = pd.DataFrame([2, 2, 2], index=list("ABD"))
   df1 + df2

Comparison operators
""""""""""""""""""""

Comparison operators raise ``ValueError`` when ``.index`` are different.

**Previous behavior** (``Series``):

``Series`` compared values ignoring the ``.index`` as long as both had the same length:

.. code-block:: ipython

   In [1]: s1 == s2
   Out[1]:
   A    False
   B     True
   C    False
   dtype: bool

**New behavior** (``Series``):

.. code-block:: ipython

   In [2]: s1 == s2
   Out[2]:
   ValueError: Can only compare identically-labeled Series objects

.. note::

   To achieve the same result as previous versions (compare values based on locations ignoring ``.index``), compare both ``.values``.

   .. ipython:: python

      s1.values == s2.values

   If you want to compare ``Series`` aligning its ``.index``, see flexible comparison methods section below:

   .. ipython:: python

      s1.eq(s2)

**Current behavior** (``DataFrame``, no change):

.. code-block:: ipython

   In [3]: df1 == df2
   Out[3]:
   ValueError: Can only compare identically-labeled DataFrame objects

Logical operators
"""""""""""""""""

Logical operators align both ``.index`` of left and right hand side.

**Previous behavior** (``Series``), only left hand side ``index`` was kept:

.. code-block:: ipython

   In [4]: s1 = pd.Series([True, False, True], index=list('ABC'))
   In [5]: s2 = pd.Series([True, True, True], index=list('ABD'))
   In [6]: s1 & s2
   Out[6]:
   A     True
   B    False
   C    False
   dtype: bool

**New behavior** (``Series``):

.. ipython:: python

   s1 = pd.Series([True, False, True], index=list("ABC"))
   s2 = pd.Series([True, True, True], index=list("ABD"))
   s1 & s2

.. note::
   ``Series`` logical operators fill a ``NaN`` result with ``False``.

.. note::
   To achieve the same result as previous versions (compare values based on only left hand side index), you can use ``reindex_like``:

   .. ipython:: python

      s1 & s2.reindex_like(s1)

**Current behavior** (``DataFrame``, no change):

.. ipython:: python

   df1 = pd.DataFrame([True, False, True], index=list("ABC"))
   df2 = pd.DataFrame([True, True, True], index=list("ABD"))
   df1 & df2

Flexible comparison methods
"""""""""""""""""""""""""""

``Series`` flexible comparison methods like ``eq``, ``ne``, ``le``, ``lt``, ``ge`` and ``gt`` now align both ``index``. Use these operators if you want to compare two ``Series``
which has the different ``index``.

.. ipython:: python

   s1 = pd.Series([1, 2, 3], index=["a", "b", "c"])
   s2 = pd.Series([2, 2, 2], index=["b", "c", "d"])
   s1.eq(s2)
   s1.ge(s2)

Previously, this worked the same as comparison operators (see above).

.. _whatsnew_0190.api.promote:

``Series`` type promotion on assignment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A ``Series`` will now correctly promote its dtype for assignment with incompat values to the current dtype (:issue:`13234`)


.. ipython:: python
   :okwarning:

   s = pd.Series()

**Previous behavior**:

.. code-block:: ipython

   In [2]: s["a"] = pd.Timestamp("2016-01-01")

   In [3]: s["b"] = 3.0
   TypeError: invalid type promotion

**New behavior**:

.. ipython:: python

   s["a"] = pd.Timestamp("2016-01-01")
   s["b"] = 3.0
   s
   s.dtype

.. _whatsnew_0190.api.to_datetime_coerce:

Function ``.to_datetime()`` changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously if ``.to_datetime()`` encountered mixed integers/floats and strings, but no datetimes with ``errors='coerce'`` it would convert all to ``NaT``.

**Previous behavior**:

.. code-block:: ipython

   In [2]: pd.to_datetime([1, 'foo'], errors='coerce')
   Out[2]: DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None)

**Current behavior**:

This will now convert integers/floats with the default unit of ``ns``.

.. ipython:: python

   pd.to_datetime([1, "foo"], errors="coerce")

Bug fixes related to ``.to_datetime()``:

- Bug in ``pd.to_datetime()`` when passing integers or floats, and no ``unit`` and ``errors='coerce'`` (:issue:`13180`).
- Bug in ``pd.to_datetime()`` when passing invalid data types (e.g. bool); will now respect the ``errors`` keyword (:issue:`13176`)
- Bug in ``pd.to_datetime()`` which overflowed on ``int8``, and ``int16`` dtypes (:issue:`13451`)
- Bug in ``pd.to_datetime()`` raise ``AttributeError`` with ``NaN`` and the other string is not valid when ``errors='ignore'`` (:issue:`12424`)
- Bug in ``pd.to_datetime()`` did not cast floats correctly when ``unit`` was specified, resulting in truncated datetime (:issue:`13834`)

.. _whatsnew_0190.api.merging:

Merging changes
^^^^^^^^^^^^^^^

Merging will now preserve the dtype of the join keys (:issue:`8596`)

.. ipython:: python

   df1 = pd.DataFrame({"key": [1], "v1": [10]})
   df1
   df2 = pd.DataFrame({"key": [1, 2], "v1": [20, 30]})
   df2

**Previous behavior**:

.. code-block:: ipython

   In [5]: pd.merge(df1, df2, how='outer')
   Out[5]:
      key    v1
   0  1.0  10.0
   1  1.0  20.0
   2  2.0  30.0

   In [6]: pd.merge(df1, df2, how='outer').dtypes
   Out[6]:
   key    float64
   v1     float64
   dtype: object

**New behavior**:

We are able to preserve the join keys

.. ipython:: python

   pd.merge(df1, df2, how="outer")
   pd.merge(df1, df2, how="outer").dtypes

Of course if you have missing values that are introduced, then the
resulting dtype will be upcast, which is unchanged from previous.

.. ipython:: python

   pd.merge(df1, df2, how="outer", on="key")
   pd.merge(df1, df2, how="outer", on="key").dtypes

.. _whatsnew_0190.api.describe:

Method ``.describe()`` changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Percentile identifiers in the index of a ``.describe()`` output will now be rounded to the least precision that keeps them distinct (:issue:`13104`)

.. ipython:: python

   s = pd.Series([0, 1, 2, 3, 4])
   df = pd.DataFrame([0, 1, 2, 3, 4])

**Previous behavior**:

The percentiles were rounded to at most one decimal place, which could raise ``ValueError`` for a data frame if the percentiles were duplicated.

.. code-block:: ipython

   In [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])
   Out[3]:
   count     5.000000
   mean      2.000000
   std       1.581139
   min       0.000000
   0.0%      0.000400
   0.1%      0.002000
   0.1%      0.004000
   50%       2.000000
   99.9%     3.996000
   100.0%    3.998000
   100.0%    3.999600
   max       4.000000
   dtype: float64

   In [4]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])
   Out[4]:
   ...
   ValueError: cannot reindex from a duplicate axis

**New behavior**:

.. ipython:: python

   s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])
   df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])

Furthermore:

- Passing duplicated ``percentiles`` will now raise a ``ValueError``.
- Bug in ``.describe()`` on a DataFrame with a mixed-dtype column index, which would previously raise a ``TypeError`` (:issue:`13288`)

.. _whatsnew_0190.api.period:

``Period`` changes
^^^^^^^^^^^^^^^^^^

The ``PeriodIndex`` now has ``period`` dtype
""""""""""""""""""""""""""""""""""""""""""""

``PeriodIndex`` now has its own ``period`` dtype. The ``period`` dtype is a
pandas extension dtype like ``category`` or the :ref:`timezone aware dtype <timeseries.timezone_series>` (``datetime64[ns, tz]``) (:issue:`13941`).
As a consequence of this change, ``PeriodIndex`` no longer has an integer dtype:

**Previous behavior**:

.. code-block:: ipython

   In [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')

   In [2]: pi
   Out[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')

   In [3]: pd.api.types.is_integer_dtype(pi)
   Out[3]: True

   In [4]: pi.dtype
   Out[4]: dtype('int64')

**New behavior**:

.. ipython:: python

   pi = pd.PeriodIndex(["2016-08-01"], freq="D")
   pi
   pd.api.types.is_integer_dtype(pi)
   pd.api.types.is_period_dtype(pi)
   pi.dtype
   type(pi.dtype)

.. _whatsnew_0190.api.periodnat:

``Period('NaT')`` now returns ``pd.NaT``
""""""""""""""""""""""""""""""""""""""""

Previously, ``Period`` has its own ``Period('NaT')`` representation different from ``pd.NaT``. Now ``Period('NaT')`` has been changed to return ``pd.NaT``. (:issue:`12759`, :issue:`13582`)

**Previous behavior**:

.. code-block:: ipython

   In [5]: pd.Period('NaT', freq='D')
   Out[5]: Period('NaT', 'D')

**New behavior**:

These result in ``pd.NaT`` without providing ``freq`` option.

.. ipython:: python

   pd.Period("NaT")
   pd.Period(None)


To be compatible with ``Period`` addition and subtraction, ``pd.NaT`` now supports addition and subtraction with ``int``. Previously it raised ``ValueError``.

**Previous behavior**:

.. code-block:: ipython

   In [5]: pd.NaT + 1
   ...
   ValueError: Cannot add integral value to Timestamp without freq.

**New behavior**:

.. ipython:: python

   pd.NaT + 1
   pd.NaT - 1

``PeriodIndex.values`` now returns array of ``Period`` object
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

``.values`` is changed to return an array of ``Period`` objects, rather than an array
of integers (:issue:`13988`).

**Previous behavior**:

.. code-block:: ipython

   In [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')
   In [7]: pi.values
   Out[7]: array([492, 493])

**New behavior**:

.. ipython:: python

   pi = pd.PeriodIndex(["2011-01", "2011-02"], freq="M")
   pi.values


.. _whatsnew_0190.api.setops:

Index ``+`` / ``-`` no longer used for set operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Addition and subtraction of the base Index type and of DatetimeIndex
(not the numeric index types)
previously performed set operations (set union and difference). This
behavior was already deprecated since 0.15.0 (in favor using the specific
``.union()`` and ``.difference()`` methods), and is now disabled. When
possible, ``+`` and ``-`` are now used for element-wise operations, for
example for concatenating strings or subtracting datetimes
(:issue:`8227`, :issue:`14127`).

Previous behavior:

.. code-block:: ipython

   In [1]: pd.Index(['a', 'b']) + pd.Index(['a', 'c'])
   FutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union()
   Out[1]: Index(['a', 'b', 'c'], dtype='object')

**New behavior**: the same operation will now perform element-wise addition:

.. ipython:: python

   pd.Index(["a", "b"]) + pd.Index(["a", "c"])

Note that numeric Index objects already performed element-wise operations.
For example, the behavior of adding two integer Indexes is unchanged.
The base ``Index`` is now made consistent with this behavior.

.. ipython:: python

   pd.Index([1, 2, 3]) + pd.Index([2, 3, 4])

Further, because of this change, it is now possible to subtract two
DatetimeIndex objects resulting in a TimedeltaIndex:

**Previous behavior**:

.. code-block:: ipython

    In [1]: (pd.DatetimeIndex(['2016-01-01', '2016-01-02'])
       ...:  - pd.DatetimeIndex(['2016-01-02', '2016-01-03']))
    FutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()
    Out[1]: DatetimeIndex(['2016-01-01'], dtype='datetime64[ns]', freq=None)

**New behavior**:

.. ipython:: python

    (
        pd.DatetimeIndex(["2016-01-01", "2016-01-02"])
        - pd.DatetimeIndex(["2016-01-02", "2016-01-03"])
    )


.. _whatsnew_0190.api.difference:

``Index.difference`` and ``.symmetric_difference`` changes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Index.difference`` and ``Index.symmetric_difference`` will now, more consistently, treat ``NaN`` values as any other values. (:issue:`13514`)

.. ipython:: python

   idx1 = pd.Index([1, 2, 3, np.nan])
   idx2 = pd.Index([0, 1, np.nan])

**Previous behavior**:

.. code-block:: ipython

   In [3]: idx1.difference(idx2)
   Out[3]: Float64Index([nan, 2.0, 3.0], dtype='float64')

   In [4]: idx1.symmetric_difference(idx2)
   Out[4]: Float64Index([0.0, nan, 2.0, 3.0], dtype='float64')

**New behavior**:

.. ipython:: python

   idx1.difference(idx2)
   idx1.symmetric_difference(idx2)

.. _whatsnew_0190.api.unique_index:

``Index.unique`` consistently returns ``Index``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``Index.unique()`` now returns unique values as an
``Index`` of the appropriate ``dtype``. (:issue:`13395`).
Previously, most ``Index`` classes returned ``np.ndarray``, and ``DatetimeIndex``,
``TimedeltaIndex`` and ``PeriodIndex`` returned ``Index`` to keep metadata like timezone.

**Previous behavior**:

.. code-block:: ipython

   In [1]: pd.Index([1, 2, 3]).unique()
   Out[1]: array([1, 2, 3])

   In [2]: pd.DatetimeIndex(['2011-01-01', '2011-01-02',
      ...:                   '2011-01-03'], tz='Asia/Tokyo').unique()
   Out[2]:
   DatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',
                  '2011-01-03 00:00:00+09:00'],
                 dtype='datetime64[ns, Asia/Tokyo]', freq=None)

**New behavior**:

.. ipython:: python

   pd.Index([1, 2, 3]).unique()
   pd.DatetimeIndex(
       ["2011-01-01", "2011-01-02", "2011-01-03"], tz="Asia/Tokyo"
   ).unique()

.. _whatsnew_0190.api.multiindex:

``MultiIndex`` constructors, ``groupby`` and ``set_index`` preserve categorical dtypes
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

``MultiIndex.from_arrays`` and ``MultiIndex.from_product`` will now preserve categorical dtype
in ``MultiIndex`` levels (:issue:`13743`, :issue:`13854`).

.. ipython:: python

   cat = pd.Categorical(["a", "b"], categories=list("bac"))
   lvl1 = ["foo", "bar"]
   midx = pd.MultiIndex.from_arrays([cat, lvl1])
   midx

**Previous behavior**:

.. code-block:: ipython

   In [4]: midx.levels[0]
   Out[4]: Index(['b', 'a', 'c'], dtype='object')

   In [5]: midx.get_level_values[0]
   Out[5]: Index(['a', 'b'], dtype='object')

**New behavior**: the single level is now a ``CategoricalIndex``:

.. ipython:: python

   midx.levels[0]
   midx.get_level_values(0)

An analogous change has been made to ``MultiIndex.from_product``.
As a consequence, ``groupby`` and ``set_index`` also preserve categorical dtypes in indexes

.. ipython:: python

   df = pd.DataFrame({"A": [0, 1], "B": [10, 11], "C": cat})
   df_grouped = df.groupby(by=["A", "C"]).first()
   df_set_idx = df.set_index(["A", "C"])

**Previous behavior**:

.. code-block:: ipython

   In [11]: df_grouped.index.levels[1]
   Out[11]: Index(['b', 'a', 'c'], dtype='object', name='C')
   In [12]: df_grouped.reset_index().dtypes
   Out[12]:
   A      int64
   C     object
   B    float64
   dtype: object

   In [13]: df_set_idx.index.levels[1]
   Out[13]: Index(['b', 'a', 'c'], dtype='object', name='C')
   In [14]: df_set_idx.reset_index().dtypes
   Out[14]:
   A      int64
   C     object
   B      int64
   dtype: object

**New behavior**:

.. ipython:: python

   df_grouped.index.levels[1]
   df_grouped.reset_index().dtypes

   df_set_idx.index.levels[1]
   df_set_idx.reset_index().dtypes

.. _whatsnew_0190.api.autogenerated_chunksize_index:

Function ``read_csv`` will progressively enumerate chunks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When :func:`read_csv` is called with ``chunksize=n`` and without specifying an index,
each chunk used to have an independently generated index from ``0`` to ``n-1``.
They are now given instead a progressive index, starting from ``0`` for the first chunk,
from ``n`` for the second, and so on, so that, when concatenated, they are identical to
the result of calling :func:`read_csv` without the ``chunksize=`` argument
(:issue:`12185`).

.. ipython:: python

   data = "A,B\n0,1\n2,3\n4,5\n6,7"

**Previous behavior**:

.. code-block:: ipython

   In [2]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))
   Out[2]:
      A  B
   0  0  1
   1  2  3
   0  4  5
   1  6  7

**New behavior**:

.. ipython:: python

   pd.concat(pd.read_csv(StringIO(data), chunksize=2))

.. _whatsnew_0190.sparse:

Sparse changes
^^^^^^^^^^^^^^

These changes allow pandas to handle sparse data with more dtypes, and for work to make a smoother experience with data handling.

Types ``int64`` and ``bool`` support enhancements
"""""""""""""""""""""""""""""""""""""""""""""""""

Sparse data structures now gained enhanced support of ``int64`` and ``bool`` ``dtype`` (:issue:`667`, :issue:`13849`).

Previously, sparse data were ``float64`` dtype by default, even if all inputs were of ``int`` or ``bool`` dtype. You had to specify ``dtype`` explicitly to create sparse data with ``int64`` dtype. Also, ``fill_value`` had to be specified explicitly because the default was ``np.nan`` which doesn't appear in ``int64`` or ``bool`` data.

.. code-block:: ipython

   In [1]: pd.SparseArray([1, 2, 0, 0])
   Out[1]:
   [1.0, 2.0, 0.0, 0.0]
   Fill: nan
   IntIndex
   Indices: array([0, 1, 2, 3], dtype=int32)

   # specifying int64 dtype, but all values are stored in sp_values because
   # fill_value default is np.nan
   In [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)
   Out[2]:
   [1, 2, 0, 0]
   Fill: nan
   IntIndex
   Indices: array([0, 1, 2, 3], dtype=int32)

   In [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)
   Out[3]:
   [1, 2, 0, 0]
   Fill: 0
   IntIndex
   Indices: array([0, 1], dtype=int32)

As of v0.19.0, sparse data keeps the input dtype, and uses more appropriate ``fill_value`` defaults (``0`` for ``int64`` dtype, ``False`` for ``bool`` dtype).

.. ipython:: python
   :okwarning:

   pd.SparseArray([1, 2, 0, 0], dtype=np.int64)
   pd.SparseArray([True, False, False, False])

See the :ref:`docs <sparse.dtype>` for more details.

Operators now preserve dtypes
"""""""""""""""""""""""""""""

- Sparse data structure now can preserve ``dtype`` after arithmetic ops (:issue:`13848`)

.. code-block:: python

   s = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)
   s.dtype

   s + 1

- Sparse data structure now support ``astype`` to convert internal ``dtype`` (:issue:`13900`)

.. code-block:: python

   s = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)
   s
   s.astype(np.int64)

``astype`` fails if data contains values which cannot be converted to specified ``dtype``.
Note that the limitation is applied to ``fill_value`` which default is ``np.nan``.

.. code-block:: ipython

   In [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)
   Out[7]:
   ValueError: unable to coerce current fill_value nan to int64 dtype

Other sparse fixes
""""""""""""""""""

- Subclassed ``SparseDataFrame`` and ``SparseSeries`` now preserve class types when slicing or transposing. (:issue:`13787`)
- ``SparseArray`` with ``bool`` dtype now supports logical (bool) operators (:issue:`14000`)
- Bug in ``SparseSeries`` with ``MultiIndex`` ``[]`` indexing may raise ``IndexError`` (:issue:`13144`)
- Bug in ``SparseSeries`` with ``MultiIndex`` ``[]`` indexing result may have normal ``Index`` (:issue:`13144`)
- Bug in ``SparseDataFrame`` in which ``axis=None`` did not default to ``axis=0`` (:issue:`13048`)
- Bug in ``SparseSeries`` and ``SparseDataFrame`` creation with ``object`` dtype may raise ``TypeError`` (:issue:`11633`)
- Bug in ``SparseDataFrame`` doesn't respect passed ``SparseArray`` or ``SparseSeries`` 's dtype and ``fill_value``  (:issue:`13866`)
- Bug in ``SparseArray`` and ``SparseSeries`` don't apply ufunc to ``fill_value`` (:issue:`13853`)
- Bug in ``SparseSeries.abs`` incorrectly keeps negative ``fill_value`` (:issue:`13853`)
- Bug in single row slicing on multi-type ``SparseDataFrame`` s, types were previously forced to float (:issue:`13917`)
- Bug in ``SparseSeries`` slicing changes integer dtype to float (:issue:`8292`)
- Bug in ``SparseDataFarme`` comparison ops may raise ``TypeError`` (:issue:`13001`)
- Bug in ``SparseDataFarme.isnull`` raises ``ValueError`` (:issue:`8276`)
- Bug in ``SparseSeries`` representation with ``bool`` dtype may raise ``IndexError`` (:issue:`13110`)
- Bug in ``SparseSeries`` and ``SparseDataFrame`` of ``bool`` or ``int64`` dtype may display its values like ``float64`` dtype (:issue:`13110`)
- Bug in sparse indexing using ``SparseArray`` with ``bool`` dtype may return incorrect result  (:issue:`13985`)
- Bug in ``SparseArray`` created from ``SparseSeries`` may lose ``dtype`` (:issue:`13999`)
- Bug in ``SparseSeries`` comparison with dense returns normal ``Series`` rather than ``SparseSeries`` (:issue:`13999`)


.. _whatsnew_0190.indexer_dtype:

Indexer dtype changes
^^^^^^^^^^^^^^^^^^^^^

.. note::

   This change only affects 64 bit python running on Windows, and only affects relatively advanced
   indexing operations

Methods such as ``Index.get_indexer`` that return an indexer array, coerce that array to a "platform int", so that it can be
directly used in 3rd party library operations like ``numpy.take``.  Previously, a platform int was defined as ``np.int_``
which corresponds to a C integer, but the correct type, and what is being used now, is ``np.intp``, which corresponds
to the C integer size that can hold a pointer (:issue:`3033`, :issue:`13972`).

These types are the same on many platform, but for 64 bit python on Windows,
``np.int_`` is 32 bits, and ``np.intp`` is 64 bits.  Changing this behavior improves performance for many
operations on that platform.

**Previous behavior**:

.. code-block:: ipython

   In [1]: i = pd.Index(['a', 'b', 'c'])

   In [2]: i.get_indexer(['b', 'b', 'c']).dtype
   Out[2]: dtype('int32')

**New behavior**:

.. code-block:: ipython

   In [1]: i = pd.Index(['a', 'b', 'c'])

   In [2]: i.get_indexer(['b', 'b', 'c']).dtype
   Out[2]: dtype('int64')


.. _whatsnew_0190.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- ``Timestamp.to_pydatetime`` will issue a ``UserWarning`` when ``warn=True``, and the instance has a non-zero number of nanoseconds, previously this would print a message to stdout (:issue:`14101`).
- ``Series.unique()`` with datetime and timezone now returns return array of ``Timestamp`` with timezone (:issue:`13565`).
- ``Panel.to_sparse()`` will raise a ``NotImplementedError`` exception when called (:issue:`13778`).
- ``Index.reshape()`` will raise a ``NotImplementedError`` exception when called (:issue:`12882`).
- ``.filter()`` enforces mutual exclusion of the keyword arguments (:issue:`12399`).
- ``eval``'s upcasting rules for ``float32`` types have been updated to be more consistent with NumPy's rules.  New behavior will not upcast to ``float64`` if you multiply a pandas ``float32`` object by a scalar float64 (:issue:`12388`).
- An ``UnsupportedFunctionCall`` error is now raised if NumPy ufuncs like ``np.mean`` are called on groupby or resample objects (:issue:`12811`).
- ``__setitem__`` will no longer apply a callable rhs as a function instead of storing it. Call ``where`` directly to get the previous behavior (:issue:`13299`).
- Calls to ``.sample()`` will respect the random seed set via ``numpy.random.seed(n)`` (:issue:`13161`)
- ``Styler.apply`` is now more strict about the outputs your function must return. For ``axis=0`` or ``axis=1``, the output shape must be identical. For ``axis=None``, the output must be a DataFrame with identical columns and index labels (:issue:`13222`).
- ``Float64Index.astype(int)`` will now raise ``ValueError`` if ``Float64Index`` contains ``NaN`` values (:issue:`13149`)
- ``TimedeltaIndex.astype(int)`` and ``DatetimeIndex.astype(int)`` will now return ``Int64Index`` instead of ``np.array`` (:issue:`13209`)
- Passing ``Period`` with multiple frequencies to normal ``Index`` now returns ``Index`` with ``object`` dtype (:issue:`13664`)
- ``PeriodIndex.fillna`` with ``Period`` has different freq now coerces to ``object`` dtype (:issue:`13664`)
- Faceted boxplots from ``DataFrame.boxplot(by=col)`` now return a ``Series`` when ``return_type`` is not None. Previously these returned an ``OrderedDict``. Note that when ``return_type=None``, the default, these still return a 2-D NumPy array (:issue:`12216`, :issue:`7096`).
- ``pd.read_hdf`` will now raise a ``ValueError`` instead of ``KeyError``, if a mode other than ``r``, ``r+`` and ``a`` is supplied. (:issue:`13623`)
- ``pd.read_csv()``, ``pd.read_table()``, and ``pd.read_hdf()`` raise the builtin ``FileNotFoundError`` exception for Python 3.x when called on a nonexistent file; this is back-ported as ``IOError`` in Python 2.x (:issue:`14086`)
- More informative exceptions are passed through the csv parser. The exception type would now be the original exception type instead of ``CParserError`` (:issue:`13652`).
- ``pd.read_csv()`` in the C engine will now issue a ``ParserWarning`` or raise a ``ValueError`` when ``sep`` encoded is more than one character long (:issue:`14065`)
- ``DataFrame.values`` will now return ``float64`` with a ``DataFrame`` of mixed ``int64`` and ``uint64`` dtypes, conforming to ``np.find_common_type`` (:issue:`10364`, :issue:`13917`)
- ``.groupby.groups`` will now return a dictionary of ``Index`` objects, rather than a dictionary of ``np.ndarray`` or ``lists`` (:issue:`14293`)

.. _whatsnew_0190.deprecations:

Deprecations
~~~~~~~~~~~~
- ``Series.reshape`` and ``Categorical.reshape`` have been deprecated and will be removed in a subsequent release (:issue:`12882`, :issue:`12882`)
- ``PeriodIndex.to_datetime`` has been deprecated in favor of ``PeriodIndex.to_timestamp`` (:issue:`8254`)
- ``Timestamp.to_datetime`` has been deprecated in favor of ``Timestamp.to_pydatetime`` (:issue:`8254`)
- ``Index.to_datetime`` and ``DatetimeIndex.to_datetime`` have been deprecated in favor of ``pd.to_datetime`` (:issue:`8254`)
- ``pandas.core.datetools`` module has been deprecated and will be removed in a subsequent release (:issue:`14094`)
- ``SparseList`` has been deprecated and will be removed in a future version (:issue:`13784`)
- ``DataFrame.to_html()`` and ``DataFrame.to_latex()`` have dropped the ``colSpace`` parameter in favor of ``col_space`` (:issue:`13857`)
- ``DataFrame.to_sql()`` has deprecated the ``flavor`` parameter, as it is superfluous when SQLAlchemy is not installed (:issue:`13611`)
- Deprecated ``read_csv`` keywords:

  - ``compact_ints`` and ``use_unsigned`` have been deprecated and will be removed in a future version (:issue:`13320`)
  - ``buffer_lines`` has been deprecated and will be removed in a future version (:issue:`13360`)
  - ``as_recarray`` has been deprecated and will be removed in a future version (:issue:`13373`)
  - ``skip_footer`` has been deprecated in favor of ``skipfooter`` and will be removed in a future version (:issue:`13349`)

- top-level ``pd.ordered_merge()`` has been renamed to ``pd.merge_ordered()`` and the original name will be removed in a future version (:issue:`13358`)
- ``Timestamp.offset`` property (and named arg in the constructor), has been deprecated in favor of ``freq`` (:issue:`12160`)
- ``pd.tseries.util.pivot_annual`` is deprecated. Use ``pivot_table`` as alternative, an example is :ref:`here <cookbook.pivot>` (:issue:`736`)
- ``pd.tseries.util.isleapyear`` has been deprecated and will be removed in a subsequent release. Datetime-likes now have a ``.is_leap_year`` property (:issue:`13727`)
- ``Panel4D`` and ``PanelND`` constructors are deprecated and will be removed in a future version. The recommended way to represent these types of n-dimensional data are with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas provides a :meth:`~Panel4D.to_xarray` method to automate this conversion (:issue:`13564`).
- ``pandas.tseries.frequencies.get_standard_freq`` is deprecated. Use  ``pandas.tseries.frequencies.to_offset(freq).rule_code`` instead (:issue:`13874`)
- ``pandas.tseries.frequencies.to_offset``'s ``freqstr`` keyword is deprecated in favor of ``freq`` (:issue:`13874`)
- ``Categorical.from_array`` has been deprecated and will be removed in a future version (:issue:`13854`)

.. _whatsnew_0190.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- The ``SparsePanel`` class has been removed (:issue:`13778`)
- The ``pd.sandbox`` module has been removed in favor of the external library ``pandas-qt`` (:issue:`13670`)
- The ``pandas.io.data`` and ``pandas.io.wb`` modules are removed in favor of
  the `pandas-datareader package <https://github.com/pydata/pandas-datareader>`__ (:issue:`13724`).
- The ``pandas.tools.rplot`` module has been removed in favor of
  the `seaborn package <https://github.com/mwaskom/seaborn>`__ (:issue:`13855`)
- ``DataFrame.to_csv()`` has dropped the ``engine`` parameter, as was deprecated in 0.17.1 (:issue:`11274`, :issue:`13419`)
- ``DataFrame.to_dict()`` has dropped the ``outtype`` parameter in favor of ``orient`` (:issue:`13627`, :issue:`8486`)
- ``pd.Categorical`` has dropped setting of the ``ordered`` attribute directly in favor of the ``set_ordered`` method (:issue:`13671`)
- ``pd.Categorical`` has dropped the ``levels`` attribute in favor of ``categories`` (:issue:`8376`)
- ``DataFrame.to_sql()`` has dropped the ``mysql`` option for the ``flavor`` parameter (:issue:`13611`)
- ``Panel.shift()`` has dropped the ``lags`` parameter in favor of ``periods`` (:issue:`14041`)
- ``pd.Index`` has dropped the ``diff`` method in favor of ``difference`` (:issue:`13669`)
- ``pd.DataFrame`` has dropped the ``to_wide`` method in favor of ``to_panel`` (:issue:`14039`)
- ``Series.to_csv`` has dropped the ``nanRep`` parameter in favor of ``na_rep`` (:issue:`13804`)
- ``Series.xs``, ``DataFrame.xs``, ``Panel.xs``, ``Panel.major_xs``, and ``Panel.minor_xs`` have dropped the ``copy`` parameter (:issue:`13781`)
- ``str.split`` has dropped the ``return_type`` parameter in favor of ``expand`` (:issue:`13701`)
- Removal of the legacy time rules (offset aliases), deprecated since 0.17.0 (this has been alias since 0.8.0) (:issue:`13590`, :issue:`13868`). Now legacy time rules raises ``ValueError``. For the list of currently supported offsets, see :ref:`here <timeseries.offset_aliases>`.
- The default value for the ``return_type`` parameter for ``DataFrame.plot.box`` and ``DataFrame.boxplot`` changed from ``None`` to ``"axes"``. These methods will now return a matplotlib axes by default instead of a dictionary of artists. See :ref:`here <visualization.box.return>` (:issue:`6581`).
- The ``tquery`` and ``uquery`` functions in the ``pandas.io.sql`` module are removed (:issue:`5950`).


.. _whatsnew_0190.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improved performance of sparse ``IntIndex.intersect`` (:issue:`13082`)
- Improved performance of sparse arithmetic with ``BlockIndex`` when the number of blocks are large, though recommended to use ``IntIndex`` in such cases (:issue:`13082`)
- Improved performance of ``DataFrame.quantile()`` as it now operates per-block (:issue:`11623`)
- Improved performance of float64 hash table operations, fixing some very slow indexing and groupby operations in python 3 (:issue:`13166`, :issue:`13334`)
- Improved performance of ``DataFrameGroupBy.transform`` (:issue:`12737`)
- Improved performance of ``Index`` and ``Series`` ``.duplicated`` (:issue:`10235`)
- Improved performance of ``Index.difference`` (:issue:`12044`)
- Improved performance of ``RangeIndex.is_monotonic_increasing`` and ``is_monotonic_decreasing`` (:issue:`13749`)
- Improved performance of datetime string parsing in ``DatetimeIndex`` (:issue:`13692`)
- Improved performance of hashing ``Period`` (:issue:`12817`)
- Improved performance of ``factorize`` of datetime with timezone (:issue:`13750`)
- Improved performance of by lazily creating indexing hashtables on larger Indexes (:issue:`14266`)
- Improved performance of ``groupby.groups`` (:issue:`14293`)
- Unnecessary materializing of a MultiIndex when introspecting for memory usage (:issue:`14308`)

.. _whatsnew_0190.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in ``groupby().shift()``, which could cause a segfault or corruption in rare circumstances when grouping by columns with missing values (:issue:`13813`)
- Bug in ``groupby().cumsum()`` calculating ``cumprod`` when ``axis=1``. (:issue:`13994`)
- Bug in ``pd.to_timedelta()`` in which the ``errors`` parameter was not being respected (:issue:`13613`)
- Bug in ``io.json.json_normalize()``, where non-ascii keys raised an exception (:issue:`13213`)
- Bug when passing a not-default-indexed ``Series`` as ``xerr`` or ``yerr`` in ``.plot()`` (:issue:`11858`)
- Bug in area plot draws legend incorrectly if subplot is enabled or legend is moved after plot (matplotlib 1.5.0 is required to draw area plot legend properly) (:issue:`9161`, :issue:`13544`)
- Bug in ``DataFrame`` assignment with an object-dtyped ``Index`` where the resultant column is mutable to the original object. (:issue:`13522`)
- Bug in matplotlib ``AutoDataFormatter``; this restores the second scaled formatting and re-adds micro-second scaled formatting (:issue:`13131`)
- Bug in selection from a ``HDFStore`` with a fixed format and ``start`` and/or ``stop`` specified will now return the selected range (:issue:`8287`)
- Bug in ``Categorical.from_codes()`` where an unhelpful error was raised when an invalid ``ordered`` parameter was passed in (:issue:`14058`)
- Bug in ``Series`` construction from a tuple of integers on windows not returning default dtype (int64) (:issue:`13646`)
- Bug in ``TimedeltaIndex`` addition with a Datetime-like object where addition overflow was not being caught (:issue:`14068`)
- Bug in ``.groupby(..).resample(..)`` when the same object is called multiple times (:issue:`13174`)
- Bug in ``.to_records()`` when index name is a unicode string (:issue:`13172`)
- Bug in calling ``.memory_usage()`` on object which doesn't implement (:issue:`12924`)
- Regression in ``Series.quantile`` with nans (also shows up in ``.median()`` and ``.describe()`` ); furthermore now names the ``Series`` with the quantile (:issue:`13098`, :issue:`13146`)
- Bug in ``SeriesGroupBy.transform`` with datetime values and missing groups (:issue:`13191`)
- Bug where empty ``Series`` were incorrectly coerced in datetime-like numeric operations (:issue:`13844`)
- Bug in ``Categorical`` constructor when passed a ``Categorical`` containing datetimes with timezones (:issue:`14190`)
- Bug in ``Series.str.extractall()`` with ``str`` index raises ``ValueError``  (:issue:`13156`)
- Bug in ``Series.str.extractall()`` with single group and quantifier  (:issue:`13382`)
- Bug in ``DatetimeIndex`` and ``Period`` subtraction raises ``ValueError`` or ``AttributeError`` rather than ``TypeError`` (:issue:`13078`)
- Bug in ``Index`` and ``Series`` created with ``NaN`` and ``NaT`` mixed data may not have ``datetime64`` dtype  (:issue:`13324`)
- Bug in ``Index`` and ``Series`` may ignore ``np.datetime64('nat')`` and ``np.timdelta64('nat')`` to infer dtype (:issue:`13324`)
- Bug in ``PeriodIndex`` and ``Period`` subtraction raises ``AttributeError`` (:issue:`13071`)
- Bug in ``PeriodIndex`` construction returning a ``float64`` index in some circumstances (:issue:`13067`)
- Bug in ``.resample(..)`` with a ``PeriodIndex`` not changing its ``freq`` appropriately when empty (:issue:`13067`)
- Bug in ``.resample(..)`` with a ``PeriodIndex`` not retaining its type or name with an empty ``DataFrame`` appropriately when empty (:issue:`13212`)
- Bug in ``groupby(..).apply(..)`` when the passed function returns scalar values per group (:issue:`13468`).
- Bug in ``groupby(..).resample(..)`` where passing some keywords would raise an exception (:issue:`13235`)
- Bug in ``.tz_convert`` on a tz-aware ``DateTimeIndex`` that relied on index being sorted for correct results (:issue:`13306`)
- Bug in ``.tz_localize`` with ``dateutil.tz.tzlocal`` may return incorrect result (:issue:`13583`)
- Bug in ``DatetimeTZDtype`` dtype with ``dateutil.tz.tzlocal`` cannot be regarded as valid dtype (:issue:`13583`)
- Bug in ``pd.read_hdf()`` where attempting to load an HDF file with a single dataset, that had one or more categorical columns, failed unless the key argument was set to the name of the dataset. (:issue:`13231`)
- Bug in ``.rolling()`` that allowed a negative integer window in construction of the ``Rolling()`` object, but would later fail on aggregation (:issue:`13383`)
- Bug in ``Series`` indexing with tuple-valued data and a numeric index (:issue:`13509`)
- Bug in printing ``pd.DataFrame`` where unusual elements with the ``object`` dtype were causing segfaults (:issue:`13717`)
- Bug in ranking ``Series`` which could result in segfaults (:issue:`13445`)
- Bug in various index types, which did not propagate the name of passed index (:issue:`12309`)
- Bug in ``DatetimeIndex``, which did not honour the ``copy=True`` (:issue:`13205`)
- Bug in ``DatetimeIndex.is_normalized`` returns incorrectly for normalized date_range in case of local timezones (:issue:`13459`)
- Bug in ``pd.concat`` and ``.append`` may coerces ``datetime64`` and ``timedelta`` to ``object`` dtype containing python built-in ``datetime`` or ``timedelta`` rather than ``Timestamp`` or ``Timedelta`` (:issue:`13626`)
- Bug in ``PeriodIndex.append`` may raises ``AttributeError`` when the result is ``object`` dtype (:issue:`13221`)
- Bug in ``CategoricalIndex.append`` may accept normal ``list`` (:issue:`13626`)
- Bug in ``pd.concat`` and ``.append`` with the same timezone get reset to UTC (:issue:`7795`)
- Bug in ``Series`` and ``DataFrame`` ``.append`` raises ``AmbiguousTimeError`` if data contains datetime near DST boundary (:issue:`13626`)
- Bug in ``DataFrame.to_csv()`` in which float values were being quoted even though quotations were specified for non-numeric values only (:issue:`12922`, :issue:`13259`)
- Bug in ``DataFrame.describe()`` raising ``ValueError`` with only boolean columns (:issue:`13898`)
- Bug in ``MultiIndex`` slicing where extra elements were returned when level is non-unique (:issue:`12896`)
- Bug in ``.str.replace`` does not raise ``TypeError`` for invalid replacement (:issue:`13438`)
- Bug in ``MultiIndex.from_arrays`` which didn't check for input array lengths matching (:issue:`13599`)
- Bug in ``cartesian_product`` and ``MultiIndex.from_product`` which may raise with empty input arrays (:issue:`12258`)
- Bug in ``pd.read_csv()`` which may cause a segfault or corruption when iterating in large chunks over a stream/file under rare circumstances (:issue:`13703`)
- Bug in ``pd.read_csv()`` which caused errors to be raised when a dictionary containing scalars is passed in for ``na_values`` (:issue:`12224`)
- Bug in ``pd.read_csv()`` which caused BOM files to be incorrectly parsed by not ignoring the BOM (:issue:`4793`)
- Bug in ``pd.read_csv()`` with ``engine='python'`` which raised errors when a numpy array was passed in for ``usecols`` (:issue:`12546`)
- Bug in ``pd.read_csv()`` where the index columns were being incorrectly parsed when parsed as dates with a ``thousands`` parameter (:issue:`14066`)
- Bug in ``pd.read_csv()`` with ``engine='python'`` in which ``NaN`` values weren't being detected after data was converted to numeric values (:issue:`13314`)
- Bug in ``pd.read_csv()`` in which the ``nrows`` argument was not properly validated for both engines (:issue:`10476`)
- Bug in ``pd.read_csv()`` with ``engine='python'`` in which infinities of mixed-case forms were not being interpreted properly (:issue:`13274`)
- Bug in ``pd.read_csv()`` with ``engine='python'`` in which trailing ``NaN`` values were not being parsed (:issue:`13320`)
- Bug in ``pd.read_csv()`` with ``engine='python'`` when reading from a ``tempfile.TemporaryFile`` on Windows with Python 3 (:issue:`13398`)
- Bug in ``pd.read_csv()`` that prevents ``usecols`` kwarg from accepting single-byte unicode strings (:issue:`13219`)
- Bug in ``pd.read_csv()`` that prevents ``usecols`` from being an empty set (:issue:`13402`)
- Bug in ``pd.read_csv()`` in the C engine where the NULL character was not being parsed as NULL (:issue:`14012`)
- Bug in ``pd.read_csv()`` with ``engine='c'`` in which NULL ``quotechar`` was not accepted even though ``quoting`` was specified as ``None`` (:issue:`13411`)
- Bug in ``pd.read_csv()`` with ``engine='c'`` in which fields were not properly cast to float when quoting was specified as non-numeric (:issue:`13411`)
- Bug in ``pd.read_csv()`` in Python 2.x with non-UTF8 encoded, multi-character separated data (:issue:`3404`)
- Bug in ``pd.read_csv()``, where aliases for utf-xx (e.g. UTF-xx, UTF_xx, utf_xx) raised UnicodeDecodeError (:issue:`13549`)
- Bug in ``pd.read_csv``, ``pd.read_table``, ``pd.read_fwf``, ``pd.read_stata`` and ``pd.read_sas`` where files were opened by parsers but not closed if both ``chunksize`` and ``iterator`` were ``None``. (:issue:`13940`)
- Bug in ``StataReader``, ``StataWriter``, ``XportReader`` and ``SAS7BDATReader`` where a file was not properly closed when an error was raised. (:issue:`13940`)
- Bug in ``pd.pivot_table()`` where ``margins_name`` is ignored when ``aggfunc`` is a list (:issue:`13354`)
- Bug in ``pd.Series.str.zfill``, ``center``, ``ljust``, ``rjust``, and ``pad`` when passing non-integers, did not raise ``TypeError`` (:issue:`13598`)
- Bug in checking for any null objects in a ``TimedeltaIndex``, which always returned ``True`` (:issue:`13603`)
- Bug in ``Series`` arithmetic raises ``TypeError`` if it contains datetime-like as ``object`` dtype (:issue:`13043`)
- Bug ``Series.isnull()`` and ``Series.notnull()`` ignore ``Period('NaT')``  (:issue:`13737`)
- Bug ``Series.fillna()`` and ``Series.dropna()`` don't affect to ``Period('NaT')``  (:issue:`13737`
- Bug in ``.fillna(value=np.nan)`` incorrectly raises ``KeyError`` on a ``category`` dtyped ``Series`` (:issue:`14021`)
- Bug in extension dtype creation where the created types were not is/identical (:issue:`13285`)
- Bug in ``.resample(..)`` where incorrect warnings were triggered by IPython introspection (:issue:`13618`)
- Bug in ``NaT`` - ``Period`` raises ``AttributeError`` (:issue:`13071`)
- Bug in ``Series`` comparison may output incorrect result if rhs contains ``NaT`` (:issue:`9005`)
- Bug in ``Series`` and ``Index`` comparison may output incorrect result if it contains ``NaT`` with ``object`` dtype (:issue:`13592`)
- Bug in ``Period`` addition raises ``TypeError`` if ``Period`` is on right hand side (:issue:`13069`)
- Bug in ``Period`` and ``Series`` or ``Index`` comparison raises ``TypeError`` (:issue:`13200`)
- Bug in ``pd.set_eng_float_format()`` that would prevent NaN and Inf from formatting (:issue:`11981`)
- Bug in ``.unstack`` with ``Categorical`` dtype resets ``.ordered`` to ``True`` (:issue:`13249`)
- Clean some compile time warnings in datetime parsing (:issue:`13607`)
- Bug in ``factorize`` raises ``AmbiguousTimeError`` if data contains datetime near DST boundary (:issue:`13750`)
- Bug in ``.set_index`` raises ``AmbiguousTimeError`` if new index contains DST boundary and multi levels (:issue:`12920`)
- Bug in ``.shift`` raises ``AmbiguousTimeError`` if data contains datetime near DST boundary (:issue:`13926`)
- Bug in ``pd.read_hdf()`` returns incorrect result when a ``DataFrame`` with a ``categorical`` column and a query which doesn't match any values (:issue:`13792`)
- Bug in ``.iloc`` when indexing with a non lexsorted MultiIndex (:issue:`13797`)
- Bug in ``.loc`` when indexing with date strings in a reverse sorted ``DatetimeIndex`` (:issue:`14316`)
- Bug in ``Series`` comparison operators when dealing with zero dim NumPy arrays (:issue:`13006`)
- Bug in ``.combine_first`` may return incorrect ``dtype`` (:issue:`7630`, :issue:`10567`)
- Bug in ``groupby`` where ``apply`` returns different result depending on whether first result is ``None`` or not (:issue:`12824`)
- Bug in ``groupby(..).nth()`` where the group key is included inconsistently if called after ``.head()/.tail()`` (:issue:`12839`)
- Bug in ``.to_html``, ``.to_latex`` and ``.to_string`` silently ignore custom datetime formatter passed through the ``formatters`` key word (:issue:`10690`)
- Bug in ``DataFrame.iterrows()``, not yielding a ``Series`` subclasse if defined (:issue:`13977`)
- Bug in ``pd.to_numeric`` when ``errors='coerce'`` and input contains non-hashable objects (:issue:`13324`)
- Bug in invalid ``Timedelta`` arithmetic and comparison may raise ``ValueError`` rather than ``TypeError`` (:issue:`13624`)
- Bug in invalid datetime parsing in ``to_datetime`` and ``DatetimeIndex`` may raise ``TypeError`` rather than ``ValueError`` (:issue:`11169`, :issue:`11287`)
- Bug in ``Index`` created with tz-aware ``Timestamp`` and mismatched ``tz`` option incorrectly coerces timezone (:issue:`13692`)
- Bug in ``DatetimeIndex`` with nanosecond frequency does not include timestamp specified with ``end`` (:issue:`13672`)
- Bug in ```Series`` when setting a slice with a ``np.timedelta64`` (:issue:`14155`)
- Bug in ``Index`` raises ``OutOfBoundsDatetime`` if ``datetime`` exceeds ``datetime64[ns]`` bounds, rather than coercing to ``object`` dtype (:issue:`13663`)
- Bug in ``Index`` may ignore specified ``datetime64`` or ``timedelta64`` passed as ``dtype``  (:issue:`13981`)
- Bug in ``RangeIndex`` can be created without no arguments rather than raises ``TypeError`` (:issue:`13793`)
- Bug in ``.value_counts()`` raises ``OutOfBoundsDatetime`` if data exceeds ``datetime64[ns]`` bounds (:issue:`13663`)
- Bug in ``DatetimeIndex`` may raise ``OutOfBoundsDatetime`` if input ``np.datetime64`` has other unit than ``ns`` (:issue:`9114`)
- Bug in ``Series`` creation with ``np.datetime64`` which has other unit than ``ns`` as ``object`` dtype results in incorrect values (:issue:`13876`)
- Bug in ``resample`` with timedelta data where data was casted to float (:issue:`13119`).
- Bug in ``pd.isnull()`` ``pd.notnull()`` raise ``TypeError`` if input datetime-like has other unit than ``ns`` (:issue:`13389`)
- Bug in ``pd.merge()`` may raise ``TypeError`` if input datetime-like has other unit than ``ns`` (:issue:`13389`)
- Bug in ``HDFStore``/``read_hdf()`` discarded ``DatetimeIndex.name`` if ``tz`` was set (:issue:`13884`)
- Bug in ``Categorical.remove_unused_categories()`` changes ``.codes`` dtype to platform int (:issue:`13261`)
- Bug in ``groupby`` with ``as_index=False`` returns all NaN's when grouping on multiple columns including a categorical one (:issue:`13204`)
- Bug in ``df.groupby(...)[...]`` where getitem with ``Int64Index`` raised an error (:issue:`13731`)
- Bug in the CSS classes assigned to ``DataFrame.style`` for index names. Previously they were assigned ``"col_heading level<n> col<c>"`` where ``n`` was the number of levels + 1. Now they are assigned ``"index_name level<n>"``, where ``n`` is the correct level for that MultiIndex.
- Bug where ``pd.read_gbq()`` could throw ``ImportError: No module named discovery`` as a result of a naming conflict with another python package called apiclient  (:issue:`13454`)
- Bug in ``Index.union`` returns an incorrect result with a named empty index (:issue:`13432`)
- Bugs in ``Index.difference`` and ``DataFrame.join`` raise in Python3 when using mixed-integer indexes (:issue:`13432`, :issue:`12814`)
- Bug in subtract tz-aware ``datetime.datetime`` from tz-aware ``datetime64`` series (:issue:`14088`)
- Bug in ``.to_excel()`` when DataFrame contains a MultiIndex which contains a label with a NaN value (:issue:`13511`)
- Bug in invalid frequency offset string like "D1", "-2-3H" may not raise ``ValueError`` (:issue:`13930`)
- Bug in ``concat`` and ``groupby`` for hierarchical frames with ``RangeIndex`` levels (:issue:`13542`).
- Bug in ``Series.str.contains()`` for Series containing only ``NaN`` values of ``object`` dtype (:issue:`14171`)
- Bug in ``agg()`` function on groupby dataframe changes dtype of ``datetime64[ns]`` column to ``float64`` (:issue:`12821`)
- Bug in using NumPy ufunc with ``PeriodIndex`` to add or subtract integer raise ``IncompatibleFrequency``. Note that using standard operator like ``+`` or ``-`` is recommended, because standard operators use more efficient path (:issue:`13980`)
- Bug in operations on ``NaT`` returning ``float`` instead of ``datetime64[ns]`` (:issue:`12941`)
- Bug in ``Series`` flexible arithmetic methods (like ``.add()``) raises ``ValueError`` when ``axis=None`` (:issue:`13894`)
- Bug in ``DataFrame.to_csv()`` with ``MultiIndex`` columns in which a stray empty line was added (:issue:`6618`)
- Bug in ``DatetimeIndex``, ``TimedeltaIndex`` and ``PeriodIndex.equals()`` may return ``True`` when input isn't ``Index`` but contains the same values (:issue:`13107`)
- Bug in assignment against datetime with timezone may not work if it contains datetime near DST boundary (:issue:`14146`)
- Bug in ``pd.eval()`` and ``HDFStore`` query truncating long float literals with python 2 (:issue:`14241`)
- Bug in ``Index`` raises ``KeyError`` displaying incorrect column when column is not in the df and columns contains duplicate values (:issue:`13822`)
- Bug in ``Period`` and ``PeriodIndex`` creating wrong dates when frequency has combined offset aliases (:issue:`13874`)
- Bug in ``.to_string()`` when called with an integer ``line_width`` and ``index=False`` raises an UnboundLocalError exception because ``idx`` referenced before assignment.
- Bug in ``eval()`` where the ``resolvers`` argument would not accept a list (:issue:`14095`)
- Bugs in ``stack``, ``get_dummies``, ``make_axis_dummies`` which don't preserve categorical dtypes in (multi)indexes (:issue:`13854`)
- ``PeriodIndex`` can now accept ``list`` and ``array`` which contains ``pd.NaT`` (:issue:`13430`)
- Bug in ``df.groupby`` where ``.median()`` returns arbitrary values if grouped dataframe contains empty bins (:issue:`13629`)
- Bug in ``Index.copy()`` where ``name`` parameter was ignored (:issue:`14302`)


.. _whatsnew_0.19.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.18.1..v0.19.0
.. _whatsnew_0230:

What's new in 0.23.0 (May 15, 2018)
-----------------------------------

{{ header }}

.. ipython:: python
   :suppress:

   from pandas import * # noqa F401, F403


This is a major release from 0.22.0 and includes a number of API changes,
deprecations, new features, enhancements, and performance improvements along
with a large number of bug fixes. We recommend that all users upgrade to this
version.

Highlights include:

- :ref:`Round-trippable JSON format with 'table' orient <whatsnew_0230.enhancements.round-trippable_json>`.
- :ref:`Instantiation from dicts respects order for Python 3.6+ <whatsnew_0230.api_breaking.dict_insertion_order>`.
- :ref:`Dependent column arguments for assign <whatsnew_0230.enhancements.assign_dependent>`.
- :ref:`Merging / sorting on a combination of columns and index levels <whatsnew_0230.enhancements.merge_on_columns_and_levels>`.
- :ref:`Extending pandas with custom types <whatsnew_023.enhancements.extension>`.
- :ref:`Excluding unobserved categories from groupby <whatsnew_0230.enhancements.categorical_grouping>`.
- :ref:`Changes to make output shape of DataFrame.apply consistent <whatsnew_0230.api_breaking.apply>`.

Check the :ref:`API Changes <whatsnew_0230.api_breaking>` and :ref:`deprecations <whatsnew_0230.deprecations>` before updating.

.. warning::

   Starting January 1, 2019, pandas feature releases will support Python 3 only.
   See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

.. contents:: What's new in v0.23.0
    :local:
    :backlinks: none
    :depth: 2

.. _whatsnew_0230.enhancements:

New features
~~~~~~~~~~~~

.. _whatsnew_0230.enhancements.round-trippable_json:

JSON read/write round-trippable with ``orient='table'``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A ``DataFrame`` can now be written to and subsequently read back via JSON while preserving metadata through usage of the ``orient='table'`` argument (see :issue:`18912` and :issue:`9146`). Previously, none of the available ``orient`` values guaranteed the preservation of dtypes and index names, amongst other metadata.

.. ipython:: python

   df = pd.DataFrame({'foo': [1, 2, 3, 4],
                      'bar': ['a', 'b', 'c', 'd'],
                      'baz': pd.date_range('2018-01-01', freq='d', periods=4),
                      'qux': pd.Categorical(['a', 'b', 'c', 'c'])},
                     index=pd.Index(range(4), name='idx'))
   df
   df.dtypes
   df.to_json('test.json', orient='table')
   new_df = pd.read_json('test.json', orient='table')
   new_df
   new_df.dtypes

Please note that the string ``index`` is not supported with the round trip format, as it is used by default in ``write_json`` to indicate a missing index name.

.. ipython:: python
   :okwarning:

   df.index.name = 'index'

   df.to_json('test.json', orient='table')
   new_df = pd.read_json('test.json', orient='table')
   new_df
   new_df.dtypes

.. ipython:: python
   :suppress:

   import os
   os.remove('test.json')


.. _whatsnew_0230.enhancements.assign_dependent:


Method ``.assign()`` accepts dependent arguments
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :func:`DataFrame.assign` now accepts dependent keyword arguments for python version later than 3.6 (see also `PEP 468
<https://www.python.org/dev/peps/pep-0468/>`_). Later keyword arguments may now refer to earlier ones if the argument is a callable. See the
:ref:`documentation here <dsintro.chained_assignment>` (:issue:`14207`)

.. ipython:: python

    df = pd.DataFrame({'A': [1, 2, 3]})
    df
    df.assign(B=df.A, C=lambda x: x['A'] + x['B'])

.. warning::

  This may subtly change the behavior of your code when you're
  using ``.assign()`` to update an existing column. Previously, callables
  referring to other variables being updated would get the "old" values

  Previous behavior:

  .. code-block:: ipython

      In [2]: df = pd.DataFrame({"A": [1, 2, 3]})

      In [3]: df.assign(A=lambda df: df.A + 1, C=lambda df: df.A * -1)
      Out[3]:
         A  C
      0  2 -1
      1  3 -2
      2  4 -3

  New behavior:

  .. ipython:: python

      df.assign(A=df.A + 1, C=lambda df: df.A * -1)



.. _whatsnew_0230.enhancements.merge_on_columns_and_levels:

Merging on a combination of columns and index levels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Strings passed to :meth:`DataFrame.merge` as the ``on``, ``left_on``, and ``right_on``
parameters may now refer to either column names or index level names.
This enables merging ``DataFrame`` instances on a combination of index levels
and columns without resetting indexes. See the :ref:`Merge on columns and
levels <merging.merge_on_columns_and_levels>` documentation section.
(:issue:`14355`)

.. ipython:: python

   left_index = pd.Index(['K0', 'K0', 'K1', 'K2'], name='key1')

   left = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                        'B': ['B0', 'B1', 'B2', 'B3'],
                        'key2': ['K0', 'K1', 'K0', 'K1']},
                       index=left_index)

   right_index = pd.Index(['K0', 'K1', 'K2', 'K2'], name='key1')

   right = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],
                         'D': ['D0', 'D1', 'D2', 'D3'],
                         'key2': ['K0', 'K0', 'K0', 'K1']},
                        index=right_index)

   left.merge(right, on=['key1', 'key2'])

.. _whatsnew_0230.enhancements.sort_by_columns_and_levels:

Sorting by a combination of columns and index levels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Strings passed to :meth:`DataFrame.sort_values` as the ``by`` parameter may
now refer to either column names or index level names.  This enables sorting
``DataFrame`` instances by a combination of index levels and columns without
resetting indexes. See the :ref:`Sorting by Indexes and Values
<basics.sort_indexes_and_values>` documentation section.
(:issue:`14353`)

.. ipython:: python

   # Build MultiIndex
   idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 2),
                                    ('b', 2), ('b', 1), ('b', 1)])
   idx.names = ['first', 'second']

   # Build DataFrame
   df_multi = pd.DataFrame({'A': np.arange(6, 0, -1)},
                           index=idx)
   df_multi

   # Sort by 'second' (index) and 'A' (column)
   df_multi.sort_values(by=['second', 'A'])


.. _whatsnew_023.enhancements.extension:

Extending pandas with custom types (experimental)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

pandas now supports storing array-like objects that aren't necessarily 1-D NumPy
arrays as columns in a DataFrame or values in a Series. This allows third-party
libraries to implement extensions to NumPy's types, similar to how pandas
implemented categoricals, datetimes with timezones, periods, and intervals.

As a demonstration, we'll use cyberpandas_, which provides an ``IPArray`` type
for storing ip addresses.

.. code-block:: ipython

   In [1]: from cyberpandas import IPArray

   In [2]: values = IPArray([
      ...:     0,
      ...:     3232235777,
      ...:     42540766452641154071740215577757643572
      ...: ])
      ...:
      ...:

``IPArray`` isn't a normal 1-D NumPy array, but because it's a pandas
:class:`~pandas.api.extensions.ExtensionArray`, it can be stored properly inside pandas' containers.

.. code-block:: ipython

   In [3]: ser = pd.Series(values)

   In [4]: ser
   Out[4]:
   0                         0.0.0.0
   1                     192.168.1.1
   2    2001:db8:85a3::8a2e:370:7334
   dtype: ip

Notice that the dtype is ``ip``. The missing value semantics of the underlying
array are respected:

.. code-block:: ipython

   In [5]: ser.isna()
   Out[5]:
   0     True
   1    False
   2    False
   dtype: bool

For more, see the :ref:`extension types <extending.extension-types>`
documentation. If you build an extension array, publicize it on our
:ref:`ecosystem page <ecosystem.extensions>`.

.. _cyberpandas: https://cyberpandas.readthedocs.io/en/latest/


.. _whatsnew_0230.enhancements.categorical_grouping:

New ``observed`` keyword for excluding unobserved categories in ``GroupBy``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Grouping by a categorical includes the unobserved categories in the output.
When grouping by multiple categorical columns, this means you get the cartesian product of all the
categories, including combinations where there are no observations, which can result in a large
number of groups. We have added a keyword ``observed`` to control this behavior, it defaults to
``observed=False`` for backward-compatibility. (:issue:`14942`, :issue:`8138`, :issue:`15217`, :issue:`17594`, :issue:`8669`, :issue:`20583`, :issue:`20902`)

.. ipython:: python

   cat1 = pd.Categorical(["a", "a", "b", "b"],
                         categories=["a", "b", "z"], ordered=True)
   cat2 = pd.Categorical(["c", "d", "c", "d"],
                         categories=["c", "d", "y"], ordered=True)
   df = pd.DataFrame({"A": cat1, "B": cat2, "values": [1, 2, 3, 4]})
   df['C'] = ['foo', 'bar'] * 2
   df

To show all values, the previous behavior:

.. ipython:: python

   df.groupby(['A', 'B', 'C'], observed=False).count()


To show only observed values:

.. ipython:: python

   df.groupby(['A', 'B', 'C'], observed=True).count()

For pivoting operations, this behavior is *already* controlled by the ``dropna`` keyword:

.. ipython:: python

   cat1 = pd.Categorical(["a", "a", "b", "b"],
                         categories=["a", "b", "z"], ordered=True)
   cat2 = pd.Categorical(["c", "d", "c", "d"],
                         categories=["c", "d", "y"], ordered=True)
   df = pd.DataFrame({"A": cat1, "B": cat2, "values": [1, 2, 3, 4]})
   df

.. ipython:: python

   pd.pivot_table(df, values='values', index=['A', 'B'],
                  dropna=True)
   pd.pivot_table(df, values='values', index=['A', 'B'],
                  dropna=False)


.. _whatsnew_0230.enhancements.window_raw:

Rolling/Expanding.apply() accepts ``raw=False`` to pass a ``Series`` to the function
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`Series.rolling().apply() <pandas.core.window.Rolling.apply>`, :func:`DataFrame.rolling().apply() <pandas.core.window.Rolling.apply>`,
:func:`Series.expanding().apply() <pandas.core.window.Expanding.apply>`, and :func:`DataFrame.expanding().apply() <pandas.core.window.Expanding.apply>` have gained a ``raw=None`` parameter.
This is similar to :func:`DataFame.apply`. This parameter, if ``True`` allows one to send a ``np.ndarray`` to the applied function. If ``False`` a ``Series`` will be passed. The
default is ``None``, which preserves backward compatibility, so this will default to ``True``, sending an ``np.ndarray``.
In a future version the default will be changed to ``False``, sending a ``Series``. (:issue:`5071`, :issue:`20584`)

.. ipython:: python

   s = pd.Series(np.arange(5), np.arange(5) + 1)
   s

Pass a ``Series``:

.. ipython:: python

   s.rolling(2, min_periods=1).apply(lambda x: x.iloc[-1], raw=False)

Mimic the original behavior of passing a ndarray:

.. ipython:: python

   s.rolling(2, min_periods=1).apply(lambda x: x[-1], raw=True)


.. _whatsnew_0210.enhancements.limit_area:

``DataFrame.interpolate`` has gained the ``limit_area`` kwarg
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.interpolate` has gained a ``limit_area`` parameter to allow further control of which ``NaN`` s are replaced.
Use ``limit_area='inside'`` to fill only NaNs surrounded by valid values or use ``limit_area='outside'`` to fill only ``NaN`` s
outside the existing valid values while preserving those inside.  (:issue:`16284`) See the :ref:`full documentation here <missing_data.interp_limits>`.


.. ipython:: python

   ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan,
                    np.nan, 13, np.nan, np.nan])
   ser

Fill one consecutive inside value in both directions

.. ipython:: python

   ser.interpolate(limit_direction='both', limit_area='inside', limit=1)

Fill all consecutive outside values backward

.. ipython:: python

   ser.interpolate(limit_direction='backward', limit_area='outside')

Fill all consecutive outside values in both directions

.. ipython:: python

   ser.interpolate(limit_direction='both', limit_area='outside')

.. _whatsnew_0210.enhancements.get_dummies_dtype:

Function ``get_dummies`` now supports ``dtype`` argument
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The :func:`get_dummies` now accepts a ``dtype`` argument, which specifies a dtype for the new columns. The default remains uint8. (:issue:`18330`)

.. ipython:: python

   df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})
   pd.get_dummies(df, columns=['c']).dtypes
   pd.get_dummies(df, columns=['c'], dtype=bool).dtypes


.. _whatsnew_0230.enhancements.timedelta_mod:

Timedelta mod method
^^^^^^^^^^^^^^^^^^^^

``mod`` (%) and ``divmod`` operations are now defined on ``Timedelta`` objects
when operating with either timedelta-like or with numeric arguments.
See the :ref:`documentation here <timedeltas.mod_divmod>`. (:issue:`19365`)

.. ipython:: python

    td = pd.Timedelta(hours=37)
    td % pd.Timedelta(minutes=45)

.. _whatsnew_0230.enhancements.ran_inf:

Method ``.rank()`` handles ``inf`` values when ``NaN`` are present
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In previous versions, ``.rank()`` would assign ``inf`` elements ``NaN`` as their ranks. Now ranks are calculated properly. (:issue:`6945`)

.. ipython:: python

    s = pd.Series([-np.inf, 0, 1, np.nan, np.inf])
    s

Previous behavior:

.. code-block:: ipython

    In [11]: s.rank()
    Out[11]:
    0    1.0
    1    2.0
    2    3.0
    3    NaN
    4    NaN
    dtype: float64

Current behavior:

.. ipython:: python

    s.rank()

Furthermore, previously if you rank ``inf`` or ``-inf`` values together with ``NaN`` values, the calculation won't distinguish ``NaN`` from infinity when using 'top' or 'bottom' argument.

.. ipython:: python

    s = pd.Series([np.nan, np.nan, -np.inf, -np.inf])
    s

Previous behavior:

.. code-block:: ipython

    In [15]: s.rank(na_option='top')
    Out[15]:
    0    2.5
    1    2.5
    2    2.5
    3    2.5
    dtype: float64

Current behavior:

.. ipython:: python

    s.rank(na_option='top')

These bugs were squashed:

- Bug in :meth:`DataFrame.rank` and :meth:`Series.rank` when ``method='dense'`` and ``pct=True`` in which percentile ranks were not being used with the number of distinct observations (:issue:`15630`)
- Bug in :meth:`Series.rank` and :meth:`DataFrame.rank` when ``ascending='False'`` failed to return correct ranks for infinity if ``NaN`` were present (:issue:`19538`)
- Bug in :func:`DataFrameGroupBy.rank` where ranks were incorrect when both infinity and ``NaN`` were present (:issue:`20561`)


.. _whatsnew_0230.enhancements.str_cat_align:

``Series.str.cat`` has gained the ``join`` kwarg
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Previously, :meth:`Series.str.cat` did not -- in contrast to most of ``pandas`` -- align :class:`Series` on their index before concatenation (see :issue:`18657`).
The method has now gained a keyword ``join`` to control the manner of alignment, see examples below and :ref:`here <text.concatenate>`.

In v.0.23 ``join`` will default to None (meaning no alignment), but this default will change to ``'left'`` in a future version of pandas.

.. ipython:: python
   :okwarning:

    s = pd.Series(['a', 'b', 'c', 'd'])
    t = pd.Series(['b', 'd', 'e', 'c'], index=[1, 3, 4, 2])
    s.str.cat(t)
    s.str.cat(t, join='left', na_rep='-')

Furthermore, :meth:`Series.str.cat` now works for ``CategoricalIndex`` as well (previously raised a ``ValueError``; see :issue:`20842`).

.. _whatsnew_0230.enhancements.astype_category:

``DataFrame.astype`` performs column-wise conversion to ``Categorical``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:meth:`DataFrame.astype` can now perform column-wise conversion to ``Categorical`` by supplying the string ``'category'`` or
a :class:`~pandas.api.types.CategoricalDtype`. Previously, attempting this would raise a ``NotImplementedError``. See the
:ref:`categorical.objectcreation` section of the documentation for more details and examples. (:issue:`12860`, :issue:`18099`)

Supplying the string ``'category'`` performs column-wise conversion, with only labels appearing in a given column set as categories:

.. ipython:: python

    df = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})
    df = df.astype('category')
    df['A'].dtype
    df['B'].dtype


Supplying a ``CategoricalDtype`` will make the categories in each column consistent with the supplied dtype:

.. ipython:: python

    from pandas.api.types import CategoricalDtype
    df = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})
    cdt = CategoricalDtype(categories=list('abcd'), ordered=True)
    df = df.astype(cdt)
    df['A'].dtype
    df['B'].dtype


.. _whatsnew_0230.enhancements.other:

Other enhancements
^^^^^^^^^^^^^^^^^^

- Unary ``+`` now permitted for ``Series`` and ``DataFrame`` as  numeric operator (:issue:`16073`)
- Better support for :meth:`~pandas.io.formats.style.Styler.to_excel` output with the ``xlsxwriter`` engine. (:issue:`16149`)
- :func:`pandas.tseries.frequencies.to_offset` now accepts leading '+' signs e.g. '+1h'. (:issue:`18171`)
- :func:`MultiIndex.unique` now supports the ``level=`` argument, to get unique values from a specific index level (:issue:`17896`)
- :class:`pandas.io.formats.style.Styler` now has method ``hide_index()`` to determine whether the index will be rendered in output (:issue:`14194`)
- :class:`pandas.io.formats.style.Styler` now has method ``hide_columns()`` to determine whether columns will be hidden in output (:issue:`14194`)
- Improved wording of ``ValueError`` raised in :func:`to_datetime` when ``unit=`` is passed with a non-convertible value (:issue:`14350`)
- :func:`Series.fillna` now accepts a Series or a dict as a ``value`` for a categorical dtype (:issue:`17033`)
- :func:`pandas.read_clipboard` updated to use qtpy, falling back to PyQt5 and then PyQt4, adding compatibility with Python3 and multiple python-qt bindings (:issue:`17722`)
- Improved wording of ``ValueError`` raised in :func:`read_csv` when the ``usecols`` argument cannot match all columns. (:issue:`17301`)
- :func:`DataFrame.corrwith` now silently drops non-numeric columns when passed a Series. Before, an exception was raised (:issue:`18570`).
- :class:`IntervalIndex` now supports time zone aware ``Interval`` objects (:issue:`18537`, :issue:`18538`)
- :func:`Series` / :func:`DataFrame` tab completion also returns identifiers in the first level of a :func:`MultiIndex`. (:issue:`16326`)
- :func:`read_excel()` has gained the ``nrows`` parameter (:issue:`16645`)
- :meth:`DataFrame.append` can now in more cases preserve the type of the calling dataframe's columns (e.g. if both are ``CategoricalIndex``) (:issue:`18359`)
- :meth:`DataFrame.to_json` and :meth:`Series.to_json` now accept an ``index`` argument which allows the user to exclude the index from the JSON output (:issue:`17394`)
- ``IntervalIndex.to_tuples()`` has gained the ``na_tuple`` parameter to control whether NA is returned as a tuple of NA, or NA itself (:issue:`18756`)
- ``Categorical.rename_categories``, ``CategoricalIndex.rename_categories`` and :attr:`Series.cat.rename_categories`
  can now take a callable as their argument (:issue:`18862`)
- :class:`Interval` and :class:`IntervalIndex` have gained a ``length`` attribute (:issue:`18789`)
- ``Resampler`` objects now have a functioning :attr:`~pandas.core.resample.Resampler.pipe` method.
  Previously, calls to ``pipe`` were diverted to  the ``mean`` method (:issue:`17905`).
- :func:`~pandas.api.types.is_scalar` now returns ``True`` for ``DateOffset`` objects (:issue:`18943`).
- :func:`DataFrame.pivot` now accepts a list for the ``values=`` kwarg (:issue:`17160`).
- Added :func:`pandas.api.extensions.register_dataframe_accessor`,
  :func:`pandas.api.extensions.register_series_accessor`, and
  :func:`pandas.api.extensions.register_index_accessor`, accessor for libraries downstream of pandas
  to register custom accessors like ``.cat`` on pandas objects. See
  :ref:`Registering Custom Accessors <extending.register-accessors>` for more (:issue:`14781`).

- ``IntervalIndex.astype`` now supports conversions between subtypes when passed an ``IntervalDtype`` (:issue:`19197`)
- :class:`IntervalIndex` and its associated constructor methods (``from_arrays``, ``from_breaks``, ``from_tuples``) have gained a ``dtype`` parameter (:issue:`19262`)
- Added :func:`pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing` and :func:`pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing` (:issue:`17015`)
- For subclassed ``DataFrames``, :func:`DataFrame.apply` will now preserve the ``Series`` subclass (if defined) when passing the data to the applied function (:issue:`19822`)
- :func:`DataFrame.from_dict` now accepts a ``columns`` argument that can be used to specify the column names when ``orient='index'`` is used (:issue:`18529`)
- Added option ``display.html.use_mathjax`` so `MathJax <https://www.mathjax.org/>`_ can be disabled when rendering tables in ``Jupyter`` notebooks (:issue:`19856`, :issue:`19824`)
- :func:`DataFrame.replace` now supports the ``method`` parameter, which can be used to specify the replacement method when ``to_replace`` is a scalar, list or tuple and ``value`` is ``None`` (:issue:`19632`)
- :meth:`Timestamp.month_name`, :meth:`DatetimeIndex.month_name`, and :meth:`Series.dt.month_name` are now available (:issue:`12805`)
- :meth:`Timestamp.day_name` and :meth:`DatetimeIndex.day_name` are now available to return day names with a specified locale (:issue:`12806`)
- :meth:`DataFrame.to_sql` now performs a multi-value insert if the underlying connection supports itk rather than inserting row by row.
  ``SQLAlchemy`` dialects supporting multi-value inserts include: ``mysql``, ``postgresql``, ``sqlite`` and any dialect with ``supports_multivalues_insert``. (:issue:`14315`, :issue:`8953`)
- :func:`read_html` now accepts a ``displayed_only`` keyword argument to controls whether or not hidden elements are parsed (``True`` by default) (:issue:`20027`)
- :func:`read_html` now reads all ``<tbody>`` elements in a ``<table>``, not just the first. (:issue:`20690`)
- :meth:`~pandas.core.window.Rolling.quantile` and :meth:`~pandas.core.window.Expanding.quantile` now accept the ``interpolation`` keyword, ``linear`` by default (:issue:`20497`)
- zip compression is supported via ``compression=zip`` in :func:`DataFrame.to_pickle`, :func:`Series.to_pickle`, :func:`DataFrame.to_csv`, :func:`Series.to_csv`, :func:`DataFrame.to_json`, :func:`Series.to_json`. (:issue:`17778`)
- :class:`~pandas.tseries.offsets.WeekOfMonth` constructor now supports ``n=0`` (:issue:`20517`).
- :class:`DataFrame` and :class:`Series` now support matrix multiplication (``@``) operator (:issue:`10259`) for Python>=3.5
- Updated :meth:`DataFrame.to_gbq` and :meth:`pandas.read_gbq` signature and documentation to reflect changes from
  the pandas-gbq library version 0.4.0. Adds intersphinx mapping to pandas-gbq
  library. (:issue:`20564`)
- Added new writer for exporting Stata dta files in version 117, ``StataWriter117``.  This format supports exporting strings with lengths up to 2,000,000 characters (:issue:`16450`)
- :func:`to_hdf` and :func:`read_hdf` now accept an ``errors`` keyword argument to control encoding error handling (:issue:`20835`)
- :func:`cut` has gained the ``duplicates='raise'|'drop'`` option to control whether to raise on duplicated edges (:issue:`20947`)
- :func:`date_range`, :func:`timedelta_range`, and :func:`interval_range` now return a linearly spaced index if ``start``, ``stop``, and ``periods`` are specified, but ``freq`` is not. (:issue:`20808`, :issue:`20983`, :issue:`20976`)

.. _whatsnew_0230.api_breaking:

Backwards incompatible API changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _whatsnew_0230.api_breaking.deps:

Dependencies have increased minimum versions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We have updated our minimum supported versions of dependencies (:issue:`15184`).
If installed, we now require:

+-----------------+-----------------+----------+---------------+
| Package         | Minimum Version | Required |     Issue     |
+=================+=================+==========+===============+
| python-dateutil | 2.5.0           |    X     | :issue:`15184`|
+-----------------+-----------------+----------+---------------+
| openpyxl        | 2.4.0           |          | :issue:`15184`|
+-----------------+-----------------+----------+---------------+
| beautifulsoup4  | 4.2.1           |          | :issue:`20082`|
+-----------------+-----------------+----------+---------------+
| setuptools      | 24.2.0          |          | :issue:`20698`|
+-----------------+-----------------+----------+---------------+

.. _whatsnew_0230.api_breaking.dict_insertion_order:

Instantiation from dicts preserves dict insertion order for Python 3.6+
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Until Python 3.6, dicts in Python had no formally defined ordering. For Python
version 3.6 and later, dicts are ordered by insertion order, see
`PEP 468 <https://www.python.org/dev/peps/pep-0468/>`_.
pandas will use the dict's insertion order, when creating a ``Series`` or
``DataFrame`` from a dict and you're using Python version 3.6 or
higher. (:issue:`19884`)

Previous behavior (and current behavior if on Python < 3.6):

.. code-block:: ipython

    In [16]: pd.Series({'Income': 2000,
       ....:            'Expenses': -1500,
       ....:            'Taxes': -200,
       ....:            'Net result': 300})
    Out[16]:
    Expenses     -1500
    Income        2000
    Net result     300
    Taxes         -200
    dtype: int64

Note the Series above is ordered alphabetically by the index values.

New behavior (for Python >= 3.6):

.. ipython:: python

    pd.Series({'Income': 2000,
               'Expenses': -1500,
               'Taxes': -200,
               'Net result': 300})

Notice that the Series is now ordered by insertion order. This new behavior is
used for all relevant pandas types (``Series``, ``DataFrame``, ``SparseSeries``
and ``SparseDataFrame``).

If you wish to retain the old behavior while using Python >= 3.6, you can use
``.sort_index()``:

.. ipython:: python

    pd.Series({'Income': 2000,
               'Expenses': -1500,
               'Taxes': -200,
               'Net result': 300}).sort_index()

.. _whatsnew_0230.api_breaking.deprecate_panel:

Deprecate Panel
^^^^^^^^^^^^^^^

``Panel`` was deprecated in the 0.20.x release, showing as a ``DeprecationWarning``. Using ``Panel`` will now show a ``FutureWarning``. The recommended way to represent 3-D data are
with a ``MultiIndex`` on a ``DataFrame`` via the :meth:`~Panel.to_frame` or with the `xarray package <http://xarray.pydata.org/en/stable/>`__. pandas
provides a :meth:`~Panel.to_xarray` method to automate this conversion (:issue:`13563`, :issue:`18324`).

.. code-block:: ipython

    In [75]: import pandas._testing as tm

    In [76]: p = tm.makePanel()

    In [77]: p
    Out[77]:
    <class 'pandas.core.panel.Panel'>
    Dimensions: 3 (items) x 3 (major_axis) x 4 (minor_axis)
    Items axis: ItemA to ItemC
    Major_axis axis: 2000-01-03 00:00:00 to 2000-01-05 00:00:00
    Minor_axis axis: A to D

Convert to a MultiIndex DataFrame

.. code-block:: ipython

    In [78]: p.to_frame()
    Out[78]:
                         ItemA     ItemB     ItemC
    major      minor
    2000-01-03 A      0.469112  0.721555  0.404705
               B     -1.135632  0.271860 -1.039268
               C      0.119209  0.276232 -1.344312
               D     -2.104569  0.113648 -0.109050
    2000-01-04 A     -0.282863 -0.706771  0.577046
               B      1.212112 -0.424972 -0.370647
               C     -1.044236 -1.087401  0.844885
               D     -0.494929 -1.478427  1.643563
    2000-01-05 A     -1.509059 -1.039575 -1.715002
               B     -0.173215  0.567020 -1.157892
               C     -0.861849 -0.673690  1.075770
               D      1.071804  0.524988 -1.469388

    [12 rows x 3 columns]

Convert to an xarray DataArray

.. code-block:: ipython

    In [79]: p.to_xarray()
    Out[79]:
    <xarray.DataArray (items: 3, major_axis: 3, minor_axis: 4)>
    array([[[ 0.469112, -1.135632,  0.119209, -2.104569],
            [-0.282863,  1.212112, -1.044236, -0.494929],
            [-1.509059, -0.173215, -0.861849,  1.071804]],

           [[ 0.721555,  0.27186 ,  0.276232,  0.113648],
            [-0.706771, -0.424972, -1.087401, -1.478427],
            [-1.039575,  0.56702 , -0.67369 ,  0.524988]],

           [[ 0.404705, -1.039268, -1.344312, -0.10905 ],
            [ 0.577046, -0.370647,  0.844885,  1.643563],
            [-1.715002, -1.157892,  1.07577 , -1.469388]]])
    Coordinates:
      * items       (items) object 'ItemA' 'ItemB' 'ItemC'
      * major_axis  (major_axis) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05
      * minor_axis  (minor_axis) object 'A' 'B' 'C' 'D'


.. _whatsnew_0230.api_breaking.core_common:

pandas.core.common removals
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The following error & warning messages are removed from ``pandas.core.common`` (:issue:`13634`, :issue:`19769`):

- ``PerformanceWarning``
- ``UnsupportedFunctionCall``
- ``UnsortedIndexError``
- ``AbstractMethodError``

These are available from import from ``pandas.errors`` (since 0.19.0).


.. _whatsnew_0230.api_breaking.apply:

Changes to make output of ``DataFrame.apply`` consistent
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`DataFrame.apply` was inconsistent when applying an arbitrary user-defined-function that returned a list-like with ``axis=1``. Several bugs and inconsistencies
are resolved. If the applied function returns a Series, then pandas will return a DataFrame; otherwise a Series will be returned, this includes the case
where a list-like (e.g. ``tuple`` or ``list`` is returned) (:issue:`16353`, :issue:`17437`, :issue:`17970`, :issue:`17348`, :issue:`17892`, :issue:`18573`,
:issue:`17602`, :issue:`18775`, :issue:`18901`, :issue:`18919`).

.. ipython:: python

    df = pd.DataFrame(np.tile(np.arange(3), 6).reshape(6, -1) + 1,
                      columns=['A', 'B', 'C'])
    df

Previous behavior: if the returned shape happened to match the length of original columns, this would return a ``DataFrame``.
If the return shape did not match, a ``Series`` with lists was returned.

.. code-block:: python

   In [3]: df.apply(lambda x: [1, 2, 3], axis=1)
   Out[3]:
      A  B  C
   0  1  2  3
   1  1  2  3
   2  1  2  3
   3  1  2  3
   4  1  2  3
   5  1  2  3

   In [4]: df.apply(lambda x: [1, 2], axis=1)
   Out[4]:
   0    [1, 2]
   1    [1, 2]
   2    [1, 2]
   3    [1, 2]
   4    [1, 2]
   5    [1, 2]
   dtype: object


New behavior: When the applied function returns a list-like, this will now *always* return a ``Series``.

.. ipython:: python

    df.apply(lambda x: [1, 2, 3], axis=1)
    df.apply(lambda x: [1, 2], axis=1)

To have expanded columns, you can use ``result_type='expand'``

.. ipython:: python

    df.apply(lambda x: [1, 2, 3], axis=1, result_type='expand')

To broadcast the result across the original columns (the old behaviour for
list-likes of the correct length), you can use ``result_type='broadcast'``.
The shape must match the original columns.

.. ipython:: python

    df.apply(lambda x: [1, 2, 3], axis=1, result_type='broadcast')

Returning a ``Series`` allows one to control the exact return structure and column names:

.. ipython:: python

    df.apply(lambda x: pd.Series([1, 2, 3], index=['D', 'E', 'F']), axis=1)

.. _whatsnew_0230.api_breaking.concat:

Concatenation will no longer sort
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In a future version of pandas :func:`pandas.concat` will no longer sort the non-concatenation axis when it is not already aligned.
The current behavior is the same as the previous (sorting), but now a warning is issued when ``sort`` is not specified and the non-concatenation axis is not aligned (:issue:`4588`).

.. ipython:: python
   :okwarning:

   df1 = pd.DataFrame({"a": [1, 2], "b": [1, 2]}, columns=['b', 'a'])
   df2 = pd.DataFrame({"a": [4, 5]})

   pd.concat([df1, df2])

To keep the previous behavior (sorting) and silence the warning, pass ``sort=True``

.. ipython:: python

   pd.concat([df1, df2], sort=True)

To accept the future behavior (no sorting), pass ``sort=False``

.. ipython

   pd.concat([df1, df2], sort=False)

Note that this change also applies to :meth:`DataFrame.append`, which has also received a ``sort`` keyword for controlling this behavior.


.. _whatsnew_0230.api_breaking.build_changes:

Build changes
^^^^^^^^^^^^^

- Building pandas for development now requires ``cython >= 0.24`` (:issue:`18613`)
- Building from source now explicitly requires ``setuptools`` in ``setup.py`` (:issue:`18113`)
- Updated conda recipe to be in compliance with conda-build 3.0+ (:issue:`18002`)

.. _whatsnew_0230.api_breaking.index_division_by_zero:

Index division by zero fills correctly
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Division operations on ``Index`` and subclasses will now fill division of positive numbers by zero with ``np.inf``, division of negative numbers by zero with ``-np.inf`` and ``0 / 0`` with ``np.nan``.  This matches existing ``Series`` behavior. (:issue:`19322`, :issue:`19347`)

Previous behavior:

.. code-block:: ipython

    In [6]: index = pd.Int64Index([-1, 0, 1])

    In [7]: index / 0
    Out[7]: Int64Index([0, 0, 0], dtype='int64')

    # Previous behavior yielded different results depending on the type of zero in the divisor
    In [8]: index / 0.0
    Out[8]: Float64Index([-inf, nan, inf], dtype='float64')

    In [9]: index = pd.UInt64Index([0, 1])

    In [10]: index / np.array([0, 0], dtype=np.uint64)
    Out[10]: UInt64Index([0, 0], dtype='uint64')

    In [11]: pd.RangeIndex(1, 5) / 0
    ZeroDivisionError: integer division or modulo by zero

Current behavior:

.. code-block:: ipython

    In [12]: index = pd.Int64Index([-1, 0, 1])
    # division by zero gives -infinity where negative,
    # +infinity where positive, and NaN for 0 / 0
    In [13]: index / 0

    # The result of division by zero should not depend on
    # whether the zero is int or float
    In [14]: index / 0.0

    In [15]: index = pd.UInt64Index([0, 1])
    In [16]: index / np.array([0, 0], dtype=np.uint64)

    In [17]: pd.RangeIndex(1, 5) / 0

.. _whatsnew_0230.api_breaking.extract:

Extraction of matching patterns from strings
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, extracting matching patterns from strings with :func:`str.extract` used to return a
``Series`` if a single group was being extracted (a ``DataFrame`` if more than one group was
extracted). As of pandas 0.23.0 :func:`str.extract` always returns a ``DataFrame``, unless
``expand`` is set to ``False``. Finally, ``None`` was an accepted value for
the ``expand`` parameter (which was equivalent to ``False``), but now raises a ``ValueError``. (:issue:`11386`)

Previous behavior:

.. code-block:: ipython

    In [1]: s = pd.Series(['number 10', '12 eggs'])

    In [2]: extracted = s.str.extract(r'.*(\d\d).*')

    In [3]: extracted
    Out [3]:
    0    10
    1    12
    dtype: object

    In [4]: type(extracted)
    Out [4]:
    pandas.core.series.Series

New behavior:

.. ipython:: python

    s = pd.Series(['number 10', '12 eggs'])
    extracted = s.str.extract(r'.*(\d\d).*')
    extracted
    type(extracted)

To restore previous behavior, simply set ``expand`` to ``False``:

.. ipython:: python

    s = pd.Series(['number 10', '12 eggs'])
    extracted = s.str.extract(r'.*(\d\d).*', expand=False)
    extracted
    type(extracted)

.. _whatsnew_0230.api_breaking.cdt_ordered:

Default value for the ``ordered`` parameter of ``CategoricalDtype``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The default value of the ``ordered`` parameter for :class:`~pandas.api.types.CategoricalDtype` has changed from ``False`` to ``None`` to allow updating of ``categories`` without impacting ``ordered``.  Behavior should remain consistent for downstream objects, such as :class:`Categorical` (:issue:`18790`)

In previous versions, the default value for the ``ordered`` parameter was ``False``.  This could potentially lead to the ``ordered`` parameter unintentionally being changed from ``True`` to ``False`` when users attempt to update ``categories`` if ``ordered`` is not explicitly specified, as it would silently default to ``False``.  The new behavior for ``ordered=None`` is to retain the existing value of ``ordered``.

New behavior:

.. code-block:: ipython

    In [2]: from pandas.api.types import CategoricalDtype

    In [3]: cat = pd.Categorical(list('abcaba'), ordered=True, categories=list('cba'))

    In [4]: cat
    Out[4]:
    [a, b, c, a, b, a]
    Categories (3, object): [c < b < a]

    In [5]: cdt = CategoricalDtype(categories=list('cbad'))

    In [6]: cat.astype(cdt)
    Out[6]:
    [a, b, c, a, b, a]
    Categories (4, object): [c < b < a < d]

Notice in the example above that the converted ``Categorical`` has retained ``ordered=True``.  Had the default value for ``ordered`` remained as ``False``, the converted ``Categorical`` would have become unordered, despite ``ordered=False`` never being explicitly specified.  To change the value of ``ordered``, explicitly pass it to the new dtype, e.g. ``CategoricalDtype(categories=list('cbad'), ordered=False)``.

Note that the unintentional conversion of ``ordered`` discussed above did not arise in previous versions due to separate bugs that prevented ``astype`` from doing any type of category to category conversion (:issue:`10696`, :issue:`18593`).  These bugs have been fixed in this release, and motivated changing the default value of ``ordered``.

.. _whatsnew_0230.api_breaking.pretty_printing:

Better pretty-printing of DataFrames in a terminal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Previously, the default value for the maximum number of columns was
``pd.options.display.max_columns=20``. This meant that relatively wide data
frames would not fit within the terminal width, and pandas would introduce line
breaks to display these 20 columns. This resulted in an output that was
relatively difficult to read:

.. image:: ../_static/print_df_old.png

If Python runs in a terminal, the maximum number of columns is now determined
automatically so that the printed data frame fits within the current terminal
width (``pd.options.display.max_columns=0``) (:issue:`17023`). If Python runs
as a Jupyter kernel (such as the Jupyter QtConsole or a Jupyter notebook, as
well as in many IDEs), this value cannot be inferred automatically and is thus
set to ``20`` as in previous versions. In a terminal, this results in a much
nicer output:

.. image:: ../_static/print_df_new.png

Note that if you don't like the new default, you can always set this option
yourself. To revert to the old setting, you can run this line:

.. code-block:: python

  pd.options.display.max_columns = 20

.. _whatsnew_0230.api.datetimelike:

Datetimelike API changes
^^^^^^^^^^^^^^^^^^^^^^^^

- The default ``Timedelta`` constructor now accepts an ``ISO 8601 Duration`` string as an argument (:issue:`19040`)
- Subtracting ``NaT`` from a :class:`Series` with ``dtype='datetime64[ns]'`` returns a ``Series`` with ``dtype='timedelta64[ns]'`` instead of ``dtype='datetime64[ns]'`` (:issue:`18808`)
- Addition or subtraction of ``NaT`` from :class:`TimedeltaIndex` will return ``TimedeltaIndex`` instead of ``DatetimeIndex`` (:issue:`19124`)
- :func:`DatetimeIndex.shift` and :func:`TimedeltaIndex.shift` will now raise ``NullFrequencyError`` (which subclasses ``ValueError``, which was raised in older versions) when the index object frequency is ``None`` (:issue:`19147`)
- Addition and subtraction of ``NaN`` from a :class:`Series` with ``dtype='timedelta64[ns]'`` will raise a ``TypeError`` instead of treating the ``NaN`` as ``NaT`` (:issue:`19274`)
- ``NaT`` division with :class:`datetime.timedelta` will now return ``NaN`` instead of raising (:issue:`17876`)
- Operations between a :class:`Series` with dtype ``dtype='datetime64[ns]'`` and a :class:`PeriodIndex` will correctly raises ``TypeError`` (:issue:`18850`)
- Subtraction of :class:`Series` with timezone-aware ``dtype='datetime64[ns]'`` with mismatched timezones will raise ``TypeError`` instead of ``ValueError`` (:issue:`18817`)
- :class:`Timestamp` will no longer silently ignore unused or invalid ``tz`` or ``tzinfo`` keyword arguments (:issue:`17690`)
- :class:`Timestamp` will no longer silently ignore invalid ``freq`` arguments (:issue:`5168`)
- :class:`CacheableOffset` and :class:`WeekDay` are no longer available in the ``pandas.tseries.offsets`` module (:issue:`17830`)
- ``pandas.tseries.frequencies.get_freq_group()`` and ``pandas.tseries.frequencies.DAYS`` are removed from the public API (:issue:`18034`)
- :func:`Series.truncate` and :func:`DataFrame.truncate` will raise a ``ValueError`` if the index is not sorted instead of an unhelpful ``KeyError`` (:issue:`17935`)
- :attr:`Series.first` and :attr:`DataFrame.first` will now raise a ``TypeError``
  rather than ``NotImplementedError`` when index is not a :class:`DatetimeIndex` (:issue:`20725`).
- :attr:`Series.last` and :attr:`DataFrame.last` will now raise a ``TypeError``
  rather than ``NotImplementedError`` when index is not a :class:`DatetimeIndex` (:issue:`20725`).
- Restricted ``DateOffset`` keyword arguments. Previously, ``DateOffset`` subclasses allowed arbitrary keyword arguments which could lead to unexpected behavior. Now, only valid arguments will be accepted. (:issue:`17176`, :issue:`18226`).
- :func:`pandas.merge` provides a more informative error message when trying to merge on timezone-aware and timezone-naive columns (:issue:`15800`)
- For :class:`DatetimeIndex` and :class:`TimedeltaIndex` with ``freq=None``, addition or subtraction of integer-dtyped array or ``Index`` will raise ``NullFrequencyError`` instead of ``TypeError`` (:issue:`19895`)
- :class:`Timestamp` constructor now accepts a ``nanosecond`` keyword or positional argument (:issue:`18898`)
- :class:`DatetimeIndex` will now raise an ``AttributeError`` when the ``tz`` attribute is set after instantiation (:issue:`3746`)
- :class:`DatetimeIndex` with a ``pytz`` timezone will now return a consistent ``pytz`` timezone (:issue:`18595`)

.. _whatsnew_0230.api.other:

Other API changes
^^^^^^^^^^^^^^^^^

- :func:`Series.astype` and :func:`Index.astype` with an incompatible dtype will now raise a ``TypeError`` rather than a ``ValueError`` (:issue:`18231`)
- ``Series`` construction with an ``object`` dtyped tz-aware datetime and ``dtype=object`` specified, will now return an ``object`` dtyped ``Series``, previously this would infer the datetime dtype (:issue:`18231`)
- A :class:`Series` of ``dtype=category`` constructed from an empty ``dict`` will now have categories of ``dtype=object`` rather than ``dtype=float64``, consistently with the case in which an empty list is passed (:issue:`18515`)
- All-NaN levels in a ``MultiIndex`` are now assigned ``float`` rather than ``object`` dtype, promoting consistency with ``Index`` (:issue:`17929`).
- Levels names of a ``MultiIndex`` (when not None) are now required to be unique: trying to create a ``MultiIndex`` with repeated names will raise a ``ValueError`` (:issue:`18872`)
- Both construction and renaming of ``Index``/``MultiIndex`` with non-hashable ``name``/``names`` will now raise ``TypeError`` (:issue:`20527`)
- :func:`Index.map` can now accept ``Series`` and dictionary input objects (:issue:`12756`, :issue:`18482`, :issue:`18509`).
- :func:`DataFrame.unstack` will now default to filling with ``np.nan`` for ``object`` columns. (:issue:`12815`)
- :class:`IntervalIndex` constructor will raise if the ``closed`` parameter conflicts with how the input data is inferred to be closed (:issue:`18421`)
- Inserting missing values into indexes will work for all types of indexes and automatically insert the correct type of missing value (``NaN``, ``NaT``, etc.) regardless of the type passed in (:issue:`18295`)
- When created with duplicate labels, ``MultiIndex`` now raises a ``ValueError``. (:issue:`17464`)
- :func:`Series.fillna` now raises a ``TypeError`` instead of a ``ValueError`` when passed a list, tuple or DataFrame as a ``value`` (:issue:`18293`)
- :func:`pandas.DataFrame.merge` no longer casts a ``float`` column to ``object`` when merging on ``int`` and ``float`` columns (:issue:`16572`)
- :func:`pandas.merge` now raises a ``ValueError`` when trying to merge on incompatible data types (:issue:`9780`)
- The default NA value for :class:`UInt64Index` has changed from 0 to ``NaN``, which impacts methods that mask with NA, such as ``UInt64Index.where()`` (:issue:`18398`)
- Refactored ``setup.py`` to use ``find_packages`` instead of explicitly listing out all subpackages (:issue:`18535`)
- Rearranged the order of keyword arguments in :func:`read_excel()` to align with :func:`read_csv()` (:issue:`16672`)
- :func:`wide_to_long` previously kept numeric-like suffixes as ``object`` dtype. Now they are cast to numeric if possible (:issue:`17627`)
- In :func:`read_excel`, the ``comment`` argument is now exposed as a named parameter (:issue:`18735`)
- Rearranged the order of keyword arguments in :func:`read_excel()` to align with :func:`read_csv()` (:issue:`16672`)
- The options ``html.border`` and ``mode.use_inf_as_null`` were deprecated in prior versions, these will now show ``FutureWarning`` rather than a ``DeprecationWarning`` (:issue:`19003`)
- :class:`IntervalIndex` and ``IntervalDtype`` no longer support categorical, object, and string subtypes (:issue:`19016`)
- ``IntervalDtype`` now returns ``True`` when compared against ``'interval'`` regardless of subtype, and ``IntervalDtype.name`` now returns ``'interval'`` regardless of subtype (:issue:`18980`)
- ``KeyError`` now raises instead of ``ValueError`` in :meth:`~DataFrame.drop`, :meth:`~Panel.drop`, :meth:`~Series.drop`, :meth:`~Index.drop` when dropping a non-existent element in an axis with duplicates (:issue:`19186`)
- :func:`Series.to_csv` now accepts a ``compression`` argument that works in the same way as the ``compression`` argument in :func:`DataFrame.to_csv` (:issue:`18958`)
- Set operations (union, difference...) on :class:`IntervalIndex` with incompatible index types will now raise a ``TypeError`` rather than a ``ValueError`` (:issue:`19329`)
- :class:`DateOffset` objects render more simply, e.g. ``<DateOffset: days=1>`` instead of ``<DateOffset: kwds={'days': 1}>`` (:issue:`19403`)
- ``Categorical.fillna`` now validates its ``value`` and ``method`` keyword arguments. It now raises when both or none are specified, matching the behavior of :meth:`Series.fillna` (:issue:`19682`)
- ``pd.to_datetime('today')`` now returns a datetime, consistent with ``pd.Timestamp('today')``; previously ``pd.to_datetime('today')`` returned a ``.normalized()`` datetime (:issue:`19935`)
- :func:`Series.str.replace` now takes an optional ``regex`` keyword which, when set to ``False``, uses literal string replacement rather than regex replacement (:issue:`16808`)
- :func:`DatetimeIndex.strftime` and :func:`PeriodIndex.strftime` now return an ``Index`` instead of a numpy array to be consistent with similar accessors (:issue:`20127`)
- Constructing a Series from a list of length 1 no longer broadcasts this list when a longer index is specified (:issue:`19714`, :issue:`20391`).
- :func:`DataFrame.to_dict` with ``orient='index'`` no longer casts int columns to float for a DataFrame with only int and float columns (:issue:`18580`)
- A user-defined-function that is passed to :func:`Series.rolling().aggregate() <pandas.core.window.Rolling.aggregate>`, :func:`DataFrame.rolling().aggregate() <pandas.core.window.Rolling.aggregate>`, or its expanding cousins, will now *always* be passed a ``Series``, rather than a ``np.array``; ``.apply()`` only has the ``raw`` keyword, see :ref:`here <whatsnew_0230.enhancements.window_raw>`. This is consistent with the signatures of ``.aggregate()`` across pandas (:issue:`20584`)
- Rolling and Expanding types raise ``NotImplementedError`` upon iteration (:issue:`11704`).

.. _whatsnew_0230.deprecations:

Deprecations
~~~~~~~~~~~~

- ``Series.from_array`` and ``SparseSeries.from_array`` are deprecated. Use the normal constructor ``Series(..)`` and ``SparseSeries(..)`` instead (:issue:`18213`).
- ``DataFrame.as_matrix`` is deprecated. Use ``DataFrame.values`` instead (:issue:`18458`).
- ``Series.asobject``, ``DatetimeIndex.asobject``, ``PeriodIndex.asobject`` and ``TimeDeltaIndex.asobject`` have been deprecated. Use ``.astype(object)`` instead (:issue:`18572`)
- Grouping by a tuple of keys now emits a ``FutureWarning`` and is deprecated.
  In the future, a tuple passed to ``'by'`` will always refer to a single key
  that is the actual tuple, instead of treating the tuple as multiple keys. To
  retain the previous behavior, use a list instead of a tuple (:issue:`18314`)
- ``Series.valid`` is deprecated. Use :meth:`Series.dropna` instead (:issue:`18800`).
- :func:`read_excel` has deprecated the ``skip_footer`` parameter. Use ``skipfooter`` instead (:issue:`18836`)
- :meth:`ExcelFile.parse` has deprecated ``sheetname`` in favor of ``sheet_name`` for consistency with :func:`read_excel` (:issue:`20920`).
- The ``is_copy`` attribute is deprecated and will be removed in a future version (:issue:`18801`).
- ``IntervalIndex.from_intervals`` is deprecated in favor of the :class:`IntervalIndex` constructor (:issue:`19263`)
- ``DataFrame.from_items`` is deprecated. Use :func:`DataFrame.from_dict` instead, or ``DataFrame.from_dict(OrderedDict())`` if you wish to preserve the key order (:issue:`17320`, :issue:`17312`)
- Indexing a :class:`MultiIndex` or a :class:`FloatIndex` with a list containing some missing keys will now show a :class:`FutureWarning`, which is consistent with other types of indexes (:issue:`17758`).

- The ``broadcast`` parameter of ``.apply()`` is deprecated in favor of ``result_type='broadcast'`` (:issue:`18577`)
- The ``reduce`` parameter of ``.apply()`` is deprecated in favor of ``result_type='reduce'`` (:issue:`18577`)
- The ``order`` parameter of :func:`factorize` is deprecated and will be removed in a future release (:issue:`19727`)
- :attr:`Timestamp.weekday_name`, :attr:`DatetimeIndex.weekday_name`, and :attr:`Series.dt.weekday_name` are deprecated in favor of :meth:`Timestamp.day_name`, :meth:`DatetimeIndex.day_name`, and :meth:`Series.dt.day_name` (:issue:`12806`)

- ``pandas.tseries.plotting.tsplot`` is deprecated. Use :func:`Series.plot` instead (:issue:`18627`)
- ``Index.summary()`` is deprecated and will be removed in a future version (:issue:`18217`)
- ``NDFrame.get_ftype_counts()`` is deprecated and will be removed in a future version (:issue:`18243`)
- The ``convert_datetime64`` parameter in :func:`DataFrame.to_records` has been deprecated and will be removed in a future version. The NumPy bug motivating this parameter has been resolved. The default value for this parameter has also changed from ``True`` to ``None`` (:issue:`18160`).
- :func:`Series.rolling().apply() <pandas.core.window.Rolling.apply>`, :func:`DataFrame.rolling().apply() <pandas.core.window.Rolling.apply>`,
  :func:`Series.expanding().apply() <pandas.core.window.Expanding.apply>`, and :func:`DataFrame.expanding().apply() <pandas.core.window.Expanding.apply>` have deprecated passing an ``np.array`` by default. One will need to pass the new ``raw`` parameter to be explicit about what is passed (:issue:`20584`)
- The ``data``, ``base``, ``strides``, ``flags`` and ``itemsize`` properties
  of the ``Series`` and ``Index`` classes have been deprecated and will be
  removed in a future version (:issue:`20419`).
- ``DatetimeIndex.offset`` is deprecated. Use ``DatetimeIndex.freq`` instead (:issue:`20716`)
- Floor division between an integer ndarray and a :class:`Timedelta` is deprecated. Divide by :attr:`Timedelta.value` instead (:issue:`19761`)
- Setting ``PeriodIndex.freq`` (which was not guaranteed to work correctly) is deprecated. Use :meth:`PeriodIndex.asfreq` instead (:issue:`20678`)
- ``Index.get_duplicates()`` is deprecated and will be removed in a future version (:issue:`20239`)
- The previous default behavior of negative indices in ``Categorical.take`` is deprecated. In a future version it will change from meaning missing values to meaning positional indices from the right. The future behavior is consistent with :meth:`Series.take` (:issue:`20664`).
- Passing multiple axes to the ``axis`` parameter in :func:`DataFrame.dropna` has been deprecated and will be removed in a future version (:issue:`20987`)


.. _whatsnew_0230.prior_deprecations:

Removal of prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Warnings against the obsolete usage ``Categorical(codes, categories)``, which were emitted for instance when the first two arguments to ``Categorical()`` had different dtypes, and recommended the use of ``Categorical.from_codes``, have now been removed (:issue:`8074`)
- The ``levels`` and ``labels`` attributes of a ``MultiIndex`` can no longer be set directly (:issue:`4039`).
- ``pd.tseries.util.pivot_annual`` has been removed (deprecated since v0.19). Use ``pivot_table`` instead (:issue:`18370`)
- ``pd.tseries.util.isleapyear`` has been removed (deprecated since v0.19). Use ``.is_leap_year`` property in Datetime-likes instead (:issue:`18370`)
- ``pd.ordered_merge`` has been removed (deprecated since v0.19). Use ``pd.merge_ordered`` instead (:issue:`18459`)
- The ``SparseList`` class has been removed (:issue:`14007`)
- The ``pandas.io.wb`` and ``pandas.io.data`` stub modules have been removed (:issue:`13735`)
- ``Categorical.from_array`` has been removed (:issue:`13854`)
- The ``freq`` and ``how`` parameters have been removed from the ``rolling``/``expanding``/``ewm`` methods of DataFrame
  and Series (deprecated since v0.18). Instead, resample before calling the methods. (:issue:`18601` & :issue:`18668`)
- ``DatetimeIndex.to_datetime``, ``Timestamp.to_datetime``, ``PeriodIndex.to_datetime``, and ``Index.to_datetime`` have been removed (:issue:`8254`, :issue:`14096`, :issue:`14113`)
- :func:`read_csv` has dropped the ``skip_footer`` parameter (:issue:`13386`)
- :func:`read_csv` has dropped the ``as_recarray`` parameter (:issue:`13373`)
- :func:`read_csv` has dropped the ``buffer_lines`` parameter (:issue:`13360`)
- :func:`read_csv` has dropped the ``compact_ints`` and ``use_unsigned`` parameters (:issue:`13323`)
- The ``Timestamp`` class has dropped the ``offset`` attribute in favor of ``freq`` (:issue:`13593`)
- The ``Series``, ``Categorical``, and ``Index`` classes have dropped the ``reshape`` method (:issue:`13012`)
- ``pandas.tseries.frequencies.get_standard_freq`` has been removed in favor of ``pandas.tseries.frequencies.to_offset(freq).rule_code`` (:issue:`13874`)
- The ``freqstr`` keyword has been removed from ``pandas.tseries.frequencies.to_offset`` in favor of ``freq`` (:issue:`13874`)
- The ``Panel4D`` and ``PanelND`` classes have been removed (:issue:`13776`)
- The ``Panel`` class has dropped the ``to_long`` and ``toLong`` methods (:issue:`19077`)
- The options ``display.line_with`` and ``display.height`` are removed in favor of ``display.width`` and ``display.max_rows`` respectively (:issue:`4391`, :issue:`19107`)
- The ``labels`` attribute of the ``Categorical`` class has been removed in favor of :attr:`Categorical.codes` (:issue:`7768`)
- The ``flavor`` parameter have been removed from func:`to_sql` method (:issue:`13611`)
- The modules ``pandas.tools.hashing`` and ``pandas.util.hashing`` have been removed (:issue:`16223`)
- The top-level functions ``pd.rolling_*``, ``pd.expanding_*`` and ``pd.ewm*`` have been removed (Deprecated since v0.18).
  Instead, use the DataFrame/Series methods :attr:`~DataFrame.rolling`, :attr:`~DataFrame.expanding` and :attr:`~DataFrame.ewm` (:issue:`18723`)
- Imports from ``pandas.core.common`` for functions such as ``is_datetime64_dtype`` are now removed. These are located in ``pandas.api.types``. (:issue:`13634`, :issue:`19769`)
- The ``infer_dst`` keyword in :meth:`Series.tz_localize`, :meth:`DatetimeIndex.tz_localize`
  and :class:`DatetimeIndex` have been removed. ``infer_dst=True`` is equivalent to
  ``ambiguous='infer'``, and ``infer_dst=False`` to ``ambiguous='raise'`` (:issue:`7963`).
- When ``.resample()`` was changed from an eager to a lazy operation, like ``.groupby()`` in v0.18.0, we put in place compatibility (with a ``FutureWarning``),
  so operations would continue to work. This is now fully removed, so a ``Resampler`` will no longer forward compat operations (:issue:`20554`)
- Remove long deprecated ``axis=None`` parameter from ``.replace()`` (:issue:`20271`)

.. _whatsnew_0230.performance:

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Indexers on ``Series`` or ``DataFrame`` no longer create a reference cycle (:issue:`17956`)
- Added a keyword argument, ``cache``, to :func:`to_datetime` that improved the performance of converting duplicate datetime arguments (:issue:`11665`)
- :class:`DateOffset` arithmetic performance is improved (:issue:`18218`)
- Converting a ``Series`` of ``Timedelta`` objects to days, seconds, etc... sped up through vectorization of underlying methods (:issue:`18092`)
- Improved performance of ``.map()`` with a ``Series/dict`` input (:issue:`15081`)
- The overridden ``Timedelta`` properties of days, seconds and microseconds have been removed, leveraging their built-in Python versions instead (:issue:`18242`)
- ``Series`` construction will reduce the number of copies made of the input data in certain cases (:issue:`17449`)
- Improved performance of :func:`Series.dt.date` and :func:`DatetimeIndex.date` (:issue:`18058`)
- Improved performance of :func:`Series.dt.time` and :func:`DatetimeIndex.time` (:issue:`18461`)
- Improved performance of :func:`IntervalIndex.symmetric_difference()` (:issue:`18475`)
- Improved performance of ``DatetimeIndex`` and ``Series`` arithmetic operations with Business-Month and Business-Quarter frequencies (:issue:`18489`)
- :func:`Series` / :func:`DataFrame` tab completion limits to 100 values, for better performance. (:issue:`18587`)
- Improved performance of :func:`DataFrame.median` with ``axis=1`` when bottleneck is not installed (:issue:`16468`)
- Improved performance of :func:`MultiIndex.get_loc` for large indexes, at the cost of a reduction in performance for small ones (:issue:`18519`)
- Improved performance of :func:`MultiIndex.remove_unused_levels` when there are no unused levels, at the cost of a reduction in performance when there are (:issue:`19289`)
- Improved performance of :func:`Index.get_loc` for non-unique indexes (:issue:`19478`)
- Improved performance of pairwise ``.rolling()`` and ``.expanding()`` with ``.cov()`` and ``.corr()`` operations (:issue:`17917`)
- Improved performance of :func:`pandas.core.groupby.GroupBy.rank` (:issue:`15779`)
- Improved performance of variable ``.rolling()`` on ``.min()`` and ``.max()`` (:issue:`19521`)
- Improved performance of :func:`pandas.core.groupby.GroupBy.ffill` and :func:`pandas.core.groupby.GroupBy.bfill` (:issue:`11296`)
- Improved performance of :func:`pandas.core.groupby.GroupBy.any` and :func:`pandas.core.groupby.GroupBy.all` (:issue:`15435`)
- Improved performance of :func:`pandas.core.groupby.GroupBy.pct_change` (:issue:`19165`)
- Improved performance of :func:`Series.isin` in the case of categorical dtypes (:issue:`20003`)
- Improved performance of ``getattr(Series, attr)`` when the Series has certain index types. This manifested in slow printing of large Series with a ``DatetimeIndex`` (:issue:`19764`)
- Fixed a performance regression for :func:`GroupBy.nth` and :func:`GroupBy.last` with some object columns (:issue:`19283`)
- Improved performance of :func:`pandas.core.arrays.Categorical.from_codes` (:issue:`18501`)

.. _whatsnew_0230.docs:

Documentation changes
~~~~~~~~~~~~~~~~~~~~~

Thanks to all of the contributors who participated in the pandas Documentation
Sprint, which took place on March 10th. We had about 500 participants from over
30 locations across the world. You should notice that many of the
:ref:`API docstrings <api>` have greatly improved.

There were too many simultaneous contributions to include a release note for each
improvement, but this `GitHub search`_ should give you an idea of how many docstrings
were improved.

Special thanks to `Marc Garcia`_ for organizing the sprint. For more information,
read the `NumFOCUS blogpost`_ recapping the sprint.

.. _GitHub search: https://github.com/pandas-dev/pandas/pulls?utf8=%E2%9C%93&q=is%3Apr+label%3ADocs+created%3A2018-03-10..2018-03-15+
.. _NumFOCUS blogpost: https://www.numfocus.org/blog/worldwide-pandas-sprint/
.. _Marc Garcia: https://github.com/datapythonista

- Changed spelling of "numpy" to "NumPy", and "python" to "Python". (:issue:`19017`)
- Consistency when introducing code samples, using either colon or period.
  Rewrote some sentences for greater clarity, added more dynamic references
  to functions, methods and classes.
  (:issue:`18941`, :issue:`18948`, :issue:`18973`, :issue:`19017`)
- Added a reference to :func:`DataFrame.assign` in the concatenate section of the merging documentation (:issue:`18665`)

.. _whatsnew_0230.bug_fixes:

Bug fixes
~~~~~~~~~

Categorical
^^^^^^^^^^^

.. warning::

   A class of bugs were introduced in pandas 0.21 with ``CategoricalDtype`` that
   affects the correctness of operations like ``merge``, ``concat``, and
   indexing when comparing multiple unordered ``Categorical`` arrays that have
   the same categories, but in a different order. We highly recommend upgrading
   or manually aligning your categories before doing these operations.

- Bug in ``Categorical.equals`` returning the wrong result when comparing two
  unordered ``Categorical`` arrays with the same categories, but in a different
  order (:issue:`16603`)
- Bug in :func:`pandas.api.types.union_categoricals` returning the wrong result
  when for unordered categoricals with the categories in a different order.
  This affected :func:`pandas.concat` with Categorical data (:issue:`19096`).
- Bug in :func:`pandas.merge` returning the wrong result when joining on an
  unordered ``Categorical`` that had the same categories but in a different
  order (:issue:`19551`)
- Bug in :meth:`CategoricalIndex.get_indexer` returning the wrong result when
  ``target`` was an unordered ``Categorical`` that had the same categories as
  ``self`` but in a different order (:issue:`19551`)
- Bug in :meth:`Index.astype` with a categorical dtype where the resultant index is not converted to a :class:`CategoricalIndex` for all types of index (:issue:`18630`)
- Bug in :meth:`Series.astype` and ``Categorical.astype()`` where an existing categorical data does not get updated (:issue:`10696`, :issue:`18593`)
- Bug in :meth:`Series.str.split` with ``expand=True`` incorrectly raising an IndexError on empty strings (:issue:`20002`).
- Bug in :class:`Index` constructor with ``dtype=CategoricalDtype(...)`` where ``categories`` and ``ordered`` are not maintained (:issue:`19032`)
- Bug in :class:`Series` constructor with scalar and ``dtype=CategoricalDtype(...)`` where ``categories`` and ``ordered`` are not maintained (:issue:`19565`)
- Bug in ``Categorical.__iter__`` not converting to Python types (:issue:`19909`)
- Bug in :func:`pandas.factorize` returning the unique codes for the ``uniques``. This now returns a ``Categorical`` with the same dtype as the input (:issue:`19721`)
- Bug in :func:`pandas.factorize` including an item for missing values in the ``uniques`` return value (:issue:`19721`)
- Bug in :meth:`Series.take` with categorical data interpreting ``-1`` in ``indices`` as missing value markers, rather than the last element of the Series (:issue:`20664`)

Datetimelike
^^^^^^^^^^^^

- Bug in :func:`Series.__sub__` subtracting a non-nanosecond ``np.datetime64`` object from a ``Series`` gave incorrect results (:issue:`7996`)
- Bug in :class:`DatetimeIndex`, :class:`TimedeltaIndex` addition and subtraction of zero-dimensional integer arrays gave incorrect results (:issue:`19012`)
- Bug in :class:`DatetimeIndex` and :class:`TimedeltaIndex` where adding or subtracting an array-like of ``DateOffset`` objects either raised (``np.array``, ``pd.Index``) or broadcast incorrectly (``pd.Series``) (:issue:`18849`)
- Bug in :func:`Series.__add__` adding Series with dtype ``timedelta64[ns]`` to a timezone-aware ``DatetimeIndex`` incorrectly dropped timezone information (:issue:`13905`)
- Adding a ``Period`` object to a ``datetime`` or ``Timestamp`` object will now correctly raise a ``TypeError`` (:issue:`17983`)
- Bug in :class:`Timestamp` where comparison with an array of ``Timestamp`` objects would result in a ``RecursionError`` (:issue:`15183`)
- Bug in :class:`Series` floor-division where operating on a scalar ``timedelta`` raises an exception (:issue:`18846`)
- Bug in :class:`DatetimeIndex` where the repr was not showing high-precision time values at the end of a day (e.g., 23:59:59.999999999) (:issue:`19030`)
- Bug in ``.astype()`` to non-ns timedelta units would hold the incorrect dtype (:issue:`19176`, :issue:`19223`, :issue:`12425`)
- Bug in subtracting :class:`Series` from ``NaT`` incorrectly returning ``NaT`` (:issue:`19158`)
- Bug in :func:`Series.truncate` which raises ``TypeError`` with a monotonic ``PeriodIndex`` (:issue:`17717`)
- Bug in :func:`~DataFrame.pct_change` using ``periods`` and ``freq`` returned different length outputs (:issue:`7292`)
- Bug in comparison of :class:`DatetimeIndex` against ``None`` or ``datetime.date`` objects raising ``TypeError`` for ``==`` and ``!=`` comparisons instead of all-``False`` and all-``True``, respectively (:issue:`19301`)
- Bug in :class:`Timestamp` and :func:`to_datetime` where a string representing a barely out-of-bounds timestamp would be incorrectly rounded down instead of raising ``OutOfBoundsDatetime`` (:issue:`19382`)
- Bug in :func:`Timestamp.floor` :func:`DatetimeIndex.floor` where time stamps far in the future and past were not rounded correctly (:issue:`19206`)
- Bug in :func:`to_datetime` where passing an out-of-bounds datetime with ``errors='coerce'`` and ``utc=True`` would raise ``OutOfBoundsDatetime`` instead of parsing to ``NaT`` (:issue:`19612`)
- Bug in :class:`DatetimeIndex` and :class:`TimedeltaIndex` addition and subtraction where name of the returned object was not always set consistently. (:issue:`19744`)
- Bug in :class:`DatetimeIndex` and :class:`TimedeltaIndex` addition and subtraction where operations with numpy arrays raised ``TypeError`` (:issue:`19847`)
- Bug in :class:`DatetimeIndex` and :class:`TimedeltaIndex` where setting the ``freq`` attribute was not fully supported (:issue:`20678`)

Timedelta
^^^^^^^^^

- Bug in :func:`Timedelta.__mul__` where multiplying by ``NaT`` returned ``NaT`` instead of raising a ``TypeError`` (:issue:`19819`)
- Bug in :class:`Series` with ``dtype='timedelta64[ns]'`` where addition or subtraction of ``TimedeltaIndex`` had results cast to ``dtype='int64'`` (:issue:`17250`)
- Bug in :class:`Series` with ``dtype='timedelta64[ns]'`` where addition or subtraction of ``TimedeltaIndex`` could return a ``Series`` with an incorrect name (:issue:`19043`)
- Bug in :func:`Timedelta.__floordiv__` and :func:`Timedelta.__rfloordiv__` dividing by many incompatible numpy objects was incorrectly allowed (:issue:`18846`)
- Bug where dividing a scalar timedelta-like object with :class:`TimedeltaIndex` performed the reciprocal operation (:issue:`19125`)
- Bug in :class:`TimedeltaIndex` where division by a ``Series`` would return a ``TimedeltaIndex`` instead of a ``Series`` (:issue:`19042`)
- Bug in :func:`Timedelta.__add__`, :func:`Timedelta.__sub__` where adding or subtracting a ``np.timedelta64`` object would return another ``np.timedelta64`` instead of a ``Timedelta`` (:issue:`19738`)
- Bug in :func:`Timedelta.__floordiv__`, :func:`Timedelta.__rfloordiv__` where operating with a ``Tick`` object would raise a ``TypeError`` instead of returning a numeric value (:issue:`19738`)
- Bug in :func:`Period.asfreq` where periods near ``datetime(1, 1, 1)`` could be converted incorrectly (:issue:`19643`, :issue:`19834`)
- Bug in :func:`Timedelta.total_seconds()` causing precision errors, for example ``Timedelta('30S').total_seconds()==30.000000000000004`` (:issue:`19458`)
- Bug in :func:`Timedelta.__rmod__` where operating with a ``numpy.timedelta64`` returned a ``timedelta64`` object instead of a ``Timedelta`` (:issue:`19820`)
- Multiplication of :class:`TimedeltaIndex` by ``TimedeltaIndex`` will now raise ``TypeError`` instead of raising ``ValueError`` in cases of length mismatch (:issue:`19333`)
- Bug in indexing a :class:`TimedeltaIndex` with a ``np.timedelta64`` object which was raising a ``TypeError`` (:issue:`20393`)


Timezones
^^^^^^^^^

- Bug in creating a ``Series`` from an array that contains both tz-naive and tz-aware values will result in a ``Series`` whose dtype is tz-aware instead of object (:issue:`16406`)
- Bug in comparison of timezone-aware :class:`DatetimeIndex` against ``NaT`` incorrectly raising ``TypeError`` (:issue:`19276`)
- Bug in :meth:`DatetimeIndex.astype` when converting between timezone aware dtypes, and converting from timezone aware to naive (:issue:`18951`)
- Bug in comparing :class:`DatetimeIndex`, which failed to raise ``TypeError`` when attempting to compare timezone-aware and timezone-naive datetimelike objects (:issue:`18162`)
- Bug in localization of a naive, datetime string in a ``Series`` constructor with a ``datetime64[ns, tz]`` dtype (:issue:`174151`)
- :func:`Timestamp.replace` will now handle Daylight Savings transitions gracefully (:issue:`18319`)
- Bug in tz-aware :class:`DatetimeIndex` where addition/subtraction with a :class:`TimedeltaIndex` or array with ``dtype='timedelta64[ns]'`` was incorrect (:issue:`17558`)
- Bug in :func:`DatetimeIndex.insert` where inserting ``NaT`` into a timezone-aware index incorrectly raised (:issue:`16357`)
- Bug in :class:`DataFrame` constructor, where tz-aware Datetimeindex and a given column name will result in an empty ``DataFrame`` (:issue:`19157`)
- Bug in :func:`Timestamp.tz_localize` where localizing a timestamp near the minimum or maximum valid values could overflow and return a timestamp with an incorrect nanosecond value (:issue:`12677`)
- Bug when iterating over :class:`DatetimeIndex` that was localized with fixed timezone offset that rounded nanosecond precision to microseconds (:issue:`19603`)
- Bug in :func:`DataFrame.diff` that raised an ``IndexError`` with tz-aware values (:issue:`18578`)
- Bug in :func:`melt` that converted tz-aware dtypes to tz-naive (:issue:`15785`)
- Bug in :func:`Dataframe.count` that raised an ``ValueError``, if :func:`Dataframe.dropna` was called for a single column with timezone-aware values. (:issue:`13407`)

Offsets
^^^^^^^

- Bug in :class:`WeekOfMonth` and :class:`Week` where addition and subtraction did not roll correctly (:issue:`18510`, :issue:`18672`, :issue:`18864`)
- Bug in :class:`WeekOfMonth` and :class:`LastWeekOfMonth` where default keyword arguments for constructor raised ``ValueError`` (:issue:`19142`)
- Bug in :class:`FY5253Quarter`, :class:`LastWeekOfMonth` where rollback and rollforward behavior was inconsistent with addition and subtraction behavior (:issue:`18854`)
- Bug in :class:`FY5253` where ``datetime`` addition and subtraction incremented incorrectly for dates on the year-end but not normalized to midnight (:issue:`18854`)
- Bug in :class:`FY5253` where date offsets could incorrectly raise an ``AssertionError`` in arithmetic operations (:issue:`14774`)

Numeric
^^^^^^^
- Bug in :class:`Series` constructor with an int or float list where specifying ``dtype=str``, ``dtype='str'`` or ``dtype='U'`` failed to convert the data elements to strings (:issue:`16605`)
- Bug in :class:`Index` multiplication and division methods where operating with a ``Series`` would return an ``Index`` object instead of a ``Series`` object (:issue:`19042`)
- Bug in the :class:`DataFrame` constructor in which data containing very large positive or very large negative numbers was causing ``OverflowError`` (:issue:`18584`)
- Bug in :class:`Index` constructor with ``dtype='uint64'`` where int-like floats were not coerced to :class:`UInt64Index` (:issue:`18400`)
- Bug in :class:`DataFrame` flex arithmetic (e.g. ``df.add(other, fill_value=foo)``) with a ``fill_value`` other than ``None`` failed to raise ``NotImplementedError`` in corner cases where either the frame or ``other`` has length zero (:issue:`19522`)
- Multiplication and division of numeric-dtyped :class:`Index` objects with timedelta-like scalars returns ``TimedeltaIndex`` instead of raising ``TypeError`` (:issue:`19333`)
- Bug where ``NaN`` was returned instead of 0 by :func:`Series.pct_change` and :func:`DataFrame.pct_change` when ``fill_method`` is not ``None`` (:issue:`19873`)

Strings
^^^^^^^
- Bug in :func:`Series.str.get` with a dictionary in the values and the index not in the keys, raising ``KeyError`` (:issue:`20671`)


Indexing
^^^^^^^^

- Bug in :class:`Index` construction from list of mixed type tuples (:issue:`18505`)
- Bug in :func:`Index.drop` when passing a list of both tuples and non-tuples (:issue:`18304`)
- Bug in :func:`DataFrame.drop`, :meth:`Panel.drop`, :meth:`Series.drop`, :meth:`Index.drop` where no ``KeyError`` is raised when dropping a non-existent element from an axis that contains duplicates (:issue:`19186`)
- Bug in indexing a datetimelike ``Index`` that raised ``ValueError`` instead of ``IndexError`` (:issue:`18386`).
- :func:`Index.to_series` now accepts ``index`` and ``name`` kwargs (:issue:`18699`)
- :func:`DatetimeIndex.to_series` now accepts ``index`` and ``name`` kwargs (:issue:`18699`)
- Bug in indexing non-scalar value from ``Series`` having non-unique ``Index`` will return value flattened (:issue:`17610`)
- Bug in indexing with iterator containing only missing keys, which raised no error (:issue:`20748`)
- Fixed inconsistency in ``.ix`` between list and scalar keys when the index has integer dtype and does not include the desired keys (:issue:`20753`)
- Bug in ``__setitem__`` when indexing a :class:`DataFrame` with a 2-d boolean ndarray (:issue:`18582`)
- Bug in ``str.extractall`` when there were no matches empty :class:`Index` was returned instead of appropriate :class:`MultiIndex` (:issue:`19034`)
- Bug in :class:`IntervalIndex` where empty and purely NA data was constructed inconsistently depending on the construction method (:issue:`18421`)
- Bug in :func:`IntervalIndex.symmetric_difference` where the symmetric difference with a non-``IntervalIndex`` did not raise (:issue:`18475`)
- Bug in :class:`IntervalIndex` where set operations that returned an empty ``IntervalIndex`` had the wrong dtype (:issue:`19101`)
- Bug in :meth:`DataFrame.drop_duplicates` where no ``KeyError`` is raised when passing in columns that don't exist on the ``DataFrame`` (:issue:`19726`)
- Bug in ``Index`` subclasses constructors that ignore unexpected keyword arguments (:issue:`19348`)
- Bug in :meth:`Index.difference` when taking difference of an ``Index`` with itself (:issue:`20040`)
- Bug in :meth:`DataFrame.first_valid_index` and :meth:`DataFrame.last_valid_index` in presence of entire rows of NaNs in the middle of values (:issue:`20499`).
- Bug in :class:`IntervalIndex` where some indexing operations were not supported for overlapping or non-monotonic ``uint64`` data (:issue:`20636`)
- Bug in ``Series.is_unique`` where extraneous output in stderr is shown if Series contains objects with ``__ne__`` defined (:issue:`20661`)
- Bug in ``.loc`` assignment with a single-element list-like incorrectly assigns as a list (:issue:`19474`)
- Bug in partial string indexing on a ``Series/DataFrame`` with a monotonic decreasing ``DatetimeIndex`` (:issue:`19362`)
- Bug in performing in-place operations on a ``DataFrame`` with a duplicate ``Index`` (:issue:`17105`)
- Bug in :meth:`IntervalIndex.get_loc` and :meth:`IntervalIndex.get_indexer` when used with an :class:`IntervalIndex` containing a single interval (:issue:`17284`, :issue:`20921`)
- Bug in ``.loc`` with a ``uint64`` indexer (:issue:`20722`)

MultiIndex
^^^^^^^^^^

- Bug in :func:`MultiIndex.__contains__` where non-tuple keys would return ``True`` even if they had been dropped (:issue:`19027`)
- Bug in :func:`MultiIndex.set_labels` which would cause casting (and potentially clipping) of the new labels if the ``level`` argument is not 0 or a list like [0, 1, ... ]  (:issue:`19057`)
- Bug in :func:`MultiIndex.get_level_values` which would return an invalid index on level of ints with missing values (:issue:`17924`)
- Bug in :func:`MultiIndex.unique` when called on empty :class:`MultiIndex` (:issue:`20568`)
- Bug in :func:`MultiIndex.unique` which would not preserve level names (:issue:`20570`)
- Bug in :func:`MultiIndex.remove_unused_levels` which would fill nan values (:issue:`18417`)
- Bug in :func:`MultiIndex.from_tuples` which would fail to take zipped tuples in python3 (:issue:`18434`)
- Bug in :func:`MultiIndex.get_loc` which would fail to automatically cast values between float and int (:issue:`18818`, :issue:`15994`)
- Bug in :func:`MultiIndex.get_loc` which would cast boolean to integer labels (:issue:`19086`)
- Bug in :func:`MultiIndex.get_loc` which would fail to locate keys containing ``NaN`` (:issue:`18485`)
- Bug in :func:`MultiIndex.get_loc` in large :class:`MultiIndex`, would fail when levels had different dtypes (:issue:`18520`)
- Bug in indexing where nested indexers having only numpy arrays are handled incorrectly (:issue:`19686`)


IO
^^

- :func:`read_html` now rewinds seekable IO objects after parse failure, before attempting to parse with a new parser. If a parser errors and the object is non-seekable, an informative error is raised suggesting the use of a different parser (:issue:`17975`)
- :meth:`DataFrame.to_html` now has an option to add an id to the leading ``<table>`` tag (:issue:`8496`)
- Bug in :func:`read_msgpack` with a non existent file is passed in Python 2 (:issue:`15296`)
- Bug in :func:`read_csv` where a ``MultiIndex`` with duplicate columns was not being mangled appropriately (:issue:`18062`)
- Bug in :func:`read_csv` where missing values were not being handled properly when ``keep_default_na=False`` with dictionary ``na_values`` (:issue:`19227`)
- Bug in :func:`read_csv` causing heap corruption on 32-bit, big-endian architectures (:issue:`20785`)
- Bug in :func:`read_sas` where a file with 0 variables gave an ``AttributeError`` incorrectly. Now it gives an ``EmptyDataError`` (:issue:`18184`)
- Bug in :func:`DataFrame.to_latex()` where pairs of braces meant to serve as invisible placeholders were escaped (:issue:`18667`)
- Bug in :func:`DataFrame.to_latex()` where a ``NaN`` in a ``MultiIndex`` would cause an ``IndexError`` or incorrect output (:issue:`14249`)
- Bug in :func:`DataFrame.to_latex()` where a non-string index-level name would result in an ``AttributeError`` (:issue:`19981`)
- Bug in :func:`DataFrame.to_latex()` where the combination of an index name and the ``index_names=False`` option would result in incorrect output (:issue:`18326`)
- Bug in :func:`DataFrame.to_latex()` where a ``MultiIndex`` with an empty string as its name would result in incorrect output (:issue:`18669`)
- Bug in :func:`DataFrame.to_latex()` where missing space characters caused wrong escaping and produced non-valid latex in some cases (:issue:`20859`)
- Bug in :func:`read_json` where large numeric values were causing an ``OverflowError`` (:issue:`18842`)
- Bug in :func:`DataFrame.to_parquet` where an exception was raised if the write destination is S3 (:issue:`19134`)
- :class:`Interval` now supported in :func:`DataFrame.to_excel` for all Excel file types (:issue:`19242`)
- :class:`Timedelta` now supported in :func:`DataFrame.to_excel` for all Excel file types (:issue:`19242`, :issue:`9155`, :issue:`19900`)
- Bug in :meth:`pandas.io.stata.StataReader.value_labels` raising an ``AttributeError`` when called on very old files. Now returns an empty dict (:issue:`19417`)
- Bug in :func:`read_pickle` when unpickling objects with :class:`TimedeltaIndex` or :class:`Float64Index` created with pandas prior to version 0.20 (:issue:`19939`)
- Bug in :meth:`pandas.io.json.json_normalize` where sub-records are not properly normalized if any sub-records values are NoneType (:issue:`20030`)
- Bug in ``usecols`` parameter in :func:`read_csv` where error is not raised correctly when passing a string. (:issue:`20529`)
- Bug in :func:`HDFStore.keys` when reading a file with a soft link causes exception (:issue:`20523`)
- Bug in :func:`HDFStore.select_column` where a key which is not a valid store raised an ``AttributeError`` instead of a ``KeyError`` (:issue:`17912`)

Plotting
^^^^^^^^

- Better error message when attempting to plot but matplotlib is not installed (:issue:`19810`).
- :func:`DataFrame.plot` now raises a ``ValueError`` when the ``x`` or ``y`` argument is improperly formed (:issue:`18671`)
- Bug in :func:`DataFrame.plot` when ``x`` and ``y`` arguments given as positions caused incorrect referenced columns for line, bar and area plots (:issue:`20056`)
- Bug in formatting tick labels with ``datetime.time()`` and fractional seconds (:issue:`18478`).
- :meth:`Series.plot.kde` has exposed the args ``ind`` and ``bw_method`` in the docstring (:issue:`18461`). The argument ``ind`` may now also be an integer (number of sample points).
- :func:`DataFrame.plot` now supports multiple columns to the ``y`` argument (:issue:`19699`)


GroupBy/resample/rolling
^^^^^^^^^^^^^^^^^^^^^^^^

- Bug when grouping by a single column and aggregating with a class like ``list`` or ``tuple`` (:issue:`18079`)
- Fixed regression in :func:`DataFrame.groupby` which would not emit an error when called with a tuple key not in the index (:issue:`18798`)
- Bug in :func:`DataFrame.resample` which silently ignored unsupported (or mistyped) options for ``label``, ``closed`` and ``convention`` (:issue:`19303`)
- Bug in :func:`DataFrame.groupby` where tuples were interpreted as lists of keys rather than as keys (:issue:`17979`, :issue:`18249`)
- Bug in :func:`DataFrame.groupby` where aggregation by ``first``/``last``/``min``/``max`` was causing timestamps to lose precision (:issue:`19526`)
- Bug in :func:`DataFrame.transform` where particular aggregation functions were being incorrectly cast to match the dtype(s) of the grouped data (:issue:`19200`)
- Bug in :func:`DataFrame.groupby` passing the ``on=`` kwarg, and subsequently using ``.apply()`` (:issue:`17813`)
- Bug in :func:`DataFrame.resample().aggregate <pandas.core.resample.Resampler.aggregate>` not raising a ``KeyError`` when aggregating a non-existent column (:issue:`16766`, :issue:`19566`)
- Bug in :func:`DataFrameGroupBy.cumsum` and :func:`DataFrameGroupBy.cumprod` when ``skipna`` was passed (:issue:`19806`)
- Bug in :func:`DataFrame.resample` that dropped timezone information (:issue:`13238`)
- Bug in :func:`DataFrame.groupby` where transformations using ``np.all`` and ``np.any`` were raising a ``ValueError`` (:issue:`20653`)
- Bug in :func:`DataFrame.resample` where ``ffill``, ``bfill``, ``pad``, ``backfill``, ``fillna``, ``interpolate``, and ``asfreq`` were ignoring ``loffset``. (:issue:`20744`)
- Bug in :func:`DataFrame.groupby` when applying a function that has mixed data types and the user supplied function can fail on the grouping column (:issue:`20949`)
- Bug in :func:`DataFrameGroupBy.rolling().apply() <pandas.core.window.Rolling.apply>` where operations performed against the associated :class:`DataFrameGroupBy` object could impact the inclusion of the grouped item(s) in the result (:issue:`14013`)

Sparse
^^^^^^

- Bug in which creating a :class:`SparseDataFrame` from a dense ``Series`` or an unsupported type raised an uncontrolled exception (:issue:`19374`)
- Bug in :class:`SparseDataFrame.to_csv` causing exception (:issue:`19384`)
- Bug in :class:`SparseSeries.memory_usage` which caused segfault by accessing non sparse elements (:issue:`19368`)
- Bug in constructing a :class:`SparseArray`: if ``data`` is a scalar and ``index`` is defined it will coerce to ``float64`` regardless of scalar's dtype. (:issue:`19163`)

Reshaping
^^^^^^^^^

- Bug in :func:`DataFrame.merge` where referencing a ``CategoricalIndex`` by name, where the ``by`` kwarg would ``KeyError`` (:issue:`20777`)
- Bug in :func:`DataFrame.stack` which fails trying to sort mixed type levels under Python 3 (:issue:`18310`)
- Bug in :func:`DataFrame.unstack` which casts int to float if ``columns`` is a ``MultiIndex`` with unused levels (:issue:`17845`)
- Bug in :func:`DataFrame.unstack` which raises an error if ``index`` is a ``MultiIndex`` with unused labels on the unstacked level (:issue:`18562`)
- Fixed construction of a :class:`Series` from a ``dict`` containing ``NaN`` as key (:issue:`18480`)
- Fixed construction of a :class:`DataFrame` from a ``dict`` containing ``NaN`` as key (:issue:`18455`)
- Disabled construction of a :class:`Series` where len(index) > len(data) = 1, which previously would broadcast the data item, and now raises a ``ValueError`` (:issue:`18819`)
- Suppressed error in the construction of a :class:`DataFrame` from a ``dict`` containing scalar values when the corresponding keys are not included in the passed index (:issue:`18600`)

- Fixed (changed from ``object`` to ``float64``) dtype of :class:`DataFrame` initialized with axes, no data, and ``dtype=int`` (:issue:`19646`)
- Bug in :func:`Series.rank` where ``Series`` containing ``NaT`` modifies the ``Series`` inplace (:issue:`18521`)
- Bug in :func:`cut` which fails when using readonly arrays (:issue:`18773`)
- Bug in :func:`DataFrame.pivot_table` which fails when the ``aggfunc`` arg is of type string.  The behavior is now consistent with other methods like ``agg`` and ``apply`` (:issue:`18713`)
- Bug in :func:`DataFrame.merge` in which merging using ``Index`` objects as vectors raised an Exception (:issue:`19038`)
- Bug in :func:`DataFrame.stack`, :func:`DataFrame.unstack`, :func:`Series.unstack` which were not returning subclasses (:issue:`15563`)
- Bug in timezone comparisons, manifesting as a conversion of the index to UTC in ``.concat()`` (:issue:`18523`)
- Bug in :func:`concat` when concatenating sparse and dense series it returns only a ``SparseDataFrame``. Should be a ``DataFrame``. (:issue:`18914`, :issue:`18686`, and :issue:`16874`)
- Improved error message for :func:`DataFrame.merge` when there is no common merge key (:issue:`19427`)
- Bug in :func:`DataFrame.join` which does an ``outer`` instead of a ``left`` join when being called with multiple DataFrames and some have non-unique indices (:issue:`19624`)
- :func:`Series.rename` now accepts ``axis`` as a kwarg (:issue:`18589`)
- Bug in :func:`~DataFrame.rename` where an Index of same-length tuples was converted to a MultiIndex (:issue:`19497`)
- Comparisons between :class:`Series` and :class:`Index` would return a ``Series`` with an incorrect name, ignoring the ``Index``'s name attribute (:issue:`19582`)
- Bug in :func:`qcut` where datetime and timedelta data with ``NaT`` present raised a ``ValueError`` (:issue:`19768`)
- Bug in :func:`DataFrame.iterrows`, which would infers strings not compliant to `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ to datetimes (:issue:`19671`)
- Bug in :class:`Series` constructor with ``Categorical`` where a ``ValueError`` is not raised when an index of different length is given (:issue:`19342`)
- Bug in :meth:`DataFrame.astype` where column metadata is lost when converting to categorical or a dictionary of dtypes (:issue:`19920`)
- Bug in :func:`cut` and :func:`qcut` where timezone information was dropped (:issue:`19872`)
- Bug in :class:`Series` constructor with a ``dtype=str``, previously raised in some cases (:issue:`19853`)
- Bug in :func:`get_dummies`, and :func:`select_dtypes`, where duplicate column names caused incorrect behavior (:issue:`20848`)
- Bug in :func:`isna`, which cannot handle ambiguous typed lists (:issue:`20675`)
- Bug in :func:`concat` which raises an error when concatenating TZ-aware dataframes and all-NaT dataframes (:issue:`12396`)
- Bug in :func:`concat` which raises an error when concatenating empty TZ-aware series (:issue:`18447`)

Other
^^^^^

- Improved error message when attempting to use a Python keyword as an identifier in a ``numexpr`` backed query (:issue:`18221`)
- Bug in accessing a :func:`pandas.get_option`, which raised ``KeyError`` rather than ``OptionError`` when looking up a non-existent option key in some cases (:issue:`19789`)
- Bug in :func:`testing.assert_series_equal` and :func:`testing.assert_frame_equal` for Series or DataFrames with differing unicode data (:issue:`20503`)

.. _whatsnew_0.23.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.22.0..v0.23.0
.. _whatsnew_131:

What's new in 1.3.1 (July 25, 2021)
-----------------------------------

These are the changes in pandas 1.3.1. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_131.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Pandas could not be built on PyPy (:issue:`42355`)
- :class:`DataFrame` constructed with an older version of pandas could not be unpickled (:issue:`42345`)
- Performance regression in constructing a :class:`DataFrame` from a dictionary of dictionaries (:issue:`42248`)
- Fixed regression in :meth:`DataFrame.agg` dropping values when the DataFrame had an Extension Array dtype, a duplicate index, and ``axis=1`` (:issue:`42380`)
- Fixed regression in :meth:`DataFrame.astype` changing the order of noncontiguous data (:issue:`42396`)
- Performance regression in :class:`DataFrame` in reduction operations requiring casting such as :meth:`DataFrame.mean` on integer data (:issue:`38592`)
- Performance regression in :meth:`DataFrame.to_dict` and :meth:`Series.to_dict` when ``orient`` argument one of "records", "dict", or "split" (:issue:`42352`)
- Fixed regression in indexing with a ``list`` subclass incorrectly raising ``TypeError`` (:issue:`42433`, :issue:`42461`)
- Fixed regression in :meth:`DataFrame.isin` and :meth:`Series.isin` raising ``TypeError`` with nullable data containing at least one missing value (:issue:`42405`)
- Regression in :func:`concat` between objects with bool dtype and integer dtype casting to object instead of to integer (:issue:`42092`)
- Bug in :class:`Series` constructor not accepting a ``dask.Array`` (:issue:`38645`)
- Fixed regression for ``SettingWithCopyWarning`` displaying incorrect stacklevel (:issue:`42570`)
- Fixed regression for :func:`merge_asof` raising ``KeyError`` when one of the ``by`` columns is in the index (:issue:`34488`)
- Fixed regression in :func:`to_datetime` returning pd.NaT for inputs that produce duplicated values, when ``cache=True`` (:issue:`42259`)
- Fixed regression in :meth:`SeriesGroupBy.value_counts` that resulted in an ``IndexError`` when called on a Series with one row (:issue:`42618`)

.. ---------------------------------------------------------------------------

.. _whatsnew_131.bug_fixes:

Bug fixes
~~~~~~~~~
- Fixed bug in :meth:`DataFrame.transpose` dropping values when the DataFrame had an Extension Array dtype and a duplicate index (:issue:`42380`)
- Fixed bug in :meth:`DataFrame.to_xml` raising ``KeyError`` when called with ``index=False`` and an offset index (:issue:`42458`)
- Fixed bug in :meth:`.Styler.set_sticky` not handling index names correctly for single index columns case (:issue:`42537`)
- Fixed bug in :meth:`DataFrame.copy` failing to consolidate blocks in the result (:issue:`42579`)

.. ---------------------------------------------------------------------------

.. _whatsnew_131.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.0..v1.3.1

.. _whatsnew_103:

What's new in 1.0.3 (March 17, 2020)
------------------------------------

These are the changes in pandas 1.0.3. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_103.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in ``resample.agg`` when the underlying data is non-writeable (:issue:`31710`)
- Fixed regression in :class:`DataFrame` exponentiation with reindexing (:issue:`32685`)

.. _whatsnew_103.bug_fixes:

Bug fixes
~~~~~~~~~

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.2..v1.0.3
.. _whatsnew_0242:

What's new in 0.24.2 (March 12, 2019)
-------------------------------------

.. warning::

   The 0.24.x series of releases will be the last to support Python 2. Future feature
   releases will support Python 3 only. See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

{{ header }}

These are the changes in pandas 0.24.2. See :ref:`release` for a full changelog
including other versions of pandas.

.. _whatsnew_0242.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`DataFrame.all` and :meth:`DataFrame.any` where ``bool_only=True`` was ignored (:issue:`25101`)
- Fixed issue in ``DataFrame`` construction with passing a mixed list of mixed types could segfault. (:issue:`25075`)
- Fixed regression in :meth:`DataFrame.apply` causing ``RecursionError`` when ``dict``-like classes were passed as argument. (:issue:`25196`)
- Fixed regression in :meth:`DataFrame.replace` where ``regex=True`` was only replacing patterns matching the start of the string (:issue:`25259`)
- Fixed regression in :meth:`DataFrame.duplicated()`, where empty dataframe was not returning a boolean dtyped Series. (:issue:`25184`)
- Fixed regression in :meth:`Series.min` and :meth:`Series.max` where ``numeric_only=True`` was ignored when the ``Series`` contained ``Categorical`` data (:issue:`25299`)
- Fixed regression in subtraction between :class:`Series` objects with ``datetime64[ns]`` dtype incorrectly raising ``OverflowError`` when the ``Series`` on the right contains null values (:issue:`25317`)
- Fixed regression in :class:`TimedeltaIndex` where ``np.sum(index)`` incorrectly returned a zero-dimensional object instead of a scalar (:issue:`25282`)
- Fixed regression in ``IntervalDtype`` construction where passing an incorrect string with 'Interval' as a prefix could result in a ``RecursionError``. (:issue:`25338`)
- Fixed regression in creating a period-dtype array from a read-only NumPy array of period objects. (:issue:`25403`)
- Fixed regression in :class:`Categorical`, where constructing it from a categorical ``Series`` and an explicit ``categories=`` that differed from that in the ``Series`` created an invalid object which could trigger segfaults. (:issue:`25318`)
- Fixed regression in :func:`to_timedelta` losing precision when converting floating data to ``Timedelta`` data (:issue:`25077`).
- Fixed pip installing from source into an environment without NumPy (:issue:`25193`)
- Fixed regression in :meth:`DataFrame.replace` where large strings of numbers would be coerced into ``int64``, causing an ``OverflowError`` (:issue:`25616`)
- Fixed regression in :func:`factorize` when passing a custom ``na_sentinel`` value with ``sort=True`` (:issue:`25409`).
- Fixed regression in :meth:`DataFrame.to_csv` writing duplicate line endings with gzip compress (:issue:`25311`)

.. _whatsnew_0242.bug_fixes:

Bug fixes
~~~~~~~~~

**I/O**

- Better handling of terminal printing when the terminal dimensions are not known (:issue:`25080`)
- Bug in reading a HDF5 table-format ``DataFrame`` created in Python 2, in Python 3 (:issue:`24925`)
- Bug in reading a JSON with ``orient='table'`` generated by :meth:`DataFrame.to_json` with ``index=False`` (:issue:`25170`)
- Bug where float indexes could have misaligned values when printing (:issue:`25061`)

**Categorical**

- Bug where calling :meth:`Series.replace` on categorical data could return a ``Series`` with incorrect dimensions (:issue:`24971`)
-

**Reshaping**

- Bug in :meth:`~pandas.core.groupby.GroupBy.transform` where applying a function to a timezone aware column would return a timezone naive result (:issue:`24198`)
- Bug in :func:`DataFrame.join` when joining on a timezone aware :class:`DatetimeIndex` (:issue:`23931`)

**Visualization**

- Bug in :meth:`Series.plot` where a secondary y axis could not be set to log scale (:issue:`25545`)

**Other**

- Bug in :meth:`Series.is_unique` where single occurrences of ``NaN`` were not considered unique (:issue:`25180`)
- Bug in :func:`merge` when merging an empty ``DataFrame`` with an ``Int64`` column or a non-empty ``DataFrame`` with an ``Int64`` column that is all ``NaN`` (:issue:`25183`)
- Bug in ``IntervalTree`` where a ``RecursionError`` occurs upon construction due to an overflow when adding endpoints, which also causes :class:`IntervalIndex` to crash during indexing operations (:issue:`25485`)
- Bug in :attr:`Series.size` raising for some extension-array-backed ``Series``, rather than returning the size (:issue:`25580`)
- Bug in resampling raising for nullable integer-dtype columns (:issue:`25580`)

.. _whatsnew_0242.contributors:

Contributors
~~~~~~~~~~~~

.. Including the contributors hardcoded for this release, as backporting with
   MeeseeksDev loses the commit authors

A total of 25 people contributed patches to this release. People with a "+" by their names contributed a patch for the first time.

* Albert Villanova del Moral
* Arno Veenstra +
* chris-b1
* Devin Petersohn +
* EternalLearner42 +
* Flavien Lambert +
* gfyoung
* Gioia Ballin
* jbrockmendel
* Jeff Reback
* Jeremy Schendel
* Johan von Forstner +
* Joris Van den Bossche
* Josh
* Justin Zheng
* Kendall Masse
* Matthew Roeschke
* Max Bolingbroke +
* rbenes +
* Sterling Paramore +
* Tao He +
* Thomas A Caswell
* Tom Augspurger
* Vibhu Agarwal +
* William Ayd
* Zach Angell

.. _whatsnew_061:

Version 0.6.1 (December 13, 2011)
---------------------------------

New features
~~~~~~~~~~~~
- Can append single rows (as Series) to a DataFrame
- Add Spearman and Kendall rank :ref:`correlation <computation.correlation>`
  options to Series.corr and DataFrame.corr (:issue:`428`)
- :ref:`Added <indexing.basics.get_value>` ``get_value`` and ``set_value`` methods to
  Series, DataFrame, and Panel for very low-overhead access (>2x faster in many
  cases) to scalar elements (:issue:`437`, :issue:`438`). ``set_value`` is capable of
  producing an enlarged object.
- Add PyQt table widget to sandbox (:issue:`435`)
- DataFrame.align can :ref:`accept Series arguments <basics.align.frame.series>`
  and an :ref:`axis option <basics.df_join>` (:issue:`461`)
- Implement new :ref:`SparseArray <sparse.array>` and ``SparseList``
  data structures. SparseSeries now derives from SparseArray (:issue:`463`)
- :ref:`Better console printing options <basics.console_output>` (:issue:`453`)
- Implement fast :ref:`data ranking <computation.ranking>` for Series and
  DataFrame, fast versions of scipy.stats.rankdata (:issue:`428`)
- Implement ``DataFrame.from_items`` alternate
  constructor (:issue:`444`)
- DataFrame.convert_objects method for :ref:`inferring better dtypes <basics.cast>`
  for object columns (:issue:`302`)
- Add :ref:`rolling_corr_pairwise <window.corr_pairwise>` function for
  computing Panel of correlation matrices (:issue:`189`)
- Add :ref:`margins <reshaping.pivot.margins>` option to :ref:`pivot_table
  <reshaping.pivot>` for computing subgroup aggregates (:issue:`114`)
- Add ``Series.from_csv`` function (:issue:`482`)
- :ref:`Can pass <window.cov_corr>` DataFrame/DataFrame and
  DataFrame/Series to rolling_corr/rolling_cov (GH #462)
- MultiIndex.get_level_values can :ref:`accept the level name <advanced.get_level_values>`

Performance improvements
~~~~~~~~~~~~~~~~~~~~~~~~

- Improve memory usage of ``DataFrame.describe`` (do not copy data
  unnecessarily) (PR #425)

- Optimize scalar value lookups in the general case by 25% or more in Series
  and DataFrame

- Fix performance regression in cross-sectional count in DataFrame, affecting
  DataFrame.dropna speed
- Column deletion in DataFrame copies no data (computes views on blocks) (GH
  #158)



.. _whatsnew_0.6.1.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.6.0..v0.6.1
.. _whatsnew_0232:

What's new in 0.23.2 (July 5, 2018)
-----------------------------------

{{ header }}


This is a minor bug-fix release in the 0.23.x series and includes some small regression fixes
and bug fixes. We recommend that all users upgrade to this version.

.. note::

   pandas 0.23.2 is first pandas release that's compatible with
   Python 3.7 (:issue:`20552`)

.. warning::

   Starting January 1, 2019, pandas feature releases will support Python 3 only.
   See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

.. contents:: What's new in v0.23.2
    :local:
    :backlinks: none

.. _whatsnew_0232.enhancements:

Logical reductions over entire DataFrame
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:meth:`DataFrame.all` and :meth:`DataFrame.any` now accept ``axis=None`` to reduce over all axes to a scalar (:issue:`19976`)

.. ipython:: python

   df = pd.DataFrame({"A": [1, 2], "B": [True, False]})
   df.all(axis=None)


This also provides compatibility with NumPy 1.15, which now dispatches to ``DataFrame.all``.
With NumPy 1.15 and pandas 0.23.1 or earlier, :func:`numpy.all` will no longer reduce over every axis:

.. code-block:: python

   >>> # NumPy 1.15, pandas 0.23.1
   >>> np.any(pd.DataFrame({"A": [False], "B": [False]}))
   A    False
   B    False
   dtype: bool

With pandas 0.23.2, that will correctly return False, as it did with NumPy < 1.15.

.. ipython:: python

   np.any(pd.DataFrame({"A": [False], "B": [False]}))


.. _whatsnew_0232.fixed_regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`to_csv` when handling file-like object incorrectly (:issue:`21471`)
- Re-allowed duplicate level names of a ``MultiIndex``. Accessing a level that has a duplicate name by name still raises an error (:issue:`19029`).
- Bug in both :meth:`DataFrame.first_valid_index` and :meth:`Series.first_valid_index` raised for a row index having duplicate values (:issue:`21441`)
- Fixed printing of DataFrames with hierarchical columns with long names (:issue:`21180`)
- Fixed regression in :meth:`~DataFrame.reindex` and :meth:`~DataFrame.groupby`
  with a MultiIndex or multiple keys that contains categorical datetime-like values (:issue:`21390`).
- Fixed regression in unary negative operations with object dtype (:issue:`21380`)
- Bug in :meth:`Timestamp.ceil` and :meth:`Timestamp.floor` when timestamp is a multiple of the rounding frequency (:issue:`21262`)
- Fixed regression in :func:`to_clipboard` that defaulted to copying dataframes with space delimited instead of tab delimited (:issue:`21104`)


Build changes
~~~~~~~~~~~~~

- The source and binary distributions no longer include test data files, resulting in smaller download sizes. Tests relying on these data files will be skipped when using ``pandas.test()``. (:issue:`19320`)

.. _whatsnew_0232.bug_fixes:

Bug fixes
~~~~~~~~~

**Conversion**

- Bug in constructing :class:`Index` with an iterator or generator (:issue:`21470`)
- Bug in :meth:`Series.nlargest` for signed and unsigned integer dtypes when the minimum value is present (:issue:`21426`)

**Indexing**

- Bug in :meth:`Index.get_indexer_non_unique` with categorical key (:issue:`21448`)
- Bug in comparison operations for :class:`MultiIndex` where error was raised on equality / inequality comparison involving a MultiIndex with ``nlevels == 1`` (:issue:`21149`)
- Bug in :meth:`DataFrame.drop` behaviour is not consistent for unique and non-unique indexes (:issue:`21494`)
- Bug in :func:`DataFrame.duplicated` with a large number of columns causing a 'maximum recursion depth exceeded' (:issue:`21524`).

**I/O**

- Bug in :func:`read_csv` that caused it to incorrectly raise an error when ``nrows=0``, ``low_memory=True``, and ``index_col`` was not ``None`` (:issue:`21141`)
- Bug in :func:`json_normalize` when formatting the ``record_prefix`` with integer columns (:issue:`21536`)

**Categorical**

- Bug in rendering :class:`Series` with ``Categorical`` dtype in rare conditions under Python 2.7 (:issue:`21002`)

**Timezones**

- Bug in :class:`Timestamp` and :class:`DatetimeIndex` where passing a :class:`Timestamp` localized after a DST transition would return a datetime before the DST transition (:issue:`20854`)
- Bug in comparing :class:`DataFrame` with tz-aware :class:`DatetimeIndex` columns with a DST transition that raised a ``KeyError`` (:issue:`19970`)
- Bug in :meth:`DatetimeIndex.shift` where an ``AssertionError`` would raise when shifting across DST (:issue:`8616`)
- Bug in :class:`Timestamp` constructor where passing an invalid timezone offset designator (``Z``) would not raise a ``ValueError`` (:issue:`8910`)
- Bug in :meth:`Timestamp.replace` where replacing at a DST boundary would retain an incorrect offset (:issue:`7825`)
- Bug in :meth:`DatetimeIndex.reindex` when reindexing a tz-naive and tz-aware :class:`DatetimeIndex` (:issue:`8306`)
- Bug in :meth:`DatetimeIndex.resample` when downsampling across a DST boundary (:issue:`8531`)

**Timedelta**

- Bug in :class:`Timedelta` where non-zero timedeltas shorter than 1 microsecond were considered False (:issue:`21484`)

.. _whatsnew_0.23.2.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.23.1..v0.23.2
.. _whatsnew_0241:

What's new in 0.24.1 (February 3, 2019)
---------------------------------------

.. warning::

   The 0.24.x series of releases will be the last to support Python 2. Future feature
   releases will support Python 3 only. See `Dropping Python 2.7 <https://pandas.pydata.org/pandas-docs/version/0.24/install.html#install-dropping-27>`_ for more.

{{ header }}

These are the changes in pandas 0.24.1. See :ref:`release` for a full changelog
including other versions of pandas. See :ref:`whatsnew_0240` for the 0.24.0 changelog.

.. _whatsnew_0241.api:

API changes
~~~~~~~~~~~

Changing the ``sort`` parameter for :class:`Index` set operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The default ``sort`` value for :meth:`Index.union` has changed from ``True`` to ``None`` (:issue:`24959`).
The default *behavior*, however, remains the same: the result is sorted, unless

1. ``self`` and ``other`` are identical
2. ``self`` or ``other`` is empty
3. ``self`` or ``other`` contain values that can not be compared (a ``RuntimeWarning`` is raised).

This change will allow ``sort=True`` to mean "always sort" in a future release.

The same change applies to :meth:`Index.difference` and :meth:`Index.symmetric_difference`, which
would not sort the result when the values could not be compared.

The ``sort`` option for :meth:`Index.intersection` has changed in three ways.

1. The default has changed from ``True`` to ``False``, to restore the
   pandas 0.23.4 and earlier behavior of not sorting by default.
2. The behavior of ``sort=True`` can now be obtained with ``sort=None``.
   This will sort the result only if the values in ``self`` and ``other``
   are not identical.
3. The value ``sort=True`` is no longer allowed. A future version of pandas
   will properly support ``sort=True`` meaning "always sort".

.. _whatsnew_0241.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fixed regression in :meth:`DataFrame.to_dict` with ``records`` orient raising an
  ``AttributeError`` when the ``DataFrame`` contained more than 255 columns, or
  wrongly converting column names that were not valid python identifiers (:issue:`24939`, :issue:`24940`).
- Fixed regression in :func:`read_sql` when passing certain queries with MySQL/pymysql (:issue:`24988`).
- Fixed regression in :class:`Index.intersection` incorrectly sorting the values by default (:issue:`24959`).
- Fixed regression in :func:`merge` when merging an empty ``DataFrame`` with multiple timezone-aware columns on one of the timezone-aware columns (:issue:`25014`).
- Fixed regression in :meth:`Series.rename_axis` and :meth:`DataFrame.rename_axis` where passing ``None`` failed to remove the axis name (:issue:`25034`)
- Fixed regression in :func:`to_timedelta` with ``box=False`` incorrectly returning a ``datetime64`` object instead of a ``timedelta64`` object (:issue:`24961`)
- Fixed regression where custom hashable types could not be used as column keys in :meth:`DataFrame.set_index` (:issue:`24969`)

.. _whatsnew_0241.bug_fixes:

Bug fixes
~~~~~~~~~

**Reshaping**

- Bug in :meth:`DataFrame.groupby` with :class:`Grouper` when there is a time change (DST) and grouping frequency is ``'1d'`` (:issue:`24972`)

**Visualization**

- Fixed the warning for implicitly registered matplotlib converters not showing. See :ref:`whatsnew_0211.converters` for more (:issue:`24963`).

**Other**

- Fixed AttributeError when printing a DataFrame's HTML repr after accessing the IPython config object (:issue:`25036`)

.. _whatsnew_0.241.contributors:

Contributors
~~~~~~~~~~~~

.. Including the contributors hardcoded for this release, as backporting with
   MeeseeksDev loses the commit authors

A total of 7 people contributed patches to this release. People with a "+" by their names contributed a patch for the first time.

* Alex Buchkovsky
* Roman Yurchak
* h-vetinari
* jbrockmendel
* Jeremy Schendel
* Joris Van den Bossche
* Tom Augspurger
.. _whatsnew_112:

What's new in 1.1.2 (September 8, 2020)
---------------------------------------

These are the changes in pandas 1.1.2. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_112.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Regression in :meth:`DatetimeIndex.intersection` incorrectly raising ``AssertionError`` when intersecting against a list (:issue:`35876`)
- Fix regression in updating a column inplace (e.g. using ``df['col'].fillna(.., inplace=True)``) (:issue:`35731`)
- Fix regression in :meth:`DataFrame.append` mixing tz-aware and tz-naive datetime columns (:issue:`35460`)
- Performance regression for :meth:`RangeIndex.format` (:issue:`35712`)
- Regression where :meth:`MultiIndex.get_loc` would return a slice spanning the full index when passed an empty list (:issue:`35878`)
- Fix regression in invalid cache after an indexing operation; this can manifest when setting which does not update the data (:issue:`35521`)
- Regression in :meth:`DataFrame.replace` where a ``TypeError`` would be raised when attempting to replace elements of type :class:`Interval` (:issue:`35931`)
- Fix regression in pickle roundtrip of the ``closed`` attribute of :class:`IntervalIndex` (:issue:`35658`)
- Fixed regression in :meth:`DataFrameGroupBy.agg` where a ``ValueError: buffer source array is read-only`` would be raised when the underlying array is read-only (:issue:`36014`)
- Fixed regression in :meth:`Series.groupby.rolling` number of levels of :class:`MultiIndex` in input was compressed to one (:issue:`36018`)
- Fixed regression in :class:`DataFrameGroupBy` on an empty :class:`DataFrame` (:issue:`36197`)

.. ---------------------------------------------------------------------------

.. _whatsnew_112.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in :meth:`DataFrame.eval` with ``object`` dtype column binary operations (:issue:`35794`)
- Bug in :class:`Series` constructor raising a ``TypeError`` when constructing sparse datetime64 dtypes (:issue:`35762`)
- Bug in :meth:`DataFrame.apply` with ``result_type="reduce"`` returning with incorrect index (:issue:`35683`)
- Bug in :meth:`Series.astype` and :meth:`DataFrame.astype` not respecting the ``errors`` argument when set to ``"ignore"`` for extension dtypes (:issue:`35471`)
- Bug in :meth:`DateTimeIndex.format` and :meth:`PeriodIndex.format` with ``name=True`` setting the first item to ``"None"`` where it should be ``""`` (:issue:`35712`)
- Bug in :meth:`Float64Index.__contains__` incorrectly raising ``TypeError`` instead of returning ``False`` (:issue:`35788`)
- Bug in :class:`Series` constructor incorrectly raising a ``TypeError`` when passed an ordered set (:issue:`36044`)
- Bug in :meth:`Series.dt.isocalendar` and :meth:`DatetimeIndex.isocalendar` that returned incorrect year for certain dates (:issue:`36032`)
- Bug in :class:`DataFrame` indexing returning an incorrect :class:`Series` in some cases when the series has been altered and a cache not invalidated (:issue:`33675`)
- Bug in :meth:`DataFrame.corr` causing subsequent indexing lookups to be incorrect (:issue:`35882`)
- Bug in :meth:`import_optional_dependency` returning incorrect package names in cases where package name is different from import name (:issue:`35948`)
- Bug when setting empty :class:`DataFrame` column to a :class:`Series` in preserving name of index in frame (:issue:`31368`)

.. ---------------------------------------------------------------------------

.. _whatsnew_112.other:

Other
~~~~~
- :meth:`factorize` now supports ``na_sentinel=None`` to include NaN in the uniques of the values and remove ``dropna`` keyword which was unintentionally exposed to public facing API in 1.1 version from :meth:`factorize` (:issue:`35667`)
- :meth:`DataFrame.plot` and :meth:`Series.plot` raise ``UserWarning`` about usage of ``FixedFormatter`` and ``FixedLocator`` (:issue:`35684` and :issue:`35945`)

.. ---------------------------------------------------------------------------

.. _whatsnew_112.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.1.1..v1.1.2
.. _whatsnew_134:

What's new in 1.3.4 (October 17, 2021)
--------------------------------------

These are the changes in pandas 1.3.4. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_134.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Fixed regression in :meth:`DataFrame.convert_dtypes` incorrectly converts byte strings to strings (:issue:`43183`)
- Fixed regression in :meth:`.GroupBy.agg` where it was failing silently with mixed data types along ``axis=1`` and :class:`MultiIndex` (:issue:`43209`)
- Fixed regression in :func:`merge` with integer and ``NaN`` keys failing with ``outer`` merge (:issue:`43550`)
- Fixed regression in :meth:`DataFrame.corr` raising ``ValueError`` with ``method="spearman"`` on 32-bit platforms (:issue:`43588`)
- Fixed performance regression in :meth:`MultiIndex.equals` (:issue:`43549`)
- Fixed performance regression in :meth:`.GroupBy.first` and :meth:`.GroupBy.last` with :class:`StringDtype` (:issue:`41596`)
- Fixed regression in :meth:`Series.cat.reorder_categories` failing to update the categories on the ``Series`` (:issue:`43232`)
- Fixed regression in :meth:`Series.cat.categories` setter failing to update the categories on the ``Series`` (:issue:`43334`)
- Fixed regression in :func:`read_csv` raising ``UnicodeDecodeError`` exception when ``memory_map=True`` (:issue:`43540`)
- Fixed regression in :meth:`DataFrame.explode` raising ``AssertionError`` when ``column`` is any scalar which is not a string (:issue:`43314`)
- Fixed regression in :meth:`Series.aggregate` attempting to pass ``args`` and ``kwargs`` multiple times to the user supplied ``func`` in certain cases (:issue:`43357`)
- Fixed regression when iterating over a :class:`DataFrame.groupby.rolling` object causing the resulting DataFrames to have an incorrect index if the input groupings were not sorted (:issue:`43386`)
- Fixed regression in :meth:`DataFrame.groupby.rolling.cov` and :meth:`DataFrame.groupby.rolling.corr` computing incorrect results if the input groupings were not sorted (:issue:`43386`)

.. ---------------------------------------------------------------------------

.. _whatsnew_134.bug_fixes:

Bug fixes
~~~~~~~~~
- Fixed bug in :meth:`pandas.DataFrame.groupby.rolling` and :class:`pandas.api.indexers.FixedForwardWindowIndexer` leading to segfaults and window endpoints being mixed across groups (:issue:`43267`)
- Fixed bug in :meth:`.GroupBy.mean` with datetimelike values including ``NaT`` values returning incorrect results (:issue:`43132`)
- Fixed bug in :meth:`Series.aggregate` not passing the first ``args`` to the user supplied ``func`` in certain cases (:issue:`43357`)
- Fixed memory leaks in :meth:`Series.rolling.quantile` and :meth:`Series.rolling.median` (:issue:`43339`)

.. ---------------------------------------------------------------------------

.. _whatsnew_134.other:

Other
~~~~~
- The minimum version of Cython needed to compile pandas is now ``0.29.24`` (:issue:`43729`)

.. ---------------------------------------------------------------------------

.. _whatsnew_134.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.3..v1.3.4
.. _whatsnew_0140:

Version 0.14.0 (May 31 , 2014)
------------------------------

{{ header }}


This is a major release from 0.13.1 and includes a small number of API changes, several new features,
enhancements, and performance improvements along with a large number of bug fixes. We recommend that all
users upgrade to this version.

- Highlights include:

  - Officially support Python 3.4
  - SQL interfaces updated to use ``sqlalchemy``, See :ref:`Here<whatsnew_0140.sql>`.
  - Display interface changes, See :ref:`Here<whatsnew_0140.display>`
  - MultiIndexing Using Slicers, See :ref:`Here<whatsnew_0140.slicers>`.
  - Ability to join a singly-indexed DataFrame with a MultiIndexed DataFrame, see :ref:`Here <merging.join_on_mi>`
  - More consistency in groupby results and more flexible groupby specifications, See :ref:`Here<whatsnew_0140.groupby>`
  - Holiday calendars are now supported in ``CustomBusinessDay``, see :ref:`Here <timeseries.holiday>`
  - Several improvements in plotting functions, including: hexbin, area and pie plots, see :ref:`Here<whatsnew_0140.plotting>`.
  - Performance doc section on I/O operations, See :ref:`Here <io.perf>`

- :ref:`Other Enhancements <whatsnew_0140.enhancements>`

- :ref:`API Changes <whatsnew_0140.api>`

- :ref:`Text Parsing API Changes <whatsnew_0140.parsing>`

- :ref:`Groupby API Changes <whatsnew_0140.groupby>`

- :ref:`Performance Improvements <whatsnew_0140.performance>`

- :ref:`Prior Deprecations <whatsnew_0140.prior_deprecations>`

- :ref:`Deprecations <whatsnew_0140.deprecations>`

- :ref:`Known Issues <whatsnew_0140.knownissues>`

- :ref:`Bug Fixes <whatsnew_0140.bug_fixes>`

.. warning::

   In 0.14.0 all ``NDFrame`` based containers have undergone significant internal refactoring. Before that each block of
   homogeneous data had its own labels and extra care was necessary to keep those in sync with the parent container's labels.
   This should not have any visible user/API behavior changes (:issue:`6745`)

.. _whatsnew_0140.api:

API changes
~~~~~~~~~~~

- ``read_excel`` uses 0 as the default sheet (:issue:`6573`)
- ``iloc`` will now accept out-of-bounds indexers for slices, e.g. a value that exceeds the length of the object being
  indexed. These will be excluded. This will make pandas conform more with python/numpy indexing of out-of-bounds
  values. A single indexer that is out-of-bounds and drops the dimensions of the object will still raise
  ``IndexError`` (:issue:`6296`, :issue:`6299`). This could result in an empty axis (e.g. an empty DataFrame being returned)

  .. ipython:: python

     dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))
     dfl
     dfl.iloc[:, 2:3]
     dfl.iloc[:, 1:3]
     dfl.iloc[4:6]

  These are out-of-bounds selections

  .. code-block:: python

     >>> dfl.iloc[[4, 5, 6]]
     IndexError: positional indexers are out-of-bounds

     >>> dfl.iloc[:, 4]
     IndexError: single positional indexer is out-of-bounds

- Slicing with negative start, stop & step values handles corner cases better (:issue:`6531`):

  - ``df.iloc[:-len(df)]`` is now empty
  - ``df.iloc[len(df)::-1]`` now enumerates all elements in reverse

- The :meth:`DataFrame.interpolate` keyword ``downcast`` default has been changed from ``infer`` to
  ``None``. This is to preserve the original dtype unless explicitly requested otherwise (:issue:`6290`).
- When converting a dataframe to HTML it used to return ``Empty DataFrame``. This special case has
  been removed, instead a header with the column names is returned (:issue:`6062`).
- ``Series`` and ``Index`` now internally share more common operations, e.g. ``factorize(),nunique(),value_counts()`` are
  now supported on ``Index`` types as well. The ``Series.weekday`` property from is removed
  from Series for API consistency. Using a ``DatetimeIndex/PeriodIndex`` method on a Series will now raise a ``TypeError``.
  (:issue:`4551`, :issue:`4056`, :issue:`5519`, :issue:`6380`, :issue:`7206`).

- Add ``is_month_start``, ``is_month_end``, ``is_quarter_start``, ``is_quarter_end``, ``is_year_start``, ``is_year_end`` accessors for ``DateTimeIndex`` / ``Timestamp`` which return a boolean array of whether the timestamp(s) are at the start/end of the month/quarter/year defined by the frequency of the ``DateTimeIndex`` / ``Timestamp`` (:issue:`4565`, :issue:`6998`)

- Local variable usage has changed in
  :func:`pandas.eval`/:meth:`DataFrame.eval`/:meth:`DataFrame.query`
  (:issue:`5987`). For the :class:`~pandas.DataFrame` methods, two things have
  changed

  - Column names are now given precedence over locals
  - Local variables must be referred to explicitly. This means that even if
    you have a local variable that is *not* a column you must still refer to
    it with the ``'@'`` prefix.
  - You can have an expression like ``df.query('@a < a')`` with no complaints
    from ``pandas`` about ambiguity of the name ``a``.
  - The top-level :func:`pandas.eval` function does not allow you use the
    ``'@'`` prefix and provides you with an error message telling you so.
  - ``NameResolutionError`` was removed because it isn't necessary anymore.

- Define and document the order of column vs index names in query/eval (:issue:`6676`)
- ``concat`` will now concatenate mixed Series and DataFrames using the Series name
  or numbering columns as needed (:issue:`2385`). See :ref:`the docs <merging.mixed_ndims>`
- Slicing and advanced/boolean indexing operations on ``Index`` classes as well
  as :meth:`Index.delete` and :meth:`Index.drop` methods will no longer change the type of the
  resulting index (:issue:`6440`, :issue:`7040`)

  .. ipython:: python

     i = pd.Index([1, 2, 3, 'a', 'b', 'c'])
     i[[0, 1, 2]]
     i.drop(['a', 'b', 'c'])

  Previously, the above operation would return ``Int64Index``.  If you'd like
  to do this manually, use :meth:`Index.astype`

  .. ipython:: python

     i[[0, 1, 2]].astype(np.int_)

- ``set_index`` no longer converts MultiIndexes to an Index of tuples. For example,
  the old behavior returned an Index in this case (:issue:`6459`):

  .. ipython:: python
     :suppress:

     np.random.seed(1234)
     from itertools import product
     tuples = list(product(('a', 'b'), ('c', 'd')))
     mi = pd.MultiIndex.from_tuples(tuples)
     df_multi = pd.DataFrame(np.random.randn(4, 2), index=mi)
     tuple_ind = pd.Index(tuples, tupleize_cols=False)
     df_multi.index

  .. ipython:: python

     # Old behavior, casted MultiIndex to an Index
     tuple_ind
     df_multi.set_index(tuple_ind)

     # New behavior
     mi
     df_multi.set_index(mi)

  This also applies when passing multiple indices to ``set_index``:

  .. ipython:: python

    @suppress
    df_multi.index = tuple_ind

    # Old output, 2-level MultiIndex of tuples
    df_multi.set_index([df_multi.index, df_multi.index])

    @suppress
    df_multi.index = mi

    # New output, 4-level MultiIndex
    df_multi.set_index([df_multi.index, df_multi.index])

- ``pairwise`` keyword was added to the statistical moment functions
  ``rolling_cov``, ``rolling_corr``, ``ewmcov``, ``ewmcorr``,
  ``expanding_cov``, ``expanding_corr`` to allow the calculation of moving
  window covariance and correlation matrices (:issue:`4950`). See
  :ref:`Computing rolling pairwise covariances and correlations
  <window.corr_pairwise>` in the docs.

  .. code-block:: ipython

     In [1]: df = pd.DataFrame(np.random.randn(10, 4), columns=list('ABCD'))

     In [4]: covs = pd.rolling_cov(df[['A', 'B', 'C']],
       ....:                       df[['B', 'C', 'D']],
       ....:                       5,
       ....:                       pairwise=True)


     In [5]: covs[df.index[-1]]
     Out[5]:
               B         C         D
     A  0.035310  0.326593 -0.505430
     B  0.137748 -0.006888 -0.005383
     C -0.006888  0.861040  0.020762

- ``Series.iteritems()`` is now lazy (returns an iterator rather than a list). This was the documented behavior prior to 0.14. (:issue:`6760`)

- Added ``nunique`` and ``value_counts`` functions to ``Index`` for counting unique elements. (:issue:`6734`)
- ``stack`` and ``unstack`` now raise a ``ValueError`` when the ``level`` keyword refers
  to a non-unique item in the ``Index`` (previously raised a ``KeyError``). (:issue:`6738`)
- drop unused order argument from ``Series.sort``; args now are in the same order as ``Series.order``;
  add ``na_position`` arg to conform to ``Series.order`` (:issue:`6847`)
- default sorting algorithm for ``Series.order`` is now ``quicksort``, to conform with ``Series.sort``
  (and numpy defaults)
- add ``inplace`` keyword to ``Series.order/sort`` to make them inverses (:issue:`6859`)
- ``DataFrame.sort`` now places NaNs at the beginning or end of the sort according to the ``na_position`` parameter. (:issue:`3917`)
- accept ``TextFileReader`` in ``concat``, which was affecting a common user idiom (:issue:`6583`), this was a regression
  from 0.13.1
- Added ``factorize`` functions to ``Index`` and ``Series`` to get indexer and unique values (:issue:`7090`)
- ``describe`` on a DataFrame with a mix of Timestamp and string like objects returns a different Index (:issue:`7088`).
  Previously the index was unintentionally sorted.
- Arithmetic operations with **only** ``bool`` dtypes now give a warning indicating
  that they are evaluated in Python space for ``+``, ``-``,
  and ``*`` operations and raise for all others (:issue:`7011`, :issue:`6762`,
  :issue:`7015`, :issue:`7210`)

  .. code-block:: python

     >>> x = pd.Series(np.random.rand(10) > 0.5)
     >>> y = True
     >>> x + y  # warning generated: should do x | y instead
     UserWarning: evaluating in Python space because the '+' operator is not
     supported by numexpr for the bool dtype, use '|' instead
     >>> x / y  # this raises because it doesn't make sense
     NotImplementedError: operator '/' not implemented for bool dtypes

- In ``HDFStore``, ``select_as_multiple`` will always raise a ``KeyError``, when a key or the selector is not found (:issue:`6177`)
- ``df['col'] = value`` and ``df.loc[:,'col'] = value`` are now completely equivalent;
  previously the ``.loc`` would not necessarily coerce the dtype of the resultant series (:issue:`6149`)
- ``dtypes`` and ``ftypes`` now return a series with ``dtype=object`` on empty containers (:issue:`5740`)
- ``df.to_csv`` will now return a string of the CSV data if neither a target path nor a buffer is provided
  (:issue:`6061`)
- ``pd.infer_freq()`` will now raise a ``TypeError`` if given an invalid ``Series/Index``
  type (:issue:`6407`, :issue:`6463`)
- A tuple passed to ``DataFame.sort_index`` will be interpreted as the levels of
  the index, rather than requiring a list of tuple (:issue:`4370`)
- all offset operations now return ``Timestamp`` types (rather than datetime), Business/Week frequencies were incorrect (:issue:`4069`)
- ``to_excel`` now converts ``np.inf`` into a string representation,
  customizable by the ``inf_rep`` keyword argument (Excel has no native inf
  representation) (:issue:`6782`)
- Replace ``pandas.compat.scipy.scoreatpercentile`` with ``numpy.percentile`` (:issue:`6810`)
- ``.quantile`` on a ``datetime[ns]`` series now returns ``Timestamp`` instead
  of ``np.datetime64`` objects (:issue:`6810`)
- change ``AssertionError`` to ``TypeError`` for invalid types passed to ``concat`` (:issue:`6583`)
- Raise a ``TypeError`` when ``DataFrame`` is passed an iterator as the
  ``data`` argument (:issue:`5357`)


.. _whatsnew_0140.display:

Display changes
~~~~~~~~~~~~~~~

- The default way of printing large DataFrames has changed. DataFrames
  exceeding ``max_rows`` and/or ``max_columns`` are now displayed in a
  centrally truncated view, consistent with the printing of a
  :class:`pandas.Series` (:issue:`5603`).

  In previous versions, a DataFrame was truncated once the dimension
  constraints were reached and an ellipse (...) signaled that part of
  the data was cut off.

  .. image:: ../_static/trunc_before.png
      :alt: The previous look of truncate.

  In the current version, large DataFrames are centrally truncated,
  showing a preview of head and tail in both dimensions.

  .. image:: ../_static/trunc_after.png
     :alt: The new look.

- allow option ``'truncate'`` for ``display.show_dimensions`` to only show the dimensions if the
  frame is truncated (:issue:`6547`).

  The default for ``display.show_dimensions`` will now be ``truncate``. This is consistent with
  how Series display length.

  .. ipython:: python

     dfd = pd.DataFrame(np.arange(25).reshape(-1, 5),
                        index=[0, 1, 2, 3, 4],
                        columns=[0, 1, 2, 3, 4])

     # show dimensions since this is truncated
     with pd.option_context('display.max_rows', 2, 'display.max_columns', 2,
                            'display.show_dimensions', 'truncate'):
         print(dfd)

     # will not show dimensions since it is not truncated
     with pd.option_context('display.max_rows', 10, 'display.max_columns', 40,
                            'display.show_dimensions', 'truncate'):
         print(dfd)

- Regression in the display of a MultiIndexed Series with ``display.max_rows`` is less than the
  length of the series (:issue:`7101`)
- Fixed a bug in the HTML repr of a truncated Series or DataFrame not showing the class name with the
  ``large_repr`` set to 'info' (:issue:`7105`)
- The ``verbose`` keyword in ``DataFrame.info()``, which controls whether to shorten the ``info``
  representation, is now ``None`` by default. This will follow the global setting in
  ``display.max_info_columns``. The global setting can be overridden with ``verbose=True`` or
  ``verbose=False``.
- Fixed a bug with the ``info`` repr not honoring the ``display.max_info_columns`` setting (:issue:`6939`)
- Offset/freq info now in Timestamp __repr__ (:issue:`4553`)

.. _whatsnew_0140.parsing:

Text parsing API changes
~~~~~~~~~~~~~~~~~~~~~~~~

:func:`read_csv`/:func:`read_table` will now be noisier w.r.t invalid options rather than falling back to the ``PythonParser``.

- Raise ``ValueError`` when ``sep`` specified with
  ``delim_whitespace=True`` in :func:`read_csv`/:func:`read_table`
  (:issue:`6607`)
- Raise ``ValueError`` when ``engine='c'`` specified with unsupported
  options in :func:`read_csv`/:func:`read_table` (:issue:`6607`)
- Raise ``ValueError`` when fallback to python parser causes options to be
  ignored (:issue:`6607`)
- Produce :class:`~pandas.io.parsers.ParserWarning` on fallback to python
  parser when no options are ignored (:issue:`6607`)
- Translate ``sep='\s+'`` to ``delim_whitespace=True`` in
  :func:`read_csv`/:func:`read_table` if no other C-unsupported options
  specified (:issue:`6607`)

.. _whatsnew_0140.groupby:

GroupBy API changes
~~~~~~~~~~~~~~~~~~~

More consistent behavior for some groupby methods:

- groupby ``head`` and ``tail`` now act more like ``filter`` rather than an aggregation:

  .. ipython:: python

     df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])
     g = df.groupby('A')
     g.head(1)  # filters DataFrame

     g.apply(lambda x: x.head(1))  # used to simply fall-through

- groupby head and tail respect column selection:

  .. ipython:: python

     g[['B']].head(1)

- groupby ``nth`` now reduces by default; filtering can be achieved by passing ``as_index=False``. With an optional ``dropna`` argument to ignore
  NaN. See :ref:`the docs <groupby.nth>`.

  Reducing

  .. ipython:: python

     df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])
     g = df.groupby('A')
     g.nth(0)

     # this is equivalent to g.first()
     g.nth(0, dropna='any')

     # this is equivalent to g.last()
     g.nth(-1, dropna='any')

  Filtering

  .. ipython:: python

     gf = df.groupby('A', as_index=False)
     gf.nth(0)
     gf.nth(0, dropna='any')

- groupby will now not return the grouped column for non-cython functions (:issue:`5610`, :issue:`5614`, :issue:`6732`),
  as its already the index

  .. ipython:: python

     df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6], [5, 8]], columns=['A', 'B'])
     g = df.groupby('A')
     g.count()
     g.describe()

- passing ``as_index`` will leave the grouped column in-place (this is not change in 0.14.0)

  .. ipython:: python

     df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6], [5, 8]], columns=['A', 'B'])
     g = df.groupby('A', as_index=False)
     g.count()
     g.describe()

- Allow specification of a more complex groupby via ``pd.Grouper``, such as grouping
  by a Time and a string field simultaneously. See :ref:`the docs <groupby.specify>`. (:issue:`3794`)

- Better propagation/preservation of Series names when performing groupby
  operations:

  - ``SeriesGroupBy.agg`` will ensure that the name attribute of the original
    series is propagated to the result (:issue:`6265`).
  - If the function provided to ``GroupBy.apply`` returns a named series, the
    name of the series will be kept as the name of the column index of the
    DataFrame returned by ``GroupBy.apply`` (:issue:`6124`).  This facilitates
    ``DataFrame.stack`` operations where the name of the column index is used as
    the name of the inserted column containing the pivoted data.


.. _whatsnew_0140.sql:

SQL
~~~

The SQL reading and writing functions now support more database flavors
through SQLAlchemy (:issue:`2717`, :issue:`4163`, :issue:`5950`, :issue:`6292`).
All databases supported by SQLAlchemy can be used, such
as PostgreSQL, MySQL, Oracle, Microsoft SQL server (see documentation of
SQLAlchemy on `included dialects
<https://sqlalchemy.readthedocs.io/en/latest/dialects/index.html>`_).

The functionality of providing DBAPI connection objects will only be supported
for sqlite3 in the future. The ``'mysql'`` flavor is deprecated.

The new functions :func:`~pandas.read_sql_query` and :func:`~pandas.read_sql_table`
are introduced. The function :func:`~pandas.read_sql` is kept as a convenience
wrapper around the other two and will delegate to specific function depending on
the provided input (database table name or sql query).

In practice, you have to provide a SQLAlchemy ``engine`` to the sql functions.
To connect with SQLAlchemy you use the :func:`create_engine` function to create an engine
object from database URI. You only need to create the engine once per database you are
connecting to. For an in-memory sqlite database:

.. ipython:: python

   from sqlalchemy import create_engine
   # Create your connection.
   engine = create_engine('sqlite:///:memory:')

This ``engine`` can then be used to write or read data to/from this database:

.. ipython:: python

    df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})
    df.to_sql('db_table', engine, index=False)

You can read data from a database by specifying the table name:

.. ipython:: python

   pd.read_sql_table('db_table', engine)

or by specifying a sql query:

.. ipython:: python

   pd.read_sql_query('SELECT * FROM db_table', engine)

Some other enhancements to the sql functions include:

- support for writing the index. This can be controlled with the ``index``
  keyword (default is True).
- specify the column label to use when writing the index with ``index_label``.
- specify string columns to parse as datetimes with the ``parse_dates``
  keyword in :func:`~pandas.read_sql_query` and :func:`~pandas.read_sql_table`.

.. warning::

    Some of the existing functions or function aliases have been deprecated
    and will be removed in future versions. This includes: ``tquery``, ``uquery``,
    ``read_frame``, ``frame_query``, ``write_frame``.

.. warning::

    The support for the 'mysql' flavor when using DBAPI connection objects has been deprecated.
    MySQL will be further supported with SQLAlchemy engines (:issue:`6900`).


.. _whatsnew_0140.slicers:

Multi-indexing using slicers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In 0.14.0 we added a new way to slice MultiIndexed objects.
You can slice a MultiIndex by providing multiple indexers.

You can provide any of the selectors as if you are indexing by label, see :ref:`Selection by Label <indexing.label>`,
including slices, lists of labels, labels, and boolean indexers.

You can use ``slice(None)`` to select all the contents of *that* level. You do not need to specify all the
*deeper* levels, they will be implied as ``slice(None)``.

As usual, **both sides** of the slicers are included as this is label indexing.

See :ref:`the docs<advanced.mi_slicers>`
See also issues (:issue:`6134`, :issue:`4036`, :issue:`3057`, :issue:`2598`, :issue:`5641`, :issue:`7106`)

.. warning::

   You should specify all axes in the ``.loc`` specifier, meaning the indexer for the **index** and
   for the **columns**. Their are some ambiguous cases where the passed indexer could be mis-interpreted
   as indexing *both* axes, rather than into say the MuliIndex for the rows.

   You should do this:

  .. code-block:: python

     >>> df.loc[(slice('A1', 'A3'), ...), :]  # noqa: E901

   rather than this:

  .. code-block:: python

     >>> df.loc[(slice('A1', 'A3'), ...)]  # noqa: E901

.. warning::

   You will need to make sure that the selection axes are fully lexsorted!

.. ipython:: python

   def mklbl(prefix, n):
       return ["%s%s" % (prefix, i) for i in range(n)]

   index = pd.MultiIndex.from_product([mklbl('A', 4),
                                       mklbl('B', 2),
                                       mklbl('C', 4),
                                       mklbl('D', 2)])
   columns = pd.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'),
                                        ('b', 'foo'), ('b', 'bah')],
                                       names=['lvl0', 'lvl1'])
   df = pd.DataFrame(np.arange(len(index) * len(columns)).reshape((len(index),
                     len(columns))),
                     index=index,
                     columns=columns).sort_index().sort_index(axis=1)
   df

Basic MultiIndex slicing using slices, lists, and labels.

.. ipython:: python

   df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]

You can use a ``pd.IndexSlice`` to shortcut the creation of these slices

.. ipython:: python

   idx = pd.IndexSlice
   df.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]

It is possible to perform quite complicated selections using this method on multiple
axes at the same time.

.. ipython:: python

   df.loc['A1', (slice(None), 'foo')]
   df.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]

Using a boolean indexer you can provide selection related to the *values*.

.. ipython:: python

   mask = df[('a', 'foo')] > 200
   df.loc[idx[mask, :, ['C1', 'C3']], idx[:, 'foo']]

You can also specify the ``axis`` argument to ``.loc`` to interpret the passed
slicers on a single axis.

.. ipython:: python

   df.loc(axis=0)[:, :, ['C1', 'C3']]

Furthermore you can *set* the values using these methods

.. ipython:: python

   df2 = df.copy()
   df2.loc(axis=0)[:, :, ['C1', 'C3']] = -10
   df2

You can use a right-hand-side of an alignable object as well.

.. ipython:: python

   df2 = df.copy()
   df2.loc[idx[:, :, ['C1', 'C3']], :] = df2 * 1000
   df2

.. _whatsnew_0140.plotting:

Plotting
~~~~~~~~

- Hexagonal bin plots from ``DataFrame.plot`` with ``kind='hexbin'`` (:issue:`5478`), See :ref:`the docs<visualization.hexbin>`.
- ``DataFrame.plot`` and ``Series.plot`` now supports area plot with specifying ``kind='area'`` (:issue:`6656`), See :ref:`the docs<visualization.area_plot>`
- Pie plots from ``Series.plot`` and ``DataFrame.plot`` with ``kind='pie'`` (:issue:`6976`), See :ref:`the docs<visualization.pie>`.
- Plotting with Error Bars is now supported in the ``.plot`` method of ``DataFrame`` and ``Series`` objects (:issue:`3796`, :issue:`6834`), See :ref:`the docs<visualization.errorbars>`.
- ``DataFrame.plot`` and ``Series.plot`` now support a ``table`` keyword for plotting ``matplotlib.Table``, See :ref:`the docs<visualization.table>`. The ``table`` keyword can receive the following values.

  - ``False``: Do nothing (default).
  - ``True``: Draw a table using the ``DataFrame`` or ``Series`` called ``plot`` method. Data will be transposed to meet matplotlib's default layout.
  - ``DataFrame`` or ``Series``: Draw matplotlib.table using the passed data. The data will be drawn as displayed in print method (not transposed automatically).
    Also, helper function ``pandas.tools.plotting.table`` is added to create a table from ``DataFrame`` and ``Series``, and add it to an ``matplotlib.Axes``.

- ``plot(legend='reverse')`` will now reverse the order of legend labels for
  most plot kinds. (:issue:`6014`)
- Line plot and area plot can be stacked by ``stacked=True`` (:issue:`6656`)

- Following keywords are now acceptable for :meth:`DataFrame.plot` with ``kind='bar'`` and ``kind='barh'``:

  - ``width``: Specify the bar width. In previous versions, static value 0.5 was passed to matplotlib and it cannot be overwritten. (:issue:`6604`)
  - ``align``: Specify the bar alignment. Default is ``center`` (different from matplotlib). In previous versions, pandas passes ``align='edge'`` to matplotlib and adjust the location to ``center`` by itself, and it results ``align`` keyword is not applied as expected. (:issue:`4525`)
  - ``position``: Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1(right/top-end). Default is 0.5 (center). (:issue:`6604`)

  Because of the default ``align`` value changes, coordinates of bar plots are now located on integer values (0.0, 1.0, 2.0 ...). This is intended to make bar plot be located on the same coordinates as line plot. However, bar plot may differs unexpectedly when you manually adjust the bar location or drawing area, such as using ``set_xlim``, ``set_ylim``, etc. In this cases, please modify your script to meet with new coordinates.

- The :func:`parallel_coordinates` function now takes argument ``color``
  instead of ``colors``. A ``FutureWarning`` is raised to alert that
  the old ``colors`` argument will not be supported in a future release. (:issue:`6956`)

- The :func:`parallel_coordinates` and :func:`andrews_curves` functions now take
  positional argument ``frame`` instead of ``data``. A ``FutureWarning`` is
  raised if the old ``data`` argument is used by name. (:issue:`6956`)

- :meth:`DataFrame.boxplot` now supports ``layout`` keyword (:issue:`6769`)
- :meth:`DataFrame.boxplot` has a new keyword argument, ``return_type``. It accepts ``'dict'``,
  ``'axes'``, or ``'both'``, in which case a namedtuple with the matplotlib
  axes and a dict of matplotlib Lines is returned.


.. _whatsnew_0140.prior_deprecations:

Prior version deprecations/changes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are prior version deprecations that are taking effect as of 0.14.0.

- Remove :class:`DateRange` in favor of :class:`DatetimeIndex` (:issue:`6816`)
- Remove ``column`` keyword from ``DataFrame.sort`` (:issue:`4370`)
- Remove ``precision`` keyword from :func:`set_eng_float_format` (:issue:`395`)
- Remove ``force_unicode`` keyword from :meth:`DataFrame.to_string`,
  :meth:`DataFrame.to_latex`, and :meth:`DataFrame.to_html`; these function
  encode in unicode by default (:issue:`2224`, :issue:`2225`)
- Remove ``nanRep`` keyword from :meth:`DataFrame.to_csv` and
  :meth:`DataFrame.to_string` (:issue:`275`)
- Remove ``unique`` keyword from :meth:`HDFStore.select_column` (:issue:`3256`)
- Remove ``inferTimeRule`` keyword from :func:`Timestamp.offset` (:issue:`391`)
- Remove ``name`` keyword from :func:`get_data_yahoo` and
  :func:`get_data_google` ( `commit b921d1a <https://github.com/pandas-dev/pandas/commit/b921d1a2>`__ )
- Remove ``offset`` keyword from :class:`DatetimeIndex` constructor
  ( `commit 3136390 <https://github.com/pandas-dev/pandas/commit/3136390>`__ )
- Remove ``time_rule`` from several rolling-moment statistical functions, such
  as :func:`rolling_sum` (:issue:`1042`)
- Removed neg ``-`` boolean operations on numpy arrays in favor of inv ``~``, as this is going to
  be deprecated in numpy 1.9 (:issue:`6960`)

.. _whatsnew_0140.deprecations:

Deprecations
~~~~~~~~~~~~

- The :func:`pivot_table`/:meth:`DataFrame.pivot_table` and :func:`crosstab` functions
  now take arguments ``index`` and ``columns`` instead of ``rows`` and ``cols``.  A
  ``FutureWarning`` is raised to alert that the old ``rows`` and ``cols`` arguments
  will not be supported in a future release (:issue:`5505`)

- The :meth:`DataFrame.drop_duplicates` and :meth:`DataFrame.duplicated` methods
  now take argument ``subset`` instead of ``cols`` to better align with
  :meth:`DataFrame.dropna`.  A ``FutureWarning`` is raised to alert that the old
  ``cols`` arguments will not be supported in a future release (:issue:`6680`)

- The :meth:`DataFrame.to_csv` and :meth:`DataFrame.to_excel` functions
  now takes argument ``columns`` instead of ``cols``.  A
  ``FutureWarning`` is raised to alert that the old ``cols`` arguments
  will not be supported in a future release (:issue:`6645`)

- Indexers will warn ``FutureWarning`` when used with a scalar indexer and
  a non-floating point Index (:issue:`4892`, :issue:`6960`)

  .. code-block:: ipython

     # non-floating point indexes can only be indexed by integers / labels
     In [1]: pd.Series(1, np.arange(5))[3.0]
             pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
     Out[1]: 1

     In [2]: pd.Series(1, np.arange(5)).iloc[3.0]
             pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point
     Out[2]: 1

     In [3]: pd.Series(1, np.arange(5)).iloc[3.0:4]
             pandas/core/index.py:527: FutureWarning: slice indexers when using iloc should be integers and not floating point
     Out[3]:
             3    1
             dtype: int64

     # these are Float64Indexes, so integer or floating point is acceptable
     In [4]: pd.Series(1, np.arange(5.))[3]
     Out[4]: 1

     In [5]: pd.Series(1, np.arange(5.))[3.0]
     Out[6]: 1

- Numpy 1.9 compat w.r.t. deprecation warnings (:issue:`6960`)

- :meth:`Panel.shift` now has a function signature that matches :meth:`DataFrame.shift`.
  The old positional argument ``lags`` has been changed to a keyword argument
  ``periods`` with a default value of 1. A ``FutureWarning`` is raised if the
  old argument ``lags`` is used by name. (:issue:`6910`)
- The ``order`` keyword argument of :func:`factorize` will be removed. (:issue:`6926`).

- Remove the ``copy`` keyword from :meth:`DataFrame.xs`, :meth:`Panel.major_xs`, :meth:`Panel.minor_xs`. A view will be
  returned if possible, otherwise a copy will be made. Previously the user could think that ``copy=False`` would
  ALWAYS return a view. (:issue:`6894`)

- The :func:`parallel_coordinates` function now takes argument ``color``
  instead of ``colors``. A ``FutureWarning`` is raised to alert that
  the old ``colors`` argument will not be supported in a future release. (:issue:`6956`)

- The :func:`parallel_coordinates` and :func:`andrews_curves` functions now take
  positional argument ``frame`` instead of ``data``. A ``FutureWarning`` is
  raised if the old ``data`` argument is used by name. (:issue:`6956`)

- The support for the 'mysql' flavor when using DBAPI connection objects has been deprecated.
  MySQL will be further supported with SQLAlchemy engines (:issue:`6900`).

- The following ``io.sql`` functions have been deprecated: ``tquery``, ``uquery``, ``read_frame``, ``frame_query``, ``write_frame``.

- The ``percentile_width`` keyword argument in :meth:`~DataFrame.describe` has been deprecated.
  Use the ``percentiles`` keyword instead, which takes a list of percentiles to display. The
  default output is unchanged.

- The default return type of :func:`boxplot` will change from a dict to a matplotlib Axes
  in a future release. You can use the future behavior now by passing ``return_type='axes'``
  to boxplot.

.. _whatsnew_0140.knownissues:

Known issues
~~~~~~~~~~~~

- OpenPyXL 2.0.0 breaks backwards compatibility (:issue:`7169`)


.. _whatsnew_0140.enhancements:

Enhancements
~~~~~~~~~~~~

- DataFrame and Series will create a MultiIndex object if passed a tuples dict, See :ref:`the docs<basics.dataframe.from_dict_of_tuples>` (:issue:`3323`)

  .. ipython:: python

     pd.Series({('a', 'b'): 1, ('a', 'a'): 0,
                ('a', 'c'): 2, ('b', 'a'): 3, ('b', 'b'): 4})
     pd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},
                  ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},
                  ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},
                  ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},
                  ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}})

- Added the ``sym_diff`` method to ``Index`` (:issue:`5543`)
- ``DataFrame.to_latex`` now takes a longtable keyword, which if True will return a table in a longtable environment. (:issue:`6617`)
- Add option to turn off escaping in ``DataFrame.to_latex`` (:issue:`6472`)
- ``pd.read_clipboard`` will, if the keyword ``sep`` is unspecified, try to detect data copied from a spreadsheet
  and parse accordingly. (:issue:`6223`)
- Joining a singly-indexed DataFrame with a MultiIndexed DataFrame (:issue:`3662`)

  See :ref:`the docs<merging.join_on_mi>`. Joining MultiIndex DataFrames on both the left and right is not yet supported ATM.

  .. ipython:: python

     household = pd.DataFrame({'household_id': [1, 2, 3],
                               'male': [0, 1, 0],
                               'wealth': [196087.3, 316478.7, 294750]
                               },
                              columns=['household_id', 'male', 'wealth']
                              ).set_index('household_id')
     household
     portfolio = pd.DataFrame({'household_id': [1, 2, 2, 3, 3, 3, 4],
                               'asset_id': ["nl0000301109",
                                            "nl0000289783",
                                            "gb00b03mlx29",
                                            "gb00b03mlx29",
                                            "lu0197800237",
                                            "nl0000289965",
                                            np.nan],
                               'name': ["ABN Amro",
                                        "Robeco",
                                        "Royal Dutch Shell",
                                        "Royal Dutch Shell",
                                        "AAB Eastern Europe Equity Fund",
                                        "Postbank BioTech Fonds",
                                        np.nan],
                               'share': [1.0, 0.4, 0.6, 0.15, 0.6, 0.25, 1.0]
                               },
                              columns=['household_id', 'asset_id', 'name', 'share']
                              ).set_index(['household_id', 'asset_id'])
     portfolio

     household.join(portfolio, how='inner')

- ``quotechar``, ``doublequote``, and ``escapechar`` can now be specified when
  using ``DataFrame.to_csv`` (:issue:`5414`, :issue:`4528`)
- Partially sort by only the specified levels of a MultiIndex with the
  ``sort_remaining`` boolean kwarg. (:issue:`3984`)
- Added ``to_julian_date`` to ``TimeStamp`` and ``DatetimeIndex``.  The Julian
  Date is used primarily in astronomy and represents the number of days from
  noon, January 1, 4713 BC.  Because nanoseconds are used to define the time
  in pandas the actual range of dates that you can use is 1678 AD to 2262 AD. (:issue:`4041`)
- ``DataFrame.to_stata`` will now check data for compatibility with Stata data types
  and will upcast when needed.  When it is not possible to losslessly upcast, a warning
  is issued (:issue:`6327`)
- ``DataFrame.to_stata`` and ``StataWriter`` will accept keyword arguments time_stamp
  and data_label which allow the time stamp and dataset label to be set when creating a
  file. (:issue:`6545`)
- ``pandas.io.gbq`` now handles reading unicode strings properly. (:issue:`5940`)
- :ref:`Holidays Calendars<timeseries.holiday>` are now available and can be used with the ``CustomBusinessDay`` offset (:issue:`6719`)
- ``Float64Index`` is now backed by a ``float64`` dtype ndarray instead of an
  ``object`` dtype array (:issue:`6471`).
- Implemented ``Panel.pct_change`` (:issue:`6904`)
- Added ``how`` option to rolling-moment functions to dictate how to handle resampling; :func:`rolling_max` defaults to max,
  :func:`rolling_min` defaults to min, and all others default to mean (:issue:`6297`)
- ``CustomBusinessMonthBegin`` and ``CustomBusinessMonthEnd`` are now available (:issue:`6866`)
- :meth:`Series.quantile` and :meth:`DataFrame.quantile` now accept an array of
  quantiles.
- :meth:`~DataFrame.describe` now accepts an array of percentiles to include in the summary statistics (:issue:`4196`)
- ``pivot_table`` can now accept ``Grouper`` by ``index`` and ``columns`` keywords (:issue:`6913`)

  .. ipython:: python

     import datetime
     df = pd.DataFrame({
         'Branch': 'A A A A A B'.split(),
         'Buyer': 'Carl Mark Carl Carl Joe Joe'.split(),
         'Quantity': [1, 3, 5, 1, 8, 1],
         'Date': [datetime.datetime(2013, 11, 1, 13, 0),
                  datetime.datetime(2013, 9, 1, 13, 5),
                  datetime.datetime(2013, 10, 1, 20, 0),
                  datetime.datetime(2013, 10, 2, 10, 0),
                  datetime.datetime(2013, 11, 1, 20, 0),
                  datetime.datetime(2013, 10, 2, 10, 0)],
         'PayDay': [datetime.datetime(2013, 10, 4, 0, 0),
                    datetime.datetime(2013, 10, 15, 13, 5),
                    datetime.datetime(2013, 9, 5, 20, 0),
                    datetime.datetime(2013, 11, 2, 10, 0),
                    datetime.datetime(2013, 10, 7, 20, 0),
                    datetime.datetime(2013, 9, 5, 10, 0)]})
     df

     df.pivot_table(values='Quantity',
                    index=pd.Grouper(freq='M', key='Date'),
                    columns=pd.Grouper(freq='M', key='PayDay'),
                    aggfunc=np.sum)

- Arrays of strings can be wrapped to a specified width (``str.wrap``) (:issue:`6999`)
- Add :meth:`~Series.nsmallest` and :meth:`Series.nlargest` methods to Series, See :ref:`the docs <basics.nsorted>` (:issue:`3960`)

- ``PeriodIndex`` fully supports partial string indexing like ``DatetimeIndex`` (:issue:`7043`)

  .. ipython:: python

     prng = pd.period_range('2013-01-01 09:00', periods=100, freq='H')
     ps = pd.Series(np.random.randn(len(prng)), index=prng)
     ps
     ps['2013-01-02']

- ``read_excel`` can now read milliseconds in Excel dates and times with xlrd >= 0.9.3. (:issue:`5945`)
- ``pd.stats.moments.rolling_var`` now uses Welford's method for increased numerical stability (:issue:`6817`)
- pd.expanding_apply and pd.rolling_apply now take args and kwargs that are passed on to
  the func (:issue:`6289`)
- ``DataFrame.rank()`` now has a percentage rank option (:issue:`5971`)
- ``Series.rank()`` now has a percentage rank option (:issue:`5971`)
- ``Series.rank()`` and ``DataFrame.rank()`` now accept ``method='dense'`` for ranks without gaps (:issue:`6514`)
- Support passing ``encoding`` with xlwt (:issue:`3710`)
- Refactor Block classes removing ``Block.items`` attributes to avoid duplication
  in item handling (:issue:`6745`, :issue:`6988`).
- Testing statements updated to use specialized asserts (:issue:`6175`)



.. _whatsnew_0140.performance:

Performance
~~~~~~~~~~~

- Performance improvement when converting ``DatetimeIndex`` to floating ordinals
  using ``DatetimeConverter`` (:issue:`6636`)
- Performance improvement for  ``DataFrame.shift`` (:issue:`5609`)
- Performance improvement in indexing into a MultiIndexed Series (:issue:`5567`)
- Performance improvements in single-dtyped indexing (:issue:`6484`)
- Improve performance of DataFrame construction with certain offsets, by removing faulty caching
  (e.g. MonthEnd,BusinessMonthEnd), (:issue:`6479`)
- Improve performance of ``CustomBusinessDay`` (:issue:`6584`)
- improve performance of slice indexing on Series with string keys (:issue:`6341`, :issue:`6372`)
- Performance improvement for ``DataFrame.from_records`` when reading a
  specified number of rows from an iterable (:issue:`6700`)
- Performance improvements in timedelta conversions for integer dtypes (:issue:`6754`)
- Improved performance of compatible pickles (:issue:`6899`)
- Improve performance in certain reindexing operations by optimizing ``take_2d`` (:issue:`6749`)
- ``GroupBy.count()`` is now implemented in Cython and is much faster for large
  numbers of groups (:issue:`7016`).

Experimental
~~~~~~~~~~~~

There are no experimental changes in 0.14.0


.. _whatsnew_0140.bug_fixes:

Bug fixes
~~~~~~~~~

- Bug in Series ValueError when index doesn't match data (:issue:`6532`)
- Prevent segfault due to MultiIndex not being supported in HDFStore table
  format (:issue:`1848`)
- Bug in ``pd.DataFrame.sort_index`` where mergesort wasn't stable when ``ascending=False`` (:issue:`6399`)
- Bug in ``pd.tseries.frequencies.to_offset`` when argument has leading zeros (:issue:`6391`)
- Bug in version string gen. for dev versions with shallow clones / install from tarball (:issue:`6127`)
- Inconsistent tz parsing ``Timestamp`` / ``to_datetime`` for current year (:issue:`5958`)
- Indexing bugs with reordered indexes (:issue:`6252`, :issue:`6254`)
- Bug in ``.xs`` with a Series multiindex (:issue:`6258`, :issue:`5684`)
- Bug in conversion of a string types to a DatetimeIndex with a specified frequency (:issue:`6273`, :issue:`6274`)
- Bug in ``eval`` where type-promotion failed for large expressions (:issue:`6205`)
- Bug in interpolate with ``inplace=True`` (:issue:`6281`)
- ``HDFStore.remove`` now handles start and stop (:issue:`6177`)
- ``HDFStore.select_as_multiple`` handles start and stop the same way as ``select`` (:issue:`6177`)
- ``HDFStore.select_as_coordinates`` and ``select_column`` works with a ``where`` clause that results in filters (:issue:`6177`)
- Regression in join of non_unique_indexes (:issue:`6329`)
- Issue with groupby ``agg`` with a single function and a mixed-type frame (:issue:`6337`)
- Bug in ``DataFrame.replace()`` when passing a non- ``bool``
  ``to_replace`` argument (:issue:`6332`)
- Raise when trying to align on different levels of a MultiIndex assignment (:issue:`3738`)
- Bug in setting complex dtypes via boolean indexing (:issue:`6345`)
- Bug in TimeGrouper/resample when presented with a non-monotonic DatetimeIndex that would return invalid results. (:issue:`4161`)
- Bug in index name propagation in TimeGrouper/resample (:issue:`4161`)
- TimeGrouper has a more compatible API to the rest of the groupers (e.g. ``groups`` was missing) (:issue:`3881`)
- Bug in multiple grouping with a TimeGrouper depending on target column order (:issue:`6764`)
- Bug in ``pd.eval`` when parsing strings with possible tokens like ``'&'``
  (:issue:`6351`)
- Bug correctly handle placements of ``-inf`` in Panels when dividing by integer 0 (:issue:`6178`)
- ``DataFrame.shift`` with ``axis=1`` was raising (:issue:`6371`)
- Disabled clipboard tests until release time (run locally with ``nosetests -A disabled``) (:issue:`6048`).
- Bug in ``DataFrame.replace()`` when passing a nested ``dict`` that contained
  keys not in the values to be replaced (:issue:`6342`)
- ``str.match`` ignored the na flag (:issue:`6609`).
- Bug in take with duplicate columns that were not consolidated (:issue:`6240`)
- Bug in interpolate changing dtypes (:issue:`6290`)
- Bug in ``Series.get``, was using a buggy access method (:issue:`6383`)
- Bug in hdfstore queries of the form ``where=[('date', '>=', datetime(2013,1,1)), ('date', '<=', datetime(2014,1,1))]`` (:issue:`6313`)
- Bug in ``DataFrame.dropna`` with duplicate indices (:issue:`6355`)
- Regression in chained getitem indexing with embedded list-like from 0.12 (:issue:`6394`)
- ``Float64Index`` with nans not comparing correctly (:issue:`6401`)
- ``eval``/``query`` expressions with strings containing the ``@`` character
  will now work (:issue:`6366`).
- Bug in ``Series.reindex`` when specifying a ``method`` with some nan values was inconsistent (noted on a resample) (:issue:`6418`)
- Bug in :meth:`DataFrame.replace` where nested dicts were erroneously
  depending on the order of dictionary keys and values (:issue:`5338`).
- Performance issue in concatenating with empty objects (:issue:`3259`)
- Clarify sorting of ``sym_diff`` on ``Index`` objects with ``NaN`` values (:issue:`6444`)
- Regression in ``MultiIndex.from_product`` with a ``DatetimeIndex`` as input (:issue:`6439`)
- Bug in ``str.extract`` when passed a non-default index (:issue:`6348`)
- Bug in ``str.split`` when passed ``pat=None`` and ``n=1`` (:issue:`6466`)
- Bug in ``io.data.DataReader`` when passed ``"F-F_Momentum_Factor"`` and ``data_source="famafrench"`` (:issue:`6460`)
- Bug in ``sum`` of a ``timedelta64[ns]`` series (:issue:`6462`)
- Bug in ``resample`` with a timezone and certain offsets (:issue:`6397`)
- Bug in ``iat/iloc`` with duplicate indices on a Series (:issue:`6493`)
- Bug in ``read_html`` where nan's were incorrectly being used to indicate
  missing values in text. Should use the empty string for consistency with the
  rest of pandas (:issue:`5129`).
- Bug in ``read_html`` tests where redirected invalid URLs would make one test
  fail (:issue:`6445`).
- Bug in multi-axis indexing using ``.loc`` on non-unique indices (:issue:`6504`)
- Bug that caused _ref_locs corruption when slice indexing across columns axis of a DataFrame (:issue:`6525`)
- Regression from 0.13 in the treatment of numpy ``datetime64`` non-ns dtypes in Series creation (:issue:`6529`)
- ``.names`` attribute of MultiIndexes passed to ``set_index`` are now preserved (:issue:`6459`).
- Bug in setitem with a duplicate index and an alignable rhs (:issue:`6541`)
- Bug in setitem with ``.loc`` on mixed integer Indexes (:issue:`6546`)
- Bug in ``pd.read_stata`` which would use the wrong data types and missing values (:issue:`6327`)
- Bug in ``DataFrame.to_stata`` that lead to data loss in certain cases, and could be exported using the
  wrong data types and missing values (:issue:`6335`)
- ``StataWriter`` replaces missing values in string columns by empty string (:issue:`6802`)
- Inconsistent types in ``Timestamp`` addition/subtraction (:issue:`6543`)
- Bug in preserving frequency across Timestamp addition/subtraction (:issue:`4547`)
- Bug in empty list lookup caused ``IndexError`` exceptions (:issue:`6536`, :issue:`6551`)
- ``Series.quantile`` raising on an ``object`` dtype (:issue:`6555`)
- Bug in ``.xs`` with a ``nan`` in level when dropped (:issue:`6574`)
- Bug in fillna with ``method='bfill/ffill'`` and ``datetime64[ns]`` dtype (:issue:`6587`)
- Bug in sql writing with mixed dtypes possibly leading to data loss (:issue:`6509`)
- Bug in ``Series.pop`` (:issue:`6600`)
- Bug in ``iloc`` indexing when positional indexer matched ``Int64Index`` of the corresponding axis and no reordering happened (:issue:`6612`)
- Bug in ``fillna`` with ``limit`` and ``value`` specified
- Bug in ``DataFrame.to_stata`` when columns have non-string names (:issue:`4558`)
- Bug in compat with ``np.compress``, surfaced in (:issue:`6658`)
- Bug in binary operations with a rhs of a Series not aligning (:issue:`6681`)
- Bug in ``DataFrame.to_stata`` which incorrectly handles nan values and ignores ``with_index`` keyword argument (:issue:`6685`)
- Bug in resample with extra bins when using an evenly divisible frequency (:issue:`4076`)
- Bug in consistency of groupby aggregation when passing a custom function (:issue:`6715`)
- Bug in resample when ``how=None`` resample freq is the same as the axis frequency (:issue:`5955`)
- Bug in downcasting inference with empty arrays (:issue:`6733`)
- Bug in ``obj.blocks`` on sparse containers dropping all but the last items of same for dtype (:issue:`6748`)
- Bug in unpickling ``NaT (NaTType)`` (:issue:`4606`)
- Bug in ``DataFrame.replace()`` where regex meta characters were being treated
  as regex even when ``regex=False`` (:issue:`6777`).
- Bug in timedelta ops on 32-bit platforms (:issue:`6808`)
- Bug in setting a tz-aware index directly via ``.index`` (:issue:`6785`)
- Bug in expressions.py where numexpr would try to evaluate arithmetic ops
  (:issue:`6762`).
- Bug in Makefile where it didn't remove Cython generated C files with ``make
  clean`` (:issue:`6768`)
- Bug with numpy < 1.7.2 when reading long strings from ``HDFStore`` (:issue:`6166`)
- Bug in ``DataFrame._reduce`` where non bool-like (0/1) integers were being
  converted into bools. (:issue:`6806`)
- Regression from 0.13 with ``fillna`` and a Series on datetime-like (:issue:`6344`)
- Bug in adding ``np.timedelta64`` to ``DatetimeIndex`` with timezone outputs incorrect results (:issue:`6818`)
- Bug in ``DataFrame.replace()`` where changing a dtype through replacement
  would only replace the first occurrence of a value (:issue:`6689`)
- Better error message when passing a frequency of 'MS' in ``Period`` construction (GH5332)
- Bug in ``Series.__unicode__`` when ``max_rows=None`` and the Series has more than 1000 rows. (:issue:`6863`)
- Bug in ``groupby.get_group`` where a datelike wasn't always accepted (:issue:`5267`)
- Bug in ``groupBy.get_group`` created by ``TimeGrouper`` raises ``AttributeError`` (:issue:`6914`)
- Bug in ``DatetimeIndex.tz_localize`` and ``DatetimeIndex.tz_convert`` converting ``NaT`` incorrectly (:issue:`5546`)
- Bug in arithmetic operations affecting ``NaT`` (:issue:`6873`)
- Bug in ``Series.str.extract`` where the resulting ``Series`` from a single
  group match wasn't renamed to the group name
- Bug in ``DataFrame.to_csv`` where setting ``index=False`` ignored the
  ``header`` kwarg (:issue:`6186`)
- Bug in ``DataFrame.plot`` and ``Series.plot``, where the legend behave inconsistently when plotting to the same axes repeatedly (:issue:`6678`)
- Internal tests for patching ``__finalize__`` / bug in merge not finalizing (:issue:`6923`, :issue:`6927`)
- accept ``TextFileReader`` in ``concat``, which was affecting a common user idiom (:issue:`6583`)
- Bug in C parser with leading white space (:issue:`3374`)
- Bug in C parser with ``delim_whitespace=True`` and ``\r``-delimited lines
- Bug in python parser with explicit MultiIndex in row following column header (:issue:`6893`)
- Bug in ``Series.rank`` and ``DataFrame.rank`` that caused small floats (<1e-13) to all receive the same rank (:issue:`6886`)
- Bug in ``DataFrame.apply`` with functions that used ``*args`` or ``**kwargs`` and returned
  an empty result (:issue:`6952`)
- Bug in sum/mean on 32-bit platforms on overflows (:issue:`6915`)
- Moved ``Panel.shift`` to ``NDFrame.slice_shift`` and fixed to respect multiple dtypes. (:issue:`6959`)
- Bug in enabling ``subplots=True`` in ``DataFrame.plot`` only has single column raises ``TypeError``, and ``Series.plot`` raises ``AttributeError`` (:issue:`6951`)
- Bug in ``DataFrame.plot`` draws unnecessary axes when enabling ``subplots`` and ``kind=scatter`` (:issue:`6951`)
- Bug in ``read_csv`` from a filesystem with non-utf-8 encoding (:issue:`6807`)
- Bug in ``iloc`` when setting / aligning (:issue:`6766`)
- Bug causing UnicodeEncodeError when get_dummies called with unicode values and a prefix (:issue:`6885`)
- Bug in timeseries-with-frequency plot cursor display (:issue:`5453`)
- Bug surfaced in ``groupby.plot`` when using a ``Float64Index`` (:issue:`7025`)
- Stopped tests from failing if options data isn't able to be downloaded from Yahoo (:issue:`7034`)
- Bug in ``parallel_coordinates`` and ``radviz`` where reordering of class column
  caused possible color/class mismatch (:issue:`6956`)
- Bug in ``radviz`` and ``andrews_curves`` where multiple values of 'color'
  were being passed to plotting method (:issue:`6956`)
- Bug in ``Float64Index.isin()`` where containing ``nan`` s would make indices
  claim that they contained all the things (:issue:`7066`).
- Bug in ``DataFrame.boxplot`` where it failed to use the axis passed as the ``ax`` argument (:issue:`3578`)
- Bug in the ``XlsxWriter`` and ``XlwtWriter`` implementations that resulted in datetime columns being formatted without the time (:issue:`7075`)
  were being passed to plotting method
- :func:`read_fwf` treats ``None`` in ``colspec`` like regular python slices. It now reads from the beginning
  or until the end of the line when ``colspec`` contains a ``None`` (previously raised a ``TypeError``)
- Bug in cache coherence with chained indexing and slicing; add ``_is_view`` property to ``NDFrame`` to correctly predict
  views; mark ``is_copy`` on ``xs`` only if its an actual copy (and not a view) (:issue:`7084`)
- Bug in DatetimeIndex creation from string ndarray with ``dayfirst=True`` (:issue:`5917`)
- Bug in ``MultiIndex.from_arrays`` created from ``DatetimeIndex`` doesn't preserve ``freq`` and ``tz`` (:issue:`7090`)
- Bug in ``unstack`` raises ``ValueError`` when ``MultiIndex`` contains ``PeriodIndex`` (:issue:`4342`)
- Bug in ``boxplot`` and ``hist`` draws unnecessary axes (:issue:`6769`)
- Regression in ``groupby.nth()`` for out-of-bounds indexers (:issue:`6621`)
- Bug in ``quantile`` with datetime values (:issue:`6965`)
- Bug in ``Dataframe.set_index``, ``reindex`` and ``pivot`` don't preserve ``DatetimeIndex`` and ``PeriodIndex`` attributes (:issue:`3950`, :issue:`5878`, :issue:`6631`)
- Bug in ``MultiIndex.get_level_values`` doesn't preserve ``DatetimeIndex`` and ``PeriodIndex`` attributes (:issue:`7092`)
- Bug in ``Groupby`` doesn't preserve ``tz`` (:issue:`3950`)
- Bug in ``PeriodIndex`` partial string slicing (:issue:`6716`)
- Bug in the HTML repr of a truncated Series or DataFrame not showing the class name with the ``large_repr`` set to 'info'
  (:issue:`7105`)
- Bug in ``DatetimeIndex`` specifying ``freq`` raises ``ValueError`` when passed value is too short (:issue:`7098`)
- Fixed a bug with the ``info`` repr not honoring the ``display.max_info_columns`` setting (:issue:`6939`)
- Bug ``PeriodIndex`` string slicing with out of bounds values (:issue:`5407`)
- Fixed a memory error in the hashtable implementation/factorizer on resizing of large tables (:issue:`7157`)
- Bug in ``isnull`` when applied to 0-dimensional object arrays (:issue:`7176`)
- Bug in ``query``/``eval`` where global constants were not looked up correctly
  (:issue:`7178`)
- Bug in recognizing out-of-bounds positional list indexers with ``iloc`` and a multi-axis tuple indexer (:issue:`7189`)
- Bug in setitem with a single value, MultiIndex and integer indices (:issue:`7190`, :issue:`7218`)
- Bug in expressions evaluation with reversed ops, showing in series-dataframe ops (:issue:`7198`, :issue:`7192`)
- Bug in multi-axis indexing with > 2 ndim and a MultiIndex (:issue:`7199`)
- Fix a bug where invalid eval/query operations would blow the stack (:issue:`5198`)


.. _whatsnew_0.14.0.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v0.13.1..v0.14.0

.. _whatsnew_105:

What's new in 1.0.5 (June 17, 2020)
-----------------------------------

These are the changes in pandas 1.0.5. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_105.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

- Fix regression in :meth:`read_parquet` when reading from file-like objects
  (:issue:`34467`).
- Fix regression in reading from public S3 buckets (:issue:`34626`).

Note this disables the ability to read Parquet files from directories on S3
again (:issue:`26388`, :issue:`34632`), which was added in the 1.0.4 release,
but is now targeted for pandas 1.1.0.

- Fixed regression in :meth:`~DataFrame.replace` raising an ``AssertionError`` when replacing values in an extension dtype with values of a different dtype (:issue:`34530`)

.. _whatsnew_105.bug_fixes:

Bug fixes
~~~~~~~~~

- Fixed building from source with Python 3.8 fetching the wrong version of NumPy (:issue:`34666`)

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.4..v1.0.5|HEAD
.. _whatsnew_132:

What's new in 1.3.2 (August 15, 2021)
-------------------------------------

These are the changes in pandas 1.3.2. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_132.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~
- Performance regression in :meth:`DataFrame.isin` and :meth:`Series.isin` for nullable data types (:issue:`42714`)
- Regression in updating values of :class:`Series` using boolean index, created by using :meth:`DataFrame.pop` (:issue:`42530`)
- Regression in :meth:`DataFrame.from_records` with empty records (:issue:`42456`)
- Fixed regression in :meth:`DataFrame.shift` where ``TypeError`` occurred when shifting DataFrame created by concatenation of slices and fills with values (:issue:`42719`)
- Regression in :meth:`DataFrame.agg` when the ``func`` argument returned lists and ``axis=1`` (:issue:`42727`)
- Regression in :meth:`DataFrame.drop` does nothing if :class:`MultiIndex` has duplicates and indexer is a tuple or list of tuples (:issue:`42771`)
- Fixed regression where :func:`read_csv` raised a ``ValueError`` when parameters ``names`` and ``prefix`` were both set to ``None`` (:issue:`42387`)
- Fixed regression in comparisons between :class:`Timestamp` object and ``datetime64`` objects outside the implementation bounds for nanosecond ``datetime64`` (:issue:`42794`)
- Fixed regression in :meth:`.Styler.highlight_min` and :meth:`.Styler.highlight_max` where ``pandas.NA`` was not successfully ignored (:issue:`42650`)
- Fixed regression in :func:`concat` where ``copy=False`` was not honored in ``axis=1`` Series concatenation (:issue:`42501`)
- Regression in :meth:`Series.nlargest` and :meth:`Series.nsmallest` with nullable integer or float dtype (:issue:`42816`)
- Fixed regression in :meth:`Series.quantile` with :class:`Int64Dtype` (:issue:`42626`)
- Fixed regression in :meth:`Series.groupby` and :meth:`DataFrame.groupby` where supplying the ``by`` argument with a Series named with a tuple would incorrectly raise (:issue:`42731`)

.. ---------------------------------------------------------------------------

.. _whatsnew_132.bug_fixes:

Bug fixes
~~~~~~~~~
- Bug in :func:`read_excel` modifies the dtypes dictionary when reading a file with duplicate columns (:issue:`42462`)
- 1D slices over extension types turn into N-dimensional slices over ExtensionArrays (:issue:`42430`)
- Fixed bug in :meth:`Series.rolling` and :meth:`DataFrame.rolling` not calculating window bounds correctly for the first row when ``center=True`` and ``window`` is an offset that covers all the rows (:issue:`42753`)
- :meth:`.Styler.hide_columns` now hides the index name header row as well as column headers (:issue:`42101`)
- :meth:`.Styler.set_sticky` has amended CSS to control the column/index names and ensure the correct sticky positions (:issue:`42537`)
- Bug in de-serializing datetime indexes in PYTHONOPTIMIZED mode (:issue:`42866`)

.. ---------------------------------------------------------------------------

.. _whatsnew_132.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.3.1..v1.3.2
.. _whatsnew_102:

What's new in 1.0.2 (March 12, 2020)
------------------------------------

These are the changes in pandas 1.0.2. See :ref:`release` for a full changelog
including other versions of pandas.

{{ header }}

.. ---------------------------------------------------------------------------

.. _whatsnew_102.regressions:

Fixed regressions
~~~~~~~~~~~~~~~~~

**Groupby**

- Fixed regression in :meth:`groupby(..).agg() <pandas.core.groupby.GroupBy.agg>` which was failing on frames with :class:`MultiIndex` columns and a custom function (:issue:`31777`)
- Fixed regression in ``groupby(..).rolling(..).apply()`` (``RollingGroupby``) where the ``raw`` parameter was ignored (:issue:`31754`)
- Fixed regression in :meth:`rolling(..).corr() <pandas.core.window.rolling.Rolling.corr>` when using a time offset (:issue:`31789`)
- Fixed regression in :meth:`groupby(..).nunique() <pandas.core.groupby.DataFrameGroupBy.nunique>` which was modifying the original values if ``NaN`` values were present (:issue:`31950`)
- Fixed regression in ``DataFrame.groupby`` raising a ``ValueError`` from an internal operation (:issue:`31802`)
- Fixed regression in :meth:`groupby(..).agg() <pandas.core.groupby.GroupBy.agg>` calling a user-provided function an extra time on an empty input (:issue:`31760`)

**I/O**

- Fixed regression in :meth:`read_csv` in which the ``encoding`` option was not recognized with certain file-like objects (:issue:`31819`)
- Fixed regression in :meth:`DataFrame.to_excel` when the ``columns`` keyword argument is passed (:issue:`31677`)
- Fixed regression in :class:`ExcelFile` where the stream passed into the function was closed by the destructor. (:issue:`31467`)
- Fixed regression where :func:`read_pickle` raised a ``UnicodeDecodeError`` when reading a py27 pickle with :class:`MultiIndex` column (:issue:`31988`).

**Reindexing/alignment**

- Fixed regression in :meth:`Series.align` when ``other`` is a :class:`DataFrame` and ``method`` is not ``None`` (:issue:`31785`)
- Fixed regression in :meth:`DataFrame.reindex` and :meth:`Series.reindex` when reindexing with (tz-aware) index and ``method=nearest`` (:issue:`26683`)
- Fixed regression in :meth:`DataFrame.reindex_like` on a :class:`DataFrame` subclass raised an  ``AssertionError`` (:issue:`31925`)
- Fixed regression in :class:`DataFrame` arithmetic operations with mis-matched columns (:issue:`31623`)

**Other**

- Fixed regression in joining on :class:`DatetimeIndex` or :class:`TimedeltaIndex` to preserve ``freq`` in simple cases (:issue:`32166`)
- Fixed regression in :meth:`Series.shift` with ``datetime64`` dtype when passing an integer ``fill_value`` (:issue:`32591`)
- Fixed regression in the repr of an object-dtype :class:`Index` with bools and missing values (:issue:`32146`)


.. ---------------------------------------------------------------------------

Indexing with nullable boolean arrays
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Previously indexing with a nullable Boolean array containing ``NA`` would raise a ``ValueError``, however this is now permitted with ``NA`` being treated as ``False``. (:issue:`31503`)

.. ipython:: python

    s = pd.Series([1, 2, 3, 4])
    mask = pd.array([True, True, False, None], dtype="boolean")
    s
    mask

*pandas 1.0.0-1.0.1*

.. code-block:: python

    >>> s[mask]
    Traceback (most recent call last):
    ...
    ValueError: cannot mask with array containing NA / NaN values

*pandas 1.0.2*

.. ipython:: python

    s[mask]

.. _whatsnew_102.bug_fixes:

Bug fixes
~~~~~~~~~

**Datetimelike**

- Bug in :meth:`Series.astype` not copying for tz-naive and tz-aware ``datetime64`` dtype (:issue:`32490`)
- Bug where :func:`to_datetime` would raise when passed ``pd.NA`` (:issue:`32213`)
- Improved error message when subtracting two :class:`Timestamp` that result in an out-of-bounds :class:`Timedelta` (:issue:`31774`)

**Categorical**

- Fixed bug where :meth:`Categorical.from_codes` improperly raised a ``ValueError`` when passed nullable integer codes. (:issue:`31779`)
- Fixed bug where :meth:`Categorical` constructor would raise a ``TypeError`` when given a numpy array containing ``pd.NA``. (:issue:`31927`)
- Bug in :class:`Categorical` that would ignore or crash when calling :meth:`Series.replace` with a list-like ``to_replace`` (:issue:`31720`)

**I/O**

- Using ``pd.NA`` with :meth:`DataFrame.to_json` now correctly outputs a null value instead of an empty object (:issue:`31615`)
- Bug in :meth:`pandas.json_normalize` when value in meta path is not iterable (:issue:`31507`)
- Fixed pickling of ``pandas.NA``. Previously a new object was returned, which broke computations relying on ``NA`` being a singleton (:issue:`31847`)
- Fixed bug in parquet roundtrip with nullable unsigned integer dtypes (:issue:`31896`).

**Experimental dtypes**

- Fixed bug in :meth:`DataFrame.convert_dtypes` for columns that were already using the ``"string"`` dtype (:issue:`31731`).
- Fixed bug in :meth:`DataFrame.convert_dtypes` for series with mix of integers and strings (:issue:`32117`)
- Fixed bug in :meth:`DataFrame.convert_dtypes` where ``BooleanDtype`` columns were converted to ``Int64`` (:issue:`32287`)
- Fixed bug in setting values using a slice indexer with string dtype (:issue:`31772`)
- Fixed bug where :meth:`pandas.core.groupby.GroupBy.first` and :meth:`pandas.core.groupby.GroupBy.last` would raise a ``TypeError`` when groups contained ``pd.NA`` in a column of object dtype (:issue:`32123`)
- Fixed bug where :meth:`DataFrameGroupBy.mean`, :meth:`DataFrameGroupBy.median`, :meth:`DataFrameGroupBy.var`, and :meth:`DataFrameGroupBy.std` would raise a ``TypeError`` on ``Int64`` dtype columns (:issue:`32219`)

**Strings**

- Using ``pd.NA`` with :meth:`Series.str.repeat` now correctly outputs a null value instead of raising error for vector inputs (:issue:`31632`)

**Rolling**

- Fixed rolling operations with variable window (defined by time duration) on decreasing time index (:issue:`32385`).

.. ---------------------------------------------------------------------------

.. _whatsnew_102.contributors:

Contributors
~~~~~~~~~~~~

.. contributors:: v1.0.1..v1.0.2
.. _contributing:

{{ header }}

**********************
Contributing to pandas
**********************

.. contents:: Table of contents:
   :local:

Where to start?
===============

All contributions, bug reports, bug fixes, documentation improvements,
enhancements, and ideas are welcome.

If you are brand new to pandas or open-source development, we recommend going
through the `GitHub "issues" tab <https://github.com/pandas-dev/pandas/issues>`_
to find issues that interest you. There are a number of issues listed under `Docs
<https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open>`_
and `good first issue
<https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open>`_
where you could start out. Once you've found an interesting issue, you can
return here to get your development environment setup.

When you start working on an issue, it's a good idea to assign the issue to yourself,
so nobody else duplicates the work on it. GitHub restricts assigning issues to maintainers
of the project only. In most projects, and until recently in pandas, contributors added a
comment letting others know they are working on an issue. While this is ok, you need to
check each issue individually, and it's not possible to find the unassigned ones.

For this reason, we implemented a workaround consisting of adding a comment with the exact
text ``take``. When you do it, a GitHub action will automatically assign you the issue
(this will take seconds, and may require refreshing the page to see it).
By doing this, it's possible to filter the list of issues and find only the unassigned ones.

So, a good way to find an issue to start contributing to pandas is to check the list of
`unassigned good first issues <https://github.com/pandas-dev/pandas/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22+no%3Aassignee>`_
and assign yourself one you like by writing a comment with the exact text ``take``.

If for whatever reason you are not able to continue working with the issue, please try to
unassign it, so other people know it's available again. You can check the list of
assigned issues, since people may not be working in them anymore. If you want to work on one
that is assigned, feel free to kindly ask the current assignee if you can take it
(please allow at least a week of inactivity before considering work in the issue discontinued).

Feel free to ask questions on the `mailing list
<https://groups.google.com/forum/?fromgroups#!forum/pydata>`_ or on `Gitter`_.

.. _contributing.bug_reports:

Bug reports and enhancement requests
====================================

Bug reports are an important part of making pandas more stable. Having a complete bug report
will allow others to reproduce the bug and provide insight into fixing. See
`this stackoverflow article <https://stackoverflow.com/help/mcve>`_ and
`this blogpost <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>`_
for tips on writing a good bug report.

Trying the bug-producing code out on the *main* branch is often a worthwhile exercise
to confirm the bug still exists. It is also worth searching existing bug reports and pull requests
to see if the issue has already been reported and/or fixed.

Bug reports must:

#. Include a short, self-contained Python snippet reproducing the problem.
   You can format the code nicely by using `GitHub Flavored Markdown
   <https://github.github.com/github-flavored-markdown/>`_::

      ```python
      >>> from pandas import DataFrame
      >>> df = DataFrame(...)
      ...
      ```

#. Include the full version string of pandas and its dependencies. You can use the built-in function::

      >>> import pandas as pd
      >>> pd.show_versions()

#. Explain why the current behavior is wrong/not desired and what you expect instead.

The issue will then show up to the pandas community and be open to comments/ideas from others.

.. _contributing.github:

Working with the code
=====================

Now that you have an issue you want to fix, enhancement to add, or documentation to improve,
you need to learn how to work with GitHub and the pandas code base.

.. _contributing.version_control:

Version control, Git, and GitHub
--------------------------------

To the new user, working with Git is one of the more daunting aspects of contributing to pandas.
It can very quickly become overwhelming, but sticking to the guidelines below will help keep the process
straightforward and mostly trouble free.  As always, if you are having difficulties please
feel free to ask for help.

The code is hosted on `GitHub <https://www.github.com/pandas-dev/pandas>`_. To
contribute you will need to sign up for a `free GitHub account
<https://github.com/signup/free>`_. We use `Git <https://git-scm.com/>`_ for
version control to allow many people to work together on the project.

Some great resources for learning Git:

* the `GitHub help pages <https://help.github.com/>`_.
* the `NumPy documentation <https://numpy.org/doc/stable/dev/index.html>`_.
* Matthew Brett's `Pydagogue <https://matthew-brett.github.io/pydagogue/>`_.

Getting started with Git
------------------------

`GitHub has instructions <https://help.github.com/set-up-git-redirect>`__ for installing git,
setting up your SSH key, and configuring git.  All these steps need to be completed before
you can work seamlessly between your local repository and GitHub.

.. _contributing.forking:

Forking
-------

You will need your own fork to work on the code. Go to the `pandas project
page <https://github.com/pandas-dev/pandas>`_ and hit the ``Fork`` button. You will
want to clone your fork to your machine::

    git clone https://github.com/your-user-name/pandas.git pandas-yourname
    cd pandas-yourname
    git remote add upstream https://github.com/pandas-dev/pandas.git

This creates the directory ``pandas-yourname`` and connects your repository to
the upstream (main project) *pandas* repository.

Note that performing a shallow clone (with ``--depth==N``, for some ``N`` greater
or equal to 1) might break some tests and features as ``pd.show_versions()``
as the version number cannot be computed anymore.

Creating a branch
-----------------

You want your main branch to reflect only production-ready code, so create a
feature branch for making your changes. For example::

    git branch shiny-new-feature
    git checkout shiny-new-feature

The above can be simplified to::

    git checkout -b shiny-new-feature

This changes your working directory to the shiny-new-feature branch.  Keep any
changes in this branch specific to one bug or feature so it is clear
what the branch brings to pandas. You can have many shiny-new-features
and switch in between them using the git checkout command.

When creating this branch, make sure your main branch is up to date with
the latest upstream main version. To update your local main branch, you
can do::

    git checkout main
    git pull upstream main --ff-only

When you want to update the feature branch with changes in main after
you created the branch, check the section on
:ref:`updating a PR <contributing.update-pr>`.

Contributing your changes to pandas
=====================================

.. _contributing.commit-code:

Committing your code
--------------------

Keep style fixes to a separate commit to make your pull request more readable.

Once you've made changes, you can see them by typing::

    git status

If you have created a new file, it is not being tracked by git. Add it by typing::

    git add path/to/file-to-be-added.py

Doing 'git status' again should give something like::

    # On branch shiny-new-feature
    #
    #       modified:   /relative/path/to/file-you-added.py
    #

Finally, commit your changes to your local repository with an explanatory message. pandas
uses a convention for commit message prefixes and layout.  Here are
some common prefixes along with general guidelines for when to use them:

* ENH: Enhancement, new functionality
* BUG: Bug fix
* DOC: Additions/updates to documentation
* TST: Additions/updates to tests
* BLD: Updates to the build process/scripts
* PERF: Performance improvement
* TYP: Type annotations
* CLN: Code cleanup

The following defines how a commit message should be structured.  Please reference the
relevant GitHub issues in your commit message using GH1234 or #1234.  Either style
is fine, but the former is generally preferred:

* a subject line with ``< 80`` chars.
* One blank line.
* Optionally, a commit message body.

Now you can commit your changes in your local repository::

    git commit -m

.. _contributing.push-code:

Pushing your changes
--------------------

When you want your changes to appear publicly on your GitHub page, push your
forked feature branch's commits::

    git push origin shiny-new-feature

Here ``origin`` is the default name given to your remote repository on GitHub.
You can see the remote repositories::

    git remote -v

If you added the upstream repository as described above you will see something
like::

    origin  git@github.com:yourname/pandas.git (fetch)
    origin  git@github.com:yourname/pandas.git (push)
    upstream        git://github.com/pandas-dev/pandas.git (fetch)
    upstream        git://github.com/pandas-dev/pandas.git (push)

Now your code is on GitHub, but it is not yet a part of the pandas project. For that to
happen, a pull request needs to be submitted on GitHub.

Review your code
----------------

When you're ready to ask for a code review, file a pull request. Before you do, once
again make sure that you have followed all the guidelines outlined in this document
regarding code style, tests, performance tests, and documentation. You should also
double check your branch changes against the branch it was based on:

#. Navigate to your repository on GitHub -- https://github.com/your-user-name/pandas
#. Click on ``Branches``
#. Click on the ``Compare`` button for your feature branch
#. Select the ``base`` and ``compare`` branches, if necessary. This will be ``main`` and
   ``shiny-new-feature``, respectively.

Finally, make the pull request
------------------------------

If everything looks good, you are ready to make a pull request.  A pull request is how
code from a local repository becomes available to the GitHub community and can be looked
at and eventually merged into the main version.  This pull request and its associated
changes will eventually be committed to the main branch and available in the next
release.  To submit a pull request:

#. Navigate to your repository on GitHub
#. Click on the ``Pull Request`` button
#. You can then click on ``Commits`` and ``Files Changed`` to make sure everything looks
   okay one last time
#. Write a description of your changes in the ``Preview Discussion`` tab
#. Click ``Send Pull Request``.

This request then goes to the repository maintainers, and they will review
the code.

.. _contributing.update-pr:

Updating your pull request
--------------------------

Based on the review you get on your pull request, you will probably need to make
some changes to the code. In that case, you can make them in your branch,
add a new commit to that branch, push it to GitHub, and the pull request will be
automatically updated.  Pushing them to GitHub again is done by::

    git push origin shiny-new-feature

This will automatically update your pull request with the latest code and restart the
:any:`Continuous Integration <contributing.ci>` tests.

Another reason you might need to update your pull request is to solve conflicts
with changes that have been merged into the main branch since you opened your
pull request.

To do this, you need to "merge upstream main" in your branch::

    git checkout shiny-new-feature
    git fetch upstream
    git merge upstream/main

If there are no conflicts (or they could be fixed automatically), a file with a
default commit message will open, and you can simply save and quit this file.

If there are merge conflicts, you need to solve those conflicts. See for
example at https://help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/
for an explanation on how to do this.
Once the conflicts are merged and the files where the conflicts were solved are
added, you can run ``git commit`` to save those fixes.

If you have uncommitted changes at the moment you want to update the branch with
main, you will need to ``stash`` them prior to updating (see the
`stash docs <https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning>`__).
This will effectively store your changes and they can be reapplied after updating.

After the feature branch has been update locally, you can now update your pull
request by pushing to the branch on GitHub::

    git push origin shiny-new-feature

Autofixing formatting errors
----------------------------

We use several styling checks (e.g. ``black``, ``flake8``, ``isort``) which are run after
you make a pull request. If there is a scenario where any of these checks fail then you
can comment::

    @github-actions pre-commit

on that pull request. This will trigger a workflow which will autofix formatting
errors.

To automatically fix formatting errors on each commit you make, you can
set up pre-commit yourself. First, create a Python :ref:`environment
<contributing_environment>` and then set up :ref:`pre-commit <contributing.pre-commit>`.

Delete your merged branch (optional)
------------------------------------

Once your feature branch is accepted into upstream, you'll probably want to get rid of
the branch. First, merge upstream main into your branch so git knows it is safe to
delete your branch::

    git fetch upstream
    git checkout main
    git merge upstream/main

Then you can do::

    git branch -d shiny-new-feature

Make sure you use a lower-case ``-d``, or else git won't warn you if your feature
branch has not actually been merged.

The branch will still exist on GitHub, so to delete it there do::

    git push origin --delete shiny-new-feature

.. _Gitter: https://gitter.im/pydata/pandas


Tips for a successful pull request
==================================

If you have made it to the `Review your code`_ phase, one of the core contributors may
take a look. Please note however that a handful of people are responsible for reviewing
all of the contributions, which can often lead to bottlenecks.

To improve the chances of your pull request being reviewed, you should:

- **Reference an open issue** for non-trivial changes to clarify the PR's purpose
- **Ensure you have appropriate tests**. These should be the first part of any PR
- **Keep your pull requests as simple as possible**. Larger PRs take longer to review
- **Ensure that CI is in a green state**. Reviewers may not even look otherwise
- **Keep** `Updating your pull request`_, either by request or every few days
.. _test_organization:

Test organization
=================
Ideally, there should be one, and only one, obvious place for a test to reside.
Until we reach that ideal, these are some rules of thumb for where a test should
be located.

1. Does your test depend only on code in ``pd._libs.tslibs``?
   This test likely belongs in one of:

   - tests.tslibs

     .. note::

          No file in ``tests.tslibs`` should import from any pandas modules
          outside of ``pd._libs.tslibs``

   - tests.scalar
   - tests.tseries.offsets

2. Does your test depend only on code in pd._libs?
   This test likely belongs in one of:

   - tests.libs
   - tests.groupby.test_libgroupby

3. Is your test for an arithmetic or comparison method?
   This test likely belongs in one of:

   - tests.arithmetic

     .. note::

         These are intended for tests that can be shared to test the behavior
         of DataFrame/Series/Index/ExtensionArray using the ``box_with_array``
         fixture.

   - tests.frame.test_arithmetic
   - tests.series.test_arithmetic

4. Is your test for a reduction method (min, max, sum, prod, ...)?
   This test likely belongs in one of:

   - tests.reductions

     .. note::

         These are intended for tests that can be shared to test the behavior
         of DataFrame/Series/Index/ExtensionArray.

   - tests.frame.test_reductions
   - tests.series.test_reductions
   - tests.test_nanops

5. Is your test for an indexing method?
   This is the most difficult case for deciding where a test belongs, because
   there are many of these tests, and many of them test more than one method
   (e.g. both ``Series.__getitem__`` and ``Series.loc.__getitem__``)

   A) Is the test specifically testing an Index method (e.g. ``Index.get_loc``,
      ``Index.get_indexer``)?
      This test likely belongs in one of:

      - tests.indexes.test_indexing
      - tests.indexes.fooindex.test_indexing

      Within that files there should be a method-specific test class e.g.
      ``TestGetLoc``.

      In most cases, neither ``Series`` nor ``DataFrame`` objects should be
      needed in these tests.

   B) Is the test for a Series or DataFrame indexing method *other* than
      ``__getitem__`` or ``__setitem__``, e.g. ``xs``, ``where``, ``take``,
      ``mask``, ``lookup``, or ``insert``?
      This test likely belongs in one of:

      - tests.frame.indexing.test_methodname
      - tests.series.indexing.test_methodname

   C) Is the test for any of ``loc``, ``iloc``, ``at``, or ``iat``?
      This test likely belongs in one of:

      - tests.indexing.test_loc
      - tests.indexing.test_iloc
      - tests.indexing.test_at
      - tests.indexing.test_iat

      Within the appropriate file, test classes correspond to either types of
      indexers (e.g. ``TestLocBooleanMask``) or major use cases
      (e.g. ``TestLocSetitemWithExpansion``).

      See the note in section D) about tests that test multiple indexing methods.

   D) Is the test for ``Series.__getitem__``, ``Series.__setitem__``,
      ``DataFrame.__getitem__``, or ``DataFrame.__setitem__``?
      This test likely belongs in one of:

      - tests.series.test_getitem
      - tests.series.test_setitem
      - tests.frame.test_getitem
      - tests.frame.test_setitem

      If many cases such a test may test multiple similar methods, e.g.

      .. code-block:: python

           import pandas as pd
           import pandas._testing as tm

           def test_getitem_listlike_of_ints():
               ser = pd.Series(range(5))

               result = ser[[3, 4]]
               expected = pd.Series([2, 3])
               tm.assert_series_equal(result, expected)

               result = ser.loc[[3, 4]]
               tm.assert_series_equal(result, expected)

    In cases like this, the test location should be based on the *underlying*
    method being tested.  Or in the case of a test for a bugfix, the location
    of the actual bug.  So in this example, we know that ``Series.__getitem__``
    calls ``Series.loc.__getitem__``, so this is *really* a test for
    ``loc.__getitem__``.  So this test belongs in ``tests.indexing.test_loc``.

6. Is your test for a DataFrame or Series method?

   A) Is the method a plotting method?
      This test likely belongs in one of:

      - tests.plotting

   B) Is the method an IO method?
      This test likely belongs in one of:

      - tests.io

   C) Otherwise
      This test likely belongs in one of:

      - tests.series.methods.test_mymethod
      - tests.frame.methods.test_mymethod

        .. note::

            If a test can be shared between DataFrame/Series using the
            ``frame_or_series`` fixture, by convention it goes in the
            ``tests.frame`` file.

7. Is your test for an Index method, not depending on Series/DataFrame?
   This test likely belongs in one of:

   - tests.indexes

8) Is your test for one of the pandas-provided ExtensionArrays (``Categorical``,
   ``DatetimeArray``, ``TimedeltaArray``, ``PeriodArray``, ``IntervalArray``,
   ``PandasArray``, ``FloatArray``, ``BoolArray``, ``StringArray``)?
   This test likely belongs in one of:

   - tests.arrays

9) Is your test for *all* ExtensionArray subclasses (the "EA Interface")?
   This test likely belongs in one of:

   - tests.extension
.. _developer:

{{ header }}

.. currentmodule:: pandas

*********
Developer
*********

This section will focus on downstream applications of pandas.

.. _apache.parquet:

Storing pandas DataFrame objects in Apache Parquet format
---------------------------------------------------------

The `Apache Parquet <https://github.com/apache/parquet-format>`__ format
provides key-value metadata at the file and column level, stored in the footer
of the Parquet file:

.. code-block:: shell

  5: optional list<KeyValue> key_value_metadata

where ``KeyValue`` is

.. code-block:: shell

   struct KeyValue {
     1: required string key
     2: optional string value
   }

So that a ``pandas.DataFrame`` can be faithfully reconstructed, we store a
``pandas`` metadata key in the ``FileMetaData`` with the value stored as :

.. code-block:: text

   {'index_columns': [<descr0>, <descr1>, ...],
    'column_indexes': [<ci0>, <ci1>, ..., <ciN>],
    'columns': [<c0>, <c1>, ...],
    'pandas_version': $VERSION,
    'creator': {
      'library': $LIBRARY,
      'version': $LIBRARY_VERSION
    }}

The "descriptor" values ``<descr0>`` in the ``'index_columns'`` field are
strings (referring to a column) or dictionaries with values as described below.

The ``<c0>``/``<ci0>`` and so forth are dictionaries containing the metadata
for each column, *including the index columns*. This has JSON form:

.. code-block:: text

   {'name': column_name,
    'field_name': parquet_column_name,
    'pandas_type': pandas_type,
    'numpy_type': numpy_type,
    'metadata': metadata}

See below for the detailed specification for these.

Index metadata descriptors
~~~~~~~~~~~~~~~~~~~~~~~~~~

``RangeIndex`` can be stored as metadata only, not requiring serialization. The
descriptor format for these as is follows:

.. code-block:: python

   index = pd.RangeIndex(0, 10, 2)
   {
       "kind": "range",
       "name": index.name,
       "start": index.start,
       "stop": index.stop,
       "step": index.step,
   }

Other index types must be serialized as data columns along with the other
DataFrame columns. The metadata for these is a string indicating the name of
the field in the data columns, for example ``'__index_level_0__'``.

If an index has a non-None ``name`` attribute, and there is no other column
with a name matching that value, then the ``index.name`` value can be used as
the descriptor. Otherwise (for unnamed indexes and ones with names colliding
with other column names) a disambiguating name with pattern matching
``__index_level_\d+__`` should be used. In cases of named indexes as data
columns, ``name`` attribute is always stored in the column descriptors as
above.

Column metadata
~~~~~~~~~~~~~~~

``pandas_type`` is the logical type of the column, and is one of:

* Boolean: ``'bool'``
* Integers: ``'int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64'``
* Floats: ``'float16', 'float32', 'float64'``
* Date and Time Types: ``'datetime', 'datetimetz'``, ``'timedelta'``
* String: ``'unicode', 'bytes'``
* Categorical: ``'categorical'``
* Other Python objects: ``'object'``

The ``numpy_type`` is the physical storage type of the column, which is the
result of ``str(dtype)`` for the underlying NumPy array that holds the data. So
for ``datetimetz`` this is ``datetime64[ns]`` and for categorical, it may be
any of the supported integer categorical types.

The ``metadata`` field is ``None`` except for:

* ``datetimetz``: ``{'timezone': zone, 'unit': 'ns'}``, e.g. ``{'timezone',
  'America/New_York', 'unit': 'ns'}``. The ``'unit'`` is optional, and if
  omitted it is assumed to be nanoseconds.
* ``categorical``: ``{'num_categories': K, 'ordered': is_ordered, 'type': $TYPE}``

    * Here ``'type'`` is optional, and can be a nested pandas type specification
      here (but not categorical)

* ``unicode``: ``{'encoding': encoding}``

    * The encoding is optional, and if not present is UTF-8

* ``object``: ``{'encoding': encoding}``. Objects can be serialized and stored
  in ``BYTE_ARRAY`` Parquet columns. The encoding can be one of:

    * ``'pickle'``
    * ``'bson'``
    * ``'json'``

* ``timedelta``: ``{'unit': 'ns'}``. The ``'unit'`` is optional, and if omitted
  it is assumed to be nanoseconds. This metadata is optional altogether

For types other than these, the ``'metadata'`` key can be
omitted. Implementations can assume ``None`` if the key is not present.

As an example of fully-formed metadata:

.. code-block:: text

   {'index_columns': ['__index_level_0__'],
    'column_indexes': [
        {'name': None,
         'field_name': 'None',
         'pandas_type': 'unicode',
         'numpy_type': 'object',
         'metadata': {'encoding': 'UTF-8'}}
    ],
    'columns': [
        {'name': 'c0',
         'field_name': 'c0',
         'pandas_type': 'int8',
         'numpy_type': 'int8',
         'metadata': None},
        {'name': 'c1',
         'field_name': 'c1',
         'pandas_type': 'bytes',
         'numpy_type': 'object',
         'metadata': None},
        {'name': 'c2',
         'field_name': 'c2',
         'pandas_type': 'categorical',
         'numpy_type': 'int16',
         'metadata': {'num_categories': 1000, 'ordered': False}},
        {'name': 'c3',
         'field_name': 'c3',
         'pandas_type': 'datetimetz',
         'numpy_type': 'datetime64[ns]',
         'metadata': {'timezone': 'America/Los_Angeles'}},
        {'name': 'c4',
         'field_name': 'c4',
         'pandas_type': 'object',
         'numpy_type': 'object',
         'metadata': {'encoding': 'pickle'}},
        {'name': None,
         'field_name': '__index_level_0__',
         'pandas_type': 'int64',
         'numpy_type': 'int64',
         'metadata': None}
    ],
    'pandas_version': '1.4.0',
    'creator': {
      'library': 'pyarrow',
      'version': '0.13.0'
    }}
.. _meeting:

==================
Developer meetings
==================

We hold regular developer meetings on the second Wednesday
of each month at 18:00 UTC. These meetings and their minutes are open to
the public. All are welcome to join.

Minutes
-------

The minutes of past meetings are available in `this Google Document <https://docs.google.com/document/d/1tGbTiYORHiSPgVMXawiweGJlBw5dOkVJLY-licoBmBU/edit?usp=sharing>`__.

Calendar
--------

This calendar shows all the developer meetings.

.. raw:: html

   <iframe src="https://calendar.google.com/calendar/embed?src=pgbn14p6poja8a1cf2dv2jhrmg%40group.calendar.google.com" style="border: 0" width="800" height="600" frameborder="0" scrolling="no"></iframe>

You can subscribe to this calendar with the following links:

* `iCal <https://calendar.google.com/calendar/ical/pgbn14p6poja8a1cf2dv2jhrmg%40group.calendar.google.com/public/basic.ics>`__
* `Google calendar <https://calendar.google.com/calendar/r?cid=pgbn14p6poja8a1cf2dv2jhrmg@group.calendar.google.com>`__

Additionally, we'll sometimes have one-off meetings on specific topics.
These will be published on the same calendar.
.. _contributing_environment:

{{ header }}

==================================
Creating a development environment
==================================

To test out code changes, you'll need to build pandas from source, which
requires a C/C++ compiler and Python environment. If you're making documentation
changes, you can skip to :ref:`contributing to the documentation <contributing_documentation>` but if you skip
creating the development environment you won't be able to build the documentation
locally before pushing your changes.

.. contents:: Table of contents:
   :local:


Creating an environment using Docker
--------------------------------------

Instead of manually setting up a development environment, you can use `Docker
<https://docs.docker.com/get-docker/>`_ to automatically create the environment with just several
commands. pandas provides a ``DockerFile`` in the root directory to build a Docker image
with a full pandas development environment.

**Docker Commands**

Pass your GitHub username in the ``DockerFile`` to use your own fork::

    # Build the image pandas-yourname-env
    docker build --tag pandas-yourname-env .
    # Run a container and bind your local forked repo, pandas-yourname, to the container
    docker run -it --rm -v path-to-pandas-yourname:/home/pandas-yourname pandas-yourname-env

Even easier, you can integrate Docker with the following IDEs:

**Visual Studio Code**

You can use the DockerFile to launch a remote session with Visual Studio Code,
a popular free IDE, using the ``.devcontainer.json`` file.
See https://code.visualstudio.com/docs/remote/containers for details.

**PyCharm (Professional)**

Enable Docker support and use the Services tool window to build and manage images as well as
run and interact with containers.
See https://www.jetbrains.com/help/pycharm/docker.html for details.

Note that you might need to rebuild the C extensions if/when you merge with upstream/main using::

    python setup.py build_ext -j 4


Creating an environment without Docker
---------------------------------------

Installing a C compiler
~~~~~~~~~~~~~~~~~~~~~~~

pandas uses C extensions (mostly written using Cython) to speed up certain
operations. To install pandas from source, you need to compile these C
extensions, which means you need a C compiler. This process depends on which
platform you're using.

If you have setup your environment using ``conda``, the packages ``c-compiler``
and ``cxx-compiler`` will install a fitting compiler for your platform that is
compatible with the remaining conda packages. On Windows and macOS, you will
also need to install the SDKs as they have to be distributed separately.
These packages will automatically be installed by using the ``pandas``
``environment.yml`` file.

**Windows**

You will need `Build Tools for Visual Studio 2019
<https://visualstudio.microsoft.com/downloads/>`_.

.. warning::
	You DO NOT need to install Visual Studio 2019.
	You only need "Build Tools for Visual Studio 2019" found by
	scrolling down to "All downloads" -> "Tools for Visual Studio 2019".
	In the installer, select the "C++ build tools" workload.

You can install the necessary components on the commandline using
`vs_buildtools.exe <https://download.visualstudio.microsoft.com/download/pr/9a26f37e-6001-429b-a5db-c5455b93953c/460d80ab276046de2455a4115cc4e2f1e6529c9e6cb99501844ecafd16c619c4/vs_BuildTools.exe>`_:

.. code::

    vs_buildtools.exe --quiet --wait --norestart --nocache ^
        --installPath C:\BuildTools ^
        --add "Microsoft.VisualStudio.Workload.VCTools;includeRecommended" ^
        --add Microsoft.VisualStudio.Component.VC.v141 ^
        --add Microsoft.VisualStudio.Component.VC.v141.x86.x64 ^
        --add Microsoft.VisualStudio.Component.Windows10SDK.17763

To setup the right paths on the commandline, call
``"C:\BuildTools\VC\Auxiliary\Build\vcvars64.bat" -vcvars_ver=14.16 10.0.17763.0``.

**macOS**

To use the ``conda``-based compilers, you will need to install the
Developer Tools using ``xcode-select --install``. Otherwise
information about compiler installation can be found here:
https://devguide.python.org/setup/#macos

**Linux**

For Linux-based ``conda`` installations, you won't have to install any
additional components outside of the conda environment. The instructions
below are only needed if your setup isn't based on conda environments.

Some Linux distributions will come with a pre-installed C compiler. To find out
which compilers (and versions) are installed on your system::

    # for Debian/Ubuntu:
    dpkg --list | grep compiler
    # for Red Hat/RHEL/CentOS/Fedora:
    yum list installed | grep -i --color compiler

`GCC (GNU Compiler Collection) <https://gcc.gnu.org/>`_, is a widely used
compiler, which supports C and a number of other languages. If GCC is listed
as an installed compiler nothing more is required. If no C compiler is
installed (or you wish to install a newer version) you can install a compiler
(GCC in the example code below) with::

    # for recent Debian/Ubuntu:
    sudo apt install build-essential
    # for Red Had/RHEL/CentOS/Fedora
    yum groupinstall "Development Tools"

For other Linux distributions, consult your favorite search engine for
compiler installation instructions.

Let us know if you have any difficulties by opening an issue or reaching out on `Gitter <https://gitter.im/pydata/pandas/>`_.

Creating a Python environment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now create an isolated pandas development environment:

* Install either `Anaconda <https://www.anaconda.com/products/individual>`_, `miniconda
  <https://docs.conda.io/en/latest/miniconda.html>`_, or `miniforge <https://github.com/conda-forge/miniforge>`_
* Make sure your conda is up to date (``conda update conda``)
* Make sure that you have :any:`cloned the repository <contributing.forking>`
* ``cd`` to the pandas source directory

We'll now kick off a three-step process:

1. Install the build dependencies
2. Build and install pandas
3. Install the optional dependencies

.. code-block:: none

   # Create and activate the build environment
   conda env create -f environment.yml
   conda activate pandas-dev

   # or with older versions of Anaconda:
   source activate pandas-dev

   # Build and install pandas
   python setup.py build_ext -j 4
   python -m pip install -e . --no-build-isolation --no-use-pep517

At this point you should be able to import pandas from your locally built version::

   $ python
   >>> import pandas
   >>> print(pandas.__version__)
   0.22.0.dev0+29.g4ad6d4d74

This will create the new environment, and not touch any of your existing environments,
nor any existing Python installation.

To view your environments::

      conda info -e

To return to your root environment::

      conda deactivate

See the full conda docs `here <https://conda.io/projects/conda/en/latest/>`__.


Creating a Python environment (pip)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you aren't using conda for your development environment, follow these instructions.
You'll need to have at least the :ref:`minimum Python version <install.version>` that pandas supports.
You also need to have ``setuptools`` 51.0.0 or later to build pandas.

**Unix**/**macOS with virtualenv**

.. code-block:: bash

   # Create a virtual environment
   # Use an ENV_DIR of your choice. We'll use ~/virtualenvs/pandas-dev
   # Any parent directories should already exist
   python3 -m venv ~/virtualenvs/pandas-dev

   # Activate the virtualenv
   . ~/virtualenvs/pandas-dev/bin/activate

   # Install the build dependencies
   python -m pip install -r requirements-dev.txt

   # Build and install pandas
   python setup.py build_ext -j 4
   python -m pip install -e . --no-build-isolation --no-use-pep517

**Unix**/**macOS with pyenv**

Consult the docs for setting up pyenv `here <https://github.com/pyenv/pyenv>`__.

.. code-block:: bash

   # Create a virtual environment
   # Use an ENV_DIR of your choice. We'll use ~/Users/<yourname>/.pyenv/versions/pandas-dev

   pyenv virtualenv <version> <name-to-give-it>

   # For instance:
   pyenv virtualenv 3.9.10 pandas-dev

   # Activate the virtualenv
   pyenv activate pandas-dev

   # Now install the build dependencies in the cloned pandas repo
   python -m pip install -r requirements-dev.txt

   # Build and install pandas
   python setup.py build_ext -j 4
   python -m pip install -e . --no-build-isolation --no-use-pep517

**Windows**

Below is a brief overview on how to set-up a virtual environment with Powershell
under Windows. For details please refer to the
`official virtualenv user guide <https://virtualenv.pypa.io/en/latest/user_guide.html#activators>`__

Use an ENV_DIR of your choice. We'll use ~\\virtualenvs\\pandas-dev where
'~' is the folder pointed to by either $env:USERPROFILE (Powershell) or
%USERPROFILE% (cmd.exe) environment variable. Any parent directories
should already exist.

.. code-block:: powershell

   # Create a virtual environment
   python -m venv $env:USERPROFILE\virtualenvs\pandas-dev

   # Activate the virtualenv. Use activate.bat for cmd.exe
   ~\virtualenvs\pandas-dev\Scripts\Activate.ps1

   # Install the build dependencies
   python -m pip install -r requirements-dev.txt

   # Build and install pandas
   python setup.py build_ext -j 4
   python -m pip install -e . --no-build-isolation --no-use-pep517
.. _contributing_codebase:

{{ header }}

=============================
Contributing to the code base
=============================

.. contents:: Table of Contents:
   :local:

Code standards
--------------

Writing good code is not just about what you write. It is also about *how* you
write it. During :ref:`Continuous Integration <contributing.ci>` testing, several
tools will be run to check your code for stylistic errors.
Generating any warnings will cause the test to fail.
Thus, good style is a requirement for submitting code to pandas.

There is a tool in pandas to help contributors verify their changes before
contributing them to the project::

   ./ci/code_checks.sh

The script validates the doctests, formatting in docstrings, static typing, and
imported modules. It is possible to run the checks independently by using the
parameters ``docstring``, ``code``, ``typing``, and ``doctests``
(e.g. ``./ci/code_checks.sh doctests``).

In addition, because a lot of people use our library, it is important that we
do not make sudden changes to the code that could have the potential to break
a lot of user code as a result, that is, we need it to be as *backwards compatible*
as possible to avoid mass breakages.

In addition to ``./ci/code_checks.sh``, some extra checks are run by
``pre-commit`` - see :ref:`here <contributing.pre-commit>` for how to
run them.

Additional standards are outlined on the :ref:`pandas code style guide <code_style>`.

.. _contributing.pre-commit:

Pre-commit
----------

Additionally, :ref:`Continuous Integration <contributing.ci>` will run code formatting checks
like ``black``, ``flake8``, ``isort``, and ``cpplint`` and more using `pre-commit hooks <https://pre-commit.com/>`_
Any warnings from these checks will cause the :ref:`Continuous Integration <contributing.ci>` to fail; therefore,
it is helpful to run the check yourself before submitting code. This
can be done by installing ``pre-commit``::

    pip install pre-commit

and then running::

    pre-commit install

from the root of the pandas repository. Now all of the styling checks will be
run each time you commit changes without your needing to run each one manually.
In addition, using ``pre-commit`` will also allow you to more easily
remain up-to-date with our code checks as they change.

Note that if needed, you can skip these checks with ``git commit --no-verify``.

If you don't want to use ``pre-commit`` as part of your workflow, you can still use it
to run its checks with::

    pre-commit run --files <files you have modified>

without needing to have done ``pre-commit install`` beforehand.

If you want to run checks on all recently committed files on upstream/main you can use::

    pre-commit run --from-ref=upstream/main --to-ref=HEAD --all-files

without needing to have done ``pre-commit install`` beforehand.

.. note::

    If you have conflicting installations of ``virtualenv``, then you may get an
    error - see `here <https://github.com/pypa/virtualenv/issues/1875>`_.

    Also, due to a `bug in virtualenv <https://github.com/pypa/virtualenv/issues/1986>`_,
    you may run into issues if you're using conda. To solve this, you can downgrade
    ``virtualenv`` to version ``20.0.33``.

Optional dependencies
---------------------

Optional dependencies (e.g. matplotlib) should be imported with the private helper
``pandas.compat._optional.import_optional_dependency``. This ensures a
consistent error message when the dependency is not met.

All methods using an optional dependency should include a test asserting that an
``ImportError`` is raised when the optional dependency is not found. This test
should be skipped if the library is present.

All optional dependencies should be documented in
:ref:`install.optional_dependencies` and the minimum required version should be
set in the ``pandas.compat._optional.VERSIONS`` dict.

Backwards compatibility
-----------------------

Please try to maintain backward compatibility. pandas has lots of users with lots of
existing code, so don't break it if at all possible.  If you think breakage is required,
clearly state why as part of the pull request.  Also, be careful when changing method
signatures and add deprecation warnings where needed. Also, add the deprecated sphinx
directive to the deprecated functions or methods.

If a function with the same arguments as the one being deprecated exist, you can use
the ``pandas.util._decorators.deprecate``:

.. code-block:: python

    from pandas.util._decorators import deprecate

    deprecate('old_func', 'new_func', '1.1.0')

Otherwise, you need to do it manually:

.. code-block:: python

    import warnings


    def old_func():
        """Summary of the function.

        .. deprecated:: 1.1.0
           Use new_func instead.
        """
        warnings.warn('Use new_func instead.', FutureWarning, stacklevel=2)
        new_func()


    def new_func():
        pass

You'll also need to

1. Write a new test that asserts a warning is issued when calling with the deprecated argument
2. Update all of pandas existing tests and code to use the new argument

See :ref:`contributing.warnings` for more.

.. _contributing.type_hints:

Type hints
----------

pandas strongly encourages the use of :pep:`484` style type hints. New development should contain type hints and pull requests to annotate existing code are accepted as well!

Style guidelines
~~~~~~~~~~~~~~~~

Type imports should follow the ``from typing import ...`` convention. Some types do not need to be imported since :pep:`585` some builtin constructs, such as ``list`` and ``tuple``, can directly be used for type annotations. So rather than

.. code-block:: python

   import typing

   primes: typing.List[int] = []

You should write

.. code-block:: python

   primes: list[int] = []

``Optional`` should be  avoided in favor of the shorter ``| None``, so instead of

.. code-block:: python

   from typing import Union

   maybe_primes: list[Union[int, None]] = []

or

.. code-block:: python

   from typing import Optional

   maybe_primes: list[Optional[int]] = []

You should write

.. code-block:: python

   from __future__ import annotations  # noqa: F404

   maybe_primes: list[int | None] = []

In some cases in the code base classes may define class variables that shadow builtins. This causes an issue as described in `Mypy 1775 <https://github.com/python/mypy/issues/1775#issuecomment-310969854>`_. The defensive solution here is to create an unambiguous alias of the builtin and use that without your annotation. For example, if you come across a definition like

.. code-block:: python

   class SomeClass1:
       str = None

The appropriate way to annotate this would be as follows

.. code-block:: python

   str_type = str

   class SomeClass2:
       str: str_type = None

In some cases you may be tempted to use ``cast`` from the typing module when you know better than the analyzer. This occurs particularly when using custom inference functions. For example

.. code-block:: python

   from typing import cast

   from pandas.core.dtypes.common import is_number

   def cannot_infer_bad(obj: Union[str, int, float]):

       if is_number(obj):
           ...
       else:  # Reasonably only str objects would reach this but...
           obj = cast(str, obj)  # Mypy complains without this!
	   return obj.upper()

The limitation here is that while a human can reasonably understand that ``is_number`` would catch the ``int`` and ``float`` types mypy cannot make that same inference just yet (see `mypy #5206 <https://github.com/python/mypy/issues/5206>`_. While the above works, the use of ``cast`` is **strongly discouraged**. Where applicable a refactor of the code to appease static analysis is preferable

.. code-block:: python

   def cannot_infer_good(obj: Union[str, int, float]):

       if isinstance(obj, str):
           return obj.upper()
       else:
           ...

With custom types and inference this is not always possible so exceptions are made, but every effort should be exhausted to avoid ``cast`` before going down such paths.

pandas-specific types
~~~~~~~~~~~~~~~~~~~~~

Commonly used types specific to pandas will appear in `pandas._typing <https://github.com/pandas-dev/pandas/blob/main/pandas/_typing.py>`_ and you should use these where applicable. This module is private for now but ultimately this should be exposed to third party libraries who want to implement type checking against pandas.

For example, quite a few functions in pandas accept a ``dtype`` argument. This can be expressed as a string like ``"object"``, a ``numpy.dtype`` like ``np.int64`` or even a pandas ``ExtensionDtype`` like ``pd.CategoricalDtype``. Rather than burden the user with having to constantly annotate all of those options, this can simply be imported and reused from the pandas._typing module

.. code-block:: python

   from pandas._typing import Dtype

   def as_type(dtype: Dtype) -> ...:
       ...

This module will ultimately house types for repeatedly used concepts like "path-like", "array-like", "numeric", etc... and can also hold aliases for commonly appearing parameters like ``axis``. Development of this module is active so be sure to refer to the source for the most up to date list of available types.

Validating type hints
~~~~~~~~~~~~~~~~~~~~~

pandas uses `mypy <http://mypy-lang.org>`_ and `pyright <https://github.com/microsoft/pyright>`_ to statically analyze the code base and type hints. After making any change you can ensure your type hints are correct by running

.. code-block:: shell

   ./ci/code_checks.sh typing

A recent version of ``numpy`` (>=1.21.0) is required for type validation.

.. _contributing.ci:

Testing type hints in code using pandas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. warning::

    * Pandas is not yet a py.typed library (:pep:`561`)!
      The primary purpose of locally declaring pandas as a py.typed library is to test and
      improve the pandas-builtin type annotations.

Until pandas becomes a py.typed library, it is possible to easily experiment with the type
annotations shipped with pandas by creating an empty file named "py.typed" in the pandas
installation folder:

.. code-block:: none

   python -c "import pandas; import pathlib; (pathlib.Path(pandas.__path__[0]) / 'py.typed').touch()"

The existence of the py.typed file signals to type checkers that pandas is already a py.typed
library. This makes type checkers aware of the type annotations shipped with pandas.

Testing with continuous integration
-----------------------------------

The pandas test suite will run automatically on `GitHub Actions <https://github.com/features/actions/>`__ and
`Azure Pipelines <https://azure.microsoft.com/en-us/services/devops/pipelines/>`__
continuous integration services, once your pull request is submitted.
However, if you wish to run the test suite on a branch prior to submitting the pull request,
then the continuous integration services need to be hooked to your GitHub repository. Instructions are here
for `GitHub Actions <https://docs.github.com/en/actions/>`__ and
`Azure Pipelines <https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops>`__.

A pull-request will be considered for merging when you have an all 'green' build. If any tests are failing,
then you will get a red 'X', where you can click through to see the individual failed tests.
This is an example of a green build.

.. image:: ../_static/ci.png

.. _contributing.tdd:


Test-driven development/code writing
------------------------------------

pandas is serious about testing and strongly encourages contributors to embrace
`test-driven development (TDD) <https://en.wikipedia.org/wiki/Test-driven_development>`_.
This development process "relies on the repetition of a very short development cycle:
first the developer writes an (initially failing) automated test case that defines a desired
improvement or new function, then produces the minimum amount of code to pass that test."
So, before actually writing any code, you should write your tests.  Often the test can be
taken from the original GitHub issue.  However, it is always worth considering additional
use cases and writing corresponding tests.

Adding tests is one of the most common requests after code is pushed to pandas.  Therefore,
it is worth getting in the habit of writing tests ahead of time so this is never an issue.

Writing tests
~~~~~~~~~~~~~

All tests should go into the ``tests`` subdirectory of the specific package.
This folder contains many current examples of tests, and we suggest looking to these for
inspiration. Please reference our :ref:`testing location guide <test_organization>` if you are unsure
where to place a new unit test.

Using ``pytest``
~~~~~~~~~~~~~~~~

Test structure
^^^^^^^^^^^^^^

pandas existing test structure is *mostly* class-based, meaning that you will typically find tests wrapped in a class.

.. code-block:: python

    class TestReallyCoolFeature:
        pass

Going forward, we are moving to a more *functional* style using the `pytest <https://docs.pytest.org/en/latest/>`__ framework, which offers a richer testing
framework that will facilitate testing and developing. Thus, instead of writing test classes, we will write test functions like this:

.. code-block:: python

    def test_really_cool_feature():
        pass

Preferred idioms
^^^^^^^^^^^^^^^^

* Functional tests named ``def test_*`` and *only* take arguments that are either fixtures or parameters.
* Use a bare ``assert`` for testing scalars and truth-testing
* Use ``tm.assert_series_equal(result, expected)`` and ``tm.assert_frame_equal(result, expected)`` for comparing :class:`Series` and :class:`DataFrame` results respectively.
* Use `@pytest.mark.parameterize <https://docs.pytest.org/en/latest/how-to/parametrize.html>`__ when testing multiple cases.
* Use `pytest.mark.xfail <https://docs.pytest.org/en/latest/reference/reference.html?#pytest.mark.xfail>`__ when a test case is expected to fail.
* Use `pytest.mark.skip <https://docs.pytest.org/en/latest/reference/reference.html?#pytest.mark.skip>`__ when a test case is never expected to pass.
* Use `pytest.param <https://docs.pytest.org/en/latest/reference/reference.html?#pytest-param>`__ when a test case needs a particular mark.
* Use `@pytest.fixture <https://docs.pytest.org/en/latest/reference/reference.html?#pytest-fixture>`__ if multiple tests can share a setup object.

.. warning::

    Do not use ``pytest.xfail`` (which is different than ``pytest.mark.xfail``) since it immediately stops the
    test and does not check if the test will fail. If this is the behavior you desire, use ``pytest.skip`` instead.

If a test is known to fail but the manner in which it fails
is not meant to be captured, use ``pytest.mark.xfail`` It is common to use this method for a test that
exhibits buggy behavior or a non-implemented feature. If
the failing test has flaky behavior, use the argument ``strict=False``. This
will make it so pytest does not fail if the test happens to pass.

Prefer the decorator ``@pytest.mark.xfail`` and the argument ``pytest.param``
over usage within a test so that the test is appropriately marked during the
collection phase of pytest. For xfailing a test that involves multiple
parameters, a fixture, or a combination of these, it is only possible to
xfail during the testing phase. To do so, use the ``request`` fixture:

.. code-block:: python

    def test_xfail(request):
        mark = pytest.mark.xfail(raises=TypeError, reason="Indicate why here")
        request.node.add_marker(mark)

xfail is not to be used for tests involving failure due to invalid user arguments.
For these tests, we need to verify the correct exception type and error message
is being raised, using ``pytest.raises`` instead.

If your test requires working with files or
network connectivity, there is more information on the :wiki:`Testing` of the wiki.


Example
^^^^^^^

Here is an example of a self-contained set of tests in a file ``pandas/tests/test_cool_feature.py``
that illustrate multiple features that we like to use. Please remember to add the Github Issue Number
as a comment to a new test.

.. code-block:: python

   import pytest
   import numpy as np
   import pandas as pd


   @pytest.mark.parametrize('dtype', ['int8', 'int16', 'int32', 'int64'])
   def test_dtypes(dtype):
       assert str(np.dtype(dtype)) == dtype


   @pytest.mark.parametrize(
       'dtype', ['float32', pytest.param('int16', marks=pytest.mark.skip),
                 pytest.param('int32', marks=pytest.mark.xfail(
                     reason='to show how it works'))])
   def test_mark(dtype):
       assert str(np.dtype(dtype)) == 'float32'


   @pytest.fixture
   def series():
       return pd.Series([1, 2, 3])


   @pytest.fixture(params=['int8', 'int16', 'int32', 'int64'])
   def dtype(request):
       return request.param


   def test_series(series, dtype):
       # GH <issue_number>
       result = series.astype(dtype)
       assert result.dtype == dtype

       expected = pd.Series([1, 2, 3], dtype=dtype)
       tm.assert_series_equal(result, expected)


A test run of this yields

.. code-block:: shell

   ((pandas) bash-3.2$ pytest  test_cool_feature.py  -v
   =========================== test session starts ===========================
   platform darwin -- Python 3.6.2, pytest-3.6.0, py-1.4.31, pluggy-0.4.0
   collected 11 items

   tester.py::test_dtypes[int8] PASSED
   tester.py::test_dtypes[int16] PASSED
   tester.py::test_dtypes[int32] PASSED
   tester.py::test_dtypes[int64] PASSED
   tester.py::test_mark[float32] PASSED
   tester.py::test_mark[int16] SKIPPED
   tester.py::test_mark[int32] xfail
   tester.py::test_series[int8] PASSED
   tester.py::test_series[int16] PASSED
   tester.py::test_series[int32] PASSED
   tester.py::test_series[int64] PASSED

Tests that we have ``parametrized`` are now accessible via the test name, for example we could run these with ``-k int8`` to sub-select *only* those tests which match ``int8``.


.. code-block:: shell

   ((pandas) bash-3.2$ pytest  test_cool_feature.py  -v -k int8
   =========================== test session starts ===========================
   platform darwin -- Python 3.6.2, pytest-3.6.0, py-1.4.31, pluggy-0.4.0
   collected 11 items

   test_cool_feature.py::test_dtypes[int8] PASSED
   test_cool_feature.py::test_series[int8] PASSED


.. _using-hypothesis:

Using ``hypothesis``
~~~~~~~~~~~~~~~~~~~~

Hypothesis is a library for property-based testing. Instead of explicitly
parametrizing a test, you can describe *all* valid inputs and let Hypothesis
try to find a failing input.  Even better, no matter how many random examples
it tries, Hypothesis always reports a single minimal counterexample to your
assertions - often an example that you would never have thought to test.

See `Getting Started with Hypothesis <https://hypothesis.works/articles/getting-started-with-hypothesis/>`_
for more of an introduction, then `refer to the Hypothesis documentation
for details <https://hypothesis.readthedocs.io/en/latest/index.html>`_.

.. code-block:: python

    import json
    from hypothesis import given, strategies as st

    any_json_value = st.deferred(lambda: st.one_of(
        st.none(), st.booleans(), st.floats(allow_nan=False), st.text(),
        st.lists(any_json_value), st.dictionaries(st.text(), any_json_value)
    ))


    @given(value=any_json_value)
    def test_json_roundtrip(value):
        result = json.loads(json.dumps(value))
        assert value == result

This test shows off several useful features of Hypothesis, as well as
demonstrating a good use-case: checking properties that should hold over
a large or complicated domain of inputs.

To keep the pandas test suite running quickly, parametrized tests are
preferred if the inputs or logic are simple, with Hypothesis tests reserved
for cases with complex logic or where there are too many combinations of
options or subtle interactions to test (or think of!) all of them.

.. _contributing.warnings:

Testing warnings
~~~~~~~~~~~~~~~~

By default, the :ref:`Continuous Integration <contributing.ci>` will fail if any unhandled warnings are emitted.

If your change involves checking that a warning is actually emitted, use
``tm.assert_produces_warning(ExpectedWarning)``.


.. code-block:: python

   import pandas._testing as tm


   df = pd.DataFrame()
   with tm.assert_produces_warning(FutureWarning):
       df.some_operation()

We prefer this to the ``pytest.warns`` context manager because ours checks that the warning's
stacklevel is set correctly. The stacklevel is what ensure the *user's* file name and line number
is printed in the warning, rather than something internal to pandas. It represents the number of
function calls from user code (e.g. ``df.some_operation()``) to the function that actually emits
the warning. Our linter will fail the build if you use ``pytest.warns`` in a test.

If you have a test that would emit a warning, but you aren't actually testing the
warning itself (say because it's going to be removed in the future, or because we're
matching a 3rd-party library's behavior), then use ``pytest.mark.filterwarnings`` to
ignore the error.

.. code-block:: python

   @pytest.mark.filterwarnings("ignore:msg:category")
   def test_thing(self):
       ...

If the test generates a warning of class ``category`` whose message starts
with ``msg``, the warning will be ignored and the test will pass.

If you need finer-grained control, you can use Python's usual
`warnings module <https://docs.python.org/3/library/warnings.html>`__
to control whether a warning is ignored / raised at different places within
a single test.

.. code-block:: python

   with warnings.catch_warnings():
       warnings.simplefilter("ignore", FutureWarning)
       # Or use warnings.filterwarnings(...)

Alternatively, consider breaking up the unit test.


Running the test suite
----------------------

The tests can then be run directly inside your Git clone (without having to
install pandas) by typing::

    pytest pandas

Often it is worth running only a subset of tests first around your changes before running the
entire suite.

The easiest way to do this is with::

    pytest pandas/path/to/test.py -k regex_matching_test_name

Or with one of the following constructs::

    pytest pandas/tests/[test-module].py
    pytest pandas/tests/[test-module].py::[TestClass]
    pytest pandas/tests/[test-module].py::[TestClass]::[test_method]

Using `pytest-xdist <https://pypi.org/project/pytest-xdist>`_, one can
speed up local testing on multicore machines. To use this feature, you will
need to install ``pytest-xdist`` via::

    pip install pytest-xdist

Two scripts are provided to assist with this.  These scripts distribute
testing across 4 threads.

On Unix variants, one can type::

    test_fast.sh

On Windows, one can type::

    test_fast.bat

This can significantly reduce the time it takes to locally run tests before
submitting a pull request.

For more, see the `pytest <https://docs.pytest.org/en/latest/>`_ documentation.

Furthermore one can run

.. code-block:: python

   pd.test()

with an imported pandas to run tests similarly.

Running the performance test suite
----------------------------------

Performance matters and it is worth considering whether your code has introduced
performance regressions. pandas is in the process of migrating to
`asv benchmarks <https://github.com/airspeed-velocity/asv>`__
to enable easy monitoring of the performance of critical pandas operations.
These benchmarks are all found in the ``pandas/asv_bench`` directory, and the
test results can be found `here <https://pandas.pydata.org/speed/pandas/>`__.

To use all features of asv, you will need either ``conda`` or
``virtualenv``. For more details please check the `asv installation
webpage <https://asv.readthedocs.io/en/latest/installing.html>`_.

To install asv::

    pip install git+https://github.com/airspeed-velocity/asv

If you need to run a benchmark, change your directory to ``asv_bench/`` and run::

    asv continuous -f 1.1 upstream/main HEAD

You can replace ``HEAD`` with the name of the branch you are working on,
and report benchmarks that changed by more than 10%.
The command uses ``conda`` by default for creating the benchmark
environments. If you want to use virtualenv instead, write::

    asv continuous -f 1.1 -E virtualenv upstream/main HEAD

The ``-E virtualenv`` option should be added to all ``asv`` commands
that run benchmarks. The default value is defined in ``asv.conf.json``.

Running the full benchmark suite can be an all-day process, depending on your
hardware and its resource utilization. However, usually it is sufficient to paste
only a subset of the results into the pull request to show that the committed changes
do not cause unexpected performance regressions.  You can run specific benchmarks
using the ``-b`` flag, which takes a regular expression. For example, this will
only run benchmarks from a ``pandas/asv_bench/benchmarks/groupby.py`` file::

    asv continuous -f 1.1 upstream/main HEAD -b ^groupby

If you want to only run a specific group of benchmarks from a file, you can do it
using ``.`` as a separator. For example::

    asv continuous -f 1.1 upstream/main HEAD -b groupby.GroupByMethods

will only run the ``GroupByMethods`` benchmark defined in ``groupby.py``.

You can also run the benchmark suite using the version of ``pandas``
already installed in your current Python environment. This can be
useful if you do not have virtualenv or conda, or are using the
``setup.py develop`` approach discussed above; for the in-place build
you need to set ``PYTHONPATH``, e.g.
``PYTHONPATH="$PWD/.." asv [remaining arguments]``.
You can run benchmarks using an existing Python
environment by::

    asv run -e -E existing

or, to use a specific Python interpreter,::

    asv run -e -E existing:python3.6

This will display stderr from the benchmarks, and use your local
``python`` that comes from your ``$PATH``.

Information on how to write a benchmark and how to use asv can be found in the
`asv documentation <https://asv.readthedocs.io/en/latest/writing_benchmarks.html>`_.

Documenting your code
---------------------

Changes should be reflected in the release notes located in ``doc/source/whatsnew/vx.y.z.rst``.
This file contains an ongoing change log for each release.  Add an entry to this file to
document your fix, enhancement or (unavoidable) breaking change.  Make sure to include the
GitHub issue number when adding your entry (using ``:issue:`1234``` where ``1234`` is the
issue/pull request number). Your entry should be written using full sentences and proper
grammar.

When mentioning parts of the API, use a Sphinx ``:func:``, ``:meth:``, or ``:class:``
directive as appropriate. Not all public API functions and methods have a
documentation page; ideally links would only be added if they resolve. You can
usually find similar examples by checking the release notes for one of the previous
versions.

If your code is a bugfix, add your entry to the relevant bugfix section. Avoid
adding to the ``Other`` section; only in rare cases should entries go there.
Being as concise as possible, the description of the bug should include how the
user may encounter it and an indication of the bug itself, e.g.
"produces incorrect results" or "incorrectly raises". It may be necessary to also
indicate the new behavior.

If your code is an enhancement, it is most likely necessary to add usage
examples to the existing documentation.  This can be done following the section
regarding :ref:`documentation <contributing_documentation>`.
Further, to let users know when this feature was added, the ``versionadded``
directive is used. The sphinx syntax for that is:

.. code-block:: rst

  .. versionadded:: 1.1.0

This will put the text *New in version 1.1.0* wherever you put the sphinx
directive. This should also be put in the docstring when adding a new function
or method (`example <https://github.com/pandas-dev/pandas/blob/v0.20.2/pandas/core/frame.py#L1495>`__)
or a new keyword argument (`example <https://github.com/pandas-dev/pandas/blob/v0.20.2/pandas/core/generic.py#L568>`__).
.. _contributing_documentation:

{{ header }}

=================================
Contributing to the documentation
=================================

Contributing to the documentation benefits everyone who uses pandas.
We encourage you to help us improve the documentation, and
you don't have to be an expert on pandas to do so! In fact,
there are sections of the docs that are worse off after being written by
experts. If something in the docs doesn't make sense to you, updating the
relevant section after you figure it out is a great way to ensure it will help
the next person.

.. contents:: Documentation:
   :local:


About the pandas documentation
--------------------------------

The documentation is written in **reStructuredText**, which is almost like writing
in plain English, and built using `Sphinx <https://www.sphinx-doc.org/en/master/>`__. The
Sphinx Documentation has an excellent `introduction to reST
<https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html>`__. Review the Sphinx docs to perform more
complex changes to the documentation as well.

Some other important things to know about the docs:

* The pandas documentation consists of two parts: the docstrings in the code
  itself and the docs in this folder ``doc/``.

  The docstrings provide a clear explanation of the usage of the individual
  functions, while the documentation in this folder consists of tutorial-like
  overviews per topic together with some other information (what's new,
  installation, etc).

* The docstrings follow a pandas convention, based on the **Numpy Docstring
  Standard**. Follow the :ref:`pandas docstring guide <docstring>` for detailed
  instructions on how to write a correct docstring.

  .. toctree::
     :maxdepth: 2

     contributing_docstring.rst

* The tutorials make heavy use of the `IPython directive
  <https://matplotlib.org/sampledoc/ipython_directive.html>`_ sphinx extension.
  This directive lets you put code in the documentation which will be run
  during the doc build. For example::

      .. ipython:: python

          x = 2
          x**3

  will be rendered as::

      In [1]: x = 2

      In [2]: x**3
      Out[2]: 8

  Almost all code examples in the docs are run (and the output saved) during the
  doc build. This approach means that code examples will always be up to date,
  but it does make the doc building a bit more complex.

* Our API documentation files in ``doc/source/reference`` house the auto-generated
  documentation from the docstrings. For classes, there are a few subtleties
  around controlling which methods and attributes have pages auto-generated.

  We have two autosummary templates for classes.

  1. ``_templates/autosummary/class.rst``. Use this when you want to
     automatically generate a page for every public method and attribute on the
     class. The ``Attributes`` and ``Methods`` sections will be automatically
     added to the class' rendered documentation by numpydoc. See ``DataFrame``
     for an example.

  2. ``_templates/autosummary/class_without_autosummary``. Use this when you
     want to pick a subset of methods / attributes to auto-generate pages for.
     When using this template, you should include an ``Attributes`` and
     ``Methods`` section in the class docstring. See ``CategoricalIndex`` for an
     example.

  Every method should be included in a ``toctree`` in one of the documentation files in
  ``doc/source/reference``, else Sphinx
  will emit a warning.

The utility script ``scripts/validate_docstrings.py`` can be used to get a csv
summary of the API documentation. And also validate common errors in the docstring
of a specific class, function or method. The summary also compares the list of
methods documented in the files in ``doc/source/reference`` (which is used to generate
the `API Reference <https://pandas.pydata.org/pandas-docs/stable/api.html>`_ page)
and the actual public methods.
This will identify methods documented in ``doc/source/reference`` that are not actually
class methods, and existing methods that are not documented in ``doc/source/reference``.


Updating a pandas docstring
-----------------------------

When improving a single function or method's docstring, it is not necessarily
needed to build the full documentation (see next section).
However, there is a script that checks a docstring (for example for the ``DataFrame.mean`` method)::

    python scripts/validate_docstrings.py pandas.DataFrame.mean

This script will indicate some formatting errors if present, and will also
run and test the examples included in the docstring.
Check the :ref:`pandas docstring guide <docstring>` for a detailed guide
on how to format the docstring.

The examples in the docstring ('doctests') must be valid Python code,
that in a deterministic way returns the presented output, and that can be
copied and run by users. This can be checked with the script above, and is
also tested on Travis. A failing doctest will be a blocker for merging a PR.
Check the :ref:`examples <docstring.examples>` section in the docstring guide
for some tips and tricks to get the doctests passing.

When doing a PR with a docstring update, it is good to post the
output of the validation script in a comment on github.


How to build the pandas documentation
---------------------------------------

Requirements
~~~~~~~~~~~~

First, you need to have a development environment to be able to build pandas
(see the docs on :ref:`creating a development environment <contributing_environment>`).

Building the documentation
~~~~~~~~~~~~~~~~~~~~~~~~~~

So how do you build the docs? Navigate to your local
``doc/`` directory in the console and run::

    python make.py html

Then you can find the HTML output in the folder ``doc/build/html/``.

The first time you build the docs, it will take quite a while because it has to run
all the code examples and build all the generated docstring pages. In subsequent
evocations, sphinx will try to only build the pages that have been modified.

If you want to do a full clean build, do::

    python make.py clean
    python make.py html

You can tell ``make.py`` to compile only a single section of the docs, greatly
reducing the turn-around time for checking your changes.

::

    # omit autosummary and API section
    python make.py clean
    python make.py --no-api

    # compile the docs with only a single section, relative to the "source" folder.
    # For example, compiling only this guide (doc/source/development/contributing.rst)
    python make.py clean
    python make.py --single development/contributing.rst

    # compile the reference docs for a single function
    python make.py clean
    python make.py --single pandas.DataFrame.join

    # compile whatsnew and API section (to resolve links in the whatsnew)
    python make.py clean
    python make.py --whatsnew

For comparison, a full documentation build may take 15 minutes, but a single
section may take 15 seconds. Subsequent builds, which only process portions
you have changed, will be faster.

The build will automatically use the number of cores available on your machine
to speed up the documentation build. You can override this::

    python make.py html --num-jobs 4

Open the following file in a web browser to see the full documentation you
just built::

    doc/build/html/index.html

And you'll have the satisfaction of seeing your new and improved documentation!

.. _contributing.dev_docs:

Building main branch documentation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When pull requests are merged into the pandas ``main`` branch, the main parts of
the documentation are also built by Travis-CI. These docs are then hosted `here
<https://pandas.pydata.org/docs/dev/>`__, see also
the :any:`Continuous Integration <contributing.ci>` section.

Previewing changes
------------------

Once, the pull request is submitted, GitHub Actions will automatically build the
documentation. To view the built site:

#. Wait for the ``CI / Web and docs`` check to complete.
#. Click ``Details`` next to it.
#. From the ``Artifacts`` drop-down, click ``docs`` or ``website`` to download
   the site as a ZIP file.
.. _develop.policies:

********
Policies
********

.. _policies.version:

Version policy
~~~~~~~~~~~~~~

.. versionchanged:: 1.0.0

pandas uses a loose variant of semantic versioning (`SemVer`_) to govern
deprecations, API compatibility, and version numbering.

A pandas release number is made up of ``MAJOR.MINOR.PATCH``.

API breaking changes should only occur in **major** releases. These changes
will be documented, with clear guidance on what is changing, why it's changing,
and how to migrate existing code to the new behavior.

Whenever possible, a deprecation path will be provided rather than an outright
breaking change.

pandas will introduce deprecations in **minor** releases. These deprecations
will preserve the existing behavior while emitting a warning that provide
guidance on:

* How to achieve similar behavior if an alternative is available
* The pandas version in which the deprecation will be enforced.

We will not introduce new deprecations in patch releases.

Deprecations will only be enforced in **major** releases. For example, if a
behavior is deprecated in pandas 1.2.0, it will continue to work, with a
warning, for all releases in the 1.x series. The behavior will change and the
deprecation removed in the next major release (2.0.0).

.. note::

   pandas will sometimes make *behavior changing* bug fixes, as part of
   minor or patch releases. Whether or not a change is a bug fix or an
   API-breaking change is a judgement call. We'll do our best, and we
   invite you to participate in development discussion on the issue
   tracker or mailing list.

These policies do not apply to features marked as **experimental** in the documentation.
pandas may change the behavior of experimental features at any time.

Python support
~~~~~~~~~~~~~~

pandas will only drop support for specific Python versions (e.g. 3.6.x, 3.7.x) in
pandas **major** or **minor** releases.

.. _SemVer: https://semver.org
.. _code_style:

{{ header }}

=======================
pandas code style guide
=======================

.. contents:: Table of contents:
   :local:

Patterns
========

We use a ``flake8`` plugin, `pandas-dev-flaker <https://github.com/pandas-dev/pandas-dev-flaker>`_, to
check our codebase for unwanted patterns. See its ``README`` for the up-to-date list of rules we enforce.

Miscellaneous
=============

Reading from a url
------------------

**Good:**

.. code-block:: python

    from pandas.io.common import urlopen

    with urlopen("http://www.google.com") as url:
        raw_text = url.read()
.. _roadmap:

=======
Roadmap
=======

This page provides an overview of the major themes in pandas' development. Each of
these items requires a relatively large amount of effort to implement. These may
be achieved more quickly with dedicated funding or interest from contributors.

An item being on the roadmap does not mean that it will *necessarily* happen, even
with unlimited funding. During the implementation period we may discover issues
preventing the adoption of the feature.

Additionally, an item *not* being on the roadmap does not exclude it from inclusion
in pandas. The roadmap is intended for larger, fundamental changes to the project that
are likely to take months or years of developer time. Smaller-scoped items will continue
to be tracked on our `issue tracker <https://github.com/pandas-dev/pandas/issues>`__.

See :ref:`roadmap.evolution` for proposing changes to this document.

Extensibility
-------------

pandas :ref:`extending.extension-types` allow for extending NumPy types with custom
data types and array storage. pandas uses extension types internally, and provides
an interface for 3rd-party libraries to define their own custom data types.

Many parts of pandas still unintentionally convert data to a NumPy array.
These problems are especially pronounced for nested data.

We'd like to improve the handling of extension arrays throughout the library,
making their behavior more consistent with the handling of NumPy arrays. We'll do this
by cleaning up pandas' internals and adding new methods to the extension array interface.

String data type
----------------

Currently, pandas stores text data in an ``object`` -dtype NumPy array.
The current implementation has two primary drawbacks: First, ``object`` -dtype
is not specific to strings: any Python object can be stored in an ``object`` -dtype
array, not just strings. Second: this is not efficient. The NumPy memory model
isn't especially well-suited to variable width text data.

To solve the first issue, we propose a new extension type for string data. This
will initially be opt-in, with users explicitly requesting ``dtype="string"``.
The array backing this string dtype may initially be the current implementation:
an ``object`` -dtype NumPy array of Python strings.

To solve the second issue (performance), we'll explore alternative in-memory
array libraries (for example, Apache Arrow). As part of the work, we may
need to implement certain operations expected by pandas users (for example
the algorithm used in, ``Series.str.upper``). That work may be done outside of
pandas.

Consistent missing value handling
---------------------------------

Currently, pandas handles missing data differently for different data types. We
use different types to indicate that a value is missing (``np.nan`` for
floating-point data, ``np.nan`` or ``None`` for object-dtype data -- typically
strings or booleans -- with missing values, and ``pd.NaT`` for datetimelike
data). Integer data cannot store missing data or are cast to float. In addition,
pandas 1.0 introduced a new missing value sentinel, ``pd.NA``, which is being
used for the experimental nullable integer, boolean, and string data types.

These different missing values have different behaviors in user-facing
operations. Specifically, we introduced different semantics for the nullable
data types for certain operations (e.g. propagating in comparison operations
instead of comparing as False).

Long term, we want to introduce consistent missing data handling for all data
types. This includes consistent behavior in all operations (indexing, arithmetic
operations, comparisons, etc.). There has been discussion of eventually making
the new semantics the default.

This has been discussed at :issue:`28095` (and
linked issues), and described in more detail in this
`design doc <https://hackmd.io/@jorisvandenbossche/Sk0wMeAmB>`__.

Apache Arrow interoperability
-----------------------------

`Apache Arrow <https://arrow.apache.org>`__ is a cross-language development
platform for in-memory data. The Arrow logical types are closely aligned with
typical pandas use cases.

We'd like to provide better-integrated support for Arrow memory and data types
within pandas. This will let us take advantage of its I/O capabilities and
provide for better interoperability with other languages and libraries
using Arrow.

Block manager rewrite
---------------------

We'd like to replace pandas current internal data structures (a collection of
1 or 2-D arrays) with a simpler collection of 1-D arrays.

pandas internal data model is quite complex. A DataFrame is made up of
one or more 2-dimensional "blocks", with one or more blocks per dtype. This
collection of 2-D arrays is managed by the BlockManager.

The primary benefit of the BlockManager is improved performance on certain
operations (construction from a 2D array, binary operations, reductions across the columns),
especially for wide DataFrames. However, the BlockManager substantially increases the
complexity and maintenance burden of pandas.

By replacing the BlockManager we hope to achieve

* Substantially simpler code
* Easier extensibility with new logical types
* Better user control over memory use and layout
* Improved micro-performance
* Option to provide a C / Cython API to pandas' internals

See `these design documents <https://dev.pandas.io/pandas2/internal-architecture.html#removal-of-blockmanager-new-dataframe-internals>`__
for more.

Decoupling of indexing and internals
------------------------------------

The code for getting and setting values in pandas' data structures needs refactoring.
In particular, we must clearly separate code that converts keys (e.g., the argument
to ``DataFrame.loc``) to positions from code that uses these positions to get
or set values. This is related to the proposed BlockManager rewrite. Currently, the
BlockManager sometimes uses label-based, rather than position-based, indexing.
We propose that it should only work with positional indexing, and the translation of keys
to positions should be entirely done at a higher level.

Indexing is a complicated API with many subtleties. This refactor will require care
and attention. More details are discussed at :wiki:`(Tentative)-rules-for-restructuring-indexing-code`

Numba-accelerated operations
----------------------------

`Numba <https://numba.pydata.org>`__ is a JIT compiler for Python code. We'd like to provide
ways for users to apply their own Numba-jitted functions where pandas accepts user-defined functions
(for example, :meth:`Series.apply`, :meth:`DataFrame.apply`, :meth:`DataFrame.applymap`,
and in groupby and window contexts). This will improve the performance of
user-defined-functions in these operations by staying within compiled code.

Performance monitoring
----------------------

pandas uses `airspeed velocity <https://asv.readthedocs.io/en/stable/>`__ to
monitor for performance regressions. ASV itself is a fabulous tool, but requires
some additional work to be integrated into an open source project's workflow.

The `asv-runner <https://github.com/asv-runner>`__ organization, currently made up
of pandas maintainers, provides tools built on top of ASV. We have a physical
machine for running a number of project's benchmarks, and tools managing the
benchmark runs and reporting on results.

We'd like to fund improvements and maintenance of these tools to

* Be more stable. Currently, they're maintained on the nights and weekends when
  a maintainer has free time.
* Tune the system for benchmarks to improve stability, following
  https://pyperf.readthedocs.io/en/latest/system.html
* Build a GitHub bot to request ASV runs *before* a PR is merged. Currently, the
  benchmarks are only run nightly.

.. _roadmap.evolution:

Roadmap evolution
-----------------

pandas continues to evolve. The direction is primarily determined by community
interest. Everyone is welcome to review existing items on the roadmap and
to propose a new item.

Each item on the roadmap should be a short summary of a larger design proposal.
The proposal should include

1. Short summary of the changes, which would be appropriate for inclusion in
   the roadmap if accepted.
2. Motivation for the changes.
3. An explanation of why the change is in scope for pandas.
4. Detailed design: Preferably with example-usage (even if not implemented yet)
   and API documentation
5. API Change: Any API changes that may result from the proposal.

That proposal may then be submitted as a GitHub issue, where the pandas maintainers
can review and comment on the design. The `pandas mailing list <https://mail.python.org/mailman/listinfo/pandas-dev>`__
should be notified of the proposal.

When there's agreement that an implementation
would be welcome, the roadmap should be updated to include the summary and a
link to the discussion issue.

Completed items
---------------

This section records now completed items from the pandas roadmap.

Documentation improvements
~~~~~~~~~~~~~~~~~~~~~~~~~~

We improved the pandas documentation

* The pandas community worked with others to build the `pydata-sphinx-theme`_,
  which is now used for https://pandas.pydata.org/docs/ (:issue:`15556`).
* :ref:`getting_started` contains a number of resources intended for new
  pandas users coming from a variety of backgrounds (:issue:`26831`).

.. _pydata-sphinx-theme: https://github.com/pydata/pydata-sphinx-theme
.. _extending:

{{ header }}

****************
Extending pandas
****************

While pandas provides a rich set of methods, containers, and data types, your
needs may not be fully satisfied. pandas offers a few options for extending
pandas.

.. _extending.register-accessors:

Registering custom accessors
----------------------------

Libraries can use the decorators
:func:`pandas.api.extensions.register_dataframe_accessor`,
:func:`pandas.api.extensions.register_series_accessor`, and
:func:`pandas.api.extensions.register_index_accessor`, to add additional
"namespaces" to pandas objects. All of these follow a similar convention: you
decorate a class, providing the name of attribute to add. The class's
``__init__`` method gets the object being decorated. For example:

.. code-block:: python

   @pd.api.extensions.register_dataframe_accessor("geo")
   class GeoAccessor:
       def __init__(self, pandas_obj):
           self._validate(pandas_obj)
           self._obj = pandas_obj

       @staticmethod
       def _validate(obj):
           # verify there is a column latitude and a column longitude
           if "latitude" not in obj.columns or "longitude" not in obj.columns:
               raise AttributeError("Must have 'latitude' and 'longitude'.")

       @property
       def center(self):
           # return the geographic center point of this DataFrame
           lat = self._obj.latitude
           lon = self._obj.longitude
           return (float(lon.mean()), float(lat.mean()))

       def plot(self):
           # plot this array's data on a map, e.g., using Cartopy
           pass

Now users can access your methods using the ``geo`` namespace:

      >>> ds = pd.DataFrame(
      ...     {"longitude": np.linspace(0, 10), "latitude": np.linspace(0, 20)}
      ... )
      >>> ds.geo.center
      (5.0, 10.0)
      >>> ds.geo.plot()
      # plots data on a map

This can be a convenient way to extend pandas objects without subclassing them.
If you write a custom accessor, make a pull request adding it to our
:ref:`ecosystem` page.

We highly recommend validating the data in your accessor's ``__init__``.
In our ``GeoAccessor``, we validate that the data contains the expected columns,
raising an ``AttributeError`` when the validation fails.
For a ``Series`` accessor, you should validate the ``dtype`` if the accessor
applies only to certain dtypes.


.. _extending.extension-types:

Extension types
---------------

.. warning::

   The :class:`pandas.api.extensions.ExtensionDtype` and :class:`pandas.api.extensions.ExtensionArray` APIs are new and
   experimental. They may change between versions without warning.

pandas defines an interface for implementing data types and arrays that *extend*
NumPy's type system. pandas itself uses the extension system for some types
that aren't built into NumPy (categorical, period, interval, datetime with
timezone).

Libraries can define a custom array and data type. When pandas encounters these
objects, they will be handled properly (i.e. not converted to an ndarray of
objects). Many methods like :func:`pandas.isna` will dispatch to the extension
type's implementation.

If you're building a library that implements the interface, please publicize it
on :ref:`ecosystem.extensions`.

The interface consists of two classes.

:class:`~pandas.api.extensions.ExtensionDtype`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A :class:`pandas.api.extensions.ExtensionDtype` is similar to a ``numpy.dtype`` object. It describes the
data type. Implementors are responsible for a few unique items like the name.

One particularly important item is the ``type`` property. This should be the
class that is the scalar type for your data. For example, if you were writing an
extension array for IP Address data, this might be ``ipaddress.IPv4Address``.

See the `extension dtype source`_ for interface definition.

:class:`pandas.api.extensions.ExtensionDtype` can be registered to pandas to allow creation via a string dtype name.
This allows one to instantiate ``Series`` and ``.astype()`` with a registered string name, for
example ``'category'`` is a registered string accessor for the ``CategoricalDtype``.

See the `extension dtype dtypes`_ for more on how to register dtypes.

:class:`~pandas.api.extensions.ExtensionArray`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This class provides all the array-like functionality. ExtensionArrays are
limited to 1 dimension. An ExtensionArray is linked to an ExtensionDtype via the
``dtype`` attribute.

pandas makes no restrictions on how an extension array is created via its
``__new__`` or ``__init__``, and puts no restrictions on how you store your
data. We do require that your array be convertible to a NumPy array, even if
this is relatively expensive (as it is for ``Categorical``).

They may be backed by none, one, or many NumPy arrays. For example,
:class:`pandas.Categorical` is an extension array backed by two arrays,
one for codes and one for categories. An array of IPv6 addresses may
be backed by a NumPy structured array with two fields, one for the
lower 64 bits and one for the upper 64 bits. Or they may be backed
by some other storage type, like Python lists.

See the `extension array source`_ for the interface definition. The docstrings
and comments contain guidance for properly implementing the interface.

.. _extending.extension.operator:

:class:`~pandas.api.extensions.ExtensionArray` operator support
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

By default, there are no operators defined for the class :class:`~pandas.api.extensions.ExtensionArray`.
There are two approaches for providing operator support for your ExtensionArray:

1. Define each of the operators on your ``ExtensionArray`` subclass.
2. Use an operator implementation from pandas that depends on operators that are already defined
   on the underlying elements (scalars) of the ExtensionArray.

.. note::

   Regardless of the approach, you may want to set ``__array_priority__``
   if you want your implementation to be called when involved in binary operations
   with NumPy arrays.

For the first approach, you define selected operators, e.g., ``__add__``, ``__le__``, etc. that
you want your ``ExtensionArray`` subclass to support.

The second approach assumes that the underlying elements (i.e., scalar type) of the ``ExtensionArray``
have the individual operators already defined.  In other words, if your ``ExtensionArray``
named ``MyExtensionArray`` is implemented so that each element is an instance
of the class ``MyExtensionElement``, then if the operators are defined
for ``MyExtensionElement``, the second approach will automatically
define the operators for ``MyExtensionArray``.

A mixin class, :class:`~pandas.api.extensions.ExtensionScalarOpsMixin` supports this second
approach.  If developing an ``ExtensionArray`` subclass, for example ``MyExtensionArray``,
can simply include ``ExtensionScalarOpsMixin`` as a parent class of ``MyExtensionArray``,
and then call the methods :meth:`~MyExtensionArray._add_arithmetic_ops` and/or
:meth:`~MyExtensionArray._add_comparison_ops` to hook the operators into
your ``MyExtensionArray`` class, as follows:

.. code-block:: python

    from pandas.api.extensions import ExtensionArray, ExtensionScalarOpsMixin


    class MyExtensionArray(ExtensionArray, ExtensionScalarOpsMixin):
        pass


    MyExtensionArray._add_arithmetic_ops()
    MyExtensionArray._add_comparison_ops()


.. note::

   Since ``pandas`` automatically calls the underlying operator on each
   element one-by-one, this might not be as performant as implementing your own
   version of the associated operators directly on the ``ExtensionArray``.

For arithmetic operations, this implementation will try to reconstruct a new
``ExtensionArray`` with the result of the element-wise operation. Whether
or not that succeeds depends on whether the operation returns a result
that's valid for the ``ExtensionArray``. If an ``ExtensionArray`` cannot
be reconstructed, an ndarray containing the scalars returned instead.

For ease of implementation and consistency with operations between pandas
and NumPy ndarrays, we recommend *not* handling Series and Indexes in your binary ops.
Instead, you should detect these cases and return ``NotImplemented``.
When pandas encounters an operation like ``op(Series, ExtensionArray)``, pandas
will

1. unbox the array from the ``Series`` (``Series.array``)
2. call ``result = op(values, ExtensionArray)``
3. re-box the result in a ``Series``

.. _extending.extension.ufunc:

NumPy universal functions
^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`Series` implements ``__array_ufunc__``. As part of the implementation,
pandas unboxes the ``ExtensionArray`` from the :class:`Series`, applies the ufunc,
and re-boxes it if necessary.

If applicable, we highly recommend that you implement ``__array_ufunc__`` in your
extension array to avoid coercion to an ndarray. See
`the NumPy documentation <https://numpy.org/doc/stable/reference/generated/numpy.lib.mixins.NDArrayOperatorsMixin.html>`__
for an example.

As part of your implementation, we require that you defer to pandas when a pandas
container (:class:`Series`, :class:`DataFrame`, :class:`Index`) is detected in ``inputs``.
If any of those is present, you should return ``NotImplemented``. pandas will take care of
unboxing the array from the container and re-calling the ufunc with the unwrapped input.

.. _extending.extension.testing:

Testing extension arrays
^^^^^^^^^^^^^^^^^^^^^^^^

We provide a test suite for ensuring that your extension arrays satisfy the expected
behavior. To use the test suite, you must provide several pytest fixtures and inherit
from the base test class. The required fixtures are found in
https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/conftest.py.

To use a test, subclass it:

.. code-block:: python

   from pandas.tests.extension import base


   class TestConstructors(base.BaseConstructorsTests):
       pass


See https://github.com/pandas-dev/pandas/blob/main/pandas/tests/extension/base/__init__.py
for a list of all the tests available.

.. _extending.extension.arrow:

Compatibility with Apache Arrow
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An ``ExtensionArray`` can support conversion to / from ``pyarrow`` arrays
(and thus support for example serialization to the Parquet file format)
by implementing two methods: ``ExtensionArray.__arrow_array__`` and
``ExtensionDtype.__from_arrow__``.

The ``ExtensionArray.__arrow_array__`` ensures that ``pyarrow`` knowns how
to convert the specific extension array into a ``pyarrow.Array`` (also when
included as a column in a pandas DataFrame):

.. code-block:: python

    class MyExtensionArray(ExtensionArray):
        ...

        def __arrow_array__(self, type=None):
            # convert the underlying array values to a pyarrow Array
            import pyarrow

            return pyarrow.array(..., type=type)

The ``ExtensionDtype.__from_arrow__`` method then controls the conversion
back from pyarrow to a pandas ExtensionArray. This method receives a pyarrow
``Array`` or ``ChunkedArray`` as only argument and is expected to return the
appropriate pandas ``ExtensionArray`` for this dtype and the passed values:

.. code-block:: none

    class ExtensionDtype:
        ...

        def __from_arrow__(self, array: pyarrow.Array/ChunkedArray) -> ExtensionArray:
            ...

See more in the `Arrow documentation <https://arrow.apache.org/docs/python/extending_types.html>`__.

Those methods have been implemented for the nullable integer and string extension
dtypes included in pandas, and ensure roundtrip to pyarrow and the Parquet file format.

.. _extension dtype dtypes: https://github.com/pandas-dev/pandas/blob/main/pandas/core/dtypes/dtypes.py
.. _extension dtype source: https://github.com/pandas-dev/pandas/blob/main/pandas/core/dtypes/base.py
.. _extension array source: https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/base.py

.. _extending.subclassing-pandas:

Subclassing pandas data structures
----------------------------------

.. warning:: There are some easier alternatives before considering subclassing ``pandas`` data structures.

  1. Extensible method chains with :ref:`pipe <basics.pipe>`

  2. Use *composition*. See `here <https://en.wikipedia.org/wiki/Composition_over_inheritance>`_.

  3. Extending by :ref:`registering an accessor <extending.register-accessors>`

  4. Extending by :ref:`extension type <extending.extension-types>`

This section describes how to subclass ``pandas`` data structures to meet more specific needs. There are two points that need attention:

1. Override constructor properties.
2. Define original properties

.. note::

   You can find a nice example in `geopandas <https://github.com/geopandas/geopandas>`_ project.

Override constructor properties
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Each data structure has several *constructor properties* for returning a new
data structure as the result of an operation. By overriding these properties,
you can retain subclasses through ``pandas`` data manipulations.

There are 3 possible constructor properties to be defined on a subclass:

* ``DataFrame/Series._constructor``: Used when a manipulation result has the same dimension as the original.
* ``DataFrame._constructor_sliced``: Used when a ``DataFrame`` (sub-)class manipulation result should be a ``Series`` (sub-)class.
* ``Series._constructor_expanddim``: Used when a ``Series`` (sub-)class manipulation result should be a ``DataFrame`` (sub-)class, e.g. ``Series.to_frame()``.

Below example shows how to define ``SubclassedSeries`` and ``SubclassedDataFrame`` overriding constructor properties.

.. code-block:: python

   class SubclassedSeries(pd.Series):
       @property
       def _constructor(self):
           return SubclassedSeries

       @property
       def _constructor_expanddim(self):
           return SubclassedDataFrame


   class SubclassedDataFrame(pd.DataFrame):
       @property
       def _constructor(self):
           return SubclassedDataFrame

       @property
       def _constructor_sliced(self):
           return SubclassedSeries

.. code-block:: python

   >>> s = SubclassedSeries([1, 2, 3])
   >>> type(s)
   <class '__main__.SubclassedSeries'>

   >>> to_framed = s.to_frame()
   >>> type(to_framed)
   <class '__main__.SubclassedDataFrame'>

   >>> df = SubclassedDataFrame({"A": [1, 2, 3], "B": [4, 5, 6], "C": [7, 8, 9]})
   >>> df
      A  B  C
   0  1  4  7
   1  2  5  8
   2  3  6  9

   >>> type(df)
   <class '__main__.SubclassedDataFrame'>

   >>> sliced1 = df[["A", "B"]]
   >>> sliced1
      A  B
   0  1  4
   1  2  5
   2  3  6

   >>> type(sliced1)
   <class '__main__.SubclassedDataFrame'>

   >>> sliced2 = df["A"]
   >>> sliced2
   0    1
   1    2
   2    3
   Name: A, dtype: int64

   >>> type(sliced2)
   <class '__main__.SubclassedSeries'>

Define original properties
^^^^^^^^^^^^^^^^^^^^^^^^^^

To let original data structures have additional properties, you should let ``pandas`` know what properties are added. ``pandas`` maps unknown properties to data names overriding ``__getattribute__``. Defining original properties can be done in one of 2 ways:

1. Define ``_internal_names`` and ``_internal_names_set`` for temporary properties which WILL NOT be passed to manipulation results.
2. Define ``_metadata`` for normal properties which will be passed to manipulation results.

Below is an example to define two original properties, "internal_cache" as a temporary property and "added_property" as a normal property

.. code-block:: python

   class SubclassedDataFrame2(pd.DataFrame):

       # temporary properties
       _internal_names = pd.DataFrame._internal_names + ["internal_cache"]
       _internal_names_set = set(_internal_names)

       # normal properties
       _metadata = ["added_property"]

       @property
       def _constructor(self):
           return SubclassedDataFrame2

.. code-block:: python

   >>> df = SubclassedDataFrame2({"A": [1, 2, 3], "B": [4, 5, 6], "C": [7, 8, 9]})
   >>> df
      A  B  C
   0  1  4  7
   1  2  5  8
   2  3  6  9

   >>> df.internal_cache = "cached"
   >>> df.added_property = "property"

   >>> df.internal_cache
   cached
   >>> df.added_property
   property

   # properties defined in _internal_names is reset after manipulation
   >>> df[["A", "B"]].internal_cache
   AttributeError: 'SubclassedDataFrame2' object has no attribute 'internal_cache'

   # properties defined in _metadata are retained
   >>> df[["A", "B"]].added_property
   property

.. _extending.plotting-backends:

Plotting backends
-----------------

Starting in 0.25 pandas can be extended with third-party plotting backends. The
main idea is letting users select a plotting backend different than the provided
one based on Matplotlib. For example:

.. code-block:: python

    >>> pd.set_option("plotting.backend", "backend.module")
    >>> pd.Series([1, 2, 3]).plot()

This would be more or less equivalent to:

.. code-block:: python

    >>> import backend.module
    >>> backend.module.plot(pd.Series([1, 2, 3]))

The backend module can then use other visualization tools (Bokeh, Altair,...)
to generate the plots.

Libraries implementing the plotting backend should use `entry points <https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`__
to make their backend discoverable to pandas. The key is ``"pandas_plotting_backends"``. For example, pandas
registers the default "matplotlib" backend as follows.

.. code-block:: python

   # in setup.py
   setup(  # noqa: F821
       ...,
       entry_points={
           "pandas_plotting_backends": [
               "matplotlib = pandas:plotting._matplotlib",
           ],
       },
   )


More information on how to implement a third-party plotting backend can be found at
https://github.com/pandas-dev/pandas/blob/main/pandas/plotting/__init__.py#L1.
.. _maintaining:

******************
pandas maintenance
******************

This guide is for pandas' maintainers. It may also be interesting to contributors
looking to understand the pandas development process and what steps are necessary
to become a maintainer.

The main contributing guide is available at :ref:`contributing`.

Roles
-----

pandas uses two levels of permissions: **triage** and **core** team members.

Triage members can label and close issues and pull requests.

Core team members can label and close issues and pull request, and can merge
pull requests.

GitHub publishes the full `list of permissions`_.

Tasks
-----

pandas is largely a volunteer project, so these tasks shouldn't be read as
"expectations" of triage and maintainers. Rather, they're general descriptions
of what it means to be a maintainer.

* Triage newly filed issues (see :ref:`maintaining.triage`)
* Review newly opened pull requests
* Respond to updates on existing issues and pull requests
* Drive discussion and decisions on stalled issues and pull requests
* Provide experience / wisdom on API design questions to ensure consistency and maintainability
* Project organization (run / attend developer meetings, represent pandas)

https://matthewrocklin.com/blog/2019/05/18/maintainer may be interesting background
reading.

.. _maintaining.triage:

Issue triage
------------


Here's a typical workflow for triaging a newly opened issue.

1. **Thank the reporter for opening an issue**

   The issue tracker is many people's first interaction with the pandas project itself,
   beyond just using the library. As such, we want it to be a welcoming, pleasant
   experience.

2. **Is the necessary information provided?**

   Ideally reporters would fill out the issue template, but many don't.
   If crucial information (like the version of pandas they used), is missing
   feel free to ask for that and label the issue with "Needs info". The
   report should follow the guidelines in :ref:`contributing.bug_reports`.
   You may want to link to that if they didn't follow the template.

   Make sure that the title accurately reflects the issue. Edit it yourself
   if it's not clear.

3. **Is this a duplicate issue?**

   We have many open issues. If a new issue is clearly a duplicate, label the
   new issue as "Duplicate" assign the milestone "No Action", and close the issue
   with a link to the original issue. Make sure to still thank the reporter, and
   encourage them to chime in on the original issue, and perhaps try to fix it.

   If the new issue provides relevant information, such as a better or slightly
   different example, add it to the original issue as a comment or an edit to
   the original post.

4. **Is the issue minimal and reproducible**?

   For bug reports, we ask that the reporter provide a minimal reproducible
   example. See https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports
   for a good explanation. If the example is not reproducible, or if it's
   *clearly* not minimal, feel free to ask the reporter if they can provide
   and example or simplify the provided one. Do acknowledge that writing
   minimal reproducible examples is hard work. If the reporter is struggling,
   you can try to write one yourself and we'll edit the original post to include it.

   If a reproducible example can't be provided, add the "Needs info" label.

   If a reproducible example is provided, but you see a simplification,
   edit the original post with your simpler reproducible example.

5. **Is this a clearly defined feature request?**

   Generally, pandas prefers to discuss and design new features in issues, before
   a pull request is made. Encourage the submitter to include a proposed API
   for the new feature. Having them write a full docstring is a good way to
   pin down specifics.

   We'll need a discussion from several pandas maintainers before deciding whether
   the proposal is in scope for pandas.

6. **Is this a usage question?**

   We prefer that usage questions are asked on StackOverflow with the pandas
   tag. https://stackoverflow.com/questions/tagged/pandas

   If it's easy to answer, feel free to link to the relevant documentation section,
   let them know that in the future this kind of question should be on
   StackOverflow, and close the issue.

7. **What labels and milestones should I add?**

   Apply the relevant labels. This is a bit of an art, and comes with experience.
   Look at similar issues to get a feel for how things are labeled.

   If the issue is clearly defined and the fix seems relatively straightforward,
   label the issue as "Good first issue".

   Typically, new issues will be assigned the "Contributions welcome" milestone,
   unless it's know that this issue should be addressed in a specific release (say
   because it's a large regression).

.. _maintaining.closing:

Closing issues
--------------

Be delicate here: many people interpret closing an issue as us saying that the
conversation is over. It's typically best to give the reporter some time to
respond or self-close their issue if it's determined that the behavior is not a bug,
or the feature is out of scope. Sometimes reporters just go away though, and
we'll close the issue after the conversation has died.

.. _maintaining.reviewing:

Reviewing pull requests
-----------------------

Anybody can review a pull request: regular contributors, triagers, or core-team
members. But only core-team members can merge pull requets when they're ready.

Here are some things to check when reviewing a pull request.

* Tests should be in a sensible location: in the same file as closely related tests.
* New public APIs should be included somewhere in ``doc/source/reference/``.
* New / changed API should use the ``versionadded`` or ``versionchanged`` directives in the docstring.
* User-facing changes should have a whatsnew in the appropriate file.
* Regression tests should reference the original GitHub issue number like ``# GH-1234``.
* The pull request should be labeled and assigned the appropriate milestone (the next patch release
  for regression fixes and small bug fixes, the next minor milestone otherwise)
* Changes should comply with our :ref:`policies.version`.

Backporting
-----------

In the case you want to apply changes to a stable branch from a newer branch then you
can comment::

    @meeseeksdev backport version-branch

This will trigger a workflow which will backport a given change to a branch
(e.g. @meeseeksdev backport 1.2.x)

Cleaning up old issues
----------------------

Every open issue in pandas has a cost. Open issues make finding duplicates harder,
and can make it harder to know what needs to be done in pandas. That said, closing
issues isn't a goal on its own. Our goal is to make pandas the best it can be,
and that's best done by ensuring that the quality of our open issues is high.

Occasionally, bugs are fixed but the issue isn't linked to in the Pull Request.
In these cases, comment that "This has been fixed, but could use a test." and
label the issue as "Good First Issue" and "Needs Test".

If an older issue doesn't follow our issue template, edit the original post to
include a minimal example, the actual output, and the expected output. Uniformity
in issue reports is valuable.

If an older issue lacks a reproducible example, label it as "Needs Info" and
ask them to provide one (or write one yourself if possible). If one isn't
provide reasonably soon, close it according to the policies in :ref:`maintaining.closing`.

Cleaning up old pull requests
-----------------------------

Occasionally, contributors are unable to finish off a pull request.
If some time has passed (two weeks, say) since the last review requesting changes,
gently ask if they're still interested in working on this. If another two weeks or
so passes with no response, thank them for their work and close the pull request.
Comment on the original issue that "There's a stalled PR at #1234 that may be
helpful.", and perhaps label the issue as "Good first issue" if the PR was relatively
close to being accepted.

Additionally, core-team members can push to contributors branches. This can be
helpful for pushing an important PR across the line, or for fixing a small
merge conflict.

Becoming a pandas maintainer
----------------------------

The full process is outlined in our `governance documents`_. In summary,
we're happy to give triage permissions to anyone who shows interest by
being helpful on the issue tracker.

The current list of core-team members is at
https://github.com/pandas-dev/pandas-governance/blob/master/people.md


.. _maintaining.merging:

Merging pull requests
---------------------

Only core team members can merge pull requests. We have a few guidelines.

1. You should typically not self-merge your own pull requests. Exceptions include
   things like small changes to fix CI (e.g. pinning a package version).
2. You should not merge pull requests that have an active discussion, or pull
   requests that has any ``-1`` votes from a core maintainer. pandas operates
   by consensus.
3. For larger changes, it's good to have a +1 from at least two core team members.

In addition to the items listed in :ref:`maintaining.closing`, you should verify
that the pull request is assigned the correct milestone.

Pull requests merged with a patch-release milestone will typically be backported
by our bot. Verify that the bot noticed the merge (it will leave a comment within
a minute typically). If a manual backport is needed please do that, and remove
the "Needs backport" label once you've done it manually. If you forget to assign
a milestone before tagging, you can request the bot to backport it with:

.. code-block:: console

   @Meeseeksdev backport <branch>


.. _governance documents: https://github.com/pandas-dev/pandas-governance
.. _list of permissions: https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/repository-roles-for-an-organization
.. _docstring:

{{ header }}

======================
pandas docstring guide
======================

About docstrings and standards
------------------------------

A Python docstring is a string used to document a Python module, class,
function or method, so programmers can understand what it does without having
to read the details of the implementation.

Also, it is a common practice to generate online (html) documentation
automatically from docstrings. `Sphinx <https://www.sphinx-doc.org>`_ serves
this purpose.

The next example gives an idea of what a docstring looks like:

.. code-block:: python

    def add(num1, num2):
        """
        Add up two integer numbers.

        This function simply wraps the ``+`` operator, and does not
        do anything interesting, except for illustrating what
        the docstring of a very simple function looks like.

        Parameters
        ----------
        num1 : int
            First number to add.
        num2 : int
            Second number to add.

        Returns
        -------
        int
            The sum of ``num1`` and ``num2``.

        See Also
        --------
        subtract : Subtract one integer from another.

        Examples
        --------
        >>> add(2, 2)
        4
        >>> add(25, 0)
        25
        >>> add(10, -10)
        0
        """
        return num1 + num2

Some standards regarding docstrings exist, which make them easier to read, and allow them
be easily exported to other formats such as html or pdf.

The first conventions every Python docstring should follow are defined in
`PEP-257 <https://www.python.org/dev/peps/pep-0257/>`_.

As PEP-257 is quite broad, other more specific standards also exist. In the
case of pandas, the NumPy docstring convention is followed. These conventions are
explained in this document:

* `numpydoc docstring guide <https://numpydoc.readthedocs.io/en/latest/format.html>`_
  (which is based in the original `Guide to NumPy/SciPy documentation
  <https://github.com/numpy/numpy/blob/main/doc/HOWTO_DOCUMENT.rst.txt>`_)

numpydoc is a Sphinx extension to support the NumPy docstring convention.

The standard uses reStructuredText (reST). reStructuredText is a markup
language that allows encoding styles in plain text files. Documentation
about reStructuredText can be found in:

* `Sphinx reStructuredText primer <https://www.sphinx-doc.org/en/stable/rest.html>`_
* `Quick reStructuredText reference <https://docutils.sourceforge.io/docs/user/rst/quickref.html>`_
* `Full reStructuredText specification <https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html>`_

pandas has some helpers for sharing docstrings between related classes, see
:ref:`docstring.sharing`.

The rest of this document will summarize all the above guidelines, and will
provide additional conventions specific to the pandas project.

.. _docstring.tutorial:

Writing a docstring
-------------------

.. _docstring.general:

General rules
~~~~~~~~~~~~~

Docstrings must be defined with three double-quotes. No blank lines should be
left before or after the docstring. The text starts in the next line after the
opening quotes. The closing quotes have their own line
(meaning that they are not at the end of the last sentence).

On rare occasions reST styles like bold text or italics will be used in
docstrings, but is it common to have inline code, which is presented between
backticks. The following are considered inline code:

* The name of a parameter
* Python code, a module, function, built-in, type, literal... (e.g. ``os``,
  ``list``, ``numpy.abs``, ``datetime.date``, ``True``)
* A pandas class (in the form ``:class:`pandas.Series```)
* A pandas method (in the form ``:meth:`pandas.Series.sum```)
* A pandas function (in the form ``:func:`pandas.to_datetime```)

.. note::
    To display only the last component of the linked class, method or
    function, prefix it with ``~``. For example, ``:class:`~pandas.Series```
    will link to ``pandas.Series`` but only display the last part, ``Series``
    as the link text. See `Sphinx cross-referencing syntax
    <https://www.sphinx-doc.org/en/stable/domains.html#cross-referencing-syntax>`_
    for details.

**Good:**

.. code-block:: python

    def add_values(arr):
        """
        Add the values in ``arr``.

        This is equivalent to Python ``sum`` of :meth:`pandas.Series.sum`.

        Some sections are omitted here for simplicity.
        """
        return sum(arr)

**Bad:**

.. code-block:: python

    def func():

        """Some function.

        With several mistakes in the docstring.

        It has a blank like after the signature ``def func():``.

        The text 'Some function' should go in the line after the
        opening quotes of the docstring, not in the same line.

        There is a blank line between the docstring and the first line
        of code ``foo = 1``.

        The closing quotes should be in the next line, not in this one."""

        foo = 1
        bar = 2
        return foo + bar

.. _docstring.short_summary:

Section 1: short summary
~~~~~~~~~~~~~~~~~~~~~~~~

The short summary is a single sentence that expresses what the function does in
a concise way.

The short summary must start with a capital letter, end with a dot, and fit in
a single line. It needs to express what the object does without providing
details. For functions and methods, the short summary must start with an
infinitive verb.

**Good:**

.. code-block:: python

    def astype(dtype):
        """
        Cast Series type.

        This section will provide further details.
        """
        pass

**Bad:**

.. code-block:: python

    def astype(dtype):
        """
        Casts Series type.

        Verb in third-person of the present simple, should be infinitive.
        """
        pass

.. code-block:: python

    def astype(dtype):
        """
        Method to cast Series type.

        Does not start with verb.
        """
        pass

.. code-block:: python

    def astype(dtype):
        """
        Cast Series type

        Missing dot at the end.
        """
        pass

.. code-block:: python

    def astype(dtype):
        """
        Cast Series type from its current type to the new type defined in
        the parameter dtype.

        Summary is too verbose and doesn't fit in a single line.
        """
        pass

.. _docstring.extended_summary:

Section 2: extended summary
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The extended summary provides details on what the function does. It should not
go into the details of the parameters, or discuss implementation notes, which
go in other sections.

A blank line is left between the short summary and the extended summary.
Every paragraph in the extended summary ends with a dot.

The extended summary should provide details on why the function is useful and
their use cases, if it is not too generic.

.. code-block:: python

    def unstack():
        """
        Pivot a row index to columns.

        When using a MultiIndex, a level can be pivoted so each value in
        the index becomes a column. This is especially useful when a subindex
        is repeated for the main index, and data is easier to visualize as a
        pivot table.

        The index level will be automatically removed from the index when added
        as columns.
        """
        pass

.. _docstring.parameters:

Section 3: parameters
~~~~~~~~~~~~~~~~~~~~~

The details of the parameters will be added in this section. This section has
the title "Parameters", followed by a line with a hyphen under each letter of
the word "Parameters". A blank line is left before the section title, but not
after, and not between the line with the word "Parameters" and the one with
the hyphens.

After the title, each parameter in the signature must be documented, including
``*args`` and ``**kwargs``, but not ``self``.

The parameters are defined by their name, followed by a space, a colon, another
space, and the type (or types). Note that the space between the name and the
colon is important. Types are not defined for ``*args`` and ``**kwargs``, but must
be defined for all other parameters. After the parameter definition, it is
required to have a line with the parameter description, which is indented, and
can have multiple lines. The description must start with a capital letter, and
finish with a dot.

For keyword arguments with a default value, the default will be listed after a
comma at the end of the type. The exact form of the type in this case will be
"int, default 0". In some cases it may be useful to explain what the default
argument means, which can be added after a comma "int, default -1, meaning all
cpus".

In cases where the default value is ``None``, meaning that the value will not be
used. Instead of ``"str, default None"``, it is preferred to write ``"str, optional"``.
When ``None`` is a value being used, we will keep the form "str, default None".
For example, in ``df.to_csv(compression=None)``, ``None`` is not a value being used,
but means that compression is optional, and no compression is being used if not
provided. In this case we will use ``"str, optional"``. Only in cases like
``func(value=None)`` and ``None`` is being used in the same way as ``0`` or ``foo``
would be used, then we will specify "str, int or None, default None".

**Good:**

.. code-block:: python

    class Series:
        def plot(self, kind, color='blue', **kwargs):
            """
            Generate a plot.

            Render the data in the Series as a matplotlib plot of the
            specified kind.

            Parameters
            ----------
            kind : str
                Kind of matplotlib plot.
            color : str, default 'blue'
                Color name or rgb code.
            **kwargs
                These parameters will be passed to the matplotlib plotting
                function.
            """
            pass

**Bad:**

.. code-block:: python

    class Series:
        def plot(self, kind, **kwargs):
            """
            Generate a plot.

            Render the data in the Series as a matplotlib plot of the
            specified kind.

            Note the blank line between the parameters title and the first
            parameter. Also, note that after the name of the parameter ``kind``
            and before the colon, a space is missing.

            Also, note that the parameter descriptions do not start with a
            capital letter, and do not finish with a dot.

            Finally, the ``**kwargs`` parameter is missing.

            Parameters
            ----------

            kind: str
                kind of matplotlib plot
            """
            pass

.. _docstring.parameter_types:

Parameter types
^^^^^^^^^^^^^^^

When specifying the parameter types, Python built-in data types can be used
directly (the Python type is preferred to the more verbose string, integer,
boolean, etc):

* int
* float
* str
* bool

For complex types, define the subtypes. For ``dict`` and ``tuple``, as more than
one type is present, we use the brackets to help read the type (curly brackets
for ``dict`` and normal brackets for ``tuple``):

* list of int
* dict of {str : int}
* tuple of (str, int, int)
* tuple of (str,)
* set of str

In case where there are just a set of values allowed, list them in curly
brackets and separated by commas (followed by a space). If the values are
ordinal and they have an order, list them in this order. Otherwise, list
the default value first, if there is one:

* {0, 10, 25}
* {'simple', 'advanced'}
* {'low', 'medium', 'high'}
* {'cat', 'dog', 'bird'}

If the type is defined in a Python module, the module must be specified:

* datetime.date
* datetime.datetime
* decimal.Decimal

If the type is in a package, the module must be also specified:

* numpy.ndarray
* scipy.sparse.coo_matrix

If the type is a pandas type, also specify pandas except for Series and
DataFrame:

* Series
* DataFrame
* pandas.Index
* pandas.Categorical
* pandas.arrays.SparseArray

If the exact type is not relevant, but must be compatible with a NumPy
array, array-like can be specified. If Any type that can be iterated is
accepted, iterable can be used:

* array-like
* iterable

If more than one type is accepted, separate them by commas, except the
last two types, that need to be separated by the word 'or':

* int or float
* float, decimal.Decimal or None
* str or list of str

If ``None`` is one of the accepted values, it always needs to be the last in
the list.

For axis, the convention is to use something like:

* axis : {0 or 'index', 1 or 'columns', None}, default None

.. _docstring.returns:

Section 4: returns or yields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the method returns a value, it will be documented in this section. Also
if the method yields its output.

The title of the section will be defined in the same way as the "Parameters".
With the names "Returns" or "Yields" followed by a line with as many hyphens
as the letters in the preceding word.

The documentation of the return is also similar to the parameters. But in this
case, no name will be provided, unless the method returns or yields more than
one value (a tuple of values).

The types for "Returns" and "Yields" are the same as the ones for the
"Parameters". Also, the description must finish with a dot.

For example, with a single value:

.. code-block:: python

    def sample():
        """
        Generate and return a random number.

        The value is sampled from a continuous uniform distribution between
        0 and 1.

        Returns
        -------
        float
            Random number generated.
        """
        return np.random.random()

With more than one value:

.. code-block:: python

    import string

    def random_letters():
        """
        Generate and return a sequence of random letters.

        The length of the returned string is also random, and is also
        returned.

        Returns
        -------
        length : int
            Length of the returned string.
        letters : str
            String of random letters.
        """
        length = np.random.randint(1, 10)
        letters = ''.join(np.random.choice(string.ascii_lowercase)
                          for i in range(length))
        return length, letters

If the method yields its value:

.. code-block:: python

    def sample_values():
        """
        Generate an infinite sequence of random numbers.

        The values are sampled from a continuous uniform distribution between
        0 and 1.

        Yields
        ------
        float
            Random number generated.
        """
        while True:
            yield np.random.random()

.. _docstring.see_also:

Section 5: see also
~~~~~~~~~~~~~~~~~~~

This section is used to let users know about pandas functionality
related to the one being documented. In rare cases, if no related methods
or functions can be found at all, this section can be skipped.

An obvious example would be the ``head()`` and ``tail()`` methods. As ``tail()`` does
the equivalent as ``head()`` but at the end of the ``Series`` or ``DataFrame``
instead of at the beginning, it is good to let the users know about it.

To give an intuition on what can be considered related, here there are some
examples:

* ``loc`` and ``iloc``, as they do the same, but in one case providing indices
  and in the other positions
* ``max`` and ``min``, as they do the opposite
* ``iterrows``, ``itertuples`` and ``items``, as it is easy that a user
  looking for the method to iterate over columns ends up in the method to
  iterate over rows, and vice-versa
* ``fillna`` and ``dropna``, as both methods are used to handle missing values
* ``read_csv`` and ``to_csv``, as they are complementary
* ``merge`` and ``join``, as one is a generalization of the other
* ``astype`` and ``pandas.to_datetime``, as users may be reading the
  documentation of ``astype`` to know how to cast as a date, and the way to do
  it is with ``pandas.to_datetime``
* ``where`` is related to ``numpy.where``, as its functionality is based on it

When deciding what is related, you should mainly use your common sense and
think about what can be useful for the users reading the documentation,
especially the less experienced ones.

When relating to other libraries (mainly ``numpy``), use the name of the module
first (not an alias like ``np``). If the function is in a module which is not
the main one, like ``scipy.sparse``, list the full module (e.g.
``scipy.sparse.coo_matrix``).

This section has a header, "See Also" (note the capital
S and A), followed by the line with hyphens and preceded by a blank line.

After the header, we will add a line for each related method or function,
followed by a space, a colon, another space, and a short description that
illustrates what this method or function does, why is it relevant in this
context, and what the key differences are between the documented function and
the one being referenced. The description must also end with a dot.

Note that in "Returns" and "Yields", the description is located on the line
after the type. In this section, however, it is located on the same
line, with a colon in between. If the description does not fit on the same
line, it can continue onto other lines which must be further indented.

For example:

.. code-block:: python

    class Series:
        def head(self):
            """
            Return the first 5 elements of the Series.

            This function is mainly useful to preview the values of the
            Series without displaying the whole of it.

            Returns
            -------
            Series
                Subset of the original series with the 5 first values.

            See Also
            --------
            Series.tail : Return the last 5 elements of the Series.
            Series.iloc : Return a slice of the elements in the Series,
                which can also be used to return the first or last n.
            """
            return self.iloc[:5]

.. _docstring.notes:

Section 6: notes
~~~~~~~~~~~~~~~~

This is an optional section used for notes about the implementation of the
algorithm, or to document technical aspects of the function behavior.

Feel free to skip it, unless you are familiar with the implementation of the
algorithm, or you discover some counter-intuitive behavior while writing the
examples for the function.

This section follows the same format as the extended summary section.

.. _docstring.examples:

Section 7: examples
~~~~~~~~~~~~~~~~~~~

This is one of the most important sections of a docstring, despite being
placed in the last position, as often people understand concepts better
by example than through accurate explanations.

Examples in docstrings, besides illustrating the usage of the function or
method, must be valid Python code, that returns the given output in a
deterministic way, and that can be copied and run by users.

Examples are presented as a session in the Python terminal. ``>>>`` is used to
present code. ``...`` is used for code continuing from the previous line.
Output is presented immediately after the last line of code generating the
output (no blank lines in between). Comments describing the examples can
be added with blank lines before and after them.

The way to present examples is as follows:

1. Import required libraries (except ``numpy`` and ``pandas``)

2. Create the data required for the example

3. Show a very basic example that gives an idea of the most common use case

4. Add examples with explanations that illustrate how the parameters can be
   used for extended functionality

A simple example could be:

.. code-block:: python

    class Series:

        def head(self, n=5):
            """
            Return the first elements of the Series.

            This function is mainly useful to preview the values of the
            Series without displaying all of it.

            Parameters
            ----------
            n : int
                Number of values to return.

            Return
            ------
            pandas.Series
                Subset of the original series with the n first values.

            See Also
            --------
            tail : Return the last n elements of the Series.

            Examples
            --------
            >>> s = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',
            ...                'Lion', 'Monkey', 'Rabbit', 'Zebra'])
            >>> s.head()
            0   Ant
            1   Bear
            2   Cow
            3   Dog
            4   Falcon
            dtype: object

            With the ``n`` parameter, we can change the number of returned rows:

            >>> s.head(n=3)
            0   Ant
            1   Bear
            2   Cow
            dtype: object
            """
            return self.iloc[:n]

The examples should be as concise as possible. In cases where the complexity of
the function requires long examples, is recommended to use blocks with headers
in bold. Use double star ``**`` to make a text bold, like in ``**this example**``.

.. _docstring.example_conventions:

Conventions for the examples
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Code in examples is assumed to always start with these two lines which are not
shown:

.. code-block:: python

    import numpy as np
    import pandas as pd

Any other module used in the examples must be explicitly imported, one per line (as
recommended in :pep:`8#imports`)
and avoiding aliases. Avoid excessive imports, but if needed, imports from
the standard library go first, followed by third-party libraries (like
matplotlib).

When illustrating examples with a single ``Series`` use the name ``s``, and if
illustrating with a single ``DataFrame`` use the name ``df``. For indices,
``idx`` is the preferred name. If a set of homogeneous ``Series`` or
``DataFrame`` is used, name them ``s1``, ``s2``, ``s3``...  or ``df1``,
``df2``, ``df3``... If the data is not homogeneous, and more than one structure
is needed, name them with something meaningful, for example ``df_main`` and
``df_to_join``.

Data used in the example should be as compact as possible. The number of rows
is recommended to be around 4, but make it a number that makes sense for the
specific example. For example in the ``head`` method, it requires to be higher
than 5, to show the example with the default values. If doing the ``mean``, we
could use something like ``[1, 2, 3]``, so it is easy to see that the value
returned is the mean.

For more complex examples (grouping for example), avoid using data without
interpretation, like a matrix of random numbers with columns A, B, C, D...
And instead use a meaningful example, which makes it easier to understand the
concept. Unless required by the example, use names of animals, to keep examples
consistent. And numerical properties of them.

When calling the method, keywords arguments ``head(n=3)`` are preferred to
positional arguments ``head(3)``.

**Good:**

.. code-block:: python

    class Series:

        def mean(self):
            """
            Compute the mean of the input.

            Examples
            --------
            >>> s = pd.Series([1, 2, 3])
            >>> s.mean()
            2
            """
            pass


        def fillna(self, value):
            """
            Replace missing values by ``value``.

            Examples
            --------
            >>> s = pd.Series([1, np.nan, 3])
            >>> s.fillna(0)
            [1, 0, 3]
            """
            pass

        def groupby_mean(self):
            """
            Group by index and return mean.

            Examples
            --------
            >>> s = pd.Series([380., 370., 24., 26],
            ...               name='max_speed',
            ...               index=['falcon', 'falcon', 'parrot', 'parrot'])
            >>> s.groupby_mean()
            index
            falcon    375.0
            parrot     25.0
            Name: max_speed, dtype: float64
            """
            pass

        def contains(self, pattern, case_sensitive=True, na=numpy.nan):
            """
            Return whether each value contains ``pattern``.

            In this case, we are illustrating how to use sections, even
            if the example is simple enough and does not require them.

            Examples
            --------
            >>> s = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)
            >>> s.contains(pattern='a')
            0    False
            1    False
            2     True
            3      NaN
            dtype: bool

            **Case sensitivity**

            With ``case_sensitive`` set to ``False`` we can match ``a`` with both
            ``a`` and ``A``:

            >>> s.contains(pattern='a', case_sensitive=False)
            0     True
            1    False
            2     True
            3      NaN
            dtype: bool

            **Missing values**

            We can fill missing values in the output using the ``na`` parameter:

            >>> s.contains(pattern='a', na=False)
            0    False
            1    False
            2     True
            3    False
            dtype: bool
            """
            pass

**Bad:**

.. code-block:: python

    def method(foo=None, bar=None):
        """
        A sample DataFrame method.

        Do not import NumPy and pandas.

        Try to use meaningful data, when it makes the example easier
        to understand.

        Try to avoid positional arguments like in ``df.method(1)``. They
        can be all right if previously defined with a meaningful name,
        like in ``present_value(interest_rate)``, but avoid them otherwise.

        When presenting the behavior with different parameters, do not place
        all the calls one next to the other. Instead, add a short sentence
        explaining what the example shows.

        Examples
        --------
        >>> import numpy as np
        >>> import pandas as pd
        >>> df = pd.DataFrame(np.random.randn(3, 3),
        ...                   columns=('a', 'b', 'c'))
        >>> df.method(1)
        21
        >>> df.method(bar=14)
        123
        """
        pass


.. _docstring.doctest_tips:

Tips for getting your examples pass the doctests
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Getting the examples pass the doctests in the validation script can sometimes
be tricky. Here are some attention points:

* Import all needed libraries (except for pandas and NumPy, those are already
  imported as ``import pandas as pd`` and ``import numpy as np``) and define
  all variables you use in the example.

* Try to avoid using random data. However random data might be OK in some
  cases, like if the function you are documenting deals with probability
  distributions, or if the amount of data needed to make the function result
  meaningful is too much, such that creating it manually is very cumbersome.
  In those cases, always use a fixed random seed to make the generated examples
  predictable. Example::

    >>> np.random.seed(42)
    >>> df = pd.DataFrame({'normal': np.random.normal(100, 5, 20)})

* If you have a code snippet that wraps multiple lines, you need to use '...'
  on the continued lines: ::

    >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['a', 'b', 'c'],
    ...                   columns=['A', 'B'])

* If you want to show a case where an exception is raised, you can do::

    >>> pd.to_datetime(["712-01-01"])
    Traceback (most recent call last):
    OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 712-01-01 00:00:00

  It is essential to include the "Traceback (most recent call last):", but for
  the actual error only the error name is sufficient.

* If there is a small part of the result that can vary (e.g. a hash in an object
  representation), you can use ``...`` to represent this part.

  If you want to show that ``s.plot()`` returns a matplotlib AxesSubplot object,
  this will fail the doctest ::

    >>> s.plot()
    <matplotlib.axes._subplots.AxesSubplot at 0x7efd0c0b0690>

  However, you can do (notice the comment that needs to be added) ::

    >>> s.plot()  # doctest: +ELLIPSIS
    <matplotlib.axes._subplots.AxesSubplot at ...>


.. _docstring.example_plots:

Plots in examples
^^^^^^^^^^^^^^^^^

There are some methods in pandas returning plots. To render the plots generated
by the examples in the documentation, the ``.. plot::`` directive exists.

To use it, place the next code after the "Examples" header as shown below. The
plot will be generated automatically when building the documentation.

.. code-block:: python

    class Series:
        def plot(self):
            """
            Generate a plot with the ``Series`` data.

            Examples
            --------

            .. plot::
                :context: close-figs

                >>> s = pd.Series([1, 2, 3])
                >>> s.plot()
            """
            pass

.. _docstring.sharing:

Sharing docstrings
------------------

pandas has a system for sharing docstrings, with slight variations, between
classes. This helps us keep docstrings consistent, while keeping things clear
for the user reading. It comes at the cost of some complexity when writing.

Each shared docstring will have a base template with variables, like
``{klass}``. The variables filled in later on using the ``doc`` decorator.
Finally, docstrings can also be appended to with the ``doc`` decorator.

In this example, we'll create a parent docstring normally (this is like
``pandas.core.generic.NDFrame``. Then we'll have two children (like
``pandas.core.series.Series`` and ``pandas.core.frame.DataFrame``). We'll
substitute the class names in this docstring.

.. code-block:: python

   class Parent:
       @doc(klass="Parent")
       def my_function(self):
           """Apply my function to {klass}."""
           ...


   class ChildA(Parent):
       @doc(Parent.my_function, klass="ChildA")
       def my_function(self):
           ...


   class ChildB(Parent):
       @doc(Parent.my_function, klass="ChildB")
       def my_function(self):
           ...

The resulting docstrings are

.. code-block:: python

   >>> print(Parent.my_function.__doc__)
   Apply my function to Parent.
   >>> print(ChildA.my_function.__doc__)
   Apply my function to ChildA.
   >>> print(ChildB.my_function.__doc__)
   Apply my function to ChildB.

Notice:

1. We "append" the parent docstring to the children docstrings, which are
   initially empty.

Our files will often contain a module-level ``_shared_doc_kwargs`` with some
common substitution values (things like ``klass``, ``axes``, etc).

You can substitute and append in one shot with something like

.. code-block:: python

   @doc(template, **_shared_doc_kwargs)
   def my_function(self):
       ...

where ``template`` may come from a module-level ``_shared_docs`` dictionary
mapping function names to docstrings. Wherever possible, we prefer using
``doc``, since the docstring-writing processes is slightly closer to normal.

See ``pandas.core.generic.NDFrame.fillna`` for an example template, and
``pandas.core.series.Series.fillna`` and ``pandas.core.generic.frame.fillna``
for the filled versions.
.. _internals:

{{ header }}

*********
Internals
*********

This section will provide a look into some of pandas internals. It's primarily
intended for developers of pandas itself.

Indexing
--------

In pandas there are a few objects implemented which can serve as valid
containers for the axis labels:

* ``Index``: the generic "ordered set" object, an ndarray of object dtype
  assuming nothing about its contents. The labels must be hashable (and
  likely immutable) and unique. Populates a dict of label to location in
  Cython to do ``O(1)`` lookups.
* ``Int64Index``: a version of ``Index`` highly optimized for 64-bit integer
  data, such as time stamps
* ``Float64Index``: a version of ``Index`` highly optimized for 64-bit float data
* ``MultiIndex``: the standard hierarchical index object
* ``DatetimeIndex``: An Index object with ``Timestamp`` boxed elements (impl are the int64 values)
* ``TimedeltaIndex``: An Index object with ``Timedelta`` boxed elements (impl are the in64 values)
* ``PeriodIndex``: An Index object with Period elements

There are functions that make the creation of a regular index easy:

* ``date_range``: fixed frequency date range generated from a time rule or
  DateOffset. An ndarray of Python datetime objects
* ``period_range``: fixed frequency date range generated from a time rule or
  DateOffset. An ndarray of ``Period`` objects, representing timespans

The motivation for having an ``Index`` class in the first place was to enable
different implementations of indexing. This means that it's possible for you,
the user, to implement a custom ``Index`` subclass that may be better suited to
a particular application than the ones provided in pandas.

From an internal implementation point of view, the relevant methods that an
``Index`` must define are one or more of the following (depending on how
incompatible the new object internals are with the ``Index`` functions):

* ``get_loc``: returns an "indexer" (an integer, or in some cases a
  slice object) for a label
* ``slice_locs``: returns the "range" to slice between two labels
* ``get_indexer``: Computes the indexing vector for reindexing / data
  alignment purposes. See the source / docstrings for more on this
* ``get_indexer_non_unique``: Computes the indexing vector for reindexing / data
  alignment purposes when the index is non-unique. See the source / docstrings
  for more on this
* ``reindex``: Does any pre-conversion of the input index then calls
  ``get_indexer``
* ``union``, ``intersection``: computes the union or intersection of two
  Index objects
* ``insert``: Inserts a new label into an Index, yielding a new object
* ``delete``: Delete a label, yielding a new object
* ``drop``: Deletes a set of labels
* ``take``: Analogous to ndarray.take

MultiIndex
~~~~~~~~~~

Internally, the ``MultiIndex`` consists of a few things: the **levels**, the
integer **codes** (until version 0.24 named *labels*), and the level **names**:

.. ipython:: python

   index = pd.MultiIndex.from_product(
       [range(3), ["one", "two"]], names=["first", "second"]
   )
   index
   index.levels
   index.codes
   index.names

You can probably guess that the codes determine which unique element is
identified with that location at each layer of the index. It's important to
note that sortedness is determined **solely** from the integer codes and does
not check (or care) whether the levels themselves are sorted. Fortunately, the
constructors ``from_tuples`` and ``from_arrays`` ensure that this is true, but
if you compute the levels and codes yourself, please be careful.

Values
~~~~~~

pandas extends NumPy's type system with custom types, like ``Categorical`` or
datetimes with a timezone, so we have multiple notions of "values". For 1-D
containers (``Index`` classes and ``Series``) we have the following convention:

* ``cls._values`` refers is the "best possible" array. This could be an
  ``ndarray`` or ``ExtensionArray``.

So, for example, ``Series[category]._values`` is a ``Categorical``.

.. _ref-subclassing-pandas:

Subclassing pandas data structures
----------------------------------

This section has been moved to :ref:`extending.subclassing-pandas`.
{{ header }}

.. _development:

===========
Development
===========

.. If you update this toctree, also update the manual toctree in the
   main index.rst.template

.. toctree::
    :maxdepth: 2

    contributing
    contributing_environment
    contributing_documentation
    contributing_codebase
    code_style
    maintaining
    internals
    test_writing
    debugging_extensions
    extending
    developer
    policies
    roadmap
    meeting
.. _debugging_c_extensions:

{{ header }}

======================
Debugging C extensions
======================

Pandas uses select C extensions for high performance IO operations. In case you need to debug segfaults or general issues with those extensions, the following steps may be helpful.

First, be sure to compile the extensions with the appropriate flags to generate debug symbols and remove optimizations. This can be achieved as follows:

.. code-block:: sh

   python setup.py build_ext --inplace -j4 --with-debugging-symbols

Using a debugger
================

Assuming you are on a Unix-like operating system, you can use either lldb or gdb to debug. The choice between either is largely dependent on your compilation toolchain - typically you would use lldb if using clang and gdb if using gcc. For macOS users, please note that ``gcc`` is on modern systems an alias for ``clang``, so if using Xcode you usually opt for lldb. Regardless of which debugger you choose, please refer to your operating systems instructions on how to install.

After installing a debugger you can create a script that hits the extension module you are looking to debug. For demonstration purposes, let's assume you have a script called ``debug_testing.py`` with the following contents:

.. code-block:: python

   import pandas as pd

   pd.DataFrame([[1, 2]]).to_json()

Place the ``debug_testing.py`` script in the project root and launch a Python process under your debugger. If using lldb:

.. code-block:: sh

   lldb python

If using gdb:

.. code-block:: sh

   gdb python

Before executing our script, let's set a breakpoint in our JSON serializer in its entry function called ``objToJSON``. The lldb syntax would look as follows:

.. code-block:: sh

   breakpoint set --name objToJSON

Similarly for gdb:

.. code-block:: sh

   break objToJSON

.. note::

   You may get a warning that this breakpoint cannot be resolved in lldb. gdb may give a similar warning and prompt you to make the breakpoint on a future library load, which you should say yes to. This should only happen on the very first invocation as the module you wish to debug has not yet been loaded into memory.

Now go ahead and execute your script:

.. code-block:: sh

   run <the_script>.py

Code execution will halt at the breakpoint defined or at the occurrence of any segfault. LLDB's `GDB to LLDB command map <https://lldb.llvm.org/use/map.html>`_ provides a listing of debugger command that you can execute using either debugger.

Another option to execute the entire test suite under lldb would be to run the following:

.. code-block:: sh

   lldb -- python -m pytest

Or for gdb

.. code-block:: sh

   gdb --args python -m pytest

Once the process launches, simply type ``run`` and the test suite will begin, stopping at any segmentation fault that may occur.

Checking memory leaks with valgrind
===================================

You can use `Valgrind <https://valgrind.org/>`_ to check for and log memory leaks in extensions. For instance, to check for a memory leak in a test from the suite you can run:

.. code-block:: sh

   PYTHONMALLOC=malloc valgrind --leak-check=yes --track-origins=yes --log-file=valgrind-log.txt python -m pytest <path_to_a_test>

Note that code execution under valgrind will take much longer than usual. While you can run valgrind against extensions compiled with any optimization level, it is suggested to have optimizations turned off from compiled extensions to reduce the amount of false positives. The ``--with-debugging-symbols`` flag passed during package setup will do this for you automatically.

.. note::

   For best results, you should run use a Python installation configured with Valgrind support (--with-valgrind)
.. _computation:

{{ header }}

Computational tools
===================


Statistical functions
---------------------

.. _computation.pct_change:

Percent change
~~~~~~~~~~~~~~

``Series`` and ``DataFrame`` have a method
:meth:`~DataFrame.pct_change` to compute the percent change over a given number
of periods (using ``fill_method`` to fill NA/null values *before* computing
the percent change).

.. ipython:: python

   ser = pd.Series(np.random.randn(8))

   ser.pct_change()

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 4))

   df.pct_change(periods=3)

.. _computation.covariance:

Covariance
~~~~~~~~~~

:meth:`Series.cov` can be used to compute covariance between series
(excluding missing values).

.. ipython:: python

   s1 = pd.Series(np.random.randn(1000))
   s2 = pd.Series(np.random.randn(1000))
   s1.cov(s2)

Analogously, :meth:`DataFrame.cov` to compute pairwise covariances among the
series in the DataFrame, also excluding NA/null values.

.. _computation.covariance.caveats:

.. note::

    Assuming the missing data are missing at random this results in an estimate
    for the covariance matrix which is unbiased. However, for many applications
    this estimate may not be acceptable because the estimated covariance matrix
    is not guaranteed to be positive semi-definite. This could lead to
    estimated correlations having absolute values which are greater than one,
    and/or a non-invertible covariance matrix. See `Estimation of covariance
    matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_matrices>`_
    for more details.

.. ipython:: python

   frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d", "e"])
   frame.cov()

``DataFrame.cov`` also supports an optional ``min_periods`` keyword that
specifies the required minimum number of observations for each column pair
in order to have a valid result.

.. ipython:: python

   frame = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])
   frame.loc[frame.index[:5], "a"] = np.nan
   frame.loc[frame.index[5:10], "b"] = np.nan

   frame.cov()

   frame.cov(min_periods=12)


.. _computation.correlation:

Correlation
~~~~~~~~~~~

Correlation may be computed using the :meth:`~DataFrame.corr` method.
Using the ``method`` parameter, several methods for computing correlations are
provided:

.. csv-table::
    :header: "Method name", "Description"
    :widths: 20, 80

    ``pearson (default)``, Standard correlation coefficient
    ``kendall``, Kendall Tau correlation coefficient
    ``spearman``, Spearman rank correlation coefficient

.. \rho = \cov(x, y) / \sigma_x \sigma_y

All of these are currently computed using pairwise complete observations.
Wikipedia has articles covering the above correlation coefficients:

* `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_
* `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_
* `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_

.. note::

    Please see the :ref:`caveats <computation.covariance.caveats>` associated
    with this method of calculating correlation matrices in the
    :ref:`covariance section <computation.covariance>`.

.. ipython:: python

   frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d", "e"])
   frame.iloc[::2] = np.nan

   # Series with Series
   frame["a"].corr(frame["b"])
   frame["a"].corr(frame["b"], method="spearman")

   # Pairwise correlation of DataFrame columns
   frame.corr()

Note that non-numeric columns will be automatically excluded from the
correlation calculation.

Like ``cov``, ``corr`` also supports the optional ``min_periods`` keyword:

.. ipython:: python

   frame = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])
   frame.loc[frame.index[:5], "a"] = np.nan
   frame.loc[frame.index[5:10], "b"] = np.nan

   frame.corr()

   frame.corr(min_periods=12)


The ``method`` argument can also be a callable for a generic correlation
calculation. In this case, it should be a single function
that produces a single value from two ndarray inputs. Suppose we wanted to
compute the correlation based on histogram intersection:

.. ipython:: python

   # histogram intersection
   def histogram_intersection(a, b):
       return np.minimum(np.true_divide(a, a.sum()), np.true_divide(b, b.sum())).sum()


   frame.corr(method=histogram_intersection)

A related method :meth:`~DataFrame.corrwith` is implemented on DataFrame to
compute the correlation between like-labeled Series contained in different
DataFrame objects.

.. ipython:: python

   index = ["a", "b", "c", "d", "e"]
   columns = ["one", "two", "three", "four"]
   df1 = pd.DataFrame(np.random.randn(5, 4), index=index, columns=columns)
   df2 = pd.DataFrame(np.random.randn(4, 4), index=index[:4], columns=columns)
   df1.corrwith(df2)
   df2.corrwith(df1, axis=1)

.. _computation.ranking:

Data ranking
~~~~~~~~~~~~

The :meth:`~Series.rank` method produces a data ranking with ties being
assigned the mean of the ranks (by default) for the group:

.. ipython:: python

   s = pd.Series(np.random.randn(5), index=list("abcde"))
   s["d"] = s["b"]  # so there's a tie
   s.rank()

:meth:`~DataFrame.rank` is also a DataFrame method and can rank either the rows
(``axis=0``) or the columns (``axis=1``). ``NaN`` values are excluded from the
ranking.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 6))
   df[4] = df[2][:5]  # some ties
   df
   df.rank(1)

``rank`` optionally takes a parameter ``ascending`` which by default is true;
when false, data is reverse-ranked, with larger values assigned a smaller rank.

``rank`` supports different tie-breaking methods, specified with the ``method``
parameter:

  - ``average`` : average rank of tied group
  - ``min`` : lowest rank in the group
  - ``max`` : highest rank in the group
  - ``first`` : ranks assigned in the order they appear in the array

.. _computation.windowing:

Windowing functions
~~~~~~~~~~~~~~~~~~~

See :ref:`the window operations user guide <window.overview>` for an overview of windowing functions.
.. _timeseries:

{{ header }}

********************************
Time series / date functionality
********************************

pandas contains extensive capabilities and features for working with time series data for all domains.
Using the NumPy ``datetime64`` and ``timedelta64`` dtypes, pandas has consolidated a large number of
features from other Python libraries like ``scikits.timeseries`` as well as created
a tremendous amount of new functionality for manipulating time series data.

For example, pandas supports:

Parsing time series information from various sources and formats

.. ipython:: python

   import datetime

   dti = pd.to_datetime(
       ["1/1/2018", np.datetime64("2018-01-01"), datetime.datetime(2018, 1, 1)]
   )
   dti

Generate sequences of fixed-frequency dates and time spans

.. ipython:: python

   dti = pd.date_range("2018-01-01", periods=3, freq="H")
   dti

Manipulating and converting date times with timezone information

.. ipython:: python

   dti = dti.tz_localize("UTC")
   dti
   dti.tz_convert("US/Pacific")

Resampling or converting a time series to a particular frequency

.. ipython:: python

   idx = pd.date_range("2018-01-01", periods=5, freq="H")
   ts = pd.Series(range(len(idx)), index=idx)
   ts
   ts.resample("2H").mean()

Performing date and time arithmetic with absolute or relative time increments

.. ipython:: python

    friday = pd.Timestamp("2018-01-05")
    friday.day_name()
    # Add 1 day
    saturday = friday + pd.Timedelta("1 day")
    saturday.day_name()
    # Add 1 business day (Friday --> Monday)
    monday = friday + pd.offsets.BDay()
    monday.day_name()

pandas provides a relatively compact and self-contained set of tools for
performing the above tasks and more.


.. _timeseries.overview:

Overview
--------

pandas captures 4 general time related concepts:

#. Date times: A specific date and time with timezone support. Similar to ``datetime.datetime`` from the standard library.
#. Time deltas: An absolute time duration. Similar to ``datetime.timedelta`` from the standard library.
#. Time spans: A span of time defined by a point in time and its associated frequency.
#. Date offsets: A relative time duration that respects calendar arithmetic. Similar to ``dateutil.relativedelta.relativedelta`` from the ``dateutil`` package.

=====================   =================  ===================   ============================================  ========================================
Concept                 Scalar Class       Array Class           pandas Data Type                              Primary Creation Method
=====================   =================  ===================   ============================================  ========================================
Date times              ``Timestamp``      ``DatetimeIndex``     ``datetime64[ns]`` or ``datetime64[ns, tz]``  ``to_datetime`` or ``date_range``
Time deltas             ``Timedelta``      ``TimedeltaIndex``    ``timedelta64[ns]``                           ``to_timedelta`` or ``timedelta_range``
Time spans              ``Period``         ``PeriodIndex``       ``period[freq]``                              ``Period`` or ``period_range``
Date offsets            ``DateOffset``     ``None``              ``None``                                      ``DateOffset``
=====================   =================  ===================   ============================================  ========================================

For time series data, it's conventional to represent the time component in the index of a :class:`Series` or :class:`DataFrame`
so manipulations can be performed with respect to the time element.

.. ipython:: python

   pd.Series(range(3), index=pd.date_range("2000", freq="D", periods=3))

However, :class:`Series` and :class:`DataFrame` can directly also support the time component as data itself.

.. ipython:: python

   pd.Series(pd.date_range("2000", freq="D", periods=3))

:class:`Series` and :class:`DataFrame` have extended data type support and functionality for ``datetime``, ``timedelta``
and ``Period`` data when passed into those constructors. ``DateOffset``
data however will be stored as ``object`` data.

.. ipython:: python

   pd.Series(pd.period_range("1/1/2011", freq="M", periods=3))
   pd.Series([pd.DateOffset(1), pd.DateOffset(2)])
   pd.Series(pd.date_range("1/1/2011", freq="M", periods=3))

Lastly, pandas represents null date times, time deltas, and time spans as ``NaT`` which
is useful for representing missing or null date like values and behaves similar
as ``np.nan`` does for float data.

.. ipython:: python

   pd.Timestamp(pd.NaT)
   pd.Timedelta(pd.NaT)
   pd.Period(pd.NaT)
   # Equality acts as np.nan would
   pd.NaT == pd.NaT

.. _timeseries.representation:

Timestamps vs. time spans
-------------------------

Timestamped data is the most basic type of time series data that associates
values with points in time. For pandas objects it means using the points in
time.

.. ipython:: python

   pd.Timestamp(datetime.datetime(2012, 5, 1))
   pd.Timestamp("2012-05-01")
   pd.Timestamp(2012, 5, 1)

However, in many cases it is more natural to associate things like change
variables with a time span instead. The span represented by ``Period`` can be
specified explicitly, or inferred from datetime string format.

For example:

.. ipython:: python

   pd.Period("2011-01")

   pd.Period("2012-05", freq="D")

:class:`Timestamp` and :class:`Period` can serve as an index. Lists of
``Timestamp`` and ``Period`` are automatically coerced to :class:`DatetimeIndex`
and :class:`PeriodIndex` respectively.

.. ipython:: python

   dates = [
       pd.Timestamp("2012-05-01"),
       pd.Timestamp("2012-05-02"),
       pd.Timestamp("2012-05-03"),
   ]
   ts = pd.Series(np.random.randn(3), dates)

   type(ts.index)
   ts.index

   ts

   periods = [pd.Period("2012-01"), pd.Period("2012-02"), pd.Period("2012-03")]

   ts = pd.Series(np.random.randn(3), periods)

   type(ts.index)
   ts.index

   ts

pandas allows you to capture both representations and
convert between them. Under the hood, pandas represents timestamps using
instances of ``Timestamp`` and sequences of timestamps using instances of
``DatetimeIndex``. For regular time spans, pandas uses ``Period`` objects for
scalar values and ``PeriodIndex`` for sequences of spans. Better support for
irregular intervals with arbitrary start and end points are forth-coming in
future releases.


.. _timeseries.converting:

Converting to timestamps
------------------------

To convert a :class:`Series` or list-like object of date-like objects e.g. strings,
epochs, or a mixture, you can use the ``to_datetime`` function. When passed
a ``Series``, this returns a ``Series`` (with the same index), while a list-like
is converted to a ``DatetimeIndex``:

.. ipython:: python

    pd.to_datetime(pd.Series(["Jul 31, 2009", "2010-01-10", None]))

    pd.to_datetime(["2005/11/23", "2010.12.31"])

If you use dates which start with the day first (i.e. European style),
you can pass the ``dayfirst`` flag:

.. ipython:: python
   :okwarning:

    pd.to_datetime(["04-01-2012 10:00"], dayfirst=True)

    pd.to_datetime(["14-01-2012", "01-14-2012"], dayfirst=True)

.. warning::

   You see in the above example that ``dayfirst`` isn't strict. If a date
   can't be parsed with the day being first it will be parsed as if
   ``dayfirst`` were False, and in the case of parsing delimited date strings
   (e.g. ``31-12-2012``) then a warning will also be raised.

If you pass a single string to ``to_datetime``, it returns a single ``Timestamp``.
``Timestamp`` can also accept string input, but it doesn't accept string parsing
options like ``dayfirst`` or ``format``, so use ``to_datetime`` if these are required.

.. ipython:: python

    pd.to_datetime("2010/11/12")

    pd.Timestamp("2010/11/12")

You can also use the ``DatetimeIndex`` constructor directly:

.. ipython:: python

    pd.DatetimeIndex(["2018-01-01", "2018-01-03", "2018-01-05"])

The string 'infer' can be passed in order to set the frequency of the index as the
inferred frequency upon creation:

.. ipython:: python

    pd.DatetimeIndex(["2018-01-01", "2018-01-03", "2018-01-05"], freq="infer")

.. _timeseries.converting.format:

Providing a format argument
~~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition to the required datetime string, a ``format`` argument can be passed to ensure specific parsing.
This could also potentially speed up the conversion considerably.

.. ipython:: python

    pd.to_datetime("2010/11/12", format="%Y/%m/%d")

    pd.to_datetime("12-11-2010 00:00", format="%d-%m-%Y %H:%M")

For more information on the choices available when specifying the ``format``
option, see the Python `datetime documentation`_.

.. _datetime documentation: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior

Assembling datetime from multiple DataFrame columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can also pass a ``DataFrame`` of integer or string columns to assemble into a ``Series`` of ``Timestamps``.

.. ipython:: python

   df = pd.DataFrame(
       {"year": [2015, 2016], "month": [2, 3], "day": [4, 5], "hour": [2, 3]}
   )
   pd.to_datetime(df)


You can pass only the columns that you need to assemble.

.. ipython:: python

   pd.to_datetime(df[["year", "month", "day"]])

``pd.to_datetime`` looks for standard designations of the datetime component in the column names, including:

* required: ``year``, ``month``, ``day``
* optional: ``hour``, ``minute``, ``second``, ``millisecond``, ``microsecond``, ``nanosecond``

Invalid data
~~~~~~~~~~~~

The default behavior, ``errors='raise'``, is to raise when unparsable:

.. code-block:: ipython

    In [2]: pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
    ValueError: Unknown string format

Pass ``errors='ignore'`` to return the original input when unparsable:

.. ipython:: python

   pd.to_datetime(["2009/07/31", "asd"], errors="ignore")

Pass ``errors='coerce'`` to convert unparsable data to ``NaT`` (not a time):

.. ipython:: python

   pd.to_datetime(["2009/07/31", "asd"], errors="coerce")


.. _timeseries.converting.epoch:

Epoch timestamps
~~~~~~~~~~~~~~~~

pandas supports converting integer or float epoch times to ``Timestamp`` and
``DatetimeIndex``. The default unit is nanoseconds, since that is how ``Timestamp``
objects are stored internally. However, epochs are often stored in another ``unit``
which can be specified. These are computed from the starting point specified by the
``origin`` parameter.

.. ipython:: python

   pd.to_datetime(
       [1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit="s"
   )

   pd.to_datetime(
       [1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500],
       unit="ms",
   )

.. note::

   The ``unit`` parameter does not use the same strings as the ``format`` parameter
   that was discussed :ref:`above<timeseries.converting.format>`). The
   available units are listed on the documentation for :func:`pandas.to_datetime`.

.. versionchanged:: 1.0.0

Constructing a :class:`Timestamp` or :class:`DatetimeIndex` with an epoch timestamp
with the ``tz`` argument specified will raise a ValueError. If you have
epochs in wall time in another timezone, you can read the epochs
as timezone-naive timestamps and then localize to the appropriate timezone:

.. ipython:: python

   pd.Timestamp(1262347200000000000).tz_localize("US/Pacific")
   pd.DatetimeIndex([1262347200000000000]).tz_localize("US/Pacific")

.. note::

   Epoch times will be rounded to the nearest nanosecond.

.. warning::

   Conversion of float epoch times can lead to inaccurate and unexpected results.
   :ref:`Python floats <python:tut-fp-issues>` have about 15 digits precision in
   decimal. Rounding during conversion from float to high precision ``Timestamp`` is
   unavoidable. The only way to achieve exact precision is to use a fixed-width
   types (e.g. an int64).

   .. ipython:: python

      pd.to_datetime([1490195805.433, 1490195805.433502912], unit="s")
      pd.to_datetime(1490195805433502912, unit="ns")

.. seealso::

   :ref:`timeseries.origin`

.. _timeseries.converting.epoch_inverse:

From timestamps to epoch
~~~~~~~~~~~~~~~~~~~~~~~~

To invert the operation from above, namely, to convert from a ``Timestamp`` to a 'unix' epoch:

.. ipython:: python

   stamps = pd.date_range("2012-10-08 18:15:05", periods=4, freq="D")
   stamps

We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the
"unit" (1 second).

.. ipython:: python

   (stamps - pd.Timestamp("1970-01-01")) // pd.Timedelta("1s")

.. _timeseries.origin:

Using the ``origin`` Parameter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using the ``origin`` parameter, one can specify an alternative starting point for creation
of a ``DatetimeIndex``. For example, to use 1960-01-01 as the starting date:

.. ipython:: python

   pd.to_datetime([1, 2, 3], unit="D", origin=pd.Timestamp("1960-01-01"))

The default is set at ``origin='unix'``, which defaults to ``1970-01-01 00:00:00``.
Commonly called 'unix epoch' or POSIX time.

.. ipython:: python

   pd.to_datetime([1, 2, 3], unit="D")

.. _timeseries.daterange:

Generating ranges of timestamps
-------------------------------

To generate an index with timestamps, you can use either the ``DatetimeIndex`` or
``Index`` constructor and pass in a list of datetime objects:

.. ipython:: python

   dates = [
       datetime.datetime(2012, 5, 1),
       datetime.datetime(2012, 5, 2),
       datetime.datetime(2012, 5, 3),
   ]

   # Note the frequency information
   index = pd.DatetimeIndex(dates)
   index

   # Automatically converted to DatetimeIndex
   index = pd.Index(dates)
   index

In practice this becomes very cumbersome because we often need a very long
index with a large number of timestamps. If we need timestamps on a regular
frequency, we can use the :func:`date_range` and :func:`bdate_range` functions
to create a ``DatetimeIndex``. The default frequency for ``date_range`` is a
**calendar day** while the default for ``bdate_range`` is a **business day**:

.. ipython:: python

   start = datetime.datetime(2011, 1, 1)
   end = datetime.datetime(2012, 1, 1)

   index = pd.date_range(start, end)
   index

   index = pd.bdate_range(start, end)
   index

Convenience functions like ``date_range`` and ``bdate_range`` can utilize a
variety of :ref:`frequency aliases <timeseries.offset_aliases>`:

.. ipython:: python

   pd.date_range(start, periods=1000, freq="M")

   pd.bdate_range(start, periods=250, freq="BQS")

``date_range`` and ``bdate_range`` make it easy to generate a range of dates
using various combinations of parameters like ``start``, ``end``, ``periods``,
and ``freq``. The start and end dates are strictly inclusive, so dates outside
of those specified will not be generated:

.. ipython:: python

   pd.date_range(start, end, freq="BM")

   pd.date_range(start, end, freq="W")

   pd.bdate_range(end=end, periods=20)

   pd.bdate_range(start=start, periods=20)

Specifying ``start``, ``end``, and ``periods`` will generate a range of evenly spaced
dates from ``start`` to ``end`` inclusively, with ``periods`` number of elements in the
resulting ``DatetimeIndex``:

.. ipython:: python

   pd.date_range("2018-01-01", "2018-01-05", periods=5)

   pd.date_range("2018-01-01", "2018-01-05", periods=10)

.. _timeseries.custom-freq-ranges:

Custom frequency ranges
~~~~~~~~~~~~~~~~~~~~~~~

``bdate_range`` can also generate a range of custom frequency dates by using
the ``weekmask`` and ``holidays`` parameters.  These parameters will only be
used if a custom frequency string is passed.

.. ipython:: python

   weekmask = "Mon Wed Fri"

   holidays = [datetime.datetime(2011, 1, 5), datetime.datetime(2011, 3, 14)]

   pd.bdate_range(start, end, freq="C", weekmask=weekmask, holidays=holidays)

   pd.bdate_range(start, end, freq="CBMS", weekmask=weekmask)

.. seealso::

   :ref:`timeseries.custombusinessdays`

.. _timeseries.timestamp-limits:

Timestamp limitations
---------------------

Since pandas represents timestamps in nanosecond resolution, the time span that
can be represented using a 64-bit integer is limited to approximately 584 years:

.. ipython:: python

   pd.Timestamp.min
   pd.Timestamp.max

.. seealso::

   :ref:`timeseries.oob`

.. _timeseries.datetimeindex:

Indexing
--------

One of the main uses for ``DatetimeIndex`` is as an index for pandas objects.
The ``DatetimeIndex`` class contains many time series related optimizations:

* A large range of dates for various offsets are pre-computed and cached
  under the hood in order to make generating subsequent date ranges very fast
  (just have to grab a slice).
* Fast shifting using the ``shift`` method on pandas objects.
* Unioning of overlapping ``DatetimeIndex`` objects with the same frequency is
  very fast (important for fast data alignment).
* Quick access to date fields via properties such as ``year``, ``month``, etc.
* Regularization functions like ``snap`` and very fast ``asof`` logic.

``DatetimeIndex`` objects have all the basic functionality of regular ``Index``
objects, and a smorgasbord of advanced time series specific methods for easy
frequency processing.

.. seealso::
    :ref:`Reindexing methods <basics.reindexing>`

.. note::

    While pandas does not force you to have a sorted date index, some of these
    methods may have unexpected or incorrect behavior if the dates are unsorted.

``DatetimeIndex`` can be used like a regular index and offers all of its
intelligent functionality like selection, slicing, etc.

.. ipython:: python

   rng = pd.date_range(start, end, freq="BM")
   ts = pd.Series(np.random.randn(len(rng)), index=rng)
   ts.index
   ts[:5].index
   ts[::2].index

.. _timeseries.partialindexing:

Partial string indexing
~~~~~~~~~~~~~~~~~~~~~~~

Dates and strings that parse to timestamps can be passed as indexing parameters:

.. ipython:: python

   ts["1/31/2011"]

   ts[datetime.datetime(2011, 12, 25):]

   ts["10/31/2011":"12/31/2011"]

To provide convenience for accessing longer time series, you can also pass in
the year or year and month as strings:

.. ipython:: python

   ts["2011"]

   ts["2011-6"]

This type of slicing will work on a ``DataFrame`` with a ``DatetimeIndex`` as well. Since the
partial string selection is a form of label slicing, the endpoints **will be** included. This
would include matching times on an included date:

.. warning::

   Indexing ``DataFrame`` rows with a *single* string with getitem (e.g. ``frame[dtstring]``)
   is deprecated starting with pandas 1.2.0 (given the ambiguity whether it is indexing
   the rows or selecting a column) and will be removed in a future version. The equivalent
   with ``.loc`` (e.g. ``frame.loc[dtstring]``) is still supported.

.. ipython:: python

   dft = pd.DataFrame(
       np.random.randn(100000, 1),
       columns=["A"],
       index=pd.date_range("20130101", periods=100000, freq="T"),
   )
   dft
   dft.loc["2013"]

This starts on the very first time in the month, and includes the last date and
time for the month:

.. ipython:: python

   dft["2013-1":"2013-2"]

This specifies a stop time **that includes all of the times on the last day**:

.. ipython:: python

   dft["2013-1":"2013-2-28"]

This specifies an **exact** stop time (and is not the same as the above):

.. ipython:: python

   dft["2013-1":"2013-2-28 00:00:00"]

We are stopping on the included end-point as it is part of the index:

.. ipython:: python

   dft["2013-1-15":"2013-1-15 12:30:00"]

``DatetimeIndex`` partial string indexing also works on a ``DataFrame`` with a ``MultiIndex``:

.. ipython:: python

   dft2 = pd.DataFrame(
       np.random.randn(20, 1),
       columns=["A"],
       index=pd.MultiIndex.from_product(
           [pd.date_range("20130101", periods=10, freq="12H"), ["a", "b"]]
       ),
   )
   dft2
   dft2.loc["2013-01-05"]
   idx = pd.IndexSlice
   dft2 = dft2.swaplevel(0, 1).sort_index()
   dft2.loc[idx[:, "2013-01-05"], :]

.. versionadded:: 0.25.0

Slicing with string indexing also honors UTC offset.

.. ipython:: python

    df = pd.DataFrame([0], index=pd.DatetimeIndex(["2019-01-01"], tz="US/Pacific"))
    df
    df["2019-01-01 12:00:00+04:00":"2019-01-01 13:00:00+04:00"]

.. _timeseries.slice_vs_exact_match:

Slice vs. exact match
~~~~~~~~~~~~~~~~~~~~~

The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match.

Consider a ``Series`` object with a minute resolution index:

.. ipython:: python

    series_minute = pd.Series(
        [1, 2, 3],
        pd.DatetimeIndex(
            ["2011-12-31 23:59:00", "2012-01-01 00:00:00", "2012-01-01 00:02:00"]
        ),
    )
    series_minute.index.resolution

A timestamp string less accurate than a minute gives a ``Series`` object.

.. ipython:: python

    series_minute["2011-12-31 23"]

A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice.

.. ipython:: python

    series_minute["2011-12-31 23:59"]
    series_minute["2011-12-31 23:59:00"]

If index resolution is second, then the minute-accurate timestamp gives a
``Series``.

.. ipython:: python

    series_second = pd.Series(
        [1, 2, 3],
        pd.DatetimeIndex(
            ["2011-12-31 23:59:59", "2012-01-01 00:00:00", "2012-01-01 00:00:01"]
        ),
    )
    series_second.index.resolution
    series_second["2011-12-31 23:59"]

If the timestamp string is treated as a slice, it can be used to index ``DataFrame`` with ``.loc[]`` as well.

.. ipython:: python

    dft_minute = pd.DataFrame(
        {"a": [1, 2, 3], "b": [4, 5, 6]}, index=series_minute.index
    )
    dft_minute.loc["2011-12-31 23"]


.. warning::

   However, if the string is treated as an exact match, the selection in ``DataFrame``'s ``[]`` will be column-wise and not row-wise, see :ref:`Indexing Basics <indexing.basics>`. For example ``dft_minute['2011-12-31 23:59']`` will raise ``KeyError`` as ``'2012-12-31 23:59'`` has the same resolution as the index and there is no column with such name:

   To *always* have unambiguous selection, whether the row is treated as a slice or a single selection, use ``.loc``.

   .. ipython:: python

      dft_minute.loc["2011-12-31 23:59"]

Note also that ``DatetimeIndex`` resolution cannot be less precise than day.

.. ipython:: python

    series_monthly = pd.Series(
        [1, 2, 3], pd.DatetimeIndex(["2011-12", "2012-01", "2012-02"])
    )
    series_monthly.index.resolution
    series_monthly["2011-12"]  # returns Series


Exact indexing
~~~~~~~~~~~~~~

As discussed in previous section, indexing a ``DatetimeIndex`` with a partial string depends on the "accuracy" of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with ``Timestamp`` or ``datetime`` objects is exact, because the objects have exact meaning. These also follow the semantics of *including both endpoints*.

These ``Timestamp`` and ``datetime`` objects have exact ``hours, minutes,`` and ``seconds``, even though they were not explicitly specified (they are ``0``).

.. ipython:: python

   dft[datetime.datetime(2013, 1, 1): datetime.datetime(2013, 2, 28)]

With no defaults.

.. ipython:: python

   dft[
       datetime.datetime(2013, 1, 1, 10, 12, 0): datetime.datetime(
           2013, 2, 28, 10, 12, 0
       )
   ]

Truncating & fancy indexing
~~~~~~~~~~~~~~~~~~~~~~~~~~~

A :meth:`~DataFrame.truncate` convenience function is provided that is similar
to slicing. Note that ``truncate`` assumes a 0 value for any unspecified date
component in a ``DatetimeIndex`` in contrast to slicing which returns any
partially matching dates:

.. ipython:: python

   rng2 = pd.date_range("2011-01-01", "2012-01-01", freq="W")
   ts2 = pd.Series(np.random.randn(len(rng2)), index=rng2)

   ts2.truncate(before="2011-11", after="2011-12")
   ts2["2011-11":"2011-12"]

Even complicated fancy indexing that breaks the ``DatetimeIndex`` frequency
regularity will result in a ``DatetimeIndex``, although frequency is lost:

.. ipython:: python

   ts2[[0, 2, 6]].index

.. _timeseries.components:

Time/date components
--------------------

There are several time/date properties that one can access from ``Timestamp`` or a collection of timestamps like a ``DatetimeIndex``.

.. csv-table::
    :header: "Property", "Description"
    :widths: 15, 65

    year, "The year of the datetime"
    month,"The month of the datetime"
    day,"The days of the datetime"
    hour,"The hour of the datetime"
    minute,"The minutes of the datetime"
    second,"The seconds of the datetime"
    microsecond,"The microseconds of the datetime"
    nanosecond,"The nanoseconds of the datetime"
    date,"Returns datetime.date (does not contain timezone information)"
    time,"Returns datetime.time (does not contain timezone information)"
    timetz,"Returns datetime.time as local time with timezone information"
    dayofyear,"The ordinal day of year"
    day_of_year,"The ordinal day of year"
    weekofyear,"The week ordinal of the year"
    week,"The week ordinal of the year"
    dayofweek,"The number of the day of the week with Monday=0, Sunday=6"
    day_of_week,"The number of the day of the week with Monday=0, Sunday=6"
    weekday,"The number of the day of the week with Monday=0, Sunday=6"
    quarter,"Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc."
    days_in_month,"The number of days in the month of the datetime"
    is_month_start,"Logical indicating if first day of month (defined by frequency)"
    is_month_end,"Logical indicating if last day of month (defined by frequency)"
    is_quarter_start,"Logical indicating if first day of quarter (defined by frequency)"
    is_quarter_end,"Logical indicating if last day of quarter (defined by frequency)"
    is_year_start,"Logical indicating if first day of year (defined by frequency)"
    is_year_end,"Logical indicating if last day of year (defined by frequency)"
    is_leap_year,"Logical indicating if the date belongs to a leap year"

Furthermore, if you have a ``Series`` with datetimelike values, then you can
access these properties via the ``.dt`` accessor, as detailed in the section
on :ref:`.dt accessors<basics.dt_accessors>`.

.. versionadded:: 1.1.0

You may obtain the year, week and day components of the ISO year from the ISO 8601 standard:

.. ipython:: python

   idx = pd.date_range(start="2019-12-29", freq="D", periods=4)
   idx.isocalendar()
   idx.to_series().dt.isocalendar()

.. _timeseries.offsets:

DateOffset objects
------------------

In the preceding examples, frequency strings (e.g. ``'D'``) were used to specify
a frequency that defined:

* how the date times in :class:`DatetimeIndex` were spaced when using :meth:`date_range`
* the frequency of a :class:`Period` or :class:`PeriodIndex`

These frequency strings map to a :class:`DateOffset` object and its subclasses. A :class:`DateOffset`
is similar to a :class:`Timedelta` that represents a duration of time but follows specific calendar duration rules.
For example, a :class:`Timedelta` day will always increment ``datetimes`` by 24 hours, while a :class:`DateOffset` day
will increment ``datetimes`` to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight
savings time. However, all :class:`DateOffset` subclasses that are an hour or smaller
(``Hour``, ``Minute``, ``Second``, ``Milli``, ``Micro``, ``Nano``) behave like
:class:`Timedelta` and respect absolute time.

The basic :class:`DateOffset` acts similar to ``dateutil.relativedelta`` (`relativedelta documentation`_)
that shifts a date time by the corresponding calendar duration specified. The
arithmetic operator (``+``) can be used to perform the shift.

.. ipython:: python

   # This particular day contains a day light savings time transition
   ts = pd.Timestamp("2016-10-30 00:00:00", tz="Europe/Helsinki")
   # Respects absolute time
   ts + pd.Timedelta(days=1)
   # Respects calendar time
   ts + pd.DateOffset(days=1)
   friday = pd.Timestamp("2018-01-05")
   friday.day_name()
   # Add 2 business days (Friday --> Tuesday)
   two_business_days = 2 * pd.offsets.BDay()
   friday + two_business_days
   (friday + two_business_days).day_name()

Most ``DateOffsets`` have associated frequencies strings, or offset aliases, that can be passed
into ``freq`` keyword arguments. The available date offsets and associated frequency strings can be found below:

.. csv-table::
    :header: "Date Offset", "Frequency String", "Description"
    :widths: 15, 15, 65

    :class:`~pandas.tseries.offsets.DateOffset`, None, "Generic offset class, defaults to absolute 24 hours"
    :class:`~pandas.tseries.offsets.BDay` or :class:`~pandas.tseries.offsets.BusinessDay`, ``'B'``,"business day (weekday)"
    :class:`~pandas.tseries.offsets.CDay` or :class:`~pandas.tseries.offsets.CustomBusinessDay`, ``'C'``, "custom business day"
    :class:`~pandas.tseries.offsets.Week`, ``'W'``, "one week, optionally anchored on a day of the week"
    :class:`~pandas.tseries.offsets.WeekOfMonth`, ``'WOM'``, "the x-th day of the y-th week of each month"
    :class:`~pandas.tseries.offsets.LastWeekOfMonth`, ``'LWOM'``, "the x-th day of the last week of each month"
    :class:`~pandas.tseries.offsets.MonthEnd`, ``'M'``, "calendar month end"
    :class:`~pandas.tseries.offsets.MonthBegin`, ``'MS'``, "calendar month begin"
    :class:`~pandas.tseries.offsets.BMonthEnd` or :class:`~pandas.tseries.offsets.BusinessMonthEnd`, ``'BM'``, "business month end"
    :class:`~pandas.tseries.offsets.BMonthBegin` or :class:`~pandas.tseries.offsets.BusinessMonthBegin`, ``'BMS'``, "business month begin"
    :class:`~pandas.tseries.offsets.CBMonthEnd` or :class:`~pandas.tseries.offsets.CustomBusinessMonthEnd`, ``'CBM'``, "custom business month end"
    :class:`~pandas.tseries.offsets.CBMonthBegin` or :class:`~pandas.tseries.offsets.CustomBusinessMonthBegin`, ``'CBMS'``, "custom business month begin"
    :class:`~pandas.tseries.offsets.SemiMonthEnd`, ``'SM'``, "15th (or other day_of_month) and calendar month end"
    :class:`~pandas.tseries.offsets.SemiMonthBegin`, ``'SMS'``, "15th (or other day_of_month) and calendar month begin"
    :class:`~pandas.tseries.offsets.QuarterEnd`, ``'Q'``, "calendar quarter end"
    :class:`~pandas.tseries.offsets.QuarterBegin`, ``'QS'``, "calendar quarter begin"
    :class:`~pandas.tseries.offsets.BQuarterEnd`, ``'BQ``, "business quarter end"
    :class:`~pandas.tseries.offsets.BQuarterBegin`, ``'BQS'``, "business quarter begin"
    :class:`~pandas.tseries.offsets.FY5253Quarter`, ``'REQ'``, "retail (aka 52-53 week) quarter"
    :class:`~pandas.tseries.offsets.YearEnd`, ``'A'``, "calendar year end"
    :class:`~pandas.tseries.offsets.YearBegin`, ``'AS'`` or ``'BYS'``,"calendar year begin"
    :class:`~pandas.tseries.offsets.BYearEnd`, ``'BA'``, "business year end"
    :class:`~pandas.tseries.offsets.BYearBegin`, ``'BAS'``, "business year begin"
    :class:`~pandas.tseries.offsets.FY5253`, ``'RE'``, "retail (aka 52-53 week) year"
    :class:`~pandas.tseries.offsets.Easter`, None, "Easter holiday"
    :class:`~pandas.tseries.offsets.BusinessHour`, ``'BH'``, "business hour"
    :class:`~pandas.tseries.offsets.CustomBusinessHour`, ``'CBH'``, "custom business hour"
    :class:`~pandas.tseries.offsets.Day`, ``'D'``, "one absolute day"
    :class:`~pandas.tseries.offsets.Hour`, ``'H'``, "one hour"
    :class:`~pandas.tseries.offsets.Minute`, ``'T'`` or ``'min'``,"one minute"
    :class:`~pandas.tseries.offsets.Second`, ``'S'``, "one second"
    :class:`~pandas.tseries.offsets.Milli`, ``'L'`` or ``'ms'``, "one millisecond"
    :class:`~pandas.tseries.offsets.Micro`, ``'U'`` or ``'us'``, "one microsecond"
    :class:`~pandas.tseries.offsets.Nano`, ``'N'``, "one nanosecond"

``DateOffsets`` additionally have :meth:`rollforward` and :meth:`rollback`
methods for moving a date forward or backward respectively to a valid offset
date relative to the offset. For example, business offsets will roll dates
that land on the weekends (Saturday and Sunday) forward to Monday since
business offsets operate on the weekdays.

.. ipython:: python

   ts = pd.Timestamp("2018-01-06 00:00:00")
   ts.day_name()
   # BusinessHour's valid offset dates are Monday through Friday
   offset = pd.offsets.BusinessHour(start="09:00")
   # Bring the date to the closest offset date (Monday)
   offset.rollforward(ts)
   # Date is brought to the closest offset date first and then the hour is added
   ts + offset

These operations preserve time (hour, minute, etc) information by default.
To reset time to midnight, use :meth:`normalize` before or after applying
the operation (depending on whether you want the time information included
in the operation).

.. ipython:: python

   ts = pd.Timestamp("2014-01-01 09:00")
   day = pd.offsets.Day()
   day + ts
   (day + ts).normalize()

   ts = pd.Timestamp("2014-01-01 22:00")
   hour = pd.offsets.Hour()
   hour + ts
   (hour + ts).normalize()
   (hour + pd.Timestamp("2014-01-01 23:30")).normalize()

.. _relativedelta documentation: https://dateutil.readthedocs.io/en/stable/relativedelta.html


Parametric offsets
~~~~~~~~~~~~~~~~~~

Some of the offsets can be "parameterized" when created to result in different
behaviors. For example, the ``Week`` offset for generating weekly data accepts a
``weekday`` parameter which results in the generated dates always lying on a
particular day of the week:

.. ipython:: python

   d = datetime.datetime(2008, 8, 18, 9, 0)
   d
   d + pd.offsets.Week()
   d + pd.offsets.Week(weekday=4)
   (d + pd.offsets.Week(weekday=4)).weekday()

   d - pd.offsets.Week()

The ``normalize`` option will be effective for addition and subtraction.

.. ipython:: python

   d + pd.offsets.Week(normalize=True)
   d - pd.offsets.Week(normalize=True)


Another example is parameterizing ``YearEnd`` with the specific ending month:

.. ipython:: python

   d + pd.offsets.YearEnd()
   d + pd.offsets.YearEnd(month=6)


.. _timeseries.offsetseries:

Using offsets with ``Series`` / ``DatetimeIndex``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Offsets can be used with either a ``Series`` or ``DatetimeIndex`` to
apply the offset to each element.

.. ipython:: python

   rng = pd.date_range("2012-01-01", "2012-01-03")
   s = pd.Series(rng)
   rng
   rng + pd.DateOffset(months=2)
   s + pd.DateOffset(months=2)
   s - pd.DateOffset(months=2)

If the offset class maps directly to a ``Timedelta`` (``Day``, ``Hour``,
``Minute``, ``Second``, ``Micro``, ``Milli``, ``Nano``) it can be
used exactly like a ``Timedelta`` - see the
:ref:`Timedelta section<timedeltas.operations>` for more examples.

.. ipython:: python

   s - pd.offsets.Day(2)
   td = s - pd.Series(pd.date_range("2011-12-29", "2011-12-31"))
   td
   td + pd.offsets.Minute(15)

Note that some offsets (such as ``BQuarterEnd``) do not have a
vectorized implementation.  They can still be used but may
calculate significantly slower and will show a ``PerformanceWarning``

.. ipython:: python
   :okwarning:

   rng + pd.offsets.BQuarterEnd()


.. _timeseries.custombusinessdays:

Custom business days
~~~~~~~~~~~~~~~~~~~~

The ``CDay`` or ``CustomBusinessDay`` class provides a parametric
``BusinessDay`` class which can be used to create customized business day
calendars which account for local holidays and local weekend conventions.

As an interesting example, let's look at Egypt where a Friday-Saturday weekend is observed.

.. ipython:: python

    weekmask_egypt = "Sun Mon Tue Wed Thu"

    # They also observe International Workers' Day so let's
    # add that for a couple of years

    holidays = [
        "2012-05-01",
        datetime.datetime(2013, 5, 1),
        np.datetime64("2014-05-01"),
    ]
    bday_egypt = pd.offsets.CustomBusinessDay(
        holidays=holidays,
        weekmask=weekmask_egypt,
    )
    dt = datetime.datetime(2013, 4, 30)
    dt + 2 * bday_egypt

Let's map to the weekday names:

.. ipython:: python

    dts = pd.date_range(dt, periods=5, freq=bday_egypt)

    pd.Series(dts.weekday, dts).map(pd.Series("Mon Tue Wed Thu Fri Sat Sun".split()))

Holiday calendars can be used to provide the list of holidays.  See the
:ref:`holiday calendar<timeseries.holiday>` section for more information.

.. ipython:: python

    from pandas.tseries.holiday import USFederalHolidayCalendar

    bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())

    # Friday before MLK Day
    dt = datetime.datetime(2014, 1, 17)

    # Tuesday after MLK Day (Monday is skipped because it's a holiday)
    dt + bday_us

Monthly offsets that respect a certain holiday calendar can be defined
in the usual way.

.. ipython:: python

    bmth_us = pd.offsets.CustomBusinessMonthBegin(calendar=USFederalHolidayCalendar())

    # Skip new years
    dt = datetime.datetime(2013, 12, 17)
    dt + bmth_us

    # Define date index with custom offset
    pd.date_range(start="20100101", end="20120101", freq=bmth_us)

.. note::

    The frequency string 'C' is used to indicate that a CustomBusinessDay
    DateOffset is used, it is important to note that since CustomBusinessDay is
    a parameterised type, instances of CustomBusinessDay may differ and this is
    not detectable from the 'C' frequency string. The user therefore needs to
    ensure that the 'C' frequency string is used consistently within the user's
    application.

.. _timeseries.businesshour:

Business hour
~~~~~~~~~~~~~

The ``BusinessHour`` class provides a business hour representation on ``BusinessDay``,
allowing to use specific start and end times.

By default, ``BusinessHour`` uses 9:00 - 17:00 as business hours.
Adding ``BusinessHour`` will increment ``Timestamp`` by hourly frequency.
If target ``Timestamp`` is out of business hours, move to the next business hour
then increment it. If the result exceeds the business hours end, the remaining
hours are added to the next business day.

.. ipython:: python

    bh = pd.offsets.BusinessHour()
    bh

    # 2014-08-01 is Friday
    pd.Timestamp("2014-08-01 10:00").weekday()
    pd.Timestamp("2014-08-01 10:00") + bh

    # Below example is the same as: pd.Timestamp('2014-08-01 09:00') + bh
    pd.Timestamp("2014-08-01 08:00") + bh

    # If the results is on the end time, move to the next business day
    pd.Timestamp("2014-08-01 16:00") + bh

    # Remainings are added to the next day
    pd.Timestamp("2014-08-01 16:30") + bh

    # Adding 2 business hours
    pd.Timestamp("2014-08-01 10:00") + pd.offsets.BusinessHour(2)

    # Subtracting 3 business hours
    pd.Timestamp("2014-08-01 10:00") + pd.offsets.BusinessHour(-3)

You can also specify ``start`` and ``end`` time by keywords. The argument must
be a ``str`` with an ``hour:minute`` representation or a ``datetime.time``
instance. Specifying seconds, microseconds and nanoseconds as business hour
results in ``ValueError``.

.. ipython:: python

    bh = pd.offsets.BusinessHour(start="11:00", end=datetime.time(20, 0))
    bh

    pd.Timestamp("2014-08-01 13:00") + bh
    pd.Timestamp("2014-08-01 09:00") + bh
    pd.Timestamp("2014-08-01 18:00") + bh

Passing ``start`` time later than ``end`` represents midnight business hour.
In this case, business hour exceeds midnight and overlap to the next day.
Valid business hours are distinguished by whether it started from valid ``BusinessDay``.

.. ipython:: python

    bh = pd.offsets.BusinessHour(start="17:00", end="09:00")
    bh

    pd.Timestamp("2014-08-01 17:00") + bh
    pd.Timestamp("2014-08-01 23:00") + bh

    # Although 2014-08-02 is Saturday,
    # it is valid because it starts from 08-01 (Friday).
    pd.Timestamp("2014-08-02 04:00") + bh

    # Although 2014-08-04 is Monday,
    # it is out of business hours because it starts from 08-03 (Sunday).
    pd.Timestamp("2014-08-04 04:00") + bh

Applying ``BusinessHour.rollforward`` and ``rollback`` to out of business hours results in
the next business hour start or previous day's end. Different from other offsets, ``BusinessHour.rollforward``
may output different results from ``apply`` by definition.

This is because one day's business hour end is equal to next day's business hour start. For example,
under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between ``2014-08-01 17:00`` and
``2014-08-04 09:00``.

.. ipython:: python

    # This adjusts a Timestamp to business hour edge
    pd.offsets.BusinessHour().rollback(pd.Timestamp("2014-08-02 15:00"))
    pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02 15:00"))

    # It is the same as BusinessHour() + pd.Timestamp('2014-08-01 17:00').
    # And it is the same as BusinessHour() + pd.Timestamp('2014-08-04 09:00')
    pd.offsets.BusinessHour() + pd.Timestamp("2014-08-02 15:00")

    # BusinessDay results (for reference)
    pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02"))

    # It is the same as BusinessDay() + pd.Timestamp('2014-08-01')
    # The result is the same as rollworward because BusinessDay never overlap.
    pd.offsets.BusinessHour() + pd.Timestamp("2014-08-02")

``BusinessHour`` regards Saturday and Sunday as holidays. To use arbitrary
holidays, you can use ``CustomBusinessHour`` offset, as explained in the
following subsection.

.. _timeseries.custombusinesshour:

Custom business hour
~~~~~~~~~~~~~~~~~~~~

The ``CustomBusinessHour`` is a mixture of ``BusinessHour`` and ``CustomBusinessDay`` which
allows you to specify arbitrary holidays. ``CustomBusinessHour`` works as the same
as ``BusinessHour`` except that it skips specified custom holidays.

.. ipython:: python

    from pandas.tseries.holiday import USFederalHolidayCalendar

    bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar())
    # Friday before MLK Day
    dt = datetime.datetime(2014, 1, 17, 15)

    dt + bhour_us

    # Tuesday after MLK Day (Monday is skipped because it's a holiday)
    dt + bhour_us * 2

You can use keyword arguments supported by either ``BusinessHour`` and ``CustomBusinessDay``.

.. ipython:: python

    bhour_mon = pd.offsets.CustomBusinessHour(start="10:00", weekmask="Tue Wed Thu Fri")

    # Monday is skipped because it's a holiday, business hour starts from 10:00
    dt + bhour_mon * 2

.. _timeseries.offset_aliases:

Offset aliases
~~~~~~~~~~~~~~

A number of string aliases are given to useful common time series
frequencies. We will refer to these aliases as *offset aliases*.

.. csv-table::
    :header: "Alias", "Description"
    :widths: 15, 100

    "B", "business day frequency"
    "C", "custom business day frequency"
    "D", "calendar day frequency"
    "W", "weekly frequency"
    "M", "month end frequency"
    "SM", "semi-month end frequency (15th and end of month)"
    "BM", "business month end frequency"
    "CBM", "custom business month end frequency"
    "MS", "month start frequency"
    "SMS", "semi-month start frequency (1st and 15th)"
    "BMS", "business month start frequency"
    "CBMS", "custom business month start frequency"
    "Q", "quarter end frequency"
    "BQ", "business quarter end frequency"
    "QS", "quarter start frequency"
    "BQS", "business quarter start frequency"
    "A, Y", "year end frequency"
    "BA, BY", "business year end frequency"
    "AS, YS", "year start frequency"
    "BAS, BYS", "business year start frequency"
    "BH", "business hour frequency"
    "H", "hourly frequency"
    "T, min", "minutely frequency"
    "S", "secondly frequency"
    "L, ms", "milliseconds"
    "U, us", "microseconds"
    "N", "nanoseconds"

.. note::

    When using the offset aliases above, it should be noted that functions
    such as :func:`date_range`, :func:`bdate_range`, will only return
    timestamps that are in the interval defined by ``start_date`` and
    ``end_date``. If the ``start_date`` does not correspond to the frequency,
    the returned timestamps will start at the next valid timestamp, same for
    ``end_date``, the returned timestamps will stop at the previous valid
    timestamp.

   For example, for the offset ``MS``, if the ``start_date`` is not the first
   of the month, the returned timestamps will start with the first day of the
   next month. If ``end_date`` is not the first day of a month, the last
   returned timestamp will be the first day of the corresponding month.

   .. ipython:: python

       dates_lst_1 = pd.date_range("2020-01-06", "2020-04-03", freq="MS")
       dates_lst_1

       dates_lst_2 = pd.date_range("2020-01-01", "2020-04-01", freq="MS")
       dates_lst_2

   We can see in the above example :func:`date_range` and
   :func:`bdate_range` will only return the valid timestamps between the
   ``start_date`` and ``end_date``. If these are not valid timestamps for the
   given frequency it will roll to the next value for ``start_date``
   (respectively previous for the ``end_date``)


Combining aliases
~~~~~~~~~~~~~~~~~

As we have seen previously, the alias and the offset instance are fungible in
most functions:

.. ipython:: python

   pd.date_range(start, periods=5, freq="B")

   pd.date_range(start, periods=5, freq=pd.offsets.BDay())

You can combine together day and intraday offsets:

.. ipython:: python

   pd.date_range(start, periods=10, freq="2h20min")

   pd.date_range(start, periods=10, freq="1D10U")

Anchored offsets
~~~~~~~~~~~~~~~~

For some frequencies you can specify an anchoring suffix:

.. csv-table::
    :header: "Alias", "Description"
    :widths: 15, 100

    "W\-SUN", "weekly frequency (Sundays). Same as 'W'"
    "W\-MON", "weekly frequency (Mondays)"
    "W\-TUE", "weekly frequency (Tuesdays)"
    "W\-WED", "weekly frequency (Wednesdays)"
    "W\-THU", "weekly frequency (Thursdays)"
    "W\-FRI", "weekly frequency (Fridays)"
    "W\-SAT", "weekly frequency (Saturdays)"
    "(B)Q(S)\-DEC", "quarterly frequency, year ends in December. Same as 'Q'"
    "(B)Q(S)\-JAN", "quarterly frequency, year ends in January"
    "(B)Q(S)\-FEB", "quarterly frequency, year ends in February"
    "(B)Q(S)\-MAR", "quarterly frequency, year ends in March"
    "(B)Q(S)\-APR", "quarterly frequency, year ends in April"
    "(B)Q(S)\-MAY", "quarterly frequency, year ends in May"
    "(B)Q(S)\-JUN", "quarterly frequency, year ends in June"
    "(B)Q(S)\-JUL", "quarterly frequency, year ends in July"
    "(B)Q(S)\-AUG", "quarterly frequency, year ends in August"
    "(B)Q(S)\-SEP", "quarterly frequency, year ends in September"
    "(B)Q(S)\-OCT", "quarterly frequency, year ends in October"
    "(B)Q(S)\-NOV", "quarterly frequency, year ends in November"
    "(B)A(S)\-DEC", "annual frequency, anchored end of December. Same as 'A'"
    "(B)A(S)\-JAN", "annual frequency, anchored end of January"
    "(B)A(S)\-FEB", "annual frequency, anchored end of February"
    "(B)A(S)\-MAR", "annual frequency, anchored end of March"
    "(B)A(S)\-APR", "annual frequency, anchored end of April"
    "(B)A(S)\-MAY", "annual frequency, anchored end of May"
    "(B)A(S)\-JUN", "annual frequency, anchored end of June"
    "(B)A(S)\-JUL", "annual frequency, anchored end of July"
    "(B)A(S)\-AUG", "annual frequency, anchored end of August"
    "(B)A(S)\-SEP", "annual frequency, anchored end of September"
    "(B)A(S)\-OCT", "annual frequency, anchored end of October"
    "(B)A(S)\-NOV", "annual frequency, anchored end of November"

These can be used as arguments to ``date_range``, ``bdate_range``, constructors
for ``DatetimeIndex``, as well as various other timeseries-related functions
in pandas.

Anchored offset semantics
~~~~~~~~~~~~~~~~~~~~~~~~~

For those offsets that are anchored to the start or end of specific
frequency (``MonthEnd``, ``MonthBegin``, ``WeekEnd``, etc), the following
rules apply to rolling forward and backwards.

When ``n`` is not 0, if the given date is not on an anchor point, it snapped to the next(previous)
anchor point, and moved ``|n|-1`` additional steps forwards or backwards.

.. ipython:: python

   pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=1)
   pd.Timestamp("2014-01-02") + pd.offsets.MonthEnd(n=1)

   pd.Timestamp("2014-01-02") - pd.offsets.MonthBegin(n=1)
   pd.Timestamp("2014-01-02") - pd.offsets.MonthEnd(n=1)

   pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=4)
   pd.Timestamp("2014-01-02") - pd.offsets.MonthBegin(n=4)

If the given date *is* on an anchor point, it is moved ``|n|`` points forwards
or backwards.

.. ipython:: python

   pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=1)
   pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=1)

   pd.Timestamp("2014-01-01") - pd.offsets.MonthBegin(n=1)
   pd.Timestamp("2014-01-31") - pd.offsets.MonthEnd(n=1)

   pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=4)
   pd.Timestamp("2014-01-31") - pd.offsets.MonthBegin(n=4)

For the case when ``n=0``, the date is not moved if on an anchor point, otherwise
it is rolled forward to the next anchor point.

.. ipython:: python

   pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=0)
   pd.Timestamp("2014-01-02") + pd.offsets.MonthEnd(n=0)

   pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=0)
   pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=0)

.. _timeseries.holiday:

Holidays / holiday calendars
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Holidays and calendars provide a simple way to define holiday rules to be used
with ``CustomBusinessDay`` or in other analysis that requires a predefined
set of holidays.  The ``AbstractHolidayCalendar`` class provides all the necessary
methods to return a list of holidays and only ``rules`` need to be defined
in a specific holiday calendar class. Furthermore, the ``start_date`` and ``end_date``
class attributes determine over what date range holidays are generated.  These
should be overwritten on the ``AbstractHolidayCalendar`` class to have the range
apply to all calendar subclasses.  ``USFederalHolidayCalendar`` is the
only calendar that exists and primarily serves as an example for developing
other calendars.

For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an
observance rule determines when that holiday is observed if it falls on a weekend
or some other non-observed day.  Defined observance rules are:

.. csv-table::
    :header: "Rule", "Description"
    :widths: 15, 70

    "nearest_workday", "move Saturday to Friday and Sunday to Monday"
    "sunday_to_monday", "move Sunday to following Monday"
    "next_monday_or_tuesday", "move Saturday to Monday and Sunday/Monday to Tuesday"
    "previous_friday", move Saturday and Sunday to previous Friday"
    "next_monday", "move Saturday and Sunday to following Monday"

An example of how holidays and holiday calendars are defined:

.. ipython:: python

    from pandas.tseries.holiday import (
        Holiday,
        USMemorialDay,
        AbstractHolidayCalendar,
        nearest_workday,
        MO,
    )

    class ExampleCalendar(AbstractHolidayCalendar):
        rules = [
            USMemorialDay,
            Holiday("July 4th", month=7, day=4, observance=nearest_workday),
            Holiday(
                "Columbus Day",
                month=10,
                day=1,
                offset=pd.DateOffset(weekday=MO(2)),
            ),
        ]

    cal = ExampleCalendar()
    cal.holidays(datetime.datetime(2012, 1, 1), datetime.datetime(2012, 12, 31))

:hint:
   **weekday=MO(2)** is same as **2 * Week(weekday=2)**

Using this calendar, creating an index or doing offset arithmetic skips weekends
and holidays (i.e., Memorial Day/July 4th).  For example, the below defines
a custom business day offset using the ``ExampleCalendar``.  Like any other offset,
it can be used to create a ``DatetimeIndex`` or added to ``datetime``
or ``Timestamp`` objects.

.. ipython:: python

    pd.date_range(
        start="7/1/2012", end="7/10/2012", freq=pd.offsets.CDay(calendar=cal)
    ).to_pydatetime()
    offset = pd.offsets.CustomBusinessDay(calendar=cal)
    datetime.datetime(2012, 5, 25) + offset
    datetime.datetime(2012, 7, 3) + offset
    datetime.datetime(2012, 7, 3) + 2 * offset
    datetime.datetime(2012, 7, 6) + offset

Ranges are defined by the ``start_date`` and ``end_date`` class attributes
of ``AbstractHolidayCalendar``.  The defaults are shown below.

.. ipython:: python

    AbstractHolidayCalendar.start_date
    AbstractHolidayCalendar.end_date

These dates can be overwritten by setting the attributes as
datetime/Timestamp/string.

.. ipython:: python

    AbstractHolidayCalendar.start_date = datetime.datetime(2012, 1, 1)
    AbstractHolidayCalendar.end_date = datetime.datetime(2012, 12, 31)
    cal.holidays()

Every calendar class is accessible by name using the ``get_calendar`` function
which returns a holiday class instance.  Any imported calendar class will
automatically be available by this function.  Also, ``HolidayCalendarFactory``
provides an easy interface to create calendars that are combinations of calendars
or calendars with additional rules.

.. ipython:: python

    from pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, USLaborDay

    cal = get_calendar("ExampleCalendar")
    cal.rules
    new_cal = HolidayCalendarFactory("NewExampleCalendar", cal, USLaborDay)
    new_cal.rules

.. _timeseries.advanced_datetime:

Time series-related instance methods
------------------------------------

Shifting / lagging
~~~~~~~~~~~~~~~~~~

One may want to *shift* or *lag* the values in a time series back and forward in
time. The method for this is :meth:`~Series.shift`, which is available on all of
the pandas objects.

.. ipython:: python

   ts = pd.Series(range(len(rng)), index=rng)
   ts = ts[:5]
   ts.shift(1)

The ``shift`` method accepts an ``freq`` argument which can accept a
``DateOffset`` class or other ``timedelta``-like object or also an
:ref:`offset alias <timeseries.offset_aliases>`.

When ``freq`` is specified, ``shift`` method changes all the dates in the index
rather than changing the alignment of the data and the index:

.. ipython:: python

   ts.shift(5, freq="D")
   ts.shift(5, freq=pd.offsets.BDay())
   ts.shift(5, freq="BM")

Note that with when ``freq`` is specified, the leading entry is no longer NaN
because the data is not being realigned.

Frequency conversion
~~~~~~~~~~~~~~~~~~~~

The primary function for changing frequencies is the :meth:`~Series.asfreq`
method. For a ``DatetimeIndex``, this is basically just a thin, but convenient
wrapper around :meth:`~Series.reindex`  which generates a ``date_range`` and
calls ``reindex``.

.. ipython:: python

   dr = pd.date_range("1/1/2010", periods=3, freq=3 * pd.offsets.BDay())
   ts = pd.Series(np.random.randn(3), index=dr)
   ts
   ts.asfreq(pd.offsets.BDay())

``asfreq`` provides a further convenience so you can specify an interpolation
method for any gaps that may appear after the frequency conversion.

.. ipython:: python

   ts.asfreq(pd.offsets.BDay(), method="pad")

Filling forward / backward
~~~~~~~~~~~~~~~~~~~~~~~~~~

Related to ``asfreq`` and ``reindex`` is :meth:`~Series.fillna`, which is
documented in the :ref:`missing data section <missing_data.fillna>`.

Converting to Python datetimes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``DatetimeIndex`` can be converted to an array of Python native
:py:class:`datetime.datetime` objects using the ``to_pydatetime`` method.

.. _timeseries.resampling:

Resampling
----------

pandas has a simple, powerful, and efficient functionality for performing
resampling operations during frequency conversion (e.g., converting secondly
data into 5-minutely data). This is extremely common in, but not limited to,
financial applications.

:meth:`~Series.resample` is a time-based groupby, followed by a reduction method
on each of its groups. See some :ref:`cookbook examples <cookbook.resample>` for
some advanced strategies.

The ``resample()`` method can be used directly from ``DataFrameGroupBy`` objects,
see the :ref:`groupby docs <groupby.transform.window_resample>`.

Basics
~~~~~~

.. ipython:: python

   rng = pd.date_range("1/1/2012", periods=100, freq="S")

   ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)

   ts.resample("5Min").sum()

The ``resample`` function is very flexible and allows you to specify many
different parameters to control the frequency conversion and resampling
operation.

Any function available via :ref:`dispatching <groupby.dispatch>` is available as
a method of the returned object, including ``sum``, ``mean``, ``std``, ``sem``,
``max``, ``min``, ``median``, ``first``, ``last``, ``ohlc``:

.. ipython:: python

   ts.resample("5Min").mean()

   ts.resample("5Min").ohlc()

   ts.resample("5Min").max()


For downsampling, ``closed`` can be set to 'left' or 'right' to specify which
end of the interval is closed:

.. ipython:: python

   ts.resample("5Min", closed="right").mean()

   ts.resample("5Min", closed="left").mean()

Parameters like ``label`` are used to manipulate the resulting labels.
``label`` specifies whether the result is labeled with the beginning or
the end of the interval.

.. ipython:: python

   ts.resample("5Min").mean()  # by default label='left'

   ts.resample("5Min", label="left").mean()

.. warning::

    The default values for ``label`` and ``closed`` is '**left**' for all
    frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W'
    which all have a default of 'right'.

    This might unintendedly lead to looking ahead, where the value for a later
    time is pulled back to a previous time as in the following example with
    the :class:`~pandas.tseries.offsets.BusinessDay` frequency:

    .. ipython:: python

        s = pd.date_range("2000-01-01", "2000-01-05").to_series()
        s.iloc[2] = pd.NaT
        s.dt.day_name()

        # default: label='left', closed='left'
        s.resample("B").last().dt.day_name()

    Notice how the value for Sunday got pulled back to the previous Friday.
    To get the behavior where the value for Sunday is pushed to Monday, use
    instead

    .. ipython:: python

        s.resample("B", label="right", closed="right").last().dt.day_name()

The ``axis`` parameter can be set to 0 or 1 and allows you to resample the
specified axis for a ``DataFrame``.

``kind`` can be set to 'timestamp' or 'period' to convert the resulting index
to/from timestamp and time span representations. By default ``resample``
retains the input representation.

``convention`` can be set to 'start' or 'end' when resampling period data
(detail below). It specifies how low frequency periods are converted to higher
frequency periods.


Upsampling
~~~~~~~~~~

For upsampling, you can specify a way to upsample and the ``limit`` parameter to interpolate over the gaps that are created:

.. ipython:: python

   # from secondly to every 250 milliseconds

   ts[:2].resample("250L").asfreq()

   ts[:2].resample("250L").ffill()

   ts[:2].resample("250L").ffill(limit=2)

Sparse resampling
~~~~~~~~~~~~~~~~~

Sparse timeseries are the ones where you have a lot fewer points relative
to the amount of time you are looking to resample. Naively upsampling a sparse
series can potentially generate lots of intermediate values. When you don't want
to use a method to fill these values, e.g. ``fill_method`` is ``None``, then
intermediate values will be filled with ``NaN``.

Since ``resample`` is a time-based groupby, the following is a method to efficiently
resample only the groups that are not all ``NaN``.

.. ipython:: python

    rng = pd.date_range("2014-1-1", periods=100, freq="D") + pd.Timedelta("1s")
    ts = pd.Series(range(100), index=rng)

If we want to resample to the full range of the series:

.. ipython:: python

    ts.resample("3T").sum()

We can instead only resample those groups where we have points as follows:

.. ipython:: python

    from functools import partial
    from pandas.tseries.frequencies import to_offset

    def round(t, freq):
        # round a Timestamp to a specified freq
        freq = to_offset(freq)
        return pd.Timestamp((t.value // freq.delta.value) * freq.delta.value)

    ts.groupby(partial(round, freq="3T")).sum()

.. _timeseries.aggregate:

Aggregation
~~~~~~~~~~~

Similar to the :ref:`aggregating API <basics.aggregate>`, :ref:`groupby API <groupby.aggregate>`, and the :ref:`window API <window.overview>`,
a ``Resampler`` can be selectively resampled.

Resampling a ``DataFrame``, the default will be to act on all columns with the same function.

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(1000, 3),
       index=pd.date_range("1/1/2012", freq="S", periods=1000),
       columns=["A", "B", "C"],
   )
   r = df.resample("3T")
   r.mean()

We can select a specific column or columns using standard getitem.

.. ipython:: python

   r["A"].mean()

   r[["A", "B"]].mean()

You can pass a list or dict of functions to do aggregation with, outputting a ``DataFrame``:

.. ipython:: python

   r["A"].agg([np.sum, np.mean, np.std])

On a resampled ``DataFrame``, you can pass a list of functions to apply to each
column, which produces an aggregated result with a hierarchical index:

.. ipython:: python

   r.agg([np.sum, np.mean])

By passing a dict to ``aggregate`` you can apply a different aggregation to the
columns of a ``DataFrame``:

.. ipython:: python
   :okexcept:

   r.agg({"A": np.sum, "B": lambda x: np.std(x, ddof=1)})

The function names can also be strings. In order for a string to be valid it
must be implemented on the resampled object:

.. ipython:: python

   r.agg({"A": "sum", "B": "std"})

Furthermore, you can also specify multiple aggregation functions for each column separately.

.. ipython:: python

   r.agg({"A": ["sum", "std"], "B": ["mean", "std"]})


If a ``DataFrame`` does not have a datetimelike index, but instead you want
to resample based on datetimelike column in the frame, it can passed to the
``on`` keyword.

.. ipython:: python

   df = pd.DataFrame(
       {"date": pd.date_range("2015-01-01", freq="W", periods=5), "a": np.arange(5)},
       index=pd.MultiIndex.from_arrays(
           [[1, 2, 3, 4, 5], pd.date_range("2015-01-01", freq="W", periods=5)],
           names=["v", "d"],
       ),
   )
   df
   df.resample("M", on="date").sum()

Similarly, if you instead want to resample by a datetimelike
level of ``MultiIndex``, its name or location can be passed to the
``level`` keyword.

.. ipython:: python

   df.resample("M", level="d").sum()

.. _timeseries.iterating-label:

Iterating through groups
~~~~~~~~~~~~~~~~~~~~~~~~

With the ``Resampler`` object in hand, iterating through the grouped data is very
natural and functions similarly to :py:func:`itertools.groupby`:

.. ipython:: python

   small = pd.Series(
       range(6),
       index=pd.to_datetime(
           [
               "2017-01-01T00:00:00",
               "2017-01-01T00:30:00",
               "2017-01-01T00:31:00",
               "2017-01-01T01:00:00",
               "2017-01-01T03:00:00",
               "2017-01-01T03:05:00",
           ]
       ),
   )
   resampled = small.resample("H")

   for name, group in resampled:
       print("Group: ", name)
       print("-" * 27)
       print(group, end="\n\n")

See :ref:`groupby.iterating-label` or :class:`Resampler.__iter__` for more.

.. _timeseries.adjust-the-start-of-the-bins:

Use ``origin`` or ``offset`` to adjust the start of the bins
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.1.0

The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like ``30D``) or that divide a day evenly (like ``90s`` or ``1min``). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument ``origin``.

For example:

.. ipython:: python

    start, end = "2000-10-01 23:30:00", "2000-10-02 00:30:00"
    middle = "2000-10-02 00:00:00"
    rng = pd.date_range(start, end, freq="7min")
    ts = pd.Series(np.arange(len(rng)) * 3, index=rng)
    ts

Here we can see that, when using ``origin`` with its default value (``'start_day'``), the result after ``'2000-10-02 00:00:00'`` are not identical depending on the start of time series:

.. ipython:: python

    ts.resample("17min", origin="start_day").sum()
    ts[middle:end].resample("17min", origin="start_day").sum()


Here we can see that, when setting ``origin`` to ``'epoch'``, the result after ``'2000-10-02 00:00:00'`` are identical depending on the start of time series:

.. ipython:: python

   ts.resample("17min", origin="epoch").sum()
   ts[middle:end].resample("17min", origin="epoch").sum()


If needed you can use a custom timestamp for ``origin``:

.. ipython:: python

   ts.resample("17min", origin="2001-01-01").sum()
   ts[middle:end].resample("17min", origin=pd.Timestamp("2001-01-01")).sum()

If needed you can just adjust the bins with an ``offset`` Timedelta that would be added to the default ``origin``.
Those two examples are equivalent for this time series:

.. ipython:: python

    ts.resample("17min", origin="start").sum()
    ts.resample("17min", offset="23h30min").sum()


Note the use of ``'start'`` for ``origin`` on the last example. In that case, ``origin`` will be set to the first value of the timeseries.

Backward resample
~~~~~~~~~~~~~~~~~

.. versionadded:: 1.3.0

Instead of adjusting the beginning of bins, sometimes we need to fix the end of the bins to make a backward resample with a given ``freq``. The backward resample sets ``closed`` to ``'right'`` by default since the last value should be considered as the edge point for the last bin.

We can set ``origin`` to ``'end'``. The value for a specific ``Timestamp`` index stands for the resample result from the current ``Timestamp`` minus ``freq`` to the current ``Timestamp`` with a right close.

.. ipython:: python

   ts.resample('17min', origin='end').sum()

Besides, in contrast with the ``'start_day'`` option, ``end_day`` is supported. This will set the origin as the ceiling midnight of the largest ``Timestamp``.

.. ipython:: python

   ts.resample('17min', origin='end_day').sum()

The above result uses ``2000-10-02 00:29:00`` as the last bin's right edge since the following computation.

.. ipython:: python

   ceil_mid = rng.max().ceil('D')
   freq = pd.offsets.Minute(17)
   bin_res = ceil_mid - freq * ((ceil_mid - rng.max()) // freq)
   bin_res

.. _timeseries.periods:

Time span representation
------------------------

Regular intervals of time are represented by ``Period`` objects in pandas while
sequences of ``Period`` objects are collected in a ``PeriodIndex``, which can
be created with the convenience function ``period_range``.

Period
~~~~~~

A ``Period`` represents a span of time (e.g., a day, a month, a quarter, etc).
You can specify the span via ``freq`` keyword using a frequency alias like below.
Because ``freq`` represents a span of ``Period``, it cannot be negative like "-3D".

.. ipython:: python

   pd.Period("2012", freq="A-DEC")

   pd.Period("2012-1-1", freq="D")

   pd.Period("2012-1-1 19:00", freq="H")

   pd.Period("2012-1-1 19:00", freq="5H")

Adding and subtracting integers from periods shifts the period by its own
frequency. Arithmetic is not allowed between ``Period`` with different ``freq`` (span).

.. ipython:: python

   p = pd.Period("2012", freq="A-DEC")
   p + 1
   p - 3
   p = pd.Period("2012-01", freq="2M")
   p + 2
   p - 1
   @okexcept
   p == pd.Period("2012-01", freq="3M")


If ``Period`` freq is daily or higher (``D``, ``H``, ``T``, ``S``, ``L``, ``U``, ``N``), ``offsets`` and ``timedelta``-like can be added if the result can have the same freq. Otherwise, ``ValueError`` will be raised.

.. ipython:: python

   p = pd.Period("2014-07-01 09:00", freq="H")
   p + pd.offsets.Hour(2)
   p + datetime.timedelta(minutes=120)
   p + np.timedelta64(7200, "s")

.. code-block:: ipython

   In [1]: p + pd.offsets.Minute(5)
   Traceback
      ...
   ValueError: Input has different freq from Period(freq=H)

If ``Period`` has other frequencies, only the same ``offsets`` can be added. Otherwise, ``ValueError`` will be raised.

.. ipython:: python

   p = pd.Period("2014-07", freq="M")
   p + pd.offsets.MonthEnd(3)

.. code-block:: ipython

   In [1]: p + pd.offsets.MonthBegin(3)
   Traceback
      ...
   ValueError: Input has different freq from Period(freq=M)

Taking the difference of ``Period`` instances with the same frequency will
return the number of frequency units between them:

.. ipython:: python

   pd.Period("2012", freq="A-DEC") - pd.Period("2002", freq="A-DEC")

PeriodIndex and period_range
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Regular sequences of ``Period`` objects can be collected in a ``PeriodIndex``,
which can be constructed using the ``period_range`` convenience function:

.. ipython:: python

   prng = pd.period_range("1/1/2011", "1/1/2012", freq="M")
   prng

The ``PeriodIndex`` constructor can also be used directly:

.. ipython:: python

   pd.PeriodIndex(["2011-1", "2011-2", "2011-3"], freq="M")

Passing multiplied frequency outputs a sequence of ``Period`` which
has multiplied span.

.. ipython:: python

   pd.period_range(start="2014-01", freq="3M", periods=4)

If ``start`` or ``end`` are ``Period`` objects, they will be used as anchor
endpoints for a ``PeriodIndex`` with frequency matching that of the
``PeriodIndex`` constructor.

.. ipython:: python

   pd.period_range(
       start=pd.Period("2017Q1", freq="Q"), end=pd.Period("2017Q2", freq="Q"), freq="M"
   )

Just like ``DatetimeIndex``, a ``PeriodIndex`` can also be used to index pandas
objects:

.. ipython:: python

   ps = pd.Series(np.random.randn(len(prng)), prng)
   ps

``PeriodIndex`` supports addition and subtraction with the same rule as ``Period``.

.. ipython:: python

   idx = pd.period_range("2014-07-01 09:00", periods=5, freq="H")
   idx
   idx + pd.offsets.Hour(2)

   idx = pd.period_range("2014-07", periods=5, freq="M")
   idx
   idx + pd.offsets.MonthEnd(3)

``PeriodIndex`` has its own dtype named ``period``, refer to :ref:`Period Dtypes <timeseries.period_dtype>`.

.. _timeseries.period_dtype:

Period dtypes
~~~~~~~~~~~~~

``PeriodIndex`` has a custom ``period`` dtype. This is a pandas extension
dtype similar to the :ref:`timezone aware dtype <timeseries.timezone_series>` (``datetime64[ns, tz]``).

The ``period`` dtype holds the ``freq`` attribute and is represented with
``period[freq]`` like ``period[D]`` or ``period[M]``, using :ref:`frequency strings <timeseries.offset_aliases>`.

.. ipython:: python

   pi = pd.period_range("2016-01-01", periods=3, freq="M")
   pi
   pi.dtype

The ``period`` dtype can be used in ``.astype(...)``. It allows one to change the
``freq`` of a ``PeriodIndex`` like ``.asfreq()`` and convert a
``DatetimeIndex`` to ``PeriodIndex`` like ``to_period()``:

.. ipython:: python

   # change monthly freq to daily freq
   pi.astype("period[D]")

   # convert to DatetimeIndex
   pi.astype("datetime64[ns]")

   # convert to PeriodIndex
   dti = pd.date_range("2011-01-01", freq="M", periods=3)
   dti
   dti.astype("period[M]")

PeriodIndex partial string indexing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

PeriodIndex now supports partial string slicing with non-monotonic indexes.

.. versionadded:: 1.1.0

You can pass in dates and strings to ``Series`` and ``DataFrame`` with ``PeriodIndex``, in the same manner as ``DatetimeIndex``. For details, refer to :ref:`DatetimeIndex Partial String Indexing <timeseries.partialindexing>`.

.. ipython:: python

   ps["2011-01"]

   ps[datetime.datetime(2011, 12, 25):]

   ps["10/31/2011":"12/31/2011"]

Passing a string representing a lower frequency than ``PeriodIndex`` returns partial sliced data.

.. ipython:: python

   ps["2011"]

   dfp = pd.DataFrame(
       np.random.randn(600, 1),
       columns=["A"],
       index=pd.period_range("2013-01-01 9:00", periods=600, freq="T"),
   )
   dfp
   dfp.loc["2013-01-01 10H"]

As with ``DatetimeIndex``, the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59.

.. ipython:: python

   dfp["2013-01-01 10H":"2013-01-01 11H"]


Frequency conversion and resampling with PeriodIndex
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The frequency of ``Period`` and ``PeriodIndex`` can be converted via the ``asfreq``
method. Let's start with the fiscal year 2011, ending in December:

.. ipython:: python

   p = pd.Period("2011", freq="A-DEC")
   p

We can convert it to a monthly frequency. Using the ``how`` parameter, we can
specify whether to return the starting or ending month:

.. ipython:: python

   p.asfreq("M", how="start")

   p.asfreq("M", how="end")

The shorthands 's' and 'e' are provided for convenience:

.. ipython:: python

   p.asfreq("M", "s")
   p.asfreq("M", "e")

Converting to a "super-period" (e.g., annual frequency is a super-period of
quarterly frequency) automatically returns the super-period that includes the
input period:

.. ipython:: python

   p = pd.Period("2011-12", freq="M")

   p.asfreq("A-NOV")

Note that since we converted to an annual frequency that ends the year in
November, the monthly period of December 2011 is actually in the 2012 A-NOV
period.

.. _timeseries.quarterly:

Period conversions with anchored frequencies are particularly useful for
working with various quarterly data common to economics, business, and other
fields. Many organizations define quarters relative to the month in which their
fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or
a few months into 2011. Via anchored frequencies, pandas works for all quarterly
frequencies ``Q-JAN`` through ``Q-DEC``.

``Q-DEC`` define regular calendar quarters:

.. ipython:: python

   p = pd.Period("2012Q1", freq="Q-DEC")

   p.asfreq("D", "s")

   p.asfreq("D", "e")

``Q-MAR`` defines fiscal year end in March:

.. ipython:: python

   p = pd.Period("2011Q4", freq="Q-MAR")

   p.asfreq("D", "s")

   p.asfreq("D", "e")

.. _timeseries.interchange:

Converting between representations
----------------------------------

Timestamped data can be converted to PeriodIndex-ed data using ``to_period``
and vice-versa using ``to_timestamp``:

.. ipython:: python

   rng = pd.date_range("1/1/2012", periods=5, freq="M")

   ts = pd.Series(np.random.randn(len(rng)), index=rng)

   ts

   ps = ts.to_period()

   ps

   ps.to_timestamp()

Remember that 's' and 'e' can be used to return the timestamps at the start or
end of the period:

.. ipython:: python

   ps.to_timestamp("D", how="s")

Converting between period and timestamp enables some convenient arithmetic
functions to be used. In the following example, we convert a quarterly
frequency with year ending in November to 9am of the end of the month following
the quarter end:

.. ipython:: python

   prng = pd.period_range("1990Q1", "2000Q4", freq="Q-NOV")

   ts = pd.Series(np.random.randn(len(prng)), prng)

   ts.index = (prng.asfreq("M", "e") + 1).asfreq("H", "s") + 9

   ts.head()

.. _timeseries.oob:

Representing out-of-bounds spans
--------------------------------

If you have data that is outside of the ``Timestamp`` bounds, see :ref:`Timestamp limitations <timeseries.timestamp-limits>`,
then you can use a ``PeriodIndex`` and/or ``Series`` of ``Periods`` to do computations.

.. ipython:: python

   span = pd.period_range("1215-01-01", "1381-01-01", freq="D")
   span

To convert from an ``int64`` based YYYYMMDD representation.

.. ipython:: python

   s = pd.Series([20121231, 20141130, 99991231])
   s

   def conv(x):
       return pd.Period(year=x // 10000, month=x // 100 % 100, day=x % 100, freq="D")

   s.apply(conv)
   s.apply(conv)[2]

These can easily be converted to a ``PeriodIndex``:

.. ipython:: python

   span = pd.PeriodIndex(s.apply(conv))
   span

.. _timeseries.timezone:

Time zone handling
------------------

pandas provides rich support for working with timestamps in different time
zones using the ``pytz`` and ``dateutil`` libraries or :class:`datetime.timezone`
objects from the standard library.


Working with time zones
~~~~~~~~~~~~~~~~~~~~~~~

By default, pandas objects are time zone unaware:

.. ipython:: python

   rng = pd.date_range("3/6/2012 00:00", periods=15, freq="D")
   rng.tz is None

To localize these dates to a time zone (assign a particular time zone to a naive date),
you can use the ``tz_localize`` method or the ``tz`` keyword argument in
:func:`date_range`, :class:`Timestamp`, or :class:`DatetimeIndex`.
You can either pass ``pytz`` or ``dateutil`` time zone objects or Olson time zone database strings.
Olson time zone strings will return ``pytz`` time zone objects by default.
To return ``dateutil`` time zone objects, append ``dateutil/`` before the string.

* In ``pytz`` you can find a list of common (and less common) time zones using
  ``from pytz import common_timezones, all_timezones``.
* ``dateutil`` uses the OS time zones so there isn't a fixed list available. For
  common zones, the names are the same as ``pytz``.

.. ipython:: python

   import dateutil

   # pytz
   rng_pytz = pd.date_range("3/6/2012 00:00", periods=3, freq="D", tz="Europe/London")
   rng_pytz.tz

   # dateutil
   rng_dateutil = pd.date_range("3/6/2012 00:00", periods=3, freq="D")
   rng_dateutil = rng_dateutil.tz_localize("dateutil/Europe/London")
   rng_dateutil.tz

   # dateutil - utc special case
   rng_utc = pd.date_range(
       "3/6/2012 00:00",
       periods=3,
       freq="D",
       tz=dateutil.tz.tzutc(),
   )
   rng_utc.tz

.. versionadded:: 0.25.0

.. ipython:: python

   # datetime.timezone
   rng_utc = pd.date_range(
       "3/6/2012 00:00",
       periods=3,
       freq="D",
       tz=datetime.timezone.utc,
   )
   rng_utc.tz

Note that the ``UTC`` time zone is a special case in ``dateutil`` and should be constructed explicitly
as an instance of ``dateutil.tz.tzutc``. You can also construct other time
zones objects explicitly first.

.. ipython:: python

   import pytz

   # pytz
   tz_pytz = pytz.timezone("Europe/London")
   rng_pytz = pd.date_range("3/6/2012 00:00", periods=3, freq="D")
   rng_pytz = rng_pytz.tz_localize(tz_pytz)
   rng_pytz.tz == tz_pytz

   # dateutil
   tz_dateutil = dateutil.tz.gettz("Europe/London")
   rng_dateutil = pd.date_range("3/6/2012 00:00", periods=3, freq="D", tz=tz_dateutil)
   rng_dateutil.tz == tz_dateutil

To convert a time zone aware pandas object from one time zone to another,
you can use the ``tz_convert`` method.

.. ipython:: python

   rng_pytz.tz_convert("US/Eastern")

.. note::

    When using ``pytz`` time zones, :class:`DatetimeIndex` will construct a different
    time zone object than a :class:`Timestamp` for the same time zone input. A :class:`DatetimeIndex`
    can hold a collection of :class:`Timestamp` objects that may have different UTC offsets and cannot be
    succinctly represented by one ``pytz`` time zone instance while one :class:`Timestamp`
    represents one point in time with a specific UTC offset.

    .. ipython:: python

       dti = pd.date_range("2019-01-01", periods=3, freq="D", tz="US/Pacific")
       dti.tz
       ts = pd.Timestamp("2019-01-01", tz="US/Pacific")
       ts.tz

.. warning::

	Be wary of conversions between libraries. For some time zones, ``pytz`` and ``dateutil`` have different
	definitions of the zone. This is more of a problem for unusual time zones than for
	'standard' zones like ``US/Eastern``.

.. warning::

    Be aware that a time zone definition across versions of time zone libraries may not
    be considered equal.  This may cause problems when working with stored data that
    is localized using one version and operated on with a different version.
    See :ref:`here<io.hdf5-notes>` for how to handle such a situation.

.. warning::

    For ``pytz`` time zones, it is incorrect to pass a time zone object directly into
    the ``datetime.datetime`` constructor
    (e.g., ``datetime.datetime(2011, 1, 1, tzinfo=pytz.timezone('US/Eastern'))``.
    Instead, the datetime needs to be localized using the ``localize`` method
    on the ``pytz`` time zone object.

.. warning::

    Be aware that for times in the future, correct conversion between time zones
    (and UTC) cannot be guaranteed by any time zone library because a timezone's
    offset from UTC may be changed by the respective government.

.. warning::

    If you are using dates beyond 2038-01-18, due to current deficiencies
    in the underlying libraries caused by the year 2038 problem, daylight saving time (DST) adjustments
    to timezone aware dates will not be applied. If and when the underlying libraries are fixed,
    the DST transitions will be applied.

    For example, for two dates that are in British Summer Time (and so would normally be GMT+1), both the following asserts evaluate as true:

    .. ipython:: python

       d_2037 = "2037-03-31T010101"
       d_2038 = "2038-03-31T010101"
       DST = "Europe/London"
       assert pd.Timestamp(d_2037, tz=DST) != pd.Timestamp(d_2037, tz="GMT")
       assert pd.Timestamp(d_2038, tz=DST) == pd.Timestamp(d_2038, tz="GMT")

Under the hood, all timestamps are stored in UTC. Values from a time zone aware
:class:`DatetimeIndex` or :class:`Timestamp` will have their fields (day, hour, minute, etc.)
localized to the time zone. However, timestamps with the same UTC value are
still considered to be equal even if they are in different time zones:

.. ipython:: python

   rng_eastern = rng_utc.tz_convert("US/Eastern")
   rng_berlin = rng_utc.tz_convert("Europe/Berlin")

   rng_eastern[2]
   rng_berlin[2]
   rng_eastern[2] == rng_berlin[2]

Operations between :class:`Series` in different time zones will yield UTC
:class:`Series`, aligning the data on the UTC timestamps:

.. ipython:: python

   ts_utc = pd.Series(range(3), pd.date_range("20130101", periods=3, tz="UTC"))
   eastern = ts_utc.tz_convert("US/Eastern")
   berlin = ts_utc.tz_convert("Europe/Berlin")
   result = eastern + berlin
   result
   result.index

To remove time zone information, use ``tz_localize(None)`` or ``tz_convert(None)``.
``tz_localize(None)`` will remove the time zone yielding the local time representation.
``tz_convert(None)`` will remove the time zone after converting to UTC time.

.. ipython:: python

   didx = pd.date_range(start="2014-08-01 09:00", freq="H", periods=3, tz="US/Eastern")
   didx
   didx.tz_localize(None)
   didx.tz_convert(None)

   # tz_convert(None) is identical to tz_convert('UTC').tz_localize(None)
   didx.tz_convert("UTC").tz_localize(None)

.. _timeseries.fold:

Fold
~~~~

.. versionadded:: 1.1.0

For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument.
Due to daylight saving time, one wall clock time can occur twice when shifting
from summer to winter time; fold describes whether the datetime-like corresponds
to the first (0) or the second time (1) the wall clock hits the ambiguous time.
Fold is supported only for constructing from naive ``datetime.datetime``
(see `datetime documentation <https://docs.python.org/3/library/datetime.html>`__ for details) or from :class:`Timestamp`
or for constructing from components (see below). Only ``dateutil`` timezones are supported
(see `dateutil documentation <https://dateutil.readthedocs.io/en/stable/tz.html#dateutil.tz.enfold>`__
for ``dateutil`` methods that deal with ambiguous datetimes) as ``pytz``
timezones do not support fold (see `pytz documentation <http://pytz.sourceforge.net/index.html>`__
for details on how ``pytz`` deals with ambiguous datetimes). To localize an ambiguous datetime
with ``pytz``, please use :meth:`Timestamp.tz_localize`. In general, we recommend to rely
on :meth:`Timestamp.tz_localize` when localizing ambiguous datetimes if you need direct
control over how they are handled.

.. ipython:: python

   pd.Timestamp(
       datetime.datetime(2019, 10, 27, 1, 30, 0, 0),
       tz="dateutil/Europe/London",
       fold=0,
   )
   pd.Timestamp(
       year=2019,
       month=10,
       day=27,
       hour=1,
       minute=30,
       tz="dateutil/Europe/London",
       fold=1,
   )

.. _timeseries.timezone_ambiguous:

Ambiguous times when localizing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``tz_localize`` may not be able to determine the UTC offset of a timestamp
because daylight savings time (DST) in a local time zone causes some times to occur
twice within one day ("clocks fall back"). The following options are available:

* ``'raise'``: Raises a ``pytz.AmbiguousTimeError`` (the default behavior)
* ``'infer'``: Attempt to determine the correct offset base on the monotonicity of the timestamps
* ``'NaT'``: Replaces ambiguous times with ``NaT``
* ``bool``: ``True`` represents a DST time, ``False`` represents non-DST time. An array-like of ``bool`` values is supported for a sequence of times.

.. ipython:: python

   rng_hourly = pd.DatetimeIndex(
       ["11/06/2011 00:00", "11/06/2011 01:00", "11/06/2011 01:00", "11/06/2011 02:00"]
   )

This will fail as there are ambiguous times (``'11/06/2011 01:00'``)

.. code-block:: ipython

   In [2]: rng_hourly.tz_localize('US/Eastern')
   AmbiguousTimeError: Cannot infer dst time from Timestamp('2011-11-06 01:00:00'), try using the 'ambiguous' argument

Handle these ambiguous times by specifying the following.

.. ipython:: python

   rng_hourly.tz_localize("US/Eastern", ambiguous="infer")
   rng_hourly.tz_localize("US/Eastern", ambiguous="NaT")
   rng_hourly.tz_localize("US/Eastern", ambiguous=[True, True, False, False])

.. _timeseries.timezone_nonexistent:

Nonexistent times when localizing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A DST transition may also shift the local time ahead by 1 hour creating nonexistent
local times ("clocks spring forward"). The behavior of localizing a timeseries with nonexistent times
can be controlled by the ``nonexistent`` argument. The following options are available:

* ``'raise'``: Raises a ``pytz.NonExistentTimeError`` (the default behavior)
* ``'NaT'``: Replaces nonexistent times with ``NaT``
* ``'shift_forward'``: Shifts nonexistent times forward to the closest real time
* ``'shift_backward'``: Shifts nonexistent times backward to the closest real time
* timedelta object: Shifts nonexistent times by the timedelta duration

.. ipython:: python

    dti = pd.date_range(start="2015-03-29 02:30:00", periods=3, freq="H")
    # 2:30 is a nonexistent time

Localization of nonexistent times will raise an error by default.

.. code-block:: ipython

   In [2]: dti.tz_localize('Europe/Warsaw')
   NonExistentTimeError: 2015-03-29 02:30:00

Transform nonexistent times to ``NaT`` or shift the times.

.. ipython:: python

    dti
    dti.tz_localize("Europe/Warsaw", nonexistent="shift_forward")
    dti.tz_localize("Europe/Warsaw", nonexistent="shift_backward")
    dti.tz_localize("Europe/Warsaw", nonexistent=pd.Timedelta(1, unit="H"))
    dti.tz_localize("Europe/Warsaw", nonexistent="NaT")


.. _timeseries.timezone_series:

Time zone series operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~

A :class:`Series` with time zone **naive** values is
represented with a dtype of ``datetime64[ns]``.

.. ipython:: python

   s_naive = pd.Series(pd.date_range("20130101", periods=3))
   s_naive

A :class:`Series` with a time zone **aware** values is
represented with a dtype of ``datetime64[ns, tz]`` where ``tz`` is the time zone

.. ipython:: python

   s_aware = pd.Series(pd.date_range("20130101", periods=3, tz="US/Eastern"))
   s_aware

Both of these :class:`Series` time zone information
can be manipulated via the ``.dt`` accessor, see :ref:`the dt accessor section <basics.dt_accessors>`.

For example, to localize and convert a naive stamp to time zone aware.

.. ipython:: python

   s_naive.dt.tz_localize("UTC").dt.tz_convert("US/Eastern")

Time zone information can also be manipulated using the ``astype`` method.
This method can convert between different timezone-aware dtypes.

.. ipython:: python

   # convert to a new time zone
   s_aware.astype("datetime64[ns, CET]")

.. note::

   Using :meth:`Series.to_numpy` on a ``Series``, returns a NumPy array of the data.
   NumPy does not currently support time zones (even though it is *printing* in the local time zone!),
   therefore an object array of Timestamps is returned for time zone aware data:

   .. ipython:: python

      s_naive.to_numpy()
      s_aware.to_numpy()

   By converting to an object array of Timestamps, it preserves the time zone
   information. For example, when converting back to a Series:

   .. ipython:: python

      pd.Series(s_aware.to_numpy())

   However, if you want an actual NumPy ``datetime64[ns]`` array (with the values
   converted to UTC) instead of an array of objects, you can specify the
   ``dtype`` argument:

   .. ipython:: python

      s_aware.to_numpy(dtype="datetime64[ns]")
.. _advanced:

{{ header }}

******************************
MultiIndex / advanced indexing
******************************

This section covers :ref:`indexing with a MultiIndex <advanced.hierarchical>`
and :ref:`other advanced indexing features <advanced.index_types>`.

See the :ref:`Indexing and Selecting Data <indexing>` for general indexing documentation.

.. warning::

   Whether a copy or a reference is returned for a setting operation may
   depend on the context.  This is sometimes called ``chained assignment`` and
   should be avoided.  See :ref:`Returning a View versus Copy
   <indexing.view_versus_copy>`.

See the :ref:`cookbook<cookbook.selection>` for some advanced strategies.

.. _advanced.hierarchical:

Hierarchical indexing (MultiIndex)
----------------------------------

Hierarchical / Multi-level indexing is very exciting as it opens the door to some
quite sophisticated data analysis and manipulation, especially for working with
higher dimensional data. In essence, it enables you to store and manipulate
data with an arbitrary number of dimensions in lower dimensional data
structures like ``Series`` (1d) and ``DataFrame`` (2d).

In this section, we will show what exactly we mean by "hierarchical" indexing
and how it integrates with all of the pandas indexing functionality
described above and in prior sections. Later, when discussing :ref:`group by
<groupby>` and :ref:`pivoting and reshaping data <reshaping>`, we'll show
non-trivial applications to illustrate how it aids in structuring data for
analysis.

See the :ref:`cookbook<cookbook.multi_index>` for some advanced strategies.

Creating a MultiIndex (hierarchical index) object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :class:`MultiIndex` object is the hierarchical analogue of the standard
:class:`Index` object which typically stores the axis labels in pandas objects. You
can think of ``MultiIndex`` as an array of tuples where each tuple is unique. A
``MultiIndex`` can be created from a list of arrays (using
:meth:`MultiIndex.from_arrays`), an array of tuples (using
:meth:`MultiIndex.from_tuples`), a crossed set of iterables (using
:meth:`MultiIndex.from_product`), or a :class:`DataFrame` (using
:meth:`MultiIndex.from_frame`).  The ``Index`` constructor will attempt to return
a ``MultiIndex`` when it is passed a list of tuples.  The following examples
demonstrate different ways to initialize MultiIndexes.


.. ipython:: python

   arrays = [
       ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
       ["one", "two", "one", "two", "one", "two", "one", "two"],
   ]
   tuples = list(zip(*arrays))
   tuples

   index = pd.MultiIndex.from_tuples(tuples, names=["first", "second"])
   index

   s = pd.Series(np.random.randn(8), index=index)
   s

When you want every pairing of the elements in two iterables, it can be easier
to use the :meth:`MultiIndex.from_product` method:

.. ipython:: python

   iterables = [["bar", "baz", "foo", "qux"], ["one", "two"]]
   pd.MultiIndex.from_product(iterables, names=["first", "second"])

You can also construct a ``MultiIndex`` from a ``DataFrame`` directly, using
the method :meth:`MultiIndex.from_frame`. This is a complementary method to
:meth:`MultiIndex.to_frame`.

.. ipython:: python

   df = pd.DataFrame(
       [["bar", "one"], ["bar", "two"], ["foo", "one"], ["foo", "two"]],
       columns=["first", "second"],
   )
   pd.MultiIndex.from_frame(df)

As a convenience, you can pass a list of arrays directly into ``Series`` or
``DataFrame`` to construct a ``MultiIndex`` automatically:

.. ipython:: python

   arrays = [
       np.array(["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"]),
       np.array(["one", "two", "one", "two", "one", "two", "one", "two"]),
   ]
   s = pd.Series(np.random.randn(8), index=arrays)
   s
   df = pd.DataFrame(np.random.randn(8, 4), index=arrays)
   df

All of the ``MultiIndex`` constructors accept a ``names`` argument which stores
string names for the levels themselves. If no names are provided, ``None`` will
be assigned:

.. ipython:: python

   df.index.names

This index can back any axis of a pandas object, and the number of **levels**
of the index is up to you:

.. ipython:: python

   df = pd.DataFrame(np.random.randn(3, 8), index=["A", "B", "C"], columns=index)
   df
   pd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6])

We've "sparsified" the higher levels of the indexes to make the console output a
bit easier on the eyes. Note that how the index is displayed can be controlled using the
``multi_sparse`` option in ``pandas.set_options()``:

.. ipython:: python

   with pd.option_context("display.multi_sparse", False):
       df

It's worth keeping in mind that there's nothing preventing you from using
tuples as atomic labels on an axis:

.. ipython:: python

   pd.Series(np.random.randn(8), index=tuples)

The reason that the ``MultiIndex`` matters is that it can allow you to do
grouping, selection, and reshaping operations as we will describe below and in
subsequent areas of the documentation. As you will see in later sections, you
can find yourself working with hierarchically-indexed data without creating a
``MultiIndex`` explicitly yourself. However, when loading data from a file, you
may wish to generate your own ``MultiIndex`` when preparing the data set.

.. _advanced.get_level_values:

Reconstructing the level labels
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The method :meth:`~MultiIndex.get_level_values` will return a vector of the labels for each
location at a particular level:

.. ipython:: python

   index.get_level_values(0)
   index.get_level_values("second")

Basic indexing on axis with MultiIndex
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

One of the important features of hierarchical indexing is that you can select
data by a "partial" label identifying a subgroup in the data. **Partial**
selection "drops" levels of the hierarchical index in the result in a
completely analogous way to selecting a column in a regular DataFrame:

.. ipython:: python

   df["bar"]
   df["bar", "one"]
   df["bar"]["one"]
   s["qux"]

See :ref:`Cross-section with hierarchical index <advanced.xs>` for how to select
on a deeper level.

.. _advanced.shown_levels:

Defined levels
~~~~~~~~~~~~~~

The :class:`MultiIndex` keeps all the defined levels of an index, even
if they are not actually used. When slicing an index, you may notice this.
For example:

.. ipython:: python

   df.columns.levels  # original MultiIndex

   df[["foo","qux"]].columns.levels  # sliced

This is done to avoid a recomputation of the levels in order to make slicing
highly performant. If you want to see only the used levels, you can use the
:meth:`~MultiIndex.get_level_values` method.

.. ipython:: python

   df[["foo", "qux"]].columns.to_numpy()

   # for a specific level
   df[["foo", "qux"]].columns.get_level_values(0)

To reconstruct the ``MultiIndex`` with only the used levels, the
:meth:`~MultiIndex.remove_unused_levels` method may be used.

.. ipython:: python

   new_mi = df[["foo", "qux"]].columns.remove_unused_levels()
   new_mi.levels

Data alignment and using ``reindex``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Operations between differently-indexed objects having ``MultiIndex`` on the
axes will work as you expect; data alignment will work the same as an Index of
tuples:

.. ipython:: python

   s + s[:-2]
   s + s[::2]

The :meth:`~DataFrame.reindex` method of ``Series``/``DataFrames`` can be
called with another ``MultiIndex``, or even a list or array of tuples:

.. ipython:: python

   s.reindex(index[:3])
   s.reindex([("foo", "two"), ("bar", "one"), ("qux", "one"), ("baz", "one")])

.. _advanced.advanced_hierarchical:

Advanced indexing with hierarchical index
-----------------------------------------

Syntactically integrating ``MultiIndex`` in advanced indexing with ``.loc`` is a
bit challenging, but we've made every effort to do so. In general, MultiIndex
keys take the form of tuples. For example, the following works as you would expect:

.. ipython:: python

   df = df.T
   df
   df.loc[("bar", "two")]

Note that ``df.loc['bar', 'two']`` would also work in this example, but this shorthand
notation can lead to ambiguity in general.

If you also want to index a specific column with ``.loc``, you must use a tuple
like this:

.. ipython:: python

   df.loc[("bar", "two"), "A"]

You don't have to specify all levels of the ``MultiIndex`` by passing only the
first elements of the tuple. For example, you can use "partial" indexing to
get all elements with ``bar`` in the first level as follows:

.. ipython:: python

   df.loc["bar"]

This is a shortcut for the slightly more verbose notation ``df.loc[('bar',),]`` (equivalent
to ``df.loc['bar',]`` in this example).

"Partial" slicing also works quite nicely.

.. ipython:: python

   df.loc["baz":"foo"]

You can slice with a 'range' of values, by providing a slice of tuples.

.. ipython:: python

   df.loc[("baz", "two"):("qux", "one")]
   df.loc[("baz", "two"):"foo"]

Passing a list of labels or tuples works similar to reindexing:

.. ipython:: python

   df.loc[[("bar", "two"), ("qux", "one")]]

.. note::

   It is important to note that tuples and lists are not treated identically
   in pandas when it comes to indexing. Whereas a tuple is interpreted as one
   multi-level key, a list is used to specify several keys. Or in other words,
   tuples go horizontally (traversing levels), lists go vertically (scanning levels).

Importantly, a list of tuples indexes several complete ``MultiIndex`` keys,
whereas a tuple of lists refer to several values within a level:

.. ipython:: python

   s = pd.Series(
       [1, 2, 3, 4, 5, 6],
       index=pd.MultiIndex.from_product([["A", "B"], ["c", "d", "e"]]),
   )
   s.loc[[("A", "c"), ("B", "d")]]  # list of tuples
   s.loc[(["A", "B"], ["c", "d"])]  # tuple of lists


.. _advanced.mi_slicers:

Using slicers
~~~~~~~~~~~~~

You can slice a ``MultiIndex`` by providing multiple indexers.

You can provide any of the selectors as if you are indexing by label, see :ref:`Selection by Label <indexing.label>`,
including slices, lists of labels, labels, and boolean indexers.

You can use ``slice(None)`` to select all the contents of *that* level. You do not need to specify all the
*deeper* levels, they will be implied as ``slice(None)``.

As usual, **both sides** of the slicers are included as this is label indexing.

.. warning::

   You should specify all axes in the ``.loc`` specifier, meaning the indexer for the **index** and
   for the **columns**. There are some ambiguous cases where the passed indexer could be mis-interpreted
   as indexing *both* axes, rather than into say the ``MultiIndex`` for the rows.

   You should do this:

   .. code-block:: python

      df.loc[(slice("A1", "A3"), ...), :]  # noqa: E999

   You should **not** do this:
 
   .. code-block:: python

      df.loc[(slice("A1", "A3"), ...)]  # noqa: E999

.. ipython:: python

   def mklbl(prefix, n):
       return ["%s%s" % (prefix, i) for i in range(n)]


   miindex = pd.MultiIndex.from_product(
       [mklbl("A", 4), mklbl("B", 2), mklbl("C", 4), mklbl("D", 2)]
   )
   micolumns = pd.MultiIndex.from_tuples(
       [("a", "foo"), ("a", "bar"), ("b", "foo"), ("b", "bah")], names=["lvl0", "lvl1"]
   )
   dfmi = (
       pd.DataFrame(
           np.arange(len(miindex) * len(micolumns)).reshape(
               (len(miindex), len(micolumns))
           ),
           index=miindex,
           columns=micolumns,
       )
       .sort_index()
       .sort_index(axis=1)
   )
   dfmi

Basic MultiIndex slicing using slices, lists, and labels.

.. ipython:: python

   dfmi.loc[(slice("A1", "A3"), slice(None), ["C1", "C3"]), :]


You can use :class:`pandas.IndexSlice` to facilitate a more natural syntax
using ``:``, rather than using ``slice(None)``.

.. ipython:: python

   idx = pd.IndexSlice
   dfmi.loc[idx[:, :, ["C1", "C3"]], idx[:, "foo"]]

It is possible to perform quite complicated selections using this method on multiple
axes at the same time.

.. ipython:: python

   dfmi.loc["A1", (slice(None), "foo")]
   dfmi.loc[idx[:, :, ["C1", "C3"]], idx[:, "foo"]]

Using a boolean indexer you can provide selection related to the *values*.

.. ipython:: python

   mask = dfmi[("a", "foo")] > 200
   dfmi.loc[idx[mask, :, ["C1", "C3"]], idx[:, "foo"]]

You can also specify the ``axis`` argument to ``.loc`` to interpret the passed
slicers on a single axis.

.. ipython:: python

   dfmi.loc(axis=0)[:, :, ["C1", "C3"]]

Furthermore, you can *set* the values using the following methods.

.. ipython:: python

   df2 = dfmi.copy()
   df2.loc(axis=0)[:, :, ["C1", "C3"]] = -10
   df2

You can use a right-hand-side of an alignable object as well.

.. ipython:: python

   df2 = dfmi.copy()
   df2.loc[idx[:, :, ["C1", "C3"]], :] = df2 * 1000
   df2

.. _advanced.xs:

Cross-section
~~~~~~~~~~~~~

The :meth:`~DataFrame.xs` method of ``DataFrame`` additionally takes a level argument to make
selecting data at a particular level of a ``MultiIndex`` easier.

.. ipython:: python

   df
   df.xs("one", level="second")

.. ipython:: python

   # using the slicers
   df.loc[(slice(None), "one"), :]

You can also select on the columns with ``xs``, by
providing the axis argument.

.. ipython:: python

   df = df.T
   df.xs("one", level="second", axis=1)

.. ipython:: python

   # using the slicers
   df.loc[:, (slice(None), "one")]

``xs`` also allows selection with multiple keys.

.. ipython:: python

   df.xs(("one", "bar"), level=("second", "first"), axis=1)

.. ipython:: python

   # using the slicers
   df.loc[:, ("bar", "one")]

You can pass ``drop_level=False`` to ``xs`` to retain
the level that was selected.

.. ipython:: python

   df.xs("one", level="second", axis=1, drop_level=False)

Compare the above with the result using ``drop_level=True`` (the default value).

.. ipython:: python

   df.xs("one", level="second", axis=1, drop_level=True)

.. ipython:: python
   :suppress:

   df = df.T

.. _advanced.advanced_reindex:

Advanced reindexing and alignment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using the parameter ``level`` in the :meth:`~DataFrame.reindex` and
:meth:`~DataFrame.align` methods of pandas objects is useful to broadcast
values across a level. For instance:

.. ipython:: python

   midx = pd.MultiIndex(
       levels=[["zero", "one"], ["x", "y"]], codes=[[1, 1, 0, 0], [1, 0, 1, 0]]
   )
   df = pd.DataFrame(np.random.randn(4, 2), index=midx)
   df
   df2 = df.groupby(level=0).mean()
   df2
   df2.reindex(df.index, level=0)

   # aligning
   df_aligned, df2_aligned = df.align(df2, level=0)
   df_aligned
   df2_aligned


Swapping levels with ``swaplevel``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~MultiIndex.swaplevel` method can switch the order of two levels:

.. ipython:: python

   df[:5]
   df[:5].swaplevel(0, 1, axis=0)

.. _advanced.reorderlevels:

Reordering levels with ``reorder_levels``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~MultiIndex.reorder_levels` method generalizes the ``swaplevel``
method, allowing you to permute the hierarchical index levels in one step:

.. ipython:: python

   df[:5].reorder_levels([1, 0], axis=0)

.. _advanced.index_names:

Renaming names of an ``Index`` or ``MultiIndex``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~DataFrame.rename` method is used to rename the labels of a
``MultiIndex``, and is typically used to rename the columns of a ``DataFrame``.
The ``columns`` argument of ``rename`` allows a dictionary to be specified
that includes only the columns you wish to rename.

.. ipython:: python

   df.rename(columns={0: "col0", 1: "col1"})

This method can also be used to rename specific labels of the main index
of the ``DataFrame``.

.. ipython:: python

   df.rename(index={"one": "two", "y": "z"})

The :meth:`~DataFrame.rename_axis` method is used to rename the name of a
``Index`` or ``MultiIndex``. In particular, the names of the levels of a
``MultiIndex`` can be specified, which is useful if ``reset_index()`` is later
used to move the values from the ``MultiIndex`` to a column.

.. ipython:: python

   df.rename_axis(index=["abc", "def"])

Note that the columns of a ``DataFrame`` are an index, so that using
``rename_axis`` with the ``columns`` argument will change the name of that
index.

.. ipython:: python

   df.rename_axis(columns="Cols").columns

Both ``rename`` and ``rename_axis`` support specifying a dictionary,
``Series`` or a mapping function to map labels/names to new values.

When working with an ``Index`` object directly, rather than via a ``DataFrame``,
:meth:`Index.set_names` can be used to change the names.

.. ipython:: python

   mi = pd.MultiIndex.from_product([[1, 2], ["a", "b"]], names=["x", "y"])
   mi.names

   mi2 = mi.rename("new name", level=0)
   mi2


You cannot set the names of the MultiIndex via a level.

.. ipython:: python
   :okexcept:

   mi.levels[0].name = "name via level"

Use :meth:`Index.set_names` instead.

Sorting a ``MultiIndex``
------------------------

For :class:`MultiIndex`-ed objects to be indexed and sliced effectively,
they need to be sorted. As with any index, you can use :meth:`~DataFrame.sort_index`.

.. ipython:: python

   import random

   random.shuffle(tuples)
   s = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples))
   s
   s.sort_index()
   s.sort_index(level=0)
   s.sort_index(level=1)

.. _advanced.sortlevel_byname:

You may also pass a level name to ``sort_index`` if the ``MultiIndex`` levels
are named.

.. ipython:: python

   s.index.set_names(["L1", "L2"], inplace=True)
   s.sort_index(level="L1")
   s.sort_index(level="L2")

On higher dimensional objects, you can sort any of the other axes by level if
they have a ``MultiIndex``:

.. ipython:: python

   df.T.sort_index(level=1, axis=1)

Indexing will work even if the data are not sorted, but will be rather
inefficient (and show a ``PerformanceWarning``). It will also
return a copy of the data rather than a view:

.. ipython:: python

   dfm = pd.DataFrame(
       {"jim": [0, 0, 1, 1], "joe": ["x", "x", "z", "y"], "jolie": np.random.rand(4)}
   )
   dfm = dfm.set_index(["jim", "joe"])
   dfm

.. code-block:: ipython

   In [4]: dfm.loc[(1, 'z')]
   PerformanceWarning: indexing past lexsort depth may impact performance.

   Out[4]:
              jolie
   jim joe
   1   z    0.64094

.. _advanced.unsorted:

Furthermore, if you try to index something that is not fully lexsorted, this can raise:

.. code-block:: ipython

    In [5]: dfm.loc[(0, 'y'):(1, 'z')]
    UnsortedIndexError: 'Key length (2) was greater than MultiIndex lexsort depth (1)'

The :meth:`~MultiIndex.is_monotonic_increasing` method on a ``MultiIndex`` shows if the
index is sorted:

.. ipython:: python

   dfm.index.is_monotonic_increasing

.. ipython:: python

   dfm = dfm.sort_index()
   dfm
   dfm.index.is_monotonic_increasing

And now selection works as expected.

.. ipython:: python

   dfm.loc[(0, "y"):(1, "z")]

Take methods
------------

.. _advanced.take:

Similar to NumPy ndarrays, pandas ``Index``, ``Series``, and ``DataFrame`` also provides
the :meth:`~DataFrame.take` method that retrieves elements along a given axis at the given
indices. The given indices must be either a list or an ndarray of integer
index positions. ``take`` will also accept negative integers as relative positions to the end of the object.

.. ipython:: python

   index = pd.Index(np.random.randint(0, 1000, 10))
   index

   positions = [0, 9, 3]

   index[positions]
   index.take(positions)

   ser = pd.Series(np.random.randn(10))

   ser.iloc[positions]
   ser.take(positions)

For DataFrames, the given indices should be a 1d list or ndarray that specifies
row or column positions.

.. ipython:: python

   frm = pd.DataFrame(np.random.randn(5, 3))

   frm.take([1, 4, 3])

   frm.take([0, 2], axis=1)

It is important to note that the ``take`` method on pandas objects are not
intended to work on boolean indices and may return unexpected results.

.. ipython:: python

   arr = np.random.randn(10)
   arr.take([False, False, True, True])
   arr[[0, 1]]

   ser = pd.Series(np.random.randn(10))
   ser.take([False, False, True, True])
   ser.iloc[[0, 1]]

Finally, as a small note on performance, because the ``take`` method handles
a narrower range of inputs, it can offer performance that is a good deal
faster than fancy indexing.

.. ipython:: python

   arr = np.random.randn(10000, 5)
   indexer = np.arange(10000)
   random.shuffle(indexer)

   %timeit arr[indexer]
   %timeit arr.take(indexer, axis=0)

.. ipython:: python

   ser = pd.Series(arr[:, 0])
   %timeit ser.iloc[indexer]
   %timeit ser.take(indexer)

.. _advanced.index_types:

Index types
-----------

We have discussed ``MultiIndex`` in the previous sections pretty extensively.
Documentation about ``DatetimeIndex`` and ``PeriodIndex`` are shown :ref:`here <timeseries.overview>`,
and documentation about ``TimedeltaIndex`` is found :ref:`here <timedeltas.index>`.

In the following sub-sections we will highlight some other index types.

.. _advanced.categoricalindex:

CategoricalIndex
~~~~~~~~~~~~~~~~

:class:`CategoricalIndex` is a type of index that is useful for supporting
indexing with duplicates. This is a container around a :class:`Categorical`
and allows efficient indexing and storage of an index with a large number of duplicated elements.

.. ipython:: python

   from pandas.api.types import CategoricalDtype

   df = pd.DataFrame({"A": np.arange(6), "B": list("aabbca")})
   df["B"] = df["B"].astype(CategoricalDtype(list("cab")))
   df
   df.dtypes
   df["B"].cat.categories

Setting the index will create a ``CategoricalIndex``.

.. ipython:: python

   df2 = df.set_index("B")
   df2.index

Indexing with ``__getitem__/.iloc/.loc`` works similarly to an ``Index`` with duplicates.
The indexers **must** be in the category or the operation will raise a ``KeyError``.

.. ipython:: python

   df2.loc["a"]

The ``CategoricalIndex`` is **preserved** after indexing:

.. ipython:: python

   df2.loc["a"].index

Sorting the index will sort by the order of the categories (recall that we
created the index with ``CategoricalDtype(list('cab'))``, so the sorted
order is ``cab``).

.. ipython:: python

   df2.sort_index()

Groupby operations on the index will preserve the index nature as well.

.. ipython:: python

   df2.groupby(level=0).sum()
   df2.groupby(level=0).sum().index

Reindexing operations will return a resulting index based on the type of the passed
indexer. Passing a list will return a plain-old ``Index``; indexing with
a ``Categorical`` will return a ``CategoricalIndex``, indexed according to the categories
of the **passed** ``Categorical`` dtype. This allows one to arbitrarily index these even with
values **not** in the categories, similarly to how you can reindex **any** pandas index.

.. ipython:: python

   df3 = pd.DataFrame(
       {"A": np.arange(3), "B": pd.Series(list("abc")).astype("category")}
   )
   df3 = df3.set_index("B")
   df3

.. ipython:: python

   df3.reindex(["a", "e"])
   df3.reindex(["a", "e"]).index
   df3.reindex(pd.Categorical(["a", "e"], categories=list("abe")))
   df3.reindex(pd.Categorical(["a", "e"], categories=list("abe"))).index

.. warning::

   Reshaping and Comparison operations on a ``CategoricalIndex`` must have the same categories
   or a ``TypeError`` will be raised.

   .. ipython:: python

      df4 = pd.DataFrame({"A": np.arange(2), "B": list("ba")})
      df4["B"] = df4["B"].astype(CategoricalDtype(list("ab")))
      df4 = df4.set_index("B")
      df4.index

      df5 = pd.DataFrame({"A": np.arange(2), "B": list("bc")})
      df5["B"] = df5["B"].astype(CategoricalDtype(list("bc")))
      df5 = df5.set_index("B")
      df5.index

   .. code-block:: ipython

      In [1]: pd.concat([df4, df5])
      TypeError: categories must match existing categories when appending

.. _advanced.rangeindex:

Int64Index and RangeIndex
~~~~~~~~~~~~~~~~~~~~~~~~~

.. deprecated:: 1.4.0
    In pandas 2.0, :class:`Index` will become the default index type for numeric types
    instead of ``Int64Index``, ``Float64Index`` and ``UInt64Index`` and those index types
    are therefore deprecated and will be removed in a futire version.
    ``RangeIndex`` will not be removed, as it represents an optimized version of an integer index.

:class:`Int64Index` is a fundamental basic index in pandas. This is an immutable array
implementing an ordered, sliceable set.

:class:`RangeIndex` is a sub-class of ``Int64Index``  that provides the default index for all ``NDFrame`` objects.
``RangeIndex`` is an optimized version of ``Int64Index`` that can represent a monotonic ordered set. These are analogous to Python `range types <https://docs.python.org/3/library/stdtypes.html#typesseq-range>`__.

.. _advanced.float64index:

Float64Index
~~~~~~~~~~~~

.. deprecated:: 1.4.0
    :class:`Index` will become the default index type for numeric types in the future
    instead of ``Int64Index``, ``Float64Index`` and ``UInt64Index`` and those index types
    are therefore deprecated and will be removed in a future version of Pandas.
    ``RangeIndex`` will not be removed as it represents an optimized version of an integer index.

By default a :class:`Float64Index` will be automatically created when passing floating, or mixed-integer-floating values in index creation.
This enables a pure label-based slicing paradigm that makes ``[],ix,loc`` for scalar indexing and slicing work exactly the
same.

.. ipython:: python

   indexf = pd.Index([1.5, 2, 3, 4.5, 5])
   indexf
   sf = pd.Series(range(5), index=indexf)
   sf

Scalar selection for ``[],.loc`` will always be label based. An integer will match an equal float index (e.g. ``3`` is equivalent to ``3.0``).

.. ipython:: python

   sf[3]
   sf[3.0]
   sf.loc[3]
   sf.loc[3.0]

The only positional indexing is via ``iloc``.

.. ipython:: python

   sf.iloc[3]

A scalar index that is not found will raise a ``KeyError``.
Slicing is primarily on the values of the index when using ``[],ix,loc``, and
**always** positional when using ``iloc``. The exception is when the slice is
boolean, in which case it will always be positional.

.. ipython:: python

   sf[2:4]
   sf.loc[2:4]
   sf.iloc[2:4]

In float indexes, slicing using floats is allowed.

.. ipython:: python

   sf[2.1:4.6]
   sf.loc[2.1:4.6]

In non-float indexes, slicing using floats will raise a ``TypeError``.

.. code-block:: ipython

   In [1]: pd.Series(range(5))[3.5]
   TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)

   In [1]: pd.Series(range(5))[3.5:4.5]
   TypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)

Here is a typical use-case for using this type of indexing. Imagine that you have a somewhat
irregular timedelta-like indexing scheme, but the data is recorded as floats. This could, for
example, be millisecond offsets.

.. ipython:: python

   dfir = pd.concat(
       [
           pd.DataFrame(
               np.random.randn(5, 2), index=np.arange(5) * 250.0, columns=list("AB")
           ),
           pd.DataFrame(
               np.random.randn(6, 2),
               index=np.arange(4, 10) * 250.1,
               columns=list("AB"),
           ),
       ]
   )
   dfir

Selection operations then will always work on a value basis, for all selection operators.

.. ipython:: python

   dfir[0:1000.4]
   dfir.loc[0:1001, "A"]
   dfir.loc[1000.4]

You could retrieve the first 1 second (1000 ms) of data as such:

.. ipython:: python

   dfir[0:1000]

If you need integer based selection, you should use ``iloc``:

.. ipython:: python

   dfir.iloc[0:5]


.. _advanced.intervalindex:

IntervalIndex
~~~~~~~~~~~~~

:class:`IntervalIndex` together with its own dtype, :class:`~pandas.api.types.IntervalDtype`
as well as the :class:`Interval` scalar type,  allow first-class support in pandas
for interval notation.

The ``IntervalIndex`` allows some unique indexing and is also used as a
return type for the categories in :func:`cut` and :func:`qcut`.

Indexing with an ``IntervalIndex``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

An ``IntervalIndex`` can be used in ``Series`` and in ``DataFrame`` as the index.

.. ipython:: python

   df = pd.DataFrame(
       {"A": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4])
   )
   df

Label based indexing via ``.loc`` along the edges of an interval works as you would expect,
selecting that particular interval.

.. ipython:: python

   df.loc[2]
   df.loc[[2, 3]]

If you select a label *contained* within an interval, this will also select the interval.

.. ipython:: python

   df.loc[2.5]
   df.loc[[2.5, 3.5]]

Selecting using an ``Interval`` will only return exact matches (starting from pandas 0.25.0).

.. ipython:: python

   df.loc[pd.Interval(1, 2)]

Trying to select an ``Interval`` that is not exactly contained in the ``IntervalIndex`` will raise a ``KeyError``.

.. code-block:: python

   In [7]: df.loc[pd.Interval(0.5, 2.5)]
   ---------------------------------------------------------------------------
   KeyError: Interval(0.5, 2.5, closed='right')

Selecting all ``Intervals`` that overlap a given ``Interval`` can be performed using the
:meth:`~IntervalIndex.overlaps` method to create a boolean indexer.

.. ipython:: python

   idxr = df.index.overlaps(pd.Interval(0.5, 2.5))
   idxr
   df[idxr]

Binning data with ``cut`` and ``qcut``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

:func:`cut` and :func:`qcut` both return a ``Categorical`` object, and the bins they
create are stored as an ``IntervalIndex`` in its ``.categories`` attribute.

.. ipython:: python

   c = pd.cut(range(4), bins=2)
   c
   c.categories

:func:`cut` also accepts an ``IntervalIndex`` for its ``bins`` argument, which enables
a useful pandas idiom. First, We call :func:`cut` with some data and ``bins`` set to a
fixed number, to generate the bins. Then, we pass the values of ``.categories`` as the
``bins`` argument in subsequent calls to :func:`cut`, supplying new data which will be
binned into the same bins.

.. ipython:: python

   pd.cut([0, 3, 5, 1], bins=c.categories)

Any value which falls outside all bins will be assigned a ``NaN`` value.

Generating ranges of intervals
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If we need intervals on a regular frequency, we can use the :func:`interval_range` function
to create an ``IntervalIndex`` using various combinations of ``start``, ``end``, and ``periods``.
The default frequency for ``interval_range`` is a 1 for numeric intervals, and calendar day for
datetime-like intervals:

.. ipython:: python

   pd.interval_range(start=0, end=5)

   pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4)

   pd.interval_range(end=pd.Timedelta("3 days"), periods=3)

The ``freq`` parameter can used to specify non-default frequencies, and can utilize a variety
of :ref:`frequency aliases <timeseries.offset_aliases>` with datetime-like intervals:

.. ipython:: python

   pd.interval_range(start=0, periods=5, freq=1.5)

   pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4, freq="W")

   pd.interval_range(start=pd.Timedelta("0 days"), periods=3, freq="9H")

Additionally, the ``closed`` parameter can be used to specify which side(s) the intervals
are closed on.  Intervals are closed on the right side by default.

.. ipython:: python

   pd.interval_range(start=0, end=4, closed="both")

   pd.interval_range(start=0, end=4, closed="neither")

Specifying ``start``, ``end``, and ``periods`` will generate a range of evenly spaced
intervals from ``start`` to ``end`` inclusively, with ``periods`` number of elements
in the resulting ``IntervalIndex``:

.. ipython:: python

   pd.interval_range(start=0, end=6, periods=4)

   pd.interval_range(pd.Timestamp("2018-01-01"), pd.Timestamp("2018-02-28"), periods=3)

Miscellaneous indexing FAQ
--------------------------

Integer indexing
~~~~~~~~~~~~~~~~

Label-based indexing with integer axis labels is a thorny topic. It has been
discussed heavily on mailing lists and among various members of the scientific
Python community. In pandas, our general viewpoint is that labels matter more
than integer locations. Therefore, with an integer axis index *only*
label-based indexing is possible with the standard tools like ``.loc``. The
following code will generate exceptions:

.. ipython:: python
   :okexcept:

   s = pd.Series(range(5))
   s[-1]
   df = pd.DataFrame(np.random.randn(5, 4))
   df
   df.loc[-2:]

This deliberate decision was made to prevent ambiguities and subtle bugs (many
users reported finding bugs when the API change was made to stop "falling back"
on position-based indexing).

Non-monotonic indexes require exact matches
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the index of a ``Series`` or ``DataFrame`` is monotonically increasing or decreasing, then the bounds
of a label-based slice can be outside the range of the index, much like slice indexing a
normal Python ``list``. Monotonicity of an index can be tested with the :meth:`~Index.is_monotonic_increasing` and
:meth:`~Index.is_monotonic_decreasing` attributes.

.. ipython:: python

    df = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=["data"], data=list(range(5)))
    df.index.is_monotonic_increasing

    # no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4:
    df.loc[0:4, :]

    # slice is are outside the index, so empty DataFrame is returned
    df.loc[13:15, :]

On the other hand, if the index is not monotonic, then both slice bounds must be
*unique* members of the index.

.. ipython:: python

    df = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=["data"], data=list(range(6)))
    df.index.is_monotonic_increasing

    # OK because 2 and 4 are in the index
    df.loc[2:4, :]

.. code-block:: ipython

    # 0 is not in the index
    In [9]: df.loc[0:4, :]
    KeyError: 0

    # 3 is not a unique label
    In [11]: df.loc[2:3, :]
    KeyError: 'Cannot get right slice bound for non-unique label: 3'

``Index.is_monotonic_increasing`` and ``Index.is_monotonic_decreasing`` only check that
an index is weakly monotonic. To check for strict monotonicity, you can combine one of those with
the :meth:`~Index.is_unique` attribute.

.. ipython:: python

   weakly_monotonic = pd.Index(["a", "b", "c", "c"])
   weakly_monotonic
   weakly_monotonic.is_monotonic_increasing
   weakly_monotonic.is_monotonic_increasing & weakly_monotonic.is_unique

.. _advanced.endpoints_are_inclusive:

Endpoints are inclusive
~~~~~~~~~~~~~~~~~~~~~~~

Compared with standard Python sequence slicing in which the slice endpoint is
not inclusive, label-based slicing in pandas **is inclusive**. The primary
reason for this is that it is often not possible to easily determine the
"successor" or next element after a particular label in an index. For example,
consider the following ``Series``:

.. ipython:: python

   s = pd.Series(np.random.randn(6), index=list("abcdef"))
   s

Suppose we wished to slice from ``c`` to ``e``, using integers this would be
accomplished as such:

.. ipython:: python

   s[2:5]

However, if you only had ``c`` and ``e``, determining the next element in the
index can be somewhat complicated. For example, the following does not work:

::

    s.loc['c':'e' + 1]

A very common use case is to limit a time series to start and end at two
specific dates. To enable this, we made the design choice to make label-based
slicing include both endpoints:

.. ipython:: python

    s.loc["c":"e"]

This is most definitely a "practicality beats purity" sort of thing, but it is
something to watch out for if you expect label-based slicing to behave exactly
in the way that standard Python integer slicing works.


Indexing potentially changes underlying Series dtype
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The different indexing operation can potentially change the dtype of a ``Series``.

.. ipython:: python

   series1 = pd.Series([1, 2, 3])
   series1.dtype
   res = series1.reindex([0, 4])
   res.dtype
   res

.. ipython:: python

   series2 = pd.Series([True])
   series2.dtype
   res = series2.reindex_like(series1)
   res.dtype
   res

This is because the (re)indexing operations above silently inserts ``NaNs`` and the ``dtype``
changes accordingly.  This can cause some issues when using ``numpy`` ``ufuncs``
such as ``numpy.logical_and``.

See the :issue:`2388` for a more
detailed discussion.
.. _cookbook:

{{ header }}

********
Cookbook
********

This is a repository for *short and sweet* examples and links for useful pandas recipes.
We encourage users to add to this documentation.

Adding interesting links and/or inline examples to this section is a great *First Pull Request*.

Simplified, condensed, new-user friendly, in-line examples have been inserted where possible to
augment the Stack-Overflow and GitHub links.  Many of the links contain expanded information,
above what the in-line examples offer.

pandas (pd) and NumPy (np) are the only two abbreviated imported modules. The rest are kept
explicitly imported for newer users.

Idioms
------

.. _cookbook.idioms:

These are some neat pandas ``idioms``

`if-then/if-then-else on one column, and assignment to another one or more columns:
<https://stackoverflow.com/questions/17128302/python-pandas-idiom-for-if-then-else>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

if-then...
**********

An if-then on one column

.. ipython:: python

   df.loc[df.AAA >= 5, "BBB"] = -1
   df

An if-then with assignment to 2 columns:

.. ipython:: python

   df.loc[df.AAA >= 5, ["BBB", "CCC"]] = 555
   df

Add another line with different logic, to do the -else

.. ipython:: python

   df.loc[df.AAA < 5, ["BBB", "CCC"]] = 2000
   df

Or use pandas where after you've set up a mask

.. ipython:: python

   df_mask = pd.DataFrame(
       {"AAA": [True] * 4, "BBB": [False] * 4, "CCC": [True, False] * 2}
   )
   df.where(df_mask, -1000)

`if-then-else using NumPy's where()
<https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df
   df["logic"] = np.where(df["AAA"] > 5, "high", "low")
   df

Splitting
*********

`Split a frame with a boolean criterion
<https://stackoverflow.com/questions/14957116/how-to-split-a-dataframe-according-to-a-boolean-criterion>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

   df[df.AAA <= 5]
   df[df.AAA > 5]

Building criteria
*****************

`Select with multi-column criteria
<https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

...and (without assignment returns a Series)

.. ipython:: python

   df.loc[(df["BBB"] < 25) & (df["CCC"] >= -40), "AAA"]

...or (without assignment returns a Series)

.. ipython:: python

   df.loc[(df["BBB"] > 25) | (df["CCC"] >= -40), "AAA"]

...or (with assignment modifies the DataFrame.)

.. ipython:: python

   df.loc[(df["BBB"] > 25) | (df["CCC"] >= 75), "AAA"] = 0.1
   df

`Select rows with data closest to certain value using argsort
<https://stackoverflow.com/questions/17758023/return-rows-in-a-dataframe-closest-to-a-user-defined-number>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df
   aValue = 43.0
   df.loc[(df.CCC - aValue).abs().argsort()]

`Dynamically reduce a list of criteria using a binary operators
<https://stackoverflow.com/questions/21058254/pandas-boolean-operation-in-a-python-list/21058331>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

   Crit1 = df.AAA <= 5.5
   Crit2 = df.BBB == 10.0
   Crit3 = df.CCC > -40.0

One could hard code:

.. ipython:: python

   AllCrit = Crit1 & Crit2 & Crit3

...Or it can be done with a list of dynamically built criteria

.. ipython:: python

   import functools

   CritList = [Crit1, Crit2, Crit3]
   AllCrit = functools.reduce(lambda x, y: x & y, CritList)

   df[AllCrit]

.. _cookbook.selection:

Selection
---------

Dataframes
**********

The :ref:`indexing <indexing>` docs.

`Using both row labels and value conditionals
<https://stackoverflow.com/questions/14725068/pandas-using-row-labels-in-boolean-indexing>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

   df[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]

Use loc for label-oriented slicing and iloc positional slicing :issue:`2904`

.. ipython:: python

  df = pd.DataFrame(
      {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]},
      index=["foo", "bar", "boo", "kar"],
  )


There are 2 explicit slicing methods, with a third general case

1. Positional-oriented (Python slicing style : exclusive of end)
2. Label-oriented (Non-Python slicing style : inclusive of end)
3. General (Either slicing style : depends on if the slice contains labels or positions)

.. ipython:: python
   df.iloc[0:3]  # Positional

   df.loc["bar":"kar"]  # Label

   # Generic
   df[0:3]
   df["bar":"kar"]

Ambiguity arises when an index consists of integers with a non-zero start or non-unit increment.

.. ipython:: python

   data = {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   df2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.
   df2.iloc[1:3]  # Position-oriented
   df2.loc[1:3]  # Label-oriented

`Using inverse operator (~) to take the complement of a mask
<https://stackoverflow.com/q/14986510>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [4, 5, 6, 7], "BBB": [10, 20, 30, 40], "CCC": [100, 50, -30, -50]}
   )
   df

   df[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]

New columns
***********

`Efficiently and dynamically creating new columns using applymap
<https://stackoverflow.com/questions/16575868/efficiently-creating-additional-columns-in-a-pandas-dataframe-using-map>`__

.. ipython:: python

   df = pd.DataFrame({"AAA": [1, 2, 1, 3], "BBB": [1, 1, 2, 2], "CCC": [2, 1, 3, 1]})
   df

   source_cols = df.columns  # Or some subset would work too
   new_cols = [str(x) + "_cat" for x in source_cols]
   categories = {1: "Alpha", 2: "Beta", 3: "Charlie"}

   df[new_cols] = df[source_cols].applymap(categories.get)
   df

`Keep other columns when using min() with groupby
<https://stackoverflow.com/q/23394476>`__

.. ipython:: python

   df = pd.DataFrame(
       {"AAA": [1, 1, 1, 2, 2, 2, 3, 3], "BBB": [2, 1, 3, 4, 5, 1, 2, 3]}
   )
   df

Method 1 : idxmin() to get the index of the minimums

.. ipython:: python

   df.loc[df.groupby("AAA")["BBB"].idxmin()]

Method 2 : sort then take first of each

.. ipython:: python

   df.sort_values(by="BBB").groupby("AAA", as_index=False).first()

Notice the same results, with the exception of the index.

.. _cookbook.multi_index:

Multiindexing
-------------

The :ref:`multindexing <advanced.hierarchical>` docs.

`Creating a MultiIndex from a labeled frame
<https://stackoverflow.com/questions/14916358/reshaping-dataframes-in-pandas-based-on-column-labels>`__

.. ipython:: python

   df = pd.DataFrame(
       {
           "row": [0, 1, 2],
           "One_X": [1.1, 1.1, 1.1],
           "One_Y": [1.2, 1.2, 1.2],
           "Two_X": [1.11, 1.11, 1.11],
           "Two_Y": [1.22, 1.22, 1.22],
       }
   )
   df

   # As Labelled Index
   df = df.set_index("row")
   df
   # With Hierarchical Columns
   df.columns = pd.MultiIndex.from_tuples([tuple(c.split("_")) for c in df.columns])
   df
   # Now stack & Reset
   df = df.stack(0).reset_index(1)
   df
   # And fix the labels (Notice the label 'level_1' got added automatically)
   df.columns = ["Sample", "All_X", "All_Y"]
   df

Arithmetic
**********

`Performing arithmetic with a MultiIndex that needs broadcasting
<https://stackoverflow.com/questions/19501510/divide-entire-pandas-multiindex-dataframe-by-dataframe-variable/19502176#19502176>`__

.. ipython:: python

   cols = pd.MultiIndex.from_tuples(
       [(x, y) for x in ["A", "B", "C"] for y in ["O", "I"]]
   )
   df = pd.DataFrame(np.random.randn(2, 6), index=["n", "m"], columns=cols)
   df
   df = df.div(df["C"], level=1)
   df

Slicing
*******

`Slicing a MultiIndex with xs
<https://stackoverflow.com/questions/12590131/how-to-slice-multindex-columns-in-pandas-dataframes>`__

.. ipython:: python

   coords = [("AA", "one"), ("AA", "six"), ("BB", "one"), ("BB", "two"), ("BB", "six")]
   index = pd.MultiIndex.from_tuples(coords)
   df = pd.DataFrame([11, 22, 33, 44, 55], index, ["MyData"])
   df

To take the cross section of the 1st level and 1st axis the index:

.. ipython:: python

   # Note : level and axis are optional, and default to zero
   df.xs("BB", level=0, axis=0)

...and now the 2nd level of the 1st axis.

.. ipython:: python

   df.xs("six", level=1, axis=0)

`Slicing a MultiIndex with xs, method #2
<https://stackoverflow.com/questions/14964493/multiindex-based-indexing-in-pandas>`__

.. ipython:: python

   import itertools

   index = list(itertools.product(["Ada", "Quinn", "Violet"], ["Comp", "Math", "Sci"]))
   headr = list(itertools.product(["Exams", "Labs"], ["I", "II"]))
   indx = pd.MultiIndex.from_tuples(index, names=["Student", "Course"])
   cols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named
   data = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]
   df = pd.DataFrame(data, indx, cols)
   df

   All = slice(None)
   df.loc["Violet"]
   df.loc[(All, "Math"), All]
   df.loc[(slice("Ada", "Quinn"), "Math"), All]
   df.loc[(All, "Math"), ("Exams")]
   df.loc[(All, "Math"), (All, "II")]

`Setting portions of a MultiIndex with xs
<https://stackoverflow.com/questions/19319432/pandas-selecting-a-lower-level-in-a-dataframe-to-do-a-ffill>`__

Sorting
*******

`Sort by specific column or an ordered list of columns, with a MultiIndex
<https://stackoverflow.com/q/14733871>`__

.. ipython:: python

   df.sort_values(by=("Labs", "II"), ascending=False)

Partial selection, the need for sortedness :issue:`2995`

Levels
******

`Prepending a level to a multiindex
<https://stackoverflow.com/questions/14744068/prepend-a-level-to-a-pandas-multiindex>`__

`Flatten Hierarchical columns
<https://stackoverflow.com/q/14507794>`__

.. _cookbook.missing_data:

Missing data
------------

The :ref:`missing data<missing_data>` docs.

Fill forward a reversed timeseries

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(6, 1),
       index=pd.date_range("2013-08-01", periods=6, freq="B"),
       columns=list("A"),
   )
   df.loc[df.index[3], "A"] = np.nan
   df
   df.reindex(df.index[::-1]).ffill()

`cumsum reset at NaN values
<https://stackoverflow.com/questions/18196811/cumsum-reset-at-nan>`__

Replace
*******

`Using replace with backrefs
<https://stackoverflow.com/questions/16818871/extracting-value-and-creating-new-column-out-of-it>`__

.. _cookbook.grouping:

Grouping
--------

The :ref:`grouping <groupby>` docs.

`Basic grouping with apply
<https://stackoverflow.com/questions/15322632/python-pandas-df-groupy-agg-column-reference-in-agg>`__

Unlike agg, apply's callable is passed a sub-DataFrame which gives you access to all the columns

.. ipython:: python

   df = pd.DataFrame(
       {
           "animal": "cat dog cat fish dog cat cat".split(),
           "size": list("SSMMMLL"),
           "weight": [8, 10, 11, 1, 20, 12, 12],
           "adult": [False] * 5 + [True] * 2,
       }
   )
   df

   # List the size of the animals with the highest weight.
   df.groupby("animal").apply(lambda subf: subf["size"][subf["weight"].idxmax()])

`Using get_group
<https://stackoverflow.com/questions/14734533/how-to-access-pandas-groupby-dataframe-by-key>`__

.. ipython:: python

   gb = df.groupby(["animal"])
   gb.get_group("cat")

`Apply to different items in a group
<https://stackoverflow.com/questions/15262134/apply-different-functions-to-different-items-in-group-object-python-pandas>`__

.. ipython:: python

   def GrowUp(x):
       avg_weight = sum(x[x["size"] == "S"].weight * 1.5)
       avg_weight += sum(x[x["size"] == "M"].weight * 1.25)
       avg_weight += sum(x[x["size"] == "L"].weight)
       avg_weight /= len(x)
       return pd.Series(["L", avg_weight, True], index=["size", "weight", "adult"])


   expected_df = gb.apply(GrowUp)
   expected_df

`Expanding apply
<https://stackoverflow.com/questions/14542145/reductions-down-a-column-in-pandas>`__

.. ipython:: python

   S = pd.Series([i / 100.0 for i in range(1, 11)])

   def cum_ret(x, y):
       return x * (1 + y)

   def red(x):
       return functools.reduce(cum_ret, x, 1.0)

   S.expanding().apply(red, raw=True)


`Replacing some values with mean of the rest of a group
<https://stackoverflow.com/questions/14760757/replacing-values-with-groupby-means>`__

.. ipython:: python

   df = pd.DataFrame({"A": [1, 1, 2, 2], "B": [1, -1, 1, 2]})
   gb = df.groupby("A")

   def replace(g):
       mask = g < 0
       return g.where(mask, g[~mask].mean())

   gb.transform(replace)

`Sort groups by aggregated data
<https://stackoverflow.com/questions/14941366/pandas-sort-by-group-aggregate-and-column>`__

.. ipython:: python

   df = pd.DataFrame(
       {
           "code": ["foo", "bar", "baz"] * 2,
           "data": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],
           "flag": [False, True] * 3,
       }
   )

   code_groups = df.groupby("code")

   agg_n_sort_order = code_groups[["data"]].transform(sum).sort_values(by="data")

   sorted_df = df.loc[agg_n_sort_order.index]

   sorted_df

`Create multiple aggregated columns
<https://stackoverflow.com/questions/14897100/create-multiple-columns-in-pandas-aggregation-function>`__

.. ipython:: python

   rng = pd.date_range(start="2014-10-07", periods=10, freq="2min")
   ts = pd.Series(data=list(range(10)), index=rng)

   def MyCust(x):
       if len(x) > 2:
           return x[1] * 1.234
       return pd.NaT

   mhc = {"Mean": np.mean, "Max": np.max, "Custom": MyCust}
   ts.resample("5min").apply(mhc)
   ts

`Create a value counts column and reassign back to the DataFrame
<https://stackoverflow.com/q/17709270>`__

.. ipython:: python

   df = pd.DataFrame(
       {"Color": "Red Red Red Blue".split(), "Value": [100, 150, 50, 50]}
   )
   df
   df["Counts"] = df.groupby(["Color"]).transform(len)
   df

`Shift groups of the values in a column based on the index
<https://stackoverflow.com/q/23198053/190597>`__

.. ipython:: python

   df = pd.DataFrame(
       {"line_race": [10, 10, 8, 10, 10, 8], "beyer": [99, 102, 103, 103, 88, 100]},
       index=[
           "Last Gunfighter",
           "Last Gunfighter",
           "Last Gunfighter",
           "Paynter",
           "Paynter",
           "Paynter",
       ],
   )
   df
   df["beyer_shifted"] = df.groupby(level=0)["beyer"].shift(1)
   df

`Select row with maximum value from each group
<https://stackoverflow.com/q/26701849/190597>`__

.. ipython:: python

   df = pd.DataFrame(
       {
           "host": ["other", "other", "that", "this", "this"],
           "service": ["mail", "web", "mail", "mail", "web"],
           "no": [1, 2, 1, 2, 1],
       }
   ).set_index(["host", "service"])
   mask = df.groupby(level=0).agg("idxmax")
   df_count = df.loc[mask["no"]].reset_index()
   df_count

`Grouping like Python's itertools.groupby
<https://stackoverflow.com/q/29142487/846892>`__

.. ipython:: python

   df = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=["A"])
   df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).groups
   df["A"].groupby((df["A"] != df["A"].shift()).cumsum()).cumsum()

Expanding data
**************

`Alignment and to-date
<https://stackoverflow.com/questions/15489011/python-time-series-alignment-and-to-date-functions>`__

`Rolling Computation window based on values instead of counts
<https://stackoverflow.com/questions/14300768/pandas-rolling-computation-with-window-based-on-values-instead-of-counts>`__

`Rolling Mean by Time Interval
<https://stackoverflow.com/questions/15771472/pandas-rolling-mean-by-time-interval>`__

Splitting
*********

`Splitting a frame
<https://stackoverflow.com/questions/13353233/best-way-to-split-a-dataframe-given-an-edge/15449992#15449992>`__

Create a list of dataframes, split using a delineation based on logic included in rows.

.. ipython:: python

   df = pd.DataFrame(
       data={
           "Case": ["A", "A", "A", "B", "A", "A", "B", "A", "A"],
           "Data": np.random.randn(9),
       }
   )

   dfs = list(
       zip(
           *df.groupby(
               (1 * (df["Case"] == "B"))
               .cumsum()
               .rolling(window=3, min_periods=1)
               .median()
           )
       )
   )[-1]

   dfs[0]
   dfs[1]
   dfs[2]

.. _cookbook.pivot:

Pivot
*****
The :ref:`Pivot <reshaping.pivot>` docs.

`Partial sums and subtotals
<https://stackoverflow.com/a/15574875>`__

.. ipython:: python

   df = pd.DataFrame(
       data={
           "Province": ["ON", "QC", "BC", "AL", "AL", "MN", "ON"],
           "City": [
               "Toronto",
               "Montreal",
               "Vancouver",
               "Calgary",
               "Edmonton",
               "Winnipeg",
               "Windsor",
           ],
           "Sales": [13, 6, 16, 8, 4, 3, 1],
       }
   )
   table = pd.pivot_table(
       df,
       values=["Sales"],
       index=["Province"],
       columns=["City"],
       aggfunc=np.sum,
       margins=True,
   )
   table.stack("City")

`Frequency table like plyr in R
<https://stackoverflow.com/questions/15589354/frequency-tables-in-pandas-like-plyr-in-r>`__

.. ipython:: python

   grades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]
   df = pd.DataFrame(
       {
           "ID": ["x%d" % r for r in range(10)],
           "Gender": ["F", "M", "F", "M", "F", "M", "F", "M", "M", "M"],
           "ExamYear": [
               "2007",
               "2007",
               "2007",
               "2008",
               "2008",
               "2008",
               "2008",
               "2009",
               "2009",
               "2009",
           ],
           "Class": [
               "algebra",
               "stats",
               "bio",
               "algebra",
               "algebra",
               "stats",
               "stats",
               "algebra",
               "bio",
               "bio",
           ],
           "Participated": [
               "yes",
               "yes",
               "yes",
               "yes",
               "no",
               "yes",
               "yes",
               "yes",
               "yes",
               "yes",
           ],
           "Passed": ["yes" if x > 50 else "no" for x in grades],
           "Employed": [
               True,
               True,
               True,
               False,
               False,
               False,
               False,
               True,
               True,
               False,
           ],
           "Grade": grades,
       }
   )

   df.groupby("ExamYear").agg(
       {
           "Participated": lambda x: x.value_counts()["yes"],
           "Passed": lambda x: sum(x == "yes"),
           "Employed": lambda x: sum(x),
           "Grade": lambda x: sum(x) / len(x),
       }
   )

`Plot pandas DataFrame with year over year data
<https://stackoverflow.com/questions/30379789/plot-pandas-data-frame-with-year-over-year-data>`__

To create year and month cross tabulation:

.. ipython:: python

   df = pd.DataFrame(
       {"value": np.random.randn(36)},
       index=pd.date_range("2011-01-01", freq="M", periods=36),
   )

   pd.pivot_table(
       df, index=df.index.month, columns=df.index.year, values="value", aggfunc="sum"
   )

Apply
*****

`Rolling apply to organize - Turning embedded lists into a MultiIndex frame
<https://stackoverflow.com/questions/17349981/converting-pandas-dataframe-with-categorical-values-into-binary-values>`__

.. ipython:: python

   df = pd.DataFrame(
       data={
           "A": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],
           "B": [["a", "b", "c"], ["jj", "kk"], ["ccc"]],
       },
       index=["I", "II", "III"],
   )

   def SeriesFromSubList(aList):
       return pd.Series(aList)

   df_orgz = pd.concat(
       {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}
   )
   df_orgz

`Rolling apply with a DataFrame returning a Series
<https://stackoverflow.com/questions/19121854/using-rolling-apply-on-a-dataframe-object>`__

Rolling Apply to multiple columns where function calculates a Series before a Scalar from the Series is returned

.. ipython:: python

   df = pd.DataFrame(
       data=np.random.randn(2000, 2) / 10000,
       index=pd.date_range("2001-01-01", periods=2000),
       columns=["A", "B"],
   )
   df

   def gm(df, const):
       v = ((((df["A"] + df["B"]) + 1).cumprod()) - 1) * const
       return v.iloc[-1]

   s = pd.Series(
       {
           df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)
           for i in range(len(df) - 50)
       }
   )
   s

`Rolling apply with a DataFrame returning a Scalar
<https://stackoverflow.com/questions/21040766/python-pandas-rolling-apply-two-column-input-into-function/21045831#21045831>`__

Rolling Apply to multiple columns where function returns a Scalar (Volume Weighted Average Price)

.. ipython:: python

   rng = pd.date_range(start="2014-01-01", periods=100)
   df = pd.DataFrame(
       {
           "Open": np.random.randn(len(rng)),
           "Close": np.random.randn(len(rng)),
           "Volume": np.random.randint(100, 2000, len(rng)),
       },
       index=rng,
   )
   df

   def vwap(bars):
       return (bars.Close * bars.Volume).sum() / bars.Volume.sum()

   window = 5
   s = pd.concat(
       [
           (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))
           for i in range(len(df) - window)
       ]
   )
   s.round(2)

Timeseries
----------

`Between times
<https://stackoverflow.com/questions/14539992/pandas-drop-rows-outside-of-time-range>`__

`Using indexer between time
<https://stackoverflow.com/questions/17559885/pandas-dataframe-mask-based-on-index>`__

`Constructing a datetime range that excludes weekends and includes only certain times
<https://stackoverflow.com/a/24014440>`__

`Vectorized Lookup
<https://stackoverflow.com/questions/13893227/vectorized-look-up-of-values-in-pandas-dataframe>`__

`Aggregation and plotting time series
<https://nipunbatra.github.io/blog/visualisation/2013/05/01/aggregation-timeseries.html>`__

Turn a matrix with hours in columns and days in rows into a continuous row sequence in the form of a time series.
`How to rearrange a Python pandas DataFrame?
<https://stackoverflow.com/questions/15432659/how-to-rearrange-a-python-pandas-dataframe>`__

`Dealing with duplicates when reindexing a timeseries to a specified frequency
<https://stackoverflow.com/questions/22244383/pandas-df-refill-adding-two-columns-of-different-shape>`__

Calculate the first day of the month for each entry in a DatetimeIndex

.. ipython:: python

   dates = pd.date_range("2000-01-01", periods=5)
   dates.to_period(freq="M").to_timestamp()

.. _cookbook.resample:

Resampling
**********

The :ref:`Resample <timeseries.resampling>` docs.

`Using Grouper instead of TimeGrouper for time grouping of values
<https://stackoverflow.com/questions/15297053/how-can-i-divide-single-values-of-a-dataframe-by-monthly-averages>`__

`Time grouping with some missing values
<https://stackoverflow.com/questions/33637312/pandas-grouper-by-frequency-with-completeness-requirement>`__

Valid frequency arguments to Grouper :ref:`Timeseries <timeseries.offset_aliases>`

`Grouping using a MultiIndex
<https://stackoverflow.com/questions/41483763/pandas-timegrouper-on-multiindex>`__

Using TimeGrouper and another grouping to create subgroups, then apply a custom function :issue:`3791`

`Resampling with custom periods
<https://stackoverflow.com/questions/15408156/resampling-with-custom-periods>`__

`Resample intraday frame without adding new days
<https://stackoverflow.com/questions/14898574/resample-intrday-pandas-dataframe-without-add-new-days>`__

`Resample minute data
<https://stackoverflow.com/questions/14861023/resampling-minute-data>`__

`Resample with groupby <https://stackoverflow.com/q/18677271/564538>`__

.. _cookbook.merge:

Merge
-----

The :ref:`Join <merging.join>` docs.

`Concatenate two dataframes with overlapping index (emulate R rbind)
<https://stackoverflow.com/questions/14988480/pandas-version-of-rbind>`__

.. ipython:: python

   rng = pd.date_range("2000-01-01", periods=6)
   df1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=["A", "B", "C"])
   df2 = df1.copy()

Depending on df construction, ``ignore_index`` may be needed

.. ipython:: python

   df = pd.concat([df1, df2], ignore_index=True)
   df

Self Join of a DataFrame :issue:`2996`

.. ipython:: python

   df = pd.DataFrame(
       data={
           "Area": ["A"] * 5 + ["C"] * 2,
           "Bins": [110] * 2 + [160] * 3 + [40] * 2,
           "Test_0": [0, 1, 0, 1, 2, 0, 1],
           "Data": np.random.randn(7),
       }
   )
   df

   df["Test_1"] = df["Test_0"] - 1

   pd.merge(
       df,
       df,
       left_on=["Bins", "Area", "Test_0"],
       right_on=["Bins", "Area", "Test_1"],
       suffixes=("_L", "_R"),
   )

`How to set the index and join
<https://stackoverflow.com/questions/14341805/pandas-merge-pd-merge-how-to-set-the-index-and-join>`__

`KDB like asof join
<https://stackoverflow.com/questions/12322289/kdb-like-asof-join-for-timeseries-data-in-pandas/12336039#12336039>`__

`Join with a criteria based on the values
<https://stackoverflow.com/questions/15581829/how-to-perform-an-inner-or-outer-join-of-dataframes-with-pandas-on-non-simplisti>`__

`Using searchsorted to merge based on values inside a range
<https://stackoverflow.com/questions/25125626/pandas-merge-with-logic/2512764>`__

.. _cookbook.plotting:

Plotting
--------

The :ref:`Plotting <visualization>` docs.

`Make Matplotlib look like R
<https://stackoverflow.com/questions/14349055/making-matplotlib-graphs-look-like-r-by-default>`__

`Setting x-axis major and minor labels
<https://stackoverflow.com/questions/12945971/pandas-timeseries-plot-setting-x-axis-major-and-minor-ticks-and-labels>`__

`Plotting multiple charts in an IPython Jupyter notebook
<https://stackoverflow.com/questions/16392921/make-more-than-one-chart-in-same-ipython-notebook-cell>`__

`Creating a multi-line plot
<https://stackoverflow.com/questions/16568964/make-a-multiline-plot-from-csv-file-in-matplotlib>`__

`Plotting a heatmap
<https://stackoverflow.com/questions/17050202/plot-timeseries-of-histograms-in-python>`__

`Annotate a time-series plot
<https://stackoverflow.com/questions/11067368/annotate-time-series-plot-in-matplotlib>`__

`Annotate a time-series plot #2
<https://stackoverflow.com/questions/17891493/annotating-points-from-a-pandas-dataframe-in-matplotlib-plot>`__

`Generate Embedded plots in excel files using Pandas, Vincent and xlsxwriter
<https://pandas-xlsxwriter-charts.readthedocs.io/>`__

`Boxplot for each quartile of a stratifying variable
<https://stackoverflow.com/questions/23232989/boxplot-stratified-by-column-in-python-pandas>`__

.. ipython:: python

   df = pd.DataFrame(
       {
           "stratifying_var": np.random.uniform(0, 100, 20),
           "price": np.random.normal(100, 5, 20),
       }
   )

   df["quartiles"] = pd.qcut(
       df["stratifying_var"], 4, labels=["0-25%", "25-50%", "50-75%", "75-100%"]
   )

   @savefig quartile_boxplot.png
   df.boxplot(column="price", by="quartiles")

Data in/out
-----------

`Performance comparison of SQL vs HDF5
<https://stackoverflow.com/q/16628329>`__

.. _cookbook.csv:

CSV
***

The :ref:`CSV <io.read_csv_table>` docs

`read_csv in action <https://wesmckinney.com/blog/update-on-upcoming-pandas-v0-10-new-file-parser-other-performance-wins/>`__

`appending to a csv
<https://stackoverflow.com/questions/17134942/pandas-dataframe-output-end-of-csv>`__

`Reading a csv chunk-by-chunk
<https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas/12193309#12193309>`__

`Reading only certain rows of a csv chunk-by-chunk
<https://stackoverflow.com/questions/19674212/pandas-data-frame-select-rows-and-clear-memory>`__

`Reading the first few lines of a frame
<https://stackoverflow.com/questions/15008970/way-to-read-first-few-lines-for-pandas-dataframe>`__

Reading a file that is compressed but not by ``gzip/bz2`` (the native compressed formats which ``read_csv`` understands).
This example shows a ``WinZipped`` file, but is a general application of opening the file within a context manager and
using that handle to read.
`See here
<https://stackoverflow.com/questions/17789907/pandas-convert-winzipped-csv-file-to-data-frame>`__

`Inferring dtypes from a file
<https://stackoverflow.com/questions/15555005/get-inferred-dataframe-types-iteratively-using-chunksize>`__

Dealing with bad lines :issue:`2886`

`Write a multi-row index CSV without writing duplicates
<https://stackoverflow.com/questions/17349574/pandas-write-multiindex-rows-with-to-csv>`__

.. _cookbook.csv.multiple_files:

Reading multiple files to create a single DataFrame
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The best way to combine multiple files into a single DataFrame is to read the individual frames one by one, put all
of the individual frames into a list, and then combine the frames in the list using :func:`pd.concat`:

.. ipython:: python

    for i in range(3):
        data = pd.DataFrame(np.random.randn(10, 4))
        data.to_csv("file_{}.csv".format(i))

    files = ["file_0.csv", "file_1.csv", "file_2.csv"]
    result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)

You can use the same approach to read all files matching a pattern.  Here is an example using ``glob``:

.. ipython:: python

    import glob
    import os

    files = glob.glob("file_*.csv")
    result = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)

Finally, this strategy will work with the other ``pd.read_*(...)`` functions described in the :ref:`io docs<io>`.

.. ipython:: python
    :suppress:

    for i in range(3):
        os.remove("file_{}.csv".format(i))

Parsing date components in multi-columns
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Parsing date components in multi-columns is faster with a format

.. ipython:: python

    i = pd.date_range("20000101", periods=10000)
    df = pd.DataFrame({"year": i.year, "month": i.month, "day": i.day})
    df.head()

    %timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')
    ds = df.apply(lambda x: "%04d%02d%02d" % (x["year"], x["month"], x["day"]), axis=1)
    ds.head()
    %timeit pd.to_datetime(ds)


Skip row between header and data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. ipython:: python

    data = """;;;;
     ;;;;
     ;;;;
     ;;;;
     ;;;;
     ;;;;
    ;;;;
     ;;;;
     ;;;;
    ;;;;
    date;Param1;Param2;Param4;Param5
        ;m²;°C;m²;m
    ;;;;
    01.01.1990 00:00;1;1;2;3
    01.01.1990 01:00;5;3;4;5
    01.01.1990 02:00;9;5;6;7
    01.01.1990 03:00;13;7;8;9
    01.01.1990 04:00;17;9;10;11
    01.01.1990 05:00;21;11;12;13
    """

Option 1: pass rows explicitly to skip rows
"""""""""""""""""""""""""""""""""""""""""""

.. ipython:: python

    from io import StringIO

    pd.read_csv(
        StringIO(data),
        sep=";",
        skiprows=[11, 12],
        index_col=0,
        parse_dates=True,
        header=10,
    )

Option 2: read column names and then data
"""""""""""""""""""""""""""""""""""""""""

.. ipython:: python

    pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
    columns = pd.read_csv(StringIO(data), sep=";", header=10, nrows=10).columns
    pd.read_csv(
        StringIO(data), sep=";", index_col=0, header=12, parse_dates=True, names=columns
    )


.. _cookbook.sql:

SQL
***

The :ref:`SQL <io.sql>` docs

`Reading from databases with SQL
<https://stackoverflow.com/questions/10065051/python-pandas-and-databases-like-mysql>`__

.. _cookbook.excel:

Excel
*****

The :ref:`Excel <io.excel>` docs

`Reading from a filelike handle
<https://stackoverflow.com/questions/15588713/sheets-of-excel-workbook-from-a-url-into-a-pandas-dataframe>`__

`Modifying formatting in XlsxWriter output
<https://pbpython.com/improve-pandas-excel-output.html>`__

Loading only visible sheets :issue:`19842#issuecomment-892150745`

.. _cookbook.html:

HTML
****

`Reading HTML tables from a server that cannot handle the default request
header <https://stackoverflow.com/a/18939272/564538>`__

.. _cookbook.hdf:

HDFStore
********

The :ref:`HDFStores <io.hdf5>` docs

`Simple queries with a Timestamp Index
<https://stackoverflow.com/questions/13926089/selecting-columns-from-pandas-hdfstore-table>`__

Managing heterogeneous data using a linked multiple table hierarchy :issue:`3032`

`Merging on-disk tables with millions of rows
<https://stackoverflow.com/questions/14614512/merging-two-tables-with-millions-of-rows-in-python/14617925#14617925>`__

`Avoiding inconsistencies when writing to a store from multiple processes/threads
<https://stackoverflow.com/a/29014295/2858145>`__

De-duplicating a large store by chunks, essentially a recursive reduction operation. Shows a function for taking in data from
csv file and creating a store by chunks, with date parsing as well.
`See here
<https://stackoverflow.com/questions/16110252/need-to-compare-very-large-files-around-1-5gb-in-python/16110391#16110391>`__

`Creating a store chunk-by-chunk from a csv file
<https://stackoverflow.com/questions/20428355/appending-column-to-frame-of-hdf-file-in-pandas/20428786#20428786>`__

`Appending to a store, while creating a unique index
<https://stackoverflow.com/questions/16997048/how-does-one-append-large-amounts-of-data-to-a-pandas-hdfstore-and-get-a-natural/16999397#16999397>`__

`Large Data work flows
<https://stackoverflow.com/q/14262433>`__

`Reading in a sequence of files, then providing a global unique index to a store while appending
<https://stackoverflow.com/questions/16997048/how-does-one-append-large-amounts-of-data-to-a-pandas-hdfstore-and-get-a-natural>`__

`Groupby on a HDFStore with low group density
<https://stackoverflow.com/questions/15798209/pandas-group-by-query-on-large-data-in-hdfstore>`__

`Groupby on a HDFStore with high group density
<https://stackoverflow.com/questions/25459982/trouble-with-grouby-on-millions-of-keys-on-a-chunked-file-in-python-pandas/25471765#25471765>`__

`Hierarchical queries on a HDFStore
<https://stackoverflow.com/questions/22777284/improve-query-performance-from-a-large-hdfstore-table-with-pandas/22820780#22820780>`__

`Counting with a HDFStore
<https://stackoverflow.com/questions/20497897/converting-dict-of-dicts-into-pandas-dataframe-memory-issues>`__

`Troubleshoot HDFStore exceptions
<https://stackoverflow.com/questions/15488809/how-to-trouble-shoot-hdfstore-exception-cannot-find-the-correct-atom-type>`__

`Setting min_itemsize with strings
<https://stackoverflow.com/questions/15988871/hdfstore-appendstring-dataframe-fails-when-string-column-contents-are-longer>`__

`Using ptrepack to create a completely-sorted-index on a store
<https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index>`__

Storing Attributes to a group node

.. ipython:: python

   df = pd.DataFrame(np.random.randn(8, 3))
   store = pd.HDFStore("test.h5")
   store.put("df", df)

   # you can store an arbitrary Python object via pickle
   store.get_storer("df").attrs.my_attribute = {"A": 10}
   store.get_storer("df").attrs.my_attribute

.. ipython:: python
   :suppress:

   store.close()
   os.remove("test.h5")

You can create or load a HDFStore in-memory  by passing the ``driver``
parameter to PyTables. Changes are only written to disk when the HDFStore
is closed.

.. ipython:: python

   store = pd.HDFStore("test.h5", "w", driver="H5FD_CORE")

   df = pd.DataFrame(np.random.randn(8, 3))
   store["test"] = df

   # only after closing the store, data is written to disk:
   store.close()

.. ipython:: python
   :suppress:

   os.remove("test.h5")

.. _cookbook.binary:

Binary files
************

pandas readily accepts NumPy record arrays, if you need to read in a binary
file consisting of an array of C structs. For example, given this C program
in a file called ``main.c`` compiled with ``gcc main.c -std=gnu99`` on a
64-bit machine,

.. code-block:: c

   #include <stdio.h>
   #include <stdint.h>

   typedef struct _Data
   {
       int32_t count;
       double avg;
       float scale;
   } Data;

   int main(int argc, const char *argv[])
   {
       size_t n = 10;
       Data d[n];

       for (int i = 0; i < n; ++i)
       {
           d[i].count = i;
           d[i].avg = i + 1.0;
           d[i].scale = (float) i + 2.0f;
       }

       FILE *file = fopen("binary.dat", "wb");
       fwrite(&d, sizeof(Data), n, file);
       fclose(file);

       return 0;
   }

the following Python code will read the binary file ``'binary.dat'`` into a
pandas ``DataFrame``, where each element of the struct corresponds to a column
in the frame:

.. code-block:: python

   names = "count", "avg", "scale"

   # note that the offsets are larger than the size of the type because of
   # struct padding
   offsets = 0, 8, 16
   formats = "i4", "f8", "f4"
   dt = np.dtype({"names": names, "offsets": offsets, "formats": formats}, align=True)
   df = pd.DataFrame(np.fromfile("binary.dat", dt))

.. note::

   The offsets of the structure elements may be different depending on the
   architecture of the machine on which the file was created. Using a raw
   binary file format like this for general data storage is not recommended, as
   it is not cross platform. We recommended either HDF5 or parquet, both of
   which are supported by pandas' IO facilities.

Computation
-----------

`Numerical integration (sample-based) of a time series
<https://nbviewer.ipython.org/gist/metakermit/5720498>`__

Correlation
***********

Often it's useful to obtain the lower (or upper) triangular form of a correlation matrix calculated from :func:`DataFrame.corr`.  This can be achieved by passing a boolean mask to ``where`` as follows:

.. ipython:: python

    df = pd.DataFrame(np.random.random(size=(100, 5)))

    corr_mat = df.corr()
    mask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)

    corr_mat.where(mask)

The ``method`` argument within ``DataFrame.corr`` can accept a callable in addition to the named correlation types.  Here we compute the `distance correlation <https://en.wikipedia.org/wiki/Distance_correlation>`__ matrix for a ``DataFrame`` object.

.. ipython:: python

   def distcorr(x, y):
       n = len(x)
       a = np.zeros(shape=(n, n))
       b = np.zeros(shape=(n, n))
       for i in range(n):
           for j in range(i + 1, n):
               a[i, j] = abs(x[i] - x[j])
               b[i, j] = abs(y[i] - y[j])
       a += a.T
       b += b.T
       a_bar = np.vstack([np.nanmean(a, axis=0)] * n)
       b_bar = np.vstack([np.nanmean(b, axis=0)] * n)
       A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())
       B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())
       cov_ab = np.sqrt(np.nansum(A * B)) / n
       std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)
       std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)
       return cov_ab / std_a / std_b


   df = pd.DataFrame(np.random.normal(size=(100, 3)))
   df.corr(method=distcorr)

Timedeltas
----------

The :ref:`Timedeltas <timedeltas.timedeltas>` docs.

`Using timedeltas
<https://github.com/pandas-dev/pandas/pull/2899>`__

.. ipython:: python

   import datetime

   s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))

   s - s.max()

   s.max() - s

   s - datetime.datetime(2011, 1, 1, 3, 5)

   s + datetime.timedelta(minutes=5)

   datetime.datetime(2011, 1, 1, 3, 5) - s

   datetime.timedelta(minutes=5) + s

`Adding and subtracting deltas and dates
<https://stackoverflow.com/questions/16385785/add-days-to-dates-in-dataframe>`__

.. ipython:: python

   deltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])

   df = pd.DataFrame({"A": s, "B": deltas})
   df

   df["New Dates"] = df["A"] + df["B"]

   df["Delta"] = df["A"] - df["New Dates"]
   df

   df.dtypes

`Another example
<https://stackoverflow.com/questions/15683588/iterating-through-a-pandas-dataframe>`__

Values can be set to NaT using np.nan, similar to datetime

.. ipython:: python

   y = s - s.shift()
   y

   y[1] = np.nan
   y

Creating example data
---------------------

To create a dataframe from every combination of some given values, like R's ``expand.grid()``
function, we can create a dict where the keys are column names and the values are lists
of the data values:

.. ipython:: python

   def expand_grid(data_dict):
       rows = itertools.product(*data_dict.values())
       return pd.DataFrame.from_records(rows, columns=data_dict.keys())


   df = expand_grid(
       {"height": [60, 70], "weight": [100, 140, 180], "sex": ["Male", "Female"]}
   )
   df
.. _reshaping:

{{ header }}

**************************
Reshaping and pivot tables
**************************

.. _reshaping.reshaping:

Reshaping by pivoting DataFrame objects
---------------------------------------

.. image:: ../_static/reshaping_pivot.png

Data is often stored in so-called "stacked" or "record" format:

.. ipython:: python

   import pandas._testing as tm

   def unpivot(frame):
       N, K = frame.shape
       data = {
           "value": frame.to_numpy().ravel("F"),
           "variable": np.asarray(frame.columns).repeat(N),
           "date": np.tile(np.asarray(frame.index), K),
       }
       return pd.DataFrame(data, columns=["date", "variable", "value"])

   df = unpivot(tm.makeTimeDataFrame(3))
   df

To select out everything for variable ``A`` we could do:

.. ipython:: python

   filtered = df[df["variable"] == "A"]
   filtered

But suppose we wish to do time series operations with the variables. A better
representation would be where the ``columns`` are the unique variables and an
``index`` of dates identifies individual observations. To reshape the data into
this form, we use the :meth:`DataFrame.pivot` method (also implemented as a
top level function :func:`~pandas.pivot`):

.. ipython:: python

   pivoted = df.pivot(index="date", columns="variable", values="value")
   pivoted

If the ``values`` argument is omitted, and the input :class:`DataFrame` has more than
one column of values which are not used as column or index inputs to :meth:`~DataFrame.pivot`,
then the resulting "pivoted" :class:`DataFrame` will have :ref:`hierarchical columns
<advanced.hierarchical>` whose topmost level indicates the respective value
column:

.. ipython:: python

   df["value2"] = df["value"] * 2
   pivoted = df.pivot(index="date", columns="variable")
   pivoted

You can then select subsets from the pivoted :class:`DataFrame`:

.. ipython:: python

   pivoted["value2"]

Note that this returns a view on the underlying data in the case where the data
are homogeneously-typed.

.. note::
   :func:`~pandas.pivot` will error with a ``ValueError: Index contains duplicate
   entries, cannot reshape`` if the index/column pair is not unique. In this
   case, consider using :func:`~pandas.pivot_table` which is a generalization
   of pivot that can handle duplicate values for one index/column pair.

.. _reshaping.stacking:

Reshaping by stacking and unstacking
------------------------------------

.. image:: ../_static/reshaping_stack.png

Closely related to the :meth:`~DataFrame.pivot` method are the related
:meth:`~DataFrame.stack` and :meth:`~DataFrame.unstack` methods available on
:class:`Series` and :class:`DataFrame`. These methods are designed to work together with
:class:`MultiIndex` objects (see the section on :ref:`hierarchical indexing
<advanced.hierarchical>`). Here are essentially what these methods do:

* :meth:`~DataFrame.stack`: "pivot" a level of the (possibly hierarchical) column labels,
  returning a :class:`DataFrame` with an index with a new inner-most level of row
  labels.
* :meth:`~DataFrame.unstack`: (inverse operation of :meth:`~DataFrame.stack`) "pivot" a level of the
  (possibly hierarchical) row index to the column axis, producing a reshaped
  :class:`DataFrame` with a new inner-most level of column labels.

.. image:: ../_static/reshaping_unstack.png

The clearest way to explain is by example. Let's take a prior example data set
from the hierarchical indexing section:

.. ipython:: python

   tuples = list(
       zip(
           *[
               ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
               ["one", "two", "one", "two", "one", "two", "one", "two"],
           ]
       )
   )
   index = pd.MultiIndex.from_tuples(tuples, names=["first", "second"])
   df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=["A", "B"])
   df2 = df[:4]
   df2

The :meth:`~DataFrame.stack` function "compresses" a level in the :class:`DataFrame` columns to
produce either:

* A :class:`Series`, in the case of a simple column Index.
* A :class:`DataFrame`, in the case of a :class:`MultiIndex` in the columns.

If the columns have a :class:`MultiIndex`, you can choose which level to stack. The
stacked level becomes the new lowest level in a :class:`MultiIndex` on the columns:

.. ipython:: python

   stacked = df2.stack()
   stacked

With a "stacked" :class:`DataFrame` or :class:`Series` (having a :class:`MultiIndex` as the
``index``), the inverse operation of :meth:`~DataFrame.stack` is :meth:`~DataFrame.unstack`, which by default
unstacks the **last level**:

.. ipython:: python

   stacked.unstack()
   stacked.unstack(1)
   stacked.unstack(0)

.. _reshaping.unstack_by_name:

.. image:: ../_static/reshaping_unstack_1.png

If the indexes have names, you can use the level names instead of specifying
the level numbers:

.. ipython:: python

   stacked.unstack("second")


.. image:: ../_static/reshaping_unstack_0.png

Notice that the :meth:`~DataFrame.stack` and :meth:`~DataFrame.unstack` methods implicitly sort the index
levels involved. Hence a call to :meth:`~DataFrame.stack` and then :meth:`~DataFrame.unstack`, or vice versa,
will result in a **sorted** copy of the original :class:`DataFrame` or :class:`Series`:

.. ipython:: python

   index = pd.MultiIndex.from_product([[2, 1], ["a", "b"]])
   df = pd.DataFrame(np.random.randn(4), index=index, columns=["A"])
   df
   all(df.unstack().stack() == df.sort_index())

The above code will raise a ``TypeError`` if the call to :meth:`~DataFrame.sort_index` is
removed.

.. _reshaping.stack_multiple:

Multiple levels
~~~~~~~~~~~~~~~

You may also stack or unstack more than one level at a time by passing a list
of levels, in which case the end result is as if each level in the list were
processed individually.

.. ipython:: python

    columns = pd.MultiIndex.from_tuples(
        [
            ("A", "cat", "long"),
            ("B", "cat", "long"),
            ("A", "dog", "short"),
            ("B", "dog", "short"),
        ],
        names=["exp", "animal", "hair_length"],
    )
    df = pd.DataFrame(np.random.randn(4, 4), columns=columns)
    df

    df.stack(level=["animal", "hair_length"])

The list of levels can contain either level names or level numbers (but
not a mixture of the two).

.. ipython:: python

    # df.stack(level=['animal', 'hair_length'])
    # from above is equivalent to:
    df.stack(level=[1, 2])

Missing data
~~~~~~~~~~~~

These functions are intelligent about handling missing data and do not expect
each subgroup within the hierarchical index to have the same set of labels.
They also can handle the index being unsorted (but you can make it sorted by
calling :meth:`~DataFrame.sort_index`, of course). Here is a more complex example:

.. ipython:: python

   columns = pd.MultiIndex.from_tuples(
       [
           ("A", "cat"),
           ("B", "dog"),
           ("B", "cat"),
           ("A", "dog"),
       ],
       names=["exp", "animal"],
   )
   index = pd.MultiIndex.from_product(
       [("bar", "baz", "foo", "qux"), ("one", "two")], names=["first", "second"]
   )
   df = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns)
   df2 = df.iloc[[0, 1, 2, 4, 5, 7]]
   df2

As mentioned above, :meth:`~DataFrame.stack` can be called with a ``level`` argument to select
which level in the columns to stack:

.. ipython:: python

   df2.stack("exp")
   df2.stack("animal")

Unstacking can result in missing values if subgroups do not have the same
set of labels.  By default, missing values will be replaced with the default
fill value for that data type, ``NaN`` for float, ``NaT`` for datetimelike,
etc.  For integer types, by default data will converted to float and missing
values will be set to ``NaN``.

.. ipython:: python

   df3 = df.iloc[[0, 1, 4, 7], [1, 2]]
   df3
   df3.unstack()

Alternatively, unstack takes an optional ``fill_value`` argument, for specifying
the value of missing data.

.. ipython:: python

   df3.unstack(fill_value=-1e9)

With a MultiIndex
~~~~~~~~~~~~~~~~~

Unstacking when the columns are a :class:`MultiIndex` is also careful about doing
the right thing:

.. ipython:: python

   df[:3].unstack(0)
   df2.unstack(1)

.. _reshaping.melt:

Reshaping by melt
-----------------

.. image:: ../_static/reshaping_melt.png

The top-level :func:`~pandas.melt` function and the corresponding :meth:`DataFrame.melt`
are useful to massage a :class:`DataFrame` into a format where one or more columns
are *identifier variables*, while all other columns, considered *measured
variables*, are "unpivoted" to the row axis, leaving just two non-identifier
columns, "variable" and "value". The names of those columns can be customized
by supplying the ``var_name`` and ``value_name`` parameters.

For instance,

.. ipython:: python

   cheese = pd.DataFrame(
       {
           "first": ["John", "Mary"],
           "last": ["Doe", "Bo"],
           "height": [5.5, 6.0],
           "weight": [130, 150],
       }
   )
   cheese
   cheese.melt(id_vars=["first", "last"])
   cheese.melt(id_vars=["first", "last"], var_name="quantity")

When transforming a DataFrame using :func:`~pandas.melt`, the index will be ignored. The original index values can be kept around by setting the ``ignore_index`` parameter to ``False`` (default is ``True``). This will however duplicate them.

.. versionadded:: 1.1.0

.. ipython:: python

   index = pd.MultiIndex.from_tuples([("person", "A"), ("person", "B")])
   cheese = pd.DataFrame(
       {
           "first": ["John", "Mary"],
           "last": ["Doe", "Bo"],
           "height": [5.5, 6.0],
           "weight": [130, 150],
       },
       index=index,
   )
   cheese
   cheese.melt(id_vars=["first", "last"])
   cheese.melt(id_vars=["first", "last"], ignore_index=False)

Another way to transform is to use the :func:`~pandas.wide_to_long` panel data
convenience function. It is less flexible than :func:`~pandas.melt`, but more
user-friendly.

.. ipython:: python

  dft = pd.DataFrame(
      {
          "A1970": {0: "a", 1: "b", 2: "c"},
          "A1980": {0: "d", 1: "e", 2: "f"},
          "B1970": {0: 2.5, 1: 1.2, 2: 0.7},
          "B1980": {0: 3.2, 1: 1.3, 2: 0.1},
          "X": dict(zip(range(3), np.random.randn(3))),
      }
  )
  dft["id"] = dft.index
  dft
  pd.wide_to_long(dft, ["A", "B"], i="id", j="year")

.. _reshaping.combine_with_groupby:

Combining with stats and GroupBy
--------------------------------

It should be no shock that combining :meth:`~DataFrame.pivot` / :meth:`~DataFrame.stack` / :meth:`~DataFrame.unstack` with
GroupBy and the basic Series and DataFrame statistical functions can produce
some very expressive and fast data manipulations.

.. ipython:: python

   df
   df.stack().mean(1).unstack()

   # same result, another way
   df.groupby(level=1, axis=1).mean()

   df.stack().groupby(level=1).mean()

   df.mean().unstack(0)


Pivot tables
------------

.. _reshaping.pivot:

While :meth:`~DataFrame.pivot` provides general purpose pivoting with various
data types (strings, numerics, etc.), pandas also provides :func:`~pandas.pivot_table`
for pivoting with aggregation of numeric data.

The function :func:`~pandas.pivot_table` can be used to create spreadsheet-style
pivot tables. See the :ref:`cookbook<cookbook.pivot>` for some advanced
strategies.

It takes a number of arguments:

* ``data``: a DataFrame object.
* ``values``: a column or a list of columns to aggregate.
* ``index``: a column, Grouper, array which has the same length as data, or list of them.
  Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.
* ``columns``: a column, Grouper, array which has the same length as data, or list of them.
  Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.
* ``aggfunc``: function to use for aggregation, defaulting to ``numpy.mean``.

Consider a data set like this:

.. ipython:: python

   import datetime

   df = pd.DataFrame(
       {
           "A": ["one", "one", "two", "three"] * 6,
           "B": ["A", "B", "C"] * 8,
           "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 4,
           "D": np.random.randn(24),
           "E": np.random.randn(24),
           "F": [datetime.datetime(2013, i, 1) for i in range(1, 13)]
           + [datetime.datetime(2013, i, 15) for i in range(1, 13)],
       }
   )
   df

We can produce pivot tables from this data very easily:

.. ipython:: python

   pd.pivot_table(df, values="D", index=["A", "B"], columns=["C"])
   pd.pivot_table(df, values="D", index=["B"], columns=["A", "C"], aggfunc=np.sum)
   pd.pivot_table(
       df, values=["D", "E"],
       index=["B"],
       columns=["A", "C"],
       aggfunc=np.sum,
   )

The result object is a :class:`DataFrame` having potentially hierarchical indexes on the
rows and columns. If the ``values`` column name is not given, the pivot table
will include all of the data that can be aggregated in an additional level of
hierarchy in the columns:

.. ipython:: python

   pd.pivot_table(df, index=["A", "B"], columns=["C"])

Also, you can use :class:`Grouper` for ``index`` and ``columns`` keywords. For detail of :class:`Grouper`, see :ref:`Grouping with a Grouper specification <groupby.specify>`.

.. ipython:: python

   pd.pivot_table(df, values="D", index=pd.Grouper(freq="M", key="F"), columns="C")

You can render a nice output of the table omitting the missing values by
calling :meth:`~DataFrame.to_string` if you wish:

.. ipython:: python

   table = pd.pivot_table(df, index=["A", "B"], columns=["C"])
   print(table.to_string(na_rep=""))

Note that :meth:`~DataFrame.pivot_table` is also available as an instance method on DataFrame,
 i.e. :meth:`DataFrame.pivot_table`.

.. _reshaping.pivot.margins:

Adding margins
~~~~~~~~~~~~~~

If you pass ``margins=True`` to :meth:`~DataFrame.pivot_table`, special ``All`` columns and
rows will be added with partial group aggregates across the categories on the
rows and columns:

.. ipython:: python

   table = df.pivot_table(index=["A", "B"], columns="C", margins=True, aggfunc=np.std)
   table

Additionally, you can call :meth:`DataFrame.stack` to display a pivoted DataFrame
as having a multi-level index:

.. ipython:: python

    table.stack()

.. _reshaping.crosstabulations:

Cross tabulations
-----------------

Use :func:`~pandas.crosstab` to compute a cross-tabulation of two (or more)
factors. By default :func:`~pandas.crosstab` computes a frequency table of the factors
unless an array of values and an aggregation function are passed.

It takes a number of arguments

* ``index``: array-like, values to group by in the rows.
* ``columns``: array-like, values to group by in the columns.
* ``values``: array-like, optional, array of values to aggregate according to
  the factors.
* ``aggfunc``: function, optional, If no values array is passed, computes a
  frequency table.
* ``rownames``: sequence, default ``None``, must match number of row arrays passed.
* ``colnames``: sequence, default ``None``, if passed, must match number of column
  arrays passed.
* ``margins``: boolean, default ``False``, Add row/column margins (subtotals)
* ``normalize``: boolean, {'all', 'index', 'columns'}, or {0,1}, default ``False``.
  Normalize by dividing all values by the sum of values.


Any :class:`Series` passed will have their name attributes used unless row or column
names for the cross-tabulation are specified

For example:

.. ipython:: python

    foo, bar, dull, shiny, one, two = "foo", "bar", "dull", "shiny", "one", "two"
    a = np.array([foo, foo, bar, bar, foo, foo], dtype=object)
    b = np.array([one, one, two, one, two, one], dtype=object)
    c = np.array([dull, dull, shiny, dull, dull, shiny], dtype=object)
    pd.crosstab(a, [b, c], rownames=["a"], colnames=["b", "c"])


If :func:`~pandas.crosstab` receives only two Series, it will provide a frequency table.

.. ipython:: python

    df = pd.DataFrame(
        {"A": [1, 2, 2, 2, 2], "B": [3, 3, 4, 4, 4], "C": [1, 1, np.nan, 1, 1]}
    )
    df

    pd.crosstab(df["A"], df["B"])

:func:`~pandas.crosstab` can also be implemented
to :class:`Categorical` data.

.. ipython:: python

    foo = pd.Categorical(["a", "b"], categories=["a", "b", "c"])
    bar = pd.Categorical(["d", "e"], categories=["d", "e", "f"])
    pd.crosstab(foo, bar)

If you want to include **all** of data categories even if the actual data does
not contain any instances of a particular category, you should set ``dropna=False``.

For example:

.. ipython:: python

    pd.crosstab(foo, bar, dropna=False)

Normalization
~~~~~~~~~~~~~

Frequency tables can also be normalized to show percentages rather than counts
using the ``normalize`` argument:

.. ipython:: python

   pd.crosstab(df["A"], df["B"], normalize=True)

``normalize`` can also normalize values within each row or within each column:

.. ipython:: python

   pd.crosstab(df["A"], df["B"], normalize="columns")

:func:`~pandas.crosstab` can also be passed a third :class:`Series` and an aggregation function
(``aggfunc``) that will be applied to the values of the third :class:`Series` within
each group defined by the first two :class:`Series`:

.. ipython:: python

   pd.crosstab(df["A"], df["B"], values=df["C"], aggfunc=np.sum)

Adding margins
~~~~~~~~~~~~~~

Finally, one can also add margins or normalize this output.

.. ipython:: python

   pd.crosstab(
       df["A"], df["B"], values=df["C"], aggfunc=np.sum, normalize=True, margins=True
   )

.. _reshaping.tile:
.. _reshaping.tile.cut:

Tiling
------

The :func:`~pandas.cut` function computes groupings for the values of the input
array and is often used to transform continuous variables to discrete or
categorical variables:

.. ipython:: python

   ages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60])

   pd.cut(ages, bins=3)

If the ``bins`` keyword is an integer, then equal-width bins are formed.
Alternatively we can specify custom bin-edges:

.. ipython:: python

   c = pd.cut(ages, bins=[0, 18, 35, 70])
   c

If the ``bins`` keyword is an :class:`IntervalIndex`, then these will be
used to bin the passed data.::

   pd.cut([25, 20, 50], bins=c.categories)


.. _reshaping.dummies:

Computing indicator / dummy variables
-------------------------------------

To convert a categorical variable into a "dummy" or "indicator" :class:`DataFrame`,
for example a column in a :class:`DataFrame` (a :class:`Series`) which has ``k`` distinct
values, can derive a :class:`DataFrame` containing ``k`` columns of 1s and 0s using
:func:`~pandas.get_dummies`:

.. ipython:: python

   df = pd.DataFrame({"key": list("bbacab"), "data1": range(6)})

   pd.get_dummies(df["key"])

Sometimes it's useful to prefix the column names, for example when merging the result
with the original :class:`DataFrame`:

.. ipython:: python

   dummies = pd.get_dummies(df["key"], prefix="key")
   dummies

   df[["data1"]].join(dummies)

This function is often used along with discretization functions like :func:`~pandas.cut`:

.. ipython:: python

   values = np.random.randn(10)
   values

   bins = [0, 0.2, 0.4, 0.6, 0.8, 1]

   pd.get_dummies(pd.cut(values, bins))

See also :func:`Series.str.get_dummies <pandas.Series.str.get_dummies>`.

:func:`get_dummies` also accepts a :class:`DataFrame`. By default all categorical
variables (categorical in the statistical sense, those with ``object`` or
``categorical`` dtype) are encoded as dummy variables.


.. ipython:: python

    df = pd.DataFrame({"A": ["a", "b", "a"], "B": ["c", "c", "b"], "C": [1, 2, 3]})
    pd.get_dummies(df)

All non-object columns are included untouched in the output. You can control
the columns that are encoded with the ``columns`` keyword.

.. ipython:: python

    pd.get_dummies(df, columns=["A"])

Notice that the ``B`` column is still included in the output, it just hasn't
been encoded. You can drop ``B`` before calling ``get_dummies`` if you don't
want to include it in the output.

As with the :class:`Series` version, you can pass values for the ``prefix`` and
``prefix_sep``. By default the column name is used as the prefix, and ``_`` as
the prefix separator. You can specify ``prefix`` and ``prefix_sep`` in 3 ways:

* string: Use the same value for ``prefix`` or ``prefix_sep`` for each column
  to be encoded.
* list: Must be the same length as the number of columns being encoded.
* dict: Mapping column name to prefix.

.. ipython:: python

    simple = pd.get_dummies(df, prefix="new_prefix")
    simple
    from_list = pd.get_dummies(df, prefix=["from_A", "from_B"])
    from_list
    from_dict = pd.get_dummies(df, prefix={"B": "from_B", "A": "from_A"})
    from_dict

Sometimes it will be useful to only keep k-1 levels of a categorical
variable to avoid collinearity when feeding the result to statistical models.
You can switch to this mode by turn on ``drop_first``.

.. ipython:: python

    s = pd.Series(list("abcaa"))

    pd.get_dummies(s)

    pd.get_dummies(s, drop_first=True)

When a column contains only one level, it will be omitted in the result.

.. ipython:: python

    df = pd.DataFrame({"A": list("aaaaa"), "B": list("ababc")})

    pd.get_dummies(df)

    pd.get_dummies(df, drop_first=True)

By default new columns will have ``np.uint8`` dtype.
To choose another dtype, use the ``dtype`` argument:

.. ipython:: python

    df = pd.DataFrame({"A": list("abc"), "B": [1.1, 2.2, 3.3]})

    pd.get_dummies(df, dtype=bool).dtypes


.. _reshaping.factorize:

Factorizing values
------------------

To encode 1-d values as an enumerated type use :func:`~pandas.factorize`:

.. ipython:: python

   x = pd.Series(["A", "A", np.nan, "B", 3.14, np.inf])
   x
   labels, uniques = pd.factorize(x)
   labels
   uniques

Note that :func:`~pandas.factorize` is similar to ``numpy.unique``, but differs in its
handling of NaN:

.. note::
   The following ``numpy.unique`` will fail under Python 3 with a ``TypeError``
   because of an ordering bug. See also
   `here <https://github.com/numpy/numpy/issues/641>`__.

.. ipython:: python
   :okexcept:

   ser = pd.Series(['A', 'A', np.nan, 'B', 3.14, np.inf])
   pd.factorize(ser, sort=True)
   np.unique(ser, return_inverse=True)[::-1]

.. note::
    If you just want to handle one column as a categorical variable (like R's factor),
    you can use  ``df["cat_col"] = pd.Categorical(df["col"])`` or
    ``df["cat_col"] = df["col"].astype("category")``. For full docs on :class:`~pandas.Categorical`,
    see the :ref:`Categorical introduction <categorical>` and the
    :ref:`API documentation <api.arrays.categorical>`.

Examples
--------

In this section, we will review frequently asked questions and examples. The
column names and relevant column values are named to correspond with how this
DataFrame will be pivoted in the answers below.

.. ipython:: python

   np.random.seed([3, 1415])
   n = 20

   cols = np.array(["key", "row", "item", "col"])
   df = cols + pd.DataFrame(
       (np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str)
   )
   df.columns = cols
   df = df.join(pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix("val"))

   df

Pivoting with single aggregations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose we wanted to pivot ``df`` such that the ``col`` values are columns,
``row`` values are the index, and the mean of ``val0`` are the values? In
particular, the resulting DataFrame should look like:

.. code-block:: text

    col   col0   col1   col2   col3  col4
    row
    row0  0.77  0.605    NaN  0.860  0.65
    row2  0.13    NaN  0.395  0.500  0.25
    row3   NaN  0.310    NaN  0.545   NaN
    row4   NaN  0.100  0.395  0.760  0.24

This solution uses :func:`~pandas.pivot_table`. Also note that
``aggfunc='mean'`` is the default. It is included here to be explicit.

.. ipython:: python

   df.pivot_table(values="val0", index="row", columns="col", aggfunc="mean")

Note that we can also replace the missing values by using the ``fill_value``
parameter.

.. ipython:: python

   df.pivot_table(
       values="val0",
       index="row",
       columns="col",
       aggfunc="mean",
       fill_value=0,
   )

Also note that we can pass in other aggregation functions as well. For example,
we can also pass in ``sum``.

.. ipython:: python

   df.pivot_table(
       values="val0",
       index="row",
       columns="col",
       aggfunc="sum",
       fill_value=0,
   )

Another aggregation we can do is calculate the frequency in which the columns
and rows occur together a.k.a. "cross tabulation". To do this, we can pass
``size`` to the ``aggfunc`` parameter.

.. ipython:: python

   df.pivot_table(index="row", columns="col", fill_value=0, aggfunc="size")

Pivoting with multiple aggregations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can also perform multiple aggregations. For example, to perform both a
``sum`` and ``mean``, we can pass in a list to the ``aggfunc`` argument.

.. ipython:: python

   df.pivot_table(
       values="val0",
       index="row",
       columns="col",
       aggfunc=["mean", "sum"],
   )

Note to aggregate over multiple value columns, we can pass in a list to the
``values`` parameter.

.. ipython:: python

   df.pivot_table(
       values=["val0", "val1"],
       index="row",
       columns="col",
       aggfunc=["mean"],
   )

Note to subdivide over multiple columns we can pass in a list to the
``columns`` parameter.

.. ipython:: python

   df.pivot_table(
       values=["val0"],
       index="row",
       columns=["item", "col"],
       aggfunc=["mean"],
   )

.. _reshaping.explode:

Exploding a list-like column
----------------------------

.. versionadded:: 0.25.0

Sometimes the values in a column are list-like.

.. ipython:: python

   keys = ["panda1", "panda2", "panda3"]
   values = [["eats", "shoots"], ["shoots", "leaves"], ["eats", "leaves"]]
   df = pd.DataFrame({"keys": keys, "values": values})
   df

We can 'explode' the ``values`` column, transforming each list-like to a separate row, by using :meth:`~Series.explode`. This will replicate the index values from the original row:

.. ipython:: python

   df["values"].explode()

You can also explode the column in the :class:`DataFrame`.

.. ipython:: python

   df.explode("values")

:meth:`Series.explode` will replace empty lists with ``np.nan`` and preserve scalar entries. The dtype of the resulting :class:`Series` is always ``object``.

.. ipython:: python

   s = pd.Series([[1, 2, 3], "foo", [], ["a", "b"]])
   s
   s.explode()

Here is a typical usecase. You have comma separated strings in a column and want to expand this.

.. ipython:: python

    df = pd.DataFrame([{"var1": "a,b,c", "var2": 1}, {"var1": "d,e,f", "var2": 2}])
    df

Creating a long form DataFrame is now straightforward using explode and chained operations

.. ipython:: python

   df.assign(var1=df.var1.str.split(",")).explode("var1")
.. _indexing:

{{ header }}

***************************
Indexing and selecting data
***************************

The axis labeling information in pandas objects serves many purposes:

* Identifies data (i.e. provides *metadata*) using known indicators,
  important for analysis, visualization, and interactive console display.
* Enables automatic and explicit data alignment.
* Allows intuitive getting and setting of subsets of the data set.

In this section, we will focus on the final point: namely, how to slice, dice,
and generally get and set subsets of pandas objects. The primary focus will be
on Series and DataFrame as they have received more development attention in
this area.

.. note::

   The Python and NumPy indexing operators ``[]`` and attribute operator ``.``
   provide quick and easy access to pandas data structures across a wide range
   of use cases. This makes interactive work intuitive, as there's little new
   to learn if you already know how to deal with Python dictionaries and NumPy
   arrays. However, since the type of the data to be accessed isn't known in
   advance, directly using standard operators has some optimization limits. For
   production code, we recommended that you take advantage of the optimized
   pandas data access methods exposed in this chapter.

.. warning::

   Whether a copy or a reference is returned for a setting operation, may
   depend on the context. This is sometimes called ``chained assignment`` and
   should be avoided. See :ref:`Returning a View versus Copy
   <indexing.view_versus_copy>`.

See the :ref:`MultiIndex / Advanced Indexing <advanced>` for ``MultiIndex`` and more advanced indexing documentation.

See the :ref:`cookbook<cookbook.selection>` for some advanced strategies.

.. _indexing.choice:

Different choices for indexing
------------------------------

Object selection has had a number of user-requested additions in order to
support more explicit location based indexing. pandas now supports three types
of multi-axis indexing.

* ``.loc`` is primarily label based, but may also be used with a boolean array. ``.loc`` will raise ``KeyError`` when the items are not found. Allowed inputs are:

    * A single label, e.g. ``5`` or ``'a'`` (Note that ``5`` is interpreted as a
      *label* of the index. This use is **not** an integer position along the
      index.).
    * A list or array of labels ``['a', 'b', 'c']``.
    * A slice object with labels ``'a':'f'`` (Note that contrary to usual Python
      slices, **both** the start and the stop are included, when present in the
      index! See :ref:`Slicing with labels <indexing.slicing_with_labels>`
      and :ref:`Endpoints are inclusive <advanced.endpoints_are_inclusive>`.)
    * A boolean array (any ``NA`` values will be treated as ``False``).
    * A ``callable`` function with one argument (the calling Series or DataFrame) and
      that returns valid output for indexing (one of the above).

  See more at :ref:`Selection by Label <indexing.label>`.

* ``.iloc`` is primarily integer position based (from ``0`` to
  ``length-1`` of the axis), but may also be used with a boolean
  array.  ``.iloc`` will raise ``IndexError`` if a requested
  indexer is out-of-bounds, except *slice* indexers which allow
  out-of-bounds indexing.  (this conforms with Python/NumPy *slice*
  semantics).  Allowed inputs are:

    * An integer e.g. ``5``.
    * A list or array of integers ``[4, 3, 0]``.
    * A slice object with ints ``1:7``.
    * A boolean array (any ``NA`` values will be treated as ``False``).
    * A ``callable`` function with one argument (the calling Series or DataFrame) and
      that returns valid output for indexing (one of the above).

  See more at :ref:`Selection by Position <indexing.integer>`,
  :ref:`Advanced Indexing <advanced>` and :ref:`Advanced
  Hierarchical <advanced.advanced_hierarchical>`.

* ``.loc``, ``.iloc``, and also ``[]`` indexing can accept a ``callable`` as indexer. See more at :ref:`Selection By Callable <indexing.callable>`.

Getting values from an object with multi-axes selection uses the following
notation (using ``.loc`` as an example, but the following applies to ``.iloc`` as
well). Any of the axes accessors may be the null slice ``:``. Axes left out of
the specification are assumed to be ``:``, e.g. ``p.loc['a']`` is equivalent to
``p.loc['a', :, :]``.

.. csv-table::
    :header: "Object Type", "Indexers"
    :widths: 30, 50
    :delim: ;

    Series; ``s.loc[indexer]``
    DataFrame; ``df.loc[row_indexer,column_indexer]``

.. _indexing.basics:

Basics
------

As mentioned when introducing the data structures in the :ref:`last section
<basics>`, the primary function of indexing with ``[]`` (a.k.a. ``__getitem__``
for those familiar with implementing class behavior in Python) is selecting out
lower-dimensional slices. The following table shows return type values when
indexing pandas objects with ``[]``:

.. csv-table::
    :header: "Object Type", "Selection", "Return Value Type"
    :widths: 30, 30, 60
    :delim: ;

    Series; ``series[label]``; scalar value
    DataFrame; ``frame[colname]``; ``Series`` corresponding to colname

Here we construct a simple time series data set to use for illustrating the
indexing functionality:

.. ipython:: python

   dates = pd.date_range('1/1/2000', periods=8)
   df = pd.DataFrame(np.random.randn(8, 4),
                     index=dates, columns=['A', 'B', 'C', 'D'])
   df

.. note::

   None of the indexing functionality is time series specific unless
   specifically stated.

Thus, as per above, we have the most basic indexing using ``[]``:

.. ipython:: python

   s = df['A']
   s[dates[5]]

You can pass a list of columns to ``[]`` to select columns in that order.
If a column is not contained in the DataFrame, an exception will be
raised. Multiple columns can also be set in this manner:

.. ipython:: python

   df
   df[['B', 'A']] = df[['A', 'B']]
   df

You may find this useful for applying a transform (in-place) to a subset of the
columns.

.. warning::

   pandas aligns all AXES when setting ``Series`` and ``DataFrame`` from ``.loc``, and ``.iloc``.

   This will **not** modify ``df`` because the column alignment is before value assignment.

   .. ipython:: python

      df[['A', 'B']]
      df.loc[:, ['B', 'A']] = df[['A', 'B']]
      df[['A', 'B']]

   The correct way to swap column values is by using raw values:

   .. ipython:: python

      df.loc[:, ['B', 'A']] = df[['A', 'B']].to_numpy()
      df[['A', 'B']]


Attribute access
----------------

.. _indexing.columns.multiple:

.. _indexing.df_cols:

.. _indexing.attribute_access:

You may access an index on a ``Series`` or  column on a ``DataFrame`` directly
as an attribute:

.. ipython:: python

   sa = pd.Series([1, 2, 3], index=list('abc'))
   dfa = df.copy()

.. ipython:: python

   sa.b
   dfa.A

.. ipython:: python

   sa.a = 5
   sa
   dfa.A = list(range(len(dfa.index)))  # ok if A already exists
   dfa
   dfa['A'] = list(range(len(dfa.index)))  # use this form to create a new column
   dfa

.. warning::

   - You can use this access only if the index element is a valid Python identifier, e.g. ``s.1`` is not allowed.
     See `here for an explanation of valid identifiers
     <https://docs.python.org/3/reference/lexical_analysis.html#identifiers>`__.

   - The attribute will not be available if it conflicts with an existing method name, e.g. ``s.min`` is not allowed, but ``s['min']`` is possible.

   - Similarly, the attribute will not be available if it conflicts with any of the following list: ``index``,
     ``major_axis``, ``minor_axis``, ``items``.

   - In any of these cases, standard indexing will still work, e.g. ``s['1']``, ``s['min']``, and ``s['index']`` will
     access the corresponding element or column.

If you are using the IPython environment, you may also use tab-completion to
see these accessible attributes.

You can also assign a ``dict`` to a row of a ``DataFrame``:

.. ipython:: python

   x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})
   x.iloc[1] = {'x': 9, 'y': 99}
   x

You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful;
if you try to use attribute access to create a new column, it creates a new attribute rather than a
new column. In 0.21.0 and later, this will raise a ``UserWarning``:

.. code-block:: ipython

    In [1]: df = pd.DataFrame({'one': [1., 2., 3.]})
    In [2]: df.two = [4, 5, 6]
    UserWarning: Pandas doesn't allow Series to be assigned into nonexistent columns - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute_access
    In [3]: df
    Out[3]:
       one
    0  1.0
    1  2.0
    2  3.0

Slicing ranges
--------------

The most robust and consistent way of slicing ranges along arbitrary axes is
described in the :ref:`Selection by Position <indexing.integer>` section
detailing the ``.iloc`` method. For now, we explain the semantics of slicing using the ``[]`` operator.

With Series, the syntax works exactly as with an ndarray, returning a slice of
the values and the corresponding labels:

.. ipython:: python

   s[:5]
   s[::2]
   s[::-1]

Note that setting works as well:

.. ipython:: python

   s2 = s.copy()
   s2[:5] = 0
   s2

With DataFrame, slicing inside of ``[]`` **slices the rows**. This is provided
largely as a convenience since it is such a common operation.

.. ipython:: python

   df[:3]
   df[::-1]

.. _indexing.label:

Selection by label
------------------

.. warning::

   Whether a copy or a reference is returned for a setting operation, may depend on the context.
   This is sometimes called ``chained assignment`` and should be avoided.
   See :ref:`Returning a View versus Copy <indexing.view_versus_copy>`.

.. warning::

   ``.loc`` is strict when you present slicers that are not compatible (or convertible) with the index type. For example
   using integers in a ``DatetimeIndex``. These will raise a ``TypeError``.

  .. ipython:: python

     dfl = pd.DataFrame(np.random.randn(5, 4),
                        columns=list('ABCD'),
                        index=pd.date_range('20130101', periods=5))
     dfl

  .. code-block:: ipython

     In [4]: dfl.loc[2:3]
     TypeError: cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with these indexers [2] of <type 'int'>

  String likes in slicing *can* be convertible to the type of the index and lead to natural slicing.

  .. ipython:: python

     dfl.loc['20130102':'20130104']

.. warning::

   .. versionchanged:: 1.0.0

   pandas will raise a ``KeyError`` if indexing with a list with missing labels. See :ref:`list-like Using loc with
   missing keys in a list is Deprecated <indexing.deprecate_loc_reindex_listlike>`.

pandas provides a suite of methods in order to have **purely label based indexing**. This is a strict inclusion based protocol.
Every label asked for must be in the index, or a ``KeyError`` will be raised.
When slicing, both the start bound **AND** the stop bound are *included*, if present in the index.
Integers are valid labels, but they refer to the label **and not the position**.

The ``.loc`` attribute is the primary access method. The following are valid inputs:

* A single label, e.g. ``5`` or ``'a'`` (Note that ``5`` is interpreted as a *label* of the index. This use is **not** an integer position along the index.).
* A list or array of labels ``['a', 'b', 'c']``.
* A slice object with labels ``'a':'f'`` (Note that contrary to usual Python
  slices, **both** the start and the stop are included, when present in the
  index! See :ref:`Slicing with labels <indexing.slicing_with_labels>`.
* A boolean array.
* A ``callable``, see :ref:`Selection By Callable <indexing.callable>`.

.. ipython:: python

   s1 = pd.Series(np.random.randn(6), index=list('abcdef'))
   s1
   s1.loc['c':]
   s1.loc['b']

Note that setting works as well:

.. ipython:: python

   s1.loc['c':] = 0
   s1

With a DataFrame:

.. ipython:: python

   df1 = pd.DataFrame(np.random.randn(6, 4),
                      index=list('abcdef'),
                      columns=list('ABCD'))
   df1
   df1.loc[['a', 'b', 'd'], :]

Accessing via label slices:

.. ipython:: python

   df1.loc['d':, 'A':'C']

For getting a cross section using a label (equivalent to ``df.xs('a')``):

.. ipython:: python

   df1.loc['a']

For getting values with a boolean array:

.. ipython:: python

   df1.loc['a'] > 0
   df1.loc[:, df1.loc['a'] > 0]

NA values in a boolean array propagate as ``False``:

.. versionchanged:: 1.0.2

.. ipython:: python

   mask = pd.array([True, False, True, False, pd.NA, False], dtype="boolean")
   mask
   df1[mask]

For getting a value explicitly:

.. ipython:: python

   # this is also equivalent to ``df1.at['a','A']``
   df1.loc['a', 'A']

.. _indexing.slicing_with_labels:

Slicing with labels
~~~~~~~~~~~~~~~~~~~

When using ``.loc`` with slices, if both the start and the stop labels are
present in the index, then elements *located* between the two (including them)
are returned:

.. ipython:: python

   s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])
   s.loc[3:5]

If at least one of the two is absent, but the index is sorted, and can be
compared against start and stop labels, then slicing will still work as
expected, by selecting labels which *rank* between the two:

.. ipython:: python

   s.sort_index()
   s.sort_index().loc[1:6]

However, if at least one of the two is absent *and* the index is not sorted, an
error will be raised (since doing otherwise would be computationally expensive,
as well as potentially ambiguous for mixed type indexes). For instance, in the
above example, ``s.loc[1:6]`` would raise ``KeyError``.

For the rationale behind this behavior, see
:ref:`Endpoints are inclusive <advanced.endpoints_are_inclusive>`.

.. ipython:: python

   s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])
   s.loc[3:5]

Also, if the index has duplicate labels *and* either the start or the stop label is duplicated,
an error will be raised. For instance, in the above example, ``s.loc[2:5]`` would raise a ``KeyError``.

For more information about duplicate labels, see
:ref:`Duplicate Labels <duplicates>`.

.. _indexing.integer:

Selection by position
---------------------

.. warning::

   Whether a copy or a reference is returned for a setting operation, may depend on the context.
   This is sometimes called ``chained assignment`` and should be avoided.
   See :ref:`Returning a View versus Copy <indexing.view_versus_copy>`.

pandas provides a suite of methods in order to get **purely integer based indexing**. The semantics follow closely Python and NumPy slicing. These are ``0-based`` indexing. When slicing, the start bound is *included*, while the upper bound is *excluded*. Trying to use a non-integer, even a **valid** label will raise an ``IndexError``.

The ``.iloc`` attribute is the primary access method. The following are valid inputs:

* An integer e.g. ``5``.
* A list or array of integers ``[4, 3, 0]``.
* A slice object with ints ``1:7``.
* A boolean array.
* A ``callable``, see :ref:`Selection By Callable <indexing.callable>`.

.. ipython:: python

   s1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2)))
   s1
   s1.iloc[:3]
   s1.iloc[3]

Note that setting works as well:

.. ipython:: python

   s1.iloc[:3] = 0
   s1

With a DataFrame:

.. ipython:: python

   df1 = pd.DataFrame(np.random.randn(6, 4),
                      index=list(range(0, 12, 2)),
                      columns=list(range(0, 8, 2)))
   df1

Select via integer slicing:

.. ipython:: python

   df1.iloc[:3]
   df1.iloc[1:5, 2:4]

Select via integer list:

.. ipython:: python

   df1.iloc[[1, 3, 5], [1, 3]]

.. ipython:: python

   df1.iloc[1:3, :]

.. ipython:: python

   df1.iloc[:, 1:3]

.. ipython:: python

   # this is also equivalent to ``df1.iat[1,1]``
   df1.iloc[1, 1]

For getting a cross section using an integer position (equiv to ``df.xs(1)``):

.. ipython:: python

   df1.iloc[1]

Out of range slice indexes are handled gracefully just as in Python/NumPy.

.. ipython:: python

    # these are allowed in Python/NumPy.
    x = list('abcdef')
    x
    x[4:10]
    x[8:10]
    s = pd.Series(x)
    s
    s.iloc[4:10]
    s.iloc[8:10]

Note that using slices that go out of bounds can result in
an empty axis (e.g. an empty DataFrame being returned).

.. ipython:: python

   dfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))
   dfl
   dfl.iloc[:, 2:3]
   dfl.iloc[:, 1:3]
   dfl.iloc[4:6]

A single indexer that is out of bounds will raise an ``IndexError``.
A list of indexers where any element is out of bounds will raise an
``IndexError``.

.. code-block:: python

   >>> dfl.iloc[[4, 5, 6]]
   IndexError: positional indexers are out-of-bounds

   >>> dfl.iloc[:, 4]
   IndexError: single positional indexer is out-of-bounds

.. _indexing.callable:

Selection by callable
---------------------

``.loc``, ``.iloc``, and also ``[]`` indexing can accept a ``callable`` as indexer.
The ``callable`` must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing.

.. ipython:: python

   df1 = pd.DataFrame(np.random.randn(6, 4),
                      index=list('abcdef'),
                      columns=list('ABCD'))
   df1

   df1.loc[lambda df: df['A'] > 0, :]
   df1.loc[:, lambda df: ['A', 'B']]

   df1.iloc[:, lambda df: [0, 1]]

   df1[lambda df: df.columns[0]]


You can use callable indexing in ``Series``.

.. ipython:: python

   df1['A'].loc[lambda s: s > 0]

Using these methods / indexers, you can chain data selection operations
without using a temporary variable.

.. ipython:: python

   bb = pd.read_csv('data/baseball.csv', index_col='id')
   (bb.groupby(['year', 'team']).sum()
      .loc[lambda df: df['r'] > 100])


.. _combining_positional_and_label_based_indexing:

Combining positional and label-based indexing
---------------------------------------------

If you wish to get the 0th and the 2nd elements from the index in the 'A' column, you can do:

.. ipython:: python

  dfd = pd.DataFrame({'A': [1, 2, 3],
                      'B': [4, 5, 6]},
                     index=list('abc'))
  dfd
  dfd.loc[dfd.index[[0, 2]], 'A']

This can also be expressed using ``.iloc``, by explicitly getting locations on the indexers, and using
*positional* indexing to select things.

.. ipython:: python

  dfd.iloc[[0, 2], dfd.columns.get_loc('A')]

For getting *multiple* indexers, using ``.get_indexer``:

.. ipython:: python

  dfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])]


.. _deprecate_loc_reindex_listlike:
.. _indexing.deprecate_loc_reindex_listlike:

Indexing with list with missing labels is deprecated
----------------------------------------------------

.. warning::

   .. versionchanged:: 1.0.0

   Using ``.loc`` or ``[]`` with a list with one or more missing labels will no longer reindex, in favor of ``.reindex``.

In prior versions, using ``.loc[list-of-labels]`` would work as long as *at least 1* of the keys was found (otherwise it
would raise a ``KeyError``). This behavior was changed and will now raise a ``KeyError`` if at least one label is missing.
The recommended alternative is to use ``.reindex()``.

For example.

.. ipython:: python

   s = pd.Series([1, 2, 3])
   s

Selection with all keys found is unchanged.

.. ipython:: python

   s.loc[[1, 2]]

Previous behavior

.. code-block:: ipython

   In [4]: s.loc[[1, 2, 3]]
   Out[4]:
   1    2.0
   2    3.0
   3    NaN
   dtype: float64


Current behavior

.. code-block:: ipython

   In [4]: s.loc[[1, 2, 3]]
   Passing list-likes to .loc with any non-matching elements will raise
   KeyError in the future, you can use .reindex() as an alternative.

   See the documentation here:
   https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike

   Out[4]:
   1    2.0
   2    3.0
   3    NaN
   dtype: float64


Reindexing
~~~~~~~~~~

The idiomatic way to achieve selecting potentially not-found elements is via ``.reindex()``. See also the section on :ref:`reindexing <basics.reindexing>`.

.. ipython:: python

  s.reindex([1, 2, 3])

Alternatively, if you want to select only *valid* keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection.

.. ipython:: python

   labels = [1, 2, 3]
   s.loc[s.index.intersection(labels)]

Having a duplicated index will raise for a ``.reindex()``:

.. ipython:: python

   s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])
   labels = ['c', 'd']

.. code-block:: ipython

   In [17]: s.reindex(labels)
   ValueError: cannot reindex on an axis with duplicate labels

Generally, you can intersect the desired labels with the current
axis, and then reindex.

.. ipython:: python

   s.loc[s.index.intersection(labels)].reindex(labels)

However, this would *still* raise if your resulting index is duplicated.

.. code-block:: ipython

   In [41]: labels = ['a', 'd']

   In [42]: s.loc[s.index.intersection(labels)].reindex(labels)
   ValueError: cannot reindex on an axis with duplicate labels


.. _indexing.basics.partial_setting:

Selecting random samples
------------------------

A random selection of rows or columns from a Series or DataFrame with the :meth:`~DataFrame.sample` method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows.

.. ipython:: python

    s = pd.Series([0, 1, 2, 3, 4, 5])

    # When no arguments are passed, returns 1 row.
    s.sample()

    # One may specify either a number of rows:
    s.sample(n=3)

    # Or a fraction of the rows:
    s.sample(frac=0.5)

By default, ``sample`` will return each row at most once, but one can also sample with replacement
using the ``replace`` option:

.. ipython:: python

    s = pd.Series([0, 1, 2, 3, 4, 5])

    # Without replacement (default):
    s.sample(n=6, replace=False)

    # With replacement:
    s.sample(n=6, replace=True)


By default, each row has an equal probability of being selected, but if you want rows
to have different probabilities, you can pass the ``sample`` function sampling weights as
``weights``. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example:

.. ipython:: python

    s = pd.Series([0, 1, 2, 3, 4, 5])
    example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]
    s.sample(n=3, weights=example_weights)

    # Weights will be re-normalized automatically
    example_weights2 = [0.5, 0, 0, 0, 0, 0]
    s.sample(n=1, weights=example_weights2)

When applied to a DataFrame, you can use a column of the DataFrame as sampling weights
(provided you are sampling rows and not columns) by simply passing the name of the column
as a string.

.. ipython:: python

    df2 = pd.DataFrame({'col1': [9, 8, 7, 6],
                        'weight_column': [0.5, 0.4, 0.1, 0]})
    df2.sample(n=3, weights='weight_column')

``sample`` also allows users to sample columns instead of rows using the ``axis`` argument.

.. ipython:: python

    df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})
    df3.sample(n=1, axis=1)

Finally, one can also set a seed for ``sample``'s random number generator using the ``random_state`` argument, which will accept either an integer (as a seed) or a NumPy RandomState object.

.. ipython:: python

    df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})

    # With a given seed, the sample will always draw the same rows.
    df4.sample(n=2, random_state=2)
    df4.sample(n=2, random_state=2)



Setting with enlargement
------------------------

The ``.loc/[]`` operations can perform enlargement when setting a non-existent key for that axis.

In the ``Series`` case this is effectively an appending operation.

.. ipython:: python

   se = pd.Series([1, 2, 3])
   se
   se[5] = 5.
   se

A ``DataFrame`` can be enlarged on either axis via ``.loc``.

.. ipython:: python

   dfi = pd.DataFrame(np.arange(6).reshape(3, 2),
                      columns=['A', 'B'])
   dfi
   dfi.loc[:, 'C'] = dfi.loc[:, 'A']
   dfi

This is like an ``append`` operation on the ``DataFrame``.

.. ipython:: python

   dfi.loc[3] = 5
   dfi

.. _indexing.basics.get_value:

Fast scalar value getting and setting
-------------------------------------

Since indexing with ``[]`` must handle a lot of cases (single-label access,
slicing, boolean indexing, etc.), it has a bit of overhead in order to figure
out what you're asking for. If you only want to access a scalar value, the
fastest way is to use the ``at`` and ``iat`` methods, which are implemented on
all of the data structures.

Similarly to ``loc``, ``at`` provides **label** based scalar lookups, while, ``iat`` provides **integer** based lookups analogously to ``iloc``

.. ipython:: python

   s.iat[5]
   df.at[dates[5], 'A']
   df.iat[3, 0]

You can also set using these same indexers.

.. ipython:: python

   df.at[dates[5], 'E'] = 7
   df.iat[3, 0] = 7

``at`` may enlarge the object in-place as above if the indexer is missing.

.. ipython:: python

   df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7
   df

Boolean indexing
----------------

.. _indexing.boolean:

Another common operation is the use of boolean vectors to filter the data.
The operators are: ``|`` for ``or``, ``&`` for ``and``, and ``~`` for ``not``.
These **must** be grouped by using parentheses, since by default Python will
evaluate an expression such as ``df['A'] > 2 & df['B'] < 3`` as
``df['A'] > (2 & df['B']) < 3``, while the desired evaluation order is
``(df['A'] > 2) & (df['B'] < 3)``.

Using a boolean vector to index a Series works exactly as in a NumPy ndarray:

.. ipython:: python

   s = pd.Series(range(-3, 4))
   s
   s[s > 0]
   s[(s < -1) | (s > 0.5)]
   s[~(s < 0)]

You may select rows from a DataFrame using a boolean vector the same length as
the DataFrame's index (for example, something derived from one of the columns
of the DataFrame):

.. ipython:: python

   df[df['A'] > 0]

List comprehensions and the ``map`` method of Series can also be used to produce
more complex criteria:

.. ipython:: python

   df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'],
                       'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'],
                       'c': np.random.randn(7)})

   # only want 'two' or 'three'
   criterion = df2['a'].map(lambda x: x.startswith('t'))

   df2[criterion]

   # equivalent but slower
   df2[[x.startswith('t') for x in df2['a']]]

   # Multiple criteria
   df2[criterion & (df2['b'] == 'x')]

With the choice methods :ref:`Selection by Label <indexing.label>`, :ref:`Selection by Position <indexing.integer>`,
and :ref:`Advanced Indexing <advanced>` you may select along more than one axis using boolean vectors combined with other indexing expressions.

.. ipython:: python

   df2.loc[criterion & (df2['b'] == 'x'), 'b':'c']

.. warning::

   ``iloc`` supports two kinds of boolean indexing. If the indexer is a boolean ``Series``,
   an error will be raised. For instance, in the following example, ``df.iloc[s.values, 1]`` is ok.
   The boolean indexer is an array. But ``df.iloc[s, 1]`` would raise ``ValueError``.

   .. ipython:: python

      df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],
                        index=list('abc'),
                        columns=['A', 'B'])
      s = (df['A'] > 2)
      s

      df.loc[s, 'B']

      df.iloc[s.values, 1]

.. _indexing.basics.indexing_isin:

Indexing with isin
------------------

Consider the :meth:`~Series.isin` method of ``Series``, which returns a boolean
vector that is true wherever the ``Series`` elements exist in the passed list.
This allows you to select rows where one or more columns have values you want:

.. ipython:: python

   s = pd.Series(np.arange(5), index=np.arange(5)[::-1], dtype='int64')
   s
   s.isin([2, 4, 6])
   s[s.isin([2, 4, 6])]

The same method is available for ``Index`` objects and is useful for the cases
when you don't know which of the sought labels are in fact present:

.. ipython:: python

   s[s.index.isin([2, 4, 6])]

   # compare it to the following
   s.reindex([2, 4, 6])

In addition to that, ``MultiIndex`` allows selecting a separate level to use
in the membership check:

.. ipython:: python

   s_mi = pd.Series(np.arange(6),
                    index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))
   s_mi
   s_mi.iloc[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]
   s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]

DataFrame also has an :meth:`~DataFrame.isin` method.  When calling ``isin``, pass a set of
values as either an array or dict.  If values is an array, ``isin`` returns
a DataFrame of booleans that is the same shape as the original DataFrame, with True
wherever the element is in the sequence of values.

.. ipython:: python

   df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                      'ids2': ['a', 'n', 'c', 'n']})

   values = ['a', 'b', 1, 3]

   df.isin(values)

Oftentimes you'll want to match certain values with certain columns.
Just make values a ``dict`` where the key is the column, and the value is
a list of items you want to check for.

.. ipython:: python

   values = {'ids': ['a', 'b'], 'vals': [1, 3]}

   df.isin(values)

To return the DataFrame of booleans where the values are *not* in the original DataFrame,
use the ``~`` operator:

.. ipython:: python

   values = {'ids': ['a', 'b'], 'vals': [1, 3]}

   ~df.isin(values)

Combine DataFrame's ``isin`` with the ``any()`` and ``all()`` methods to
quickly select subsets of your data that meet a given criteria.
To select a row where each column meets its own criterion:

.. ipython:: python

  values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}

  row_mask = df.isin(values).all(1)

  df[row_mask]

.. _indexing.where_mask:

The :meth:`~pandas.DataFrame.where` Method and Masking
------------------------------------------------------

Selecting values from a Series with a boolean vector generally returns a
subset of the data. To guarantee that selection output has the same shape as
the original data, you can use the ``where`` method in ``Series`` and ``DataFrame``.

To return only the selected rows:

.. ipython:: python

   s[s > 0]

To return a Series of the same shape as the original:

.. ipython:: python

   s.where(s > 0)

Selecting values from a DataFrame with a boolean criterion now also preserves
input data shape. ``where`` is used under the hood as the implementation.
The code below is equivalent to ``df.where(df < 0)``.

.. ipython:: python
   :suppress:

   dates = pd.date_range('1/1/2000', periods=8)
   df = pd.DataFrame(np.random.randn(8, 4),
                     index=dates, columns=['A', 'B', 'C', 'D'])

.. ipython:: python

   df[df < 0]

In addition, ``where`` takes an optional ``other`` argument for replacement of
values where the condition is False, in the returned copy.

.. ipython:: python

   df.where(df < 0, -df)

You may wish to set values based on some boolean criteria.
This can be done intuitively like so:

.. ipython:: python

   s2 = s.copy()
   s2[s2 < 0] = 0
   s2

   df2 = df.copy()
   df2[df2 < 0] = 0
   df2

By default, ``where`` returns a modified copy of the data. There is an
optional parameter ``inplace`` so that the original data can be modified
without creating a copy:

.. ipython:: python

   df_orig = df.copy()
   df_orig.where(df > 0, -df, inplace=True)
   df_orig

.. note::

   The signature for :func:`DataFrame.where` differs from :func:`numpy.where`.
   Roughly ``df1.where(m, df2)`` is equivalent to ``np.where(m, df1, df2)``.

   .. ipython:: python

      df.where(df < 0, -df) == np.where(df < 0, df, -df)

**Alignment**

Furthermore, ``where`` aligns the input boolean condition (ndarray or DataFrame),
such that partial selection with setting is possible. This is analogous to
partial setting via ``.loc`` (but on the contents rather than the axis labels).

.. ipython:: python

   df2 = df.copy()
   df2[df2[1:4] > 0] = 3
   df2

Where can also accept ``axis`` and ``level`` parameters to align the input when
performing the ``where``.

.. ipython:: python

   df2 = df.copy()
   df2.where(df2 > 0, df2['A'], axis='index')

This is equivalent to (but faster than) the following.

.. ipython:: python

   df2 = df.copy()
   df.apply(lambda x, y: x.where(x > 0, y), y=df['A'])

``where`` can accept a callable as condition and ``other`` arguments. The function must
be with one argument (the calling Series or DataFrame) and that returns valid output
as condition and ``other`` argument.

.. ipython:: python

   df3 = pd.DataFrame({'A': [1, 2, 3],
                       'B': [4, 5, 6],
                       'C': [7, 8, 9]})
   df3.where(lambda x: x > 4, lambda x: x + 10)

Mask
~~~~

:meth:`~pandas.DataFrame.mask` is the inverse boolean operation of ``where``.

.. ipython:: python

   s.mask(s >= 0)
   df.mask(df >= 0)

.. _indexing.np_where:

Setting with enlargement conditionally using :func:`numpy`
----------------------------------------------------------

An alternative to :meth:`~pandas.DataFrame.where` is to use :func:`numpy.where`.
Combined with setting a new column, you can use it to enlarge a DataFrame where the
values are determined conditionally.

Consider you have two choices to choose from in the following DataFrame. And you want to
set a new column color to 'green' when the second column has 'Z'.  You can do the
following:

.. ipython:: python

   df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})
   df['color'] = np.where(df['col2'] == 'Z', 'green', 'red')
   df

If you have multiple conditions, you can use :func:`numpy.select` to achieve that.  Say
corresponding to three conditions there are three choice of colors, with a fourth color
as a fallback, you can do the following.

.. ipython:: python

   conditions = [
       (df['col2'] == 'Z') & (df['col1'] == 'A'),
       (df['col2'] == 'Z') & (df['col1'] == 'B'),
       (df['col1'] == 'B')
   ]
   choices = ['yellow', 'blue', 'purple']
   df['color'] = np.select(conditions, choices, default='black')
   df

.. _indexing.query:

The :meth:`~pandas.DataFrame.query` Method
------------------------------------------

:class:`~pandas.DataFrame` objects have a :meth:`~pandas.DataFrame.query`
method that allows selection using an expression.

You can get the value of the frame where column ``b`` has values
between the values of columns ``a`` and ``c``. For example:

.. ipython:: python

   n = 10
   df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
   df

   # pure python
   df[(df['a'] < df['b']) & (df['b'] < df['c'])]

   # query
   df.query('(a < b) & (b < c)')

Do the same thing but fall back on a named index if there is no column
with the name ``a``.

.. ipython:: python

   df = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc'))
   df.index.name = 'a'
   df
   df.query('a < b and b < c')

If instead you don't want to or cannot name your index, you can use the name
``index`` in your query expression:

.. ipython:: python

   df = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc'))
   df
   df.query('index < b < c')

.. note::

   If the name of your index overlaps with a column name, the column name is
   given precedence. For example,

   .. ipython:: python

      df = pd.DataFrame({'a': np.random.randint(5, size=5)})
      df.index.name = 'a'
      df.query('a > 2')  # uses the column 'a', not the index

   You can still use the index in a query expression by using the special
   identifier 'index':

   .. ipython:: python

      df.query('index > 2')

   If for some reason you have a column named ``index``, then you can refer to
   the index as ``ilevel_0`` as well, but at this point you should consider
   renaming your columns to something less ambiguous.


:class:`~pandas.MultiIndex` :meth:`~pandas.DataFrame.query` Syntax
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can also use the levels of a ``DataFrame`` with a
:class:`~pandas.MultiIndex` as if they were columns in the frame:

.. ipython:: python

   n = 10
   colors = np.random.choice(['red', 'green'], size=n)
   foods = np.random.choice(['eggs', 'ham'], size=n)
   colors
   foods

   index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])
   df = pd.DataFrame(np.random.randn(n, 2), index=index)
   df
   df.query('color == "red"')

If the levels of the ``MultiIndex`` are unnamed, you can refer to them using
special names:

.. ipython:: python

   df.index.names = [None, None]
   df
   df.query('ilevel_0 == "red"')


The convention is ``ilevel_0``, which means "index level 0" for the 0th level
of the ``index``.


:meth:`~pandas.DataFrame.query` Use Cases
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A use case for :meth:`~pandas.DataFrame.query` is when you have a collection of
:class:`~pandas.DataFrame` objects that have a subset of column names (or index
levels/names) in common. You can pass the same query to both frames *without*
having to specify which frame you're interested in querying

.. ipython:: python

   df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
   df
   df2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns)
   df2
   expr = '0.0 <= a <= c <= 0.5'
   map(lambda frame: frame.query(expr), [df, df2])

:meth:`~pandas.DataFrame.query` Python versus pandas Syntax Comparison
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Full numpy-like syntax:

.. ipython:: python

   df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))
   df
   df.query('(a < b) & (b < c)')
   df[(df['a'] < df['b']) & (df['b'] < df['c'])]

Slightly nicer by removing the parentheses (comparison operators bind tighter
than ``&`` and ``|``):

.. ipython:: python

   df.query('a < b & b < c')

Use English instead of symbols:

.. ipython:: python

   df.query('a < b and b < c')

Pretty close to how you might write it on paper:

.. ipython:: python

   df.query('a < b < c')

The ``in`` and ``not in`` operators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:meth:`~pandas.DataFrame.query` also supports special use of Python's ``in`` and
``not in`` comparison operators, providing a succinct syntax for calling the
``isin`` method of a ``Series`` or ``DataFrame``.

.. ipython:: python

   # get all rows where columns "a" and "b" have overlapping values
   df = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'),
                      'c': np.random.randint(5, size=12),
                      'd': np.random.randint(9, size=12)})
   df
   df.query('a in b')

   # How you'd do it in pure Python
   df[df['a'].isin(df['b'])]

   df.query('a not in b')

   # pure Python
   df[~df['a'].isin(df['b'])]


You can combine this with other expressions for very succinct queries:


.. ipython:: python

   # rows where cols a and b have overlapping values
   # and col c's values are less than col d's
   df.query('a in b and c < d')

   # pure Python
   df[df['b'].isin(df['a']) & (df['c'] < df['d'])]


.. note::

   Note that ``in`` and ``not in`` are evaluated in Python, since ``numexpr``
   has no equivalent of this operation. However, **only the** ``in``/``not in``
   **expression itself** is evaluated in vanilla Python. For example, in the
   expression

   .. code-block:: python

      df.query('a in b + c + d')

   ``(b + c + d)`` is evaluated by ``numexpr`` and *then* the ``in``
   operation is evaluated in plain Python. In general, any operations that can
   be evaluated using ``numexpr`` will be.

Special use of the ``==`` operator with ``list`` objects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Comparing a ``list`` of values to a column using ``==``/``!=`` works similarly
to ``in``/``not in``.

.. ipython:: python

   df.query('b == ["a", "b", "c"]')

   # pure Python
   df[df['b'].isin(["a", "b", "c"])]

   df.query('c == [1, 2]')

   df.query('c != [1, 2]')

   # using in/not in
   df.query('[1, 2] in c')

   df.query('[1, 2] not in c')

   # pure Python
   df[df['c'].isin([1, 2])]


Boolean operators
~~~~~~~~~~~~~~~~~

You can negate boolean expressions with the word ``not`` or the ``~`` operator.

.. ipython:: python

   df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
   df['bools'] = np.random.rand(len(df)) > 0.5
   df.query('~bools')
   df.query('not bools')
   df.query('not bools') == df[~df['bools']]

Of course, expressions can be arbitrarily complex too:

.. ipython:: python

   # short query syntax
   shorter = df.query('a < b < c and (not bools) or bools > 2')

   # equivalent in pure Python
   longer = df[(df['a'] < df['b'])
               & (df['b'] < df['c'])
               & (~df['bools'])
               | (df['bools'] > 2)]

   shorter
   longer

   shorter == longer


Performance of :meth:`~pandas.DataFrame.query`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``DataFrame.query()`` using ``numexpr`` is slightly faster than Python for
large frames.

.. image:: ../_static/query-perf.png

.. note::

   You will only see the performance benefits of using the ``numexpr`` engine
   with ``DataFrame.query()`` if your frame has more than approximately 200,000
   rows.

      .. image:: ../_static/query-perf-small.png

This plot was created using a ``DataFrame`` with 3 columns each containing
floating point values generated using ``numpy.random.randn()``.

.. ipython:: python
   :suppress:

   df = pd.DataFrame(np.random.randn(8, 4),
                     index=dates, columns=['A', 'B', 'C', 'D'])
   df2 = df.copy()


Duplicate data
--------------

.. _indexing.duplicate:

If you want to identify and remove duplicate rows in a DataFrame,  there are
two methods that will help: ``duplicated`` and ``drop_duplicates``. Each
takes as an argument the columns to use to identify duplicated rows.

* ``duplicated`` returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated.
* ``drop_duplicates`` removes duplicate rows.

By default, the first observed row of a duplicate set is considered unique, but
each method has a ``keep`` parameter to specify targets to be kept.

* ``keep='first'`` (default): mark / drop duplicates except for the first occurrence.
* ``keep='last'``: mark / drop duplicates except for the last occurrence.
* ``keep=False``: mark  / drop all duplicates.

.. ipython:: python

   df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],
                       'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],
                       'c': np.random.randn(7)})
   df2
   df2.duplicated('a')
   df2.duplicated('a', keep='last')
   df2.duplicated('a', keep=False)
   df2.drop_duplicates('a')
   df2.drop_duplicates('a', keep='last')
   df2.drop_duplicates('a', keep=False)

Also, you can pass a list of columns to identify duplications.

.. ipython:: python

   df2.duplicated(['a', 'b'])
   df2.drop_duplicates(['a', 'b'])

To drop duplicates by index value, use ``Index.duplicated`` then perform slicing.
The same set of options are available for the ``keep`` parameter.

.. ipython:: python

   df3 = pd.DataFrame({'a': np.arange(6),
                       'b': np.random.randn(6)},
                      index=['a', 'a', 'b', 'c', 'b', 'a'])
   df3
   df3.index.duplicated()
   df3[~df3.index.duplicated()]
   df3[~df3.index.duplicated(keep='last')]
   df3[~df3.index.duplicated(keep=False)]

.. _indexing.dictionarylike:

Dictionary-like :meth:`~pandas.DataFrame.get` method
----------------------------------------------------

Each of Series or DataFrame have a ``get`` method which can return a
default value.

.. ipython:: python

   s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
   s.get('a')  # equivalent to s['a']
   s.get('x', default=-1)

.. _indexing.lookup:

Looking up values by index/column labels
----------------------------------------

Sometimes you want to extract a set of values given a sequence of row labels
and column labels, this can be achieved by ``pandas.factorize``  and NumPy indexing.
For instance:

.. ipython:: python

    df = pd.DataFrame({'col': ["A", "A", "B", "B"],
                       'A': [80, 23, np.nan, 22],
                       'B': [80, 55, 76, 67]})
    df
    idx, cols = pd.factorize(df['col'])
    df.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx]

Formerly this could be achieved with the dedicated ``DataFrame.lookup`` method
which was deprecated in version 1.2.0.

.. _indexing.class:

Index objects
-------------

The pandas :class:`~pandas.Index` class and its subclasses can be viewed as
implementing an *ordered multiset*. Duplicates are allowed. However, if you try
to convert an :class:`~pandas.Index` object with duplicate entries into a
``set``, an exception will be raised.

:class:`~pandas.Index` also provides the infrastructure necessary for
lookups, data alignment, and reindexing. The easiest way to create an
:class:`~pandas.Index` directly is to pass a ``list`` or other sequence to
:class:`~pandas.Index`:

.. ipython:: python

   index = pd.Index(['e', 'd', 'a', 'b'])
   index
   'd' in index

You can also pass a ``name`` to be stored in the index:


.. ipython:: python

   index = pd.Index(['e', 'd', 'a', 'b'], name='something')
   index.name

The name, if set, will be shown in the console display:

.. ipython:: python

   index = pd.Index(list(range(5)), name='rows')
   columns = pd.Index(['A', 'B', 'C'], name='cols')
   df = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns)
   df
   df['A']

.. _indexing.set_metadata:

Setting metadata
~~~~~~~~~~~~~~~~

Indexes are "mostly immutable", but it is possible to set and change their
``name`` attribute. You can use the ``rename``, ``set_names`` to set these attributes
directly, and they default to returning a copy.

See :ref:`Advanced Indexing <advanced>` for usage of MultiIndexes.

.. ipython:: python

  ind = pd.Index([1, 2, 3])
  ind.rename("apple")
  ind
  ind.set_names(["apple"], inplace=True)
  ind.name = "bob"
  ind

``set_names``, ``set_levels``, and ``set_codes`` also take an optional
``level`` argument

.. ipython:: python

  index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])
  index
  index.levels[1]
  index.set_levels(["a", "b"], level=1)

.. _indexing.set_ops:

Set operations on Index objects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The two main operations are ``union`` and ``intersection``.
Difference is provided via the ``.difference()`` method.

.. ipython:: python

   a = pd.Index(['c', 'b', 'a'])
   b = pd.Index(['c', 'e', 'd'])
   a.difference(b)

Also available is the ``symmetric_difference`` operation, which returns elements
that appear in either ``idx1`` or ``idx2``, but not in both. This is
equivalent to the Index created by ``idx1.difference(idx2).union(idx2.difference(idx1))``,
with duplicates dropped.

.. ipython:: python

   idx1 = pd.Index([1, 2, 3, 4])
   idx2 = pd.Index([2, 3, 4, 5])
   idx1.symmetric_difference(idx2)

.. note::

   The resulting index from a set operation will be sorted in ascending order.

When performing :meth:`Index.union` between indexes with different dtypes, the indexes
must be cast to a common dtype. Typically, though not always, this is object dtype. The
exception is when performing a union between integer and float data. In this case, the
integer values are converted to float

.. ipython:: python

   idx1 = pd.Index([0, 1, 2])
   idx2 = pd.Index([0.5, 1.5])
   idx1.union(idx2)

.. _indexing.missing:

Missing values
~~~~~~~~~~~~~~

.. important::

   Even though ``Index`` can hold missing values (``NaN``), it should be avoided
   if you do not want any unexpected results. For example, some operations
   exclude missing values implicitly.

``Index.fillna`` fills missing values with specified scalar value.

.. ipython:: python

   idx1 = pd.Index([1, np.nan, 3, 4])
   idx1
   idx1.fillna(2)

   idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),
                            pd.NaT,
                            pd.Timestamp('2011-01-03')])
   idx2
   idx2.fillna(pd.Timestamp('2011-01-02'))

Set / reset index
-----------------

Occasionally you will load or create a data set into a DataFrame and want to
add an index after you've already done so. There are a couple of different
ways.

.. _indexing.set_index:

Set an index
~~~~~~~~~~~~

DataFrame has a :meth:`~DataFrame.set_index` method which takes a column name
(for a regular ``Index``) or a list of column names (for a ``MultiIndex``).
To create a new, re-indexed DataFrame:

.. ipython:: python
   :suppress:

   data = pd.DataFrame({'a': ['bar', 'bar', 'foo', 'foo'],
                        'b': ['one', 'two', 'one', 'two'],
                        'c': ['z', 'y', 'x', 'w'],
                        'd': [1., 2., 3, 4]})

.. ipython:: python

   data
   indexed1 = data.set_index('c')
   indexed1
   indexed2 = data.set_index(['a', 'b'])
   indexed2

The ``append`` keyword option allow you to keep the existing index and append
the given columns to a MultiIndex:

.. ipython:: python

   frame = data.set_index('c', drop=False)
   frame = frame.set_index(['a', 'b'], append=True)
   frame

Other options in ``set_index`` allow you not drop the index columns or to add
the index in-place (without creating a new object):

.. ipython:: python

   data.set_index('c', drop=False)
   data.set_index(['a', 'b'], inplace=True)
   data

Reset the index
~~~~~~~~~~~~~~~

As a convenience, there is a new function on DataFrame called
:meth:`~DataFrame.reset_index` which transfers the index values into the
DataFrame's columns and sets a simple integer index.
This is the inverse operation of :meth:`~DataFrame.set_index`.


.. ipython:: python

   data
   data.reset_index()

The output is more similar to a SQL table or a record array. The names for the
columns derived from the index are the ones stored in the ``names`` attribute.

You can use the ``level`` keyword to remove only a portion of the index:

.. ipython:: python

   frame
   frame.reset_index(level=1)


``reset_index`` takes an optional parameter ``drop`` which if true simply
discards the index, instead of putting index values in the DataFrame's columns.

Adding an ad hoc index
~~~~~~~~~~~~~~~~~~~~~~

If you create an index yourself, you can just assign it to the ``index`` field:

.. code-block:: python

   data.index = index

.. _indexing.view_versus_copy:

Returning a view versus a copy
------------------------------

When setting values in a pandas object, care must be taken to avoid what is called
``chained indexing``. Here is an example.

.. ipython:: python

   dfmi = pd.DataFrame([list('abcd'),
                        list('efgh'),
                        list('ijkl'),
                        list('mnop')],
                       columns=pd.MultiIndex.from_product([['one', 'two'],
                                                           ['first', 'second']]))
   dfmi

Compare these two access methods:

.. ipython:: python

   dfmi['one']['second']

.. ipython:: python

   dfmi.loc[:, ('one', 'second')]

These both yield the same results, so which should you use? It is instructive to understand the order
of operations on these and why method 2 (``.loc``) is much preferred over method 1 (chained ``[]``).

``dfmi['one']`` selects the first level of the columns and returns a DataFrame that is singly-indexed.
Then another Python operation ``dfmi_with_one['second']`` selects the series indexed by ``'second'``.
This is indicated by the variable ``dfmi_with_one`` because pandas sees these operations as separate events.
e.g. separate calls to ``__getitem__``, so it has to treat them as linear operations, they happen one after another.

Contrast this to ``df.loc[:,('one','second')]`` which passes a nested tuple of ``(slice(None),('one','second'))`` to a single call to
``__getitem__``. This allows pandas to deal with this as a single entity. Furthermore this order of operations *can* be significantly
faster, and allows one to index *both* axes if so desired.

Why does assignment fail when using chained indexing?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The problem in the previous section is just a performance issue. What's up with
the ``SettingWithCopy`` warning? We don't **usually** throw warnings around when
you do something that might cost a few extra milliseconds!

But it turns out that assigning to the product of chained indexing has
inherently unpredictable results. To see this, think about how the Python
interpreter executes this code:

.. ipython:: python
    :suppress:

    value = None

.. code-block:: python

   dfmi.loc[:, ('one', 'second')] = value
   # becomes
   dfmi.loc.__setitem__((slice(None), ('one', 'second')), value)

But this code is handled differently:

.. code-block:: python

   dfmi['one']['second'] = value
   # becomes
   dfmi.__getitem__('one').__setitem__('second', value)

See that ``__getitem__`` in there? Outside of simple cases, it's very hard to
predict whether it will return a view or a copy (it depends on the memory layout
of the array, about which pandas makes no guarantees), and therefore whether
the ``__setitem__`` will modify ``dfmi`` or a temporary object that gets thrown
out immediately afterward. **That's** what ``SettingWithCopy`` is warning you
about!

.. note:: You may be wondering whether we should be concerned about the ``loc``
   property in the first example. But ``dfmi.loc`` is guaranteed to be ``dfmi``
   itself with modified indexing behavior, so ``dfmi.loc.__getitem__`` /
   ``dfmi.loc.__setitem__`` operate on ``dfmi`` directly. Of course,
   ``dfmi.loc.__getitem__(idx)`` may be a view or a copy of ``dfmi``.

Sometimes a ``SettingWithCopy`` warning will arise at times when there's no
obvious chained indexing going on. **These** are the bugs that
``SettingWithCopy`` is designed to catch! pandas is probably trying to warn you
that you've done this:

.. code-block:: python

   def do_something(df):
       foo = df[['bar', 'baz']]  # Is foo a view? A copy? Nobody knows!
       # ... many lines here ...
       # We don't know whether this will modify df or not!
       foo['quux'] = value
       return foo

Yikes!

.. _indexing.evaluation_order:

Evaluation order matters
~~~~~~~~~~~~~~~~~~~~~~~~

When you use chained indexing, the order and type of the indexing operation
partially determine whether the result is a slice into the original object, or
a copy of the slice.

pandas has the ``SettingWithCopyWarning`` because assigning to a copy of a
slice is frequently not intentional, but a mistake caused by chained indexing
returning a copy where a slice was expected.

If you would like pandas to be more or less trusting about assignment to a
chained indexing expression, you can set the :ref:`option <options>`
``mode.chained_assignment`` to one of these values:

* ``'warn'``, the default, means a ``SettingWithCopyWarning`` is printed.
* ``'raise'`` means pandas will raise a ``SettingWithCopyException``
  you have to deal with.
* ``None`` will suppress the warnings entirely.

.. ipython:: python
   :okwarning:

   dfb = pd.DataFrame({'a': ['one', 'one', 'two',
                             'three', 'two', 'one', 'six'],
                       'c': np.arange(7)})

   # This will show the SettingWithCopyWarning
   # but the frame values will be set
   dfb['c'][dfb['a'].str.startswith('o')] = 42

This however is operating on a copy and will not work.

::

   >>> pd.set_option('mode.chained_assignment','warn')
   >>> dfb[dfb['a'].str.startswith('o')]['c'] = 42
   Traceback (most recent call last)
        ...
   SettingWithCopyWarning:
        A value is trying to be set on a copy of a slice from a DataFrame.
        Try using .loc[row_index,col_indexer] = value instead

A chained assignment can also crop up in setting in a mixed dtype frame.

.. note::

   These setting rules apply to all of ``.loc/.iloc``.

The following is the recommended access method using ``.loc`` for multiple items (using ``mask``) and a single item using a fixed index:

.. ipython:: python

   dfc = pd.DataFrame({'a': ['one', 'one', 'two',
                             'three', 'two', 'one', 'six'],
                       'c': np.arange(7)})
   dfd = dfc.copy()
   # Setting multiple items using a mask
   mask = dfd['a'].str.startswith('o')
   dfd.loc[mask, 'c'] = 42
   dfd

   # Setting a single item
   dfd = dfc.copy()
   dfd.loc[2, 'a'] = 11
   dfd

The following *can* work at times, but it is not guaranteed to, and therefore should be avoided:

.. ipython:: python
   :okwarning:

   dfd = dfc.copy()
   dfd['a'][2] = 111
   dfd

Last, the subsequent example will **not** work at all, and so should be avoided:

::

   >>> pd.set_option('mode.chained_assignment','raise')
   >>> dfd.loc[0]['a'] = 1111
   Traceback (most recent call last)
        ...
   SettingWithCopyException:
        A value is trying to be set on a copy of a slice from a DataFrame.
        Try using .loc[row_index,col_indexer] = value instead

.. warning::

   The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid
   assignment. There may be false positives; situations where a chained assignment is inadvertently
   reported.
.. _window:

{{ header }}

********************
Windowing Operations
********************

pandas contains a compact set of APIs for performing windowing operations - an operation that performs
an aggregation over a sliding partition of values. The API functions similarly to the ``groupby`` API
in that :class:`Series` and :class:`DataFrame` call the windowing method with
necessary parameters and then subsequently call the aggregation function.

.. ipython:: python

   s = pd.Series(range(5))
   s.rolling(window=2).sum()

The windows are comprised by looking back the length of the window from the current observation.
The result above can be derived by taking the sum of the following windowed partitions of data:

.. ipython:: python

   for window in s.rolling(window=2):
       print(window)


.. _window.overview:

Overview
--------

pandas supports 4 types of windowing operations:

#. Rolling window: Generic fixed or variable sliding window over the values.
#. Weighted window: Weighted, non-rectangular window supplied by the ``scipy.signal`` library.
#. Expanding window: Accumulating window over the values.
#. Exponentially Weighted window: Accumulating and exponentially weighted window over the values.

=============================   =================  ===========================   ===========================  ========================  ===================================  ===========================
Concept                         Method             Returned Object               Supports time-based windows  Supports chained groupby  Supports table method                Supports online operations
=============================   =================  ===========================   ===========================  ========================  ===================================  ===========================
Rolling window                  ``rolling``        ``Rolling``                   Yes                          Yes                       Yes (as of version 1.3)              No
Weighted window                 ``rolling``        ``Window``                    No                           No                        No                                   No
Expanding window                ``expanding``      ``Expanding``                 No                           Yes                       Yes (as of version 1.3)              No
Exponentially Weighted window   ``ewm``            ``ExponentialMovingWindow``   No                           Yes (as of version 1.2)   No                                   Yes (as of version 1.3)
=============================   =================  ===========================   ===========================  ========================  ===================================  ===========================

As noted above, some operations support specifying a window based on a time offset:

.. ipython:: python

   s = pd.Series(range(5), index=pd.date_range('2020-01-01', periods=5, freq='1D'))
   s.rolling(window='2D').sum()

Additionally, some methods support chaining a ``groupby`` operation with a windowing operation
which will first group the data by the specified keys and then perform a windowing operation per group.

.. ipython:: python

   df = pd.DataFrame({'A': ['a', 'b', 'a', 'b', 'a'], 'B': range(5)})
   df.groupby('A').expanding().sum()

.. note::

   Windowing operations currently only support numeric data (integer and float)
   and will always return ``float64`` values.

.. warning::

    Some windowing aggregation, ``mean``, ``sum``, ``var`` and ``std`` methods may suffer from numerical
    imprecision due to the underlying windowing algorithms accumulating sums. When values differ
    with magnitude :math:`1/np.finfo(np.double).eps` this results in truncation. It must be
    noted, that large values may have an impact on windows, which do not include these values. `Kahan summation
    <https://en.wikipedia.org/wiki/Kahan_summation_algorithm>`__ is used
    to compute the rolling sums to preserve accuracy as much as possible.


.. versionadded:: 1.3.0

Some windowing operations also support the ``method='table'`` option in the constructor which
performs the windowing operation over an entire :class:`DataFrame` instead of a single column or row at a time.
This can provide a useful performance benefit for a :class:`DataFrame` with many columns or rows
(with the corresponding ``axis`` argument) or the ability to utilize other columns during the windowing
operation. The ``method='table'`` option can only be used if ``engine='numba'`` is specified
in the corresponding method call.

For example, a `weighted mean <https://en.wikipedia.org/wiki/Weighted_arithmetic_mean>`__ calculation can
be calculated with :meth:`~Rolling.apply` by specifying a separate column of weights.

.. ipython:: python

   def weighted_mean(x):
       arr = np.ones((1, x.shape[1]))
       arr[:, :2] = (x[:, :2] * x[:, 2]).sum(axis=0) / x[:, 2].sum()
       return arr

   df = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]])
   df.rolling(2, method="table", min_periods=0).apply(weighted_mean, raw=True, engine="numba")  # noqa:E501

.. versionadded:: 1.3

Some windowing operations also support an ``online`` method after constructing a windowing object
which returns a new object that supports passing in new :class:`DataFrame` or :class:`Series` objects
to continue the windowing calculation with the new values (i.e. online calculations).

The methods on this new windowing objects must call the aggregation method first to "prime" the initial
state of the online calculation. Then, new :class:`DataFrame` or :class:`Series` objects can be passed in
the ``update`` argument to continue the windowing calculation.

.. ipython:: python

   df = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]])
   df.ewm(0.5).mean()

.. ipython:: python

   online_ewm = df.head(2).ewm(0.5).online()
   online_ewm.mean()
   online_ewm.mean(update=df.tail(1))

All windowing operations support a ``min_periods`` argument that dictates the minimum amount of
non-``np.nan`` values a window must have; otherwise, the resulting value is ``np.nan``.
``min_periods`` defaults to 1 for time-based windows and ``window`` for fixed windows

.. ipython:: python

   s = pd.Series([np.nan, 1, 2, np.nan, np.nan, 3])
   s.rolling(window=3, min_periods=1).sum()
   s.rolling(window=3, min_periods=2).sum()
   # Equivalent to min_periods=3
   s.rolling(window=3, min_periods=None).sum()


Additionally, all windowing operations supports the ``aggregate`` method for returning a result
of multiple aggregations applied to a window.

.. ipython:: python

   df = pd.DataFrame({"A": range(5), "B": range(10, 15)})
   df.expanding().agg([np.sum, np.mean, np.std])


.. _window.generic:

Rolling window
--------------

Generic rolling windows support specifying windows as a fixed number of observations or variable
number of observations based on an offset. If a time based offset is provided, the corresponding
time based index must be monotonic.

.. ipython:: python

   times = ['2020-01-01', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-29']
   s = pd.Series(range(5), index=pd.DatetimeIndex(times))
   s
   # Window with 2 observations
   s.rolling(window=2).sum()
   # Window with 2 days worth of observations
   s.rolling(window='2D').sum()

For all supported aggregation functions, see :ref:`api.functions_rolling`.

.. _window.center:

Centering windows
~~~~~~~~~~~~~~~~~

By default the labels are set to the right edge of the window, but a
``center`` keyword is available so the labels can be set at the center.

.. ipython:: python

   s = pd.Series(range(10))
   s.rolling(window=5).mean()
   s.rolling(window=5, center=True).mean()


This can also be applied to datetime-like indices.

.. versionadded:: 1.3.0

.. ipython:: python

    df = pd.DataFrame(
        {"A": [0, 1, 2, 3, 4]}, index=pd.date_range("2020", periods=5, freq="1D")
    )
    df
    df.rolling("2D", center=False).mean()
    df.rolling("2D", center=True).mean()


.. _window.endpoints:

Rolling window endpoints
~~~~~~~~~~~~~~~~~~~~~~~~

The inclusion of the interval endpoints in rolling window calculations can be specified with the ``closed``
parameter:

=============  ====================
Value          Behavior
=============  ====================
``'right'``     close right endpoint
``'left'``     close left endpoint
``'both'``     close both endpoints
``'neither'``  open endpoints
=============  ====================

For example, having the right endpoint open is useful in many problems that require that there is no contamination
from present information back to past information. This allows the rolling window to compute statistics
"up to that point in time", but not including that point in time.

.. ipython:: python

   df = pd.DataFrame(
       {"x": 1},
       index=[
           pd.Timestamp("20130101 09:00:01"),
           pd.Timestamp("20130101 09:00:02"),
           pd.Timestamp("20130101 09:00:03"),
           pd.Timestamp("20130101 09:00:04"),
           pd.Timestamp("20130101 09:00:06"),
       ],
   )

   df["right"] = df.rolling("2s", closed="right").x.sum()  # default
   df["both"] = df.rolling("2s", closed="both").x.sum()
   df["left"] = df.rolling("2s", closed="left").x.sum()
   df["neither"] = df.rolling("2s", closed="neither").x.sum()

   df

.. _window.custom_rolling_window:

Custom window rolling
~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.0

In addition to accepting an integer or offset as a ``window`` argument, ``rolling`` also accepts
a ``BaseIndexer`` subclass that allows a user to define a custom method for calculating window bounds.
The ``BaseIndexer`` subclass will need to define a ``get_window_bounds`` method that returns
a tuple of two arrays, the first being the starting indices of the windows and second being the
ending indices of the windows. Additionally, ``num_values``, ``min_periods``, ``center``, ``closed``
and will automatically be passed to ``get_window_bounds`` and the defined method must
always accept these arguments.

For example, if we have the following :class:`DataFrame`

.. ipython:: python

   use_expanding = [True, False, True, False, True]
   use_expanding
   df = pd.DataFrame({"values": range(5)})
   df

and we want to use an expanding window where ``use_expanding`` is ``True`` otherwise a window of size
1, we can create the following ``BaseIndexer`` subclass:

.. code-block:: ipython

   In [2]: from pandas.api.indexers import BaseIndexer

   In [3]: class CustomIndexer(BaseIndexer):
      ...:     def get_window_bounds(self, num_values, min_periods, center, closed):
      ...:         start = np.empty(num_values, dtype=np.int64)
      ...:         end = np.empty(num_values, dtype=np.int64)
      ...:         for i in range(num_values):
      ...:             if self.use_expanding[i]:
      ...:                 start[i] = 0
      ...:                 end[i] = i + 1
      ...:             else:
      ...:                 start[i] = i
      ...:                 end[i] = i + self.window_size
      ...:         return start, end

   In [4]: indexer = CustomIndexer(window_size=1, use_expanding=use_expanding)

   In [5]: df.rolling(indexer).sum()
   Out[5]:
       values
   0     0.0
   1     1.0
   2     3.0
   3     3.0
   4    10.0

You can view other examples of ``BaseIndexer`` subclasses `here <https://github.com/pandas-dev/pandas/blob/main/pandas/core/indexers/objects.py>`__

.. versionadded:: 1.1

One subclass of note within those examples is the ``VariableOffsetWindowIndexer`` that allows
rolling operations over a non-fixed offset like a ``BusinessDay``.

.. ipython:: python

   from pandas.api.indexers import VariableOffsetWindowIndexer

   df = pd.DataFrame(range(10), index=pd.date_range("2020", periods=10))
   offset = pd.offsets.BDay(1)
   indexer = VariableOffsetWindowIndexer(index=df.index, offset=offset)
   df
   df.rolling(indexer).sum()

For some problems knowledge of the future is available for analysis. For example, this occurs when
each data point is a full time series read from an experiment, and the task is to extract underlying
conditions. In these cases it can be useful to perform forward-looking rolling window computations.
:func:`FixedForwardWindowIndexer <pandas.api.indexers.FixedForwardWindowIndexer>` class is available for this purpose.
This :func:`BaseIndexer <pandas.api.indexers.BaseIndexer>` subclass implements a closed fixed-width
forward-looking rolling window, and we can use it as follows:

.. ipython:: python

   from pandas.api.indexers import FixedForwardWindowIndexer
   indexer = FixedForwardWindowIndexer(window_size=2)
   df.rolling(indexer, min_periods=1).sum()

We can also achieve this by using slicing, applying rolling aggregation, and then flipping the result as shown in example below:

.. ipython:: python

   df = pd.DataFrame(
       data=[
           [pd.Timestamp("2018-01-01 00:00:00"), 100],
           [pd.Timestamp("2018-01-01 00:00:01"), 101],
           [pd.Timestamp("2018-01-01 00:00:03"), 103],
           [pd.Timestamp("2018-01-01 00:00:04"), 111],
       ],
       columns=["time", "value"],
   ).set_index("time")
   df

   reversed_df = df[::-1].rolling("2s").sum()[::-1]
   reversed_df

.. _window.rolling_apply:

Rolling apply
~~~~~~~~~~~~~

The :meth:`~Rolling.apply` function takes an extra ``func`` argument and performs
generic rolling computations. The ``func`` argument should be a single function
that produces a single value from an ndarray input. ``raw`` specifies whether
the windows are cast as :class:`Series` objects (``raw=False``) or ndarray objects (``raw=True``).

.. ipython:: python

   def mad(x):
       return np.fabs(x - x.mean()).mean()

   s = pd.Series(range(10))
   s.rolling(window=4).apply(mad, raw=True)

.. _window.numba_engine:

Numba engine
~~~~~~~~~~~~

.. versionadded:: 1.0

Additionally, :meth:`~Rolling.apply` can leverage `Numba <https://numba.pydata.org/>`__
if installed as an optional dependency. The apply aggregation can be executed using Numba by specifying
``engine='numba'`` and ``engine_kwargs`` arguments (``raw`` must also be set to ``True``).
See :ref:`enhancing performance with Numba <enhancingperf.numba>` for general usage of the arguments and performance considerations.

Numba will be applied in potentially two routines:

#. If ``func`` is a standard Python function, the engine will `JIT <https://numba.pydata.org/numba-doc/latest/user/overview.html>`__ the passed function. ``func`` can also be a JITed function in which case the engine will not JIT the function again.
#. The engine will JIT the for loop where the apply function is applied to each window.

The ``engine_kwargs`` argument is a dictionary of keyword arguments that will be passed into the
`numba.jit decorator <https://numba.pydata.org/numba-doc/latest/reference/jit-compilation.html#numba.jit>`__.
These keyword arguments will be applied to *both* the passed function (if a standard Python function)
and the apply for loop over each window.

.. versionadded:: 1.3.0

``mean``, ``median``, ``max``, ``min``, and ``sum`` also support the ``engine`` and ``engine_kwargs`` arguments.

.. _window.cov_corr:

Binary window functions
~~~~~~~~~~~~~~~~~~~~~~~

:meth:`~Rolling.cov` and :meth:`~Rolling.corr` can compute moving window statistics about
two :class:`Series` or any combination of :class:`DataFrame`/:class:`Series` or
:class:`DataFrame`/:class:`DataFrame`. Here is the behavior in each case:

* two :class:`Series`: compute the statistic for the pairing.
* :class:`DataFrame`/:class:`Series`: compute the statistics for each column of the DataFrame
  with the passed Series, thus returning a DataFrame.
* :class:`DataFrame`/:class:`DataFrame`: by default compute the statistic for matching column
  names, returning a DataFrame. If the keyword argument ``pairwise=True`` is
  passed then computes the statistic for each pair of columns, returning a :class:`DataFrame` with a
  :class:`MultiIndex` whose values are the dates in question (see :ref:`the next section
  <window.corr_pairwise>`).

For example:

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(10, 4),
       index=pd.date_range("2020-01-01", periods=10),
       columns=["A", "B", "C", "D"],
   )
   df = df.cumsum()

   df2 = df[:4]
   df2.rolling(window=2).corr(df2["B"])

.. _window.corr_pairwise:

Computing rolling pairwise covariances and correlations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In financial data analysis and other fields it's common to compute covariance
and correlation matrices for a collection of time series. Often one is also
interested in moving-window covariance and correlation matrices. This can be
done by passing the ``pairwise`` keyword argument, which in the case of
:class:`DataFrame` inputs will yield a MultiIndexed :class:`DataFrame` whose ``index`` are the dates in
question. In the case of a single DataFrame argument the ``pairwise`` argument
can even be omitted:

.. note::

    Missing values are ignored and each entry is computed using the pairwise
    complete observations.  Please see the :ref:`covariance section
    <computation.covariance>` for :ref:`caveats
    <computation.covariance.caveats>` associated with this method of
    calculating covariance and correlation matrices.

.. ipython:: python

   covs = (
       df[["B", "C", "D"]]
       .rolling(window=4)
       .cov(df[["A", "B", "C"]], pairwise=True)
   )
   covs


.. _window.weighted:

Weighted window
---------------

The ``win_type`` argument in ``.rolling`` generates a weighted windows that are commonly used in filtering
and spectral estimation. ``win_type`` must be string that corresponds to a `scipy.signal window function
<https://docs.scipy.org/doc/scipy/reference/signal.windows.html#module-scipy.signal.windows>`__.
Scipy must be installed in order to use these windows, and supplementary arguments
that the Scipy window methods take must be specified in the aggregation function.


.. ipython:: python

   s = pd.Series(range(10))
   s.rolling(window=5).mean()
   s.rolling(window=5, win_type="triang").mean()
   # Supplementary Scipy arguments passed in the aggregation function
   s.rolling(window=5, win_type="gaussian").mean(std=0.1)

For all supported aggregation functions, see :ref:`api.functions_window`.

.. _window.expanding:

Expanding window
----------------

An expanding window yields the value of an aggregation statistic with all the data available up to that
point in time. Since these calculations are a special case of rolling statistics,
they are implemented in pandas such that the following two calls are equivalent:

.. ipython:: python

   df = pd.DataFrame(range(5))
   df.rolling(window=len(df), min_periods=1).mean()
   df.expanding(min_periods=1).mean()

For all supported aggregation functions, see :ref:`api.functions_expanding`.


.. _window.exponentially_weighted:

Exponentially Weighted window
-----------------------------

An exponentially weighted window is similar to an expanding window but with each prior point
being exponentially weighted down relative to the current point.

In general, a weighted moving average is calculated as

.. math::

    y_t = \frac{\sum_{i=0}^t w_i x_{t-i}}{\sum_{i=0}^t w_i},

where :math:`x_t` is the input, :math:`y_t` is the result and the :math:`w_i`
are the weights.

For all supported aggregation functions, see :ref:`api.functions_ewm`.

The EW functions support two variants of exponential weights.
The default, ``adjust=True``, uses the weights :math:`w_i = (1 - \alpha)^i`
which gives

.. math::

    y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...
    + (1 - \alpha)^t x_{0}}{1 + (1 - \alpha) + (1 - \alpha)^2 + ...
    + (1 - \alpha)^t}

When ``adjust=False`` is specified, moving averages are calculated as

.. math::

    y_0 &= x_0 \\
    y_t &= (1 - \alpha) y_{t-1} + \alpha x_t,

which is equivalent to using weights

.. math::

    w_i = \begin{cases}
        \alpha (1 - \alpha)^i & \text{if } i < t \\
        (1 - \alpha)^i        & \text{if } i = t.
    \end{cases}

.. note::

   These equations are sometimes written in terms of :math:`\alpha' = 1 - \alpha`, e.g.

   .. math::

      y_t = \alpha' y_{t-1} + (1 - \alpha') x_t.

The difference between the above two variants arises because we are
dealing with series which have finite history. Consider a series of infinite
history, with ``adjust=True``:

.. math::

    y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...}
    {1 + (1 - \alpha) + (1 - \alpha)^2 + ...}

Noting that the denominator is a geometric series with initial term equal to 1
and a ratio of :math:`1 - \alpha` we have

.. math::

    y_t &= \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...}
    {\frac{1}{1 - (1 - \alpha)}}\\
    &= [x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...] \alpha \\
    &= \alpha x_t + [(1-\alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ...]\alpha \\
    &= \alpha x_t + (1 - \alpha)[x_{t-1} + (1 - \alpha) x_{t-2} + ...]\alpha\\
    &= \alpha x_t + (1 - \alpha) y_{t-1}

which is the same expression as ``adjust=False`` above and therefore
shows the equivalence of the two variants for infinite series.
When ``adjust=False``, we have :math:`y_0 = x_0` and
:math:`y_t = \alpha x_t + (1 - \alpha) y_{t-1}`.
Therefore, there is an assumption that :math:`x_0` is not an ordinary value
but rather an exponentially weighted moment of the infinite series up to that
point.

One must have :math:`0 < \alpha \leq 1`, and while it is possible to pass
:math:`\alpha` directly, it's often easier to think about either the
**span**, **center  of mass (com)** or **half-life** of an EW moment:

.. math::

   \alpha =
    \begin{cases}
        \frac{2}{s + 1},               & \text{for span}\ s \geq 1\\
        \frac{1}{1 + c},               & \text{for center of mass}\ c \geq 0\\
        1 - \exp^{\frac{\log 0.5}{h}}, & \text{for half-life}\ h > 0
    \end{cases}

One must specify precisely one of **span**, **center of mass**, **half-life**
and **alpha** to the EW functions:

* **Span** corresponds to what is commonly called an "N-day EW moving average".
* **Center of mass** has a more physical interpretation and can be thought of
  in terms of span: :math:`c = (s - 1) / 2`.
* **Half-life** is the period of time for the exponential weight to reduce to
  one half.
* **Alpha** specifies the smoothing factor directly.

.. versionadded:: 1.1.0

You can also specify ``halflife`` in terms of a timedelta convertible unit to specify the amount of
time it takes for an observation to decay to half its value when also specifying a sequence
of ``times``.

.. ipython:: python

    df = pd.DataFrame({"B": [0, 1, 2, np.nan, 4]})
    df
    times = ["2020-01-01", "2020-01-03", "2020-01-10", "2020-01-15", "2020-01-17"]
    df.ewm(halflife="4 days", times=pd.DatetimeIndex(times)).mean()

The following formula is used to compute exponentially weighted mean with an input vector of times:

.. math::

    y_t = \frac{\sum_{i=0}^t 0.5^\frac{t_{t} - t_{i}}{\lambda} x_{t-i}}{\sum_{i=0}^t 0.5^\frac{t_{t} - t_{i}}{\lambda}},


ExponentialMovingWindow also has an ``ignore_na`` argument, which determines how
intermediate null values affect the calculation of the weights.
When ``ignore_na=False`` (the default), weights are calculated based on absolute
positions, so that intermediate null values affect the result.
When ``ignore_na=True``,
weights are calculated by ignoring intermediate null values.
For example, assuming ``adjust=True``, if ``ignore_na=False``, the weighted
average of ``3, NaN, 5`` would be calculated as

.. math::

	\frac{(1-\alpha)^2 \cdot 3 + 1 \cdot 5}{(1-\alpha)^2 + 1}.

Whereas if ``ignore_na=True``, the weighted average would be calculated as

.. math::

	\frac{(1-\alpha) \cdot 3 + 1 \cdot 5}{(1-\alpha) + 1}.

The :meth:`~Ewm.var`, :meth:`~Ewm.std`, and :meth:`~Ewm.cov` functions have a ``bias`` argument,
specifying whether the result should contain biased or unbiased statistics.
For example, if ``bias=True``, ``ewmvar(x)`` is calculated as
``ewmvar(x) = ewma(x**2) - ewma(x)**2``;
whereas if ``bias=False`` (the default), the biased variance statistics
are scaled by debiasing factors

.. math::

    \frac{\left(\sum_{i=0}^t w_i\right)^2}{\left(\sum_{i=0}^t w_i\right)^2 - \sum_{i=0}^t w_i^2}.

(For :math:`w_i = 1`, this reduces to the usual :math:`N / (N - 1)` factor,
with :math:`N = t + 1`.)
See `Weighted Sample Variance <https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Weighted_sample_variance>`__
on Wikipedia for further details.
.. currentmodule:: pandas

{{ header }}

.. _integer_na:

**************************
Nullable integer data type
**************************

.. note::

   IntegerArray is currently experimental. Its API or implementation may
   change without warning.

.. versionchanged:: 1.0.0

   Now uses :attr:`pandas.NA` as the missing value rather
   than :attr:`numpy.nan`.

In :ref:`missing_data`, we saw that pandas primarily uses ``NaN`` to represent
missing data. Because ``NaN`` is a float, this forces an array of integers with
any missing values to become floating point. In some cases, this may not matter
much. But if your integer column is, say, an identifier, casting to float can
be problematic. Some integers cannot even be represented as floating point
numbers.

Construction
------------

pandas can represent integer data with possibly missing values using
:class:`arrays.IntegerArray`. This is an :ref:`extension types <extending.extension-types>`
implemented within pandas.

.. ipython:: python

   arr = pd.array([1, 2, None], dtype=pd.Int64Dtype())
   arr

Or the string alias ``"Int64"`` (note the capital ``"I"``, to differentiate from
NumPy's ``'int64'`` dtype:

.. ipython:: python

   pd.array([1, 2, np.nan], dtype="Int64")

All NA-like values are replaced with :attr:`pandas.NA`.

.. ipython:: python

   pd.array([1, 2, np.nan, None, pd.NA], dtype="Int64")

This array can be stored in a :class:`DataFrame` or :class:`Series` like any
NumPy array.

.. ipython:: python

   pd.Series(arr)

You can also pass the list-like object to the :class:`Series` constructor
with the dtype.

.. warning::

   Currently :meth:`pandas.array` and :meth:`pandas.Series` use different
   rules for dtype inference. :meth:`pandas.array` will infer a nullable-
   integer dtype

   .. ipython:: python

      pd.array([1, None])
      pd.array([1, 2])

   For backwards-compatibility, :class:`Series` infers these as either
   integer or float dtype

   .. ipython:: python

      pd.Series([1, None])
      pd.Series([1, 2])

   We recommend explicitly providing the dtype to avoid confusion.

   .. ipython:: python

      pd.array([1, None], dtype="Int64")
      pd.Series([1, None], dtype="Int64")

   In the future, we may provide an option for :class:`Series` to infer a
   nullable-integer dtype.

Operations
----------

Operations involving an integer array will behave similar to NumPy arrays.
Missing values will be propagated, and the data will be coerced to another
dtype if needed.

.. ipython:: python

   s = pd.Series([1, 2, None], dtype="Int64")

   # arithmetic
   s + 1

   # comparison
   s == 1

   # indexing
   s.iloc[1:3]

   # operate with other dtypes
   s + s.iloc[1:3].astype("Int8")

   # coerce when needed
   s + 0.01

These dtypes can operate as part of ``DataFrame``.

.. ipython:: python

   df = pd.DataFrame({"A": s, "B": [1, 1, 3], "C": list("aab")})
   df
   df.dtypes


These dtypes can be merged & reshaped & casted.

.. ipython:: python

   pd.concat([df[["A"]], df[["B", "C"]]], axis=1).dtypes
   df["A"].astype(float)

Reduction and groupby operations such as 'sum' work as well.

.. ipython:: python

   df.sum()
   df.groupby("B").A.sum()

Scalar NA Value
---------------

:class:`arrays.IntegerArray` uses :attr:`pandas.NA` as its scalar
missing value. Slicing a single element that's missing will return
:attr:`pandas.NA`

.. ipython:: python

   a = pd.array([1, None], dtype="Int64")
   a[1]
.. _visualization:

{{ header }}

*******************
Chart Visualization
*******************

This section demonstrates visualization through charting. For information on
visualization of tabular data please see the section on `Table Visualization <style.ipynb>`_.

We use the standard convention for referencing the matplotlib API:

.. ipython:: python

   import matplotlib.pyplot as plt

   plt.close("all")

We provide the basics in pandas to easily create decent looking plots.
See the :ref:`ecosystem <ecosystem.visualization>` section for visualization
libraries that go beyond the basics documented here.

.. note::

   All calls to ``np.random`` are seeded with 123456.

.. _visualization.basic:

Basic plotting: ``plot``
------------------------

We will demonstrate the basics, see the :ref:`cookbook<cookbook.plotting>` for
some advanced strategies.

The ``plot`` method on Series and DataFrame is just a simple wrapper around
:meth:`plt.plot() <matplotlib.axes.Axes.plot>`:

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   ts = pd.Series(np.random.randn(1000), index=pd.date_range("1/1/2000", periods=1000))
   ts = ts.cumsum()

   @savefig series_plot_basic.png
   ts.plot();

If the index consists of dates, it calls :meth:`gcf().autofmt_xdate() <matplotlib.figure.Figure.autofmt_xdate>`
to try to format the x-axis nicely as per above.

On DataFrame, :meth:`~DataFrame.plot` is a convenience to plot all of the columns with labels:

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD"))
   df = df.cumsum()

   plt.figure();
   @savefig frame_plot_basic.png
   df.plot();

You can plot one column versus another using the ``x`` and ``y`` keywords in
:meth:`~DataFrame.plot`:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   df3 = pd.DataFrame(np.random.randn(1000, 2), columns=["B", "C"]).cumsum()
   df3["A"] = pd.Series(list(range(len(df))))

   @savefig df_plot_xy.png
   df3.plot(x="A", y="B");

.. note::

   For more formatting and styling options, see
   :ref:`formatting <visualization.formatting>` below.

.. ipython:: python
    :suppress:

    plt.close("all")

.. _visualization.other:

Other plots
-----------

Plotting methods allow for a handful of plot styles other than the
default line plot. These methods can be provided as the ``kind``
keyword argument to :meth:`~DataFrame.plot`, and include:

* :ref:`'bar' <visualization.barplot>` or :ref:`'barh' <visualization.barplot>` for bar plots
* :ref:`'hist' <visualization.hist>` for histogram
* :ref:`'box' <visualization.box>` for boxplot
* :ref:`'kde' <visualization.kde>` or :ref:`'density' <visualization.kde>` for density plots
* :ref:`'area' <visualization.area_plot>` for area plots
* :ref:`'scatter' <visualization.scatter>` for scatter plots
* :ref:`'hexbin' <visualization.hexbin>` for hexagonal bin plots
* :ref:`'pie' <visualization.pie>` for pie plots

For example, a bar plot can be created the following way:

.. ipython:: python

   plt.figure();

   @savefig bar_plot_ex.png
   df.iloc[5].plot(kind="bar");

You can also create these other plots using the methods ``DataFrame.plot.<kind>`` instead of providing the ``kind`` keyword argument. This makes it easier to discover plot methods and the specific arguments they use:

.. ipython::
    :verbatim:

    In [14]: df = pd.DataFrame()

    In [15]: df.plot.<TAB>  # noqa: E225, E999
    df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter
    df.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie

In addition to these ``kind`` s, there are the :ref:`DataFrame.hist() <visualization.hist>`,
and :ref:`DataFrame.boxplot() <visualization.box>` methods, which use a separate interface.

Finally, there are several :ref:`plotting functions <visualization.tools>` in ``pandas.plotting``
that take a :class:`Series` or :class:`DataFrame` as an argument. These
include:

* :ref:`Scatter Matrix <visualization.scatter_matrix>`
* :ref:`Andrews Curves <visualization.andrews_curves>`
* :ref:`Parallel Coordinates <visualization.parallel_coordinates>`
* :ref:`Lag Plot <visualization.lag>`
* :ref:`Autocorrelation Plot <visualization.autocorrelation>`
* :ref:`Bootstrap Plot <visualization.bootstrap>`
* :ref:`RadViz <visualization.radviz>`

Plots may also be adorned with :ref:`errorbars <visualization.errorbars>`
or :ref:`tables <visualization.table>`.

.. _visualization.barplot:

Bar plots
~~~~~~~~~

For labeled, non-time series data, you may wish to produce a bar plot:

.. ipython:: python

   plt.figure();

   @savefig bar_plot_ex.png
   df.iloc[5].plot.bar();
   plt.axhline(0, color="k");

Calling a DataFrame's :meth:`plot.bar() <DataFrame.plot.bar>` method produces a multiple
bar plot:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   df2 = pd.DataFrame(np.random.rand(10, 4), columns=["a", "b", "c", "d"])

   @savefig bar_plot_multi_ex.png
   df2.plot.bar();

To produce a stacked bar plot, pass ``stacked=True``:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()

.. ipython:: python

   @savefig bar_plot_stacked_ex.png
   df2.plot.bar(stacked=True);

To get horizontal bar plots, use the ``barh`` method:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()

.. ipython:: python

   @savefig barh_plot_stacked_ex.png
   df2.plot.barh(stacked=True);

.. _visualization.hist:

Histograms
~~~~~~~~~~

Histograms can be drawn by using the :meth:`DataFrame.plot.hist` and :meth:`Series.plot.hist` methods.

.. ipython:: python

   df4 = pd.DataFrame(
       {
           "a": np.random.randn(1000) + 1,
           "b": np.random.randn(1000),
           "c": np.random.randn(1000) - 1,
       },
       columns=["a", "b", "c"],
   )

   plt.figure();

   @savefig hist_new.png
   df4.plot.hist(alpha=0.5);


.. ipython:: python
   :suppress:

   plt.close("all")

A histogram can be stacked using ``stacked=True``. Bin size can be changed
using the ``bins`` keyword.

.. ipython:: python

   plt.figure();

   @savefig hist_new_stacked.png
   df4.plot.hist(stacked=True, bins=20);

.. ipython:: python
   :suppress:

   plt.close("all")

You can pass other keywords supported by matplotlib ``hist``. For example,
horizontal and cumulative histograms can be drawn by
``orientation='horizontal'`` and ``cumulative=True``.

.. ipython:: python

   plt.figure();

   @savefig hist_new_kwargs.png
   df4["a"].plot.hist(orientation="horizontal", cumulative=True);

.. ipython:: python
   :suppress:

   plt.close("all")

See the :meth:`hist <matplotlib.axes.Axes.hist>` method and the
`matplotlib hist documentation <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html>`__ for more.


The existing interface ``DataFrame.hist`` to plot histogram still can be used.

.. ipython:: python

   plt.figure();

   @savefig hist_plot_ex.png
   df["A"].diff().hist();

.. ipython:: python
   :suppress:

   plt.close("all")

:meth:`DataFrame.hist` plots the histograms of the columns on multiple
subplots:

.. ipython:: python

   plt.figure();

   @savefig frame_hist_ex.png
   df.diff().hist(color="k", alpha=0.5, bins=50);


The ``by`` keyword can be specified to plot grouped histograms:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   data = pd.Series(np.random.randn(1000))

   @savefig grouped_hist.png
   data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4));

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

In addition, the ``by`` keyword can also be specified in :meth:`DataFrame.plot.hist`.

.. versionchanged:: 1.4.0

.. ipython:: python

   data = pd.DataFrame(
       {
           "a": np.random.choice(["x", "y", "z"], 1000),
           "b": np.random.choice(["e", "f", "g"], 1000),
           "c": np.random.randn(1000),
           "d": np.random.randn(1000) - 1,
       },
   )

   @savefig grouped_hist_by.png
   data.plot.hist(by=["a", "b"], figsize=(10, 5));

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.box:

Box plots
~~~~~~~~~

Boxplot can be drawn calling :meth:`Series.plot.box` and :meth:`DataFrame.plot.box`,
or :meth:`DataFrame.boxplot` to visualize the distribution of values within each column.

For instance, here is a boxplot representing five trials of 10 observations of
a uniform random variable on [0,1).

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.rand(10, 5), columns=["A", "B", "C", "D", "E"])

   @savefig box_plot_new.png
   df.plot.box();

Boxplot can be colorized by passing ``color`` keyword. You can pass a ``dict``
whose keys are ``boxes``, ``whiskers``, ``medians`` and ``caps``.
If some keys are missing in the ``dict``, default colors are used
for the corresponding artists. Also, boxplot has ``sym`` keyword to specify fliers style.

When you pass other type of arguments via ``color`` keyword, it will be directly
passed to matplotlib for all the ``boxes``, ``whiskers``, ``medians`` and ``caps``
colorization.

The colors are applied to every boxes to be drawn. If you want
more complicated colorization, you can get each drawn artists by passing
:ref:`return_type <visualization.box.return>`.

.. ipython:: python

   color = {
       "boxes": "DarkGreen",
       "whiskers": "DarkOrange",
       "medians": "DarkBlue",
       "caps": "Gray",
   }

   @savefig box_new_colorize.png
   df.plot.box(color=color, sym="r+");

.. ipython:: python
   :suppress:

   plt.close("all")

Also, you can pass other keywords supported by matplotlib ``boxplot``.
For example, horizontal and custom-positioned boxplot can be drawn by
``vert=False`` and ``positions`` keywords.

.. ipython:: python

   @savefig box_new_kwargs.png
   df.plot.box(vert=False, positions=[1, 4, 5, 6, 8]);


See the :meth:`boxplot <matplotlib.axes.Axes.boxplot>` method and the
`matplotlib boxplot documentation <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html>`__ for more.


The existing interface ``DataFrame.boxplot`` to plot boxplot still can be used.

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python
   :okwarning:

   df = pd.DataFrame(np.random.rand(10, 5))
   plt.figure();

   @savefig box_plot_ex.png
   bp = df.boxplot()

You can create a stratified boxplot using the ``by`` keyword argument to create
groupings.  For instance,

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python
   :okwarning:

   df = pd.DataFrame(np.random.rand(10, 2), columns=["Col1", "Col2"])
   df["X"] = pd.Series(["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"])

   plt.figure();

   @savefig box_plot_ex2.png
   bp = df.boxplot(by="X")

You can also pass a subset of columns to plot, as well as group by multiple
columns:

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python
   :okwarning:

   df = pd.DataFrame(np.random.rand(10, 3), columns=["Col1", "Col2", "Col3"])
   df["X"] = pd.Series(["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"])
   df["Y"] = pd.Series(["A", "B", "A", "B", "A", "B", "A", "B", "A", "B"])

   plt.figure();

   @savefig box_plot_ex3.png
   bp = df.boxplot(column=["Col1", "Col2"], by=["X", "Y"])

.. ipython:: python
   :suppress:

    plt.close("all")

You could also create groupings with :meth:`DataFrame.plot.box`, for instance:

.. versionchanged:: 1.4.0

.. ipython:: python
   :suppress:

   plt.close("all")
   np.random.seed(123456)

.. ipython:: python
   :okwarning:

   df = pd.DataFrame(np.random.rand(10, 3), columns=["Col1", "Col2", "Col3"])
   df["X"] = pd.Series(["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"])

   plt.figure();

   @savefig box_plot_ex4.png
   bp = df.plot.box(column=["Col1", "Col2"], by="X")

.. ipython:: python
   :suppress:

    plt.close("all")

.. _visualization.box.return:

In ``boxplot``, the return type can be controlled by the ``return_type``, keyword. The valid choices are ``{"axes", "dict", "both", None}``.
Faceting, created by ``DataFrame.boxplot`` with the ``by``
keyword, will affect the output type as well:

================ ======= ==========================
``return_type``  Faceted Output type
================ ======= ==========================
``None``         No      axes
``None``         Yes     2-D ndarray of axes
``'axes'``       No      axes
``'axes'``       Yes     Series of axes
``'dict'``       No      dict of artists
``'dict'``       Yes     Series of dicts of artists
``'both'``       No      namedtuple
``'both'``       Yes     Series of namedtuples
================ ======= ==========================

``Groupby.boxplot`` always returns a ``Series`` of ``return_type``.

.. ipython:: python
   :okwarning:

   np.random.seed(1234)
   df_box = pd.DataFrame(np.random.randn(50, 2))
   df_box["g"] = np.random.choice(["A", "B"], size=50)
   df_box.loc[df_box["g"] == "B", 1] += 3

   @savefig boxplot_groupby.png
   bp = df_box.boxplot(by="g")

.. ipython:: python
   :suppress:

   plt.close("all")

The subplots above are split by the numeric columns first, then the value of
the ``g`` column. Below the subplots are first split by the value of ``g``,
then by the numeric columns.

.. ipython:: python
   :okwarning:

   @savefig groupby_boxplot_vis.png
   bp = df_box.groupby("g").boxplot()

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.area_plot:

Area plot
~~~~~~~~~

You can create area plots with :meth:`Series.plot.area` and :meth:`DataFrame.plot.area`.
Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values.

When input data contains ``NaN``, it will be automatically filled by 0. If you want to drop or fill by different values, use :func:`dataframe.dropna` or :func:`dataframe.fillna` before calling ``plot``.

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   plt.figure()

.. ipython:: python

   df = pd.DataFrame(np.random.rand(10, 4), columns=["a", "b", "c", "d"])

   @savefig area_plot_stacked.png
   df.plot.area();

To produce an unstacked plot, pass ``stacked=False``. Alpha value is set to 0.5 unless otherwise specified:

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()

.. ipython:: python

   @savefig area_plot_unstacked.png
   df.plot.area(stacked=False);

.. _visualization.scatter:

Scatter plot
~~~~~~~~~~~~

Scatter plot can be drawn by using the :meth:`DataFrame.plot.scatter` method.
Scatter plot requires numeric columns for the x and y axes.
These can be specified by the ``x`` and ``y`` keywords.

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   plt.close("all")
   plt.figure()

.. ipython:: python

   df = pd.DataFrame(np.random.rand(50, 4), columns=["a", "b", "c", "d"])
   df["species"] = pd.Categorical(
       ["setosa"] * 20 + ["versicolor"] * 20 + ["virginica"] * 10
   )

   @savefig scatter_plot.png
   df.plot.scatter(x="a", y="b");

To plot multiple column groups in a single axes, repeat ``plot`` method specifying target ``ax``.
It is recommended to specify ``color`` and ``label`` keywords to distinguish each groups.

.. ipython:: python

   ax = df.plot.scatter(x="a", y="b", color="DarkBlue", label="Group 1")
   @savefig scatter_plot_repeated.png
   df.plot.scatter(x="c", y="d", color="DarkGreen", label="Group 2", ax=ax);

.. ipython:: python
   :suppress:

   plt.close("all")

The keyword ``c`` may be given as the name of a column to provide colors for
each point:

.. ipython:: python

   @savefig scatter_plot_colored.png
   df.plot.scatter(x="a", y="b", c="c", s=50);


.. ipython:: python
   :suppress:

   plt.close("all")

If a categorical column is passed to ``c``, then a discrete colorbar will be produced:

.. versionadded:: 1.3.0

.. ipython:: python

   @savefig scatter_plot_categorical.png
   df.plot.scatter(x="a", y="b", c="species", cmap="viridis", s=50);


.. ipython:: python
   :suppress:

   plt.close("all")

You can pass other keywords supported by matplotlib
:meth:`scatter <matplotlib.axes.Axes.scatter>`. The example  below shows a
bubble chart using a column of the ``DataFrame`` as the bubble size.

.. ipython:: python

   @savefig scatter_plot_bubble.png
   df.plot.scatter(x="a", y="b", s=df["c"] * 200);

.. ipython:: python
   :suppress:

   plt.close("all")

See the :meth:`scatter <matplotlib.axes.Axes.scatter>` method and the
`matplotlib scatter documentation <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html>`__ for more.

.. _visualization.hexbin:

Hexagonal bin plot
~~~~~~~~~~~~~~~~~~

You can create hexagonal bin plots with :meth:`DataFrame.plot.hexbin`.
Hexbin plots can be a useful alternative to scatter plots if your data are
too dense to plot each point individually.

.. ipython:: python
   :suppress:

   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(1000, 2), columns=["a", "b"])
   df["b"] = df["b"] + np.arange(1000)

   @savefig hexbin_plot.png
   df.plot.hexbin(x="a", y="b", gridsize=25);


A useful keyword argument is ``gridsize``; it controls the number of hexagons
in the x-direction, and defaults to 100. A larger ``gridsize`` means more, smaller
bins.

By default, a histogram of the counts around each ``(x, y)`` point is computed.
You can specify alternative aggregations by passing values to the ``C`` and
``reduce_C_function`` arguments. ``C`` specifies the value at each ``(x, y)`` point
and ``reduce_C_function`` is a function of one argument that reduces all the
values in a bin to a single number (e.g. ``mean``, ``max``, ``sum``, ``std``).  In this
example the positions are given by columns ``a`` and ``b``, while the value is
given by column ``z``. The bins are aggregated with NumPy's ``max`` function.

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(1000, 2), columns=["a", "b"])
   df["b"] = df["b"] + np.arange(1000)
   df["z"] = np.random.uniform(0, 3, 1000)

   @savefig hexbin_plot_agg.png
   df.plot.hexbin(x="a", y="b", C="z", reduce_C_function=np.max, gridsize=25);

.. ipython:: python
   :suppress:

   plt.close("all")

See the :meth:`hexbin <matplotlib.axes.Axes.hexbin>` method and the
`matplotlib hexbin documentation <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hexbin.html>`__ for more.

.. _visualization.pie:

Pie plot
~~~~~~~~

You can create a pie plot with :meth:`DataFrame.plot.pie` or :meth:`Series.plot.pie`.
If your data includes any ``NaN``, they will be automatically filled with 0.
A ``ValueError`` will be raised if there are any negative values in your data.

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   plt.figure()

.. ipython:: python
   :okwarning:

   series = pd.Series(3 * np.random.rand(4), index=["a", "b", "c", "d"], name="series")

   @savefig series_pie_plot.png
   series.plot.pie(figsize=(6, 6));

.. ipython:: python
   :suppress:

   plt.close("all")

For pie plots it's best to use square figures, i.e. a figure aspect ratio 1.
You can create the figure with equal width and height, or force the aspect ratio
to be equal after plotting by calling ``ax.set_aspect('equal')`` on the returned
``axes`` object.

Note that pie plot with :class:`DataFrame` requires that you either specify a
target column by the ``y`` argument or ``subplots=True``. When ``y`` is
specified, pie plot of selected column will be drawn. If ``subplots=True`` is
specified, pie plots for each column are drawn as subplots. A legend will be
drawn in each pie plots by default; specify ``legend=False`` to hide it.

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   plt.figure()

.. ipython:: python

   df = pd.DataFrame(
       3 * np.random.rand(4, 2), index=["a", "b", "c", "d"], columns=["x", "y"]
   )

   @savefig df_pie_plot.png
   df.plot.pie(subplots=True, figsize=(8, 4));

.. ipython:: python
   :suppress:

   plt.close("all")

You can use the ``labels`` and ``colors`` keywords to specify the labels and colors of each wedge.

.. warning::

   Most pandas plots use the ``label`` and ``color`` arguments (note the lack of "s" on those).
   To be consistent with :func:`matplotlib.pyplot.pie` you must use ``labels`` and ``colors``.

If you want to hide wedge labels, specify ``labels=None``.
If ``fontsize`` is specified, the value will be applied to wedge labels.
Also, other keywords supported by :func:`matplotlib.pyplot.pie` can be used.


.. ipython:: python
   :suppress:

   plt.figure()

.. ipython:: python

   @savefig series_pie_plot_options.png
   series.plot.pie(
       labels=["AA", "BB", "CC", "DD"],
       colors=["r", "g", "b", "c"],
       autopct="%.2f",
       fontsize=20,
       figsize=(6, 6),
   );

If you pass values whose sum total is less than 1.0, matplotlib draws a semicircle.

.. ipython:: python
   :suppress:

   plt.close("all")
   plt.figure()

.. ipython:: python
   :okwarning:

   series = pd.Series([0.1] * 4, index=["a", "b", "c", "d"], name="series2")

   @savefig series_pie_plot_semi.png
   series.plot.pie(figsize=(6, 6));

See the `matplotlib pie documentation <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html>`__ for more.

.. ipython:: python
    :suppress:

    plt.close("all")

.. _visualization.missing_data:

Plotting with missing data
--------------------------

pandas tries to be pragmatic about plotting ``DataFrames`` or ``Series``
that contain missing data. Missing values are dropped, left out, or filled
depending on the plot type.

+----------------+--------------------------------------+
| Plot Type      | NaN Handling                         |
+================+======================================+
| Line           | Leave gaps at NaNs                   |
+----------------+--------------------------------------+
| Line (stacked) | Fill 0's                             |
+----------------+--------------------------------------+
| Bar            | Fill 0's                             |
+----------------+--------------------------------------+
| Scatter        | Drop NaNs                            |
+----------------+--------------------------------------+
| Histogram      | Drop NaNs (column-wise)              |
+----------------+--------------------------------------+
| Box            | Drop NaNs (column-wise)              |
+----------------+--------------------------------------+
| Area           | Fill 0's                             |
+----------------+--------------------------------------+
| KDE            | Drop NaNs (column-wise)              |
+----------------+--------------------------------------+
| Hexbin         | Drop NaNs                            |
+----------------+--------------------------------------+
| Pie            | Fill 0's                             |
+----------------+--------------------------------------+

If any of these defaults are not what you want, or if you want to be
explicit about how missing values are handled, consider using
:meth:`~pandas.DataFrame.fillna` or :meth:`~pandas.DataFrame.dropna`
before plotting.

.. _visualization.tools:

Plotting tools
--------------

These functions can be imported from ``pandas.plotting``
and take a :class:`Series` or :class:`DataFrame` as an argument.

.. _visualization.scatter_matrix:

Scatter matrix plot
~~~~~~~~~~~~~~~~~~~

You can create a scatter plot matrix using the
``scatter_matrix`` method in ``pandas.plotting``:

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   from pandas.plotting import scatter_matrix

   df = pd.DataFrame(np.random.randn(1000, 4), columns=["a", "b", "c", "d"])

   @savefig scatter_matrix_kde.png
   scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal="kde");

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.kde:

Density plot
~~~~~~~~~~~~

You can create density plots using the :meth:`Series.plot.kde` and :meth:`DataFrame.plot.kde` methods.

.. ipython:: python
   :suppress:

   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   ser = pd.Series(np.random.randn(1000))

   @savefig kde_plot.png
   ser.plot.kde();

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.andrews_curves:

Andrews curves
~~~~~~~~~~~~~~

Andrews curves allow one to plot multivariate data as a large number
of curves that are created using the attributes of samples as coefficients
for Fourier series, see the `Wikipedia entry <https://en.wikipedia.org/wiki/Andrews_plot>`__
for more information. By coloring these curves differently for each class
it is possible to visualize data clustering. Curves belonging to samples
of the same class will usually be closer together and form larger structures.

**Note**: The "Iris" dataset is available `here <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv>`__.

.. ipython:: python

   from pandas.plotting import andrews_curves

   data = pd.read_csv("data/iris.data")

   plt.figure();

   @savefig andrews_curves.png
   andrews_curves(data, "Name");

.. _visualization.parallel_coordinates:

Parallel coordinates
~~~~~~~~~~~~~~~~~~~~

Parallel coordinates is a plotting technique for plotting multivariate data,
see the `Wikipedia entry <https://en.wikipedia.org/wiki/Parallel_coordinates>`__
for an introduction.
Parallel coordinates allows one to see clusters in data and to estimate other statistics visually.
Using parallel coordinates points are represented as connected line segments.
Each vertical line represents one attribute. One set of connected line segments
represents one data point. Points that tend to cluster will appear closer together.

.. ipython:: python

   from pandas.plotting import parallel_coordinates

   data = pd.read_csv("data/iris.data")

   plt.figure();

   @savefig parallel_coordinates.png
   parallel_coordinates(data, "Name");

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.lag:

Lag plot
~~~~~~~~

Lag plots are used to check if a data set or time series is random. Random
data should not exhibit any structure in the lag plot. Non-random structure
implies that the underlying data are not random. The ``lag`` argument may
be passed, and when ``lag=1`` the plot is essentially ``data[:-1]`` vs.
``data[1:]``.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   from pandas.plotting import lag_plot

   plt.figure();

   spacing = np.linspace(-99 * np.pi, 99 * np.pi, num=1000)
   data = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(spacing))

   @savefig lag_plot.png
   lag_plot(data);

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.autocorrelation:

Autocorrelation plot
~~~~~~~~~~~~~~~~~~~~

Autocorrelation plots are often used for checking randomness in time series.
This is done by computing autocorrelations for data values at varying time lags.
If time series is random, such autocorrelations should be near zero for any and
all time-lag separations. If time series is non-random then one or more of the
autocorrelations will be significantly non-zero. The horizontal lines displayed
in the plot correspond to 95% and 99% confidence bands. The dashed line is 99%
confidence band. See the
`Wikipedia entry <https://en.wikipedia.org/wiki/Correlogram>`__ for more about
autocorrelation plots.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   from pandas.plotting import autocorrelation_plot

   plt.figure();

   spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)
   data = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))

   @savefig autocorrelation_plot.png
   autocorrelation_plot(data);

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.bootstrap:

Bootstrap plot
~~~~~~~~~~~~~~

Bootstrap plots are used to visually assess the uncertainty of a statistic, such
as mean, median, midrange, etc. A random subset of a specified size is selected
from a data set, the statistic in question is computed for this subset and the
process is repeated a specified number of times. Resulting plots and histograms
are what constitutes the bootstrap plot.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   from pandas.plotting import bootstrap_plot

   data = pd.Series(np.random.rand(1000))

   @savefig bootstrap_plot.png
   bootstrap_plot(data, size=50, samples=500, color="grey");

.. ipython:: python
   :suppress:

    plt.close("all")

.. _visualization.radviz:

RadViz
~~~~~~

RadViz is a way of visualizing multi-variate data. It is based on a simple
spring tension minimization algorithm. Basically you set up a bunch of points in
a plane. In our case they are equally spaced on a unit circle. Each point
represents a single attribute. You then pretend that each sample in the data set
is attached to each of these points by a spring, the stiffness of which is
proportional to the numerical value of that attribute (they are normalized to
unit interval). The point in the plane, where our sample settles to (where the
forces acting on our sample are at an equilibrium) is where a dot representing
our sample will be drawn. Depending on which class that sample belongs it will
be colored differently.
See the R package `Radviz <https://cran.r-project.org/web/packages/Radviz/index.html>`__
for more information.

**Note**: The "Iris" dataset is available `here <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv>`__.

.. ipython:: python

   from pandas.plotting import radviz

   data = pd.read_csv("data/iris.data")

   plt.figure();

   @savefig radviz.png
   radviz(data, "Name");

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.formatting:

Plot formatting
---------------

Setting the plot style
~~~~~~~~~~~~~~~~~~~~~~

From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the
style can be used to easily give plots the general look that you want.
Setting the style is as easy as calling ``matplotlib.style.use(my_plot_style)`` before
creating your plot. For example you could write ``matplotlib.style.use('ggplot')`` for ggplot-style
plots.

You can see the various available style names at ``matplotlib.style.available`` and it's very
easy to try them out.

General plot style arguments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Most plotting methods have a set of keyword arguments that control the
layout and formatting of the returned plot:

.. ipython:: python

   plt.figure();
   @savefig series_plot_basic2.png
   ts.plot(style="k--", label="Series");

.. ipython:: python
   :suppress:

   plt.close("all")

For each kind of plot (e.g. ``line``, ``bar``, ``scatter``) any additional arguments
keywords are passed along to the corresponding matplotlib function
(:meth:`ax.plot() <matplotlib.axes.Axes.plot>`,
:meth:`ax.bar() <matplotlib.axes.Axes.bar>`,
:meth:`ax.scatter() <matplotlib.axes.Axes.scatter>`). These can be used
to control additional styling, beyond what pandas provides.

Controlling the legend
~~~~~~~~~~~~~~~~~~~~~~

You may set the ``legend`` argument to ``False`` to hide the legend, which is
shown by default.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD"))
   df = df.cumsum()

   @savefig frame_plot_basic_noleg.png
   df.plot(legend=False);

.. ipython:: python
   :suppress:

   plt.close("all")


Controlling the labels
~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.1.0

You may set the ``xlabel`` and ``ylabel`` arguments to give the plot custom labels
for x and y axis. By default, pandas will pick up index name as xlabel, while leaving
it empty for ylabel.

.. ipython:: python
   :suppress:

   plt.figure();

.. ipython:: python

   df.plot();

   @savefig plot_xlabel_ylabel.png
   df.plot(xlabel="new x", ylabel="new y");

.. ipython:: python
   :suppress:

   plt.close("all")


Scales
~~~~~~

You may pass ``logy`` to get a log-scale Y axis.

.. ipython:: python
   :suppress:

   plt.figure()
   np.random.seed(123456)

.. ipython:: python

   ts = pd.Series(np.random.randn(1000), index=pd.date_range("1/1/2000", periods=1000))
   ts = np.exp(ts.cumsum())

   @savefig series_plot_logy.png
   ts.plot(logy=True);

.. ipython:: python
   :suppress:

   plt.close("all")

See also the ``logx`` and ``loglog`` keyword arguments.

Plotting on a secondary y-axis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To plot data on a secondary y-axis, use the ``secondary_y`` keyword:

.. ipython:: python
   :suppress:

   plt.figure()

.. ipython:: python

   df["A"].plot();

   @savefig series_plot_secondary_y.png
   df["B"].plot(secondary_y=True, style="g");

.. ipython:: python
   :suppress:

   plt.close("all")

To plot some columns in a ``DataFrame``, give the column names to the ``secondary_y``
keyword:

.. ipython:: python

   plt.figure();
   ax = df.plot(secondary_y=["A", "B"])
   ax.set_ylabel("CD scale");
   @savefig frame_plot_secondary_y.png
   ax.right_ax.set_ylabel("AB scale");

.. ipython:: python
   :suppress:

   plt.close("all")

Note that the columns plotted on the secondary y-axis is automatically marked
with "(right)" in the legend. To turn off the automatic marking, use the
``mark_right=False`` keyword:

.. ipython:: python

   plt.figure();

   @savefig frame_plot_secondary_y_no_right.png
   df.plot(secondary_y=["A", "B"], mark_right=False);

.. ipython:: python
   :suppress:

   plt.close("all")

.. _plotting.formatters:

Custom formatters for timeseries plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 1.0.0

pandas provides custom formatters for timeseries plots. These change the
formatting of the axis labels for dates and times. By default,
the custom formatters are applied only to plots created by pandas with
:meth:`DataFrame.plot` or :meth:`Series.plot`. To have them apply to all
plots, including those made by matplotlib, set the option
``pd.options.plotting.matplotlib.register_converters = True`` or use
:meth:`pandas.plotting.register_matplotlib_converters`.

Suppressing tick resolution adjustment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

pandas includes automatic tick resolution adjustment for regular frequency
time-series data. For limited cases where pandas cannot infer the frequency
information (e.g., in an externally created ``twinx``), you can choose to
suppress this behavior for alignment purposes.

Here is the default behavior, notice how the x-axis tick labeling is performed:

.. ipython:: python

   plt.figure();

   @savefig ser_plot_suppress.png
   df["A"].plot();

.. ipython:: python
   :suppress:

   plt.close("all")

Using the ``x_compat`` parameter, you can suppress this behavior:

.. ipython:: python

   plt.figure();

   @savefig ser_plot_suppress_parm.png
   df["A"].plot(x_compat=True);

.. ipython:: python
   :suppress:

   plt.close("all")

If you have more than one plot that needs to be suppressed, the ``use`` method
in ``pandas.plotting.plot_params`` can be used in a ``with`` statement:

.. ipython:: python

   plt.figure();

   @savefig ser_plot_suppress_context.png
   with pd.plotting.plot_params.use("x_compat", True):
       df["A"].plot(color="r")
       df["B"].plot(color="g")
       df["C"].plot(color="b")

.. ipython:: python
   :suppress:

   plt.close("all")

Automatic date tick adjustment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``TimedeltaIndex`` now uses the native matplotlib
tick locator methods, it is useful to call the automatic
date tick adjustment from matplotlib for figures whose ticklabels overlap.

See the :meth:`autofmt_xdate <matplotlib.figure.autofmt_xdate>` method and the
`matplotlib documentation <https://matplotlib.org/2.0.2/users/recipes.html#fixing-common-date-annoyances>`__ for more.

Subplots
~~~~~~~~

Each ``Series`` in a ``DataFrame`` can be plotted on a different axis
with the ``subplots`` keyword:

.. ipython:: python

   @savefig frame_plot_subplots.png
   df.plot(subplots=True, figsize=(6, 6));

.. ipython:: python
   :suppress:

   plt.close("all")

Using layout and targeting multiple axes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The layout of subplots can be specified by the ``layout`` keyword. It can accept
``(rows, columns)``. The ``layout`` keyword can be used in
``hist`` and ``boxplot`` also. If the input is invalid, a ``ValueError`` will be raised.

The number of axes which can be contained by rows x columns specified by ``layout`` must be
larger than the number of required subplots. If layout can contain more axes than required,
blank axes are not drawn. Similar to a NumPy array's ``reshape`` method, you
can use ``-1`` for one dimension to automatically calculate the number of rows
or columns needed, given the other.

.. ipython:: python

   @savefig frame_plot_subplots_layout.png
   df.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);

.. ipython:: python
   :suppress:

   plt.close("all")

The above example is identical to using:

.. ipython:: python

   df.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False);

.. ipython:: python
   :suppress:

   plt.close("all")

The required number of columns (3) is inferred from the number of series to plot
and the given number of rows (2).

You can pass multiple axes created beforehand as list-like via ``ax`` keyword.
This allows more complicated layouts.
The passed axes must be the same number as the subplots being drawn.

When multiple axes are passed via the ``ax`` keyword, ``layout``, ``sharex`` and ``sharey`` keywords
don't affect to the output. You should explicitly pass ``sharex=False`` and ``sharey=False``,
otherwise you will see a warning.

.. ipython:: python

   fig, axes = plt.subplots(4, 4, figsize=(9, 9))
   plt.subplots_adjust(wspace=0.5, hspace=0.5)
   target1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]]
   target2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]]

   df.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False);
   @savefig frame_plot_subplots_multi_ax.png
   (-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);

.. ipython:: python
   :suppress:

   plt.close("all")

Another option is passing an ``ax`` argument to :meth:`Series.plot` to plot on a particular axis:

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   ts = pd.Series(np.random.randn(1000), index=pd.date_range("1/1/2000", periods=1000))
   ts = ts.cumsum()

   df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list("ABCD"))
   df = df.cumsum()

.. ipython:: python
   :suppress:

   plt.close("all")

.. ipython:: python

   fig, axes = plt.subplots(nrows=2, ncols=2)
   plt.subplots_adjust(wspace=0.2, hspace=0.5)
   df["A"].plot(ax=axes[0, 0]);
   axes[0, 0].set_title("A");
   df["B"].plot(ax=axes[0, 1]);
   axes[0, 1].set_title("B");
   df["C"].plot(ax=axes[1, 0]);
   axes[1, 0].set_title("C");
   df["D"].plot(ax=axes[1, 1]);
   @savefig series_plot_multi.png
   axes[1, 1].set_title("D");

.. ipython:: python
   :suppress:

    plt.close("all")

.. _visualization.errorbars:

Plotting with error bars
~~~~~~~~~~~~~~~~~~~~~~~~

Plotting with error bars is supported in :meth:`DataFrame.plot` and :meth:`Series.plot`.

Horizontal and vertical error bars can be supplied to the ``xerr`` and ``yerr`` keyword arguments to :meth:`~DataFrame.plot()`. The error values can be specified using a variety of formats:

* As a :class:`DataFrame` or ``dict`` of errors with column names matching the ``columns`` attribute of the plotting :class:`DataFrame` or matching the ``name`` attribute of the :class:`Series`.
* As a ``str`` indicating which of the columns of plotting :class:`DataFrame` contain the error values.
* As raw values (``list``, ``tuple``, or ``np.ndarray``). Must be the same length as the plotting :class:`DataFrame`/:class:`Series`.

Here is an example of one way to easily plot group means with standard deviations from the raw data.

.. ipython:: python

   # Generate the data
   ix3 = pd.MultiIndex.from_arrays(
       [
           ["a", "a", "a", "a", "a", "b", "b", "b", "b", "b"],
           ["foo", "foo", "foo", "bar", "bar", "foo", "foo", "bar", "bar", "bar"],
       ],
       names=["letter", "word"],
   )

   df3 = pd.DataFrame(
       {
           "data1": [9, 3, 2, 4, 3, 2, 4, 6, 3, 2],
           "data2": [9, 6, 5, 7, 5, 4, 5, 6, 5, 1],
       },
       index=ix3,
   )

   # Group by index labels and take the means and standard deviations
   # for each group
   gp3 = df3.groupby(level=("letter", "word"))
   means = gp3.mean()
   errors = gp3.std()
   means
   errors

   # Plot
   fig, ax = plt.subplots()
   @savefig errorbar_example.png
   means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);

.. ipython:: python
   :suppress:

   plt.close("all")

Asymmetrical error bars are also supported, however raw error values must be provided in this case. For a ``N`` length :class:`Series`, a ``2xN`` array should be provided indicating lower and upper (or left and right) errors. For a ``MxN`` :class:`DataFrame`, asymmetrical errors should be in a ``Mx2xN`` array.

Here is an example of one way to plot the min/max range using asymmetrical error bars.

.. ipython:: python

   mins = gp3.min()
   maxs = gp3.max()

   # errors should be positive, and defined in the order of lower, upper
   errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in df3.columns]

   # Plot
   fig, ax = plt.subplots()
   @savefig errorbar_asymmetrical_example.png
   means.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);

.. ipython:: python
   :suppress:

   plt.close("all")

.. _visualization.table:

Plotting tables
~~~~~~~~~~~~~~~

Plotting with matplotlib table is now supported in  :meth:`DataFrame.plot` and :meth:`Series.plot` with a ``table`` keyword. The ``table`` keyword can accept ``bool``, :class:`DataFrame` or :class:`Series`. The simple way to draw a table is to specify ``table=True``. Data will be transposed to meet matplotlib's default layout.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   fig, ax = plt.subplots(1, 1, figsize=(7, 6.5))
   df = pd.DataFrame(np.random.rand(5, 3), columns=["a", "b", "c"])
   ax.xaxis.tick_top()  # Display x-axis ticks on top.

   @savefig line_plot_table_true.png
   df.plot(table=True, ax=ax);

.. ipython:: python
   :suppress:

   plt.close("all")

Also, you can pass a different :class:`DataFrame` or :class:`Series` to the
``table`` keyword. The data will be drawn as displayed in print method
(not transposed automatically). If required, it should be transposed manually
as seen in the example below.

.. ipython:: python

   fig, ax = plt.subplots(1, 1, figsize=(7, 6.75))
   ax.xaxis.tick_top()  # Display x-axis ticks on top.

   @savefig line_plot_table_data.png
   df.plot(table=np.round(df.T, 2), ax=ax);

.. ipython:: python
   :suppress:

   plt.close("all")

There also exists a helper function ``pandas.plotting.table``, which creates a
table from :class:`DataFrame` or :class:`Series`, and adds it to an
``matplotlib.Axes`` instance. This function can accept keywords which the
matplotlib `table <https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.table.html>`__ has.

.. ipython:: python

   from pandas.plotting import table

   fig, ax = plt.subplots(1, 1)

   table(ax, np.round(df.describe(), 2), loc="upper right", colWidths=[0.2, 0.2, 0.2]);

   @savefig line_plot_table_describe.png
   df.plot(ax=ax, ylim=(0, 2), legend=None);

.. ipython:: python
   :suppress:

   plt.close("all")

**Note**: You can get table instances on the axes using ``axes.tables`` property for further decorations. See the `matplotlib table documentation <https://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.table>`__ for more.

.. _visualization.colormaps:

Colormaps
~~~~~~~~~

A potential issue when plotting a large number of columns is that it can be
difficult to distinguish some series due to repetition in the default colors. To
remedy this, ``DataFrame`` plotting supports the use of the ``colormap`` argument,
which accepts either a Matplotlib `colormap <https://matplotlib.org/api/cm_api.html>`__
or a string that is a name of a colormap registered with Matplotlib. A
visualization of the default matplotlib colormaps is available `here
<https://matplotlib.org/stable/gallery/color/colormap_reference.html>`__.

As matplotlib does not directly support colormaps for line-based plots, the
colors are selected based on an even spacing determined by the number of columns
in the ``DataFrame``. There is no consideration made for background color, so some
colormaps will produce lines that are not easily visible.

To use the cubehelix colormap, we can pass ``colormap='cubehelix'``.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(1000, 10), index=ts.index)
   df = df.cumsum()

   plt.figure();

   @savefig cubehelix.png
   df.plot(colormap="cubehelix");

.. ipython:: python
   :suppress:

   plt.close("all")

Alternatively, we can pass the colormap itself:

.. ipython:: python

   from matplotlib import cm

   plt.figure();

   @savefig cubehelix_cm.png
   df.plot(colormap=cm.cubehelix);

.. ipython:: python
   :suppress:

   plt.close("all")

Colormaps can also be used other plot types, like bar charts:

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   dd = pd.DataFrame(np.random.randn(10, 10)).applymap(abs)
   dd = dd.cumsum()

   plt.figure();

   @savefig greens.png
   dd.plot.bar(colormap="Greens");

.. ipython:: python
   :suppress:

   plt.close("all")

Parallel coordinates charts:

.. ipython:: python

   plt.figure();

   @savefig parallel_gist_rainbow.png
   parallel_coordinates(data, "Name", colormap="gist_rainbow");

.. ipython:: python
   :suppress:

   plt.close("all")

Andrews curves charts:

.. ipython:: python

   plt.figure();

   @savefig andrews_curve_winter.png
   andrews_curves(data, "Name", colormap="winter");

.. ipython:: python
   :suppress:

   plt.close("all")

Plotting directly with matplotlib
---------------------------------

In some situations it may still be preferable or necessary to prepare plots
directly with matplotlib, for instance when a certain type of plot or
customization is not (yet) supported by pandas. ``Series`` and ``DataFrame``
objects behave like arrays and can therefore be passed directly to
matplotlib functions without explicit casts.

pandas also automatically registers formatters and locators that recognize date
indices, thereby extending date and time support to practically all plot types
available in matplotlib. Although this formatting does not provide the same
level of refinement you would get when plotting via pandas, it can be faster
when plotting a large number of points.

.. ipython:: python
   :suppress:

   np.random.seed(123456)

.. ipython:: python

   price = pd.Series(
       np.random.randn(150).cumsum(),
       index=pd.date_range("2000-1-1", periods=150, freq="B"),
   )
   ma = price.rolling(20).mean()
   mstd = price.rolling(20).std()

   plt.figure();

   plt.plot(price.index, price, "k");
   plt.plot(ma.index, ma, "b");
   @savefig bollinger.png
   plt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color="b", alpha=0.2);

.. ipython:: python
   :suppress:

    plt.close("all")

Plotting backends
-----------------

Starting in version 0.25, pandas can be extended with third-party plotting backends. The
main idea is letting users select a plotting backend different than the provided
one based on Matplotlib.

This can be done by passing 'backend.module' as the argument ``backend`` in ``plot``
function. For example:

.. code-block:: python

    >>> Series([1, 2, 3]).plot(backend="backend.module")

Alternatively, you can also set this option globally, do you don't need to specify
the keyword in each ``plot`` call. For example:

.. code-block:: python

    >>> pd.set_option("plotting.backend", "backend.module")
    >>> pd.Series([1, 2, 3]).plot()

Or:

.. code-block:: python

    >>> pd.options.plotting.backend = "backend.module"
    >>> pd.Series([1, 2, 3]).plot()

This would be more or less equivalent to:

.. code-block:: python

    >>> import backend.module
    >>> backend.module.plot(pd.Series([1, 2, 3]))

The backend module can then use other visualization tools (Bokeh, Altair, hvplot,...)
to generate the plots. Some libraries implementing a backend for pandas are listed
on the ecosystem :ref:`ecosystem.visualization` page.

Developers guide can be found at
https://pandas.pydata.org/docs/dev/development/extending.html#plotting-backends
.. _groupby:

{{ header }}

*****************************
Group by: split-apply-combine
*****************************

By "group by" we are referring to a process involving one or more of the following
steps:

* **Splitting** the data into groups based on some criteria.
* **Applying** a function to each group independently.
* **Combining** the results into a data structure.

Out of these, the split step is the most straightforward. In fact, in many
situations we may wish to split the data set into groups and do something with
those groups. In the apply step, we might wish to do one of the
following:

* **Aggregation**: compute a summary statistic (or statistics) for each
  group. Some examples:

    * Compute group sums or means.
    * Compute group sizes / counts.

* **Transformation**: perform some group-specific computations and return a
  like-indexed object. Some examples:

    * Standardize data (zscore) within a group.
    * Filling NAs within groups with a value derived from each group.

* **Filtration**: discard some groups, according to a group-wise computation
  that evaluates True or False. Some examples:

    * Discard data that belongs to groups with only a few members.
    * Filter out data based on the group sum or mean.

* Some combination of the above: GroupBy will examine the results of the apply
  step and try to return a sensibly combined result if it doesn't fit into
  either of the above two categories.

Since the set of object instance methods on pandas data structures are generally
rich and expressive, we often simply want to invoke, say, a DataFrame function
on each group. The name GroupBy should be quite familiar to those who have used
a SQL-based tool (or ``itertools``), in which you can write code like:

.. code-block:: sql

   SELECT Column1, Column2, mean(Column3), sum(Column4)
   FROM SomeTable
   GROUP BY Column1, Column2

We aim to make operations like this natural and easy to express using
pandas. We'll address each area of GroupBy functionality then provide some
non-trivial examples / use cases.

See the :ref:`cookbook<cookbook.grouping>` for some advanced strategies.

.. _groupby.split:

Splitting an object into groups
-------------------------------

pandas objects can be split on any of their axes. The abstract definition of
grouping is to provide a mapping of labels to group names. To create a GroupBy
object (more on what the GroupBy object is later), you may do the following:

.. ipython:: python

    df = pd.DataFrame(
        [
            ("bird", "Falconiformes", 389.0),
            ("bird", "Psittaciformes", 24.0),
            ("mammal", "Carnivora", 80.2),
            ("mammal", "Primates", np.nan),
            ("mammal", "Carnivora", 58),
        ],
        index=["falcon", "parrot", "lion", "monkey", "leopard"],
        columns=("class", "order", "max_speed"),
    )
    df

    # default is axis=0
    grouped = df.groupby("class")
    grouped = df.groupby("order", axis="columns")
    grouped = df.groupby(["class", "order"])

The mapping can be specified many different ways:

* A Python function, to be called on each of the axis labels.
* A list or NumPy array of the same length as the selected axis.
* A dict or ``Series``, providing a ``label -> group name`` mapping.
* For ``DataFrame`` objects, a string indicating either a column name or
  an index level name to be used to group.
* ``df.groupby('A')`` is just syntactic sugar for ``df.groupby(df['A'])``.
* A list of any of the above things.

Collectively we refer to the grouping objects as the **keys**. For example,
consider the following ``DataFrame``:

.. note::

   A string passed to ``groupby`` may refer to either a column or an index level.
   If a string matches both a column name and an index level name, a
   ``ValueError`` will be raised.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
           "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
           "C": np.random.randn(8),
           "D": np.random.randn(8),
       }
   )
   df

On a DataFrame, we obtain a GroupBy object by calling :meth:`~DataFrame.groupby`.
We could naturally group by either the ``A`` or ``B`` columns, or both:

.. ipython:: python

   grouped = df.groupby("A")
   grouped = df.groupby(["A", "B"])

If we also have a MultiIndex on columns ``A`` and ``B``, we can group by all
but the specified columns

.. ipython:: python

   df2 = df.set_index(["A", "B"])
   grouped = df2.groupby(level=df2.index.names.difference(["B"]))
   grouped.sum()

These will split the DataFrame on its index (rows). We could also split by the
columns:

.. ipython::

    In [4]: def get_letter_type(letter):
       ...:     if letter.lower() in 'aeiou':
       ...:         return 'vowel'
       ...:     else:
       ...:         return 'consonant'
       ...:

    In [5]: grouped = df.groupby(get_letter_type, axis=1)

pandas :class:`~pandas.Index` objects support duplicate values. If a
non-unique index is used as the group key in a groupby operation, all values
for the same index value will be considered to be in one group and thus the
output of aggregation functions will only contain unique index values:

.. ipython:: python

   lst = [1, 2, 3, 1, 2, 3]

   s = pd.Series([1, 2, 3, 10, 20, 30], lst)

   grouped = s.groupby(level=0)

   grouped.first()

   grouped.last()

   grouped.sum()

Note that **no splitting occurs** until it's needed. Creating the GroupBy object
only verifies that you've passed a valid mapping.

.. note::

   Many kinds of complicated data manipulations can be expressed in terms of
   GroupBy operations (though can't be guaranteed to be the most
   efficient). You can get quite creative with the label mapping functions.

.. _groupby.sorting:

GroupBy sorting
~~~~~~~~~~~~~~~~~~~~~~~~~

By default the group keys are sorted during the ``groupby`` operation. You may however pass ``sort=False`` for potential speedups:

.. ipython:: python

   df2 = pd.DataFrame({"X": ["B", "B", "A", "A"], "Y": [1, 2, 3, 4]})
   df2.groupby(["X"]).sum()
   df2.groupby(["X"], sort=False).sum()


Note that ``groupby`` will preserve the order in which *observations* are sorted *within* each group.
For example, the groups created by ``groupby()`` below are in the order they appeared in the original ``DataFrame``:

.. ipython:: python

   df3 = pd.DataFrame({"X": ["A", "B", "A", "B"], "Y": [1, 4, 3, 2]})
   df3.groupby(["X"]).get_group("A")

   df3.groupby(["X"]).get_group("B")


.. _groupby.dropna:

.. versionadded:: 1.1.0

GroupBy dropna
^^^^^^^^^^^^^^

By default ``NA`` values are excluded from group keys during the ``groupby`` operation. However,
in case you want to include ``NA`` values in group keys, you could pass ``dropna=False`` to achieve it.

.. ipython:: python

    df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]
    df_dropna = pd.DataFrame(df_list, columns=["a", "b", "c"])

    df_dropna

.. ipython:: python

    # Default ``dropna`` is set to True, which will exclude NaNs in keys
    df_dropna.groupby(by=["b"], dropna=True).sum()

    # In order to allow NaN in keys, set ``dropna`` to False
    df_dropna.groupby(by=["b"], dropna=False).sum()

The default setting of ``dropna`` argument is ``True`` which means ``NA`` are not included in group keys.


.. _groupby.attributes:

GroupBy object attributes
~~~~~~~~~~~~~~~~~~~~~~~~~

The ``groups`` attribute is a dict whose keys are the computed unique groups
and corresponding values being the axis labels belonging to each group. In the
above example we have:

.. ipython:: python

   df.groupby("A").groups
   df.groupby(get_letter_type, axis=1).groups

Calling the standard Python ``len`` function on the GroupBy object just returns
the length of the ``groups`` dict, so it is largely just a convenience:

.. ipython:: python

   grouped = df.groupby(["A", "B"])
   grouped.groups
   len(grouped)


.. _groupby.tabcompletion:

``GroupBy`` will tab complete column names (and other attributes):

.. ipython:: python
   :suppress:

   n = 10
   weight = np.random.normal(166, 20, size=n)
   height = np.random.normal(60, 10, size=n)
   time = pd.date_range("1/1/2000", periods=n)
   gender = np.random.choice(["male", "female"], size=n)
   df = pd.DataFrame(
       {"height": height, "weight": weight, "gender": gender}, index=time
   )

.. ipython:: python

   df
   gb = df.groupby("gender")


.. ipython::

   @verbatim
   In [1]: gb.<TAB>  # noqa: E225, E999
   gb.agg        gb.boxplot    gb.cummin     gb.describe   gb.filter     gb.get_group  gb.height     gb.last       gb.median     gb.ngroups    gb.plot       gb.rank       gb.std        gb.transform
   gb.aggregate  gb.count      gb.cumprod    gb.dtype      gb.first      gb.groups     gb.hist       gb.max        gb.min        gb.nth        gb.prod       gb.resample   gb.sum        gb.var
   gb.apply      gb.cummax     gb.cumsum     gb.fillna     gb.gender     gb.head       gb.indices    gb.mean       gb.name       gb.ohlc       gb.quantile   gb.size       gb.tail       gb.weight

.. _groupby.multiindex:

GroupBy with MultiIndex
~~~~~~~~~~~~~~~~~~~~~~~

With :ref:`hierarchically-indexed data <advanced.hierarchical>`, it's quite
natural to group by one of the levels of the hierarchy.

Let's create a Series with a two-level ``MultiIndex``.

.. ipython:: python


   arrays = [
       ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
       ["one", "two", "one", "two", "one", "two", "one", "two"],
   ]
   index = pd.MultiIndex.from_arrays(arrays, names=["first", "second"])
   s = pd.Series(np.random.randn(8), index=index)
   s

We can then group by one of the levels in ``s``.

.. ipython:: python

   grouped = s.groupby(level=0)
   grouped.sum()

If the MultiIndex has names specified, these can be passed instead of the level
number:

.. ipython:: python

   s.groupby(level="second").sum()

Grouping with multiple levels is supported.

.. ipython:: python
   :suppress:

   arrays = [
       ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
       ["doo", "doo", "bee", "bee", "bop", "bop", "bop", "bop"],
       ["one", "two", "one", "two", "one", "two", "one", "two"],
   ]
   tuples = list(zip(*arrays))
   index = pd.MultiIndex.from_tuples(tuples, names=["first", "second", "third"])
   s = pd.Series(np.random.randn(8), index=index)

.. ipython:: python

   s
   s.groupby(level=["first", "second"]).sum()

Index level names may be supplied as keys.

.. ipython:: python

   s.groupby(["first", "second"]).sum()

More on the ``sum`` function and aggregation later.

Grouping DataFrame with Index levels and columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
A DataFrame may be grouped by a combination of columns and index levels by
specifying the column names as strings and the index levels as ``pd.Grouper``
objects.

.. ipython:: python

   arrays = [
       ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
       ["one", "two", "one", "two", "one", "two", "one", "two"],
   ]

   index = pd.MultiIndex.from_arrays(arrays, names=["first", "second"])

   df = pd.DataFrame({"A": [1, 1, 1, 1, 2, 2, 3, 3], "B": np.arange(8)}, index=index)

   df

The following example groups ``df`` by the ``second`` index level and
the ``A`` column.

.. ipython:: python

   df.groupby([pd.Grouper(level=1), "A"]).sum()

Index levels may also be specified by name.

.. ipython:: python

   df.groupby([pd.Grouper(level="second"), "A"]).sum()

Index level names may be specified as keys directly to ``groupby``.

.. ipython:: python

   df.groupby(["second", "A"]).sum()

DataFrame column selection in GroupBy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Once you have created the GroupBy object from a DataFrame, you might want to do
something different for each of the columns. Thus, using ``[]`` similar to
getting a column from a DataFrame, you can do:

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
           "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
           "C": np.random.randn(8),
           "D": np.random.randn(8),
       }
   )

   df

   grouped = df.groupby(["A"])
   grouped_C = grouped["C"]
   grouped_D = grouped["D"]

This is mainly syntactic sugar for the alternative and much more verbose:

.. ipython:: python

   df["C"].groupby(df["A"])

Additionally this method avoids recomputing the internal grouping information
derived from the passed key.

.. _groupby.iterating-label:

Iterating through groups
------------------------

With the GroupBy object in hand, iterating through the grouped data is very
natural and functions similarly to :py:func:`itertools.groupby`:

.. ipython::

   In [4]: grouped = df.groupby('A')

   In [5]: for name, group in grouped:
      ...:     print(name)
      ...:     print(group)
      ...:

In the case of grouping by multiple keys, the group name will be a tuple:

.. ipython::

   In [5]: for name, group in df.groupby(['A', 'B']):
      ...:     print(name)
      ...:     print(group)
      ...:

See :ref:`timeseries.iterating-label`.

Selecting a group
-----------------

A single group can be selected using
:meth:`~pandas.core.groupby.DataFrameGroupBy.get_group`:

.. ipython:: python

   grouped.get_group("bar")

Or for an object grouped on multiple columns:

.. ipython:: python

   df.groupby(["A", "B"]).get_group(("bar", "one"))

.. _groupby.aggregate:

Aggregation
-----------

Once the GroupBy object has been created, several methods are available to
perform a computation on the grouped data. These operations are similar to the
:ref:`aggregating API <basics.aggregate>`, :ref:`window API <window.overview>`,
and :ref:`resample API <timeseries.aggregate>`.

An obvious one is aggregation via the
:meth:`~pandas.core.groupby.DataFrameGroupBy.aggregate` or equivalently
:meth:`~pandas.core.groupby.DataFrameGroupBy.agg` method:

.. ipython:: python

   grouped = df.groupby("A")
   grouped.aggregate(np.sum)

   grouped = df.groupby(["A", "B"])
   grouped.aggregate(np.sum)

As you can see, the result of the aggregation will have the group names as the
new index along the grouped axis. In the case of multiple keys, the result is a
:ref:`MultiIndex <advanced.hierarchical>` by default, though this can be
changed by using the ``as_index`` option:

.. ipython:: python

   grouped = df.groupby(["A", "B"], as_index=False)
   grouped.aggregate(np.sum)

   df.groupby("A", as_index=False).sum()

Note that you could use the ``reset_index`` DataFrame function to achieve the
same result as the column names are stored in the resulting ``MultiIndex``:

.. ipython:: python

   df.groupby(["A", "B"]).sum().reset_index()

Another simple aggregation example is to compute the size of each group.
This is included in GroupBy as the ``size`` method. It returns a Series whose
index are the group names and whose values are the sizes of each group.

.. ipython:: python

   grouped.size()

.. ipython:: python

   grouped.describe()

Another aggregation example is to compute the number of unique values of each group. This is similar to the ``value_counts`` function, except that it only counts unique values.

.. ipython:: python

   ll = [['foo', 1], ['foo', 2], ['foo', 2], ['bar', 1], ['bar', 1]]
   df4 = pd.DataFrame(ll, columns=["A", "B"])
   df4
   df4.groupby("A")["B"].nunique()

.. note::

   Aggregation functions **will not** return the groups that you are aggregating over
   if they are named *columns*, when ``as_index=True``, the default. The grouped columns will
   be the **indices** of the returned object.

   Passing ``as_index=False`` **will** return the groups that you are aggregating over, if they are
   named *columns*.

Aggregating functions are the ones that reduce the dimension of the returned objects.
Some common aggregating functions are tabulated below:

.. csv-table::
    :header: "Function", "Description"
    :widths: 20, 80
    :delim: ;

	:meth:`~pd.core.groupby.DataFrameGroupBy.mean`;Compute mean of groups
	:meth:`~pd.core.groupby.DataFrameGroupBy.sum`;Compute sum of group values
	:meth:`~pd.core.groupby.DataFrameGroupBy.size`;Compute group sizes
	:meth:`~pd.core.groupby.DataFrameGroupBy.count`;Compute count of group
	:meth:`~pd.core.groupby.DataFrameGroupBy.std`;Standard deviation of groups
	:meth:`~pd.core.groupby.DataFrameGroupBy.var`;Compute variance of groups
	:meth:`~pd.core.groupby.DataFrameGroupBy.sem`;Standard error of the mean of groups
	:meth:`~pd.core.groupby.DataFrameGroupBy.describe`;Generates descriptive statistics
	:meth:`~pd.core.groupby.DataFrameGroupBy.first`;Compute first of group values
	:meth:`~pd.core.groupby.DataFrameGroupBy.last`;Compute last of group values
	:meth:`~pd.core.groupby.DataFrameGroupBy.nth`;Take nth value, or a subset if n is a list
	:meth:`~pd.core.groupby.DataFrameGroupBy.min`;Compute min of group values
	:meth:`~pd.core.groupby.DataFrameGroupBy.max`;Compute max of group values


The aggregating functions above will exclude NA values. Any function which
reduces a :class:`Series` to a scalar value is an aggregation function and will work,
a trivial example is ``df.groupby('A').agg(lambda ser: 1)``. Note that
:meth:`~pd.core.groupby.DataFrameGroupBy.nth` can act as a reducer *or* a
filter, see :ref:`here <groupby.nth>`.

.. _groupby.aggregate.multifunc:

Applying multiple functions at once
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With grouped ``Series`` you can also pass a list or dict of functions to do
aggregation with, outputting a DataFrame:

.. ipython:: python

   grouped = df.groupby("A")
   grouped["C"].agg([np.sum, np.mean, np.std])

On a grouped ``DataFrame``, you can pass a list of functions to apply to each
column, which produces an aggregated result with a hierarchical index:

.. ipython:: python

   grouped[["C", "D"]].agg([np.sum, np.mean, np.std])


The resulting aggregations are named for the functions themselves. If you
need to rename, then you can add in a chained operation for a ``Series`` like this:

.. ipython:: python

   (
       grouped["C"]
       .agg([np.sum, np.mean, np.std])
       .rename(columns={"sum": "foo", "mean": "bar", "std": "baz"})
   )

For a grouped ``DataFrame``, you can rename in a similar manner:

.. ipython:: python

   (
       grouped[["C", "D"]].agg([np.sum, np.mean, np.std]).rename(
           columns={"sum": "foo", "mean": "bar", "std": "baz"}
       )
   )

.. note::

   In general, the output column names should be unique. You can't apply
   the same function (or two functions with the same name) to the same
   column.

   .. ipython:: python
      :okexcept:

      grouped["C"].agg(["sum", "sum"])


   pandas *does* allow you to provide multiple lambdas. In this case, pandas
   will mangle the name of the (nameless) lambda functions, appending ``_<i>``
   to each subsequent lambda.

   .. ipython:: python

      grouped["C"].agg([lambda x: x.max() - x.min(), lambda x: x.median() - x.mean()])



.. _groupby.aggregate.named:

Named aggregation
~~~~~~~~~~~~~~~~~

.. versionadded:: 0.25.0

To support column-specific aggregation *with control over the output column names*, pandas
accepts the special syntax in :meth:`GroupBy.agg`, known as "named aggregation", where

- The keywords are the *output* column names
- The values are tuples whose first element is the column to select
  and the second element is the aggregation to apply to that column. pandas
  provides the ``pandas.NamedAgg`` namedtuple with the fields ``['column', 'aggfunc']``
  to make it clearer what the arguments are. As usual, the aggregation can
  be a callable or a string alias.

.. ipython:: python

   animals = pd.DataFrame(
       {
           "kind": ["cat", "dog", "cat", "dog"],
           "height": [9.1, 6.0, 9.5, 34.0],
           "weight": [7.9, 7.5, 9.9, 198.0],
       }
   )
   animals

   animals.groupby("kind").agg(
       min_height=pd.NamedAgg(column="height", aggfunc="min"),
       max_height=pd.NamedAgg(column="height", aggfunc="max"),
       average_weight=pd.NamedAgg(column="weight", aggfunc=np.mean),
   )


``pandas.NamedAgg`` is just a ``namedtuple``. Plain tuples are allowed as well.

.. ipython:: python

   animals.groupby("kind").agg(
       min_height=("height", "min"),
       max_height=("height", "max"),
       average_weight=("weight", np.mean),
   )


If your desired output column names are not valid Python keywords, construct a dictionary
and unpack the keyword arguments

.. ipython:: python

   animals.groupby("kind").agg(
       **{
           "total weight": pd.NamedAgg(column="weight", aggfunc=sum)
       }
   )

Additional keyword arguments are not passed through to the aggregation functions. Only pairs
of ``(column, aggfunc)`` should be passed as ``**kwargs``. If your aggregation functions
requires additional arguments, partially apply them with :meth:`functools.partial`.

.. note::

   For Python 3.5 and earlier, the order of ``**kwargs`` in a functions was not
   preserved. This means that the output column ordering would not be
   consistent. To ensure consistent ordering, the keys (and so output columns)
   will always be sorted for Python 3.5.

Named aggregation is also valid for Series groupby aggregations. In this case there's
no column selection, so the values are just the functions.

.. ipython:: python

   animals.groupby("kind").height.agg(
       min_height="min",
       max_height="max",
   )

Applying different functions to DataFrame columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By passing a dict to ``aggregate`` you can apply a different aggregation to the
columns of a DataFrame:

.. ipython:: python

   grouped.agg({"C": np.sum, "D": lambda x: np.std(x, ddof=1)})

The function names can also be strings. In order for a string to be valid it
must be either implemented on GroupBy or available via :ref:`dispatching
<groupby.dispatch>`:

.. ipython:: python

   grouped.agg({"C": "sum", "D": "std"})

.. _groupby.aggregate.cython:

Cython-optimized aggregation functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Some common aggregations, currently only ``sum``, ``mean``, ``std``, and ``sem``, have
optimized Cython implementations:

.. ipython:: python

   df.groupby("A").sum()
   df.groupby(["A", "B"]).mean()

Of course ``sum`` and ``mean`` are implemented on pandas objects, so the above
code would work even without the special versions via dispatching (see below).

.. _groupby.aggregate.udfs:

Aggregations with User-Defined Functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Users can also provide their own functions for custom aggregations. When aggregating
with a User-Defined Function (UDF), the UDF should not mutate the provided ``Series``, see
:ref:`gotchas.udf-mutation` for more information.

.. ipython:: python

   animals.groupby("kind")[["height"]].agg(lambda x: set(x))

The resulting dtype will reflect that of the aggregating function. If the results from different groups have
different dtypes, then a common dtype will be determined in the same way as ``DataFrame`` construction.

.. ipython:: python

   animals.groupby("kind")[["height"]].agg(lambda x: x.astype(int).sum())

.. _groupby.transform:

Transformation
--------------

The ``transform`` method returns an object that is indexed the same (same size)
as the one being grouped. The transform function must:

* Return a result that is either the same size as the group chunk or
  broadcastable to the size of the group chunk (e.g., a scalar,
  ``grouped.transform(lambda x: x.iloc[-1])``).
* Operate column-by-column on the group chunk.  The transform is applied to
  the first group chunk using chunk.apply.
* Not perform in-place operations on the group chunk. Group chunks should
  be treated as immutable, and changes to a group chunk may produce unexpected
  results. For example, when using ``fillna``, ``inplace`` must be ``False``
  (``grouped.transform(lambda x: x.fillna(inplace=False))``).
* (Optionally) operates on the entire group chunk. If this is supported, a
  fast path is used starting from the *second* chunk.

Similar to :ref:`groupby.aggregate.udfs`, the resulting dtype will reflect that of the
transformation function. If the results from different groups have different dtypes, then
a common dtype will be determined in the same way as ``DataFrame`` construction.

Suppose we wished to standardize the data within each group:

.. ipython:: python

   index = pd.date_range("10/1/1999", periods=1100)
   ts = pd.Series(np.random.normal(0.5, 2, 1100), index)
   ts = ts.rolling(window=100, min_periods=100).mean().dropna()

   ts.head()
   ts.tail()

   transformed = ts.groupby(lambda x: x.year).transform(
       lambda x: (x - x.mean()) / x.std()
   )


We would expect the result to now have mean 0 and standard deviation 1 within
each group, which we can easily check:

.. ipython:: python

   # Original Data
   grouped = ts.groupby(lambda x: x.year)
   grouped.mean()
   grouped.std()

   # Transformed Data
   grouped_trans = transformed.groupby(lambda x: x.year)
   grouped_trans.mean()
   grouped_trans.std()

We can also visually compare the original and transformed data sets.

.. ipython:: python

   compare = pd.DataFrame({"Original": ts, "Transformed": transformed})

   @savefig groupby_transform_plot.png
   compare.plot()

Transformation functions that have lower dimension outputs are broadcast to
match the shape of the input array.

.. ipython:: python

   ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())

Alternatively, the built-in methods could be used to produce the same outputs.

.. ipython:: python

   max = ts.groupby(lambda x: x.year).transform("max")
   min = ts.groupby(lambda x: x.year).transform("min")

   max - min

Another common data transform is to replace missing data with the group mean.

.. ipython:: python
   :suppress:

   cols = ["A", "B", "C"]
   values = np.random.randn(1000, 3)
   values[np.random.randint(0, 1000, 100), 0] = np.nan
   values[np.random.randint(0, 1000, 50), 1] = np.nan
   values[np.random.randint(0, 1000, 200), 2] = np.nan
   data_df = pd.DataFrame(values, columns=cols)

.. ipython:: python

   data_df

   countries = np.array(["US", "UK", "GR", "JP"])
   key = countries[np.random.randint(0, 4, 1000)]

   grouped = data_df.groupby(key)

   # Non-NA count in each group
   grouped.count()

   transformed = grouped.transform(lambda x: x.fillna(x.mean()))

We can verify that the group means have not changed in the transformed data
and that the transformed data contains no NAs.

.. ipython:: python

   grouped_trans = transformed.groupby(key)

   grouped.mean()  # original group means
   grouped_trans.mean()  # transformation did not change group means

   grouped.count()  # original has some missing data points
   grouped_trans.count()  # counts after transformation
   grouped_trans.size()  # Verify non-NA count equals group size

.. note::

   Some functions will automatically transform the input when applied to a
   GroupBy object, but returning an object of the same shape as the original.
   Passing ``as_index=False`` will not affect these transformation methods.

   For example: ``fillna, ffill, bfill, shift.``.

   .. ipython:: python

      grouped.ffill()


.. _groupby.transform.window_resample:

Window and resample operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It is possible to use ``resample()``, ``expanding()`` and
``rolling()`` as methods on groupbys.

The example below will apply the ``rolling()`` method on the samples of
the column B based on the groups of column A.

.. ipython:: python

   df_re = pd.DataFrame({"A": [1] * 10 + [5] * 10, "B": np.arange(20)})
   df_re

   df_re.groupby("A").rolling(4).B.mean()


The ``expanding()`` method will accumulate a given operation
(``sum()`` in the example) for all the members of each particular
group.

.. ipython:: python

   df_re.groupby("A").expanding().sum()


Suppose you want to use the ``resample()`` method to get a daily
frequency in each group of your dataframe and wish to complete the
missing values with the ``ffill()`` method.

.. ipython:: python

   df_re = pd.DataFrame(
       {
           "date": pd.date_range(start="2016-01-01", periods=4, freq="W"),
           "group": [1, 1, 2, 2],
           "val": [5, 6, 7, 8],
       }
   ).set_index("date")
   df_re

   df_re.groupby("group").resample("1D").ffill()

.. _groupby.filter:

Filtration
----------

The ``filter`` method returns a subset of the original object. Suppose we
want to take only elements that belong to groups with a group sum greater
than 2.

.. ipython:: python

   sf = pd.Series([1, 1, 2, 3, 3, 3])
   sf.groupby(sf).filter(lambda x: x.sum() > 2)

The argument of ``filter`` must be a function that, applied to the group as a
whole, returns ``True`` or ``False``.

Another useful operation is filtering out elements that belong to groups
with only a couple members.

.. ipython:: python

   dff = pd.DataFrame({"A": np.arange(8), "B": list("aabbbbcc")})
   dff.groupby("B").filter(lambda x: len(x) > 2)

Alternatively, instead of dropping the offending groups, we can return a
like-indexed objects where the groups that do not pass the filter are filled
with NaNs.

.. ipython:: python

   dff.groupby("B").filter(lambda x: len(x) > 2, dropna=False)

For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion.

.. ipython:: python

   dff["C"] = np.arange(8)
   dff.groupby("B").filter(lambda x: len(x["C"]) > 2)

.. note::

   Some functions when applied to a groupby object will act as a **filter** on the input, returning
   a reduced shape of the original (and potentially eliminating groups), but with the index unchanged.
   Passing ``as_index=False`` will not affect these transformation methods.

   For example: ``head, tail``.

   .. ipython:: python

      dff.groupby("B").head(2)


.. _groupby.dispatch:

Dispatching to instance methods
-------------------------------

When doing an aggregation or transformation, you might just want to call an
instance method on each data group. This is pretty easy to do by passing lambda
functions:

.. ipython:: python
   :okwarning:

   grouped = df.groupby("A")
   grouped.agg(lambda x: x.std())

But, it's rather verbose and can be untidy if you need to pass additional
arguments. Using a bit of metaprogramming cleverness, GroupBy now has the
ability to "dispatch" method calls to the groups:

.. ipython:: python
   :okwarning:

   grouped.std()

What is actually happening here is that a function wrapper is being
generated. When invoked, it takes any passed arguments and invokes the function
with any arguments on each group (in the above example, the ``std``
function). The results are then combined together much in the style of ``agg``
and ``transform`` (it actually uses ``apply`` to infer the gluing, documented
next). This enables some operations to be carried out rather succinctly:

.. ipython:: python

   tsdf = pd.DataFrame(
       np.random.randn(1000, 3),
       index=pd.date_range("1/1/2000", periods=1000),
       columns=["A", "B", "C"],
   )
   tsdf.iloc[::2] = np.nan
   grouped = tsdf.groupby(lambda x: x.year)
   grouped.fillna(method="pad")

In this example, we chopped the collection of time series into yearly chunks
then independently called :ref:`fillna <missing_data.fillna>` on the
groups.

The ``nlargest`` and ``nsmallest`` methods work on ``Series`` style groupbys:

.. ipython:: python

   s = pd.Series([9, 8, 7, 5, 19, 1, 4.2, 3.3])
   g = pd.Series(list("abababab"))
   gb = s.groupby(g)
   gb.nlargest(3)
   gb.nsmallest(3)

.. _groupby.apply:

Flexible ``apply``
------------------

Some operations on the grouped data might not fit into either the aggregate or
transform categories. Or, you may simply want GroupBy to infer how to combine
the results. For these, use the ``apply`` function, which can be substituted
for both ``aggregate`` and ``transform`` in many standard use cases. However,
``apply`` can handle some exceptional use cases, for example:

.. ipython:: python

   df
   grouped = df.groupby("A")

   # could also just call .describe()
   grouped["C"].apply(lambda x: x.describe())

The dimension of the returned result can also change:

.. ipython::

    In [8]: grouped = df.groupby('A')['C']

    In [10]: def f(group):
       ....:     return pd.DataFrame({'original': group,
       ....:                          'demeaned': group - group.mean()})
       ....:

    In [11]: grouped.apply(f)

``apply`` on a Series can operate on a returned value from the applied function,
that is itself a series, and possibly upcast the result to a DataFrame:

.. ipython:: python

    def f(x):
        return pd.Series([x, x ** 2], index=["x", "x^2"])


    s = pd.Series(np.random.rand(5))
    s
    s.apply(f)

.. note::

   ``apply`` can act as a reducer, transformer, *or* filter function, depending on exactly what is passed to it.
   So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in
   the output as well as set the indices.

Similar to :ref:`groupby.aggregate.udfs`, the resulting dtype will reflect that of the
apply function. If the results from different groups have different dtypes, then
a common dtype will be determined in the same way as ``DataFrame`` construction.


Numba Accelerated Routines
--------------------------

.. versionadded:: 1.1

If `Numba <https://numba.pydata.org/>`__ is installed as an optional dependency, the ``transform`` and
``aggregate`` methods support ``engine='numba'`` and ``engine_kwargs`` arguments.
See :ref:`enhancing performance with Numba <enhancingperf.numba>` for general usage of the arguments
and performance considerations.

The function signature must start with ``values, index`` **exactly** as the data belonging to each group
will be passed into ``values``, and the group index will be passed into ``index``.

.. warning::

   When using ``engine='numba'``, there will be no "fall back" behavior internally. The group
   data and group index will be passed as NumPy arrays to the JITed user defined function, and no
   alternative execution attempts will be tried.

Other useful features
---------------------

Automatic exclusion of "nuisance" columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Again consider the example DataFrame we've been looking at:

.. ipython:: python

   df

Suppose we wish to compute the standard deviation grouped by the ``A``
column. There is a slight problem, namely that we don't care about the data in
column ``B``. We refer to this as a "nuisance" column. If the passed
aggregation function can't be applied to some columns, the troublesome columns
will be (silently) dropped. Thus, this does not pose any problems:

.. ipython:: python

   df.groupby("A").std()

Note that ``df.groupby('A').colname.std().`` is more efficient than
``df.groupby('A').std().colname``, so if the result of an aggregation function
is only interesting over one column (here ``colname``), it may be filtered
*before* applying the aggregation function.

.. note::
   Any object column, also if it contains numerical values such as ``Decimal``
   objects, is considered as a "nuisance" columns. They are excluded from
   aggregate functions automatically in groupby.

   If you do wish to include decimal or object columns in an aggregation with
   other non-nuisance data types, you must do so explicitly.

.. ipython:: python

    from decimal import Decimal

    df_dec = pd.DataFrame(
        {
            "id": [1, 2, 1, 2],
            "int_column": [1, 2, 3, 4],
            "dec_column": [
                Decimal("0.50"),
                Decimal("0.15"),
                Decimal("0.25"),
                Decimal("0.40"),
            ],
        }
    )

    # Decimal columns can be sum'd explicitly by themselves...
    df_dec.groupby(["id"])[["dec_column"]].sum()

    # ...but cannot be combined with standard data types or they will be excluded
    df_dec.groupby(["id"])[["int_column", "dec_column"]].sum()

    # Use .agg function to aggregate over standard and "nuisance" data types
    # at the same time
    df_dec.groupby(["id"]).agg({"int_column": "sum", "dec_column": "sum"})

.. _groupby.observed:

Handling of (un)observed Categorical values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When using a ``Categorical`` grouper (as a single grouper, or as part of multiple groupers), the ``observed`` keyword
controls whether to return a cartesian product of all possible groupers values (``observed=False``) or only those
that are observed groupers (``observed=True``).

Show all values:

.. ipython:: python

   pd.Series([1, 1, 1]).groupby(
       pd.Categorical(["a", "a", "a"], categories=["a", "b"]), observed=False
   ).count()

Show only the observed values:

.. ipython:: python

   pd.Series([1, 1, 1]).groupby(
       pd.Categorical(["a", "a", "a"], categories=["a", "b"]), observed=True
   ).count()

The returned dtype of the grouped will *always* include *all* of the categories that were grouped.

.. ipython:: python

   s = (
       pd.Series([1, 1, 1])
       .groupby(pd.Categorical(["a", "a", "a"], categories=["a", "b"]), observed=False)
       .count()
   )
   s.index.dtype

.. _groupby.missing:

NA and NaT group handling
~~~~~~~~~~~~~~~~~~~~~~~~~

If there are any NaN or NaT values in the grouping key, these will be
automatically excluded. In other words, there will never be an "NA group" or
"NaT group". This was not the case in older versions of pandas, but users were
generally discarding the NA group anyway (and supporting it was an
implementation headache).

Grouping with ordered factors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Categorical variables represented as instance of pandas's ``Categorical`` class
can be used as group keys. If so, the order of the levels will be preserved:

.. ipython:: python

   data = pd.Series(np.random.randn(100))

   factor = pd.qcut(data, [0, 0.25, 0.5, 0.75, 1.0])

   data.groupby(factor).mean()

.. _groupby.specify:

Grouping with a grouper specification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You may need to specify a bit more data to properly group. You can
use the ``pd.Grouper`` to provide this local control.

.. ipython:: python

   import datetime

   df = pd.DataFrame(
       {
           "Branch": "A A A A A A A B".split(),
           "Buyer": "Carl Mark Carl Carl Joe Joe Joe Carl".split(),
           "Quantity": [1, 3, 5, 1, 8, 1, 9, 3],
           "Date": [
               datetime.datetime(2013, 1, 1, 13, 0),
               datetime.datetime(2013, 1, 1, 13, 5),
               datetime.datetime(2013, 10, 1, 20, 0),
               datetime.datetime(2013, 10, 2, 10, 0),
               datetime.datetime(2013, 10, 1, 20, 0),
               datetime.datetime(2013, 10, 2, 10, 0),
               datetime.datetime(2013, 12, 2, 12, 0),
               datetime.datetime(2013, 12, 2, 14, 0),
           ],
       }
   )

   df

Groupby a specific column with the desired frequency. This is like resampling.

.. ipython:: python

   df.groupby([pd.Grouper(freq="1M", key="Date"), "Buyer"]).sum()

You have an ambiguous specification in that you have a named index and a column
that could be potential groupers.

.. ipython:: python

   df = df.set_index("Date")
   df["Date"] = df.index + pd.offsets.MonthEnd(2)
   df.groupby([pd.Grouper(freq="6M", key="Date"), "Buyer"]).sum()

   df.groupby([pd.Grouper(freq="6M", level="Date"), "Buyer"]).sum()


Taking the first rows of each group
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just like for a DataFrame or Series you can call head and tail on a groupby:

.. ipython:: python

   df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=["A", "B"])
   df

   g = df.groupby("A")
   g.head(1)

   g.tail(1)

This shows the first or last n rows from each group.

.. _groupby.nth:

Taking the nth row of each group
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To select from a DataFrame or Series the nth item, use
:meth:`~pd.core.groupby.DataFrameGroupBy.nth`. This is a reduction method, and
will return a single row (or no row) per group if you pass an int for n:

.. ipython:: python

   df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=["A", "B"])
   g = df.groupby("A")

   g.nth(0)
   g.nth(-1)
   g.nth(1)

If you want to select the nth not-null item, use the ``dropna`` kwarg. For a DataFrame this should be either ``'any'`` or ``'all'`` just like you would pass to dropna:

.. ipython:: python

   # nth(0) is the same as g.first()
   g.nth(0, dropna="any")
   g.first()

   # nth(-1) is the same as g.last()
   g.nth(-1, dropna="any")  # NaNs denote group exhausted when using dropna
   g.last()

   g.B.nth(0, dropna="all")

As with other methods, passing ``as_index=False``, will achieve a filtration, which returns the grouped row.

.. ipython:: python

   df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=["A", "B"])
   g = df.groupby("A", as_index=False)

   g.nth(0)
   g.nth(-1)

You can also select multiple rows from each group by specifying multiple nth values as a list of ints.

.. ipython:: python

   business_dates = pd.date_range(start="4/1/2014", end="6/30/2014", freq="B")
   df = pd.DataFrame(1, index=business_dates, columns=["a", "b"])
   # get the first, 4th, and last date index for each month
   df.groupby([df.index.year, df.index.month]).nth([0, 3, -1])

Enumerate group items
~~~~~~~~~~~~~~~~~~~~~

To see the order in which each row appears within its group, use the
``cumcount`` method:

.. ipython:: python

   dfg = pd.DataFrame(list("aaabba"), columns=["A"])
   dfg

   dfg.groupby("A").cumcount()

   dfg.groupby("A").cumcount(ascending=False)

.. _groupby.ngroup:

Enumerate groups
~~~~~~~~~~~~~~~~

To see the ordering of the groups (as opposed to the order of rows
within a group given by ``cumcount``) you can use
:meth:`~pandas.core.groupby.DataFrameGroupBy.ngroup`.



Note that the numbers given to the groups match the order in which the
groups would be seen when iterating over the groupby object, not the
order they are first observed.

.. ipython:: python

   dfg = pd.DataFrame(list("aaabba"), columns=["A"])
   dfg

   dfg.groupby("A").ngroup()

   dfg.groupby("A").ngroup(ascending=False)

Plotting
~~~~~~~~

Groupby also works with some plotting methods.  For example, suppose we
suspect that some features in a DataFrame may differ by group, in this case,
the values in column 1 where the group is "B" are 3 higher on average.

.. ipython:: python

   np.random.seed(1234)
   df = pd.DataFrame(np.random.randn(50, 2))
   df["g"] = np.random.choice(["A", "B"], size=50)
   df.loc[df["g"] == "B", 1] += 3

We can easily visualize this with a boxplot:

.. ipython:: python
   :okwarning:

   @savefig groupby_boxplot.png
   df.groupby("g").boxplot()

The result of calling ``boxplot`` is a dictionary whose keys are the values
of our grouping column ``g`` ("A" and "B"). The values of the resulting dictionary
can be controlled by the ``return_type`` keyword of ``boxplot``.
See the :ref:`visualization documentation<visualization.box>` for more.

.. warning::

  For historical reasons, ``df.groupby("g").boxplot()`` is not equivalent
  to ``df.boxplot(by="g")``. See :ref:`here<visualization.box.return>` for
  an explanation.

.. _groupby.pipe:

Piping function calls
~~~~~~~~~~~~~~~~~~~~~

Similar to the functionality provided by ``DataFrame`` and ``Series``, functions
that take ``GroupBy`` objects can be chained together using a ``pipe`` method to
allow for a cleaner, more readable syntax. To read about ``.pipe`` in general terms,
see :ref:`here <basics.pipe>`.

Combining ``.groupby`` and ``.pipe`` is often useful when you need to reuse
GroupBy objects.

As an example, imagine having a DataFrame with columns for stores, products,
revenue and quantity sold. We'd like to do a groupwise calculation of *prices*
(i.e. revenue/quantity) per store and per product. We could do this in a
multi-step operation, but expressing it in terms of piping can make the
code more readable. First we set the data:

.. ipython:: python

   n = 1000
   df = pd.DataFrame(
       {
           "Store": np.random.choice(["Store_1", "Store_2"], n),
           "Product": np.random.choice(["Product_1", "Product_2"], n),
           "Revenue": (np.random.random(n) * 50 + 10).round(2),
           "Quantity": np.random.randint(1, 10, size=n),
       }
   )
   df.head(2)

Now, to find prices per store/product, we can simply do:

.. ipython:: python

   (
       df.groupby(["Store", "Product"])
       .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum())
       .unstack()
       .round(2)
   )

Piping can also be expressive when you want to deliver a grouped object to some
arbitrary function, for example:

.. ipython:: python

   def mean(groupby):
       return groupby.mean()


   df.groupby(["Store", "Product"]).pipe(mean)

where ``mean`` takes a GroupBy object and finds the mean of the Revenue and Quantity
columns respectively for each Store-Product combination. The ``mean`` function can
be any function that takes in a GroupBy object; the ``.pipe`` will pass the GroupBy
object as a parameter into the function you specify.

Examples
--------

Regrouping by factor
~~~~~~~~~~~~~~~~~~~~

Regroup columns of a DataFrame according to their sum, and sum the aggregated ones.

.. ipython:: python

   df = pd.DataFrame({"a": [1, 0, 0], "b": [0, 1, 0], "c": [1, 0, 0], "d": [2, 3, 4]})
   df
   df.groupby(df.sum(), axis=1).sum()

.. _groupby.multicolumn_factorization:

Multi-column factorization
~~~~~~~~~~~~~~~~~~~~~~~~~~

By using :meth:`~pandas.core.groupby.DataFrameGroupBy.ngroup`, we can extract
information about the groups in a way similar to :func:`factorize` (as described
further in the :ref:`reshaping API <reshaping.factorize>`) but which applies
naturally to multiple columns of mixed type and different
sources. This can be useful as an intermediate categorical-like step
in processing, when the relationships between the group rows are more
important than their content, or as input to an algorithm which only
accepts the integer encoding. (For more information about support in
pandas for full categorical data, see the :ref:`Categorical
introduction <categorical>` and the
:ref:`API documentation <api.arrays.categorical>`.)

.. ipython:: python

    dfg = pd.DataFrame({"A": [1, 1, 2, 3, 2], "B": list("aaaba")})

    dfg

    dfg.groupby(["A", "B"]).ngroup()

    dfg.groupby(["A", [0, 0, 0, 1, 1]]).ngroup()

Groupby by indexer to 'resample' data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples.

In order to resample to work on indices that are non-datetimelike, the following procedure can be utilized.

In the following examples, **df.index // 5** returns a binary array which is used to determine what gets selected for the groupby operation.

.. note:: The below example shows how we can downsample by consolidation of samples into fewer samples. Here by using **df.index // 5**, we are aggregating the samples in bins. By applying **std()** function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 2))
   df
   df.index // 5
   df.groupby(df.index // 5).std()

Returning a Series to propagate names
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Group DataFrame columns, compute a set of metrics and return a named Series.
The Series name is used as the name for the column index. This is especially
useful in conjunction with reshaping operations such as stacking in which the
column index name will be used as the name of the inserted column:

.. ipython:: python

   df = pd.DataFrame(
       {
           "a": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],
           "b": [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],
           "c": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],
           "d": [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],
       }
   )

   def compute_metrics(x):
       result = {"b_sum": x["b"].sum(), "c_mean": x["c"].mean()}
       return pd.Series(result, name="metrics")

   result = df.groupby("a").apply(compute_metrics)

   result

   result.stack()
.. _sparse:

{{ header }}

**********************
Sparse data structures
**********************

pandas provides data structures for efficiently storing sparse data.
These are not necessarily sparse in the typical "mostly 0". Rather, you can view these
objects as being "compressed" where any data matching a specific value (``NaN`` / missing value, though any value
can be chosen, including 0) is omitted. The compressed values are not actually stored in the array.

.. ipython:: python

   arr = np.random.randn(10)
   arr[2:-2] = np.nan
   ts = pd.Series(pd.arrays.SparseArray(arr))
   ts

Notice the dtype, ``Sparse[float64, nan]``. The ``nan`` means that elements in the
array that are ``nan`` aren't actually stored, only the non-``nan`` elements are.
Those non-``nan`` elements have a ``float64`` dtype.

The sparse objects exist for memory efficiency reasons. Suppose you had a
large, mostly NA :class:`DataFrame`:

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10000, 4))
   df.iloc[:9998] = np.nan
   sdf = df.astype(pd.SparseDtype("float", np.nan))
   sdf.head()
   sdf.dtypes
   sdf.sparse.density

As you can see, the density (% of values that have not been "compressed") is
extremely low. This sparse object takes up much less memory on disk (pickled)
and in the Python interpreter.

.. ipython:: python

   'dense : {:0.2f} bytes'.format(df.memory_usage().sum() / 1e3)
   'sparse: {:0.2f} bytes'.format(sdf.memory_usage().sum() / 1e3)

Functionally, their behavior should be nearly
identical to their dense counterparts.

.. _sparse.array:

SparseArray
-----------

:class:`arrays.SparseArray` is a :class:`~pandas.api.extensions.ExtensionArray`
for storing an array of sparse values (see :ref:`basics.dtypes` for more
on extension arrays). It is a 1-dimensional ndarray-like object storing
only values distinct from the ``fill_value``:

.. ipython:: python

   arr = np.random.randn(10)
   arr[2:5] = np.nan
   arr[7:8] = np.nan
   sparr = pd.arrays.SparseArray(arr)
   sparr

A sparse array can be converted to a regular (dense) ndarray with :meth:`numpy.asarray`

.. ipython:: python

   np.asarray(sparr)


.. _sparse.dtype:

SparseDtype
-----------

The :attr:`SparseArray.dtype` property stores two pieces of information

1. The dtype of the non-sparse values
2. The scalar fill value


.. ipython:: python

   sparr.dtype


A :class:`SparseDtype` may be constructed by passing only a dtype

.. ipython:: python

   pd.SparseDtype(np.dtype('datetime64[ns]'))

in which case a default fill value will be used (for NumPy dtypes this is often the
"missing" value for that dtype). To override this default an explicit fill value may be
passed instead

.. ipython:: python

   pd.SparseDtype(np.dtype('datetime64[ns]'),
                  fill_value=pd.Timestamp('2017-01-01'))

Finally, the string alias ``'Sparse[dtype]'`` may be used to specify a sparse dtype
in many places

.. ipython:: python

   pd.array([1, 0, 0, 2], dtype='Sparse[int]')

.. _sparse.accessor:

Sparse accessor
---------------

pandas provides a ``.sparse`` accessor, similar to ``.str`` for string data, ``.cat``
for categorical data, and ``.dt`` for datetime-like data. This namespace provides
attributes and methods that are specific to sparse data.

.. ipython:: python

   s = pd.Series([0, 0, 1, 2], dtype="Sparse[int]")
   s.sparse.density
   s.sparse.fill_value

This accessor is available only on data with ``SparseDtype``, and on the :class:`Series`
class itself for creating a Series with sparse data from a scipy COO matrix with.


.. versionadded:: 0.25.0

A ``.sparse`` accessor has been added for :class:`DataFrame` as well.
See :ref:`api.frame.sparse` for more.

.. _sparse.calculation:

Sparse calculation
------------------

You can apply NumPy `ufuncs <https://numpy.org/doc/stable/reference/ufuncs.html>`_
to :class:`arrays.SparseArray` and get a :class:`arrays.SparseArray` as a result.

.. ipython:: python

   arr = pd.arrays.SparseArray([1., np.nan, np.nan, -2., np.nan])
   np.abs(arr)


The *ufunc* is also applied to ``fill_value``. This is needed to get
the correct dense result.

.. ipython:: python

   arr = pd.arrays.SparseArray([1., -1, -1, -2., -1], fill_value=-1)
   np.abs(arr)
   np.abs(arr).to_dense()

.. _sparse.migration:

Migrating
---------

.. note::

   ``SparseSeries`` and ``SparseDataFrame`` were removed in pandas 1.0.0. This migration
   guide is present to aid in migrating from previous versions.

In older versions of pandas, the ``SparseSeries`` and ``SparseDataFrame`` classes (documented below)
were the preferred way to work with sparse data. With the advent of extension arrays, these subclasses
are no longer needed. Their purpose is better served by using a regular Series or DataFrame with
sparse values instead.

.. note::

  There's no performance or memory penalty to using a Series or DataFrame with sparse values,
  rather than a SparseSeries or SparseDataFrame.

This section provides some guidance on migrating your code to the new style. As a reminder,
you can use the Python warnings module to control warnings. But we recommend modifying
your code, rather than ignoring the warning.

**Construction**

From an array-like, use the regular :class:`Series` or
:class:`DataFrame` constructors with :class:`arrays.SparseArray` values.

.. code-block:: python

   # Previous way
   >>> pd.SparseDataFrame({"A": [0, 1]})

.. ipython:: python

   # New way
   pd.DataFrame({"A": pd.arrays.SparseArray([0, 1])})

From a SciPy sparse matrix, use :meth:`DataFrame.sparse.from_spmatrix`,

.. code-block:: python

   # Previous way
   >>> from scipy import sparse
   >>> mat = sparse.eye(3)
   >>> df = pd.SparseDataFrame(mat, columns=['A', 'B', 'C'])

.. ipython:: python

   # New way
   from scipy import sparse
   mat = sparse.eye(3)
   df = pd.DataFrame.sparse.from_spmatrix(mat, columns=['A', 'B', 'C'])
   df.dtypes

**Conversion**

From sparse to dense, use the ``.sparse`` accessors

.. ipython:: python

   df.sparse.to_dense()
   df.sparse.to_coo()

From dense to sparse, use :meth:`DataFrame.astype` with a :class:`SparseDtype`.

.. ipython:: python

   dense = pd.DataFrame({"A": [1, 0, 0, 1]})
   dtype = pd.SparseDtype(int, fill_value=0)
   dense.astype(dtype)

**Sparse Properties**

Sparse-specific properties, like ``density``, are available on the ``.sparse`` accessor.

.. ipython:: python

   df.sparse.density

**General differences**

In a ``SparseDataFrame``, *all* columns were sparse. A :class:`DataFrame` can have a mixture of
sparse and dense columns. As a consequence, assigning new columns to a :class:`DataFrame` with sparse
values will not automatically convert the input to be sparse.

.. code-block:: python

   # Previous Way
   >>> df = pd.SparseDataFrame({"A": [0, 1]})
   >>> df['B'] = [0, 0]  # implicitly becomes Sparse
   >>> df['B'].dtype
   Sparse[int64, nan]

Instead, you'll need to ensure that the values being assigned are sparse

.. ipython:: python

   df = pd.DataFrame({"A": pd.arrays.SparseArray([0, 1])})
   df['B'] = [0, 0]  # remains dense
   df['B'].dtype
   df['B'] = pd.arrays.SparseArray([0, 0])
   df['B'].dtype

The ``SparseDataFrame.default_kind`` and ``SparseDataFrame.default_fill_value`` attributes
have no replacement.

.. _sparse.scipysparse:

Interaction with scipy.sparse
-----------------------------

Use :meth:`DataFrame.sparse.from_spmatrix` to create a :class:`DataFrame` with sparse values from a sparse matrix.

.. versionadded:: 0.25.0

.. ipython:: python

   from scipy.sparse import csr_matrix

   arr = np.random.random(size=(1000, 5))
   arr[arr < .9] = 0

   sp_arr = csr_matrix(arr)
   sp_arr

   sdf = pd.DataFrame.sparse.from_spmatrix(sp_arr)
   sdf.head()
   sdf.dtypes

All sparse formats are supported, but matrices that are not in :mod:`COOrdinate <scipy.sparse>` format will be converted, copying data as needed.
To convert back to sparse SciPy matrix in COO format, you can use the :meth:`DataFrame.sparse.to_coo` method:

.. ipython:: python

   sdf.sparse.to_coo()

:meth:`Series.sparse.to_coo` is implemented for transforming a :class:`Series` with sparse values indexed by a :class:`MultiIndex` to a :class:`scipy.sparse.coo_matrix`.

The method requires a :class:`MultiIndex` with two or more levels.

.. ipython:: python

   s = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])
   s.index = pd.MultiIndex.from_tuples(
       [
           (1, 2, "a", 0),
           (1, 2, "a", 1),
           (1, 1, "b", 0),
           (1, 1, "b", 1),
           (2, 1, "b", 0),
           (2, 1, "b", 1),
       ],
       names=["A", "B", "C", "D"],
   )
   ss = s.astype('Sparse')
   ss

In the example below, we transform the :class:`Series` to a sparse representation of a 2-d array by specifying that the first and second ``MultiIndex`` levels define labels for the rows and the third and fourth levels define labels for the columns. We also specify that the column and row labels should be sorted in the final sparse representation.

.. ipython:: python

   A, rows, columns = ss.sparse.to_coo(
       row_levels=["A", "B"], column_levels=["C", "D"], sort_labels=True
   )

   A
   A.todense()
   rows
   columns

Specifying different row and column labels (and not sorting them) yields a different sparse matrix:

.. ipython:: python

   A, rows, columns = ss.sparse.to_coo(
       row_levels=["A", "B", "C"], column_levels=["D"], sort_labels=False
   )

   A
   A.todense()
   rows
   columns

A convenience method :meth:`Series.sparse.from_coo` is implemented for creating a :class:`Series` with sparse values from a ``scipy.sparse.coo_matrix``.

.. ipython:: python

   from scipy import sparse
   A = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))
   A
   A.todense()

The default behaviour (with ``dense_index=False``) simply returns a :class:`Series` containing
only the non-null entries.

.. ipython:: python

   ss = pd.Series.sparse.from_coo(A)
   ss

Specifying ``dense_index=True`` will result in an index that is the Cartesian product of the
row and columns coordinates of the matrix. Note that this will consume a significant amount of memory
(relative to ``dense_index=False``) if the sparse matrix is large (and sparse) enough.

.. ipython:: python

   ss_dense = pd.Series.sparse.from_coo(A, dense_index=True)
   ss_dense
.. _dsintro:

{{ header }}

************************
Intro to data structures
************************

We'll start with a quick, non-comprehensive overview of the fundamental data
structures in pandas to get you started. The fundamental behavior about data
types, indexing, and axis labeling / alignment apply across all of the
objects. To get started, import NumPy and load pandas into your namespace:

.. ipython:: python

   import numpy as np
   import pandas as pd

Here is a basic tenet to keep in mind: **data alignment is intrinsic**. The link
between labels and data will not be broken unless done so explicitly by you.

We'll give a brief intro to the data structures, then consider all of the broad
categories of functionality and methods in separate sections.

.. _basics.series:

Series
------

:class:`Series` is a one-dimensional labeled array capable of holding any data
type (integers, strings, floating point numbers, Python objects, etc.). The axis
labels are collectively referred to as the **index**. The basic method to create a Series is to call:

::

    >>> s = pd.Series(data, index=index)

Here, ``data`` can be many different things:

* a Python dict
* an ndarray
* a scalar value (like 5)

The passed **index** is a list of axis labels. Thus, this separates into a few
cases depending on what **data is**:

**From ndarray**

If ``data`` is an ndarray, **index** must be the same length as **data**. If no
index is passed, one will be created having values ``[0, ..., len(data) - 1]``.

.. ipython:: python

   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   s
   s.index

   pd.Series(np.random.randn(5))

.. note::

    pandas supports non-unique index values. If an operation
    that does not support duplicate index values is attempted, an exception
    will be raised at that time. The reason for being lazy is nearly all performance-based
    (there are many instances in computations, like parts of GroupBy, where the index
    is not used).

**From dict**

Series can be instantiated from dicts:

.. ipython:: python

   d = {"b": 1, "a": 0, "c": 2}
   pd.Series(d)

.. note::

   When the data is a dict, and an index is not passed, the ``Series`` index
   will be ordered by the dict's insertion order, if you're using Python
   version >= 3.6 and pandas version >= 0.23.

   If you're using Python < 3.6 or pandas < 0.23, and an index is not passed,
   the ``Series`` index will be the lexically ordered list of dict keys.

In the example above, if you were on a Python version lower than 3.6 or a
pandas version lower than 0.23, the ``Series`` would be ordered by the lexical
order of the dict keys (i.e. ``['a', 'b', 'c']`` rather than ``['b', 'a', 'c']``).

If an index is passed, the values in data corresponding to the labels in the
index will be pulled out.

.. ipython:: python

   d = {"a": 0.0, "b": 1.0, "c": 2.0}
   pd.Series(d)
   pd.Series(d, index=["b", "c", "d", "a"])

.. note::

    NaN (not a number) is the standard missing data marker used in pandas.

**From scalar value**

If ``data`` is a scalar value, an index must be
provided. The value will be repeated to match the length of **index**.

.. ipython:: python

   pd.Series(5.0, index=["a", "b", "c", "d", "e"])

Series is ndarray-like
~~~~~~~~~~~~~~~~~~~~~~

``Series`` acts very similarly to a ``ndarray``, and is a valid argument to most NumPy functions.
However, operations such as slicing will also slice the index.

.. ipython:: python

    s[0]
    s[:3]
    s[s > s.median()]
    s[[4, 3, 1]]
    np.exp(s)

.. note::

   We will address array-based indexing like ``s[[4, 3, 1]]``
   in :ref:`section on indexing <indexing>`.

Like a NumPy array, a pandas Series has a :attr:`~Series.dtype`.

.. ipython:: python

   s.dtype

This is often a NumPy dtype. However, pandas and 3rd-party libraries
extend NumPy's type system in a few places, in which case the dtype would
be an :class:`~pandas.api.extensions.ExtensionDtype`. Some examples within
pandas are :ref:`categorical` and :ref:`integer_na`. See :ref:`basics.dtypes`
for more.

If you need the actual array backing a ``Series``, use :attr:`Series.array`.

.. ipython:: python

   s.array

Accessing the array can be useful when you need to do some operation without the
index (to disable :ref:`automatic alignment <dsintro.alignment>`, for example).

:attr:`Series.array` will always be an :class:`~pandas.api.extensions.ExtensionArray`.
Briefly, an ExtensionArray is a thin wrapper around one or more *concrete* arrays like a
:class:`numpy.ndarray`. pandas knows how to take an ``ExtensionArray`` and
store it in a ``Series`` or a column of a ``DataFrame``.
See :ref:`basics.dtypes` for more.

While Series is ndarray-like, if you need an *actual* ndarray, then use
:meth:`Series.to_numpy`.

.. ipython:: python

   s.to_numpy()

Even if the Series is backed by a :class:`~pandas.api.extensions.ExtensionArray`,
:meth:`Series.to_numpy` will return a NumPy ndarray.

Series is dict-like
~~~~~~~~~~~~~~~~~~~

A Series is like a fixed-size dict in that you can get and set values by index
label:

.. ipython:: python

    s["a"]
    s["e"] = 12.0
    s
    "e" in s
    "f" in s

If a label is not contained, an exception is raised:

.. code-block:: python

    >>> s["f"]
    KeyError: 'f'

Using the ``get`` method, a missing label will return None or specified default:

.. ipython:: python

   s.get("f")

   s.get("f", np.nan)

See also the :ref:`section on attribute access<indexing.attribute_access>`.

Vectorized operations and label alignment with Series
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When working with raw NumPy arrays, looping through value-by-value is usually
not necessary. The same is true when working with Series in pandas.
Series can also be passed into most NumPy methods expecting an ndarray.

.. ipython:: python

    s + s
    s * 2
    np.exp(s)

A key difference between Series and ndarray is that operations between Series
automatically align the data based on label. Thus, you can write computations
without giving consideration to whether the Series involved have the same
labels.

.. ipython:: python

    s[1:] + s[:-1]

The result of an operation between unaligned Series will have the **union** of
the indexes involved. If a label is not found in one Series or the other, the
result will be marked as missing ``NaN``. Being able to write code without doing
any explicit data alignment grants immense freedom and flexibility in
interactive data analysis and research. The integrated data alignment features
of the pandas data structures set pandas apart from the majority of related
tools for working with labeled data.

.. note::

    In general, we chose to make the default result of operations between
    differently indexed objects yield the **union** of the indexes in order to
    avoid loss of information. Having an index label, though the data is
    missing, is typically important information as part of a computation. You
    of course have the option of dropping labels with missing data via the
    **dropna** function.

Name attribute
~~~~~~~~~~~~~~

.. _dsintro.name_attribute:

Series can also have a ``name`` attribute:

.. ipython:: python

   s = pd.Series(np.random.randn(5), name="something")
   s
   s.name

The Series ``name`` will be assigned automatically in many cases, in particular
when taking 1D slices of DataFrame as you will see below.

You can rename a Series with the :meth:`pandas.Series.rename` method.

.. ipython:: python

   s2 = s.rename("different")
   s2.name

Note that ``s`` and ``s2`` refer to different objects.

.. _basics.dataframe:

DataFrame
---------

**DataFrame** is a 2-dimensional labeled data structure with columns of
potentially different types. You can think of it like a spreadsheet or SQL
table, or a dict of Series objects. It is generally the most commonly used
pandas object. Like Series, DataFrame accepts many different kinds of input:

* Dict of 1D ndarrays, lists, dicts, or Series
* 2-D numpy.ndarray
* `Structured or record
  <https://numpy.org/doc/stable/user/basics.rec.html>`__ ndarray
* A ``Series``
* Another ``DataFrame``

Along with the data, you can optionally pass **index** (row labels) and
**columns** (column labels) arguments. If you pass an index and / or columns,
you are guaranteeing the index and / or columns of the resulting
DataFrame. Thus, a dict of Series plus a specific index will discard all data
not matching up to the passed index.

If axis labels are not passed, they will be constructed from the input data
based on common sense rules.

.. note::

   When the data is a dict, and ``columns`` is not specified, the ``DataFrame``
   columns will be ordered by the dict's insertion order, if you are using
   Python version >= 3.6 and pandas >= 0.23.

   If you are using Python < 3.6 or pandas < 0.23, and ``columns`` is not
   specified, the ``DataFrame`` columns will be the lexically ordered list of dict
   keys.

From dict of Series or dicts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The resulting **index** will be the **union** of the indexes of the various
Series. If there are any nested dicts, these will first be converted to
Series. If no columns are passed, the columns will be the ordered list of dict
keys.

.. ipython:: python

    d = {
        "one": pd.Series([1.0, 2.0, 3.0], index=["a", "b", "c"]),
        "two": pd.Series([1.0, 2.0, 3.0, 4.0], index=["a", "b", "c", "d"]),
    }
    df = pd.DataFrame(d)
    df

    pd.DataFrame(d, index=["d", "b", "a"])
    pd.DataFrame(d, index=["d", "b", "a"], columns=["two", "three"])

The row and column labels can be accessed respectively by accessing the
**index** and **columns** attributes:

.. note::

   When a particular set of columns is passed along with a dict of data, the
   passed columns override the keys in the dict.

.. ipython:: python

   df.index
   df.columns

From dict of ndarrays / lists
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ndarrays must all be the same length. If an index is passed, it must
clearly also be the same length as the arrays. If no index is passed, the
result will be ``range(n)``, where ``n`` is the array length.

.. ipython:: python

   d = {"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]}
   pd.DataFrame(d)
   pd.DataFrame(d, index=["a", "b", "c", "d"])

From structured or record array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This case is handled identically to a dict of arrays.

.. ipython:: python

   data = np.zeros((2,), dtype=[("A", "i4"), ("B", "f4"), ("C", "a10")])
   data[:] = [(1, 2.0, "Hello"), (2, 3.0, "World")]

   pd.DataFrame(data)
   pd.DataFrame(data, index=["first", "second"])
   pd.DataFrame(data, columns=["C", "A", "B"])

.. note::

    DataFrame is not intended to work exactly like a 2-dimensional NumPy
    ndarray.

.. _basics.dataframe.from_list_of_dicts:

From a list of dicts
~~~~~~~~~~~~~~~~~~~~

.. ipython:: python

   data2 = [{"a": 1, "b": 2}, {"a": 5, "b": 10, "c": 20}]
   pd.DataFrame(data2)
   pd.DataFrame(data2, index=["first", "second"])
   pd.DataFrame(data2, columns=["a", "b"])

.. _basics.dataframe.from_dict_of_tuples:

From a dict of tuples
~~~~~~~~~~~~~~~~~~~~~

You can automatically create a MultiIndexed frame by passing a tuples
dictionary.

.. ipython:: python

   pd.DataFrame(
       {
           ("a", "b"): {("A", "B"): 1, ("A", "C"): 2},
           ("a", "a"): {("A", "C"): 3, ("A", "B"): 4},
           ("a", "c"): {("A", "B"): 5, ("A", "C"): 6},
           ("b", "a"): {("A", "C"): 7, ("A", "B"): 8},
           ("b", "b"): {("A", "D"): 9, ("A", "B"): 10},
       }
   )

.. _basics.dataframe.from_series:

From a Series
~~~~~~~~~~~~~

The result will be a DataFrame with the same index as the input Series, and
with one column whose name is the original name of the Series (only if no other
column name provided).


.. _basics.dataframe.from_list_namedtuples:

From a list of namedtuples
~~~~~~~~~~~~~~~~~~~~~~~~~~

The field names of the first ``namedtuple`` in the list determine the columns
of the ``DataFrame``. The remaining namedtuples (or tuples) are simply unpacked
and their values are fed into the rows of the ``DataFrame``. If any of those
tuples is shorter than the first ``namedtuple`` then the later columns in the
corresponding row are marked as missing values. If any are longer than the
first ``namedtuple``, a ``ValueError`` is raised.

.. ipython:: python

    from collections import namedtuple

    Point = namedtuple("Point", "x y")

    pd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)])

    Point3D = namedtuple("Point3D", "x y z")

    pd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)])


.. _basics.dataframe.from_list_dataclasses:

From a list of dataclasses
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.1.0

Data Classes as introduced in `PEP557 <https://www.python.org/dev/peps/pep-0557>`__,
can be passed into the DataFrame constructor.
Passing a list of dataclasses is equivalent to passing a list of dictionaries.

Please be aware, that all values in the list should be dataclasses, mixing
types in the list would result in a TypeError.

.. ipython:: python

    from dataclasses import make_dataclass

    Point = make_dataclass("Point", [("x", int), ("y", int)])

    pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])

**Missing data**

Much more will be said on this topic in the :ref:`Missing data <missing_data>`
section. To construct a DataFrame with missing data, we use ``np.nan`` to
represent missing values. Alternatively, you may pass a ``numpy.MaskedArray``
as the data argument to the DataFrame constructor, and its masked entries will
be considered missing.

Alternate constructors
~~~~~~~~~~~~~~~~~~~~~~

.. _basics.dataframe.from_dict:

**DataFrame.from_dict**

``DataFrame.from_dict`` takes a dict of dicts or a dict of array-like sequences
and returns a DataFrame. It operates like the ``DataFrame`` constructor except
for the ``orient`` parameter which is ``'columns'`` by default, but which can be
set to ``'index'`` in order to use the dict keys as row labels.


.. ipython:: python

   pd.DataFrame.from_dict(dict([("A", [1, 2, 3]), ("B", [4, 5, 6])]))

If you pass ``orient='index'``, the keys will be the row labels. In this
case, you can also pass the desired column names:

.. ipython:: python

   pd.DataFrame.from_dict(
       dict([("A", [1, 2, 3]), ("B", [4, 5, 6])]),
       orient="index",
       columns=["one", "two", "three"],
   )

.. _basics.dataframe.from_records:

**DataFrame.from_records**

``DataFrame.from_records`` takes a list of tuples or an ndarray with structured
dtype. It works analogously to the normal ``DataFrame`` constructor, except that
the resulting DataFrame index may be a specific field of the structured
dtype. For example:

.. ipython:: python

   data
   pd.DataFrame.from_records(data, index="C")

.. _basics.dataframe.sel_add_del:

Column selection, addition, deletion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can treat a DataFrame semantically like a dict of like-indexed Series
objects. Getting, setting, and deleting columns works with the same syntax as
the analogous dict operations:

.. ipython:: python

   df["one"]
   df["three"] = df["one"] * df["two"]
   df["flag"] = df["one"] > 2
   df

Columns can be deleted or popped like with a dict:

.. ipython:: python

   del df["two"]
   three = df.pop("three")
   df

When inserting a scalar value, it will naturally be propagated to fill the
column:

.. ipython:: python

   df["foo"] = "bar"
   df

When inserting a Series that does not have the same index as the DataFrame, it
will be conformed to the DataFrame's index:

.. ipython:: python

   df["one_trunc"] = df["one"][:2]
   df

You can insert raw ndarrays but their length must match the length of the
DataFrame's index.

By default, columns get inserted at the end. The ``insert`` function is
available to insert at a particular location in the columns:

.. ipython:: python

   df.insert(1, "bar", df["one"])
   df

.. _dsintro.chained_assignment:

Assigning new columns in method chains
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Inspired by `dplyr's
<https://dplyr.tidyverse.org/reference/mutate.html>`__
``mutate`` verb, DataFrame has an :meth:`~pandas.DataFrame.assign`
method that allows you to easily create new columns that are potentially
derived from existing columns.

.. ipython:: python

   iris = pd.read_csv("data/iris.data")
   iris.head()
   iris.assign(sepal_ratio=iris["SepalWidth"] / iris["SepalLength"]).head()

In the example above, we inserted a precomputed value. We can also pass in
a function of one argument to be evaluated on the DataFrame being assigned to.

.. ipython:: python

   iris.assign(sepal_ratio=lambda x: (x["SepalWidth"] / x["SepalLength"])).head()

``assign`` **always** returns a copy of the data, leaving the original
DataFrame untouched.

Passing a callable, as opposed to an actual value to be inserted, is
useful when you don't have a reference to the DataFrame at hand. This is
common when using ``assign`` in a chain of operations. For example,
we can limit the DataFrame to just those observations with a Sepal Length
greater than 5, calculate the ratio, and plot:

.. ipython:: python

   @savefig basics_assign.png
   (
       iris.query("SepalLength > 5")
       .assign(
           SepalRatio=lambda x: x.SepalWidth / x.SepalLength,
           PetalRatio=lambda x: x.PetalWidth / x.PetalLength,
       )
       .plot(kind="scatter", x="SepalRatio", y="PetalRatio")
   )

Since a function is passed in, the function is computed on the DataFrame
being assigned to. Importantly, this is the DataFrame that's been filtered
to those rows with sepal length greater than 5. The filtering happens first,
and then the ratio calculations. This is an example where we didn't
have a reference to the *filtered* DataFrame available.

The function signature for ``assign`` is simply ``**kwargs``. The keys
are the column names for the new fields, and the values are either a value
to be inserted (for example, a ``Series`` or NumPy array), or a function
of one argument to be called on the ``DataFrame``. A *copy* of the original
DataFrame is returned, with the new values inserted.

Starting with Python 3.6 the order of ``**kwargs`` is preserved. This allows
for *dependent* assignment, where an expression later in ``**kwargs`` can refer
to a column created earlier in the same :meth:`~DataFrame.assign`.

.. ipython:: python

   dfa = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
   dfa.assign(C=lambda x: x["A"] + x["B"], D=lambda x: x["A"] + x["C"])

In the second expression, ``x['C']`` will refer to the newly created column,
that's equal to ``dfa['A'] + dfa['B']``.


Indexing / selection
~~~~~~~~~~~~~~~~~~~~
The basics of indexing are as follows:

.. csv-table::
    :header: "Operation", "Syntax", "Result"
    :widths: 30, 20, 10

    Select column, ``df[col]``, Series
    Select row by label, ``df.loc[label]``, Series
    Select row by integer location, ``df.iloc[loc]``, Series
    Slice rows, ``df[5:10]``, DataFrame
    Select rows by boolean vector, ``df[bool_vec]``, DataFrame

Row selection, for example, returns a Series whose index is the columns of the
DataFrame:

.. ipython:: python

   df.loc["b"]
   df.iloc[2]

For a more exhaustive treatment of sophisticated label-based indexing and
slicing, see the :ref:`section on indexing <indexing>`. We will address the
fundamentals of reindexing / conforming to new sets of labels in the
:ref:`section on reindexing <basics.reindexing>`.

.. _dsintro.alignment:

Data alignment and arithmetic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Data alignment between DataFrame objects automatically align on **both the
columns and the index (row labels)**. Again, the resulting object will have the
union of the column and row labels.

.. ipython:: python

    df = pd.DataFrame(np.random.randn(10, 4), columns=["A", "B", "C", "D"])
    df2 = pd.DataFrame(np.random.randn(7, 3), columns=["A", "B", "C"])
    df + df2

When doing an operation between DataFrame and Series, the default behavior is
to align the Series **index** on the DataFrame **columns**, thus `broadcasting
<https://numpy.org/doc/stable/user/basics.broadcasting.html>`__
row-wise. For example:

.. ipython:: python

   df - df.iloc[0]

For explicit control over the matching and broadcasting behavior, see the
section on :ref:`flexible binary operations <basics.binop>`.

Operations with scalars are just as you would expect:

.. ipython:: python

   df * 5 + 2
   1 / df
   df ** 4

.. _dsintro.boolean:

Boolean operators work as well:

.. ipython:: python

   df1 = pd.DataFrame({"a": [1, 0, 1], "b": [0, 1, 1]}, dtype=bool)
   df2 = pd.DataFrame({"a": [0, 1, 1], "b": [1, 1, 0]}, dtype=bool)
   df1 & df2
   df1 | df2
   df1 ^ df2
   -df1

Transposing
~~~~~~~~~~~

To transpose, access the ``T`` attribute (also the ``transpose`` function),
similar to an ndarray:

.. ipython:: python

   # only show the first 5 rows
   df[:5].T

.. _dsintro.numpy_interop:

DataFrame interoperability with NumPy functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Elementwise NumPy ufuncs (log, exp, sqrt, ...) and various other NumPy functions
can be used with no issues on Series and DataFrame, assuming the data within
are numeric:

.. ipython:: python

   np.exp(df)
   np.asarray(df)

DataFrame is not intended to be a drop-in replacement for ndarray as its
indexing semantics and data model are quite different in places from an n-dimensional
array.

:class:`Series` implements ``__array_ufunc__``, which allows it to work with NumPy's
`universal functions <https://numpy.org/doc/stable/reference/ufuncs.html>`_.

The ufunc is applied to the underlying array in a Series.

.. ipython:: python

   ser = pd.Series([1, 2, 3, 4])
   np.exp(ser)

.. versionchanged:: 0.25.0

   When multiple ``Series`` are passed to a ufunc, they are aligned before
   performing the operation.

Like other parts of the library, pandas will automatically align labeled inputs
as part of a ufunc with multiple inputs. For example, using :meth:`numpy.remainder`
on two :class:`Series` with differently ordered labels will align before the operation.

.. ipython:: python

   ser1 = pd.Series([1, 2, 3], index=["a", "b", "c"])
   ser2 = pd.Series([1, 3, 5], index=["b", "a", "c"])
   ser1
   ser2
   np.remainder(ser1, ser2)

As usual, the union of the two indices is taken, and non-overlapping values are filled
with missing values.

.. ipython:: python

   ser3 = pd.Series([2, 4, 6], index=["b", "c", "d"])
   ser3
   np.remainder(ser1, ser3)

When a binary ufunc is applied to a :class:`Series` and :class:`Index`, the Series
implementation takes precedence and a Series is returned.

.. ipython:: python

   ser = pd.Series([1, 2, 3])
   idx = pd.Index([4, 5, 6])

   np.maximum(ser, idx)

NumPy ufuncs are safe to apply to :class:`Series` backed by non-ndarray arrays,
for example :class:`arrays.SparseArray` (see :ref:`sparse.calculation`). If possible,
the ufunc is applied without converting the underlying data to an ndarray.

Console display
~~~~~~~~~~~~~~~

Very large DataFrames will be truncated to display them in the console.
You can also get a summary using :meth:`~pandas.DataFrame.info`.
(Here I am reading a CSV version of the **baseball** dataset from the **plyr**
R package):

.. ipython:: python
   :suppress:

   # force a summary to be printed
   pd.set_option("display.max_rows", 5)

.. ipython:: python

   baseball = pd.read_csv("data/baseball.csv")
   print(baseball)
   baseball.info()

.. ipython:: python
   :suppress:
   :okwarning:

   # restore GlobalPrintConfig
   pd.reset_option(r"^display\.")

However, using ``to_string`` will return a string representation of the
DataFrame in tabular form, though it won't always fit the console width:

.. ipython:: python

   print(baseball.iloc[-20:, :12].to_string())

Wide DataFrames will be printed across multiple rows by
default:

.. ipython:: python

   pd.DataFrame(np.random.randn(3, 12))

You can change how much to print on a single row by setting the ``display.width``
option:

.. ipython:: python

   pd.set_option("display.width", 40)  # default is 80

   pd.DataFrame(np.random.randn(3, 12))

You can adjust the max width of the individual columns by setting ``display.max_colwidth``

.. ipython:: python

   datafile = {
       "filename": ["filename_01", "filename_02"],
       "path": [
           "media/user_name/storage/folder_01/filename_01",
           "media/user_name/storage/folder_02/filename_02",
       ],
   }

   pd.set_option("display.max_colwidth", 30)
   pd.DataFrame(datafile)

   pd.set_option("display.max_colwidth", 100)
   pd.DataFrame(datafile)

.. ipython:: python
   :suppress:

   pd.reset_option("display.width")
   pd.reset_option("display.max_colwidth")

You can also disable this feature via the ``expand_frame_repr`` option.
This will print the table in one block.

DataFrame column attribute access and IPython completion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If a DataFrame column label is a valid Python variable name, the column can be
accessed like an attribute:

.. ipython:: python

   df = pd.DataFrame({"foo1": np.random.randn(5), "foo2": np.random.randn(5)})
   df
   df.foo1

The columns are also connected to the `IPython <https://ipython.org>`__
completion mechanism so they can be tab-completed:

.. code-block:: ipython

    In [5]: df.foo<TAB>  # noqa: E225, E999
    df.foo1  df.foo2
.. _duplicates:

****************
Duplicate Labels
****************

:class:`Index` objects are not required to be unique; you can have duplicate row
or column labels. This may be a bit confusing at first. If you're familiar with
SQL, you know that row labels are similar to a primary key on a table, and you
would never want duplicates in a SQL table. But one of pandas' roles is to clean
messy, real-world data before it goes to some downstream system. And real-world
data has duplicates, even in fields that are supposed to be unique.

This section describes how duplicate labels change the behavior of certain
operations, and how prevent duplicates from arising during operations, or to
detect them if they do.

.. ipython:: python

   import pandas as pd
   import numpy as np

Consequences of Duplicate Labels
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Some pandas methods (:meth:`Series.reindex` for example) just don't work with
duplicates present. The output can't be determined, and so pandas raises.

.. ipython:: python
   :okexcept:
   :okwarning:

   s1 = pd.Series([0, 1, 2], index=["a", "b", "b"])
   s1.reindex(["a", "b", "c"])

Other methods, like indexing, can give very surprising results. Typically
indexing with a scalar will *reduce dimensionality*. Slicing a ``DataFrame``
with a scalar will return a ``Series``. Slicing a ``Series`` with a scalar will
return a scalar. But with duplicates, this isn't the case.

.. ipython:: python

   df1 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=["A", "A", "B"])
   df1

We have duplicates in the columns. If we slice ``'B'``, we get back a ``Series``

.. ipython:: python

   df1["B"]  # a series

But slicing ``'A'`` returns a ``DataFrame``


.. ipython:: python

   df1["A"]  # a DataFrame

This applies to row labels as well

.. ipython:: python

   df2 = pd.DataFrame({"A": [0, 1, 2]}, index=["a", "a", "b"])
   df2
   df2.loc["b", "A"]  # a scalar
   df2.loc["a", "A"]  # a Series

Duplicate Label Detection
~~~~~~~~~~~~~~~~~~~~~~~~~

You can check whether an :class:`Index` (storing the row or column labels) is
unique with :attr:`Index.is_unique`:

.. ipython:: python

   df2
   df2.index.is_unique
   df2.columns.is_unique

.. note::

   Checking whether an index is unique is somewhat expensive for large datasets.
   pandas does cache this result, so re-checking on the same index is very fast.

:meth:`Index.duplicated` will return a boolean ndarray indicating whether a
label is repeated.

.. ipython:: python

   df2.index.duplicated()

Which can be used as a boolean filter to drop duplicate rows.

.. ipython:: python

   df2.loc[~df2.index.duplicated(), :]

If you need additional logic to handle duplicate labels, rather than just
dropping the repeats, using :meth:`~DataFrame.groupby` on the index is a common
trick. For example, we'll resolve duplicates by taking the average of all rows
with the same label.

.. ipython:: python

   df2.groupby(level=0).mean()

.. _duplicates.disallow:

Disallowing Duplicate Labels
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 1.2.0

As noted above, handling duplicates is an important feature when reading in raw
data. That said, you may want to avoid introducing duplicates as part of a data
processing pipeline (from methods like :meth:`pandas.concat`,
:meth:`~DataFrame.rename`, etc.). Both :class:`Series` and :class:`DataFrame`
*disallow* duplicate labels by calling ``.set_flags(allows_duplicate_labels=False)``.
(the default is to allow them). If there are duplicate labels, an exception
will be raised.

.. ipython:: python
   :okexcept:

   pd.Series([0, 1, 2], index=["a", "b", "b"]).set_flags(allows_duplicate_labels=False)

This applies to both row and column labels for a :class:`DataFrame`

.. ipython:: python
   :okexcept:

   pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=["A", "B", "C"],).set_flags(
       allows_duplicate_labels=False
   )

This attribute can be checked or set with :attr:`~DataFrame.flags.allows_duplicate_labels`,
which indicates whether that object can have duplicate labels.

.. ipython:: python

   df = pd.DataFrame({"A": [0, 1, 2, 3]}, index=["x", "y", "X", "Y"]).set_flags(
       allows_duplicate_labels=False
   )
   df
   df.flags.allows_duplicate_labels

:meth:`DataFrame.set_flags` can be used to return a new ``DataFrame`` with attributes
like ``allows_duplicate_labels`` set to some value

.. ipython:: python

   df2 = df.set_flags(allows_duplicate_labels=True)
   df2.flags.allows_duplicate_labels

The new ``DataFrame`` returned is a view on the same data as the old ``DataFrame``.
Or the property can just be set directly on the same object


.. ipython:: python

   df2.flags.allows_duplicate_labels = False
   df2.flags.allows_duplicate_labels

When processing raw, messy data you might initially read in the messy data
(which potentially has duplicate labels), deduplicate, and then disallow duplicates
going forward, to ensure that your data pipeline doesn't introduce duplicates.


.. code-block:: python

   >>> raw = pd.read_csv("...")
   >>> deduplicated = raw.groupby(level=0).first()  # remove duplicates
   >>> deduplicated.flags.allows_duplicate_labels = False  # disallow going forward

Setting ``allows_duplicate_labels=True`` on a ``Series`` or ``DataFrame`` with duplicate
labels or performing an operation that introduces duplicate labels on a ``Series`` or
``DataFrame`` that disallows duplicates will raise an
:class:`errors.DuplicateLabelError`.

.. ipython:: python
   :okexcept:

   df.rename(str.upper)

This error message contains the labels that are duplicated, and the numeric positions
of all the duplicates (including the "original") in the ``Series`` or ``DataFrame``

Duplicate Label Propagation
^^^^^^^^^^^^^^^^^^^^^^^^^^^

In general, disallowing duplicates is "sticky". It's preserved through
operations.

.. ipython:: python
   :okexcept:

   s1 = pd.Series(0, index=["a", "b"]).set_flags(allows_duplicate_labels=False)
   s1
   s1.head().rename({"a": "b"})

.. warning::

   This is an experimental feature. Currently, many methods fail to
   propagate the ``allows_duplicate_labels`` value. In future versions
   it is expected that every method taking or returning one or more
   DataFrame or Series objects will propagate ``allows_duplicate_labels``.
.. _scale:

*************************
Scaling to large datasets
*************************

pandas provides data structures for in-memory analytics, which makes using pandas
to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets
that are a sizable fraction of memory become unwieldy, as some pandas operations need
to make intermediate copies.

This document provides a few recommendations for scaling your analysis to larger datasets.
It's a complement to :ref:`enhancingperf`, which focuses on speeding up analysis
for datasets that fit in memory.

But first, it's worth considering *not using pandas*. pandas isn't the right
tool for all situations. If you're working with very large datasets and a tool
like PostgreSQL fits your needs, then you should probably be using that.
Assuming you want or need the expressiveness and power of pandas, let's carry on.

Load less data
--------------

Suppose our raw dataset on disk has many columns::

                        id_0    name_0       x_0       y_0  id_1   name_1       x_1  ...  name_8       x_8       y_8  id_9   name_9       x_9       y_9
   timestamp                                                                         ...
   2000-01-01 00:00:00  1015   Michael -0.399453  0.095427   994    Frank -0.176842  ...     Dan -0.315310  0.713892  1025   Victor -0.135779  0.346801
   2000-01-01 00:01:00   969  Patricia  0.650773 -0.874275  1003    Laura  0.459153  ...  Ursula  0.913244 -0.630308  1047    Wendy -0.886285  0.035852
   2000-01-01 00:02:00  1016    Victor -0.721465 -0.584710  1046  Michael  0.524994  ...     Ray -0.656593  0.692568  1064   Yvonne  0.070426  0.432047
   2000-01-01 00:03:00   939     Alice -0.746004 -0.908008   996   Ingrid -0.414523  ...   Jerry -0.958994  0.608210   978    Wendy  0.855949 -0.648988
   2000-01-01 00:04:00  1017       Dan  0.919451 -0.803504  1048    Jerry -0.569235  ...   Frank -0.577022 -0.409088   994      Bob -0.270132  0.335176
   ...                   ...       ...       ...       ...   ...      ...       ...  ...     ...       ...       ...   ...      ...       ...       ...
   2000-12-30 23:56:00   999       Tim  0.162578  0.512817   973    Kevin -0.403352  ...     Tim -0.380415  0.008097  1041  Charlie  0.191477 -0.599519
   2000-12-30 23:57:00   970     Laura -0.433586 -0.600289   958   Oliver -0.966577  ...   Zelda  0.971274  0.402032  1038   Ursula  0.574016 -0.930992
   2000-12-30 23:58:00  1065     Edith  0.232211 -0.454540   971      Tim  0.158484  ...   Alice -0.222079 -0.919274  1022      Dan  0.031345 -0.657755
   2000-12-30 23:59:00  1019    Ingrid  0.322208 -0.615974   981   Hannah  0.607517  ...   Sarah -0.424440 -0.117274   990   George -0.375530  0.563312
   2000-12-31 00:00:00   937    Ursula -0.906523  0.943178  1018    Alice -0.564513  ...   Jerry  0.236837  0.807650   985   Oliver  0.777642  0.783392

   [525601 rows x 40 columns]

That can be generated by the following code snippet:

.. ipython:: python

   import pandas as pd
   import numpy as np

   def make_timeseries(start="2000-01-01", end="2000-12-31", freq="1D", seed=None):
       index = pd.date_range(start=start, end=end, freq=freq, name="timestamp")
       n = len(index)
       state = np.random.RandomState(seed)
       columns = {
           "name": state.choice(["Alice", "Bob", "Charlie"], size=n),
           "id": state.poisson(1000, size=n),
           "x": state.rand(n) * 2 - 1,
           "y": state.rand(n) * 2 - 1,
       }
       df = pd.DataFrame(columns, index=index, columns=sorted(columns))
       if df.index[-1] == end:
           df = df.iloc[:-1]
       return df

   timeseries = [
       make_timeseries(freq="1T", seed=i).rename(columns=lambda x: f"{x}_{i}")
       for i in range(10)
   ]
   ts_wide = pd.concat(timeseries, axis=1)
   ts_wide.to_parquet("timeseries_wide.parquet")

To load the columns we want, we have two options.
Option 1 loads in all the data and then filters to what we need.

.. ipython:: python

   columns = ["id_0", "name_0", "x_0", "y_0"]

   pd.read_parquet("timeseries_wide.parquet")[columns]

Option 2 only loads the columns we request.

.. ipython:: python

   pd.read_parquet("timeseries_wide.parquet", columns=columns)

If we were to measure the memory usage of the two calls, we'd see that specifying
``columns`` uses about 1/10th the memory in this case.

With :func:`pandas.read_csv`, you can specify ``usecols`` to limit the columns
read into memory. Not all file formats that can be read by pandas provide an option
to read a subset of columns.

Use efficient datatypes
-----------------------

The default pandas data types are not the most memory efficient. This is
especially true for text data columns with relatively few unique values (commonly
referred to as "low-cardinality" data). By using more efficient data types, you
can store larger datasets in memory.

.. ipython:: python

   ts = make_timeseries(freq="30S", seed=0)
   ts.to_parquet("timeseries.parquet")
   ts = pd.read_parquet("timeseries.parquet")
   ts

Now, let's inspect the data types and memory usage to see where we should focus our
attention.

.. ipython:: python

   ts.dtypes

.. ipython:: python

   ts.memory_usage(deep=True)  # memory usage in bytes


The ``name`` column is taking up much more memory than any other. It has just a
few unique values, so it's a good candidate for converting to a
:class:`pandas.Categorical`. With a :class:`pandas.Categorical`, we store each unique name once and use
space-efficient integers to know which specific name is used in each row.


.. ipython:: python

   ts2 = ts.copy()
   ts2["name"] = ts2["name"].astype("category")
   ts2.memory_usage(deep=True)

We can go a bit further and downcast the numeric columns to their smallest types
using :func:`pandas.to_numeric`.

.. ipython:: python

   ts2["id"] = pd.to_numeric(ts2["id"], downcast="unsigned")
   ts2[["x", "y"]] = ts2[["x", "y"]].apply(pd.to_numeric, downcast="float")
   ts2.dtypes

.. ipython:: python

   ts2.memory_usage(deep=True)

.. ipython:: python

   reduction = ts2.memory_usage(deep=True).sum() / ts.memory_usage(deep=True).sum()
   print(f"{reduction:0.2f}")

In all, we've reduced the in-memory footprint of this dataset to 1/5 of its
original size.

See :ref:`categorical` for more on :class:`pandas.Categorical` and :ref:`basics.dtypes`
for an overview of all of pandas' dtypes.

Use chunking
------------

Some workloads can be achieved with chunking: splitting a large problem like "convert this
directory of CSVs to parquet" into a bunch of small problems ("convert this individual CSV
file into a Parquet file. Now repeat that for each file in this directory."). As long as each chunk
fits in memory, you can work with datasets that are much larger than memory.

.. note::

   Chunking works well when the operation you're performing requires zero or minimal
   coordination between chunks. For more complicated workflows, you're better off
   :ref:`using another library <scale.other_libraries>`.

Suppose we have an even larger "logical dataset" on disk that's a directory of parquet
files. Each file in the directory represents a different year of the entire dataset.

.. ipython:: python

   import pathlib

   N = 12
   starts = [f"20{i:>02d}-01-01" for i in range(N)]
   ends = [f"20{i:>02d}-12-13" for i in range(N)]

   pathlib.Path("data/timeseries").mkdir(exist_ok=True)

   for i, (start, end) in enumerate(zip(starts, ends)):
       ts = make_timeseries(start=start, end=end, freq="1T", seed=i)
       ts.to_parquet(f"data/timeseries/ts-{i:0>2d}.parquet")


::

   data
   └── timeseries
       ├── ts-00.parquet
       ├── ts-01.parquet
       ├── ts-02.parquet
       ├── ts-03.parquet
       ├── ts-04.parquet
       ├── ts-05.parquet
       ├── ts-06.parquet
       ├── ts-07.parquet
       ├── ts-08.parquet
       ├── ts-09.parquet
       ├── ts-10.parquet
       └── ts-11.parquet

Now we'll implement an out-of-core :meth:`pandas.Series.value_counts`. The peak memory usage of this
workflow is the single largest chunk, plus a small series storing the unique value
counts up to this point. As long as each individual file fits in memory, this will
work for arbitrary-sized datasets.

.. ipython:: python

   %%time
   files = pathlib.Path("data/timeseries/").glob("ts*.parquet")
   counts = pd.Series(dtype=int)
   for path in files:
       df = pd.read_parquet(path)
       counts = counts.add(df["name"].value_counts(), fill_value=0)
   counts.astype(int)

Some readers, like :meth:`pandas.read_csv`, offer parameters to control the
``chunksize`` when reading a single file.

Manually chunking is an OK option for workflows that don't
require too sophisticated of operations. Some operations, like :meth:`pandas.DataFrame.groupby`, are
much harder to do chunkwise. In these cases, you may be better switching to a
different library that implements these out-of-core algorithms for you.

.. _scale.other_libraries:

Use other libraries
-------------------

pandas is just one library offering a DataFrame API. Because of its popularity,
pandas' API has become something of a standard that other libraries implement.
The pandas documentation maintains a list of libraries implementing a DataFrame API
in :ref:`our ecosystem page <ecosystem.out-of-core>`.

For example, `Dask`_, a parallel computing library, has `dask.dataframe`_, a
pandas-like API for working with larger than memory datasets in parallel. Dask
can use multiple threads or processes on a single machine, or a cluster of
machines to process data in parallel.


We'll import ``dask.dataframe`` and notice that the API feels similar to pandas.
We can use Dask's ``read_parquet`` function, but provide a globstring of files to read in.

.. ipython:: python
   :okwarning:

   import dask.dataframe as dd

   ddf = dd.read_parquet("data/timeseries/ts*.parquet", engine="pyarrow")
   ddf

Inspecting the ``ddf`` object, we see a few things

* There are familiar attributes like ``.columns`` and ``.dtypes``
* There are familiar methods like ``.groupby``, ``.sum``, etc.
* There are new attributes like ``.npartitions`` and ``.divisions``

The partitions and divisions are how Dask parallelizes computation. A **Dask**
DataFrame is made up of many pandas :class:`pandas.DataFrame`. A single method call on a
Dask DataFrame ends up making many pandas method calls, and Dask knows how to
coordinate everything to get the result.

.. ipython:: python

   ddf.columns
   ddf.dtypes
   ddf.npartitions

One major difference: the ``dask.dataframe`` API is *lazy*. If you look at the
repr above, you'll notice that the values aren't actually printed out; just the
column names and dtypes. That's because Dask hasn't actually read the data yet.
Rather than executing immediately, doing operations build up a **task graph**.

.. ipython:: python
   :okwarning:

   ddf
   ddf["name"]
   ddf["name"].value_counts()

Each of these calls is instant because the result isn't being computed yet.
We're just building up a list of computation to do when someone needs the
result. Dask knows that the return type of a :class:`pandas.Series.value_counts`
is a pandas :class:`pandas.Series` with a certain dtype and a certain name. So the Dask version
returns a Dask Series with the same dtype and the same name.

To get the actual result you can call ``.compute()``.

.. ipython:: python

   %time ddf["name"].value_counts().compute()

At that point, you get back the same thing you'd get with pandas, in this case
a concrete pandas :class:`pandas.Series` with the count of each ``name``.

Calling ``.compute`` causes the full task graph to be executed. This includes
reading the data, selecting the columns, and doing the ``value_counts``. The
execution is done *in parallel* where possible, and Dask tries to keep the
overall memory footprint small. You can work with datasets that are much larger
than memory, as long as each partition (a regular pandas :class:`pandas.DataFrame`) fits in memory.

By default, ``dask.dataframe`` operations use a threadpool to do operations in
parallel. We can also connect to a cluster to distribute the work on many
machines. In this case we'll connect to a local "cluster" made up of several
processes on this single machine.

.. code-block:: python

   >>> from dask.distributed import Client, LocalCluster

   >>> cluster = LocalCluster()
   >>> client = Client(cluster)
   >>> client
   <Client: 'tcp://127.0.0.1:53349' processes=4 threads=8, memory=17.18 GB>

Once this ``client`` is created, all of Dask's computation will take place on
the cluster (which is just processes in this case).

Dask implements the most used parts of the pandas API. For example, we can do
a familiar groupby aggregation.

.. ipython:: python

   %time ddf.groupby("name")[["x", "y"]].mean().compute().head()

The grouping and aggregation is done out-of-core and in parallel.

When Dask knows the ``divisions`` of a dataset, certain optimizations are
possible. When reading parquet datasets written by dask, the divisions will be
known automatically. In this case, since we created the parquet files manually,
we need to supply the divisions manually.

.. ipython:: python
   :okwarning:

   N = 12
   starts = [f"20{i:>02d}-01-01" for i in range(N)]
   ends = [f"20{i:>02d}-12-13" for i in range(N)]

   divisions = tuple(pd.to_datetime(starts)) + (pd.Timestamp(ends[-1]),)
   ddf.divisions = divisions
   ddf

Now we can do things like fast random access with ``.loc``.

.. ipython:: python
   :okwarning:

   ddf.loc["2002-01-01 12:01":"2002-01-01 12:05"].compute()

Dask knows to just look in the 3rd partition for selecting values in 2002. It
doesn't need to look at any other data.

Many workflows involve a large amount of data and processing it in a way that
reduces the size to something that fits in memory. In this case, we'll resample
to daily frequency and take the mean. Once we've taken the mean, we know the
results will fit in memory, so we can safely call ``compute`` without running
out of memory. At that point it's just a regular pandas object.

.. ipython:: python
   :okwarning:

   @savefig dask_resample.png
   ddf[["x", "y"]].resample("1D").mean().cumsum().compute().plot()

These Dask examples have all be done using multiple processes on a single
machine. Dask can be `deployed on a cluster
<https://docs.dask.org/en/latest/setup.html>`_ to scale up to even larger
datasets.

You see more dask examples at https://examples.dask.org.

.. _Dask: https://dask.org
.. _dask.dataframe: https://docs.dask.org/en/latest/dataframe.html
.. _categorical:

{{ header }}

****************
Categorical data
****************

This is an introduction to pandas categorical data type, including a short comparison
with R's ``factor``.

``Categoricals`` are a pandas data type corresponding to categorical variables in
statistics. A categorical variable takes on a limited, and usually fixed,
number of possible values (``categories``; ``levels`` in R). Examples are gender,
social class, blood type, country affiliation, observation time or rating via
Likert scales.

In contrast to statistical categorical variables, categorical data might have an order (e.g.
'strongly agree' vs 'agree' or 'first observation' vs. 'second observation'), but numerical
operations (additions, divisions, ...) are not possible.

All values of categorical data are either in ``categories`` or ``np.nan``. Order is defined by
the order of ``categories``, not lexical order of the values. Internally, the data structure
consists of a ``categories`` array and an integer array of ``codes`` which point to the real value in
the ``categories`` array.

The categorical data type is useful in the following cases:

* A string variable consisting of only a few different values. Converting such a string
  variable to a categorical variable will save some memory, see :ref:`here <categorical.memory>`.
* The lexical order of a variable is not the same as the logical order ("one", "two", "three").
  By converting to a categorical and specifying an order on the categories, sorting and
  min/max will use the logical order instead of the lexical order, see :ref:`here <categorical.sort>`.
* As a signal to other Python libraries that this column should be treated as a categorical
  variable (e.g. to use suitable statistical methods or plot types).

See also the :ref:`API docs on categoricals<api.arrays.categorical>`.

.. _categorical.objectcreation:

Object creation
---------------

Series creation
~~~~~~~~~~~~~~~

Categorical ``Series`` or columns in a ``DataFrame`` can be created in several ways:

By specifying ``dtype="category"`` when constructing a ``Series``:

.. ipython:: python

    s = pd.Series(["a", "b", "c", "a"], dtype="category")
    s

By converting an existing ``Series`` or column to a ``category`` dtype:

.. ipython:: python

    df = pd.DataFrame({"A": ["a", "b", "c", "a"]})
    df["B"] = df["A"].astype("category")
    df

By using special functions, such as :func:`~pandas.cut`, which groups data into
discrete bins. See the :ref:`example on tiling <reshaping.tile.cut>` in the docs.

.. ipython:: python

    df = pd.DataFrame({"value": np.random.randint(0, 100, 20)})
    labels = ["{0} - {1}".format(i, i + 9) for i in range(0, 100, 10)]

    df["group"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)
    df.head(10)

By passing a :class:`pandas.Categorical` object to a ``Series`` or assigning it to a ``DataFrame``.

.. ipython:: python

    raw_cat = pd.Categorical(
        ["a", "b", "c", "a"], categories=["b", "c", "d"], ordered=False
    )
    s = pd.Series(raw_cat)
    s
    df = pd.DataFrame({"A": ["a", "b", "c", "a"]})
    df["B"] = raw_cat
    df

Categorical data has a specific ``category`` :ref:`dtype <basics.dtypes>`:

.. ipython:: python

    df.dtypes

DataFrame creation
~~~~~~~~~~~~~~~~~~

Similar to the previous section where a single column was converted to categorical, all columns in a
``DataFrame`` can be batch converted to categorical either during or after construction.

This can be done during construction by specifying ``dtype="category"`` in the ``DataFrame`` constructor:

.. ipython:: python

    df = pd.DataFrame({"A": list("abca"), "B": list("bccd")}, dtype="category")
    df.dtypes

Note that the categories present in each column differ; the conversion is done column by column, so
only labels present in a given column are categories:

.. ipython:: python

    df["A"]
    df["B"]


Analogously, all columns in an existing ``DataFrame`` can be batch converted using :meth:`DataFrame.astype`:

.. ipython:: python

    df = pd.DataFrame({"A": list("abca"), "B": list("bccd")})
    df_cat = df.astype("category")
    df_cat.dtypes

This conversion is likewise done column by column:

.. ipython:: python

    df_cat["A"]
    df_cat["B"]


Controlling behavior
~~~~~~~~~~~~~~~~~~~~

In the examples above where we passed ``dtype='category'``, we used the default
behavior:

1. Categories are inferred from the data.
2. Categories are unordered.

To control those behaviors, instead of passing ``'category'``, use an instance
of :class:`~pandas.api.types.CategoricalDtype`.

.. ipython:: python

    from pandas.api.types import CategoricalDtype

    s = pd.Series(["a", "b", "c", "a"])
    cat_type = CategoricalDtype(categories=["b", "c", "d"], ordered=True)
    s_cat = s.astype(cat_type)
    s_cat

Similarly, a ``CategoricalDtype`` can be used with a ``DataFrame`` to ensure that categories
are consistent among all columns.

.. ipython:: python

    from pandas.api.types import CategoricalDtype

    df = pd.DataFrame({"A": list("abca"), "B": list("bccd")})
    cat_type = CategoricalDtype(categories=list("abcd"), ordered=True)
    df_cat = df.astype(cat_type)
    df_cat["A"]
    df_cat["B"]

.. note::

    To perform table-wise conversion, where all labels in the entire ``DataFrame`` are used as
    categories for each column, the ``categories`` parameter can be determined programmatically by
    ``categories = pd.unique(df.to_numpy().ravel())``.

If you already have ``codes`` and ``categories``, you can use the
:func:`~pandas.Categorical.from_codes` constructor to save the factorize step
during normal constructor mode:

.. ipython:: python

    splitter = np.random.choice([0, 1], 5, p=[0.5, 0.5])
    s = pd.Series(pd.Categorical.from_codes(splitter, categories=["train", "test"]))


Regaining original data
~~~~~~~~~~~~~~~~~~~~~~~

To get back to the original ``Series`` or NumPy array, use
``Series.astype(original_dtype)`` or ``np.asarray(categorical)``:

.. ipython:: python

    s = pd.Series(["a", "b", "c", "a"])
    s
    s2 = s.astype("category")
    s2
    s2.astype(str)
    np.asarray(s2)

.. note::

    In contrast to R's ``factor`` function, categorical data is not converting input values to
    strings; categories will end up the same data type as the original values.

.. note::

    In contrast to R's ``factor`` function, there is currently no way to assign/change labels at
    creation time. Use ``categories`` to change the categories after creation time.

.. _categorical.categoricaldtype:

CategoricalDtype
----------------

A categorical's type is fully described by

1. ``categories``: a sequence of unique values and no missing values
2. ``ordered``: a boolean

This information can be stored in a :class:`~pandas.api.types.CategoricalDtype`.
The ``categories`` argument is optional, which implies that the actual categories
should be inferred from whatever is present in the data when the
:class:`pandas.Categorical` is created. The categories are assumed to be unordered
by default.

.. ipython:: python

   from pandas.api.types import CategoricalDtype

   CategoricalDtype(["a", "b", "c"])
   CategoricalDtype(["a", "b", "c"], ordered=True)
   CategoricalDtype()

A :class:`~pandas.api.types.CategoricalDtype` can be used in any place pandas
expects a ``dtype``. For example :func:`pandas.read_csv`,
:func:`pandas.DataFrame.astype`, or in the ``Series`` constructor.

.. note::

    As a convenience, you can use the string ``'category'`` in place of a
    :class:`~pandas.api.types.CategoricalDtype` when you want the default behavior of
    the categories being unordered, and equal to the set values present in the
    array. In other words, ``dtype='category'`` is equivalent to
    ``dtype=CategoricalDtype()``.

Equality semantics
~~~~~~~~~~~~~~~~~~

Two instances of :class:`~pandas.api.types.CategoricalDtype` compare equal
whenever they have the same categories and order. When comparing two
unordered categoricals, the order of the ``categories`` is not considered.

.. ipython:: python

   c1 = CategoricalDtype(["a", "b", "c"], ordered=False)

   # Equal, since order is not considered when ordered=False
   c1 == CategoricalDtype(["b", "c", "a"], ordered=False)

   # Unequal, since the second CategoricalDtype is ordered
   c1 == CategoricalDtype(["a", "b", "c"], ordered=True)

All instances of ``CategoricalDtype`` compare equal to the string ``'category'``.

.. ipython:: python

   c1 == "category"

.. warning::

   Since ``dtype='category'`` is essentially ``CategoricalDtype(None, False)``,
   and since all instances ``CategoricalDtype`` compare equal to ``'category'``,
   all instances of ``CategoricalDtype`` compare equal to a
   ``CategoricalDtype(None, False)``, regardless of ``categories`` or
   ``ordered``.

Description
-----------

Using :meth:`~DataFrame.describe` on categorical data will produce similar
output to a ``Series`` or ``DataFrame`` of type ``string``.

.. ipython:: python

    cat = pd.Categorical(["a", "c", "c", np.nan], categories=["b", "a", "c"])
    df = pd.DataFrame({"cat": cat, "s": ["a", "c", "c", np.nan]})
    df.describe()
    df["cat"].describe()

.. _categorical.cat:

Working with categories
-----------------------

Categorical data has a ``categories`` and a ``ordered`` property, which list their
possible values and whether the ordering matters or not. These properties are
exposed as ``s.cat.categories`` and ``s.cat.ordered``. If you don't manually
specify categories and ordering, they are inferred from the passed arguments.

.. ipython:: python

    s = pd.Series(["a", "b", "c", "a"], dtype="category")
    s.cat.categories
    s.cat.ordered

It's also possible to pass in the categories in a specific order:

.. ipython:: python

    s = pd.Series(pd.Categorical(["a", "b", "c", "a"], categories=["c", "b", "a"]))
    s.cat.categories
    s.cat.ordered

.. note::

    New categorical data are **not** automatically ordered. You must explicitly
    pass ``ordered=True`` to indicate an ordered ``Categorical``.


.. note::

    The result of :meth:`~Series.unique` is not always the same as ``Series.cat.categories``,
    because ``Series.unique()`` has a couple of guarantees, namely that it returns categories
    in the order of appearance, and it only includes values that are actually present.

    .. ipython:: python

         s = pd.Series(list("babc")).astype(CategoricalDtype(list("abcd")))
         s

         # categories
         s.cat.categories

         # uniques
         s.unique()

Renaming categories
~~~~~~~~~~~~~~~~~~~

Renaming categories is done by assigning new values to the
``Series.cat.categories`` property or by using the
:meth:`~pandas.Categorical.rename_categories` method:


.. ipython:: python

    s = pd.Series(["a", "b", "c", "a"], dtype="category")
    s
    s.cat.categories = ["Group %s" % g for g in s.cat.categories]
    s
    s = s.cat.rename_categories([1, 2, 3])
    s
    # You can also pass a dict-like object to map the renaming
    s = s.cat.rename_categories({1: "x", 2: "y", 3: "z"})
    s

.. note::

    In contrast to R's ``factor``, categorical data can have categories of other types than string.

.. note::

    Be aware that assigning new categories is an inplace operation, while most other operations
    under ``Series.cat`` per default return a new ``Series`` of dtype ``category``.

Categories must be unique or a ``ValueError`` is raised:

.. ipython:: python

    try:
        s.cat.categories = [1, 1, 1]
    except ValueError as e:
        print("ValueError:", str(e))

Categories must also not be ``NaN`` or a ``ValueError`` is raised:

.. ipython:: python

    try:
        s.cat.categories = [1, 2, np.nan]
    except ValueError as e:
        print("ValueError:", str(e))

Appending new categories
~~~~~~~~~~~~~~~~~~~~~~~~

Appending categories can be done by using the
:meth:`~pandas.Categorical.add_categories` method:

.. ipython:: python

    s = s.cat.add_categories([4])
    s.cat.categories
    s

Removing categories
~~~~~~~~~~~~~~~~~~~

Removing categories can be done by using the
:meth:`~pandas.Categorical.remove_categories` method. Values which are removed
are replaced by ``np.nan``.:

.. ipython:: python

    s = s.cat.remove_categories([4])
    s

Removing unused categories
~~~~~~~~~~~~~~~~~~~~~~~~~~

Removing unused categories can also be done:

.. ipython:: python

    s = pd.Series(pd.Categorical(["a", "b", "a"], categories=["a", "b", "c", "d"]))
    s
    s.cat.remove_unused_categories()

Setting categories
~~~~~~~~~~~~~~~~~~

If you want to do remove and add new categories in one step (which has some
speed advantage), or simply set the categories to a predefined scale,
use :meth:`~pandas.Categorical.set_categories`.


.. ipython:: python

    s = pd.Series(["one", "two", "four", "-"], dtype="category")
    s
    s = s.cat.set_categories(["one", "two", "three", "four"])
    s

.. note::
    Be aware that :func:`Categorical.set_categories` cannot know whether some category is omitted
    intentionally or because it is misspelled or (under Python3) due to a type difference (e.g.,
    NumPy S1 dtype and Python strings). This can result in surprising behaviour!

Sorting and order
-----------------

.. _categorical.sort:

If categorical data is ordered (``s.cat.ordered == True``), then the order of the categories has a
meaning and certain operations are possible. If the categorical is unordered, ``.min()/.max()`` will raise a ``TypeError``.

.. ipython:: python

    s = pd.Series(pd.Categorical(["a", "b", "c", "a"], ordered=False))
    s.sort_values(inplace=True)
    s = pd.Series(["a", "b", "c", "a"]).astype(CategoricalDtype(ordered=True))
    s.sort_values(inplace=True)
    s
    s.min(), s.max()

You can set categorical data to be ordered by using ``as_ordered()`` or unordered by using ``as_unordered()``. These will by
default return a *new* object.

.. ipython:: python

    s.cat.as_ordered()
    s.cat.as_unordered()

Sorting will use the order defined by categories, not any lexical order present on the data type.
This is even true for strings and numeric data:

.. ipython:: python

    s = pd.Series([1, 2, 3, 1], dtype="category")
    s = s.cat.set_categories([2, 3, 1], ordered=True)
    s
    s.sort_values(inplace=True)
    s
    s.min(), s.max()


Reordering
~~~~~~~~~~

Reordering the categories is possible via the :meth:`Categorical.reorder_categories` and
the :meth:`Categorical.set_categories` methods. For :meth:`Categorical.reorder_categories`, all
old categories must be included in the new categories and no new categories are allowed. This will
necessarily make the sort order the same as the categories order.

.. ipython:: python

    s = pd.Series([1, 2, 3, 1], dtype="category")
    s = s.cat.reorder_categories([2, 3, 1], ordered=True)
    s
    s.sort_values(inplace=True)
    s
    s.min(), s.max()

.. note::

    Note the difference between assigning new categories and reordering the categories: the first
    renames categories and therefore the individual values in the ``Series``, but if the first
    position was sorted last, the renamed value will still be sorted last. Reordering means that the
    way values are sorted is different afterwards, but not that individual values in the
    ``Series`` are changed.

.. note::

    If the ``Categorical`` is not ordered, :meth:`Series.min` and :meth:`Series.max` will raise
    ``TypeError``. Numeric operations like ``+``, ``-``, ``*``, ``/`` and operations based on them
    (e.g. :meth:`Series.median`, which would need to compute the mean between two values if the length
    of an array is even) do not work and raise a ``TypeError``.

Multi column sorting
~~~~~~~~~~~~~~~~~~~~

A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns.
The ordering of the categorical is determined by the ``categories`` of that column.

.. ipython:: python

   dfs = pd.DataFrame(
       {
           "A": pd.Categorical(
               list("bbeebbaa"),
               categories=["e", "a", "b"],
               ordered=True,
           ),
           "B": [1, 2, 1, 2, 2, 1, 2, 1],
       }
   )
   dfs.sort_values(by=["A", "B"])

Reordering the ``categories`` changes a future sort.

.. ipython:: python

   dfs["A"] = dfs["A"].cat.reorder_categories(["a", "b", "e"])
   dfs.sort_values(by=["A", "B"])

Comparisons
-----------

Comparing categorical data with other objects is possible in three cases:

* Comparing equality (``==`` and ``!=``) to a list-like object (list, Series, array,
  ...) of the same length as the categorical data.
* All comparisons (``==``, ``!=``, ``>``, ``>=``, ``<``, and ``<=``) of categorical data to
  another categorical Series, when ``ordered==True`` and the ``categories`` are the same.
* All comparisons of a categorical data to a scalar.

All other comparisons, especially "non-equality" comparisons of two categoricals with different
categories or a categorical with any list-like object, will raise a ``TypeError``.

.. note::

    Any "non-equality" comparisons of categorical data with a ``Series``, ``np.array``, ``list`` or
    categorical data with different categories or ordering will raise a ``TypeError`` because custom
    categories ordering could be interpreted in two ways: one with taking into account the
    ordering and one without.

.. ipython:: python

    cat = pd.Series([1, 2, 3]).astype(CategoricalDtype([3, 2, 1], ordered=True))
    cat_base = pd.Series([2, 2, 2]).astype(CategoricalDtype([3, 2, 1], ordered=True))
    cat_base2 = pd.Series([2, 2, 2]).astype(CategoricalDtype(ordered=True))

    cat
    cat_base
    cat_base2

Comparing to a categorical with the same categories and ordering or to a scalar works:

.. ipython:: python

    cat > cat_base
    cat > 2

Equality comparisons work with any list-like object of same length and scalars:

.. ipython:: python

    cat == cat_base
    cat == np.array([1, 2, 3])
    cat == 2

This doesn't work because the categories are not the same:

.. ipython:: python

    try:
        cat > cat_base2
    except TypeError as e:
        print("TypeError:", str(e))

If you want to do a "non-equality" comparison of a categorical series with a list-like object
which is not categorical data, you need to be explicit and convert the categorical data back to
the original values:

.. ipython:: python

    base = np.array([1, 2, 3])

    try:
        cat > base
    except TypeError as e:
        print("TypeError:", str(e))

    np.asarray(cat) > base

When you compare two unordered categoricals with the same categories, the order is not considered:

.. ipython:: python

   c1 = pd.Categorical(["a", "b"], categories=["a", "b"], ordered=False)
   c2 = pd.Categorical(["a", "b"], categories=["b", "a"], ordered=False)
   c1 == c2

Operations
----------

Apart from :meth:`Series.min`, :meth:`Series.max` and :meth:`Series.mode`, the
following operations are possible with categorical data:

``Series`` methods like :meth:`Series.value_counts` will use all categories,
even if some categories are not present in the data:

.. ipython:: python

    s = pd.Series(pd.Categorical(["a", "b", "c", "c"], categories=["c", "a", "b", "d"]))
    s.value_counts()

``DataFrame`` methods like :meth:`DataFrame.sum` also show "unused" categories.

.. ipython:: python

    columns = pd.Categorical(
        ["One", "One", "Two"], categories=["One", "Two", "Three"], ordered=True
    )
    df = pd.DataFrame(
        data=[[1, 2, 3], [4, 5, 6]],
        columns=pd.MultiIndex.from_arrays([["A", "B", "B"], columns]),
    )
    df.groupby(axis=1, level=1).sum()

Groupby will also show "unused" categories:

.. ipython:: python

    cats = pd.Categorical(
        ["a", "b", "b", "b", "c", "c", "c"], categories=["a", "b", "c", "d"]
    )
    df = pd.DataFrame({"cats": cats, "values": [1, 2, 2, 2, 3, 4, 5]})
    df.groupby("cats").mean()

    cats2 = pd.Categorical(["a", "a", "b", "b"], categories=["a", "b", "c"])
    df2 = pd.DataFrame(
        {
            "cats": cats2,
            "B": ["c", "d", "c", "d"],
            "values": [1, 2, 3, 4],
        }
    )
    df2.groupby(["cats", "B"]).mean()


Pivot tables:

.. ipython:: python

    raw_cat = pd.Categorical(["a", "a", "b", "b"], categories=["a", "b", "c"])
    df = pd.DataFrame({"A": raw_cat, "B": ["c", "d", "c", "d"], "values": [1, 2, 3, 4]})
    pd.pivot_table(df, values="values", index=["A", "B"])

Data munging
------------

The optimized pandas data access methods  ``.loc``, ``.iloc``, ``.at``, and ``.iat``,
work as normal. The only difference is the return type (for getting) and
that only values already in ``categories`` can be assigned.

Getting
~~~~~~~

If the slicing operation returns either a ``DataFrame`` or a column of type
``Series``, the ``category`` dtype is preserved.

.. ipython:: python

    idx = pd.Index(["h", "i", "j", "k", "l", "m", "n"])
    cats = pd.Series(["a", "b", "b", "b", "c", "c", "c"], dtype="category", index=idx)
    values = [1, 2, 2, 2, 3, 4, 5]
    df = pd.DataFrame({"cats": cats, "values": values}, index=idx)
    df.iloc[2:4, :]
    df.iloc[2:4, :].dtypes
    df.loc["h":"j", "cats"]
    df[df["cats"] == "b"]

An example where the category type is not preserved is if you take one single
row: the resulting ``Series`` is of dtype ``object``:

.. ipython:: python

    # get the complete "h" row as a Series
    df.loc["h", :]

Returning a single item from categorical data will also return the value, not a categorical
of length "1".

.. ipython:: python

    df.iat[0, 0]
    df["cats"].cat.categories = ["x", "y", "z"]
    df.at["h", "cats"]  # returns a string

.. note::
    The is in contrast to R's ``factor`` function, where ``factor(c(1,2,3))[1]``
    returns a single value ``factor``.

To get a single value ``Series`` of type ``category``, you pass in a list with
a single value:

.. ipython:: python

    df.loc[["h"], "cats"]

String and datetime accessors
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The accessors  ``.dt`` and ``.str`` will work if the ``s.cat.categories`` are of
an appropriate type:


.. ipython:: python

    str_s = pd.Series(list("aabb"))
    str_cat = str_s.astype("category")
    str_cat
    str_cat.str.contains("a")

    date_s = pd.Series(pd.date_range("1/1/2015", periods=5))
    date_cat = date_s.astype("category")
    date_cat
    date_cat.dt.day

.. note::

    The returned ``Series`` (or ``DataFrame``) is of the same type as if you used the
    ``.str.<method>`` / ``.dt.<method>`` on a ``Series`` of that type (and not of
    type ``category``!).

That means, that the returned values from methods and properties on the accessors of a
``Series`` and the returned values from methods and properties on the accessors of this
``Series`` transformed to one of type ``category`` will be equal:

.. ipython:: python

    ret_s = str_s.str.contains("a")
    ret_cat = str_cat.str.contains("a")
    ret_s.dtype == ret_cat.dtype
    ret_s == ret_cat

.. note::

    The work is done on the ``categories`` and then a new ``Series`` is constructed. This has
    some performance implication if you have a ``Series`` of type string, where lots of elements
    are repeated (i.e. the number of unique elements in the ``Series`` is a lot smaller than the
    length of the ``Series``). In this case it can be faster to convert the original ``Series``
    to one of type ``category`` and use ``.str.<method>`` or ``.dt.<property>`` on that.

Setting
~~~~~~~

Setting values in a categorical column (or ``Series``) works as long as the
value is included in the ``categories``:

.. ipython:: python

    idx = pd.Index(["h", "i", "j", "k", "l", "m", "n"])
    cats = pd.Categorical(["a", "a", "a", "a", "a", "a", "a"], categories=["a", "b"])
    values = [1, 1, 1, 1, 1, 1, 1]
    df = pd.DataFrame({"cats": cats, "values": values}, index=idx)

    df.iloc[2:4, :] = [["b", 2], ["b", 2]]
    df
    try:
        df.iloc[2:4, :] = [["c", 3], ["c", 3]]
    except TypeError as e:
        print("TypeError:", str(e))

Setting values by assigning categorical data will also check that the ``categories`` match:

.. ipython:: python

    df.loc["j":"k", "cats"] = pd.Categorical(["a", "a"], categories=["a", "b"])
    df
    try:
        df.loc["j":"k", "cats"] = pd.Categorical(["b", "b"], categories=["a", "b", "c"])
    except TypeError as e:
        print("TypeError:", str(e))

Assigning a ``Categorical`` to parts of a column of other types will use the values:

.. ipython:: python

    df = pd.DataFrame({"a": [1, 1, 1, 1, 1], "b": ["a", "a", "a", "a", "a"]})
    df.loc[1:2, "a"] = pd.Categorical(["b", "b"], categories=["a", "b"])
    df.loc[2:3, "b"] = pd.Categorical(["b", "b"], categories=["a", "b"])
    df
    df.dtypes

.. _categorical.merge:
.. _categorical.concat:

Merging / concatenation
~~~~~~~~~~~~~~~~~~~~~~~

By default, combining ``Series`` or ``DataFrames`` which contain the same
categories results in ``category`` dtype, otherwise results will depend on the
dtype of the underlying categories. Merges that result in non-categorical
dtypes will likely have higher memory usage. Use ``.astype`` or
``union_categoricals`` to ensure ``category`` results.

.. ipython:: python

   from pandas.api.types import union_categoricals

   # same categories
   s1 = pd.Series(["a", "b"], dtype="category")
   s2 = pd.Series(["a", "b", "a"], dtype="category")
   pd.concat([s1, s2])

   # different categories
   s3 = pd.Series(["b", "c"], dtype="category")
   pd.concat([s1, s3])

   # Output dtype is inferred based on categories values
   int_cats = pd.Series([1, 2], dtype="category")
   float_cats = pd.Series([3.0, 4.0], dtype="category")
   pd.concat([int_cats, float_cats])

   pd.concat([s1, s3]).astype("category")
   union_categoricals([s1.array, s3.array])

The following table summarizes the results of merging ``Categoricals``:

+-------------------+------------------------+----------------------+-----------------------------+
| arg1              | arg2                   |      identical       | result                      |
+===================+========================+======================+=============================+
| category          | category               | True                 | category                    |
+-------------------+------------------------+----------------------+-----------------------------+
| category (object) | category (object)      | False                | object (dtype is inferred)  |
+-------------------+------------------------+----------------------+-----------------------------+
| category (int)    | category (float)       | False                | float (dtype is inferred)   |
+-------------------+------------------------+----------------------+-----------------------------+

See also the section on :ref:`merge dtypes<merging.dtypes>` for notes about
preserving merge dtypes and performance.

.. _categorical.union:

Unioning
~~~~~~~~

If you want to combine categoricals that do not necessarily have the same
categories, the :func:`~pandas.api.types.union_categoricals` function will
combine a list-like of categoricals. The new categories will be the union of
the categories being combined.

.. ipython:: python

    from pandas.api.types import union_categoricals

    a = pd.Categorical(["b", "c"])
    b = pd.Categorical(["a", "b"])
    union_categoricals([a, b])

By default, the resulting categories will be ordered as
they appear in the data. If you want the categories to
be lexsorted, use ``sort_categories=True`` argument.

.. ipython:: python

    union_categoricals([a, b], sort_categories=True)

``union_categoricals`` also works with the "easy" case of combining two
categoricals of the same categories and order information
(e.g. what you could also ``append`` for).

.. ipython:: python

    a = pd.Categorical(["a", "b"], ordered=True)
    b = pd.Categorical(["a", "b", "a"], ordered=True)
    union_categoricals([a, b])

The below raises ``TypeError`` because the categories are ordered and not identical.

.. code-block:: ipython

   In [1]: a = pd.Categorical(["a", "b"], ordered=True)
   In [2]: b = pd.Categorical(["a", "b", "c"], ordered=True)
   In [3]: union_categoricals([a, b])
   Out[3]:
   TypeError: to union ordered Categoricals, all categories must be the same

Ordered categoricals with different categories or orderings can be combined by
using the ``ignore_ordered=True`` argument.

.. ipython:: python

    a = pd.Categorical(["a", "b", "c"], ordered=True)
    b = pd.Categorical(["c", "b", "a"], ordered=True)
    union_categoricals([a, b], ignore_order=True)

:func:`~pandas.api.types.union_categoricals` also works with a
``CategoricalIndex``, or ``Series`` containing categorical data, but note that
the resulting array will always be a plain ``Categorical``:

.. ipython:: python

    a = pd.Series(["b", "c"], dtype="category")
    b = pd.Series(["a", "b"], dtype="category")
    union_categoricals([a, b])

.. note::

   ``union_categoricals`` may recode the integer codes for categories
   when combining categoricals.  This is likely what you want,
   but if you are relying on the exact numbering of the categories, be
   aware.

   .. ipython:: python

      c1 = pd.Categorical(["b", "c"])
      c2 = pd.Categorical(["a", "b"])

      c1
      # "b" is coded to 0
      c1.codes

      c2
      # "b" is coded to 1
      c2.codes

      c = union_categoricals([c1, c2])
      c
      # "b" is coded to 0 throughout, same as c1, different from c2
      c.codes


Getting data in/out
-------------------

You can write data that contains ``category`` dtypes to a ``HDFStore``.
See :ref:`here <io.hdf5-categorical>` for an example and caveats.

It is also possible to write data to and reading data from *Stata* format files.
See :ref:`here <io.stata-categorical>` for an example and caveats.

Writing to a CSV file will convert the data, effectively removing any information about the
categorical (categories and ordering). So if you read back the CSV file you have to convert the
relevant columns back to ``category`` and assign the right categories and categories ordering.

.. ipython:: python
    :okwarning:

    import io

    s = pd.Series(pd.Categorical(["a", "b", "b", "a", "a", "d"]))
    # rename the categories
    s.cat.categories = ["very good", "good", "bad"]
    # reorder the categories and add missing categories
    s = s.cat.set_categories(["very bad", "bad", "medium", "good", "very good"])
    df = pd.DataFrame({"cats": s, "vals": [1, 2, 3, 4, 5, 6]})
    csv = io.StringIO()
    df.to_csv(csv)
    df2 = pd.read_csv(io.StringIO(csv.getvalue()))
    df2.dtypes
    df2["cats"]
    # Redo the category
    df2["cats"] = df2["cats"].astype("category")
    df2["cats"].cat.set_categories(
        ["very bad", "bad", "medium", "good", "very good"], inplace=True
    )
    df2.dtypes
    df2["cats"]

The same holds for writing to a SQL database with ``to_sql``.

Missing data
------------

pandas primarily uses the value ``np.nan`` to represent missing data. It is by
default not included in computations. See the :ref:`Missing Data section
<missing_data>`.

Missing values should **not** be included in the Categorical's ``categories``,
only in the ``values``.
Instead, it is understood that NaN is different, and is always a possibility.
When working with the Categorical's ``codes``, missing values will always have
a code of ``-1``.

.. ipython:: python

    s = pd.Series(["a", "b", np.nan, "a"], dtype="category")
    # only two categories
    s
    s.cat.codes


Methods for working with missing data, e.g. :meth:`~Series.isna`, :meth:`~Series.fillna`,
:meth:`~Series.dropna`, all work normally:

.. ipython:: python

    s = pd.Series(["a", "b", np.nan], dtype="category")
    s
    pd.isna(s)
    s.fillna("a")

Differences to R's ``factor``
-----------------------------

The following differences to R's factor functions can be observed:

* R's ``levels`` are named ``categories``.
* R's ``levels`` are always of type string, while ``categories`` in pandas can be of any dtype.
* It's not possible to specify labels at creation time. Use ``s.cat.rename_categories(new_labels)``
  afterwards.
* In contrast to R's ``factor`` function, using categorical data as the sole input to create a
  new categorical series will *not* remove unused categories but create a new categorical series
  which is equal to the passed in one!
* R allows for missing values to be included in its ``levels`` (pandas' ``categories``). pandas
  does not allow ``NaN`` categories, but missing values can still be in the ``values``.


Gotchas
-------

.. _categorical.rfactor:

Memory usage
~~~~~~~~~~~~

.. _categorical.memory:

The memory usage of a ``Categorical`` is proportional to the number of categories plus the length of the data. In contrast,
an ``object`` dtype is a constant times the length of the data.

.. ipython:: python

   s = pd.Series(["foo", "bar"] * 1000)

   # object dtype
   s.nbytes

   # category dtype
   s.astype("category").nbytes

.. note::

   If the number of categories approaches the length of the data, the ``Categorical`` will use nearly the same or
   more memory than an equivalent ``object`` dtype representation.

   .. ipython:: python

      s = pd.Series(["foo%04d" % i for i in range(2000)])

      # object dtype
      s.nbytes

      # category dtype
      s.astype("category").nbytes


``Categorical`` is not a ``numpy`` array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Currently, categorical data and the underlying ``Categorical`` is implemented as a Python
object and not as a low-level NumPy array dtype. This leads to some problems.

NumPy itself doesn't know about the new ``dtype``:

.. ipython:: python

    try:
        np.dtype("category")
    except TypeError as e:
        print("TypeError:", str(e))

    dtype = pd.Categorical(["a"]).dtype
    try:
        np.dtype(dtype)
    except TypeError as e:
        print("TypeError:", str(e))

Dtype comparisons work:

.. ipython:: python

    dtype == np.str_
    np.str_ == dtype

To check if a Series contains Categorical data, use ``hasattr(s, 'cat')``:

.. ipython:: python

    hasattr(pd.Series(["a"], dtype="category"), "cat")
    hasattr(pd.Series(["a"]), "cat")

Using NumPy functions on a ``Series`` of type ``category`` should not work as ``Categoricals``
are not numeric data (even in the case that ``.categories`` is numeric).

.. ipython:: python

    s = pd.Series(pd.Categorical([1, 2, 3, 4]))
    try:
        np.sum(s)
        # same with np.log(s),...
    except TypeError as e:
        print("TypeError:", str(e))

.. note::
    If such a function works, please file a bug at https://github.com/pandas-dev/pandas!

dtype in apply
~~~~~~~~~~~~~~

pandas currently does not preserve the dtype in apply functions: If you apply along rows you get
a ``Series`` of ``object`` ``dtype`` (same as getting a row -> getting one element will return a
basic type) and applying along columns will also convert to object. ``NaN`` values are unaffected.
You can use ``fillna`` to handle missing values before applying a function.

.. ipython:: python

    df = pd.DataFrame(
        {
            "a": [1, 2, 3, 4],
            "b": ["a", "b", "c", "d"],
            "cats": pd.Categorical([1, 2, 3, 2]),
        }
    )
    df.apply(lambda row: type(row["cats"]), axis=1)
    df.apply(lambda col: col.dtype, axis=0)

Categorical index
~~~~~~~~~~~~~~~~~

``CategoricalIndex`` is a type of index that is useful for supporting
indexing with duplicates. This is a container around a ``Categorical``
and allows efficient indexing and storage of an index with a large number of duplicated elements.
See the :ref:`advanced indexing docs <advanced.categoricalindex>` for a more detailed
explanation.

Setting the index will create a ``CategoricalIndex``:

.. ipython:: python

    cats = pd.Categorical([1, 2, 3, 4], categories=[4, 2, 3, 1])
    strings = ["a", "b", "c", "d"]
    values = [4, 2, 3, 1]
    df = pd.DataFrame({"strings": strings, "values": values}, index=cats)
    df.index
    # This now sorts by the categories order
    df.sort_index()

Side effects
~~~~~~~~~~~~

Constructing a ``Series`` from a ``Categorical`` will not copy the input
``Categorical``. This means that changes to the ``Series`` will in most cases
change the original ``Categorical``:

.. ipython:: python

    cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])
    s = pd.Series(cat, name="cat")
    cat
    s.iloc[0:2] = 10
    cat
    df = pd.DataFrame(s)
    df["cat"].cat.categories = [1, 2, 3, 4, 5]
    cat

Use ``copy=True`` to prevent such a behaviour or simply don't reuse ``Categoricals``:

.. ipython:: python

    cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])
    s = pd.Series(cat, name="cat", copy=True)
    cat
    s.iloc[0:2] = 10
    cat

.. note::

    This also happens in some cases when you supply a NumPy array instead of a ``Categorical``:
    using an int array (e.g. ``np.array([1,2,3,4])``) will exhibit the same behavior, while using
    a string array (e.g. ``np.array(["a","b","c","a"])``) will not.
.. _basics:

{{ header }}

==============================
 Essential basic functionality
==============================

Here we discuss a lot of the essential functionality common to the pandas data
structures. To begin, let's create some example objects like we did in
the :ref:`10 minutes to pandas <10min>` section:

.. ipython:: python

   index = pd.date_range("1/1/2000", periods=8)
   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=["A", "B", "C"])

.. _basics.head_tail:

Head and tail
-------------

To view a small sample of a Series or DataFrame object, use the
:meth:`~DataFrame.head` and :meth:`~DataFrame.tail` methods. The default number
of elements to display is five, but you may pass a custom number.

.. ipython:: python

   long_series = pd.Series(np.random.randn(1000))
   long_series.head()
   long_series.tail(3)

.. _basics.attrs:

Attributes and underlying data
------------------------------

pandas objects have a number of attributes enabling you to access the metadata

* **shape**: gives the axis dimensions of the object, consistent with ndarray
* Axis labels
    * **Series**: *index* (only axis)
    * **DataFrame**: *index* (rows) and *columns*

Note, **these attributes can be safely assigned to**!

.. ipython:: python

   df[:2]
   df.columns = [x.lower() for x in df.columns]
   df

pandas objects (:class:`Index`, :class:`Series`, :class:`DataFrame`) can be
thought of as containers for arrays, which hold the actual data and do the
actual computation. For many types, the underlying array is a
:class:`numpy.ndarray`. However, pandas and 3rd party libraries may *extend*
NumPy's type system to add support for custom arrays
(see :ref:`basics.dtypes`).

To get the actual data inside a :class:`Index` or :class:`Series`, use
the ``.array`` property

.. ipython:: python

   s.array
   s.index.array

:attr:`~Series.array` will always be an :class:`~pandas.api.extensions.ExtensionArray`.
The exact details of what an :class:`~pandas.api.extensions.ExtensionArray` is and why pandas uses them are a bit
beyond the scope of this introduction. See :ref:`basics.dtypes` for more.

If you know you need a NumPy array, use :meth:`~Series.to_numpy`
or :meth:`numpy.asarray`.

.. ipython:: python

   s.to_numpy()
   np.asarray(s)

When the Series or Index is backed by
an :class:`~pandas.api.extensions.ExtensionArray`, :meth:`~Series.to_numpy`
may involve copying data and coercing values. See :ref:`basics.dtypes` for more.

:meth:`~Series.to_numpy` gives some control over the ``dtype`` of the
resulting :class:`numpy.ndarray`. For example, consider datetimes with timezones.
NumPy doesn't have a dtype to represent timezone-aware datetimes, so there
are two possibly useful representations:

1. An object-dtype :class:`numpy.ndarray` with :class:`Timestamp` objects, each
   with the correct ``tz``
2. A ``datetime64[ns]`` -dtype :class:`numpy.ndarray`, where the values have
   been converted to UTC and the timezone discarded

Timezones may be preserved with ``dtype=object``

.. ipython:: python

   ser = pd.Series(pd.date_range("2000", periods=2, tz="CET"))
   ser.to_numpy(dtype=object)

Or thrown away with ``dtype='datetime64[ns]'``

.. ipython:: python

   ser.to_numpy(dtype="datetime64[ns]")

Getting the "raw data" inside a :class:`DataFrame` is possibly a bit more
complex. When your ``DataFrame`` only has a single data type for all the
columns, :meth:`DataFrame.to_numpy` will return the underlying data:

.. ipython:: python

   df.to_numpy()

If a DataFrame contains homogeneously-typed data, the ndarray can
actually be modified in-place, and the changes will be reflected in the data
structure. For heterogeneous data (e.g. some of the DataFrame's columns are not
all the same dtype), this will not be the case. The values attribute itself,
unlike the axis labels, cannot be assigned to.

.. note::

    When working with heterogeneous data, the dtype of the resulting ndarray
    will be chosen to accommodate all of the data involved. For example, if
    strings are involved, the result will be of object dtype. If there are only
    floats and integers, the resulting array will be of float dtype.

In the past, pandas recommended :attr:`Series.values` or :attr:`DataFrame.values`
for extracting the data from a Series or DataFrame. You'll still find references
to these in old code bases and online. Going forward, we recommend avoiding
``.values`` and using ``.array`` or ``.to_numpy()``. ``.values`` has the following
drawbacks:

1. When your Series contains an :ref:`extension type <extending.extension-types>`, it's
   unclear whether :attr:`Series.values` returns a NumPy array or the extension array.
   :attr:`Series.array` will always return an :class:`~pandas.api.extensions.ExtensionArray`, and will never
   copy data. :meth:`Series.to_numpy` will always return a NumPy array,
   potentially at the cost of copying / coercing values.
2. When your DataFrame contains a mixture of data types, :attr:`DataFrame.values` may
   involve copying data and coercing values to a common dtype, a relatively expensive
   operation. :meth:`DataFrame.to_numpy`, being a method, makes it clearer that the
   returned NumPy array may not be a view on the same data in the DataFrame.

.. _basics.accelerate:

Accelerated operations
----------------------

pandas has support for accelerating certain types of binary numerical and boolean operations using
the ``numexpr`` library and the ``bottleneck`` libraries.

These libraries are especially useful when dealing with large data sets, and provide large
speedups. ``numexpr`` uses smart chunking, caching, and multiple cores. ``bottleneck`` is
a set of specialized cython routines that are especially fast when dealing with arrays that have
``nans``.

Here is a sample (using 100 column x 100,000 row ``DataFrames``):

.. csv-table::
    :header: "Operation", "0.11.0 (ms)", "Prior Version (ms)", "Ratio to Prior"
    :widths: 25, 25, 25, 25
    :delim: ;

    ``df1 > df2``; 13.32; 125.35;  0.1063
    ``df1 * df2``; 21.71;  36.63;  0.5928
    ``df1 + df2``; 22.04;  36.50;  0.6039

You are highly encouraged to install both libraries. See the section
:ref:`Recommended Dependencies <install.recommended_dependencies>` for more installation info.

These are both enabled to be used by default, you can control this by setting the options:

.. code-block:: python

   pd.set_option("compute.use_bottleneck", False)
   pd.set_option("compute.use_numexpr", False)

.. _basics.binop:

Flexible binary operations
--------------------------

With binary operations between pandas data structures, there are two key points
of interest:

* Broadcasting behavior between higher- (e.g. DataFrame) and
  lower-dimensional (e.g. Series) objects.
* Missing data in computations.

We will demonstrate how to manage these issues independently, though they can
be handled simultaneously.

Matching / broadcasting behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

DataFrame has the methods :meth:`~DataFrame.add`, :meth:`~DataFrame.sub`,
:meth:`~DataFrame.mul`, :meth:`~DataFrame.div` and related functions
:meth:`~DataFrame.radd`, :meth:`~DataFrame.rsub`, ...
for carrying out binary operations. For broadcasting behavior,
Series input is of primary interest. Using these functions, you can use to
either match on the *index* or *columns* via the **axis** keyword:

.. ipython:: python

   df = pd.DataFrame(
       {
           "one": pd.Series(np.random.randn(3), index=["a", "b", "c"]),
           "two": pd.Series(np.random.randn(4), index=["a", "b", "c", "d"]),
           "three": pd.Series(np.random.randn(3), index=["b", "c", "d"]),
       }
   )
   df
   row = df.iloc[1]
   column = df["two"]

   df.sub(row, axis="columns")
   df.sub(row, axis=1)

   df.sub(column, axis="index")
   df.sub(column, axis=0)

.. ipython:: python
   :suppress:

   df_orig = df

Furthermore you can align a level of a MultiIndexed DataFrame with a Series.

.. ipython:: python

   dfmi = df.copy()
   dfmi.index = pd.MultiIndex.from_tuples(
       [(1, "a"), (1, "b"), (1, "c"), (2, "a")], names=["first", "second"]
   )
   dfmi.sub(column, axis=0, level="second")

Series and Index also support the :func:`divmod` builtin. This function takes
the floor division and modulo operation at the same time returning a two-tuple
of the same type as the left hand side. For example:

.. ipython:: python

   s = pd.Series(np.arange(10))
   s
   div, rem = divmod(s, 3)
   div
   rem

   idx = pd.Index(np.arange(10))
   idx
   div, rem = divmod(idx, 3)
   div
   rem

We can also do elementwise :func:`divmod`:

.. ipython:: python

   div, rem = divmod(s, [2, 2, 3, 3, 4, 4, 5, 5, 6, 6])
   div
   rem

Missing data / operations with fill values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In Series and DataFrame, the arithmetic functions have the option of inputting
a *fill_value*, namely a value to substitute when at most one of the values at
a location are missing. For example, when adding two DataFrame objects, you may
wish to treat NaN as 0 unless both DataFrames are missing that value, in which
case the result will be NaN (you can later replace NaN with some other value
using ``fillna`` if you wish).

.. ipython:: python
   :suppress:

   df2 = df.copy()
   df2["three"]["a"] = 1.0

.. ipython:: python

   df
   df2
   df + df2
   df.add(df2, fill_value=0)

.. _basics.compare:

Flexible comparisons
~~~~~~~~~~~~~~~~~~~~

Series and DataFrame have the binary comparison methods ``eq``, ``ne``, ``lt``, ``gt``,
``le``, and ``ge`` whose behavior is analogous to the binary
arithmetic operations described above:

.. ipython:: python

   df.gt(df2)
   df2.ne(df)

These operations produce a pandas object of the same type as the left-hand-side
input that is of dtype ``bool``. These ``boolean`` objects can be used in
indexing operations, see the section on :ref:`Boolean indexing<indexing.boolean>`.

.. _basics.reductions:

Boolean reductions
~~~~~~~~~~~~~~~~~~

You can apply the reductions: :attr:`~DataFrame.empty`, :meth:`~DataFrame.any`,
:meth:`~DataFrame.all`, and :meth:`~DataFrame.bool` to provide a
way to summarize a boolean result.

.. ipython:: python

   (df > 0).all()
   (df > 0).any()

You can reduce to a final boolean value.

.. ipython:: python

   (df > 0).any().any()

You can test if a pandas object is empty, via the :attr:`~DataFrame.empty` property.

.. ipython:: python

   df.empty
   pd.DataFrame(columns=list("ABC")).empty

To evaluate single-element pandas objects in a boolean context, use the method
:meth:`~DataFrame.bool`:

.. ipython:: python

   pd.Series([True]).bool()
   pd.Series([False]).bool()
   pd.DataFrame([[True]]).bool()
   pd.DataFrame([[False]]).bool()

.. warning::

   You might be tempted to do the following:

   .. code-block:: python

      >>> if df:
      ...     pass

   Or

   .. code-block:: python

      >>> df and df2

   These will both raise errors, as you are trying to compare multiple values.::

       ValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().

See :ref:`gotchas<gotchas.truth>` for a more detailed discussion.

.. _basics.equals:

Comparing if objects are equivalent
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Often you may find that there is more than one way to compute the same
result.  As a simple example, consider ``df + df`` and ``df * 2``. To test
that these two computations produce the same result, given the tools
shown above, you might imagine using ``(df + df == df * 2).all()``. But in
fact, this expression is False:

.. ipython:: python

   df + df == df * 2
   (df + df == df * 2).all()

Notice that the boolean DataFrame ``df + df == df * 2`` contains some False values!
This is because NaNs do not compare as equals:

.. ipython:: python

   np.nan == np.nan

So, NDFrames (such as Series and DataFrames)
have an :meth:`~DataFrame.equals` method for testing equality, with NaNs in
corresponding locations treated as equal.

.. ipython:: python

   (df + df).equals(df * 2)

Note that the Series or DataFrame index needs to be in the same order for
equality to be True:

.. ipython:: python

   df1 = pd.DataFrame({"col": ["foo", 0, np.nan]})
   df2 = pd.DataFrame({"col": [np.nan, 0, "foo"]}, index=[2, 1, 0])
   df1.equals(df2)
   df1.equals(df2.sort_index())

Comparing array-like objects
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can conveniently perform element-wise comparisons when comparing a pandas
data structure with a scalar value:

.. ipython:: python

   pd.Series(["foo", "bar", "baz"]) == "foo"
   pd.Index(["foo", "bar", "baz"]) == "foo"

pandas also handles element-wise comparisons between different array-like
objects of the same length:

.. ipython:: python

    pd.Series(["foo", "bar", "baz"]) == pd.Index(["foo", "bar", "qux"])
    pd.Series(["foo", "bar", "baz"]) == np.array(["foo", "bar", "qux"])

Trying to compare ``Index`` or ``Series`` objects of different lengths will
raise a ValueError:

.. code-block:: ipython

    In [55]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar'])
    ValueError: Series lengths must match to compare

    In [56]: pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo'])
    ValueError: Series lengths must match to compare

Note that this is different from the NumPy behavior where a comparison can
be broadcast:

.. ipython:: python

    np.array([1, 2, 3]) == np.array([2])

or it can return False if broadcasting can not be done:

.. ipython:: python
   :okwarning:

    np.array([1, 2, 3]) == np.array([1, 2])

Combining overlapping data sets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A problem occasionally arising is the combination of two similar data sets
where values in one are preferred over the other. An example would be two data
series representing a particular economic indicator where one is considered to
be of "higher quality". However, the lower quality series might extend further
back in history or have more complete data coverage. As such, we would like to
combine two DataFrame objects where missing values in one DataFrame are
conditionally filled with like-labeled values from the other DataFrame. The
function implementing this operation is :meth:`~DataFrame.combine_first`,
which we illustrate:

.. ipython:: python

   df1 = pd.DataFrame(
       {"A": [1.0, np.nan, 3.0, 5.0, np.nan], "B": [np.nan, 2.0, 3.0, np.nan, 6.0]}
   )
   df2 = pd.DataFrame(
       {
           "A": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],
           "B": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],
       }
   )
   df1
   df2
   df1.combine_first(df2)

General DataFrame combine
~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~DataFrame.combine_first` method above calls the more general
:meth:`DataFrame.combine`. This method takes another DataFrame
and a combiner function, aligns the input DataFrame and then passes the combiner
function pairs of Series (i.e., columns whose names are the same).

So, for instance, to reproduce :meth:`~DataFrame.combine_first` as above:

.. ipython:: python

   def combiner(x, y):
       return np.where(pd.isna(x), y, x)


   df1.combine(df2, combiner)

.. _basics.stats:

Descriptive statistics
----------------------

There exists a large number of methods for computing descriptive statistics and
other related operations on :ref:`Series <api.series.stats>`, :ref:`DataFrame
<api.dataframe.stats>`. Most of these
are aggregations (hence producing a lower-dimensional result) like
:meth:`~DataFrame.sum`, :meth:`~DataFrame.mean`, and :meth:`~DataFrame.quantile`,
but some of them, like :meth:`~DataFrame.cumsum` and :meth:`~DataFrame.cumprod`,
produce an object of the same size. Generally speaking, these methods take an
**axis** argument, just like *ndarray.{sum, std, ...}*, but the axis can be
specified by name or integer:

* **Series**: no axis argument needed
* **DataFrame**: "index" (axis=0, default), "columns" (axis=1)

For example:

.. ipython:: python

   df
   df.mean(0)
   df.mean(1)

All such methods have a ``skipna`` option signaling whether to exclude missing
data (``True`` by default):

.. ipython:: python

   df.sum(0, skipna=False)
   df.sum(axis=1, skipna=True)

Combined with the broadcasting / arithmetic behavior, one can describe various
statistical procedures, like standardization (rendering data zero mean and
standard deviation of 1), very concisely:

.. ipython:: python

   ts_stand = (df - df.mean()) / df.std()
   ts_stand.std()
   xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0)
   xs_stand.std(1)

Note that methods like :meth:`~DataFrame.cumsum` and :meth:`~DataFrame.cumprod`
preserve the location of ``NaN`` values. This is somewhat different from
:meth:`~DataFrame.expanding` and :meth:`~DataFrame.rolling` since ``NaN`` behavior
is furthermore dictated by a ``min_periods`` parameter.

.. ipython:: python

   df.cumsum()

Here is a quick reference summary table of common functions. Each also takes an
optional ``level`` parameter which applies only if the object has a
:ref:`hierarchical index<advanced.hierarchical>`.

.. csv-table::
    :header: "Function", "Description"
    :widths: 20, 80

    ``count``, Number of non-NA observations
    ``sum``, Sum of values
    ``mean``, Mean of values
    ``mad``, Mean absolute deviation
    ``median``, Arithmetic median of values
    ``min``, Minimum
    ``max``, Maximum
    ``mode``, Mode
    ``abs``, Absolute Value
    ``prod``, Product of values
    ``std``, Bessel-corrected sample standard deviation
    ``var``, Unbiased variance
    ``sem``, Standard error of the mean
    ``skew``, Sample skewness (3rd moment)
    ``kurt``, Sample kurtosis (4th moment)
    ``quantile``, Sample quantile (value at %)
    ``cumsum``, Cumulative sum
    ``cumprod``, Cumulative product
    ``cummax``, Cumulative maximum
    ``cummin``, Cumulative minimum

Note that by chance some NumPy methods, like ``mean``, ``std``, and ``sum``,
will exclude NAs on Series input by default:

.. ipython:: python

   np.mean(df["one"])
   np.mean(df["one"].to_numpy())

:meth:`Series.nunique` will return the number of unique non-NA values in a
Series:

.. ipython:: python

   series = pd.Series(np.random.randn(500))
   series[20:500] = np.nan
   series[10:20] = 5
   series.nunique()

.. _basics.describe:

Summarizing data: describe
~~~~~~~~~~~~~~~~~~~~~~~~~~

There is a convenient :meth:`~DataFrame.describe` function which computes a variety of summary
statistics about a Series or the columns of a DataFrame (excluding NAs of
course):

.. ipython:: python

    series = pd.Series(np.random.randn(1000))
    series[::2] = np.nan
    series.describe()
    frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d", "e"])
    frame.iloc[::2] = np.nan
    frame.describe()

You can select specific percentiles to include in the output:

.. ipython:: python

    series.describe(percentiles=[0.05, 0.25, 0.75, 0.95])

By default, the median is always included.

For a non-numerical Series object, :meth:`~Series.describe` will give a simple
summary of the number of unique values and most frequently occurring values:

.. ipython:: python

   s = pd.Series(["a", "a", "b", "b", "a", "a", np.nan, "c", "d", "a"])
   s.describe()

Note that on a mixed-type DataFrame object, :meth:`~DataFrame.describe` will
restrict the summary to include only numerical columns or, if none are, only
categorical columns:

.. ipython:: python

    frame = pd.DataFrame({"a": ["Yes", "Yes", "No", "No"], "b": range(4)})
    frame.describe()

This behavior can be controlled by providing a list of types as ``include``/``exclude``
arguments. The special value ``all`` can also be used:

.. ipython:: python

    frame.describe(include=["object"])
    frame.describe(include=["number"])
    frame.describe(include="all")

That feature relies on :ref:`select_dtypes <basics.selectdtypes>`. Refer to
there for details about accepted inputs.

.. _basics.idxmin:

Index of min/max values
~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~DataFrame.idxmin` and :meth:`~DataFrame.idxmax` functions on Series
and DataFrame compute the index labels with the minimum and maximum
corresponding values:

.. ipython:: python

   s1 = pd.Series(np.random.randn(5))
   s1
   s1.idxmin(), s1.idxmax()

   df1 = pd.DataFrame(np.random.randn(5, 3), columns=["A", "B", "C"])
   df1
   df1.idxmin(axis=0)
   df1.idxmax(axis=1)

When there are multiple rows (or columns) matching the minimum or maximum
value, :meth:`~DataFrame.idxmin` and :meth:`~DataFrame.idxmax` return the first
matching index:

.. ipython:: python

   df3 = pd.DataFrame([2, 1, 1, 3, np.nan], columns=["A"], index=list("edcba"))
   df3
   df3["A"].idxmin()

.. note::

   ``idxmin`` and ``idxmax`` are called ``argmin`` and ``argmax`` in NumPy.

.. _basics.discretization:

Value counts (histogramming) / mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~Series.value_counts` Series method and top-level function computes a histogram
of a 1D array of values. It can also be used as a function on regular arrays:

.. ipython:: python

   data = np.random.randint(0, 7, size=50)
   data
   s = pd.Series(data)
   s.value_counts()
   pd.value_counts(data)

.. versionadded:: 1.1.0

The :meth:`~DataFrame.value_counts` method can be used to count combinations across multiple columns.
By default all columns are used but a subset can be selected using the ``subset`` argument.

.. ipython:: python

    data = {"a": [1, 2, 3, 4], "b": ["x", "x", "y", "y"]}
    frame = pd.DataFrame(data)
    frame.value_counts()

Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:

.. ipython:: python

    s5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7])
    s5.mode()
    df5 = pd.DataFrame(
        {
            "A": np.random.randint(0, 7, size=50),
            "B": np.random.randint(-10, 15, size=50),
        }
    )
    df5.mode()


Discretization and quantiling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Continuous values can be discretized using the :func:`cut` (bins based on values)
and :func:`qcut` (bins based on sample quantiles) functions:

.. ipython:: python

   arr = np.random.randn(20)
   factor = pd.cut(arr, 4)
   factor

   factor = pd.cut(arr, [-5, -1, 0, 1, 5])
   factor

:func:`qcut` computes sample quantiles. For example, we could slice up some
normally distributed data into equal-size quartiles like so:

.. ipython:: python

   arr = np.random.randn(30)
   factor = pd.qcut(arr, [0, 0.25, 0.5, 0.75, 1])
   factor
   pd.value_counts(factor)

We can also pass infinite values to define the bins:

.. ipython:: python

   arr = np.random.randn(20)
   factor = pd.cut(arr, [-np.inf, 0, np.inf])
   factor

.. _basics.apply:

Function application
--------------------

To apply your own or another library's functions to pandas objects,
you should be aware of the three methods below. The appropriate
method to use depends on whether your function expects to operate
on an entire ``DataFrame`` or ``Series``, row- or column-wise, or elementwise.

1. `Tablewise Function Application`_: :meth:`~DataFrame.pipe`
2. `Row or Column-wise Function Application`_: :meth:`~DataFrame.apply`
3. `Aggregation API`_: :meth:`~DataFrame.agg` and :meth:`~DataFrame.transform`
4. `Applying Elementwise Functions`_: :meth:`~DataFrame.applymap`

.. _basics.pipe:

Tablewise function application
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``DataFrames`` and ``Series`` can be passed into functions.
However, if the function needs to be called in a chain, consider using the :meth:`~DataFrame.pipe` method.

First some setup:

.. ipython:: python

    def extract_city_name(df):
        """
        Chicago, IL -> Chicago for city_name column
        """
        df["city_name"] = df["city_and_code"].str.split(",").str.get(0)
        return df


    def add_country_name(df, country_name=None):
        """
        Chicago -> Chicago-US for city_name column
        """
        col = "city_name"
        df["city_and_country"] = df[col] + country_name
        return df


    df_p = pd.DataFrame({"city_and_code": ["Chicago, IL"]})


``extract_city_name`` and ``add_country_name`` are functions taking and returning ``DataFrames``.

Now compare the following:

.. ipython:: python

    add_country_name(extract_city_name(df_p), country_name="US")

Is equivalent to:

.. ipython:: python

    df_p.pipe(extract_city_name).pipe(add_country_name, country_name="US")

pandas encourages the second style, which is known as method chaining.
``pipe`` makes it easy to use your own or another library's functions
in method chains, alongside pandas' methods.

In the example above, the functions ``extract_city_name`` and ``add_country_name`` each expected a ``DataFrame`` as the first positional argument.
What if the function you wish to apply takes its data as, say, the second argument?
In this case, provide ``pipe`` with a tuple of ``(callable, data_keyword)``.
``.pipe`` will route the ``DataFrame`` to the argument specified in the tuple.

For example, we can fit a regression using statsmodels. Their API expects a formula first and a ``DataFrame`` as the second argument, ``data``. We pass in the function, keyword pair ``(sm.ols, 'data')`` to ``pipe``:

.. ipython:: python
   :okwarning:

   import statsmodels.formula.api as sm

   bb = pd.read_csv("data/baseball.csv", index_col="id")

   (
       bb.query("h > 0")
       .assign(ln_h=lambda df: np.log(df.h))
       .pipe((sm.ols, "data"), "hr ~ ln_h + year + g + C(lg)")
       .fit()
       .summary()
   )

The pipe method is inspired by unix pipes and more recently dplyr_ and magrittr_, which
have introduced the popular ``(%>%)`` (read pipe) operator for R_.
The implementation of ``pipe`` here is quite clean and feels right at home in Python.
We encourage you to view the source code of :meth:`~DataFrame.pipe`.

.. _dplyr: https://github.com/tidyverse/dplyr
.. _magrittr: https://github.com/tidyverse/magrittr
.. _R: https://www.r-project.org


Row or column-wise function application
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Arbitrary functions can be applied along the axes of a DataFrame
using the :meth:`~DataFrame.apply` method, which, like the descriptive
statistics methods, takes an optional ``axis`` argument:

.. ipython:: python

   df.apply(np.mean)
   df.apply(np.mean, axis=1)
   df.apply(lambda x: x.max() - x.min())
   df.apply(np.cumsum)
   df.apply(np.exp)

The :meth:`~DataFrame.apply` method will also dispatch on a string method name.

.. ipython:: python

   df.apply("mean")
   df.apply("mean", axis=1)

The return type of the function passed to :meth:`~DataFrame.apply` affects the
type of the final output from ``DataFrame.apply`` for the default behaviour:

* If the applied function returns a ``Series``, the final output is a ``DataFrame``.
  The columns match the index of the ``Series`` returned by the applied function.
* If the applied function returns any other type, the final output is a ``Series``.

This default behaviour can be overridden using the ``result_type``, which
accepts three options: ``reduce``, ``broadcast``, and ``expand``.
These will determine how list-likes return values expand (or not) to a ``DataFrame``.

:meth:`~DataFrame.apply` combined with some cleverness can be used to answer many questions
about a data set. For example, suppose we wanted to extract the date where the
maximum value for each column occurred:

.. ipython:: python

   tsdf = pd.DataFrame(
       np.random.randn(1000, 3),
       columns=["A", "B", "C"],
       index=pd.date_range("1/1/2000", periods=1000),
   )
   tsdf.apply(lambda x: x.idxmax())

You may also pass additional arguments and keyword arguments to the :meth:`~DataFrame.apply`
method. For instance, consider the following function you would like to apply:

.. code-block:: python

   def subtract_and_divide(x, sub, divide=1):
       return (x - sub) / divide

You may then apply this function as follows:

.. code-block:: python

   df.apply(subtract_and_divide, args=(5,), divide=3)

Another useful feature is the ability to pass Series methods to carry out some
Series operation on each column or row:

.. ipython:: python
   :suppress:

   tsdf = pd.DataFrame(
       np.random.randn(10, 3),
       columns=["A", "B", "C"],
       index=pd.date_range("1/1/2000", periods=10),
   )
   tsdf.iloc[3:7] = np.nan

.. ipython:: python

   tsdf
   tsdf.apply(pd.Series.interpolate)


Finally, :meth:`~DataFrame.apply` takes an argument ``raw`` which is False by default, which
converts each row or column into a Series before applying the function. When
set to True, the passed function will instead receive an ndarray object, which
has positive performance implications if you do not need the indexing
functionality.

.. _basics.aggregate:

Aggregation API
~~~~~~~~~~~~~~~

The aggregation API allows one to express possibly multiple aggregation operations in a single concise way.
This API is similar across pandas objects, see :ref:`groupby API <groupby.aggregate>`, the
:ref:`window API <window.overview>`, and the :ref:`resample API <timeseries.aggregate>`.
The entry point for aggregation is :meth:`DataFrame.aggregate`, or the alias
:meth:`DataFrame.agg`.

We will use a similar starting frame from above:

.. ipython:: python

   tsdf = pd.DataFrame(
       np.random.randn(10, 3),
       columns=["A", "B", "C"],
       index=pd.date_range("1/1/2000", periods=10),
   )
   tsdf.iloc[3:7] = np.nan
   tsdf

Using a single function is equivalent to :meth:`~DataFrame.apply`. You can also
pass named methods as strings. These will return a ``Series`` of the aggregated
output:

.. ipython:: python

   tsdf.agg(np.sum)

   tsdf.agg("sum")

   # these are equivalent to a ``.sum()`` because we are aggregating
   # on a single function
   tsdf.sum()

Single aggregations on a ``Series`` this will return a scalar value:

.. ipython:: python

   tsdf["A"].agg("sum")


Aggregating with multiple functions
+++++++++++++++++++++++++++++++++++

You can pass multiple aggregation arguments as a list.
The results of each of the passed functions will be a row in the resulting ``DataFrame``.
These are naturally named from the aggregation function.

.. ipython:: python

   tsdf.agg(["sum"])

Multiple functions yield multiple rows:

.. ipython:: python

   tsdf.agg(["sum", "mean"])

On a ``Series``, multiple functions return a ``Series``, indexed by the function names:

.. ipython:: python

   tsdf["A"].agg(["sum", "mean"])

Passing a ``lambda`` function will yield a ``<lambda>`` named row:

.. ipython:: python

   tsdf["A"].agg(["sum", lambda x: x.mean()])

Passing a named function will yield that name for the row:

.. ipython:: python

   def mymean(x):
       return x.mean()


   tsdf["A"].agg(["sum", mymean])

Aggregating with a dict
+++++++++++++++++++++++

Passing a dictionary of column names to a scalar or a list of scalars, to ``DataFrame.agg``
allows you to customize which functions are applied to which columns. Note that the results
are not in any particular order, you can use an ``OrderedDict`` instead to guarantee ordering.

.. ipython:: python

   tsdf.agg({"A": "mean", "B": "sum"})

Passing a list-like will generate a ``DataFrame`` output. You will get a matrix-like output
of all of the aggregators. The output will consist of all unique functions. Those that are
not noted for a particular column will be ``NaN``:

.. ipython:: python

   tsdf.agg({"A": ["mean", "min"], "B": "sum"})

.. _basics.aggregation.mixed_string:

Mixed dtypes
++++++++++++

.. deprecated:: 1.4.0
   Attempting to determine which columns cannot be aggregated and silently dropping them from the results is deprecated and will be removed in a future version. If any porition of the columns or operations provided fail, the call to ``.agg`` will raise.

When presented with mixed dtypes that cannot aggregate, ``.agg`` will only take the valid
aggregations. This is similar to how ``.groupby.agg`` works.

.. ipython:: python

   mdf = pd.DataFrame(
       {
           "A": [1, 2, 3],
           "B": [1.0, 2.0, 3.0],
           "C": ["foo", "bar", "baz"],
           "D": pd.date_range("20130101", periods=3),
       }
   )
   mdf.dtypes

.. ipython:: python
   :okwarning:

   mdf.agg(["min", "sum"])

.. _basics.aggregation.custom_describe:

Custom describe
+++++++++++++++

With ``.agg()`` it is possible to easily create a custom describe function, similar
to the built in :ref:`describe function <basics.describe>`.

.. ipython:: python

   from functools import partial

   q_25 = partial(pd.Series.quantile, q=0.25)
   q_25.__name__ = "25%"
   q_75 = partial(pd.Series.quantile, q=0.75)
   q_75.__name__ = "75%"

   tsdf.agg(["count", "mean", "std", "min", q_25, "median", q_75, "max"])

.. _basics.transform:

Transform API
~~~~~~~~~~~~~

The :meth:`~DataFrame.transform` method returns an object that is indexed the same (same size)
as the original. This API allows you to provide *multiple* operations at the same
time rather than one-by-one. Its API is quite similar to the ``.agg`` API.

We create a frame similar to the one used in the above sections.

.. ipython:: python

   tsdf = pd.DataFrame(
       np.random.randn(10, 3),
       columns=["A", "B", "C"],
       index=pd.date_range("1/1/2000", periods=10),
   )
   tsdf.iloc[3:7] = np.nan
   tsdf

Transform the entire frame. ``.transform()`` allows input functions as: a NumPy function, a string
function name or a user defined function.

.. ipython:: python
   :okwarning:

   tsdf.transform(np.abs)
   tsdf.transform("abs")
   tsdf.transform(lambda x: x.abs())

Here :meth:`~DataFrame.transform` received a single function; this is equivalent to a `ufunc
<https://numpy.org/doc/stable/reference/ufuncs.html>`__ application.

.. ipython:: python

   np.abs(tsdf)

Passing a single function to ``.transform()`` with a ``Series`` will yield a single ``Series`` in return.

.. ipython:: python

   tsdf["A"].transform(np.abs)


Transform with multiple functions
+++++++++++++++++++++++++++++++++

Passing multiple functions will yield a column MultiIndexed DataFrame.
The first level will be the original frame column names; the second level
will be the names of the transforming functions.

.. ipython:: python

   tsdf.transform([np.abs, lambda x: x + 1])

Passing multiple functions to a Series will yield a DataFrame. The
resulting column names will be the transforming functions.

.. ipython:: python

   tsdf["A"].transform([np.abs, lambda x: x + 1])


Transforming with a dict
++++++++++++++++++++++++


Passing a dict of functions will allow selective transforming per column.

.. ipython:: python

   tsdf.transform({"A": np.abs, "B": lambda x: x + 1})

Passing a dict of lists will generate a MultiIndexed DataFrame with these
selective transforms.

.. ipython:: python
   :okwarning:

   tsdf.transform({"A": np.abs, "B": [lambda x: x + 1, "sqrt"]})

.. _basics.elementwise:

Applying elementwise functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since not all functions can be vectorized (accept NumPy arrays and return
another array or value), the methods :meth:`~DataFrame.applymap` on DataFrame
and analogously :meth:`~Series.map` on Series accept any Python function taking
a single value and returning a single value. For example:

.. ipython:: python
   :suppress:

   df4 = df_orig.copy()

.. ipython:: python

   df4

   def f(x):
       return len(str(x))

   df4["one"].map(f)
   df4.applymap(f)

:meth:`Series.map` has an additional feature; it can be used to easily
"link" or "map" values defined by a secondary series. This is closely related
to :ref:`merging/joining functionality <merging>`:

.. ipython:: python

   s = pd.Series(
       ["six", "seven", "six", "seven", "six"], index=["a", "b", "c", "d", "e"]
   )
   t = pd.Series({"six": 6.0, "seven": 7.0})
   s
   s.map(t)


.. _basics.reindexing:

Reindexing and altering labels
------------------------------

:meth:`~Series.reindex` is the fundamental data alignment method in pandas.
It is used to implement nearly all other features relying on label-alignment
functionality. To *reindex* means to conform the data to match a given set of
labels along a particular axis. This accomplishes several things:

* Reorders the existing data to match a new set of labels
* Inserts missing value (NA) markers in label locations where no data for
  that label existed
* If specified, **fill** data for missing labels using logic (highly relevant
  to working with time series data)

Here is a simple example:

.. ipython:: python

   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   s
   s.reindex(["e", "b", "f", "d"])

Here, the ``f`` label was not contained in the Series and hence appears as
``NaN`` in the result.

With a DataFrame, you can simultaneously reindex the index and columns:

.. ipython:: python

   df
   df.reindex(index=["c", "f", "b"], columns=["three", "two", "one"])

You may also use ``reindex`` with an ``axis`` keyword:

.. ipython:: python

   df.reindex(["c", "f", "b"], axis="index")

Note that the ``Index`` objects containing the actual axis labels can be
**shared** between objects. So if we have a Series and a DataFrame, the
following can be done:

.. ipython:: python

   rs = s.reindex(df.index)
   rs
   rs.index is df.index

This means that the reindexed Series's index is the same Python object as the
DataFrame's index.

:meth:`DataFrame.reindex` also supports an "axis-style" calling convention,
where you specify a single ``labels`` argument and the ``axis`` it applies to.

.. ipython:: python

   df.reindex(["c", "f", "b"], axis="index")
   df.reindex(["three", "two", "one"], axis="columns")

.. seealso::

   :ref:`MultiIndex / Advanced Indexing <advanced>` is an even more concise way of
   doing reindexing.

.. note::

    When writing performance-sensitive code, there is a good reason to spend
    some time becoming a reindexing ninja: **many operations are faster on
    pre-aligned data**. Adding two unaligned DataFrames internally triggers a
    reindexing step. For exploratory analysis you will hardly notice the
    difference (because ``reindex`` has been heavily optimized), but when CPU
    cycles matter sprinkling a few explicit ``reindex`` calls here and there can
    have an impact.

.. _basics.reindex_like:

Reindexing to align with another object
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You may wish to take an object and reindex its axes to be labeled the same as
another object. While the syntax for this is straightforward albeit verbose, it
is a common enough operation that the :meth:`~DataFrame.reindex_like` method is
available to make this simpler:

.. ipython:: python
   :suppress:

   df2 = df.reindex(["a", "b", "c"], columns=["one", "two"])
   df3 = df2 - df2.mean()


.. ipython:: python

   df2
   df3
   df.reindex_like(df2)

.. _basics.align:

Aligning objects with each other with ``align``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~Series.align` method is the fastest way to simultaneously align two objects. It
supports a ``join`` argument (related to :ref:`joining and merging <merging>`):

  - ``join='outer'``: take the union of the indexes (default)
  - ``join='left'``: use the calling object's index
  - ``join='right'``: use the passed object's index
  - ``join='inner'``: intersect the indexes

It returns a tuple with both of the reindexed Series:

.. ipython:: python

   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   s1 = s[:4]
   s2 = s[1:]
   s1.align(s2)
   s1.align(s2, join="inner")
   s1.align(s2, join="left")

.. _basics.df_join:

For DataFrames, the join method will be applied to both the index and the
columns by default:

.. ipython:: python

   df.align(df2, join="inner")

You can also pass an ``axis`` option to only align on the specified axis:

.. ipython:: python

   df.align(df2, join="inner", axis=0)

.. _basics.align.frame.series:

If you pass a Series to :meth:`DataFrame.align`, you can choose to align both
objects either on the DataFrame's index or columns using the ``axis`` argument:

.. ipython:: python

   df.align(df2.iloc[0], axis=1)

.. _basics.reindex_fill:

Filling while reindexing
~~~~~~~~~~~~~~~~~~~~~~~~

:meth:`~Series.reindex` takes an optional parameter ``method`` which is a
filling method chosen from the following table:

.. csv-table::
    :header: "Method", "Action"
    :widths: 30, 50

    pad / ffill, Fill values forward
    bfill / backfill, Fill values backward
    nearest, Fill from the nearest index value

We illustrate these fill methods on a simple Series:

.. ipython:: python

   rng = pd.date_range("1/3/2000", periods=8)
   ts = pd.Series(np.random.randn(8), index=rng)
   ts2 = ts[[0, 3, 6]]
   ts
   ts2

   ts2.reindex(ts.index)
   ts2.reindex(ts.index, method="ffill")
   ts2.reindex(ts.index, method="bfill")
   ts2.reindex(ts.index, method="nearest")

These methods require that the indexes are **ordered** increasing or
decreasing.

Note that the same result could have been achieved using
:ref:`fillna <missing_data.fillna>` (except for ``method='nearest'``) or
:ref:`interpolate <missing_data.interpolate>`:

.. ipython:: python

   ts2.reindex(ts.index).fillna(method="ffill")

:meth:`~Series.reindex` will raise a ValueError if the index is not monotonically
increasing or decreasing. :meth:`~Series.fillna` and :meth:`~Series.interpolate`
will not perform any checks on the order of the index.

.. _basics.limits_on_reindex_fill:

Limits on filling while reindexing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``limit`` and ``tolerance`` arguments provide additional control over
filling while reindexing. Limit specifies the maximum count of consecutive
matches:

.. ipython:: python

   ts2.reindex(ts.index, method="ffill", limit=1)

In contrast, tolerance specifies the maximum distance between the index and
indexer values:

.. ipython:: python

   ts2.reindex(ts.index, method="ffill", tolerance="1 day")

Notice that when used on a ``DatetimeIndex``, ``TimedeltaIndex`` or
``PeriodIndex``, ``tolerance`` will coerced into a ``Timedelta`` if possible.
This allows you to specify tolerance with appropriate strings.

.. _basics.drop:

Dropping labels from an axis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A method closely related to ``reindex`` is the :meth:`~DataFrame.drop` function.
It removes a set of labels from an axis:

.. ipython:: python

   df
   df.drop(["a", "d"], axis=0)
   df.drop(["one"], axis=1)

Note that the following also works, but is a bit less obvious / clean:

.. ipython:: python

   df.reindex(df.index.difference(["a", "d"]))

.. _basics.rename:

Renaming / mapping labels
~~~~~~~~~~~~~~~~~~~~~~~~~

The :meth:`~DataFrame.rename` method allows you to relabel an axis based on some
mapping (a dict or Series) or an arbitrary function.

.. ipython:: python

   s
   s.rename(str.upper)

If you pass a function, it must return a value when called with any of the
labels (and must produce a set of unique values). A dict or
Series can also be used:

.. ipython:: python

   df.rename(
       columns={"one": "foo", "two": "bar"},
       index={"a": "apple", "b": "banana", "d": "durian"},
   )

If the mapping doesn't include a column/index label, it isn't renamed. Note that
extra labels in the mapping don't throw an error.

:meth:`DataFrame.rename` also supports an "axis-style" calling convention, where
you specify a single ``mapper`` and the ``axis`` to apply that mapping to.

.. ipython:: python

   df.rename({"one": "foo", "two": "bar"}, axis="columns")
   df.rename({"a": "apple", "b": "banana", "d": "durian"}, axis="index")


The :meth:`~DataFrame.rename` method also provides an ``inplace`` named
parameter that is by default ``False`` and copies the underlying data. Pass
``inplace=True`` to rename the data in place.

Finally, :meth:`~Series.rename` also accepts a scalar or list-like
for altering the ``Series.name`` attribute.

.. ipython:: python

   s.rename("scalar-name")

.. _basics.rename_axis:

The methods :meth:`DataFrame.rename_axis` and :meth:`Series.rename_axis`
allow specific names of a ``MultiIndex`` to be changed (as opposed to the
labels).

.. ipython:: python

   df = pd.DataFrame(
       {"x": [1, 2, 3, 4, 5, 6], "y": [10, 20, 30, 40, 50, 60]},
       index=pd.MultiIndex.from_product(
           [["a", "b", "c"], [1, 2]], names=["let", "num"]
       ),
   )
   df
   df.rename_axis(index={"let": "abc"})
   df.rename_axis(index=str.upper)

.. _basics.iteration:

Iteration
---------

The behavior of basic iteration over pandas objects depends on the type.
When iterating over a Series, it is regarded as array-like, and basic iteration
produces the values. DataFrames follow the dict-like convention of iterating
over the "keys" of the objects.

In short, basic iteration (``for i in object``) produces:

* **Series**: values
* **DataFrame**: column labels

Thus, for example, iterating over a DataFrame gives you the column names:

.. ipython:: python

   df = pd.DataFrame(
       {"col1": np.random.randn(3), "col2": np.random.randn(3)}, index=["a", "b", "c"]
   )

   for col in df:
       print(col)


pandas objects also have the dict-like :meth:`~DataFrame.items` method to
iterate over the (key, value) pairs.

To iterate over the rows of a DataFrame, you can use the following methods:

* :meth:`~DataFrame.iterrows`: Iterate over the rows of a DataFrame as (index, Series) pairs.
  This converts the rows to Series objects, which can change the dtypes and has some
  performance implications.
* :meth:`~DataFrame.itertuples`: Iterate over the rows of a DataFrame
  as namedtuples of the values.  This is a lot faster than
  :meth:`~DataFrame.iterrows`, and is in most cases preferable to use
  to iterate over the values of a DataFrame.

.. warning::

  Iterating through pandas objects is generally **slow**. In many cases,
  iterating manually over the rows is not needed and can be avoided with
  one of the following approaches:

  * Look for a *vectorized* solution: many operations can be performed using
    built-in methods or NumPy functions, (boolean) indexing, ...

  * When you have a function that cannot work on the full DataFrame/Series
    at once, it is better to use :meth:`~DataFrame.apply` instead of iterating
    over the values. See the docs on :ref:`function application <basics.apply>`.

  * If you need to do iterative manipulations on the values but performance is
    important, consider writing the inner loop with cython or numba.
    See the :ref:`enhancing performance <enhancingperf>` section for some
    examples of this approach.

.. warning::

  You should **never modify** something you are iterating over.
  This is not guaranteed to work in all cases. Depending on the
  data types, the iterator returns a copy and not a view, and writing
  to it will have no effect!

  For example, in the following case setting the value has no effect:

  .. ipython:: python

    df = pd.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})

    for index, row in df.iterrows():
        row["a"] = 10

    df

items
~~~~~

Consistent with the dict-like interface, :meth:`~DataFrame.items` iterates
through key-value pairs:

* **Series**: (index, scalar value) pairs
* **DataFrame**: (column, Series) pairs

For example:

.. ipython:: python

   for label, ser in df.items():
       print(label)
       print(ser)

.. _basics.iterrows:

iterrows
~~~~~~~~

:meth:`~DataFrame.iterrows` allows you to iterate through the rows of a
DataFrame as Series objects. It returns an iterator yielding each
index value along with a Series containing the data in each row:

.. ipython:: python

   for row_index, row in df.iterrows():
       print(row_index, row, sep="\n")

.. note::

   Because :meth:`~DataFrame.iterrows` returns a Series for each row,
   it does **not** preserve dtypes across the rows (dtypes are
   preserved across columns for DataFrames). For example,

   .. ipython:: python

      df_orig = pd.DataFrame([[1, 1.5]], columns=["int", "float"])
      df_orig.dtypes
      row = next(df_orig.iterrows())[1]
      row

   All values in ``row``, returned as a Series, are now upcasted
   to floats, also the original integer value in column ``x``:

   .. ipython:: python

      row["int"].dtype
      df_orig["int"].dtype

   To preserve dtypes while iterating over the rows, it is better
   to use :meth:`~DataFrame.itertuples` which returns namedtuples of the values
   and which is generally much faster than :meth:`~DataFrame.iterrows`.

For instance, a contrived way to transpose the DataFrame would be:

.. ipython:: python

   df2 = pd.DataFrame({"x": [1, 2, 3], "y": [4, 5, 6]})
   print(df2)
   print(df2.T)

   df2_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()})
   print(df2_t)

itertuples
~~~~~~~~~~

The :meth:`~DataFrame.itertuples` method will return an iterator
yielding a namedtuple for each row in the DataFrame. The first element
of the tuple will be the row's corresponding index value, while the
remaining values are the row values.

For instance:

.. ipython:: python

   for row in df.itertuples():
       print(row)

This method does not convert the row to a Series object; it merely
returns the values inside a namedtuple. Therefore,
:meth:`~DataFrame.itertuples` preserves the data type of the values
and is generally faster as :meth:`~DataFrame.iterrows`.

.. note::

   The column names will be renamed to positional names if they are
   invalid Python identifiers, repeated, or start with an underscore.
   With a large number of columns (>255), regular tuples are returned.

.. _basics.dt_accessors:

.dt accessor
------------

``Series`` has an accessor to succinctly return datetime like properties for the
*values* of the Series, if it is a datetime/period like Series.
This will return a Series, indexed like the existing Series.

.. ipython:: python

   # datetime
   s = pd.Series(pd.date_range("20130101 09:10:12", periods=4))
   s
   s.dt.hour
   s.dt.second
   s.dt.day

This enables nice expressions like this:

.. ipython:: python

   s[s.dt.day == 2]

You can easily produces tz aware transformations:

.. ipython:: python

   stz = s.dt.tz_localize("US/Eastern")
   stz
   stz.dt.tz

You can also chain these types of operations:

.. ipython:: python

   s.dt.tz_localize("UTC").dt.tz_convert("US/Eastern")

You can also format datetime values as strings with :meth:`Series.dt.strftime` which
supports the same format as the standard :meth:`~datetime.datetime.strftime`.

.. ipython:: python

   # DatetimeIndex
   s = pd.Series(pd.date_range("20130101", periods=4))
   s
   s.dt.strftime("%Y/%m/%d")

.. ipython:: python

   # PeriodIndex
   s = pd.Series(pd.period_range("20130101", periods=4))
   s
   s.dt.strftime("%Y/%m/%d")

The ``.dt`` accessor works for period and timedelta dtypes.

.. ipython:: python

   # period
   s = pd.Series(pd.period_range("20130101", periods=4, freq="D"))
   s
   s.dt.year
   s.dt.day

.. ipython:: python

   # timedelta
   s = pd.Series(pd.timedelta_range("1 day 00:00:05", periods=4, freq="s"))
   s
   s.dt.days
   s.dt.seconds
   s.dt.components

.. note::

   ``Series.dt`` will raise a ``TypeError`` if you access with a non-datetime-like values.

Vectorized string methods
-------------------------

Series is equipped with a set of string processing methods that make it easy to
operate on each element of the array. Perhaps most importantly, these methods
exclude missing/NA values automatically. These are accessed via the Series's
``str`` attribute and generally have names matching the equivalent (scalar)
built-in string methods. For example:

 .. ipython:: python

  s = pd.Series(
      ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string"
  )
  s.str.lower()

Powerful pattern-matching methods are provided as well, but note that
pattern-matching generally uses `regular expressions
<https://docs.python.org/3/library/re.html>`__ by default (and in some cases
always uses them).

.. note::

   Prior to pandas 1.0, string methods were only available on ``object`` -dtype
   ``Series``. pandas 1.0 added the :class:`StringDtype` which is dedicated
   to strings. See :ref:`text.types` for more.

Please see :ref:`Vectorized String Methods <text.string_methods>` for a complete
description.

.. _basics.sorting:

Sorting
-------

pandas supports three kinds of sorting: sorting by index labels,
sorting by column values, and sorting by a combination of both.

.. _basics.sort_index:

By index
~~~~~~~~

The :meth:`Series.sort_index` and :meth:`DataFrame.sort_index` methods are
used to sort a pandas object by its index levels.

.. ipython:: python

   df = pd.DataFrame(
       {
           "one": pd.Series(np.random.randn(3), index=["a", "b", "c"]),
           "two": pd.Series(np.random.randn(4), index=["a", "b", "c", "d"]),
           "three": pd.Series(np.random.randn(3), index=["b", "c", "d"]),
       }
   )

   unsorted_df = df.reindex(
       index=["a", "d", "c", "b"], columns=["three", "two", "one"]
   )
   unsorted_df

   # DataFrame
   unsorted_df.sort_index()
   unsorted_df.sort_index(ascending=False)
   unsorted_df.sort_index(axis=1)

   # Series
   unsorted_df["three"].sort_index()

.. _basics.sort_index_key:

.. versionadded:: 1.1.0

Sorting by index also supports a ``key`` parameter that takes a callable
function to apply to the index being sorted. For ``MultiIndex`` objects,
the key is applied per-level to the levels specified by ``level``.

.. ipython:: python

   s1 = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3], "c": [2, 3, 4]}).set_index(
       list("ab")
   )
   s1

.. ipython:: python

   s1.sort_index(level="a")
   s1.sort_index(level="a", key=lambda idx: idx.str.lower())

For information on key sorting by value, see :ref:`value sorting
<basics.sort_value_key>`.

.. _basics.sort_values:

By values
~~~~~~~~~

The :meth:`Series.sort_values` method is used to sort a ``Series`` by its values. The
:meth:`DataFrame.sort_values` method is used to sort a ``DataFrame`` by its column or row values.
The optional ``by`` parameter to :meth:`DataFrame.sort_values` may used to specify one or more columns
to use to determine the sorted order.

.. ipython:: python

   df1 = pd.DataFrame(
       {"one": [2, 1, 1, 1], "two": [1, 3, 2, 4], "three": [5, 4, 3, 2]}
   )
   df1.sort_values(by="two")

The ``by`` parameter can take a list of column names, e.g.:

.. ipython:: python

   df1[["one", "two", "three"]].sort_values(by=["one", "two"])

These methods have special treatment of NA values via the ``na_position``
argument:

.. ipython:: python

   s[2] = np.nan
   s.sort_values()
   s.sort_values(na_position="first")

.. _basics.sort_value_key:

.. versionadded:: 1.1.0

Sorting also supports a ``key`` parameter that takes a callable function
to apply to the values being sorted.

.. ipython:: python

   s1 = pd.Series(["B", "a", "C"])

.. ipython:: python

   s1.sort_values()
   s1.sort_values(key=lambda x: x.str.lower())

``key`` will be given the :class:`Series` of values and should return a ``Series``
or array of the same shape with the transformed values. For ``DataFrame`` objects,
the key is applied per column, so the key should still expect a Series and return
a Series, e.g.

.. ipython:: python

   df = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3]})

.. ipython:: python

   df.sort_values(by="a")
   df.sort_values(by="a", key=lambda col: col.str.lower())

The name or type of each column can be used to apply different functions to
different columns.

.. _basics.sort_indexes_and_values:

By indexes and values
~~~~~~~~~~~~~~~~~~~~~

Strings passed as the ``by`` parameter to :meth:`DataFrame.sort_values` may
refer to either columns or index level names.

.. ipython:: python

   # Build MultiIndex
   idx = pd.MultiIndex.from_tuples(
       [("a", 1), ("a", 2), ("a", 2), ("b", 2), ("b", 1), ("b", 1)]
   )
   idx.names = ["first", "second"]

   # Build DataFrame
   df_multi = pd.DataFrame({"A": np.arange(6, 0, -1)}, index=idx)
   df_multi

Sort by 'second' (index) and 'A' (column)

.. ipython:: python

   df_multi.sort_values(by=["second", "A"])

.. note::

   If a string matches both a column name and an index level name then a
   warning is issued and the column takes precedence. This will result in an
   ambiguity error in a future version.

.. _basics.searchsorted:

searchsorted
~~~~~~~~~~~~

Series has the :meth:`~Series.searchsorted` method, which works similarly to
:meth:`numpy.ndarray.searchsorted`.

.. ipython:: python

   ser = pd.Series([1, 2, 3])
   ser.searchsorted([0, 3])
   ser.searchsorted([0, 4])
   ser.searchsorted([1, 3], side="right")
   ser.searchsorted([1, 3], side="left")
   ser = pd.Series([3, 1, 2])
   ser.searchsorted([0, 3], sorter=np.argsort(ser))

.. _basics.nsorted:

smallest / largest values
~~~~~~~~~~~~~~~~~~~~~~~~~

``Series`` has the :meth:`~Series.nsmallest` and :meth:`~Series.nlargest` methods which return the
smallest or largest :math:`n` values. For a large ``Series`` this can be much
faster than sorting the entire Series and calling ``head(n)`` on the result.

.. ipython:: python

   s = pd.Series(np.random.permutation(10))
   s
   s.sort_values()
   s.nsmallest(3)
   s.nlargest(3)

``DataFrame`` also has the ``nlargest`` and ``nsmallest`` methods.

.. ipython:: python

   df = pd.DataFrame(
       {
           "a": [-2, -1, 1, 10, 8, 11, -1],
           "b": list("abdceff"),
           "c": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],
       }
   )
   df.nlargest(3, "a")
   df.nlargest(5, ["a", "c"])
   df.nsmallest(3, "a")
   df.nsmallest(5, ["a", "c"])


.. _basics.multiindex_sorting:

Sorting by a MultiIndex column
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You must be explicit about sorting when the column is a MultiIndex, and fully specify
all levels to ``by``.

.. ipython:: python

   df1.columns = pd.MultiIndex.from_tuples(
       [("a", "one"), ("a", "two"), ("b", "three")]
   )
   df1.sort_values(by=("a", "two"))


Copying
-------

The :meth:`~DataFrame.copy` method on pandas objects copies the underlying data (though not
the axis indexes, since they are immutable) and returns a new object. Note that
**it is seldom necessary to copy objects**. For example, there are only a
handful of ways to alter a DataFrame *in-place*:

* Inserting, deleting, or modifying a column.
* Assigning to the ``index`` or ``columns`` attributes.
* For homogeneous data, directly modifying the values via the ``values``
  attribute or advanced indexing.

To be clear, no pandas method has the side effect of modifying your data;
almost every method returns a new object, leaving the original object
untouched. If the data is modified, it is because you did so explicitly.

.. _basics.dtypes:

dtypes
------

For the most part, pandas uses NumPy arrays and dtypes for Series or individual
columns of a DataFrame. NumPy provides support for ``float``,
``int``, ``bool``, ``timedelta64[ns]`` and ``datetime64[ns]`` (note that NumPy
does not support timezone-aware datetimes).

pandas and third-party libraries *extend* NumPy's type system in a few places.
This section describes the extensions pandas has made internally.
See :ref:`extending.extension-types` for how to write your own extension that
works with pandas. See :ref:`ecosystem.extensions` for a list of third-party
libraries that have implemented an extension.

The following table lists all of pandas extension types. For methods requiring ``dtype``
arguments, strings can be specified as indicated. See the respective
documentation sections for more on each type.

+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| Kind of Data                                    | Data Type                 | Scalar             | Array                         | String Aliases                         |
+=================================================+===============+===========+========+===========+===============================+========================================+
| :ref:`tz-aware datetime <timeseries.timezone>`  | :class:`DatetimeTZDtype`  | :class:`Timestamp` | :class:`arrays.DatetimeArray` | ``'datetime64[ns, <tz>]'``             |
|                                                 |                           |                    |                               |                                        |
+-------------------------------------------------+---------------+-----------+--------------------+-------------------------------+----------------------------------------+
| :ref:`Categorical <categorical>`                | :class:`CategoricalDtype` | (none)             | :class:`Categorical`          | ``'category'``                         |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`period (time spans) <timeseries.periods>` | :class:`PeriodDtype`      | :class:`Period`    | :class:`arrays.PeriodArray`   | ``'period[<freq>]'``,                  |
|                                                 |                           |                    | ``'Period[<freq>]'``          |                                        |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`sparse <sparse>`                          | :class:`SparseDtype`      | (none)             | :class:`arrays.SparseArray`   | ``'Sparse'``, ``'Sparse[int]'``,       |
|                                                 |                           |                    |                               | ``'Sparse[float]'``                    |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`intervals <advanced.intervalindex>`       | :class:`IntervalDtype`    | :class:`Interval`  | :class:`arrays.IntervalArray` | ``'interval'``, ``'Interval'``,        |
|                                                 |                           |                    |                               | ``'Interval[<numpy_dtype>]'``,         |
|                                                 |                           |                    |                               | ``'Interval[datetime64[ns, <tz>]]'``,  |
|                                                 |                           |                    |                               | ``'Interval[timedelta64[<freq>]]'``    |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`nullable integer <integer_na>`            | :class:`Int64Dtype`, ...  | (none)             | :class:`arrays.IntegerArray`  | ``'Int8'``, ``'Int16'``, ``'Int32'``,  |
|                                                 |                           |                    |                               | ``'Int64'``, ``'UInt8'``, ``'UInt16'``,|
|                                                 |                           |                    |                               | ``'UInt32'``, ``'UInt64'``             |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`Strings <text>`                           | :class:`StringDtype`      | :class:`str`       | :class:`arrays.StringArray`   | ``'string'``                           |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+
| :ref:`Boolean (with NA) <api.arrays.bool>`      | :class:`BooleanDtype`     | :class:`bool`      | :class:`arrays.BooleanArray`  | ``'boolean'``                          |
+-------------------------------------------------+---------------------------+--------------------+-------------------------------+----------------------------------------+

pandas has two ways to store strings.

1. ``object`` dtype, which can hold any Python object, including strings.
2. :class:`StringDtype`, which is dedicated to strings.

Generally, we recommend using :class:`StringDtype`. See :ref:`text.types` for more.

Finally, arbitrary objects may be stored using the ``object`` dtype, but should
be avoided to the extent possible (for performance and interoperability with
other libraries and methods. See :ref:`basics.object_conversion`).

A convenient :attr:`~DataFrame.dtypes` attribute for DataFrame returns a Series
with the data type of each column.

.. ipython:: python

   dft = pd.DataFrame(
       {
           "A": np.random.rand(3),
           "B": 1,
           "C": "foo",
           "D": pd.Timestamp("20010102"),
           "E": pd.Series([1.0] * 3).astype("float32"),
           "F": False,
           "G": pd.Series([1] * 3, dtype="int8"),
       }
   )
   dft
   dft.dtypes

On a ``Series`` object, use the :attr:`~Series.dtype` attribute.

.. ipython:: python

   dft["A"].dtype

If a pandas object contains data with multiple dtypes *in a single column*, the
dtype of the column will be chosen to accommodate all of the data types
(``object`` is the most general).

.. ipython:: python

   # these ints are coerced to floats
   pd.Series([1, 2, 3, 4, 5, 6.0])

   # string data forces an ``object`` dtype
   pd.Series([1, 2, 3, 6.0, "foo"])

The number of columns of each type in a ``DataFrame`` can be found by calling
``DataFrame.dtypes.value_counts()``.

.. ipython:: python

   dft.dtypes.value_counts()

Numeric dtypes will propagate and can coexist in DataFrames.
If a dtype is passed (either directly via the ``dtype`` keyword, a passed ``ndarray``,
or a passed ``Series``), then it will be preserved in DataFrame operations. Furthermore,
different numeric dtypes will **NOT** be combined. The following example will give you a taste.

.. ipython:: python

   df1 = pd.DataFrame(np.random.randn(8, 1), columns=["A"], dtype="float32")
   df1
   df1.dtypes
   df2 = pd.DataFrame(
       {
           "A": pd.Series(np.random.randn(8), dtype="float16"),
           "B": pd.Series(np.random.randn(8)),
           "C": pd.Series(np.array(np.random.randn(8), dtype="uint8")),
       }
   )
   df2
   df2.dtypes

defaults
~~~~~~~~

By default integer types are ``int64`` and float types are ``float64``,
*regardless* of platform (32-bit or 64-bit).
The following will all result in ``int64`` dtypes.

.. ipython:: python

   pd.DataFrame([1, 2], columns=["a"]).dtypes
   pd.DataFrame({"a": [1, 2]}).dtypes
   pd.DataFrame({"a": 1}, index=list(range(2))).dtypes

Note that Numpy will choose *platform-dependent* types when creating arrays.
The following **WILL** result in ``int32`` on 32-bit platform.

.. ipython:: python

   frame = pd.DataFrame(np.array([1, 2]))


upcasting
~~~~~~~~~

Types can potentially be *upcasted* when combined with other types, meaning they are promoted
from the current type (e.g. ``int`` to ``float``).

.. ipython:: python

   df3 = df1.reindex_like(df2).fillna(value=0.0) + df2
   df3
   df3.dtypes

:meth:`DataFrame.to_numpy` will return the *lower-common-denominator* of the dtypes, meaning
the dtype that can accommodate **ALL** of the types in the resulting homogeneous dtyped NumPy array. This can
force some *upcasting*.

.. ipython:: python

   df3.to_numpy().dtype

astype
~~~~~~

.. _basics.cast:

You can use the :meth:`~DataFrame.astype` method to explicitly convert dtypes from one to another. These will by default return a copy,
even if the dtype was unchanged (pass ``copy=False`` to change this behavior). In addition, they will raise an
exception if the astype operation is invalid.

Upcasting is always according to the **NumPy** rules. If two different dtypes are involved in an operation,
then the more *general* one will be used as the result of the operation.

.. ipython:: python

   df3
   df3.dtypes

   # conversion of dtypes
   df3.astype("float32").dtypes


Convert a subset of columns to a specified type using :meth:`~DataFrame.astype`.

.. ipython:: python

   dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
   dft[["a", "b"]] = dft[["a", "b"]].astype(np.uint8)
   dft
   dft.dtypes

Convert certain columns to a specific dtype by passing a dict to :meth:`~DataFrame.astype`.

.. ipython:: python

   dft1 = pd.DataFrame({"a": [1, 0, 1], "b": [4, 5, 6], "c": [7, 8, 9]})
   dft1 = dft1.astype({"a": np.bool_, "c": np.float64})
   dft1
   dft1.dtypes

.. note::

    When trying to convert a subset of columns to a specified type using :meth:`~DataFrame.astype`  and :meth:`~DataFrame.loc`, upcasting occurs.

    :meth:`~DataFrame.loc` tries to fit in what we are assigning to the current dtypes, while ``[]`` will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result.

    .. ipython:: python

       dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
       dft.loc[:, ["a", "b"]].astype(np.uint8).dtypes
       dft.loc[:, ["a", "b"]] = dft.loc[:, ["a", "b"]].astype(np.uint8)
       dft.dtypes

.. _basics.object_conversion:

object conversion
~~~~~~~~~~~~~~~~~

pandas offers various functions to try to force conversion of types from the ``object`` dtype to other types.
In cases where the data is already of the correct type, but stored in an ``object`` array, the
:meth:`DataFrame.infer_objects` and :meth:`Series.infer_objects` methods can be used to soft convert
to the correct type.

  .. ipython:: python

     import datetime

     df = pd.DataFrame(
         [
             [1, 2],
             ["a", "b"],
             [datetime.datetime(2016, 3, 2), datetime.datetime(2016, 3, 2)],
         ]
     )
     df = df.T
     df
     df.dtypes

Because the data was transposed the original inference stored all columns as object, which
``infer_objects`` will correct.

  .. ipython:: python

     df.infer_objects().dtypes

The following functions are available for one dimensional object arrays or scalars to perform
hard conversion of objects to a specified type:

* :meth:`~pandas.to_numeric` (conversion to numeric dtypes)

  .. ipython:: python

     m = ["1.1", 2, 3]
     pd.to_numeric(m)

* :meth:`~pandas.to_datetime` (conversion to datetime objects)

  .. ipython:: python

     import datetime

     m = ["2016-07-09", datetime.datetime(2016, 3, 2)]
     pd.to_datetime(m)

* :meth:`~pandas.to_timedelta` (conversion to timedelta objects)

  .. ipython:: python

     m = ["5us", pd.Timedelta("1day")]
     pd.to_timedelta(m)

To force a conversion, we can pass in an ``errors`` argument, which specifies how pandas should deal with elements
that cannot be converted to desired dtype or object. By default, ``errors='raise'``, meaning that any errors encountered
will be raised during the conversion process. However, if ``errors='coerce'``, these errors will be ignored and pandas
will convert problematic elements to ``pd.NaT`` (for datetime and timedelta) or ``np.nan`` (for numeric). This might be
useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has
non-conforming elements intermixed that you want to represent as missing:

.. ipython:: python

    import datetime

    m = ["apple", datetime.datetime(2016, 3, 2)]
    pd.to_datetime(m, errors="coerce")

    m = ["apple", 2, 3]
    pd.to_numeric(m, errors="coerce")

    m = ["apple", pd.Timedelta("1day")]
    pd.to_timedelta(m, errors="coerce")

The ``errors`` parameter has a third option of ``errors='ignore'``, which will simply return the passed in data if it
encounters any errors with the conversion to a desired data type:

.. ipython:: python

    import datetime

    m = ["apple", datetime.datetime(2016, 3, 2)]
    pd.to_datetime(m, errors="ignore")

    m = ["apple", 2, 3]
    pd.to_numeric(m, errors="ignore")

    m = ["apple", pd.Timedelta("1day")]
    pd.to_timedelta(m, errors="ignore")

In addition to object conversion, :meth:`~pandas.to_numeric` provides another argument ``downcast``, which gives the
option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:

.. ipython:: python

    m = ["1", 2, 3]
    pd.to_numeric(m, downcast="integer")  # smallest signed int dtype
    pd.to_numeric(m, downcast="signed")  # same as 'integer'
    pd.to_numeric(m, downcast="unsigned")  # smallest unsigned int dtype
    pd.to_numeric(m, downcast="float")  # smallest float dtype

As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such
as DataFrames. However, with :meth:`~pandas.DataFrame.apply`, we can "apply" the function over each column efficiently:

.. ipython:: python

    import datetime

    df = pd.DataFrame([["2016-07-09", datetime.datetime(2016, 3, 2)]] * 2, dtype="O")
    df
    df.apply(pd.to_datetime)

    df = pd.DataFrame([["1.1", 2, 3]] * 2, dtype="O")
    df
    df.apply(pd.to_numeric)

    df = pd.DataFrame([["5us", pd.Timedelta("1day")]] * 2, dtype="O")
    df
    df.apply(pd.to_timedelta)

gotchas
~~~~~~~

Performing selection operations on ``integer`` type data can easily upcast the data to ``floating``.
The dtype of the input data will be preserved in cases where ``nans`` are not introduced.
See also :ref:`Support for integer NA <gotchas.intna>`.

.. ipython:: python

   dfi = df3.astype("int32")
   dfi["E"] = 1
   dfi
   dfi.dtypes

   casted = dfi[dfi > 0]
   casted
   casted.dtypes

While float dtypes are unchanged.

.. ipython:: python

   dfa = df3.copy()
   dfa["A"] = dfa["A"].astype("float32")
   dfa.dtypes

   casted = dfa[df2 > 0]
   casted
   casted.dtypes

Selecting columns based on ``dtype``
------------------------------------

.. _basics.selectdtypes:

The :meth:`~DataFrame.select_dtypes` method implements subsetting of columns
based on their ``dtype``.

First, let's create a :class:`DataFrame` with a slew of different
dtypes:

.. ipython:: python

   df = pd.DataFrame(
       {
           "string": list("abc"),
           "int64": list(range(1, 4)),
           "uint8": np.arange(3, 6).astype("u1"),
           "float64": np.arange(4.0, 7.0),
           "bool1": [True, False, True],
           "bool2": [False, True, False],
           "dates": pd.date_range("now", periods=3),
           "category": pd.Series(list("ABC")).astype("category"),
       }
   )
   df["tdeltas"] = df.dates.diff()
   df["uint64"] = np.arange(3, 6).astype("u8")
   df["other_dates"] = pd.date_range("20130101", periods=3)
   df["tz_aware_dates"] = pd.date_range("20130101", periods=3, tz="US/Eastern")
   df

And the dtypes:

.. ipython:: python

   df.dtypes

:meth:`~DataFrame.select_dtypes` has two parameters ``include`` and ``exclude`` that allow you to
say "give me the columns *with* these dtypes" (``include``) and/or "give the
columns *without* these dtypes" (``exclude``).

For example, to select ``bool`` columns:

.. ipython:: python

   df.select_dtypes(include=[bool])

You can also pass the name of a dtype in the `NumPy dtype hierarchy
<https://numpy.org/doc/stable/reference/arrays.scalars.html>`__:

.. ipython:: python

   df.select_dtypes(include=["bool"])

:meth:`~pandas.DataFrame.select_dtypes` also works with generic dtypes as well.

For example, to select all numeric and boolean columns while excluding unsigned
integers:

.. ipython:: python

   df.select_dtypes(include=["number", "bool"], exclude=["unsignedinteger"])

To select string columns you must use the ``object`` dtype:

.. ipython:: python

   df.select_dtypes(include=["object"])

To see all the child dtypes of a generic ``dtype`` like ``numpy.number`` you
can define a function that returns a tree of child dtypes:

.. ipython:: python

   def subdtypes(dtype):
       subs = dtype.__subclasses__()
       if not subs:
           return dtype
       return [dtype, [subdtypes(dt) for dt in subs]]

All NumPy dtypes are subclasses of ``numpy.generic``:

.. ipython:: python

    subdtypes(np.generic)

.. note::

    pandas also defines the types ``category``, and ``datetime64[ns, tz]``, which are not integrated into the normal
    NumPy hierarchy and won't show up with the above function.
.. _enhancingperf:

{{ header }}

*********************
Enhancing performance
*********************

In this part of the tutorial, we will investigate how to speed up certain
functions operating on pandas :class:`DataFrame` using three different techniques:
Cython, Numba and :func:`pandas.eval`. We will see a speed improvement of ~200
when we use Cython and Numba on a test function operating row-wise on the
:class:`DataFrame`. Using :func:`pandas.eval` we will speed up a sum by an order of
~2.

.. note::

   In addition to following the steps in this tutorial, users interested in enhancing
   performance are highly encouraged to install the
   :ref:`recommended dependencies<install.recommended_dependencies>` for pandas.
   These dependencies are often not installed by default, but will offer speed
   improvements if present.

.. _enhancingperf.cython:

Cython (writing C extensions for pandas)
----------------------------------------

For many use cases writing pandas in pure Python and NumPy is sufficient. In some
computationally heavy applications however, it can be possible to achieve sizable
speed-ups by offloading work to `cython <https://cython.org/>`__.

This tutorial assumes you have refactored as much as possible in Python, for example
by trying to remove for-loops and making use of NumPy vectorization. It's always worth
optimising in Python first.

This tutorial walks through a "typical" process of cythonizing a slow computation.
We use an `example from the Cython documentation <https://docs.cython.org/en/latest/src/quickstart/cythonize.html>`__
but in the context of pandas. Our final cythonized solution is around 100 times
faster than the pure Python solution.

.. _enhancingperf.pure:

Pure Python
~~~~~~~~~~~

We have a :class:`DataFrame` to which we want to apply a function row-wise.

.. ipython:: python

   df = pd.DataFrame(
       {
           "a": np.random.randn(1000),
           "b": np.random.randn(1000),
           "N": np.random.randint(100, 1000, (1000)),
           "x": "x",
       }
   )
   df

Here's the function in pure Python:

.. ipython:: python

   def f(x):
       return x * (x - 1)


   def integrate_f(a, b, N):
       s = 0
       dx = (b - a) / N
       for i in range(N):
           s += f(a + i * dx)
       return s * dx

We achieve our result by using :meth:`DataFrame.apply` (row-wise):

.. ipython:: python

   %timeit df.apply(lambda x: integrate_f(x["a"], x["b"], x["N"]), axis=1)

But clearly this isn't fast enough for us. Let's take a look and see where the
time is spent during this operation (limited to the most time consuming
four calls) using the `prun ipython magic function <https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-prun>`__:

.. ipython:: python

   %prun -l 4 df.apply(lambda x: integrate_f(x["a"], x["b"], x["N"]), axis=1)  # noqa E999

By far the majority of time is spend inside either ``integrate_f`` or ``f``,
hence we'll concentrate our efforts cythonizing these two functions.

.. _enhancingperf.plain:

Plain Cython
~~~~~~~~~~~~

First we're going to need to import the Cython magic function to IPython:

.. ipython:: python
   :okwarning:

   %load_ext Cython


Now, let's simply copy our functions over to Cython as is (the suffix
is here to distinguish between function versions):

.. ipython::

   In [2]: %%cython
      ...: def f_plain(x):
      ...:     return x * (x - 1)
      ...: def integrate_f_plain(a, b, N):
      ...:     s = 0
      ...:     dx = (b - a) / N
      ...:     for i in range(N):
      ...:         s += f_plain(a + i * dx)
      ...:     return s * dx
      ...:

.. note::

  If you're having trouble pasting the above into your ipython, you may need
  to be using bleeding edge IPython for paste to play well with cell magics.


.. ipython:: python

   %timeit df.apply(lambda x: integrate_f_plain(x["a"], x["b"], x["N"]), axis=1)

Already this has shaved a third off, not too bad for a simple copy and paste.

.. _enhancingperf.type:

Adding type
~~~~~~~~~~~

We get another huge improvement simply by providing type information:

.. ipython::

   In [3]: %%cython
      ...: cdef double f_typed(double x) except? -2:
      ...:     return x * (x - 1)
      ...: cpdef double integrate_f_typed(double a, double b, int N):
      ...:     cdef int i
      ...:     cdef double s, dx
      ...:     s = 0
      ...:     dx = (b - a) / N
      ...:     for i in range(N):
      ...:         s += f_typed(a + i * dx)
      ...:     return s * dx
      ...:

.. ipython:: python

   %timeit df.apply(lambda x: integrate_f_typed(x["a"], x["b"], x["N"]), axis=1)

Now, we're talking! It's now over ten times faster than the original Python
implementation, and we haven't *really* modified the code. Let's have another
look at what's eating up time:

.. ipython:: python

   %prun -l 4 df.apply(lambda x: integrate_f_typed(x["a"], x["b"], x["N"]), axis=1)

.. _enhancingperf.ndarray:

Using ndarray
~~~~~~~~~~~~~

It's calling series a lot! It's creating a :class:`Series` from each row, and calling get from both
the index and the series (three times for each row). Function calls are expensive
in Python, so maybe we could minimize these by cythonizing the apply part.

.. note::

  We are now passing ndarrays into the Cython function, fortunately Cython plays
  very nicely with NumPy.

.. ipython::

   In [4]: %%cython
      ...: cimport numpy as np
      ...: import numpy as np
      ...: cdef double f_typed(double x) except? -2:
      ...:     return x * (x - 1)
      ...: cpdef double integrate_f_typed(double a, double b, int N):
      ...:     cdef int i
      ...:     cdef double s, dx
      ...:     s = 0
      ...:     dx = (b - a) / N
      ...:     for i in range(N):
      ...:         s += f_typed(a + i * dx)
      ...:     return s * dx
      ...: cpdef np.ndarray[double] apply_integrate_f(np.ndarray col_a, np.ndarray col_b,
      ...:                                            np.ndarray col_N):
      ...:     assert (col_a.dtype == np.float_
      ...:             and col_b.dtype == np.float_ and col_N.dtype == np.int_)
      ...:     cdef Py_ssize_t i, n = len(col_N)
      ...:     assert (len(col_a) == len(col_b) == n)
      ...:     cdef np.ndarray[double] res = np.empty(n)
      ...:     for i in range(len(col_a)):
      ...:         res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])
      ...:     return res
      ...:


The implementation is simple, it creates an array of zeros and loops over
the rows, applying our ``integrate_f_typed``, and putting this in the zeros array.


.. warning::

   You can **not pass** a :class:`Series` directly as a ``ndarray`` typed parameter
   to a Cython function. Instead pass the actual ``ndarray`` using the
   :meth:`Series.to_numpy`. The reason is that the Cython
   definition is specific to an ndarray and not the passed :class:`Series`.

   So, do not do this:

   .. code-block:: python

        apply_integrate_f(df["a"], df["b"], df["N"])

   But rather, use :meth:`Series.to_numpy` to get the underlying ``ndarray``:

   .. code-block:: python

        apply_integrate_f(df["a"].to_numpy(), df["b"].to_numpy(), df["N"].to_numpy())

.. note::

    Loops like this would be *extremely* slow in Python, but in Cython looping
    over NumPy arrays is *fast*.

.. ipython:: python

   %timeit apply_integrate_f(df["a"].to_numpy(), df["b"].to_numpy(), df["N"].to_numpy())

We've gotten another big improvement. Let's check again where the time is spent:

.. ipython:: python

   %prun -l 4 apply_integrate_f(df["a"].to_numpy(), df["b"].to_numpy(), df["N"].to_numpy())

As one might expect, the majority of the time is now spent in ``apply_integrate_f``,
so if we wanted to make anymore efficiencies we must continue to concentrate our
efforts here.

.. _enhancingperf.boundswrap:

More advanced techniques
~~~~~~~~~~~~~~~~~~~~~~~~

There is still hope for improvement. Here's an example of using some more
advanced Cython techniques:

.. ipython::

   In [5]: %%cython
      ...: cimport cython
      ...: cimport numpy as np
      ...: import numpy as np
      ...: cdef np.float64_t f_typed(np.float64_t x) except? -2:
      ...:     return x * (x - 1)
      ...: cpdef np.float64_t integrate_f_typed(np.float64_t a, np.float64_t b, np.int64_t N):
      ...:     cdef np.int64_t i
      ...:     cdef np.float64_t s = 0.0, dx
      ...:     dx = (b - a) / N
      ...:     for i in range(N):
      ...:         s += f_typed(a + i * dx)
      ...:     return s * dx
      ...: @cython.boundscheck(False)
      ...: @cython.wraparound(False)
      ...: cpdef np.ndarray[np.float64_t] apply_integrate_f_wrap(
      ...:     np.ndarray[np.float64_t] col_a,
      ...:     np.ndarray[np.float64_t] col_b,
      ...:     np.ndarray[np.int64_t] col_N
      ...: ):
      ...:     cdef np.int64_t i, n = len(col_N)
      ...:     assert len(col_a) == len(col_b) == n
      ...:     cdef np.ndarray[np.float64_t] res = np.empty(n, dtype=np.float64)
      ...:     for i in range(n):
      ...:         res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])
      ...:     return res
      ...:

.. ipython:: python

   %timeit apply_integrate_f_wrap(df["a"].to_numpy(), df["b"].to_numpy(), df["N"].to_numpy())

Even faster, with the caveat that a bug in our Cython code (an off-by-one error,
for example) might cause a segfault because memory access isn't checked.
For more about ``boundscheck`` and ``wraparound``, see the Cython docs on
`compiler directives <https://cython.readthedocs.io/en/latest/src/reference/compilation.html?highlight=wraparound#compiler-directives>`__.

.. _enhancingperf.numba:

Numba (JIT compilation)
-----------------------

An alternative to statically compiling Cython code is to use a dynamic just-in-time (JIT) compiler with `Numba <https://numba.pydata.org/>`__.

Numba allows you to write a pure Python function which can be JIT compiled to native machine instructions, similar in performance to C, C++ and Fortran,
by decorating your function with ``@jit``.

Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool).
Numba supports compilation of Python to run on either CPU or GPU hardware and is designed to integrate with the Python scientific software stack.

.. note::

    The ``@jit`` compilation will add overhead to the runtime of the function, so performance benefits may not be realized especially when using small data sets.
    Consider `caching <https://numba.readthedocs.io/en/stable/developer/caching.html>`__ your function to avoid compilation overhead each time your function is run.

Numba can be used in 2 ways with pandas:

#. Specify the ``engine="numba"`` keyword in select pandas methods
#. Define your own Python function decorated with ``@jit`` and pass the underlying NumPy array of :class:`Series` or :class:`DataFrame` (using ``to_numpy()``) into the function

pandas Numba Engine
~~~~~~~~~~~~~~~~~~~

If Numba is installed, one can specify ``engine="numba"`` in select pandas methods to execute the method using Numba.
Methods that support ``engine="numba"`` will also have an ``engine_kwargs`` keyword that accepts a dictionary that allows one to specify
``"nogil"``, ``"nopython"`` and ``"parallel"`` keys with boolean values to pass into the ``@jit`` decorator.
If ``engine_kwargs`` is not specified, it defaults to ``{"nogil": False, "nopython": True, "parallel": False}`` unless otherwise specified.

In terms of performance, **the first time a function is run using the Numba engine will be slow**
as Numba will have some function compilation overhead. However, the JIT compiled functions are cached,
and subsequent calls will be fast. In general, the Numba engine is performant with
a larger amount of data points (e.g. 1+ million).

.. code-block:: ipython

   In [1]: data = pd.Series(range(1_000_000))  # noqa: E225

   In [2]: roll = data.rolling(10)

   In [3]: def f(x):
      ...:     return np.sum(x) + 5
   # Run the first time, compilation time will affect performance
   In [4]: %timeit -r 1 -n 1 roll.apply(f, engine='numba', raw=True)
   1.23 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
   # Function is cached and performance will improve
   In [5]: %timeit roll.apply(f, engine='numba', raw=True)
   188 ms ± 1.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [6]: %timeit roll.apply(f, engine='cython', raw=True)
   3.92 s ± 59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Custom Function Examples
~~~~~~~~~~~~~~~~~~~~~~~~

A custom Python function decorated with ``@jit`` can be used with pandas objects by passing their NumPy array
representations with ``to_numpy()``.

.. code-block:: python

   import numba


   @numba.jit
   def f_plain(x):
       return x * (x - 1)


   @numba.jit
   def integrate_f_numba(a, b, N):
       s = 0
       dx = (b - a) / N
       for i in range(N):
           s += f_plain(a + i * dx)
       return s * dx


   @numba.jit
   def apply_integrate_f_numba(col_a, col_b, col_N):
       n = len(col_N)
       result = np.empty(n, dtype="float64")
       assert len(col_a) == len(col_b) == n
       for i in range(n):
           result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])
       return result


   def compute_numba(df):
       result = apply_integrate_f_numba(
           df["a"].to_numpy(), df["b"].to_numpy(), df["N"].to_numpy()
       )
       return pd.Series(result, index=df.index, name="result")


.. code-block:: ipython

   In [4]: %timeit compute_numba(df)
   1000 loops, best of 3: 798 us per loop

In this example, using Numba was faster than Cython.

Numba can also be used to write vectorized functions that do not require the user to explicitly
loop over the observations of a vector; a vectorized function will be applied to each row automatically.
Consider the following example of doubling each observation:

.. code-block:: python

   import numba


   def double_every_value_nonumba(x):
       return x * 2


   @numba.vectorize
   def double_every_value_withnumba(x):  # noqa E501
       return x * 2

.. code-block:: ipython

   # Custom function without numba
   In [5]: %timeit df["col1_doubled"] = df["a"].apply(double_every_value_nonumba)  # noqa E501
   1000 loops, best of 3: 797 us per loop

   # Standard implementation (faster than a custom function)
   In [6]: %timeit df["col1_doubled"] = df["a"] * 2
   1000 loops, best of 3: 233 us per loop

   # Custom function with numba
   In [7]: %timeit df["col1_doubled"] = double_every_value_withnumba(df["a"].to_numpy())
   1000 loops, best of 3: 145 us per loop

Caveats
~~~~~~~

Numba is best at accelerating functions that apply numerical functions to NumPy
arrays. If you try to ``@jit`` a function that contains unsupported `Python <https://numba.readthedocs.io/en/stable/reference/pysupported.html>`__
or `NumPy <https://numba.readthedocs.io/en/stable/reference/numpysupported.html>`__
code, compilation will revert `object mode <https://numba.readthedocs.io/en/stable/glossary.html#term-object-mode>`__ which
will mostly likely not speed up your function. If you would
prefer that Numba throw an error if it cannot compile a function in a way that
speeds up your code, pass Numba the argument
``nopython=True`` (e.g.  ``@jit(nopython=True)``). For more on
troubleshooting Numba modes, see the `Numba troubleshooting page
<https://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#the-compiled-code-is-too-slow>`__.

Using ``parallel=True`` (e.g. ``@jit(parallel=True)``) may result in a ``SIGABRT`` if the threading layer leads to unsafe
behavior. You can first `specify a safe threading layer <https://numba.readthedocs.io/en/stable/user/threading-layer.html#selecting-a-threading-layer-for-safe-parallel-execution>`__
before running a JIT function with ``parallel=True``.

Generally if the you encounter a segfault (``SIGSEGV``) while using Numba, please report the issue
to the `Numba issue tracker. <https://github.com/numba/numba/issues/new/choose>`__

.. _enhancingperf.eval:

Expression evaluation via :func:`~pandas.eval`
-----------------------------------------------

The top-level function :func:`pandas.eval` implements expression evaluation of
:class:`~pandas.Series` and :class:`~pandas.DataFrame` objects.

.. note::

   To benefit from using :func:`~pandas.eval` you need to
   install ``numexpr``. See the :ref:`recommended dependencies section
   <install.recommended_dependencies>` for more details.

The point of using :func:`~pandas.eval` for expression evaluation rather than
plain Python is two-fold: 1) large :class:`~pandas.DataFrame` objects are
evaluated more efficiently and 2) large arithmetic and boolean expressions are
evaluated all at once by the underlying engine (by default ``numexpr`` is used
for evaluation).

.. note::

   You should not use :func:`~pandas.eval` for simple
   expressions or for expressions involving small DataFrames. In fact,
   :func:`~pandas.eval` is many orders of magnitude slower for
   smaller expressions/objects than plain ol' Python. A good rule of thumb is
   to only use :func:`~pandas.eval` when you have a
   :class:`~pandas.core.frame.DataFrame` with more than 10,000 rows.


:func:`~pandas.eval` supports all arithmetic expressions supported by the
engine in addition to some extensions available only in pandas.

.. note::

   The larger the frame and the larger the expression the more speedup you will
   see from using :func:`~pandas.eval`.

Supported syntax
~~~~~~~~~~~~~~~~

These operations are supported by :func:`pandas.eval`:

* Arithmetic operations except for the left shift (``<<``) and right shift
  (``>>``) operators, e.g., ``df + 2 * pi / s ** 4 % 42 - the_golden_ratio``
* Comparison operations, including chained comparisons, e.g., ``2 < df < df2``
* Boolean operations, e.g., ``df < df2 and df3 < df4 or not df_bool``
* ``list`` and ``tuple`` literals, e.g., ``[1, 2]`` or ``(1, 2)``
* Attribute access, e.g., ``df.a``
* Subscript expressions, e.g., ``df[0]``
* Simple variable evaluation, e.g., ``pd.eval("df")`` (this is not very useful)
* Math functions: ``sin``, ``cos``, ``exp``, ``log``, ``expm1``, ``log1p``,
  ``sqrt``, ``sinh``, ``cosh``, ``tanh``, ``arcsin``, ``arccos``, ``arctan``, ``arccosh``,
  ``arcsinh``, ``arctanh``, ``abs``, ``arctan2`` and ``log10``.

This Python syntax is **not** allowed:

* Expressions

    * Function calls other than math functions.
    * ``is``/``is not`` operations
    * ``if`` expressions
    * ``lambda`` expressions
    * ``list``/``set``/``dict`` comprehensions
    * Literal ``dict`` and ``set`` expressions
    * ``yield`` expressions
    * Generator expressions
    * Boolean expressions consisting of only scalar values

* Statements

    * Neither `simple <https://docs.python.org/3/reference/simple_stmts.html>`__
      nor `compound <https://docs.python.org/3/reference/compound_stmts.html>`__
      statements are allowed. This includes things like ``for``, ``while``, and
      ``if``.



:func:`~pandas.eval` examples
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:func:`pandas.eval` works well with expressions containing large arrays.

First let's create a few decent-sized arrays to play with:

.. ipython:: python

   nrows, ncols = 20000, 100
   df1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)]


Now let's compare adding them together using plain ol' Python versus
:func:`~pandas.eval`:

.. ipython:: python

   %timeit df1 + df2 + df3 + df4

.. ipython:: python

   %timeit pd.eval("df1 + df2 + df3 + df4")


Now let's do the same thing but with comparisons:

.. ipython:: python

   %timeit (df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)

.. ipython:: python

   %timeit pd.eval("(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)")


:func:`~pandas.eval` also works with unaligned pandas objects:

.. ipython:: python

   s = pd.Series(np.random.randn(50))
   %timeit df1 + df2 + df3 + df4 + s

.. ipython:: python

   %timeit pd.eval("df1 + df2 + df3 + df4 + s")

.. note::

   Operations such as

      .. code-block:: python

         1 and 2  # would parse to 1 & 2, but should evaluate to 2
         3 or 4  # would parse to 3 | 4, but should evaluate to 3
         ~1  # this is okay, but slower when using eval

   should be performed in Python. An exception will be raised if you try to
   perform any boolean/bitwise operations with scalar operands that are not
   of type ``bool`` or ``np.bool_``. Again, you should perform these kinds of
   operations in plain Python.

The :meth:`DataFrame.eval` method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition to the top level :func:`pandas.eval` function you can also
evaluate an expression in the "context" of a :class:`~pandas.DataFrame`.

.. ipython:: python
   :suppress:

   try:
       del a
   except NameError:
       pass

   try:
       del b
   except NameError:
       pass

.. ipython:: python

   df = pd.DataFrame(np.random.randn(5, 2), columns=["a", "b"])
   df.eval("a + b")

Any expression that is a valid :func:`pandas.eval` expression is also a valid
:meth:`DataFrame.eval` expression, with the added benefit that you don't have to
prefix the name of the :class:`~pandas.DataFrame` to the column(s) you're
interested in evaluating.

In addition, you can perform assignment of columns within an expression.
This allows for *formulaic evaluation*.  The assignment target can be a
new column name or an existing column name, and it must be a valid Python
identifier.

The ``inplace`` keyword determines whether this assignment will performed
on the original :class:`DataFrame` or return a copy with the new column.

.. ipython:: python

   df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))
   df.eval("c = a + b", inplace=True)
   df.eval("d = a + b + c", inplace=True)
   df.eval("a = 1", inplace=True)
   df

When ``inplace`` is set to ``False``, the default, a copy of the :class:`DataFrame` with the
new or modified columns is returned and the original frame is unchanged.

.. ipython:: python

   df
   df.eval("e = a - c", inplace=False)
   df

As a convenience, multiple assignments can be performed by using a
multi-line string.

.. ipython:: python

   df.eval(
       """
   c = a + b
   d = a + b + c
   a = 1""",
       inplace=False,
   )

The equivalent in standard Python would be

.. ipython:: python

   df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))
   df["c"] = df["a"] + df["b"]
   df["d"] = df["a"] + df["b"] + df["c"]
   df["a"] = 1
   df

The :class:`DataFrame.query` method has a ``inplace`` keyword which determines
whether the query modifies the original frame.

.. ipython:: python

   df = pd.DataFrame(dict(a=range(5), b=range(5, 10)))
   df.query("a > 2")
   df.query("a > 2", inplace=True)
   df

Local variables
~~~~~~~~~~~~~~~

You must *explicitly reference* any local variable that you want to use in an
expression by placing the ``@`` character in front of the name. For example,

.. ipython:: python

   df = pd.DataFrame(np.random.randn(5, 2), columns=list("ab"))
   newcol = np.random.randn(len(df))
   df.eval("b + @newcol")
   df.query("b < @newcol")

If you don't prefix the local variable with ``@``, pandas will raise an
exception telling you the variable is undefined.

When using :meth:`DataFrame.eval` and :meth:`DataFrame.query`, this allows you
to have a local variable and a :class:`~pandas.DataFrame` column with the same
name in an expression.


.. ipython:: python

   a = np.random.randn()
   df.query("@a < a")
   df.loc[a < df["a"]]  # same as the previous expression

With :func:`pandas.eval` you cannot use the ``@`` prefix *at all*, because it
isn't defined in that context. pandas will let you know this if you try to
use ``@`` in a top-level call to :func:`pandas.eval`. For example,

.. ipython:: python
   :okexcept:

   a, b = 1, 2
   pd.eval("@a + b")

In this case, you should simply refer to the variables like you would in
standard Python.

.. ipython:: python

   pd.eval("a + b")


:func:`pandas.eval` parsers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are two different parsers and two different engines you can use as
the backend.

The default ``'pandas'`` parser allows a more intuitive syntax for expressing
query-like operations (comparisons, conjunctions and disjunctions). In
particular, the precedence of the ``&`` and ``|`` operators is made equal to
the precedence of the corresponding boolean operations ``and`` and ``or``.

For example, the above conjunction can be written without parentheses.
Alternatively, you can use the ``'python'`` parser to enforce strict Python
semantics.

.. ipython:: python

   expr = "(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)"
   x = pd.eval(expr, parser="python")
   expr_no_parens = "df1 > 0 & df2 > 0 & df3 > 0 & df4 > 0"
   y = pd.eval(expr_no_parens, parser="pandas")
   np.all(x == y)


The same expression can be "anded" together with the word :keyword:`and` as
well:

.. ipython:: python

   expr = "(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)"
   x = pd.eval(expr, parser="python")
   expr_with_ands = "df1 > 0 and df2 > 0 and df3 > 0 and df4 > 0"
   y = pd.eval(expr_with_ands, parser="pandas")
   np.all(x == y)


The ``and`` and ``or`` operators here have the same precedence that they would
in vanilla Python.


:func:`pandas.eval` backends
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There's also the option to make :func:`~pandas.eval` operate identical to plain
ol' Python.

.. note::

   Using the ``'python'`` engine is generally *not* useful, except for testing
   other evaluation engines against it. You will achieve **no** performance
   benefits using :func:`~pandas.eval` with ``engine='python'`` and in fact may
   incur a performance hit.

You can see this by using :func:`pandas.eval` with the ``'python'`` engine. It
is a bit slower (not by much) than evaluating the same expression in Python

.. ipython:: python

   %timeit df1 + df2 + df3 + df4

.. ipython:: python

   %timeit pd.eval("df1 + df2 + df3 + df4", engine="python")


:func:`pandas.eval` performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:func:`~pandas.eval` is intended to speed up certain kinds of operations. In
particular, those operations involving complex expressions with large
:class:`~pandas.DataFrame`/:class:`~pandas.Series` objects should see a
significant performance benefit.  Here is a plot showing the running time of
:func:`pandas.eval` as function of the size of the frame involved in the
computation. The two lines are two different engines.


.. image:: ../_static/eval-perf.png


.. note::

   Operations with smallish objects (around 15k-20k rows) are faster using
   plain Python:

       .. image:: ../_static/eval-perf-small.png


This plot was created using a :class:`DataFrame` with 3 columns each containing
floating point values generated using ``numpy.random.randn()``.

Technical minutia regarding expression evaluation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Expressions that would result in an object dtype or involve datetime operations
(because of ``NaT``) must be evaluated in Python space. The main reason for
this behavior is to maintain backwards compatibility with versions of NumPy <
1.7. In those versions of NumPy a call to ``ndarray.astype(str)`` will
truncate any strings that are more than 60 characters in length. Second, we
can't pass ``object`` arrays to ``numexpr`` thus string comparisons must be
evaluated in Python space.

The upshot is that this *only* applies to object-dtype expressions. So, if
you have an expression--for example

.. ipython:: python

   df = pd.DataFrame(
       {"strings": np.repeat(list("cba"), 3), "nums": np.repeat(range(3), 3)}
   )
   df
   df.query("strings == 'a' and nums == 1")

the numeric part of the comparison (``nums == 1``) will be evaluated by
``numexpr``.

In general, :meth:`DataFrame.query`/:func:`pandas.eval` will
evaluate the subexpressions that *can* be evaluated by ``numexpr`` and those
that must be evaluated in Python space transparently to the user. This is done
by inferring the result type of an expression from its arguments and operators.
.. _options:

{{ header }}

********************
Options and settings
********************

Overview
--------
pandas has an options system that lets you customize some aspects of its behaviour,
display-related options being those the user is most likely to adjust.

Options have a full "dotted-style", case-insensitive name (e.g. ``display.max_rows``).
You can get/set options directly as attributes of the top-level ``options`` attribute:

.. ipython:: python

   import pandas as pd

   pd.options.display.max_rows
   pd.options.display.max_rows = 999
   pd.options.display.max_rows

The API is composed of 5 relevant functions, available directly from the ``pandas``
namespace:

* :func:`~pandas.get_option` / :func:`~pandas.set_option` - get/set the value of a single option.
* :func:`~pandas.reset_option` - reset one or more options to their default value.
* :func:`~pandas.describe_option` - print the descriptions of one or more options.
* :func:`~pandas.option_context` - execute a codeblock with a set of options
  that revert to prior settings after execution.

**Note:** Developers can check out `pandas/core/config_init.py <https://github.com/pandas-dev/pandas/blob/main/pandas/core/config_init.py>`_ for more information.

All of the functions above accept a regexp pattern (``re.search`` style) as an argument,
and so passing in a substring will work - as long as it is unambiguous:

.. ipython:: python

   pd.get_option("display.chop_threshold")
   pd.set_option("display.chop_threshold", 2)
   pd.get_option("display.chop_threshold")
   pd.set_option("chop", 4)
   pd.get_option("display.chop_threshold")


The following will **not work** because it matches multiple option names, e.g.
``display.max_colwidth``, ``display.max_rows``, ``display.max_columns``:

.. ipython:: python
   :okexcept:

   try:
       pd.get_option("max")
   except KeyError as e:
       print(e)


**Note:** Using this form of shorthand may cause your code to break if new options with similar names are added in future versions.


You can get a list of available options and their descriptions with ``describe_option``. When called
with no argument ``describe_option`` will print out the descriptions for all available options.

.. ipython:: python
   :suppress:
   :okwarning:

   pd.reset_option("all")

Getting and setting options
---------------------------

As described above, :func:`~pandas.get_option` and :func:`~pandas.set_option`
are available from the pandas namespace.  To change an option, call
``set_option('option regex', new_value)``.

.. ipython:: python

   pd.get_option("mode.sim_interactive")
   pd.set_option("mode.sim_interactive", True)
   pd.get_option("mode.sim_interactive")

**Note:** The option 'mode.sim_interactive' is mostly used for debugging purposes.

All options also have a default value, and you can use ``reset_option`` to do just that:

.. ipython:: python
   :suppress:

   pd.reset_option("display.max_rows")

.. ipython:: python

   pd.get_option("display.max_rows")
   pd.set_option("display.max_rows", 999)
   pd.get_option("display.max_rows")
   pd.reset_option("display.max_rows")
   pd.get_option("display.max_rows")


It's also possible to reset multiple options at once (using a regex):

.. ipython:: python
   :okwarning:

   pd.reset_option("^display")


``option_context`` context manager has been exposed through
the top-level API, allowing you to execute code with given option values. Option values
are restored automatically when you exit the ``with`` block:

.. ipython:: python

   with pd.option_context("display.max_rows", 10, "display.max_columns", 5):
       print(pd.get_option("display.max_rows"))
       print(pd.get_option("display.max_columns"))
   print(pd.get_option("display.max_rows"))
   print(pd.get_option("display.max_columns"))


Setting startup options in Python/IPython environment
-----------------------------------------------------

Using startup scripts for the Python/IPython environment to import pandas and set options makes working with pandas more efficient.  To do this, create a .py or .ipy script in the startup directory of the desired profile.  An example where the startup folder is in a default IPython profile can be found at:

.. code-block:: none

  $IPYTHONDIR/profile_default/startup

More information can be found in the `IPython documentation
<https://ipython.org/ipython-doc/stable/interactive/tutorial.html#startup-files>`__.  An example startup script for pandas is displayed below:

.. code-block:: python

  import pandas as pd

  pd.set_option("display.max_rows", 999)
  pd.set_option("display.precision", 5)

.. _options.frequently_used:

Frequently used options
-----------------------
The following is a walk-through of the more frequently used display options.

``display.max_rows`` and ``display.max_columns`` sets the maximum number
of rows and columns displayed when a frame is pretty-printed.  Truncated
lines are replaced by an ellipsis.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(7, 2))
   pd.set_option("display.max_rows", 7)
   df
   pd.set_option("display.max_rows", 5)
   df
   pd.reset_option("display.max_rows")

Once the ``display.max_rows`` is exceeded, the ``display.min_rows`` options
determines how many rows are shown in the truncated repr.

.. ipython:: python

   pd.set_option("display.max_rows", 8)
   pd.set_option("display.min_rows", 4)
   # below max_rows -> all rows shown
   df = pd.DataFrame(np.random.randn(7, 2))
   df
   # above max_rows -> only min_rows (4) rows shown
   df = pd.DataFrame(np.random.randn(9, 2))
   df
   pd.reset_option("display.max_rows")
   pd.reset_option("display.min_rows")

``display.expand_frame_repr`` allows for the representation of
dataframes to stretch across pages, wrapped over the full column vs row-wise.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(5, 10))
   pd.set_option("expand_frame_repr", True)
   df
   pd.set_option("expand_frame_repr", False)
   df
   pd.reset_option("expand_frame_repr")

``display.large_repr`` lets you select whether to display dataframes that exceed
``max_columns`` or ``max_rows`` as a truncated frame, or as a summary.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 10))
   pd.set_option("display.max_rows", 5)
   pd.set_option("large_repr", "truncate")
   df
   pd.set_option("large_repr", "info")
   df
   pd.reset_option("large_repr")
   pd.reset_option("display.max_rows")

``display.max_colwidth`` sets the maximum width of columns.  Cells
of this length or longer will be truncated with an ellipsis.

.. ipython:: python

   df = pd.DataFrame(
       np.array(
           [
               ["foo", "bar", "bim", "uncomfortably long string"],
               ["horse", "cow", "banana", "apple"],
           ]
       )
   )
   pd.set_option("max_colwidth", 40)
   df
   pd.set_option("max_colwidth", 6)
   df
   pd.reset_option("max_colwidth")

``display.max_info_columns`` sets a threshold for when by-column info
will be given.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 10))
   pd.set_option("max_info_columns", 11)
   df.info()
   pd.set_option("max_info_columns", 5)
   df.info()
   pd.reset_option("max_info_columns")

``display.max_info_rows``: ``df.info()`` will usually show null-counts for each column.
For large frames this can be quite slow. ``max_info_rows`` and ``max_info_cols``
limit this null check only to frames with smaller dimensions then specified. Note that you
can specify the option ``df.info(null_counts=True)`` to override on showing a particular frame.

.. ipython:: python

   df = pd.DataFrame(np.random.choice([0, 1, np.nan], size=(10, 10)))
   df
   pd.set_option("max_info_rows", 11)
   df.info()
   pd.set_option("max_info_rows", 5)
   df.info()
   pd.reset_option("max_info_rows")

``display.precision`` sets the output display precision in terms of decimal places.
This is only a suggestion.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(5, 5))
   pd.set_option("display.precision", 7)
   df
   pd.set_option("display.precision", 4)
   df

``display.chop_threshold`` sets at what level pandas rounds to zero when
it displays a Series of DataFrame. This setting does not change the
precision at which the number is stored.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(6, 6))
   pd.set_option("chop_threshold", 0)
   df
   pd.set_option("chop_threshold", 0.5)
   df
   pd.reset_option("chop_threshold")

``display.colheader_justify`` controls the justification of the headers.
The options are 'right', and 'left'.

.. ipython:: python

   df = pd.DataFrame(
       np.array([np.random.randn(6), np.random.randint(1, 9, 6) * 0.1, np.zeros(6)]).T,
       columns=["A", "B", "C"],
       dtype="float",
   )
   pd.set_option("colheader_justify", "right")
   df
   pd.set_option("colheader_justify", "left")
   df
   pd.reset_option("colheader_justify")



.. _options.available:

Available options
-----------------

======================================= ============ ==================================
Option                                  Default      Function
======================================= ============ ==================================
display.chop_threshold                  None         If set to a float value, all float
                                                     values smaller then the given
                                                     threshold will be displayed as
                                                     exactly 0 by repr and friends.
display.colheader_justify               right        Controls the justification of
                                                     column headers. used by DataFrameFormatter.
display.column_space                    12           No description available.
display.date_dayfirst                   False        When True, prints and parses dates
                                                     with the day first, eg 20/01/2005
display.date_yearfirst                  False        When True, prints and parses dates
                                                     with the year first, eg 2005/01/20
display.encoding                        UTF-8        Defaults to the detected encoding
                                                     of the console. Specifies the encoding
                                                     to be used for strings returned by
                                                     to_string, these are generally strings
                                                     meant to be displayed on the console.
display.expand_frame_repr               True         Whether to print out the full DataFrame
                                                     repr for wide DataFrames across
                                                     multiple lines, ``max_columns`` is
                                                     still respected, but the output will
                                                     wrap-around across multiple "pages"
                                                     if its width exceeds ``display.width``.
display.float_format                    None         The callable should accept a floating
                                                     point number and return a string with
                                                     the desired format of the number.
                                                     This is used in some places like
                                                     SeriesFormatter.
                                                     See core.format.EngFormatter for an example.
display.large_repr                      truncate     For DataFrames exceeding max_rows/max_cols,
                                                     the repr (and HTML repr) can show
                                                     a truncated table (the default),
                                                     or switch to the view from df.info()
                                                     (the behaviour in earlier versions of pandas).
                                                     allowable settings, ['truncate', 'info']
display.latex.repr                      False        Whether to produce a latex DataFrame
                                                     representation for Jupyter frontends
                                                     that support it.
display.latex.escape                    True         Escapes special characters in DataFrames, when
                                                     using the to_latex method.
display.latex.longtable                 False        Specifies if the to_latex method of a DataFrame
                                                     uses the longtable format.
display.latex.multicolumn               True         Combines columns when using a MultiIndex
display.latex.multicolumn_format        'l'          Alignment of multicolumn labels
display.latex.multirow                  False        Combines rows when using a MultiIndex.
                                                     Centered instead of top-aligned,
                                                     separated by clines.
display.max_columns                     0 or 20      max_rows and max_columns are used
                                                     in __repr__() methods to decide if
                                                     to_string() or info() is used to
                                                     render an object to a string.  In
                                                     case Python/IPython is running in
                                                     a terminal this is set to 0 by default and
                                                     pandas will correctly auto-detect
                                                     the width of the terminal and switch to
                                                     a smaller format in case all columns
                                                     would not fit vertically. The IPython
                                                     notebook, IPython qtconsole, or IDLE
                                                     do not run in a terminal and hence
                                                     it is not possible to do correct
                                                     auto-detection, in which case the default
                                                     is set to 20. 'None' value means unlimited.
display.max_colwidth                    50           The maximum width in characters of
                                                     a column in the repr of a pandas
                                                     data structure. When the column overflows,
                                                     a "..." placeholder is embedded in
                                                     the output. 'None' value means unlimited.
display.max_info_columns                100          max_info_columns is used in DataFrame.info
                                                     method to decide if per column information
                                                     will be printed.
display.max_info_rows                   1690785      df.info() will usually show null-counts
                                                     for each column. For large frames
                                                     this can be quite slow. max_info_rows
                                                     and max_info_cols limit this null
                                                     check only to frames with smaller
                                                     dimensions then specified.
display.max_rows                        60           This sets the maximum number of rows
                                                     pandas should output when printing
                                                     out various output. For example,
                                                     this value determines whether the
                                                     repr() for a dataframe prints out
                                                     fully or just a truncated or summary repr.
                                                     'None' value means unlimited.
display.min_rows                        10           The numbers of rows to show in a truncated
                                                     repr (when ``max_rows`` is exceeded). Ignored
                                                     when ``max_rows`` is set to None or 0. When set
                                                     to None, follows the value of ``max_rows``.
display.max_seq_items                   100          when pretty-printing a long sequence,
                                                     no more then ``max_seq_items`` will
                                                     be printed. If items are omitted,
                                                     they will be denoted by the addition
                                                     of "..." to the resulting string.
                                                     If set to None, the number of items
                                                     to be printed is unlimited.
display.memory_usage                    True         This specifies if the memory usage of
                                                     a DataFrame should be displayed when the
                                                     df.info() method is invoked.
display.multi_sparse                    True         "Sparsify" MultiIndex display (don't
                                                     display repeated elements in outer
                                                     levels within groups)
display.notebook_repr_html              True         When True, IPython notebook will
                                                     use html representation for
                                                     pandas objects (if it is available).
display.pprint_nest_depth               3            Controls the number of nested levels
                                                     to process when pretty-printing
display.precision                       6            Floating point output precision in
                                                     terms of number of places after the
                                                     decimal, for regular formatting as well
                                                     as scientific notation. Similar to
                                                     numpy's ``precision`` print option
display.show_dimensions                 truncate     Whether to print out dimensions
                                                     at the end of DataFrame repr.
                                                     If 'truncate' is specified, only
                                                     print out the dimensions if the
                                                     frame is truncated (e.g. not display
                                                     all rows and/or columns)
display.width                           80           Width of the display in characters.
                                                     In case Python/IPython is running in
                                                     a terminal this can be set to None
                                                     and pandas will correctly auto-detect
                                                     the width. Note that the IPython notebook,
                                                     IPython qtconsole, or IDLE do not run in a
                                                     terminal and hence it is not possible
                                                     to correctly detect the width.
display.html.table_schema               False        Whether to publish a Table Schema
                                                     representation for frontends that
                                                     support it.
display.html.border                     1            A ``border=value`` attribute is
                                                     inserted in the ``<table>`` tag
                                                     for the DataFrame HTML repr.
display.html.use_mathjax                True         When True, Jupyter notebook will process
                                                     table contents using MathJax, rendering
                                                     mathematical expressions enclosed by the
                                                     dollar symbol.
display.max_dir_items                   100          The number of columns from a dataframe that
                                                     are added to dir. These columns can then be
                                                     suggested by tab completion. 'None' value means
                                                     unlimited.
io.excel.xls.writer                     xlwt         The default Excel writer engine for
                                                     'xls' files.

                                                     .. deprecated:: 1.2.0

                                                        As `xlwt <https://pypi.org/project/xlwt/>`__
                                                        package is no longer maintained, the ``xlwt``
                                                        engine will be removed in a future version of
                                                        pandas. Since this is the only engine in pandas
                                                        that supports writing to ``.xls`` files,
                                                        this option will also be removed.

io.excel.xlsm.writer                    openpyxl     The default Excel writer engine for
                                                     'xlsm' files. Available options:
                                                     'openpyxl' (the default).
io.excel.xlsx.writer                    openpyxl     The default Excel writer engine for
                                                     'xlsx' files.
io.hdf.default_format                   None         default format writing format, if
                                                     None, then put will default to
                                                     'fixed' and append will default to
                                                     'table'
io.hdf.dropna_table                     True         drop ALL nan rows when appending
                                                     to a table
io.parquet.engine                       None         The engine to use as a default for
                                                     parquet reading and writing. If None
                                                     then try 'pyarrow' and 'fastparquet'
io.sql.engine                           None         The engine to use as a default for
                                                     sql reading and writing, with SQLAlchemy
                                                     as a higher level interface. If None
                                                     then try 'sqlalchemy'
mode.chained_assignment                 warn         Controls ``SettingWithCopyWarning``:
                                                     'raise', 'warn', or None. Raise an
                                                     exception, warn, or no action if
                                                     trying to use :ref:`chained assignment <indexing.evaluation_order>`.
mode.sim_interactive                    False        Whether to simulate interactive mode
                                                     for purposes of testing.
mode.use_inf_as_na                      False        True means treat None, NaN, -INF,
                                                     INF as NA (old way), False means
                                                     None and NaN are null, but INF, -INF
                                                     are not NA (new way).
compute.use_bottleneck                  True         Use the bottleneck library to accelerate
                                                     computation if it is installed.
compute.use_numexpr                     True         Use the numexpr library to accelerate
                                                     computation if it is installed.
plotting.backend                        matplotlib   Change the plotting backend to a different
                                                     backend than the current matplotlib one.
                                                     Backends can be implemented as third-party
                                                     libraries implementing the pandas plotting
                                                     API. They can use other plotting libraries
                                                     like Bokeh, Altair, etc.
plotting.matplotlib.register_converters True         Register custom converters with
                                                     matplotlib. Set to False to de-register.
styler.sparse.index                     True         "Sparsify" MultiIndex display for rows
                                                     in Styler output (don't display repeated
                                                     elements in outer levels within groups).
styler.sparse.columns                   True         "Sparsify" MultiIndex display for columns
                                                     in Styler output.
styler.render.repr                      html         Standard output format for Styler rendered in Jupyter Notebook.
                                                     Should be one of "html" or "latex".
styler.render.max_elements              262144       Maximum number of datapoints that Styler will render
                                                     trimming either rows, columns or both to fit.
styler.render.max_rows                  None         Maximum number of rows that Styler will render. By default
                                                     this is dynamic based on ``max_elements``.
styler.render.max_columns               None         Maximum number of columns that Styler will render. By default
                                                     this is dynamic based on ``max_elements``.
styler.render.encoding                  utf-8        Default encoding for output HTML or LaTeX files.
styler.format.formatter                 None         Object to specify formatting functions to ``Styler.format``.
styler.format.na_rep                    None         String representation for missing data.
styler.format.precision                 6            Precision to display floating point and complex numbers.
styler.format.decimal                   .            String representation for decimal point separator for floating
                                                     point and complex numbers.
styler.format.thousands                 None         String representation for thousands separator for
                                                     integers, and floating point and complex numbers.
styler.format.escape                    None         Whether to escape "html" or "latex" special
                                                     characters in the display representation.
styler.html.mathjax                     True         If set to False will render specific CSS classes to
                                                     table attributes that will prevent Mathjax from rendering
                                                     in Jupyter Notebook.
styler.latex.multicol_align             r            Alignment of headers in a merged column due to sparsification. Can be in {"r", "c", "l"}.
styler.latex.multirow_align             c            Alignment of index labels in a merged row due to sparsification. Can be in {"c", "t", "b"}.
styler.latex.environment                None         If given will replace the default ``\\begin{table}`` environment. If "longtable" is specified
                                                     this will render with a specific "longtable" template with longtable features.
styler.latex.hrules                     False        If set to True will render ``\\toprule``, ``\\midrule``, and ``\bottomrule`` by default.
======================================= ============ ==================================


.. _basics.console_output:

Number formatting
------------------

pandas also allows you to set how numbers are displayed in the console.
This option is not set through the ``set_options`` API.

Use the ``set_eng_float_format`` function
to alter the floating-point formatting of pandas objects to produce a particular
format.

For instance:

.. ipython:: python

   import numpy as np

   pd.set_eng_float_format(accuracy=3, use_eng_prefix=True)
   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   s / 1.0e3
   s / 1.0e6

.. ipython:: python
   :suppress:
   :okwarning:

   pd.reset_option("^display")

To round floats on a case-by-case basis, you can also use :meth:`~pandas.Series.round` and :meth:`~pandas.DataFrame.round`.

.. _options.east_asian_width:

Unicode formatting
------------------

.. warning::

   Enabling this option will affect the performance for printing of DataFrame and Series (about 2 times slower).
   Use only when it is actually required.

Some East Asian countries use Unicode characters whose width corresponds to two Latin characters.
If a DataFrame or Series contains these characters, the default output mode may not align them properly.

.. note:: Screen captures are attached for each output to show the actual results.

.. ipython:: python

   df = pd.DataFrame({"国籍": ["UK", "日本"], "名前": ["Alice", "しのぶ"]})
   df

.. image:: ../_static/option_unicode01.png

Enabling ``display.unicode.east_asian_width`` allows pandas to check each character's "East Asian Width" property.
These characters can be aligned properly by setting this option to ``True``. However, this will result in longer render
times than the standard ``len`` function.

.. ipython:: python

   pd.set_option("display.unicode.east_asian_width", True)
   df

.. image:: ../_static/option_unicode02.png

In addition, Unicode characters whose width is "Ambiguous" can either be 1 or 2 characters wide depending on the
terminal setting or encoding. The option ``display.unicode.ambiguous_as_wide`` can be used to handle the ambiguity.

By default, an "Ambiguous" character's width, such as "¡" (inverted exclamation) in the example below, is taken to be 1.

.. ipython:: python

   df = pd.DataFrame({"a": ["xxx", "¡¡"], "b": ["yyy", "¡¡"]})
   df

.. image:: ../_static/option_unicode03.png

Enabling ``display.unicode.ambiguous_as_wide`` makes pandas interpret these characters' widths to be 2.
(Note that this option will only be effective when ``display.unicode.east_asian_width`` is enabled.)

However, setting this option incorrectly for your terminal will cause these characters to be aligned incorrectly:

.. ipython:: python

   pd.set_option("display.unicode.ambiguous_as_wide", True)
   df

.. image:: ../_static/option_unicode04.png

.. ipython:: python
   :suppress:

   pd.set_option("display.unicode.east_asian_width", False)
   pd.set_option("display.unicode.ambiguous_as_wide", False)

.. _options.table_schema:

Table schema display
--------------------

``DataFrame`` and ``Series`` will publish a Table Schema representation
by default. False by default, this can be enabled globally with the
``display.html.table_schema`` option:

.. ipython:: python

  pd.set_option("display.html.table_schema", True)

Only ``'display.max_rows'`` are serialized and published.


.. ipython:: python
    :suppress:

    pd.reset_option("display.html.table_schema")
.. _merging:

{{ header }}

.. ipython:: python
   :suppress:

   from matplotlib import pyplot as plt
   import pandas.util._doctools as doctools

   p = doctools.TablePlotter()


************************************
Merge, join, concatenate and compare
************************************

pandas provides various facilities for easily combining together Series or
DataFrame with various kinds of set logic for the indexes
and relational algebra functionality in the case of join / merge-type
operations.

In addition, pandas also provides utilities to compare two Series or DataFrame
and summarize their differences.

.. _merging.concat:

Concatenating objects
---------------------

The :func:`~pandas.concat` function (in the main pandas namespace) does all of
the heavy lifting of performing concatenation operations along an axis while
performing optional set logic (union or intersection) of the indexes (if any) on
the other axes. Note that I say "if any" because there is only a single possible
axis of concatenation for Series.

Before diving into all of the details of ``concat`` and what it can do, here is
a simple example:

.. ipython:: python

   df1 = pd.DataFrame(
       {
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
           "C": ["C0", "C1", "C2", "C3"],
           "D": ["D0", "D1", "D2", "D3"],
       },
       index=[0, 1, 2, 3],
   )

   df2 = pd.DataFrame(
       {
           "A": ["A4", "A5", "A6", "A7"],
           "B": ["B4", "B5", "B6", "B7"],
           "C": ["C4", "C5", "C6", "C7"],
           "D": ["D4", "D5", "D6", "D7"],
       },
       index=[4, 5, 6, 7],
   )

   df3 = pd.DataFrame(
       {
           "A": ["A8", "A9", "A10", "A11"],
           "B": ["B8", "B9", "B10", "B11"],
           "C": ["C8", "C9", "C10", "C11"],
           "D": ["D8", "D9", "D10", "D11"],
       },
       index=[8, 9, 10, 11],
   )

   frames = [df1, df2, df3]
   result = pd.concat(frames)

.. ipython:: python
   :suppress:

   @savefig merging_concat_basic.png
   p.plot(frames, result, labels=["df1", "df2", "df3"], vertical=True);
   plt.close("all");

Like its sibling function on ndarrays, ``numpy.concatenate``, ``pandas.concat``
takes a list or dict of homogeneously-typed objects and concatenates them with
some configurable handling of "what to do with the other axes":

::

    pd.concat(
        objs,
        axis=0,
        join="outer",
        ignore_index=False,
        keys=None,
        levels=None,
        names=None,
        verify_integrity=False,
        copy=True,
    )

* ``objs`` : a sequence or mapping of Series or DataFrame objects. If a
  dict is passed, the sorted keys will be used as the ``keys`` argument, unless
  it is passed, in which case the values will be selected (see below). Any None
  objects will be dropped silently unless they are all None in which case a
  ValueError will be raised.
* ``axis`` : {0, 1, ...}, default 0. The axis to concatenate along.
* ``join`` : {'inner', 'outer'}, default 'outer'. How to handle indexes on
  other axis(es). Outer for union and inner for intersection.
* ``ignore_index`` : boolean, default False. If True, do not use the index
  values on the concatenation axis. The resulting axis will be labeled 0, ...,
  n - 1. This is useful if you are concatenating objects where the
  concatenation axis does not have meaningful indexing information. Note
  the index values on the other axes are still respected in the join.
* ``keys`` : sequence, default None. Construct hierarchical index using the
  passed keys as the outermost level. If multiple levels passed, should
  contain tuples.
* ``levels`` : list of sequences, default None. Specific levels (unique values)
  to use for constructing a MultiIndex. Otherwise they will be inferred from the
  keys.
* ``names`` : list, default None. Names for the levels in the resulting
  hierarchical index.
* ``verify_integrity`` : boolean, default False. Check whether the new
  concatenated axis contains duplicates. This can be very expensive relative
  to the actual data concatenation.
* ``copy`` : boolean, default True. If False, do not copy data unnecessarily.

Without a little bit of context many of these arguments don't make much sense.
Let's revisit the above example. Suppose we wanted to associate specific keys
with each of the pieces of the chopped up DataFrame. We can do this using the
``keys`` argument:

.. ipython:: python

   result = pd.concat(frames, keys=["x", "y", "z"])

.. ipython:: python
   :suppress:

   @savefig merging_concat_keys.png
   p.plot(frames, result, labels=["df1", "df2", "df3"], vertical=True)
   plt.close("all");

As you can see (if you've read the rest of the documentation), the resulting
object's index has a :ref:`hierarchical index <advanced.hierarchical>`. This
means that we can now select out each chunk by key:

.. ipython:: python

   result.loc["y"]

It's not a stretch to see how this can be very useful. More detail on this
functionality below.

.. note::
   It is worth noting that :func:`~pandas.concat` (and therefore
   :func:`~pandas.append`) makes a full copy of the data, and that constantly
   reusing this function can create a significant performance hit. If you need
   to use the operation over several datasets, use a list comprehension.

::

   frames = [ process_your_file(f) for f in files ]
   result = pd.concat(frames)

.. note::

   When concatenating DataFrames with named axes, pandas will attempt to preserve
   these index/column names whenever possible. In the case where all inputs share a
   common name, this name will be assigned to the result. When the input names do
   not all agree, the result will be unnamed. The same is true for :class:`MultiIndex`,
   but the logic is applied separately on a level-by-level basis.


Set logic on the other axes
~~~~~~~~~~~~~~~~~~~~~~~~~~~

When gluing together multiple DataFrames, you have a choice of how to handle
the other axes (other than the one being concatenated). This can be done in
the following two ways:

* Take the union of them all, ``join='outer'``. This is the default
  option as it results in zero information loss.
* Take the intersection, ``join='inner'``.

Here is an example of each of these methods. First, the default ``join='outer'``
behavior:

.. ipython:: python

   df4 = pd.DataFrame(
       {
           "B": ["B2", "B3", "B6", "B7"],
           "D": ["D2", "D3", "D6", "D7"],
           "F": ["F2", "F3", "F6", "F7"],
       },
       index=[2, 3, 6, 7],
   )
   result = pd.concat([df1, df4], axis=1)


.. ipython:: python
   :suppress:

   @savefig merging_concat_axis1.png
   p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);
   plt.close("all");

Here is the same thing with ``join='inner'``:

.. ipython:: python

   result = pd.concat([df1, df4], axis=1, join="inner")

.. ipython:: python
   :suppress:

   @savefig merging_concat_axis1_inner.png
   p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);
   plt.close("all");

Lastly, suppose we just wanted to reuse the *exact index* from the original
DataFrame:

.. ipython:: python

   result = pd.concat([df1, df4], axis=1).reindex(df1.index)

Similarly, we could index before the concatenation:

.. ipython:: python

    pd.concat([df1, df4.reindex(df1.index)], axis=1)

.. ipython:: python
   :suppress:

   @savefig merging_concat_axis1_join_axes.png
   p.plot([df1, df4], result, labels=["df1", "df4"], vertical=False);
   plt.close("all");

.. _merging.ignore_index:

Ignoring indexes on the concatenation axis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For ``DataFrame`` objects which don't have a meaningful index, you may wish
to append them and ignore the fact that they may have overlapping indexes. To
do this, use the ``ignore_index`` argument:

.. ipython:: python

   result = pd.concat([df1, df4], ignore_index=True, sort=False)

.. ipython:: python
   :suppress:

   @savefig merging_concat_ignore_index.png
   p.plot([df1, df4], result, labels=["df1", "df4"], vertical=True);
   plt.close("all");

.. _merging.mixed_ndims:

Concatenating with mixed ndims
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can concatenate a mix of ``Series`` and ``DataFrame`` objects. The
``Series`` will be transformed to ``DataFrame`` with the column name as
the name of the ``Series``.

.. ipython:: python

   s1 = pd.Series(["X0", "X1", "X2", "X3"], name="X")
   result = pd.concat([df1, s1], axis=1)

.. ipython:: python
   :suppress:

   @savefig merging_concat_mixed_ndim.png
   p.plot([df1, s1], result, labels=["df1", "s1"], vertical=False);
   plt.close("all");

.. note::

   Since we're concatenating a ``Series`` to a ``DataFrame``, we could have
   achieved the same result with :meth:`DataFrame.assign`. To concatenate an
   arbitrary number of pandas objects (``DataFrame`` or ``Series``), use
   ``concat``.

If unnamed ``Series`` are passed they will be numbered consecutively.

.. ipython:: python

   s2 = pd.Series(["_0", "_1", "_2", "_3"])
   result = pd.concat([df1, s2, s2, s2], axis=1)

.. ipython:: python
   :suppress:

   @savefig merging_concat_unnamed_series.png
   p.plot([df1, s2], result, labels=["df1", "s2"], vertical=False);
   plt.close("all");

Passing ``ignore_index=True`` will drop all name references.

.. ipython:: python

   result = pd.concat([df1, s1], axis=1, ignore_index=True)

.. ipython:: python
   :suppress:

   @savefig merging_concat_series_ignore_index.png
   p.plot([df1, s1], result, labels=["df1", "s1"], vertical=False);
   plt.close("all");

More concatenating with group keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A fairly common use of the ``keys`` argument is to override the column names
when creating a new ``DataFrame`` based on existing ``Series``.
Notice how the default behaviour consists on letting the resulting ``DataFrame``
inherit the parent ``Series``' name, when these existed.

.. ipython:: python

   s3 = pd.Series([0, 1, 2, 3], name="foo")
   s4 = pd.Series([0, 1, 2, 3])
   s5 = pd.Series([0, 1, 4, 5])

   pd.concat([s3, s4, s5], axis=1)

Through the ``keys`` argument we can override the existing column names.

.. ipython:: python

   pd.concat([s3, s4, s5], axis=1, keys=["red", "blue", "yellow"])

Let's consider a variation of the very first example presented:

.. ipython:: python

   result = pd.concat(frames, keys=["x", "y", "z"])

.. ipython:: python
   :suppress:

   @savefig merging_concat_group_keys2.png
   p.plot(frames, result, labels=["df1", "df2", "df3"], vertical=True);
   plt.close("all");

You can also pass a dict to ``concat`` in which case the dict keys will be used
for the ``keys`` argument (unless other keys are specified):

.. ipython:: python

   pieces = {"x": df1, "y": df2, "z": df3}
   result = pd.concat(pieces)

.. ipython:: python
   :suppress:

   @savefig merging_concat_dict.png
   p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);
   plt.close("all");

.. ipython:: python

   result = pd.concat(pieces, keys=["z", "y"])

.. ipython:: python
   :suppress:

   @savefig merging_concat_dict_keys.png
   p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);
   plt.close("all");

The MultiIndex created has levels that are constructed from the passed keys and
the index of the ``DataFrame`` pieces:

.. ipython:: python

   result.index.levels

If you wish to specify other levels (as will occasionally be the case), you can
do so using the ``levels`` argument:

.. ipython:: python

   result = pd.concat(
       pieces, keys=["x", "y", "z"], levels=[["z", "y", "x", "w"]], names=["group_key"]
   )

.. ipython:: python
   :suppress:

   @savefig merging_concat_dict_keys_names.png
   p.plot([df1, df2, df3], result, labels=["df1", "df2", "df3"], vertical=True);
   plt.close("all");

.. ipython:: python

   result.index.levels

This is fairly esoteric, but it is actually necessary for implementing things
like GroupBy where the order of a categorical variable is meaningful.

.. _merging.append.row:

Appending rows to a DataFrame
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have a series that you want to append as a single row to a ``DataFrame``, you can convert the row into a
``DataFrame`` and use ``concat``

.. ipython:: python

   s2 = pd.Series(["X0", "X1", "X2", "X3"], index=["A", "B", "C", "D"])
   result = pd.concat([df1, s2.to_frame().T], ignore_index=True)

.. ipython:: python
   :suppress:

   @savefig merging_append_series_as_row.png
   p.plot([df1, s2], result, labels=["df1", "s2"], vertical=True);
   plt.close("all");

You should use ``ignore_index`` with this method to instruct DataFrame to
discard its index. If you wish to preserve the index, you should construct an
appropriately-indexed DataFrame and append or concatenate those objects.

.. _merging.join:

Database-style DataFrame or named Series joining/merging
--------------------------------------------------------

pandas has full-featured, **high performance** in-memory join operations
idiomatically very similar to relational databases like SQL. These methods
perform significantly better (in some cases well over an order of magnitude
better) than other open source implementations (like ``base::merge.data.frame``
in R). The reason for this is careful algorithmic design and the internal layout
of the data in ``DataFrame``.

See the :ref:`cookbook<cookbook.merge>` for some advanced strategies.

Users who are familiar with SQL but new to pandas might be interested in a
:ref:`comparison with SQL<compare_with_sql.join>`.

pandas provides a single function, :func:`~pandas.merge`, as the entry point for
all standard database join operations between ``DataFrame`` or named ``Series`` objects:

::

    pd.merge(
        left,
        right,
        how="inner",
        on=None,
        left_on=None,
        right_on=None,
        left_index=False,
        right_index=False,
        sort=True,
        suffixes=("_x", "_y"),
        copy=True,
        indicator=False,
        validate=None,
    )

* ``left``: A DataFrame or named Series object.
* ``right``: Another DataFrame or named Series object.
* ``on``: Column or index level names to join on. Must be found in both the left
  and right DataFrame and/or Series objects. If not passed and ``left_index`` and
  ``right_index`` are ``False``, the intersection of the columns in the
  DataFrames and/or Series will be inferred to be the join keys.
* ``left_on``: Columns or index levels from the left DataFrame or Series to use as
  keys. Can either be column names, index level names, or arrays with length
  equal to the length of the DataFrame or Series.
* ``right_on``: Columns or index levels from the right DataFrame or Series to use as
  keys. Can either be column names, index level names, or arrays with length
  equal to the length of the DataFrame or Series.
* ``left_index``: If ``True``, use the index (row labels) from the left
  DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex
  (hierarchical), the number of levels must match the number of join keys
  from the right DataFrame or Series.
* ``right_index``: Same usage as ``left_index`` for the right DataFrame or Series
* ``how``: One of ``'left'``, ``'right'``, ``'outer'``, ``'inner'``, ``'cross'``. Defaults
  to ``inner``. See below for more detailed description of each method.
* ``sort``: Sort the result DataFrame by the join keys in lexicographical
  order. Defaults to ``True``, setting to ``False`` will improve performance
  substantially in many cases.
* ``suffixes``: A tuple of string suffixes to apply to overlapping
  columns. Defaults to ``('_x', '_y')``.
* ``copy``: Always copy data (default ``True``) from the passed DataFrame or named Series
  objects, even when reindexing is not necessary. Cannot be avoided in many
  cases but may improve performance / memory usage. The cases where copying
  can be avoided are somewhat pathological but this option is provided
  nonetheless.
* ``indicator``: Add a column to the output DataFrame called ``_merge``
  with information on the source of each row. ``_merge`` is Categorical-type
  and takes on a value of ``left_only`` for observations whose merge key
  only appears in ``'left'`` DataFrame or Series, ``right_only`` for observations whose
  merge key only appears in ``'right'`` DataFrame or Series, and ``both`` if the
  observation's merge key is found in both.

* ``validate`` : string, default None.
  If specified, checks if merge is of specified type.

    * "one_to_one" or "1:1": checks if merge keys are unique in both
      left and right datasets.
    * "one_to_many" or "1:m": checks if merge keys are unique in left
      dataset.
    * "many_to_one" or "m:1": checks if merge keys are unique in right
      dataset.
    * "many_to_many" or "m:m": allowed, but does not result in checks.

.. note::

   Support for specifying index levels as the ``on``, ``left_on``, and
   ``right_on`` parameters was added in version 0.23.0.
   Support for merging named ``Series`` objects was added in version 0.24.0.

The return type will be the same as ``left``. If ``left`` is a ``DataFrame`` or named ``Series``
and ``right`` is a subclass of ``DataFrame``, the return type will still be ``DataFrame``.

``merge`` is a function in the pandas namespace, and it is also available as a
``DataFrame`` instance method :meth:`~DataFrame.merge`, with the calling
``DataFrame`` being implicitly considered the left object in the join.

The related :meth:`~DataFrame.join` method, uses ``merge`` internally for the
index-on-index (by default) and column(s)-on-index join. If you are joining on
index only, you may wish to use ``DataFrame.join`` to save yourself some typing.

Brief primer on merge methods (relational algebra)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Experienced users of relational databases like SQL will be familiar with the
terminology used to describe join operations between two SQL-table like
structures (``DataFrame`` objects). There are several cases to consider which
are very important to understand:

* **one-to-one** joins: for example when joining two ``DataFrame`` objects on
  their indexes (which must contain unique values).
* **many-to-one** joins: for example when joining an index (unique) to one or
  more columns in a different ``DataFrame``.
* **many-to-many** joins: joining columns on columns.

.. note::

   When joining columns on columns (potentially a many-to-many join), any
   indexes on the passed ``DataFrame`` objects **will be discarded**.


It is worth spending some time understanding the result of the **many-to-many**
join case. In SQL / standard relational algebra, if a key combination appears
more than once in both tables, the resulting table will have the **Cartesian
product** of the associated data. Here is a very basic example with one unique
key combination:

.. ipython:: python

   left = pd.DataFrame(
       {
           "key": ["K0", "K1", "K2", "K3"],
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
       }
   )

   right = pd.DataFrame(
       {
           "key": ["K0", "K1", "K2", "K3"],
           "C": ["C0", "C1", "C2", "C3"],
           "D": ["D0", "D1", "D2", "D3"],
       }
   )
   result = pd.merge(left, right, on="key")

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

Here is a more complicated example with multiple join keys. Only the keys
appearing in ``left`` and ``right`` are present (the intersection), since
``how='inner'`` by default.

.. ipython:: python

   left = pd.DataFrame(
       {
           "key1": ["K0", "K0", "K1", "K2"],
           "key2": ["K0", "K1", "K0", "K1"],
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
       }
   )

   right = pd.DataFrame(
       {
           "key1": ["K0", "K1", "K1", "K2"],
           "key2": ["K0", "K0", "K0", "K0"],
           "C": ["C0", "C1", "C2", "C3"],
           "D": ["D0", "D1", "D2", "D3"],
       }
   )

   result = pd.merge(left, right, on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_multiple.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

The ``how`` argument to ``merge`` specifies how to determine which keys are to
be included in the resulting table. If a key combination **does not appear** in
either the left or right tables, the values in the joined table will be
``NA``. Here is a summary of the ``how`` options and their SQL equivalent names:

.. csv-table::
    :header: "Merge method", "SQL Join Name", "Description"
    :widths: 20, 20, 60

    ``left``, ``LEFT OUTER JOIN``, Use keys from left frame only
    ``right``, ``RIGHT OUTER JOIN``, Use keys from right frame only
    ``outer``, ``FULL OUTER JOIN``, Use union of keys from both frames
    ``inner``, ``INNER JOIN``, Use intersection of keys from both frames
    ``cross``, ``CROSS JOIN``, Create the cartesian product of rows of both frames

.. ipython:: python

   result = pd.merge(left, right, how="left", on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_left.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(left, right, how="right", on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_right.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);

.. ipython:: python

   result = pd.merge(left, right, how="outer", on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_outer.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(left, right, how="inner", on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_inner.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(left, right, how="cross")

.. ipython:: python
   :suppress:

   @savefig merging_merge_cross.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

You can merge a mult-indexed Series and a DataFrame, if the names of
the MultiIndex correspond to the columns from the DataFrame. Transform
the Series to a DataFrame using :meth:`Series.reset_index` before merging,
as shown in the following example.

.. ipython:: python

   df = pd.DataFrame({"Let": ["A", "B", "C"], "Num": [1, 2, 3]})
   df

   ser = pd.Series(
       ["a", "b", "c", "d", "e", "f"],
       index=pd.MultiIndex.from_arrays(
           [["A", "B", "C"] * 2, [1, 2, 3, 4, 5, 6]], names=["Let", "Num"]
       ),
   )
   ser

   pd.merge(df, ser.reset_index(), on=["Let", "Num"])


Here is another example with duplicate join keys in DataFrames:

.. ipython:: python

   left = pd.DataFrame({"A": [1, 2], "B": [2, 2]})

   right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})

   result = pd.merge(left, right, on="B", how="outer")

.. ipython:: python
   :suppress:

   @savefig merging_merge_on_key_dup.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");


.. warning::

  Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, which may result in memory overflow. It is the user' s responsibility to manage duplicate values in keys before joining large DataFrames.

.. _merging.validation:

Checking for duplicate keys
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Users can use the ``validate`` argument to automatically check whether there
are unexpected duplicates in their merge keys. Key uniqueness is checked before
merge operations and so should protect against memory overflows. Checking key
uniqueness is also a good way to ensure user data structures are as expected.

In the following example, there are duplicate values of ``B`` in the right
``DataFrame``. As this is not a one-to-one merge -- as specified in the
``validate`` argument -- an exception will be raised.


.. ipython:: python

  left = pd.DataFrame({"A": [1, 2], "B": [1, 2]})
  right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})

.. code-block:: ipython

  In [53]: result = pd.merge(left, right, on="B", how="outer", validate="one_to_one")
  ...
  MergeError: Merge keys are not unique in right dataset; not a one-to-one merge

If the user is aware of the duplicates in the right ``DataFrame`` but wants to
ensure there are no duplicates in the left DataFrame, one can use the
``validate='one_to_many'`` argument instead, which will not raise an exception.

.. ipython:: python

   pd.merge(left, right, on="B", how="outer", validate="one_to_many")


.. _merging.indicator:

The merge indicator
~~~~~~~~~~~~~~~~~~~

:func:`~pandas.merge` accepts the argument ``indicator``. If ``True``, a
Categorical-type column called ``_merge`` will be added to the output object
that takes on values:

  ===================================   ================
  Observation Origin                    ``_merge`` value
  ===================================   ================
  Merge key only in ``'left'`` frame    ``left_only``
  Merge key only in ``'right'`` frame   ``right_only``
  Merge key in both frames              ``both``
  ===================================   ================

.. ipython:: python

   df1 = pd.DataFrame({"col1": [0, 1], "col_left": ["a", "b"]})
   df2 = pd.DataFrame({"col1": [1, 2, 2], "col_right": [2, 2, 2]})
   pd.merge(df1, df2, on="col1", how="outer", indicator=True)

The ``indicator`` argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column.

.. ipython:: python

   pd.merge(df1, df2, on="col1", how="outer", indicator="indicator_column")


.. _merging.dtypes:

Merge dtypes
~~~~~~~~~~~~

Merging will preserve the dtype of the join keys.

.. ipython:: python

   left = pd.DataFrame({"key": [1], "v1": [10]})
   left
   right = pd.DataFrame({"key": [1, 2], "v1": [20, 30]})
   right

We are able to preserve the join keys:

.. ipython:: python

   pd.merge(left, right, how="outer")
   pd.merge(left, right, how="outer").dtypes

Of course if you have missing values that are introduced, then the
resulting dtype will be upcast.

.. ipython:: python

   pd.merge(left, right, how="outer", on="key")
   pd.merge(left, right, how="outer", on="key").dtypes

Merging will preserve ``category`` dtypes of the mergands. See also the section on :ref:`categoricals <categorical.merge>`.

The left frame.

.. ipython:: python

   from pandas.api.types import CategoricalDtype

   X = pd.Series(np.random.choice(["foo", "bar"], size=(10,)))
   X = X.astype(CategoricalDtype(categories=["foo", "bar"]))

   left = pd.DataFrame(
       {"X": X, "Y": np.random.choice(["one", "two", "three"], size=(10,))}
   )
   left
   left.dtypes

The right frame.

.. ipython:: python

   right = pd.DataFrame(
       {
           "X": pd.Series(["foo", "bar"], dtype=CategoricalDtype(["foo", "bar"])),
           "Z": [1, 2],
       }
   )
   right
   right.dtypes

The merged result:

.. ipython:: python

   result = pd.merge(left, right, how="outer")
   result
   result.dtypes

.. note::

   The category dtypes must be *exactly* the same, meaning the same categories and the ordered attribute.
   Otherwise the result will coerce to the categories' dtype.

.. note::

   Merging on ``category`` dtypes that are the same can be quite performant compared to ``object`` dtype merging.

.. _merging.join.index:

Joining on index
~~~~~~~~~~~~~~~~

:meth:`DataFrame.join` is a convenient method for combining the columns of two
potentially differently-indexed ``DataFrames`` into a single result
``DataFrame``. Here is a very basic example:

.. ipython:: python

   left = pd.DataFrame(
       {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]}, index=["K0", "K1", "K2"]
   )

   right = pd.DataFrame(
       {"C": ["C0", "C2", "C3"], "D": ["D0", "D2", "D3"]}, index=["K0", "K2", "K3"]
   )

   result = left.join(right)

.. ipython:: python
   :suppress:

   @savefig merging_join.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = left.join(right, how="outer")

.. ipython:: python
   :suppress:

   @savefig merging_join_outer.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

The same as above, but with ``how='inner'``.

.. ipython:: python

   result = left.join(right, how="inner")

.. ipython:: python
   :suppress:

   @savefig merging_join_inner.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

The data alignment here is on the indexes (row labels). This same behavior can
be achieved using ``merge`` plus additional arguments instructing it to use the
indexes:

.. ipython:: python

   result = pd.merge(left, right, left_index=True, right_index=True, how="outer")

.. ipython:: python
   :suppress:

   @savefig merging_merge_index_outer.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(left, right, left_index=True, right_index=True, how="inner")

.. ipython:: python
   :suppress:

   @savefig merging_merge_index_inner.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

Joining key columns on an index
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:meth:`~DataFrame.join` takes an optional ``on`` argument which may be a column
or multiple column names, which specifies that the passed ``DataFrame`` is to be
aligned on that column in the ``DataFrame``. These two function calls are
completely equivalent:

::

    left.join(right, on=key_or_keys)
    pd.merge(
        left, right, left_on=key_or_keys, right_index=True, how="left", sort=False
    )

Obviously you can choose whichever form you find more convenient. For
many-to-one joins (where one of the ``DataFrame``'s is already indexed by the
join key), using ``join`` may be more convenient. Here is a simple example:

.. ipython:: python

   left = pd.DataFrame(
       {
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
           "key": ["K0", "K1", "K0", "K1"],
       }
   )

   right = pd.DataFrame({"C": ["C0", "C1"], "D": ["D0", "D1"]}, index=["K0", "K1"])

   result = left.join(right, on="key")

.. ipython:: python
   :suppress:

   @savefig merging_join_key_columns.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(
       left, right, left_on="key", right_index=True, how="left", sort=False
   )

.. ipython:: python
   :suppress:

   @savefig merging_merge_key_columns.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. _merging.multikey_join:

To join on multiple keys, the passed DataFrame must have a ``MultiIndex``:

.. ipython:: python

   left = pd.DataFrame(
       {
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
           "key1": ["K0", "K0", "K1", "K2"],
           "key2": ["K0", "K1", "K0", "K1"],
       }
   )

   index = pd.MultiIndex.from_tuples(
       [("K0", "K0"), ("K1", "K0"), ("K2", "K0"), ("K2", "K1")]
   )
   right = pd.DataFrame(
       {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]}, index=index
   )

Now this can be joined by passing the two key column names:

.. ipython:: python

   result = left.join(right, on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merging_join_multikeys.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. _merging.df_inner_join:

The default for ``DataFrame.join`` is to perform a left join (essentially a
"VLOOKUP" operation, for Excel users), which uses only the keys found in the
calling DataFrame. Other join types, for example inner join, can be just as
easily performed:

.. ipython:: python

   result = left.join(right, on=["key1", "key2"], how="inner")

.. ipython:: python
   :suppress:

   @savefig merging_join_multikeys_inner.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

As you can see, this drops any rows where there was no match.

.. _merging.join_on_mi:

Joining a single Index to a MultiIndex
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can join a singly-indexed ``DataFrame`` with a level of a MultiIndexed ``DataFrame``.
The level will match on the name of the index of the singly-indexed frame against
a level name of the MultiIndexed frame.

..  ipython:: python

    left = pd.DataFrame(
        {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]},
        index=pd.Index(["K0", "K1", "K2"], name="key"),
    )

    index = pd.MultiIndex.from_tuples(
        [("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")],
        names=["key", "Y"],
    )
    right = pd.DataFrame(
        {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]},
        index=index,
    )

    result = left.join(right, how="inner")


.. ipython:: python
   :suppress:

   @savefig merging_join_multiindex_inner.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

This is equivalent but less verbose and more memory efficient / faster than this.

..  ipython:: python

    result = pd.merge(
        left.reset_index(), right.reset_index(), on=["key"], how="inner"
    ).set_index(["key","Y"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_multiindex_alternative.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. _merging.join_with_two_multi_indexes:

Joining with two MultiIndexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is supported in a limited way, provided that the index for the right
argument is completely used in the join, and is a subset of the indices in
the left argument, as in this example:

.. ipython:: python

   leftindex = pd.MultiIndex.from_product(
       [list("abc"), list("xy"), [1, 2]], names=["abc", "xy", "num"]
   )
   left = pd.DataFrame({"v1": range(12)}, index=leftindex)
   left

   rightindex = pd.MultiIndex.from_product(
       [list("abc"), list("xy")], names=["abc", "xy"]
   )
   right = pd.DataFrame({"v2": [100 * i for i in range(1, 7)]}, index=rightindex)
   right

   left.join(right, on=["abc", "xy"], how="inner")

If that condition is not satisfied, a join with two multi-indexes can be
done using the following code.

.. ipython:: python

   leftindex = pd.MultiIndex.from_tuples(
       [("K0", "X0"), ("K0", "X1"), ("K1", "X2")], names=["key", "X"]
   )
   left = pd.DataFrame(
       {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]}, index=leftindex
   )

   rightindex = pd.MultiIndex.from_tuples(
       [("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")], names=["key", "Y"]
   )
   right = pd.DataFrame(
       {"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]}, index=rightindex
   )

   result = pd.merge(
       left.reset_index(), right.reset_index(), on=["key"], how="inner"
   ).set_index(["key", "X", "Y"])

.. ipython:: python
   :suppress:

   @savefig merging_merge_two_multiindex.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. _merging.merge_on_columns_and_levels:

Merging on a combination of columns and index levels
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Strings passed as the ``on``, ``left_on``, and ``right_on`` parameters
may refer to either column names or index level names.  This enables merging
``DataFrame`` instances on a combination of index levels and columns without
resetting indexes.

.. ipython:: python

   left_index = pd.Index(["K0", "K0", "K1", "K2"], name="key1")

   left = pd.DataFrame(
       {
           "A": ["A0", "A1", "A2", "A3"],
           "B": ["B0", "B1", "B2", "B3"],
           "key2": ["K0", "K1", "K0", "K1"],
       },
       index=left_index,
   )

   right_index = pd.Index(["K0", "K1", "K2", "K2"], name="key1")

   right = pd.DataFrame(
       {
           "C": ["C0", "C1", "C2", "C3"],
           "D": ["D0", "D1", "D2", "D3"],
           "key2": ["K0", "K0", "K0", "K1"],
       },
       index=right_index,
   )

   result = left.merge(right, on=["key1", "key2"])

.. ipython:: python
   :suppress:

   @savefig merge_on_index_and_column.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. note::

   When DataFrames are merged on a string that matches an index level in both
   frames, the index level is preserved as an index level in the resulting
   DataFrame.

.. note::
   When DataFrames are merged using only some of the levels of a ``MultiIndex``,
   the extra levels will be dropped from the resulting merge. In order to
   preserve those levels, use ``reset_index`` on those level names to move
   those levels to columns prior to doing the merge.

.. note::

   If a string matches both a column name and an index level name, then a
   warning is issued and the column takes precedence. This will result in an
   ambiguity error in a future version.

Overlapping value columns
~~~~~~~~~~~~~~~~~~~~~~~~~

The merge ``suffixes`` argument takes a tuple of list of strings to append to
overlapping column names in the input ``DataFrame``\ s to disambiguate the result
columns:

.. ipython:: python

   left = pd.DataFrame({"k": ["K0", "K1", "K2"], "v": [1, 2, 3]})
   right = pd.DataFrame({"k": ["K0", "K0", "K3"], "v": [4, 5, 6]})

   result = pd.merge(left, right, on="k")

.. ipython:: python
   :suppress:

   @savefig merging_merge_overlapped.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. ipython:: python

   result = pd.merge(left, right, on="k", suffixes=("_l", "_r"))

.. ipython:: python
   :suppress:

   @savefig merging_merge_overlapped_suffix.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

:meth:`DataFrame.join` has ``lsuffix`` and ``rsuffix`` arguments which behave
similarly.

.. ipython:: python

   left = left.set_index("k")
   right = right.set_index("k")
   result = left.join(right, lsuffix="_l", rsuffix="_r")

.. ipython:: python
   :suppress:

   @savefig merging_merge_overlapped_multi_suffix.png
   p.plot([left, right], result, labels=["left", "right"], vertical=False);
   plt.close("all");

.. _merging.multiple_join:

Joining multiple DataFrames
~~~~~~~~~~~~~~~~~~~~~~~~~~~

A list or tuple of ``DataFrames`` can also be passed to :meth:`~DataFrame.join`
to join them together on their indexes.

.. ipython:: python

   right2 = pd.DataFrame({"v": [7, 8, 9]}, index=["K1", "K1", "K2"])
   result = left.join([right, right2])

.. ipython:: python
   :suppress:

   @savefig merging_join_multi_df.png
   p.plot(
       [left, right, right2],
       result,
       labels=["left", "right", "right2"],
       vertical=False,
   );
   plt.close("all");

.. _merging.combine_first.update:

Merging together values within Series or DataFrame columns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Another fairly common situation is to have two like-indexed (or similarly
indexed) ``Series`` or ``DataFrame`` objects and wanting to "patch" values in
one object from values for matching indices in the other. Here is an example:

.. ipython:: python

   df1 = pd.DataFrame(
       [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]]
   )
   df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2])

For this, use the :meth:`~DataFrame.combine_first` method:

.. ipython:: python

   result = df1.combine_first(df2)

.. ipython:: python
   :suppress:

   @savefig merging_combine_first.png
   p.plot([df1, df2], result, labels=["df1", "df2"], vertical=False);
   plt.close("all");

Note that this method only takes values from the right ``DataFrame`` if they are
missing in the left ``DataFrame``. A related method, :meth:`~DataFrame.update`,
alters non-NA values in place:

.. ipython:: python
   :suppress:

   df1_copy = df1.copy()

.. ipython:: python

   df1.update(df2)

.. ipython:: python
   :suppress:

   @savefig merging_update.png
   p.plot([df1_copy, df2], df1, labels=["df1", "df2"], vertical=False);
   plt.close("all");

.. _merging.time_series:

Timeseries friendly merging
---------------------------

.. _merging.merge_ordered:

Merging ordered data
~~~~~~~~~~~~~~~~~~~~

A :func:`merge_ordered` function allows combining time series and other
ordered data. In particular it has an optional ``fill_method`` keyword to
fill/interpolate missing data:

.. ipython:: python

   left = pd.DataFrame(
       {"k": ["K0", "K1", "K1", "K2"], "lv": [1, 2, 3, 4], "s": ["a", "b", "c", "d"]}
   )

   right = pd.DataFrame({"k": ["K1", "K2", "K4"], "rv": [1, 2, 3]})

   pd.merge_ordered(left, right, fill_method="ffill", left_by="s")

.. _merging.merge_asof:

Merging asof
~~~~~~~~~~~~

A :func:`merge_asof` is similar to an ordered left-join except that we match on
nearest key rather than equal keys. For each row in the ``left`` ``DataFrame``,
we select the last row in the ``right`` ``DataFrame`` whose ``on`` key is less
than the left's key. Both DataFrames must be sorted by the key.

Optionally an asof merge can perform a group-wise merge. This matches the
``by`` key equally, in addition to the nearest match on the ``on`` key.

For example; we might have ``trades`` and ``quotes`` and we want to ``asof``
merge them.

.. ipython:: python

   trades = pd.DataFrame(
       {
           "time": pd.to_datetime(
               [
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.038",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.048",
               ]
           ),
           "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],
           "price": [51.95, 51.95, 720.77, 720.92, 98.00],
           "quantity": [75, 155, 100, 100, 100],
       },
       columns=["time", "ticker", "price", "quantity"],
   )

   quotes = pd.DataFrame(
       {
           "time": pd.to_datetime(
               [
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.023",
                   "20160525 13:30:00.030",
                   "20160525 13:30:00.041",
                   "20160525 13:30:00.048",
                   "20160525 13:30:00.049",
                   "20160525 13:30:00.072",
                   "20160525 13:30:00.075",
               ]
           ),
           "ticker": ["GOOG", "MSFT", "MSFT", "MSFT", "GOOG", "AAPL", "GOOG", "MSFT"],
           "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
           "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],
       },
       columns=["time", "ticker", "bid", "ask"],
   )

.. ipython:: python

   trades
   quotes

By default we are taking the asof of the quotes.

.. ipython:: python

   pd.merge_asof(trades, quotes, on="time", by="ticker")

We only asof within ``2ms`` between the quote time and the trade time.

.. ipython:: python

   pd.merge_asof(trades, quotes, on="time", by="ticker", tolerance=pd.Timedelta("2ms"))

We only asof within ``10ms`` between the quote time and the trade time and we
exclude exact matches on time. Note that though we exclude the exact matches
(of the quotes), prior quotes **do** propagate to that point in time.

.. ipython:: python

   pd.merge_asof(
       trades,
       quotes,
       on="time",
       by="ticker",
       tolerance=pd.Timedelta("10ms"),
       allow_exact_matches=False,
   )

.. _merging.compare:

Comparing objects
-----------------

The :meth:`~Series.compare` and :meth:`~DataFrame.compare` methods allow you to
compare two DataFrame or Series, respectively, and summarize their differences.

This feature was added in :ref:`V1.1.0 <whatsnew_110.dataframe_or_series_comparing>`.

For example, you might want to compare two ``DataFrame`` and stack their differences
side by side.

.. ipython:: python

   df = pd.DataFrame(
       {
           "col1": ["a", "a", "b", "b", "a"],
           "col2": [1.0, 2.0, 3.0, np.nan, 5.0],
           "col3": [1.0, 2.0, 3.0, 4.0, 5.0],
       },
       columns=["col1", "col2", "col3"],
   )
   df

.. ipython:: python

   df2 = df.copy()
   df2.loc[0, "col1"] = "c"
   df2.loc[2, "col3"] = 4.0
   df2

.. ipython:: python

   df.compare(df2)

By default, if two corresponding values are equal, they will be shown as ``NaN``.
Furthermore, if all values in an entire row / column, the row / column will be
omitted from the result. The remaining differences will be aligned on columns.

If you wish, you may choose to stack the differences on rows.

.. ipython:: python

   df.compare(df2, align_axis=0)

If you wish to keep all original rows and columns, set ``keep_shape`` argument
to ``True``.

.. ipython:: python

   df.compare(df2, keep_shape=True)

You may also keep all the original values even if they are equal.

.. ipython:: python

   df.compare(df2, keep_shape=True, keep_equal=True)
.. _missing_data:

{{ header }}

*************************
Working with missing data
*************************

In this section, we will discuss missing (also referred to as NA) values in
pandas.

.. note::

    The choice of using ``NaN`` internally to denote missing data was largely
    for simplicity and performance reasons.
    Starting from pandas 1.0, some optional data types start experimenting
    with a native ``NA`` scalar using a mask-based approach. See
    :ref:`here <missing_data.NA>` for more.

See the :ref:`cookbook<cookbook.missing_data>` for some advanced strategies.

Values considered "missing"
~~~~~~~~~~~~~~~~~~~~~~~~~~~

As data comes in many shapes and forms, pandas aims to be flexible with regard
to handling missing data. While ``NaN`` is the default missing value marker for
reasons of computational speed and convenience, we need to be able to easily
detect this value with data of different types: floating point, integer,
boolean, and general object. In many cases, however, the Python ``None`` will
arise and we wish to also consider that "missing" or "not available" or "NA".

.. note::

   If you want to consider ``inf`` and ``-inf`` to be "NA" in computations,
   you can set ``pandas.options.mode.use_inf_as_na = True``.

.. _missing.isna:

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(5, 3),
       index=["a", "c", "e", "f", "h"],
       columns=["one", "two", "three"],
   )
   df["four"] = "bar"
   df["five"] = df["one"] > 0
   df
   df2 = df.reindex(["a", "b", "c", "d", "e", "f", "g", "h"])
   df2

To make detecting missing values easier (and across different array dtypes),
pandas provides the :func:`isna` and
:func:`notna` functions, which are also methods on
Series and DataFrame objects:

.. ipython:: python

   df2["one"]
   pd.isna(df2["one"])
   df2["four"].notna()
   df2.isna()

.. warning::

   One has to be mindful that in Python (and NumPy), the ``nan's`` don't compare equal, but ``None's`` **do**.
   Note that pandas/NumPy uses the fact that ``np.nan != np.nan``, and treats ``None`` like ``np.nan``.

   .. ipython:: python

      None == None  # noqa: E711
      np.nan == np.nan

   So as compared to above, a scalar equality comparison versus a ``None/np.nan`` doesn't provide useful information.

   .. ipython:: python

      df2["one"] == np.nan

Integer dtypes and missing data
-------------------------------

Because ``NaN`` is a float, a column of integers with even one missing values
is cast to floating-point dtype (see :ref:`gotchas.intna` for more). pandas
provides a nullable integer array, which can be used by explicitly requesting
the dtype:

.. ipython:: python

   pd.Series([1, 2, np.nan, 4], dtype=pd.Int64Dtype())

Alternatively, the string alias ``dtype='Int64'`` (note the capital ``"I"``) can be
used.

See :ref:`integer_na` for more.

Datetimes
---------

For datetime64[ns] types, ``NaT`` represents missing values. This is a pseudo-native
sentinel value that can be represented by NumPy in a singular dtype (datetime64[ns]).
pandas objects provide compatibility between ``NaT`` and ``NaN``.

.. ipython:: python

   df2 = df.copy()
   df2["timestamp"] = pd.Timestamp("20120101")
   df2
   df2.loc[["a", "c", "h"], ["one", "timestamp"]] = np.nan
   df2
   df2.dtypes.value_counts()

.. _missing.inserting:

Inserting missing data
~~~~~~~~~~~~~~~~~~~~~~

You can insert missing values by simply assigning to containers. The
actual missing value used will be chosen based on the dtype.

For example, numeric containers will always use ``NaN`` regardless of
the missing value type chosen:

.. ipython:: python

   s = pd.Series([1, 2, 3])
   s.loc[0] = None
   s

Likewise, datetime containers will always use ``NaT``.

For object containers, pandas will use the value given:

.. ipython:: python

   s = pd.Series(["a", "b", "c"])
   s.loc[0] = None
   s.loc[1] = np.nan
   s

.. _missing_data.calculations:

Calculations with missing data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Missing values propagate naturally through arithmetic operations between pandas
objects.

.. ipython:: python
   :suppress:

   df = df2.loc[:, ["one", "two", "three"]]
   a = df2.loc[df2.index[:5], ["one", "two"]].fillna(method="pad")
   b = df2.loc[df2.index[:5], ["one", "two", "three"]]

.. ipython:: python

   a
   b
   a + b

The descriptive statistics and computational methods discussed in the
:ref:`data structure overview <basics.stats>` (and listed :ref:`here
<api.series.stats>` and :ref:`here <api.dataframe.stats>`) are all written to
account for missing data. For example:

* When summing data, NA (missing) values will be treated as zero.
* If the data are all NA, the result will be 0.
* Cumulative methods like :meth:`~DataFrame.cumsum` and :meth:`~DataFrame.cumprod` ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use ``skipna=False``.

.. ipython:: python

   df
   df["one"].sum()
   df.mean(1)
   df.cumsum()
   df.cumsum(skipna=False)


.. _missing_data.numeric_sum:

Sum/prod of empties/nans
~~~~~~~~~~~~~~~~~~~~~~~~

.. warning::

   This behavior is now standard as of v0.22.0 and is consistent with the default in ``numpy``; previously sum/prod of all-NA or empty Series/DataFrames would return NaN.
   See :ref:`v0.22.0 whatsnew <whatsnew_0220>` for more.

The sum of an empty or all-NA Series or column of a DataFrame is 0.

.. ipython:: python

   pd.Series([np.nan]).sum()

   pd.Series([], dtype="float64").sum()

The product of an empty or all-NA Series or column of a DataFrame is 1.

.. ipython:: python

   pd.Series([np.nan]).prod()

   pd.Series([], dtype="float64").prod()


NA values in GroupBy
~~~~~~~~~~~~~~~~~~~~

NA groups in GroupBy are automatically excluded. This behavior is consistent
with R, for example:

.. ipython:: python

    df
    df.groupby("one").mean()

See the groupby section :ref:`here <groupby.missing>` for more information.

Cleaning / filling missing data
--------------------------------

pandas objects are equipped with various data manipulation methods for dealing
with missing data.

.. _missing_data.fillna:

Filling missing values: fillna
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:meth:`~DataFrame.fillna` can "fill in" NA values with non-NA data in a couple
of ways, which we illustrate:

**Replace NA with a scalar value**

.. ipython:: python

   df2
   df2.fillna(0)
   df2["one"].fillna("missing")

**Fill gaps forward or backward**

Using the same filling arguments as :ref:`reindexing <basics.reindexing>`, we
can propagate non-NA values forward or backward:

.. ipython:: python

   df
   df.fillna(method="pad")

.. _missing_data.fillna.limit:

**Limit the amount of filling**

If we only want consecutive gaps filled up to a certain number of data points,
we can use the ``limit`` keyword:

.. ipython:: python
   :suppress:

   df.iloc[2:4, :] = np.nan

.. ipython:: python

   df
   df.fillna(method="pad", limit=1)

To remind you, these are the available filling methods:

.. csv-table::
    :header: "Method", "Action"
    :widths: 30, 50

    pad / ffill, Fill values forward
    bfill / backfill, Fill values backward

With time series data, using pad/ffill is extremely common so that the "last
known value" is available at every time point.

:meth:`~DataFrame.ffill` is equivalent to ``fillna(method='ffill')``
and :meth:`~DataFrame.bfill` is equivalent to ``fillna(method='bfill')``

.. _missing_data.PandasObject:

Filling with a PandasObject
~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can also fillna using a dict or Series that is alignable. The labels of the dict or index of the Series
must match the columns of the frame you wish to fill. The
use case of this is to fill a DataFrame with the mean of that column.

.. ipython:: python

        dff = pd.DataFrame(np.random.randn(10, 3), columns=list("ABC"))
        dff.iloc[3:5, 0] = np.nan
        dff.iloc[4:6, 1] = np.nan
        dff.iloc[5:8, 2] = np.nan
        dff

        dff.fillna(dff.mean())
        dff.fillna(dff.mean()["B":"C"])

Same result as above, but is aligning the 'fill' value which is
a Series in this case.

.. ipython:: python

        dff.where(pd.notna(dff), dff.mean(), axis="columns")


.. _missing_data.dropna:

Dropping axis labels with missing data: dropna
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You may wish to simply exclude labels from a data set which refer to missing
data. To do this, use :meth:`~DataFrame.dropna`:

.. ipython:: python
   :suppress:

   df["two"] = df["two"].fillna(0)
   df["three"] = df["three"].fillna(0)

.. ipython:: python

   df
   df.dropna(axis=0)
   df.dropna(axis=1)
   df["one"].dropna()

An equivalent :meth:`~Series.dropna` is available for Series.
DataFrame.dropna has considerably more options than Series.dropna, which can be
examined :ref:`in the API <api.dataframe.missing>`.

.. _missing_data.interpolate:

Interpolation
~~~~~~~~~~~~~

Both Series and DataFrame objects have :meth:`~DataFrame.interpolate`
that, by default, performs linear interpolation at missing data points.

.. ipython:: python
   :suppress:

   np.random.seed(123456)
   idx = pd.date_range("1/1/2000", periods=100, freq="BM")
   ts = pd.Series(np.random.randn(100), index=idx)
   ts[1:5] = np.nan
   ts[20:30] = np.nan
   ts[60:80] = np.nan
   ts = ts.cumsum()

.. ipython:: python

   ts
   ts.count()
   @savefig series_before_interpolate.png
   ts.plot()

.. ipython:: python

   ts.interpolate()
   ts.interpolate().count()

   @savefig series_interpolate.png
   ts.interpolate().plot()

Index aware interpolation is available via the ``method`` keyword:

.. ipython:: python
   :suppress:

   ts2 = ts[[0, 1, 30, 60, 99]]

.. ipython:: python

   ts2
   ts2.interpolate()
   ts2.interpolate(method="time")

For a floating-point index, use ``method='values'``:

.. ipython:: python
   :suppress:

   idx = [0.0, 1.0, 10.0]
   ser = pd.Series([0.0, np.nan, 10.0], idx)

.. ipython:: python

   ser
   ser.interpolate()
   ser.interpolate(method="values")

You can also interpolate with a DataFrame:

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": [1, 2.1, np.nan, 4.7, 5.6, 6.8],
           "B": [0.25, np.nan, np.nan, 4, 12.2, 14.4],
       }
   )
   df
   df.interpolate()

The ``method`` argument gives access to fancier interpolation methods.
If you have scipy_ installed, you can pass the name of a 1-d interpolation routine to ``method``.
You'll want to consult the full scipy interpolation documentation_ and reference guide_ for details.
The appropriate interpolation method will depend on the type of data you are working with.

* If you are dealing with a time series that is growing at an increasing rate,
  ``method='quadratic'`` may be appropriate.
* If you have values approximating a cumulative distribution function,
  then ``method='pchip'`` should work well.
* To fill missing values with goal of smooth plotting, consider ``method='akima'``.

.. warning::

   These methods require ``scipy``.

.. ipython:: python

   df.interpolate(method="barycentric")

   df.interpolate(method="pchip")

   df.interpolate(method="akima")

When interpolating via a polynomial or spline approximation, you must also specify
the degree or order of the approximation:

.. ipython:: python

   df.interpolate(method="spline", order=2)

   df.interpolate(method="polynomial", order=2)

Compare several methods:

.. ipython:: python

   np.random.seed(2)

   ser = pd.Series(np.arange(1, 10.1, 0.25) ** 2 + np.random.randn(37))
   missing = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])
   ser[missing] = np.nan
   methods = ["linear", "quadratic", "cubic"]

   df = pd.DataFrame({m: ser.interpolate(method=m) for m in methods})
   @savefig compare_interpolations.png
   df.plot()

Another use case is interpolation at *new* values.
Suppose you have 100 observations from some distribution. And let's suppose
that you're particularly interested in what's happening around the middle.
You can mix pandas' ``reindex`` and ``interpolate`` methods to interpolate
at the new values.

.. ipython:: python

   ser = pd.Series(np.sort(np.random.uniform(size=100)))

   # interpolate at new_index
   new_index = ser.index.union(pd.Index([49.25, 49.5, 49.75, 50.25, 50.5, 50.75]))
   interp_s = ser.reindex(new_index).interpolate(method="pchip")
   interp_s[49:51]

.. _scipy: https://scipy.org/
.. _documentation: https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation
.. _guide: https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html

.. _missing_data.interp_limits:

Interpolation limits
--------------------

Like other pandas fill methods, :meth:`~DataFrame.interpolate` accepts a ``limit`` keyword
argument. Use this argument to limit the number of consecutive ``NaN`` values
filled since the last valid observation:

.. ipython:: python

   ser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13, np.nan, np.nan])
   ser

   # fill all consecutive values in a forward direction
   ser.interpolate()

   # fill one consecutive value in a forward direction
   ser.interpolate(limit=1)

By default, ``NaN`` values are filled in a ``forward`` direction. Use
``limit_direction`` parameter to fill ``backward`` or from ``both`` directions.

.. ipython:: python

   # fill one consecutive value backwards
   ser.interpolate(limit=1, limit_direction="backward")

   # fill one consecutive value in both directions
   ser.interpolate(limit=1, limit_direction="both")

   # fill all consecutive values in both directions
   ser.interpolate(limit_direction="both")

By default, ``NaN`` values are filled whether they are inside (surrounded by)
existing valid values, or outside existing valid values. The ``limit_area``
parameter restricts filling to either inside or outside values.

.. ipython:: python

   # fill one consecutive inside value in both directions
   ser.interpolate(limit_direction="both", limit_area="inside", limit=1)

   # fill all consecutive outside values backward
   ser.interpolate(limit_direction="backward", limit_area="outside")

   # fill all consecutive outside values in both directions
   ser.interpolate(limit_direction="both", limit_area="outside")

.. _missing_data.replace:

Replacing generic values
~~~~~~~~~~~~~~~~~~~~~~~~
Often times we want to replace arbitrary values with other values.

:meth:`~Series.replace` in Series and :meth:`~DataFrame.replace` in DataFrame provides an efficient yet
flexible way to perform such replacements.

For a Series, you can replace a single value or a list of values by another
value:

.. ipython:: python

   ser = pd.Series([0.0, 1.0, 2.0, 3.0, 4.0])

   ser.replace(0, 5)

You can replace a list of values by a list of other values:

.. ipython:: python

   ser.replace([0, 1, 2, 3, 4], [4, 3, 2, 1, 0])

You can also specify a mapping dict:

.. ipython:: python

   ser.replace({0: 10, 1: 100})

For a DataFrame, you can specify individual values by column:

.. ipython:: python

   df = pd.DataFrame({"a": [0, 1, 2, 3, 4], "b": [5, 6, 7, 8, 9]})

   df.replace({"a": 0, "b": 5}, 100)

Instead of replacing with specified values, you can treat all given values as
missing and interpolate over them:

.. ipython:: python

   ser.replace([1, 2, 3], method="pad")

.. _missing_data.replace_expression:

String/regular expression replacement
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

   Python strings prefixed with the ``r`` character such as ``r'hello world'``
   are so-called "raw" strings. They have different semantics regarding
   backslashes than strings without this prefix. Backslashes in raw strings
   will be interpreted as an escaped backslash, e.g., ``r'\' == '\\'``. You
   should `read about them
   <https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals>`__
   if this is unclear.

Replace the '.' with ``NaN`` (str -> str):

.. ipython:: python

   d = {"a": list(range(4)), "b": list("ab.."), "c": ["a", "b", np.nan, "d"]}
   df = pd.DataFrame(d)
   df.replace(".", np.nan)

Now do it with a regular expression that removes surrounding whitespace
(regex -> regex):

.. ipython:: python

   df.replace(r"\s*\.\s*", np.nan, regex=True)

Replace a few different values (list -> list):

.. ipython:: python

   df.replace(["a", "."], ["b", np.nan])

list of regex -> list of regex:

.. ipython:: python

   df.replace([r"\.", r"(a)"], ["dot", r"\1stuff"], regex=True)

Only search in column ``'b'`` (dict -> dict):

.. ipython:: python

   df.replace({"b": "."}, {"b": np.nan})

Same as the previous example, but use a regular expression for
searching instead (dict of regex -> dict):

.. ipython:: python

   df.replace({"b": r"\s*\.\s*"}, {"b": np.nan}, regex=True)

You can pass nested dictionaries of regular expressions that use ``regex=True``:

.. ipython:: python

   df.replace({"b": {"b": r""}}, regex=True)

Alternatively, you can pass the nested dictionary like so:

.. ipython:: python

   df.replace(regex={"b": {r"\s*\.\s*": np.nan}})

You can also use the group of a regular expression match when replacing (dict
of regex -> dict of regex), this works for lists as well.

.. ipython:: python

   df.replace({"b": r"\s*(\.)\s*"}, {"b": r"\1ty"}, regex=True)

You can pass a list of regular expressions, of which those that match
will be replaced with a scalar (list of regex -> regex).

.. ipython:: python

   df.replace([r"\s*\.\s*", r"a|b"], np.nan, regex=True)

All of the regular expression examples can also be passed with the
``to_replace`` argument as the ``regex`` argument. In this case the ``value``
argument must be passed explicitly by name or ``regex`` must be a nested
dictionary. The previous example, in this case, would then be:

.. ipython:: python

   df.replace(regex=[r"\s*\.\s*", r"a|b"], value=np.nan)

This can be convenient if you do not want to pass ``regex=True`` every time you
want to use a regular expression.

.. note::

   Anywhere in the above ``replace`` examples that you see a regular expression
   a compiled regular expression is valid as well.

Numeric replacement
~~~~~~~~~~~~~~~~~~~

:meth:`~DataFrame.replace` is similar to :meth:`~DataFrame.fillna`.

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 2))
   df[np.random.rand(df.shape[0]) > 0.5] = 1.5
   df.replace(1.5, np.nan)

Replacing more than one value is possible by passing a list.

.. ipython:: python

   df00 = df.iloc[0, 0]
   df.replace([1.5, df00], [np.nan, "a"])
   df[1].dtype

You can also operate on the DataFrame in place:

.. ipython:: python

   df.replace(1.5, np.nan, inplace=True)

Missing data casting rules and indexing
---------------------------------------

While pandas supports storing arrays of integer and boolean type, these types
are not capable of storing missing data. Until we can switch to using a native
NA type in NumPy, we've established some "casting rules". When a reindexing
operation introduces missing data, the Series will be cast according to the
rules introduced in the table below.

.. csv-table::
    :header: "data type", "Cast to"
    :widths: 40, 40

    integer, float
    boolean, object
    float, no cast
    object, no cast

For example:

.. ipython:: python

   s = pd.Series(np.random.randn(5), index=[0, 2, 4, 6, 7])
   s > 0
   (s > 0).dtype
   crit = (s > 0).reindex(list(range(8)))
   crit
   crit.dtype

Ordinarily NumPy will complain if you try to use an object array (even if it
contains boolean values) instead of a boolean array to get or set values from
an ndarray (e.g. selecting values based on some criteria). If a boolean vector
contains NAs, an exception will be generated:

.. ipython:: python
   :okexcept:

   reindexed = s.reindex(list(range(8))).fillna(0)
   reindexed[crit]

However, these can be filled in using :meth:`~DataFrame.fillna` and it will work fine:

.. ipython:: python

   reindexed[crit.fillna(False)]
   reindexed[crit.fillna(True)]

pandas provides a nullable integer dtype, but you must explicitly request it
when creating the series or column. Notice that we use a capital "I" in
the ``dtype="Int64"``.

.. ipython:: python

   s = pd.Series([0, 1, np.nan, 3, 4], dtype="Int64")
   s

See :ref:`integer_na` for more.


.. _missing_data.NA:

Experimental ``NA`` scalar to denote missing values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. warning::

   Experimental: the behaviour of ``pd.NA`` can still change without warning.

.. versionadded:: 1.0.0

Starting from pandas 1.0, an experimental ``pd.NA`` value (singleton) is
available to represent scalar missing values. At this moment, it is used in
the nullable :doc:`integer <integer_na>`, boolean and
:ref:`dedicated string <text.types>` data types as the missing value indicator.

The goal of ``pd.NA`` is provide a "missing" indicator that can be used
consistently across data types (instead of ``np.nan``, ``None`` or ``pd.NaT``
depending on the data type).

For example, when having missing values in a Series with the nullable integer
dtype, it will use ``pd.NA``:

.. ipython:: python

    s = pd.Series([1, 2, None], dtype="Int64")
    s
    s[2]
    s[2] is pd.NA

Currently, pandas does not yet use those data types by default (when creating
a DataFrame or Series, or when reading in data), so you need to specify
the dtype explicitly.  An easy way to convert to those dtypes is explained
:ref:`here <missing_data.NA.conversion>`.

Propagation in arithmetic and comparison operations
---------------------------------------------------

In general, missing values *propagate* in operations involving ``pd.NA``. When
one of the operands is unknown, the outcome of the operation is also unknown.

For example, ``pd.NA`` propagates in arithmetic operations, similarly to
``np.nan``:

.. ipython:: python

   pd.NA + 1
   "a" * pd.NA

There are a few special cases when the result is known, even when one of the
operands is ``NA``.

.. ipython:: python

   pd.NA ** 0
   1 ** pd.NA

In equality and comparison operations, ``pd.NA`` also propagates. This deviates
from the behaviour of ``np.nan``, where comparisons with ``np.nan`` always
return ``False``.

.. ipython:: python

   pd.NA == 1
   pd.NA == pd.NA
   pd.NA < 2.5

To check if a value is equal to ``pd.NA``, the :func:`isna` function can be
used:

.. ipython:: python

   pd.isna(pd.NA)

An exception on this basic propagation rule are *reductions* (such as the
mean or the minimum), where pandas defaults to skipping missing values. See
:ref:`above <missing_data.calculations>` for more.

Logical operations
------------------

For logical operations, ``pd.NA`` follows the rules of the
`three-valued logic <https://en.wikipedia.org/wiki/Three-valued_logic>`__ (or
*Kleene logic*, similarly to R, SQL and Julia). This logic means to only
propagate missing values when it is logically required.

For example, for the logical "or" operation (``|``), if one of the operands
is ``True``, we already know the result will be ``True``, regardless of the
other value (so regardless the missing value would be ``True`` or ``False``).
In this case, ``pd.NA`` does not propagate:

.. ipython:: python

   True | False
   True | pd.NA
   pd.NA | True

On the other hand, if one of the operands is ``False``, the result depends
on the value of the other operand. Therefore, in this case ``pd.NA``
propagates:

.. ipython:: python

   False | True
   False | False
   False | pd.NA

The behaviour of the logical "and" operation (``&``) can be derived using
similar logic (where now ``pd.NA`` will not propagate if one of the operands
is already ``False``):

.. ipython:: python

   False & True
   False & False
   False & pd.NA

.. ipython:: python

   True & True
   True & False
   True & pd.NA


``NA`` in a boolean context
---------------------------

Since the actual value of an NA is unknown, it is ambiguous to convert NA
to a boolean value. The following raises an error:

.. ipython:: python
   :okexcept:

   bool(pd.NA)

This also means that ``pd.NA`` cannot be used in a context where it is
evaluated to a boolean, such as ``if condition: ...`` where ``condition`` can
potentially be ``pd.NA``. In such cases, :func:`isna` can be used to check
for ``pd.NA`` or ``condition`` being ``pd.NA`` can be avoided, for example by
filling missing values beforehand.

A similar situation occurs when using Series or DataFrame objects in ``if``
statements, see :ref:`gotchas.truth`.

NumPy ufuncs
------------

:attr:`pandas.NA` implements NumPy's ``__array_ufunc__`` protocol. Most ufuncs
work with ``NA``, and generally return ``NA``:

.. ipython:: python

   np.log(pd.NA)
   np.add(pd.NA, 1)

.. warning::

   Currently, ufuncs involving an ndarray and ``NA`` will return an
   object-dtype filled with NA values.

   .. ipython:: python

      a = np.array([1, 2, 3])
      np.greater(a, pd.NA)

   The return type here may change to return a different array type
   in the future.

See :ref:`dsintro.numpy_interop` for more on ufuncs.

.. _missing_data.NA.conversion:

Conversion
----------

If you have a DataFrame or Series using traditional types that have missing data
represented using ``np.nan``, there are convenience methods
:meth:`~Series.convert_dtypes` in Series and :meth:`~DataFrame.convert_dtypes`
in DataFrame that can convert data to use the newer dtypes for integers, strings and
booleans listed :ref:`here <basics.dtypes>`. This is especially helpful after reading
in data sets when letting the readers such as :meth:`read_csv` and :meth:`read_excel`
infer default dtypes.

In this example, while the dtypes of all columns are changed, we show the results for
the first 10 columns.

.. ipython:: python

   bb = pd.read_csv("data/baseball.csv", index_col="id")
   bb[bb.columns[:10]].dtypes

.. ipython:: python

   bbn = bb.convert_dtypes()
   bbn[bbn.columns[:10]].dtypes
.. currentmodule:: pandas

.. ipython:: python
   :suppress:

   import pandas as pd
   import numpy as np

.. _boolean:

**************************
Nullable Boolean data type
**************************

.. note::

   BooleanArray is currently experimental. Its API or implementation may
   change without warning.

.. versionadded:: 1.0.0


.. _boolean.indexing:

Indexing with NA values
-----------------------

pandas allows indexing with ``NA`` values in a boolean array, which are treated as ``False``.

.. versionchanged:: 1.0.2

.. ipython:: python
   :okexcept:

   s = pd.Series([1, 2, 3])
   mask = pd.array([True, False, pd.NA], dtype="boolean")
   s[mask]

If you would prefer to keep the ``NA`` values you can manually fill them with ``fillna(True)``.

.. ipython:: python

   s[mask.fillna(True)]

.. _boolean.kleene:

Kleene logical operations
-------------------------

:class:`arrays.BooleanArray` implements `Kleene Logic`_ (sometimes called three-value logic) for
logical operations like ``&`` (and), ``|`` (or) and ``^`` (exclusive-or).

This table demonstrates the results for every combination. These operations are symmetrical,
so flipping the left- and right-hand side makes no difference in the result.

================= =========
Expression        Result
================= =========
``True & True``   ``True``
``True & False``  ``False``
``True & NA``     ``NA``
``False & False`` ``False``
``False & NA``    ``False``
``NA & NA``       ``NA``
``True | True``   ``True``
``True | False``  ``True``
``True | NA``     ``True``
``False | False`` ``False``
``False | NA``    ``NA``
``NA | NA``       ``NA``
``True ^ True``   ``False``
``True ^ False``  ``True``
``True ^ NA``     ``NA``
``False ^ False`` ``False``
``False ^ NA``    ``NA``
``NA ^ NA``       ``NA``
================= =========

When an ``NA`` is present in an operation, the output value is ``NA`` only if
the result cannot be determined solely based on the other input. For example,
``True | NA`` is ``True``, because both ``True | True`` and ``True | False``
are ``True``. In that case, we don't actually need to consider the value
of the ``NA``.

On the other hand, ``True & NA`` is ``NA``. The result depends on whether
the ``NA`` really is ``True`` or ``False``, since ``True & True`` is ``True``,
but ``True & False`` is ``False``, so we can't determine the output.


This differs from how ``np.nan`` behaves in logical operations. pandas treated
``np.nan`` is *always false in the output*.

In ``or``

.. ipython:: python

   pd.Series([True, False, np.nan], dtype="object") | True
   pd.Series([True, False, np.nan], dtype="boolean") | True

In ``and``

.. ipython:: python

   pd.Series([True, False, np.nan], dtype="object") & True
   pd.Series([True, False, np.nan], dtype="boolean") & True

.. _Kleene Logic: https://en.wikipedia.org/wiki/Three-valued_logic#Kleene_and_Priest_logics
.. _timedeltas:

{{ header }}

.. _timedeltas.timedeltas:

***********
Time deltas
***********

Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes,
seconds. They can be both positive and negative.

``Timedelta`` is a subclass of ``datetime.timedelta``, and behaves in a similar manner,
but allows compatibility with ``np.timedelta64`` types as well as a host of custom representation,
parsing, and attributes.

Parsing
-------

You can construct a ``Timedelta`` scalar through various arguments, including `ISO 8601 Duration`_ strings.

.. ipython:: python

   import datetime

   # strings
   pd.Timedelta("1 days")
   pd.Timedelta("1 days 00:00:00")
   pd.Timedelta("1 days 2 hours")
   pd.Timedelta("-1 days 2 min 3us")

   # like datetime.timedelta
   # note: these MUST be specified as keyword arguments
   pd.Timedelta(days=1, seconds=1)

   # integers with a unit
   pd.Timedelta(1, unit="d")

   # from a datetime.timedelta/np.timedelta64
   pd.Timedelta(datetime.timedelta(days=1, seconds=1))
   pd.Timedelta(np.timedelta64(1, "ms"))

   # negative Timedeltas have this string repr
   # to be more consistent with datetime.timedelta conventions
   pd.Timedelta("-1us")

   # a NaT
   pd.Timedelta("nan")
   pd.Timedelta("nat")

   # ISO 8601 Duration strings
   pd.Timedelta("P0DT0H1M0S")
   pd.Timedelta("P0DT0H0M0.000000123S")

:ref:`DateOffsets<timeseries.offsets>` (``Day, Hour, Minute, Second, Milli, Micro, Nano``) can also be used in construction.

.. ipython:: python

   pd.Timedelta(pd.offsets.Second(2))

Further, operations among the scalars yield another scalar ``Timedelta``.

.. ipython:: python

   pd.Timedelta(pd.offsets.Day(2)) + pd.Timedelta(pd.offsets.Second(2)) + pd.Timedelta(
       "00:00:00.000123"
   )

to_timedelta
~~~~~~~~~~~~

Using the top-level ``pd.to_timedelta``, you can convert a scalar, array, list,
or Series from a recognized timedelta format / value into a ``Timedelta`` type.
It will construct Series if the input is a Series, a scalar if the input is
scalar-like, otherwise it will output a ``TimedeltaIndex``.

You can parse a single string to a Timedelta:

.. ipython:: python

   pd.to_timedelta("1 days 06:05:01.00003")
   pd.to_timedelta("15.5us")

or a list/array of strings:

.. ipython:: python

   pd.to_timedelta(["1 days 06:05:01.00003", "15.5us", "nan"])

The ``unit`` keyword argument specifies the unit of the Timedelta if the input
is numeric:

.. ipython:: python

   pd.to_timedelta(np.arange(5), unit="s")
   pd.to_timedelta(np.arange(5), unit="d")

.. warning::
    If a string or array of strings is passed as an input then the ``unit`` keyword
    argument will be ignored. If a string without units is passed then the default
    unit of nanoseconds is assumed.

.. _timedeltas.limitations:

Timedelta limitations
~~~~~~~~~~~~~~~~~~~~~

pandas represents ``Timedeltas`` in nanosecond resolution using
64 bit integers. As such, the 64 bit integer limits determine
the ``Timedelta`` limits.

.. ipython:: python

   pd.Timedelta.min
   pd.Timedelta.max

.. _timedeltas.operations:

Operations
----------

You can operate on Series/DataFrames and construct ``timedelta64[ns]`` Series through
subtraction operations on ``datetime64[ns]`` Series, or ``Timestamps``.

.. ipython:: python

   s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))
   td = pd.Series([pd.Timedelta(days=i) for i in range(3)])
   df = pd.DataFrame({"A": s, "B": td})
   df
   df["C"] = df["A"] + df["B"]
   df
   df.dtypes

   s - s.max()
   s - datetime.datetime(2011, 1, 1, 3, 5)
   s + datetime.timedelta(minutes=5)
   s + pd.offsets.Minute(5)
   s + pd.offsets.Minute(5) + pd.offsets.Milli(5)

Operations with scalars from a ``timedelta64[ns]`` series:

.. ipython:: python

   y = s - s[0]
   y

Series of timedeltas with ``NaT`` values are supported:

.. ipython:: python

   y = s - s.shift()
   y

Elements can be set to ``NaT`` using ``np.nan`` analogously to datetimes:

.. ipython:: python

   y[1] = np.nan
   y

Operands can also appear in a reversed order (a singular object operated with a Series):

.. ipython:: python

   s.max() - s
   datetime.datetime(2011, 1, 1, 3, 5) - s
   datetime.timedelta(minutes=5) + s

``min, max`` and the corresponding ``idxmin, idxmax`` operations are supported on frames:

.. ipython:: python

   A = s - pd.Timestamp("20120101") - pd.Timedelta("00:05:05")
   B = s - pd.Series(pd.date_range("2012-1-2", periods=3, freq="D"))

   df = pd.DataFrame({"A": A, "B": B})
   df

   df.min()
   df.min(axis=1)

   df.idxmin()
   df.idxmax()

``min, max, idxmin, idxmax`` operations are supported on Series as well. A scalar result will be a ``Timedelta``.

.. ipython:: python

   df.min().max()
   df.min(axis=1).min()

   df.min().idxmax()
   df.min(axis=1).idxmin()

You can fillna on timedeltas, passing a timedelta to get a particular value.

.. ipython:: python

   y.fillna(pd.Timedelta(0))
   y.fillna(pd.Timedelta(10, unit="s"))
   y.fillna(pd.Timedelta("-1 days, 00:00:05"))

You can also negate, multiply and use ``abs`` on ``Timedeltas``:

.. ipython:: python

   td1 = pd.Timedelta("-1 days 2 hours 3 seconds")
   td1
   -1 * td1
   -td1
   abs(td1)

.. _timedeltas.timedeltas_reductions:

Reductions
----------

Numeric reduction operation for ``timedelta64[ns]`` will return ``Timedelta`` objects. As usual
``NaT`` are skipped during evaluation.

.. ipython:: python

   y2 = pd.Series(
       pd.to_timedelta(["-1 days +00:00:05", "nat", "-1 days +00:00:05", "1 days"])
   )
   y2
   y2.mean()
   y2.median()
   y2.quantile(0.1)
   y2.sum()

.. _timedeltas.timedeltas_convert:

Frequency conversion
--------------------

Timedelta Series, ``TimedeltaIndex``, and ``Timedelta`` scalars can be converted to other 'frequencies' by dividing by another timedelta,
or by astyping to a specific timedelta type. These operations yield Series and propagate ``NaT`` -> ``nan``.
Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division.

.. ipython:: python

   december = pd.Series(pd.date_range("20121201", periods=4))
   january = pd.Series(pd.date_range("20130101", periods=4))
   td = january - december

   td[2] += datetime.timedelta(minutes=5, seconds=3)
   td[3] = np.nan
   td

   # to days
   td / np.timedelta64(1, "D")
   td.astype("timedelta64[D]")

   # to seconds
   td / np.timedelta64(1, "s")
   td.astype("timedelta64[s]")

   # to months (these are constant months)
   td / np.timedelta64(1, "M")

Dividing or multiplying a ``timedelta64[ns]`` Series by an integer or integer Series
yields another ``timedelta64[ns]`` dtypes Series.

.. ipython:: python

   td * -1
   td * pd.Series([1, 2, 3, 4])

Rounded division (floor-division) of a ``timedelta64[ns]`` Series by a scalar
``Timedelta`` gives a series of integers.

.. ipython:: python

   td // pd.Timedelta(days=3, hours=4)
   pd.Timedelta(days=3, hours=4) // td

.. _timedeltas.mod_divmod:

The mod (%) and divmod operations are defined for ``Timedelta`` when operating with another timedelta-like or with a numeric argument.

.. ipython:: python

   pd.Timedelta(hours=37) % datetime.timedelta(hours=2)

   # divmod against a timedelta-like returns a pair (int, Timedelta)
   divmod(datetime.timedelta(hours=2), pd.Timedelta(minutes=11))

   # divmod against a numeric returns a pair (Timedelta, Timedelta)
   divmod(pd.Timedelta(hours=25), 86400000000000)

Attributes
----------

You can access various components of the ``Timedelta`` or ``TimedeltaIndex`` directly using the attributes ``days,seconds,microseconds,nanoseconds``. These are identical to the values returned by ``datetime.timedelta``, in that, for example, the ``.seconds`` attribute represents the number of seconds >= 0 and < 1 day. These are signed according to whether the ``Timedelta`` is signed.

These operations can also be directly accessed via the ``.dt`` property of the ``Series`` as well.

.. note::

   Note that the attributes are NOT the displayed values of the ``Timedelta``. Use ``.components`` to retrieve the displayed values.

For a ``Series``:

.. ipython:: python

   td.dt.days
   td.dt.seconds

You can access the value of the fields for a scalar ``Timedelta`` directly.

.. ipython:: python

   tds = pd.Timedelta("31 days 5 min 3 sec")
   tds.days
   tds.seconds
   (-tds).seconds

You can use the ``.components`` property to access a reduced form of the timedelta. This returns a ``DataFrame`` indexed
similarly to the ``Series``. These are the *displayed* values of the ``Timedelta``.

.. ipython:: python

   td.dt.components
   td.dt.components.seconds

.. _timedeltas.isoformat:

You can convert a ``Timedelta`` to an `ISO 8601 Duration`_ string with the
``.isoformat`` method

.. ipython:: python

    pd.Timedelta(
        days=6, minutes=50, seconds=3, milliseconds=10, microseconds=10, nanoseconds=12
    ).isoformat()

.. _ISO 8601 Duration: https://en.wikipedia.org/wiki/ISO_8601#Durations

.. _timedeltas.index:

TimedeltaIndex
--------------

To generate an index with time delta, you can use either the :class:`TimedeltaIndex` or
the :func:`timedelta_range` constructor.

Using ``TimedeltaIndex`` you can pass string-like, ``Timedelta``, ``timedelta``,
or ``np.timedelta64`` objects. Passing ``np.nan/pd.NaT/nat`` will represent missing values.

.. ipython:: python

   pd.TimedeltaIndex(
       [
           "1 days",
           "1 days, 00:00:05",
           np.timedelta64(2, "D"),
           datetime.timedelta(days=2, seconds=2),
       ]
   )

The string 'infer' can be passed in order to set the frequency of the index as the
inferred frequency upon creation:

.. ipython:: python

   pd.TimedeltaIndex(["0 days", "10 days", "20 days"], freq="infer")

Generating ranges of time deltas
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Similar to :func:`date_range`, you can construct regular ranges of a ``TimedeltaIndex``
using :func:`timedelta_range`.  The default frequency for ``timedelta_range`` is
calendar day:

.. ipython:: python

   pd.timedelta_range(start="1 days", periods=5)

Various combinations of ``start``, ``end``, and ``periods`` can be used with
``timedelta_range``:

.. ipython:: python

   pd.timedelta_range(start="1 days", end="5 days")

   pd.timedelta_range(end="10 days", periods=4)

The ``freq`` parameter can passed a variety of :ref:`frequency aliases <timeseries.offset_aliases>`:

.. ipython:: python

   pd.timedelta_range(start="1 days", end="2 days", freq="30T")

   pd.timedelta_range(start="1 days", periods=5, freq="2D5H")


Specifying ``start``, ``end``, and ``periods`` will generate a range of evenly spaced
timedeltas from ``start`` to ``end`` inclusively, with ``periods`` number of elements
in the resulting ``TimedeltaIndex``:

.. ipython:: python

   pd.timedelta_range("0 days", "4 days", periods=5)

   pd.timedelta_range("0 days", "4 days", periods=10)

Using the TimedeltaIndex
~~~~~~~~~~~~~~~~~~~~~~~~

Similarly to other of the datetime-like indices, ``DatetimeIndex`` and ``PeriodIndex``, you can use
``TimedeltaIndex`` as the index of pandas objects.

.. ipython:: python

   s = pd.Series(
       np.arange(100),
       index=pd.timedelta_range("1 days", periods=100, freq="h"),
   )
   s

Selections work similarly, with coercion on string-likes and slices:

.. ipython:: python

   s["1 day":"2 day"]
   s["1 day 01:00:00"]
   s[pd.Timedelta("1 day 1h")]

Furthermore you can use partial string selection and the range will be inferred:

.. ipython:: python

   s["1 day":"1 day 5 hours"]

Operations
~~~~~~~~~~

Finally, the combination of ``TimedeltaIndex`` with ``DatetimeIndex`` allow certain combination operations that are NaT preserving:

.. ipython:: python

   tdi = pd.TimedeltaIndex(["1 days", pd.NaT, "2 days"])
   tdi.to_list()
   dti = pd.date_range("20130101", periods=3)
   dti.to_list()
   (dti + tdi).to_list()
   (dti - tdi).to_list()

Conversions
~~~~~~~~~~~

Similarly to frequency conversion on a ``Series`` above, you can convert these indices to yield another Index.

.. ipython:: python

   tdi / np.timedelta64(1, "s")
   tdi.astype("timedelta64[s]")

Scalars type ops work as well. These can potentially return a *different* type of index.

.. ipython:: python

   # adding or timedelta and date -> datelike
   tdi + pd.Timestamp("20130101")

   # subtraction of a date and a timedelta -> datelike
   # note that trying to subtract a date from a Timedelta will raise an exception
   (pd.Timestamp("20130101") - tdi).to_list()

   # timedelta + timedelta -> timedelta
   tdi + pd.Timedelta("10 days")

   # division can result in a Timedelta if the divisor is an integer
   tdi / 2

   # or a Float64Index if the divisor is a Timedelta
   tdi / tdi[0]

.. _timedeltas.resampling:

Resampling
----------

Similar to :ref:`timeseries resampling <timeseries.resampling>`, we can resample with a ``TimedeltaIndex``.

.. ipython:: python

   s.resample("D").mean()
.. _gotchas:

{{ header }}

********************************
Frequently Asked Questions (FAQ)
********************************

.. _df-memory-usage:

DataFrame memory usage
----------------------
The memory usage of a :class:`DataFrame` (including the index) is shown when calling
the :meth:`~DataFrame.info`. A configuration option, ``display.memory_usage``
(see :ref:`the list of options <options.available>`), specifies if the
:class:`DataFrame` memory usage will be displayed when invoking the ``df.info()``
method.

For example, the memory usage of the :class:`DataFrame` below is shown
when calling :meth:`~DataFrame.info`:

.. ipython:: python

    dtypes = [
        "int64",
        "float64",
        "datetime64[ns]",
        "timedelta64[ns]",
        "complex128",
        "object",
        "bool",
    ]
    n = 5000
    data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}
    df = pd.DataFrame(data)
    df["categorical"] = df["object"].astype("category")

    df.info()

The ``+`` symbol indicates that the true memory usage could be higher, because
pandas does not count the memory used by values in columns with
``dtype=object``.

Passing ``memory_usage='deep'`` will enable a more accurate memory usage report,
accounting for the full usage of the contained objects. This is optional
as it can be expensive to do this deeper introspection.

.. ipython:: python

   df.info(memory_usage="deep")

By default the display option is set to ``True`` but can be explicitly
overridden by passing the ``memory_usage`` argument when invoking ``df.info()``.

The memory usage of each column can be found by calling the
:meth:`~DataFrame.memory_usage` method. This returns a :class:`Series` with an index
represented by column names and memory usage of each column shown in bytes. For
the :class:`DataFrame` above, the memory usage of each column and the total memory
usage can be found with the ``memory_usage`` method:

.. ipython:: python

    df.memory_usage()

    # total memory usage of dataframe
    df.memory_usage().sum()

By default the memory usage of the :class:`DataFrame` index is shown in the
returned :class:`Series`, the memory usage of the index can be suppressed by passing
the ``index=False`` argument:

.. ipython:: python

    df.memory_usage(index=False)

The memory usage displayed by the :meth:`~DataFrame.info` method utilizes the
:meth:`~DataFrame.memory_usage` method to determine the memory usage of a
:class:`DataFrame` while also formatting the output in human-readable units (base-2
representation; i.e. 1KB = 1024 bytes).

See also :ref:`Categorical Memory Usage <categorical.memory>`.

.. _gotchas.truth:

Using if/truth statements with pandas
-------------------------------------

pandas follows the NumPy convention of raising an error when you try to convert
something to a ``bool``. This happens in an ``if``-statement or when using the
boolean operations: ``and``, ``or``, and ``not``. It is not clear what the result
of the following code should be:

.. code-block:: python

    >>> if pd.Series([False, True, False]):
    ...     pass

Should it be ``True`` because it's not zero-length, or ``False`` because there
are ``False`` values? It is unclear, so instead, pandas raises a ``ValueError``:

.. ipython:: python
    :okexcept:

    if pd.Series([False, True, False]):
        print("I was true")

You need to explicitly choose what you want to do with the :class:`DataFrame`, e.g.
use :meth:`~DataFrame.any`, :meth:`~DataFrame.all` or :meth:`~DataFrame.empty`.
Alternatively, you might want to compare if the pandas object is ``None``:

.. ipython:: python

    if pd.Series([False, True, False]) is not None:
        print("I was not None")


Below is how to check if any of the values are ``True``:

.. ipython:: python

    if pd.Series([False, True, False]).any():
        print("I am any")

To evaluate single-element pandas objects in a boolean context, use the method
:meth:`~DataFrame.bool`:

.. ipython:: python

   pd.Series([True]).bool()
   pd.Series([False]).bool()
   pd.DataFrame([[True]]).bool()
   pd.DataFrame([[False]]).bool()

Bitwise boolean
~~~~~~~~~~~~~~~

Bitwise boolean operators like ``==`` and ``!=`` return a boolean :class:`Series`
which performs an element-wise comparison when compared to a scalar.

.. ipython:: python

   s = pd.Series(range(5))
   s == 4

See :ref:`boolean comparisons<basics.compare>` for more examples.

Using the ``in`` operator
~~~~~~~~~~~~~~~~~~~~~~~~~

Using the Python ``in`` operator on a :class:`Series` tests for membership in the
**index**, not membership among the values.

.. ipython:: python

    s = pd.Series(range(5), index=list("abcde"))
    2 in s
    'b' in s

If this behavior is surprising, keep in mind that using ``in`` on a Python
dictionary tests keys, not values, and :class:`Series` are dict-like.
To test for membership in the values, use the method :meth:`~pandas.Series.isin`:

.. ipython:: python

    s.isin([2])
    s.isin([2]).any()

For :class:`DataFrame`, likewise, ``in`` applies to the column axis,
testing for membership in the list of column names.

.. _gotchas.udf-mutation:

Mutating with User Defined Function (UDF) methods
-------------------------------------------------

This section applies to pandas methods that take a UDF. In particular, the methods
``.apply``, ``.aggregate``, ``.transform``, and ``.filter``.

It is a general rule in programming that one should not mutate a container
while it is being iterated over. Mutation will invalidate the iterator,
causing unexpected behavior. Consider the example:

.. ipython:: python

   values = [0, 1, 2, 3, 4, 5]
   n_removed = 0
   for k, value in enumerate(values):
       idx = k - n_removed
       if value % 2 == 1:
           del values[idx]
           n_removed += 1
       else:
           values[idx] = value + 1
   values

One probably would have expected that the result would be ``[1, 3, 5]``.
When using a pandas method that takes a UDF, internally pandas is often
iterating over the
:class:`DataFrame` or other pandas object. Therefore, if the UDF mutates (changes)
the :class:`DataFrame`, unexpected behavior can arise.

Here is a similar example with :meth:`DataFrame.apply`:

.. ipython:: python

   def f(s):
       s.pop("a")
       return s

   df = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})
   try:
       df.apply(f, axis="columns")
   except Exception as err:
       print(repr(err))

To resolve this issue, one can make a copy so that the mutation does
not apply to the container being iterated over.

.. ipython:: python

   values = [0, 1, 2, 3, 4, 5]
   n_removed = 0
   for k, value in enumerate(values.copy()):
       idx = k - n_removed
       if value % 2 == 1:
           del values[idx]
           n_removed += 1
       else:
           values[idx] = value + 1
   values

.. ipython:: python

   def f(s):
       s = s.copy()
       s.pop("a")
       return s

   df = pd.DataFrame({"a": [1, 2, 3], 'b': [4, 5, 6]})
   df.apply(f, axis="columns")

``NaN``, Integer ``NA`` values and ``NA`` type promotions
---------------------------------------------------------

Choice of ``NA`` representation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For lack of ``NA`` (missing) support from the ground up in NumPy and Python in
general, we were given the difficult choice between either:

* A *masked array* solution: an array of data and an array of boolean values
  indicating whether a value is there or is missing.
* Using a special sentinel value, bit pattern, or set of sentinel values to
  denote ``NA`` across the dtypes.

For many reasons we chose the latter. After years of production use it has
proven, at least in my opinion, to be the best decision given the state of
affairs in NumPy and Python in general. The special value ``NaN``
(Not-A-Number) is used everywhere as the ``NA`` value, and there are API
functions :meth:`DataFrame.isna` and :meth:`DataFrame.notna` which can be used across the dtypes to
detect NA values.

However, it comes with it a couple of trade-offs which I most certainly have
not ignored.

.. _gotchas.intna:

Support for integer ``NA``
~~~~~~~~~~~~~~~~~~~~~~~~~~

In the absence of high performance ``NA`` support being built into NumPy from
the ground up, the primary casualty is the ability to represent NAs in integer
arrays. For example:

.. ipython:: python

   s = pd.Series([1, 2, 3, 4, 5], index=list("abcde"))
   s
   s.dtype

   s2 = s.reindex(["a", "b", "c", "f", "u"])
   s2
   s2.dtype

This trade-off is made largely for memory and performance reasons, and also so
that the resulting :class:`Series` continues to be "numeric".

If you need to represent integers with possibly missing values, use one of
the nullable-integer extension dtypes provided by pandas

* :class:`Int8Dtype`
* :class:`Int16Dtype`
* :class:`Int32Dtype`
* :class:`Int64Dtype`

.. ipython:: python

   s_int = pd.Series([1, 2, 3, 4, 5], index=list("abcde"), dtype=pd.Int64Dtype())
   s_int
   s_int.dtype

   s2_int = s_int.reindex(["a", "b", "c", "f", "u"])
   s2_int
   s2_int.dtype

See :ref:`integer_na` for more.

``NA`` type promotions
~~~~~~~~~~~~~~~~~~~~~~

When introducing NAs into an existing :class:`Series` or :class:`DataFrame` via
:meth:`~Series.reindex` or some other means, boolean and integer types will be
promoted to a different dtype in order to store the NAs. The promotions are
summarized in this table:

.. csv-table::
   :header: "Typeclass","Promotion dtype for storing NAs"
   :widths: 40,60

   ``floating``, no change
   ``object``, no change
   ``integer``, cast to ``float64``
   ``boolean``, cast to ``object``

While this may seem like a heavy trade-off, I have found very few cases where
this is an issue in practice i.e. storing values greater than 2**53. Some
explanation for the motivation is in the next section.

Why not make NumPy like R?
~~~~~~~~~~~~~~~~~~~~~~~~~~

Many people have suggested that NumPy should simply emulate the ``NA`` support
present in the more domain-specific statistical programming language `R
<https://www.r-project.org/>`__. Part of the reason is the NumPy type hierarchy:

.. csv-table::
   :header: "Typeclass","Dtypes"
   :widths: 30,70
   :delim: |

   ``numpy.floating`` | ``float16, float32, float64, float128``
   ``numpy.integer`` | ``int8, int16, int32, int64``
   ``numpy.unsignedinteger`` | ``uint8, uint16, uint32, uint64``
   ``numpy.object_`` | ``object_``
   ``numpy.bool_`` | ``bool_``
   ``numpy.character`` | ``string_, unicode_``

The R language, by contrast, only has a handful of built-in data types:
``integer``, ``numeric`` (floating-point), ``character``, and
``boolean``. ``NA`` types are implemented by reserving special bit patterns for
each type to be used as the missing value. While doing this with the full NumPy
type hierarchy would be possible, it would be a more substantial trade-off
(especially for the 8- and 16-bit data types) and implementation undertaking.

An alternate approach is that of using masked arrays. A masked array is an
array of data with an associated boolean *mask* denoting whether each value
should be considered ``NA`` or not. I am personally not in love with this
approach as I feel that overall it places a fairly heavy burden on the user and
the library implementer. Additionally, it exacts a fairly high performance cost
when working with numerical data compared with the simple approach of using
``NaN``. Thus, I have chosen the Pythonic "practicality beats purity" approach
and traded integer ``NA`` capability for a much simpler approach of using a
special value in float and object arrays to denote ``NA``, and promoting
integer arrays to floating when NAs must be introduced.


Differences with NumPy
----------------------
For :class:`Series` and :class:`DataFrame` objects, :meth:`~DataFrame.var` normalizes by
``N-1`` to produce unbiased estimates of the sample variance, while NumPy's
:meth:`numpy.var` normalizes by N, which measures the variance of the sample. Note that
:meth:`~DataFrame.cov` normalizes by ``N-1`` in both pandas and NumPy.

.. _gotchas.thread-safety:

Thread-safety
-------------

pandas is not 100% thread safe. The known issues relate to
the :meth:`~DataFrame.copy` method. If you are doing a lot of copying of
:class:`DataFrame` objects shared among threads, we recommend holding locks inside
the threads where the data copying occurs.

See `this link <https://stackoverflow.com/questions/13592618/python-pandas-dataframe-thread-safe>`__
for more information.


Byte-ordering issues
--------------------
Occasionally you may have to deal with data that were created on a machine with
a different byte order than the one on which you are running Python. A common
symptom of this issue is an error like::

    Traceback
        ...
    ValueError: Big-endian buffer not supported on little-endian compiler

To deal
with this issue you should convert the underlying NumPy array to the native
system byte order *before* passing it to :class:`Series` or :class:`DataFrame`
constructors using something similar to the following:

.. ipython:: python

   x = np.array(list(range(10)), ">i4")  # big endian
   newx = x.byteswap().newbyteorder()  # force native byteorder
   s = pd.Series(newx)

See `the NumPy documentation on byte order
<https://numpy.org/doc/stable/user/basics.byteswapping.html>`__ for more
details.
{{ header }}

.. _user_guide:

==========
User Guide
==========

The User Guide covers all of pandas by topic area. Each of the subsections
introduces a topic (such as "working with missing data"), and discusses how
pandas approaches the problem, with many examples throughout.

Users brand-new to pandas should start with :ref:`10min`.

For a high level summary of the pandas fundamentals, see :ref:`dsintro` and :ref:`basics`.

Further information on any specific method can be obtained in the
:ref:`api`.

.. If you update this toctree, also update the manual toctree in the
   main index.rst.template

.. toctree::
    :maxdepth: 2

    10min
    dsintro
    basics
    io
    indexing
    advanced
    merging
    reshaping
    text
    missing_data
    duplicates
    categorical
    integer_na
    boolean
    visualization
    style
    computation
    groupby
    window
    timeseries
    timedeltas
    options
    enhancingperf
    scale
    sparse
    gotchas
    cookbook
.. _io:

.. currentmodule:: pandas


===============================
IO tools (text, CSV, HDF5, ...)
===============================

The pandas I/O API is a set of top level ``reader`` functions accessed like
:func:`pandas.read_csv` that generally return a pandas object. The corresponding
``writer`` functions are object methods that are accessed like
:meth:`DataFrame.to_csv`. Below is a table containing available ``readers`` and
``writers``.

.. csv-table::
    :header: "Format Type", "Data Description", "Reader", "Writer"
    :widths: 30, 100, 60, 60
    :delim: ;

    text;`CSV <https://en.wikipedia.org/wiki/Comma-separated_values>`__;:ref:`read_csv<io.read_csv_table>`;:ref:`to_csv<io.store_in_csv>`
    text;Fixed-Width Text File;:ref:`read_fwf<io.fwf_reader>`
    text;`JSON <https://www.json.org/>`__;:ref:`read_json<io.json_reader>`;:ref:`to_json<io.json_writer>`
    text;`HTML <https://en.wikipedia.org/wiki/HTML>`__;:ref:`read_html<io.read_html>`;:ref:`to_html<io.html>`
    text;`LaTeX <https://en.wikipedia.org/wiki/LaTeX>`__;;:ref:`Styler.to_latex<io.latex>`
    text;`XML <https://www.w3.org/standards/xml/core>`__;:ref:`read_xml<io.read_xml>`;:ref:`to_xml<io.xml>`
    text; Local clipboard;:ref:`read_clipboard<io.clipboard>`;:ref:`to_clipboard<io.clipboard>`
    binary;`MS Excel <https://en.wikipedia.org/wiki/Microsoft_Excel>`__;:ref:`read_excel<io.excel_reader>`;:ref:`to_excel<io.excel_writer>`
    binary;`OpenDocument <http://opendocumentformat.org>`__;:ref:`read_excel<io.ods>`;
    binary;`HDF5 Format <https://support.hdfgroup.org/HDF5/whatishdf5.html>`__;:ref:`read_hdf<io.hdf5>`;:ref:`to_hdf<io.hdf5>`
    binary;`Feather Format <https://github.com/wesm/feather>`__;:ref:`read_feather<io.feather>`;:ref:`to_feather<io.feather>`
    binary;`Parquet Format <https://parquet.apache.org/>`__;:ref:`read_parquet<io.parquet>`;:ref:`to_parquet<io.parquet>`
    binary;`ORC Format <https://orc.apache.org/>`__;:ref:`read_orc<io.orc>`;
    binary;`Stata <https://en.wikipedia.org/wiki/Stata>`__;:ref:`read_stata<io.stata_reader>`;:ref:`to_stata<io.stata_writer>`
    binary;`SAS <https://en.wikipedia.org/wiki/SAS_(software)>`__;:ref:`read_sas<io.sas_reader>`;
    binary;`SPSS <https://en.wikipedia.org/wiki/SPSS>`__;:ref:`read_spss<io.spss_reader>`;
    binary;`Python Pickle Format <https://docs.python.org/3/library/pickle.html>`__;:ref:`read_pickle<io.pickle>`;:ref:`to_pickle<io.pickle>`
    SQL;`SQL <https://en.wikipedia.org/wiki/SQL>`__;:ref:`read_sql<io.sql>`;:ref:`to_sql<io.sql>`
    SQL;`Google BigQuery <https://en.wikipedia.org/wiki/BigQuery>`__;:ref:`read_gbq<io.bigquery>`;:ref:`to_gbq<io.bigquery>`

:ref:`Here <io.perf>` is an informal performance comparison for some of these IO methods.

.. note::
   For examples that use the ``StringIO`` class, make sure you import it
   with ``from io import StringIO`` for Python 3.

.. _io.read_csv_table:

CSV & text files
----------------

The workhorse function for reading text files (a.k.a. flat files) is
:func:`read_csv`. See the :ref:`cookbook<cookbook.csv>` for some advanced strategies.

Parsing options
'''''''''''''''

:func:`read_csv` accepts the following common arguments:

Basic
+++++

filepath_or_buffer : various
  Either a path to a file (a :class:`python:str`, :class:`python:pathlib.Path`,
  or :class:`py:py._path.local.LocalPath`), URL (including http, ftp, and S3
  locations), or any object with a ``read()`` method (such as an open file or
  :class:`~python:io.StringIO`).
sep : str, defaults to ``','`` for :func:`read_csv`, ``\t`` for :func:`read_table`
  Delimiter to use. If sep is ``None``, the C engine cannot automatically detect
  the separator, but the Python parsing engine can, meaning the latter will be
  used and automatically detect the separator by Python's builtin sniffer tool,
  :class:`python:csv.Sniffer`. In addition, separators longer than 1 character and
  different from ``'\s+'`` will be interpreted as regular expressions and
  will also force the use of the Python parsing engine. Note that regex
  delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.
delimiter : str, default ``None``
  Alternative argument name for sep.
delim_whitespace : boolean, default False
  Specifies whether or not whitespace (e.g. ``' '`` or ``'\t'``)
  will be used as the delimiter. Equivalent to setting ``sep='\s+'``.
  If this option is set to ``True``, nothing should be passed in for the
  ``delimiter`` parameter.

Column and index locations and names
++++++++++++++++++++++++++++++++++++

header : int or list of ints, default ``'infer'``
  Row number(s) to use as the column names, and the start of the
  data. Default behavior is to infer the column names: if no names are
  passed the behavior is identical to ``header=0`` and column names
  are inferred from the first line of the file, if column names are
  passed explicitly then the behavior is identical to
  ``header=None``. Explicitly pass ``header=0`` to be able to replace
  existing names.

  The header can be a list of ints that specify row locations
  for a MultiIndex on the columns e.g. ``[0,1,3]``. Intervening rows
  that are not specified will be skipped (e.g. 2 in this example is
  skipped). Note that this parameter ignores commented lines and empty
  lines if ``skip_blank_lines=True``, so header=0 denotes the first
  line of data rather than the first line of the file.
names : array-like, default ``None``
  List of column names to use. If file contains no header row, then you should
  explicitly pass ``header=None``. Duplicates in this list are not allowed.
index_col : int, str, sequence of int / str, or False, optional, default ``None``
  Column(s) to use as the row labels of the ``DataFrame``, either given as
  string name or column index. If a sequence of int / str is given, a
  MultiIndex is used.

  Note: ``index_col=False`` can be used to force pandas to *not* use the first
  column as the index, e.g. when you have a malformed file with delimiters at
  the end of each line.

  The default value of ``None`` instructs pandas to guess. If the number of
  fields in the column header row is equal to the number of fields in the body
  of the data file, then a default index is used.  If it is larger, then
  the first columns are used as index so that the remaining number of fields in
  the body are equal to the number of fields in the header.

  The first row after the header is used to determine the number of columns,
  which will go into the index. If the subsequent rows contain less columns
  than the first row, they are filled with ``NaN``.

  This can be avoided through ``usecols``. This ensures that the columns are
  taken as is and the trailing data are ignored.
usecols : list-like or callable, default ``None``
  Return a subset of the columns. If list-like, all elements must either
  be positional (i.e. integer indices into the document columns) or strings
  that correspond to column names provided either by the user in ``names`` or
  inferred from the document header row(s). If ``names`` are given, the document
  header row(s) are not taken into account. For example, a valid list-like
  ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.

  Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``. To
  instantiate a DataFrame from ``data`` with element order preserved use
  ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns
  in ``['foo', 'bar']`` order or
  ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]`` for
  ``['bar', 'foo']`` order.

  If callable, the callable function will be evaluated against the column names,
  returning names where the callable function evaluates to True:

  .. ipython:: python

     import pandas as pd
     from io import StringIO

     data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
     pd.read_csv(StringIO(data))
     pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in ["COL1", "COL3"])

  Using this parameter results in much faster parsing time and lower memory usage
  when using the c engine. The Python engine loads the data first before deciding
  which columns to drop.
squeeze : boolean, default ``False``
  If the parsed data only contains one column then return a ``Series``.

  .. deprecated:: 1.4.0
     Append ``.squeeze("columns")`` to the call to ``{func_name}`` to squeeze
     the data.
prefix : str, default ``None``
  Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...

  .. deprecated:: 1.4.0
     Use a list comprehension on the DataFrame's columns after calling ``read_csv``.

  .. ipython:: python

     data = "col1,col2,col3\na,b,1"

     df = pd.read_csv(StringIO(data))
     df.columns = [f"pre_{col}" for col in df.columns]
     df

mangle_dupe_cols : boolean, default ``True``
  Duplicate columns will be specified as 'X', 'X.1'...'X.N', rather than 'X'...'X'.
  Passing in ``False`` will cause data to be overwritten if there are duplicate
  names in the columns.

General parsing configuration
+++++++++++++++++++++++++++++

dtype : Type name or dict of column -> type, default ``None``
  Data type for data or columns. E.g. ``{'a': np.float64, 'b': np.int32}``
  (unsupported with ``engine='python'``). Use ``str`` or ``object`` together
  with suitable ``na_values`` settings to preserve and
  not interpret dtype.
engine : {``'c'``, ``'python'``, ``'pyarrow'``}
  Parser engine to use. The C and pyarrow engines are faster, while the python engine
  is currently more feature-complete. Multithreading is currently only supported by
  the pyarrow engine.

  .. versionadded:: 1.4.0

     The "pyarrow" engine was added as an *experimental* engine, and some features
     are unsupported, or may not work correctly, with this engine.
converters : dict, default ``None``
  Dict of functions for converting values in certain columns. Keys can either be
  integers or column labels.
true_values : list, default ``None``
  Values to consider as ``True``.
false_values : list, default ``None``
  Values to consider as ``False``.
skipinitialspace : boolean, default ``False``
  Skip spaces after delimiter.
skiprows : list-like or integer, default ``None``
  Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
  of the file.

  If callable, the callable function will be evaluated against the row
  indices, returning True if the row should be skipped and False otherwise:

  .. ipython:: python

     data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
     pd.read_csv(StringIO(data))
     pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)

skipfooter : int, default ``0``
  Number of lines at bottom of file to skip (unsupported with engine='c').

nrows : int, default ``None``
  Number of rows of file to read. Useful for reading pieces of large files.
low_memory : boolean, default ``True``
  Internally process the file in chunks, resulting in lower memory use
  while parsing, but possibly mixed type inference.  To ensure no mixed
  types either set ``False``, or specify the type with the ``dtype`` parameter.
  Note that the entire file is read into a single ``DataFrame`` regardless,
  use the ``chunksize`` or ``iterator`` parameter to return the data in chunks.
  (Only valid with C parser)
memory_map : boolean, default False
  If a filepath is provided for ``filepath_or_buffer``, map the file object
  directly onto memory and access the data directly from there. Using this
  option can improve performance because there is no longer any I/O overhead.

NA and missing data handling
++++++++++++++++++++++++++++

na_values : scalar, str, list-like, or dict, default ``None``
  Additional strings to recognize as NA/NaN. If dict passed, specific per-column
  NA values. See :ref:`na values const <io.navaluesconst>` below
  for a list of the values interpreted as NaN by default.

keep_default_na : boolean, default ``True``
  Whether or not to include the default NaN values when parsing the data.
  Depending on whether ``na_values`` is passed in, the behavior is as follows:

  * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``
    is appended to the default NaN values used for parsing.
  * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only
    the default NaN values are used for parsing.
  * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only
    the NaN values specified ``na_values`` are used for parsing.
  * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no
    strings will be parsed as NaN.

  Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and
  ``na_values`` parameters will be ignored.
na_filter : boolean, default ``True``
  Detect missing value markers (empty strings and the value of na_values). In
  data without any NAs, passing ``na_filter=False`` can improve the performance
  of reading a large file.
verbose : boolean, default ``False``
  Indicate number of NA values placed in non-numeric columns.
skip_blank_lines : boolean, default ``True``
  If ``True``, skip over blank lines rather than interpreting as NaN values.

.. _io.read_csv_table.datetime:

Datetime handling
+++++++++++++++++

parse_dates : boolean or list of ints or names or list of lists or dict, default ``False``.
  * If ``True`` -> try parsing the index.
  * If ``[1, 2, 3]`` ->  try parsing columns 1, 2, 3 each as a separate date
    column.
  * If ``[[1, 3]]`` -> combine columns 1 and 3 and parse as a single date
    column.
  * If ``{'foo': [1, 3]}`` -> parse columns 1, 3 as date and call result 'foo'.
    A fast-path exists for iso8601-formatted dates.
infer_datetime_format : boolean, default ``False``
  If ``True`` and parse_dates is enabled for a column, attempt to infer the
  datetime format to speed up the processing.
keep_date_col : boolean, default ``False``
  If ``True`` and parse_dates specifies combining multiple columns then keep the
  original columns.
date_parser : function, default ``None``
  Function to use for converting a sequence of string columns to an array of
  datetime instances. The default uses ``dateutil.parser.parser`` to do the
  conversion. pandas will try to call date_parser in three different ways,
  advancing to the next if an exception occurs: 1) Pass one or more arrays (as
  defined by parse_dates) as arguments; 2) concatenate (row-wise) the string
  values from the columns defined by parse_dates into a single array and pass
  that; and 3) call date_parser once for each row using one or more strings
  (corresponding to the columns defined by parse_dates) as arguments.
dayfirst : boolean, default ``False``
  DD/MM format dates, international and European format.
cache_dates : boolean, default True
  If True, use a cache of unique, converted dates to apply the datetime
  conversion. May produce significant speed-up when parsing duplicate
  date strings, especially ones with timezone offsets.

  .. versionadded:: 0.25.0

Iteration
+++++++++

iterator : boolean, default ``False``
  Return ``TextFileReader`` object for iteration or getting chunks with
  ``get_chunk()``.
chunksize : int, default ``None``
  Return ``TextFileReader`` object for iteration. See :ref:`iterating and chunking
  <io.chunking>` below.

Quoting, compression, and file format
+++++++++++++++++++++++++++++++++++++

compression : {``'infer'``, ``'gzip'``, ``'bz2'``, ``'zip'``, ``'xz'``, ``'zstd'``, ``None``, ``dict``}, default ``'infer'``
  For on-the-fly decompression of on-disk data. If 'infer', then use gzip,
  bz2, zip, xz, or zstandard if ``filepath_or_buffer`` is path-like ending in '.gz', '.bz2',
  '.zip', '.xz', '.zst', respectively, and no decompression otherwise. If using 'zip',
  the ZIP file must contain only one data file to be read in.
  Set to ``None`` for no decompression. Can also be a dict with key ``'method'``
  set to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``} and other key-value pairs are
  forwarded to ``zipfile.ZipFile``, ``gzip.GzipFile``, ``bz2.BZ2File``, or ``zstandard.ZstdDecompressor``.
  As an example, the following could be passed for faster compression and to
  create a reproducible gzip archive:
  ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.

  .. versionchanged:: 1.1.0 dict option extended to support ``gzip`` and ``bz2``.
  .. versionchanged:: 1.2.0 Previous versions forwarded dict entries for 'gzip' to ``gzip.open``.
thousands : str, default ``None``
  Thousands separator.
decimal : str, default ``'.'``
  Character to recognize as decimal point. E.g. use ``','`` for European data.
float_precision : string, default None
  Specifies which converter the C engine should use for floating-point values.
  The options are ``None`` for the ordinary converter, ``high`` for the
  high-precision converter, and ``round_trip`` for the round-trip converter.
lineterminator : str (length 1), default ``None``
  Character to break file into lines. Only valid with C parser.
quotechar : str (length 1)
  The character used to denote the start and end of a quoted item. Quoted items
  can include the delimiter and it will be ignored.
quoting : int or ``csv.QUOTE_*`` instance, default ``0``
  Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of
  ``QUOTE_MINIMAL`` (0), ``QUOTE_ALL`` (1), ``QUOTE_NONNUMERIC`` (2) or
  ``QUOTE_NONE`` (3).
doublequote : boolean, default ``True``
   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``,
   indicate whether or not to interpret two consecutive ``quotechar`` elements
   **inside** a field as a single ``quotechar`` element.
escapechar : str (length 1), default ``None``
  One-character string used to escape delimiter when quoting is ``QUOTE_NONE``.
comment : str, default ``None``
  Indicates remainder of line should not be parsed. If found at the beginning of
  a line, the line will be ignored altogether. This parameter must be a single
  character. Like empty lines (as long as ``skip_blank_lines=True``), fully
  commented lines are ignored by the parameter ``header`` but not by ``skiprows``.
  For example, if ``comment='#'``, parsing '#empty\\na,b,c\\n1,2,3' with
  ``header=0`` will result in 'a,b,c' being treated as the header.
encoding : str, default ``None``
  Encoding to use for UTF when reading/writing (e.g. ``'utf-8'``). `List of
  Python standard encodings
  <https://docs.python.org/3/library/codecs.html#standard-encodings>`_.
dialect : str or :class:`python:csv.Dialect` instance, default ``None``
  If provided, this parameter will override values (default or not) for the
  following parameters: ``delimiter``, ``doublequote``, ``escapechar``,
  ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to
  override values, a ParserWarning will be issued. See :class:`python:csv.Dialect`
  documentation for more details.

Error handling
++++++++++++++

error_bad_lines : boolean, optional, default ``None``
  Lines with too many fields (e.g. a csv line with too many commas) will by
  default cause an exception to be raised, and no ``DataFrame`` will be
  returned. If ``False``, then these "bad lines" will dropped from the
  ``DataFrame`` that is returned. See :ref:`bad lines <io.bad_lines>`
  below.

  .. deprecated:: 1.3.0
     The ``on_bad_lines`` parameter should be used instead to specify behavior upon
     encountering a bad line instead.
warn_bad_lines : boolean, optional, default ``None``
  If error_bad_lines is ``False``, and warn_bad_lines is ``True``, a warning for
  each "bad line" will be output.

  .. deprecated:: 1.3.0
     The ``on_bad_lines`` parameter should be used instead to specify behavior upon
     encountering a bad line instead.
on_bad_lines : {{'error', 'warn', 'skip'}}, default 'error'
    Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :

        - 'error', raise an ParserError when a bad line is encountered.
        - 'warn', print a warning when a bad line is encountered and skip that line.
        - 'skip', skip bad lines without raising or warning when they are encountered.

    .. versionadded:: 1.3.0

.. _io.dtypes:

Specifying column data types
''''''''''''''''''''''''''''

You can indicate the data type for the whole ``DataFrame`` or individual
columns:

.. ipython:: python

    import numpy as np

    data = "a,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11"
    print(data)

    df = pd.read_csv(StringIO(data), dtype=object)
    df
    df["a"][0]
    df = pd.read_csv(StringIO(data), dtype={"b": object, "c": np.float64, "d": "Int64"})
    df.dtypes

Fortunately, pandas offers more than one way to ensure that your column(s)
contain only one ``dtype``. If you're unfamiliar with these concepts, you can
see :ref:`here<basics.dtypes>` to learn more about dtypes, and
:ref:`here<basics.object_conversion>` to learn more about ``object`` conversion in
pandas.


For instance, you can use the ``converters`` argument
of :func:`~pandas.read_csv`:

.. ipython:: python

    data = "col_1\n1\n2\n'A'\n4.22"
    df = pd.read_csv(StringIO(data), converters={"col_1": str})
    df
    df["col_1"].apply(type).value_counts()

Or you can use the :func:`~pandas.to_numeric` function to coerce the
dtypes after reading in the data,

.. ipython:: python

    df2 = pd.read_csv(StringIO(data))
    df2["col_1"] = pd.to_numeric(df2["col_1"], errors="coerce")
    df2
    df2["col_1"].apply(type).value_counts()

which will convert all valid parsing to floats, leaving the invalid parsing
as ``NaN``.

Ultimately, how you deal with reading in columns containing mixed dtypes
depends on your specific needs. In the case above, if you wanted to ``NaN`` out
the data anomalies, then :func:`~pandas.to_numeric` is probably your best option.
However, if you wanted for all the data to be coerced, no matter the type, then
using the ``converters`` argument of :func:`~pandas.read_csv` would certainly be
worth trying.

.. note::
   In some cases, reading in abnormal data with columns containing mixed dtypes
   will result in an inconsistent dataset. If you rely on pandas to infer the
   dtypes of your columns, the parsing engine will go and infer the dtypes for
   different chunks of the data, rather than the whole dataset at once. Consequently,
   you can end up with column(s) with mixed dtypes. For example,

   .. ipython:: python
        :okwarning:

        col_1 = list(range(500000)) + ["a", "b"] + list(range(500000))
        df = pd.DataFrame({"col_1": col_1})
        df.to_csv("foo.csv")
        mixed_df = pd.read_csv("foo.csv")
        mixed_df["col_1"].apply(type).value_counts()
        mixed_df["col_1"].dtype

   will result with ``mixed_df`` containing an ``int`` dtype for certain chunks
   of the column, and ``str`` for others due to the mixed dtypes from the
   data that was read in. It is important to note that the overall column will be
   marked with a ``dtype`` of ``object``, which is used for columns with mixed dtypes.

.. ipython:: python
   :suppress:

   import os

   os.remove("foo.csv")

.. _io.categorical:

Specifying categorical dtype
''''''''''''''''''''''''''''

``Categorical`` columns can be parsed directly by specifying ``dtype='category'`` or
``dtype=CategoricalDtype(categories, ordered)``.

.. ipython:: python

   data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"

   pd.read_csv(StringIO(data))
   pd.read_csv(StringIO(data)).dtypes
   pd.read_csv(StringIO(data), dtype="category").dtypes

Individual columns can be parsed as a ``Categorical`` using a dict
specification:

.. ipython:: python

   pd.read_csv(StringIO(data), dtype={"col1": "category"}).dtypes

Specifying ``dtype='category'`` will result in an unordered ``Categorical``
whose ``categories`` are the unique values observed in the data. For more
control on the categories and order, create a
:class:`~pandas.api.types.CategoricalDtype` ahead of time, and pass that for
that column's ``dtype``.

.. ipython:: python

   from pandas.api.types import CategoricalDtype

   dtype = CategoricalDtype(["d", "c", "b", "a"], ordered=True)
   pd.read_csv(StringIO(data), dtype={"col1": dtype}).dtypes

When using ``dtype=CategoricalDtype``, "unexpected" values outside of
``dtype.categories`` are treated as missing values.

.. ipython:: python

   dtype = CategoricalDtype(["a", "b", "d"])  # No 'c'
   pd.read_csv(StringIO(data), dtype={"col1": dtype}).col1

This matches the behavior of :meth:`Categorical.set_categories`.

.. note::

   With ``dtype='category'``, the resulting categories will always be parsed
   as strings (object dtype). If the categories are numeric they can be
   converted using the :func:`to_numeric` function, or as appropriate, another
   converter such as :func:`to_datetime`.

   When ``dtype`` is a ``CategoricalDtype`` with homogeneous ``categories`` (
   all numeric, all datetimes, etc.), the conversion is done automatically.

   .. ipython:: python

      df = pd.read_csv(StringIO(data), dtype="category")
      df.dtypes
      df["col3"]
      df["col3"].cat.categories = pd.to_numeric(df["col3"].cat.categories)
      df["col3"]


Naming and using columns
''''''''''''''''''''''''

.. _io.headers:

Handling column names
+++++++++++++++++++++

A file may or may not have a header row. pandas assumes the first row should be
used as the column names:

.. ipython:: python

    data = "a,b,c\n1,2,3\n4,5,6\n7,8,9"
    print(data)
    pd.read_csv(StringIO(data))

By specifying the ``names`` argument in conjunction with ``header`` you can
indicate other names to use and whether or not to throw away the header row (if
any):

.. ipython:: python

    print(data)
    pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=0)
    pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=None)

If the header is in a row other than the first, pass the row number to
``header``. This will skip the preceding rows:

.. ipython:: python

    data = "skip this skip it\na,b,c\n1,2,3\n4,5,6\n7,8,9"
    pd.read_csv(StringIO(data), header=1)

.. note::

  Default behavior is to infer the column names: if no names are
  passed the behavior is identical to ``header=0`` and column names
  are inferred from the first non-blank line of the file, if column
  names are passed explicitly then the behavior is identical to
  ``header=None``.

.. _io.dupe_names:

Duplicate names parsing
'''''''''''''''''''''''

If the file or header contains duplicate names, pandas will by default
distinguish between them so as to prevent overwriting data:

.. ipython:: python

   data = "a,b,a\n0,1,2\n3,4,5"
   pd.read_csv(StringIO(data))

There is no more duplicate data because ``mangle_dupe_cols=True`` by default,
which modifies a series of duplicate columns 'X', ..., 'X' to become
'X', 'X.1', ..., 'X.N'.  If ``mangle_dupe_cols=False``, duplicate data can
arise:

.. code-block:: ipython

   In [2]: data = 'a,b,a\n0,1,2\n3,4,5'
   In [3]: pd.read_csv(StringIO(data), mangle_dupe_cols=False)
   Out[3]:
      a  b  a
   0  2  1  2
   1  5  4  5

To prevent users from encountering this problem with duplicate data, a ``ValueError``
exception is raised if ``mangle_dupe_cols != True``:

.. code-block:: ipython

   In [2]: data = 'a,b,a\n0,1,2\n3,4,5'
   In [3]: pd.read_csv(StringIO(data), mangle_dupe_cols=False)
   ...
   ValueError: Setting mangle_dupe_cols=False is not supported yet

.. _io.usecols:

Filtering columns (``usecols``)
+++++++++++++++++++++++++++++++

The ``usecols`` argument allows you to select any subset of the columns in a
file, either using the column names, position numbers or a callable:

.. ipython:: python

    data = "a,b,c,d\n1,2,3,foo\n4,5,6,bar\n7,8,9,baz"
    pd.read_csv(StringIO(data))
    pd.read_csv(StringIO(data), usecols=["b", "d"])
    pd.read_csv(StringIO(data), usecols=[0, 2, 3])
    pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in ["A", "C"])

The ``usecols`` argument can also be used to specify which columns not to
use in the final result:

.. ipython:: python

   pd.read_csv(StringIO(data), usecols=lambda x: x not in ["a", "c"])

In this case, the callable is specifying that we exclude the "a" and "c"
columns from the output.

Comments and empty lines
''''''''''''''''''''''''

.. _io.skiplines:

Ignoring line comments and empty lines
++++++++++++++++++++++++++++++++++++++

If the ``comment`` parameter is specified, then completely commented lines will
be ignored. By default, completely blank lines will be ignored as well.

.. ipython:: python

   data = "\na,b,c\n  \n# commented line\n1,2,3\n\n4,5,6"
   print(data)
   pd.read_csv(StringIO(data), comment="#")

If ``skip_blank_lines=False``, then ``read_csv`` will not ignore blank lines:

.. ipython:: python

   data = "a,b,c\n\n1,2,3\n\n\n4,5,6"
   pd.read_csv(StringIO(data), skip_blank_lines=False)

.. warning::

   The presence of ignored lines might create ambiguities involving line numbers;
   the parameter ``header`` uses row numbers (ignoring commented/empty
   lines), while ``skiprows`` uses line numbers (including commented/empty lines):

   .. ipython:: python

      data = "#comment\na,b,c\nA,B,C\n1,2,3"
      pd.read_csv(StringIO(data), comment="#", header=1)
      data = "A,B,C\n#comment\na,b,c\n1,2,3"
      pd.read_csv(StringIO(data), comment="#", skiprows=2)

   If both ``header`` and ``skiprows`` are specified, ``header`` will be
   relative to the end of ``skiprows``. For example:

.. ipython:: python

   data = (
       "# empty\n"
       "# second empty line\n"
       "# third emptyline\n"
       "X,Y,Z\n"
       "1,2,3\n"
       "A,B,C\n"
       "1,2.,4.\n"
       "5.,NaN,10.0\n"
   )
   print(data)
   pd.read_csv(StringIO(data), comment="#", skiprows=4, header=1)

.. _io.comments:

Comments
++++++++

Sometimes comments or meta data may be included in a file:

.. ipython:: python
   :suppress:

   data = (
       "ID,level,category\n"
       "Patient1,123000,x # really unpleasant\n"
       "Patient2,23000,y # wouldn't take his medicine\n"
       "Patient3,1234018,z # awesome"
   )

   with open("tmp.csv", "w") as fh:
       fh.write(data)

.. ipython:: python

   print(open("tmp.csv").read())

By default, the parser includes the comments in the output:

.. ipython:: python

   df = pd.read_csv("tmp.csv")
   df

We can suppress the comments using the ``comment`` keyword:

.. ipython:: python

   df = pd.read_csv("tmp.csv", comment="#")
   df

.. ipython:: python
   :suppress:

   os.remove("tmp.csv")

.. _io.unicode:

Dealing with Unicode data
'''''''''''''''''''''''''

The ``encoding`` argument should be used for encoded unicode data, which will
result in byte strings being decoded to unicode in the result:

.. ipython:: python

   from io import BytesIO

   data = b"word,length\n" b"Tr\xc3\xa4umen,7\n" b"Gr\xc3\xbc\xc3\x9fe,5"
   data = data.decode("utf8").encode("latin-1")
   df = pd.read_csv(BytesIO(data), encoding="latin-1")
   df
   df["word"][1]

Some formats which encode all characters as multiple bytes, like UTF-16, won't
parse correctly at all without specifying the encoding. `Full list of Python
standard encodings
<https://docs.python.org/3/library/codecs.html#standard-encodings>`_.

.. _io.index_col:

Index columns and trailing delimiters
'''''''''''''''''''''''''''''''''''''

If a file has one more column of data than the number of column names, the
first column will be used as the ``DataFrame``'s row names:

.. ipython:: python

    data = "a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
    pd.read_csv(StringIO(data))

.. ipython:: python

    data = "index,a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
    pd.read_csv(StringIO(data), index_col=0)

Ordinarily, you can achieve this behavior using the ``index_col`` option.

There are some exception cases when a file has been prepared with delimiters at
the end of each data line, confusing the parser. To explicitly disable the
index column inference and discard the last column, pass ``index_col=False``:

.. ipython:: python

    data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
    print(data)
    pd.read_csv(StringIO(data))
    pd.read_csv(StringIO(data), index_col=False)

If a subset of data is being parsed using the ``usecols`` option, the
``index_col`` specification is based on that subset, not the original data.

.. ipython:: python

    data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
    print(data)
    pd.read_csv(StringIO(data), usecols=["b", "c"])
    pd.read_csv(StringIO(data), usecols=["b", "c"], index_col=0)

.. _io.parse_dates:

Date Handling
'''''''''''''

Specifying date columns
+++++++++++++++++++++++

To better facilitate working with datetime data, :func:`read_csv`
uses the keyword arguments ``parse_dates`` and ``date_parser``
to allow users to specify a variety of columns and date/time formats to turn the
input text data into ``datetime`` objects.

The simplest case is to just pass in ``parse_dates=True``:

.. ipython:: python
   :suppress:

   with open("foo.csv", mode="w") as f:
       f.write("date,A,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5")

.. ipython:: python

   # Use a column as an index, and parse it as dates.
   df = pd.read_csv("foo.csv", index_col=0, parse_dates=True)
   df

   # These are Python datetime objects
   df.index

It is often the case that we may want to store date and time data separately,
or store various date fields separately. the ``parse_dates`` keyword can be
used to specify a combination of columns to parse the dates and/or times from.

You can specify a list of column lists to ``parse_dates``, the resulting date
columns will be prepended to the output (so as to not affect the existing column
order) and the new column names will be the concatenation of the component
column names:

.. ipython:: python
   :suppress:

   data = (
       "KORD,19990127, 19:00:00, 18:56:00, 0.8100\n"
       "KORD,19990127, 20:00:00, 19:56:00, 0.0100\n"
       "KORD,19990127, 21:00:00, 20:56:00, -0.5900\n"
       "KORD,19990127, 21:00:00, 21:18:00, -0.9900\n"
       "KORD,19990127, 22:00:00, 21:56:00, -0.5900\n"
       "KORD,19990127, 23:00:00, 22:56:00, -0.5900"
   )

   with open("tmp.csv", "w") as fh:
       fh.write(data)

.. ipython:: python

    print(open("tmp.csv").read())
    df = pd.read_csv("tmp.csv", header=None, parse_dates=[[1, 2], [1, 3]])
    df

By default the parser removes the component date columns, but you can choose
to retain them via the ``keep_date_col`` keyword:

.. ipython:: python

   df = pd.read_csv(
       "tmp.csv", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True
   )
   df

Note that if you wish to combine multiple columns into a single date column, a
nested list must be used. In other words, ``parse_dates=[1, 2]`` indicates that
the second and third columns should each be parsed as separate date columns
while ``parse_dates=[[1, 2]]`` means the two columns should be parsed into a
single column.

You can also use a dict to specify custom name columns:

.. ipython:: python

   date_spec = {"nominal": [1, 2], "actual": [1, 3]}
   df = pd.read_csv("tmp.csv", header=None, parse_dates=date_spec)
   df

It is important to remember that if multiple text columns are to be parsed into
a single date column, then a new column is prepended to the data. The ``index_col``
specification is based off of this new set of columns rather than the original
data columns:


.. ipython:: python

   date_spec = {"nominal": [1, 2], "actual": [1, 3]}
   df = pd.read_csv(
       "tmp.csv", header=None, parse_dates=date_spec, index_col=0
   )  # index is the nominal column
   df

.. note::
   If a column or index contains an unparsable date, the entire column or
   index will be returned unaltered as an object data type. For non-standard
   datetime parsing, use :func:`to_datetime` after ``pd.read_csv``.


.. note::
   read_csv has a fast_path for parsing datetime strings in iso8601 format,
   e.g "2000-01-01T00:01:02+00:00" and similar variations. If you can arrange
   for your data to store datetimes in this format, load times will be
   significantly faster, ~20x has been observed.


Date parsing functions
++++++++++++++++++++++

Finally, the parser allows you to specify a custom ``date_parser`` function to
take full advantage of the flexibility of the date parsing API:

.. ipython:: python

   df = pd.read_csv(
       "tmp.csv", header=None, parse_dates=date_spec, date_parser=pd.to_datetime
   )
   df

pandas will try to call the ``date_parser`` function in three different ways. If
an exception is raised, the next one is tried:

1. ``date_parser`` is first called with one or more arrays as arguments,
   as defined using ``parse_dates`` (e.g., ``date_parser(['2013', '2013'], ['1', '2'])``).

2. If #1 fails, ``date_parser`` is called with all the columns
   concatenated row-wise into a single array (e.g., ``date_parser(['2013 1', '2013 2'])``).

Note that performance-wise, you should try these methods of parsing dates in order:

1. Try to infer the format using ``infer_datetime_format=True`` (see section below).

2. If you know the format, use ``pd.to_datetime()``:
   ``date_parser=lambda x: pd.to_datetime(x, format=...)``.

3. If you have a really non-standard format, use a custom ``date_parser`` function.
   For optimal performance, this should be vectorized, i.e., it should accept arrays
   as arguments.


.. ipython:: python
   :suppress:

   os.remove("tmp.csv")


.. _io.csv.mixed_timezones:

Parsing a CSV with mixed timezones
++++++++++++++++++++++++++++++++++

pandas cannot natively represent a column or index with mixed timezones. If your CSV
file contains columns with a mixture of timezones, the default result will be
an object-dtype column with strings, even with ``parse_dates``.


.. ipython:: python

   content = """\
   a
   2000-01-01T00:00:00+05:00
   2000-01-01T00:00:00+06:00"""
   df = pd.read_csv(StringIO(content), parse_dates=["a"])
   df["a"]

To parse the mixed-timezone values as a datetime column, pass a partially-applied
:func:`to_datetime` with ``utc=True`` as the ``date_parser``.

.. ipython:: python

   df = pd.read_csv(
       StringIO(content),
       parse_dates=["a"],
       date_parser=lambda col: pd.to_datetime(col, utc=True),
   )
   df["a"]


.. _io.dayfirst:


Inferring datetime format
+++++++++++++++++++++++++

If you have ``parse_dates`` enabled for some or all of your columns, and your
datetime strings are all formatted the same way, you may get a large speed
up by setting ``infer_datetime_format=True``.  If set, pandas will attempt
to guess the format of your datetime strings, and then use a faster means
of parsing the strings.  5-10x parsing speeds have been observed.  pandas
will fallback to the usual parsing if either the format cannot be guessed
or the format that was guessed cannot properly parse the entire column
of strings.  So in general, ``infer_datetime_format`` should not have any
negative consequences if enabled.

Here are some examples of datetime strings that can be guessed (All
representing December 30th, 2011 at 00:00:00):

* "20111230"
* "2011/12/30"
* "20111230 00:00:00"
* "12/30/2011 00:00:00"
* "30/Dec/2011 00:00:00"
* "30/December/2011 00:00:00"

Note that ``infer_datetime_format`` is sensitive to ``dayfirst``.  With
``dayfirst=True``, it will guess "01/12/2011" to be December 1st. With
``dayfirst=False`` (default) it will guess "01/12/2011" to be January 12th.

.. ipython:: python

   # Try to infer the format for the index column
   df = pd.read_csv(
       "foo.csv",
       index_col=0,
       parse_dates=True,
       infer_datetime_format=True,
   )
   df

.. ipython:: python
   :suppress:

   os.remove("foo.csv")

International date formats
++++++++++++++++++++++++++

While US date formats tend to be MM/DD/YYYY, many international formats use
DD/MM/YYYY instead. For convenience, a ``dayfirst`` keyword is provided:

.. ipython:: python
   :suppress:

   data = "date,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c"
   with open("tmp.csv", "w") as fh:
       fh.write(data)

.. ipython:: python

   print(open("tmp.csv").read())

   pd.read_csv("tmp.csv", parse_dates=[0])
   pd.read_csv("tmp.csv", dayfirst=True, parse_dates=[0])

Writing CSVs to binary file objects
+++++++++++++++++++++++++++++++++++

.. versionadded:: 1.2.0

``df.to_csv(..., mode="wb")`` allows writing a CSV to a file object
opened binary mode. In most cases, it is not necessary to specify
``mode`` as Pandas will auto-detect whether the file object is
opened in text or binary mode.

.. ipython:: python

   import io

   data = pd.DataFrame([0, 1, 2])
   buffer = io.BytesIO()
   data.to_csv(buffer, encoding="utf-8", compression="gzip")

.. _io.float_precision:

Specifying method for floating-point conversion
'''''''''''''''''''''''''''''''''''''''''''''''

The parameter ``float_precision`` can be specified in order to use
a specific floating-point converter during parsing with the C engine.
The options are the ordinary converter, the high-precision converter, and
the round-trip converter (which is guaranteed to round-trip values after
writing to a file). For example:

.. ipython:: python

   val = "0.3066101993807095471566981359501369297504425048828125"
   data = "a,b,c\n1,2,{0}".format(val)
   abs(
       pd.read_csv(
           StringIO(data),
           engine="c",
           float_precision=None,
       )["c"][0] - float(val)
   )
   abs(
       pd.read_csv(
           StringIO(data),
           engine="c",
           float_precision="high",
       )["c"][0] - float(val)
   )
   abs(
       pd.read_csv(StringIO(data), engine="c", float_precision="round_trip")["c"][0]
       - float(val)
   )


.. _io.thousands:

Thousand separators
'''''''''''''''''''

For large numbers that have been written with a thousands separator, you can
set the ``thousands`` keyword to a string of length 1 so that integers will be parsed
correctly:

.. ipython:: python
   :suppress:

   data = (
       "ID|level|category\n"
       "Patient1|123,000|x\n"
       "Patient2|23,000|y\n"
       "Patient3|1,234,018|z"
   )

   with open("tmp.csv", "w") as fh:
       fh.write(data)

By default, numbers with a thousands separator will be parsed as strings:

.. ipython:: python

    print(open("tmp.csv").read())
    df = pd.read_csv("tmp.csv", sep="|")
    df

    df.level.dtype

The ``thousands`` keyword allows integers to be parsed correctly:

.. ipython:: python

    print(open("tmp.csv").read())
    df = pd.read_csv("tmp.csv", sep="|", thousands=",")
    df

    df.level.dtype

.. ipython:: python
   :suppress:

   os.remove("tmp.csv")

.. _io.na_values:

NA values
'''''''''

To control which values are parsed as missing values (which are signified by
``NaN``), specify a string in ``na_values``. If you specify a list of strings,
then all values in it are considered to be missing values. If you specify a
number (a ``float``, like ``5.0`` or an ``integer`` like ``5``), the
corresponding equivalent values will also imply a missing value (in this case
effectively ``[5.0, 5]`` are recognized as ``NaN``).

To completely override the default values that are recognized as missing, specify ``keep_default_na=False``.

.. _io.navaluesconst:

The default ``NaN`` recognized values are ``['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A',
'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', '']``.

Let us consider some examples:

.. code-block:: python

   pd.read_csv("path_to_file.csv", na_values=[5])

In the example above ``5`` and ``5.0`` will be recognized as ``NaN``, in
addition to the defaults. A string will first be interpreted as a numerical
``5``, then as a ``NaN``.

.. code-block:: python

   pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=[""])

Above, only an empty field will be recognized as ``NaN``.

.. code-block:: python

   pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=["NA", "0"])

Above, both ``NA`` and ``0`` as strings are ``NaN``.

.. code-block:: python

   pd.read_csv("path_to_file.csv", na_values=["Nope"])

The default values, in addition to the string ``"Nope"`` are recognized as
``NaN``.

.. _io.infinity:

Infinity
''''''''

``inf`` like values will be parsed as ``np.inf`` (positive infinity), and ``-inf`` as ``-np.inf`` (negative infinity).
These will ignore the case of the value, meaning ``Inf``, will also be parsed as ``np.inf``.


Returning Series
''''''''''''''''

Using the ``squeeze`` keyword, the parser will return output with a single column
as a ``Series``:

.. deprecated:: 1.4.0
   Users should append ``.squeeze("columns")`` to the DataFrame returned by
   ``read_csv`` instead.

.. ipython:: python
   :suppress:

   data = "level\nPatient1,123000\nPatient2,23000\nPatient3,1234018"

   with open("tmp.csv", "w") as fh:
       fh.write(data)

.. ipython:: python
   :okwarning:

   print(open("tmp.csv").read())

   output = pd.read_csv("tmp.csv", squeeze=True)
   output

   type(output)

.. ipython:: python
   :suppress:

   os.remove("tmp.csv")

.. _io.boolean:

Boolean values
''''''''''''''

The common values ``True``, ``False``, ``TRUE``, and ``FALSE`` are all
recognized as boolean. Occasionally you might want to recognize other values
as being boolean. To do this, use the ``true_values`` and ``false_values``
options as follows:

.. ipython:: python

    data = "a,b,c\n1,Yes,2\n3,No,4"
    print(data)
    pd.read_csv(StringIO(data))
    pd.read_csv(StringIO(data), true_values=["Yes"], false_values=["No"])

.. _io.bad_lines:

Handling "bad" lines
''''''''''''''''''''

Some files may have malformed lines with too few fields or too many. Lines with
too few fields will have NA values filled in the trailing fields. Lines with
too many fields will raise an error by default:

.. ipython:: python
    :okexcept:

    data = "a,b,c\n1,2,3\n4,5,6,7\n8,9,10"
    pd.read_csv(StringIO(data))

You can elect to skip bad lines:

.. code-block:: ipython

    In [29]: pd.read_csv(StringIO(data), on_bad_lines="warn")
    Skipping line 3: expected 3 fields, saw 4

    Out[29]:
       a  b   c
    0  1  2   3
    1  8  9  10

Or pass a callable function to handle the bad line if ``engine="python"``.
The bad line will be a list of strings that was split by the ``sep``:

.. code-block:: ipython

    In [29]: external_list = []

    In [30]: def bad_lines_func(line):
        ...:     external_list.append(line)
        ...:     return line[-3:]

    In [31]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine="python")
    Out[31]:
       a  b   c
    0  1  2   3
    1  5  6   7
    2  8  9  10

    In [32]: external_list
    Out[32]: [4, 5, 6, 7]

    .. versionadded:: 1.4.0


You can also use the ``usecols`` parameter to eliminate extraneous column
data that appear in some lines but not others:

.. code-block:: ipython

   In [33]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])

    Out[33]:
       a  b   c
    0  1  2   3
    1  4  5   6
    2  8  9  10

In case you want to keep all data including the lines with too many fields, you can
specify a sufficient number of ``names``. This ensures that lines with not enough
fields are filled with ``NaN``.

.. code-block:: ipython

   In [34]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])

   Out[34]:
       a  b   c  d
    0  1  2   3  NaN
    1  4  5   6  7
    2  8  9  10  NaN

.. _io.dialect:

Dialect
'''''''

The ``dialect`` keyword gives greater flexibility in specifying the file format.
By default it uses the Excel dialect but you can specify either the dialect name
or a :class:`python:csv.Dialect` instance.

.. ipython:: python
   :suppress:

   data = "label1,label2,label3\n" 'index1,"a,c,e\n' "index2,b,d,f"

Suppose you had data with unenclosed quotes:

.. ipython:: python

   print(data)

By default, ``read_csv`` uses the Excel dialect and treats the double quote as
the quote character, which causes it to fail when it finds a newline before it
finds the closing double quote.

We can get around this using ``dialect``:

.. ipython:: python
   :okwarning:

   import csv

   dia = csv.excel()
   dia.quoting = csv.QUOTE_NONE
   pd.read_csv(StringIO(data), dialect=dia)

All of the dialect options can be specified separately by keyword arguments:

.. ipython:: python

    data = "a,b,c~1,2,3~4,5,6"
    pd.read_csv(StringIO(data), lineterminator="~")

Another common dialect option is ``skipinitialspace``, to skip any whitespace
after a delimiter:

.. ipython:: python

   data = "a, b, c\n1, 2, 3\n4, 5, 6"
   print(data)
   pd.read_csv(StringIO(data), skipinitialspace=True)

The parsers make every attempt to "do the right thing" and not be fragile. Type
inference is a pretty big deal. If a column can be coerced to integer dtype
without altering the contents, the parser will do so. Any non-numeric
columns will come through as object dtype as with the rest of pandas objects.

.. _io.quoting:

Quoting and Escape Characters
'''''''''''''''''''''''''''''

Quotes (and other escape characters) in embedded fields can be handled in any
number of ways. One way is to use backslashes; to properly parse this data, you
should pass the ``escapechar`` option:

.. ipython:: python

   data = 'a,b\n"hello, \\"Bob\\", nice to see you",5'
   print(data)
   pd.read_csv(StringIO(data), escapechar="\\")

.. _io.fwf_reader:
.. _io.fwf:

Files with fixed width columns
''''''''''''''''''''''''''''''

While :func:`read_csv` reads delimited data, the :func:`read_fwf` function works
with data files that have known and fixed column widths. The function parameters
to ``read_fwf`` are largely the same as ``read_csv`` with two extra parameters, and
a different usage of the ``delimiter`` parameter:

* ``colspecs``: A list of pairs (tuples) giving the extents of the
  fixed-width fields of each line as half-open intervals (i.e.,  [from, to[ ).
  String value 'infer' can be used to instruct the parser to try detecting
  the column specifications from the first 100 rows of the data. Default
  behavior, if not specified, is to infer.
* ``widths``: A list of field widths which can be used instead of 'colspecs'
  if the intervals are contiguous.
* ``delimiter``: Characters to consider as filler characters in the fixed-width file.
  Can be used to specify the filler character of the fields
  if it is not spaces (e.g., '~').

.. ipython:: python
   :suppress:

   data1 = (
       "id8141    360.242940   149.910199   11950.7\n"
       "id1594    444.953632   166.985655   11788.4\n"
       "id1849    364.136849   183.628767   11806.2\n"
       "id1230    413.836124   184.375703   11916.8\n"
       "id1948    502.953953   173.237159   12468.3"
   )
   with open("bar.csv", "w") as f:
       f.write(data1)

Consider a typical fixed-width data file:

.. ipython:: python

   print(open("bar.csv").read())

In order to parse this file into a ``DataFrame``, we simply need to supply the
column specifications to the ``read_fwf`` function along with the file name:

.. ipython:: python

   # Column specifications are a list of half-intervals
   colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]
   df = pd.read_fwf("bar.csv", colspecs=colspecs, header=None, index_col=0)
   df

Note how the parser automatically picks column names X.<column number> when
``header=None`` argument is specified. Alternatively, you can supply just the
column widths for contiguous columns:

.. ipython:: python

   # Widths are a list of integers
   widths = [6, 14, 13, 10]
   df = pd.read_fwf("bar.csv", widths=widths, header=None)
   df

The parser will take care of extra white spaces around the columns
so it's ok to have extra separation between the columns in the file.

By default, ``read_fwf`` will try to infer the file's ``colspecs`` by using the
first 100 rows of the file. It can do it only in cases when the columns are
aligned and correctly separated by the provided ``delimiter`` (default delimiter
is whitespace).

.. ipython:: python

   df = pd.read_fwf("bar.csv", header=None, index_col=0)
   df

``read_fwf`` supports the ``dtype`` parameter for specifying the types of
parsed columns to be different from the inferred type.

.. ipython:: python

   pd.read_fwf("bar.csv", header=None, index_col=0).dtypes
   pd.read_fwf("bar.csv", header=None, dtype={2: "object"}).dtypes

.. ipython:: python
   :suppress:

   os.remove("bar.csv")


Indexes
'''''''

Files with an "implicit" index column
+++++++++++++++++++++++++++++++++++++

.. ipython:: python
   :suppress:

   f = open("foo.csv", "w")
   f.write("A,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5")
   f.close()

Consider a file with one less entry in the header than the number of data
column:

.. ipython:: python

   print(open("foo.csv").read())

In this special case, ``read_csv`` assumes that the first column is to be used
as the index of the ``DataFrame``:

.. ipython:: python

   pd.read_csv("foo.csv")

Note that the dates weren't automatically parsed. In that case you would need
to do as before:

.. ipython:: python

   df = pd.read_csv("foo.csv", parse_dates=True)
   df.index

.. ipython:: python
   :suppress:

   os.remove("foo.csv")


Reading an index with a ``MultiIndex``
++++++++++++++++++++++++++++++++++++++

.. _io.csv_multiindex:

Suppose you have data indexed by two columns:

.. ipython:: python

   print(open("data/mindex_ex.csv").read())

The ``index_col`` argument to ``read_csv`` can take a list of
column numbers to turn multiple columns into a ``MultiIndex`` for the index of the
returned object:

.. ipython:: python

   df = pd.read_csv("data/mindex_ex.csv", index_col=[0, 1])
   df
   df.loc[1978]

.. _io.multi_index_columns:

Reading columns with a ``MultiIndex``
+++++++++++++++++++++++++++++++++++++

By specifying list of row locations for the ``header`` argument, you
can read in a ``MultiIndex`` for the columns. Specifying non-consecutive
rows will skip the intervening rows.

.. ipython:: python

   from pandas._testing import makeCustomDataframe as mkdf

   df = mkdf(5, 3, r_idx_nlevels=2, c_idx_nlevels=4)
   df.to_csv("mi.csv")
   print(open("mi.csv").read())
   pd.read_csv("mi.csv", header=[0, 1, 2, 3], index_col=[0, 1])

``read_csv`` is also able to interpret a more common format
of multi-columns indices.

.. ipython:: python
   :suppress:

   data = ",a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12"
   with open("mi2.csv", "w") as fh:
       fh.write(data)

.. ipython:: python

   print(open("mi2.csv").read())
   pd.read_csv("mi2.csv", header=[0, 1], index_col=0)

Note: If an ``index_col`` is not specified (e.g. you don't have an index, or wrote it
with ``df.to_csv(..., index=False)``, then any ``names`` on the columns index will be *lost*.

.. ipython:: python
   :suppress:

   os.remove("mi.csv")
   os.remove("mi2.csv")

.. _io.sniff:

Automatically "sniffing" the delimiter
''''''''''''''''''''''''''''''''''''''

``read_csv`` is capable of inferring delimited (not necessarily
comma-separated) files, as pandas uses the :class:`python:csv.Sniffer`
class of the csv module. For this, you have to specify ``sep=None``.

.. ipython:: python
   :suppress:

   df = pd.DataFrame(np.random.randn(10, 4))
   df.to_csv("tmp.sv", sep="|")
   df.to_csv("tmp2.sv", sep=":")

.. ipython:: python

   print(open("tmp2.sv").read())
   pd.read_csv("tmp2.sv", sep=None, engine="python")

.. _io.multiple_files:

Reading multiple files to create a single DataFrame
'''''''''''''''''''''''''''''''''''''''''''''''''''

It's best to use :func:`~pandas.concat` to combine multiple files.
See the :ref:`cookbook<cookbook.csv.multiple_files>` for an example.

.. _io.chunking:

Iterating through files chunk by chunk
''''''''''''''''''''''''''''''''''''''

Suppose you wish to iterate through a (potentially very large) file lazily
rather than reading the entire file into memory, such as the following:


.. ipython:: python

   print(open("tmp.sv").read())
   table = pd.read_csv("tmp.sv", sep="|")
   table


By specifying a ``chunksize`` to ``read_csv``, the return
value will be an iterable object of type ``TextFileReader``:

.. ipython:: python

   with pd.read_csv("tmp.sv", sep="|", chunksize=4) as reader:
       reader
       for chunk in reader:
           print(chunk)

.. versionchanged:: 1.2

  ``read_csv/json/sas`` return a context-manager when iterating through a file.

Specifying ``iterator=True`` will also return the ``TextFileReader`` object:

.. ipython:: python

   with pd.read_csv("tmp.sv", sep="|", iterator=True) as reader:
       reader.get_chunk(5)

.. ipython:: python
   :suppress:

   os.remove("tmp.sv")
   os.remove("tmp2.sv")

Specifying the parser engine
''''''''''''''''''''''''''''

Pandas currently supports three engines, the C engine, the python engine, and an experimental
pyarrow engine (requires the ``pyarrow`` package). In general, the pyarrow engine is fastest
on larger workloads and is equivalent in speed to the C engine on most other workloads.
The python engine tends to be slower than the pyarrow and C engines on most workloads. However,
the pyarrow engine is much less robust than the C engine, which lacks a few features compared to the
Python engine.

Where possible, pandas uses the C parser (specified as ``engine='c'``), but it may fall
back to Python if C-unsupported options are specified.

Currently, options unsupported by the C and pyarrow engines include:

* ``sep`` other than a single character (e.g. regex separators)
* ``skipfooter``
* ``sep=None`` with ``delim_whitespace=False``

Specifying any of the above options will produce a ``ParserWarning`` unless the
python engine is selected explicitly using ``engine='python'``.

Options that are unsupported by the pyarrow engine which are not covered by the list above include:

* ``float_precision``
* ``chunksize``
* ``comment``
* ``nrows``
* ``thousands``
* ``memory_map``
* ``dialect``
* ``warn_bad_lines``
* ``error_bad_lines``
* ``on_bad_lines``
* ``delim_whitespace``
* ``quoting``
* ``lineterminator``
* ``converters``
* ``decimal``
* ``iterator``
* ``dayfirst``
* ``infer_datetime_format``
* ``verbose``
* ``skipinitialspace``
* ``low_memory``

Specifying these options with ``engine='pyarrow'`` will raise a ``ValueError``.

.. _io.remote:

Reading/writing remote files
''''''''''''''''''''''''''''

You can pass in a URL to read or write remote files to many of pandas' IO
functions - the following example shows reading a CSV file:

.. code-block:: python

   df = pd.read_csv("https://download.bls.gov/pub/time.series/cu/cu.item", sep="\t")

.. versionadded:: 1.3.0

A custom header can be sent alongside HTTP(s) requests by passing a dictionary
of header key value mappings to the ``storage_options`` keyword argument as shown below:

.. code-block:: python

   headers = {"User-Agent": "pandas"}
   df = pd.read_csv(
       "https://download.bls.gov/pub/time.series/cu/cu.item",
       sep="\t",
       storage_options=headers
   )

All URLs which are not local files or HTTP(s) are handled by
`fsspec`_, if installed, and its various filesystem implementations
(including Amazon S3, Google Cloud, SSH, FTP, webHDFS...).
Some of these implementations will require additional packages to be
installed, for example
S3 URLs require the `s3fs
<https://pypi.org/project/s3fs/>`_ library:

.. code-block:: python

   df = pd.read_json("s3://pandas-test/adatafile.json")

When dealing with remote storage systems, you might need
extra configuration with environment variables or config files in
special locations. For example, to access data in your S3 bucket,
you will need to define credentials in one of the several ways listed in
the `S3Fs documentation
<https://s3fs.readthedocs.io/en/latest/#credentials>`_. The same is true
for several of the storage backends, and you should follow the links
at `fsimpl1`_ for implementations built into ``fsspec`` and `fsimpl2`_
for those not included in the main ``fsspec``
distribution.

You can also pass parameters directly to the backend driver. For example,
if you do *not* have S3 credentials, you can still access public data by
specifying an anonymous connection, such as

.. versionadded:: 1.2.0

.. code-block:: python

   pd.read_csv(
       "s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013"
       "-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv",
       storage_options={"anon": True},
   )

``fsspec`` also allows complex URLs, for accessing data in compressed
archives, local caching of files, and more. To locally cache the above
example, you would modify the call to

.. code-block:: python

   pd.read_csv(
       "simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/"
       "SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv",
       storage_options={"s3": {"anon": True}},
   )

where we specify that the "anon" parameter is meant for the "s3" part of
the implementation, not to the caching implementation. Note that this caches to a temporary
directory for the duration of the session only, but you can also specify
a permanent store.

.. _fsspec: https://filesystem-spec.readthedocs.io/en/latest/
.. _fsimpl1: https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations
.. _fsimpl2: https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations

Writing out data
''''''''''''''''

.. _io.store_in_csv:

Writing to CSV format
+++++++++++++++++++++

The ``Series`` and ``DataFrame`` objects have an instance method ``to_csv`` which
allows storing the contents of the object as a comma-separated-values file. The
function takes a number of arguments. Only the first is required.

* ``path_or_buf``: A string path to the file to write or a file object.  If a file object it must be opened with ``newline=''``
* ``sep`` : Field delimiter for the output file (default ",")
* ``na_rep``: A string representation of a missing value (default '')
* ``float_format``: Format string for floating point numbers
* ``columns``: Columns to write (default None)
* ``header``: Whether to write out the column names (default True)
* ``index``: whether to write row (index) names (default True)
* ``index_label``: Column label(s) for index column(s) if desired. If None
  (default), and ``header`` and ``index`` are True, then the index names are
  used. (A sequence should be given if the ``DataFrame`` uses MultiIndex).
* ``mode`` : Python write mode, default 'w'
* ``encoding``: a string representing the encoding to use if the contents are
  non-ASCII, for Python versions prior to 3
* ``lineterminator``: Character sequence denoting line end (default ``os.linesep``)
* ``quoting``: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set a ``float_format`` then floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric
* ``quotechar``: Character used to quote fields (default '"')
* ``doublequote``: Control quoting of ``quotechar`` in fields (default True)
* ``escapechar``: Character used to escape ``sep`` and ``quotechar`` when
  appropriate (default None)
* ``chunksize``: Number of rows to write at a time
* ``date_format``: Format string for datetime objects

Writing a formatted string
++++++++++++++++++++++++++

.. _io.formatting:

The ``DataFrame`` object has an instance method ``to_string`` which allows control
over the string representation of the object. All arguments are optional:

* ``buf`` default None, for example a StringIO object
* ``columns`` default None, which columns to write
* ``col_space`` default None, minimum width of each column.
* ``na_rep`` default ``NaN``, representation of NA value
* ``formatters`` default None, a dictionary (by column) of functions each of
  which takes a single argument and returns a formatted string
* ``float_format`` default None, a function which takes a single (float)
  argument and returns a formatted string; to be applied to floats in the
  ``DataFrame``.
* ``sparsify`` default True, set to False for a ``DataFrame`` with a hierarchical
  index to print every MultiIndex key at each row.
* ``index_names`` default True, will print the names of the indices
* ``index`` default True, will print the index (ie, row labels)
* ``header`` default True, will print the column labels
* ``justify`` default ``left``, will print column headers left- or
  right-justified

The ``Series`` object also has a ``to_string`` method, but with only the ``buf``,
``na_rep``, ``float_format`` arguments. There is also a ``length`` argument
which, if set to ``True``, will additionally output the length of the Series.

.. _io.json:

JSON
----

Read and write ``JSON`` format files and strings.

.. _io.json_writer:

Writing JSON
''''''''''''

A ``Series`` or ``DataFrame`` can be converted to a valid JSON string. Use ``to_json``
with optional parameters:

* ``path_or_buf`` : the pathname or buffer to write the output
  This can be ``None`` in which case a JSON string is returned
* ``orient`` :

  ``Series``:
      * default is ``index``
      * allowed values are {``split``, ``records``, ``index``}

  ``DataFrame``:
      * default is ``columns``
      * allowed values are {``split``, ``records``, ``index``, ``columns``, ``values``, ``table``}

  The format of the JSON string

  .. csv-table::
     :widths: 20, 150
     :delim: ;

     ``split``; dict like {index -> [index], columns -> [columns], data -> [values]}
     ``records``; list like [{column -> value}, ... , {column -> value}]
     ``index``; dict like {index -> {column -> value}}
     ``columns``; dict like {column -> {index -> value}}
     ``values``; just the values array
     ``table``; adhering to the JSON `Table Schema`_

* ``date_format`` : string, type of date conversion, 'epoch' for timestamp, 'iso' for ISO8601.
* ``double_precision`` : The number of decimal places to use when encoding floating point values, default 10.
* ``force_ascii`` : force encoded string to be ASCII, default True.
* ``date_unit`` : The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us' or 'ns' for seconds, milliseconds, microseconds and nanoseconds respectively. Default 'ms'.
* ``default_handler`` : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.
* ``lines`` : If ``records`` orient, then will write each record per line as json.

Note ``NaN``'s, ``NaT``'s and ``None`` will be converted to ``null`` and ``datetime`` objects will be converted based on the ``date_format`` and ``date_unit`` parameters.

.. ipython:: python

   dfj = pd.DataFrame(np.random.randn(5, 2), columns=list("AB"))
   json = dfj.to_json()
   json

Orient options
++++++++++++++

There are a number of different options for the format of the resulting JSON
file / string. Consider the following ``DataFrame`` and ``Series``:

.. ipython:: python

  dfjo = pd.DataFrame(
      dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),
      columns=list("ABC"),
      index=list("xyz"),
  )
  dfjo
  sjo = pd.Series(dict(x=15, y=16, z=17), name="D")
  sjo

**Column oriented** (the default for ``DataFrame``) serializes the data as
nested JSON objects with column labels acting as the primary index:

.. ipython:: python

  dfjo.to_json(orient="columns")
  # Not available for Series

**Index oriented** (the default for ``Series``) similar to column oriented
but the index labels are now primary:

.. ipython:: python

  dfjo.to_json(orient="index")
  sjo.to_json(orient="index")

**Record oriented** serializes the data to a JSON array of column -> value records,
index labels are not included. This is useful for passing ``DataFrame`` data to plotting
libraries, for example the JavaScript library ``d3.js``:

.. ipython:: python

  dfjo.to_json(orient="records")
  sjo.to_json(orient="records")

**Value oriented** is a bare-bones option which serializes to nested JSON arrays of
values only, column and index labels are not included:

.. ipython:: python

  dfjo.to_json(orient="values")
  # Not available for Series

**Split oriented** serializes to a JSON object containing separate entries for
values, index and columns. Name is also included for ``Series``:

.. ipython:: python

  dfjo.to_json(orient="split")
  sjo.to_json(orient="split")

**Table oriented** serializes to the JSON `Table Schema`_, allowing for the
preservation of metadata including but not limited to dtypes and index names.

.. note::

  Any orient option that encodes to a JSON object will not preserve the ordering of
  index and column labels during round-trip serialization. If you wish to preserve
  label ordering use the ``split`` option as it uses ordered containers.

Date handling
+++++++++++++

Writing in ISO date format:

.. ipython:: python

   dfd = pd.DataFrame(np.random.randn(5, 2), columns=list("AB"))
   dfd["date"] = pd.Timestamp("20130101")
   dfd = dfd.sort_index(axis=1, ascending=False)
   json = dfd.to_json(date_format="iso")
   json

Writing in ISO date format, with microseconds:

.. ipython:: python

   json = dfd.to_json(date_format="iso", date_unit="us")
   json

Epoch timestamps, in seconds:

.. ipython:: python

   json = dfd.to_json(date_format="epoch", date_unit="s")
   json

Writing to a file, with a date index and a date column:

.. ipython:: python

   dfj2 = dfj.copy()
   dfj2["date"] = pd.Timestamp("20130101")
   dfj2["ints"] = list(range(5))
   dfj2["bools"] = True
   dfj2.index = pd.date_range("20130101", periods=5)
   dfj2.to_json("test.json")

   with open("test.json") as fh:
       print(fh.read())

Fallback behavior
+++++++++++++++++

If the JSON serializer cannot handle the container contents directly it will
fall back in the following manner:

* if the dtype is unsupported (e.g. ``np.complex_``) then the ``default_handler``, if provided, will be called
  for each value, otherwise an exception is raised.

* if an object is unsupported it will attempt the following:


    * check if the object has defined a ``toDict`` method and call it.
      A ``toDict`` method should return a ``dict`` which will then be JSON serialized.

    * invoke the ``default_handler`` if one was provided.

    * convert the object to a ``dict`` by traversing its contents. However this will often fail
      with an ``OverflowError`` or give unexpected results.

In general the best approach for unsupported objects or dtypes is to provide a ``default_handler``.
For example:

.. code-block:: python

  >>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises
  RuntimeError: Unhandled numpy dtype 15

can be dealt with by specifying a simple ``default_handler``:

.. ipython:: python

   pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)

.. _io.json_reader:

Reading JSON
''''''''''''

Reading a JSON string to pandas object can take a number of parameters.
The parser will try to parse a ``DataFrame`` if ``typ`` is not supplied or
is ``None``. To explicitly force ``Series`` parsing, pass ``typ=series``

* ``filepath_or_buffer`` : a **VALID** JSON string or file handle / StringIO. The string could be
  a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host
  is expected. For instance, a local file could be
  file ://localhost/path/to/table.json
* ``typ``    : type of object to recover (series or frame), default 'frame'
* ``orient`` :

  Series :
      * default is ``index``
      * allowed values are {``split``, ``records``, ``index``}

  DataFrame
      * default is ``columns``
      * allowed values are {``split``, ``records``, ``index``, ``columns``, ``values``, ``table``}

  The format of the JSON string

  .. csv-table::
     :widths: 20, 150
     :delim: ;

     ``split``; dict like {index -> [index], columns -> [columns], data -> [values]}
     ``records``; list like [{column -> value}, ... , {column -> value}]
     ``index``; dict like {index -> {column -> value}}
     ``columns``; dict like {column -> {index -> value}}
     ``values``; just the values array
     ``table``; adhering to the JSON `Table Schema`_


* ``dtype`` : if True, infer dtypes, if a dict of column to dtype, then use those, if ``False``, then don't infer dtypes at all, default is True, apply only to the data.
* ``convert_axes`` : boolean, try to convert the axes to the proper dtypes, default is ``True``
* ``convert_dates`` : a list of columns to parse for dates; If ``True``, then try to parse date-like columns, default is ``True``.
* ``keep_default_dates`` : boolean, default ``True``. If parsing dates, then parse the default date-like columns.
* ``numpy`` : direct decoding to NumPy arrays. default is ``False``;
  Supports numeric data only, although labels may be non-numeric. Also note that the JSON ordering **MUST** be the same for each term if ``numpy=True``.
* ``precise_float`` : boolean, default ``False``. Set to enable usage of higher precision (strtod) function when decoding string to double values. Default (``False``) is to use fast but less precise builtin functionality.
* ``date_unit`` : string, the timestamp unit to detect if converting dates. Default
  None. By default the timestamp precision will be detected, if this is not desired
  then pass one of 's', 'ms', 'us' or 'ns' to force timestamp precision to
  seconds, milliseconds, microseconds or nanoseconds respectively.
* ``lines`` : reads file as one json object per line.
* ``encoding`` : The encoding to use to decode py3 bytes.
* ``chunksize`` : when used in combination with ``lines=True``, return a JsonReader which reads in ``chunksize`` lines per iteration.

The parser will raise one of ``ValueError/TypeError/AssertionError`` if the JSON is not parseable.

If a non-default ``orient`` was used when encoding to JSON be sure to pass the same
option here so that decoding produces sensible results, see `Orient Options`_ for an
overview.

Data conversion
+++++++++++++++

The default of ``convert_axes=True``, ``dtype=True``, and ``convert_dates=True``
will try to parse the axes, and all of the data into appropriate types,
including dates. If you need to override specific dtypes, pass a dict to
``dtype``. ``convert_axes`` should only be set to ``False`` if you need to
preserve string-like numbers (e.g. '1', '2') in an axes.

.. note::

  Large integer values may be converted to dates if ``convert_dates=True`` and the data and / or column labels appear 'date-like'. The exact threshold depends on the ``date_unit`` specified. 'date-like' means that the column label meets one of the following criteria:

     * it ends with ``'_at'``
     * it ends with ``'_time'``
     * it begins with ``'timestamp'``
     * it is ``'modified'``
     * it is ``'date'``

.. warning::

   When reading JSON data, automatic coercing into dtypes has some quirks:

     * an index can be reconstructed in a different order from serialization, that is, the returned order is not guaranteed to be the same as before serialization
     * a column that was ``float`` data will be converted to ``integer`` if it can be done safely, e.g. a column of ``1.``
     * bool columns will be converted to ``integer`` on reconstruction

   Thus there are times where you may want to specify specific dtypes via the ``dtype`` keyword argument.

Reading from a JSON string:

.. ipython:: python

   pd.read_json(json)

Reading from a file:

.. ipython:: python

   pd.read_json("test.json")

Don't convert any data (but still convert axes and dates):

.. ipython:: python

   pd.read_json("test.json", dtype=object).dtypes

Specify dtypes for conversion:

.. ipython:: python

   pd.read_json("test.json", dtype={"A": "float32", "bools": "int8"}).dtypes

Preserve string indices:

.. ipython:: python

   si = pd.DataFrame(
       np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]
   )
   si
   si.index
   si.columns
   json = si.to_json()

   sij = pd.read_json(json, convert_axes=False)
   sij
   sij.index
   sij.columns

Dates written in nanoseconds need to be read back in nanoseconds:

.. ipython:: python

   json = dfj2.to_json(date_unit="ns")

   # Try to parse timestamps as milliseconds -> Won't Work
   dfju = pd.read_json(json, date_unit="ms")
   dfju

   # Let pandas detect the correct precision
   dfju = pd.read_json(json)
   dfju

   # Or specify that all timestamps are in nanoseconds
   dfju = pd.read_json(json, date_unit="ns")
   dfju

The Numpy parameter
+++++++++++++++++++

.. note::
  This param has been deprecated as of version 1.0.0 and will raise a ``FutureWarning``.

  This supports numeric data only. Index and columns labels may be non-numeric, e.g. strings, dates etc.

If ``numpy=True`` is passed to ``read_json`` an attempt will be made to sniff
an appropriate dtype during deserialization and to subsequently decode directly
to NumPy arrays, bypassing the need for intermediate Python objects.

This can provide speedups if you are deserialising a large amount of numeric
data:

.. ipython:: python

   randfloats = np.random.uniform(-100, 1000, 10000)
   randfloats.shape = (1000, 10)
   dffloats = pd.DataFrame(randfloats, columns=list("ABCDEFGHIJ"))

   jsonfloats = dffloats.to_json()

.. ipython:: python

   %timeit pd.read_json(jsonfloats)

.. ipython:: python
   :okwarning:

   %timeit pd.read_json(jsonfloats, numpy=True)

The speedup is less noticeable for smaller datasets:

.. ipython:: python

   jsonfloats = dffloats.head(100).to_json()

.. ipython:: python

   %timeit pd.read_json(jsonfloats)

.. ipython:: python
   :okwarning:

   %timeit pd.read_json(jsonfloats, numpy=True)

.. warning::

   Direct NumPy decoding makes a number of assumptions and may fail or produce
   unexpected output if these assumptions are not satisfied:

    - data is numeric.

    - data is uniform. The dtype is sniffed from the first value decoded.
      A ``ValueError`` may be raised, or incorrect output may be produced
      if this condition is not satisfied.

    - labels are ordered. Labels are only read from the first container, it is assumed
      that each subsequent row / column has been encoded in the same order. This should be satisfied if the
      data was encoded using ``to_json`` but may not be the case if the JSON
      is from another source.

.. ipython:: python
   :suppress:

   os.remove("test.json")

.. _io.json_normalize:

Normalization
'''''''''''''

pandas provides a utility function to take a dict or list of dicts and *normalize* this semi-structured data
into a flat table.

.. ipython:: python

   data = [
       {"id": 1, "name": {"first": "Coleen", "last": "Volk"}},
       {"name": {"given": "Mark", "family": "Regner"}},
       {"id": 2, "name": "Faye Raker"},
   ]
   pd.json_normalize(data)

.. ipython:: python

   data = [
       {
           "state": "Florida",
           "shortname": "FL",
           "info": {"governor": "Rick Scott"},
           "county": [
               {"name": "Dade", "population": 12345},
               {"name": "Broward", "population": 40000},
               {"name": "Palm Beach", "population": 60000},
           ],
       },
       {
           "state": "Ohio",
           "shortname": "OH",
           "info": {"governor": "John Kasich"},
           "county": [
               {"name": "Summit", "population": 1234},
               {"name": "Cuyahoga", "population": 1337},
           ],
       },
   ]

   pd.json_normalize(data, "county", ["state", "shortname", ["info", "governor"]])

The max_level parameter provides more control over which level to end normalization.
With max_level=1 the following snippet normalizes until 1st nesting level of the provided dict.

.. ipython:: python

    data = [
        {
            "CreatedBy": {"Name": "User001"},
            "Lookup": {
                "TextField": "Some text",
                "UserField": {"Id": "ID001", "Name": "Name001"},
            },
            "Image": {"a": "b"},
        }
    ]
    pd.json_normalize(data, max_level=1)

.. _io.jsonl:

Line delimited json
'''''''''''''''''''

pandas is able to read and write line-delimited json files that are common in data processing pipelines
using Hadoop or Spark.

For line-delimited json files, pandas can also return an iterator which reads in ``chunksize`` lines at a time. This can be useful for large files or to read from a stream.

.. ipython:: python

  jsonl = """
      {"a": 1, "b": 2}
      {"a": 3, "b": 4}
  """
  df = pd.read_json(jsonl, lines=True)
  df
  df.to_json(orient="records", lines=True)

  # reader is an iterator that returns ``chunksize`` lines each iteration
  with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:
      reader
      for chunk in reader:
          print(chunk)

.. _io.table_schema:

Table schema
''''''''''''

`Table Schema`_ is a spec for describing tabular datasets as a JSON
object. The JSON includes information on the field names, types, and
other attributes. You can use the orient ``table`` to build
a JSON string with two fields, ``schema`` and ``data``.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": [1, 2, 3],
           "B": ["a", "b", "c"],
           "C": pd.date_range("2016-01-01", freq="d", periods=3),
       },
       index=pd.Index(range(3), name="idx"),
   )
   df
   df.to_json(orient="table", date_format="iso")

The ``schema`` field contains the ``fields`` key, which itself contains
a list of column name to type pairs, including the ``Index`` or ``MultiIndex``
(see below for a list of types).
The ``schema`` field also contains a ``primaryKey`` field if the (Multi)index
is unique.

The second field, ``data``, contains the serialized data with the ``records``
orient.
The index is included, and any datetimes are ISO 8601 formatted, as required
by the Table Schema spec.

The full list of types supported are described in the Table Schema
spec. This table shows the mapping from pandas types:

=============== =================
pandas type     Table Schema type
=============== =================
int64           integer
float64         number
bool            boolean
datetime64[ns]  datetime
timedelta64[ns] duration
categorical     any
object          str
=============== =================

A few notes on the generated table schema:

* The ``schema`` object contains a ``pandas_version`` field. This contains
  the version of pandas' dialect of the schema, and will be incremented
  with each revision.
* All dates are converted to UTC when serializing. Even timezone naive values,
  which are treated as UTC with an offset of 0.

  .. ipython:: python

     from pandas.io.json import build_table_schema

     s = pd.Series(pd.date_range("2016", periods=4))
     build_table_schema(s)

* datetimes with a timezone (before serializing), include an additional field
  ``tz`` with the time zone name (e.g. ``'US/Central'``).

  .. ipython:: python

     s_tz = pd.Series(pd.date_range("2016", periods=12, tz="US/Central"))
     build_table_schema(s_tz)

* Periods are converted to timestamps before serialization, and so have the
  same behavior of being converted to UTC. In addition, periods will contain
  and additional field ``freq`` with the period's frequency, e.g. ``'A-DEC'``.

  .. ipython:: python

     s_per = pd.Series(1, index=pd.period_range("2016", freq="A-DEC", periods=4))
     build_table_schema(s_per)

* Categoricals use the ``any`` type and an ``enum`` constraint listing
  the set of possible values. Additionally, an ``ordered`` field is included:

  .. ipython:: python

     s_cat = pd.Series(pd.Categorical(["a", "b", "a"]))
     build_table_schema(s_cat)

* A ``primaryKey`` field, containing an array of labels, is included
  *if the index is unique*:

  .. ipython:: python

     s_dupe = pd.Series([1, 2], index=[1, 1])
     build_table_schema(s_dupe)

* The ``primaryKey`` behavior is the same with MultiIndexes, but in this
  case the ``primaryKey`` is an array:

  .. ipython:: python

     s_multi = pd.Series(1, index=pd.MultiIndex.from_product([("a", "b"), (0, 1)]))
     build_table_schema(s_multi)

* The default naming roughly follows these rules:

    * For series, the ``object.name`` is used. If that's none, then the
      name is ``values``
    * For ``DataFrames``, the stringified version of the column name is used
    * For ``Index`` (not ``MultiIndex``), ``index.name`` is used, with a
      fallback to ``index`` if that is None.
    * For ``MultiIndex``, ``mi.names`` is used. If any level has no name,
      then ``level_<i>`` is used.

``read_json`` also accepts ``orient='table'`` as an argument. This allows for
the preservation of metadata such as dtypes and index names in a
round-trippable manner.

  .. ipython:: python

   df = pd.DataFrame(
       {
           "foo": [1, 2, 3, 4],
           "bar": ["a", "b", "c", "d"],
           "baz": pd.date_range("2018-01-01", freq="d", periods=4),
           "qux": pd.Categorical(["a", "b", "c", "c"]),
       },
       index=pd.Index(range(4), name="idx"),
   )
   df
   df.dtypes

   df.to_json("test.json", orient="table")
   new_df = pd.read_json("test.json", orient="table")
   new_df
   new_df.dtypes

Please note that the literal string 'index' as the name of an :class:`Index`
is not round-trippable, nor are any names beginning with ``'level_'`` within a
:class:`MultiIndex`. These are used by default in :func:`DataFrame.to_json` to
indicate missing values and the subsequent read cannot distinguish the intent.

.. ipython:: python
   :okwarning:

   df.index.name = "index"
   df.to_json("test.json", orient="table")
   new_df = pd.read_json("test.json", orient="table")
   print(new_df.index.name)

.. ipython:: python
   :suppress:

   os.remove("test.json")

When using ``orient='table'`` along with user-defined ``ExtensionArray``,
the generated schema will contain an additional ``extDtype`` key in the respective
``fields`` element. This extra key is not standard but does enable JSON roundtrips
for extension types (e.g. ``read_json(df.to_json(orient="table"), orient="table")``).

The ``extDtype`` key carries the name of the extension, if you have properly registered
the ``ExtensionDtype``, pandas will use said name to perform a lookup into the registry
and re-convert the serialized data into your custom dtype.

.. _Table Schema: https://specs.frictionlessdata.io/table-schema/


HTML
----

.. _io.read_html:

Reading HTML content
''''''''''''''''''''''

.. warning::

   We **highly encourage** you to read the :ref:`HTML Table Parsing gotchas <io.html.gotchas>`
   below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.

The top-level :func:`~pandas.io.html.read_html` function can accept an HTML
string/file/URL and will parse HTML tables into list of pandas ``DataFrames``.
Let's look at a few examples.

.. note::

   ``read_html`` returns a ``list`` of ``DataFrame`` objects, even if there is
   only a single table contained in the HTML content.

Read a URL with no options:

.. ipython:: python

   url = "https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list"
   dfs = pd.read_html(url)
   dfs

.. note::

   The data from the above URL changes every Monday so the resulting data above
   and the data below may be slightly different.

Read in the content of the file from the above URL and pass it to ``read_html``
as a string:

.. ipython:: python
   :suppress:

   rel_path = os.path.join("..", "pandas", "tests", "io", "data", "html",
                           "banklist.html")
   file_path = os.path.abspath(rel_path)

.. ipython:: python

   with open(file_path, "r") as f:
       dfs = pd.read_html(f.read())
   dfs

You can even pass in an instance of ``StringIO`` if you so desire:

.. ipython:: python

   with open(file_path, "r") as f:
       sio = StringIO(f.read())

   dfs = pd.read_html(sio)
   dfs

.. note::

   The following examples are not run by the IPython evaluator due to the fact
   that having so many network-accessing functions slows down the documentation
   build. If you spot an error or an example that doesn't run, please do not
   hesitate to report it over on `pandas GitHub issues page
   <https://github.com/pandas-dev/pandas/issues>`__.


Read a URL and match a table that contains specific text:

.. code-block:: python

   match = "Metcalf Bank"
   df_list = pd.read_html(url, match=match)

Specify a header row (by default ``<th>`` or ``<td>`` elements located within a
``<thead>`` are used to form the column index, if multiple rows are contained within
``<thead>`` then a MultiIndex is created); if specified, the header row is taken
from the data minus the parsed header elements (``<th>`` elements).

.. code-block:: python

   dfs = pd.read_html(url, header=0)

Specify an index column:

.. code-block:: python

   dfs = pd.read_html(url, index_col=0)

Specify a number of rows to skip:

.. code-block:: python

   dfs = pd.read_html(url, skiprows=0)

Specify a number of rows to skip using a list (``range`` works
as well):

.. code-block:: python

   dfs = pd.read_html(url, skiprows=range(2))

Specify an HTML attribute:

.. code-block:: python

   dfs1 = pd.read_html(url, attrs={"id": "table"})
   dfs2 = pd.read_html(url, attrs={"class": "sortable"})
   print(np.array_equal(dfs1[0], dfs2[0]))  # Should be True

Specify values that should be converted to NaN:

.. code-block:: python

   dfs = pd.read_html(url, na_values=["No Acquirer"])

Specify whether to keep the default set of NaN values:

.. code-block:: python

   dfs = pd.read_html(url, keep_default_na=False)

Specify converters for columns. This is useful for numerical text data that has
leading zeros.  By default columns that are numerical are cast to numeric
types and the leading zeros are lost. To avoid this, we can convert these
columns to strings.

.. code-block:: python

   url_mcc = "https://en.wikipedia.org/wiki/Mobile_country_code"
   dfs = pd.read_html(
       url_mcc,
       match="Telekom Albania",
       header=0,
       converters={"MNC": str},
   )

Use some combination of the above:

.. code-block:: python

   dfs = pd.read_html(url, match="Metcalf Bank", index_col=0)

Read in pandas ``to_html`` output (with some loss of floating point precision):

.. code-block:: python

   df = pd.DataFrame(np.random.randn(2, 2))
   s = df.to_html(float_format="{0:.40g}".format)
   dfin = pd.read_html(s, index_col=0)

The ``lxml`` backend will raise an error on a failed parse if that is the only
parser you provide. If you only have a single parser you can provide just a
string, but it is considered good practice to pass a list with one string if,
for example, the function expects a sequence of strings. You may use:

.. code-block:: python

   dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor=["lxml"])

Or you could pass ``flavor='lxml'`` without a list:

.. code-block:: python

   dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor="lxml")

However, if you have bs4 and html5lib installed and pass ``None`` or ``['lxml',
'bs4']`` then the parse will most likely succeed. Note that *as soon as a parse
succeeds, the function will return*.

.. code-block:: python

   dfs = pd.read_html(url, "Metcalf Bank", index_col=0, flavor=["lxml", "bs4"])


.. _io.html:

Writing to HTML files
''''''''''''''''''''''

``DataFrame`` objects have an instance method ``to_html`` which renders the
contents of the ``DataFrame`` as an HTML table. The function arguments are as
in the method ``to_string`` described above.

.. note::

   Not all of the possible options for ``DataFrame.to_html`` are shown here for
   brevity's sake. See :func:`~pandas.core.frame.DataFrame.to_html` for the
   full set of options.

.. ipython:: python
   :suppress:

   def write_html(df, filename, *args, **kwargs):
       static = os.path.abspath(os.path.join("source", "_static"))
       with open(os.path.join(static, filename + ".html"), "w") as f:
           df.to_html(f, *args, **kwargs)

.. ipython:: python

   df = pd.DataFrame(np.random.randn(2, 2))
   df
   print(df.to_html())  # raw html

.. ipython:: python
   :suppress:

   write_html(df, "basic")

HTML:

.. raw:: html
   :file: ../_static/basic.html

The ``columns`` argument will limit the columns shown:

.. ipython:: python

   print(df.to_html(columns=[0]))

.. ipython:: python
   :suppress:

   write_html(df, "columns", columns=[0])

HTML:

.. raw:: html
   :file: ../_static/columns.html

``float_format`` takes a Python callable to control the precision of floating
point values:

.. ipython:: python

   print(df.to_html(float_format="{0:.10f}".format))

.. ipython:: python
   :suppress:

   write_html(df, "float_format", float_format="{0:.10f}".format)

HTML:

.. raw:: html
   :file: ../_static/float_format.html

``bold_rows`` will make the row labels bold by default, but you can turn that
off:

.. ipython:: python

   print(df.to_html(bold_rows=False))

.. ipython:: python
   :suppress:

   write_html(df, "nobold", bold_rows=False)

.. raw:: html
   :file: ../_static/nobold.html

The ``classes`` argument provides the ability to give the resulting HTML
table CSS classes. Note that these classes are *appended* to the existing
``'dataframe'`` class.

.. ipython:: python

   print(df.to_html(classes=["awesome_table_class", "even_more_awesome_class"]))

The ``render_links`` argument provides the ability to add hyperlinks to cells
that contain URLs.

.. ipython:: python

   url_df = pd.DataFrame(
       {
           "name": ["Python", "pandas"],
           "url": ["https://www.python.org/", "https://pandas.pydata.org"],
       }
   )
   print(url_df.to_html(render_links=True))

.. ipython:: python
   :suppress:

   write_html(url_df, "render_links", render_links=True)

HTML:

.. raw:: html
   :file: ../_static/render_links.html

Finally, the ``escape`` argument allows you to control whether the
"<", ">" and "&" characters escaped in the resulting HTML (by default it is
``True``). So to get the HTML without escaped characters pass ``escape=False``

.. ipython:: python

   df = pd.DataFrame({"a": list("&<>"), "b": np.random.randn(3)})


.. ipython:: python
   :suppress:

   write_html(df, "escape")
   write_html(df, "noescape", escape=False)

Escaped:

.. ipython:: python

   print(df.to_html())

.. raw:: html
   :file: ../_static/escape.html

Not escaped:

.. ipython:: python

   print(df.to_html(escape=False))

.. raw:: html
   :file: ../_static/noescape.html

.. note::

   Some browsers may not show a difference in the rendering of the previous two
   HTML tables.


.. _io.html.gotchas:

HTML Table Parsing Gotchas
''''''''''''''''''''''''''

There are some versioning issues surrounding the libraries that are used to
parse HTML tables in the top-level pandas io function ``read_html``.

**Issues with** |lxml|_

* Benefits

    * |lxml|_ is very fast.

    * |lxml|_ requires Cython to install correctly.

* Drawbacks

    * |lxml|_ does *not* make any guarantees about the results of its parse
      *unless* it is given |svm|_.

    * In light of the above, we have chosen to allow you, the user, to use the
      |lxml|_ backend, but **this backend will use** |html5lib|_ if |lxml|_
      fails to parse

    * It is therefore *highly recommended* that you install both
      |BeautifulSoup4|_ and |html5lib|_, so that you will still get a valid
      result (provided everything else is valid) even if |lxml|_ fails.

**Issues with** |BeautifulSoup4|_ **using** |lxml|_ **as a backend**

* The above issues hold here as well since |BeautifulSoup4|_ is essentially
  just a wrapper around a parser backend.

**Issues with** |BeautifulSoup4|_ **using** |html5lib|_ **as a backend**

* Benefits

    * |html5lib|_ is far more lenient than |lxml|_ and consequently deals
      with *real-life markup* in a much saner way rather than just, e.g.,
      dropping an element without notifying you.

    * |html5lib|_ *generates valid HTML5 markup from invalid markup
      automatically*. This is extremely important for parsing HTML tables,
      since it guarantees a valid document. However, that does NOT mean that
      it is "correct", since the process of fixing markup does not have a
      single definition.

    * |html5lib|_ is pure Python and requires no additional build steps beyond
      its own installation.

* Drawbacks

    * The biggest drawback to using |html5lib|_ is that it is slow as
      molasses.  However consider the fact that many tables on the web are not
      big enough for the parsing algorithm runtime to matter. It is more
      likely that the bottleneck will be in the process of reading the raw
      text from the URL over the web, i.e., IO (input-output). For very large
      tables, this might not be true.


.. |svm| replace:: **strictly valid markup**
.. _svm: https://validator.w3.org/docs/help.html#validation_basics

.. |html5lib| replace:: **html5lib**
.. _html5lib: https://github.com/html5lib/html5lib-python

.. |BeautifulSoup4| replace:: **BeautifulSoup4**
.. _BeautifulSoup4: https://www.crummy.com/software/BeautifulSoup

.. |lxml| replace:: **lxml**
.. _lxml: https://lxml.de

.. _io.latex:

LaTeX
-----

.. versionadded:: 1.3.0

Currently there are no methods to read from LaTeX, only output methods.

Writing to LaTeX files
''''''''''''''''''''''

.. note::

   DataFrame *and* Styler objects currently have a ``to_latex`` method. We recommend
   using the `Styler.to_latex() <../reference/api/pandas.io.formats.style.Styler.to_latex.rst>`__ method
   over `DataFrame.to_latex() <../reference/api/pandas.DataFrame.to_latex.rst>`__ due to the former's greater flexibility with
   conditional styling, and the latter's possible future deprecation.

Review the documentation for `Styler.to_latex <../reference/api/pandas.io.formats.style.Styler.to_latex.rst>`__,
which gives examples of conditional styling and explains the operation of its keyword
arguments.

For simple application the following pattern is sufficient.

.. ipython:: python

   df = pd.DataFrame([[1, 2], [3, 4]], index=["a", "b"], columns=["c", "d"])
   print(df.style.to_latex())

To format values before output, chain the `Styler.format <../reference/api/pandas.io.formats.style.Styler.format.rst>`__
method.

.. ipython:: python

   print(df.style.format("€ {}").to_latex())

XML
---

.. _io.read_xml:

Reading XML
'''''''''''

.. versionadded:: 1.3.0

The top-level :func:`~pandas.io.xml.read_xml` function can accept an XML
string/file/URL and will parse nodes and attributes into a pandas ``DataFrame``.

.. note::

   Since there is no standard XML structure where design types can vary in
   many ways, ``read_xml`` works best with flatter, shallow versions. If
   an XML document is deeply nested, use the ``stylesheet`` feature to
   transform XML into a flatter version.

Let's look at a few examples.

Read an XML string:

.. ipython:: python

   xml = """<?xml version="1.0" encoding="UTF-8"?>
   <bookstore>
     <book category="cooking">
       <title lang="en">Everyday Italian</title>
       <author>Giada De Laurentiis</author>
       <year>2005</year>
       <price>30.00</price>
     </book>
     <book category="children">
       <title lang="en">Harry Potter</title>
       <author>J K. Rowling</author>
       <year>2005</year>
       <price>29.99</price>
     </book>
     <book category="web">
       <title lang="en">Learning XML</title>
       <author>Erik T. Ray</author>
       <year>2003</year>
       <price>39.95</price>
     </book>
   </bookstore>"""

   df = pd.read_xml(xml)
   df

Read a URL with no options:

.. ipython:: python

   df = pd.read_xml("https://www.w3schools.com/xml/books.xml")
   df

Read in the content of the "books.xml" file and pass it to ``read_xml``
as a string:

.. ipython:: python
   :suppress:

   rel_path = os.path.join("..", "pandas", "tests", "io", "data", "xml",
                           "books.xml")
   file_path = os.path.abspath(rel_path)

.. ipython:: python

   with open(file_path, "r") as f:
       df = pd.read_xml(f.read())
   df

Read in the content of the "books.xml" as instance of ``StringIO`` or
``BytesIO`` and pass it to ``read_xml``:

.. ipython:: python

   with open(file_path, "r") as f:
       sio = StringIO(f.read())

   df = pd.read_xml(sio)
   df

.. ipython:: python

   with open(file_path, "rb") as f:
       bio = BytesIO(f.read())

   df = pd.read_xml(bio)
   df

Even read XML from AWS S3 buckets such as Python Software Foundation's IRS 990 Form:

.. ipython:: python
   :okwarning:

   df = pd.read_xml(
       "s3://irs-form-990/201923199349319487_public.xml",
       xpath=".//irs:Form990PartVIISectionAGrp",
       namespaces={"irs": "http://www.irs.gov/efile"}
   )
   df

With `lxml`_ as default ``parser``, you access the full-featured XML library
that extends Python's ElementTree API. One powerful tool is ability to query
nodes selectively or conditionally with more expressive XPath:

.. _lxml: https://lxml.de

.. ipython:: python

   df = pd.read_xml(file_path, xpath="//book[year=2005]")
   df

Specify only elements or only attributes to parse:

.. ipython:: python

   df = pd.read_xml(file_path, elems_only=True)
   df

.. ipython:: python

   df = pd.read_xml(file_path, attrs_only=True)
   df

XML documents can have namespaces with prefixes and default namespaces without
prefixes both of which are denoted with a special attribute ``xmlns``. In order
to parse by node under a namespace context, ``xpath`` must reference a prefix.

For example, below XML contains a namespace with prefix, ``doc``, and URI at
``https://example.com``. In order to parse ``doc:row`` nodes,
``namespaces`` must be used.

.. ipython:: python

   xml = """<?xml version='1.0' encoding='utf-8'?>
   <doc:data xmlns:doc="https://example.com">
     <doc:row>
       <doc:shape>square</doc:shape>
       <doc:degrees>360</doc:degrees>
       <doc:sides>4.0</doc:sides>
     </doc:row>
     <doc:row>
       <doc:shape>circle</doc:shape>
       <doc:degrees>360</doc:degrees>
       <doc:sides/>
     </doc:row>
     <doc:row>
       <doc:shape>triangle</doc:shape>
       <doc:degrees>180</doc:degrees>
       <doc:sides>3.0</doc:sides>
     </doc:row>
   </doc:data>"""

   df = pd.read_xml(xml,
                    xpath="//doc:row",
                    namespaces={"doc": "https://example.com"})
   df

Similarly, an XML document can have a default namespace without prefix. Failing
to assign a temporary prefix will return no nodes and raise a ``ValueError``.
But assigning *any* temporary name to correct URI allows parsing by nodes.

.. ipython:: python

   xml = """<?xml version='1.0' encoding='utf-8'?>
   <data xmlns="https://example.com">
    <row>
      <shape>square</shape>
      <degrees>360</degrees>
      <sides>4.0</sides>
    </row>
    <row>
      <shape>circle</shape>
      <degrees>360</degrees>
      <sides/>
    </row>
    <row>
      <shape>triangle</shape>
      <degrees>180</degrees>
      <sides>3.0</sides>
    </row>
   </data>"""

   df = pd.read_xml(xml,
                    xpath="//pandas:row",
                    namespaces={"pandas": "https://example.com"})
   df

However, if XPath does not reference node names such as default, ``/*``, then
``namespaces`` is not required.

With `lxml`_ as parser, you can flatten nested XML documents with an XSLT
script which also can be string/file/URL types. As background, `XSLT`_ is
a special-purpose language written in a special XML file that can transform
original XML documents into other XML, HTML, even text (CSV, JSON, etc.)
using an XSLT processor.

.. _lxml: https://lxml.de
.. _XSLT: https://www.w3.org/TR/xslt/

For example, consider this somewhat nested structure of Chicago "L" Rides
where station and rides elements encapsulate data in their own sections.
With below XSLT, ``lxml`` can transform original nested document into a flatter
output (as shown below for demonstration) for easier parse into ``DataFrame``:

.. ipython:: python

   xml = """<?xml version='1.0' encoding='utf-8'?>
    <response>
     <row>
       <station id="40850" name="Library"/>
       <month>2020-09-01T00:00:00</month>
       <rides>
         <avg_weekday_rides>864.2</avg_weekday_rides>
         <avg_saturday_rides>534</avg_saturday_rides>
         <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>
       </rides>
     </row>
     <row>
       <station id="41700" name="Washington/Wabash"/>
       <month>2020-09-01T00:00:00</month>
       <rides>
         <avg_weekday_rides>2707.4</avg_weekday_rides>
         <avg_saturday_rides>1909.8</avg_saturday_rides>
         <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>
       </rides>
     </row>
     <row>
       <station id="40380" name="Clark/Lake"/>
       <month>2020-09-01T00:00:00</month>
       <rides>
         <avg_weekday_rides>2949.6</avg_weekday_rides>
         <avg_saturday_rides>1657</avg_saturday_rides>
         <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>
       </rides>
     </row>
    </response>"""

   xsl = """<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
      <xsl:output method="xml" omit-xml-declaration="no" indent="yes"/>
      <xsl:strip-space elements="*"/>
      <xsl:template match="/response">
         <xsl:copy>
           <xsl:apply-templates select="row"/>
         </xsl:copy>
      </xsl:template>
      <xsl:template match="row">
         <xsl:copy>
           <station_id><xsl:value-of select="station/@id"/></station_id>
           <station_name><xsl:value-of select="station/@name"/></station_name>
           <xsl:copy-of select="month|rides/*"/>
         </xsl:copy>
      </xsl:template>
    </xsl:stylesheet>"""

   output = """<?xml version='1.0' encoding='utf-8'?>
    <response>
      <row>
         <station_id>40850</station_id>
         <station_name>Library</station_name>
         <month>2020-09-01T00:00:00</month>
         <avg_weekday_rides>864.2</avg_weekday_rides>
         <avg_saturday_rides>534</avg_saturday_rides>
         <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>
      </row>
      <row>
         <station_id>41700</station_id>
         <station_name>Washington/Wabash</station_name>
         <month>2020-09-01T00:00:00</month>
         <avg_weekday_rides>2707.4</avg_weekday_rides>
         <avg_saturday_rides>1909.8</avg_saturday_rides>
         <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>
      </row>
      <row>
         <station_id>40380</station_id>
         <station_name>Clark/Lake</station_name>
         <month>2020-09-01T00:00:00</month>
         <avg_weekday_rides>2949.6</avg_weekday_rides>
         <avg_saturday_rides>1657</avg_saturday_rides>
         <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>
      </row>
    </response>"""

   df = pd.read_xml(xml, stylesheet=xsl)
   df


.. _io.xml:

Writing XML
'''''''''''

.. versionadded:: 1.3.0

``DataFrame`` objects have an instance method ``to_xml`` which renders the
contents of the ``DataFrame`` as an XML document.

.. note::

   This method does not support special properties of XML including DTD,
   CData, XSD schemas, processing instructions, comments, and others.
   Only namespaces at the root level is supported. However, ``stylesheet``
   allows design changes after initial output.

Let's look at a few examples.

Write an XML without options:

.. ipython:: python

   geom_df = pd.DataFrame(
       {
           "shape": ["square", "circle", "triangle"],
           "degrees": [360, 360, 180],
           "sides": [4, np.nan, 3],
       }
   )

   print(geom_df.to_xml())


Write an XML with new root and row name:

.. ipython:: python

   print(geom_df.to_xml(root_name="geometry", row_name="objects"))

Write an attribute-centric XML:

.. ipython:: python

   print(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))

Write a mix of elements and attributes:

.. ipython:: python

   print(
       geom_df.to_xml(
           index=False,
           attr_cols=['shape'],
           elem_cols=['degrees', 'sides'])
   )

Any ``DataFrames`` with hierarchical columns will be flattened for XML element names
with levels delimited by underscores:

.. ipython:: python

   ext_geom_df = pd.DataFrame(
       {
           "type": ["polygon", "other", "polygon"],
           "shape": ["square", "circle", "triangle"],
           "degrees": [360, 360, 180],
           "sides": [4, np.nan, 3],
       }
   )

   pvt_df = ext_geom_df.pivot_table(index='shape',
                                    columns='type',
                                    values=['degrees', 'sides'],
                                    aggfunc='sum')
   pvt_df

   print(pvt_df.to_xml())

Write an XML with default namespace:

.. ipython:: python

   print(geom_df.to_xml(namespaces={"": "https://example.com"}))

Write an XML with namespace prefix:

.. ipython:: python

   print(
       geom_df.to_xml(namespaces={"doc": "https://example.com"},
                      prefix="doc")
   )

Write an XML without declaration or pretty print:

.. ipython:: python

   print(
       geom_df.to_xml(xml_declaration=False,
                      pretty_print=False)
   )

Write an XML and transform with stylesheet:

.. ipython:: python

   xsl = """<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
      <xsl:output method="xml" omit-xml-declaration="no" indent="yes"/>
      <xsl:strip-space elements="*"/>
      <xsl:template match="/data">
        <geometry>
          <xsl:apply-templates select="row"/>
        </geometry>
      </xsl:template>
      <xsl:template match="row">
        <object index="{index}">
          <xsl:if test="shape!='circle'">
              <xsl:attribute name="type">polygon</xsl:attribute>
          </xsl:if>
          <xsl:copy-of select="shape"/>
          <property>
              <xsl:copy-of select="degrees|sides"/>
          </property>
        </object>
      </xsl:template>
    </xsl:stylesheet>"""

   print(geom_df.to_xml(stylesheet=xsl))


XML Final Notes
'''''''''''''''

* All XML documents adhere to `W3C specifications`_. Both ``etree`` and ``lxml``
  parsers will fail to parse any markup document that is not well-formed or
  follows XML syntax rules. Do be aware HTML is not an XML document unless it
  follows XHTML specs. However, other popular markup types including KML, XAML,
  RSS, MusicML, MathML are compliant `XML schemas`_.

* For above reason, if your application builds XML prior to pandas operations,
  use appropriate DOM libraries like ``etree`` and ``lxml`` to build the necessary
  document and not by string concatenation or regex adjustments. Always remember
  XML is a *special* text file with markup rules.

* With very large XML files (several hundred MBs to GBs), XPath and XSLT
  can become memory-intensive operations. Be sure to have enough available
  RAM for reading and writing to large XML files (roughly about 5 times the
  size of text).

* Because XSLT is a programming language, use it with caution since such scripts
  can pose a security risk in your environment and can run large or infinite
  recursive operations. Always test scripts on small fragments before full run.

* The `etree`_ parser supports all functionality of both ``read_xml`` and
  ``to_xml`` except for complex XPath and any XSLT. Though limited in features,
  ``etree`` is still a reliable and capable parser and tree builder. Its
  performance may trail ``lxml`` to a certain degree for larger files but
  relatively unnoticeable on small to medium size files.

.. _`W3C specifications`: https://www.w3.org/TR/xml/
.. _`XML schemas`: https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas
.. _`etree`: https://docs.python.org/3/library/xml.etree.elementtree.html



.. _io.excel:

Excel files
-----------

The :func:`~pandas.read_excel` method can read Excel 2007+ (``.xlsx``) files
using the ``openpyxl`` Python module. Excel 2003 (``.xls``) files
can be read using ``xlrd``. Binary Excel (``.xlsb``)
files can be read using ``pyxlsb``.
The :meth:`~DataFrame.to_excel` instance method is used for
saving a ``DataFrame`` to Excel.  Generally the semantics are
similar to working with :ref:`csv<io.read_csv_table>` data.
See the :ref:`cookbook<cookbook.excel>` for some advanced strategies.

.. warning::

   The `xlwt <https://xlwt.readthedocs.io/en/latest/>`__ package for writing old-style ``.xls``
   excel files is no longer maintained.
   The `xlrd <https://xlrd.readthedocs.io/en/latest/>`__ package is now only for reading
   old-style ``.xls`` files.

   Before pandas 1.3.0, the default argument ``engine=None`` to :func:`~pandas.read_excel`
   would result in using the ``xlrd`` engine in many cases, including new
   Excel 2007+ (``.xlsx``) files. pandas will now default to using the
   `openpyxl <https://openpyxl.readthedocs.io/en/stable/>`__ engine.

   It is strongly encouraged to install ``openpyxl`` to read Excel 2007+
   (``.xlsx``) files.
   **Please do not report issues when using ``xlrd`` to read ``.xlsx`` files.**
   This is no longer supported, switch to using ``openpyxl`` instead.

   Attempting to use the the ``xlwt`` engine will raise a ``FutureWarning``
   unless the option :attr:`io.excel.xls.writer` is set to ``"xlwt"``.
   While this option is now deprecated and will also raise a ``FutureWarning``,
   it can be globally set and the warning suppressed. Users are recommended to
   write ``.xlsx`` files using the ``openpyxl`` engine instead.

.. _io.excel_reader:

Reading Excel files
'''''''''''''''''''

In the most basic use-case, ``read_excel`` takes a path to an Excel
file, and the ``sheet_name`` indicating which sheet to parse.

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.xls", sheet_name="Sheet1")


.. _io.excel.excelfile_class:

``ExcelFile`` class
+++++++++++++++++++

To facilitate working with multiple sheets from the same file, the ``ExcelFile``
class can be used to wrap the file and can be passed into ``read_excel``
There will be a performance benefit for reading multiple sheets as the file is
read into memory only once.

.. code-block:: python

   xlsx = pd.ExcelFile("path_to_file.xls")
   df = pd.read_excel(xlsx, "Sheet1")

The ``ExcelFile`` class can also be used as a context manager.

.. code-block:: python

   with pd.ExcelFile("path_to_file.xls") as xls:
       df1 = pd.read_excel(xls, "Sheet1")
       df2 = pd.read_excel(xls, "Sheet2")

The ``sheet_names`` property will generate
a list of the sheet names in the file.

The primary use-case for an ``ExcelFile`` is parsing multiple sheets with
different parameters:

.. code-block:: python

    data = {}
    # For when Sheet1's format differs from Sheet2
    with pd.ExcelFile("path_to_file.xls") as xls:
        data["Sheet1"] = pd.read_excel(xls, "Sheet1", index_col=None, na_values=["NA"])
        data["Sheet2"] = pd.read_excel(xls, "Sheet2", index_col=1)

Note that if the same parsing parameters are used for all sheets, a list
of sheet names can simply be passed to ``read_excel`` with no loss in performance.

.. code-block:: python

    # using the ExcelFile class
    data = {}
    with pd.ExcelFile("path_to_file.xls") as xls:
        data["Sheet1"] = pd.read_excel(xls, "Sheet1", index_col=None, na_values=["NA"])
        data["Sheet2"] = pd.read_excel(xls, "Sheet2", index_col=None, na_values=["NA"])

    # equivalent using the read_excel function
    data = pd.read_excel(
        "path_to_file.xls", ["Sheet1", "Sheet2"], index_col=None, na_values=["NA"]
    )

``ExcelFile`` can also be called with a ``xlrd.book.Book`` object
as a parameter. This allows the user to control how the excel file is read.
For example, sheets can be loaded on demand by calling ``xlrd.open_workbook()``
with ``on_demand=True``.

.. code-block:: python

    import xlrd

    xlrd_book = xlrd.open_workbook("path_to_file.xls", on_demand=True)
    with pd.ExcelFile(xlrd_book) as xls:
        df1 = pd.read_excel(xls, "Sheet1")
        df2 = pd.read_excel(xls, "Sheet2")

.. _io.excel.specifying_sheets:

Specifying sheets
+++++++++++++++++

.. note:: The second argument is ``sheet_name``, not to be confused with ``ExcelFile.sheet_names``.

.. note:: An ExcelFile's attribute ``sheet_names`` provides access to a list of sheets.

* The arguments ``sheet_name`` allows specifying the sheet or sheets to read.
* The default value for ``sheet_name`` is 0, indicating to read the first sheet
* Pass a string to refer to the name of a particular sheet in the workbook.
* Pass an integer to refer to the index of a sheet. Indices follow Python
  convention, beginning at 0.
* Pass a list of either strings or integers, to return a dictionary of specified sheets.
* Pass a ``None`` to return a dictionary of all available sheets.

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.xls", "Sheet1", index_col=None, na_values=["NA"])

Using the sheet index:

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.xls", 0, index_col=None, na_values=["NA"])

Using all default values:

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.xls")

Using None to get all sheets:

.. code-block:: python

   # Returns a dictionary of DataFrames
   pd.read_excel("path_to_file.xls", sheet_name=None)

Using a list to get multiple sheets:

.. code-block:: python

   # Returns the 1st and 4th sheet, as a dictionary of DataFrames.
   pd.read_excel("path_to_file.xls", sheet_name=["Sheet1", 3])

``read_excel`` can read more than one sheet, by setting ``sheet_name`` to either
a list of sheet names, a list of sheet positions, or ``None`` to read all sheets.
Sheets can be specified by sheet index or sheet name, using an integer or string,
respectively.

.. _io.excel.reading_multiindex:

Reading a ``MultiIndex``
++++++++++++++++++++++++

``read_excel`` can read a ``MultiIndex`` index, by passing a list of columns to ``index_col``
and a ``MultiIndex`` column by passing a list of rows to ``header``.  If either the ``index``
or ``columns`` have serialized level names those will be read in as well by specifying
the rows/columns that make up the levels.

For example, to read in a ``MultiIndex`` index without names:

.. ipython:: python

   df = pd.DataFrame(
       {"a": [1, 2, 3, 4], "b": [5, 6, 7, 8]},
       index=pd.MultiIndex.from_product([["a", "b"], ["c", "d"]]),
   )
   df.to_excel("path_to_file.xlsx")
   df = pd.read_excel("path_to_file.xlsx", index_col=[0, 1])
   df

If the index has level names, they will parsed as well, using the same
parameters.

.. ipython:: python

   df.index = df.index.set_names(["lvl1", "lvl2"])
   df.to_excel("path_to_file.xlsx")
   df = pd.read_excel("path_to_file.xlsx", index_col=[0, 1])
   df


If the source file has both ``MultiIndex`` index and columns, lists specifying each
should be passed to ``index_col`` and ``header``:

.. ipython:: python

   df.columns = pd.MultiIndex.from_product([["a"], ["b", "d"]], names=["c1", "c2"])
   df.to_excel("path_to_file.xlsx")
   df = pd.read_excel("path_to_file.xlsx", index_col=[0, 1], header=[0, 1])
   df

.. ipython:: python
   :suppress:

   os.remove("path_to_file.xlsx")


Parsing specific columns
++++++++++++++++++++++++

It is often the case that users will insert columns to do temporary computations
in Excel and you may not want to read in those columns. ``read_excel`` takes
a ``usecols`` keyword to allow you to specify a subset of columns to parse.

.. versionchanged:: 1.0.0

Passing in an integer for ``usecols`` will no longer work. Please pass in a list
of ints from 0 to ``usecols`` inclusive instead.

You can specify a comma-delimited set of Excel columns and ranges as a string:

.. code-block:: python

   pd.read_excel("path_to_file.xls", "Sheet1", usecols="A,C:E")

If ``usecols`` is a list of integers, then it is assumed to be the file column
indices to be parsed.

.. code-block:: python

   pd.read_excel("path_to_file.xls", "Sheet1", usecols=[0, 2, 3])

Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.

If ``usecols`` is a list of strings, it is assumed that each string corresponds
to a column name provided either by the user in ``names`` or inferred from the
document header row(s). Those strings define which columns will be parsed:

.. code-block:: python

    pd.read_excel("path_to_file.xls", "Sheet1", usecols=["foo", "bar"])

Element order is ignored, so ``usecols=['baz', 'joe']`` is the same as ``['joe', 'baz']``.

If ``usecols`` is callable, the callable function will be evaluated against
the column names, returning names where the callable function evaluates to ``True``.

.. code-block:: python

    pd.read_excel("path_to_file.xls", "Sheet1", usecols=lambda x: x.isalpha())

Parsing dates
+++++++++++++

Datetime-like values are normally automatically converted to the appropriate
dtype when reading the excel file. But if you have a column of strings that
*look* like dates (but are not actually formatted as dates in excel), you can
use the ``parse_dates`` keyword to parse those strings to datetimes:

.. code-block:: python

   pd.read_excel("path_to_file.xls", "Sheet1", parse_dates=["date_strings"])


Cell converters
+++++++++++++++

It is possible to transform the contents of Excel cells via the ``converters``
option. For instance, to convert a column to boolean:

.. code-block:: python

   pd.read_excel("path_to_file.xls", "Sheet1", converters={"MyBools": bool})

This options handles missing values and treats exceptions in the converters
as missing data. Transformations are applied cell by cell rather than to the
column as a whole, so the array dtype is not guaranteed. For instance, a
column of integers with missing values cannot be transformed to an array
with integer dtype, because NaN is strictly a float. You can manually mask
missing data to recover integer dtype:

.. code-block:: python

   def cfun(x):
       return int(x) if x else -1


   pd.read_excel("path_to_file.xls", "Sheet1", converters={"MyInts": cfun})

Dtype specifications
++++++++++++++++++++

As an alternative to converters, the type for an entire column can
be specified using the ``dtype`` keyword, which takes a dictionary
mapping column names to types.  To interpret data with
no type inference, use the type ``str`` or ``object``.

.. code-block:: python

   pd.read_excel("path_to_file.xls", dtype={"MyInts": "int64", "MyText": str})

.. _io.excel_writer:

Writing Excel files
'''''''''''''''''''

Writing Excel files to disk
+++++++++++++++++++++++++++

To write a ``DataFrame`` object to a sheet of an Excel file, you can use the
``to_excel`` instance method.  The arguments are largely the same as ``to_csv``
described above, the first argument being the name of the excel file, and the
optional second argument the name of the sheet to which the ``DataFrame`` should be
written. For example:

.. code-block:: python

   df.to_excel("path_to_file.xlsx", sheet_name="Sheet1")

Files with a ``.xls`` extension will be written using ``xlwt`` and those with a
``.xlsx`` extension will be written using ``xlsxwriter`` (if available) or
``openpyxl``.

The ``DataFrame`` will be written in a way that tries to mimic the REPL output.
The ``index_label`` will be placed in the second
row instead of the first. You can place it in the first row by setting the
``merge_cells`` option in ``to_excel()`` to ``False``:

.. code-block:: python

   df.to_excel("path_to_file.xlsx", index_label="label", merge_cells=False)

In order to write separate ``DataFrames`` to separate sheets in a single Excel file,
one can pass an :class:`~pandas.io.excel.ExcelWriter`.

.. code-block:: python

   with pd.ExcelWriter("path_to_file.xlsx") as writer:
       df1.to_excel(writer, sheet_name="Sheet1")
       df2.to_excel(writer, sheet_name="Sheet2")

.. _io.excel_writing_buffer:

Writing Excel files to memory
+++++++++++++++++++++++++++++

pandas supports writing Excel files to buffer-like objects such as ``StringIO`` or
``BytesIO`` using :class:`~pandas.io.excel.ExcelWriter`.

.. code-block:: python

   from io import BytesIO

   bio = BytesIO()

   # By setting the 'engine' in the ExcelWriter constructor.
   writer = pd.ExcelWriter(bio, engine="xlsxwriter")
   df.to_excel(writer, sheet_name="Sheet1")

   # Save the workbook
   writer.save()

   # Seek to the beginning and read to copy the workbook to a variable in memory
   bio.seek(0)
   workbook = bio.read()

.. note::

    ``engine`` is optional but recommended.  Setting the engine determines
    the version of workbook produced. Setting ``engine='xlrd'`` will produce an
    Excel 2003-format workbook (xls).  Using either ``'openpyxl'`` or
    ``'xlsxwriter'`` will produce an Excel 2007-format workbook (xlsx). If
    omitted, an Excel 2007-formatted workbook is produced.


.. _io.excel.writers:

Excel writer engines
''''''''''''''''''''

.. deprecated:: 1.2.0

   As the `xlwt <https://pypi.org/project/xlwt/>`__ package is no longer
   maintained, the ``xlwt`` engine will be removed from a future version
   of pandas. This is the only engine in pandas that supports writing to
   ``.xls`` files.

pandas chooses an Excel writer via two methods:

1. the ``engine`` keyword argument
2. the filename extension (via the default specified in config options)

By default, pandas uses the `XlsxWriter`_  for ``.xlsx``, `openpyxl`_
for ``.xlsm``, and `xlwt`_ for ``.xls`` files. If you have multiple
engines installed, you can set the default engine through :ref:`setting the
config options <options>` ``io.excel.xlsx.writer`` and
``io.excel.xls.writer``. pandas will fall back on `openpyxl`_ for ``.xlsx``
files if `Xlsxwriter`_ is not available.

.. _XlsxWriter: https://xlsxwriter.readthedocs.io
.. _openpyxl: https://openpyxl.readthedocs.io/
.. _xlwt: http://www.python-excel.org

To specify which writer you want to use, you can pass an engine keyword
argument to ``to_excel`` and to ``ExcelWriter``. The built-in engines are:

* ``openpyxl``: version 2.4 or higher is required
* ``xlsxwriter``
* ``xlwt``

.. code-block:: python

   # By setting the 'engine' in the DataFrame 'to_excel()' methods.
   df.to_excel("path_to_file.xlsx", sheet_name="Sheet1", engine="xlsxwriter")

   # By setting the 'engine' in the ExcelWriter constructor.
   writer = pd.ExcelWriter("path_to_file.xlsx", engine="xlsxwriter")

   # Or via pandas configuration.
   from pandas import options  # noqa: E402

   options.io.excel.xlsx.writer = "xlsxwriter"

   df.to_excel("path_to_file.xlsx", sheet_name="Sheet1")

.. _io.excel.style:

Style and formatting
''''''''''''''''''''

The look and feel of Excel worksheets created from pandas can be modified using the following parameters on the ``DataFrame``'s ``to_excel`` method.

* ``float_format`` : Format string for floating point numbers (default ``None``).
* ``freeze_panes`` : A tuple of two integers representing the bottommost row and rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will freeze the first row and first column (default ``None``).

Using the `Xlsxwriter`_ engine provides many options for controlling the
format of an Excel worksheet created with the ``to_excel`` method.  Excellent examples can be found in the
`Xlsxwriter`_ documentation here: https://xlsxwriter.readthedocs.io/working_with_pandas.html

.. _io.ods:

OpenDocument Spreadsheets
-------------------------

.. versionadded:: 0.25

The :func:`~pandas.read_excel` method can also read OpenDocument spreadsheets
using the ``odfpy`` module. The semantics and features for reading
OpenDocument spreadsheets match what can be done for `Excel files`_ using
``engine='odf'``.

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.ods", engine="odf")

.. note::

   Currently pandas only supports *reading* OpenDocument spreadsheets. Writing
   is not implemented.

.. _io.xlsb:

Binary Excel (.xlsb) files
--------------------------

.. versionadded:: 1.0.0

The :func:`~pandas.read_excel` method can also read binary Excel files
using the ``pyxlsb`` module. The semantics and features for reading
binary Excel files mostly match what can be done for `Excel files`_ using
``engine='pyxlsb'``. ``pyxlsb`` does not recognize datetime types
in files and will return floats instead.

.. code-block:: python

   # Returns a DataFrame
   pd.read_excel("path_to_file.xlsb", engine="pyxlsb")

.. note::

   Currently pandas only supports *reading* binary Excel files. Writing
   is not implemented.


.. _io.clipboard:

Clipboard
---------

A handy way to grab data is to use the :meth:`~DataFrame.read_clipboard` method,
which takes the contents of the clipboard buffer and passes them to the
``read_csv`` method. For instance, you can copy the following text to the
clipboard (CTRL-C on many operating systems):

.. code-block:: console

     A B C
   x 1 4 p
   y 2 5 q
   z 3 6 r

And then import the data directly to a ``DataFrame`` by calling:

.. code-block:: python

    >>> clipdf = pd.read_clipboard()
    >>> clipdf
      A B C
    x 1 4 p
    y 2 5 q
    z 3 6 r

The ``to_clipboard`` method can be used to write the contents of a ``DataFrame`` to
the clipboard. Following which you can paste the clipboard contents into other
applications (CTRL-V on many operating systems). Here we illustrate writing a
``DataFrame`` into clipboard and reading it back.

.. code-block:: python

    >>> df = pd.DataFrame(
    ...     {"A": [1, 2, 3], "B": [4, 5, 6], "C": ["p", "q", "r"]}, index=["x", "y", "z"]
    ... )

    >>> df
      A B C
    x 1 4 p
    y 2 5 q
    z 3 6 r
    >>> df.to_clipboard()
    >>> pd.read_clipboard()
      A B C
    x 1 4 p
    y 2 5 q
    z 3 6 r

We can see that we got the same content back, which we had earlier written to the clipboard.

.. note::

   You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to use these methods.

.. _io.pickle:

Pickling
--------

All pandas objects are equipped with ``to_pickle`` methods which use Python's
``cPickle`` module to save data structures to disk using the pickle format.

.. ipython:: python

   df
   df.to_pickle("foo.pkl")

The ``read_pickle`` function in the ``pandas`` namespace can be used to load
any pickled pandas object (or any other pickled object) from file:


.. ipython:: python

   pd.read_pickle("foo.pkl")

.. ipython:: python
   :suppress:

   os.remove("foo.pkl")

.. warning::

   Loading pickled data received from untrusted sources can be unsafe.

   See: https://docs.python.org/3/library/pickle.html

.. warning::

   :func:`read_pickle` is only guaranteed backwards compatible back to pandas version 0.20.3

.. _io.pickle.compression:

Compressed pickle files
'''''''''''''''''''''''

:func:`read_pickle`, :meth:`DataFrame.to_pickle` and :meth:`Series.to_pickle` can read
and write compressed pickle files. The compression types of ``gzip``, ``bz2``, ``xz``, ``zstd`` are supported for reading and writing.
The ``zip`` file format only supports reading and must contain only one data file
to be read.

The compression type can be an explicit parameter or be inferred from the file extension.
If 'infer', then use ``gzip``, ``bz2``, ``zip``, ``xz``, ``zstd`` if filename ends in ``'.gz'``, ``'.bz2'``, ``'.zip'``,
``'.xz'``, or ``'.zst'``, respectively.

The compression parameter can also be a ``dict`` in order to pass options to the
compression protocol. It must have a ``'method'`` key set to the name
of the compression protocol, which must be one of
{``'zip'``, ``'gzip'``, ``'bz2'``, ``'xz'``, ``'zstd'``}. All other key-value pairs are passed to
the underlying compression library.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": np.random.randn(1000),
           "B": "foo",
           "C": pd.date_range("20130101", periods=1000, freq="s"),
       }
   )
   df

Using an explicit compression type:

.. ipython:: python

   df.to_pickle("data.pkl.compress", compression="gzip")
   rt = pd.read_pickle("data.pkl.compress", compression="gzip")
   rt

Inferring compression type from the extension:

.. ipython:: python

   df.to_pickle("data.pkl.xz", compression="infer")
   rt = pd.read_pickle("data.pkl.xz", compression="infer")
   rt

The default is to 'infer':

.. ipython:: python

   df.to_pickle("data.pkl.gz")
   rt = pd.read_pickle("data.pkl.gz")
   rt

   df["A"].to_pickle("s1.pkl.bz2")
   rt = pd.read_pickle("s1.pkl.bz2")
   rt

Passing options to the compression protocol in order to speed up compression:

.. ipython:: python

   df.to_pickle("data.pkl.gz", compression={"method": "gzip", "compresslevel": 1})

.. ipython:: python
   :suppress:

   os.remove("data.pkl.compress")
   os.remove("data.pkl.xz")
   os.remove("data.pkl.gz")
   os.remove("s1.pkl.bz2")

.. _io.msgpack:

msgpack
-------

pandas support for ``msgpack`` has been removed in version 1.0.0. It is
recommended to use :ref:`pickle <io.pickle>` instead.

Alternatively, you can also the Arrow IPC serialization format for on-the-wire
transmission of pandas objects. For documentation on pyarrow, see
`here <https://arrow.apache.org/docs/python/ipc.html>`__.


.. _io.hdf5:

HDF5 (PyTables)
---------------

``HDFStore`` is a dict-like object which reads and writes pandas using
the high performance HDF5 format using the excellent `PyTables
<https://www.pytables.org/>`__ library. See the :ref:`cookbook <cookbook.hdf>`
for some advanced strategies

.. warning::

   pandas uses PyTables for reading and writing HDF5 files, which allows
   serializing object-dtype data with pickle. Loading pickled data received from
   untrusted sources can be unsafe.

   See: https://docs.python.org/3/library/pickle.html for more.

.. ipython:: python
   :suppress:
   :okexcept:

   os.remove("store.h5")

.. ipython:: python

   store = pd.HDFStore("store.h5")
   print(store)

Objects can be written to the file just like adding key-value pairs to a
dict:

.. ipython:: python

   index = pd.date_range("1/1/2000", periods=8)
   s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
   df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=["A", "B", "C"])

   # store.put('s', s) is an equivalent method
   store["s"] = s

   store["df"] = df

   store

In a current or later Python session, you can retrieve stored objects:

.. ipython:: python

   # store.get('df') is an equivalent method
   store["df"]

   # dotted (attribute) access provides get as well
   store.df

Deletion of the object specified by the key:

.. ipython:: python

   # store.remove('df') is an equivalent method
   del store["df"]

   store

Closing a Store and using a context manager:

.. ipython:: python

   store.close()
   store
   store.is_open

   # Working with, and automatically closing the store using a context manager
   with pd.HDFStore("store.h5") as store:
       store.keys()

.. ipython:: python
   :suppress:

   store.close()
   os.remove("store.h5")



Read/write API
''''''''''''''

``HDFStore`` supports a top-level API using  ``read_hdf`` for reading and ``to_hdf`` for writing,
similar to how ``read_csv`` and ``to_csv`` work.

.. ipython:: python

   df_tl = pd.DataFrame({"A": list(range(5)), "B": list(range(5))})
   df_tl.to_hdf("store_tl.h5", "table", append=True)
   pd.read_hdf("store_tl.h5", "table", where=["index>2"])

.. ipython:: python
   :suppress:
   :okexcept:

   os.remove("store_tl.h5")


HDFStore will by default not drop rows that are all missing. This behavior can be changed by setting ``dropna=True``.


.. ipython:: python

   df_with_missing = pd.DataFrame(
       {
           "col1": [0, np.nan, 2],
           "col2": [1, np.nan, np.nan],
       }
   )
   df_with_missing

   df_with_missing.to_hdf("file.h5", "df_with_missing", format="table", mode="w")

   pd.read_hdf("file.h5", "df_with_missing")

   df_with_missing.to_hdf(
       "file.h5", "df_with_missing", format="table", mode="w", dropna=True
   )
   pd.read_hdf("file.h5", "df_with_missing")


.. ipython:: python
   :suppress:

   os.remove("file.h5")


.. _io.hdf5-fixed:

Fixed format
''''''''''''

The examples above show storing using ``put``, which write the HDF5 to ``PyTables`` in a fixed array format, called
the ``fixed`` format. These types of stores are **not** appendable once written (though you can simply
remove them and rewrite). Nor are they **queryable**; they must be
retrieved in their entirety. They also do not support dataframes with non-unique column names.
The ``fixed`` format stores offer very fast writing and slightly faster reading than ``table`` stores.
This format is specified by default when using ``put`` or ``to_hdf`` or by ``format='fixed'`` or ``format='f'``.

.. warning::

   A ``fixed`` format will raise a ``TypeError`` if you try to retrieve using a ``where``:

   .. code-block:: python

       >>> pd.DataFrame(np.random.randn(10, 2)).to_hdf("test_fixed.h5", "df")
       >>> pd.read_hdf("test_fixed.h5", "df", where="index>5")
       TypeError: cannot pass a where specification when reading a fixed format.
                  this store must be selected in its entirety


.. _io.hdf5-table:

Table format
''''''''''''

``HDFStore`` supports another ``PyTables`` format on disk, the ``table``
format. Conceptually a ``table`` is shaped very much like a DataFrame,
with rows and columns. A ``table`` may be appended to in the same or
other sessions.  In addition, delete and query type operations are
supported. This format is specified by ``format='table'`` or ``format='t'``
to ``append`` or ``put`` or ``to_hdf``.

This format can be set as an option as well ``pd.set_option('io.hdf.default_format','table')`` to
enable ``put/append/to_hdf`` to by default store in the ``table`` format.

.. ipython:: python
   :suppress:
   :okexcept:

   os.remove("store.h5")

.. ipython:: python

   store = pd.HDFStore("store.h5")
   df1 = df[0:4]
   df2 = df[4:]

   # append data (creates a table automatically)
   store.append("df", df1)
   store.append("df", df2)
   store

   # select the entire object
   store.select("df")

   # the type of stored data
   store.root.df._v_attrs.pandas_type

.. note::

   You can also create a ``table`` by passing ``format='table'`` or ``format='t'`` to a ``put`` operation.

.. _io.hdf5-keys:

Hierarchical keys
'''''''''''''''''

Keys to a store can be specified as a string. These can be in a
hierarchical path-name like format (e.g. ``foo/bar/bah``), which will
generate a hierarchy of sub-stores (or ``Groups`` in PyTables
parlance). Keys can be specified without the leading '/' and are **always**
absolute (e.g. 'foo' refers to '/foo'). Removal operations can remove
everything in the sub-store and **below**, so be *careful*.

.. ipython:: python

   store.put("foo/bar/bah", df)
   store.append("food/orange", df)
   store.append("food/apple", df)
   store

   # a list of keys are returned
   store.keys()

   # remove all nodes under this level
   store.remove("food")
   store


You can walk through the group hierarchy using the ``walk`` method which
will yield a tuple for each group key along with the relative keys of its contents.

.. ipython:: python

   for (path, subgroups, subkeys) in store.walk():
       for subgroup in subgroups:
           print("GROUP: {}/{}".format(path, subgroup))
       for subkey in subkeys:
           key = "/".join([path, subkey])
           print("KEY: {}".format(key))
           print(store.get(key))



.. warning::

    Hierarchical keys cannot be retrieved as dotted (attribute) access as described above for items stored under the root node.

    .. code-block:: ipython

       In [8]: store.foo.bar.bah
       AttributeError: 'HDFStore' object has no attribute 'foo'

       # you can directly access the actual PyTables node but using the root node
       In [9]: store.root.foo.bar.bah
       Out[9]:
       /foo/bar/bah (Group) ''
         children := ['block0_items' (Array), 'block0_values' (Array), 'axis0' (Array), 'axis1' (Array)]

    Instead, use explicit string based keys:

    .. ipython:: python

       store["foo/bar/bah"]


.. _io.hdf5-types:

Storing types
'''''''''''''

Storing mixed types in a table
++++++++++++++++++++++++++++++

Storing mixed-dtype data is supported. Strings are stored as a
fixed-width using the maximum size of the appended column. Subsequent attempts
at appending longer strings will raise a ``ValueError``.

Passing ``min_itemsize={`values`: size}`` as a parameter to append
will set a larger minimum for the string columns. Storing ``floats,
strings, ints, bools, datetime64`` are currently supported. For string
columns, passing ``nan_rep = 'nan'`` to append will change the default
nan representation on disk (which converts to/from ``np.nan``), this
defaults to ``nan``.

.. ipython:: python

    df_mixed = pd.DataFrame(
        {
            "A": np.random.randn(8),
            "B": np.random.randn(8),
            "C": np.array(np.random.randn(8), dtype="float32"),
            "string": "string",
            "int": 1,
            "bool": True,
            "datetime64": pd.Timestamp("20010102"),
        },
        index=list(range(8)),
    )
    df_mixed.loc[df_mixed.index[3:5], ["A", "B", "string", "datetime64"]] = np.nan

    store.append("df_mixed", df_mixed, min_itemsize={"values": 50})
    df_mixed1 = store.select("df_mixed")
    df_mixed1
    df_mixed1.dtypes.value_counts()

    # we have provided a minimum string column size
    store.root.df_mixed.table

Storing MultiIndex DataFrames
+++++++++++++++++++++++++++++

Storing MultiIndex ``DataFrames`` as tables is very similar to
storing/selecting from homogeneous index ``DataFrames``.

.. ipython:: python

        index = pd.MultiIndex(
            levels=[["foo", "bar", "baz", "qux"], ["one", "two", "three"]],
            codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],
            names=["foo", "bar"],
        )
        df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=["A", "B", "C"])
        df_mi

        store.append("df_mi", df_mi)
        store.select("df_mi")

        # the levels are automatically included as data columns
        store.select("df_mi", "foo=bar")

.. note::
   The ``index`` keyword is reserved and cannot be use as a level name.

.. _io.hdf5-query:

Querying
''''''''

Querying a table
++++++++++++++++

``select`` and ``delete`` operations have an optional criterion that can
be specified to select/delete only a subset of the data. This allows one
to have a very large on-disk table and retrieve only a portion of the
data.

A query is specified using the ``Term`` class under the hood, as a boolean expression.

* ``index`` and ``columns`` are supported indexers of ``DataFrames``.
* if ``data_columns`` are specified, these can be used as additional indexers.
* level name in a MultiIndex, with default name  ``level_0``, ``level_1``, … if not provided.

Valid comparison operators are:

``=, ==, !=, >, >=, <, <=``

Valid boolean expressions are combined with:

* ``|`` : or
* ``&`` : and
* ``(`` and ``)`` : for grouping

These rules are similar to how boolean expressions are used in pandas for indexing.

.. note::

   - ``=`` will be automatically expanded to the comparison operator ``==``
   - ``~`` is the not operator, but can only be used in very limited
     circumstances
   - If a list/tuple of expressions is passed they will be combined via ``&``

The following are valid expressions:

* ``'index >= date'``
* ``"columns = ['A', 'D']"``
* ``"columns in ['A', 'D']"``
* ``'columns = A'``
* ``'columns == A'``
* ``"~(columns = ['A', 'B'])"``
* ``'index > df.index[3] & string = "bar"'``
* ``'(index > df.index[3] & index <= df.index[6]) | string = "bar"'``
* ``"ts >= Timestamp('2012-02-01')"``
* ``"major_axis>=20130101"``

The ``indexers`` are on the left-hand side of the sub-expression:

``columns``, ``major_axis``, ``ts``

The right-hand side of the sub-expression (after a comparison operator) can be:

* functions that will be evaluated, e.g. ``Timestamp('2012-02-01')``
* strings, e.g. ``"bar"``
* date-like, e.g. ``20130101``, or ``"20130101"``
* lists, e.g. ``"['A', 'B']"``
* variables that are defined in the local names space, e.g. ``date``

.. note::

   Passing a string to a query by interpolating it into the query
   expression is not recommended. Simply assign the string of interest to a
   variable and use that variable in an expression. For example, do this

   .. code-block:: python

      string = "HolyMoly'"
      store.select("df", "index == string")

   instead of this

   .. code-block:: ipython

      string = "HolyMoly'"
      store.select('df', f'index == {string}')

   The latter will **not** work and will raise a ``SyntaxError``.Note that
   there's a single quote followed by a double quote in the ``string``
   variable.

   If you *must* interpolate, use the ``'%r'`` format specifier

   .. code-block:: python

      store.select("df", "index == %r" % string)

   which will quote ``string``.


Here are some examples:

.. ipython:: python

    dfq = pd.DataFrame(
        np.random.randn(10, 4),
        columns=list("ABCD"),
        index=pd.date_range("20130101", periods=10),
    )
    store.append("dfq", dfq, format="table", data_columns=True)

Use boolean expressions, with in-line function evaluation.

.. ipython:: python

    store.select("dfq", "index>pd.Timestamp('20130104') & columns=['A', 'B']")

Use inline column reference.

.. ipython:: python

   store.select("dfq", where="A>0 or C>0")

The ``columns`` keyword can be supplied to select a list of columns to be
returned, this is equivalent to passing a
``'columns=list_of_columns_to_filter'``:

.. ipython:: python

   store.select("df", "columns=['A', 'B']")

``start`` and ``stop`` parameters can be specified to limit the total search
space. These are in terms of the total number of rows in a table.

.. note::

   ``select`` will raise a ``ValueError`` if the query expression has an unknown
   variable reference. Usually this means that you are trying to select on a column
   that is **not** a data_column.

   ``select`` will raise a ``SyntaxError`` if the query expression is not valid.


.. _io.hdf5-timedelta:

Query timedelta64[ns]
+++++++++++++++++++++

You can store and query using the ``timedelta64[ns]`` type. Terms can be
specified in the format: ``<float>(<unit>)``, where float may be signed (and fractional), and unit can be
``D,s,ms,us,ns`` for the timedelta. Here's an example:

.. ipython:: python

   from datetime import timedelta

   dftd = pd.DataFrame(
       {
           "A": pd.Timestamp("20130101"),
           "B": [
               pd.Timestamp("20130101") + timedelta(days=i, seconds=10)
               for i in range(10)
           ],
       }
   )
   dftd["C"] = dftd["A"] - dftd["B"]
   dftd
   store.append("dftd", dftd, data_columns=True)
   store.select("dftd", "C<'-3.5D'")

.. _io.query_multi:

Query MultiIndex
++++++++++++++++

Selecting from a ``MultiIndex`` can be achieved by using the name of the level.

.. ipython:: python

   df_mi.index.names
   store.select("df_mi", "foo=baz and bar=two")

If the ``MultiIndex`` levels names are ``None``, the levels are automatically made available via
the ``level_n`` keyword with ``n`` the level of the ``MultiIndex`` you want to select from.

.. ipython:: python

   index = pd.MultiIndex(
       levels=[["foo", "bar", "baz", "qux"], ["one", "two", "three"]],
       codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],
   )
   df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=["A", "B", "C"])
   df_mi_2

   store.append("df_mi_2", df_mi_2)

   # the levels are automatically included as data columns with keyword level_n
   store.select("df_mi_2", "level_0=foo and level_1=two")


Indexing
++++++++

You can create/modify an index for a table with ``create_table_index``
after data is already in the table (after and ``append/put``
operation). Creating a table index is **highly** encouraged. This will
speed your queries a great deal when you use a ``select`` with the
indexed dimension as the ``where``.

.. note::

   Indexes are automagically created on the indexables
   and any data columns you specify. This behavior can be turned off by passing
   ``index=False`` to ``append``.

.. ipython:: python

   # we have automagically already created an index (in the first section)
   i = store.root.df.table.cols.index.index
   i.optlevel, i.kind

   # change an index by passing new parameters
   store.create_table_index("df", optlevel=9, kind="full")
   i = store.root.df.table.cols.index.index
   i.optlevel, i.kind

Oftentimes when appending large amounts of data to a store, it is useful to turn off index creation for each append, then recreate at the end.

.. ipython:: python

   df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list("AB"))
   df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list("AB"))

   st = pd.HDFStore("appends.h5", mode="w")
   st.append("df", df_1, data_columns=["B"], index=False)
   st.append("df", df_2, data_columns=["B"], index=False)
   st.get_storer("df").table

Then create the index when finished appending.

.. ipython:: python

   st.create_table_index("df", columns=["B"], optlevel=9, kind="full")
   st.get_storer("df").table

   st.close()

.. ipython:: python
   :suppress:
   :okexcept:

   os.remove("appends.h5")

See `here <https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index>`__ for how to create a completely-sorted-index (CSI) on an existing store.

.. _io.hdf5-query-data-columns:

Query via data columns
++++++++++++++++++++++

You can designate (and index) certain columns that you want to be able
to perform queries (other than the ``indexable`` columns, which you can
always query). For instance say you want to perform this common
operation, on-disk, and return just the frame that matches this
query. You can specify ``data_columns = True`` to force all columns to
be ``data_columns``.

.. ipython:: python

   df_dc = df.copy()
   df_dc["string"] = "foo"
   df_dc.loc[df_dc.index[4:6], "string"] = np.nan
   df_dc.loc[df_dc.index[7:9], "string"] = "bar"
   df_dc["string2"] = "cool"
   df_dc.loc[df_dc.index[1:3], ["B", "C"]] = 1.0
   df_dc

   # on-disk operations
   store.append("df_dc", df_dc, data_columns=["B", "C", "string", "string2"])
   store.select("df_dc", where="B > 0")

   # getting creative
   store.select("df_dc", "B > 0 & C > 0 & string == foo")

   # this is in-memory version of this type of selection
   df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == "foo")]

   # we have automagically created this index and the B/C/string/string2
   # columns are stored separately as ``PyTables`` columns
   store.root.df_dc.table

There is some performance degradation by making lots of columns into
``data columns``, so it is up to the user to designate these. In addition,
you cannot change data columns (nor indexables) after the first
append/put operation (Of course you can simply read in the data and
create a new table!).

Iterator
++++++++

You can pass ``iterator=True`` or ``chunksize=number_in_a_chunk``
to ``select`` and ``select_as_multiple`` to return an iterator on the results.
The default is 50,000 rows returned in a chunk.

.. ipython:: python

   for df in store.select("df", chunksize=3):
       print(df)

.. note::

   You can also use the iterator with ``read_hdf`` which will open, then
   automatically close the store when finished iterating.

   .. code-block:: python

      for df in pd.read_hdf("store.h5", "df", chunksize=3):
          print(df)

Note, that the chunksize keyword applies to the **source** rows. So if you
are doing a query, then the chunksize will subdivide the total rows in the table
and the query applied, returning an iterator on potentially unequal sized chunks.

Here is a recipe for generating a query and using it to create equal sized return
chunks.

.. ipython:: python

   dfeq = pd.DataFrame({"number": np.arange(1, 11)})
   dfeq

   store.append("dfeq", dfeq, data_columns=["number"])

   def chunks(l, n):
       return [l[i: i + n] for i in range(0, len(l), n)]

   evens = [2, 4, 6, 8, 10]
   coordinates = store.select_as_coordinates("dfeq", "number=evens")
   for c in chunks(coordinates, 2):
       print(store.select("dfeq", where=c))

Advanced queries
++++++++++++++++

Select a single column
^^^^^^^^^^^^^^^^^^^^^^

To retrieve a single indexable or data column, use the
method ``select_column``. This will, for example, enable you to get the index
very quickly. These return a ``Series`` of the result, indexed by the row number.
These do not currently accept the ``where`` selector.

.. ipython:: python

   store.select_column("df_dc", "index")
   store.select_column("df_dc", "string")

.. _io.hdf5-selecting_coordinates:

Selecting coordinates
^^^^^^^^^^^^^^^^^^^^^

Sometimes you want to get the coordinates (a.k.a the index locations) of your query. This returns an
``Int64Index`` of the resulting locations. These coordinates can also be passed to subsequent
``where`` operations.

.. ipython:: python

   df_coord = pd.DataFrame(
       np.random.randn(1000, 2), index=pd.date_range("20000101", periods=1000)
   )
   store.append("df_coord", df_coord)
   c = store.select_as_coordinates("df_coord", "index > 20020101")
   c
   store.select("df_coord", where=c)

.. _io.hdf5-where_mask:

Selecting using a where mask
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sometime your query can involve creating a list of rows to select. Usually this ``mask`` would
be a resulting ``index`` from an indexing operation. This example selects the months of
a datetimeindex which are 5.

.. ipython:: python

   df_mask = pd.DataFrame(
       np.random.randn(1000, 2), index=pd.date_range("20000101", periods=1000)
   )
   store.append("df_mask", df_mask)
   c = store.select_column("df_mask", "index")
   where = c[pd.DatetimeIndex(c).month == 5].index
   store.select("df_mask", where=where)

Storer object
^^^^^^^^^^^^^

If you want to inspect the stored object, retrieve via
``get_storer``. You could use this programmatically to say get the number
of rows in an object.

.. ipython:: python

   store.get_storer("df_dc").nrows


Multiple table queries
++++++++++++++++++++++

The methods ``append_to_multiple`` and
``select_as_multiple`` can perform appending/selecting from
multiple tables at once. The idea is to have one table (call it the
selector table) that you index most/all of the columns, and perform your
queries. The other table(s) are data tables with an index matching the
selector table's index. You can then perform a very fast query
on the selector table, yet get lots of data back. This method is similar to
having a very wide table, but enables more efficient queries.

The ``append_to_multiple`` method splits a given single DataFrame
into multiple tables according to ``d``, a dictionary that maps the
table names to a list of 'columns' you want in that table. If ``None``
is used in place of a list, that table will have the remaining
unspecified columns of the given DataFrame. The argument ``selector``
defines which table is the selector table (which you can make queries from).
The argument ``dropna`` will drop rows from the input ``DataFrame`` to ensure
tables are synchronized.  This means that if a row for one of the tables
being written to is entirely ``np.NaN``, that row will be dropped from all tables.

If ``dropna`` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
Remember that entirely ``np.Nan`` rows are not written to the HDFStore, so if
you choose to call ``dropna=False``, some tables may have more rows than others,
and therefore ``select_as_multiple`` may not work or it may return unexpected
results.

.. ipython:: python

   df_mt = pd.DataFrame(
       np.random.randn(8, 6),
       index=pd.date_range("1/1/2000", periods=8),
       columns=["A", "B", "C", "D", "E", "F"],
   )
   df_mt["foo"] = "bar"
   df_mt.loc[df_mt.index[1], ("A", "B")] = np.nan

   # you can also create the tables individually
   store.append_to_multiple(
       {"df1_mt": ["A", "B"], "df2_mt": None}, df_mt, selector="df1_mt"
   )
   store

   # individual tables were created
   store.select("df1_mt")
   store.select("df2_mt")

   # as a multiple
   store.select_as_multiple(
       ["df1_mt", "df2_mt"],
       where=["A>0", "B>0"],
       selector="df1_mt",
   )


Delete from a table
'''''''''''''''''''

You can delete from a table selectively by specifying a ``where``. In
deleting rows, it is important to understand the ``PyTables`` deletes
rows by erasing the rows, then **moving** the following data. Thus
deleting can potentially be a very expensive operation depending on the
orientation of your data. To get optimal performance, it's
worthwhile to have the dimension you are deleting be the first of the
``indexables``.

Data is ordered (on the disk) in terms of the ``indexables``. Here's a
simple use case. You store panel-type data, with dates in the
``major_axis`` and ids in the ``minor_axis``. The data is then
interleaved like this:

* date_1
    * id_1
    * id_2
    *  .
    * id_n
* date_2
    * id_1
    *  .
    * id_n

It should be clear that a delete operation on the ``major_axis`` will be
fairly quick, as one chunk is removed, then the following data moved. On
the other hand a delete operation on the ``minor_axis`` will be very
expensive. In this case it would almost certainly be faster to rewrite
the table using a ``where`` that selects all but the missing data.

.. warning::

   Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files
   automatically. Thus, repeatedly deleting (or removing nodes) and adding
   again, **WILL TEND TO INCREASE THE FILE SIZE**.

   To *repack and clean* the file, use :ref:`ptrepack <io.hdf5-ptrepack>`.

.. _io.hdf5-notes:

Notes & caveats
'''''''''''''''


Compression
+++++++++++

``PyTables`` allows the stored data to be compressed. This applies to
all kinds of stores, not just tables. Two parameters are used to
control compression: ``complevel`` and ``complib``.

* ``complevel`` specifies if and how hard data is to be compressed.
  ``complevel=0`` and ``complevel=None`` disables compression and
  ``0<complevel<10`` enables compression.

* ``complib`` specifies which compression library to use.
  If nothing is  specified the default library ``zlib`` is used. A
  compression library usually optimizes for either good compression rates
  or speed and the results will depend on the type of data. Which type of
  compression to choose depends on your specific needs and data. The list
  of supported compression libraries:

  - `zlib <https://zlib.net/>`_: The default compression library.
    A classic in terms of compression, achieves good compression
    rates but is somewhat slow.
  - `lzo <https://www.oberhumer.com/opensource/lzo/>`_: Fast
    compression and decompression.
  - `bzip2 <https://sourceware.org/bzip2/>`_: Good compression rates.
  - `blosc <https://www.blosc.org/>`_: Fast compression and
    decompression.

    Support for alternative blosc compressors:

    - `blosc:blosclz <https://www.blosc.org/>`_ This is the
      default compressor for ``blosc``
    - `blosc:lz4
      <https://fastcompression.blogspot.com/p/lz4.html>`_:
      A compact, very popular and fast compressor.
    - `blosc:lz4hc
      <https://fastcompression.blogspot.com/p/lz4.html>`_:
      A tweaked version of LZ4, produces better
      compression ratios at the expense of speed.
    - `blosc:snappy <https://google.github.io/snappy/>`_:
      A popular compressor used in many places.
    - `blosc:zlib <https://zlib.net/>`_: A classic;
      somewhat slower than the previous ones, but
      achieving better compression ratios.
    - `blosc:zstd <https://facebook.github.io/zstd/>`_: An
      extremely well balanced codec; it provides the best
      compression ratios among the others above, and at
      reasonably fast speed.

  If ``complib`` is defined as something other than the listed libraries a
  ``ValueError`` exception is issued.

.. note::

   If the library specified with the ``complib`` option is missing on your platform,
   compression defaults to ``zlib`` without further ado.

Enable compression for all objects within the file:

.. code-block:: python

   store_compressed = pd.HDFStore(
       "store_compressed.h5", complevel=9, complib="blosc:blosclz"
   )

Or on-the-fly compression (this only applies to tables) in stores where compression is not enabled:

.. code-block:: python

   store.append("df", df, complib="zlib", complevel=5)

.. _io.hdf5-ptrepack:

ptrepack
++++++++

``PyTables`` offers better write performance when tables are compressed after
they are written, as opposed to turning on compression at the very
beginning. You can use the supplied ``PyTables`` utility
``ptrepack``. In addition, ``ptrepack`` can change compression levels
after the fact.

.. code-block:: console

   ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5

Furthermore ``ptrepack in.h5 out.h5`` will *repack* the file to allow
you to reuse previously deleted space. Alternatively, one can simply
remove the file and write again, or use the ``copy`` method.

.. _io.hdf5-caveats:

Caveats
+++++++

.. warning::

   ``HDFStore`` is **not-threadsafe for writing**. The underlying
   ``PyTables`` only supports concurrent reads (via threading or
   processes). If you need reading and writing *at the same time*, you
   need to serialize these operations in a single thread in a single
   process. You will corrupt your data otherwise. See the (:issue:`2397`) for more information.

* If you use locks to manage write access between multiple processes, you
  may want to use :py:func:`~os.fsync` before releasing write locks. For
  convenience you can use ``store.flush(fsync=True)`` to do this for you.
* Once a ``table`` is created columns (DataFrame)
  are fixed; only exactly the same columns can be appended
* Be aware that timezones (e.g., ``pytz.timezone('US/Eastern')``)
  are not necessarily equal across timezone versions.  So if data is
  localized to a specific timezone in the HDFStore using one version
  of a timezone library and that data is updated with another version, the data
  will be converted to UTC since these timezones are not considered
  equal.  Either use the same version of timezone library or use ``tz_convert`` with
  the updated timezone definition.

.. warning::

   ``PyTables`` will show a ``NaturalNameWarning`` if a column name
   cannot be used as an attribute selector.
   *Natural* identifiers contain only letters, numbers, and underscores,
   and may not begin with a number.
   Other identifiers cannot be used in a ``where`` clause
   and are generally a bad idea.

.. _io.hdf5-data_types:

DataTypes
'''''''''

``HDFStore`` will map an object dtype to the ``PyTables`` underlying
dtype. This means the following types are known to work:

======================================================  =========================
Type                                                    Represents missing values
======================================================  =========================
floating : ``float64, float32, float16``                ``np.nan``
integer : ``int64, int32, int8, uint64,uint32, uint8``
boolean
``datetime64[ns]``                                      ``NaT``
``timedelta64[ns]``                                     ``NaT``
categorical : see the section below
object : ``strings``                                    ``np.nan``
======================================================  =========================

``unicode`` columns are not supported, and **WILL FAIL**.

.. _io.hdf5-categorical:

Categorical data
++++++++++++++++

You can write data that contains ``category`` dtypes to a ``HDFStore``.
Queries work the same as if it was an object array. However, the ``category`` dtyped data is
stored in a more efficient manner.

.. ipython:: python

   dfcat = pd.DataFrame(
       {"A": pd.Series(list("aabbcdba")).astype("category"), "B": np.random.randn(8)}
   )
   dfcat
   dfcat.dtypes
   cstore = pd.HDFStore("cats.h5", mode="w")
   cstore.append("dfcat", dfcat, format="table", data_columns=["A"])
   result = cstore.select("dfcat", where="A in ['b', 'c']")
   result
   result.dtypes

.. ipython:: python
   :suppress:
   :okexcept:

   cstore.close()
   os.remove("cats.h5")


String columns
++++++++++++++

**min_itemsize**

The underlying implementation of ``HDFStore`` uses a fixed column width (itemsize) for string columns.
A string column itemsize is calculated as the maximum of the
length of data (for that column) that is passed to the ``HDFStore``, **in the first append**. Subsequent appends,
may introduce a string for a column **larger** than the column can hold, an Exception will be raised (otherwise you
could have a silent truncation of these columns, leading to loss of information). In the future we may relax this and
allow a user-specified truncation to occur.

Pass ``min_itemsize`` on the first table creation to a-priori specify the minimum length of a particular string column.
``min_itemsize`` can be an integer, or a dict mapping a column name to an integer. You can pass ``values`` as a key to
allow all *indexables* or *data_columns* to have this min_itemsize.

Passing a ``min_itemsize`` dict will cause all passed columns to be created as *data_columns* automatically.

.. note::

   If you are not passing any ``data_columns``, then the ``min_itemsize`` will be the maximum of the length of any string passed

.. ipython:: python

   dfs = pd.DataFrame({"A": "foo", "B": "bar"}, index=list(range(5)))
   dfs

   # A and B have a size of 30
   store.append("dfs", dfs, min_itemsize=30)
   store.get_storer("dfs").table

   # A is created as a data_column with a size of 30
   # B is size is calculated
   store.append("dfs2", dfs, min_itemsize={"A": 30})
   store.get_storer("dfs2").table

**nan_rep**

String columns will serialize a ``np.nan`` (a missing value) with the ``nan_rep`` string representation. This defaults to the string value ``nan``.
You could inadvertently turn an actual ``nan`` value into a missing value.

.. ipython:: python

   dfss = pd.DataFrame({"A": ["foo", "bar", "nan"]})
   dfss

   store.append("dfss", dfss)
   store.select("dfss")

   # here you need to specify a different nan rep
   store.append("dfss2", dfss, nan_rep="_nan_")
   store.select("dfss2")

.. _io.external_compatibility:

External compatibility
''''''''''''''''''''''

``HDFStore`` writes ``table`` format objects in specific formats suitable for
producing loss-less round trips to pandas objects. For external
compatibility, ``HDFStore`` can read native ``PyTables`` format
tables.

It is possible to write an ``HDFStore`` object that can easily be imported into ``R`` using the
``rhdf5`` library (`Package website`_). Create a table format store like this:

.. _package website: https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html

.. ipython:: python

   df_for_r = pd.DataFrame(
       {
           "first": np.random.rand(100),
           "second": np.random.rand(100),
           "class": np.random.randint(0, 2, (100,)),
       },
       index=range(100),
   )
   df_for_r.head()

   store_export = pd.HDFStore("export.h5")
   store_export.append("df_for_r", df_for_r, data_columns=df_dc.columns)
   store_export

.. ipython:: python
   :suppress:

   store_export.close()
   os.remove("export.h5")

In R this file can be read into a ``data.frame`` object using the ``rhdf5``
library. The following example function reads the corresponding column names
and data values from the values and assembles them into a ``data.frame``:

.. code-block:: R

   # Load values and column names for all datasets from corresponding nodes and
   # insert them into one data.frame object.

   library(rhdf5)

   loadhdf5data <- function(h5File) {

   listing <- h5ls(h5File)
   # Find all data nodes, values are stored in *_values and corresponding column
   # titles in *_items
   data_nodes <- grep("_values", listing$name)
   name_nodes <- grep("_items", listing$name)
   data_paths = paste(listing$group[data_nodes], listing$name[data_nodes], sep = "/")
   name_paths = paste(listing$group[name_nodes], listing$name[name_nodes], sep = "/")
   columns = list()
   for (idx in seq(data_paths)) {
     # NOTE: matrices returned by h5read have to be transposed to obtain
     # required Fortran order!
     data <- data.frame(t(h5read(h5File, data_paths[idx])))
     names <- t(h5read(h5File, name_paths[idx]))
     entry <- data.frame(data)
     colnames(entry) <- names
     columns <- append(columns, entry)
   }

   data <- data.frame(columns)

   return(data)
   }

Now you can import the ``DataFrame`` into R:

.. code-block:: R

   > data = loadhdf5data("transfer.hdf5")
   > head(data)
            first    second class
   1 0.4170220047 0.3266449     0
   2 0.7203244934 0.5270581     0
   3 0.0001143748 0.8859421     1
   4 0.3023325726 0.3572698     1
   5 0.1467558908 0.9085352     1
   6 0.0923385948 0.6233601     1

.. note::
   The R function lists the entire HDF5 file's contents and assembles the
   ``data.frame`` object from all matching nodes, so use this only as a
   starting point if you have stored multiple ``DataFrame`` objects to a
   single HDF5 file.


Performance
'''''''''''

* ``tables`` format come with a writing performance penalty as compared to
  ``fixed`` stores. The benefit is the ability to append/delete and
  query (potentially very large amounts of data).  Write times are
  generally longer as compared with regular stores. Query times can
  be quite fast, especially on an indexed axis.
* You can pass ``chunksize=<int>`` to ``append``, specifying the
  write chunksize (default is 50000). This will significantly lower
  your memory usage on writing.
* You can pass ``expectedrows=<int>`` to the first ``append``,
  to set the TOTAL number of rows that ``PyTables`` will expect.
  This will optimize read/write performance.
* Duplicate rows can be written to tables, but are filtered out in
  selection (with the last items being selected; thus a table is
  unique on major, minor pairs)
* A ``PerformanceWarning`` will be raised if you are attempting to
  store types that will be pickled by PyTables (rather than stored as
  endemic types). See
  `Here <https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190>`__
  for more information and some solutions.


.. ipython:: python
   :suppress:

   store.close()
   os.remove("store.h5")


.. _io.feather:

Feather
-------

Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data
frames efficient, and to make sharing data across data analysis languages easy.

Feather is designed to faithfully serialize and de-serialize DataFrames, supporting all of the pandas
dtypes, including extension dtypes such as categorical and datetime with tz.

Several caveats:

* The format will NOT write an ``Index``, or ``MultiIndex`` for the
  ``DataFrame`` and will raise an error if a non-default one is provided. You
  can ``.reset_index()`` to store the index or ``.reset_index(drop=True)`` to
  ignore it.
* Duplicate column names and non-string columns names are not supported
* Actual Python objects in object dtype columns are not supported. These will
  raise a helpful error message on an attempt at serialization.

See the `Full Documentation <https://github.com/wesm/feather>`__.

.. ipython:: python

   df = pd.DataFrame(
       {
           "a": list("abc"),
           "b": list(range(1, 4)),
           "c": np.arange(3, 6).astype("u1"),
           "d": np.arange(4.0, 7.0, dtype="float64"),
           "e": [True, False, True],
           "f": pd.Categorical(list("abc")),
           "g": pd.date_range("20130101", periods=3),
           "h": pd.date_range("20130101", periods=3, tz="US/Eastern"),
           "i": pd.date_range("20130101", periods=3, freq="ns"),
       }
   )

   df
   df.dtypes

Write to a feather file.

.. ipython:: python

   df.to_feather("example.feather")

Read from a feather file.

.. ipython:: python
   :okwarning:

   result = pd.read_feather("example.feather")
   result

   # we preserve dtypes
   result.dtypes

.. ipython:: python
   :suppress:

   os.remove("example.feather")


.. _io.parquet:

Parquet
-------

`Apache Parquet <https://parquet.apache.org/>`__ provides a partitioned binary columnar serialization for data frames. It is designed to
make reading and writing data frames efficient, and to make sharing data across data analysis
languages easy. Parquet can use a variety of compression techniques to shrink the file size as much as possible
while still maintaining good read performance.

Parquet is designed to faithfully serialize and de-serialize ``DataFrame`` s, supporting all of the pandas
dtypes, including extension dtypes such as datetime with tz.

Several caveats.

* Duplicate column names and non-string columns names are not supported.
* The ``pyarrow`` engine always writes the index to the output, but ``fastparquet`` only writes non-default
  indexes. This extra column can cause problems for non-pandas consumers that are not expecting it. You can
  force including or omitting indexes with the ``index`` argument, regardless of the underlying engine.
* Index level names, if specified, must be strings.
* In the ``pyarrow`` engine, categorical dtypes for non-string types can be serialized to parquet, but will de-serialize as their primitive dtype.
* The ``pyarrow`` engine preserves the ``ordered`` flag of categorical dtypes with string types. ``fastparquet`` does not preserve the ``ordered`` flag.
* Non supported types include ``Interval`` and actual Python object types. These will raise a helpful error message
  on an attempt at serialization. ``Period`` type is supported with pyarrow >= 0.16.0.
* The ``pyarrow`` engine preserves extension data types such as the nullable integer and string data
  type (requiring pyarrow >= 0.16.0, and requiring the extension type to implement the needed protocols,
  see the :ref:`extension types documentation <extending.extension.arrow>`).

You can specify an ``engine`` to direct the serialization. This can be one of ``pyarrow``, or ``fastparquet``, or ``auto``.
If the engine is NOT specified, then the ``pd.options.io.parquet.engine`` option is checked; if this is also ``auto``,
then ``pyarrow`` is tried, and falling back to ``fastparquet``.

See the documentation for `pyarrow <https://arrow.apache.org/docs/python/>`__ and `fastparquet <https://fastparquet.readthedocs.io/en/latest/>`__.

.. note::

   These engines are very similar and should read/write nearly identical parquet format files.
   Currently ``pyarrow`` does not support timedelta data, ``fastparquet>=0.1.4`` supports timezone aware datetimes.
   These libraries differ by having different underlying dependencies (``fastparquet`` by using ``numba``, while ``pyarrow`` uses a c-library).

.. ipython:: python

   df = pd.DataFrame(
       {
           "a": list("abc"),
           "b": list(range(1, 4)),
           "c": np.arange(3, 6).astype("u1"),
           "d": np.arange(4.0, 7.0, dtype="float64"),
           "e": [True, False, True],
           "f": pd.date_range("20130101", periods=3),
           "g": pd.date_range("20130101", periods=3, tz="US/Eastern"),
           "h": pd.Categorical(list("abc")),
           "i": pd.Categorical(list("abc"), ordered=True),
       }
   )

   df
   df.dtypes

Write to a parquet file.

.. ipython:: python
   :okwarning:

   df.to_parquet("example_pa.parquet", engine="pyarrow")
   df.to_parquet("example_fp.parquet", engine="fastparquet")

Read from a parquet file.

.. ipython:: python
   :okwarning:

   result = pd.read_parquet("example_fp.parquet", engine="fastparquet")
   result = pd.read_parquet("example_pa.parquet", engine="pyarrow")

   result.dtypes

Read only certain columns of a parquet file.

.. ipython:: python

   result = pd.read_parquet(
       "example_fp.parquet",
       engine="fastparquet",
       columns=["a", "b"],
   )
   result = pd.read_parquet(
       "example_pa.parquet",
       engine="pyarrow",
       columns=["a", "b"],
   )
   result.dtypes


.. ipython:: python
   :suppress:

   os.remove("example_pa.parquet")
   os.remove("example_fp.parquet")


Handling indexes
''''''''''''''''

Serializing a ``DataFrame`` to parquet may include the implicit index as one or
more columns in the output file. Thus, this code:

.. ipython:: python

    df = pd.DataFrame({"a": [1, 2], "b": [3, 4]})
    df.to_parquet("test.parquet", engine="pyarrow")

creates a parquet file with *three* columns if you use ``pyarrow`` for serialization:
``a``, ``b``, and ``__index_level_0__``. If you're using ``fastparquet``, the
index `may or may not <https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write>`_
be written to the file.

This unexpected extra column causes some databases like Amazon Redshift to reject
the file, because that column doesn't exist in the target table.

If you want to omit a dataframe's indexes when writing, pass ``index=False`` to
:func:`~pandas.DataFrame.to_parquet`:

.. ipython:: python

    df.to_parquet("test.parquet", index=False)

This creates a parquet file with just the two expected columns, ``a`` and ``b``.
If your ``DataFrame`` has a custom index, you won't get it back when you load
this file into a ``DataFrame``.

Passing ``index=True`` will *always* write the index, even if that's not the
underlying engine's default behavior.

.. ipython:: python
   :suppress:

   os.remove("test.parquet")


Partitioning Parquet files
''''''''''''''''''''''''''

Parquet supports partitioning of data based on the values of one or more columns.

.. ipython:: python

    df = pd.DataFrame({"a": [0, 0, 1, 1], "b": [0, 1, 0, 1]})
    df.to_parquet(path="test", engine="pyarrow", partition_cols=["a"], compression=None)

The ``path`` specifies the parent directory to which data will be saved.
The ``partition_cols`` are the column names by which the dataset will be partitioned.
Columns are partitioned in the order they are given. The partition splits are
determined by the unique values in the partition columns.
The above example creates a partitioned dataset that may look like:

.. code-block:: text

    test
    ├── a=0
    │   ├── 0bac803e32dc42ae83fddfd029cbdebc.parquet
    │   └──  ...
    └── a=1
        ├── e6ab24a4f45147b49b54a662f0c412a3.parquet
        └── ...

.. ipython:: python
   :suppress:

   from shutil import rmtree

   try:
       rmtree("test")
   except OSError:
       pass

.. _io.orc:

ORC
---

.. versionadded:: 1.0.0

Similar to the :ref:`parquet <io.parquet>` format, the `ORC Format <https://orc.apache.org/>`__ is a binary columnar serialization
for data frames. It is designed to make reading data frames efficient. pandas provides *only* a reader for the
ORC format, :func:`~pandas.read_orc`. This requires the `pyarrow <https://arrow.apache.org/docs/python/>`__ library.

.. warning::

   * It is *highly recommended* to install pyarrow using conda due to some issues occurred by pyarrow.
   * :func:`~pandas.read_orc` is not supported on Windows yet, you can find valid environments on :ref:`install optional dependencies <install.warn_orc>`.

.. _io.sql:

SQL queries
-----------

The :mod:`pandas.io.sql` module provides a collection of query wrappers to both
facilitate data retrieval and to reduce dependency on DB-specific API. Database abstraction
is provided by SQLAlchemy if installed. In addition you will need a driver library for
your database. Examples of such drivers are `psycopg2 <https://www.psycopg.org/>`__
for PostgreSQL or `pymysql <https://github.com/PyMySQL/PyMySQL>`__ for MySQL.
For `SQLite <https://docs.python.org/3/library/sqlite3.html>`__ this is
included in Python's standard library by default.
You can find an overview of supported drivers for each SQL dialect in the
`SQLAlchemy docs <https://docs.sqlalchemy.org/en/latest/dialects/index.html>`__.

If SQLAlchemy is not installed, a fallback is only provided for sqlite (and
for mysql for backwards compatibility, but this is deprecated and will be
removed in a future version).
This mode requires a Python database adapter which respect the `Python
DB-API <https://www.python.org/dev/peps/pep-0249/>`__.

See also some :ref:`cookbook examples <cookbook.sql>` for some advanced strategies.

The key functions are:

.. autosummary::

    read_sql_table
    read_sql_query
    read_sql
    DataFrame.to_sql

.. note::

    The function :func:`~pandas.read_sql` is a convenience wrapper around
    :func:`~pandas.read_sql_table` and :func:`~pandas.read_sql_query` (and for
    backward compatibility) and will delegate to specific function depending on
    the provided input (database table name or sql query).
    Table names do not need to be quoted if they have special characters.

In the following example, we use the `SQlite <https://www.sqlite.org/index.html>`__ SQL database
engine. You can use a temporary SQLite database where data are stored in
"memory".

To connect with SQLAlchemy you use the :func:`create_engine` function to create an engine
object from database URI. You only need to create the engine once per database you are
connecting to.
For more information on :func:`create_engine` and the URI formatting, see the examples
below and the SQLAlchemy `documentation <https://docs.sqlalchemy.org/en/latest/core/engines.html>`__

.. ipython:: python

   from sqlalchemy import create_engine

   # Create your engine.
   engine = create_engine("sqlite:///:memory:")

If you want to manage your own connections you can pass one of those instead. The example below opens a
connection to the database using a Python context manager that automatically closes the connection after
the block has completed.
See the `SQLAlchemy docs <https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage>`__
for an explanation of how the database connection is handled.

.. code-block:: python

   with engine.connect() as conn, conn.begin():
       data = pd.read_sql_table("data", conn)

.. warning::

	When you open a connection to a database you are also responsible for closing it.
	Side effects of leaving a connection open may include locking the database or
	other breaking behaviour.

Writing DataFrames
''''''''''''''''''

Assuming the following data is in a ``DataFrame`` ``data``, we can insert it into
the database using :func:`~pandas.DataFrame.to_sql`.

+-----+------------+-------+-------+-------+
| id  |    Date    | Col_1 | Col_2 | Col_3 |
+=====+============+=======+=======+=======+
| 26  | 2012-10-18 |   X   |  25.7 | True  |
+-----+------------+-------+-------+-------+
| 42  | 2012-10-19 |   Y   | -12.4 | False |
+-----+------------+-------+-------+-------+
| 63  | 2012-10-20 |   Z   |  5.73 | True  |
+-----+------------+-------+-------+-------+


.. ipython:: python
   :suppress:

   import datetime

   c = ["id", "Date", "Col_1", "Col_2", "Col_3"]
   d = [
       (26, datetime.datetime(2010, 10, 18), "X", 27.5, True),
       (42, datetime.datetime(2010, 10, 19), "Y", -12.5, False),
       (63, datetime.datetime(2010, 10, 20), "Z", 5.73, True),
   ]

   data = pd.DataFrame(d, columns=c)

.. ipython:: python

    data
    data.to_sql("data", engine)

With some databases, writing large DataFrames can result in errors due to
packet size limitations being exceeded. This can be avoided by setting the
``chunksize`` parameter when calling ``to_sql``.  For example, the following
writes ``data`` to the database in batches of 1000 rows at a time:

.. ipython:: python

    data.to_sql("data_chunked", engine, chunksize=1000)

SQL data types
++++++++++++++

:func:`~pandas.DataFrame.to_sql` will try to map your data to an appropriate
SQL data type based on the dtype of the data. When you have columns of dtype
``object``, pandas will try to infer the data type.

You can always override the default type by specifying the desired SQL type of
any of the columns by using the ``dtype`` argument. This argument needs a
dictionary mapping column names to SQLAlchemy types (or strings for the sqlite3
fallback mode).
For example, specifying to use the sqlalchemy ``String`` type instead of the
default ``Text`` type for string columns:

.. ipython:: python

    from sqlalchemy.types import String

    data.to_sql("data_dtype", engine, dtype={"Col_1": String})

.. note::

    Due to the limited support for timedelta's in the different database
    flavors, columns with type ``timedelta64`` will be written as integer
    values as nanoseconds to the database and a warning will be raised.

.. note::

    Columns of ``category`` dtype will be converted to the dense representation
    as you would get with ``np.asarray(categorical)`` (e.g. for string categories
    this gives an array of strings).
    Because of this, reading the database table back in does **not** generate
    a categorical.

.. _io.sql_datetime_data:

Datetime data types
'''''''''''''''''''

Using SQLAlchemy, :func:`~pandas.DataFrame.to_sql` is capable of writing
datetime data that is timezone naive or timezone aware. However, the resulting
data stored in the database ultimately depends on the supported data type
for datetime data of the database system being used.

The following table lists supported data types for datetime data for some
common databases. Other database dialects may have different data types for
datetime data.

===========   =============================================  ===================
Database      SQL Datetime Types                             Timezone Support
===========   =============================================  ===================
SQLite        ``TEXT``                                       No
MySQL         ``TIMESTAMP`` or ``DATETIME``                  No
PostgreSQL    ``TIMESTAMP`` or ``TIMESTAMP WITH TIME ZONE``  Yes
===========   =============================================  ===================

When writing timezone aware data to databases that do not support timezones,
the data will be written as timezone naive timestamps that are in local time
with respect to the timezone.

:func:`~pandas.read_sql_table` is also capable of reading datetime data that is
timezone aware or naive. When reading ``TIMESTAMP WITH TIME ZONE`` types, pandas
will convert the data to UTC.

.. _io.sql.method:

Insertion method
++++++++++++++++

The parameter ``method`` controls the SQL insertion clause used.
Possible values are:

- ``None``: Uses standard SQL ``INSERT`` clause (one per row).
- ``'multi'``: Pass multiple values in a single ``INSERT`` clause.
  It uses a *special* SQL syntax not supported by all backends.
  This usually provides better performance for analytic databases
  like *Presto* and *Redshift*, but has worse performance for
  traditional SQL backend if the table contains many columns.
  For more information check the SQLAlchemy `documentation
  <https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args>`__.
- callable with signature ``(pd_table, conn, keys, data_iter)``:
  This can be used to implement a more performant insertion method based on
  specific backend dialect features.

Example of a callable using PostgreSQL `COPY clause
<https://www.postgresql.org/docs/current/sql-copy.html>`__::

  # Alternative to_sql() *method* for DBs that support COPY FROM
  import csv
  from io import StringIO

  def psql_insert_copy(table, conn, keys, data_iter):
      """
      Execute SQL statement inserting data

      Parameters
      ----------
      table : pandas.io.sql.SQLTable
      conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection
      keys : list of str
          Column names
      data_iter : Iterable that iterates the values to be inserted
      """
      # gets a DBAPI connection that can provide a cursor
      dbapi_conn = conn.connection
      with dbapi_conn.cursor() as cur:
          s_buf = StringIO()
          writer = csv.writer(s_buf)
          writer.writerows(data_iter)
          s_buf.seek(0)

          columns = ', '.join(['"{}"'.format(k) for k in keys])
          if table.schema:
              table_name = '{}.{}'.format(table.schema, table.name)
          else:
              table_name = table.name

          sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(
              table_name, columns)
          cur.copy_expert(sql=sql, file=s_buf)

Reading tables
''''''''''''''

:func:`~pandas.read_sql_table` will read a database table given the
table name and optionally a subset of columns to read.

.. note::

    In order to use :func:`~pandas.read_sql_table`, you **must** have the
    SQLAlchemy optional dependency installed.

.. ipython:: python

   pd.read_sql_table("data", engine)

.. note::

  Note that pandas infers column dtypes from query outputs, and not by looking
  up data types in the physical database schema. For example, assume ``userid``
  is an integer column in a table. Then, intuitively, ``select userid ...`` will
  return integer-valued series, while ``select cast(userid as text) ...`` will
  return object-valued (str) series. Accordingly, if the query output is empty,
  then all resulting columns will be returned as object-valued (since they are
  most general). If you foresee that your query will sometimes generate an empty
  result, you may want to explicitly typecast afterwards to ensure dtype
  integrity.

You can also specify the name of the column as the ``DataFrame`` index,
and specify a subset of columns to be read.

.. ipython:: python

   pd.read_sql_table("data", engine, index_col="id")
   pd.read_sql_table("data", engine, columns=["Col_1", "Col_2"])

And you can explicitly force columns to be parsed as dates:

.. ipython:: python

   pd.read_sql_table("data", engine, parse_dates=["Date"])

If needed you can explicitly specify a format string, or a dict of arguments
to pass to :func:`pandas.to_datetime`:

.. code-block:: python

   pd.read_sql_table("data", engine, parse_dates={"Date": "%Y-%m-%d"})
   pd.read_sql_table(
       "data",
       engine,
       parse_dates={"Date": {"format": "%Y-%m-%d %H:%M:%S"}},
   )


You can check if a table exists using :func:`~pandas.io.sql.has_table`

Schema support
''''''''''''''

Reading from and writing to different schema's is supported through the ``schema``
keyword in the :func:`~pandas.read_sql_table` and :func:`~pandas.DataFrame.to_sql`
functions. Note however that this depends on the database flavor (sqlite does not
have schema's). For example:

.. code-block:: python

   df.to_sql("table", engine, schema="other_schema")
   pd.read_sql_table("table", engine, schema="other_schema")

Querying
''''''''

You can query using raw SQL in the :func:`~pandas.read_sql_query` function.
In this case you must use the SQL variant appropriate for your database.
When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs,
which are database-agnostic.

.. ipython:: python

   pd.read_sql_query("SELECT * FROM data", engine)

Of course, you can specify a more "complex" query.

.. ipython:: python

   pd.read_sql_query("SELECT id, Col_1, Col_2 FROM data WHERE id = 42;", engine)

The :func:`~pandas.read_sql_query` function supports a ``chunksize`` argument.
Specifying this will return an iterator through chunks of the query result:

.. ipython:: python

    df = pd.DataFrame(np.random.randn(20, 3), columns=list("abc"))
    df.to_sql("data_chunks", engine, index=False)

.. ipython:: python

    for chunk in pd.read_sql_query("SELECT * FROM data_chunks", engine, chunksize=5):
        print(chunk)

You can also run a plain query without creating a ``DataFrame`` with
:func:`~pandas.io.sql.execute`. This is useful for queries that don't return values,
such as INSERT. This is functionally equivalent to calling ``execute`` on the
SQLAlchemy engine or db connection object. Again, you must use the SQL syntax
variant appropriate for your database.

.. code-block:: python

   from pandas.io import sql

   sql.execute("SELECT * FROM table_name", engine)
   sql.execute(
       "INSERT INTO table_name VALUES(?, ?, ?)", engine, params=[("id", 1, 12.2, True)]
   )


Engine connection examples
''''''''''''''''''''''''''

To connect with SQLAlchemy you use the :func:`create_engine` function to create an engine
object from database URI. You only need to create the engine once per database you are
connecting to.

.. code-block:: python

   from sqlalchemy import create_engine

   engine = create_engine("postgresql://scott:tiger@localhost:5432/mydatabase")

   engine = create_engine("mysql+mysqldb://scott:tiger@localhost/foo")

   engine = create_engine("oracle://scott:tiger@127.0.0.1:1521/sidname")

   engine = create_engine("mssql+pyodbc://mydsn")

   # sqlite://<nohostname>/<path>
   # where <path> is relative:
   engine = create_engine("sqlite:///foo.db")

   # or absolute, starting with a slash:
   engine = create_engine("sqlite:////absolute/path/to/foo.db")

For more information see the examples the SQLAlchemy `documentation <https://docs.sqlalchemy.org/en/latest/core/engines.html>`__


Advanced SQLAlchemy queries
'''''''''''''''''''''''''''

You can use SQLAlchemy constructs to describe your query.

Use :func:`sqlalchemy.text` to specify query parameters in a backend-neutral way

.. ipython:: python

   import sqlalchemy as sa

   pd.read_sql(
       sa.text("SELECT * FROM data where Col_1=:col1"), engine, params={"col1": "X"}
   )

If you have an SQLAlchemy description of your database you can express where conditions using SQLAlchemy expressions

.. ipython:: python

   metadata = sa.MetaData()
   data_table = sa.Table(
       "data",
       metadata,
       sa.Column("index", sa.Integer),
       sa.Column("Date", sa.DateTime),
       sa.Column("Col_1", sa.String),
       sa.Column("Col_2", sa.Float),
       sa.Column("Col_3", sa.Boolean),
   )

   pd.read_sql(sa.select([data_table]).where(data_table.c.Col_3 is True), engine)

You can combine SQLAlchemy expressions with parameters passed to :func:`read_sql` using :func:`sqlalchemy.bindparam`

.. ipython:: python

    import datetime as dt

    expr = sa.select([data_table]).where(data_table.c.Date > sa.bindparam("date"))
    pd.read_sql(expr, engine, params={"date": dt.datetime(2010, 10, 18)})


Sqlite fallback
'''''''''''''''

The use of sqlite is supported without using SQLAlchemy.
This mode requires a Python database adapter which respect the `Python
DB-API <https://www.python.org/dev/peps/pep-0249/>`__.

You can create connections like so:

.. code-block:: python

   import sqlite3

   con = sqlite3.connect(":memory:")

And then issue the following queries:

.. code-block:: python

   data.to_sql("data", con)
   pd.read_sql_query("SELECT * FROM data", con)


.. _io.bigquery:

Google BigQuery
---------------

.. warning::

   Starting in 0.20.0, pandas has split off Google BigQuery support into the
   separate package ``pandas-gbq``. You can ``pip install pandas-gbq`` to get it.

The ``pandas-gbq`` package provides functionality to read/write from Google BigQuery.

pandas integrates with this external package. if ``pandas-gbq`` is installed, you can
use the pandas methods ``pd.read_gbq`` and ``DataFrame.to_gbq``, which will call the
respective functions from ``pandas-gbq``.

Full documentation can be found `here <https://pandas-gbq.readthedocs.io/en/latest/>`__.

.. _io.stata:

Stata format
------------

.. _io.stata_writer:

Writing to stata format
'''''''''''''''''''''''

The method :func:`~pandas.core.frame.DataFrame.to_stata` will write a DataFrame
into a .dta file. The format version of this file is always 115 (Stata 12).

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 2), columns=list("AB"))
   df.to_stata("stata.dta")

*Stata* data files have limited data type support; only strings with
244 or fewer characters, ``int8``, ``int16``, ``int32``, ``float32``
and ``float64`` can be stored in ``.dta`` files.  Additionally,
*Stata* reserves certain values to represent missing data. Exporting a
non-missing value that is outside of the permitted range in Stata for
a particular data type will retype the variable to the next larger
size.  For example, ``int8`` values are restricted to lie between -127
and 100 in Stata, and so variables with values above 100 will trigger
a conversion to ``int16``. ``nan`` values in floating points data
types are stored as the basic missing data type (``.`` in *Stata*).

.. note::

    It is not possible to export missing data values for integer data types.


The *Stata* writer gracefully handles other data types including ``int64``,
``bool``, ``uint8``, ``uint16``, ``uint32`` by casting to
the smallest supported type that can represent the data.  For example, data
with a type of ``uint8`` will be cast to ``int8`` if all values are less than
100 (the upper bound for non-missing ``int8`` data in *Stata*), or, if values are
outside of this range, the variable is cast to ``int16``.


.. warning::

   Conversion from ``int64`` to ``float64`` may result in a loss of precision
   if ``int64`` values are larger than 2**53.

.. warning::

  :class:`~pandas.io.stata.StataWriter` and
  :func:`~pandas.core.frame.DataFrame.to_stata` only support fixed width
  strings containing up to 244 characters, a limitation imposed by the version
  115 dta file format. Attempting to write *Stata* dta files with strings
  longer than 244 characters raises a ``ValueError``.

.. _io.stata_reader:

Reading from Stata format
'''''''''''''''''''''''''

The top-level function ``read_stata`` will read a dta file and return
either a ``DataFrame`` or a :class:`~pandas.io.stata.StataReader` that can
be used to read the file incrementally.

.. ipython:: python

   pd.read_stata("stata.dta")

Specifying a ``chunksize`` yields a
:class:`~pandas.io.stata.StataReader` instance that can be used to
read ``chunksize`` lines from the file at a time.  The ``StataReader``
object can be used as an iterator.

.. ipython:: python

  with pd.read_stata("stata.dta", chunksize=3) as reader:
      for df in reader:
          print(df.shape)

For more fine-grained control, use ``iterator=True`` and specify
``chunksize`` with each call to
:func:`~pandas.io.stata.StataReader.read`.

.. ipython:: python

  with pd.read_stata("stata.dta", iterator=True) as reader:
      chunk1 = reader.read(5)
      chunk2 = reader.read(5)

Currently the ``index`` is retrieved as a column.

The parameter ``convert_categoricals`` indicates whether value labels should be
read and used to create a ``Categorical`` variable from them. Value labels can
also be retrieved by the function ``value_labels``, which requires :func:`~pandas.io.stata.StataReader.read`
to be called before use.

The parameter ``convert_missing`` indicates whether missing value
representations in Stata should be preserved.  If ``False`` (the default),
missing values are represented as ``np.nan``.  If ``True``, missing values are
represented using ``StataMissingValue`` objects, and columns containing missing
values will have ``object`` data type.

.. note::

   :func:`~pandas.read_stata` and
   :class:`~pandas.io.stata.StataReader` support .dta formats 113-115
   (Stata 10-12), 117 (Stata 13), and 118 (Stata 14).

.. note::

   Setting ``preserve_dtypes=False`` will upcast to the standard pandas data types:
   ``int64`` for all integer types and ``float64`` for floating point data.  By default,
   the Stata data types are preserved when importing.

.. ipython:: python
   :suppress:

   os.remove("stata.dta")

.. _io.stata-categorical:

Categorical data
++++++++++++++++

``Categorical`` data can be exported to *Stata* data files as value labeled data.
The exported data consists of the underlying category codes as integer data values
and the categories as value labels.  *Stata* does not have an explicit equivalent
to a ``Categorical`` and information about *whether* the variable is ordered
is lost when exporting.

.. warning::

    *Stata* only supports string value labels, and so ``str`` is called on the
    categories when exporting data.  Exporting ``Categorical`` variables with
    non-string categories produces a warning, and can result a loss of
    information if the ``str`` representations of the categories are not unique.

Labeled data can similarly be imported from *Stata* data files as ``Categorical``
variables using the keyword argument ``convert_categoricals`` (``True`` by default).
The keyword argument ``order_categoricals`` (``True`` by default) determines
whether imported ``Categorical`` variables are ordered.

.. note::

    When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since ``Categorical`` variables always
    use integer data types between ``-1`` and ``n-1`` where ``n`` is the number
    of categories. If the original values in the *Stata* data file are required,
    these can be imported by setting ``convert_categoricals=False``, which will
    import original data (but not the variable labels). The original values can
    be matched to the imported categorical data since there is a simple mapping
    between the original *Stata* data values and the category codes of imported
    Categorical variables: missing values are assigned code ``-1``, and the
    smallest original value is assigned ``0``, the second smallest is assigned
    ``1`` and so on until the largest original value is assigned the code ``n-1``.

.. note::

    *Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a ``Categorical`` with string categories for the values that are labeled and
    numeric categories for values with no label.

.. _io.sas:

.. _io.sas_reader:

SAS formats
-----------

The top-level function :func:`read_sas` can read (but not write) SAS
XPORT (.xpt) and (since *v0.18.0*) SAS7BDAT (.sas7bdat) format files.

SAS files only contain two value types: ASCII text and floating point
values (usually 8 bytes but sometimes truncated).  For xport files,
there is no automatic type conversion to integers, dates, or
categoricals.  For SAS7BDAT files, the format codes may allow date
variables to be automatically converted to dates.  By default the
whole file is read and returned as a ``DataFrame``.

Specify a ``chunksize`` or use ``iterator=True`` to obtain reader
objects (``XportReader`` or ``SAS7BDATReader``) for incrementally
reading the file.  The reader objects also have attributes that
contain additional information about the file and its variables.

Read a SAS7BDAT file:

.. code-block:: python

    df = pd.read_sas("sas_data.sas7bdat")

Obtain an iterator and read an XPORT file 100,000 lines at a time:

.. code-block:: python

    def do_something(chunk):
        pass


    with pd.read_sas("sas_xport.xpt", chunk=100000) as rdr:
        for chunk in rdr:
            do_something(chunk)

The specification_ for the xport file format is available from the SAS
web site.

.. _specification: https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf

No official documentation is available for the SAS7BDAT format.

.. _io.spss:

.. _io.spss_reader:

SPSS formats
------------

.. versionadded:: 0.25.0

The top-level function :func:`read_spss` can read (but not write) SPSS
SAV (.sav) and  ZSAV (.zsav) format files.

SPSS files contain column names. By default the
whole file is read, categorical columns are converted into ``pd.Categorical``,
and a ``DataFrame`` with all columns is returned.

Specify the ``usecols`` parameter to obtain a subset of columns. Specify ``convert_categoricals=False``
to avoid converting categorical columns into ``pd.Categorical``.

Read an SPSS file:

.. code-block:: python

    df = pd.read_spss("spss_data.sav")

Extract a subset of columns contained in ``usecols`` from an SPSS file and
avoid converting categorical columns into ``pd.Categorical``:

.. code-block:: python

    df = pd.read_spss(
        "spss_data.sav",
        usecols=["foo", "bar"],
        convert_categoricals=False,
    )

More information about the SAV and ZSAV file formats is available here_.

.. _here: https://www.ibm.com/docs/en/spss-statistics/22.0.0

.. _io.other:

Other file formats
------------------

pandas itself only supports IO with a limited set of file formats that map
cleanly to its tabular data model. For reading and writing other file formats
into and from pandas, we recommend these packages from the broader community.

netCDF
''''''

xarray_ provides data structures inspired by the pandas ``DataFrame`` for working
with multi-dimensional datasets, with a focus on the netCDF file format and
easy conversion to and from pandas.

.. _xarray: https://xarray.pydata.org/en/stable/

.. _io.perf:

Performance considerations
--------------------------

This is an informal comparison of various IO methods, using pandas
0.24.2. Timings are machine dependent and small differences should be
ignored.

.. code-block:: ipython

   In [1]: sz = 1000000
   In [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})

   In [3]: df.info()
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 1000000 entries, 0 to 999999
   Data columns (total 2 columns):
   A    1000000 non-null float64
   B    1000000 non-null int64
   dtypes: float64(1), int64(1)
   memory usage: 15.3 MB

The following test functions will be used below to compare the performance of several IO methods:

.. code-block:: python



   import numpy as np

   import os

   sz = 1000000
   df = pd.DataFrame({"A": np.random.randn(sz), "B": [1] * sz})

   sz = 1000000
   np.random.seed(42)
   df = pd.DataFrame({"A": np.random.randn(sz), "B": [1] * sz})


   def test_sql_write(df):
       if os.path.exists("test.sql"):
           os.remove("test.sql")
       sql_db = sqlite3.connect("test.sql")
       df.to_sql(name="test_table", con=sql_db)
       sql_db.close()


   def test_sql_read():
       sql_db = sqlite3.connect("test.sql")
       pd.read_sql_query("select * from test_table", sql_db)
       sql_db.close()


   def test_hdf_fixed_write(df):
       df.to_hdf("test_fixed.hdf", "test", mode="w")


   def test_hdf_fixed_read():
       pd.read_hdf("test_fixed.hdf", "test")


   def test_hdf_fixed_write_compress(df):
       df.to_hdf("test_fixed_compress.hdf", "test", mode="w", complib="blosc")


   def test_hdf_fixed_read_compress():
       pd.read_hdf("test_fixed_compress.hdf", "test")


   def test_hdf_table_write(df):
       df.to_hdf("test_table.hdf", "test", mode="w", format="table")


   def test_hdf_table_read():
       pd.read_hdf("test_table.hdf", "test")


   def test_hdf_table_write_compress(df):
       df.to_hdf(
           "test_table_compress.hdf", "test", mode="w", complib="blosc", format="table"
       )


   def test_hdf_table_read_compress():
       pd.read_hdf("test_table_compress.hdf", "test")


   def test_csv_write(df):
       df.to_csv("test.csv", mode="w")


   def test_csv_read():
       pd.read_csv("test.csv", index_col=0)


   def test_feather_write(df):
       df.to_feather("test.feather")


   def test_feather_read():
       pd.read_feather("test.feather")


   def test_pickle_write(df):
       df.to_pickle("test.pkl")


   def test_pickle_read():
       pd.read_pickle("test.pkl")


   def test_pickle_write_compress(df):
       df.to_pickle("test.pkl.compress", compression="xz")


   def test_pickle_read_compress():
       pd.read_pickle("test.pkl.compress", compression="xz")


   def test_parquet_write(df):
       df.to_parquet("test.parquet")


   def test_parquet_read():
       pd.read_parquet("test.parquet")

When writing, the top three functions in terms of speed are ``test_feather_write``, ``test_hdf_fixed_write`` and ``test_hdf_fixed_write_compress``.

.. code-block:: ipython

   In [4]: %timeit test_sql_write(df)
   3.29 s ± 43.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [5]: %timeit test_hdf_fixed_write(df)
   19.4 ms ± 560 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [6]: %timeit test_hdf_fixed_write_compress(df)
   19.6 ms ± 308 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [7]: %timeit test_hdf_table_write(df)
   449 ms ± 5.61 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [8]: %timeit test_hdf_table_write_compress(df)
   448 ms ± 11.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [9]: %timeit test_csv_write(df)
   3.66 s ± 26.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [10]: %timeit test_feather_write(df)
   9.75 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

   In [11]: %timeit test_pickle_write(df)
   30.1 ms ± 229 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [12]: %timeit test_pickle_write_compress(df)
   4.29 s ± 15.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [13]: %timeit test_parquet_write(df)
   67.6 ms ± 706 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

When reading, the top three functions in terms of speed are ``test_feather_read``, ``test_pickle_read`` and
``test_hdf_fixed_read``.


.. code-block:: ipython

   In [14]: %timeit test_sql_read()
   1.77 s ± 17.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [15]: %timeit test_hdf_fixed_read()
   19.4 ms ± 436 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [16]: %timeit test_hdf_fixed_read_compress()
   19.5 ms ± 222 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [17]: %timeit test_hdf_table_read()
   38.6 ms ± 857 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [18]: %timeit test_hdf_table_read_compress()
   38.8 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

   In [19]: %timeit test_csv_read()
   452 ms ± 9.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [20]: %timeit test_feather_read()
   12.4 ms ± 99.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

   In [21]: %timeit test_pickle_read()
   18.4 ms ± 191 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

   In [22]: %timeit test_pickle_read_compress()
   915 ms ± 7.48 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

   In [23]: %timeit test_parquet_read()
   24.4 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)


The files ``test.pkl.compress``, ``test.parquet`` and ``test.feather`` took the least space on disk (in bytes).

.. code-block:: none

    29519500 Oct 10 06:45 test.csv
    16000248 Oct 10 06:45 test.feather
    8281983  Oct 10 06:49 test.parquet
    16000857 Oct 10 06:47 test.pkl
    7552144  Oct 10 06:48 test.pkl.compress
    34816000 Oct 10 06:42 test.sql
    24009288 Oct 10 06:43 test_fixed.hdf
    24009288 Oct 10 06:43 test_fixed_compress.hdf
    24458940 Oct 10 06:44 test_table.hdf
    24458940 Oct 10 06:44 test_table_compress.hdf
.. _text:

{{ header }}

======================
Working with text data
======================

.. _text.types:

Text data types
---------------

.. versionadded:: 1.0.0

There are two ways to store text data in pandas:

1. ``object`` -dtype NumPy array.
2. :class:`StringDtype` extension type.

We recommend using :class:`StringDtype` to store text data.

Prior to pandas 1.0, ``object`` dtype was the only option. This was unfortunate
for many reasons:

1. You can accidentally store a *mixture* of strings and non-strings in an
   ``object`` dtype array. It's better to have a dedicated dtype.
2. ``object`` dtype breaks dtype-specific operations like :meth:`DataFrame.select_dtypes`.
   There isn't a clear way to select *just* text while excluding non-text
   but still object-dtype columns.
3. When reading code, the contents of an ``object`` dtype array is less clear
   than ``'string'``.

Currently, the performance of ``object`` dtype arrays of strings and
:class:`arrays.StringArray` are about the same. We expect future enhancements
to significantly increase the performance and lower the memory overhead of
:class:`~arrays.StringArray`.

.. warning::

   ``StringArray`` is currently considered experimental. The implementation
   and parts of the API may change without warning.

For backwards-compatibility, ``object`` dtype remains the default type we
infer a list of strings to

.. ipython:: python

   pd.Series(["a", "b", "c"])

To explicitly request ``string`` dtype, specify the ``dtype``

.. ipython:: python

   pd.Series(["a", "b", "c"], dtype="string")
   pd.Series(["a", "b", "c"], dtype=pd.StringDtype())

Or ``astype`` after the ``Series`` or ``DataFrame`` is created

.. ipython:: python

   s = pd.Series(["a", "b", "c"])
   s
   s.astype("string")


.. versionchanged:: 1.1.0

You can also use :class:`StringDtype`/``"string"`` as the dtype on non-string data and
it will be converted to ``string`` dtype:

.. ipython:: python

   s = pd.Series(["a", 2, np.nan], dtype="string")
   s
   type(s[1])

or convert from existing pandas data:

.. ipython:: python

   s1 = pd.Series([1, 2, np.nan], dtype="Int64")
   s1
   s2 = s1.astype("string")
   s2
   type(s2[0])


.. _text.differences:

Behavior differences
^^^^^^^^^^^^^^^^^^^^

These are places where the behavior of ``StringDtype`` objects differ from
``object`` dtype

l. For ``StringDtype``, :ref:`string accessor methods<api.series.str>`
   that return **numeric** output will always return a nullable integer dtype,
   rather than either int or float dtype, depending on the presence of NA values.
   Methods returning **boolean** output will return a nullable boolean dtype.

   .. ipython:: python

      s = pd.Series(["a", None, "b"], dtype="string")
      s
      s.str.count("a")
      s.dropna().str.count("a")

   Both outputs are ``Int64`` dtype. Compare that with object-dtype

   .. ipython:: python

      s2 = pd.Series(["a", None, "b"], dtype="object")
      s2.str.count("a")
      s2.dropna().str.count("a")

   When NA values are present, the output dtype is float64. Similarly for
   methods returning boolean values.

   .. ipython:: python

      s.str.isdigit()
      s.str.match("a")

2. Some string methods, like :meth:`Series.str.decode` are not available
   on ``StringArray`` because ``StringArray`` only holds strings, not
   bytes.
3. In comparison operations, :class:`arrays.StringArray` and ``Series`` backed
   by a ``StringArray`` will return an object with :class:`BooleanDtype`,
   rather than a ``bool`` dtype object. Missing values in a ``StringArray``
   will propagate in comparison operations, rather than always comparing
   unequal like :attr:`numpy.nan`.

Everything else that follows in the rest of this document applies equally to
``string`` and ``object`` dtype.

.. _text.string_methods:

String methods
--------------

Series and Index are equipped with a set of string processing methods
that make it easy to operate on each element of the array. Perhaps most
importantly, these methods exclude missing/NA values automatically. These are
accessed via the ``str`` attribute and generally have names matching
the equivalent (scalar) built-in string methods:

.. ipython:: python

   s = pd.Series(
       ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string"
   )
   s.str.lower()
   s.str.upper()
   s.str.len()

.. ipython:: python

   idx = pd.Index([" jack", "jill ", " jesse ", "frank"])
   idx.str.strip()
   idx.str.lstrip()
   idx.str.rstrip()

The string methods on Index are especially useful for cleaning up or
transforming DataFrame columns. For instance, you may have columns with
leading or trailing whitespace:

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(3, 2), columns=[" Column A ", " Column B "], index=range(3)
   )
   df

Since ``df.columns`` is an Index object, we can use the ``.str`` accessor

.. ipython:: python

   df.columns.str.strip()
   df.columns.str.lower()

These string methods can then be used to clean up the columns as needed.
Here we are removing leading and trailing whitespaces, lower casing all names,
and replacing any remaining whitespaces with underscores:

.. ipython:: python

   df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
   df

.. note::

    If you have a ``Series`` where lots of elements are repeated
    (i.e. the number of unique elements in the ``Series`` is a lot smaller than the length of the
    ``Series``), it can be faster to convert the original ``Series`` to one of type
    ``category`` and then use ``.str.<method>`` or ``.dt.<property>`` on that.
    The performance difference comes from the fact that, for ``Series`` of type ``category``, the
    string operations are done on the ``.categories`` and not on each element of the
    ``Series``.

    Please note that a ``Series`` of type ``category`` with string ``.categories`` has
    some limitations in comparison to ``Series`` of type string (e.g. you can't add strings to
    each other: ``s + " " + s`` won't work if ``s`` is a ``Series`` of type ``category``). Also,
    ``.str`` methods which operate on elements of type ``list`` are not available on such a
    ``Series``.

.. _text.warn_types:

.. warning::

    Before v.0.25.0, the ``.str``-accessor did only the most rudimentary type checks. Starting with
    v.0.25.0, the type of the Series is inferred and the allowed types (i.e. strings) are enforced more rigorously.

    Generally speaking, the ``.str`` accessor is intended to work only on strings. With very few
    exceptions, other uses are not supported, and may be disabled at a later point.

.. _text.split:

Splitting and replacing strings
-------------------------------

Methods like ``split`` return a Series of lists:

.. ipython:: python

   s2 = pd.Series(["a_b_c", "c_d_e", np.nan, "f_g_h"], dtype="string")
   s2.str.split("_")

Elements in the split lists can be accessed using ``get`` or ``[]`` notation:

.. ipython:: python

   s2.str.split("_").str.get(1)
   s2.str.split("_").str[1]

It is easy to expand this to return a DataFrame using ``expand``.

.. ipython:: python

   s2.str.split("_", expand=True)

When original ``Series`` has :class:`StringDtype`, the output columns will all
be :class:`StringDtype` as well.

It is also possible to limit the number of splits:

.. ipython:: python

   s2.str.split("_", expand=True, n=1)

``rsplit`` is similar to ``split`` except it works in the reverse direction,
i.e., from the end of the string to the beginning of the string:

.. ipython:: python

   s2.str.rsplit("_", expand=True, n=1)

``replace`` optionally uses `regular expressions
<https://docs.python.org/3/library/re.html>`__:

.. ipython:: python

   s3 = pd.Series(
       ["A", "B", "C", "Aaba", "Baca", "", np.nan, "CABA", "dog", "cat"],
       dtype="string",
   )
   s3
   s3.str.replace("^.a|dog", "XX-XX ", case=False, regex=True)

.. warning::

    Some caution must be taken when dealing with regular expressions! The current behavior
    is to treat single character patterns as literal strings, even when ``regex`` is set
    to ``True``. This behavior is deprecated and will be removed in a future version so
    that the ``regex`` keyword is always respected.

.. versionchanged:: 1.2.0

If you want literal replacement of a string (equivalent to :meth:`str.replace`), you
can set the optional ``regex`` parameter to ``False``, rather than escaping each
character. In this case both ``pat`` and ``repl`` must be strings:

.. ipython:: python

    dollars = pd.Series(["12", "-$10", "$10,000"], dtype="string")

    # These lines are equivalent
    dollars.str.replace(r"-\$", "-", regex=True)
    dollars.str.replace("-$", "-", regex=False)

The ``replace`` method can also take a callable as replacement. It is called
on every ``pat`` using :func:`re.sub`. The callable should expect one
positional argument (a regex object) and return a string.

.. ipython:: python

   # Reverse every lowercase alphabetic word
   pat = r"[a-z]+"

   def repl(m):
       return m.group(0)[::-1]

   pd.Series(["foo 123", "bar baz", np.nan], dtype="string").str.replace(
       pat, repl, regex=True
   )

   # Using regex groups
   pat = r"(?P<one>\w+) (?P<two>\w+) (?P<three>\w+)"

   def repl(m):
       return m.group("two").swapcase()

   pd.Series(["Foo Bar Baz", np.nan], dtype="string").str.replace(
       pat, repl, regex=True
   )

The ``replace`` method also accepts a compiled regular expression object
from :func:`re.compile` as a pattern. All flags should be included in the
compiled regular expression object.

.. ipython:: python

   import re

   regex_pat = re.compile(r"^.a|dog", flags=re.IGNORECASE)
   s3.str.replace(regex_pat, "XX-XX ", regex=True)

Including a ``flags`` argument when calling ``replace`` with a compiled
regular expression object will raise a ``ValueError``.

.. ipython::

    @verbatim
    In [1]: s3.str.replace(regex_pat, 'XX-XX ', flags=re.IGNORECASE)
    ---------------------------------------------------------------------------
    ValueError: case and flags cannot be set when pat is a compiled regex

``removeprefix`` and ``removesuffix`` have the same effect as ``str.removeprefix`` and ``str.removesuffix`` added in Python 3.9
<https://docs.python.org/3/library/stdtypes.html#str.removeprefix>`__:

.. versionadded:: 1.4.0

.. ipython:: python

   s = pd.Series(["str_foo", "str_bar", "no_prefix"])
   s.str.removeprefix("str_")

   s = pd.Series(["foo_str", "bar_str", "no_suffix"])
   s.str.removesuffix("_str")

.. _text.concatenate:

Concatenation
-------------

There are several ways to concatenate a ``Series`` or ``Index``, either with itself or others, all based on :meth:`~Series.str.cat`,
resp. ``Index.str.cat``.

Concatenating a single Series into a string
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The content of a ``Series`` (or ``Index``) can be concatenated:

.. ipython:: python

    s = pd.Series(["a", "b", "c", "d"], dtype="string")
    s.str.cat(sep=",")

If not specified, the keyword ``sep`` for the separator defaults to the empty string, ``sep=''``:

.. ipython:: python

    s.str.cat()

By default, missing values are ignored. Using ``na_rep``, they can be given a representation:

.. ipython:: python

    t = pd.Series(["a", "b", np.nan, "d"], dtype="string")
    t.str.cat(sep=",")
    t.str.cat(sep=",", na_rep="-")

Concatenating a Series and something list-like into a Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The first argument to :meth:`~Series.str.cat` can be a list-like object, provided that it matches the length of the calling ``Series`` (or ``Index``).

.. ipython:: python

    s.str.cat(["A", "B", "C", "D"])

Missing values on either side will result in missing values in the result as well, *unless* ``na_rep`` is specified:

.. ipython:: python

    s.str.cat(t)
    s.str.cat(t, na_rep="-")

Concatenating a Series and something array-like into a Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The parameter ``others`` can also be two-dimensional. In this case, the number or rows must match the lengths of the calling ``Series`` (or ``Index``).

.. ipython:: python

    d = pd.concat([t, s], axis=1)
    s
    d
    s.str.cat(d, na_rep="-")

Concatenating a Series and an indexed object into a Series, with alignment
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For concatenation with a ``Series`` or ``DataFrame``, it is possible to align the indexes before concatenation by setting
the ``join``-keyword.

.. ipython:: python
   :okwarning:

   u = pd.Series(["b", "d", "a", "c"], index=[1, 3, 0, 2], dtype="string")
   s
   u
   s.str.cat(u)
   s.str.cat(u, join="left")

.. warning::

    If the ``join`` keyword is not passed, the method :meth:`~Series.str.cat` will currently fall back to the behavior before version 0.23.0 (i.e. no alignment),
    but a ``FutureWarning`` will be raised if any of the involved indexes differ, since this default will change to ``join='left'`` in a future version.

The usual options are available for ``join`` (one of ``'left', 'outer', 'inner', 'right'``).
In particular, alignment also means that the different lengths do not need to coincide anymore.

.. ipython:: python

    v = pd.Series(["z", "a", "b", "d", "e"], index=[-1, 0, 1, 3, 4], dtype="string")
    s
    v
    s.str.cat(v, join="left", na_rep="-")
    s.str.cat(v, join="outer", na_rep="-")

The same alignment can be used when ``others`` is a ``DataFrame``:

.. ipython:: python

    f = d.loc[[3, 2, 1, 0], :]
    s
    f
    s.str.cat(f, join="left", na_rep="-")

Concatenating a Series and many objects into a Series
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Several array-like items (specifically: ``Series``, ``Index``, and 1-dimensional variants of ``np.ndarray``)
can be combined in a list-like container (including iterators, ``dict``-views, etc.).

.. ipython:: python

    s
    u
    s.str.cat([u, u.to_numpy()], join="left")

All elements without an index (e.g. ``np.ndarray``) within the passed list-like must match in length to the calling ``Series`` (or ``Index``),
but ``Series`` and ``Index`` may have arbitrary length (as long as alignment is not disabled with ``join=None``):

.. ipython:: python

    v
    s.str.cat([v, u, u.to_numpy()], join="outer", na_rep="-")

If using ``join='right'`` on a list-like of ``others`` that contains different indexes,
the union of these indexes will be used as the basis for the final concatenation:

.. ipython:: python

    u.loc[[3]]
    v.loc[[-1, 0]]
    s.str.cat([u.loc[[3]], v.loc[[-1, 0]]], join="right", na_rep="-")

Indexing with ``.str``
----------------------

.. _text.indexing:

You can use ``[]`` notation to directly index by position locations. If you index past the end
of the string, the result will be a ``NaN``.


.. ipython:: python

   s = pd.Series(
       ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string"
   )

   s.str[0]
   s.str[1]

Extracting substrings
---------------------

.. _text.extract:

Extract first match in each subject (extract)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning::

   Before version 0.23, argument ``expand`` of the ``extract`` method defaulted to
   ``False``. When ``expand=False``, ``expand`` returns a ``Series``, ``Index``, or
   ``DataFrame``, depending on the subject and regular expression
   pattern. When ``expand=True``, it always returns a ``DataFrame``,
   which is more consistent and less confusing from the perspective of a user.
   ``expand=True`` has been the default since version 0.23.0.

The ``extract`` method accepts a `regular expression
<https://docs.python.org/3/library/re.html>`__ with at least one
capture group.

Extracting a regular expression with more than one group returns a
DataFrame with one column per group.

.. ipython:: python

   pd.Series(
       ["a1", "b2", "c3"],
       dtype="string",
   ).str.extract(r"([ab])(\d)", expand=False)

Elements that do not match return a row filled with ``NaN``. Thus, a
Series of messy strings can be "converted" into a like-indexed Series
or DataFrame of cleaned-up or more useful strings, without
necessitating ``get()`` to access tuples or ``re.match`` objects. The
dtype of the result is always object, even if no match is found and
the result only contains ``NaN``.

Named groups like

.. ipython:: python

   pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(
       r"(?P<letter>[ab])(?P<digit>\d)", expand=False
   )

and optional groups like

.. ipython:: python

   pd.Series(
       ["a1", "b2", "3"],
       dtype="string",
   ).str.extract(r"([ab])?(\d)", expand=False)

can also be used. Note that any capture group names in the regular
expression will be used for column names; otherwise capture group
numbers will be used.

Extracting a regular expression with one group returns a ``DataFrame``
with one column if ``expand=True``.

.. ipython:: python

   pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)", expand=True)

It returns a Series if ``expand=False``.

.. ipython:: python

   pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)", expand=False)

Calling on an ``Index`` with a regex with exactly one capture group
returns a ``DataFrame`` with one column if ``expand=True``.

.. ipython:: python

   s = pd.Series(["a1", "b2", "c3"], ["A11", "B22", "C33"], dtype="string")
   s
   s.index.str.extract("(?P<letter>[a-zA-Z])", expand=True)

It returns an ``Index`` if ``expand=False``.

.. ipython:: python

   s.index.str.extract("(?P<letter>[a-zA-Z])", expand=False)

Calling on an ``Index`` with a regex with more than one capture group
returns a ``DataFrame`` if ``expand=True``.

.. ipython:: python

   s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=True)

It raises ``ValueError`` if ``expand=False``.

.. code-block:: python

    >>> s.index.str.extract("(?P<letter>[a-zA-Z])([0-9]+)", expand=False)
    ValueError: only one regex group is supported with Index

The table below summarizes the behavior of ``extract(expand=False)``
(input subject in first column, number of groups in regex in
first row)

+--------+---------+------------+
|        | 1 group | >1 group   |
+--------+---------+------------+
| Index  | Index   | ValueError |
+--------+---------+------------+
| Series | Series  | DataFrame  |
+--------+---------+------------+

Extract all matches in each subject (extractall)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. _text.extractall:

Unlike ``extract`` (which returns only the first match),

.. ipython:: python

   s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"], dtype="string")
   s
   two_groups = "(?P<letter>[a-z])(?P<digit>[0-9])"
   s.str.extract(two_groups, expand=True)

the ``extractall`` method returns every match. The result of
``extractall`` is always a ``DataFrame`` with a ``MultiIndex`` on its
rows. The last level of the ``MultiIndex`` is named ``match`` and
indicates the order in the subject.

.. ipython:: python

   s.str.extractall(two_groups)

When each subject string in the Series has exactly one match,

.. ipython:: python

   s = pd.Series(["a3", "b3", "c2"], dtype="string")
   s

then ``extractall(pat).xs(0, level='match')`` gives the same result as
``extract(pat)``.

.. ipython:: python

   extract_result = s.str.extract(two_groups, expand=True)
   extract_result
   extractall_result = s.str.extractall(two_groups)
   extractall_result
   extractall_result.xs(0, level="match")

``Index`` also supports ``.str.extractall``. It returns a ``DataFrame`` which has the
same result as a ``Series.str.extractall`` with a default index (starts from 0).

.. ipython:: python

   pd.Index(["a1a2", "b1", "c1"]).str.extractall(two_groups)

   pd.Series(["a1a2", "b1", "c1"], dtype="string").str.extractall(two_groups)


Testing for strings that match or contain a pattern
---------------------------------------------------

You can check whether elements contain a pattern:

.. ipython:: python

   pattern = r"[0-9][a-z]"
   pd.Series(
       ["1", "2", "3a", "3b", "03c", "4dx"],
       dtype="string",
   ).str.contains(pattern)

Or whether elements match a pattern:

.. ipython:: python

   pd.Series(
       ["1", "2", "3a", "3b", "03c", "4dx"],
       dtype="string",
   ).str.match(pattern)

.. versionadded:: 1.1.0

.. ipython:: python

   pd.Series(
       ["1", "2", "3a", "3b", "03c", "4dx"],
       dtype="string",
   ).str.fullmatch(pattern)

.. note::

    The distinction between ``match``, ``fullmatch``, and ``contains`` is strictness:
    ``fullmatch`` tests whether the entire string matches the regular expression;
    ``match`` tests whether there is a match of the regular expression that begins
    at the first character of the string; and ``contains`` tests whether there is
    a match of the regular expression at any position within the string.

    The corresponding functions in the ``re`` package for these three match modes are
    `re.fullmatch <https://docs.python.org/3/library/re.html#re.fullmatch>`_,
    `re.match <https://docs.python.org/3/library/re.html#re.match>`_, and
    `re.search <https://docs.python.org/3/library/re.html#re.search>`_,
    respectively.

Methods like ``match``, ``fullmatch``, ``contains``, ``startswith``, and
``endswith`` take an extra ``na`` argument so missing values can be considered
True or False:

.. ipython:: python

   s4 = pd.Series(
       ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string"
   )
   s4.str.contains("A", na=False)

.. _text.indicator:

Creating indicator variables
----------------------------

You can extract dummy variables from string columns.
For example if they are separated by a ``'|'``:

.. ipython:: python

    s = pd.Series(["a", "a|b", np.nan, "a|c"], dtype="string")
    s.str.get_dummies(sep="|")

String ``Index`` also supports ``get_dummies`` which returns a ``MultiIndex``.

.. ipython:: python

    idx = pd.Index(["a", "a|b", np.nan, "a|c"])
    idx.str.get_dummies(sep="|")

See also :func:`~pandas.get_dummies`.

Method summary
--------------

.. _text.summary:

.. csv-table::
    :header: "Method", "Description"
    :widths: 20, 80
    :delim: ;

    :meth:`~Series.str.cat`;Concatenate strings
    :meth:`~Series.str.split`;Split strings on delimiter
    :meth:`~Series.str.rsplit`;Split strings on delimiter working from the end of the string
    :meth:`~Series.str.get`;Index into each element (retrieve i-th element)
    :meth:`~Series.str.join`;Join strings in each element of the Series with passed separator
    :meth:`~Series.str.get_dummies`;Split strings on the delimiter returning DataFrame of dummy variables
    :meth:`~Series.str.contains`;Return boolean array if each string contains pattern/regex
    :meth:`~Series.str.replace`;Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence
    :meth:`~Series.str.removeprefix`;Remove prefix from string, i.e. only remove if string starts with prefix.
    :meth:`~Series.str.removesuffix`;Remove suffix from string, i.e. only remove if string ends with suffix.
    :meth:`~Series.str.repeat`;Duplicate values (``s.str.repeat(3)`` equivalent to ``x * 3``)
    :meth:`~Series.str.pad`;"Add whitespace to left, right, or both sides of strings"
    :meth:`~Series.str.center`;Equivalent to ``str.center``
    :meth:`~Series.str.ljust`;Equivalent to ``str.ljust``
    :meth:`~Series.str.rjust`;Equivalent to ``str.rjust``
    :meth:`~Series.str.zfill`;Equivalent to ``str.zfill``
    :meth:`~Series.str.wrap`;Split long strings into lines with length less than a given width
    :meth:`~Series.str.slice`;Slice each string in the Series
    :meth:`~Series.str.slice_replace`;Replace slice in each string with passed value
    :meth:`~Series.str.count`;Count occurrences of pattern
    :meth:`~Series.str.startswith`;Equivalent to ``str.startswith(pat)`` for each element
    :meth:`~Series.str.endswith`;Equivalent to ``str.endswith(pat)`` for each element
    :meth:`~Series.str.findall`;Compute list of all occurrences of pattern/regex for each string
    :meth:`~Series.str.match`;"Call ``re.match`` on each element, returning matched groups as list"
    :meth:`~Series.str.extract`;"Call ``re.search`` on each element, returning DataFrame with one row for each element and one column for each regex capture group"
    :meth:`~Series.str.extractall`;"Call ``re.findall`` on each element, returning DataFrame with one row for each match and one column for each regex capture group"
    :meth:`~Series.str.len`;Compute string lengths
    :meth:`~Series.str.strip`;Equivalent to ``str.strip``
    :meth:`~Series.str.rstrip`;Equivalent to ``str.rstrip``
    :meth:`~Series.str.lstrip`;Equivalent to ``str.lstrip``
    :meth:`~Series.str.partition`;Equivalent to ``str.partition``
    :meth:`~Series.str.rpartition`;Equivalent to ``str.rpartition``
    :meth:`~Series.str.lower`;Equivalent to ``str.lower``
    :meth:`~Series.str.casefold`;Equivalent to ``str.casefold``
    :meth:`~Series.str.upper`;Equivalent to ``str.upper``
    :meth:`~Series.str.find`;Equivalent to ``str.find``
    :meth:`~Series.str.rfind`;Equivalent to ``str.rfind``
    :meth:`~Series.str.index`;Equivalent to ``str.index``
    :meth:`~Series.str.rindex`;Equivalent to ``str.rindex``
    :meth:`~Series.str.capitalize`;Equivalent to ``str.capitalize``
    :meth:`~Series.str.swapcase`;Equivalent to ``str.swapcase``
    :meth:`~Series.str.normalize`;Return Unicode normal form. Equivalent to ``unicodedata.normalize``
    :meth:`~Series.str.translate`;Equivalent to ``str.translate``
    :meth:`~Series.str.isalnum`;Equivalent to ``str.isalnum``
    :meth:`~Series.str.isalpha`;Equivalent to ``str.isalpha``
    :meth:`~Series.str.isdigit`;Equivalent to ``str.isdigit``
    :meth:`~Series.str.isspace`;Equivalent to ``str.isspace``
    :meth:`~Series.str.islower`;Equivalent to ``str.islower``
    :meth:`~Series.str.isupper`;Equivalent to ``str.isupper``
    :meth:`~Series.str.istitle`;Equivalent to ``str.istitle``
    :meth:`~Series.str.isnumeric`;Equivalent to ``str.isnumeric``
    :meth:`~Series.str.isdecimal`;Equivalent to ``str.isdecimal``
.. _10min:

{{ header }}

********************
10 minutes to pandas
********************

This is a short introduction to pandas, geared mainly for new users.
You can see more complex recipes in the :ref:`Cookbook<cookbook>`.

Customarily, we import as follows:

.. ipython:: python

   import numpy as np
   import pandas as pd

Object creation
---------------

See the :ref:`Intro to data structures section <dsintro>`.

Creating a :class:`Series` by passing a list of values, letting pandas create
a default integer index:

.. ipython:: python

   s = pd.Series([1, 3, 5, np.nan, 6, 8])
   s

Creating a :class:`DataFrame` by passing a NumPy array, with a datetime index using :func:`date_range`
and labeled columns:

.. ipython:: python

   dates = pd.date_range("20130101", periods=6)
   dates
   df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list("ABCD"))
   df

Creating a :class:`DataFrame` by passing a dictionary of objects that can be
converted into a series-like structure:

.. ipython:: python

   df2 = pd.DataFrame(
       {
           "A": 1.0,
           "B": pd.Timestamp("20130102"),
           "C": pd.Series(1, index=list(range(4)), dtype="float32"),
           "D": np.array([3] * 4, dtype="int32"),
           "E": pd.Categorical(["test", "train", "test", "train"]),
           "F": "foo",
       }
   )
   df2

The columns of the resulting :class:`DataFrame` have different
:ref:`dtypes <basics.dtypes>`:

.. ipython:: python

   df2.dtypes

If you're using IPython, tab completion for column names (as well as public
attributes) is automatically enabled. Here's a subset of the attributes that
will be completed:

.. ipython::

   @verbatim
   In [1]: df2.<TAB>  # noqa: E225, E999
   df2.A                  df2.bool
   df2.abs                df2.boxplot
   df2.add                df2.C
   df2.add_prefix         df2.clip
   df2.add_suffix         df2.columns
   df2.align              df2.copy
   df2.all                df2.count
   df2.any                df2.combine
   df2.append             df2.D
   df2.apply              df2.describe
   df2.applymap           df2.diff
   df2.B                  df2.duplicated

As you can see, the columns ``A``, ``B``, ``C``, and ``D`` are automatically
tab completed. ``E`` and ``F`` are there as well; the rest of the attributes have been
truncated for brevity.

Viewing data
------------

See the :ref:`Basics section <basics>`.

Use :meth:`DataFrame.head` and :meth:`DataFrame.tail` to view the top and bottom rows of the frame
respectively:

.. ipython:: python

   df.head()
   df.tail(3)

Display the :attr:`DataFrame.index` or :attr:`DataFrame.columns`:

.. ipython:: python

   df.index
   df.columns

:meth:`DataFrame.to_numpy` gives a NumPy representation of the underlying data.
Note that this can be an expensive operation when your :class:`DataFrame` has
columns with different data types, which comes down to a fundamental difference
between pandas and NumPy: **NumPy arrays have one dtype for the entire array,
while pandas DataFrames have one dtype per column**. When you call
:meth:`DataFrame.to_numpy`, pandas will find the NumPy dtype that can hold *all*
of the dtypes in the DataFrame. This may end up being ``object``, which requires
casting every value to a Python object.

For ``df``, our :class:`DataFrame` of all floating-point values, and
:meth:`DataFrame.to_numpy` is fast and doesn't require copying data:

.. ipython:: python

   df.to_numpy()

For ``df2``, the :class:`DataFrame` with multiple dtypes,
:meth:`DataFrame.to_numpy` is relatively expensive:

.. ipython:: python

   df2.to_numpy()

.. note::

   :meth:`DataFrame.to_numpy` does *not* include the index or column
   labels in the output.

:func:`~DataFrame.describe` shows a quick statistic summary of your data:

.. ipython:: python

   df.describe()

Transposing your data:

.. ipython:: python

   df.T

:meth:`DataFrame.sort_index` sorts by an axis:

.. ipython:: python

   df.sort_index(axis=1, ascending=False)

:meth:`DataFrame.sort_values` sorts by values:

.. ipython:: python

   df.sort_values(by="B")

Selection
---------

.. note::

   While standard Python / NumPy expressions for selecting and setting are
   intuitive and come in handy for interactive work, for production code, we
   recommend the optimized pandas data access methods, :meth:`DataFrame.at`, :meth:`DataFrame.iat`,
   :meth:`DataFrame.loc` and :meth:`DataFrame.iloc`.

See the indexing documentation :ref:`Indexing and Selecting Data <indexing>` and :ref:`MultiIndex / Advanced Indexing <advanced>`.

Getting
~~~~~~~

Selecting a single column, which yields a :class:`Series`,
equivalent to ``df.A``:

.. ipython:: python

   df["A"]

Selecting via ``[]`` (``__getitem__``), which slices the rows:

.. ipython:: python

   df[0:3]
   df["20130102":"20130104"]

Selection by label
~~~~~~~~~~~~~~~~~~

See more in :ref:`Selection by Label <indexing.label>` using :meth:`DataFrame.loc` or :meth:`DataFrame.at`.

For getting a cross section using a label:

.. ipython:: python

   df.loc[dates[0]]

Selecting on a multi-axis by label:

.. ipython:: python

   df.loc[:, ["A", "B"]]

Showing label slicing, both endpoints are *included*:

.. ipython:: python

   df.loc["20130102":"20130104", ["A", "B"]]

Reduction in the dimensions of the returned object:

.. ipython:: python

   df.loc["20130102", ["A", "B"]]

For getting a scalar value:

.. ipython:: python

   df.loc[dates[0], "A"]

For getting fast access to a scalar (equivalent to the prior method):

.. ipython:: python

   df.at[dates[0], "A"]

Selection by position
~~~~~~~~~~~~~~~~~~~~~

See more in :ref:`Selection by Position <indexing.integer>` using :meth:`DataFrame.iloc` or :meth:`DataFrame.at`.

Select via the position of the passed integers:

.. ipython:: python

   df.iloc[3]

By integer slices, acting similar to NumPy/Python:

.. ipython:: python

   df.iloc[3:5, 0:2]

By lists of integer position locations, similar to the NumPy/Python style:

.. ipython:: python

   df.iloc[[1, 2, 4], [0, 2]]

For slicing rows explicitly:

.. ipython:: python

   df.iloc[1:3, :]

For slicing columns explicitly:

.. ipython:: python

   df.iloc[:, 1:3]

For getting a value explicitly:

.. ipython:: python

   df.iloc[1, 1]

For getting fast access to a scalar (equivalent to the prior method):

.. ipython:: python

   df.iat[1, 1]

Boolean indexing
~~~~~~~~~~~~~~~~

Using a single column's values to select data:

.. ipython:: python

   df[df["A"] > 0]

Selecting values from a DataFrame where a boolean condition is met:

.. ipython:: python

   df[df > 0]

Using the :func:`~Series.isin` method for filtering:

.. ipython:: python

   df2 = df.copy()
   df2["E"] = ["one", "one", "two", "three", "four", "three"]
   df2
   df2[df2["E"].isin(["two", "four"])]

Setting
~~~~~~~

Setting a new column automatically aligns the data
by the indexes:

.. ipython:: python

   s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range("20130102", periods=6))
   s1
   df["F"] = s1

Setting values by label:

.. ipython:: python

   df.at[dates[0], "A"] = 0

Setting values by position:

.. ipython:: python

   df.iat[0, 1] = 0

Setting by assigning with a NumPy array:

.. ipython:: python

   df.loc[:, "D"] = np.array([5] * len(df))

The result of the prior setting operations:

.. ipython:: python

   df

A ``where`` operation with setting:

.. ipython:: python

   df2 = df.copy()
   df2[df2 > 0] = -df2
   df2


Missing data
------------

pandas primarily uses the value ``np.nan`` to represent missing data. It is by
default not included in computations. See the :ref:`Missing Data section
<missing_data>`.

Reindexing allows you to change/add/delete the index on a specified axis. This
returns a copy of the data:

.. ipython:: python

   df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ["E"])
   df1.loc[dates[0] : dates[1], "E"] = 1
   df1

:meth:`DataFrame.dropna` drops any rows that have missing data:

.. ipython:: python

   df1.dropna(how="any")

:meth:`DataFrame.fillna` fills missing data:

.. ipython:: python

   df1.fillna(value=5)

:func:`isna` gets the boolean mask where values are ``nan``:

.. ipython:: python

   pd.isna(df1)


Operations
----------

See the :ref:`Basic section on Binary Ops <basics.binop>`.

Stats
~~~~~

Operations in general *exclude* missing data.

Performing a descriptive statistic:

.. ipython:: python

   df.mean()

Same operation on the other axis:

.. ipython:: python

   df.mean(1)

Operating with objects that have different dimensionality and need alignment.
In addition, pandas automatically broadcasts along the specified dimension:

.. ipython:: python

   s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)
   s
   df.sub(s, axis="index")


Apply
~~~~~

:meth:`DataFrame.apply` applies a user defined function to the data:

.. ipython:: python

   df.apply(np.cumsum)
   df.apply(lambda x: x.max() - x.min())

Histogramming
~~~~~~~~~~~~~

See more at :ref:`Histogramming and Discretization <basics.discretization>`.

.. ipython:: python

   s = pd.Series(np.random.randint(0, 7, size=10))
   s
   s.value_counts()

String Methods
~~~~~~~~~~~~~~

Series is equipped with a set of string processing methods in the ``str``
attribute that make it easy to operate on each element of the array, as in the
code snippet below. Note that pattern-matching in ``str`` generally uses `regular
expressions <https://docs.python.org/3/library/re.html>`__ by default (and in
some cases always uses them). See more at :ref:`Vectorized String Methods
<text.string_methods>`.

.. ipython:: python

   s = pd.Series(["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"])
   s.str.lower()

Merge
-----

Concat
~~~~~~

pandas provides various facilities for easily combining together Series and
DataFrame objects with various kinds of set logic for the indexes
and relational algebra functionality in the case of join / merge-type
operations.

See the :ref:`Merging section <merging>`.

Concatenating pandas objects together along an axis with :func:`concat`:

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 4))
   df

   # break it into pieces
   pieces = [df[:3], df[3:7], df[7:]]

   pd.concat(pieces)

.. note::
   Adding a column to a :class:`DataFrame` is relatively fast. However, adding
   a row requires a copy, and may be expensive. We recommend passing a
   pre-built list of records to the :class:`DataFrame` constructor instead
   of building a :class:`DataFrame` by iteratively appending records to it.

Join
~~~~

:func:`merge` enables SQL style join types along specific columns. See the :ref:`Database style joining <merging.join>` section.

.. ipython:: python

   left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
   right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
   left
   right
   pd.merge(left, right, on="key")

Another example that can be given is:

.. ipython:: python

   left = pd.DataFrame({"key": ["foo", "bar"], "lval": [1, 2]})
   right = pd.DataFrame({"key": ["foo", "bar"], "rval": [4, 5]})
   left
   right
   pd.merge(left, right, on="key")

Grouping
--------

By "group by" we are referring to a process involving one or more of the
following steps:

 - **Splitting** the data into groups based on some criteria
 - **Applying** a function to each group independently
 - **Combining** the results into a data structure

See the :ref:`Grouping section <groupby>`.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
           "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
           "C": np.random.randn(8),
           "D": np.random.randn(8),
       }
   )
   df

Grouping and then applying the :meth:`~pandas.core.groupby.GroupBy.sum` function to the resulting
groups:

.. ipython:: python

   df.groupby("A").sum()

Grouping by multiple columns forms a hierarchical index, and again we can
apply the :meth:`~pandas.core.groupby.GroupBy.sum` function:

.. ipython:: python

   df.groupby(["A", "B"]).sum()

Reshaping
---------

See the sections on :ref:`Hierarchical Indexing <advanced.hierarchical>` and
:ref:`Reshaping <reshaping.stacking>`.

Stack
~~~~~

.. ipython:: python

   tuples = list(
       zip(
           ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
           ["one", "two", "one", "two", "one", "two", "one", "two"],
       )
   )
   index = pd.MultiIndex.from_tuples(tuples, names=["first", "second"])
   df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=["A", "B"])
   df2 = df[:4]
   df2

The :meth:`~DataFrame.stack` method "compresses" a level in the DataFrame's
columns:

.. ipython:: python

   stacked = df2.stack()
   stacked

With a "stacked" DataFrame or Series (having a :class:`MultiIndex` as the
``index``), the inverse operation of :meth:`~DataFrame.stack` is
:meth:`~DataFrame.unstack`, which by default unstacks the **last level**:

.. ipython:: python

   stacked.unstack()
   stacked.unstack(1)
   stacked.unstack(0)

Pivot tables
~~~~~~~~~~~~
See the section on :ref:`Pivot Tables <reshaping.pivot>`.

.. ipython:: python

   df = pd.DataFrame(
       {
           "A": ["one", "one", "two", "three"] * 3,
           "B": ["A", "B", "C"] * 4,
           "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,
           "D": np.random.randn(12),
           "E": np.random.randn(12),
       }
   )
   df

:func:`pivot_table` pivots a :class:`DataFrame` specifying the ``values``, ``index`` and ``columns``

.. ipython:: python

   pd.pivot_table(df, values="D", index=["A", "B"], columns=["C"])


Time series
-----------

pandas has simple, powerful, and efficient functionality for performing
resampling operations during frequency conversion (e.g., converting secondly
data into 5-minutely data). This is extremely common in, but not limited to,
financial applications. See the :ref:`Time Series section <timeseries>`.

.. ipython:: python

   rng = pd.date_range("1/1/2012", periods=100, freq="S")
   ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)
   ts.resample("5Min").sum()

:meth:`Series.tz_localize` localizes a time series to a time zone:

.. ipython:: python

   rng = pd.date_range("3/6/2012 00:00", periods=5, freq="D")
   ts = pd.Series(np.random.randn(len(rng)), rng)
   ts
   ts_utc = ts.tz_localize("UTC")
   ts_utc

:meth:`Series.tz_convert` converts a timezones aware time series to another time zone:

.. ipython:: python

   ts_utc.tz_convert("US/Eastern")

Converting between time span representations:

.. ipython:: python

   rng = pd.date_range("1/1/2012", periods=5, freq="M")
   ts = pd.Series(np.random.randn(len(rng)), index=rng)
   ts
   ps = ts.to_period()
   ps
   ps.to_timestamp()

Converting between period and timestamp enables some convenient arithmetic
functions to be used. In the following example, we convert a quarterly
frequency with year ending in November to 9am of the end of the month following
the quarter end:

.. ipython:: python

   prng = pd.period_range("1990Q1", "2000Q4", freq="Q-NOV")
   ts = pd.Series(np.random.randn(len(prng)), prng)
   ts.index = (prng.asfreq("M", "e") + 1).asfreq("H", "s") + 9
   ts.head()

Categoricals
------------

pandas can include categorical data in a :class:`DataFrame`. For full docs, see the
:ref:`categorical introduction <categorical>` and the :ref:`API documentation <api.arrays.categorical>`.

.. ipython:: python

    df = pd.DataFrame(
        {"id": [1, 2, 3, 4, 5, 6], "raw_grade": ["a", "b", "b", "a", "a", "e"]}
    )



Converting the raw grades to a categorical data type:

.. ipython:: python

    df["grade"] = df["raw_grade"].astype("category")
    df["grade"]

Rename the categories to more meaningful names (assigning to
:meth:`Series.cat.categories` is in place!):

.. ipython:: python

    df["grade"].cat.categories = ["very good", "good", "very bad"]

Reorder the categories and simultaneously add the missing categories (methods under :meth:`Series.cat` return a new :class:`Series` by default):

.. ipython:: python

    df["grade"] = df["grade"].cat.set_categories(
        ["very bad", "bad", "medium", "good", "very good"]
    )
    df["grade"]

Sorting is per order in the categories, not lexical order:

.. ipython:: python

    df.sort_values(by="grade")

Grouping by a categorical column also shows empty categories:

.. ipython:: python

    df.groupby("grade").size()


Plotting
--------

See the :ref:`Plotting <visualization>` docs.

We use the standard convention for referencing the matplotlib API:

.. ipython:: python

   import matplotlib.pyplot as plt

   plt.close("all")

The ``plt.close`` method is used to `close <https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.close.html>`__ a figure window:

.. ipython:: python

   ts = pd.Series(np.random.randn(1000), index=pd.date_range("1/1/2000", periods=1000))
   ts = ts.cumsum()

   @savefig series_plot_basic.png
   ts.plot();

If running under Jupyter Notebook, the plot will appear on :meth:`~Series.plot`.  Otherwise use
`matplotlib.pyplot.show <https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.show.html>`__ to show it or
`matplotlib.pyplot.savefig <https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html>`__ to write it to a file.

.. ipython:: python

   plt.show();

On a DataFrame, the :meth:`~DataFrame.plot` method is a convenience to plot all
of the columns with labels:

.. ipython:: python

   df = pd.DataFrame(
       np.random.randn(1000, 4), index=ts.index, columns=["A", "B", "C", "D"]
   )

   df = df.cumsum()

   plt.figure();
   df.plot();
   @savefig frame_plot_basic.png
   plt.legend(loc='best');

Importing and exporting data
----------------------------

CSV
~~~

:ref:`Writing to a csv file: <io.store_in_csv>` using :meth:`DataFrame.to_csv`

.. ipython:: python

   df.to_csv("foo.csv")

:ref:`Reading from a csv file: <io.read_csv_table>` using :func:`read_csv`

.. ipython:: python

   pd.read_csv("foo.csv")

.. ipython:: python
   :suppress:

   import os

   os.remove("foo.csv")

HDF5
~~~~

Reading and writing to :ref:`HDFStores <io.hdf5>`.

Writing to a HDF5 Store using :meth:`DataFrame.to_hdf`:

.. ipython:: python

   df.to_hdf("foo.h5", "df")

Reading from a HDF5 Store using :func:`read_hdf`:

.. ipython:: python

   pd.read_hdf("foo.h5", "df")

.. ipython:: python
   :suppress:

   os.remove("foo.h5")

Excel
~~~~~

Reading and writing to :ref:`Excel <io.excel>`.

Writing to an excel file using :meth:`DataFrame.to_excel`:

.. ipython:: python

   df.to_excel("foo.xlsx", sheet_name="Sheet1")

Reading from an excel file using :func:`read_excel`:

.. ipython:: python

   pd.read_excel("foo.xlsx", "Sheet1", index_col=None, na_values=["NA"])

.. ipython:: python
   :suppress:

   os.remove("foo.xlsx")

Gotchas
-------

If you are attempting to perform a boolean operation on a :class:`Series` or :class:`DataFrame`
you might see an exception like:

.. ipython:: python
   :okexcept:

    if pd.Series([False, True, False]):
        print("I was true")

See :ref:`Comparisons<basics.compare>` and :ref:`Gotchas<gotchas>` for an explanation and what to do.
{{ header }}

.. _api.resampling:

==========
Resampling
==========
.. currentmodule:: pandas.core.resample

Resampler objects are returned by resample calls: :func:`pandas.DataFrame.resample`, :func:`pandas.Series.resample`.

Indexing, iteration
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Resampler.__iter__
   Resampler.groups
   Resampler.indices
   Resampler.get_group

Function application
~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Resampler.apply
   Resampler.aggregate
   Resampler.transform
   Resampler.pipe

Upsampling
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Resampler.ffill
   Resampler.backfill
   Resampler.bfill
   Resampler.pad
   Resampler.nearest
   Resampler.fillna
   Resampler.asfreq
   Resampler.interpolate

Computations / descriptive stats
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Resampler.count
   Resampler.nunique
   Resampler.first
   Resampler.last
   Resampler.max
   Resampler.mean
   Resampler.median
   Resampler.min
   Resampler.ohlc
   Resampler.prod
   Resampler.size
   Resampler.sem
   Resampler.std
   Resampler.sum
   Resampler.var
   Resampler.quantile
{{ header }}

.. _api.style:

=====
Style
=====
.. currentmodule:: pandas.io.formats.style

``Styler`` objects are returned by :attr:`pandas.DataFrame.style`.

Styler constructor
------------------
.. autosummary::
   :toctree: api/

   Styler
   Styler.from_custom_template

Styler properties
-----------------
.. autosummary::
   :toctree: api/

   Styler.env
   Styler.template_html
   Styler.template_html_style
   Styler.template_html_table
   Styler.template_latex
   Styler.template_string
   Styler.loader

Style application
-----------------
.. autosummary::
   :toctree: api/

   Styler.apply
   Styler.applymap
   Styler.apply_index
   Styler.applymap_index
   Styler.format
   Styler.format_index
   Styler.hide
   Styler.set_td_classes
   Styler.set_table_styles
   Styler.set_table_attributes
   Styler.set_tooltips
   Styler.set_caption
   Styler.set_sticky
   Styler.set_properties
   Styler.set_uuid
   Styler.clear
   Styler.pipe

Builtin styles
--------------
.. autosummary::
   :toctree: api/

   Styler.highlight_null
   Styler.highlight_max
   Styler.highlight_min
   Styler.highlight_between
   Styler.highlight_quantile
   Styler.background_gradient
   Styler.text_gradient
   Styler.bar

Style export and import
-----------------------
.. autosummary::
   :toctree: api/

   Styler.to_html
   Styler.to_latex
   Styler.to_excel
   Styler.to_string
   Styler.export
   Styler.use
{{ header }}

.. _api.indexing:

=============
Index objects
=============

Index
-----
.. currentmodule:: pandas

**Many of these methods or variants thereof are available on the objects
that contain an index (Series/DataFrame) and those should most likely be
used before calling these methods directly.**

.. autosummary::
   :toctree: api/

   Index

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.values
   Index.is_monotonic
   Index.is_monotonic_increasing
   Index.is_monotonic_decreasing
   Index.is_unique
   Index.has_duplicates
   Index.hasnans
   Index.dtype
   Index.inferred_type
   Index.is_all_dates
   Index.shape
   Index.name
   Index.names
   Index.nbytes
   Index.ndim
   Index.size
   Index.empty
   Index.T
   Index.memory_usage

Modifying and computations
~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.all
   Index.any
   Index.argmin
   Index.argmax
   Index.copy
   Index.delete
   Index.drop
   Index.drop_duplicates
   Index.duplicated
   Index.equals
   Index.factorize
   Index.identical
   Index.insert
   Index.is_
   Index.is_boolean
   Index.is_categorical
   Index.is_floating
   Index.is_integer
   Index.is_interval
   Index.is_mixed
   Index.is_numeric
   Index.is_object
   Index.min
   Index.max
   Index.reindex
   Index.rename
   Index.repeat
   Index.where
   Index.take
   Index.putmask
   Index.unique
   Index.nunique
   Index.value_counts

Compatibility with MultiIndex
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.set_names
   Index.droplevel

Missing values
~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.fillna
   Index.dropna
   Index.isna
   Index.notna

Conversion
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.astype
   Index.item
   Index.map
   Index.ravel
   Index.to_list
   Index.to_native_types
   Index.to_series
   Index.to_frame
   Index.view

Sorting
~~~~~~~
.. autosummary::
   :toctree: api/

   Index.argsort
   Index.searchsorted
   Index.sort_values

Time-specific operations
~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.shift

Combining / joining / set operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.append
   Index.join
   Index.intersection
   Index.union
   Index.difference
   Index.symmetric_difference

Selecting
~~~~~~~~~
.. autosummary::
   :toctree: api/

   Index.asof
   Index.asof_locs
   Index.get_indexer
   Index.get_indexer_for
   Index.get_indexer_non_unique
   Index.get_level_values
   Index.get_loc
   Index.get_slice_bound
   Index.get_value
   Index.isin
   Index.slice_indexer
   Index.slice_locs

.. _api.numericindex:

Numeric Index
-------------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   RangeIndex
   Int64Index
   UInt64Index
   Float64Index

.. We need this autosummary so that the methods are generated.
.. Separate block, since they aren't classes.

.. autosummary::
   :toctree: api/

   RangeIndex.start
   RangeIndex.stop
   RangeIndex.step
   RangeIndex.from_range

.. _api.categoricalindex:

CategoricalIndex
----------------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   CategoricalIndex

Categorical components
~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   CategoricalIndex.codes
   CategoricalIndex.categories
   CategoricalIndex.ordered
   CategoricalIndex.rename_categories
   CategoricalIndex.reorder_categories
   CategoricalIndex.add_categories
   CategoricalIndex.remove_categories
   CategoricalIndex.remove_unused_categories
   CategoricalIndex.set_categories
   CategoricalIndex.as_ordered
   CategoricalIndex.as_unordered

Modifying and computations
~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   CategoricalIndex.map
   CategoricalIndex.equals

.. _api.intervalindex:

IntervalIndex
-------------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   IntervalIndex

IntervalIndex components
~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   IntervalIndex.from_arrays
   IntervalIndex.from_tuples
   IntervalIndex.from_breaks
   IntervalIndex.left
   IntervalIndex.right
   IntervalIndex.mid
   IntervalIndex.closed
   IntervalIndex.length
   IntervalIndex.values
   IntervalIndex.is_empty
   IntervalIndex.is_non_overlapping_monotonic
   IntervalIndex.is_overlapping
   IntervalIndex.get_loc
   IntervalIndex.get_indexer
   IntervalIndex.set_closed
   IntervalIndex.contains
   IntervalIndex.overlaps
   IntervalIndex.to_tuples

.. _api.multiindex:

MultiIndex
----------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   MultiIndex

.. autosummary::
   :toctree: api/

   IndexSlice

MultiIndex constructors
~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   MultiIndex.from_arrays
   MultiIndex.from_tuples
   MultiIndex.from_product
   MultiIndex.from_frame

MultiIndex properties
~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   MultiIndex.names
   MultiIndex.levels
   MultiIndex.codes
   MultiIndex.nlevels
   MultiIndex.levshape
   MultiIndex.dtypes

MultiIndex components
~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   MultiIndex.set_levels
   MultiIndex.set_codes
   MultiIndex.to_flat_index
   MultiIndex.to_frame
   MultiIndex.sortlevel
   MultiIndex.droplevel
   MultiIndex.swaplevel
   MultiIndex.reorder_levels
   MultiIndex.remove_unused_levels

MultiIndex selecting
~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   MultiIndex.get_loc
   MultiIndex.get_locs
   MultiIndex.get_loc_level
   MultiIndex.get_indexer
   MultiIndex.get_level_values

.. _api.datetimeindex:

DatetimeIndex
-------------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   DatetimeIndex

Time/date components
~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DatetimeIndex.year
   DatetimeIndex.month
   DatetimeIndex.day
   DatetimeIndex.hour
   DatetimeIndex.minute
   DatetimeIndex.second
   DatetimeIndex.microsecond
   DatetimeIndex.nanosecond
   DatetimeIndex.date
   DatetimeIndex.time
   DatetimeIndex.timetz
   DatetimeIndex.dayofyear
   DatetimeIndex.day_of_year
   DatetimeIndex.weekofyear
   DatetimeIndex.week
   DatetimeIndex.dayofweek
   DatetimeIndex.day_of_week
   DatetimeIndex.weekday
   DatetimeIndex.quarter
   DatetimeIndex.tz
   DatetimeIndex.freq
   DatetimeIndex.freqstr
   DatetimeIndex.is_month_start
   DatetimeIndex.is_month_end
   DatetimeIndex.is_quarter_start
   DatetimeIndex.is_quarter_end
   DatetimeIndex.is_year_start
   DatetimeIndex.is_year_end
   DatetimeIndex.is_leap_year
   DatetimeIndex.inferred_freq

Selecting
~~~~~~~~~
.. autosummary::
   :toctree: api/

   DatetimeIndex.indexer_at_time
   DatetimeIndex.indexer_between_time


Time-specific operations
~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DatetimeIndex.normalize
   DatetimeIndex.strftime
   DatetimeIndex.snap
   DatetimeIndex.tz_convert
   DatetimeIndex.tz_localize
   DatetimeIndex.round
   DatetimeIndex.floor
   DatetimeIndex.ceil
   DatetimeIndex.month_name
   DatetimeIndex.day_name

Conversion
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DatetimeIndex.to_period
   DatetimeIndex.to_perioddelta
   DatetimeIndex.to_pydatetime
   DatetimeIndex.to_series
   DatetimeIndex.to_frame

Methods
~~~~~~~
.. autosummary::
    :toctree: api/

    DatetimeIndex.mean
    DatetimeIndex.std

TimedeltaIndex
--------------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   TimedeltaIndex

Components
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   TimedeltaIndex.days
   TimedeltaIndex.seconds
   TimedeltaIndex.microseconds
   TimedeltaIndex.nanoseconds
   TimedeltaIndex.components
   TimedeltaIndex.inferred_freq

Conversion
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   TimedeltaIndex.to_pytimedelta
   TimedeltaIndex.to_series
   TimedeltaIndex.round
   TimedeltaIndex.floor
   TimedeltaIndex.ceil
   TimedeltaIndex.to_frame

Methods
~~~~~~~
.. autosummary::
    :toctree: api/

    TimedeltaIndex.mean

.. currentmodule:: pandas

PeriodIndex
-----------
.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   PeriodIndex

Properties
~~~~~~~~~~
.. autosummary::
    :toctree: api/

    PeriodIndex.day
    PeriodIndex.dayofweek
    PeriodIndex.day_of_week
    PeriodIndex.dayofyear
    PeriodIndex.day_of_year
    PeriodIndex.days_in_month
    PeriodIndex.daysinmonth
    PeriodIndex.end_time
    PeriodIndex.freq
    PeriodIndex.freqstr
    PeriodIndex.hour
    PeriodIndex.is_leap_year
    PeriodIndex.minute
    PeriodIndex.month
    PeriodIndex.quarter
    PeriodIndex.qyear
    PeriodIndex.second
    PeriodIndex.start_time
    PeriodIndex.week
    PeriodIndex.weekday
    PeriodIndex.weekofyear
    PeriodIndex.year

Methods
~~~~~~~
.. autosummary::
    :toctree: api/

    PeriodIndex.asfreq
    PeriodIndex.strftime
    PeriodIndex.to_timestamp
{{ header }}

.. _api.window:

======
Window
======

Rolling objects are returned by ``.rolling`` calls: :func:`pandas.DataFrame.rolling`, :func:`pandas.Series.rolling`, etc.
Expanding objects are returned by ``.expanding`` calls: :func:`pandas.DataFrame.expanding`, :func:`pandas.Series.expanding`, etc.
ExponentialMovingWindow objects are returned by ``.ewm`` calls: :func:`pandas.DataFrame.ewm`, :func:`pandas.Series.ewm`, etc.

.. _api.functions_rolling:

Rolling window functions
------------------------
.. currentmodule:: pandas.core.window.rolling

.. autosummary::
   :toctree: api/

   Rolling.count
   Rolling.sum
   Rolling.mean
   Rolling.median
   Rolling.var
   Rolling.std
   Rolling.min
   Rolling.max
   Rolling.corr
   Rolling.cov
   Rolling.skew
   Rolling.kurt
   Rolling.apply
   Rolling.aggregate
   Rolling.quantile
   Rolling.sem
   Rolling.rank

.. _api.functions_window:

Weighted window functions
-------------------------
.. currentmodule:: pandas.core.window.rolling

.. autosummary::
   :toctree: api/

   Window.mean
   Window.sum
   Window.var
   Window.std

.. _api.functions_expanding:

Expanding window functions
--------------------------
.. currentmodule:: pandas.core.window.expanding

.. autosummary::
   :toctree: api/

   Expanding.count
   Expanding.sum
   Expanding.mean
   Expanding.median
   Expanding.var
   Expanding.std
   Expanding.min
   Expanding.max
   Expanding.corr
   Expanding.cov
   Expanding.skew
   Expanding.kurt
   Expanding.apply
   Expanding.aggregate
   Expanding.quantile
   Expanding.sem
   Expanding.rank

.. _api.functions_ewm:

Exponentially-weighted window functions
---------------------------------------
.. currentmodule:: pandas.core.window.ewm

.. autosummary::
   :toctree: api/

   ExponentialMovingWindow.mean
   ExponentialMovingWindow.sum
   ExponentialMovingWindow.std
   ExponentialMovingWindow.var
   ExponentialMovingWindow.corr
   ExponentialMovingWindow.cov

.. _api.indexers_window:

Window indexer
--------------
.. currentmodule:: pandas

Base class for defining custom window boundaries.

.. autosummary::
   :toctree: api/

   api.indexers.BaseIndexer
   api.indexers.FixedForwardWindowIndexer
   api.indexers.VariableOffsetWindowIndexer
{{ header }}

.. _api.series:

======
Series
======
.. currentmodule:: pandas

Constructor
-----------
.. autosummary::
   :toctree: api/

   Series

Attributes
----------
**Axes**

.. autosummary::
   :toctree: api/

   Series.index
   Series.array
   Series.values
   Series.dtype
   Series.shape
   Series.nbytes
   Series.ndim
   Series.size
   Series.T
   Series.memory_usage
   Series.hasnans
   Series.empty
   Series.dtypes
   Series.name
   Series.flags
   Series.set_flags

Conversion
----------
.. autosummary::
   :toctree: api/

   Series.astype
   Series.convert_dtypes
   Series.infer_objects
   Series.copy
   Series.bool
   Series.to_numpy
   Series.to_period
   Series.to_timestamp
   Series.to_list
   Series.__array__

Indexing, iteration
-------------------
.. autosummary::
   :toctree: api/

   Series.get
   Series.at
   Series.iat
   Series.loc
   Series.iloc
   Series.__iter__
   Series.items
   Series.iteritems
   Series.keys
   Series.pop
   Series.item
   Series.xs

For more information on ``.at``, ``.iat``, ``.loc``, and
``.iloc``,  see the :ref:`indexing documentation <indexing>`.

Binary operator functions
-------------------------
.. autosummary::
   :toctree: api/

   Series.add
   Series.sub
   Series.mul
   Series.div
   Series.truediv
   Series.floordiv
   Series.mod
   Series.pow
   Series.radd
   Series.rsub
   Series.rmul
   Series.rdiv
   Series.rtruediv
   Series.rfloordiv
   Series.rmod
   Series.rpow
   Series.combine
   Series.combine_first
   Series.round
   Series.lt
   Series.gt
   Series.le
   Series.ge
   Series.ne
   Series.eq
   Series.product
   Series.dot

Function application, GroupBy & window
--------------------------------------
.. autosummary::
   :toctree: api/

   Series.apply
   Series.agg
   Series.aggregate
   Series.transform
   Series.map
   Series.groupby
   Series.rolling
   Series.expanding
   Series.ewm
   Series.pipe

.. _api.series.stats:

Computations / descriptive stats
--------------------------------
.. autosummary::
   :toctree: api/

   Series.abs
   Series.all
   Series.any
   Series.autocorr
   Series.between
   Series.clip
   Series.corr
   Series.count
   Series.cov
   Series.cummax
   Series.cummin
   Series.cumprod
   Series.cumsum
   Series.describe
   Series.diff
   Series.factorize
   Series.kurt
   Series.mad
   Series.max
   Series.mean
   Series.median
   Series.min
   Series.mode
   Series.nlargest
   Series.nsmallest
   Series.pct_change
   Series.prod
   Series.quantile
   Series.rank
   Series.sem
   Series.skew
   Series.std
   Series.sum
   Series.var
   Series.kurtosis
   Series.unique
   Series.nunique
   Series.is_unique
   Series.is_monotonic
   Series.is_monotonic_increasing
   Series.is_monotonic_decreasing
   Series.value_counts

Reindexing / selection / label manipulation
-------------------------------------------
.. autosummary::
   :toctree: api/

   Series.align
   Series.drop
   Series.droplevel
   Series.drop_duplicates
   Series.duplicated
   Series.equals
   Series.first
   Series.head
   Series.idxmax
   Series.idxmin
   Series.isin
   Series.last
   Series.reindex
   Series.reindex_like
   Series.rename
   Series.rename_axis
   Series.reset_index
   Series.sample
   Series.set_axis
   Series.take
   Series.tail
   Series.truncate
   Series.where
   Series.mask
   Series.add_prefix
   Series.add_suffix
   Series.filter

Missing data handling
---------------------
.. autosummary::
   :toctree: api/

   Series.backfill
   Series.bfill
   Series.dropna
   Series.ffill
   Series.fillna
   Series.interpolate
   Series.isna
   Series.isnull
   Series.notna
   Series.notnull
   Series.pad
   Series.replace

Reshaping, sorting
------------------
.. autosummary::
   :toctree: api/

   Series.argsort
   Series.argmin
   Series.argmax
   Series.reorder_levels
   Series.sort_values
   Series.sort_index
   Series.swaplevel
   Series.unstack
   Series.explode
   Series.searchsorted
   Series.ravel
   Series.repeat
   Series.squeeze
   Series.view

Combining / comparing / joining / merging
-----------------------------------------
.. autosummary::
   :toctree: api/

   Series.append
   Series.compare
   Series.update

Time Series-related
-------------------
.. autosummary::
   :toctree: api/

   Series.asfreq
   Series.asof
   Series.shift
   Series.first_valid_index
   Series.last_valid_index
   Series.resample
   Series.tz_convert
   Series.tz_localize
   Series.at_time
   Series.between_time
   Series.tshift
   Series.slice_shift

Accessors
---------

pandas provides dtype-specific methods under various accessors.
These are separate namespaces within :class:`Series` that only apply
to specific data types.

=========================== =================================
Data Type                   Accessor
=========================== =================================
Datetime, Timedelta, Period :ref:`dt <api.series.dt>`
String                      :ref:`str <api.series.str>`
Categorical                 :ref:`cat <api.series.cat>`
Sparse                      :ref:`sparse <api.series.sparse>`
=========================== =================================

.. _api.series.dt:

Datetimelike properties
~~~~~~~~~~~~~~~~~~~~~~~

``Series.dt`` can be used to access the values of the series as
datetimelike and return several properties.
These can be accessed like ``Series.dt.<property>``.

Datetime properties
^^^^^^^^^^^^^^^^^^^

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   Series.dt.date
   Series.dt.time
   Series.dt.timetz
   Series.dt.year
   Series.dt.month
   Series.dt.day
   Series.dt.hour
   Series.dt.minute
   Series.dt.second
   Series.dt.microsecond
   Series.dt.nanosecond
   Series.dt.week
   Series.dt.weekofyear
   Series.dt.dayofweek
   Series.dt.day_of_week
   Series.dt.weekday
   Series.dt.dayofyear
   Series.dt.day_of_year
   Series.dt.quarter
   Series.dt.is_month_start
   Series.dt.is_month_end
   Series.dt.is_quarter_start
   Series.dt.is_quarter_end
   Series.dt.is_year_start
   Series.dt.is_year_end
   Series.dt.is_leap_year
   Series.dt.daysinmonth
   Series.dt.days_in_month
   Series.dt.tz
   Series.dt.freq

Datetime methods
^^^^^^^^^^^^^^^^

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.dt.to_period
   Series.dt.to_pydatetime
   Series.dt.tz_localize
   Series.dt.tz_convert
   Series.dt.normalize
   Series.dt.strftime
   Series.dt.round
   Series.dt.floor
   Series.dt.ceil
   Series.dt.month_name
   Series.dt.day_name

Period properties
^^^^^^^^^^^^^^^^^

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   Series.dt.qyear
   Series.dt.start_time
   Series.dt.end_time

Timedelta properties
^^^^^^^^^^^^^^^^^^^^

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   Series.dt.days
   Series.dt.seconds
   Series.dt.microseconds
   Series.dt.nanoseconds
   Series.dt.components

Timedelta methods
^^^^^^^^^^^^^^^^^

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.dt.to_pytimedelta
   Series.dt.total_seconds


.. _api.series.str:

String handling
~~~~~~~~~~~~~~~

``Series.str`` can be used to access the values of the series as
strings and apply several methods to it. These can be accessed like
``Series.str.<function/property>``.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.str.capitalize
   Series.str.casefold
   Series.str.cat
   Series.str.center
   Series.str.contains
   Series.str.count
   Series.str.decode
   Series.str.encode
   Series.str.endswith
   Series.str.extract
   Series.str.extractall
   Series.str.find
   Series.str.findall
   Series.str.fullmatch
   Series.str.get
   Series.str.index
   Series.str.join
   Series.str.len
   Series.str.ljust
   Series.str.lower
   Series.str.lstrip
   Series.str.match
   Series.str.normalize
   Series.str.pad
   Series.str.partition
   Series.str.removeprefix
   Series.str.removesuffix
   Series.str.repeat
   Series.str.replace
   Series.str.rfind
   Series.str.rindex
   Series.str.rjust
   Series.str.rpartition
   Series.str.rstrip
   Series.str.slice
   Series.str.slice_replace
   Series.str.split
   Series.str.rsplit
   Series.str.startswith
   Series.str.strip
   Series.str.swapcase
   Series.str.title
   Series.str.translate
   Series.str.upper
   Series.str.wrap
   Series.str.zfill
   Series.str.isalnum
   Series.str.isalpha
   Series.str.isdigit
   Series.str.isspace
   Series.str.islower
   Series.str.isupper
   Series.str.istitle
   Series.str.isnumeric
   Series.str.isdecimal
   Series.str.get_dummies

..
    The following is needed to ensure the generated pages are created with the
    correct template (otherwise they would be created in the Series/Index class page)

..
    .. autosummary::
       :toctree: api/
       :template: autosummary/accessor.rst

       Series.str
       Series.cat
       Series.dt
       Series.sparse
       DataFrame.sparse
       Index.str

.. _api.series.cat:

Categorical accessor
~~~~~~~~~~~~~~~~~~~~

Categorical-dtype specific methods and attributes are available under
the ``Series.cat`` accessor.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   Series.cat.categories
   Series.cat.ordered
   Series.cat.codes

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.cat.rename_categories
   Series.cat.reorder_categories
   Series.cat.add_categories
   Series.cat.remove_categories
   Series.cat.remove_unused_categories
   Series.cat.set_categories
   Series.cat.as_ordered
   Series.cat.as_unordered


.. _api.series.sparse:

Sparse accessor
~~~~~~~~~~~~~~~

Sparse-dtype specific methods and attributes are provided under the
``Series.sparse`` accessor.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   Series.sparse.npoints
   Series.sparse.density
   Series.sparse.fill_value
   Series.sparse.sp_values

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.sparse.from_coo
   Series.sparse.to_coo

.. _api.series.flags:

Flags
~~~~~

Flags refer to attributes of the pandas object. Properties of the dataset (like
the date is was recorded, the URL it was accessed from, etc.) should be stored
in :attr:`Series.attrs`.

.. autosummary::
   :toctree: api/

   Flags

.. _api.series.metadata:

Metadata
~~~~~~~~

:attr:`Series.attrs` is a dictionary for storing global metadata for this Series.

.. warning:: ``Series.attrs`` is considered experimental and may change without warning.

.. autosummary::
   :toctree: api/

   Series.attrs


Plotting
--------
``Series.plot`` is both a callable method and a namespace attribute for
specific plotting methods of the form ``Series.plot.<kind>``.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_callable.rst

   Series.plot

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   Series.plot.area
   Series.plot.bar
   Series.plot.barh
   Series.plot.box
   Series.plot.density
   Series.plot.hist
   Series.plot.kde
   Series.plot.line
   Series.plot.pie

.. autosummary::
   :toctree: api/

   Series.hist

Serialization / IO / conversion
-------------------------------
.. autosummary::
   :toctree: api/

   Series.to_pickle
   Series.to_csv
   Series.to_dict
   Series.to_excel
   Series.to_frame
   Series.to_xarray
   Series.to_hdf
   Series.to_sql
   Series.to_json
   Series.to_string
   Series.to_clipboard
   Series.to_latex
   Series.to_markdown
{{ header }}

.. _api.groupby:

=======
GroupBy
=======
.. currentmodule:: pandas.core.groupby

GroupBy objects are returned by groupby calls: :func:`pandas.DataFrame.groupby`, :func:`pandas.Series.groupby`, etc.

Indexing, iteration
-------------------
.. autosummary::
   :toctree: api/

   GroupBy.__iter__
   GroupBy.groups
   GroupBy.indices
   GroupBy.get_group

.. currentmodule:: pandas

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   Grouper

.. currentmodule:: pandas.core.groupby

Function application
--------------------
.. autosummary::
   :toctree: api/

   GroupBy.apply
   GroupBy.agg
   SeriesGroupBy.aggregate
   DataFrameGroupBy.aggregate
   SeriesGroupBy.transform
   DataFrameGroupBy.transform
   GroupBy.pipe

Computations / descriptive stats
--------------------------------
.. autosummary::
   :toctree: api/

   GroupBy.all
   GroupBy.any
   GroupBy.bfill
   GroupBy.backfill
   GroupBy.count
   GroupBy.cumcount
   GroupBy.cummax
   GroupBy.cummin
   GroupBy.cumprod
   GroupBy.cumsum
   GroupBy.ffill
   GroupBy.first
   GroupBy.head
   GroupBy.last
   GroupBy.max
   GroupBy.mean
   GroupBy.median
   GroupBy.min
   GroupBy.ngroup
   GroupBy.nth
   GroupBy.ohlc
   GroupBy.pad
   GroupBy.prod
   GroupBy.rank
   GroupBy.pct_change
   GroupBy.size
   GroupBy.sem
   GroupBy.std
   GroupBy.sum
   GroupBy.var
   GroupBy.tail

The following methods are available in both ``SeriesGroupBy`` and
``DataFrameGroupBy`` objects, but may differ slightly, usually in that
the ``DataFrameGroupBy`` version usually permits the specification of an
axis argument, and often an argument indicating whether to restrict
application to columns of a specific data type.

.. autosummary::
   :toctree: api/

   DataFrameGroupBy.all
   DataFrameGroupBy.any
   DataFrameGroupBy.backfill
   DataFrameGroupBy.bfill
   DataFrameGroupBy.corr
   DataFrameGroupBy.count
   DataFrameGroupBy.cov
   DataFrameGroupBy.cumcount
   DataFrameGroupBy.cummax
   DataFrameGroupBy.cummin
   DataFrameGroupBy.cumprod
   DataFrameGroupBy.cumsum
   DataFrameGroupBy.describe
   DataFrameGroupBy.diff
   DataFrameGroupBy.ffill
   DataFrameGroupBy.fillna
   DataFrameGroupBy.filter
   DataFrameGroupBy.hist
   DataFrameGroupBy.idxmax
   DataFrameGroupBy.idxmin
   DataFrameGroupBy.mad
   DataFrameGroupBy.nunique
   DataFrameGroupBy.pad
   DataFrameGroupBy.pct_change
   DataFrameGroupBy.plot
   DataFrameGroupBy.quantile
   DataFrameGroupBy.rank
   DataFrameGroupBy.resample
   DataFrameGroupBy.sample
   DataFrameGroupBy.shift
   DataFrameGroupBy.size
   DataFrameGroupBy.skew
   DataFrameGroupBy.take
   DataFrameGroupBy.tshift
   DataFrameGroupBy.value_counts

The following methods are available only for ``SeriesGroupBy`` objects.

.. autosummary::
   :toctree: api/

   SeriesGroupBy.hist
   SeriesGroupBy.nlargest
   SeriesGroupBy.nsmallest
   SeriesGroupBy.nunique
   SeriesGroupBy.unique
   SeriesGroupBy.value_counts
   SeriesGroupBy.is_monotonic_increasing
   SeriesGroupBy.is_monotonic_decreasing

The following methods are available only for ``DataFrameGroupBy`` objects.

.. autosummary::
   :toctree: api/

   DataFrameGroupBy.corrwith
   DataFrameGroupBy.boxplot
{{ header }}

.. _api.dateoffsets:

============
Date offsets
============
.. currentmodule:: pandas.tseries.offsets

DateOffset
----------
.. autosummary::
   :toctree: api/

    DateOffset

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    DateOffset.freqstr
    DateOffset.kwds
    DateOffset.name
    DateOffset.nanos
    DateOffset.normalize
    DateOffset.rule_code
    DateOffset.n
    DateOffset.is_month_start
    DateOffset.is_month_end

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    DateOffset.apply
    DateOffset.apply_index
    DateOffset.copy
    DateOffset.isAnchored
    DateOffset.onOffset
    DateOffset.is_anchored
    DateOffset.is_on_offset
    DateOffset.__call__
    DateOffset.is_month_start
    DateOffset.is_month_end
    DateOffset.is_quarter_start
    DateOffset.is_quarter_end
    DateOffset.is_year_start
    DateOffset.is_year_end

BusinessDay
-----------

.. autosummary::
   :toctree: api/

    BusinessDay

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   BDay

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessDay.freqstr
    BusinessDay.kwds
    BusinessDay.name
    BusinessDay.nanos
    BusinessDay.normalize
    BusinessDay.rule_code
    BusinessDay.n
    BusinessDay.weekmask
    BusinessDay.holidays
    BusinessDay.calendar

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessDay.apply
    BusinessDay.apply_index
    BusinessDay.copy
    BusinessDay.isAnchored
    BusinessDay.onOffset
    BusinessDay.is_anchored
    BusinessDay.is_on_offset
    BusinessDay.__call__
    BusinessDay.is_month_start
    BusinessDay.is_month_end
    BusinessDay.is_quarter_start
    BusinessDay.is_quarter_end
    BusinessDay.is_year_start
    BusinessDay.is_year_end

BusinessHour
------------
.. autosummary::
   :toctree: api/

    BusinessHour

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessHour.freqstr
    BusinessHour.kwds
    BusinessHour.name
    BusinessHour.nanos
    BusinessHour.normalize
    BusinessHour.rule_code
    BusinessHour.n
    BusinessHour.start
    BusinessHour.end
    BusinessHour.weekmask
    BusinessHour.holidays
    BusinessHour.calendar

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessHour.apply
    BusinessHour.apply_index
    BusinessHour.copy
    BusinessHour.isAnchored
    BusinessHour.onOffset
    BusinessHour.is_anchored
    BusinessHour.is_on_offset
    BusinessHour.__call__
    BusinessHour.is_month_start
    BusinessHour.is_month_end
    BusinessHour.is_quarter_start
    BusinessHour.is_quarter_end
    BusinessHour.is_year_start
    BusinessHour.is_year_end

CustomBusinessDay
-----------------

.. autosummary::
   :toctree: api/

    CustomBusinessDay

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   CDay

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessDay.freqstr
    CustomBusinessDay.kwds
    CustomBusinessDay.name
    CustomBusinessDay.nanos
    CustomBusinessDay.normalize
    CustomBusinessDay.rule_code
    CustomBusinessDay.n
    CustomBusinessDay.weekmask
    CustomBusinessDay.calendar
    CustomBusinessDay.holidays

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessDay.apply_index
    CustomBusinessDay.apply
    CustomBusinessDay.copy
    CustomBusinessDay.isAnchored
    CustomBusinessDay.onOffset
    CustomBusinessDay.is_anchored
    CustomBusinessDay.is_on_offset
    CustomBusinessDay.__call__
    CustomBusinessDay.is_month_start
    CustomBusinessDay.is_month_end
    CustomBusinessDay.is_quarter_start
    CustomBusinessDay.is_quarter_end
    CustomBusinessDay.is_year_start
    CustomBusinessDay.is_year_end

CustomBusinessHour
------------------
.. autosummary::
   :toctree: api/

    CustomBusinessHour

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessHour.freqstr
    CustomBusinessHour.kwds
    CustomBusinessHour.name
    CustomBusinessHour.nanos
    CustomBusinessHour.normalize
    CustomBusinessHour.rule_code
    CustomBusinessHour.n
    CustomBusinessHour.weekmask
    CustomBusinessHour.calendar
    CustomBusinessHour.holidays
    CustomBusinessHour.start
    CustomBusinessHour.end

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessHour.apply
    CustomBusinessHour.apply_index
    CustomBusinessHour.copy
    CustomBusinessHour.isAnchored
    CustomBusinessHour.onOffset
    CustomBusinessHour.is_anchored
    CustomBusinessHour.is_on_offset
    CustomBusinessHour.__call__
    CustomBusinessHour.is_month_start
    CustomBusinessHour.is_month_end
    CustomBusinessHour.is_quarter_start
    CustomBusinessHour.is_quarter_end
    CustomBusinessHour.is_year_start
    CustomBusinessHour.is_year_end

MonthEnd
--------
.. autosummary::
   :toctree: api/

    MonthEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    MonthEnd.freqstr
    MonthEnd.kwds
    MonthEnd.name
    MonthEnd.nanos
    MonthEnd.normalize
    MonthEnd.rule_code
    MonthEnd.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    MonthEnd.apply
    MonthEnd.apply_index
    MonthEnd.copy
    MonthEnd.isAnchored
    MonthEnd.onOffset
    MonthEnd.is_anchored
    MonthEnd.is_on_offset
    MonthEnd.__call__
    MonthEnd.is_month_start
    MonthEnd.is_month_end
    MonthEnd.is_quarter_start
    MonthEnd.is_quarter_end
    MonthEnd.is_year_start
    MonthEnd.is_year_end

MonthBegin
----------
.. autosummary::
   :toctree: api/

    MonthBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    MonthBegin.freqstr
    MonthBegin.kwds
    MonthBegin.name
    MonthBegin.nanos
    MonthBegin.normalize
    MonthBegin.rule_code
    MonthBegin.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    MonthBegin.apply
    MonthBegin.apply_index
    MonthBegin.copy
    MonthBegin.isAnchored
    MonthBegin.onOffset
    MonthBegin.is_anchored
    MonthBegin.is_on_offset
    MonthBegin.__call__
    MonthBegin.is_month_start
    MonthBegin.is_month_end
    MonthBegin.is_quarter_start
    MonthBegin.is_quarter_end
    MonthBegin.is_year_start
    MonthBegin.is_year_end

BusinessMonthEnd
----------------

.. autosummary::
   :toctree: api/

    BusinessMonthEnd

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   BMonthEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessMonthEnd.freqstr
    BusinessMonthEnd.kwds
    BusinessMonthEnd.name
    BusinessMonthEnd.nanos
    BusinessMonthEnd.normalize
    BusinessMonthEnd.rule_code
    BusinessMonthEnd.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessMonthEnd.apply
    BusinessMonthEnd.apply_index
    BusinessMonthEnd.copy
    BusinessMonthEnd.isAnchored
    BusinessMonthEnd.onOffset
    BusinessMonthEnd.is_anchored
    BusinessMonthEnd.is_on_offset
    BusinessMonthEnd.__call__
    BusinessMonthEnd.is_month_start
    BusinessMonthEnd.is_month_end
    BusinessMonthEnd.is_quarter_start
    BusinessMonthEnd.is_quarter_end
    BusinessMonthEnd.is_year_start
    BusinessMonthEnd.is_year_end

BusinessMonthBegin
------------------

.. autosummary::
   :toctree: api/

    BusinessMonthBegin

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   BMonthBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessMonthBegin.freqstr
    BusinessMonthBegin.kwds
    BusinessMonthBegin.name
    BusinessMonthBegin.nanos
    BusinessMonthBegin.normalize
    BusinessMonthBegin.rule_code
    BusinessMonthBegin.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BusinessMonthBegin.apply
    BusinessMonthBegin.apply_index
    BusinessMonthBegin.copy
    BusinessMonthBegin.isAnchored
    BusinessMonthBegin.onOffset
    BusinessMonthBegin.is_anchored
    BusinessMonthBegin.is_on_offset
    BusinessMonthBegin.__call__
    BusinessMonthBegin.is_month_start
    BusinessMonthBegin.is_month_end
    BusinessMonthBegin.is_quarter_start
    BusinessMonthBegin.is_quarter_end
    BusinessMonthBegin.is_year_start
    BusinessMonthBegin.is_year_end

CustomBusinessMonthEnd
----------------------

.. autosummary::
   :toctree: api/

    CustomBusinessMonthEnd

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   CBMonthEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessMonthEnd.freqstr
    CustomBusinessMonthEnd.kwds
    CustomBusinessMonthEnd.m_offset
    CustomBusinessMonthEnd.name
    CustomBusinessMonthEnd.nanos
    CustomBusinessMonthEnd.normalize
    CustomBusinessMonthEnd.rule_code
    CustomBusinessMonthEnd.n
    CustomBusinessMonthEnd.weekmask
    CustomBusinessMonthEnd.calendar
    CustomBusinessMonthEnd.holidays

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessMonthEnd.apply
    CustomBusinessMonthEnd.apply_index
    CustomBusinessMonthEnd.copy
    CustomBusinessMonthEnd.isAnchored
    CustomBusinessMonthEnd.onOffset
    CustomBusinessMonthEnd.is_anchored
    CustomBusinessMonthEnd.is_on_offset
    CustomBusinessMonthEnd.__call__
    CustomBusinessMonthEnd.is_month_start
    CustomBusinessMonthEnd.is_month_end
    CustomBusinessMonthEnd.is_quarter_start
    CustomBusinessMonthEnd.is_quarter_end
    CustomBusinessMonthEnd.is_year_start
    CustomBusinessMonthEnd.is_year_end

CustomBusinessMonthBegin
------------------------

.. autosummary::
   :toctree: api/

    CustomBusinessMonthBegin

Alias:

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   CBMonthBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessMonthBegin.freqstr
    CustomBusinessMonthBegin.kwds
    CustomBusinessMonthBegin.m_offset
    CustomBusinessMonthBegin.name
    CustomBusinessMonthBegin.nanos
    CustomBusinessMonthBegin.normalize
    CustomBusinessMonthBegin.rule_code
    CustomBusinessMonthBegin.n
    CustomBusinessMonthBegin.weekmask
    CustomBusinessMonthBegin.calendar
    CustomBusinessMonthBegin.holidays

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    CustomBusinessMonthBegin.apply
    CustomBusinessMonthBegin.apply_index
    CustomBusinessMonthBegin.copy
    CustomBusinessMonthBegin.isAnchored
    CustomBusinessMonthBegin.onOffset
    CustomBusinessMonthBegin.is_anchored
    CustomBusinessMonthBegin.is_on_offset
    CustomBusinessMonthBegin.__call__
    CustomBusinessMonthBegin.is_month_start
    CustomBusinessMonthBegin.is_month_end
    CustomBusinessMonthBegin.is_quarter_start
    CustomBusinessMonthBegin.is_quarter_end
    CustomBusinessMonthBegin.is_year_start
    CustomBusinessMonthBegin.is_year_end

SemiMonthEnd
------------
.. autosummary::
   :toctree: api/

    SemiMonthEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    SemiMonthEnd.freqstr
    SemiMonthEnd.kwds
    SemiMonthEnd.name
    SemiMonthEnd.nanos
    SemiMonthEnd.normalize
    SemiMonthEnd.rule_code
    SemiMonthEnd.n
    SemiMonthEnd.day_of_month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    SemiMonthEnd.apply
    SemiMonthEnd.apply_index
    SemiMonthEnd.copy
    SemiMonthEnd.isAnchored
    SemiMonthEnd.onOffset
    SemiMonthEnd.is_anchored
    SemiMonthEnd.is_on_offset
    SemiMonthEnd.__call__
    SemiMonthEnd.is_month_start
    SemiMonthEnd.is_month_end
    SemiMonthEnd.is_quarter_start
    SemiMonthEnd.is_quarter_end
    SemiMonthEnd.is_year_start
    SemiMonthEnd.is_year_end

SemiMonthBegin
--------------
.. autosummary::
   :toctree: api/

    SemiMonthBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    SemiMonthBegin.freqstr
    SemiMonthBegin.kwds
    SemiMonthBegin.name
    SemiMonthBegin.nanos
    SemiMonthBegin.normalize
    SemiMonthBegin.rule_code
    SemiMonthBegin.n
    SemiMonthBegin.day_of_month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    SemiMonthBegin.apply
    SemiMonthBegin.apply_index
    SemiMonthBegin.copy
    SemiMonthBegin.isAnchored
    SemiMonthBegin.onOffset
    SemiMonthBegin.is_anchored
    SemiMonthBegin.is_on_offset
    SemiMonthBegin.__call__
    SemiMonthBegin.is_month_start
    SemiMonthBegin.is_month_end
    SemiMonthBegin.is_quarter_start
    SemiMonthBegin.is_quarter_end
    SemiMonthBegin.is_year_start
    SemiMonthBegin.is_year_end

Week
----
.. autosummary::
   :toctree: api/

    Week

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Week.freqstr
    Week.kwds
    Week.name
    Week.nanos
    Week.normalize
    Week.rule_code
    Week.n
    Week.weekday

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Week.apply
    Week.apply_index
    Week.copy
    Week.isAnchored
    Week.onOffset
    Week.is_anchored
    Week.is_on_offset
    Week.__call__
    Week.is_month_start
    Week.is_month_end
    Week.is_quarter_start
    Week.is_quarter_end
    Week.is_year_start
    Week.is_year_end

WeekOfMonth
-----------
.. autosummary::
   :toctree: api/

    WeekOfMonth

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    WeekOfMonth.freqstr
    WeekOfMonth.kwds
    WeekOfMonth.name
    WeekOfMonth.nanos
    WeekOfMonth.normalize
    WeekOfMonth.rule_code
    WeekOfMonth.n
    WeekOfMonth.week

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    WeekOfMonth.apply
    WeekOfMonth.apply_index
    WeekOfMonth.copy
    WeekOfMonth.isAnchored
    WeekOfMonth.onOffset
    WeekOfMonth.is_anchored
    WeekOfMonth.is_on_offset
    WeekOfMonth.__call__
    WeekOfMonth.weekday
    WeekOfMonth.is_month_start
    WeekOfMonth.is_month_end
    WeekOfMonth.is_quarter_start
    WeekOfMonth.is_quarter_end
    WeekOfMonth.is_year_start
    WeekOfMonth.is_year_end

LastWeekOfMonth
---------------
.. autosummary::
   :toctree: api/

    LastWeekOfMonth

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    LastWeekOfMonth.freqstr
    LastWeekOfMonth.kwds
    LastWeekOfMonth.name
    LastWeekOfMonth.nanos
    LastWeekOfMonth.normalize
    LastWeekOfMonth.rule_code
    LastWeekOfMonth.n
    LastWeekOfMonth.weekday
    LastWeekOfMonth.week

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    LastWeekOfMonth.apply
    LastWeekOfMonth.apply_index
    LastWeekOfMonth.copy
    LastWeekOfMonth.isAnchored
    LastWeekOfMonth.onOffset
    LastWeekOfMonth.is_anchored
    LastWeekOfMonth.is_on_offset
    LastWeekOfMonth.__call__
    LastWeekOfMonth.is_month_start
    LastWeekOfMonth.is_month_end
    LastWeekOfMonth.is_quarter_start
    LastWeekOfMonth.is_quarter_end
    LastWeekOfMonth.is_year_start
    LastWeekOfMonth.is_year_end

BQuarterEnd
-----------
.. autosummary::
   :toctree: api/

    BQuarterEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BQuarterEnd.freqstr
    BQuarterEnd.kwds
    BQuarterEnd.name
    BQuarterEnd.nanos
    BQuarterEnd.normalize
    BQuarterEnd.rule_code
    BQuarterEnd.n
    BQuarterEnd.startingMonth

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BQuarterEnd.apply
    BQuarterEnd.apply_index
    BQuarterEnd.copy
    BQuarterEnd.isAnchored
    BQuarterEnd.onOffset
    BQuarterEnd.is_anchored
    BQuarterEnd.is_on_offset
    BQuarterEnd.__call__
    BQuarterEnd.is_month_start
    BQuarterEnd.is_month_end
    BQuarterEnd.is_quarter_start
    BQuarterEnd.is_quarter_end
    BQuarterEnd.is_year_start
    BQuarterEnd.is_year_end

BQuarterBegin
-------------
.. autosummary::
   :toctree: api/

    BQuarterBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BQuarterBegin.freqstr
    BQuarterBegin.kwds
    BQuarterBegin.name
    BQuarterBegin.nanos
    BQuarterBegin.normalize
    BQuarterBegin.rule_code
    BQuarterBegin.n
    BQuarterBegin.startingMonth

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BQuarterBegin.apply
    BQuarterBegin.apply_index
    BQuarterBegin.copy
    BQuarterBegin.isAnchored
    BQuarterBegin.onOffset
    BQuarterBegin.is_anchored
    BQuarterBegin.is_on_offset
    BQuarterBegin.__call__
    BQuarterBegin.is_month_start
    BQuarterBegin.is_month_end
    BQuarterBegin.is_quarter_start
    BQuarterBegin.is_quarter_end
    BQuarterBegin.is_year_start
    BQuarterBegin.is_year_end

QuarterEnd
----------
.. autosummary::
   :toctree: api/

    QuarterEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    QuarterEnd.freqstr
    QuarterEnd.kwds
    QuarterEnd.name
    QuarterEnd.nanos
    QuarterEnd.normalize
    QuarterEnd.rule_code
    QuarterEnd.n
    QuarterEnd.startingMonth

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    QuarterEnd.apply
    QuarterEnd.apply_index
    QuarterEnd.copy
    QuarterEnd.isAnchored
    QuarterEnd.onOffset
    QuarterEnd.is_anchored
    QuarterEnd.is_on_offset
    QuarterEnd.__call__
    QuarterEnd.is_month_start
    QuarterEnd.is_month_end
    QuarterEnd.is_quarter_start
    QuarterEnd.is_quarter_end
    QuarterEnd.is_year_start
    QuarterEnd.is_year_end

QuarterBegin
------------
.. autosummary::
   :toctree: api/

    QuarterBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    QuarterBegin.freqstr
    QuarterBegin.kwds
    QuarterBegin.name
    QuarterBegin.nanos
    QuarterBegin.normalize
    QuarterBegin.rule_code
    QuarterBegin.n
    QuarterBegin.startingMonth

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    QuarterBegin.apply
    QuarterBegin.apply_index
    QuarterBegin.copy
    QuarterBegin.isAnchored
    QuarterBegin.onOffset
    QuarterBegin.is_anchored
    QuarterBegin.is_on_offset
    QuarterBegin.__call__
    QuarterBegin.is_month_start
    QuarterBegin.is_month_end
    QuarterBegin.is_quarter_start
    QuarterBegin.is_quarter_end
    QuarterBegin.is_year_start
    QuarterBegin.is_year_end

BYearEnd
--------
.. autosummary::
   :toctree: api/

    BYearEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BYearEnd.freqstr
    BYearEnd.kwds
    BYearEnd.name
    BYearEnd.nanos
    BYearEnd.normalize
    BYearEnd.rule_code
    BYearEnd.n
    BYearEnd.month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BYearEnd.apply
    BYearEnd.apply_index
    BYearEnd.copy
    BYearEnd.isAnchored
    BYearEnd.onOffset
    BYearEnd.is_anchored
    BYearEnd.is_on_offset
    BYearEnd.__call__
    BYearEnd.is_month_start
    BYearEnd.is_month_end
    BYearEnd.is_quarter_start
    BYearEnd.is_quarter_end
    BYearEnd.is_year_start
    BYearEnd.is_year_end

BYearBegin
----------
.. autosummary::
   :toctree: api/

    BYearBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    BYearBegin.freqstr
    BYearBegin.kwds
    BYearBegin.name
    BYearBegin.nanos
    BYearBegin.normalize
    BYearBegin.rule_code
    BYearBegin.n
    BYearBegin.month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    BYearBegin.apply
    BYearBegin.apply_index
    BYearBegin.copy
    BYearBegin.isAnchored
    BYearBegin.onOffset
    BYearBegin.is_anchored
    BYearBegin.is_on_offset
    BYearBegin.__call__
    BYearBegin.is_month_start
    BYearBegin.is_month_end
    BYearBegin.is_quarter_start
    BYearBegin.is_quarter_end
    BYearBegin.is_year_start
    BYearBegin.is_year_end

YearEnd
-------
.. autosummary::
   :toctree: api/

    YearEnd

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    YearEnd.freqstr
    YearEnd.kwds
    YearEnd.name
    YearEnd.nanos
    YearEnd.normalize
    YearEnd.rule_code
    YearEnd.n
    YearEnd.month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    YearEnd.apply
    YearEnd.apply_index
    YearEnd.copy
    YearEnd.isAnchored
    YearEnd.onOffset
    YearEnd.is_anchored
    YearEnd.is_on_offset
    YearEnd.__call__
    YearEnd.is_month_start
    YearEnd.is_month_end
    YearEnd.is_quarter_start
    YearEnd.is_quarter_end
    YearEnd.is_year_start
    YearEnd.is_year_end

YearBegin
---------
.. autosummary::
   :toctree: api/

    YearBegin

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    YearBegin.freqstr
    YearBegin.kwds
    YearBegin.name
    YearBegin.nanos
    YearBegin.normalize
    YearBegin.rule_code
    YearBegin.n
    YearBegin.month

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    YearBegin.apply
    YearBegin.apply_index
    YearBegin.copy
    YearBegin.isAnchored
    YearBegin.onOffset
    YearBegin.is_anchored
    YearBegin.is_on_offset
    YearBegin.__call__
    YearBegin.is_month_start
    YearBegin.is_month_end
    YearBegin.is_quarter_start
    YearBegin.is_quarter_end
    YearBegin.is_year_start
    YearBegin.is_year_end

FY5253
------
.. autosummary::
   :toctree: api/

    FY5253

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    FY5253.freqstr
    FY5253.kwds
    FY5253.name
    FY5253.nanos
    FY5253.normalize
    FY5253.rule_code
    FY5253.n
    FY5253.startingMonth
    FY5253.variation
    FY5253.weekday

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    FY5253.apply
    FY5253.apply_index
    FY5253.copy
    FY5253.get_rule_code_suffix
    FY5253.get_year_end
    FY5253.isAnchored
    FY5253.onOffset
    FY5253.is_anchored
    FY5253.is_on_offset
    FY5253.__call__
    FY5253.is_month_start
    FY5253.is_month_end
    FY5253.is_quarter_start
    FY5253.is_quarter_end
    FY5253.is_year_start
    FY5253.is_year_end

FY5253Quarter
-------------
.. autosummary::
   :toctree: api/

    FY5253Quarter

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    FY5253Quarter.freqstr
    FY5253Quarter.kwds
    FY5253Quarter.name
    FY5253Quarter.nanos
    FY5253Quarter.normalize
    FY5253Quarter.rule_code
    FY5253Quarter.n
    FY5253Quarter.qtr_with_extra_week
    FY5253Quarter.startingMonth
    FY5253Quarter.variation
    FY5253Quarter.weekday

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    FY5253Quarter.apply
    FY5253Quarter.apply_index
    FY5253Quarter.copy
    FY5253Quarter.get_rule_code_suffix
    FY5253Quarter.get_weeks
    FY5253Quarter.isAnchored
    FY5253Quarter.onOffset
    FY5253Quarter.is_anchored
    FY5253Quarter.is_on_offset
    FY5253Quarter.year_has_extra_week
    FY5253Quarter.__call__
    FY5253Quarter.is_month_start
    FY5253Quarter.is_month_end
    FY5253Quarter.is_quarter_start
    FY5253Quarter.is_quarter_end
    FY5253Quarter.is_year_start
    FY5253Quarter.is_year_end

Easter
------
.. autosummary::
   :toctree: api/

    Easter

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Easter.freqstr
    Easter.kwds
    Easter.name
    Easter.nanos
    Easter.normalize
    Easter.rule_code
    Easter.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Easter.apply
    Easter.apply_index
    Easter.copy
    Easter.isAnchored
    Easter.onOffset
    Easter.is_anchored
    Easter.is_on_offset
    Easter.__call__
    Easter.is_month_start
    Easter.is_month_end
    Easter.is_quarter_start
    Easter.is_quarter_end
    Easter.is_year_start
    Easter.is_year_end

Tick
----
.. autosummary::
   :toctree: api/

    Tick

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Tick.delta
    Tick.freqstr
    Tick.kwds
    Tick.name
    Tick.nanos
    Tick.normalize
    Tick.rule_code
    Tick.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Tick.copy
    Tick.isAnchored
    Tick.onOffset
    Tick.is_anchored
    Tick.is_on_offset
    Tick.__call__
    Tick.apply
    Tick.apply_index
    Tick.is_month_start
    Tick.is_month_end
    Tick.is_quarter_start
    Tick.is_quarter_end
    Tick.is_year_start
    Tick.is_year_end

Day
---
.. autosummary::
   :toctree: api/

    Day

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Day.delta
    Day.freqstr
    Day.kwds
    Day.name
    Day.nanos
    Day.normalize
    Day.rule_code
    Day.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Day.copy
    Day.isAnchored
    Day.onOffset
    Day.is_anchored
    Day.is_on_offset
    Day.__call__
    Day.apply
    Day.apply_index
    Day.is_month_start
    Day.is_month_end
    Day.is_quarter_start
    Day.is_quarter_end
    Day.is_year_start
    Day.is_year_end

Hour
----
.. autosummary::
   :toctree: api/

    Hour

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Hour.delta
    Hour.freqstr
    Hour.kwds
    Hour.name
    Hour.nanos
    Hour.normalize
    Hour.rule_code
    Hour.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Hour.copy
    Hour.isAnchored
    Hour.onOffset
    Hour.is_anchored
    Hour.is_on_offset
    Hour.__call__
    Hour.apply
    Hour.apply_index
    Hour.is_month_start
    Hour.is_month_end
    Hour.is_quarter_start
    Hour.is_quarter_end
    Hour.is_year_start
    Hour.is_year_end

Minute
------
.. autosummary::
   :toctree: api/

    Minute

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Minute.delta
    Minute.freqstr
    Minute.kwds
    Minute.name
    Minute.nanos
    Minute.normalize
    Minute.rule_code
    Minute.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Minute.copy
    Minute.isAnchored
    Minute.onOffset
    Minute.is_anchored
    Minute.is_on_offset
    Minute.__call__
    Minute.apply
    Minute.apply_index
    Minute.is_month_start
    Minute.is_month_end
    Minute.is_quarter_start
    Minute.is_quarter_end
    Minute.is_year_start
    Minute.is_year_end

Second
------
.. autosummary::
   :toctree: api/

    Second

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Second.delta
    Second.freqstr
    Second.kwds
    Second.name
    Second.nanos
    Second.normalize
    Second.rule_code
    Second.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Second.copy
    Second.isAnchored
    Second.onOffset
    Second.is_anchored
    Second.is_on_offset
    Second.__call__
    Second.apply
    Second.apply_index
    Second.is_month_start
    Second.is_month_end
    Second.is_quarter_start
    Second.is_quarter_end
    Second.is_year_start
    Second.is_year_end

Milli
-----
.. autosummary::
   :toctree: api/

    Milli

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Milli.delta
    Milli.freqstr
    Milli.kwds
    Milli.name
    Milli.nanos
    Milli.normalize
    Milli.rule_code
    Milli.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Milli.copy
    Milli.isAnchored
    Milli.onOffset
    Milli.is_anchored
    Milli.is_on_offset
    Milli.__call__
    Milli.apply
    Milli.apply_index
    Milli.is_month_start
    Milli.is_month_end
    Milli.is_quarter_start
    Milli.is_quarter_end
    Milli.is_year_start
    Milli.is_year_end

Micro
-----
.. autosummary::
   :toctree: api/

    Micro

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Micro.delta
    Micro.freqstr
    Micro.kwds
    Micro.name
    Micro.nanos
    Micro.normalize
    Micro.rule_code
    Micro.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Micro.copy
    Micro.isAnchored
    Micro.onOffset
    Micro.is_anchored
    Micro.is_on_offset
    Micro.__call__
    Micro.apply
    Micro.apply_index
    Micro.is_month_start
    Micro.is_month_end
    Micro.is_quarter_start
    Micro.is_quarter_end
    Micro.is_year_start
    Micro.is_year_end

Nano
----
.. autosummary::
   :toctree: api/

    Nano

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

    Nano.delta
    Nano.freqstr
    Nano.kwds
    Nano.name
    Nano.nanos
    Nano.normalize
    Nano.rule_code
    Nano.n

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

    Nano.copy
    Nano.isAnchored
    Nano.onOffset
    Nano.is_anchored
    Nano.is_on_offset
    Nano.__call__
    Nano.apply
    Nano.apply_index
    Nano.is_month_start
    Nano.is_month_end
    Nano.is_quarter_start
    Nano.is_quarter_end
    Nano.is_year_start
    Nano.is_year_end

.. _api.frequencies:

===========
Frequencies
===========
.. currentmodule:: pandas.tseries.frequencies

.. _api.offsets:

.. autosummary::
   :toctree: api/

   to_offset
{{ header }}

.. _api.extensions:

==========
Extensions
==========
.. currentmodule:: pandas

These are primarily intended for library authors looking to extend pandas
objects.

.. autosummary::
   :toctree: api/

   api.extensions.register_extension_dtype
   api.extensions.register_dataframe_accessor
   api.extensions.register_series_accessor
   api.extensions.register_index_accessor
   api.extensions.ExtensionDtype

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   api.extensions.ExtensionArray
   arrays.PandasArray

.. We need this autosummary so that methods and attributes are generated.
.. Separate block, since they aren't classes.

   .. autosummary::
      :toctree: api/

      api.extensions.ExtensionArray._concat_same_type
      api.extensions.ExtensionArray._formatter
      api.extensions.ExtensionArray._from_factorized
      api.extensions.ExtensionArray._from_sequence
      api.extensions.ExtensionArray._from_sequence_of_strings
      api.extensions.ExtensionArray._reduce
      api.extensions.ExtensionArray._values_for_argsort
      api.extensions.ExtensionArray._values_for_factorize
      api.extensions.ExtensionArray.argsort
      api.extensions.ExtensionArray.astype
      api.extensions.ExtensionArray.copy
      api.extensions.ExtensionArray.view
      api.extensions.ExtensionArray.dropna
      api.extensions.ExtensionArray.equals
      api.extensions.ExtensionArray.factorize
      api.extensions.ExtensionArray.fillna
      api.extensions.ExtensionArray.insert
      api.extensions.ExtensionArray.isin
      api.extensions.ExtensionArray.isna
      api.extensions.ExtensionArray.ravel
      api.extensions.ExtensionArray.repeat
      api.extensions.ExtensionArray.searchsorted
      api.extensions.ExtensionArray.shift
      api.extensions.ExtensionArray.take
      api.extensions.ExtensionArray.unique
      api.extensions.ExtensionArray.dtype
      api.extensions.ExtensionArray.nbytes
      api.extensions.ExtensionArray.ndim
      api.extensions.ExtensionArray.shape
      api.extensions.ExtensionArray.tolist

Additionally, we have some utility methods for ensuring your object
behaves correctly.

.. autosummary::
  :toctree: api/

  api.indexers.check_array_indexer


The sentinel ``pandas.api.extensions.no_default`` is used as the default
value in some methods. Use an ``is`` comparison to check if the user
provides a non-default value.
{{ header }}

.. _api.general_functions:

=================
General functions
=================
.. currentmodule:: pandas

Data manipulations
~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   melt
   pivot
   pivot_table
   crosstab
   cut
   qcut
   merge
   merge_ordered
   merge_asof
   concat
   get_dummies
   factorize
   unique
   wide_to_long

Top-level missing data
~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   isna
   isnull
   notna
   notnull

Top-level dealing with numeric data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   to_numeric

Top-level dealing with datetimelike data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   to_datetime
   to_timedelta
   date_range
   bdate_range
   period_range
   timedelta_range
   infer_freq

Top-level dealing with Interval data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   interval_range

Top-level evaluation
~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   eval

Hashing
~~~~~~~
.. autosummary::
   :toctree: api/

   util.hash_array
   util.hash_pandas_object

Testing
~~~~~~~
.. autosummary::
   :toctree: api/

   test
{{ header }}

.. _api.arrays:

======================================
pandas arrays, scalars, and data types
======================================

.. currentmodule:: pandas

For most data types, pandas uses NumPy arrays as the concrete
objects contained with a :class:`Index`, :class:`Series`, or
:class:`DataFrame`.

For some data types, pandas extends NumPy's type system. String aliases for these types
can be found at :ref:`basics.dtypes`.

=================== ========================= ================== =============================
Kind of Data        pandas Data Type          Scalar             Array
=================== ========================= ================== =============================
TZ-aware datetime   :class:`DatetimeTZDtype`  :class:`Timestamp` :ref:`api.arrays.datetime`
Timedeltas          (none)                    :class:`Timedelta` :ref:`api.arrays.timedelta`
Period (time spans) :class:`PeriodDtype`      :class:`Period`    :ref:`api.arrays.period`
Intervals           :class:`IntervalDtype`    :class:`Interval`  :ref:`api.arrays.interval`
Nullable Integer    :class:`Int64Dtype`, ...  (none)             :ref:`api.arrays.integer_na`
Categorical         :class:`CategoricalDtype` (none)             :ref:`api.arrays.categorical`
Sparse              :class:`SparseDtype`      (none)             :ref:`api.arrays.sparse`
Strings             :class:`StringDtype`      :class:`str`       :ref:`api.arrays.string`
Boolean (with NA)   :class:`BooleanDtype`     :class:`bool`      :ref:`api.arrays.bool`
=================== ========================= ================== =============================

pandas and third-party libraries can extend NumPy's type system (see :ref:`extending.extension-types`).
The top-level :meth:`array` method can be used to create a new array, which may be
stored in a :class:`Series`, :class:`Index`, or as a column in a :class:`DataFrame`.

.. autosummary::
   :toctree: api/

   array

.. _api.arrays.datetime:

Datetime data
-------------

NumPy cannot natively represent timezone-aware datetimes. pandas supports this
with the :class:`arrays.DatetimeArray` extension array, which can hold timezone-naive
or timezone-aware values.

:class:`Timestamp`, a subclass of :class:`datetime.datetime`, is pandas'
scalar type for timezone-naive or timezone-aware datetime data.

.. autosummary::
   :toctree: api/

   Timestamp

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Timestamp.asm8
   Timestamp.day
   Timestamp.dayofweek
   Timestamp.day_of_week
   Timestamp.dayofyear
   Timestamp.day_of_year
   Timestamp.days_in_month
   Timestamp.daysinmonth
   Timestamp.fold
   Timestamp.hour
   Timestamp.is_leap_year
   Timestamp.is_month_end
   Timestamp.is_month_start
   Timestamp.is_quarter_end
   Timestamp.is_quarter_start
   Timestamp.is_year_end
   Timestamp.is_year_start
   Timestamp.max
   Timestamp.microsecond
   Timestamp.min
   Timestamp.minute
   Timestamp.month
   Timestamp.nanosecond
   Timestamp.quarter
   Timestamp.resolution
   Timestamp.second
   Timestamp.tz
   Timestamp.tzinfo
   Timestamp.value
   Timestamp.week
   Timestamp.weekofyear
   Timestamp.year

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

   Timestamp.astimezone
   Timestamp.ceil
   Timestamp.combine
   Timestamp.ctime
   Timestamp.date
   Timestamp.day_name
   Timestamp.dst
   Timestamp.floor
   Timestamp.freq
   Timestamp.freqstr
   Timestamp.fromordinal
   Timestamp.fromtimestamp
   Timestamp.isocalendar
   Timestamp.isoformat
   Timestamp.isoweekday
   Timestamp.month_name
   Timestamp.normalize
   Timestamp.now
   Timestamp.replace
   Timestamp.round
   Timestamp.strftime
   Timestamp.strptime
   Timestamp.time
   Timestamp.timestamp
   Timestamp.timetuple
   Timestamp.timetz
   Timestamp.to_datetime64
   Timestamp.to_numpy
   Timestamp.to_julian_date
   Timestamp.to_period
   Timestamp.to_pydatetime
   Timestamp.today
   Timestamp.toordinal
   Timestamp.tz_convert
   Timestamp.tz_localize
   Timestamp.tzname
   Timestamp.utcfromtimestamp
   Timestamp.utcnow
   Timestamp.utcoffset
   Timestamp.utctimetuple
   Timestamp.weekday

A collection of timestamps may be stored in a :class:`arrays.DatetimeArray`.
For timezone-aware data, the ``.dtype`` of a :class:`arrays.DatetimeArray` is a
:class:`DatetimeTZDtype`. For timezone-naive data, ``np.dtype("datetime64[ns]")``
is used.

If the data are timezone-aware, then every value in the array must have the same timezone.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.DatetimeArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   DatetimeTZDtype

.. _api.arrays.timedelta:

Timedelta data
--------------

NumPy can natively represent timedeltas. pandas provides :class:`Timedelta`
for symmetry with :class:`Timestamp`.

.. autosummary::
   :toctree: api/

   Timedelta

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Timedelta.asm8
   Timedelta.components
   Timedelta.days
   Timedelta.delta
   Timedelta.freq
   Timedelta.is_populated
   Timedelta.max
   Timedelta.microseconds
   Timedelta.min
   Timedelta.nanoseconds
   Timedelta.resolution
   Timedelta.seconds
   Timedelta.value
   Timedelta.view

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

   Timedelta.ceil
   Timedelta.floor
   Timedelta.isoformat
   Timedelta.round
   Timedelta.to_pytimedelta
   Timedelta.to_timedelta64
   Timedelta.to_numpy
   Timedelta.total_seconds

A collection of :class:`Timedelta` may be stored in a :class:`TimedeltaArray`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.TimedeltaArray

.. _api.arrays.period:

Timespan data
-------------

pandas represents spans of times as :class:`Period` objects.

Period
------
.. autosummary::
   :toctree: api/

   Period

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Period.day
   Period.dayofweek
   Period.day_of_week
   Period.dayofyear
   Period.day_of_year
   Period.days_in_month
   Period.daysinmonth
   Period.end_time
   Period.freq
   Period.freqstr
   Period.hour
   Period.is_leap_year
   Period.minute
   Period.month
   Period.ordinal
   Period.quarter
   Period.qyear
   Period.second
   Period.start_time
   Period.week
   Period.weekday
   Period.weekofyear
   Period.year

Methods
~~~~~~~
.. autosummary::
   :toctree: api/

   Period.asfreq
   Period.now
   Period.strftime
   Period.to_timestamp

A collection of :class:`Period` may be stored in a :class:`arrays.PeriodArray`.
Every period in a :class:`arrays.PeriodArray` must have the same ``freq``.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.PeriodArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   PeriodDtype

.. _api.arrays.interval:

Interval data
-------------

Arbitrary intervals can be represented as :class:`Interval` objects.

.. autosummary::
   :toctree: api/

    Interval

Properties
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   Interval.closed
   Interval.closed_left
   Interval.closed_right
   Interval.is_empty
   Interval.left
   Interval.length
   Interval.mid
   Interval.open_left
   Interval.open_right
   Interval.overlaps
   Interval.right

A collection of intervals may be stored in an :class:`arrays.IntervalArray`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.IntervalArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   IntervalDtype


.. Those attributes and methods are included in the API because the docstrings
.. of IntervalIndex and IntervalArray are shared. Including it here to make
.. sure a docstring page is built for them to avoid warnings

..
    .. autosummary::
      :toctree: api/

      arrays.IntervalArray.left
      arrays.IntervalArray.right
      arrays.IntervalArray.closed
      arrays.IntervalArray.mid
      arrays.IntervalArray.length
      arrays.IntervalArray.is_empty
      arrays.IntervalArray.is_non_overlapping_monotonic
      arrays.IntervalArray.from_arrays
      arrays.IntervalArray.from_tuples
      arrays.IntervalArray.from_breaks
      arrays.IntervalArray.contains
      arrays.IntervalArray.overlaps
      arrays.IntervalArray.set_closed
      arrays.IntervalArray.to_tuples


.. _api.arrays.integer_na:

Nullable integer
----------------

:class:`numpy.ndarray` cannot natively represent integer-data with missing values.
pandas provides this through :class:`arrays.IntegerArray`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.IntegerArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   Int8Dtype
   Int16Dtype
   Int32Dtype
   Int64Dtype
   UInt8Dtype
   UInt16Dtype
   UInt32Dtype
   UInt64Dtype

.. _api.arrays.categorical:

Categorical data
----------------

pandas defines a custom data type for representing data that can take only a
limited, fixed set of values. The dtype of a :class:`Categorical` can be described by
a :class:`CategoricalDtype`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   CategoricalDtype

.. autosummary::
   :toctree: api/

   CategoricalDtype.categories
   CategoricalDtype.ordered

Categorical data can be stored in a :class:`pandas.Categorical`

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   Categorical

The alternative :meth:`Categorical.from_codes` constructor can be used when you
have the categories and integer codes already:

.. autosummary::
   :toctree: api/

   Categorical.from_codes

The dtype information is available on the :class:`Categorical`

.. autosummary::
   :toctree: api/

   Categorical.dtype
   Categorical.categories
   Categorical.ordered
   Categorical.codes

``np.asarray(categorical)`` works by implementing the array interface. Be aware, that this converts
the :class:`Categorical` back to a NumPy array, so categories and order information is not preserved!

.. autosummary::
   :toctree: api/

   Categorical.__array__

A :class:`Categorical` can be stored in a :class:`Series` or :class:`DataFrame`.
To create a Series of dtype ``category``, use ``cat = s.astype(dtype)`` or
``Series(..., dtype=dtype)`` where ``dtype`` is either

* the string ``'category'``
* an instance of :class:`CategoricalDtype`.

If the :class:`Series` is of dtype :class:`CategoricalDtype`, ``Series.cat`` can be used to change the categorical
data. See :ref:`api.series.cat` for more.

.. _api.arrays.sparse:

Sparse data
-----------

Data where a single value is repeated many times (e.g. ``0`` or ``NaN``) may
be stored efficiently as a :class:`arrays.SparseArray`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.SparseArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   SparseDtype

The ``Series.sparse`` accessor may be used to access sparse-specific attributes
and methods if the :class:`Series` contains sparse values. See
:ref:`api.series.sparse` and :ref:`the user guide <sparse>` for more.


.. _api.arrays.string:

Text data
---------

When working with text data, where each valid element is a string or missing,
we recommend using :class:`StringDtype` (with the alias ``"string"``).

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.StringArray
   arrays.ArrowStringArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   StringDtype

The ``Series.str`` accessor is available for :class:`Series` backed by a :class:`arrays.StringArray`.
See :ref:`api.series.str` for more.


.. _api.arrays.bool:

Boolean data with missing values
--------------------------------

The boolean dtype (with the alias ``"boolean"``) provides support for storing
boolean data (``True``, ``False``) with missing values, which is not possible
with a bool :class:`numpy.ndarray`.

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   arrays.BooleanArray

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   BooleanDtype


.. Dtype attributes which are manually listed in their docstrings: including
.. it here to make sure a docstring page is built for them

..
    .. autosummary::
      :toctree: api/

      DatetimeTZDtype.unit
      DatetimeTZDtype.tz
      PeriodDtype.freq
      IntervalDtype.subtype
{{ header }}

.. _api:

=============
API reference
=============

This page gives an overview of all public pandas objects, functions and
methods. All classes and functions exposed in ``pandas.*`` namespace are public.

Some subpackages are public which include ``pandas.errors``,
``pandas.plotting``, and ``pandas.testing``. Public functions in
``pandas.io`` and ``pandas.tseries`` submodules are mentioned in
the documentation. ``pandas.api.types`` subpackage holds some
public functions related to data types in pandas.

.. warning::

    The ``pandas.core``, ``pandas.compat``, and ``pandas.util`` top-level modules are PRIVATE. Stable functionality in such modules is not guaranteed.

.. If you update this toctree, also update the manual toctree in the
   main index.rst.template

.. toctree::
   :maxdepth: 2

   io
   general_functions
   series
   frame
   arrays
   indexing
   offset_frequency
   window
   groupby
   resampling
   style
   plotting
   general_utility_functions
   extensions

.. This is to prevent warnings in the doc build. We don't want to encourage
.. these methods.

..
    .. toctree::

        api/pandas.DataFrame.blocks
        api/pandas.DataFrame.as_matrix
        api/pandas.Index.asi8
        api/pandas.Index.data
        api/pandas.Index.flags
        api/pandas.Index.holds_integer
        api/pandas.Index.is_type_compatible
        api/pandas.Index.nlevels
        api/pandas.Index.sort
        api/pandas.Series.asobject
        api/pandas.Series.blocks
        api/pandas.Series.from_array
        api/pandas.Series.imag
        api/pandas.Series.real


.. Can't convince sphinx to generate toctree for this class attribute.
.. So we do it manually to avoid a warning

..
    .. toctree::

        api/pandas.api.extensions.ExtensionDtype.na_value
{{ header }}

.. _api.io:

============
Input/output
============
.. currentmodule:: pandas

Pickling
~~~~~~~~
.. autosummary::
   :toctree: api/

   read_pickle
   DataFrame.to_pickle

Flat file
~~~~~~~~~
.. autosummary::
   :toctree: api/

   read_table
   read_csv
   DataFrame.to_csv
   read_fwf

Clipboard
~~~~~~~~~
.. autosummary::
   :toctree: api/

   read_clipboard
   DataFrame.to_clipboard

Excel
~~~~~
.. autosummary::
   :toctree: api/

   read_excel
   DataFrame.to_excel
   ExcelFile.parse

.. currentmodule:: pandas.io.formats.style

.. autosummary::
   :toctree: api/

   Styler.to_excel

.. currentmodule:: pandas

.. autosummary::
   :toctree: api/
   :template: autosummary/class_without_autosummary.rst

   ExcelWriter

.. currentmodule:: pandas

JSON
~~~~
.. autosummary::
   :toctree: api/

   read_json
   json_normalize
   DataFrame.to_json

.. currentmodule:: pandas.io.json

.. autosummary::
   :toctree: api/

   build_table_schema

.. currentmodule:: pandas

HTML
~~~~
.. autosummary::
   :toctree: api/

   read_html
   DataFrame.to_html

.. currentmodule:: pandas.io.formats.style

.. autosummary::
   :toctree: api/

   Styler.to_html

.. currentmodule:: pandas

XML
~~~~
.. autosummary::
   :toctree: api/

   read_xml
   DataFrame.to_xml

Latex
~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.to_latex

.. currentmodule:: pandas.io.formats.style

.. autosummary::
   :toctree: api/

   Styler.to_latex

.. currentmodule:: pandas

HDFStore: PyTables (HDF5)
~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   read_hdf
   HDFStore.put
   HDFStore.append
   HDFStore.get
   HDFStore.select
   HDFStore.info
   HDFStore.keys
   HDFStore.groups
   HDFStore.walk

.. warning::

   One can store a subclass of :class:`DataFrame` or :class:`Series` to HDF5,
   but the type of the subclass is lost upon storing.

Feather
~~~~~~~
.. autosummary::
   :toctree: api/

   read_feather
   DataFrame.to_feather

Parquet
~~~~~~~
.. autosummary::
   :toctree: api/

   read_parquet
   DataFrame.to_parquet

ORC
~~~
.. autosummary::
   :toctree: api/

   read_orc

SAS
~~~
.. autosummary::
   :toctree: api/

   read_sas

SPSS
~~~~
.. autosummary::
   :toctree: api/

   read_spss

SQL
~~~
.. autosummary::
   :toctree: api/

   read_sql_table
   read_sql_query
   read_sql
   DataFrame.to_sql

Google BigQuery
~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   read_gbq

STATA
~~~~~
.. autosummary::
   :toctree: api/

   read_stata
   DataFrame.to_stata

.. currentmodule:: pandas.io.stata

.. autosummary::
   :toctree: api/

   StataReader.data_label
   StataReader.value_labels
   StataReader.variable_labels
   StataWriter.write_file
{{ header }}

.. _api.general_utility_functions:

=========================
General utility functions
=========================
.. currentmodule:: pandas

Working with options
--------------------
.. autosummary::
   :toctree: api/

   describe_option
   reset_option
   get_option
   set_option
   option_context

.. _api.general.testing:

Testing functions
-----------------
.. autosummary::
   :toctree: api/

   testing.assert_frame_equal
   testing.assert_series_equal
   testing.assert_index_equal
   testing.assert_extension_array_equal

Exceptions and warnings
-----------------------
.. autosummary::
   :toctree: api/

   errors.AbstractMethodError
   errors.AccessorRegistrationWarning
   errors.DtypeWarning
   errors.DuplicateLabelError
   errors.EmptyDataError
   errors.InvalidIndexError
   errors.IntCastingNaNError
   errors.MergeError
   errors.NullFrequencyError
   errors.NumbaUtilError
   errors.OptionError
   errors.OutOfBoundsDatetime
   errors.OutOfBoundsTimedelta
   errors.ParserError
   errors.ParserWarning
   errors.PerformanceWarning
   errors.UnsortedIndexError
   errors.UnsupportedFunctionCall

Data types related functionality
--------------------------------
.. autosummary::
   :toctree: api/

   api.types.union_categoricals
   api.types.infer_dtype
   api.types.pandas_dtype

Dtype introspection
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

    api.types.is_bool_dtype
    api.types.is_categorical_dtype
    api.types.is_complex_dtype
    api.types.is_datetime64_any_dtype
    api.types.is_datetime64_dtype
    api.types.is_datetime64_ns_dtype
    api.types.is_datetime64tz_dtype
    api.types.is_extension_type
    api.types.is_extension_array_dtype
    api.types.is_float_dtype
    api.types.is_int64_dtype
    api.types.is_integer_dtype
    api.types.is_interval_dtype
    api.types.is_numeric_dtype
    api.types.is_object_dtype
    api.types.is_period_dtype
    api.types.is_signed_integer_dtype
    api.types.is_string_dtype
    api.types.is_timedelta64_dtype
    api.types.is_timedelta64_ns_dtype
    api.types.is_unsigned_integer_dtype
    api.types.is_sparse

Iterable introspection
~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

    api.types.is_dict_like
    api.types.is_file_like
    api.types.is_list_like
    api.types.is_named_tuple
    api.types.is_iterator

Scalar introspection
~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

    api.types.is_bool
    api.types.is_categorical
    api.types.is_complex
    api.types.is_float
    api.types.is_hashable
    api.types.is_integer
    api.types.is_interval
    api.types.is_number
    api.types.is_re
    api.types.is_re_compilable
    api.types.is_scalar

Bug report function
-------------------
.. autosummary::
   :toctree: api/

   show_versions
{{ header }}

.. _api.plotting:

========
Plotting
========
.. currentmodule:: pandas.plotting

The following functions are contained in the ``pandas.plotting`` module.

.. autosummary::
   :toctree: api/

   andrews_curves
   autocorrelation_plot
   bootstrap_plot
   boxplot
   deregister_matplotlib_converters
   lag_plot
   parallel_coordinates
   plot_params
   radviz
   register_matplotlib_converters
   scatter_matrix
   table
{{ header }}

.. _api.dataframe:

=========
DataFrame
=========
.. currentmodule:: pandas

Constructor
~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame

Attributes and underlying data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
**Axes**

.. autosummary::
   :toctree: api/

   DataFrame.index
   DataFrame.columns

.. autosummary::
   :toctree: api/

   DataFrame.dtypes
   DataFrame.info
   DataFrame.select_dtypes
   DataFrame.values
   DataFrame.axes
   DataFrame.ndim
   DataFrame.size
   DataFrame.shape
   DataFrame.memory_usage
   DataFrame.empty
   DataFrame.set_flags

Conversion
~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.astype
   DataFrame.convert_dtypes
   DataFrame.infer_objects
   DataFrame.copy
   DataFrame.bool

Indexing, iteration
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.head
   DataFrame.at
   DataFrame.iat
   DataFrame.loc
   DataFrame.iloc
   DataFrame.insert
   DataFrame.__iter__
   DataFrame.items
   DataFrame.iteritems
   DataFrame.keys
   DataFrame.iterrows
   DataFrame.itertuples
   DataFrame.lookup
   DataFrame.pop
   DataFrame.tail
   DataFrame.xs
   DataFrame.get
   DataFrame.isin
   DataFrame.where
   DataFrame.mask
   DataFrame.query

For more information on ``.at``, ``.iat``, ``.loc``, and
``.iloc``,  see the :ref:`indexing documentation <indexing>`.

Binary operator functions
~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.add
   DataFrame.sub
   DataFrame.mul
   DataFrame.div
   DataFrame.truediv
   DataFrame.floordiv
   DataFrame.mod
   DataFrame.pow
   DataFrame.dot
   DataFrame.radd
   DataFrame.rsub
   DataFrame.rmul
   DataFrame.rdiv
   DataFrame.rtruediv
   DataFrame.rfloordiv
   DataFrame.rmod
   DataFrame.rpow
   DataFrame.lt
   DataFrame.gt
   DataFrame.le
   DataFrame.ge
   DataFrame.ne
   DataFrame.eq
   DataFrame.combine
   DataFrame.combine_first

Function application, GroupBy & window
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.apply
   DataFrame.applymap
   DataFrame.pipe
   DataFrame.agg
   DataFrame.aggregate
   DataFrame.transform
   DataFrame.groupby
   DataFrame.rolling
   DataFrame.expanding
   DataFrame.ewm

.. _api.dataframe.stats:

Computations / descriptive stats
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.abs
   DataFrame.all
   DataFrame.any
   DataFrame.clip
   DataFrame.corr
   DataFrame.corrwith
   DataFrame.count
   DataFrame.cov
   DataFrame.cummax
   DataFrame.cummin
   DataFrame.cumprod
   DataFrame.cumsum
   DataFrame.describe
   DataFrame.diff
   DataFrame.eval
   DataFrame.kurt
   DataFrame.kurtosis
   DataFrame.mad
   DataFrame.max
   DataFrame.mean
   DataFrame.median
   DataFrame.min
   DataFrame.mode
   DataFrame.pct_change
   DataFrame.prod
   DataFrame.product
   DataFrame.quantile
   DataFrame.rank
   DataFrame.round
   DataFrame.sem
   DataFrame.skew
   DataFrame.sum
   DataFrame.std
   DataFrame.var
   DataFrame.nunique
   DataFrame.value_counts

Reindexing / selection / label manipulation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.add_prefix
   DataFrame.add_suffix
   DataFrame.align
   DataFrame.at_time
   DataFrame.between_time
   DataFrame.drop
   DataFrame.drop_duplicates
   DataFrame.duplicated
   DataFrame.equals
   DataFrame.filter
   DataFrame.first
   DataFrame.head
   DataFrame.idxmax
   DataFrame.idxmin
   DataFrame.last
   DataFrame.reindex
   DataFrame.reindex_like
   DataFrame.rename
   DataFrame.rename_axis
   DataFrame.reset_index
   DataFrame.sample
   DataFrame.set_axis
   DataFrame.set_index
   DataFrame.tail
   DataFrame.take
   DataFrame.truncate

.. _api.dataframe.missing:

Missing data handling
~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.backfill
   DataFrame.bfill
   DataFrame.dropna
   DataFrame.ffill
   DataFrame.fillna
   DataFrame.interpolate
   DataFrame.isna
   DataFrame.isnull
   DataFrame.notna
   DataFrame.notnull
   DataFrame.pad
   DataFrame.replace

Reshaping, sorting, transposing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.droplevel
   DataFrame.pivot
   DataFrame.pivot_table
   DataFrame.reorder_levels
   DataFrame.sort_values
   DataFrame.sort_index
   DataFrame.nlargest
   DataFrame.nsmallest
   DataFrame.swaplevel
   DataFrame.stack
   DataFrame.unstack
   DataFrame.swapaxes
   DataFrame.melt
   DataFrame.explode
   DataFrame.squeeze
   DataFrame.to_xarray
   DataFrame.T
   DataFrame.transpose

Combining / comparing / joining / merging
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.append
   DataFrame.assign
   DataFrame.compare
   DataFrame.join
   DataFrame.merge
   DataFrame.update

Time Series-related
~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.asfreq
   DataFrame.asof
   DataFrame.shift
   DataFrame.slice_shift
   DataFrame.tshift
   DataFrame.first_valid_index
   DataFrame.last_valid_index
   DataFrame.resample
   DataFrame.to_period
   DataFrame.to_timestamp
   DataFrame.tz_convert
   DataFrame.tz_localize

.. _api.frame.flags:

Flags
~~~~~

Flags refer to attributes of the pandas object. Properties of the dataset (like
the date is was recorded, the URL it was accessed from, etc.) should be stored
in :attr:`DataFrame.attrs`.

.. autosummary::
   :toctree: api/

   Flags


.. _api.frame.metadata:

Metadata
~~~~~~~~

:attr:`DataFrame.attrs` is a dictionary for storing global metadata for this DataFrame.

.. warning:: ``DataFrame.attrs`` is considered experimental and may change without warning.

.. autosummary::
   :toctree: api/

   DataFrame.attrs


.. _api.dataframe.plotting:

Plotting
~~~~~~~~
``DataFrame.plot`` is both a callable method and a namespace attribute for
specific plotting methods of the form ``DataFrame.plot.<kind>``.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_callable.rst

   DataFrame.plot

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   DataFrame.plot.area
   DataFrame.plot.bar
   DataFrame.plot.barh
   DataFrame.plot.box
   DataFrame.plot.density
   DataFrame.plot.hexbin
   DataFrame.plot.hist
   DataFrame.plot.kde
   DataFrame.plot.line
   DataFrame.plot.pie
   DataFrame.plot.scatter

.. autosummary::
   :toctree: api/

   DataFrame.boxplot
   DataFrame.hist


.. _api.frame.sparse:

Sparse accessor
~~~~~~~~~~~~~~~

Sparse-dtype specific methods and attributes are provided under the
``DataFrame.sparse`` accessor.

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_attribute.rst

   DataFrame.sparse.density

.. autosummary::
   :toctree: api/
   :template: autosummary/accessor_method.rst

   DataFrame.sparse.from_spmatrix
   DataFrame.sparse.to_coo
   DataFrame.sparse.to_dense


Serialization / IO / conversion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. autosummary::
   :toctree: api/

   DataFrame.from_dict
   DataFrame.from_records
   DataFrame.to_parquet
   DataFrame.to_pickle
   DataFrame.to_csv
   DataFrame.to_hdf
   DataFrame.to_sql
   DataFrame.to_dict
   DataFrame.to_excel
   DataFrame.to_json
   DataFrame.to_html
   DataFrame.to_feather
   DataFrame.to_latex
   DataFrame.to_stata
   DataFrame.to_gbq
   DataFrame.to_records
   DataFrame.to_string
   DataFrame.to_clipboard
   DataFrame.to_markdown
   DataFrame.style
.. _communitytutorials:

{{ header }}

*******************
Community tutorials
*******************

This is a guide to many pandas tutorials by the community, geared mainly for new users.

pandas cookbook by Julia Evans
------------------------------

The goal of this 2015 cookbook (by `Julia Evans <https://jvns.ca>`_) is to
give you some concrete examples for getting started with pandas. These
are examples with real-world data, and all the bugs and weirdness that
entails.
For the table of contents, see the `pandas-cookbook GitHub
repository <https://github.com/jvns/pandas-cookbook>`_.

pandas workshop by Stefanie Molin
---------------------------------

An introductory workshop by `Stefanie Molin <https://github.com/stefmolin>`_
designed to quickly get you up to speed with pandas using real-world datasets.
It covers getting started with pandas, data wrangling, and data visualization
(with some exposure to matplotlib and seaborn). The
`pandas-workshop GitHub repository <https://github.com/stefmolin/pandas-workshop>`_
features detailed environment setup instructions (including a Binder environment),
slides and notebooks for following along, and exercises to practice the concepts.
There is also a lab with new exercises on a dataset not covered in the workshop for
additional practice.

Learn pandas by Hernan Rojas
----------------------------

A set of lesson for new pandas users: https://bitbucket.org/hrojas/learn-pandas

Practical data analysis with Python
-----------------------------------

This `guide <https://wavedatalab.github.io/datawithpython>`_ is an introduction to the data analysis process using the Python data ecosystem and an interesting open dataset.
There are four sections covering selected topics as `munging data <https://wavedatalab.github.io/datawithpython/munge.html>`__,
`aggregating data <https://wavedatalab.github.io/datawithpython/aggregate.html>`_, `visualizing data <https://wavedatalab.github.io/datawithpython/visualize.html>`_
and `time series <https://wavedatalab.github.io/datawithpython/timeseries.html>`_.

.. _tutorial-exercises-new-users:

Exercises for new users
-----------------------
Practice your skills with real data sets and exercises.
For more resources, please visit the main `repository <https://github.com/guipsamora/pandas_exercises>`__.


.. _tutorial-modern:

Modern pandas
-------------

Tutorial series written in 2016 by
`Tom Augspurger <https://github.com/TomAugspurger>`_.
The source may be found in the GitHub repository
`TomAugspurger/effective-pandas <https://github.com/TomAugspurger/effective-pandas>`_.

* `Modern Pandas <https://tomaugspurger.github.io/modern-1-intro.html>`_
* `Method Chaining <https://tomaugspurger.github.io/method-chaining.html>`_
* `Indexes <https://tomaugspurger.github.io/modern-3-indexes.html>`_
* `Performance <https://tomaugspurger.github.io/modern-4-performance.html>`_
* `Tidy Data <https://tomaugspurger.github.io/modern-5-tidy.html>`_
* `Visualization <https://tomaugspurger.github.io/modern-6-visualization.html>`_
* `Timeseries <https://tomaugspurger.github.io/modern-7-timeseries.html>`_

Excel charts with pandas, vincent and xlsxwriter
------------------------------------------------

*  `Using Pandas and XlsxWriter to create Excel charts <https://pandas-xlsxwriter-charts.readthedocs.io/>`_

Video tutorials
---------------

* `Pandas From The Ground Up <https://www.youtube.com/watch?v=5JnMutdy6Fw>`_
  (2015) (2:24)
  `GitHub repo <https://github.com/brandon-rhodes/pycon-pandas-tutorial>`__
* `Introduction Into Pandas <https://www.youtube.com/watch?v=-NR-ynQg0YM>`_
  (2016) (1:28)
  `GitHub repo <https://github.com/chendaniely/2016-pydata-carolinas-pandas>`__
* `Pandas: .head() to .tail() <https://www.youtube.com/watch?v=7vuO9QXDN50>`_
  (2016) (1:26)
  `GitHub repo <https://github.com/TomAugspurger/pydata-chi-h2t>`__
* `Data analysis in Python with pandas <https://www.youtube.com/playlist?list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y>`_
  (2016-2018)
  `GitHub repo <https://github.com/justmarkham/pandas-videos>`__ and
  `Jupyter Notebook <https://nbviewer.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb>`__
* `Best practices with pandas <https://www.youtube.com/playlist?list=PL5-da3qGB5IBITZj_dYSFqnd_15JgqwA6>`_
  (2018)
  `GitHub repo <https://github.com/justmarkham/pycon-2018-tutorial>`__ and
  `Jupyter Notebook <https://nbviewer.org/github/justmarkham/pycon-2018-tutorial/blob/master/tutorial.ipynb>`__


Various tutorials
-----------------

* `Wes McKinney's (pandas BDFL) blog <https://wesmckinney.com/archives.html>`_
* `Statistical analysis made easy in Python with SciPy and pandas DataFrames, by Randal Olson <http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/>`_
* `Statistical Data Analysis in Python, tutorial videos, by Christopher Fonnesbeck from SciPy 2013 <https://conference.scipy.org/scipy2013/tutorial_detail.php?id=109>`_
* `Financial analysis in Python, by Thomas Wiecki <https://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb>`_
* `Intro to pandas data structures, by Greg Reda <http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/>`_
* `Pandas and Python: Top 10, by Manish Amde <https://manishamde.github.io/blog/2013/03/07/pandas-and-python-top-10/>`_
* `Pandas DataFrames Tutorial, by Karlijn Willems <https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python>`_
* `A concise tutorial with real life examples <https://tutswiki.com/pandas-cookbook/chapter1/>`_
.. _overview:

{{ header }}

****************
Package overview
****************

pandas is a `Python <https://www.python.org>`__ package providing fast,
flexible, and expressive data structures designed to make working with
"relational" or "labeled" data both easy and intuitive. It aims to be the
fundamental high-level building block for doing practical, **real-world** data
analysis in Python. Additionally, it has the broader goal of becoming **the
most powerful and flexible open source data analysis/manipulation tool
available in any language**. It is already well on its way toward this goal.

pandas is well suited for many different kinds of data:

  - Tabular data with heterogeneously-typed columns, as in an SQL table or
    Excel spreadsheet
  - Ordered and unordered (not necessarily fixed-frequency) time series data.
  - Arbitrary matrix data (homogeneously typed or heterogeneous) with row and
    column labels
  - Any other form of observational / statistical data sets. The data
    need not be labeled at all to be placed into a pandas data structure

The two primary data structures of pandas, :class:`Series` (1-dimensional)
and :class:`DataFrame` (2-dimensional), handle the vast majority of typical use
cases in finance, statistics, social science, and many areas of
engineering. For R users, :class:`DataFrame` provides everything that R's
``data.frame`` provides and much more. pandas is built on top of `NumPy
<https://numpy.org>`__ and is intended to integrate well within a scientific
computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:

  - Easy handling of **missing data** (represented as NaN) in floating point as
    well as non-floating point data
  - Size mutability: columns can be **inserted and deleted** from DataFrame and
    higher dimensional objects
  - Automatic and explicit **data alignment**: objects can be explicitly
    aligned to a set of labels, or the user can simply ignore the labels and
    let ``Series``, ``DataFrame``, etc. automatically align the data for you in
    computations
  - Powerful, flexible **group by** functionality to perform
    split-apply-combine operations on data sets, for both aggregating and
    transforming data
  - Make it **easy to convert** ragged, differently-indexed data in other
    Python and NumPy data structures into DataFrame objects
  - Intelligent label-based **slicing**, **fancy indexing**, and **subsetting**
    of large data sets
  - Intuitive **merging** and **joining** data sets
  - Flexible **reshaping** and pivoting of data sets
  - **Hierarchical** labeling of axes (possible to have multiple labels per
    tick)
  - Robust IO tools for loading data from **flat files** (CSV and delimited),
    Excel files, databases, and saving / loading data from the ultrafast **HDF5
    format**
  - **Time series**-specific functionality: date range generation and frequency
    conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently
experienced using other languages / scientific research environments. For data
scientists, working with data is typically divided into multiple stages:
munging and cleaning data, analyzing / modeling it, then organizing the results
of the analysis into a form suitable for plotting or tabular display. pandas
is the ideal tool for all of these tasks.

Some other notes

 - pandas is **fast**. Many of the low-level algorithmic bits have been
   extensively tweaked in `Cython <https://cython.org>`__ code. However, as with
   anything else generalization usually sacrifices performance. So if you focus
   on one feature for your application you may be able to create a faster
   specialized tool.

 - pandas is a dependency of `statsmodels
   <https://www.statsmodels.org/>`__, making it an important part of the
   statistical computing ecosystem in Python.

 - pandas has been used extensively in production in financial applications.

Data structures
---------------

.. csv-table::
    :header: "Dimensions", "Name", "Description"
    :widths: 15, 20, 50

    1, "Series", "1D labeled homogeneously-typed array"
    2, "DataFrame", "General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column"

Why more than one data structure?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The best way to think about the pandas data structures is as flexible
containers for lower dimensional data. For example, DataFrame is a container
for Series, and Series is a container for scalars. We would like to be
able to insert and remove objects from these containers in a dictionary-like
fashion.

Also, we would like sensible default behaviors for the common API functions
which take into account the typical orientation of time series and
cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional
data, a burden is placed on the user to consider the orientation of the data
set when writing functions; axes are considered more or less equivalent (except
when C- or Fortran-contiguousness matters for performance). In pandas, the axes
are intended to lend more semantic meaning to the data; i.e., for a particular
data set, there is likely to be a "right" way to orient the data. The goal,
then, is to reduce the amount of mental effort required to code up data
transformations in downstream functions.

For example, with tabular data (DataFrame) it is more semantically helpful to
think of the **index** (the rows) and the **columns** rather than axis 0 and
axis 1. Iterating through the columns of the DataFrame thus results in more
readable code:

::

    for col in df.columns:
        series = df[col]
        # do something with series

Mutability and copying of data
------------------------------

All pandas data structures are value-mutable (the values they contain can be
altered) but not always size-mutable. The length of a Series cannot be
changed, but, for example, columns can be inserted into a DataFrame. However,
the vast majority of methods produce new objects and leave the input data
untouched. In general we like to **favor immutability** where sensible.

Getting support
---------------

The first stop for pandas issues and ideas is the `Github Issue Tracker
<https://github.com/pandas-dev/pandas/issues>`__. If you have a general question,
pandas community experts can answer through `Stack Overflow
<https://stackoverflow.com/questions/tagged/pandas>`__.

Community
---------

pandas is actively supported today by a community of like-minded individuals around
the world who contribute their valuable time and energy to help make open source
pandas possible. Thanks to `all of our contributors <https://github.com/pandas-dev/pandas/graphs/contributors>`__.

If you're interested in contributing, please visit the :ref:`contributing guide <contributing>`.

pandas is a `NumFOCUS <https://numfocus.org/sponsored-projects>`__ sponsored project.
This will help ensure the success of the development of pandas as a world-class open-source
project and makes it possible to `donate <https://pandas.pydata.org/donate.html>`__ to the project.

Project governance
------------------

The governance process that pandas project has used informally since its inception in 2008 is formalized in `Project Governance documents <https://github.com/pandas-dev/pandas-governance>`__.
The documents clarify how decisions are made and how the various elements of our community interact, including the relationship between open source collaborative development and work that may be funded by for-profit or non-profit entities.

Wes McKinney is the Benevolent Dictator for Life (BDFL).

Development team
-----------------

The list of the Core Team members and more detailed information can be found on the `people’s page <https://github.com/pandas-dev/pandas-governance/blob/master/people.md>`__ of the governance repo.


Institutional partners
----------------------

The information about current institutional partners can be found on `pandas website page <https://pandas.pydata.org/about/sponsors.html>`__.

License
-------

.. literalinclude:: ../../../LICENSE
{{ header }}

.. _getting_started:

===============
Getting started
===============

Installation
------------

.. panels::
    :card: + install-card
    :column: col-lg-6 col-md-6 col-sm-12 col-xs-12 p-3

    Working with conda?
    ^^^^^^^^^^^^^^^^^^^

    pandas is part of the `Anaconda <https://docs.continuum.io/anaconda/>`__
    distribution and can be installed with Anaconda or Miniconda:

    ++++++++++++++++++++++

    .. code-block:: bash

        conda install pandas

    ---

    Prefer pip?
    ^^^^^^^^^^^

    pandas can be installed via pip from `PyPI <https://pypi.org/project/pandas>`__.

    ++++

    .. code-block:: bash

        pip install pandas

    ---
    :column: col-12 p-3

    In-depth instructions?
    ^^^^^^^^^^^^^^^^^^^^^^

    Installing a specific version? Installing from source? Check the advanced
    installation page.

    .. link-button:: ./install.html
        :type: url
        :text: Learn more
        :classes: btn-secondary stretched-link


.. _gentle_intro:

Intro to pandas
---------------

.. raw:: html

    <div class="container">
    <div id="accordion" class="shadow tutorial-accordion">

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseOne">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        What kind of data does pandas handle?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_01_tableoriented>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseOne" class="collapse" data-parent="#accordion">
                <div class="card-body">

When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you
to explore, clean, and process your data. In pandas, a data table is called a :class:`DataFrame`.

.. image:: ../_static/schemas/01_table_dataframe.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_01_tableoriented>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <dsintro>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseTwo">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How do I read and write tabular data?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_02_read_write>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseTwo" class="collapse" data-parent="#accordion">
                <div class="card-body">

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,…). Importing data from each of these
data sources is provided by function with the prefix ``read_*``. Similarly, the ``to_*`` methods are used to store data.

.. image:: ../_static/schemas/02_io_readwrite.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_02_read_write>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <io>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseThree">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How do I select a subset of a table?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_03_subset>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseThree" class="collapse" data-parent="#accordion">
                <div class="card-body">

Selecting or filtering specific rows and/or columns? Filtering the data on a condition? Methods for slicing, selecting, and extracting the
data you need are available in pandas.

.. image:: ../_static/schemas/03_subset_columns_rows.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_03_subset>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <indexing>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseFour">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to create plots in pandas?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_04_plotting>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseFour" class="collapse" data-parent="#accordion">
                <div class="card-body">

pandas provides plotting your data out of the box, using the power of Matplotlib. You can pick the plot type (scatter, bar, boxplot,...)
corresponding to your data.

.. image:: ../_static/schemas/04_plot_overview.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_04_plotting>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <visualization>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseFive">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to create new columns derived from existing columns?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_05_columns>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseFive" class="collapse" data-parent="#accordion">
                <div class="card-body">

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise.
Adding a column to a :class:`DataFrame` based on existing data in other columns is straightforward.

.. image:: ../_static/schemas/05_newcolumn_2.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_05_columns>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <basics.dataframe.sel_add_del>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseSix">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to calculate summary statistics?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_06_stats>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseSix" class="collapse" data-parent="#accordion">
                <div class="card-body">

Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire
data set, a sliding window of the data, or grouped by categories. The latter is also known as the split-apply-combine approach.

.. image:: ../_static/schemas/06_groupby.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_06_stats>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <groupby>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseSeven">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to reshape the layout of tables?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_07_reshape>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseSeven" class="collapse" data-parent="#accordion">
                <div class="card-body">

Change the structure of your data table in multiple ways. You can :func:`~pandas.melt` your data table from wide to long/tidy form or :func:`~pandas.pivot`
from long to wide format. With aggregations built-in, a pivot table is created with a single command.

.. image:: ../_static/schemas/07_melt.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_07_reshape>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <reshaping>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseEight">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to combine data from multiple tables?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_08_combine>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseEight" class="collapse" data-parent="#accordion">
                <div class="card-body">

Multiple tables can be concatenated both column wise and row wise as database-like join/merge operations are provided to combine multiple tables of data.

.. image:: ../_static/schemas/08_concat_row.svg
   :align: center

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_08_combine>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <merging>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseNine">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to handle time series data?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_09_timeseries>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseNine" class="collapse" data-parent="#accordion">
                <div class="card-body">

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_09_timeseries>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <timeseries>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

        <div class="card tutorial-card">
            <div class="card-header collapsed card-link" data-toggle="collapse" data-target="#collapseTen">
                <div class="d-flex flex-row tutorial-card-header-1">
                    <div class="d-flex flex-row tutorial-card-header-2">
                        <button class="btn btn-dark btn-sm"></button>
                        How to manipulate textual data?
                    </div>
                    <span class="badge gs-badge-link">

:ref:`Straight to tutorial...<10min_tut_10_text>`

.. raw:: html

                    </span>
                </div>
            </div>
            <div id="collapseTen" class="collapse" data-parent="#accordion">
                <div class="card-body">

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

.. raw:: html

                    <div class="d-flex flex-row">
                        <span class="badge gs-badge-link">

:ref:`To introduction tutorial <10min_tut_10_text>`

.. raw:: html

                        </span>
                        <span class="badge gs-badge-link">

:ref:`To user guide <text>`

.. raw:: html

                        </span>
                    </div>
                </div>
            </div>
        </div>

    </div>
    </div>


.. _comingfrom:

Coming from...
--------------

Are you familiar with other software for manipulating tablular data? Learn
the pandas-equivalent operations compared to software you already know:

.. panels::
    :card: + comparison-card text-center shadow
    :column: col-lg-6 col-md-6 col-sm-6 col-xs-12 d-flex

    ---
    :card: + comparison-card-r
    :img-top: ../_static/logo_r.svg

    The `R programming language <https://www.r-project.org/>`__ provides the
    ``data.frame`` data structure and multiple packages, such as
    `tidyverse <https://www.tidyverse.org>`__ use and extend ``data.frame``
    for convenient data handling functionalities similar to pandas.

    +++

    .. link-button:: compare_with_r
        :type: ref
        :text: Learn more
        :classes: btn-secondary stretched-link


    ---
    :card: + comparison-card-sql
    :img-top: ../_static/logo_sql.svg

    Already familiar to ``SELECT``, ``GROUP BY``, ``JOIN``, etc.?
    Most of these SQL manipulations do have equivalents in pandas.

    +++

    .. link-button:: compare_with_sql
        :type: ref
        :text: Learn more
        :classes: btn-secondary stretched-link


    ---
    :card: + comparison-card-stata
    :img-top: ../_static/logo_stata.svg

    The ``data set`` included in the `STATA <https://en.wikipedia.org/wiki/Stata>`__
    statistical software suite corresponds to the pandas ``DataFrame``.
    Many of the operations known from STATA have an equivalent in pandas.

    +++

    .. link-button:: compare_with_stata
        :type: ref
        :text: Learn more
        :classes: btn-secondary stretched-link


    ---
    :card: + comparison-card-excel
    :img-top: ../_static/spreadsheets/logo_excel.svg

    Users of `Excel <https://en.wikipedia.org/wiki/Microsoft_Excel>`__
    or other spreadsheet programs will find that many of the concepts are
    transferrable to pandas.

    +++

    .. link-button:: compare_with_spreadsheets
        :type: ref
        :text: Learn more
        :classes: btn-secondary stretched-link


    ---
    :card: + comparison-card-sas
    :img-top: ../_static/logo_sas.svg

    The `SAS <https://en.wikipedia.org/wiki/SAS_(software)>`__ statistical software suite
    also provides the ``data set`` corresponding to the pandas ``DataFrame``.
    Also SAS vectorized operations, filtering, string processing operations,
    and more have similar functions in pandas.

    +++

    .. link-button:: compare_with_sas
        :type: ref
        :text: Learn more
        :classes: btn-secondary stretched-link


Tutorials
---------

For a quick overview of pandas functionality, see :ref:`10 Minutes to pandas<10min>`.

You can also reference the pandas `cheat sheet <https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf>`_
for a succinct guide for manipulating data with pandas.

The community produces a wide variety of tutorials available online. Some of the
material is enlisted in the community contributed :ref:`communitytutorials`.


.. If you update this toctree, also update the manual toctree in the
   main index.rst.template

.. toctree::
    :maxdepth: 2
    :hidden:

    install
    overview
    intro_tutorials/index
    comparison/index
    tutorials
.. _install:

{{ header }}

============
Installation
============

The easiest way to install pandas is to install it
as part of the `Anaconda <https://docs.continuum.io/anaconda/>`__ distribution, a
cross platform distribution for data analysis and scientific computing.
This is the recommended installation method for most users.

Instructions for installing from source,
`PyPI <https://pypi.org/project/pandas>`__, `ActivePython <https://www.activestate.com/products/python/>`__, various Linux distributions, or a
`development version <https://github.com/pandas-dev/pandas>`__ are also provided.

.. _install.version:

Python version support
----------------------

Officially Python 3.8, and 3.9.

Installing pandas
-----------------

.. _install.anaconda:

Installing with Anaconda
~~~~~~~~~~~~~~~~~~~~~~~~

Installing pandas and the rest of the `NumPy <https://numpy.org/>`__ and
`SciPy <https://scipy.org/>`__ stack can be a little
difficult for inexperienced users.

The simplest way to install not only pandas, but Python and the most popular
packages that make up the `SciPy <https://scipy.org/>`__ stack
(`IPython <https://ipython.org/>`__, `NumPy <https://numpy.org/>`__,
`Matplotlib <https://matplotlib.org/>`__, ...) is with
`Anaconda <https://docs.continuum.io/anaconda/>`__, a cross-platform
(Linux, macOS, Windows) Python distribution for data analytics and
scientific computing.

After running the installer, the user will have access to pandas and the
rest of the `SciPy <https://scipy.org/>`__ stack without needing to install
anything else, and without needing to wait for any software to be compiled.

Installation instructions for `Anaconda <https://docs.continuum.io/anaconda/>`__
`can be found here <https://docs.continuum.io/anaconda/install/>`__.

A full list of the packages available as part of the
`Anaconda <https://docs.continuum.io/anaconda/>`__ distribution
`can be found here <https://docs.continuum.io/anaconda/packages/pkg-docs/>`__.

Another advantage to installing Anaconda is that you don't need
admin rights to install it. Anaconda can install in the user's home directory,
which makes it trivial to delete Anaconda if you decide (just delete
that folder).

.. _install.miniconda:

Installing with Miniconda
~~~~~~~~~~~~~~~~~~~~~~~~~

The previous section outlined how to get pandas installed as part of the
`Anaconda <https://docs.continuum.io/anaconda/>`__ distribution.
However this approach means you will install well over one hundred packages
and involves downloading the installer which is a few hundred megabytes in size.

If you want to have more control on which packages, or have a limited internet
bandwidth, then installing pandas with
`Miniconda <https://docs.conda.io/en/latest/miniconda.html>`__ may be a better solution.

`Conda <https://conda.io/en/latest/>`__ is the package manager that the
`Anaconda <https://docs.continuum.io/anaconda/>`__ distribution is built upon.
It is a package manager that is both cross-platform and language agnostic
(it can play a similar role to a pip and virtualenv combination).

`Miniconda <https://conda.pydata.org/miniconda.html>`__ allows you to create a
minimal self contained Python installation, and then use the
`Conda <https://conda.io/en/latest/>`__ command to install additional packages.

First you will need `Conda <https://conda.io/en/latest/>`__ to be installed and
downloading and running the `Miniconda
<https://conda.pydata.org/miniconda.html>`__
will do this for you. The installer
`can be found here <https://conda.pydata.org/miniconda.html>`__

The next step is to create a new conda environment. A conda environment is like a
virtualenv that allows you to specify a specific version of Python and set of libraries.
Run the following commands from a terminal window::

    conda create -n name_of_my_env python

This will create a minimal environment with only Python installed in it.
To put your self inside this environment run::

    source activate name_of_my_env

On Windows the command is::

    activate name_of_my_env

The final step required is to install pandas. This can be done with the
following command::

    conda install pandas

To install a specific pandas version::

    conda install pandas=0.20.3

To install other packages, IPython for example::

    conda install ipython

To install the full `Anaconda <https://docs.continuum.io/anaconda/>`__
distribution::

    conda install anaconda

If you need packages that are available to pip but not conda, then
install pip, and then use pip to install those packages::

    conda install pip
    pip install django

Installing from PyPI
~~~~~~~~~~~~~~~~~~~~

pandas can be installed via pip from
`PyPI <https://pypi.org/project/pandas>`__.

.. note::
    You must have ``pip>=19.3`` to install from PyPI.

::

    pip install pandas

Installing with ActivePython
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Installation instructions for
`ActivePython <https://www.activestate.com/products/python/>`__ can be found
`here <https://www.activestate.com/products/python/>`__. Versions
2.7, 3.5 and 3.6 include pandas.

Installing using your Linux distribution's package manager.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The commands in this table will install pandas for Python 3 from your distribution.

.. csv-table::
    :header: "Distribution", "Status", "Download / Repository Link", "Install method"
    :widths: 10, 10, 20, 50


    Debian, stable, `official Debian repository <https://packages.debian.org/search?keywords=pandas&searchon=names&suite=all&section=all>`__ , ``sudo apt-get install python3-pandas``
    Debian & Ubuntu, unstable (latest packages), `NeuroDebian <https://neuro.debian.net/index.html#how-to-use-this-repository>`__ , ``sudo apt-get install python3-pandas``
    Ubuntu, stable, `official Ubuntu repository <https://packages.ubuntu.com/search?keywords=pandas&searchon=names&suite=all&section=all>`__ , ``sudo apt-get install python3-pandas``
    OpenSuse, stable, `OpenSuse Repository  <https://software.opensuse.org/package/python-pandas?search_term=pandas>`__ , ``zypper in python3-pandas``
    Fedora, stable, `official Fedora repository  <https://src.fedoraproject.org/rpms/python-pandas>`__ , ``dnf install python3-pandas``
    Centos/RHEL, stable, `EPEL repository <https://admin.fedoraproject.org/pkgdb/package/rpms/python-pandas/>`__ , ``yum install python3-pandas``

**However**, the packages in the linux package managers are often a few versions behind, so
to get the newest version of pandas, it's recommended to install using the ``pip`` or ``conda``
methods described above.

Handling ImportErrors
~~~~~~~~~~~~~~~~~~~~~~

If you encounter an ImportError, it usually means that Python couldn't find pandas in the list of available
libraries. Python internally has a list of directories it searches through, to find packages. You can
obtain these directories with::

            import sys
            sys.path

One way you could be encountering this error is if you have multiple Python installations on your system
and you don't have pandas installed in the Python installation you're currently using.
In Linux/Mac you can run ``which python`` on your terminal and it will tell you which Python installation you're
using. If it's something like "/usr/bin/python", you're using the Python from the system, which is not recommended.

It is highly recommended to use ``conda``, for quick installation and for package and dependency updates.
You can find simple installation instructions for pandas in this document: ``installation instructions </getting_started.html>``.

Installing from source
~~~~~~~~~~~~~~~~~~~~~~

See the :ref:`contributing guide <contributing>` for complete instructions on building from the git source tree. Further, see :ref:`creating a development environment <contributing_environment>` if you wish to create a pandas development environment.

Running the test suite
----------------------

pandas is equipped with an exhaustive set of unit tests, covering about 97% of
the code base as of this writing. To run it on your machine to verify that
everything is working (and that you have all of the dependencies, soft and hard,
installed), make sure you have `pytest
<https://docs.pytest.org/en/latest/>`__ >= 6.0 and `Hypothesis
<https://hypothesis.readthedocs.io/en/latest/>`__ >= 3.58, then run:

::

    >>> pd.test()
    running: pytest --skip-slow --skip-network C:\Users\TP\Anaconda3\envs\py36\lib\site-packages\pandas
    ============================= test session starts =============================
    platform win32 -- Python 3.6.2, pytest-3.6.0, py-1.4.34, pluggy-0.4.0
    rootdir: C:\Users\TP\Documents\Python\pandasdev\pandas, inifile: setup.cfg
    collected 12145 items / 3 skipped

    ..................................................................S......
    ........S................................................................
    .........................................................................

    ==================== 12130 passed, 12 skipped in 368.339 seconds =====================

.. _install.dependencies:

Dependencies
------------

================================================================ ==========================
Package                                                          Minimum supported version
================================================================ ==========================
`NumPy <https://numpy.org>`__                                    1.18.5
`python-dateutil <https://dateutil.readthedocs.io/en/stable/>`__ 2.8.1
`pytz <https://pypi.org/project/pytz/>`__                        2020.1
================================================================ ==========================

.. _install.recommended_dependencies:

Recommended dependencies
~~~~~~~~~~~~~~~~~~~~~~~~

* `numexpr <https://github.com/pydata/numexpr>`__: for accelerating certain numerical operations.
  ``numexpr`` uses multiple cores as well as smart chunking and caching to achieve large speedups.
  If installed, must be Version 2.7.1 or higher.

* `bottleneck <https://github.com/pydata/bottleneck>`__: for accelerating certain types of ``nan``
  evaluations. ``bottleneck`` uses specialized cython routines to achieve large speedups. If installed,
  must be Version 1.3.1 or higher.

.. note::

   You are highly encouraged to install these libraries, as they provide speed improvements, especially
   when working with large data sets.


.. _install.optional_dependencies:

Optional dependencies
~~~~~~~~~~~~~~~~~~~~~

pandas has many optional dependencies that are only used for specific methods.
For example, :func:`pandas.read_hdf` requires the ``pytables`` package, while
:meth:`DataFrame.to_markdown` requires the ``tabulate`` package. If the
optional dependency is not installed, pandas will raise an ``ImportError`` when
the method requiring that dependency is called.

Visualization
^^^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
matplotlib                3.3.2              Plotting library
Jinja2                    2.11               Conditional formatting with DataFrame.style
tabulate                  0.8.7              Printing in Markdown-friendly format (see `tabulate`_)
========================= ================== =============================================================

Computation
^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
SciPy                     1.14.1             Miscellaneous statistical functions
numba                     0.50.1             Alternative execution engine for rolling operations
                                             (see :ref:`Enhancing Performance <enhancingperf.numba>`)
xarray                    0.15.1             pandas-like API for N-dimensional data
========================= ================== =============================================================

Excel files
^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
xlrd                      2.0.1              Reading Excel
xlwt                      1.3.0              Writing Excel
xlsxwriter                1.2.2              Writing Excel
openpyxl                  3.0.3              Reading / writing for xlsx files
pyxlsb                    1.0.6              Reading for xlsb files
========================= ================== =============================================================

HTML
^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
BeautifulSoup4            4.8.2              HTML parser for read_html
html5lib                  1.1                HTML parser for read_html
lxml                      4.5.0              HTML parser for read_html
========================= ================== =============================================================

One of the following combinations of libraries is needed to use the
top-level :func:`~pandas.read_html` function:

* `BeautifulSoup4`_ and `html5lib`_
* `BeautifulSoup4`_ and `lxml`_
* `BeautifulSoup4`_ and `html5lib`_ and `lxml`_
* Only `lxml`_, although see :ref:`HTML Table Parsing <io.html.gotchas>`
  for reasons as to why you should probably **not** take this approach.

.. warning::

    * if you install `BeautifulSoup4`_ you must install either
      `lxml`_ or `html5lib`_ or both.
      :func:`~pandas.read_html` will **not** work with *only*
      `BeautifulSoup4`_ installed.
    * You are highly encouraged to read :ref:`HTML Table Parsing gotchas <io.html.gotchas>`.
      It explains issues surrounding the installation and
      usage of the above three libraries.

.. _html5lib: https://github.com/html5lib/html5lib-python
.. _BeautifulSoup4: https://www.crummy.com/software/BeautifulSoup
.. _lxml: https://lxml.de
.. _tabulate: https://github.com/astanin/python-tabulate

XML
^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
lxml                      4.5.0              XML parser for read_xml and tree builder for to_xml
========================= ================== =============================================================

SQL databases
^^^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
SQLAlchemy                1.4.0               SQL support for databases other than sqlite
psycopg2                  2.8.4               PostgreSQL engine for sqlalchemy
pymysql                   0.10.1              MySQL engine for sqlalchemy
========================= ================== =============================================================

Other data sources
^^^^^^^^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
PyTables                  3.6.1              HDF5-based reading / writing
blosc                     1.20.1             Compression for HDF5
zlib                                         Compression for HDF5
fastparquet               0.4.0              Parquet reading / writing
pyarrow                   1.0.1              Parquet, ORC, and feather reading / writing
pyreadstat                1.1.0              SPSS files (.sav) reading
========================= ================== =============================================================

.. _install.warn_orc:

.. warning::

    * If you want to use :func:`~pandas.read_orc`, it is highly recommended to install pyarrow using conda.
      The following is a summary of the environment in which :func:`~pandas.read_orc` can work.

      ========================= ================== =============================================================
      System                    Conda              PyPI
      ========================= ================== =============================================================
      Linux                     Successful         Failed(pyarrow==3.0 Successful)
      macOS                     Successful         Failed
      Windows                   Failed             Failed
      ========================= ================== =============================================================

Access data in the cloud
^^^^^^^^^^^^^^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
fsspec                    0.7.4              Handling files aside from simple local and HTTP
gcsfs                     0.6.0              Google Cloud Storage access
pandas-gbq                0.14.0             Google Big Query access
s3fs                      0.4.0              Amazon S3 access
========================= ================== =============================================================

Clipboard
^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
PyQt4/PyQt5                                  Clipboard I/O
qtpy                                         Clipboard I/O
xclip                                        Clipboard I/O on linux
xsel                                         Clipboard I/O on linux
========================= ================== =============================================================


Compression
^^^^^^^^^^^

========================= ================== =============================================================
Dependency                Minimum Version    Notes
========================= ================== =============================================================
Zstandard                                    Zstandard compression
========================= ================== =============================================================
.. _10min_tut_01_tableoriented:

{{ header }}

What kind of data does pandas handle?
=====================================

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to start using pandas

.. ipython:: python

    import pandas as pd

To load the pandas package and start working with it, import the
package. The community agreed alias for pandas is ``pd``, so loading
pandas as ``pd`` is assumed standard practice for all of the pandas
documentation.

.. raw:: html

        </li>
    </ul>

pandas data table representation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/01_table_dataframe.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

.. ipython:: python

    df = pd.DataFrame(
        {
            "Name": [
                "Braund, Mr. Owen Harris",
                "Allen, Mr. William Henry",
                "Bonnell, Miss. Elizabeth",
            ],
            "Age": [22, 35, 58],
            "Sex": ["male", "male", "female"],
        }
    )
    df

To manually store data in a table, create a ``DataFrame``. When using a Python dictionary of lists, the dictionary keys will be used as column headers and
the values in each list as columns of the ``DataFrame``.

.. raw:: html

        </li>
    </ul>

A :class:`DataFrame` is a 2-dimensional data structure that can store data of
different types (including characters, integers, floating point values,
categorical data and more) in columns. It is similar to a spreadsheet, a
SQL table or the ``data.frame`` in R.

-  The table has 3 columns, each of them with a column label. The column
   labels are respectively ``Name``, ``Age`` and ``Sex``.
-  The column ``Name`` consists of textual data with each value a
   string, the column ``Age`` are numbers and the column ``Sex`` is
   textual data.

In spreadsheet software, the table representation of our data would look
very similar:

.. image:: ../../_static/schemas/01_table_spreadsheet.png
   :align: center

Each column in a ``DataFrame`` is a ``Series``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/01_table_series.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m just interested in working with the data in the column ``Age``

.. ipython:: python

    df["Age"]

When selecting a single column of a pandas :class:`DataFrame`, the result is
a pandas :class:`Series`. To select the column, use the column label in
between square brackets ``[]``.

.. raw:: html

        </li>
    </ul>

.. note::
    If you are familiar to Python
    :ref:`dictionaries <python:tut-dictionaries>`, the selection of a
    single column is very similar to selection of dictionary values based on
    the key.

You can create a ``Series`` from scratch as well:

.. ipython:: python

    ages = pd.Series([22, 35, 58], name="Age")
    ages

A pandas ``Series`` has no column labels, as it is just a single column
of a ``DataFrame``. A Series does have row labels.

Do something with a DataFrame or Series
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to know the maximum Age of the passengers

We can do this on the ``DataFrame`` by selecting the ``Age`` column and
applying ``max()``:

.. ipython:: python

    df["Age"].max()

Or to the ``Series``:

.. ipython:: python

    ages.max()

.. raw:: html

        </li>
    </ul>

As illustrated by the ``max()`` method, you can *do* things with a
``DataFrame`` or ``Series``. pandas provides a lot of functionalities,
each of them a *method* you can apply to a ``DataFrame`` or ``Series``.
As methods are functions, do not forget to use parentheses ``()``.

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in some basic statistics of the numerical data of my data table

.. ipython:: python

    df.describe()

The :func:`~DataFrame.describe` method provides a quick overview of the numerical data in
a ``DataFrame``. As the ``Name`` and ``Sex`` columns are textual data,
these are by default not taken into account by the :func:`~DataFrame.describe` method.

.. raw:: html

        </li>
    </ul>

Many pandas operations return a ``DataFrame`` or a ``Series``. The
:func:`~DataFrame.describe` method is an example of a pandas operation returning a
pandas ``Series`` or a pandas ``DataFrame``.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Check more options on ``describe`` in the user guide section about :ref:`aggregations with describe <basics.describe>`

.. raw:: html

    </div>

.. note::
    This is just a starting point. Similar to spreadsheet
    software, pandas represents data as a table with columns and rows. Apart
    from the representation, also the data manipulations and calculations
    you would do in spreadsheet software are supported by pandas. Continue
    reading the next tutorials to get started!

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Import the package, aka ``import pandas as pd``
-  A table of data is stored as a pandas ``DataFrame``
-  Each column in a ``DataFrame`` is a ``Series``
-  You can do things by applying a method to a ``DataFrame`` or ``Series``

.. raw:: html

    </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A more extended explanation to ``DataFrame`` and ``Series`` is provided in the :ref:`introduction to data structures <dsintro>`.

.. raw:: html

    </div>
.. _10min_tut_05_columns:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/air_quality_no2.rst

.. ipython:: python

    air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
    air_quality.head()

.. raw:: html

        </li>
    </ul>
    </div>

How to create new columns derived from existing columns?
--------------------------------------------------------

.. image:: ../../_static/schemas/05_newcolumn_1.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to express the :math:`NO_2` concentration of the station in London in mg/m\ :math:`^3`

(*If we assume temperature of 25 degrees Celsius and pressure of 1013
hPa, the conversion factor is 1.882*)

.. ipython:: python

    air_quality["london_mg_per_cubic"] = air_quality["station_london"] * 1.882
    air_quality.head()

To create a new column, use the ``[]`` brackets with the new column name
at the left side of the assignment.

.. raw:: html

        </li>
    </ul>

.. note::
    The calculation of the values is done **element_wise**. This
    means all values in the given column are multiplied by the value 1.882
    at once. You do not need to use a loop to iterate each of the rows!

.. image:: ../../_static/schemas/05_newcolumn_2.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to check the ratio of the values in Paris versus Antwerp and save the result in a new column

.. ipython:: python

    air_quality["ratio_paris_antwerp"] = (
        air_quality["station_paris"] / air_quality["station_antwerp"]
    )
    air_quality.head()

The calculation is again element-wise, so the ``/`` is applied *for the
values in each row*.

.. raw:: html

        </li>
    </ul>

Also other mathematical operators (``+``, ``-``, ``\*``, ``/``) or
logical operators (``<``, ``>``, ``=``,…) work element wise. The latter was already
used in the :ref:`subset data tutorial <10min_tut_03_subset>` to filter
rows of a table using a conditional expression.

If you need more advanced logic, you can use arbitrary Python code via :meth:`~DataFrame.apply`.

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to rename the data columns to the corresponding station identifiers used by openAQ

.. ipython:: python

    air_quality_renamed = air_quality.rename(
        columns={
            "station_antwerp": "BETR801",
            "station_paris": "FR04014",
            "station_london": "London Westminster",
        }
    )

.. ipython:: python

    air_quality_renamed.head()

The :meth:`~DataFrame.rename` function can be used for both row labels and column
labels. Provide a dictionary with the keys the current names and the
values the new names to update the corresponding names.

.. raw:: html

        </li>
    </ul>

The mapping should not be restricted to fixed names only, but can be a
mapping function as well. For example, converting the column names to
lowercase letters can be done using a function as well:

.. ipython:: python

    air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
    air_quality_renamed.head()

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Details about column or row label renaming is provided in the user guide section on :ref:`renaming labels <basics.rename>`.

.. raw:: html

   </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Create a new column by assigning the output to the DataFrame with a
   new column name in between the ``[]``.
-  Operations are element-wise, no need to loop over rows.
-  Use ``rename`` with a dictionary or function to rename row labels or
   column names.

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

The user guide contains a separate section on :ref:`column addition and deletion <basics.dataframe.sel_add_del>`.

.. raw:: html

   </div>
.. _10min_tut_06_stats:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/titanic.rst

.. ipython:: python

    titanic = pd.read_csv("data/titanic.csv")
    titanic.head()

.. raw:: html

            </li>
        </ul>
    </div>

How to calculate summary statistics?
------------------------------------

Aggregating statistics
~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/06_aggregate.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the average age of the Titanic passengers?

.. ipython:: python

    titanic["Age"].mean()

.. raw:: html

        </li>
    </ul>

Different statistics are available and can be applied to columns with
numerical data. Operations in general exclude missing data and operate
across rows by default.

.. image:: ../../_static/schemas/06_reduction.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the median age and ticket fare price of the Titanic passengers?

.. ipython:: python

    titanic[["Age", "Fare"]].median()

The statistic applied to multiple columns of a ``DataFrame`` (the selection of two columns
return a ``DataFrame``, see the :ref:`subset data tutorial <10min_tut_03_subset>`) is calculated for each numeric column.

.. raw:: html

        </li>
    </ul>

The aggregating statistic can be calculated for multiple columns at the
same time. Remember the ``describe`` function from :ref:`first tutorial <10min_tut_01_tableoriented>`?

.. ipython:: python

    titanic[["Age", "Fare"]].describe()

Instead of the predefined statistics, specific combinations of
aggregating statistics for given columns can be defined using the
:func:`DataFrame.agg` method:

.. ipython:: python

    titanic.agg(
        {
            "Age": ["min", "max", "median", "skew"],
            "Fare": ["min", "max", "median", "mean"],
        }
    )

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Details about descriptive statistics are provided in the user guide section on :ref:`descriptive statistics <basics.stats>`.

.. raw:: html

   </div>


Aggregating statistics grouped by category
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/06_groupby.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the average age for male versus female Titanic passengers?

.. ipython:: python

    titanic[["Sex", "Age"]].groupby("Sex").mean()

As our interest is the average age for each gender, a subselection on
these two columns is made first: ``titanic[["Sex", "Age"]]``. Next, the
:meth:`~DataFrame.groupby` method is applied on the ``Sex`` column to make a group per
category. The average age *for each gender* is calculated and
returned.

.. raw:: html

        </li>
    </ul>

Calculating a given statistic (e.g. ``mean`` age) *for each category in
a column* (e.g. male/female in the ``Sex`` column) is a common pattern.
The ``groupby`` method is used to support this type of operations. More
general, this fits in the more general ``split-apply-combine`` pattern:

-  **Split** the data into groups
-  **Apply** a function to each group independently
-  **Combine** the results into a data structure

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If
not, the ``mean`` method is applied to each column containing numerical
columns:

.. ipython:: python

    titanic.groupby("Sex").mean()

It does not make much sense to get the average value of the ``Pclass``.
if we are only interested in the average age for each gender, the
selection of columns (rectangular brackets ``[]`` as usual) is supported
on the grouped data as well:

.. ipython:: python

    titanic.groupby("Sex")["Age"].mean()

.. image:: ../../_static/schemas/06_groupby_select_detail.svg
   :align: center

.. note::
    The ``Pclass`` column contains numerical data but actually
    represents 3 categories (or factors) with respectively the labels ‘1’,
    ‘2’ and ‘3’. Calculating statistics on these does not make much sense.
    Therefore, pandas provides a ``Categorical`` data type to handle this
    type of data. More information is provided in the user guide
    :ref:`categorical` section.

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the mean ticket fare price for each of the sex and cabin class combinations?

.. ipython:: python

    titanic.groupby(["Sex", "Pclass"])["Fare"].mean()

Grouping can be done by multiple columns at the same time. Provide the
column names as a list to the :meth:`~DataFrame.groupby` method.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full description on the split-apply-combine approach is provided in the user guide section on :ref:`groupby operations <groupby>`.

.. raw:: html

   </div>

Count number of records by category
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/06_valuecounts.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the number of passengers in each of the cabin classes?

.. ipython:: python

    titanic["Pclass"].value_counts()

The :meth:`~Series.value_counts` method counts the number of records for each
category in a column.

.. raw:: html

        </li>
    </ul>

The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records
within each group:

.. ipython:: python

    titanic.groupby("Pclass")["Pclass"].count()

.. note::
    Both ``size`` and ``count`` can be used in combination with
    ``groupby``. Whereas ``size`` includes ``NaN`` values and just provides
    the number of rows (size of the table), ``count`` excludes the missing
    values. In the ``value_counts`` method, use the ``dropna`` argument to
    include or exclude the ``NaN`` values.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

The user guide has a dedicated section on ``value_counts`` , see page on :ref:`discretization <basics.discretization>`.

.. raw:: html

   </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Aggregation statistics can be calculated on entire columns or rows
-  ``groupby`` provides the power of the *split-apply-combine* pattern
-  ``value_counts`` is a convenient shortcut to count the number of
   entries in each category of a variable

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full description on the split-apply-combine approach is provided in the user guide pages about :ref:`groupby operations <groupby>`.

.. raw:: html

   </div>
.. _10min_tut_04_plotting:

{{ header }}

.. ipython:: python

    import pandas as pd
    import matplotlib.pyplot as plt

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/air_quality_no2.rst

.. ipython:: python

    air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
    air_quality.head()

.. note::
    The usage of the ``index_col`` and ``parse_dates`` parameters of the ``read_csv`` function to define the first (0th) column as
    index of the resulting ``DataFrame`` and convert the dates in the column to :class:`Timestamp` objects, respectively.

.. raw:: html

        </li>
    </ul>
    </div>

How to create plots in pandas?
------------------------------

.. image:: ../../_static/schemas/04_plot_overview.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want a quick visual check of the data.

.. ipython:: python

    @savefig 04_airqual_quick.png
    air_quality.plot()

With a ``DataFrame``, pandas creates by default one line plot for each of
the columns with numeric data.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to plot only the columns of the data table with the data from Paris.

.. ipython:: python

    @savefig 04_airqual_paris.png
    air_quality["station_paris"].plot()

To plot a specific column, use the selection method of the
:ref:`subset data tutorial <10min_tut_03_subset>` in combination with the :meth:`~DataFrame.plot`
method. Hence, the :meth:`~DataFrame.plot` method works on both ``Series`` and
``DataFrame``.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to visually compare the :math:`N0_2` values measured in London versus Paris.

.. ipython:: python

    @savefig 04_airqual_scatter.png
    air_quality.plot.scatter(x="station_london", y="station_paris", alpha=0.5)

.. raw:: html

        </li>
    </ul>

Apart from the default ``line`` plot when using the ``plot`` function, a
number of alternatives are available to plot data. Let’s use some
standard Python to get an overview of the available plot methods:

.. ipython:: python

    [
        method_name
        for method_name in dir(air_quality.plot)
        if not method_name.startswith("_")
    ]

.. note::
    In many development environments as well as IPython and
    Jupyter Notebook, use the TAB button to get an overview of the available
    methods, for example ``air_quality.plot.`` + TAB.

One of the options is :meth:`DataFrame.plot.box`, which refers to a
`boxplot <https://en.wikipedia.org/wiki/Box_plot>`__. The ``box``
method is applicable on the air quality example data:

.. ipython:: python

    @savefig 04_airqual_boxplot.png
    air_quality.plot.box()

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For an introduction to plots other than the default line plot, see the user guide section about :ref:`supported plot styles <visualization.other>`.

.. raw:: html

   </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want each of the columns in a separate subplot.

.. ipython:: python

    @savefig 04_airqual_area_subplot.png
    axs = air_quality.plot.area(figsize=(12, 4), subplots=True)

Separate subplots for each of the data columns are supported by the ``subplots`` argument
of the ``plot`` functions. The builtin options available in each of the pandas plot
functions are worth reviewing.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Some more formatting options are explained in the user guide section on :ref:`plot formatting <visualization.formatting>`.

.. raw:: html

   </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to further customize, extend or save the resulting plot.

.. ipython:: python

    fig, axs = plt.subplots(figsize=(12, 4))
    air_quality.plot.area(ax=axs)
    @savefig 04_airqual_customized.png
    axs.set_ylabel("NO$_2$ concentration")
    fig.savefig("no2_concentrations.png")

.. ipython:: python
   :suppress:

   import os

   os.remove("no2_concentrations.png")

.. raw:: html

        </li>
    </ul>

Each of the plot objects created by pandas is a
`matplotlib <https://matplotlib.org/>`__ object. As Matplotlib provides
plenty of options to customize plots, making the link between pandas and
Matplotlib explicit enables all the power of matplotlib to the plot.
This strategy is applied in the previous example:

::

   fig, axs = plt.subplots(figsize=(12, 4))        # Create an empty matplotlib Figure and Axes
   air_quality.plot.area(ax=axs)                   # Use pandas to put the area plot on the prepared Figure/Axes
   axs.set_ylabel("NO$_2$ concentration")          # Do any matplotlib customization you like
   fig.savefig("no2_concentrations.png")           # Save the Figure/Axes using the existing matplotlib method.

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  The ``.plot.*`` methods are applicable on both Series and DataFrames
-  By default, each of the columns is plotted as a different element
   (line, boxplot,…)
-  Any plot created by pandas is a Matplotlib object.

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full overview of plotting in pandas is provided in the :ref:`visualization pages <visualization>`.

.. raw:: html

   </div>
.. _10min_tut_10_text:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
.. include:: includes/titanic.rst

.. ipython:: python

    titanic = pd.read_csv("data/titanic.csv")
    titanic.head()

.. raw:: html

            </li>
        </ul>
    </div>

How to manipulate textual data?
-------------------------------

.. raw:: html

    <ul class="task-bullet">
        <li>

Make all name characters lowercase.

.. ipython:: python

    titanic["Name"].str.lower()

To make each of the strings in the ``Name`` column lowercase, select the ``Name`` column
(see the :ref:`tutorial on selection of data <10min_tut_03_subset>`), add the ``str`` accessor and
apply the ``lower`` method. As such, each of the strings is converted element-wise.

.. raw:: html

        </li>
    </ul>

Similar to datetime objects in the :ref:`time series tutorial <10min_tut_09_timeseries>`
having a ``dt`` accessor, a number of
specialized string methods are available when using the ``str``
accessor. These methods have in general matching names with the
equivalent built-in string methods for single elements, but are applied
element-wise (remember :ref:`element-wise calculations <10min_tut_05_columns>`?)
on each of the values of the columns.

.. raw:: html

    <ul class="task-bullet">
        <li>

Create a new column ``Surname`` that contains the surname of the passengers by extracting the part before the comma.

.. ipython:: python

    titanic["Name"].str.split(",")

Using the :meth:`Series.str.split` method, each of the values is returned as a list of
2 elements. The first element is the part before the comma and the
second element is the part after the comma.

.. ipython:: python

    titanic["Surname"] = titanic["Name"].str.split(",").str.get(0)
    titanic["Surname"]

As we are only interested in the first part representing the surname
(element 0), we can again use the ``str`` accessor and apply :meth:`Series.str.get` to
extract the relevant part. Indeed, these string functions can be
concatenated to combine multiple functions at once!

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More information on extracting parts of strings is available in the user guide section on :ref:`splitting and replacing strings <text.split>`.

.. raw:: html

   </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

Extract the passenger data about the countesses on board of the Titanic.

.. ipython:: python

    titanic["Name"].str.contains("Countess")

.. ipython:: python

    titanic[titanic["Name"].str.contains("Countess")]

(*Interested in her story? See* `Wikipedia <https://en.wikipedia.org/wiki/No%C3%ABl_Leslie,_Countess_of_Rothes>`__\ *!*)

The string method :meth:`Series.str.contains` checks for each of the values in the
column ``Name`` if the string contains the word ``Countess`` and returns
for each of the values ``True`` (``Countess`` is part of the name) or
``False`` (``Countess`` is not part of the name). This output can be used
to subselect the data using conditional (boolean) indexing introduced in
the :ref:`subsetting of data tutorial <10min_tut_03_subset>`. As there was
only one countess on the Titanic, we get one row as a result.

.. raw:: html

        </li>
    </ul>

.. note::
    More powerful extractions on strings are supported, as the
    :meth:`Series.str.contains` and :meth:`Series.str.extract` methods accept `regular
    expressions <https://docs.python.org/3/library/re.html>`__, but out of
    scope of this tutorial.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More information on extracting parts of strings is available in the user guide section on :ref:`string matching and extracting <text.extract>`.

.. raw:: html

   </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

Which passenger of the Titanic has the longest name?

.. ipython:: python

    titanic["Name"].str.len()

To get the longest name we first have to get the lengths of each of the
names in the ``Name`` column. By using pandas string methods, the
:meth:`Series.str.len` function is applied to each of the names individually
(element-wise).

.. ipython:: python

    titanic["Name"].str.len().idxmax()

Next, we need to get the corresponding location, preferably the index
label, in the table for which the name length is the largest. The
:meth:`~Series.idxmax` method does exactly that. It is not a string method and is
applied to integers, so no ``str`` is used.

.. ipython:: python

    titanic.loc[titanic["Name"].str.len().idxmax(), "Name"]

Based on the index name of the row (``307``) and the column (``Name``),
we can do a selection using the ``loc`` operator, introduced in the
`tutorial on subsetting <3_subset_data.ipynb>`__.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

In the "Sex" column, replace values of "male" by "M" and values of "female" by "F".

.. ipython:: python

    titanic["Sex_short"] = titanic["Sex"].replace({"male": "M", "female": "F"})
    titanic["Sex_short"]

Whereas :meth:`~Series.replace` is not a string method, it provides a convenient way
to use mappings or vocabularies to translate certain values. It requires
a ``dictionary`` to define the mapping ``{from : to}``.

.. raw:: html

        </li>
    </ul>

.. warning::
    There is also a :meth:`~Series.str.replace` method available to replace a
    specific set of characters. However, when having a mapping of multiple
    values, this would become:

    ::

        titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
        titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")

    This would become cumbersome and easily lead to mistakes. Just think (or
    try out yourself) what would happen if those two statements are applied
    in the opposite order…

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  String methods are available using the ``str`` accessor.
-  String methods work element-wise and can be used for conditional
   indexing.
-  The ``replace`` method is a convenient method to convert values
   according to a given dictionary.

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full overview is provided in the user guide pages on :ref:`working with text data <text>`.

.. raw:: html

   </div>
.. _10min_tut_03_subset:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/titanic.rst

.. ipython:: python

    titanic = pd.read_csv("data/titanic.csv")
    titanic.head()

.. raw:: html

            </li>
        </ul>
    </div>

How do I select a subset of a ``DataFrame``?
============================================

How do I select specific columns from a ``DataFrame``?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/03_subset_columns.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in the age of the Titanic passengers.

.. ipython:: python

    ages = titanic["Age"]
    ages.head()

To select a single column, use square brackets ``[]`` with the column
name of the column of interest.

.. raw:: html

        </li>
    </ul>

Each column in a :class:`DataFrame` is a :class:`Series`. As a single column is
selected, the returned object is a pandas :class:`Series`. We can verify this
by checking the type of the output:

.. ipython:: python

    type(titanic["Age"])

And have a look at the ``shape`` of the output:

.. ipython:: python

    titanic["Age"].shape

:attr:`DataFrame.shape` is an attribute (remember :ref:`tutorial on reading and writing <10min_tut_02_read_write>`, do not use parentheses for attributes) of a
pandas ``Series`` and ``DataFrame`` containing the number of rows and
columns: *(nrows, ncolumns)*. A pandas Series is 1-dimensional and only
the number of rows is returned.

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in the age and sex of the Titanic passengers.

.. ipython:: python

    age_sex = titanic[["Age", "Sex"]]
    age_sex.head()

To select multiple columns, use a list of column names within the
selection brackets ``[]``.

.. raw:: html

        </li>
    </ul>

.. note::
    The inner square brackets define a
    :ref:`Python list <python:tut-morelists>` with column names, whereas
    the outer brackets are used to select the data from a pandas
    ``DataFrame`` as seen in the previous example.

The returned data type is a pandas DataFrame:

.. ipython:: python

    type(titanic[["Age", "Sex"]])

.. ipython:: python

    titanic[["Age", "Sex"]].shape

The selection returned a ``DataFrame`` with 891 rows and 2 columns. Remember, a
``DataFrame`` is 2-dimensional with both a row and column dimension.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For basic information on indexing, see the user guide section on :ref:`indexing and selecting data <indexing.basics>`.

.. raw:: html

    </div>

How do I filter specific rows from a ``DataFrame``?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/03_subset_rows.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in the passengers older than 35 years.

.. ipython:: python

    above_35 = titanic[titanic["Age"] > 35]
    above_35.head()

To select rows based on a conditional expression, use a condition inside
the selection brackets ``[]``.

.. raw:: html

        </li>
    </ul>

The condition inside the selection
brackets ``titanic["Age"] > 35`` checks for which rows the ``Age``
column has a value larger than 35:

.. ipython:: python

    titanic["Age"] > 35

The output of the conditional expression (``>``, but also ``==``,
``!=``, ``<``, ``<=``,… would work) is actually a pandas ``Series`` of
boolean values (either ``True`` or ``False``) with the same number of
rows as the original ``DataFrame``. Such a ``Series`` of boolean values
can be used to filter the ``DataFrame`` by putting it in between the
selection brackets ``[]``. Only rows for which the value is ``True``
will be selected.

We know from before that the original Titanic ``DataFrame`` consists of
891 rows. Let’s have a look at the number of rows which satisfy the
condition by checking the ``shape`` attribute of the resulting
``DataFrame`` ``above_35``:

.. ipython:: python

    above_35.shape

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in the Titanic passengers from cabin class 2 and 3.

.. ipython:: python

    class_23 = titanic[titanic["Pclass"].isin([2, 3])]
    class_23.head()

Similar to the conditional expression, the :func:`~Series.isin` conditional function
returns a ``True`` for each row the values are in the provided list. To
filter the rows based on such a function, use the conditional function
inside the selection brackets ``[]``. In this case, the condition inside
the selection brackets ``titanic["Pclass"].isin([2, 3])`` checks for
which rows the ``Pclass`` column is either 2 or 3.

.. raw:: html

        </li>
    </ul>

The above is equivalent to filtering by rows for which the class is
either 2 or 3 and combining the two statements with an ``|`` (or)
operator:

.. ipython:: python

    class_23 = titanic[(titanic["Pclass"] == 2) | (titanic["Pclass"] == 3)]
    class_23.head()

.. note::
    When combining multiple conditional statements, each condition
    must be surrounded by parentheses ``()``. Moreover, you can not use
    ``or``/``and`` but need to use the ``or`` operator ``|`` and the ``and``
    operator ``&``.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

See the dedicated section in the user guide about :ref:`boolean indexing <indexing.boolean>` or about the :ref:`isin function <indexing.basics.indexing_isin>`.

.. raw:: html

    </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to work with passenger data for which the age is known.

.. ipython:: python

    age_no_na = titanic[titanic["Age"].notna()]
    age_no_na.head()

The :meth:`~Series.notna` conditional function returns a ``True`` for each row the
values are not an ``Null`` value. As such, this can be combined with the
selection brackets ``[]`` to filter the data table.

.. raw:: html

        </li>
    </ul>

You might wonder what actually changed, as the first 5 lines are still
the same values. One way to verify is to check if the shape has changed:

.. ipython:: python

    age_no_na.shape

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For more dedicated functions on missing values, see the user guide section about :ref:`handling missing data <missing_data>`.

.. raw:: html

    </div>

.. _10min_tut_03_subset.rows_and_columns:

How do I select specific rows and columns from a ``DataFrame``?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/03_subset_columns_rows.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in the names of the passengers older than 35 years.

.. ipython:: python

    adult_names = titanic.loc[titanic["Age"] > 35, "Name"]
    adult_names.head()

In this case, a subset of both rows and columns is made in one go and
just using selection brackets ``[]`` is not sufficient anymore. The
``loc``/``iloc`` operators are required in front of the selection
brackets ``[]``. When using ``loc``/``iloc``, the part before the comma
is the rows you want, and the part after the comma is the columns you
want to select.

.. raw:: html

        </li>
    </ul>

When using the column names, row labels or a condition expression, use
the ``loc`` operator in front of the selection brackets ``[]``. For both
the part before and after the comma, you can use a single label, a list
of labels, a slice of labels, a conditional expression or a colon. Using
a colon specifies you want to select all rows or columns.

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in rows 10 till 25 and columns 3 to 5.

.. ipython:: python

    titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just
using selection brackets ``[]`` is not sufficient anymore. When
specifically interested in certain rows and/or columns based on their
position in the table, use the ``iloc`` operator in front of the
selection brackets ``[]``.

.. raw:: html

        </li>
    </ul>

When selecting specific rows and/or columns with ``loc`` or ``iloc``,
new values can be assigned to the selected data. For example, to assign
the name ``anonymous`` to the first 3 elements of the third column:

.. ipython:: python

    titanic.iloc[0:3, 3] = "anonymous"
    titanic.head()

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

See the user guide section on :ref:`different choices for indexing <indexing.choice>` to get more insight in the usage of ``loc`` and ``iloc``.

.. raw:: html

    </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  When selecting subsets of data, square brackets ``[]`` are used.
-  Inside these brackets, you can use a single column/row label, a list
   of column/row labels, a slice of labels, a conditional expression or
   a colon.
-  Select specific rows and/or columns using ``loc`` when using the row
   and column names
-  Select specific rows and/or columns using ``iloc`` when using the
   positions in the table
-  You can assign new values to a selection based on ``loc``/``iloc``.

.. raw:: html

    </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full overview of indexing is provided in the user guide pages on :ref:`indexing and selecting data <indexing>`.

.. raw:: html

    </div>
.. _10min_tut_09_timeseries:

{{ header }}

.. ipython:: python

    import pandas as pd
    import matplotlib.pyplot as plt

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
                <div data-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
                    <span class="badge badge-dark">Air quality data</span>
                </div>
                <div class="collapse" id="collapsedata">
                    <div class="card-body">
                        <p class="card-text">

For this tutorial, air quality data about :math:`NO_2` and Particulate
matter less than 2.5 micrometers is used, made available by
`openaq <https://openaq.org>`__ and downloaded using the
`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.
The ``air_quality_no2_long.csv"`` data set provides :math:`NO_2` values
for the measurement stations *FR04014*, *BETR801* and *London
Westminster* in respectively Paris, Antwerp and London.

.. raw:: html

                        </p>
                    <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2_long.csv" class="btn btn-dark btn-sm">To raw data</a>
                </div>
            </div>

.. ipython:: python

    air_quality = pd.read_csv("data/air_quality_no2_long.csv")
    air_quality = air_quality.rename(columns={"date.utc": "datetime"})
    air_quality.head()

.. ipython:: python

    air_quality.city.unique()

.. raw:: html

        </li>
    </ul>
    </div>

How to handle time series data with ease?
-----------------------------------------

.. _10min_tut_09_timeseries.properties:

Using pandas datetime properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to work with the dates in the column ``datetime`` as datetime objects instead of plain text

.. ipython:: python

    air_quality["datetime"] = pd.to_datetime(air_quality["datetime"])
    air_quality["datetime"]

Initially, the values in ``datetime`` are character strings and do not
provide any datetime operations (e.g. extract the year, day of the
week,…). By applying the ``to_datetime`` function, pandas interprets the
strings and convert these to datetime (i.e. ``datetime64[ns, UTC]``)
objects. In pandas we call these datetime objects similar to
``datetime.datetime`` from the standard library as :class:`pandas.Timestamp`.

.. raw:: html

        </li>
    </ul>

.. note::
    As many data sets do contain datetime information in one of
    the columns, pandas input function like :func:`pandas.read_csv` and :func:`pandas.read_json`
    can do the transformation to dates when reading the data using the
    ``parse_dates`` parameter with a list of the columns to read as
    Timestamp:

    ::

        pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

Why are these :class:`pandas.Timestamp` objects useful? Let’s illustrate the added
value with some example cases.

   What is the start and end date of the time series data set we are working
   with?

.. ipython:: python

    air_quality["datetime"].min(), air_quality["datetime"].max()

Using :class:`pandas.Timestamp` for datetimes enables us to calculate with date
information and make them comparable. Hence, we can use this to get the
length of our time series:

.. ipython:: python

    air_quality["datetime"].max() - air_quality["datetime"].min()

The result is a :class:`pandas.Timedelta` object, similar to ``datetime.timedelta``
from the standard Python library and defining a time duration.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

The various time concepts supported by pandas are explained in the user guide section on :ref:`time related concepts <timeseries.overview>`.

.. raw:: html

    </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to add a new column to the ``DataFrame`` containing only the month of the measurement

.. ipython:: python

    air_quality["month"] = air_quality["datetime"].dt.month
    air_quality.head()

By using ``Timestamp`` objects for dates, a lot of time-related
properties are provided by pandas. For example the ``month``, but also
``year``, ``weekofyear``, ``quarter``,… All of these properties are
accessible by the ``dt`` accessor.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

An overview of the existing date properties is given in the
:ref:`time and date components overview table <timeseries.components>`. More details about the ``dt`` accessor
to return datetime like properties are explained in a dedicated section on the  :ref:`dt accessor <basics.dt_accessors>`.

.. raw:: html

    </div>

.. raw:: html

    <ul class="task-bullet">
        <li>

What is the average :math:`NO_2` concentration for each day of the week for each of the measurement locations?

.. ipython:: python

    air_quality.groupby(
        [air_quality["datetime"].dt.weekday, "location"])["value"].mean()

Remember the split-apply-combine pattern provided by ``groupby`` from the
:ref:`tutorial on statistics calculation <10min_tut_06_stats>`?
Here, we want to calculate a given statistic (e.g. mean :math:`NO_2`)
**for each weekday** and **for each measurement location**. To group on
weekdays, we use the datetime property ``weekday`` (with Monday=0 and
Sunday=6) of pandas ``Timestamp``, which is also accessible by the
``dt`` accessor. The grouping on both locations and weekdays can be done
to split the calculation of the mean on each of these combinations.

.. danger::
    As we are working with a very short time series in these
    examples, the analysis does not provide a long-term representative
    result!

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

Plot the typical :math:`NO_2` pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

.. ipython:: python

    fig, axs = plt.subplots(figsize=(12, 4))
    air_quality.groupby(air_quality["datetime"].dt.hour)["value"].mean().plot(
        kind='bar', rot=0, ax=axs
    )
    plt.xlabel("Hour of the day");  # custom x label using matplotlib
    @savefig 09_bar_chart.png
    plt.ylabel("$NO_2 (µg/m^3)$");

Similar to the previous case, we want to calculate a given statistic
(e.g. mean :math:`NO_2`) **for each hour of the day** and we can use the
split-apply-combine approach again. For this case, we use the datetime property ``hour``
of pandas ``Timestamp``, which is also accessible by the ``dt`` accessor.

.. raw:: html

        </li>
    </ul>

Datetime as index
~~~~~~~~~~~~~~~~~

In the :ref:`tutorial on reshaping <10min_tut_07_reshape>`,
:meth:`~pandas.pivot` was introduced to reshape the data table with each of the
measurements locations as a separate column:

.. ipython:: python

    no_2 = air_quality.pivot(index="datetime", columns="location", values="value")
    no_2.head()

.. note::
    By pivoting the data, the datetime information became the
    index of the table. In general, setting a column as an index can be
    achieved by the ``set_index`` function.

Working with a datetime index (i.e. ``DatetimeIndex``) provides powerful
functionalities. For example, we do not need the ``dt`` accessor to get
the time series properties, but have these properties available on the
index directly:

.. ipython:: python

    no_2.index.year, no_2.index.weekday

Some other advantages are the convenient subsetting of time period or
the adapted time scale on plots. Let’s apply this on our data.

.. raw:: html

    <ul class="task-bullet">
        <li>

Create a plot of the :math:`NO_2` values in the different stations from the 20th of May till the end of 21st of May

.. ipython:: python
    :okwarning:

    @savefig 09_time_section.png
    no_2["2019-05-20":"2019-05-21"].plot();

By providing a **string that parses to a datetime**, a specific subset of the data can be selected on a ``DatetimeIndex``.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More information on the ``DatetimeIndex`` and the slicing by using strings is provided in the section on :ref:`time series indexing <timeseries.datetimeindex>`.

.. raw:: html

    </div>

Resample a time series to another frequency
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <ul class="task-bullet">
        <li>

Aggregate the current hourly time series values to the monthly maximum value in each of the stations.

.. ipython:: python

    monthly_max = no_2.resample("M").max()
    monthly_max

A very powerful method on time series data with a datetime index, is the
ability to :meth:`~Series.resample` time series to another frequency (e.g.,
converting secondly data into 5-minutely data).

.. raw:: html

        </li>
    </ul>

The :meth:`~Series.resample` method is similar to a groupby operation:

-  it provides a time-based grouping, by using a string (e.g. ``M``,
   ``5H``,…) that defines the target frequency
-  it requires an aggregation function such as ``mean``, ``max``,…

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

An overview of the aliases used to define time series frequencies is given in the :ref:`offset aliases overview table <timeseries.offset_aliases>`.

.. raw:: html

    </div>

When defined, the frequency of the time series is provided by the
``freq`` attribute:

.. ipython:: python

    monthly_max.index.freq

.. raw:: html

    <ul class="task-bullet">
        <li>

Make a plot of the daily mean :math:`NO_2` value in each of the stations.

.. ipython:: python
    :okwarning:

    @savefig 09_resample_mean.png
    no_2.resample("D").mean().plot(style="-o", figsize=(10, 5));

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More details on the power of time series ``resampling`` is provided in the user guide section on :ref:`resampling <timeseries.resampling>`.

.. raw:: html

    </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Valid date strings can be converted to datetime objects using
   ``to_datetime`` function or as part of read functions.
-  Datetime objects in pandas support calculations, logical operations
   and convenient date-related properties using the ``dt`` accessor.
-  A ``DatetimeIndex`` contains these date-related properties and
   supports convenient slicing.
-  ``Resample`` is a powerful method to change the frequency of a time
   series.

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full overview on time series is given on the pages on :ref:`time series and date functionality <timeseries>`.

.. raw:: html

   </div>
.. _10min_tut_08_combine:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
                <div data-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
                    <span class="badge badge-dark">Air quality Nitrate data</span>
                </div>
                <div class="collapse" id="collapsedata">
                    <div class="card-body">
                        <p class="card-text">

For this tutorial, air quality data about :math:`NO_2` is used, made available by
`openaq <https://openaq.org>`__ and downloaded using the
`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.

The ``air_quality_no2_long.csv`` data set provides :math:`NO_2`
values for the measurement stations *FR04014*, *BETR801* and *London
Westminster* in respectively Paris, Antwerp and London.

.. raw:: html

                        </p>
                    <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2_long.csv" class="btn btn-dark btn-sm">To raw data</a>
                </div>
            </div>

.. ipython:: python

    air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv",
                                  parse_dates=True)
    air_quality_no2 = air_quality_no2[["date.utc", "location",
                                       "parameter", "value"]]
    air_quality_no2.head()

.. raw:: html

        </li>
        <li class="list-group-item">
            <div data-toggle="collapse" href="#collapsedata2" role="button" aria-expanded="false" aria-controls="collapsedata2">
                <span class="badge badge-dark">Air quality Particulate matter data</span>
            </div>
            <div class="collapse" id="collapsedata2">
                <div class="card-body">
                    <p class="card-text">

For this tutorial, air quality data about Particulate
matter less than 2.5 micrometers is used, made available by
`openaq <https://openaq.org>`__ and downloaded using the
`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.

The ``air_quality_pm25_long.csv`` data set provides :math:`PM_{25}`
values for the measurement stations *FR04014*, *BETR801* and *London
Westminster* in respectively Paris, Antwerp and London.

.. raw:: html

                    </p>
                <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_pm25_long.csv" class="btn btn-dark btn-sm">To raw data</a>
            </div>
        </div>

.. ipython:: python

    air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",
                                   parse_dates=True)
    air_quality_pm25 = air_quality_pm25[["date.utc", "location",
                                         "parameter", "value"]]
    air_quality_pm25.head()

.. raw:: html

        </li>
    </ul>
    </div>


How to combine data from multiple tables?
-----------------------------------------

Concatenating objects
~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/08_concat_row.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to combine the measurements of :math:`NO_2` and :math:`PM_{25}`, two tables with a similar structure, in a single table

.. ipython:: python

    air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
    air_quality.head()

The :func:`~pandas.concat` function performs concatenation operations of multiple
tables along one of the axis (row-wise or column-wise).

.. raw:: html

        </li>
    </ul>

By default concatenation is along axis 0, so the resulting table combines the rows
of the input tables. Let’s check the shape of the original and the
concatenated tables to verify the operation:

.. ipython:: python

    print('Shape of the ``air_quality_pm25`` table: ', air_quality_pm25.shape)
    print('Shape of the ``air_quality_no2`` table: ', air_quality_no2.shape)
    print('Shape of the resulting ``air_quality`` table: ', air_quality.shape)

Hence, the resulting table has 3178 = 1110 + 2068 rows.

.. note::
    The **axis** argument will return in a number of pandas
    methods that can be applied **along an axis**. A ``DataFrame`` has two
    corresponding axes: the first running vertically downwards across rows
    (axis 0), and the second running horizontally across columns (axis 1).
    Most operations like concatenation or summary statistics are by default
    across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the
combination of both tables, with the ``parameter`` column defining the
origin of the table (either ``no2`` from table ``air_quality_no2`` or
``pm25`` from table ``air_quality_pm25``):

.. ipython:: python

    air_quality = air_quality.sort_values("date.utc")
    air_quality.head()

In this specific example, the ``parameter`` column provided by the data
ensures that each of the original tables can be identified. This is not
always the case. the ``concat`` function provides a convenient solution
with the ``keys`` argument, adding an additional (hierarchical) row
index. For example:

.. ipython:: python

    air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", "NO2"])

.. ipython:: python

    air_quality_.head()

.. note::
    The existence of multiple row/column indices at the same time
    has not been mentioned within these tutorials. *Hierarchical indexing*
    or *MultiIndex* is an advanced and powerful pandas feature to analyze
    higher dimensional data.

    Multi-indexing is out of scope for this pandas introduction. For the
    moment, remember that the function ``reset_index`` can be used to
    convert any level of an index to a column, e.g.
    ``air_quality.reset_index(level=0)``

    .. raw:: html

        <div class="d-flex flex-row  gs-torefguide">
            <span class="badge badge-info">To user guide</span>

    Feel free to dive into the world of multi-indexing at the user guide section on :ref:`advanced indexing <advanced>`.

    .. raw:: html

        </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More options on table concatenation (row and column
wise) and how ``concat`` can be used to define the logic (union or
intersection) of the indexes on the other axes is provided at the section on
:ref:`object concatenation <merging.concat>`.

.. raw:: html

    </div>

Join tables using a common identifier
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../../_static/schemas/08_merge_left.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

.. warning::
    The air quality measurement station coordinates are stored in a data
    file ``air_quality_stations.csv``, downloaded using the
    `py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.

.. ipython:: python

    stations_coord = pd.read_csv("data/air_quality_stations.csv")
    stations_coord.head()

.. note::
    The stations used in this example (FR04014, BETR801 and London
    Westminster) are just three entries enlisted in the metadata table. We
    only want to add the coordinates of these three to the measurements
    table, each on the corresponding rows of the ``air_quality`` table.

.. ipython:: python

    air_quality.head()

.. ipython:: python

    air_quality = pd.merge(air_quality, stations_coord, how="left", on="location")
    air_quality.head()

Using the :meth:`~pandas.merge` function, for each of the rows in the
``air_quality`` table, the corresponding coordinates are added from the
``air_quality_stations_coord`` table. Both tables have the column
``location`` in common which is used as a key to combine the
information. By choosing the ``left`` join, only the locations available
in the ``air_quality`` (left) table, i.e. FR04014, BETR801 and London
Westminster, end up in the resulting table. The ``merge`` function
supports multiple join options similar to database-style operations.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

.. warning::
    The air quality parameters metadata are stored in a data file
    ``air_quality_parameters.csv``, downloaded using the
    `py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.

.. ipython:: python

    air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
    air_quality_parameters.head()

.. ipython:: python

    air_quality = pd.merge(air_quality, air_quality_parameters,
                           how='left', left_on='parameter', right_on='id')
    air_quality.head()

Compared to the previous example, there is no common column name.
However, the ``parameter`` column in the ``air_quality`` table and the
``id`` column in the ``air_quality_parameters_name`` both provide the
measured variable in a common format. The ``left_on`` and ``right_on``
arguments are used here (instead of just ``on``) to make the link
between the two tables.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

pandas supports also inner, outer, and right joins.
More information on join/merge of tables is provided in the user guide section on
:ref:`database style merging of tables <merging.join>`. Or have a look at the
:ref:`comparison with SQL<compare_with_sql.join>` page.

.. raw:: html

   </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Multiple tables can be concatenated both column-wise and row-wise using
   the ``concat`` function.
-  For database-like merging/joining of tables, use the ``merge``
   function.

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

See the user guide for a full description of the various :ref:`facilities to combine data tables <merging>`.

.. raw:: html

   </div>
.. _10min_tut_07_reshape:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/titanic.rst

.. ipython:: python

    titanic = pd.read_csv("data/titanic.csv")
    titanic.head()

.. raw:: html

        </li>
        <li class="list-group-item">
            <div data-toggle="collapse" href="#collapsedata2" role="button" aria-expanded="false" aria-controls="collapsedata2">
                <span class="badge badge-dark">Air quality data</span>
            </div>
            <div class="collapse" id="collapsedata2">
                <div class="card-body">
                    <p class="card-text">

This tutorial uses air quality data about :math:`NO_2` and Particulate matter less than 2.5
micrometers, made available by
`openaq <https://openaq.org>`__ and using the
`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.
The ``air_quality_long.csv`` data set provides :math:`NO_2` and
:math:`PM_{25}` values for the measurement stations *FR04014*, *BETR801*
and *London Westminster* in respectively Paris, Antwerp and London.

The air-quality data set has the following columns:

-  city: city where the sensor is used, either Paris, Antwerp or London
-  country: country where the sensor is used, either FR, BE or GB
-  location: the id of the sensor, either *FR04014*, *BETR801* or
   *London Westminster*
-  parameter: the parameter measured by the sensor, either :math:`NO_2`
   or Particulate matter
-  value: the measured value
-  unit: the unit of the measured parameter, in this case ‘µg/m³’

and the index of the ``DataFrame`` is ``datetime``, the datetime of the
measurement.

.. note::
    The air-quality data is provided in a so-called *long format*
    data representation with each observation on a separate row and each
    variable a separate column of the data table. The long/narrow format is
    also known as the `tidy data
    format <https://www.jstatsoft.org/article/view/v059i10>`__.

.. raw:: html

                    </p>
                <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_long.csv" class="btn btn-dark btn-sm">To raw data</a>
            </div>
        </div>

.. ipython:: python

    air_quality = pd.read_csv(
        "data/air_quality_long.csv", index_col="date.utc", parse_dates=True
    )
    air_quality.head()

.. raw:: html

            </li>
        </ul>
    </div>

How to reshape the layout of tables?
------------------------------------

Sort table rows
~~~~~~~~~~~~~~~

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to sort the Titanic data according to the age of the passengers.

.. ipython:: python

    titanic.sort_values(by="Age").head()

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to sort the Titanic data according to the cabin class and age in descending order.

.. ipython:: python

    titanic.sort_values(by=['Pclass', 'Age'], ascending=False).head()

With :meth:`Series.sort_values`, the rows in the table are sorted according to the
defined column(s). The index will follow the row order.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

More details about sorting of tables is provided in the using guide section on :ref:`sorting data <basics.sorting>`.

.. raw:: html

   </div>

Long to wide table format
~~~~~~~~~~~~~~~~~~~~~~~~~

Let’s use a small subset of the air quality data set. We focus on
:math:`NO_2` data and only use the first two measurements of each
location (i.e. the head of each group). The subset of data will be
called ``no2_subset``

.. ipython:: python

    # filter for no2 data only
    no2 = air_quality[air_quality["parameter"] == "no2"]

.. ipython:: python

    # use 2 measurements (head) for each location (groupby)
    no2_subset = no2.sort_index().groupby(["location"]).head(2)
    no2_subset

.. image:: ../../_static/schemas/07_pivot.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want the values for the three stations as separate columns next to each other

.. ipython:: python

    no2_subset.pivot(columns="location", values="value")

The :meth:`~pandas.pivot` function is purely reshaping of the data: a single value
for each index/column combination is required.

.. raw:: html

        </li>
    </ul>

As pandas support plotting of multiple columns (see :ref:`plotting tutorial <10min_tut_04_plotting>`) out of the box, the conversion from
*long* to *wide* table format enables the plotting of the different time
series at the same time:

.. ipython:: python

    no2.head()

.. ipython:: python

    @savefig 7_reshape_columns.png
    no2.pivot(columns="location", values="value").plot()

.. note::
    When the ``index`` parameter is not defined, the existing
    index (row labels) is used.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For more information about :meth:`~DataFrame.pivot`, see the user guide section on :ref:`pivoting DataFrame objects <reshaping.reshaping>`.

.. raw:: html

   </div>

Pivot table
~~~~~~~~~~~

.. image:: ../../_static/schemas/07_pivot_table.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want the mean concentrations for :math:`NO_2` and :math:`PM_{2.5}` in each of the stations in table form

.. ipython:: python

    air_quality.pivot_table(
        values="value", index="location", columns="parameter", aggfunc="mean"
    )

In the case of :meth:`~DataFrame.pivot`, the data is only rearranged. When multiple
values need to be aggregated (in this specific case, the values on
different time steps) :meth:`~DataFrame.pivot_table` can be used, providing an
aggregation function (e.g. mean) on how to combine these values.

.. raw:: html

        </li>
    </ul>

Pivot table is a well known concept in spreadsheet software. When
interested in summary columns for each variable separately as well, put
the ``margin`` parameter to ``True``:

.. ipython:: python

    air_quality.pivot_table(
        values="value",
        index="location",
        columns="parameter",
        aggfunc="mean",
        margins=True,
    )

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For more information about :meth:`~DataFrame.pivot_table`, see the user guide section on :ref:`pivot tables <reshaping.pivot>`.

.. raw:: html

   </div>

.. note::
    In case you are wondering, :meth:`~DataFrame.pivot_table` is indeed directly linked
    to :meth:`~DataFrame.groupby`. The same result can be derived by grouping on both
    ``parameter`` and ``location``:

    ::

        air_quality.groupby(["parameter", "location"]).mean()

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Have a look at :meth:`~DataFrame.groupby` in combination with :meth:`~DataFrame.unstack` at the user guide section on :ref:`combining stats and groupby <reshaping.combine_with_groupby>`.

.. raw:: html

   </div>

Wide to long format
~~~~~~~~~~~~~~~~~~~

Starting again from the wide format table created in the previous
section:

.. ipython:: python

    no2_pivoted = no2.pivot(columns="location", values="value").reset_index()
    no2_pivoted.head()

.. image:: ../../_static/schemas/07_melt.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to collect all air quality :math:`NO_2` measurements in a single column (long format)

.. ipython:: python

    no_2 = no2_pivoted.melt(id_vars="date.utc")
    no_2.head()

The :func:`pandas.melt` method on a ``DataFrame`` converts the data table from wide
format to long format. The column headers become the variable names in a
newly created column.

.. raw:: html

        </li>
    </ul>

The solution is the short version on how to apply :func:`pandas.melt`. The method
will *melt* all columns NOT mentioned in ``id_vars`` together into two
columns: A column with the column header names and a column with the
values itself. The latter column gets by default the name ``value``.

The :func:`pandas.melt` method can be defined in more detail:

.. ipython:: python

    no_2 = no2_pivoted.melt(
        id_vars="date.utc",
        value_vars=["BETR801", "FR04014", "London Westminster"],
        value_name="NO_2",
        var_name="id_location",
    )
    no_2.head()

The result in the same, but in more detail defined:

-  ``value_vars`` defines explicitly which columns to *melt* together
-  ``value_name`` provides a custom column name for the values column
   instead of the default column name ``value``
-  ``var_name`` provides a custom column name for the column collecting
   the column header names. Otherwise it takes the index name or a
   default ``variable``

Hence, the arguments ``value_name`` and ``var_name`` are just
user-defined names for the two generated columns. The columns to melt
are defined by ``id_vars`` and ``value_vars``.

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

Conversion from wide to long format with :func:`pandas.melt` is explained in the user guide section on :ref:`reshaping by melt <reshaping.melt>`.

.. raw:: html

   </div>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Sorting by one or more columns is supported by ``sort_values``
-  The ``pivot`` function is purely restructuring of the data,
   ``pivot_table`` supports aggregations
-  The reverse of ``pivot`` (long to wide format) is ``melt`` (wide to
   long format)

.. raw:: html

   </div>

.. raw:: html

    <div class="d-flex flex-row gs-torefguide">
        <span class="badge badge-info">To user guide</span>

A full overview is available in the user guide on the pages about :ref:`reshaping and pivoting <reshaping>`.

.. raw:: html

   </div>
{{ header }}

.. _10times1minute:

=========================
Getting started tutorials
=========================

.. toctree::
    :maxdepth: 1

    01_table_oriented
    02_read_write
    03_subset_data
    04_plotting
    05_add_columns
    06_calculate_statistics
    07_reshape_table_layout
    08_combine_dataframes
    09_timeseries
    10_text_data
.. _10min_tut_02_read_write:

{{ header }}

.. ipython:: python

    import pandas as pd

.. raw:: html

    <div class="card gs-data">
        <div class="card-header">
            <div class="gs-data-title">
                Data used for this tutorial:
            </div>
        </div>
        <ul class="list-group list-group-flush">
            <li class="list-group-item">

.. include:: includes/titanic.rst

.. raw:: html

            </li>
        </ul>
    </div>

How do I read and write tabular data?
=====================================

.. image:: ../../_static/schemas/02_io_readwrite.svg
   :align: center

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to analyze the Titanic passenger data, available as a CSV file.

.. ipython:: python

    titanic = pd.read_csv("data/titanic.csv")

pandas provides the :func:`read_csv` function to read data stored as a csv
file into a pandas ``DataFrame``. pandas supports many different file
formats or data sources out of the box (csv, excel, sql, json, parquet,
…), each of them with the prefix ``read_*``.

.. raw:: html

        </li>
    </ul>

Make sure to always have a check on the data after reading in the
data. When displaying a ``DataFrame``, the first and last 5 rows will be
shown by default:

.. ipython:: python

    titanic

.. raw:: html

    <ul class="task-bullet">
        <li>

I want to see the first 8 rows of a pandas DataFrame.

.. ipython:: python

    titanic.head(8)

To see the first N rows of a ``DataFrame``, use the :meth:`~DataFrame.head` method with
the required number of rows (in this case 8) as argument.

.. raw:: html

        </li>
    </ul>

.. note::

    Interested in the last N rows instead? pandas also provides a
    :meth:`~DataFrame.tail` method. For example, ``titanic.tail(10)`` will return the last
    10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be
done by requesting the pandas ``dtypes`` attribute:

.. ipython:: python

    titanic.dtypes

For each of the columns, the used data type is enlisted. The data types
in this ``DataFrame`` are integers (``int64``), floats (``float64``) and
strings (``object``).

.. note::
    When asking for the ``dtypes``, no brackets are used!
    ``dtypes`` is an attribute of a ``DataFrame`` and ``Series``. Attributes
    of ``DataFrame`` or ``Series`` do not need brackets. Attributes
    represent a characteristic of a ``DataFrame``/``Series``, whereas a
    method (which requires brackets) *do* something with the
    ``DataFrame``/``Series`` as introduced in the :ref:`first tutorial <10min_tut_01_tableoriented>`.

.. raw:: html

    <ul class="task-bullet">
        <li>

My colleague requested the Titanic data as a spreadsheet.

.. ipython:: python

    titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas ``read_*`` functions are used to read data to pandas, the
``to_*`` methods are used to store data. The :meth:`~DataFrame.to_excel` method stores
the data as an excel file. In the example here, the ``sheet_name`` is
named *passengers* instead of the default *Sheet1*. By setting
``index=False`` the row index labels are not saved in the spreadsheet.

.. raw:: html

        </li>
    </ul>

The equivalent read function :meth:`~DataFrame.read_excel` will reload the data to a
``DataFrame``:

.. ipython:: python

    titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

.. ipython:: python

    titanic.head()

.. ipython:: python
   :suppress:

   import os

   os.remove("titanic.xlsx")

.. raw:: html

    <ul class="task-bullet">
        <li>

I’m interested in a technical summary of a ``DataFrame``

.. ipython:: python

    titanic.info()


The method :meth:`~DataFrame.info` provides technical information about a
``DataFrame``, so let’s explain the output in more detail:

-  It is indeed a :class:`DataFrame`.
-  There are 891 entries, i.e. 891 rows.
-  Each row has a row label (aka the ``index``) with values ranging from
   0 to 890.
-  The table has 12 columns. Most columns have a value for each of the
   rows (all 891 values are ``non-null``). Some columns do have missing
   values and less than 891 ``non-null`` values.
-  The columns ``Name``, ``Sex``, ``Cabin`` and ``Embarked`` consists of
   textual data (strings, aka ``object``). The other columns are
   numerical data with some of them whole numbers (aka ``integer``) and
   others are real numbers (aka ``float``).
-  The kind of data (characters, integers,…) in the different columns
   are summarized by listing the ``dtypes``.
-  The approximate amount of RAM used to hold the DataFrame is provided
   as well.

.. raw:: html

        </li>
    </ul>

.. raw:: html

    <div class="shadow gs-callout gs-callout-remember">
        <h4>REMEMBER</h4>

-  Getting data in to pandas from many different file formats or data
   sources is supported by ``read_*`` functions.
-  Exporting data out of pandas is provided by different
   ``to_*``\ methods.
-  The ``head``/``tail``/``info`` methods and the ``dtypes`` attribute
   are convenient for a first check.

.. raw:: html

    </div>

.. raw:: html

    <div class="d-flex flex-row bg-light gs-torefguide">
        <span class="badge badge-info">To user guide</span>

For a complete overview of the input and output possibilities from and to pandas, see the user guide section about :ref:`reader and writer functions <io>`.

.. raw:: html

    </div>
.. raw:: html

    <div data-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
        <span class="badge badge-dark">Titanic data</span>
    </div>
    <div class="collapse" id="collapsedata">
        <div class="card-body">
            <p class="card-text">

This tutorial uses the Titanic data set, stored as CSV. The data
consists of the following data columns:

-  PassengerId: Id of every passenger.
-  Survived: This feature have value 0 and 1. 0 for not survived and 1
   for survived.
-  Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
-  Name: Name of passenger.
-  Sex: Gender of passenger.
-  Age: Age of passenger.
-  SibSp: Indication that passenger have siblings and spouse.
-  Parch: Whether a passenger is alone or have family.
-  Ticket: Ticket number of passenger.
-  Fare: Indicating the fare.
-  Cabin: The cabin of passenger.
-  Embarked: The embarked category.

.. raw:: html

            </p>
            <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/titanic.csv" class="btn btn-dark btn-sm">To raw data</a>
        </div>
    </div>
.. raw:: html

    <div data-toggle="collapse" href="#collapsedata" role="button" aria-expanded="false" aria-controls="collapsedata">
        <span class="badge badge-dark">Air quality data</span>
    </div>
    <div class="collapse" id="collapsedata">
        <div class="card-body">
            <p class="card-text">

For this tutorial, air quality data about :math:`NO_2` is used, made
available by `openaq <https://openaq.org>`__ and using the
`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.
The ``air_quality_no2.csv`` data set provides :math:`NO_2` values for
the measurement stations *FR04014*, *BETR801* and *London Westminster*
in respectively Paris, Antwerp and London.

.. raw:: html

            </p>
            <a href="https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2.csv" class="btn btn-dark btn-sm">To raw data</a>
        </div>
    </div>
.. _compare_with_sas:

{{ header }}

Comparison with SAS
********************

For potential users coming from `SAS <https://en.wikipedia.org/wiki/SAS_(software)>`__
this page is meant to demonstrate how different SAS operations would be
performed in pandas.

.. include:: includes/introduction.rst


Data structures
---------------

General terminology translation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. csv-table::
    :header: "pandas", "SAS"
    :widths: 20, 20

    ``DataFrame``, data set
    column, variable
    row, observation
    groupby, BY-group
    ``NaN``, ``.``


``DataFrame``
~~~~~~~~~~~~~

A ``DataFrame`` in pandas is analogous to a SAS data set - a two-dimensional
data source with labeled columns that can be of different types. As will be
shown in this document, almost any operation that can be applied to a data set
using SAS's ``DATA`` step, can also be accomplished in pandas.

``Series``
~~~~~~~~~~

A ``Series`` is the data structure that represents one column of a
``DataFrame``. SAS doesn't have a separate data structure for a single column,
but in general, working with a ``Series`` is analogous to referencing a column
in the ``DATA`` step.

``Index``
~~~~~~~~~

Every ``DataFrame`` and ``Series`` has an ``Index`` - which are labels on the
*rows* of the data. SAS does not have an exactly analogous concept. A data set's
rows are essentially unlabeled, other than an implicit integer index that can be
accessed during the ``DATA`` step (``_N_``).

In pandas, if no index is specified, an integer index is also used by default
(first row = 0, second row = 1, and so on). While using a labeled ``Index`` or
``MultiIndex`` can enable sophisticated analyses and is ultimately an important
part of pandas to understand, for this comparison we will essentially ignore the
``Index`` and just treat the ``DataFrame`` as a collection of columns. Please
see the :ref:`indexing documentation<indexing>` for much more on how to use an
``Index`` effectively.


Copies vs. in place operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: includes/copies.rst


Data input / output
-------------------

Constructing a DataFrame from values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A SAS data set can be built from specified values by
placing the data after a ``datalines`` statement and
specifying the column names.

.. code-block:: sas

   data df;
       input x y;
       datalines;
       1 2
       3 4
       5 6
       ;
   run;

.. include:: includes/construct_dataframe.rst

Reading external data
~~~~~~~~~~~~~~~~~~~~~

Like SAS, pandas provides utilities for reading in data from
many formats.  The ``tips`` dataset, found within the pandas
tests (`csv <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/tips.csv>`_)
will be used in many of the following examples.

SAS provides ``PROC IMPORT`` to read csv data into a data set.

.. code-block:: sas

   proc import datafile='tips.csv' dbms=csv out=tips replace;
       getnames=yes;
   run;

The pandas method is :func:`read_csv`, which works similarly.

.. ipython:: python

   url = (
       "https://raw.github.com/pandas-dev/"
       "pandas/main/pandas/tests/io/data/csv/tips.csv"
   )
   tips = pd.read_csv(url)
   tips


Like ``PROC IMPORT``, ``read_csv`` can take a number of parameters to specify
how the data should be parsed.  For example, if the data was instead tab delimited,
and did not have column names, the pandas command would be:

.. code-block:: python

   tips = pd.read_csv("tips.csv", sep="\t", header=None)

   # alternatively, read_table is an alias to read_csv with tab delimiter
   tips = pd.read_table("tips.csv", header=None)

In addition to text/csv, pandas supports a variety of other data formats
such as Excel, HDF5, and SQL databases.  These are all read via a ``pd.read_*``
function.  See the :ref:`IO documentation<io>` for more details.

Limiting output
~~~~~~~~~~~~~~~

.. include:: includes/limit.rst

The equivalent in SAS would be:

.. code-block:: sas

   proc print data=df(obs=5);
   run;


Exporting data
~~~~~~~~~~~~~~

The inverse of ``PROC IMPORT`` in SAS is ``PROC EXPORT``

.. code-block:: sas

   proc export data=tips outfile='tips2.csv' dbms=csv;
   run;

Similarly in pandas, the opposite of ``read_csv`` is :meth:`~DataFrame.to_csv`,
and other data formats follow a similar api.

.. code-block:: python

   tips.to_csv("tips2.csv")


Data operations
---------------

Operations on columns
~~~~~~~~~~~~~~~~~~~~~

In the ``DATA`` step, arbitrary math expressions can
be used on new or existing columns.

.. code-block:: sas

   data tips;
       set tips;
       total_bill = total_bill - 2;
       new_bill = total_bill / 2;
   run;

.. include:: includes/column_operations.rst


Filtering
~~~~~~~~~

Filtering in SAS is done with an ``if`` or ``where`` statement, on one
or more columns.

.. code-block:: sas

   data tips;
       set tips;
       if total_bill > 10;
   run;

   data tips;
       set tips;
       where total_bill > 10;
       /* equivalent in this case - where happens before the
          DATA step begins and can also be used in PROC statements */
   run;

.. include:: includes/filtering.rst

If/then logic
~~~~~~~~~~~~~

In SAS, if/then logic can be used to create new columns.

.. code-block:: sas

   data tips;
       set tips;
       format bucket $4.;

       if total_bill < 10 then bucket = 'low';
       else bucket = 'high';
   run;

.. include:: includes/if_then.rst

Date functionality
~~~~~~~~~~~~~~~~~~

SAS provides a variety of functions to do operations on
date/datetime columns.

.. code-block:: sas

   data tips;
       set tips;
       format date1 date2 date1_plusmonth mmddyy10.;
       date1 = mdy(1, 15, 2013);
       date2 = mdy(2, 15, 2015);
       date1_year = year(date1);
       date2_month = month(date2);
       * shift date to beginning of next interval;
       date1_next = intnx('MONTH', date1, 1);
       * count intervals between dates;
       months_between = intck('MONTH', date1, date2);
   run;

The equivalent pandas operations are shown below.  In addition to these
functions pandas supports other Time Series features
not available in Base SAS (such as resampling and custom offsets) -
see the :ref:`timeseries documentation<timeseries>` for more details.

.. include:: includes/time_date.rst

Selection of columns
~~~~~~~~~~~~~~~~~~~~

SAS provides keywords in the ``DATA`` step to select,
drop, and rename columns.

.. code-block:: sas

   data tips;
       set tips;
       keep sex total_bill tip;
   run;

   data tips;
       set tips;
       drop sex;
   run;

   data tips;
       set tips;
       rename total_bill=total_bill_2;
   run;

.. include:: includes/column_selection.rst


Sorting by values
~~~~~~~~~~~~~~~~~

Sorting in SAS is accomplished via ``PROC SORT``

.. code-block:: sas

   proc sort data=tips;
       by sex total_bill;
   run;

.. include:: includes/sorting.rst

String processing
-----------------

Finding length of string
~~~~~~~~~~~~~~~~~~~~~~~~

SAS determines the length of a character string with the
`LENGTHN <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002284668.htm>`__
and `LENGTHC <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002283942.htm>`__
functions. ``LENGTHN`` excludes trailing blanks and ``LENGTHC`` includes trailing blanks.

.. code-block:: sas

   data _null_;
   set tips;
   put(LENGTHN(time));
   put(LENGTHC(time));
   run;

.. include:: includes/length.rst


Finding position of substring
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SAS determines the position of a character in a string with the
`FINDW <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a002978282.htm>`__ function.
``FINDW`` takes the string defined by the first argument and searches for the first position of the substring
you supply as the second argument.

.. code-block:: sas

   data _null_;
   set tips;
   put(FINDW(sex,'ale'));
   run;

.. include:: includes/find_substring.rst


Extracting substring by position
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SAS extracts a substring from a string based on its position with the
`SUBSTR <https://support.sas.com/documentation/cdl/en/imlug/66845/HTML/default/viewer.htm#imlug_langref_sect455.htm>`__ function.

.. code-block:: sas

   data _null_;
   set tips;
   put(substr(sex,1,1));
   run;

.. include:: includes/extract_substring.rst


Extracting nth word
~~~~~~~~~~~~~~~~~~~

The SAS `SCAN <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000214639.htm>`__
function returns the nth word from a string. The first argument is the string you want to parse and the
second argument specifies which word you want to extract.

.. code-block:: sas

   data firstlast;
   input String $60.;
   First_Name = scan(string, 1);
   Last_Name = scan(string, -1);
   datalines2;
   John Smith;
   Jane Cook;
   ;;;
   run;

.. include:: includes/nth_word.rst


Changing case
~~~~~~~~~~~~~

The SAS `UPCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000245965.htm>`__
`LOWCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000245912.htm>`__ and
`PROPCASE <https://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/a002598106.htm>`__
functions change the case of the argument.

.. code-block:: sas

   data firstlast;
   input String $60.;
   string_up = UPCASE(string);
   string_low = LOWCASE(string);
   string_prop = PROPCASE(string);
   datalines2;
   John Smith;
   Jane Cook;
   ;;;
   run;

.. include:: includes/case.rst


Merging
-------

.. include:: includes/merge_setup.rst

In SAS, data must be explicitly sorted before merging.  Different
types of joins are accomplished using the ``in=`` dummy
variables to track whether a match was found in one or both
input frames.

.. code-block:: sas

   proc sort data=df1;
       by key;
   run;

   proc sort data=df2;
       by key;
   run;

   data left_join inner_join right_join outer_join;
       merge df1(in=a) df2(in=b);

       if a and b then output inner_join;
       if a then output left_join;
       if b then output right_join;
       if a or b then output outer_join;
   run;

.. include:: includes/merge.rst


Missing data
------------

Both pandas and SAS have a representation for missing data.

.. include:: includes/missing_intro.rst

One difference is that missing data cannot be compared to its sentinel value.
For example, in SAS you could do this to filter missing values.

.. code-block:: sas

   data outer_join_nulls;
       set outer_join;
       if value_x = .;
   run;

   data outer_join_no_nulls;
       set outer_join;
       if value_x ^= .;
   run;

.. include:: includes/missing.rst


GroupBy
-------

Aggregation
~~~~~~~~~~~

SAS's ``PROC SUMMARY`` can be used to group by one or
more key variables and compute aggregations on
numeric columns.

.. code-block:: sas

   proc summary data=tips nway;
       class sex smoker;
       var total_bill tip;
       output out=tips_summed sum=;
   run;

.. include:: includes/groupby.rst


Transformation
~~~~~~~~~~~~~~

In SAS, if the group aggregations need to be used with
the original frame, it must be merged back together.  For
example, to subtract the mean for each observation by smoker group.

.. code-block:: sas

   proc summary data=tips missing nway;
       class smoker;
       var total_bill;
       output out=smoker_means mean(total_bill)=group_bill;
   run;

   proc sort data=tips;
       by smoker;
   run;

   data tips;
       merge tips(in=a) smoker_means(in=b);
       by smoker;
       adj_total_bill = total_bill - group_bill;
       if a and b;
   run;

.. include:: includes/transform.rst


By group processing
~~~~~~~~~~~~~~~~~~~

In addition to aggregation, pandas ``groupby`` can be used to
replicate most other by group processing from SAS. For example,
this ``DATA`` step reads the data by sex/smoker group and filters to
the first entry for each.

.. code-block:: sas

   proc sort data=tips;
      by sex smoker;
   run;

   data tips_first;
       set tips;
       by sex smoker;
       if FIRST.sex or FIRST.smoker then output;
   run;

In pandas this would be written as:

.. ipython:: python

   tips.groupby(["sex", "smoker"]).first()


Other considerations
--------------------

Disk vs memory
~~~~~~~~~~~~~~

pandas operates exclusively in memory, where a SAS data set exists on disk.
This means that the size of data able to be loaded in pandas is limited by your
machine's memory, but also that the operations on that data may be faster.

If out of core processing is needed, one possibility is the
`dask.dataframe <https://docs.dask.org/en/latest/dataframe.html>`_
library (currently in development) which
provides a subset of pandas functionality for an on-disk ``DataFrame``

Data interop
~~~~~~~~~~~~

pandas provides a :func:`read_sas` method that can read SAS data saved in
the XPORT or SAS7BDAT binary format.

.. code-block:: sas

   libname xportout xport 'transport-file.xpt';
   data xportout.tips;
       set tips(rename=(total_bill=tbill));
       * xport variable names limited to 6 characters;
   run;

.. code-block:: python

   df = pd.read_sas("transport-file.xpt")
   df = pd.read_sas("binary-file.sas7bdat")

You can also specify the file format directly. By default, pandas will try
to infer the file format based on its extension.

.. code-block:: python

   df = pd.read_sas("transport-file.xpt", format="xport")
   df = pd.read_sas("binary-file.sas7bdat", format="sas7bdat")

XPORT is a relatively limited format and the parsing of it is not as
optimized as some of the other pandas readers. An alternative way
to interop data between SAS and pandas is to serialize to csv.

.. code-block:: ipython

   # version 0.17, 10M rows

   In [8]: %time df = pd.read_sas('big.xpt')
   Wall time: 14.6 s

   In [9]: %time df = pd.read_csv('big.csv')
   Wall time: 4.86 s
.. _compare_with_spreadsheets:

{{ header }}

Comparison with spreadsheets
****************************

Since many potential pandas users have some familiarity with spreadsheet programs like
`Excel <https://support.microsoft.com/en-us/excel>`_, this page is meant to provide some examples
of how various spreadsheet operations would be performed using pandas. This page will use
terminology and link to documentation for Excel, but much will be the same/similar in
`Google Sheets <https://support.google.com/a/users/answer/9282959>`_,
`LibreOffice Calc <https://help.libreoffice.org/latest/en-US/text/scalc/main0000.html?DbPAR=CALC>`_,
`Apple Numbers <https://www.apple.com/numbers/compatibility/>`_, and other
Excel-compatible spreadsheet software.

.. include:: includes/introduction.rst

Data structures
---------------

General terminology translation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. csv-table::
    :header: "pandas", "Excel"
    :widths: 20, 20

    ``DataFrame``, worksheet
    ``Series``, column
    ``Index``, row headings
    row, row
    ``NaN``, empty cell

``DataFrame``
~~~~~~~~~~~~~

A ``DataFrame`` in pandas is analogous to an Excel worksheet. While an Excel workbook can contain
multiple worksheets, pandas ``DataFrame``\s exist independently.

``Series``
~~~~~~~~~~

A ``Series`` is the data structure that represents one column of a ``DataFrame``. Working with a
``Series`` is analogous to referencing a column of a spreadsheet.

``Index``
~~~~~~~~~

Every ``DataFrame`` and ``Series`` has an ``Index``, which are labels on the *rows* of the data. In
pandas, if no index is specified, a :class:`~pandas.RangeIndex` is used by default (first row = 0,
second row = 1, and so on), analogous to row headings/numbers in spreadsheets.

In pandas, indexes can be set to one (or multiple) unique values, which is like having a column that
is used as the row identifier in a worksheet. Unlike most spreadsheets, these ``Index`` values can
actually be used to reference the rows. (Note that `this can be done in Excel with structured
references
<https://support.microsoft.com/en-us/office/using-structured-references-with-excel-tables-f5ed2452-2337-4f71-bed3-c8ae6d2b276e>`_.)
For example, in spreadsheets, you would reference the first row as ``A1:Z1``, while in pandas you
could use ``populations.loc['Chicago']``.

Index values are also persistent, so if you re-order the rows in a ``DataFrame``, the label for a
particular row don't change.

See the :ref:`indexing documentation<indexing>` for much more on how to use an ``Index``
effectively.


Copies vs. in place operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: includes/copies.rst


Data input / output
-------------------

Constructing a DataFrame from values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In a spreadsheet, `values can be typed directly into cells <https://support.microsoft.com/en-us/office/enter-data-manually-in-worksheet-cells-c798181d-d75a-41b1-92ad-6c0800f80038>`_.

.. include:: includes/construct_dataframe.rst

Reading external data
~~~~~~~~~~~~~~~~~~~~~

Both `Excel <https://support.microsoft.com/en-us/office/import-data-from-data-sources-power-query-be4330b3-5356-486c-a168-b68e9e616f5a>`__
and :ref:`pandas <10min_tut_02_read_write>` can import data from various sources in various
formats.

CSV
'''

Let's load and display the `tips <https://github.com/pandas-dev/pandas/blob/main/pandas/tests/io/data/csv/tips.csv>`_
dataset from the pandas tests, which is a CSV file. In Excel, you would download and then
`open the CSV <https://support.microsoft.com/en-us/office/import-or-export-text-txt-or-csv-files-5250ac4c-663c-47ce-937b-339e391393ba>`_.
In pandas, you pass the URL or local path of the CSV file to :func:`~pandas.read_csv`:

.. ipython:: python

   url = (
       "https://raw.github.com/pandas-dev"
       "/pandas/main/pandas/tests/io/data/csv/tips.csv"
   )
   tips = pd.read_csv(url)
   tips

Like `Excel's Text Import Wizard <https://support.microsoft.com/en-us/office/text-import-wizard-c5b02af6-fda1-4440-899f-f78bafe41857>`_,
``read_csv`` can take a number of parameters to specify how the data should be parsed. For
example, if the data was instead tab delimited, and did not have column names, the pandas command
would be:

.. code-block:: python

   tips = pd.read_csv("tips.csv", sep="\t", header=None)

   # alternatively, read_table is an alias to read_csv with tab delimiter
   tips = pd.read_table("tips.csv", header=None)

Excel files
'''''''''''

Excel opens `various Excel file formats <https://support.microsoft.com/en-us/office/file-formats-that-are-supported-in-excel-0943ff2c-6014-4e8d-aaea-b83d51d46247>`_
by double-clicking them, or using `the Open menu <https://support.microsoft.com/en-us/office/open-files-from-the-file-menu-97f087d8-3136-4485-8e86-c5b12a8c4176>`_.
In pandas, you use :ref:`special methods for reading and writing from/to Excel files <io.excel>`.

Let's first :ref:`create a new Excel file <io.excel_writer>` based on the ``tips`` dataframe in the above example:

.. code-block:: python

    tips.to_excel("./tips.xlsx")

Should you wish to subsequently access the data in the ``tips.xlsx`` file, you can read it into your module using

.. code-block:: python

    tips_df = pd.read_excel("./tips.xlsx", index_col=0)

You have just read in an Excel file using pandas!


Limiting output
~~~~~~~~~~~~~~~

Spreadsheet programs will only show one screenful of data at a time and then allow you to scroll, so
there isn't really a need to limit output. In pandas, you'll need to put a little more thought into
controlling how your ``DataFrame``\s are displayed.

.. include:: includes/limit.rst


Exporting data
~~~~~~~~~~~~~~

By default, desktop spreadsheet software will save to its respective file format (``.xlsx``, ``.ods``, etc). You can, however, `save to other file formats <https://support.microsoft.com/en-us/office/save-a-workbook-in-another-file-format-6a16c862-4a36-48f9-a300-c2ca0065286e>`_.

:ref:`pandas can create Excel files <io.excel_writer>`, :ref:`CSV <io.store_in_csv>`, or :ref:`a number of other formats <io>`.

Data operations
---------------

Operations on columns
~~~~~~~~~~~~~~~~~~~~~

In spreadsheets, `formulas
<https://support.microsoft.com/en-us/office/overview-of-formulas-in-excel-ecfdc708-9162-49e8-b993-c311f47ca173>`_
are often created in individual cells and then `dragged
<https://support.microsoft.com/en-us/office/copy-a-formula-by-dragging-the-fill-handle-in-excel-for-mac-dd928259-622b-473f-9a33-83aa1a63e218>`_
into other cells to compute them for other columns. In pandas, you're able to do operations on whole
columns directly.

.. include:: includes/column_operations.rst

Note that we aren't having to tell it to do that subtraction cell-by-cell — pandas handles that for
us. See :ref:`how to create new columns derived from existing columns <10min_tut_05_columns>`.


Filtering
~~~~~~~~~

`In Excel, filtering is done through a graphical menu. <https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e>`_

.. image:: ../../_static/spreadsheets/filter.png
   :alt: Screenshot showing filtering of the total_bill column to values greater than 10
   :align: center

.. include:: includes/filtering.rst

If/then logic
~~~~~~~~~~~~~

Let's say we want to make a ``bucket`` column with values of ``low`` and ``high``, based on whether
the ``total_bill`` is less or more than $10.

In spreadsheets, logical comparison can be done with `conditional formulas
<https://support.microsoft.com/en-us/office/create-conditional-formulas-ca916c57-abd8-4b44-997c-c309b7307831>`_.
We'd use a formula of ``=IF(A2 < 10, "low", "high")``, dragged to all cells in a new ``bucket``
column.

.. image:: ../../_static/spreadsheets/conditional.png
   :alt: Screenshot showing the formula from above in a bucket column of the tips spreadsheet
   :align: center

.. include:: includes/if_then.rst

Date functionality
~~~~~~~~~~~~~~~~~~

*This section will refer to "dates", but timestamps are handled similarly.*

We can think of date functionality in two parts: parsing, and output. In spreadsheets, date values
are generally parsed automatically, though there is a `DATEVALUE
<https://support.microsoft.com/en-us/office/datevalue-function-df8b07d4-7761-4a93-bc33-b7471bbff252>`_
function if you need it. In pandas, you need to explicitly convert plain text to datetime objects,
either :ref:`while reading from a CSV <io.read_csv_table.datetime>` or :ref:`once in a DataFrame
<10min_tut_09_timeseries.properties>`.

Once parsed, spreadsheets display the dates in a default format, though `the format can be changed
<https://support.microsoft.com/en-us/office/format-a-date-the-way-you-want-8e10019e-d5d8-47a1-ba95-db95123d273e>`_.
In pandas, you'll generally want to keep dates as ``datetime`` objects while you're doing
calculations with them. Outputting *parts* of dates (such as the year) is done through `date
functions
<https://support.microsoft.com/en-us/office/date-and-time-functions-reference-fd1b5961-c1ae-4677-be58-074152f97b81>`_
in spreadsheets, and :ref:`datetime properties <10min_tut_09_timeseries.properties>` in pandas.

Given ``date1`` and ``date2`` in columns ``A`` and ``B`` of a spreadsheet, you might have these
formulas:

.. list-table::
    :header-rows: 1
    :widths: auto

    * - column
      - formula
    * - ``date1_year``
      - ``=YEAR(A2)``
    * - ``date2_month``
      - ``=MONTH(B2)``
    * - ``date1_next``
      - ``=DATE(YEAR(A2),MONTH(A2)+1,1)``
    * - ``months_between``
      - ``=DATEDIF(A2,B2,"M")``

The equivalent pandas operations are shown below.

.. include:: includes/time_date.rst

See :ref:`timeseries` for more details.


Selection of columns
~~~~~~~~~~~~~~~~~~~~

In spreadsheets, you can select columns you want by:

- `Hiding columns <https://support.microsoft.com/en-us/office/hide-or-show-rows-or-columns-659c2cad-802e-44ee-a614-dde8443579f8>`_
- `Deleting columns <https://support.microsoft.com/en-us/office/insert-or-delete-rows-and-columns-6f40e6e4-85af-45e0-b39d-65dd504a3246>`_
- `Referencing a range <https://support.microsoft.com/en-us/office/create-or-change-a-cell-reference-c7b8b95d-c594-4488-947e-c835903cebaa>`_ from one worksheet into another

Since spreadsheet columns are typically `named in a header row
<https://support.microsoft.com/en-us/office/turn-excel-table-headers-on-or-off-c91d1742-312c-4480-820f-cf4b534c8b3b>`_,
renaming a column is simply a matter of changing the text in that first cell.

.. include:: includes/column_selection.rst


Sorting by values
~~~~~~~~~~~~~~~~~

Sorting in spreadsheets is accomplished via `the sort dialog <https://support.microsoft.com/en-us/office/sort-data-in-a-range-or-table-62d0b95d-2a90-4610-a6ae-2e545c4a4654>`_.

.. image:: ../../_static/spreadsheets/sort.png
   :alt: Screenshot of dialog from Excel showing sorting by the sex then total_bill columns
   :align: center

.. include:: includes/sorting.rst

String processing
-----------------

Finding length of string
~~~~~~~~~~~~~~~~~~~~~~~~

In spreadsheets, the number of characters in text can be found with the `LEN
<https://support.microsoft.com/en-us/office/len-lenb-functions-29236f94-cedc-429d-affd-b5e33d2c67cb>`_
function. This can be used with the `TRIM
<https://support.microsoft.com/en-us/office/trim-function-410388fa-c5df-49c6-b16c-9e5630b479f9>`_
function to remove extra whitespace.

::

   =LEN(TRIM(A2))

.. include:: includes/length.rst

Note this will still include multiple spaces within the string, so isn't 100% equivalent.


Finding position of substring
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The `FIND
<https://support.microsoft.com/en-us/office/find-findb-functions-c7912941-af2a-4bdf-a553-d0d89b0a0628>`_
spreadsheet function returns the position of a substring, with the first character being ``1``.

.. image:: ../../_static/spreadsheets/sort.png
   :alt: Screenshot of FIND formula being used in Excel
   :align: center

.. include:: includes/find_substring.rst


Extracting substring by position
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Spreadsheets have a `MID
<https://support.microsoft.com/en-us/office/mid-midb-functions-d5f9e25c-d7d6-472e-b568-4ecb12433028>`_
formula for extracting a substring from a given position. To get the first character::

   =MID(A2,1,1)

.. include:: includes/extract_substring.rst


Extracting nth word
~~~~~~~~~~~~~~~~~~~

In Excel, you might use the `Text to Columns Wizard
<https://support.microsoft.com/en-us/office/split-text-into-different-columns-with-the-convert-text-to-columns-wizard-30b14928-5550-41f5-97ca-7a3e9c363ed7>`_
for splitting text and retrieving a specific column. (Note `it's possible to do so through a formula
as well <https://exceljet.net/formula/extract-nth-word-from-text-string>`_.)

.. include:: includes/nth_word.rst


Changing case
~~~~~~~~~~~~~

Spreadsheets provide `UPPER, LOWER, and PROPER functions
<https://support.microsoft.com/en-us/office/change-the-case-of-text-01481046-0fa7-4f3b-a693-496795a7a44d>`_
for converting text to upper, lower, and title case, respectively.

.. include:: includes/case.rst


Merging
-------

.. include:: includes/merge_setup.rst

In Excel, there are `merging of tables can be done through a VLOOKUP
<https://support.microsoft.com/en-us/office/how-can-i-merge-two-or-more-tables-c80a9fce-c1ab-4425-bb96-497dd906d656>`_.

.. image:: ../../_static/spreadsheets/vlookup.png
   :alt: Screenshot showing a VLOOKUP formula between two tables in Excel, with some values being filled in and others with "#N/A"
   :align: center

.. include:: includes/merge.rst

``merge`` has a number of advantages over ``VLOOKUP``:

* The lookup value doesn't need to be the first column of the lookup table
* If multiple rows are matched, there will be one row for each match, instead of just the first
* It will include all columns from the lookup table, instead of just a single specified column
* It supports :ref:`more complex join operations <merging.join>`


Other considerations
--------------------

Fill Handle
~~~~~~~~~~~

Create a series of numbers following a set pattern in a certain set of cells. In
a spreadsheet, this would be done by shift+drag after entering the first number or by
entering the first two or three values and then dragging.

This can be achieved by creating a series and assigning it to the desired cells.

.. ipython:: python

    df = pd.DataFrame({"AAA": [1] * 8, "BBB": list(range(0, 8))})
    df

    series = list(range(1, 5))
    series

    df.loc[2:5, "AAA"] = series

    df

Drop Duplicates
~~~~~~~~~~~~~~~

Excel has built-in functionality for `removing duplicate values <https://support.microsoft.com/en-us/office/find-and-remove-duplicates-00e35bea-b46a-4d5d-b28e-66a552dc138d>`_.
This is supported in pandas via :meth:`~DataFrame.drop_duplicates`.

.. ipython:: python

    df = pd.DataFrame(
        {
            "class": ["A", "A", "A", "B", "C", "D"],
            "student_count": [42, 35, 42, 50, 47, 45],
            "all_pass": ["Yes", "Yes", "Yes", "No", "No", "Yes"],
        }
    )

    df.drop_duplicates()

    df.drop_duplicates(["class", "student_count"])

Pivot Tables
~~~~~~~~~~~~

`PivotTables <https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576>`_
from spreadsheets can be replicated in pandas through :ref:`reshaping`. Using the ``tips`` dataset again,
let's find the average gratuity by size of the party and sex of the server.

In Excel, we use the following configuration for the PivotTable:

.. image:: ../../_static/spreadsheets/pivot.png
   :alt: Screenshot showing a PivotTable in Excel, using sex as the column, size as the rows, then average tip as the values
   :align: center

The equivalent in pandas:

.. ipython:: python

    pd.pivot_table(
        tips, values="tip", index=["size"], columns=["sex"], aggfunc=np.average
    )


Adding a row
~~~~~~~~~~~~

Assuming we are using a :class:`~pandas.RangeIndex` (numbered ``0``, ``1``, etc.), we can use :func:`concat` to add a row to the bottom of a ``DataFrame``.

.. ipython:: python

    df
    new_row = pd.DataFrame([["E", 51, True]],
                           columns=["class", "student_count", "all_pass"])
    pd.concat([df, new_row])


Find and Replace
~~~~~~~~~~~~~~~~

`Excel's Find dialog <https://support.microsoft.com/en-us/office/find-or-replace-text-and-numbers-on-a-worksheet-0e304ca5-ecef-4808-b90f-fdb42f892e90>`_
takes you to cells that match, one by one. In pandas, this operation is generally done for an
entire column or ``DataFrame`` at once through :ref:`conditional expressions <10min_tut_03_subset.rows_and_columns>`.

.. ipython:: python

    tips
    tips == "Sun"
    tips["day"].str.contains("S")

pandas' :meth:`~DataFrame.replace` is comparable to Excel's ``Replace All``.

.. ipython:: python

    tips.replace("Thu", "Thursday")
.. _compare_with_r:

{{ header }}

Comparison with R / R libraries
*******************************

Since pandas aims to provide a lot of the data manipulation and analysis
functionality that people use `R <https://www.r-project.org/>`__ for, this page
was started to provide a more detailed look at the `R language
<https://en.wikipedia.org/wiki/R_(programming_language)>`__ and its many third
party libraries as they relate to pandas. In comparisons with R and CRAN
libraries, we care about the following things:

* **Functionality / flexibility**: what can/cannot be done with each tool
* **Performance**: how fast are operations. Hard numbers/benchmarks are
  preferable
* **Ease-of-use**: Is one tool easier/harder to use (you may have to be
  the judge of this, given side-by-side code comparisons)

This page is also here to offer a bit of a translation guide for users of these
R packages.

For transfer of ``DataFrame`` objects from pandas to R, one option is to
use HDF5 files, see :ref:`io.external_compatibility` for an
example.


Quick reference
---------------

We'll start off with a quick reference guide pairing some common R
operations using `dplyr
<https://cran.r-project.org/web/packages/dplyr/index.html>`__ with
pandas equivalents.


Querying, filtering, sampling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

===========================================  ===========================================
R                                            pandas
===========================================  ===========================================
``dim(df)``                                  ``df.shape``
``head(df)``                                 ``df.head()``
``slice(df, 1:10)``                          ``df.iloc[:9]``
``filter(df, col1 == 1, col2 == 1)``         ``df.query('col1 == 1 & col2 == 1')``
``df[df$col1 == 1 & df$col2 == 1,]``         ``df[(df.col1 == 1) & (df.col2 == 1)]``
``select(df, col1, col2)``                   ``df[['col1', 'col2']]``
``select(df, col1:col3)``                    ``df.loc[:, 'col1':'col3']``
``select(df, -(col1:col3))``                 ``df.drop(cols_to_drop, axis=1)`` but see [#select_range]_
``distinct(select(df, col1))``               ``df[['col1']].drop_duplicates()``
``distinct(select(df, col1, col2))``         ``df[['col1', 'col2']].drop_duplicates()``
``sample_n(df, 10)``                         ``df.sample(n=10)``
``sample_frac(df, 0.01)``                    ``df.sample(frac=0.01)``
===========================================  ===========================================

.. [#select_range] R's shorthand for a subrange of columns
                   (``select(df, col1:col3)``) can be approached
                   cleanly in pandas, if you have the list of columns,
                   for example ``df[cols[1:3]]`` or
                   ``df.drop(cols[1:3])``, but doing this by column
                   name is a bit messy.


Sorting
~~~~~~~

===========================================  ===========================================
R                                            pandas
===========================================  ===========================================
``arrange(df, col1, col2)``                  ``df.sort_values(['col1', 'col2'])``
``arrange(df, desc(col1))``                  ``df.sort_values('col1', ascending=False)``
===========================================  ===========================================

Transforming
~~~~~~~~~~~~

===========================================  ===========================================
R                                            pandas
===========================================  ===========================================
``select(df, col_one = col1)``               ``df.rename(columns={'col1': 'col_one'})['col_one']``
``rename(df, col_one = col1)``               ``df.rename(columns={'col1': 'col_one'})``
``mutate(df, c=a-b)``                        ``df.assign(c=df['a']-df['b'])``
===========================================  ===========================================


Grouping and summarizing
~~~~~~~~~~~~~~~~~~~~~~~~

==============================================  ===========================================
R                                               pandas
==============================================  ===========================================
``summary(df)``                                 ``df.describe()``
``gdf <- group_by(df, col1)``                   ``gdf = df.groupby('col1')``
``summarise(gdf, avg=mean(col1, na.rm=TRUE))``  ``df.groupby('col1').agg({'col1': 'mean'})``
``summarise(gdf, total=sum(col1))``             ``df.groupby('col1').sum()``
==============================================  ===========================================


Base R
------

Slicing with R's |c|_
~~~~~~~~~~~~~~~~~~~~~

R makes it easy to access ``data.frame`` columns by name

.. code-block:: r

   df <- data.frame(a=rnorm(5), b=rnorm(5), c=rnorm(5), d=rnorm(5), e=rnorm(5))
   df[, c("a", "c", "e")]

or by integer location

.. code-block:: r

   df <- data.frame(matrix(rnorm(1000), ncol=100))
   df[, c(1:10, 25:30, 40, 50:100)]

Selecting multiple columns by name in pandas is straightforward

.. ipython:: python

   df = pd.DataFrame(np.random.randn(10, 3), columns=list("abc"))
   df[["a", "c"]]
   df.loc[:, ["a", "c"]]

Selecting multiple noncontiguous columns by integer location can be achieved
with a combination of the ``iloc`` indexer attribute and ``numpy.r_``.

.. ipython:: python

   named = list("abcdefg")
   n = 30
   columns = named + np.arange(len(named), n).tolist()
   df = pd.DataFrame(np.random.randn(n, n), columns=columns)

   df.iloc[:, np.r_[:10, 24:30]]

|aggregate|_
~~~~~~~~~~~~

In R you may want to split data into subsets and compute the mean for each.
Using a data.frame called ``df`` and splitting it into groups ``by1`` and
``by2``:

.. code-block:: r

   df <- data.frame(
     v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),
     v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99),
     by1 = c("red", "blue", 1, 2, NA, "big", 1, 2, "red", 1, NA, 12),
     by2 = c("wet", "dry", 99, 95, NA, "damp", 95, 99, "red", 99, NA, NA))
   aggregate(x=df[, c("v1", "v2")], by=list(mydf2$by1, mydf2$by2), FUN = mean)

The :meth:`~pandas.DataFrame.groupby` method is similar to base R ``aggregate``
function.

.. ipython:: python

   df = pd.DataFrame(
       {
           "v1": [1, 3, 5, 7, 8, 3, 5, np.nan, 4, 5, 7, 9],
           "v2": [11, 33, 55, 77, 88, 33, 55, np.nan, 44, 55, 77, 99],
           "by1": ["red", "blue", 1, 2, np.nan, "big", 1, 2, "red", 1, np.nan, 12],
           "by2": [
               "wet",
               "dry",
               99,
               95,
               np.nan,
               "damp",
               95,
               99,
               "red",
               99,
               np.nan,
               np.nan,
           ],
       }
   )

   g = df.groupby(["by1", "by2"])
   g[["v1", "v2"]].mean()

For more details and examples see :ref:`the groupby documentation
<groupby.split>`.

|match|_
~~~~~~~~~~~~

A common way to select data in R is using ``%in%`` which is defined using the
function ``match``. The operator ``%in%`` is used to return a logical vector
indicating if there is a match or not:

.. code-block:: r

   s <- 0:4
   s %in% c(2,4)

The :meth:`~pandas.DataFrame.isin` method is similar to R ``%in%`` operator:

.. ipython:: python

   s = pd.Series(np.arange(5), dtype=np.float32)
   s.isin([2, 4])

The ``match`` function returns a vector of the positions of matches
of its first argument in its second:

.. code-block:: r

   s <- 0:4
   match(s, c(2,4))

For more details and examples see :ref:`the reshaping documentation
<indexing.basics.indexing_isin>`.

|tapply|_
~~~~~~~~~

``tapply`` is similar to ``aggregate``, but data can be in a ragged array,
since the subclass sizes are possibly irregular. Using a data.frame called
``baseball``, and retrieving information based on the array ``team``:

.. code-block:: r

   baseball <-
     data.frame(team = gl(5, 5,
                labels = paste("Team", LETTERS[1:5])),
                player = sample(letters, 25),
                batting.average = runif(25, .200, .400))

   tapply(baseball$batting.average, baseball.example$team,
          max)

In pandas we may use :meth:`~pandas.pivot_table` method to handle this:

.. ipython:: python

   import random
   import string

   baseball = pd.DataFrame(
       {
           "team": ["team %d" % (x + 1) for x in range(5)] * 5,
           "player": random.sample(list(string.ascii_lowercase), 25),
           "batting avg": np.random.uniform(0.200, 0.400, 25),
       }
   )

   baseball.pivot_table(values="batting avg", columns="team", aggfunc=np.max)

For more details and examples see :ref:`the reshaping documentation
<reshaping.pivot>`.

|subset|_
~~~~~~~~~~

The :meth:`~pandas.DataFrame.query` method is similar to the base R ``subset``
function. In R you might want to get the rows of a ``data.frame`` where one
column's values are less than another column's values:

.. code-block:: r

   df <- data.frame(a=rnorm(10), b=rnorm(10))
   subset(df, a <= b)
   df[df$a <= df$b,]  # note the comma

In pandas, there are a few ways to perform subsetting. You can use
:meth:`~pandas.DataFrame.query` or pass an expression as if it were an
index/slice as well as standard boolean indexing:

.. ipython:: python

   df = pd.DataFrame({"a": np.random.randn(10), "b": np.random.randn(10)})
   df.query("a <= b")
   df[df["a"] <= df["b"]]
   df.loc[df["a"] <= df["b"]]

For more details and examples see :ref:`the query documentation
<indexing.query>`.


|with|_
~~~~~~~~

An expression using a data.frame called ``df`` in R with the columns ``a`` and
``b`` would be evaluated using ``with`` like so:

.. code-block:: r

   df <- data.frame(a=rnorm(10), b=rnorm(10))
   with(df, a + b)
   df$a + df$b  # same as the previous expression

In pandas the equivalent expression, using the
:meth:`~pandas.DataFrame.eval` method, would be:

.. ipython:: python

   df = pd.DataFrame({"a": np.random.randn(10), "b": np.random.randn(10)})
   df.eval("a + b")
   df["a"] + df["b"]  # same as the previous expression

In certain cases :meth:`~pandas.DataFrame.eval` will be much faster than
evaluation in pure Python. For more details and examples see :ref:`the eval
documentation <enhancingperf.eval>`.

plyr
----

``plyr`` is an R library for the split-apply-combine strategy for data
analysis. The functions revolve around three data structures in R, ``a``
for ``arrays``, ``l`` for ``lists``, and ``d`` for ``data.frame``. The
table below shows how these data structures could be mapped in Python.

+------------+-------------------------------+
| R          | Python                        |
+============+===============================+
| array      | list                          |
+------------+-------------------------------+
| lists      | dictionary or list of objects |
+------------+-------------------------------+
| data.frame | dataframe                     |
+------------+-------------------------------+

ddply
~~~~~

An expression using a data.frame called ``df`` in R where you want to
summarize ``x`` by ``month``:

.. code-block:: r

   require(plyr)
   df <- data.frame(
     x = runif(120, 1, 168),
     y = runif(120, 7, 334),
     z = runif(120, 1.7, 20.7),
     month = rep(c(5,6,7,8),30),
     week = sample(1:4, 120, TRUE)
   )

   ddply(df, .(month, week), summarize,
         mean = round(mean(x), 2),
         sd = round(sd(x), 2))

In pandas the equivalent expression, using the
:meth:`~pandas.DataFrame.groupby` method, would be:

.. ipython:: python

   df = pd.DataFrame(
       {
           "x": np.random.uniform(1.0, 168.0, 120),
           "y": np.random.uniform(7.0, 334.0, 120),
           "z": np.random.uniform(1.7, 20.7, 120),
           "month": [5, 6, 7, 8] * 30,
           "week": np.random.randint(1, 4, 120),
       }
   )

   grouped = df.groupby(["month", "week"])
   grouped["x"].agg([np.mean, np.std])


For more details and examples see :ref:`the groupby documentation
<groupby.aggregate>`.

reshape / reshape2
------------------

meltarray
~~~~~~~~~

An expression using a 3 dimensional array called ``a`` in R where you want to
melt it into a data.frame:

.. code-block:: r

   a <- array(c(1:23, NA), c(2,3,4))
   data.frame(melt(a))

In Python, since ``a`` is a list, you can simply use list comprehension.

.. ipython:: python

   a = np.array(list(range(1, 24)) + [np.NAN]).reshape(2, 3, 4)
   pd.DataFrame([tuple(list(x) + [val]) for x, val in np.ndenumerate(a)])

meltlist
~~~~~~~~

An expression using a list called ``a`` in R where you want to melt it
into a data.frame:

.. code-block:: r

   a <- as.list(c(1:4, NA))
   data.frame(melt(a))

In Python, this list would be a list of tuples, so
:meth:`~pandas.DataFrame` method would convert it to a dataframe as required.

.. ipython:: python

   a = list(enumerate(list(range(1, 5)) + [np.NAN]))
   pd.DataFrame(a)

For more details and examples see :ref:`the Into to Data Structures
documentation <dsintro>`.

meltdf
~~~~~~

An expression using a data.frame called ``cheese`` in R where you want to
reshape the data.frame:

.. code-block:: r

   cheese <- data.frame(
     first = c('John', 'Mary'),
     last = c('Doe', 'Bo'),
     height = c(5.5, 6.0),
     weight = c(130, 150)
   )
   melt(cheese, id=c("first", "last"))

In Python, the :meth:`~pandas.melt` method is the R equivalent:

.. ipython:: python

   cheese = pd.DataFrame(
       {
           "first": ["John", "Mary"],
           "last": ["Doe", "Bo"],
           "height": [5.5, 6.0],
           "weight": [130, 150],
       }
   )

   pd.melt(cheese, id_vars=["first", "last"])
   cheese.set_index(["first", "last"]).stack()  # alternative way

For more details and examples see :ref:`the reshaping documentation
<reshaping.melt>`.

cast
~~~~

In R ``acast`` is an expression using a data.frame called ``df`` in R to cast
into a higher dimensional array:

.. code-block:: r

   df <- data.frame(
     x = runif(12, 1, 168),
     y = runif(12, 7, 334),
     z = runif(12, 1.7, 20.7),
     month = rep(c(5,6,7),4),
     week = rep(c(1,2), 6)
   )

   mdf <- melt(df, id=c("month", "week"))
   acast(mdf, week ~ month ~ variable, mean)

In Python the best way is to make use of :meth:`~pandas.pivot_table`:

.. ipython:: python

   df = pd.DataFrame(
       {
           "x": np.random.uniform(1.0, 168.0, 12),
           "y": np.random.uniform(7.0, 334.0, 12),
           "z": np.random.uniform(1.7, 20.7, 12),
           "month": [5, 6, 7] * 4,
           "week": [1, 2] * 6,
       }
   )

   mdf = pd.melt(df, id_vars=["month", "week"])
   pd.pivot_table(
       mdf,
       values="value",
       index=["variable", "week"],
       columns=["month"],
       aggfunc=np.mean,
   )

Similarly for ``dcast`` which uses a data.frame called ``df`` in R to
aggregate information based on ``Animal`` and ``FeedType``:

.. code-block:: r

   df <- data.frame(
     Animal = c('Animal1', 'Animal2', 'Animal3', 'Animal2', 'Animal1',
                'Animal2', 'Animal3'),
     FeedType = c('A', 'B', 'A', 'A', 'B', 'B', 'A'),
     Amount = c(10, 7, 4, 2, 5, 6, 2)
   )

   dcast(df, Animal ~ FeedType, sum, fill=NaN)
   # Alternative method using base R
   with(df, tapply(Amount, list(Animal, FeedType), sum))

Python can approach this in two different ways. Firstly, similar to above
using :meth:`~pandas.pivot_table`:

.. ipython:: python

   df = pd.DataFrame(
       {
           "Animal": [
               "Animal1",
               "Animal2",
               "Animal3",
               "Animal2",
               "Animal1",
               "Animal2",
               "Animal3",
           ],
           "FeedType": ["A", "B", "A", "A", "B", "B", "A"],
           "Amount": [10, 7, 4, 2, 5, 6, 2],
       }
   )

   df.pivot_table(values="Amount", index="Animal", columns="FeedType", aggfunc="sum")

The second approach is to use the :meth:`~pandas.DataFrame.groupby` method:

.. ipython:: python

   df.groupby(["Animal", "FeedType"])["Amount"].sum()

For more details and examples see :ref:`the reshaping documentation
<reshaping.pivot>` or :ref:`the groupby documentation<groupby.split>`.

|factor|_
~~~~~~~~~

pandas has a data type for categorical data.

.. code-block:: r

   cut(c(1,2,3,4,5,6), 3)
   factor(c(1,2,3,2,2,3))

In pandas this is accomplished with ``pd.cut`` and ``astype("category")``:

.. ipython:: python

   pd.cut(pd.Series([1, 2, 3, 4, 5, 6]), 3)
   pd.Series([1, 2, 3, 2, 2, 3]).astype("category")

For more details and examples see :ref:`categorical introduction <categorical>` and the
:ref:`API documentation <api.arrays.categorical>`. There is also a documentation regarding the
:ref:`differences to R's factor <categorical.rfactor>`.


.. |c| replace:: ``c``
.. _c: https://stat.ethz.ch/R-manual/R-patched/library/base/html/c.html

.. |aggregate| replace:: ``aggregate``
.. _aggregate: https://stat.ethz.ch/R-manual/R-patched/library/stats/html/aggregate.html

.. |match| replace:: ``match`` / ``%in%``
.. _match: https://stat.ethz.ch/R-manual/R-patched/library/base/html/match.html

.. |tapply| replace:: ``tapply``
.. _tapply: https://stat.ethz.ch/R-manual/R-patched/library/base/html/tapply.html

.. |with| replace:: ``with``
.. _with: https://stat.ethz.ch/R-manual/R-patched/library/base/html/with.html

.. |subset| replace:: ``subset``
.. _subset: https://stat.ethz.ch/R-manual/R-patched/library/base/html/subset.html

.. |factor| replace:: ``factor``
.. _factor: https://stat.ethz.ch/R-manual/R-devel/library/base/html/factor.html
.. _compare_with_sql:

{{ header }}

Comparison with SQL
********************
Since many potential pandas users have some familiarity with
`SQL <https://en.wikipedia.org/wiki/SQL>`_, this page is meant to provide some examples of how
various SQL operations would be performed using pandas.

.. include:: includes/introduction.rst

Most of the examples will utilize the ``tips`` dataset found within pandas tests.  We'll read
the data into a DataFrame called ``tips`` and assume we have a database table of the same name and
structure.

.. ipython:: python

    url = (
        "https://raw.github.com/pandas-dev"
        "/pandas/main/pandas/tests/io/data/csv/tips.csv"
    )
    tips = pd.read_csv(url)
    tips


Copies vs. in place operations
------------------------------

.. include:: includes/copies.rst


SELECT
------
In SQL, selection is done using a comma-separated list of columns you'd like to select (or a ``*``
to select all columns):

.. code-block:: sql

    SELECT total_bill, tip, smoker, time
    FROM tips;

With pandas, column selection is done by passing a list of column names to your DataFrame:

.. ipython:: python

    tips[["total_bill", "tip", "smoker", "time"]]

Calling the DataFrame without the list of column names would display all columns (akin to SQL's
``*``).

In SQL, you can add a calculated column:

.. code-block:: sql

    SELECT *, tip/total_bill as tip_rate
    FROM tips;

With pandas, you can use the :meth:`DataFrame.assign` method of a DataFrame to append a new column:

.. ipython:: python

    tips.assign(tip_rate=tips["tip"] / tips["total_bill"])

WHERE
-----
Filtering in SQL is done via a WHERE clause.

.. code-block:: sql

    SELECT *
    FROM tips
    WHERE time = 'Dinner';

.. include:: includes/filtering.rst

Just like SQL's ``OR`` and ``AND``, multiple conditions can be passed to a DataFrame using ``|``
(``OR``) and ``&`` (``AND``).

Tips of more than $5 at Dinner meals:

.. code-block:: sql

    SELECT *
    FROM tips
    WHERE time = 'Dinner' AND tip > 5.00;

.. ipython:: python

    tips[(tips["time"] == "Dinner") & (tips["tip"] > 5.00)]

Tips by parties of at least 5 diners OR bill total was more than $45:

.. code-block:: sql

    SELECT *
    FROM tips
    WHERE size >= 5 OR total_bill > 45;

.. ipython:: python

    tips[(tips["size"] >= 5) | (tips["total_bill"] > 45)]

NULL checking is done using the :meth:`~pandas.Series.notna` and :meth:`~pandas.Series.isna`
methods.

.. ipython:: python

    frame = pd.DataFrame(
        {"col1": ["A", "B", np.NaN, "C", "D"], "col2": ["F", np.NaN, "G", "H", "I"]}
    )
    frame

Assume we have a table of the same structure as our DataFrame above. We can see only the records
where ``col2`` IS NULL with the following query:

.. code-block:: sql

    SELECT *
    FROM frame
    WHERE col2 IS NULL;

.. ipython:: python

    frame[frame["col2"].isna()]

Getting items where ``col1`` IS NOT NULL can be done with :meth:`~pandas.Series.notna`.

.. code-block:: sql

    SELECT *
    FROM frame
    WHERE col1 IS NOT NULL;

.. ipython:: python

    frame[frame["col1"].notna()]


GROUP BY
--------
In pandas, SQL's ``GROUP BY`` operations are performed using the similarly named
:meth:`~pandas.DataFrame.groupby` method. :meth:`~pandas.DataFrame.groupby` typically refers to a
process where we'd like to split a dataset into groups, apply some function (typically aggregation)
, and then combine the groups together.

A common SQL operation would be getting the count of records in each group throughout a dataset.
For instance, a query getting us the number of tips left by sex:

.. code-block:: sql

    SELECT sex, count(*)
    FROM tips
    GROUP BY sex;
    /*
    Female     87
    Male      157
    */


The pandas equivalent would be:

.. ipython:: python

    tips.groupby("sex").size()

Notice that in the pandas code we used :meth:`~pandas.core.groupby.DataFrameGroupBy.size` and not
:meth:`~pandas.core.groupby.DataFrameGroupBy.count`. This is because
:meth:`~pandas.core.groupby.DataFrameGroupBy.count` applies the function to each column, returning
the number of ``NOT NULL`` records within each.

.. ipython:: python

    tips.groupby("sex").count()

Alternatively, we could have applied the :meth:`~pandas.core.groupby.DataFrameGroupBy.count` method
to an individual column:

.. ipython:: python

    tips.groupby("sex")["total_bill"].count()

Multiple functions can also be applied at once. For instance, say we'd like to see how tip amount
differs by day of the week - :meth:`~pandas.core.groupby.DataFrameGroupBy.agg` allows you to pass a dictionary
to your grouped DataFrame, indicating which functions to apply to specific columns.

.. code-block:: sql

    SELECT day, AVG(tip), COUNT(*)
    FROM tips
    GROUP BY day;
    /*
    Fri   2.734737   19
    Sat   2.993103   87
    Sun   3.255132   76
    Thu  2.771452   62
    */

.. ipython:: python

    tips.groupby("day").agg({"tip": np.mean, "day": np.size})

Grouping by more than one column is done by passing a list of columns to the
:meth:`~pandas.DataFrame.groupby` method.

.. code-block:: sql

    SELECT smoker, day, COUNT(*), AVG(tip)
    FROM tips
    GROUP BY smoker, day;
    /*
    smoker day
    No     Fri      4  2.812500
           Sat     45  3.102889
           Sun     57  3.167895
           Thu    45  2.673778
    Yes    Fri     15  2.714000
           Sat     42  2.875476
           Sun     19  3.516842
           Thu    17  3.030000
    */

.. ipython:: python

    tips.groupby(["smoker", "day"]).agg({"tip": [np.size, np.mean]})

.. _compare_with_sql.join:

JOIN
----
``JOIN``\s can be performed with :meth:`~pandas.DataFrame.join` or :meth:`~pandas.merge`. By
default, :meth:`~pandas.DataFrame.join` will join the DataFrames on their indices. Each method has
parameters allowing you to specify the type of join to perform (``LEFT``, ``RIGHT``, ``INNER``,
``FULL``) or the columns to join on (column names or indices).

.. warning::

    If both key columns contain rows where the key is a null value, those
    rows will be matched against each other. This is different from usual SQL
    join behaviour and can lead to unexpected results.

.. ipython:: python

    df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})
    df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})

Assume we have two database tables of the same name and structure as our DataFrames.

Now let's go over the various types of ``JOIN``\s.

INNER JOIN
~~~~~~~~~~
.. code-block:: sql

    SELECT *
    FROM df1
    INNER JOIN df2
      ON df1.key = df2.key;

.. ipython:: python

    # merge performs an INNER JOIN by default
    pd.merge(df1, df2, on="key")

:meth:`~pandas.merge` also offers parameters for cases when you'd like to join one DataFrame's
column with another DataFrame's index.

.. ipython:: python

    indexed_df2 = df2.set_index("key")
    pd.merge(df1, indexed_df2, left_on="key", right_index=True)

LEFT OUTER JOIN
~~~~~~~~~~~~~~~

Show all records from ``df1``.

.. code-block:: sql

    SELECT *
    FROM df1
    LEFT OUTER JOIN df2
      ON df1.key = df2.key;

.. ipython:: python

    pd.merge(df1, df2, on="key", how="left")

RIGHT JOIN
~~~~~~~~~~

Show all records from ``df2``.

.. code-block:: sql

    SELECT *
    FROM df1
    RIGHT OUTER JOIN df2
      ON df1.key = df2.key;

.. ipython:: python

    pd.merge(df1, df2, on="key", how="right")

FULL JOIN
~~~~~~~~~
pandas also allows for ``FULL JOIN``\s, which display both sides of the dataset, whether or not the
joined columns find a match. As of writing, ``FULL JOIN``\s are not supported in all RDBMS (MySQL).

Show all records from both tables.

.. code-block:: sql

    SELECT *
    FROM df1
    FULL OUTER JOIN df2
      ON df1.key = df2.key;

.. ipython:: python

    pd.merge(df1, df2, on="key", how="outer")


UNION
-----

``UNION ALL`` can be performed using :meth:`~pandas.concat`.

.. ipython:: python

    df1 = pd.DataFrame(
        {"city": ["Chicago", "San Francisco", "New York City"], "rank": range(1, 4)}
    )
    df2 = pd.DataFrame(
        {"city": ["Chicago", "Boston", "Los Angeles"], "rank": [1, 4, 5]}
    )

.. code-block:: sql

    SELECT city, rank
    FROM df1
    UNION ALL
    SELECT city, rank
    FROM df2;
    /*
             city  rank
          Chicago     1
    San Francisco     2
    New York City     3
          Chicago     1
           Boston     4
      Los Angeles     5
    */

.. ipython:: python

    pd.concat([df1, df2])

SQL's ``UNION`` is similar to ``UNION ALL``, however ``UNION`` will remove duplicate rows.

.. code-block:: sql

    SELECT city, rank
    FROM df1
    UNION
    SELECT city, rank
    FROM df2;
    -- notice that there is only one Chicago record this time
    /*
             city  rank
          Chicago     1
    San Francisco     2
    New York City     3
           Boston     4
      Los Angeles     5
    */

In pandas, you can use :meth:`~pandas.concat` in conjunction with
:meth:`~pandas.DataFrame.drop_duplicates`.

.. ipython:: python

    pd.concat([df1, df2]).drop_duplicates()


LIMIT
-----

.. code-block:: sql

    SELECT * FROM tips
    LIMIT 10;

.. ipython:: python

    tips.head(10)


pandas equivalents for some SQL analytic and aggregate functions
----------------------------------------------------------------

Top n rows with offset
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: sql

    -- MySQL
    SELECT * FROM tips
    ORDER BY tip DESC
    LIMIT 10 OFFSET 5;

.. ipython:: python

    tips.nlargest(10 + 5, columns="tip").tail(10)

Top n rows per group
~~~~~~~~~~~~~~~~~~~~

.. code-block:: sql

    -- Oracle's ROW_NUMBER() analytic function
    SELECT * FROM (
      SELECT
        t.*,
        ROW_NUMBER() OVER(PARTITION BY day ORDER BY total_bill DESC) AS rn
      FROM tips t
    )
    WHERE rn < 3
    ORDER BY day, rn;


.. ipython:: python

    (
        tips.assign(
            rn=tips.sort_values(["total_bill"], ascending=False)
            .groupby(["day"])
            .cumcount()
            + 1
        )
        .query("rn < 3")
        .sort_values(["day", "rn"])
    )

the same using ``rank(method='first')`` function

.. ipython:: python

    (
        tips.assign(
            rnk=tips.groupby(["day"])["total_bill"].rank(
                method="first", ascending=False
            )
        )
        .query("rnk < 3")
        .sort_values(["day", "rnk"])
    )

.. code-block:: sql

    -- Oracle's RANK() analytic function
    SELECT * FROM (
      SELECT
        t.*,
        RANK() OVER(PARTITION BY sex ORDER BY tip) AS rnk
      FROM tips t
      WHERE tip < 2
    )
    WHERE rnk < 3
    ORDER BY sex, rnk;

Let's find tips with (rank < 3) per gender group for (tips < 2).
Notice that when using ``rank(method='min')`` function
``rnk_min`` remains the same for the same ``tip``
(as Oracle's ``RANK()`` function)

.. ipython:: python

    (
        tips[tips["tip"] < 2]
        .assign(rnk_min=tips.groupby(["sex"])["tip"].rank(method="min"))
        .query("rnk_min < 3")
        .sort_values(["sex", "rnk_min"])
    )


UPDATE
------

.. code-block:: sql

    UPDATE tips
    SET tip = tip*2
    WHERE tip < 2;

.. ipython:: python

    tips.loc[tips["tip"] < 2, "tip"] *= 2

DELETE
------

.. code-block:: sql

    DELETE FROM tips
    WHERE tip > 9;

In pandas we select the rows that should remain instead of deleting them:

.. ipython:: python

    tips = tips.loc[tips["tip"] <= 9]
{{ header }}

.. _comparison:

===========================
Comparison with other tools
===========================

.. toctree::
    :maxdepth: 2

    comparison_with_r
    comparison_with_sql
    comparison_with_spreadsheets
    comparison_with_sas
    comparison_with_stata
.. _compare_with_stata:

{{ header }}

Comparison with Stata
*********************
For potential users coming from `Stata <https://en.wikipedia.org/wiki/Stata>`__
this page is meant to demonstrate how different Stata operations would be
performed in pandas.

.. include:: includes/introduction.rst


Data structures
---------------

General terminology translation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. csv-table::
    :header: "pandas", "Stata"
    :widths: 20, 20

    ``DataFrame``, data set
    column, variable
    row, observation
    groupby, bysort
    ``NaN``, ``.``


``DataFrame``
~~~~~~~~~~~~~

A ``DataFrame`` in pandas is analogous to a Stata data set -- a two-dimensional
data source with labeled columns that can be of different types. As will be
shown in this document, almost any operation that can be applied to a data set
in Stata can also be accomplished in pandas.

``Series``
~~~~~~~~~~

A ``Series`` is the data structure that represents one column of a
``DataFrame``. Stata doesn't have a separate data structure for a single column,
but in general, working with a ``Series`` is analogous to referencing a column
of a data set in Stata.

``Index``
~~~~~~~~~

Every ``DataFrame`` and ``Series`` has an ``Index`` -- labels on the
*rows* of the data. Stata does not have an exactly analogous concept. In Stata, a data set's
rows are essentially unlabeled, other than an implicit integer index that can be
accessed with ``_n``.

In pandas, if no index is specified, an integer index is also used by default
(first row = 0, second row = 1, and so on). While using a labeled ``Index`` or
``MultiIndex`` can enable sophisticated analyses and is ultimately an important
part of pandas to understand, for this comparison we will essentially ignore the
``Index`` and just treat the ``DataFrame`` as a collection of columns. Please
see the :ref:`indexing documentation<indexing>` for much more on how to use an
``Index`` effectively.


Copies vs. in place operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: includes/copies.rst


Data input / output
-------------------

Constructing a DataFrame from values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Stata data set can be built from specified values by
placing the data after an ``input`` statement and
specifying the column names.

.. code-block:: stata

   input x y
   1 2
   3 4
   5 6
   end

.. include:: includes/construct_dataframe.rst

Reading external data
~~~~~~~~~~~~~~~~~~~~~

Like Stata, pandas provides utilities for reading in data from
many formats.  The ``tips`` data set, found within the pandas
tests (`csv <https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/tips.csv>`_)
will be used in many of the following examples.

Stata provides ``import delimited`` to read csv data into a data set in memory.
If the ``tips.csv`` file is in the current working directory, we can import it as follows.

.. code-block:: stata

   import delimited tips.csv

The pandas method is :func:`read_csv`, which works similarly. Additionally, it will automatically download
the data set if presented with a url.

.. ipython:: python

   url = (
       "https://raw.github.com/pandas-dev"
       "/pandas/main/pandas/tests/io/data/csv/tips.csv"
   )
   tips = pd.read_csv(url)
   tips

Like ``import delimited``, :func:`read_csv` can take a number of parameters to specify
how the data should be parsed.  For example, if the data were instead tab delimited,
did not have column names, and existed in the current working directory,
the pandas command would be:

.. code-block:: python

   tips = pd.read_csv("tips.csv", sep="\t", header=None)

   # alternatively, read_table is an alias to read_csv with tab delimiter
   tips = pd.read_table("tips.csv", header=None)

pandas can also read Stata data sets in ``.dta`` format with the :func:`read_stata` function.

.. code-block:: python

   df = pd.read_stata("data.dta")

In addition to text/csv and Stata files, pandas supports a variety of other data formats
such as Excel, SAS, HDF5, Parquet, and SQL databases.  These are all read via a ``pd.read_*``
function.  See the :ref:`IO documentation<io>` for more details.


Limiting output
~~~~~~~~~~~~~~~

.. include:: includes/limit.rst

The equivalent in Stata would be:

.. code-block:: stata

   list in 1/5


Exporting data
~~~~~~~~~~~~~~

The inverse of ``import delimited`` in Stata is ``export delimited``

.. code-block:: stata

   export delimited tips2.csv

Similarly in pandas, the opposite of ``read_csv`` is :meth:`DataFrame.to_csv`.

.. code-block:: python

   tips.to_csv("tips2.csv")

pandas can also export to Stata file format with the :meth:`DataFrame.to_stata` method.

.. code-block:: python

   tips.to_stata("tips2.dta")


Data operations
---------------

Operations on columns
~~~~~~~~~~~~~~~~~~~~~

In Stata, arbitrary math expressions can be used with the ``generate`` and
``replace`` commands on new or existing columns. The ``drop`` command drops
the column from the data set.

.. code-block:: stata

   replace total_bill = total_bill - 2
   generate new_bill = total_bill / 2
   drop new_bill

.. include:: includes/column_operations.rst


Filtering
~~~~~~~~~

Filtering in Stata is done with an ``if`` clause on one or more columns.

.. code-block:: stata

   list if total_bill > 10

.. include:: includes/filtering.rst

If/then logic
~~~~~~~~~~~~~

In Stata, an ``if`` clause can also be used to create new columns.

.. code-block:: stata

   generate bucket = "low" if total_bill < 10
   replace bucket = "high" if total_bill >= 10

.. include:: includes/if_then.rst

Date functionality
~~~~~~~~~~~~~~~~~~

Stata provides a variety of functions to do operations on
date/datetime columns.

.. code-block:: stata

   generate date1 = mdy(1, 15, 2013)
   generate date2 = date("Feb152015", "MDY")

   generate date1_year = year(date1)
   generate date2_month = month(date2)

   * shift date to beginning of next month
   generate date1_next = mdy(month(date1) + 1, 1, year(date1)) if month(date1) != 12
   replace date1_next = mdy(1, 1, year(date1) + 1) if month(date1) == 12
   generate months_between = mofd(date2) - mofd(date1)

   list date1 date2 date1_year date2_month date1_next months_between

The equivalent pandas operations are shown below.  In addition to these
functions, pandas supports other Time Series features
not available in Stata (such as time zone handling and custom offsets) --
see the :ref:`timeseries documentation<timeseries>` for more details.

.. include:: includes/time_date.rst

Selection of columns
~~~~~~~~~~~~~~~~~~~~

Stata provides keywords to select, drop, and rename columns.

.. code-block:: stata

   keep sex total_bill tip

   drop sex

   rename total_bill total_bill_2

.. include:: includes/column_selection.rst


Sorting by values
~~~~~~~~~~~~~~~~~

Sorting in Stata is accomplished via ``sort``

.. code-block:: stata

   sort sex total_bill

.. include:: includes/sorting.rst

String processing
-----------------

Finding length of string
~~~~~~~~~~~~~~~~~~~~~~~~

Stata determines the length of a character string with the :func:`strlen` and
:func:`ustrlen` functions for ASCII and Unicode strings, respectively.

.. code-block:: stata

   generate strlen_time = strlen(time)
   generate ustrlen_time = ustrlen(time)

.. include:: includes/length.rst


Finding position of substring
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Stata determines the position of a character in a string with the :func:`strpos` function.
This takes the string defined by the first argument and searches for the
first position of the substring you supply as the second argument.

.. code-block:: stata

   generate str_position = strpos(sex, "ale")

.. include:: includes/find_substring.rst


Extracting substring by position
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Stata extracts a substring from a string based on its position with the :func:`substr` function.

.. code-block:: stata

   generate short_sex = substr(sex, 1, 1)

.. include:: includes/extract_substring.rst


Extracting nth word
~~~~~~~~~~~~~~~~~~~

The Stata :func:`word` function returns the nth word from a string.
The first argument is the string you want to parse and the
second argument specifies which word you want to extract.

.. code-block:: stata

   clear
   input str20 string
   "John Smith"
   "Jane Cook"
   end

   generate first_name = word(name, 1)
   generate last_name = word(name, -1)

.. include:: includes/nth_word.rst


Changing case
~~~~~~~~~~~~~

The Stata :func:`strupper`, :func:`strlower`, :func:`strproper`,
:func:`ustrupper`, :func:`ustrlower`, and :func:`ustrtitle` functions
change the case of ASCII and Unicode strings, respectively.

.. code-block:: stata

   clear
   input str20 string
   "John Smith"
   "Jane Cook"
   end

   generate upper = strupper(string)
   generate lower = strlower(string)
   generate title = strproper(string)
   list

.. include:: includes/case.rst


Merging
-------

.. include:: includes/merge_setup.rst

In Stata, to perform a merge, one data set must be in memory
and the other must be referenced as a file name on disk. In
contrast, Python must have both ``DataFrames`` already in memory.

By default, Stata performs an outer join, where all observations
from both data sets are left in memory after the merge. One can
keep only observations from the initial data set, the merged data set,
or the intersection of the two by using the values created in the
``_merge`` variable.

.. code-block:: stata

   * First create df2 and save to disk
   clear
   input str1 key
   B
   D
   D
   E
   end
   generate value = rnormal()
   save df2.dta

   * Now create df1 in memory
   clear
   input str1 key
   A
   B
   C
   D
   end
   generate value = rnormal()

   preserve

   * Left join
   merge 1:n key using df2.dta
   keep if _merge == 1

   * Right join
   restore, preserve
   merge 1:n key using df2.dta
   keep if _merge == 2

   * Inner join
   restore, preserve
   merge 1:n key using df2.dta
   keep if _merge == 3

   * Outer join
   restore
   merge 1:n key using df2.dta

.. include:: includes/merge.rst


Missing data
------------

Both pandas and Stata have a representation for missing data.

.. include:: includes/missing_intro.rst

One difference is that missing data cannot be compared to its sentinel value.
For example, in Stata you could do this to filter missing values.

.. code-block:: stata

   * Keep missing values
   list if value_x == .
   * Keep non-missing values
   list if value_x != .

.. include:: includes/missing.rst


GroupBy
-------

Aggregation
~~~~~~~~~~~

Stata's ``collapse`` can be used to group by one or
more key variables and compute aggregations on
numeric columns.

.. code-block:: stata

   collapse (sum) total_bill tip, by(sex smoker)

.. include:: includes/groupby.rst


Transformation
~~~~~~~~~~~~~~

In Stata, if the group aggregations need to be used with the
original data set, one would usually use ``bysort`` with :func:`egen`.
For example, to subtract the mean for each observation by smoker group.

.. code-block:: stata

   bysort sex smoker: egen group_bill = mean(total_bill)
   generate adj_total_bill = total_bill - group_bill

.. include:: includes/transform.rst


By group processing
~~~~~~~~~~~~~~~~~~~

In addition to aggregation, pandas ``groupby`` can be used to
replicate most other ``bysort`` processing from Stata. For example,
the following example lists the first observation in the current
sort order by sex/smoker group.

.. code-block:: stata

   bysort sex smoker: list if _n == 1

In pandas this would be written as:

.. ipython:: python

   tips.groupby(["sex", "smoker"]).first()


Other considerations
--------------------

Disk vs memory
~~~~~~~~~~~~~~

pandas and Stata both operate exclusively in memory. This means that the size of
data able to be loaded in pandas is limited by your machine's memory.
If out of core processing is needed, one possibility is the
`dask.dataframe <https://docs.dask.org/en/latest/dataframe.html>`_
library, which provides a subset of pandas functionality for an
on-disk ``DataFrame``.
The same operations are expressed in pandas below.

Keep certain columns
''''''''''''''''''''

.. ipython:: python

   tips[["sex", "total_bill", "tip"]]

Drop a column
'''''''''''''

.. ipython:: python

   tips.drop("sex", axis=1)

Rename a column
'''''''''''''''

.. ipython:: python

   tips.rename(columns={"total_bill": "total_bill_2"})
pandas provides vectorized operations by specifying the individual ``Series`` in the
``DataFrame``. New columns can be assigned in the same way. The :meth:`DataFrame.drop` method drops
a column from the ``DataFrame``.

.. ipython:: python

   tips["total_bill"] = tips["total_bill"] - 2
   tips["new_bill"] = tips["total_bill"] / 2
   tips

   tips = tips.drop("new_bill", axis=1)
You can find the length of a character string with :meth:`Series.str.len`.
In Python 3, all strings are Unicode strings. ``len`` includes trailing blanks.
Use ``len`` and ``rstrip`` to exclude trailing blanks.

.. ipython:: python

   tips["time"].str.len()
   tips["time"].str.rstrip().str.len()
The same operation in pandas can be accomplished using
the ``where`` method from ``numpy``.

.. ipython:: python

   tips["bucket"] = np.where(tips["total_bill"] < 10, "low", "high")
   tips

.. ipython:: python
   :suppress:

   tips = tips.drop("bucket", axis=1)
pandas provides a flexible ``groupby`` mechanism that allows similar aggregations. See the
:ref:`groupby documentation<groupby>` for more details and examples.

.. ipython:: python

   tips_summed = tips.groupby(["sex", "smoker"])[["total_bill", "tip"]].sum()
   tips_summed
DataFrames can be filtered in multiple ways; the most intuitive of which is using
:ref:`boolean indexing <indexing.boolean>`.

.. ipython:: python

   tips[tips["total_bill"] > 10]

The above statement is simply passing a ``Series`` of ``True``/``False`` objects to the DataFrame,
returning all rows with ``True``.

.. ipython:: python

    is_dinner = tips["time"] == "Dinner"
    is_dinner
    is_dinner.value_counts()
    tips[is_dinner]
The equivalent pandas methods are :meth:`Series.str.upper`, :meth:`Series.str.lower`, and
:meth:`Series.str.title`.

.. ipython:: python

   firstlast = pd.DataFrame({"string": ["John Smith", "Jane Cook"]})
   firstlast["upper"] = firstlast["string"].str.upper()
   firstlast["lower"] = firstlast["string"].str.lower()
   firstlast["title"] = firstlast["string"].str.title()
   firstlast
With pandas you can use ``[]`` notation to extract a substring
from a string by position locations. Keep in mind that Python
indexes are zero-based.

.. ipython:: python

   tips["sex"].str[0:1]
In pandas, :meth:`Series.isna` and :meth:`Series.notna` can be used to filter the rows.

.. ipython:: python

   outer_join[outer_join["value_x"].isna()]
   outer_join[outer_join["value_x"].notna()]

pandas provides :ref:`a variety of methods to work with missing data <missing_data>`. Here are some examples:

Drop rows with missing values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. ipython:: python

   outer_join.dropna()

Forward fill from previous rows
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. ipython:: python

   outer_join.fillna(method="ffill")

Replace missing values with a specified value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using the mean:

.. ipython:: python

   outer_join["value_x"].fillna(outer_join["value_x"].mean())
By default, pandas will truncate output of large ``DataFrame``\s to show the first and last rows.
This can be overridden by :ref:`changing the pandas options <options>`, or using
:meth:`DataFrame.head` or :meth:`DataFrame.tail`.

.. ipython:: python

   tips.head(5)
The simplest way to extract words in pandas is to split the strings by spaces, then reference the
word by index. Note there are more powerful approaches should you need them.

.. ipython:: python

   firstlast = pd.DataFrame({"String": ["John Smith", "Jane Cook"]})
   firstlast["First_Name"] = firstlast["String"].str.split(" ", expand=True)[0]
   firstlast["Last_Name"] = firstlast["String"].str.rsplit(" ", expand=True)[1]
   firstlast
pandas DataFrames have a :meth:`~DataFrame.merge` method, which provides similar functionality. The
data does not have to be sorted ahead of time, and different join types are accomplished via the
``how`` keyword.

.. ipython:: python

   inner_join = df1.merge(df2, on=["key"], how="inner")
   inner_join

   left_join = df1.merge(df2, on=["key"], how="left")
   left_join

   right_join = df1.merge(df2, on=["key"], how="right")
   right_join

   outer_join = df1.merge(df2, on=["key"], how="outer")
   outer_join
.. ipython:: python

   tips["date1"] = pd.Timestamp("2013-01-15")
   tips["date2"] = pd.Timestamp("2015-02-15")
   tips["date1_year"] = tips["date1"].dt.year
   tips["date2_month"] = tips["date2"].dt.month
   tips["date1_next"] = tips["date1"] + pd.offsets.MonthBegin()
   tips["months_between"] = tips["date2"].dt.to_period("M") - tips[
       "date1"
   ].dt.to_period("M")

   tips[
       ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"]
   ]

.. ipython:: python
   :suppress:

   tips = tips.drop(
       ["date1", "date2", "date1_year", "date2_month", "date1_next", "months_between"],
       axis=1,
   )
If you're new to pandas, you might want to first read through :ref:`10 Minutes to pandas<10min>`
to familiarize yourself with the library.

As is customary, we import pandas and NumPy as follows:

.. ipython:: python

    import pandas as pd
    import numpy as np
The following tables will be used in the merge examples:

.. ipython:: python

   df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})
   df1
   df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})
   df2
Most pandas operations return copies of the ``Series``/``DataFrame``. To make the changes "stick",
you'll need to either assign to a new variable:

   .. code-block:: python

      sorted_df = df.sort_values("col1")


or overwrite the original one:

   .. code-block:: python

      df = df.sort_values("col1")

.. note::

   You will see an ``inplace=True`` keyword argument available for some methods:

   .. code-block:: python

      df.sort_values("col1", inplace=True)

   Its use is discouraged. :ref:`More information. <indexing.view_versus_copy>`
pandas provides a :ref:`groupby.transform` mechanism that allows these type of operations to be
succinctly expressed in one operation.

.. ipython:: python

   gb = tips.groupby("smoker")["total_bill"]
   tips["adj_total_bill"] = tips["total_bill"] - gb.transform("mean")
   tips
A pandas ``DataFrame`` can be constructed in many different ways,
but for a small number of values, it is often convenient to specify it as
a Python dictionary, where the keys are the column names
and the values are the data.

.. ipython:: python

   df = pd.DataFrame({"x": [1, 3, 5], "y": [2, 4, 6]})
   df
pandas represents missing data with the special float value ``NaN`` (not a number).  Many of the
semantics are the same; for example missing data propagates through numeric operations, and is
ignored by default for aggregations.

.. ipython:: python

   outer_join
   outer_join["value_x"] + outer_join["value_y"]
   outer_join["value_x"].sum()
pandas has a :meth:`DataFrame.sort_values` method, which takes a list of columns to sort by.

.. ipython:: python

   tips = tips.sort_values(["sex", "total_bill"])
   tips
You can find the position of a character in a column of strings with the :meth:`Series.str.find`
method. ``find`` searches for the first position of the substring. If the substring is found, the
method returns its position. If not found, it returns ``-1``. Keep in mind that Python indexes are
zero-based.

.. ipython:: python

   tips["sex"].str.find("ale")
sphinxext
=========

This directory contains copies of different sphinx extensions in use in the
pandas documentation. These copies originate from other projects:

- ``numpydoc`` - Numpy's Sphinx extensions: this can be found at its own
  repository: https://github.com/numpy/numpydoc
- ``ipython_directive`` and ``ipython_console_highlighting`` in the folder
  ``ipython_sphinxext`` - Sphinx extensions from IPython: these are included
  in IPython: https://github.com/ipython/ipython/tree/master/IPython/sphinxext

.. note::

    These copies are maintained at the respective projects, so fixes should,
    to the extent possible, be pushed upstream instead of only adapting our
    local copy to avoid divergence between the local and upstream version.
