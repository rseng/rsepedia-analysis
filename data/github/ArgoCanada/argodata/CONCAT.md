
<!-- README.md is generated from README.Rmd. Please edit that file -->

# argodata

<!-- badges: start -->

[![R build
status](https://github.com/ArgoCanada/argodata/workflows/R-CMD-check/badge.svg)](https://github.com/ArgoCanada/argodata/actions)
[![Codecov test
coverage](https://codecov.io/gh/ArgoCanada/argodata/branch/master/graph/badge.svg)](https://codecov.io/gh/ArgoCanada/argodata?branch=master)
<!-- badges: end -->

The goal of argodata is to provide a data frame-based interface to data
generated by the [Argo floats program](https://argo.ucsd.edu)
([doi:10.17882/42182](https://doi.org/10.17882/42182)). This package
provides low-level access to all variables contained within Argo NetCDF
files; for a higher-level interface with built-in visualization and
quality control, see the [argoFloats
package](https://github.com/ArgoCanada/argoFloats).

## Installation

You can install the development version from
[GitHub](https://github.com/) using:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

The argodata package downloads files from the [FTP and HTTPS
mirrors](http://www.argodatamgt.org/Access-to-data/Access-via-FTP-or-HTTPS-on-GDAC),
caches them, and loads them into R. You can set the mirror using
`argo_set_mirror()` and the cache directory using
`argo_set_cache_dir()`:

``` r
argo_set_mirror("https://data-argo.ifremer.fr/")
argo_set_cache_dir("my/argo/cache")
```

Optionally, you can set `options(argodata.cache_dir = "my/argo/cache")`
in your .Rprofile to persist this value between R sessions (see
`usethis::edit_r_profile()`). Cached files are used indefinitely by
default because of the considerable time it takes to refresh them. If
you do use a persistent cache, you should update the index files
regularly using `argo_update_global()` (data files are also updated
occasionally; update these using `argo_update_data()`).

## Example

The main workflow supported by argodata is:

-   Start with `argo_global_prof()`, `argo_global_traj()`,
    `argo_global_meta()`, `argo_global_bio_prof()`,
    `argo_global_bio_prof()`, or `argo_global_synthetic_prof()`. These
    functions return data frames that contain meta information about
    each data file in Argo.
-   Use `argo_filter_radius()`, other `argo_filter_*()` functions, or
    `dplyr::filter()` to subset the index using search criteria.
-   Use extractor functions like `argo_prof_levels()`,
    `argo_traj_measurement()`, `argo_meta_config_param()`, and
    `argo_tech_tech_param()` to read information for all files in the
    index subset.

``` r
library(tidyverse)
library(argodata)

# filter profile index using search criteria
prof_lab_may_2020 <- argo_global_prof() %>%
  argo_filter_rect(50, 60, -60, -50) %>% 
  filter(
    lubridate::year(date) == 2020, 
    lubridate::month(date) == 5
  )

# download, cache, and load NetCDF files
levels_lab_may_2020 <- prof_lab_may_2020 %>% 
  argo_prof_levels()
#> Downloading 55 files from 'https://data-argo.ifremer.fr'
#> Extracting from 55 files
# plot!
levels_lab_may_2020 %>% 
  filter(psal_qc == 1) %>% 
  ggplot(aes(x = psal, y = pres, col = temp)) +
  geom_point() +
  scale_y_reverse() +
  theme_bw()
```

<img src="man/figures/README-example-1.png" width="100%" />

See the reference for `argo_prof_levels()` for more ways to load Argo
profiles from `argo_global_prof()`, `argo_global_bio_prof()` and
`argo_global_synthetic_prof()`; see `argo_traj_measurement()` for ways
to load Argo trajectories from `argo_global_traj()` or
`argo_global_bio_traj()`; see `argo_meta_missions()` for ways to load
float meta from `argo_global_meta()`; see `argo_tech_tech_param()` for
ways to load float technical information from `argo_global_tech()`; and
see `argo_info()` and `argo_vars()` for ways to load global metadata
from Argo NetCDF files.

## Advanced

The argodata package also exports the low-level readers it uses to
produce tables from Argo NetCDF files. You can access these using
`argo_read_*()` functions.

``` r
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_prof_levels(prof_file)
#> # A tibble: 70 × 17
#>    N_LEVELS N_PROF   PRES PRES_QC PRES_ADJUSTED PRES_ADJUSTED_QC
#>  *    <int>  <int>  <dbl> <chr>           <dbl> <chr>           
#>  1        1      1   9.80 1                9.80 1               
#>  2        2      1  20.1  1               20.1  1               
#>  3        3      1  29.9  1               29.9  1               
#>  4        4      1  39.7  1               39.7  1               
#>  5        5      1  49.9  1               49.9  1               
#>  6        6      1  60.3  1               60.3  1               
#>  7        7      1  69.7  1               69.7  1               
#>  8        8      1  80.3  1               80.3  1               
#>  9        9      1  90.2  1               90.2  1               
#> 10       10      1 100.   1              100.   1               
#> # … with 60 more rows, and 11 more variables: PRES_ADJUSTED_ERROR <dbl>,
#> #   TEMP <dbl>, TEMP_QC <chr>, TEMP_ADJUSTED <dbl>, TEMP_ADJUSTED_QC <chr>,
#> #   TEMP_ADJUSTED_ERROR <dbl>, PSAL <dbl>, PSAL_QC <chr>, PSAL_ADJUSTED <dbl>,
#> #   PSAL_ADJUSTED_QC <chr>, PSAL_ADJUSTED_ERROR <dbl>
```

## Code of Conduct

Please note that argodata is released with a [Contributor Code of
Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html).
By contributing to this project, you agree to abide by its terms.
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards
of acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
when an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at [INSERT CONTACT
METHOD]. All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0,
available at https://www.contributor-covenant.org/version/2/0/
code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at https://
www.contributor-covenant.org/translations.
# Contributing to argodata

This outlines how to propose a change to argodata. 
For more detailed info about contributing to this, and other tidyverse packages, please see the
[**development contributing guide**](https://rstd.io/tidy-contrib). 

## Fixing typos

You can fix typos, spelling mistakes, or grammatical errors in the documentation directly using the GitHub web interface, as long as the changes are made in the _source_ file. 
This generally means you'll need to edit [roxygen2 comments](https://roxygen2.r-lib.org/articles/roxygen2.html) in an `.R`, not a `.Rd` file. 
You can find the `.R` file that generates the `.Rd` by reading the comment in the first line.

## Bigger changes

If you want to make a bigger change, it's a good idea to first file an issue and make sure someone from the team agrees that it’s needed. 
If you’ve found a bug, please file an issue that illustrates the bug with a minimal 
[reprex](https://www.tidyverse.org/help/#reprex) (this will also help you write a unit test, if needed).

### Pull request process

*   Fork the package and clone onto your computer. If you haven't done this before, we recommend using `usethis::create_from_github("ArgoCanada/argodata", fork = TRUE)`.

*   Install all development dependences with `devtools::install_dev_deps()`, and then make sure the package passes R CMD check by running `devtools::check()`. 
    If R CMD check doesn't pass cleanly, it's a good idea to ask for help before continuing. 
*   Create a Git branch for your pull request (PR). We recommend using `usethis::pr_init("brief-description-of-change")`.

*   Make your changes, commit to git, and then create a PR by running `usethis::pr_push()`, and following the prompts in your browser.
    The title of your PR should briefly describe the change.
    The body of your PR should contain `Fixes #issue-number`.

*  For user-facing changes, add a bullet to the top of `NEWS.md` (i.e. just below the first header). Follow the style described in <https://style.tidyverse.org/news.html>.

### Code style

*   New code should follow the tidyverse [style guide](https://style.tidyverse.org). 
    You can use the [styler](https://CRAN.R-project.org/package=styler) package to apply these styles, but please don't restyle code that has nothing to do with your PR.  

*  We use [roxygen2](https://cran.r-project.org/package=roxygen2), with [Markdown syntax](https://cran.r-project.org/web/packages/roxygen2/vignettes/rd-formatting.html), for documentation.  

*  We use [testthat](https://cran.r-project.org/package=testthat) for unit tests. 
   Contributions with test cases included are easier to accept.  

## Code of Conduct

Please note that the argodata project is released with a
[Contributor Code of Conduct](CODE_OF_CONDUCT.md). By contributing to this
project you agree to abide by its terms.
---
title: >
  argodata: An R interface to oceanographic data from
  the International Argo Program
tags:
  - R
  - oceanography
  - measurement
  - Argo float
authors:
  - name: Dewey Dunnington^[Corresponding author.]
    orcid: 0000-0002-9415-4582
    affiliation: 1
  - name: Jaimie Harbin
    orcid: 0000-0003-3774-3732
    affiliation: 1
  - name: Dan E. Kelley
    orcid: 0000-0001-7808-5911
    affiliation: 2
  - name: Clark Richards
    orcid: 0000-0002-7833-206X
    affiliation: 1
affiliations:
  - name: Fisheries and Oceans Canada, Bedford Institute of Oceanography, Dartmouth, NS, Canada
    index: 1
  - name: Department of Oceanography, Dalhousie University, Halifax, NS, Canada
    index: 2
citation_author: Dunnington et. al.
date: 14 May 2021
year: 2021
bibliography: paper.bib
output: rticles::joss_article
csl: apa.csl
journal: JOSS
---



# Summary

This paper describes ``argodata``, an R package that makes it easier to work with data acquired in the International Argo Program, which provides over two decades of oceanographic measurements from around the world. Although Argo data are publicly available in NetCDF format and several software packages are available to assist in locating and downloading relevant Argo data, the multidimensional arrays used can be difficult to understand for non-oceanographers, particulary for the expanding arrays of biogeochemical variables measured by Argo floats. Given the increasing use of Argo data in other disciplines, we built a minimal interface to the data set that uses the data frame as the primary data structure. This approach allows users to leverage the rich ecosystem of R packages that manipulate data frames (e.g., the ``tidyverse``) and associated instructional resources.

# Introduction

The ocean is highly variable in both space and time and mapping this variability at appropriate scales is a key factor in many scientific studies. Oceanographic data have direct applications that range from the analysis of near-bottom ecosystems to air-sea interactions.  More broadly, ocean measurements are needed to constrain the models that scientists use to understand the evolving state of the ocean and to make predictions about its future, particularly as a component of the global climate system.

The International Argo Program [@argo] deploys and collects data from several thousand devices that are programmed to drift with and move vertically through the ocean. Sensors measure electrical conductivity, temperature, pressure, and other quantities along this vertical path yielding "profiles" that are uploaded via satellite to globally distributed data assembly centres [@roemmich_argo_2001; @roemmich_argo_2009-2]. Since 1997, the International Argo Program has collected over 2.4 million profiles from around the globe and expanded its original array of sensors to measure biogeochemicical variables such as pH, chloryphyll-a, dissolved oxygen, nitrate, and many others.

Although the NetCDF data files provided by Argo data servers contain metadata that describe their contents, we identified a number of barriers to data access. These included (1) reading and decoding the index files to locate files of interest, (2) downloading and potentially caching large numbers of small NetCDF files, (3) reading the NetCDF files into a form where the data contained within can be visualized and analyzed, and (4) dealing efficiently with potentially large Argo data sets. In particular, the incorporation of biogeochemical variables in Argo NetCDF files introduced additional complexity such that a novice- to average-level programmer may have difficulty extracting and manipulating data from many profiles. Whereas a variety of applications have been created to address some of these barriers, the ``argodata`` package is our attempt to overcome these barriers for the novice- to average-level programmer who may not be familiar with oceanographic conventions for storing data.

# Statement of need

In the R language, several tools are available to access data from the International Argo Program. The ``oce`` package provides facilities to read and analyze "profile" and "trajectory" Argo NetCDF files [@kelley_oceanographic_2018; @r-oce]; the ``argoFloats`` package provides additional tools to locate, download, cache, and visualize Argo NetCDF files [@kelley_argofloats_2021]; and ``rnoaa`` provides limited access to a subset of Argo data from the North Atlantic [@r-rnoaa]. Outside of R, the ``argopy`` package for Python provides access to the Argo data set with some facilities for analysis and visualization [@maze_argopy_2020], and several web applications provide visual tools to locate relevant Argo profiles based on user-defined search criteria [@oceanops_oceanops_2021; @tucker_argovis_2020].

Several barriers we identified are not specific to the Argo data set and can be overcome with well-established R tools. To download and potentially cache Argo NetCDF files, at least one Argo mirror provides an `rsync` target for profile and index files. The ``bowerbird`` package provides similar facilities for downloading and caching large numbers of files from a remote source [@r-bowerbird]. To analyze and visualize potentially large data sets, ``dplyr`` and ``ggplot2`` within the wider ``tidyverse`` family of packages are well-established [@r-ggplot2; @r-dplyr; @r-tidyverse]. To read NetCDF files in a form that can be analyzed and plotted using ``dplyr`` and ``ggplot2``, respectively, the ``tidync`` and ``ncmeta`` packages introduce the concept of "grids" to identify groups of variables that can be loaded into a single data frame [@r-ncmeta; @r-tidync]. 

The ``argodata`` package was designed to work with a range of tools that manipulate R data frames. In particular, the ``tidyverse`` family of packages has a large user base and has widely and freely available educational material in several languages [@wickham_grolemund17]. Whereas previous packages for R and Python propagate the multidimensional array format of Argo NetCDF files when read, the ability to leverage the ``tidyverse`` depends on the representation of Argo data as data frames in  "tidy" (one observation per row, one variable per column) format [@wickham14], around which packages in the ``tidyverse`` are designed.

# Using ``argodata``

The ``argodata`` package is available as an R source package from GitHub (<https://github.com/ArgoCanada/argodata>), installable using the ``remotes`` package:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

For our example usage, we also load the ``tidyverse``:


```r
library(tidyverse)
library(argodata)
```

To locate files of interest on the Argo mirror, index files for profile, trajectory, meta, and technical parameter files are provided in compressed CSV format. ``argodata`` uses the ``vroom`` package  to efficiently load these files as they can be time-consuming to repeatedly read otherwise. The most commonly-used index is for profile files:


```r
(prof <- argo_global_prof())
```

```
## Loading argo_global_prof()
```

```
## Downloading 1 file from 'https://data-argo.ifremer.fr'
```

```
## # A tibble: 2,556,123 x 8
##    file     date                latitude longitude ocean profiler_type
##    <chr>    <dttm>                 <dbl>     <dbl> <chr>         <dbl>
##  1 aoml/13~ 1997-07-29 20:03:00    0.267     -16.0 A               845
##  2 aoml/13~ 1997-08-09 19:21:12    0.072     -17.7 A               845
##  3 aoml/13~ 1997-08-20 18:45:45    0.543     -19.6 A               845
##  4 aoml/13~ 1997-08-31 19:39:05    1.26      -20.5 A               845
##  5 aoml/13~ 1997-09-11 18:58:08    0.72      -20.8 A               845
##  6 aoml/13~ 1997-09-22 19:57:02    1.76      -21.6 A               845
##  7 aoml/13~ 1997-10-03 19:15:49    2.60      -21.6 A               845
##  8 aoml/13~ 1997-10-14 18:39:35    1.76      -21.6 A               845
##  9 aoml/13~ 1997-10-25 19:32:34    1.80      -21.8 A               845
## 10 aoml/13~ 1997-11-05 18:51:42    1.64      -21.4 A               845
## # ... with 2,556,113 more rows, and 2 more variables:
## #   institution <chr>, date_update <dttm>
```

A typical analysis will focus on a subset of profiles. Users can subset this index using existing knowledge of data frames in R; however, some common subsets are verbose using existing tools or difficult to compute without knowing Argo-specific filename conventions. To match the syntax of `dplyr::filter()`, ``argodata`` provides several `argo_filter_*()` functions to subset index data frames:


```r
prof_gulf_stream_2020 <- prof %>% 
  argo_filter_radius(latitude = 26, longitude = -84, radius = 500) %>%
  argo_filter_date("2020-01-01", "2020-12-31") %>%
  argo_filter_data_mode("delayed")
```

The next step is to download the selected files. The explicit call to `argo_download()` is typically omitted as it is done automatically for missing files by the load functions described below; however, one can manually call `argo_download()` to download (if necessary) and cache files in an index. To facilitate use of alternative cache solutions like `rsync` or ``bowerbird`` [@r-bowerbird], we use the same file structure as the mirror itself and provide `argo_set_cache_dir()` to allow this directory to be used for all calls to `argo_download()`.

To load data from NetCDF files into meaningful data frames we draw from the concept of "grids" introduced by the ``tidync`` and ``ncmeta`` packages [@r-ncmeta; @r-tidync]. For example, temperature values stored in an Argo profile NetCDF file are identified by values of `N_PROF` (an integer identifying a profile within an Argo NetCDF file) and `N_LEVEL` (an integer identifying a sampling level within a profile). Temperature values can be represented by a matrix with one row per `N_LEVELS` and one column per `N_PROF` or by a data frame with variables `N_PROF`, `N_LEVELS`, and `TEMP`. Any other variables that share the dimensions of the temperature variable can be added as additional columns in the data frame. After looping through each file in a complete copy of the Argo data set, we identified 19 grids among four Argo NetCDF file formats (profile, trajectory, metadata, and technical information) [@argo-user-manual]. The most commonly-used grid is the levels grid for Argo profile files:


```r
(levels <- prof_gulf_stream_2020 %>% 
  argo_prof_levels())
```

```
## Downloading 921 files from 'https://data-argo.ifremer.fr'
```

```
## Extracting from 921 files
```

```
## # A tibble: 1,785,666 x 18
##    file   n_levels n_prof  pres pres_qc pres_adjusted pres_adjusted_qc
##    <chr>     <int>  <int> <dbl> <chr>           <dbl> <chr>           
##  1 aoml/~        1      1  1.12 1                1.12 1               
##  2 aoml/~        2      1  2    1                2    1               
##  3 aoml/~        3      1  3    1                3    1               
##  4 aoml/~        4      1  4    1                4    1               
##  5 aoml/~        5      1  4.96 1                4.96 1               
##  6 aoml/~        6      1  6    1                6    1               
##  7 aoml/~        7      1  7    1                7    1               
##  8 aoml/~        8      1  7.92 1                7.92 1               
##  9 aoml/~        9      1  9    1                9    1               
## 10 aoml/~       10      1 10    1               10    1               
## # ... with 1,785,656 more rows, and 11 more variables:
## #   pres_adjusted_error <dbl>, temp <dbl>, temp_qc <chr>,
## #   temp_adjusted <dbl>, temp_adjusted_qc <chr>,
## #   temp_adjusted_error <dbl>, psal <dbl>, psal_qc <chr>,
## #   psal_adjusted <dbl>, psal_adjusted_qc <chr>,
## #   psal_adjusted_error <dbl>
```

Like `argo_prof_levels()`, other extraction functions use the pattern `argo_{file type}_{grid}()` and use a split-apply-combine strategy that row-binds the results obtained by reading each file individually [@wickham11]. To facilitate users who prefer to manage their own collection of Argo files, corresponding `argo_read_{file type}_{grid}()` functions that read a single file are also exported. Extraction functions are designed to return useful inputs to ``dplyr`` and ``ggplot2``. For example, a common way to visualize profile data is to plot a dependent variable (e.g., temperature) against pressure (as a proxy for depth), with pressure oriented vertically to simulate its orientation in space.


```r
ggplot(levels, aes(x = temp, y = pres)) +
  geom_line(aes(group = file), alpha = 0.01, orientation = "y") +
  scale_y_reverse() +
  scale_x_continuous(position = "top") +
  theme_bw() +
  labs(
    x = "Temperature [°C]",
    y = "Pressure [dbar]"
  )
```

![](fig-levels-1.png)<!-- --> 

# Interoperability

The ``argodata`` package was designed to interoperate with the ``argoFloats`` and ``oce`` packages for users who prefer to do part of their analyses using the facilities provided by these packages. In particular, these packages provide specialized functions for mapping and oceanographic analysis that are outside the scope of ``argodata``. For example, one can combine the trajectory plotting capability of ``argoFloats`` with a ``dplyr`` `group_by()` and `summarise()` enabled by ``argodata`` and visualized using colour palettes from ``cmocean`` [@r-cmocean].


```r
library(argoFloats)

# use argoFloats to locate profiles
index <- getIndex() %>% subset(ID = 4903252)

# calculate mean surface temperature using argodata
temp_calc <- index %>% 
  argo_prof_levels() %>% 
  filter(pres < 10) %>% 
  group_by(file) %>%
  summarise(
    near_surface_temp = mean(temp, na.rm = TRUE)
  ) %>%
  mutate(
    near_surface_temp_bin = cut_width(near_surface_temp, width = 2)
  ) %>% 
  left_join(argo_global_prof(), by = "file")

# use plot method for argoFloats index and add temperatures
par(mar = c(3, 3, 1, 2))
plot(index, which = "map", type = "l")

# plot temperatures
palette(cmocean::cmocean("thermal")(5))
points(
  temp_calc$longitude, temp_calc$latitude,
  bg = temp_calc$near_surface_temp_bin, pch = 21, cex = 1
)

legend(
  "topleft",
  levels(temp_calc$near_surface_temp_bin), pt.bg = palette(), pch = 21,
  title = "Near-surface temperature [°C]", ncol = 3
)
```

![](fig-argofloats-1.png)<!-- --> 

# Conclusion

The ``argodata`` package helps scientists analyze data from the International Argo Program using a minimal table-based interface. We hope that ``argodata`` will expand the audience of Argo data to users already familiar with data frame manipulation tools such as those provided by the ``tidyverse`` family of packages.

# Acknowledgements

We acknowledge useful discussions with Chris Gordon, especially regarding the extraction of quality control information from Argo data files. We thank the editors and reviewers for their thoughtful and careful review of this manuscript. Support for this work came from the Natural Sciences and Engineering Research Council of Canada and G7 Charlevoix Blueprint for Healthy Oceans, Seas and Resilient Coastal Communities. The data used in this paper were collected and made freely available by the International Argo Program and the national programs that contribute to it (<https://argo.ucsd.edu>, <https://www.ocean-ops.org>). The Argo Program [@argo] is part of the Global Ocean Observing System.

# References
