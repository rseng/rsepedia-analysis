
<!-- README.md is generated from README.Rmd. Please edit that file -->

# argodata

<!-- badges: start -->

[![R build
status](https://github.com/ArgoCanada/argodata/workflows/R-CMD-check/badge.svg)](https://github.com/ArgoCanada/argodata/actions)
[![Codecov test
coverage](https://codecov.io/gh/ArgoCanada/argodata/branch/master/graph/badge.svg)](https://codecov.io/gh/ArgoCanada/argodata?branch=master)
<!-- badges: end -->

The goal of argodata is to provide a data frame-based interface to data
generated by the [Argo floats program](https://argo.ucsd.edu)
([doi:10.17882/42182](https://doi.org/10.17882/42182)). This package
provides low-level access to all variables contained within Argo NetCDF
files; for a higher-level interface with built-in visualization and
quality control, see the [argoFloats
package](https://github.com/ArgoCanada/argoFloats).

## Installation

You can install the development version from
[GitHub](https://github.com/) using:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

The argodata package downloads files from the [FTP and HTTPS
mirrors](http://www.argodatamgt.org/Access-to-data/Access-via-FTP-or-HTTPS-on-GDAC),
caches them, and loads them into R. You can set the mirror using
`argo_set_mirror()` and the cache directory using
`argo_set_cache_dir()`:

``` r
argo_set_mirror("https://data-argo.ifremer.fr/")
argo_set_cache_dir("my/argo/cache")
```

Optionally, you can set `options(argodata.cache_dir = "my/argo/cache")`
in your .Rprofile to persist this value between R sessions (see
`usethis::edit_r_profile()`). Cached files are used indefinitely by
default because of the considerable time it takes to refresh them. If
you do use a persistent cache, you should update the index files
regularly using `argo_update_global()` (data files are also updated
occasionally; update these using `argo_update_data()`).

## Example

The main workflow supported by argodata is:

-   Start with `argo_global_prof()`, `argo_global_traj()`,
    `argo_global_meta()`, `argo_global_bio_prof()`,
    `argo_global_bio_prof()`, or `argo_global_synthetic_prof()`. These
    functions return data frames that contain meta information about
    each data file in Argo.
-   Use `argo_filter_radius()`, other `argo_filter_*()` functions, or
    `dplyr::filter()` to subset the index using search criteria.
-   Use extractor functions like `argo_prof_levels()`,
    `argo_traj_measurement()`, `argo_meta_config_param()`, and
    `argo_tech_tech_param()` to read information for all files in the
    index subset.

``` r
library(tidyverse)
library(argodata)

# filter profile index using search criteria
prof_lab_may_2020 <- argo_global_prof() %>%
  argo_filter_rect(50, 60, -60, -50) %>% 
  filter(
    lubridate::year(date) == 2020, 
    lubridate::month(date) == 5
  )

# download, cache, and load NetCDF files
levels_lab_may_2020 <- prof_lab_may_2020 %>% 
  argo_prof_levels()
#> Downloading 55 files from 'https://data-argo.ifremer.fr'
#> Extracting from 55 files
# plot!
levels_lab_may_2020 %>% 
  filter(psal_qc == 1) %>% 
  ggplot(aes(x = psal, y = pres, col = temp)) +
  geom_point() +
  scale_y_reverse() +
  theme_bw()
```

<img src="man/figures/README-example-1.png" width="100%" />

See the reference for `argo_prof_levels()` for more ways to load Argo
profiles from `argo_global_prof()`, `argo_global_bio_prof()` and
`argo_global_synthetic_prof()`; see `argo_traj_measurement()` for ways
to load Argo trajectories from `argo_global_traj()` or
`argo_global_bio_traj()`; see `argo_meta_missions()` for ways to load
float meta from `argo_global_meta()`; see `argo_tech_tech_param()` for
ways to load float technical information from `argo_global_tech()`; and
see `argo_info()` and `argo_vars()` for ways to load global metadata
from Argo NetCDF files.

## Advanced

The argodata package also exports the low-level readers it uses to
produce tables from Argo NetCDF files. You can access these using
`argo_read_*()` functions.

``` r
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_prof_levels(prof_file)
#> # A tibble: 70 × 17
#>    N_LEVELS N_PROF   PRES PRES_QC PRES_ADJUSTED PRES_ADJUSTED_QC
#>  *    <int>  <int>  <dbl> <chr>           <dbl> <chr>           
#>  1        1      1   9.80 1                9.80 1               
#>  2        2      1  20.1  1               20.1  1               
#>  3        3      1  29.9  1               29.9  1               
#>  4        4      1  39.7  1               39.7  1               
#>  5        5      1  49.9  1               49.9  1               
#>  6        6      1  60.3  1               60.3  1               
#>  7        7      1  69.7  1               69.7  1               
#>  8        8      1  80.3  1               80.3  1               
#>  9        9      1  90.2  1               90.2  1               
#> 10       10      1 100.   1              100.   1               
#> # … with 60 more rows, and 11 more variables: PRES_ADJUSTED_ERROR <dbl>,
#> #   TEMP <dbl>, TEMP_QC <chr>, TEMP_ADJUSTED <dbl>, TEMP_ADJUSTED_QC <chr>,
#> #   TEMP_ADJUSTED_ERROR <dbl>, PSAL <dbl>, PSAL_QC <chr>, PSAL_ADJUSTED <dbl>,
#> #   PSAL_ADJUSTED_QC <chr>, PSAL_ADJUSTED_ERROR <dbl>
```

## Code of Conduct

Please note that argodata is released with a [Contributor Code of
Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html).
By contributing to this project, you agree to abide by its terms.
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards
of acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
when an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at [INSERT CONTACT
METHOD]. All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0,
available at https://www.contributor-covenant.org/version/2/0/
code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at https://
www.contributor-covenant.org/translations.
# Contributing to argodata

This outlines how to propose a change to argodata. 
For more detailed info about contributing to this, and other tidyverse packages, please see the
[**development contributing guide**](https://rstd.io/tidy-contrib). 

## Fixing typos

You can fix typos, spelling mistakes, or grammatical errors in the documentation directly using the GitHub web interface, as long as the changes are made in the _source_ file. 
This generally means you'll need to edit [roxygen2 comments](https://roxygen2.r-lib.org/articles/roxygen2.html) in an `.R`, not a `.Rd` file. 
You can find the `.R` file that generates the `.Rd` by reading the comment in the first line.

## Bigger changes

If you want to make a bigger change, it's a good idea to first file an issue and make sure someone from the team agrees that it’s needed. 
If you’ve found a bug, please file an issue that illustrates the bug with a minimal 
[reprex](https://www.tidyverse.org/help/#reprex) (this will also help you write a unit test, if needed).

### Pull request process

*   Fork the package and clone onto your computer. If you haven't done this before, we recommend using `usethis::create_from_github("ArgoCanada/argodata", fork = TRUE)`.

*   Install all development dependences with `devtools::install_dev_deps()`, and then make sure the package passes R CMD check by running `devtools::check()`. 
    If R CMD check doesn't pass cleanly, it's a good idea to ask for help before continuing. 
*   Create a Git branch for your pull request (PR). We recommend using `usethis::pr_init("brief-description-of-change")`.

*   Make your changes, commit to git, and then create a PR by running `usethis::pr_push()`, and following the prompts in your browser.
    The title of your PR should briefly describe the change.
    The body of your PR should contain `Fixes #issue-number`.

*  For user-facing changes, add a bullet to the top of `NEWS.md` (i.e. just below the first header). Follow the style described in <https://style.tidyverse.org/news.html>.

### Code style

*   New code should follow the tidyverse [style guide](https://style.tidyverse.org). 
    You can use the [styler](https://CRAN.R-project.org/package=styler) package to apply these styles, but please don't restyle code that has nothing to do with your PR.  

*  We use [roxygen2](https://cran.r-project.org/package=roxygen2), with [Markdown syntax](https://cran.r-project.org/web/packages/roxygen2/vignettes/rd-formatting.html), for documentation.  

*  We use [testthat](https://cran.r-project.org/package=testthat) for unit tests. 
   Contributions with test cases included are easier to accept.  

## Code of Conduct

Please note that the argodata project is released with a
[Contributor Code of Conduct](CODE_OF_CONDUCT.md). By contributing to this
project you agree to abide by its terms.
---
title: >
  argodata: An R interface to oceanographic data from
  the International Argo Program
tags:
  - R
  - oceanography
  - measurement
  - Argo float
authors:
  - name: Dewey Dunnington^[Corresponding author.]
    orcid: 0000-0002-9415-4582
    affiliation: 1
  - name: Jaimie Harbin
    orcid: 0000-0003-3774-3732
    affiliation: 1
  - name: Dan E. Kelley
    orcid: 0000-0001-7808-5911
    affiliation: 2
  - name: Clark Richards
    orcid: 0000-0002-7833-206X
    affiliation: 1
affiliations:
  - name: Fisheries and Oceans Canada, Bedford Institute of Oceanography, Dartmouth, NS, Canada
    index: 1
  - name: Department of Oceanography, Dalhousie University, Halifax, NS, Canada
    index: 2
citation_author: Dunnington et. al.
date: 14 May 2021
year: 2021
bibliography: paper.bib
output: rticles::joss_article
csl: apa.csl
journal: JOSS
---



# Summary

This paper describes ``argodata``, an R package that makes it easier to work with data acquired in the International Argo Program, which provides over two decades of oceanographic measurements from around the world. Although Argo data are publicly available in NetCDF format and several software packages are available to assist in locating and downloading relevant Argo data, the multidimensional arrays used can be difficult to understand for non-oceanographers, particulary for the expanding arrays of biogeochemical variables measured by Argo floats. Given the increasing use of Argo data in other disciplines, we built a minimal interface to the data set that uses the data frame as the primary data structure. This approach allows users to leverage the rich ecosystem of R packages that manipulate data frames (e.g., the ``tidyverse``) and associated instructional resources.

# Introduction

The ocean is highly variable in both space and time and mapping this variability at appropriate scales is a key factor in many scientific studies. Oceanographic data have direct applications that range from the analysis of near-bottom ecosystems to air-sea interactions.  More broadly, ocean measurements are needed to constrain the models that scientists use to understand the evolving state of the ocean and to make predictions about its future, particularly as a component of the global climate system.

The International Argo Program [@argo] deploys and collects data from several thousand devices that are programmed to drift with and move vertically through the ocean. Sensors measure electrical conductivity, temperature, pressure, and other quantities along this vertical path yielding "profiles" that are uploaded via satellite to globally distributed data assembly centres [@roemmich_argo_2001; @roemmich_argo_2009-2]. Since 1997, the International Argo Program has collected over 2.4 million profiles from around the globe and expanded its original array of sensors to measure biogeochemicical variables such as pH, chloryphyll-a, dissolved oxygen, nitrate, and many others.

Although the NetCDF data files provided by Argo data servers contain metadata that describe their contents, we identified a number of barriers to data access. These included (1) reading and decoding the index files to locate files of interest, (2) downloading and potentially caching large numbers of small NetCDF files, (3) reading the NetCDF files into a form where the data contained within can be visualized and analyzed, and (4) dealing efficiently with potentially large Argo data sets. In particular, the incorporation of biogeochemical variables in Argo NetCDF files introduced additional complexity such that a novice- to average-level programmer may have difficulty extracting and manipulating data from many profiles. Whereas a variety of applications have been created to address some of these barriers, the ``argodata`` package is our attempt to overcome these barriers for the novice- to average-level programmer who may not be familiar with oceanographic conventions for storing data.

# Statement of need

In the R language, several tools are available to access data from the International Argo Program. The ``oce`` package provides facilities to read and analyze "profile" and "trajectory" Argo NetCDF files [@kelley_oceanographic_2018; @r-oce]; the ``argoFloats`` package provides additional tools to locate, download, cache, and visualize Argo NetCDF files [@kelley_argofloats_2021]; and ``rnoaa`` provides limited access to a subset of Argo data from the North Atlantic [@r-rnoaa]. Outside of R, the ``argopy`` package for Python provides access to the Argo data set with some facilities for analysis and visualization [@maze_argopy_2020], and several web applications provide visual tools to locate relevant Argo profiles based on user-defined search criteria [@oceanops_oceanops_2021; @tucker_argovis_2020].

Several barriers we identified are not specific to the Argo data set and can be overcome with well-established R tools. To download and potentially cache Argo NetCDF files, at least one Argo mirror provides an `rsync` target for profile and index files. The ``bowerbird`` package provides similar facilities for downloading and caching large numbers of files from a remote source [@r-bowerbird]. To analyze and visualize potentially large data sets, ``dplyr`` and ``ggplot2`` within the wider ``tidyverse`` family of packages are well-established [@r-ggplot2; @r-dplyr; @r-tidyverse]. To read NetCDF files in a form that can be analyzed and plotted using ``dplyr`` and ``ggplot2``, respectively, the ``tidync`` and ``ncmeta`` packages introduce the concept of "grids" to identify groups of variables that can be loaded into a single data frame [@r-ncmeta; @r-tidync]. 

The ``argodata`` package was designed to work with a range of tools that manipulate R data frames. In particular, the ``tidyverse`` family of packages has a large user base and has widely and freely available educational material in several languages [@wickham_grolemund17]. Whereas previous packages for R and Python propagate the multidimensional array format of Argo NetCDF files when read, the ability to leverage the ``tidyverse`` depends on the representation of Argo data as data frames in  "tidy" (one observation per row, one variable per column) format [@wickham14], around which packages in the ``tidyverse`` are designed.

# Using ``argodata``

The ``argodata`` package is available as an R source package from GitHub (<https://github.com/ArgoCanada/argodata>), installable using the ``remotes`` package:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

For our example usage, we also load the ``tidyverse``:


```r
library(tidyverse)
library(argodata)
```

To locate files of interest on the Argo mirror, index files for profile, trajectory, meta, and technical parameter files are provided in compressed CSV format. ``argodata`` uses the ``vroom`` package  to efficiently load these files as they can be time-consuming to repeatedly read otherwise. The most commonly-used index is for profile files:


```r
(prof <- argo_global_prof())
```

```
## Loading argo_global_prof()
```

```
## Downloading 1 file from 'https://data-argo.ifremer.fr'
```

```
## # A tibble: 2,556,123 x 8
##    file     date                latitude longitude ocean profiler_type
##    <chr>    <dttm>                 <dbl>     <dbl> <chr>         <dbl>
##  1 aoml/13~ 1997-07-29 20:03:00    0.267     -16.0 A               845
##  2 aoml/13~ 1997-08-09 19:21:12    0.072     -17.7 A               845
##  3 aoml/13~ 1997-08-20 18:45:45    0.543     -19.6 A               845
##  4 aoml/13~ 1997-08-31 19:39:05    1.26      -20.5 A               845
##  5 aoml/13~ 1997-09-11 18:58:08    0.72      -20.8 A               845
##  6 aoml/13~ 1997-09-22 19:57:02    1.76      -21.6 A               845
##  7 aoml/13~ 1997-10-03 19:15:49    2.60      -21.6 A               845
##  8 aoml/13~ 1997-10-14 18:39:35    1.76      -21.6 A               845
##  9 aoml/13~ 1997-10-25 19:32:34    1.80      -21.8 A               845
## 10 aoml/13~ 1997-11-05 18:51:42    1.64      -21.4 A               845
## # ... with 2,556,113 more rows, and 2 more variables:
## #   institution <chr>, date_update <dttm>
```

A typical analysis will focus on a subset of profiles. Users can subset this index using existing knowledge of data frames in R; however, some common subsets are verbose using existing tools or difficult to compute without knowing Argo-specific filename conventions. To match the syntax of `dplyr::filter()`, ``argodata`` provides several `argo_filter_*()` functions to subset index data frames:


```r
prof_gulf_stream_2020 <- prof %>% 
  argo_filter_radius(latitude = 26, longitude = -84, radius = 500) %>%
  argo_filter_date("2020-01-01", "2020-12-31") %>%
  argo_filter_data_mode("delayed")
```

The next step is to download the selected files. The explicit call to `argo_download()` is typically omitted as it is done automatically for missing files by the load functions described below; however, one can manually call `argo_download()` to download (if necessary) and cache files in an index. To facilitate use of alternative cache solutions like `rsync` or ``bowerbird`` [@r-bowerbird], we use the same file structure as the mirror itself and provide `argo_set_cache_dir()` to allow this directory to be used for all calls to `argo_download()`.

To load data from NetCDF files into meaningful data frames we draw from the concept of "grids" introduced by the ``tidync`` and ``ncmeta`` packages [@r-ncmeta; @r-tidync]. For example, temperature values stored in an Argo profile NetCDF file are identified by values of `N_PROF` (an integer identifying a profile within an Argo NetCDF file) and `N_LEVEL` (an integer identifying a sampling level within a profile). Temperature values can be represented by a matrix with one row per `N_LEVELS` and one column per `N_PROF` or by a data frame with variables `N_PROF`, `N_LEVELS`, and `TEMP`. Any other variables that share the dimensions of the temperature variable can be added as additional columns in the data frame. After looping through each file in a complete copy of the Argo data set, we identified 19 grids among four Argo NetCDF file formats (profile, trajectory, metadata, and technical information) [@argo-user-manual]. The most commonly-used grid is the levels grid for Argo profile files:


```r
(levels <- prof_gulf_stream_2020 %>% 
  argo_prof_levels())
```

```
## Downloading 921 files from 'https://data-argo.ifremer.fr'
```

```
## Extracting from 921 files
```

```
## # A tibble: 1,785,666 x 18
##    file   n_levels n_prof  pres pres_qc pres_adjusted pres_adjusted_qc
##    <chr>     <int>  <int> <dbl> <chr>           <dbl> <chr>           
##  1 aoml/~        1      1  1.12 1                1.12 1               
##  2 aoml/~        2      1  2    1                2    1               
##  3 aoml/~        3      1  3    1                3    1               
##  4 aoml/~        4      1  4    1                4    1               
##  5 aoml/~        5      1  4.96 1                4.96 1               
##  6 aoml/~        6      1  6    1                6    1               
##  7 aoml/~        7      1  7    1                7    1               
##  8 aoml/~        8      1  7.92 1                7.92 1               
##  9 aoml/~        9      1  9    1                9    1               
## 10 aoml/~       10      1 10    1               10    1               
## # ... with 1,785,656 more rows, and 11 more variables:
## #   pres_adjusted_error <dbl>, temp <dbl>, temp_qc <chr>,
## #   temp_adjusted <dbl>, temp_adjusted_qc <chr>,
## #   temp_adjusted_error <dbl>, psal <dbl>, psal_qc <chr>,
## #   psal_adjusted <dbl>, psal_adjusted_qc <chr>,
## #   psal_adjusted_error <dbl>
```

Like `argo_prof_levels()`, other extraction functions use the pattern `argo_{file type}_{grid}()` and use a split-apply-combine strategy that row-binds the results obtained by reading each file individually [@wickham11]. To facilitate users who prefer to manage their own collection of Argo files, corresponding `argo_read_{file type}_{grid}()` functions that read a single file are also exported. Extraction functions are designed to return useful inputs to ``dplyr`` and ``ggplot2``. For example, a common way to visualize profile data is to plot a dependent variable (e.g., temperature) against pressure (as a proxy for depth), with pressure oriented vertically to simulate its orientation in space.


```r
ggplot(levels, aes(x = temp, y = pres)) +
  geom_line(aes(group = file), alpha = 0.01, orientation = "y") +
  scale_y_reverse() +
  scale_x_continuous(position = "top") +
  theme_bw() +
  labs(
    x = "Temperature [°C]",
    y = "Pressure [dbar]"
  )
```

![](fig-levels-1.png)<!-- --> 

# Interoperability

The ``argodata`` package was designed to interoperate with the ``argoFloats`` and ``oce`` packages for users who prefer to do part of their analyses using the facilities provided by these packages. In particular, these packages provide specialized functions for mapping and oceanographic analysis that are outside the scope of ``argodata``. For example, one can combine the trajectory plotting capability of ``argoFloats`` with a ``dplyr`` `group_by()` and `summarise()` enabled by ``argodata`` and visualized using colour palettes from ``cmocean`` [@r-cmocean].


```r
library(argoFloats)

# use argoFloats to locate profiles
index <- getIndex() %>% subset(ID = 4903252)

# calculate mean surface temperature using argodata
temp_calc <- index %>% 
  argo_prof_levels() %>% 
  filter(pres < 10) %>% 
  group_by(file) %>%
  summarise(
    near_surface_temp = mean(temp, na.rm = TRUE)
  ) %>%
  mutate(
    near_surface_temp_bin = cut_width(near_surface_temp, width = 2)
  ) %>% 
  left_join(argo_global_prof(), by = "file")

# use plot method for argoFloats index and add temperatures
par(mar = c(3, 3, 1, 2))
plot(index, which = "map", type = "l")

# plot temperatures
palette(cmocean::cmocean("thermal")(5))
points(
  temp_calc$longitude, temp_calc$latitude,
  bg = temp_calc$near_surface_temp_bin, pch = 21, cex = 1
)

legend(
  "topleft",
  levels(temp_calc$near_surface_temp_bin), pt.bg = palette(), pch = 21,
  title = "Near-surface temperature [°C]", ncol = 3
)
```

![](fig-argofloats-1.png)<!-- --> 

# Conclusion

The ``argodata`` package helps scientists analyze data from the International Argo Program using a minimal table-based interface. We hope that ``argodata`` will expand the audience of Argo data to users already familiar with data frame manipulation tools such as those provided by the ``tidyverse`` family of packages.

# Acknowledgements

We acknowledge useful discussions with Chris Gordon, especially regarding the extraction of quality control information from Argo data files. We thank the editors and reviewers for their thoughtful and careful review of this manuscript. Support for this work came from the Natural Sciences and Engineering Research Council of Canada and G7 Charlevoix Blueprint for Healthy Oceans, Seas and Resilient Coastal Communities. The data used in this paper were collected and made freely available by the International Argo Program and the national programs that contribute to it (<https://argo.ucsd.edu>, <https://www.ocean-ops.org>). The Argo Program [@argo] is part of the Global Ocean Observing System.

# References
---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  dpi = 300
)

library(argodata)
library(tidyverse)

argo_set_cache_dir("cache-dev")
options(argodata.max_global_cache_age = Inf)
options(argodata.max_data_cache_age = Inf)

argo_global_prof()
```

# argodata

<!-- badges: start -->
[![R build status](https://github.com/ArgoCanada/argodata/workflows/R-CMD-check/badge.svg)](https://github.com/ArgoCanada/argodata/actions)
[![Codecov test coverage](https://codecov.io/gh/ArgoCanada/argodata/branch/master/graph/badge.svg)](https://codecov.io/gh/ArgoCanada/argodata?branch=master)
<!-- badges: end -->

The goal of argodata is to provide a data frame-based interface to data generated by the [Argo floats program](https://argo.ucsd.edu) ([doi:10.17882/42182](https://doi.org/10.17882/42182)). This package provides low-level access to all variables contained within Argo NetCDF files; for a higher-level interface with built-in visualization and quality control, see the [argoFloats package](https://github.com/ArgoCanada/argoFloats).

## Installation

You can install the development version from [GitHub](https://github.com/) using:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

The argodata package downloads files from the [FTP and HTTPS mirrors](http://www.argodatamgt.org/Access-to-data/Argo-GDAC-ftp-and-https-servers), caches them, and loads them into R. You can set the mirror using `argo_set_mirror()` and the cache directory using `argo_set_cache_dir()`:

```{r, eval = FALSE}
argo_set_mirror("https://data-argo.ifremer.fr/")
argo_set_cache_dir("my/argo/cache")
```

Optionally, you can set `options(argodata.cache_dir = "my/argo/cache")` in your .Rprofile to persist this value between R sessions (see `usethis::edit_r_profile()`). Cached files are used indefinitely by default because of the considerable time it takes to refresh them. If you do use a persistent cache, you should update the index files regularly using `argo_update_global()` (data files are also updated occasionally; update these using `argo_update_data()`).

## Example

The main workflow supported by argodata is:

- Start with `argo_global_prof()`, `argo_global_traj()`, `argo_global_meta()`, `argo_global_bio_prof()`, `argo_global_bio_prof()`, or `argo_global_synthetic_prof()`. These functions return data frames that contain meta information about each data file in Argo.
- Use `argo_filter_radius()`, other `argo_filter_*()` functions, or `dplyr::filter()` to subset the index using search criteria.
- Use extractor functions like `argo_prof_levels()`, `argo_traj_measurement()`, `argo_meta_config_param()`, and `argo_tech_tech_param()` to read information for all files in the index subset.

```{r example}
library(tidyverse)
library(argodata)

# filter profile index using search criteria
prof_lab_may_2020 <- argo_global_prof() %>%
  argo_filter_rect(50, 60, -60, -50) %>% 
  filter(
    lubridate::year(date) == 2020, 
    lubridate::month(date) == 5
  )

# download, cache, and load NetCDF files
levels_lab_may_2020 <- prof_lab_may_2020 %>% 
  argo_prof_levels()

# plot!
levels_lab_may_2020 %>% 
  filter(psal_qc == 1) %>% 
  ggplot(aes(x = psal, y = pres, col = temp)) +
  geom_point() +
  scale_y_reverse() +
  theme_bw()
```

See the reference for `argo_prof_levels()` for more ways to load Argo profiles from `argo_global_prof()`, `argo_global_bio_prof()` and `argo_global_synthetic_prof()`; see `argo_traj_measurement()` for ways to load Argo trajectories from `argo_global_traj()` or `argo_global_bio_traj()`; see `argo_meta_missions()` for ways to load float meta from `argo_global_meta()`; see `argo_tech_tech_param()` for ways to load float technical information from `argo_global_tech()`; and see `argo_info()` and `argo_vars()` for ways to load global metadata from Argo NetCDF files.

## Advanced

The argodata package also exports the low-level readers it uses to produce tables from Argo NetCDF files. You can access these using `argo_read_*()` functions.

```{r}
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_prof_levels(prof_file)
```

## Code of Conduct
  
Please note that argodata is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.
---
title: >
  argodata: An R interface to oceanographic data from
  the International Argo Program
tags:
  - R
  - oceanography
  - measurement
  - Argo float
authors:
  - name: Dewey Dunnington^[Corresponding author.]
    orcid: 0000-0002-9415-4582
    affiliation: 1
  - name: Jaimie Harbin
    orcid: 0000-0003-3774-3732
    affiliation: 1
  - name: Dan E. Kelley
    orcid: 0000-0001-7808-5911
    affiliation: 2
  - name: Clark Richards
    orcid: 0000-0002-7833-206X
    affiliation: 1
affiliations:
  - name: Fisheries and Oceans Canada, Bedford Institute of Oceanography, Dartmouth, NS, Canada
    index: 1
  - name: Department of Oceanography, Dalhousie University, Halifax, NS, Canada
    index: 2
citation_author: Dunnington et. al.
date: 14 May 2021
year: 2021
bibliography: paper.bib
output: rticles::joss_article
csl: apa.csl
journal: JOSS
---

```{r setup, include=FALSE}
# complete package install code
# install.packages("remotes")
# remotes::install_cran(c("rticles", "tidyverse", "oce", "ocedata", "cmocean", "ragg", "raster", "ncdf4"))
# remotes::install_github("ArgoCanada/argoFloats", ref = "develop")
# remotes::install_github("ArgoCanada/argodata")

# load packages to avoid messages later
library(argoFloats)
library(oce)
library(argodata)
library(tidyverse)


# set up temporary cache for argoFloats
temp_cache <- tempdir()
dir.create(temp_cache)
options(argoFloats.destdir = temp_cache)

# make sure a temporary cache is being used for argodata
argo_set_cache_dir(NULL)

options(width = 70)
knitr::opts_chunk$set(fig.path = "", dev = "ragg_png", dpi = 300)
```

# Summary

This paper describes ``argodata``, an R package that makes it easier to work with data acquired in the International Argo Program, which provides over two decades of oceanographic measurements from around the world. Although Argo data are publicly available in NetCDF format and several software packages are available to assist in locating and downloading relevant Argo data, the multidimensional arrays used can be difficult to understand for non-oceanographers, particulary for the expanding arrays of biogeochemical variables measured by Argo floats. Given the increasing use of Argo data in other disciplines, we built a minimal interface to the data set that uses the data frame as the primary data structure. This approach allows users to leverage the rich ecosystem of R packages that manipulate data frames (e.g., the ``tidyverse``) and associated instructional resources.

# Introduction

The ocean is highly variable in both space and time and mapping this variability at appropriate scales is a key factor in many scientific studies. Oceanographic data have direct applications that range from the analysis of near-bottom ecosystems to air-sea interactions.  More broadly, ocean measurements are needed to constrain the models that scientists use to understand the evolving state of the ocean and to make predictions about its future, particularly as a component of the global climate system.

The International Argo Program [@argo] deploys and collects data from several thousand devices that are programmed to drift with and move vertically through the ocean. Sensors measure electrical conductivity, temperature, pressure, and other quantities along this vertical path yielding "profiles" that are uploaded via satellite to globally distributed data assembly centres [@roemmich_argo_2001; @roemmich_argo_2009-2]. Since 1997, the International Argo Program has collected over 2.4 million profiles from around the globe and expanded its original array of sensors to measure biogeochemicical variables such as pH, chloryphyll-a, dissolved oxygen, nitrate, and many others.

Although the NetCDF data files provided by Argo data servers contain metadata that describe their contents, we identified a number of barriers to data access. These included (1) reading and decoding the index files to locate files of interest, (2) downloading and potentially caching large numbers of small NetCDF files, (3) reading the NetCDF files into a form where the data contained within can be visualized and analyzed, and (4) dealing efficiently with potentially large Argo data sets. In particular, the incorporation of biogeochemical variables in Argo NetCDF files introduced additional complexity such that a novice- to average-level programmer may have difficulty extracting and manipulating data from many profiles. Whereas a variety of applications have been created to address some of these barriers, the ``argodata`` package is our attempt to overcome these barriers for the novice- to average-level programmer who may not be familiar with oceanographic conventions for storing data.

# Statement of need

In the R language, several tools are available to access data from the International Argo Program. The ``oce`` package provides facilities to read and analyze "profile" and "trajectory" Argo NetCDF files [@kelley_oceanographic_2018; @r-oce]; the ``argoFloats`` package provides additional tools to locate, download, cache, and visualize Argo NetCDF files [@kelley_argofloats_2021]; and ``rnoaa`` provides limited access to a subset of Argo data from the North Atlantic [@r-rnoaa]. Outside of R, the ``argopy`` package for Python provides access to the Argo data set with some facilities for analysis and visualization [@maze_argopy_2020], and several web applications provide visual tools to locate relevant Argo profiles based on user-defined search criteria [@oceanops_oceanops_2021; @tucker_argovis_2020].

Several barriers we identified are not specific to the Argo data set and can be overcome with well-established R tools. To download and potentially cache Argo NetCDF files, at least one Argo mirror provides an `rsync` target for profile and index files. The ``bowerbird`` package provides similar facilities for downloading and caching large numbers of files from a remote source [@r-bowerbird]. To analyze and visualize potentially large data sets, ``dplyr`` and ``ggplot2`` within the wider ``tidyverse`` family of packages are well-established [@r-ggplot2; @r-dplyr; @r-tidyverse]. To read NetCDF files in a form that can be analyzed and plotted using ``dplyr`` and ``ggplot2``, respectively, the ``tidync`` and ``ncmeta`` packages introduce the concept of "grids" to identify groups of variables that can be loaded into a single data frame [@r-ncmeta; @r-tidync]. 

The ``argodata`` package was designed to work with a range of tools that manipulate R data frames. In particular, the ``tidyverse`` family of packages has a large user base and has widely and freely available educational material in several languages [@wickham_grolemund17]. Whereas previous packages for R and Python propagate the multidimensional array format of Argo NetCDF files when read, the ability to leverage the ``tidyverse`` depends on the representation of Argo data as data frames in  "tidy" (one observation per row, one variable per column) format [@wickham14], around which packages in the ``tidyverse`` are designed.

# Using ``argodata``

The ``argodata`` package is available as an R source package from GitHub (<https://github.com/ArgoCanada/argodata>), installable using the ``remotes`` package:

``` r
# install.packages("remotes")
remotes::install_github("ArgoCanada/argodata")
```

For our example usage, we also load the ``tidyverse``:

```{r, message=FALSE}
library(tidyverse)
library(argodata)
```

To locate files of interest on the Argo mirror, index files for profile, trajectory, meta, and technical parameter files are provided in compressed CSV format. ``argodata`` uses the ``vroom`` package  to efficiently load these files as they can be time-consuming to repeatedly read otherwise. The most commonly-used index is for profile files:

```{r}
(prof <- argo_global_prof())
```

A typical analysis will focus on a subset of profiles. Users can subset this index using existing knowledge of data frames in R; however, some common subsets are verbose using existing tools or difficult to compute without knowing Argo-specific filename conventions. To match the syntax of `dplyr::filter()`, ``argodata`` provides several `argo_filter_*()` functions to subset index data frames:

```{r}
prof_gulf_stream_2020 <- prof %>% 
  argo_filter_radius(latitude = 26, longitude = -84, radius = 500) %>%
  argo_filter_date("2020-01-01", "2020-12-31") %>%
  argo_filter_data_mode("delayed")
```

The next step is to download the selected files. The explicit call to `argo_download()` is typically omitted as it is done automatically for missing files by the load functions described below; however, one can manually call `argo_download()` to download (if necessary) and cache files in an index. To facilitate use of alternative cache solutions like `rsync` or ``bowerbird`` [@r-bowerbird], we use the same file structure as the mirror itself and provide `argo_set_cache_dir()` to allow this directory to be used for all calls to `argo_download()`.

To load data from NetCDF files into meaningful data frames we draw from the concept of "grids" introduced by the ``tidync`` and ``ncmeta`` packages [@r-ncmeta; @r-tidync]. For example, temperature values stored in an Argo profile NetCDF file are identified by values of `N_PROF` (an integer identifying a profile within an Argo NetCDF file) and `N_LEVEL` (an integer identifying a sampling level within a profile). Temperature values can be represented by a matrix with one row per `N_LEVELS` and one column per `N_PROF` or by a data frame with variables `N_PROF`, `N_LEVELS`, and `TEMP`. Any other variables that share the dimensions of the temperature variable can be added as additional columns in the data frame. After looping through each file in a complete copy of the Argo data set, we identified 19 grids among four Argo NetCDF file formats (profile, trajectory, metadata, and technical information) [@argo-user-manual]. The most commonly-used grid is the levels grid for Argo profile files:

```{r}
(levels <- prof_gulf_stream_2020 %>% 
  argo_prof_levels())
```

Like `argo_prof_levels()`, other extraction functions use the pattern `argo_{file type}_{grid}()` and use a split-apply-combine strategy that row-binds the results obtained by reading each file individually [@wickham11]. To facilitate users who prefer to manage their own collection of Argo files, corresponding `argo_read_{file type}_{grid}()` functions that read a single file are also exported. Extraction functions are designed to return useful inputs to ``dplyr`` and ``ggplot2``. For example, a common way to visualize profile data is to plot a dependent variable (e.g., temperature) against pressure (as a proxy for depth), with pressure oriented vertically to simulate its orientation in space.

```{r fig-levels, warning=FALSE}
ggplot(levels, aes(x = temp, y = pres)) +
  geom_line(aes(group = file), alpha = 0.01, orientation = "y") +
  scale_y_reverse() +
  scale_x_continuous(position = "top") +
  theme_bw() +
  labs(
    x = "Temperature [°C]",
    y = "Pressure [dbar]"
  )
```

# Interoperability

The ``argodata`` package was designed to interoperate with the ``argoFloats`` and ``oce`` packages for users who prefer to do part of their analyses using the facilities provided by these packages. In particular, these packages provide specialized functions for mapping and oceanographic analysis that are outside the scope of ``argodata``. For example, one can combine the trajectory plotting capability of ``argoFloats`` with a ``dplyr`` `group_by()` and `summarise()` enabled by ``argodata`` and visualized using colour palettes from ``cmocean`` [@r-cmocean].

```{r fig-argofloats, message=FALSE}
library(argoFloats)

# use argoFloats to locate profiles
index <- getIndex() %>% subset(ID = 4903252)

# calculate mean surface temperature using argodata
temp_calc <- index %>% 
  argo_prof_levels() %>% 
  filter(pres < 10) %>% 
  group_by(file) %>%
  summarise(
    near_surface_temp = mean(temp, na.rm = TRUE)
  ) %>%
  mutate(
    near_surface_temp_bin = cut_width(near_surface_temp, width = 2)
  ) %>% 
  left_join(argo_global_prof(), by = "file")

# use plot method for argoFloats index and add temperatures
par(mar = c(3, 3, 1, 2))
plot(index, which = "map", type = "l")

# plot temperatures
palette(cmocean::cmocean("thermal")(5))
points(
  temp_calc$longitude, temp_calc$latitude,
  bg = temp_calc$near_surface_temp_bin, pch = 21, cex = 1
)

legend(
  "topleft",
  levels(temp_calc$near_surface_temp_bin), pt.bg = palette(), pch = 21,
  title = "Near-surface temperature [°C]", ncol = 3
)
```

# Conclusion

The ``argodata`` package helps scientists analyze data from the International Argo Program using a minimal table-based interface. We hope that ``argodata`` will expand the audience of Argo data to users already familiar with data frame manipulation tools such as those provided by the ``tidyverse`` family of packages.

# Acknowledgements

We acknowledge useful discussions with Chris Gordon, especially regarding the extraction of quality control information from Argo data files. We thank the editors and reviewers for their thoughtful and careful review of this manuscript. Support for this work came from the Natural Sciences and Engineering Research Council of Canada and G7 Charlevoix Blueprint for Healthy Oceans, Seas and Resilient Coastal Communities. The data used in this paper were collected and made freely available by the International Argo Program and the national programs that contribute to it (<https://argo.ucsd.edu>, <https://www.ocean-ops.org>). The Argo Program [@argo] is part of the Global Ocean Observing System.

# References
---
title: "Example: Virtual moorings and sections"
---

```{r setup, include = FALSE}
library(argodata)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(ggspatial)
options(crayon.enabled = FALSE)
theme_set(theme_bw())
argo_global_prof()
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 300
)
```

The [Argo international research program](https://argo.ucsd.edu/) has collected millions of profiles from the world's oceans over the last several decades. The data set has excellent spatial and temporal coverage and is well-suited to problems that require looking back in time to obtain oceanographic measurements where ship- or mooring-based data are not available. This article covers methods that can be used to combine multiple Argo profiles into a virtual mooring or section.

We'll start by loading a few packages. In addition to argodata, we'll use the [tidyverse](https://tidyverse.org), [lubridate](https://lubridate.tidyverse.org), and later on we'll use [ggspatial](https://paleolimbot.github.io/ggspatial/) (for map mapking) and [s2](https://r-spatial.github.io/s2/) (for geometry calculations on the sphere). You can learn about the table manipulation functions and visualization functions we use here in the free online book [R for Data Science](https://r4ds.had.co.nz) (also available in [French](https://www.amazon.fr/pour-data-sciences-transformer-visualiser/dp/2212675712), [Spanish](https://es.r4ds.hadley.nz/), [German](https://oreilly.de/produkt/r-fuer-data-science/), and [Portuguese](https://www.amazon.com/R-Para-Data-Science-Portuguese-ebook/dp/B07ZPH2LVK)).

```{r, eval=FALSE}
library(argodata)
library(tidyverse)
library(lubridate)
theme_set(theme_bw())
```

## Fetch

The first step is to choose which area is representative of the region for which you would like to create an aggregated/summarized set of profiles. One way to do this is using a point and radius, for which `argo_filter_radius()` is provided to subset the global profile index `argo_global_prof()`. For example, the following point/radius results in approximately 475 profiles between Labrador and Greenland in the Labrador Sea. For the purposes of the virtual mooring, the radius needs to be small enough that the profiles are related to the conditions in the region you are attempting to model and large enough that there are enough profiles for window point of time you would like to model.

```{r}
mooring_lat <- 55.7
mooring_lon <- -49.5
mooring_radius_km <- 150
```

The next step is to (optionally) choose a window of time to consider. For the purposes of this article, we'll consider the years between 2011 and 2019, inclusive.

```{r}
mooring_start <- as.Date("2011-01-01")
mooring_end <- as.Date("2019-12-31")
```

An index of all profiles available from the Argo program is available by calling `argo_global_prof()`. This will take 20-60 seconds to load depending on your internet connection; if you would like to avoid downloading the index file more than once you can [configure a persistent cache directory](https://github.com/ArgoCanada/argodata#installation), but be aware that this index is updated frequently.

```{r}
profiles <- argo_global_prof() %>%
  argo_filter_radius(
    latitude = mooring_lat,
    longitude = mooring_lon,
    radius_km = mooring_radius_km
  ) %>% 
  argo_filter_date(
    date_min = mooring_start,
    date_max = mooring_end
  ) %>% 
  argo_extract_path_info() %>% 
  select(file, file_float, date, latitude, longitude, everything()) %>% 
  arrange(date)

profiles
```

A good sanity check is to examine the distribution of profiles in space and time, as we'll be spending a lot of time examining the interactions between these dimensions. Below I've created a bar plot to examine the distribution of profiles both within and between years: there is a lot of variability! We will come back to this later on.

```{r}
ggplot(
  profiles,
  aes(
    x = factor(year(date)),
    fill = factor(month(date))
  )
) +
  geom_bar()
```


For a quick view of the locations I used the [ggspatial](https://github.com/paleolimbot/ggspatial) package:


```{r, warning=FALSE}
library(ggspatial)

ggplot(profiles, aes(x = longitude, y = latitude)) +
  annotation_map_tile(zoomin = -1, progress = "none") +
  geom_spatial_rect(
    aes(ymin = 50, ymax = 65, xmin = -70, xmax = -30),
    fill = NA,
    # data = tibble(x = NA),
    inherit.aes = FALSE,
    crs = 4326
  ) +
  geom_spatial_point(crs = 4326) +
  facet_wrap(vars(year(date)))
```

You can use plots like this to ensure that you have a reasonable density of samples in both space and time for the question you are trying to answer. Once you've done this, you can load profile levels using `argo_prof_levels()`. This will download the files from the Argo server and load them into a table with one row per profile per sampling level. This will take about 90 seconds depending on your internet connection and system configuration.

```{r}
levels <- argo_prof_levels(profiles) %>% 
  select(file, pres, temp, temp_qc, everything())

levels
```

## Clean

Again, the first step once we have the data is to plot! Here I've coloured the points by the `_qc` column for temperature, since temperature is what we'll be examining later on. There is a `_qc` column for most variables in the levels table; you can learn more about what each flag means in the `argo_reference_qc_flag` table or the [Argo User's Manual](https://doi.org/10.13155/29825).

```{r}
ggplot(levels, aes(y = pres, x = temp, col = temp_qc)) +
  geom_point() +
  scale_y_reverse()
```

From this plot it's clear that there are some points with clearly bad values that we need to take care of. A similar plot of `pres_qc` indicates that there are some bad pressure values as well. Depending what stage you are at in your analysis, you may want to remove rows that you can't use in future analysis or set these values to `NA`. I'll demonstrate the latter here using `argo_qc_censor_if_not()`, which sets values to `NA` where the paired `_qc` column is not in a specified vector of values. The only value that makes sense to keep based on a plot of our results is `1`, which corresponds to "good data" in the reference table (beware that not all data marked "good" has been checked with the same degree of scrutiny!).

```{r}
levels_clean <- levels %>%
  argo_qc_censor_if_not(c(temp, pres), qc_flag = 1)

ggplot(levels_clean, aes(y = pres, x = temp)) +
  geom_line(
    aes(group = interaction(file, n_prof)),
    orientation = "y",
    alpha = 0.05
  ) +
  scale_y_reverse()
```

We've set a lot of values to `NA` - in some cases many profiles worth. While we do want to keep a record of levels even if the temperature was set to `NA` because of its `_qc` column value, we also don't need profiles where there is little temperature information, and including them in our data moving forward is not useful.

```{r}
levels_clean <- levels_clean %>% 
  group_by(file, n_prof) %>% 
  filter(mean(is.finite(temp)) > 0.5) %>% 
  ungroup()
```


## Model

### Bin/Aggregate

Combining information collected at specific locations over time is an entire subfield of spatial statistics. For our purposes, binning and aggregating along a few dimensions of interest is probably sufficient and should always be attempted before invoking a more complex method.

There are a few complexities associated with combining information from multiple profiles. Notably, there is a severe sampling bias: the nature of the Argo float is such that it collects detailed data along its trajectory which is clumped in both space and time. This means that certain areas and timeframes are intensely sampled whereas other areas and/or timeframes may have poor coverage. Another complexity is that certain locations may represent the "virtual mooring" location or region poorly.

We will mitigate the effects of both these complicating behaviours using weighting. To ensure that a single float does not contribute unduly to any particular prediction, we can weight each profile as `1 / n`, where `n` is the number of profiles in a given time frame contributed by a single float. We can also weight profiles taken farther from the mooring location less than those taken close to the mooring location; however, the choice of how severely to punish profiles taken far from the mooring location makes a considerable difference to the result. One method is to use the inverse of the distance, which produces a rather severe punishment as distance from the centre increases. Here we will use `1 / sqrt(distance)` to mitigate the penalty. The distance weights we can calculate before binning; the float weights depend on our choice of bins.

```{r}
profile_weighted <- profiles %>%
  mutate(
    distance_km = s2::s2_distance(
      s2::s2_lnglat(longitude, latitude), 
      s2::s2_lnglat(mooring_lon, mooring_lat)
    ) / 1000,
    distance_weight = 1 / sqrt(distance_km)
  ) %>% 
  select(file, longitude, latitude, distance_km, distance_weight)

profile_weighted
```

The next step is to choose bins that are appropriate to the scale of the problem. In this case, we're interested in monthly bins (regardless of year) and depth bins 10 decibars in height. These bin sizes reflect the amount of data we have: if we were only summarizing 2019, for which there are many profiles with high-resolution sampling intervals, we may be able to use smaller bins (e.g., one bin per month per year or 1 decibar height).

```{r}
levels_binned <- levels_clean %>%
  left_join(profiles %>% select(file, date), by = "file") %>% 
  mutate(
    pres_bin = floor(pres / 100) * 100 + 50,
    date_bin = month(date)
  ) %>% 
  select(pres_bin, date_bin, everything())

levels_binned
```

Next we can calculate the weights to ensure each profile does not contribute more than its share to the values calculated for a given bin and apply the weights based on distance we calculated above.

```{r}
levels_binned_weighted <- levels_binned %>% 
  group_by(pres_bin, date_bin, file, n_prof) %>% 
  mutate(profile_weight = 1 / n()) %>% 
  ungroup() %>% 
  left_join(profile_weighted, by = "file") %>% 
  mutate(weight = profile_weight * distance_weight) %>% 
  group_by(pres_bin, date_bin) %>% 
  mutate(weight = weight / sum(weight)) %>% 
  ungroup() %>% 
  select(pres_bin, date_bin, weight, file, date, pres, temp, everything())

levels_binned_weighted
```

The most straightforward way to aggregate is using `weighted.mean()`, which is a good way to get a preliminary view of our results:

```{r}
levels_aggregated <- levels_binned_weighted %>% 
  group_by(date_bin, pres_bin) %>% 
  filter(sum(is.finite(temp)) > 1) %>% 
  summarise(
    temp_mean = weighted.mean(temp, w = weight, na.rm = TRUE)
  ) %>% 
  ungroup()
```

```{r}
ggplot(levels_aggregated, aes(x = temp_mean, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)), 
    col = "grey80",
    orientation = "y", 
    data = levels_binned
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(date_bin))
```

This is a good first step in your analysis: if the above diagram was not in line with our knowledge of the ocean at this location, it is a clue that some part of our analysis went wrong. However, it is not able to communicate the spread of values that we observed in each bin. Are the profiles for a given month of the year similar over the last 10 years or not? 

One technique that allows calculation of error in this way is sampling: we can draw a random sample of size `n` from the values available for each bin (applying the weights such that values with a higher weight are more likely to be drawn than others). We can repeat this analysis `k` times and use the distribution of the values we observe for each bin to communicate the uncertainty of our results.

```{r}
set.seed(3948)

levels_aggregated_randomized <- levels_binned_weighted %>%
  crossing(tibble(sample_number = 1:100)) %>%
  group_by(sample_number, date_bin, pres_bin) %>%
  sample_n(size = n(), replace = TRUE) %>% 
  summarise(
    temp_weighted_mean = weighted.mean(temp, w = weight, na.rm = TRUE),
  ) %>%
  group_by(pres_bin, date_bin) %>% 
  summarise(
    temp_weighted_mean_q05 = quantile(temp_weighted_mean, 0.05, na.rm = TRUE),
    temp_weighted_mean_q50 = median(temp_weighted_mean, na.rm = TRUE),
    temp_weighted_mean_q95 = quantile(temp_weighted_mean, 0.95, na.rm = TRUE)
  ) %>% 
  ungroup()
```

```{r}
ggplot(levels_aggregated_randomized, aes(x = temp_weighted_mean_q50, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)), 
    col = "grey80",
    orientation = "y", 
    data = levels_binned
  ) +
  geom_ribbon(
    aes(xmin = temp_weighted_mean_q05, xmax = temp_weighted_mean_q95),
    alpha = 0.5
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(date_bin))
```

In this case the weighted mean and randomized approach give the same results. This is a good thing! Another thing to note is that in the above example the weighted mean varies little even with the bootstrapping approach (the `geom_ribbon()` behind each profile is barely if at all visible).

## Sections

Computing a section based on a collection of Argo profiles is similar to computing an average profile from a single point except we have an added dimension along which we want to bin the profiles. 

As an example, we'll look at a cross-section of the Gulf stream off Nova Scotia. The first step is to pick a line that defines your section. The calculations are easier when your section can be defined by exactly two points, but you can use a section with an arbitrary number of points. Because we'll be using the [s2 package](https://r-spatial.github.io/s2/) for our calculations, I'll also construct an s2 geography for the start point, end point, and the line between them.

```{r}
library(s2)

section_points_lon <- c(-62.0, -55.9)
section_points_lat <- c(42.0, 35.9)

section_start <- s2_geog_point(section_points_lon[1], section_points_lat[1])
section_end <- s2_geog_point(section_points_lon[2], section_points_lat[2])
section_line <- s2_make_line(section_points_lon, section_points_lat)
```

We can use the line to subset profiles based on the criteria we discussed above, this time adding in `s2::s2_dwithin()` to find profiles within a specified distance of our profile line. I've chosen a threshold of 100 km in this case. Because s2 package functions work on s2_geography objects, I've kept the geography vector representing the point where the profile was collected so that we can use it later without recalculating.

```{r}
section_profiles <- argo_global_prof() %>%
  argo_filter_data_mode("delayed") %>% 
  argo_filter_date(
    date_min = mooring_start,
    date_max = mooring_end
  ) %>% 
  mutate(geog = s2_geog_point(longitude, latitude)) %>% 
  filter(s2_dwithin(geog, section_line, 100 * 1000))

section_profiles
```

Similar to the profile calculation, we need to calculate some information about the suitability of each profile to our section. In addition to the distance from the section line, we will also need a measure of how far along the section line each profile is so that we can bin it accordingly. I also add a filter here to remove profiles that are "off the end" of the section (whose closest point *is* the start or end point). This is where the two-point section definition makes our life easier: distance along the profile is just the distance from the start point. If you really need a section defined by multiple points you will need to use a projection and the linear referencing function `geos::geos_project()`.

```{r}
section_profile_weighted <- section_profiles %>% 
  mutate(
    point_on_section = s2_closest_point(section_line, geog),
    distance_km = s2_distance(section_line, geog) / 1000,
    distance_along_section_km = s2_distance(
      point_on_section, 
      section_start
    ) / 1000,
    distance_weight = 1 / sqrt(distance_km)
  ) %>% 
  filter(
    !s2_equals(point_on_section, section_start),
    !s2_equals(point_on_section, section_end)
  ) %>%
  select(
    file, geog, point_on_section,
    distance_km, distance_along_section_km, distance_weight
  )
```

Now is a good time to plot our section to make sure that our math was correct!

```{r}
ggplot() +
  layer_spatial(
    sf::st_segmentize(sf::st_as_sfc(section_line), 10000),
    size = 1
  ) +
  layer_spatial(
    s2_minimum_clearance_line_between(
      section_line, 
      section_profile_weighted$geog
    ),
    col = "grey40"
  ) +
  layer_spatial(section_profile_weighted$geog)
```

From here we have enough information to load our levels.

```{r}
section_levels <- argo_prof_levels(section_profile_weighted)
```

Again, the first step once the levels are loaded is to check which QC flag values we can use to ensure reasonable values in our result. Because we're working with delayed mode data, there is a chance that the `temp_adjusted` column contains higher quality results. I've plotted both below.

```{r}
ggplot(section_levels, aes(y = pres, x = temp, col = temp_qc)) +
  geom_point() +
  scale_y_reverse()

ggplot(section_levels, aes(y = pres, x = temp_adjusted, col = temp_adjusted_qc)) +
  geom_point() +
  scale_y_reverse()
```

In many cases the adjusted values are identical to the non-adjusted values, but it is reasonable to use the adjusted values where they have been made available. Some adjusted data has been marked as not "good data", however, which we need to censor before "preferring" the adjusted value over the non-adjusted one.

```{r}
section_levels_clean <- section_levels %>% 
  argo_qc_censor_if_not(
    c(temp, pres, temp_adjusted, pres_adjusted),
    qc_flag = 1
  ) %>% 
  argo_use_adjusted(c(temp, pres))
```

As above, we can remove profiles that contain few temperature values to simplify the upcoming analysis.

```{r}
section_levels_clean <- section_levels_clean %>% 
  group_by(file, n_prof) %>% 
  filter(mean(is.finite(temp)) > 0.5) %>% 
  ungroup()
```

Finally, we can visualize all the profiles to make sure our profiles are composed of reasonable values.

```{r}
ggplot(section_levels_clean, aes(y = pres, x = temp)) +
  geom_line(
    aes(group = interaction(file, n_prof)),
    orientation = "y",
    alpha = 0.02
  ) +
  scale_y_reverse()
```

Now we're ready to bin! While it is also valid to bin using a `date_bin` if enough profiles are available, in this case I'm going to demonstrate binning along the transect such that there are still two bin dimensions. When calculating uncertainty, this should show up in our results for the top pressure bins whose temperature responds most readily to seasonal fluctuations.

```{r}
section_levels_binned <- section_levels_clean %>%
  left_join(section_profile_weighted, by = "file") %>% 
  mutate(
    pres_bin = floor(pres / 100) * 100 + 50,
    distance_bin = floor(distance_along_section_km / 40) * 40 + 20
  )

section_levels_binned
```

As above, we can aggregate using `weighted.mean()` to get a preliminary view of our results:

```{r}
section_levels_aggregated <- section_levels_binned %>%
  group_by(distance_bin, pres_bin, file, n_prof) %>% 
  mutate(
    profile_weight = 1 / n(),
    weight = profile_weight * distance_weight
  ) %>%
  group_by(distance_bin, pres_bin) %>% 
  filter(sum(is.finite(temp)) > 1) %>% 
  summarise(
    temp_mean = weighted.mean(temp, w = weight, na.rm = TRUE)
  ) %>% 
  ungroup()

section_levels_aggregated
```

There are too many bins to examine all of them for quality. Instead, I'll examine a subset of them to make sure our `weighted.mean()` calculation didn't create unrealistic estimates.

```{r}
test_distance_bins <- c(20, 220, 620, 820)

section_levels_aggregated %>% 
  filter(distance_bin %in% test_distance_bins) %>% 
  ggplot(aes(x = temp_mean, y = pres_bin)) +
  geom_line(
    aes(x = temp, y = pres, group = interaction(file, n_prof)),
    col = "grey80",
    orientation = "y",
    data = section_levels_binned %>% 
      filter(distance_bin %in% test_distance_bins)
  ) +
  geom_point() +
  scale_y_reverse() +
  facet_wrap(vars(distance_bin))
```

Finally, we can visualize all of the bins together!

```{r}
ggplot(
  section_levels_aggregated,
  aes(x = distance_bin, y = pres_bin, fill = temp_mean)
) +
  geom_raster() +
  scale_fill_viridis_c() +
  scale_y_reverse()
```
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-path.R
\name{argo_path_info}
\alias{argo_path_info}
\alias{argo_extract_path_info}
\alias{as_argo_path}
\alias{as_argo_path_aux}
\title{Extract information from an Argo path}
\usage{
argo_path_info(path)

argo_extract_path_info(tbl)

as_argo_path(path)

as_argo_path_aux(path)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{tbl}{A data.frame, ideally derived from \code{\link[=argo_global_meta]{argo_global_meta()}}
and family. The column conventions used by the global indexes is assumed
(e.g., columns \code{latitude} and \code{longitude} exist).}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with columns \code{file}, \code{file_float},
\code{file_type}, \code{file_cycle}, \code{file_data_mode}, and \code{file_modifier}.
}
\description{
Extract information from an Argo path
}
\examples{
argo_path_info("dac/nmdis/2901633/profiles/R2901633_052.nc")

with_argo_example_cache({
  argo_extract_path_info(argo_global_meta())
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_history_step}
\alias{argo_reference_history_step}
\title{Argo history step reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 9 rows and 2 columns.
}
\usage{
argo_reference_history_step
}
\description{
Contains a reference for values in the \code{history_test} column of
\code{\link[=argo_prof_history]{argo_prof_history()}} and \code{\link[=argo_traj_history]{argo_traj_history()}}.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_history(
    "dac/csio/2902746/profiles/BR2902746_001.nc",
    vars = "history_step"
  ) \%>\%
    left_join(argo_reference_history_step, by = "history_step")
})

argo_reference_history_step

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-vars.R
\name{argo_vars}
\alias{argo_vars}
\title{Load Argo NetCDF variable metadata}
\usage{
argo_vars(path, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with one row per variable and columns \code{file},
\code{name},\code{size}, \code{dim}, and \verb{att_*} for variable attributes.
}
\description{
Use \code{argo_vars()} to extract variable information from a vector of
Argo NetCDF files in the form of one row per variable. Use
\code{\link[=argo_read_vars]{argo_read_vars()}} for lower-level output from a single NetCDF file.
}
\examples{
with_argo_example_cache({
  argo_vars("dac/csio/2900313/profiles/D2900313_000.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-tech.R
\name{argo_tech_tech_param}
\alias{argo_tech_tech_param}
\title{Load Argo float meta}
\usage{
argo_tech_tech_param(path, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with one row per technical parameter
per cycle and columns \code{file}, \code{n_tech_param}, \code{technical_parameter_name},
\code{technical_parameter_value}, and \code{cycle_number}.
}
\description{
Use \code{argo_tech_tech_param()} to extract information from Argo technical
spec NetCDF files. Use \code{\link[=argo_read_tech_tech_param]{argo_read_tech_tech_param()}}
to extract information from a single previously-downloaded NetCDF file.
}
\examples{
with_argo_example_cache({
  argo_tech_tech_param("dac/csio/2900313/2900313_tech.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_profiler}
\alias{argo_reference_profiler}
\title{Argo profiler reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 30 rows and 2 columns.
}
\usage{
argo_reference_profiler
}
\description{
Contains a reference for values in the \code{profiler_type} column in the
\code{\link[=argo_global_meta]{argo_global_meta()}}.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_global_meta() \%>\%
    select(file, profiler_type) \%>\%
    left_join(argo_reference_profiler, by = "profiler_type")
})

argo_reference_profiler

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argodata-package.R
\docType{package}
\name{argodata-package}
\alias{argodata}
\alias{argodata-package}
\title{argodata: Locate, Download, and Read Argo Ocean Float Data}
\description{
Data from the international Argo
  research program <doi:10.17882/42182> is provided in the form of
  millions of NetCDF files stored on a remote server. These data contain
  in-situ oceanographic measurements from decades of observation from
  around the world. Functions are provided here to locate, download,
  and read Argo NetCDF files to accelerate oceanographic and climate
  research.
}
\seealso{
Useful links:
\itemize{
  \item \url{https://github.com/ArgoCanada/argodata}
  \item Report bugs at \url{https://github.com/ArgoCanada/argodata/issues}
}

}
\author{
\strong{Maintainer}: Dewey Dunnington \email{dewey.dunnington@dfo-mpo.gc.ca} (\href{https://orcid.org/0000-0002-9415-4582}{ORCID}) [copyright holder]

Other contributors:
\itemize{
  \item Jaimie Harbin (\href{https://orcid.org/0000-0003-3774-3732}{ORCID}) [contributor]
  \item Christopher Gordon (\href{https://orcid.org/0000-0002-1756-422X}{ORCID}) [contributor]
  \item Dan Kelley (\href{https://orcid.org/0000-0001-7808-5911}{ORCID}) [contributor]
  \item Clark Richards (\href{https://orcid.org/0000-0002-7833-206X}{ORCID}) [contributor]
}

}
\keyword{internal}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-meta.R, R/argo-tech.R
\name{argo_read_meta_config_param}
\alias{argo_read_meta_config_param}
\alias{argo_read_meta_missions}
\alias{argo_read_meta_trans_system}
\alias{argo_read_meta_positioning_system}
\alias{argo_read_meta_launch_config_param}
\alias{argo_read_meta_sensor}
\alias{argo_read_meta_param}
\alias{argo_read_tech_tech_param}
\title{Read Argo float meta}
\usage{
argo_read_meta_config_param(file, vars = NULL, quiet = FALSE)

argo_read_meta_missions(file, vars = NULL, quiet = FALSE)

argo_read_meta_trans_system(file, vars = NULL, quiet = FALSE)

argo_read_meta_positioning_system(file, vars = NULL, quiet = FALSE)

argo_read_meta_launch_config_param(file, vars = NULL, quiet = TRUE)

argo_read_meta_sensor(file, vars = NULL, quiet = quiet)

argo_read_meta_param(file, vars = NULL, quiet = quiet)

argo_read_tech_tech_param(file, vars = NULL, quiet = FALSE)
}
\arguments{
\item{file}{A previously downloaded Argo NetCDF file
(e.g., using \code{\link[=argo_download]{argo_download()}}).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{quiet}{Use \code{FALSE} to stop for malformed files, \code{NA} to
silently warn for malformed files, or \code{TRUE} to silently ignore
read errors when possible.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_meta_config_param()}: one row per configuration parameter and
columns \code{n_config_param}, \code{N_MISSIONS}, \code{CONFIG_PARAMETER_VALUE},
and \code{CONFIG_PARAMETER_NAME}.
\item \code{argo_meta_missions()}: one row per mission and columns
\code{N_MISSIONS}, \code{CONFIG_MISSION_NUMBER}, and \code{CONFIG_MISSION_COMMENT}.
\item \code{argo_meta_trans_system()}: one row per transmission system
and columns \code{N_TRANS_SYSTEM}, \code{TRANS_SYSTEM}, \code{TRANS_SYSTEM_ID},
and \code{TRANS_FREQUENCY}.
\item \code{argo_meta_positioning_system()}: one row per positioning
system and columns \code{N_POSITIONING_SYSTEM}, and
\code{POSITIONING_SYSTEM}.
\item \code{argo_meta_launch_config_param()}: one row per launch
configuration parameter and columns \code{N_LAUNCH_CONFIG_PARAM},
\code{LAUNCH_CONFIG_PARAMETER_NAME}, and \code{LAUNCH_CONFIG_PARAMETER_VALUE}.
\item \code{argo_meta_sensor()}: one row per sensor and columns
\code{N_SENSOR}, \code{SENSOR}, \code{SENSOR_MAKER}, \code{SENSOR_MODEL},
and \code{sensor_serial_no}.
\item \code{argo_meta_param()}: one row per parameter and columns
\code{N_PARAM}, \code{PARAMETER} \code{PARAMETER_SENSOR}, \code{PARAMETER_UNITS},
\code{PARAMETER_RESOLUTION}, \code{PREDEPLOYMENT_CALIB_EQUATION},
\code{PREDEPLOYMENT_CALIB_COEFFICIENT}, and \code{PREDEPLOYMENT_CALIB_COMMENT}.
}

A \code{\link[tibble:tibble]{tibble::tibble()}} with one row per technical parameter
and columns \code{N_TECH_PARAM}, \code{TECHNICAL_PARAMETER_NAME},
\code{TECHNICAL_PARAMETER_VALUE}, and \code{CYCLE_NUMBER}.
}
\description{
Use \verb{argo_read_meta_*()} functions to extract meta information from a
previously-downloaded Argo NetCDF file.

Use \code{argo_read_tech_tech_param()} to extract technical specifications from a
previously-downloaded Argo NetCDF file.
}
\examples{
meta_file <- system.file(
  "cache-test/dac/csio/2900313/2900313_meta.nc",
  package = "argodata"
)

argo_read_meta_config_param(meta_file)
argo_read_meta_missions(meta_file)
argo_read_meta_trans_system(meta_file)
argo_read_meta_positioning_system(meta_file)
argo_read_meta_launch_config_param(meta_file)
argo_read_meta_sensor(meta_file)
argo_read_meta_param(meta_file)

tech_file <- system.file(
  "cache-test/dac/csio/2900313/2900313_tech.nc",
  package = "argodata"
)

argo_read_tech_tech_param(tech_file)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_institution}
\alias{argo_reference_institution}
\title{Argo institution code reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 25 rows and 2 columns.
}
\usage{
argo_reference_institution
}
\description{
Contains a longer names of the institution abbreviations
in \code{\link[=argo_global_meta]{argo_global_meta()}} and other index files.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_global_meta() \%>\%
    select(file, institution) \%>\%
    left_join(argo_reference_institution, by = "institution")
})

argo_reference_institution

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-vars.R
\name{argo_read_vars}
\alias{argo_read_vars}
\title{Read NetCDF variable metadata}
\usage{
argo_read_vars(file, vars = NULL, quiet = FALSE)
}
\arguments{
\item{file}{A previously downloaded Argo NetCDF file
(e.g., using \code{\link[=argo_download]{argo_download()}}).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{quiet}{Use \code{FALSE} to stop for malformed files, \code{NA} to
silently warn for malformed files, or \code{TRUE} to silently ignore
read errors when possible.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with one row per variable and columns \code{name},
\code{size}, \code{dim}, and \verb{att_*} for variable attributes.
}
\description{
Use \code{argo_vars()} to extract variable information fromm an Argo NetCDF file
in the form of one row per variable.
}
\examples{
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_vars(prof_file)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-mirror.R
\name{argo_mirror}
\alias{argo_mirror}
\alias{argo_set_mirror}
\alias{with_argo_mirror}
\title{Argo Data default mirror}
\usage{
argo_mirror()

argo_set_mirror(mirror)

with_argo_mirror(mirror, expr)
}
\arguments{
\item{mirror}{The URL to an Argo mirror or a path to a directory
where Argo data has been cached or synced. According to the
\href{http://www.argodatamgt.org/Access-to-data/Argo-GDAC-ftp-and-https-servers}{Argo data access page},
The following public mirrors are available:
\itemize{
\item \url{ftp://usgodae.org/pub/outgoing/argo/}
\item \url{ftp://ftp.ifremer.fr/ifremer/argo/}
\item \url{https://data-argo.ifremer.fr}
}

Use \code{NULL} to reset to the default mirror.}

\item{expr}{An expression to be evaluated with the specified
default \code{mirror}.}
}
\value{
\itemize{
\item \code{argo_mirror()}: The current default mirror
\item \code{argo_set_mirror()}: The previously-set mirror
\item \code{with_argo_mirror()}: The result of \code{expr} evaluated using
\code{mirror} as the default mirror.
\item \code{argo_test_mirror()}: A filesystem mirror that contains several
files useful for tests and examples.
}
}
\description{
Fetch the default Argo mirror using \code{\link[=argo_mirror]{argo_mirror()}} or set it
using by \code{\link[=argo_set_mirror]{argo_set_mirror()}}. The default mirror is set to
\url{https://data-argo.ifremer.fr}. You can also set the default mirror
using \verb{options("argodata.mirror = "path/to/mirror")}. Use
\code{\link[=with_argo_mirror]{with_argo_mirror()}} to temporarily change the default mirror.
}
\examples{
argo_mirror()
with_argo_mirror("ftp://usgodae.org/pub/outgoing/argo/", argo_mirror())

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-cache.R, R/argo-mirror.R
\name{argo_cache_dir}
\alias{argo_cache_dir}
\alias{argo_update_global}
\alias{argo_update_data}
\alias{argo_set_cache_dir}
\alias{with_argo_cache_dir}
\alias{with_argo_example_cache}
\alias{argo_cached}
\alias{argo_test_mirror}
\title{Get and set the default cache directory}
\usage{
argo_cache_dir()

argo_update_global(max_global_cache_age = -Inf, all = TRUE, quiet = FALSE)

argo_update_data(max_data_cache_age = -Inf, quiet = FALSE)

argo_set_cache_dir(cache_dir)

with_argo_cache_dir(cache_dir, expr)

with_argo_example_cache(expr)

argo_cached(path)

argo_test_mirror()
}
\arguments{
\item{max_global_cache_age}{The maximum age in hours
to keep cached files. Use \code{Inf} to always use cached files; use \code{-Inf}
to always force download. You can set the default values of these using
\code{options(argodata.max_global_cache_age = ...)}
and/or \code{options(argodata.max_data_cache_age = ...)}.}

\item{all}{Should index files be downloaded even if they have not been
previously downloaded?}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}

\item{max_data_cache_age}{The maximum age in hours
to keep cached files. Use \code{Inf} to always use cached files; use \code{-Inf}
to always force download. You can set the default values of these using
\code{options(argodata.max_global_cache_age = ...)}
and/or \code{options(argodata.max_data_cache_age = ...)}.}

\item{cache_dir}{A writable directory in which downloaded files can be
cached.}

\item{expr}{An expression to be evaluated with the specified
default \code{cache_dir}.}

\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}
}
\value{
\itemize{
\item \code{argo_cache_dir()}: The directory where cache files are located
\item \code{argo_set_cache_dir()}: The previously set cache directory
\item \code{with_argo_cache_dir()}, \code{with_argo_example_cache()}: The result of
\code{expr}.
\item \code{argo_cached()}: The file path to the cached version of the Argo file
(which may or may not exist).
\item \code{argo_update_data()}, \code{argo_update_global()}: The locations of the
updated files.
}
}
\description{
The cache directory stores previously downloaded files to access them
more quickly. The cache directory stores files in the same format as a
an \code{\link[=argo_mirror]{argo_mirror()}}. By default, the cache is stored in a temporary
directory that is cleared when the session is restarted. This ensures
access to the latest index and files by default.
}
\examples{
argo_cache_dir()

temp_dir <- tempfile()
with_argo_cache_dir(temp_dir, argo_cache_dir())
unlink(temp_dir, recursive = TRUE)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-download.R
\name{argo_download}
\alias{argo_download}
\alias{argo_download_aux}
\alias{argo_should_download}
\title{Download Argo data files}
\usage{
argo_download(path, download = NULL, async = NULL, quiet = FALSE)

argo_download_aux(path, download = NULL, async = TRUE, quiet = FALSE)

argo_should_download(
  path,
  max_global_cache_age = getOption("argodata.max_global_cache_age", Inf),
  max_data_cache_age = getOption("argodata.max_data_cache_age", Inf)
)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{async}{Use \code{TRUE} to perform HTTP requests in parallel. This is much
faster for large numbers of small files.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}

\item{max_global_cache_age, max_data_cache_age}{The maximum age in hours
to keep cached files. Use \code{Inf} to always use cached files; use \code{-Inf}
to always force download. You can set the default values of these using
\code{options(argodata.max_global_cache_age = ...)}
and/or \code{options(argodata.max_data_cache_age = ...)}.}
}
\value{
A vector of cached filenames corresponding to \code{path}.
}
\description{
Download Argo data files
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-url.R
\name{argo_url}
\alias{argo_url}
\title{Construct Argo paths and URLs}
\usage{
argo_url(path)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}
}
\value{
A full URL according to the current \code{\link[=argo_mirror]{argo_mirror()}}
and \code{path}.
}
\description{
Construct Argo paths and URLs
}
\examples{
argo_url("ar_index_global_prof.txt.gz")

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-filter.R
\name{argo_filter_radius}
\alias{argo_filter_radius}
\alias{argo_filter_rect}
\alias{argo_filter_date}
\alias{argo_filter_updated}
\alias{argo_filter_float}
\alias{argo_filter_parameter}
\alias{argo_filter_data_mode}
\alias{argo_filter_parameter_data_mode}
\alias{argo_filter_direction}
\title{Select rows of Argo tables}
\usage{
argo_filter_radius(tbl, latitude, longitude, radius_km)

argo_filter_rect(tbl, latitude_min, latitude_max, longitude_min, longitude_max)

argo_filter_date(tbl, date_min, date_max = Sys.time())

argo_filter_updated(tbl, date_update_min, date_update_max = Sys.time())

argo_filter_float(tbl, float)

argo_filter_parameter(tbl, parameter)

argo_filter_data_mode(tbl, data_mode)

argo_filter_parameter_data_mode(tbl, parameter, data_mode)

argo_filter_direction(tbl, direction)
}
\arguments{
\item{tbl}{A data.frame, ideally derived from \code{\link[=argo_global_meta]{argo_global_meta()}}
and family. The column conventions used by the global indexes is assumed
(e.g., columns \code{latitude} and \code{longitude} exist).}

\item{latitude, longitude}{A location.}

\item{radius_km}{A radius from the point described by \code{latitude} and
\code{longitude}.}

\item{latitude_max, latitude_min, longitude_max, longitude_min}{A rectangle
describing the desired bounds. A rectangle where \code{longitude_min} is greater
than \code{longitude_max} are interpreted as wrapping across the international
date line.}

\item{date_min, date_max, date_update_min, date_update_max}{A range of
datetimes. Users are responsible for setting the timezone for these
objects and are encouraged to used UTC.}

\item{float}{A float identifier.}

\item{parameter}{One or more (case insensitive) parameter names in which
to search the \code{parameters} column of the bio-prof and synthetic-prof
index files.}

\item{data_mode}{One of "realtime" or "delayed".}

\item{direction}{One of "ascending" or "descending"}
}
\value{
\code{tbl} with rows that match the search criteria.
}
\description{
These functions are intended to be applied to \code{\link[=argo_global_meta]{argo_global_meta()}} and
other global index tables in the \verb{argo_global_*()} family.
}
\examples{
library(dplyr, warn.conflicts = FALSE)

\dontrun{
argo_global_prof() \%>\%
  # within 500 km of Halifax, Nova Scotia
  argo_filter_radius(45, -64, 500)
}

with_argo_example_cache({
  argo_global_traj() \%>\%
    argo_filter_rect(40, 60, -64, -54)
})

with_argo_example_cache({
  argo_global_traj() \%>\%
    argo_filter_updated("2020-01-01 00:00") \%>\%
    select(date_update, everything())
})

with_argo_example_cache({
  argo_global_traj() \%>\%
    argo_filter_float(c("13857", "15851"))
})

with_argo_example_cache({
  argo_global_traj() \%>\%
    argo_filter_data_mode("delayed")
})


}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-traj.R
\name{argo_read_traj_measurement}
\alias{argo_read_traj_measurement}
\alias{argo_read_traj_cycle}
\alias{argo_read_traj_param}
\alias{argo_read_traj_history}
\title{Read Argo trajectories}
\usage{
argo_read_traj_measurement(file, vars = NULL, quiet = FALSE)

argo_read_traj_cycle(file, vars = NULL, quiet = FALSE)

argo_read_traj_param(file, vars = NULL, quiet = FALSE)

argo_read_traj_history(file, vars = NULL, quiet = FALSE)
}
\arguments{
\item{file}{A previously downloaded Argo NetCDF file
(e.g., using \code{\link[=argo_download]{argo_download()}}).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{quiet}{Use \code{FALSE} to stop for malformed files, \code{NA} to
silently warn for malformed files, or \code{TRUE} to silently ignore
read errors when possible.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_read_traj_measurement()}: one per measurement.
\item \code{argo_read_traj_cycle()}: one per cycle.
\item \code{argo_read_traj_param()}: one per parameter.
\item \code{argo_read_traj_history()}: one row per history entry.
}
}
\description{
Use \verb{argo_read_traj_*()} functions to extract trajectory information from a
previously-downloaded Argo NetCDF file.
}
\examples{
traj_file <- system.file(
  "cache-test/dac/csio/2900313/2900313_Rtraj.nc",
  package = "argodata"
)

argo_read_traj_measurement(traj_file)
argo_read_traj_cycle(traj_file)
argo_read_traj_param(traj_file)
argo_read_traj_history(traj_file)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-global.R
\name{argo_read_global_meta}
\alias{argo_read_global_meta}
\alias{argo_read_global_prof}
\alias{argo_read_global_tech}
\alias{argo_read_global_traj}
\alias{argo_read_global_bio_traj}
\alias{argo_read_global_bio_prof}
\alias{argo_read_global_synthetic_prof}
\title{Read global index files}
\usage{
argo_read_global_meta(file)

argo_read_global_prof(file)

argo_read_global_tech(file)

argo_read_global_traj(file)

argo_read_global_bio_traj(file)

argo_read_global_bio_prof(file)

argo_read_global_synthetic_prof(file)
}
\arguments{
\item{file}{A path to a previously downloaded index file.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}}
}
\description{
Read global index files
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_history_qctest}
\alias{argo_reference_history_qctest}
\title{Argo QC test reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 22 rows and 4 columns.
}
\usage{
argo_reference_history_qctest
}
\description{
Contains a reference for values in the \code{history_qctest} column of
\code{\link[=argo_prof_history]{argo_prof_history()}} and \code{\link[=argo_traj_history]{argo_traj_history()}}. The values in this
column are a hexadecimal representation of the sum of the \code{qctest_value}
column. See \code{\link[=argo_unnest_history_qctest]{argo_unnest_history_qctest()}} to generate the values needed
to join this table.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_history(
    "dac/csio/2902746/profiles/BR2902746_001.nc",
    vars = "history_qctest"
  ) \%>\%
    argo_unnest_history_qctest() \%>\%
    left_join(argo_reference_history_qctest, by = "history_qctest") \%>\%
    select(history_qctest_description, everything())
})

argo_reference_history_qctest

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-qc.R
\name{argo_qc_censor_if_not}
\alias{argo_qc_censor_if_not}
\alias{argo_qc_censor_if}
\alias{argo_qc_keep_if_all}
\alias{argo_qc_keep_if_any}
\alias{argo_qc_discard_if_all}
\alias{argo_qc_discard_if_any}
\alias{argo_qc_cols}
\title{Apply QC flags to Argo data}
\usage{
argo_qc_censor_if_not(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_censor_if(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_keep_if_all(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_keep_if_any(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_discard_if_all(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_discard_if_any(
  tbl,
  cols,
  qc_flag,
  qc_cols = argo_qc_cols(tbl, {     {         cols     } })
)

argo_qc_cols(tbl, cols)
}
\arguments{
\item{tbl}{A data frame containing \verb{_qc} or \verb{_QC} columns.}

\item{cols}{Columns in \code{tbl}, specified using \code{\link[dplyr:select]{dplyr::select()}} syntax.}

\item{qc_flag}{One or more quality control flags. See
\code{argo_reference_qc_flag} for long-form descriptions of integer \code{qc_flag}
values.}

\item{qc_cols}{A vector of columns that contain the quality control flag
values found in columns.}
}
\value{
A modified \code{tbl}.
}
\description{
Apply QC flags to Argo data
}
\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_levels("dac/csio/2902746/profiles/BR2902746_001.nc") \%>\%
    argo_qc_censor_if_not(doxy, qc_flag = c(1, 2, 8))
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_history_action}
\alias{argo_reference_history_action}
\title{Argo history action reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 14 rows and 2 columns.
}
\usage{
argo_reference_history_action
}
\description{
This table contains descriptions for the codes used in the \code{history_action}
column present in \code{\link[=argo_prof_history]{argo_prof_history()}} and \code{\link[=argo_traj_history]{argo_traj_history()}}.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = TRUE)

with_argo_example_cache({
  argo_prof_history(
    "dac/csio/2902746/profiles/BR2902746_001.nc",
    vars = "history_action"
  ) \%>\%
    left_join(
      argo_reference_history_action,
      by = "history_action"
    )
})

argo_reference_history_action

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-info.R
\name{argo_info}
\alias{argo_info}
\title{Load NetCDF general information}
\usage{
argo_info(path, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with one row per file. Columns containing
global attribute information are prefixed with \code{att_} to differentiate
them from variables with zero dimensions.
}
\description{
Use \code{argo_info()} to extract scalar variables and global attributes
from a vector of Argo NetCDF files. Use \code{\link[=argo_read_info]{argo_read_info()}} to extract
variables from a previously-downloaded Argo NetCDF file.
}
\examples{
with_argo_example_cache({
  argo_info("dac/csio/2900313/profiles/D2900313_000.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-pivot.R
\name{argo_pivot_longer}
\alias{argo_pivot_longer}
\title{Transform Argo data to variable-long format}
\usage{
argo_pivot_longer(tbl, id_cols)
}
\arguments{
\item{tbl}{An Argo table with paired value/qc/adjusted columns, likely
from \code{\link[=argo_prof_levels]{argo_prof_levels()}} or \code{\link[=argo_traj_measurement]{argo_traj_measurement()}}.}

\item{id_cols}{A vector of column names used to identify each row in the
output using \code{\link[dplyr:select]{dplyr::select()}} syntax.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with columns \code{id_cols}, \code{variable}, \code{value},
\code{value_qc}, \code{value_adjusted}, \code{value_adjusted_qc}, and
\code{value_adjusted_error}.
}
\description{
Whereas the variable-wide format returned by most Argo read and load
functions are useful, some processing and plotting algorithms need data
in a variable-long form, where the variable name is assigned to a column
whose value is used to identify the measurement represented by each row.
}
\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_levels("dac/csio/2902746/profiles/BR2902746_001.nc") \%>\%
    argo_pivot_longer(id_cols = c(file, n_prof, pres))
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-qctest.R
\name{argo_unnest_history_qctest}
\alias{argo_unnest_history_qctest}
\title{Unnest Argo QC tests}
\usage{
argo_unnest_history_qctest(tbl)
}
\arguments{
\item{tbl}{A data frame with a \code{history_qctest} column.}
}
\value{
\code{tbl}
}
\description{
The \code{history_qctest} column in \code{\link[=argo_prof_history]{argo_prof_history()}} and
\code{\link[=argo_traj_history]{argo_traj_history()}} is a binary flags column, allowing a single integer
value to represent up to 32 logical values. This function expands its
input such that there is one test represented by each row and can be
joined to \link{argo_reference_history_qctest}
on the \code{history_qctest} column.
}
\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_history(
    "dac/csio/2902746/profiles/BR2902746_001.nc",
    vars = "history_qctest"
  ) \%>\%
    argo_unnest_history_qctest() \%>\%
    left_join(argo_reference_history_qctest, by = "history_qctest") \%>\%
    select(history_qctest_description, everything())
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_qc_flag}
\alias{argo_reference_qc_flag}
\title{Argo QC flag reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 10 rows and 4 columns.
}
\usage{
argo_reference_qc_flag
}
\description{
This table contains the quality control descriptions of the one-character
flags that appear in variables that end with \verb{_qc}. These variables appear
in \code{\link[=argo_prof_levels]{argo_prof_levels()}}, \code{\link[=argo_prof_prof]{argo_prof_prof()}}, and \code{\link[=argo_traj_measurement]{argo_traj_measurement()}}. See
also \link{argo_reference_history_qctest}
for information about specific tests.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_levels(
    "dac/csio/2902746/profiles/BR2902746_001.nc",
    vars = c("pres", "pres_qc", "doxy", "doxy_qc")
  ) \%>\%
    dplyr::left_join(
      argo_reference_qc_flag,
      by = c("doxy_qc" = "qc_flag")
    )
})

argo_reference_qc_flag

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-adjusted.R
\name{argo_use_adjusted}
\alias{argo_use_adjusted}
\alias{argo_adjusted_cols}
\title{Prefer adjusted values}
\usage{
argo_use_adjusted(
  tbl,
  cols,
  adjusted_cols = argo_adjusted_cols(tbl, {     {         cols     } })
)

argo_adjusted_cols(tbl, cols)
}
\arguments{
\item{tbl}{tbl A data frame containing \verb{_adjusted} or \verb{_ADJUSTED} columns.}

\item{cols}{Columns in \code{tbl}, specified using \code{\link[dplyr:select]{dplyr::select()}} syntax.}

\item{adjusted_cols}{A paired vector of columns to \code{cols} containing the
adjusted value to prefer over the value in \code{cols}.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
}
\description{
Many Argo tables have \verb{_adjusted} columns that contain a more reliable value.
However, in many cases, these values are blank or not present. Use
\code{argo_use_adjusted()} to replace \code{NA} values in \code{cols} with the value in
the paired \verb{_adjusted} column. You can use this with
\code{\link[=argo_qc_censor_if_not]{argo_qc_censor_if_not()}} to set values in \code{cols} to \code{NA} based on the paired
\verb{_qc} column prior to preferring the adjusted value.
}
\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_prof_levels("dac/csio/2902746/profiles/BR2902746_001.nc") \%>\%
    argo_use_adjusted(doxy)
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_location_class}
\alias{argo_reference_location_class}
\title{Argo location class reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 15 rows and 2 columns.
}
\usage{
argo_reference_location_class
}
\description{
Argo location class reference
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
argo_reference_location_class

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-map.R
\name{argo_map}
\alias{argo_map}
\alias{argo_set_mapper}
\alias{argo_map_default}
\title{Configure iteration over NetCDF files}
\usage{
argo_map(.x, .f, ...)

argo_set_mapper(.f)

argo_map_default(.x, .f, ...)
}
\arguments{
\item{.x}{A vector of NetCDF files.}

\item{.f}{A function}

\item{...}{Passed to \code{.f}}
}
\value{
\itemize{
\item \code{argo_map()}, \code{argo_map_default()}: A \code{list()} along \code{.x} with
\code{.f} applied to each element.
\item \code{argo_set_mapper()}: The previous mapper.
}
}
\description{
Several operations in argodata require iteration over many NetCDF files.
Users may wish to use parallel processing and/or
display progress during these operations; these functions allow custom
mappers and/or progress handlers to be set.
}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-traj.R
\name{argo_traj_measurement}
\alias{argo_traj_measurement}
\alias{argo_traj_cycle}
\alias{argo_traj_param}
\alias{argo_traj_history}
\title{Load Argo trajectories}
\usage{
argo_traj_measurement(path, vars = NULL, download = NULL, quiet = NA)

argo_traj_cycle(path, vars = NULL, download = NULL, quiet = NA)

argo_traj_param(path, vars = NULL, download = NULL, quiet = NA)

argo_traj_history(path, vars = NULL, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_traj_measurement()}: one row per file per measurement.
\item \code{argo_traj_cycle()}: one row per file per cycle.
\item \code{argo_traj_param()}: one row per file per parameter.
\item \code{argo_traj_history()}: one row per file per history entry.
}
}
\description{
Use \verb{argo_traj_*()} functions to extract information from Argo trajectory
NetCDF files. Use \code{\link[=argo_read_traj_measurement]{argo_read_traj_*()}}
to extract information from a single previously-downloaded NetCDF file.
}
\examples{
with_argo_example_cache({
  argo_traj_measurement("dac/csio/2900313/2900313_Rtraj.nc")
})

with_argo_example_cache({
  argo_traj_cycle("dac/csio/2900313/2900313_Rtraj.nc")
})

with_argo_example_cache({
  argo_traj_param("dac/csio/2900313/2900313_Rtraj.nc")
})

with_argo_example_cache({
  argo_traj_history("dac/csio/2900313/2900313_Rtraj.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-meta.R
\name{argo_meta_config_param}
\alias{argo_meta_config_param}
\alias{argo_meta_missions}
\alias{argo_meta_trans_system}
\alias{argo_meta_positioning_system}
\alias{argo_meta_launch_config_param}
\alias{argo_meta_sensor}
\alias{argo_meta_param}
\title{Load Argo float meta}
\usage{
argo_meta_config_param(path, download = NULL, quiet = NA)

argo_meta_missions(path, download = NULL, quiet = NA)

argo_meta_trans_system(path, download = NULL, quiet = NA)

argo_meta_positioning_system(path, download = NULL, quiet = NA)

argo_meta_launch_config_param(path, download = NULL, quiet = NA)

argo_meta_sensor(path, download = NULL, quiet = NA)

argo_meta_param(path, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_meta_config_param()}: one row per file per configuration parameter and
columns \code{file}, \code{n_config_param}, \code{n_missions}, \code{config_parameter_value},
and \code{config_parameter_name}.
\item \code{argo_meta_missions()}: one row per file per mission and columns \code{file},
\code{n_missions}, \code{config_mission_number}, and \code{config_mission_comment}.
\item \code{argo_meta_trans_system()}: one row per file per transmission system
and columns \code{file}, \code{n_trans_system}, \code{trans_system}, \code{trans_system_id},
and \code{trans_frequency}.
\item \code{argo_meta_positioning_system()}: one row per file per positioning
system and columns \code{file}, \code{n_positioning_system}, and
\code{positioning_system}.
\item \code{argo_meta_launch_config_param()}: one row per file per launch
configuration parameter and columns \code{file}, \code{n_launch_config_param},
\code{launch_config_parameter_name}, and \code{launch_config_parameter_value}.
\item \code{argo_meta_sensor()}: one row per file per sensor and columns
\code{file}, \code{n_sensor}, \code{sensor}, \code{sensor_maker}, \code{sensor_model},
and \code{sensor_serial_no}.
\item \code{argo_meta_param()}: one row per file per parameter and columns \code{file},
\code{n_param}, \code{parameter} \code{parameter_sensor}, \code{parameter_units},
\code{parameter_resolution}, \code{predeployment_calib_equation},
\code{predeployment_calib_coefficient}, and \code{predeployment_calib_comment}.
}
}
\description{
Use \verb{argo_meta_*()} functions to extract information from Argo meta
NetCDF files. Use \code{\link[=argo_read_meta_config_param]{argo_read_meta_*()}}
to extract information from a single previously-downloaded NetCDF file.
Using \code{\link[=argo_info]{argo_info()}} on meta files is also useful for extracting general
float information.
}
\examples{
with_argo_example_cache({
  argo_meta_config_param("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_missions("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_trans_system("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_positioning_system("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_launch_config_param("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_sensor("dac/csio/2900313/2900313_meta.nc")
})

with_argo_example_cache({
  argo_meta_param("dac/csio/2900313/2900313_meta.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-global.R
\name{argo_global_meta}
\alias{argo_global_meta}
\alias{argo_global_prof}
\alias{argo_global_tech}
\alias{argo_global_traj}
\alias{argo_global_bio_traj}
\alias{argo_global_bio_prof}
\alias{argo_global_synthetic_prof}
\title{Load Argo global index files}
\usage{
argo_global_meta(download = NULL, quiet = FALSE)

argo_global_prof(download = NULL, quiet = FALSE)

argo_global_tech(download = NULL, quiet = FALSE)

argo_global_traj(download = NULL, quiet = FALSE)

argo_global_bio_traj(download = NULL, quiet = FALSE)

argo_global_bio_prof(download = NULL, quiet = FALSE)

argo_global_synthetic_prof(download = NULL, quiet = FALSE)
}
\arguments{
\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
\itemize{
\item \code{argo_global_meta()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo meta file index
\item \code{argo_global_tech()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo tech file index
\item \code{argo_global_traj()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo trajectory file index
\item \code{argo_global_bio_traj()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo meta
biogeochemical trajectory index
\item \code{argo_global_prof()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo profile file index
\item \code{argo_global_bio_prof()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo biogeochemical profile
index
\item \code{argo_global_synthetic_prof()}: a \code{\link[tibble:tibble]{tibble::tibble()}} of the Argo
biogeochemical synthetic profile index
}
}
\description{
The \code{\link[=argo_mirror]{argo_mirror()}} points to a directory containing millions of NetCDF
files in which Argo data is organized. These files are indexed by category
in the root directory of the mirror. The \verb{argo_global_*()} functions provide
access to these as data frames.
}
\examples{
with_argo_example_cache({
  argo_global_meta()
})

with_argo_example_cache({
  argo_global_tech()
})

with_argo_example_cache({
  argo_global_traj()
})

with_argo_example_cache({
  argo_global_bio_traj()
})

\dontrun{
# extended download time if no cached version is available
argo_global_prof()
argo_global_bio_prof()
argo_global_synthetic_prof()
}

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-prof.R
\name{argo_read_prof_levels}
\alias{argo_read_prof_levels}
\alias{argo_read_prof_prof}
\alias{argo_read_prof_calib}
\alias{argo_read_prof_param}
\alias{argo_read_prof_history}
\alias{argo_read_prof_spectra}
\title{Read Argo profiles}
\usage{
argo_read_prof_levels(file, vars = NULL, quiet = FALSE)

argo_read_prof_prof(file, vars = NULL, quiet = FALSE)

argo_read_prof_calib(file, vars = NULL, quiet = FALSE)

argo_read_prof_param(file, vars = NULL, quiet = FALSE)

argo_read_prof_history(file, vars = NULL, quiet = FALSE)

argo_read_prof_spectra(file, vars = NULL, quiet = FALSE)
}
\arguments{
\item{file}{A previously downloaded Argo NetCDF file
(e.g., using \code{\link[=argo_download]{argo_download()}}).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{quiet}{Use \code{FALSE} to stop for malformed files, \code{NA} to
silently warn for malformed files, or \code{TRUE} to silently ignore
read errors when possible.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_read_prof_levels()}: one row per profile per sampling level.
\item \code{argo_read_prof_prof()}: one row per profile.
\item \code{argo_read_prof_calib()}: one row per profile per calibration per parameter.
\item \code{argo_read_prof_param()}: one row per profile per parameter.
\item \code{argo_read_prof_history()}: one row per profile per history entry.
\item \code{argo_read_prof_spectra()}: one row per profile per sampling level per
spectra value.
}
}
\description{
Use \verb{argo_read_prof_*()} functions to extract profile information from a
previously-downloaded Argo NetCDF file.
}
\examples{
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_prof_levels(prof_file)
argo_read_prof_prof(prof_file)
argo_read_prof_calib(prof_file)
argo_read_prof_param(prof_file)
argo_read_prof_history(prof_file)

bgc_file <- system.file(
  "cache-test/dac/aoml/5906206/profiles/BD5906206_016.nc",
  package = "argodata"
)
argo_read_prof_spectra(bgc_file)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-info.R
\name{argo_read_info}
\alias{argo_read_info}
\title{Read NetCDF general information}
\usage{
argo_read_info(file, quiet = FALSE)
}
\arguments{
\item{file}{A previously downloaded Argo NetCDF file
(e.g., using \code{\link[=argo_download]{argo_download()}}).}

\item{quiet}{Use \code{FALSE} to stop for malformed files, \code{NA} to
silently warn for malformed files, or \code{TRUE} to silently ignore
read errors when possible.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with one row. Columns containing
global attribute information are prefixed with \code{att_} to differentiate
them from variables with zero dimensions.
}
\description{
Use \code{argo_read_info()} to extract variables and global attributes from
a previously-downloaded Argo NetCDF file. The variables read by
\code{argo_read_info()} are always length 1.
}
\examples{
prof_file <- system.file(
  "cache-test/dac/csio/2900313/profiles/D2900313_000.nc",
  package = "argodata"
)

argo_read_info(prof_file)

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/argo-prof.R
\name{argo_prof_levels}
\alias{argo_prof_levels}
\alias{argo_prof_prof}
\alias{argo_prof_calib}
\alias{argo_prof_param}
\alias{argo_prof_history}
\alias{argo_prof_spectra}
\title{Load Argo profiles}
\usage{
argo_prof_levels(path, vars = NULL, download = NULL, quiet = NA)

argo_prof_prof(path, vars = NULL, download = NULL, quiet = NA)

argo_prof_calib(path, vars = NULL, download = NULL, quiet = NA)

argo_prof_param(path, vars = NULL, download = NULL, quiet = NA)

argo_prof_history(path, vars = NULL, download = NULL, quiet = NA)

argo_prof_spectra(path, vars = NULL, download = NULL, quiet = NA)
}
\arguments{
\item{path}{A path relative to the root directory of
\code{\link[=argo_mirror]{argo_mirror()}} or \code{\link[=argo_cache_dir]{argo_cache_dir()}}. This value can also
be a data.frame with a \code{file} column (e.g., a global index as
returned by \code{\link[=argo_global_meta]{argo_global_meta()}} and others).}

\item{vars}{A vector of variable names to include. Explicitly specifying
\code{vars} can lead to much faster read times when reading many files.}

\item{download}{A logical vector indicating whether or not
a file should be downloaded. Defaults to the value of
\code{\link[=argo_should_download]{argo_should_download()}}, which is \code{TRUE} for files that
do not exist in the cache.}

\item{quiet}{Use \code{FALSE} to show which files are downloaded and for more
verbose error messages.}
}
\value{
A \code{\link[tibble:tibble]{tibble::tibble()}} with
\itemize{
\item \code{argo_prof_levels()}: one row per file per profile per sampling level.
\item \code{argo_prof_prof()}: one row per file per profile.
\item \code{argo_prof_calib()}: one row per file per profile per calibration per
parameter.
\item \code{argo_prof_param()}: one row per file per profile per parameter.
\item \code{argo_prof_history()}: one row per file per profile per history entry.
\item \code{argo_prof_spectra()}: one row per file per profile per sampling level
per spectra value.
}
}
\description{
Use \verb{argo_prof_*()} functions to extract information from Argo profile
NetCDF files. Use \code{\link[=argo_read_prof_levels]{argo_read_prof_*()}}
to extract information from a single previously-downloaded NetCDF file.
}
\examples{
with_argo_example_cache({
  argo_prof_levels("dac/csio/2900313/profiles/D2900313_000.nc")
})

with_argo_example_cache({
  argo_prof_prof("dac/csio/2900313/profiles/D2900313_000.nc")
})

with_argo_example_cache({
  argo_prof_calib("dac/csio/2900313/profiles/D2900313_000.nc")
})

with_argo_example_cache({
  argo_prof_param("dac/csio/2900313/profiles/D2900313_000.nc")
})

with_argo_example_cache({
  argo_prof_history("dac/csio/2900313/profiles/D2900313_000.nc")
})

with_argo_example_cache({
  argo_prof_spectra("dac/aoml/5906206/profiles/BD5906206_016.nc")
})

}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_data_type}
\alias{argo_reference_data_type}
\title{Argo data type reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 9 rows and 2 columns.
}
\usage{
argo_reference_data_type
}
\description{
This table contains the acceptable values for the \code{data_type} field in
Argo NetCDF files. This field can be examined using \code{\link[=argo_info]{argo_info()}}.
See Reference Table 1 in the Argo User's Manual.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
with_argo_example_cache({
  argo_info("dac/csio/2900313/2900313_meta.nc")[c("file", "data_type")]
})

argo_reference_data_type

}
\keyword{datasets}
% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data.R
\docType{data}
\name{argo_reference_positioning_system}
\alias{argo_reference_positioning_system}
\title{Argo positioning system reference}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 9 rows and 2 columns.
}
\usage{
argo_reference_positioning_system
}
\description{
Contains a reference for values in the \code{positioning_system} column of
\code{\link[=argo_meta_positioning_system]{argo_meta_positioning_system()}}.
}
\section{References}{

Argo User's Manual, November 2019. \doi{10.13155/29825}
}

\examples{
library(dplyr, warn.conflicts = FALSE)

with_argo_example_cache({
  argo_meta_positioning_system("dac/csio/2900313/2900313_meta.nc") \%>\%
    left_join(argo_reference_positioning_system, by = "positioning_system")
})

argo_reference_positioning_system

}
\keyword{datasets}
