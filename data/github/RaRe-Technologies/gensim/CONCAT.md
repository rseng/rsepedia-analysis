Changes
=======

## Unreleased

* [#3194](https://github.com/RaRe-Technologies/gensim/pull/3194): Added random_seed parameter to make LsiModel reproducible, by [@parashardhapola](https://github.com/parashardhapola)
* [#3251](https://github.com/RaRe-Technologies/gensim/pull/3251): Apply new convention of delimiting instance params in str function, by [@menshikh-iv](https://github.com/menshikh-iv)
* [#3227](https://github.com/RaRe-Technologies/gensim/pull/3227): Fix FastText doc-comment example for `build_vocab` and `train` to use correct argument names, by [@HLasse](https://github.com/HLasse)
* [#3247](https://github.com/RaRe-Technologies/gensim/pull/3247): Sparse2Corpus: update __getitem__ to work on slices, lists and ellipsis, by [@PrimozGodec](https://github.com/PrimozGodec)
* [#3250](https://github.com/RaRe-Technologies/gensim/pull/3250): Make negative ns_exponent work correctly, by [@menshikh-iv](https://github.com/menshikh-iv)
* [#3258](https://github.com/RaRe-Technologies/gensim/pull/3258): Adding another check to _check_corpus_sanity for compressed files, adding test, by [@dchaplinsky](https://github.com/dchaplinsky)
* [#3274](https://github.com/RaRe-Technologies/gensim/pull/3274): Migrate setup.py from distutils to setuptools, by [@geojacobm6](https://github.com/geojacobm6)

## 4.1.2, 2021-09-17

This is a bugfix release that addresses left over compatibility issues with older versions of numpy and MacOS.

## 4.1.1, 2021-09-14

This is a bugfix release that addresses compatibility issues with older versions of numpy.

## 4.1.0, 2021-08-15

Gensim 4.1 brings two major new functionalities:

* [Ensemble LDA](https://radimrehurek.com/gensim/auto_examples/tutorials/run_ensemblelda.html) for robust training, selection and comparison of LDA models.
* [FastSS module](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/similarities/fastss.pyx) for super fast Levenshtein "fuzzy search" queries. Used e.g. for ["soft term similarity"](https://github.com/RaRe-Technologies/gensim/pull/3146) calculations.

There are several minor changes that are **not** backwards compatible with previous versions of Gensim.
The affected functionality is relatively less used, so it is unlikely to affect most users, so we have opted to not require a major version bump.
Nevertheless, we describe them below.

### Improved parameter edge-case handling in KeyedVectors most_similar and most_similar_cosmul methods

We now handle both ``positive`` and ``negative`` keyword parameters consistently.
They may now be either:

1. A string, in which case the value is reinterpreted as a list of one element (the string value)
2. A vector, in which case the value is reinterpreted as a list of one element (the vector)
3. A list of strings
4. A list of vectors

So you can now simply do:

```python
    model.most_similar(positive='war', negative='peace')
```

instead of the slightly more involved

```python
model.most_similar(positive=['war'], negative=['peace'])
```

Both invocations remain correct, so you can use whichever is most convenient.
If you were somehow expecting gensim to interpret the strings as a list of characters, e.g.

```python
model.most_similar(positive=['w', 'a', 'r'], negative=['p', 'e', 'a', 'c', 'e'])
```

then you will need to specify the lists explicitly in gensim 4.1.
### Deprecated obsolete `step` parameter from doc2vec

With the newer version, do this:

```python
model.infer_vector(..., epochs=123)
```

instead of this:

```python
model.infer_vector(..., steps=123)
```

Plus a large number of smaller improvements and fixes, as usual.

**‚ö†Ô∏è If migrating from old Gensim 3.x, read the [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) first.**

### :+1: New features

* [#3169](https://github.com/RaRe-Technologies/gensim/pull/3169): Implement `shrink_windows` argument for Word2Vec, by [@M-Demay](https://github.com/M-Demay)
* [#3163](https://github.com/RaRe-Technologies/gensim/pull/3163): Optimize word mover distance (WMD) computation, by [@flowlight0](https://github.com/flowlight0)
* [#3157](https://github.com/RaRe-Technologies/gensim/pull/3157): New KeyedVectors.vectors_for_all method for vectorizing all words in a dictionary, by [@Witiko](https://github.com/Witiko)
* [#3153](https://github.com/RaRe-Technologies/gensim/pull/3153): Vectorize word2vec.predict_output_word for speed, by [@M-Demay](https://github.com/M-Demay)
* [#3146](https://github.com/RaRe-Technologies/gensim/pull/3146): Use FastSS for fast kNN over Levenshtein distance, by [@Witiko](https://github.com/Witiko)
* [#3128](https://github.com/RaRe-Technologies/gensim/pull/3128): Materialize and copy the corpus passed to SoftCosineSimilarity, by [@Witiko](https://github.com/Witiko)
* [#3115](https://github.com/RaRe-Technologies/gensim/pull/3115): Make LSI dispatcher CLI param for number of jobs optional, by [@robguinness](https://github.com/robguinness)
* [#3091](https://github.com/RaRe-Technologies/gensim/pull/3091): LsiModel: Only log top words that actually exist in the dictionary, by [@kmurphy4](https://github.com/kmurphy4)
* [#2980](https://github.com/RaRe-Technologies/gensim/pull/2980): Added EnsembleLda for stable LDA topics, by [@sezanzeb](https://github.com/sezanzeb)
* [#2978](https://github.com/RaRe-Technologies/gensim/pull/2978): Optimize performance of Author-Topic model, by [@horpto](https://github.com/horpto)
* [#3000](https://github.com/RaRe-Technologies/gensim/pull/3000): Tidy up KeyedVectors.most_similar() API, by [@simonwiles](https://github.com/simonwiles)

### :books: Tutorials and docs

* [#3155](https://github.com/RaRe-Technologies/gensim/pull/3155): Correct parameter name in documentation of fasttext.py, by [@bizzyvinci](https://github.com/bizzyvinci)
* [#3148](https://github.com/RaRe-Technologies/gensim/pull/3148): Fix broken link to mycorpus.txt in documentation, by [@rohit901](https://github.com/rohit901)
* [#3142](https://github.com/RaRe-Technologies/gensim/pull/3142): Use more permanent pdf link and update code link, by [@dymil](https://github.com/dymil)
* [#3141](https://github.com/RaRe-Technologies/gensim/pull/3141): Update link for online LDA paper, by [@dymil](https://github.com/dymil)
* [#3133](https://github.com/RaRe-Technologies/gensim/pull/3133): Update link to Hoffman paper (online VB LDA), by [@jonaschn](https://github.com/jonaschn)
* [#3129](https://github.com/RaRe-Technologies/gensim/pull/3129): [MRG] Add bronze sponsor: TechTarget, by [@piskvorky](https://github.com/piskvorky)
* [#3126](https://github.com/RaRe-Technologies/gensim/pull/3126): Fix typos in make_wiki_online.py and make_wikicorpus.py, by [@nicolasassi](https://github.com/nicolasassi)
* [#3125](https://github.com/RaRe-Technologies/gensim/pull/3125): Improve & unify docs for dirichlet priors, by [@jonaschn](https://github.com/jonaschn)
* [#3123](https://github.com/RaRe-Technologies/gensim/pull/3123): Fix hyperlink for doc2vec tutorial, by [@AdityaSoni19031997](https://github.com/AdityaSoni19031997)
* [#3121](https://github.com/RaRe-Technologies/gensim/pull/3121): [MRG] Add bronze sponsor: eaccidents.com, by [@piskvorky](https://github.com/piskvorky)
* [#3120](https://github.com/RaRe-Technologies/gensim/pull/3120): Fix URL for ldamodel.py, by [@jonaschn](https://github.com/jonaschn)
* [#3118](https://github.com/RaRe-Technologies/gensim/pull/3118): Fix URL in doc string, by [@jonaschn](https://github.com/jonaschn)
* [#3107](https://github.com/RaRe-Technologies/gensim/pull/3107): Draw attention to sponsoring in README, by [@piskvorky](https://github.com/piskvorky)
* [#3105](https://github.com/RaRe-Technologies/gensim/pull/3105): Fix documentation links: Travis to Github Actions, by [@piskvorky](https://github.com/piskvorky)
* [#3057](https://github.com/RaRe-Technologies/gensim/pull/3057): Clarify doc comment in LdaModel.inference(), by [@yocen](https://github.com/yocen)
* [#2964](https://github.com/RaRe-Technologies/gensim/pull/2964): Document that preprocessing.strip_punctuation is limited to ASCII, by [@sciatro](https://github.com/sciatro)


### :red_circle: Bug fixes

* [#3178](https://github.com/RaRe-Technologies/gensim/pull/3178): Fix Unicode string incompatibility in gensim.similarities.fastss.editdist, by [@Witiko](https://github.com/Witiko)
* [#3174](https://github.com/RaRe-Technologies/gensim/pull/3174): Fix loading Phraser models stored in Gensim 3.x into Gensim 4.0, by [@emgucv](https://github.com/emgucv)
* [#3136](https://github.com/RaRe-Technologies/gensim/pull/3136): Fix indexing error in word2vec_inner.pyx, by [@bluekura](https://github.com/bluekura)
* [#3131](https://github.com/RaRe-Technologies/gensim/pull/3131): Add missing import to NMF docs and models/__init__.py, by [@properGrammar](https://github.com/properGrammar)
* [#3116](https://github.com/RaRe-Technologies/gensim/pull/3116): Fix bug where saved Phrases model did not load its connector_words, by [@aloknayak29](https://github.com/aloknayak29)
* [#2830](https://github.com/RaRe-Technologies/gensim/pull/2830): Fixed KeyError in coherence model, by [@pietrotrope](https://github.com/pietrotrope)


### :warning: Removed functionality & deprecations

* [#3176](https://github.com/RaRe-Technologies/gensim/pull/3176): Eliminate obsolete step parameter from doc2vec infer_vector and similarity_unseen_docs, by [@rock420](https://github.com/rock420)
* [#2965](https://github.com/RaRe-Technologies/gensim/pull/2965): Remove strip_punctuation2 alias of strip_punctuation, by [@sciatro](https://github.com/sciatro)
* [#3180](https://github.com/RaRe-Technologies/gensim/pull/3180): Move preprocessing functions from gensim.corpora.textcorpus and gensim.corpora.lowcorpus to gensim.parsing.preprocessing, by [@rock420](https://github.com/rock420)

### üîÆ Testing, CI, housekeeping

* [#3156](https://github.com/RaRe-Technologies/gensim/pull/3156): Update Numpy minimum version to 1.17.0, by [@PrimozGodec](https://github.com/PrimozGodec)
* [#3143](https://github.com/RaRe-Technologies/gensim/pull/3143): replace _mul function with explicit casts, by [@mpenkov](https://github.com/mpenkov)
* [#2952](https://github.com/RaRe-Technologies/gensim/pull/2952): Allow newer versions of the Morfessor module for the tests, by [@pabs3](https://github.com/pabs3)
* [#2965](https://github.com/RaRe-Technologies/gensim/pull/2965): Remove strip_punctuation2 alias of strip_punctuation, by [@sciatro](https://github.com/sciatro)



## 4.0.1, 2021-04-01

Bugfix release to address issues with Wheels on Windows:

- https://github.com/RaRe-Technologies/gensim/issues/3095
- https://github.com/RaRe-Technologies/gensim/issues/3097

## 4.0.0, 2021-03-24

**‚ö†Ô∏è Gensim 4.0 contains breaking API changes! See the [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to update your existing Gensim 3.x code and models.**

Gensim 4.0 is a major release with lots of performance & robustness improvements, and a new website.

### Main highlights

* Massively optimized popular algorithms the community has grown to love: [fastText](https://radimrehurek.com/gensim/models/fasttext.html), [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html), [phrases](https://radimrehurek.com/gensim/models/phrases.html):

  a. **Efficiency**

    | model | 3.8.3: wall time / peak RAM / throughput | 4.0.0: wall time / peak RAM / throughput |
    |----------|------------|--------|
    | fastText | 2.9h / 4.11 GB / 822k words/s | 2.3h / **1.26 GB** / 914k words/s |
    | word2vec | 1.7h / 0.36 GB / 1685k words/s | **1.2h** / 0.33 GB / 1762k words/s |

    In other words, fastText now needs 3x less RAM (and is faster); word2vec has 2x faster init (and needs less RAM, and is faster); detecting collocation phrases is 2x faster. ([4.0 benchmarks](https://github.com/RaRe-Technologies/gensim/issues/2887#issuecomment-711097334))

  b. **Robustness**. We fixed a bunch of long-standing bugs by refactoring the internal code structure (see üî¥ Bug fixes below)

  c. **Simplified OOP model** for easier model exports and integration with TensorFlow, PyTorch &co.

  These improvements come to you transparently aka "for free", but see [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) for some changes that break the old Gensim 3.x API. **Update your code accordingly**.

* Dropped a bunch of externally contributed modules and wrappers: summarization, pivoted TFIDF, Mallet‚Ä¶
  - Code quality was not up to our standards. Also there was no one to maintain these modules, answer user questions, support them.

    So rather than let them rot, we took the hard decision of removing these contributed modules from Gensim. If anyone's interested in maintaining them, please fork & publish into your own repo. They can live happily outside of Gensim.

* Dropped Python 2. Gensim 4.0 is Py3.6+. Read our [Python version support policy](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility).
  - If you still need Python 2 for some reason, stay at [Gensim 3.8.3](https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3).

* A new [Gensim website](https://radimrehurek.com/gensim) ‚Äì¬†finally! üôÉ

So, a major clean-up release overall. We're happy with this **tighter, leaner and faster Gensim**.

This is the direction we'll keep going forward: less kitchen-sink of "latest academic algorithms", more focus on robust engineering, targetting concrete NLP & document similarity use-cases.

### :+1: New features

* [#2947](https://github.com/RaRe-Technologies/gensim/pull/2947): Bump minimum Python version to 3.6, by [@gojomo](https://github.com/gojomo)
* [#2300](https://github.com/RaRe-Technologies/gensim/pull/2300): Use less RAM in LdaMulticore, by [@horpto](https://github.com/horpto)
* [#2698](https://github.com/RaRe-Technologies/gensim/pull/2698): Streamline KeyedVectors & X2Vec API, by [@gojomo](https://github.com/gojomo)
* [#2864](https://github.com/RaRe-Technologies/gensim/pull/2864): Speed up random number generation in word2vec, by [@zygm0nt](https://github.com/zygm0nt)
* [#2976](https://github.com/RaRe-Technologies/gensim/pull/2976): Speed up phrase (collocation) detection, by [@piskvorky](https://github.com/piskvorky)
* [#2979](https://github.com/RaRe-Technologies/gensim/pull/2979): Allow skipping common English words in multi-word phrases, by [@piskvorky](https://github.com/piskvorky)
* [#2867](https://github.com/RaRe-Technologies/gensim/pull/2867): Expose `max_final_vocab` parameter in fastText constructor, by [@mpenkov](https://github.com/mpenkov)
* [#2931](https://github.com/RaRe-Technologies/gensim/pull/2931): Clear up job queue parameters in word2vec, by [@lunastera](https://github.com/lunastera)
* [#2939](https://github.com/RaRe-Technologies/gensim/pull/2939): X2Vec SaveLoad improvements, by [@piskvorky](https://github.com/piskvorky)
* [#3060](https://github.com/RaRe-Technologies/gensim/pull/3060): Record lifecycle events in Gensim models, by [@piskvorky](https://github.com/piskvorky)
* [#3073](https://github.com/RaRe-Technologies/gensim/pull/3073): Make WMD normalization optional, by [@piskvorky](https://github.com/piskvorky)
* [#3065](https://github.com/RaRe-Technologies/gensim/pull/3065): Default to pickle protocol 4 when saving models, by [@piskvorky](https://github.com/piskvorky)
* [#3069](https://github.com/RaRe-Technologies/gensim/pull/3069): Add Github sponsor + donation nags, by [@piskvorky](https://github.com/piskvorky)

### :books: Tutorials and docs

* [#3082](https://github.com/RaRe-Technologies/gensim/pull/3082): Make LDA tutorial read NIPS data on the fly, by [@jonaschn](https://github.com/jonaschn)
* [#2954](https://github.com/RaRe-Technologies/gensim/pull/2954): New theme for the Gensin website, by [@dvorakvaclav](https://github.com/dvorakvaclav)
* [#2960](https://github.com/RaRe-Technologies/gensim/issues/2960): Added [Gensim and Compatibility](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility) Wiki page, by [@piskvorky](https://github.com/piskvorky)
* [#2960](https://github.com/RaRe-Technologies/gensim/issues/2960): Reworked & simplified the [Developer Wiki page](https://github.com/RaRe-Technologies/gensim/wiki/Developer-page), by [@piskvorky](https://github.com/piskvorky)
* [#2968](https://github.com/RaRe-Technologies/gensim/pull/2968): Migrate tutorials & how-tos to 4.0.0, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Clean up of language and formatting of docstrings, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Added documentation for NMSLIB indexer, by [@piskvorky](https://github.com/piskvorky)
* [#2832](https://github.com/RaRe-Technologies/gensim/pull/2832): Clear up LdaModel documentation, by [@FyzHsn](https://github.com/FyzHsn)
* [#2871](https://github.com/RaRe-Technologies/gensim/pull/2871): Clarify that license is LGPL-2.1, by [@pombredanne](https://github.com/pombredanne)
* [#2896](https://github.com/RaRe-Technologies/gensim/pull/2896): Make docs clearer on `alpha` parameter in LDA model, by [@xh2](https://github.com/xh2)
* [#2897](https://github.com/RaRe-Technologies/gensim/pull/2897): Update Hoffman paper link for Online LDA, by [@xh2](https://github.com/xh2)
* [#2910](https://github.com/RaRe-Technologies/gensim/pull/2910): Refresh docs for run_annoy tutorial, by [@piskvorky](https://github.com/piskvorky)
* [#2935](https://github.com/RaRe-Technologies/gensim/pull/2935): Fix "generator" language in word2vec docs, by [@polm](https://github.com/polm)
* [#3077](https://github.com/RaRe-Technologies/gensim/pull/3077): Fix various documentation warnings, by [@mpenkov](https://github.com/mpenkov)
* [#2991](https://github.com/RaRe-Technologies/gensim/pull/2991): Fix broken link in run_doc How-To, by [@sezanzeb](https://github.com/sezanzeb)
* [#3003](https://github.com/RaRe-Technologies/gensim/pull/3003): Point WordEmbeddingSimilarityIndex documentation to gensim.similarities, by [@Witiko](https://github.com/Witiko)
* [#2996](https://github.com/RaRe-Technologies/gensim/pull/2996): Make the website link to the old Gensim 3.8.3 documentation dynamic, by [@Witiko](https://github.com/Witiko)
* [#3063](https://github.com/RaRe-Technologies/gensim/pull/3063): Update link to papers in LSI model, by [@jonaschn](https://github.com/jonaschn)
* [#3080](https://github.com/RaRe-Technologies/gensim/pull/3080): Fix some of the warnings/deprecated functions, by [@FredHappyface](https://github.com/FredHappyface))

### :red_circle: Bug fixes

* [#2891](https://github.com/RaRe-Technologies/gensim/pull/2891): Fix fastText word-vectors with ngrams off, by [@gojomo](https://github.com/gojomo)
* [#2907](https://github.com/RaRe-Technologies/gensim/pull/2907): Fix doc2vec crash for large sets of doc-vectors, by [@gojomo](https://github.com/gojomo)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Fix similarity bug in NMSLIB indexer, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Fix deprecation warnings in Annoy integration, by [@piskvorky](https://github.com/piskvorky)
* [#2901](https://github.com/RaRe-Technologies/gensim/pull/2901): Fix inheritance of WikiCorpus from TextCorpus, by [@jenishah](https://github.com/jenishah)
* [#2940](https://github.com/RaRe-Technologies/gensim/pull/2940): Fix deprecations in SoftCosineSimilarity, by [@Witiko](https://github.com/Witiko)
* [#2944](https://github.com/RaRe-Technologies/gensim/pull/2944): Fix `save_facebook_model` failure after update-vocab & other initialization streamlining, by [@gojomo](https://github.com/gojomo)
* [#2846](https://github.com/RaRe-Technologies/gensim/pull/2846): Fix for Python 3.9/3.10: remove `xml.etree.cElementTree`, by [@hugovk](https://github.com/hugovk)
* [#2973](https://github.com/RaRe-Technologies/gensim/issues/2973): phrases.export_phrases() doesn't yield all bigrams, by [@piskvorky](https://github.com/piskvorky)
* [#2942](https://github.com/RaRe-Technologies/gensim/issues/2942): Segfault when training doc2vec, by [@gojomo](https://github.com/gojomo)
* [#3041](https://github.com/RaRe-Technologies/gensim/pull/3041): Fix RuntimeError in export_phrases (change defaultdict to dict), by [@thalishsajeed](https://github.com/thalishsajeed)
* [#3059](https://github.com/RaRe-Technologies/gensim/pull/3059): Fix race condition in FastText tests, by [@sleepy-owl](https://github.com/sleepy-owl)

### :warning: Removed functionality & deprecations

* Removed all code, methods, attributes and functions marked as deprecated in [Gensim 3.8.3](https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3).
* [#6](https://github.com/RaRe-Technologies/gensim-wheels/pull/6): No more binary wheels for x32 platforms, by [@menshikh-iv](https://github.com/menshikh-iv)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Renamed overly broad `similarities.index` to the more appropriate `similarities.annoy`, by [@piskvorky](https://github.com/piskvorky)
* [#2958](https://github.com/RaRe-Technologies/gensim/pull/2958): Remove gensim.summarization subpackage, docs and test data, by [@mpenkov](https://github.com/mpenkov)
* [#2926](https://github.com/RaRe-Technologies/gensim/pull/2926): Rename `num_words` to `topn` in dtm_coherence, by [@MeganStodel](https://github.com/MeganStodel)
* [#2937](https://github.com/RaRe-Technologies/gensim/pull/2937): Remove Keras dependency, by [@piskvorky](https://github.com/piskvorky)
* [#3078](https://github.com/RaRe-Technologies/gensim/pull/3078): Remove `on_batch_begin` and `on_batch_end` callbacks, by [@mpenkov](https://github.com/mpenkov)
* [#3012](https://github.com/RaRe-Technologies/gensim/pull/3012): Remove `pattern` dependency, by [@mpenkov](https://github.com/mpenkov)
* [#3055](https://github.com/RaRe-Technologies/gensim/pull/3055): Remove `gensim.viz` subpackage, by [@mpenkov](https://github.com/mpenkov)

### üîÆ Testing, CI, housekeeping

* [#2939](https://github.com/RaRe-Technologies/gensim/pull/2939) + [#2984](https://github.com/RaRe-Technologies/gensim/pull/2984): Code style & py3 migration clean up, by [@piskvorky](https://github.com/piskvorky)
* [#3058](https://github.com/RaRe-Technologies/gensim/pull/3058): Add py39 wheels to Travis/Azure, by [@FredHappyface](https://github.com/FredHappyface)
* [#3035](https://github.com/RaRe-Technologies/gensim/pull/3035): Update repos before trying to install gdb, by [@janaknat](https://github.com/janaknat)
* [#3026](https://github.com/RaRe-Technologies/gensim/pull/3026): Move x86 tests from Travis to GHA, add aarch64 wheel build to Travis, by [@janaknat](https://github.com/janaknat)
* [#3033](https://github.com/RaRe-Technologies/gensim/pull/3033): Transformed camelCase to snake_case test names, by [@sezanzeb](https://github.com/sezanzeb)
* [#3024](https://github.com/RaRe-Technologies/gensim/pull/3024): Add Github Actions x86 and mac jobs to build python wheels, by [@janaknat](https://github.com/janaknat)


## 4.0.0.rc1, 2021-03-19

**‚ö†Ô∏è Gensim 4.0 contains breaking API changes! See the [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to update your existing Gensim 3.x code and models.**

Gensim 4.0 is a major release with lots of performance & robustness improvements and a new website.

### Main highlights (see also *üëç Improvements* below)

* Massively optimized popular algorithms the community has grown to love: [fastText](https://radimrehurek.com/gensim/models/fasttext.html), [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html), [phrases](https://radimrehurek.com/gensim/models/phrases.html):

  a. **Efficiency**

    | model | 3.8.3: wall time / peak RAM / throughput | 4.0.0: wall time / peak RAM / throughput |
    |----------|------------|--------|
    | fastText | 2.9h / 4.11 GB / 822k words/s | 2.3h / **1.26 GB** / 914k words/s |
    | word2vec | 1.7h / 0.36 GB / 1685k words/s | **1.2h** / 0.33 GB / 1762k words/s |

    In other words, fastText now needs 3x less RAM (and is faster); word2vec has 2x faster init (and needs less RAM, and is faster); detecting collocation phrases is 2x faster. ([4.0 benchmarks](https://github.com/RaRe-Technologies/gensim/issues/2887#issuecomment-711097334))

  b. **Robustness**. We fixed a bunch of long-standing bugs by refactoring the internal code structure (see üî¥ Bug fixes below)

  c. **Simplified OOP model** for easier model exports and integration with TensorFlow, PyTorch &co.

  These improvements come to you transparently aka "for free", but see [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) for some changes that break the old Gensim 3.x API. **Update your code accordingly**.

* Dropped a bunch of externally contributed modules: summarization, pivoted TFIDF normalization, FIXME.
  - Code quality was not up to our standards. Also there was no one to maintain them, answer user questions, support these modules.

    So rather than let them rot, we took the hard decision of removing these contributed modules from Gensim. If anyone's interested in maintaining them please fork into your own repo, they can live happily outside of Gensim.

* Dropped Python 2. Gensim 4.0 is Py3.6+. Read our [Python version support policy](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility).
  - If you still need Python 2 for some reason, stay at [Gensim 3.8.3](https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3).

* A new [Gensim website](https://radimrehurek.com/gensim_4.0.0) ‚Äì¬†finally! üôÉ

So, a major clean-up release overall. We're happy with this **tighter, leaner and faster Gensim**.

This is the direction we'll keep going forward: less kitchen-sink of "latest academic algorithms", more focus on robust engineering, targetting common concrete NLP & document similarity use-cases.

### :star2: New Features

* Default to pickle protocol 4 when saving models (__[piskvorky](https://github.com/piskvorky)__, [#3065](https://github.com/RaRe-Technologies/gensim/pull/3065))
* Record lifecycle events in Gensim models (__[piskvorky](https://github.com/piskvorky)__, [#3060](https://github.com/RaRe-Technologies/gensim/pull/3060))
* Make WMD normalization optional (__[piskvorky](https://github.com/piskvorky)__, [#3073](https://github.com/RaRe-Technologies/gensim/pull/3073))

### :red_circle: Bug fixes

* fix RuntimeError in export_phrases (change defaultdict to dict) (__[thalishsajeed](https://github.com/thalishsajeed)__, [#3041](https://github.com/RaRe-Technologies/gensim/pull/3041))

### :books: Tutorial and doc improvements

* fix various documentation warnings (__[mpenkov](https://github.com/mpenkov)__, [#3077](https://github.com/RaRe-Technologies/gensim/pull/3077))
* Fix broken link in run_doc how-to (__[sezanzeb](https://github.com/sezanzeb)__, [#2991](https://github.com/RaRe-Technologies/gensim/pull/2991))
* Point WordEmbeddingSimilarityIndex documentation to gensim.similarities (__[Witiko](https://github.com/Witiko)__, [#3003](https://github.com/RaRe-Technologies/gensim/pull/3003))
* Make the link to the Gensim 3.8.3 documentation dynamic (__[Witiko](https://github.com/Witiko)__, [#2996](https://github.com/RaRe-Technologies/gensim/pull/2996))

### :warning: Removed functionality

* remove on_batch_begin and on_batch_end callbacks (__[mpenkov](https://github.com/mpenkov)__, [#3078](https://github.com/RaRe-Technologies/gensim/pull/3078))
* remove pattern dependency (__[mpenkov](https://github.com/mpenkov)__, [#3012](https://github.com/RaRe-Technologies/gensim/pull/3012))
* rm gensim.viz submodule (__[mpenkov](https://github.com/mpenkov)__, [#3055](https://github.com/RaRe-Technologies/gensim/pull/3055))

### üîÆ Miscellaneous

* [MRG] Add Github sponsor + donation nags (__[piskvorky](https://github.com/piskvorky)__, [#3069](https://github.com/RaRe-Technologies/gensim/pull/3069))
* Update URLs (__[jonaschn](https://github.com/jonaschn)__, [#3063](https://github.com/RaRe-Technologies/gensim/pull/3063))
* Fix race condition in FastText tests (__[sleepy-owl](https://github.com/sleepy-owl)__, [#3059](https://github.com/RaRe-Technologies/gensim/pull/3059))
* Add py39 wheels to travis/azure (__[FredHappyface](https://github.com/FredHappyface)__, [#3058](https://github.com/RaRe-Technologies/gensim/pull/3058))
* Update repos before trying to install gdb (__[janaknat](https://github.com/janaknat)__, [#3035](https://github.com/RaRe-Technologies/gensim/pull/3035))
* transformed camelCase to snake_case test names (__[sezanzeb](https://github.com/sezanzeb)__, [#3033](https://github.com/RaRe-Technologies/gensim/pull/3033))
* move x86 tests from Travis to GHA, add aarch64 wheel build to Travis (__[janaknat](https://github.com/janaknat)__, [#3026](https://github.com/RaRe-Technologies/gensim/pull/3026))
* Add Github Actions x86 and mac jobs to build python wheels (__[janaknat](https://github.com/janaknat)__, [#3024](https://github.com/RaRe-Technologies/gensim/pull/3024))


## 4.0.0beta, 2020-10-31

**‚ö†Ô∏è Gensim 4.0 contains breaking API changes! See the [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to update your existing Gensim 3.x code and models.**

Gensim 4.0 is a major release with lots of performance & robustness improvements and a new website.

### Main highlights (see also *üëç Improvements* below)

* Massively optimized popular algorithms the community has grown to love: [fastText](https://radimrehurek.com/gensim/models/fasttext.html), [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), [doc2vec](https://radimrehurek.com/gensim/models/doc2vec.html), [phrases](https://radimrehurek.com/gensim/models/phrases.html):

  a. **Efficiency**

    | model | 3.8.3: wall time / peak RAM / throughput | 4.0.0: wall time / peak RAM / throughput |
    |----------|------------|--------|
    | fastText | 2.9h / 4.11 GB / 822k words/s | 2.3h / **1.26 GB** / 914k words/s |
    | word2vec | 1.7h / 0.36 GB / 1685k words/s | **1.2h** / 0.33 GB / 1762k words/s |

    In other words, fastText now needs 3x less RAM (and is faster); word2vec has 2x faster init (and needs less RAM, and is faster); detecting collocation phrases is 2x faster. ([4.0 benchmarks](https://github.com/RaRe-Technologies/gensim/issues/2887#issuecomment-711097334))

  b. **Robustness**. We fixed a bunch of long-standing bugs by refactoring the internal code structure (see üî¥ Bug fixes below)

  c. **Simplified OOP model** for easier model exports and integration with TensorFlow, PyTorch &co.

  These improvements come to you transparently aka "for free", but see [Migration guide](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) for some changes that break the old Gensim 3.x API. **Update your code accordingly**.

* Dropped a bunch of externally contributed modules: summarization, pivoted TFIDF normalization, FIXME.
  - Code quality was not up to our standards. Also there was no one to maintain them, answer user questions, support these modules.

    So rather than let them rot, we took the hard decision of removing these contributed modules from Gensim. If anyone's interested in maintaining them please fork into your own repo, they can live happily outside of Gensim.

* Dropped Python 2. Gensim 4.0 is Py3.6+. Read our [Python version support policy](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility).
  - If you still need Python 2 for some reason, stay at [Gensim 3.8.3](https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3).

* A new [Gensim website](https://radimrehurek.com/gensim_4.0.0) ‚Äì¬†finally! üôÉ

So, a major clean-up release overall. We're happy with this **tighter, leaner and faster Gensim**.

This is the direction we'll keep going forward: less kitchen-sink of "latest academic algorithms", more focus on robust engineering, targetting common concrete NLP & document similarity use-cases.

### Why pre-release?

This 4.0.0beta pre-release is for users who want the **cutting edge performance and bug fixes**. Plus users who want to help out, by **testing and providing feedback**: code, documentation, workflows‚Ä¶ Please let us know on the [mailing list](https://groups.google.com/forum/#!forum/gensim)!

Install the pre-release with:

```bash
pip install --pre --upgrade gensim
```

### What will change between this pre-release and a "full" 4.0 release?

Production stability is important to Gensim, so we're improving the process of **upgrading already-trained saved models**. There'll be an explicit model upgrade script between each `4.n` to `4.(n+1)` Gensim release. Check progress [here](https://github.com/RaRe-Technologies/gensim/milestone/3).


### :+1: Improvements

* [#2947](https://github.com/RaRe-Technologies/gensim/pull/2947): Bump minimum Python version to 3.6, by [@gojomo](https://github.com/gojomo)
* [#2939](https://github.com/RaRe-Technologies/gensim/pull/2939) + [#2984](https://github.com/RaRe-Technologies/gensim/pull/2984): Code style & py3 migration clean up, by [@piskvorky](https://github.com/piskvorky)
* [#2300](https://github.com/RaRe-Technologies/gensim/pull/2300): Use less RAM in LdaMulticore, by [@horpto](https://github.com/horpto)
* [#2698](https://github.com/RaRe-Technologies/gensim/pull/2698): Streamline KeyedVectors & X2Vec API, by [@gojomo](https://github.com/gojomo)
* [#2864](https://github.com/RaRe-Technologies/gensim/pull/2864): Speed up random number generation in word2vec, by [@zygm0nt](https://github.com/zygm0nt)
* [#2976](https://github.com/RaRe-Technologies/gensim/pull/2976): Speed up phrase (collocation) detection, by [@piskvorky](https://github.com/piskvorky)
* [#2979](https://github.com/RaRe-Technologies/gensim/pull/2979): Allow skipping common English words in multi-word phrases, by [@piskvorky](https://github.com/piskvorky)
* [#2867](https://github.com/RaRe-Technologies/gensim/pull/2867): Expose `max_final_vocab` parameter in fastText constructor, by [@mpenkov](https://github.com/mpenkov)
* [#2931](https://github.com/RaRe-Technologies/gensim/pull/2931): Clear up job queue parameters in word2vec, by [@lunastera](https://github.com/lunastera)
* [#2939](https://github.com/RaRe-Technologies/gensim/pull/2939): X2Vec SaveLoad improvements, by [@piskvorky](https://github.com/piskvorky)

### :books: Tutorials and docs

* [#2954](https://github.com/RaRe-Technologies/gensim/pull/2954): New theme for the Gensin website, [@dvorakvaclav](https://github.com/dvorakvaclav)
* [#2960](https://github.com/RaRe-Technologies/gensim/issues/2960): Added [Gensim and Compatibility](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility) Wiki page, by [@piskvorky](https://github.com/piskvorky)
* [#2960](https://github.com/RaRe-Technologies/gensim/issues/2960): Reworked & simplified the [Developer Wiki page](https://github.com/RaRe-Technologies/gensim/wiki/Developer-page), by [@piskvorky](https://github.com/piskvorky)
* [#2968](https://github.com/RaRe-Technologies/gensim/pull/2968): Migrate tutorials & how-tos to 4.0.0, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Clean up of language and formatting of docstrings, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Added documentation for NMSLIB indexer, by [@piskvorky](https://github.com/piskvorky)
* [#2832](https://github.com/RaRe-Technologies/gensim/pull/2832): Clear up LdaModel documentation by [@FyzHsn](https://github.com/FyzHsn)
* [#2871](https://github.com/RaRe-Technologies/gensim/pull/2871): Clarify that license is LGPL-2.1, by [@pombredanne](https://github.com/pombredanne)
* [#2896](https://github.com/RaRe-Technologies/gensim/pull/2896): Make docs clearer on `alpha` parameter in LDA model, by [@xh2](https://github.com/xh2)
* [#2897](https://github.com/RaRe-Technologies/gensim/pull/2897): Update Hoffman paper link for Online LDA, by [@xh2](https://github.com/xh2)
* [#2910](https://github.com/RaRe-Technologies/gensim/pull/2910): Refresh docs for run_annoy tutorial, by [@piskvorky](https://github.com/piskvorky)
* [#2935](https://github.com/RaRe-Technologies/gensim/pull/2935): Fix "generator" language in word2vec docs, by [@polm](https://github.com/polm)

### :red_circle: Bug fixes

* [#2891](https://github.com/RaRe-Technologies/gensim/pull/2891): Fix fastText word-vectors with ngrams off, by [@gojomo](https://github.com/gojomo)
* [#2907](https://github.com/RaRe-Technologies/gensim/pull/2907): Fix doc2vec crash for large sets of doc-vectors, by [@gojomo](https://github.com/gojomo)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Fix similarity bug in NMSLIB indexer, by [@piskvorky](https://github.com/piskvorky)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Fix deprecation warnings in Annoy integration, by [@piskvorky](https://github.com/piskvorky)
* [#2901](https://github.com/RaRe-Technologies/gensim/pull/2901): Fix inheritance of WikiCorpus from TextCorpus, by [@jenishah](https://github.com/jenishah)
* [#2940](https://github.com/RaRe-Technologies/gensim/pull/2940); Fix deprecations in SoftCosineSimilarity, by [@Witiko](https://github.com/Witiko)
* [#2944](https://github.com/RaRe-Technologies/gensim/pull/2944): Fix `save_facebook_model` failure after update-vocab & other initialization streamlining, by [@gojomo](https://github.com/gojomo)
* [#2846](https://github.com/RaRe-Technologies/gensim/pull/2846): Fix for Python 3.9/3.10: remove `xml.etree.cElementTree`, by [@hugovk](https://github.com/hugovk)
* [#2973](https://github.com/RaRe-Technologies/gensim/issues/2973): phrases.export_phrases() doesn't yield all bigrams
* [#2942](https://github.com/RaRe-Technologies/gensim/issues/2942): Segfault when training doc2vec

### :warning: Removed functionality & deprecations

* [#6](https://github.com/RaRe-Technologies/gensim-wheels/pull/6): No more binary wheels for x32 platforms, by [menshikh-iv](https://github.com/menshikh-iv)
* [#2899](https://github.com/RaRe-Technologies/gensim/pull/2899): Renamed overly broad `similarities.index` to the more appropriate `similarities.annoy`, by [@piskvorky](https://github.com/piskvorky)
* [#2958](https://github.com/RaRe-Technologies/gensim/pull/2958): Remove gensim.summarization subpackage, docs and test data, by [@mpenkov](https://github.com/mpenkov)
* [#2926](https://github.com/RaRe-Technologies/gensim/pull/2926): Rename `num_words` to `topn` in dtm_coherence, by [@MeganStodel](https://github.com/MeganStodel)
* [#2937](https://github.com/RaRe-Technologies/gensim/pull/2937): Remove Keras dependency, by [@piskvorky](https://github.com/piskvorky)
* Removed all code, methods, attributes and functions marked as deprecated in [Gensim 3.8.3](https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3).
* Removed pattern dependency (PR [#3012](https://github.com/RaRe-Technologies/gensim/pull/3012), [@mpenkov](https://github.com/mpenkov)).  If you need to lemmatize, do it prior to passing the corpus to gensim.

---


## 3.8.3, 2020-05-03

**:warning: 3.8.x will be the last Gensim version to support Py2.7. Starting with 4.0.0, Gensim will only support Py3.5 and above.**

This is primarily a bugfix release to bring back Py2.7 compatibility to gensim 3.8.

### :red_circle: Bug fixes

* Bring back Py27 support (PR [#2812](https://github.com/RaRe-Technologies/gensim/pull/2812), [@mpenkov](https://github.com/mpenkov))
* Fix wrong version reported by setup.py (Issue [#2796](https://github.com/RaRe-Technologies/gensim/issues/2796))
* Fix missing C extensions (Issues [#2794](https://github.com/RaRe-Technologies/gensim/issues/2794) and [#2802](https://github.com/RaRe-Technologies/gensim/issues/2802))

### :+1: Improvements

* Wheels for Python 3.8 ([@menshikh-iv](https://github.com/menshikh-iv))
* Prepare for removal of deprecated `lxml.etree.cElementTree` (PR [#2777](https://github.com/RaRe-Technologies/gensim/pull/2777), [@tirkarthi](https://github.com/tirkarthi))

### :books: Tutorial and doc improvements

* Update test instructions in README (PR [#2814](https://github.com/RaRe-Technologies/gensim/pull/2814), [@piskvorky](https://github.com/piskvorky))

### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

---

## 3.8.2, 2020-04-10

### :red_circle: Bug fixes

* Pin `smart_open` version for compatibility with Py2.7

### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

---

## 3.8.1, 2019-09-23

### :red_circle: Bug fixes

* Fix usage of base_dir instead of BASE_DIR in _load_info in downloader. ([movb](https://github.com/movb), [#2605](https://github.com/RaRe-Technologies/gensim/pull/2605))
* Update the version of smart_open in the setup.py file ([AMR-KELEG](https://github.com/AMR-KELEG), [#2582](https://github.com/RaRe-Technologies/gensim/pull/2582))
* Properly handle unicode_errors arg parameter when loading a vocab file ([wmtzk](https://github.com/wmtzk), [#2570](https://github.com/RaRe-Technologies/gensim/pull/2570))
* Catch loading older TfidfModels without smartirs ([bnomis](https://github.com/bnomis), [#2559](https://github.com/RaRe-Technologies/gensim/pull/2559))
* Fix bug where a module import set up logging, pin doctools for Py2 ([piskvorky](https://github.com/piskvorky), [#2552](https://github.com/RaRe-Technologies/gensim/pull/2552))

### :books: Tutorial and doc improvements

* Fix usage example in phrases.py ([piskvorky](https://github.com/piskvorky), [#2575](https://github.com/RaRe-Technologies/gensim/pull/2575))

### :+1: Improvements

* Optimize Poincare model training ([koiizukag](https://github.com/koiizukag), [#2589](https://github.com/RaRe-Technologies/gensim/pull/2589))

### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

---

## 3.8.0, 2019-07-08

### :star2: New Features

* Enable online training of Poincare models ([koiizukag](https://github.com/koiizukag), [#2505](https://github.com/RaRe-Technologies/gensim/pull/2505))
* Make BM25 more scalable by adding support for generator inputs ([saraswatmks](https://github.com/saraswatmks), [#2479](https://github.com/RaRe-Technologies/gensim/pull/2479))
* Allow the Gensim dataset / pre-trained model downloader `gensim.downloader` to run offline, by introducing a local file cache ([mpenkov](https://github.com/mpenkov), [#2545](https://github.com/RaRe-Technologies/gensim/pull/2545))
* Make the `gensim.downloader` target directory configurable ([mpenkov](https://github.com/mpenkov), [#2456](https://github.com/RaRe-Technologies/gensim/pull/2456))
* Add `nmslib` indexer ([masa3141](https://github.com/masa3141), [#2417](https://github.com/RaRe-Technologies/gensim/pull/2417))

### :red_circle: Bug fixes

* Fix `smart_open` deprecation warning globally ([itayB](https://github.com/itayB), [#2530](https://github.com/RaRe-Technologies/gensim/pull/2530))
* Fix AppVeyor issues with Windows and Py2 ([mpenkov](https://github.com/mpenkov), [#2546](https://github.com/RaRe-Technologies/gensim/pull/2546))
* Fix `topn=0` versus `topn=None` bug in `most_similar`, accept `topn` of any integer type ([Witiko](https://github.com/Witiko), [#2497](https://github.com/RaRe-Technologies/gensim/pull/2497))
* Fix Python version check ([charsyam](https://github.com/charsyam), [#2547](https://github.com/RaRe-Technologies/gensim/pull/2547))
* Fix typo in FastText documentation ([Guitaricet](https://github.com/Guitaricet), [#2518](https://github.com/RaRe-Technologies/gensim/pull/2518))
* Fix "Market Matrix" to "Matrix Market" typo. ([Shooter23](https://github.com/Shooter23), [#2513](https://github.com/RaRe-Technologies/gensim/pull/2513))
* Fix auto-generated hyperlinks in `CHANGELOG.md` ([mpenkov](https://github.com/mpenkov), [#2482](https://github.com/RaRe-Technologies/gensim/pull/2482))

### :books: Tutorial and doc improvements

* Generate documentation for the `gensim.similarities.termsim` module ([Witiko](https://github.com/Witiko), [#2485](https://github.com/RaRe-Technologies/gensim/pull/2485))
* Simplify the `Support` section in README ([piskvorky](https://github.com/piskvorky), [#2542](https://github.com/RaRe-Technologies/gensim/pull/2542))

### :+1: Improvements

* Pin sklearn version for Py2, because sklearn dropped py2 support ([mpenkov](https://github.com/mpenkov), [#2510](https://github.com/RaRe-Technologies/gensim/pull/2510))


### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

## 3.7.3, 2019-05-06

### :red_circle: Bug fixes

* Fix fasttext model loading from gzip files ([mpenkov](https://github.com/mpenkov), [#2476](https://github.com/RaRe-Technologies/gensim/pull/2476))
* Fix misleading `Doc2Vec.docvecs` comment ([gojomo](https://github.com/gojomo), [#2472](https://github.com/RaRe-Technologies/gensim/pull/2472))
* NMF bugfix ([mpenkov](https://github.com/mpenkov), [#2466](https://github.com/RaRe-Technologies/gensim/pull/2466))
* Fix `WordEmbeddingsKeyedVectors.most_similar` ([Witiko](https://github.com/Witiko), [#2461](https://github.com/RaRe-Technologies/gensim/pull/2461))
* Fix LdaSequence model by updating to num_documents ([Bharat123rox](https://github.com/Bharat123rox), [#2410](https://github.com/RaRe-Technologies/gensim/pull/2410))
* Make termsim matrix positive definite even with negative similarities ([Witiko](https://github.com/Witiko), [#2397](https://github.com/RaRe-Technologies/gensim/pull/2397))
* Fix the off-by-one bug in the TFIDF model. ([AMR-KELEG](https://github.com/AMR-KELEG), [#2392](https://github.com/RaRe-Technologies/gensim/pull/2392))
* Update legacy model loading ([mpenkov](https://github.com/mpenkov), [#2454](https://github.com/RaRe-Technologies/gensim/pull/2454), [#2457](https://github.com/RaRe-Technologies/gensim/pull/2457))
* Make `matutils.unitvec` always return float norm when requested ([Witiko](https://github.com/Witiko), [#2419](https://github.com/RaRe-Technologies/gensim/pull/2419))

### :books: Tutorial and doc improvements

* Update word2vec.ipynb ([asyabo](https://github.com/asyabo), [#2423](https://github.com/RaRe-Technologies/gensim/pull/2423))

### :+1: Improvements

* Adding type check for corpus_file argument ([saraswatmks](https://github.com/saraswatmks), [#2469](https://github.com/RaRe-Technologies/gensim/pull/2469))
* Clean up FastText Cython code, fix division by zero ([mpenkov](https://github.com/mpenkov), [#2382](https://github.com/RaRe-Technologies/gensim/pull/2382))

### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

## 3.7.2, 2019-04-06

### :star2: New Features

- `gensim.models.fasttext.load_facebook_model` function: load full model (slower, more CPU/memory intensive, supports training continuation)
- `gensim.models.fasttext.load_facebook_vectors` function: load embeddings only (faster, less CPU/memory usage, does not support training continuation)

### :red_circle: Bug fixes

* Fix unicode error when loading FastText vocabulary ([@mpenkov](https://github.com/mpenkov), [#2390](https://github.com/RaRe-Technologies/gensim/pull/2390))
* Avoid division by zero in fasttext_inner.pyx ([@mpenkov](https://github.com/mpenkov), [#2404](https://github.com/RaRe-Technologies/gensim/pull/2404))
* Avoid incorrect filename inference when loading model ([@mpenkov](https://github.com/mpenkov), [#2408](https://github.com/RaRe-Technologies/gensim/pull/2408))
* Handle invalid unicode when loading native FastText models ([@mpenkov](https://github.com/mpenkov), [#2411](https://github.com/RaRe-Technologies/gensim/pull/2411))
* Avoid divide by zero when calculating vectors for terms with no ngrams ([@mpenkov](https://github.com/mpenkov), [#2411](https://github.com/RaRe-Technologies/gensim/pull/2411))

### :books: Tutorial and doc improvements

* Add link to bindr ([rogueleaderr](https://github.com/rogueleaderr), [#2387](https://github.com/RaRe-Technologies/gensim/pull/2387))

### :+1: Improvements

* Undo the hash2index optimization ([mpenkov](https://github.com/mpenkov), [#2370](https://github.com/RaRe-Technologies/gensim/pull/2370))

### :warning: Changes in FastText behavior

#### Out-of-vocab word handling

To achieve consistency with the reference implementation from Facebook,
a `FastText` model will now always report any word, out-of-vocabulary or
not, as being in the model,  and always return some vector for any word
looked-up. Specifically:

1. `'any_word' in ft_model` will always return `True`.  Previously, it
returned `True` only if the full word was in the vocabulary. (To test if a
full word is in the known vocabulary, you can consult the `wv.vocab`
property: `'any_word' in ft_model.wv.vocab` will return `False` if the full
word wasn't learned during model training.)
2. `ft_model['any_word']` will always return a vector.  Previously, it
raised `KeyError` for OOV words when the model had no vectors
for **any** ngrams of the word.
3. If no ngrams from the term are present in the model,
or when no ngrams could be extracted from the term, a vector pointing
to the origin will be returned.  Previously, a vector of NaN (not a number)
was returned as a consequence of a divide-by-zero problem.
4. Models may use more more memory, or take longer for word-vector
lookup, especially after training on smaller corpuses where the previous
non-compliant behavior discarded some ngrams from consideration.

#### Loading models in Facebook .bin format

The `gensim.models.FastText.load_fasttext_format` function (deprecated) now loads the entire model contained in the .bin file, including the shallow neural network that enables training continuation.
Loading this NN requires more CPU and RAM than previously required.

Since this function is deprecated, consider using one of its alternatives (see below).

Furthermore, you must now pass the full path to the file to load, **including the file extension.**
Previously, if you specified a model path that ends with anything other than .bin, the code automatically appended .bin to the path before loading the model.
This behavior was [confusing](https://github.com/RaRe-Technologies/gensim/issues/2407), so we removed it.

### :warning: Deprecations (will be removed in the next major release)

Remove:

- `gensim.models.FastText.load_fasttext_format`: use load_facebook_vectors to load embeddings only (faster, less CPU/memory usage, does not support training continuation) and load_facebook_model to load full model (slower, more CPU/memory intensive, supports training continuation)

## 3.7.1, 2019-01-31

### :+1: Improvements

* NMF optimization & documentation ([@anotherbugmaster](https://github.com/anotherbugmaster), [#2361](https://github.com/RaRe-Technologies/gensim/pull/2361))
* Optimize `FastText.load_fasttext_model` ([@mpenkov](https://github.com/mpenkov), [#2340](https://github.com/RaRe-Technologies/gensim/pull/2340))
* Add warning when string is used as argument to `Doc2Vec.infer_vector` ([@tobycheese](https://github.com/tobycheese), [#2347](https://github.com/RaRe-Technologies/gensim/pull/2347))
* Fix light linting issues in `LdaSeqModel` ([@horpto](https://github.com/horpto), [#2360](https://github.com/RaRe-Technologies/gensim/pull/2360))
* Move out `process_result_queue` from cycle in `LdaMulticore` ([@horpto](https://github.com/horpto), [#2358](https://github.com/RaRe-Technologies/gensim/pull/2358))


### :red_circle: Bug fixes

* Fix infinite diff in `LdaModel.do_mstep` ([@horpto](https://github.com/horpto), [#2344](https://github.com/RaRe-Technologies/gensim/pull/2344))
* Fix backward compatibility issue: loading `FastTextKeyedVectors` using `KeyedVectors` (missing attribute `compatible_hash`) ([@menshikh-iv](https://github.com/menshikh-iv), [#2349](https://github.com/RaRe-Technologies/gensim/pull/2349))
* Fix logging issue (conda-forge related) ([@menshikh-iv](https://github.com/menshikh-iv), [#2339](https://github.com/RaRe-Technologies/gensim/pull/2339))
* Fix `WordEmbeddingsKeyedVectors.most_similar` ([@Witiko](https://github.com/Witiko), [#2356](https://github.com/RaRe-Technologies/gensim/pull/2356))
* Fix issues of `flake8==3.7.1` ([@horpto](https://github.com/horpto), [#2365](https://github.com/RaRe-Technologies/gensim/pull/2365))


### :books: Tutorial and doc improvements

* Improve `FastText` documentation ([@mpenkov](https://github.com/mpenkov), [#2353](https://github.com/RaRe-Technologies/gensim/pull/2353))
* Minor corrections and improvements in `Any*Vec` docstrings ([@tobycheese](https://github.com/tobycheese), [#2345](https://github.com/RaRe-Technologies/gensim/pull/2345))
* Fix the example code for SparseTermSimilarityMatrix ([@Witiko](https://github.com/Witiko), [#2359](https://github.com/RaRe-Technologies/gensim/pull/2359))
* Update `poincare` documentation to indicate the relation format ([@AMR-KELEG](https://github.com/AMR-KELEG), [#2357](https://github.com/RaRe-Technologies/gensim/pull/2357))


### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.7.0, 2019-01-18

### :star2: New features

* Fast Online NMF ([@anotherbugmaster](https://github.com/anotherbugmaster), [#2007](https://github.com/RaRe-Technologies/gensim/pull/2007))
    - Benchmark `wiki-english-20171001`

      | Model | Perplexity | Coherence | L2 norm | Train time (minutes) |
      |-------|------------|-----------|---------|----------------------|
      | LDA | 4727.07 | -2.514 | 7.372 | 138 |
      | NMF | **975.74** | -2.814 | **7.265** | **73** |
      | NMF (with regularization) | 985.57 | **-2.436** | 7.269 | 441 |

    - Simple to use (same interface as `LdaModel`)
      ```python
      from gensim.models.nmf import Nmf
      from gensim.corpora import Dictionary
      import gensim.downloader as api

      text8 = api.load('text8')

      dictionary = Dictionary(text8)
      dictionary.filter_extremes()

      corpus = [
          dictionary.doc2bow(doc) for doc in text8
      ]

      nmf = Nmf(
          corpus=corpus,
          num_topics=5,
          id2word=dictionary,
          chunksize=2000,
          passes=5,
          random_state=42,
      )

      nmf.show_topics()
      """
      [(0, '0.007*"km" + 0.006*"est" + 0.006*"islands" + 0.004*"league" + 0.004*"rate" + 0.004*"female" + 0.004*"economy" + 0.003*"male" + 0.003*"team" + 0.003*"elections"'),
       (1, '0.006*"actor" + 0.006*"player" + 0.004*"bwv" + 0.004*"writer" + 0.004*"actress" + 0.004*"singer" + 0.003*"emperor" + 0.003*"jewish" + 0.003*"italian" + 0.003*"prize"'),
       (2, '0.036*"college" + 0.007*"institute" + 0.004*"jewish" + 0.004*"universidad" + 0.003*"engineering" + 0.003*"colleges" + 0.003*"connecticut" + 0.003*"technical" + 0.003*"jews" + 0.003*"universities"'),
       (3, '0.016*"import" + 0.008*"insubstantial" + 0.007*"y" + 0.006*"soviet" + 0.004*"energy" + 0.004*"info" + 0.003*"duplicate" + 0.003*"function" + 0.003*"z" + 0.003*"jargon"'),
       (4, '0.005*"software" + 0.004*"games" + 0.004*"windows" + 0.003*"microsoft" + 0.003*"films" + 0.003*"apple" + 0.003*"video" + 0.002*"album" + 0.002*"fiction" + 0.002*"characters"')]
      """
      ```
    - See also:
      - [NMF tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/nmf_tutorial.ipynb)
      - [Full NMF Benchmark](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/nmf_wikipedia.ipynb)

* Massive improvement`FastText` compatibilities ([@mpenkov](https://github.com/mpenkov), [#2313](https://github.com/RaRe-Technologies/gensim/pull/2313))
    ```python
    from gensim.models import FastText

    # 'cc.ru.300.bin' - Russian Facebook FT model trained on Common Crawl
    # Can be downloaded from https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.ru.300.bin.gz

    model = FastText.load_fasttext_format("cc.ru.300.bin")

    # Fixed hash-function allow to produce same output as FB FastText & works correctly for non-latin languages (for example, Russian)
    assert "–º—è—É" in m.wv.vocab  # '–º—è—É' - vocab word
    model.wv.most_similar("–º—è—É")
    """
    [('–ú—è—É', 0.6820122003555298),
     ('–ú–Ø–£', 0.6373013257980347),
     ('–º—è—É-–º—è—É', 0.593108594417572),
     ('–∫–∏—Å-–∫–∏—Å', 0.5899622440338135),
     ('–≥–∞–≤', 0.5866007804870605),
     ('–ö–∏—Å-–∫–∏—Å', 0.5798211097717285),
     ('–ö–∏—Å-–∫–∏—Å-–∫–∏—Å', 0.5742273330688477),
     ('–ú—è—É-–º—è—É', 0.5699705481529236),
     ('—Ö—Ä—é-—Ö—Ä—é', 0.5508339405059814),
     ('–∞–≤-–∞–≤', 0.5479759573936462)]
    """

    assert "–∫–æ—Ç–æ–≥–æ—Ä–æ–¥" not in m.wv.vocab  # '–∫–æ—Ç–æ–≥–æ—Ä–æ–¥' - out-of-vocab word
    model.wv.most_similar("–∫–æ—Ç–æ–≥–æ—Ä–æ–¥", topn=3)
    """
    [('–∞–≤—Ç–æ–≥–æ—Ä–æ–¥', 0.5463314652442932),
     ('–¢–∞–≥–∏–ª–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫–ù–æ–≤–æ–º–æ—Å–∫–æ–≤—Å–∫–ù–æ–≤–æ—Ä–æ—Å—Å–∏–π—Å–∫–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–ù–æ–≤–æ—Ç—Ä–æ–∏—Ü–∫–ù–æ–≤–æ—á–µ—Ä–∫–∞—Å—Å–∫–ù–æ–≤–æ—à–∞—Ö—Ç–∏–Ω—Å–∫–ù–æ–≤—ã–π',
      0.5423436164855957),
     ('–æ–±–ª–∞—Å—Ç—å–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–ë–∞—Ä–∞–±–∏–Ω—Å–∫–ë–µ—Ä–¥—Å–∫–ë–æ–ª–æ—Ç–Ω–æ–µ–ò—Å–∫–∏—Ç–∏–º–ö–∞—Ä–∞—Å—É–∫–ö–∞—Ä–≥–∞—Ç–ö—É–π–±—ã—à–µ–≤–ö—É–ø–∏–Ω–æ–û–±—å–¢–∞—Ç–∞—Ä—Å–∫–¢–æ–≥—É—á–∏–Ω–ß–µ—Ä–µ–ø–∞–Ω–æ–≤–æ',
      0.5377570390701294)]
    """

    # Now we load full model, for this reason, we can continue an training

    from gensim.test.utils import datapath
    from smart_open import smart_open

    with smart_open(datapath("crime-and-punishment.txt"), encoding="utf-8") as infile:  # russian text
        corpus = [line.strip().split() for line in infile]

    model.train(corpus, total_examples=len(corpus), epochs=5)
    ```

* Similarity search improvements ([@Witiko](https://github.com/Witiko), [#2016](https://github.com/RaRe-Technologies/gensim/pull/2016))
    - Add similarity search using the Levenshtein distance in `gensim.similarities.LevenshteinSimilarityIndex`
    - Performance optimizations to `gensim.similarities.SoftCosineSimilarity` ([full benchmark](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_benchmark.ipynb))

      | dictionary size | corpus size | speed         |
      |-----------------|-------------|--------------:|
      | 1000            | 100         | 1.0√ó          |
      | 1000            | 1000        | **53.4√ó**     |
      | 1000            | 100000      | **156784.8√ó** |
      | 100000          | 100         | **3.8√ó**      |
      | 100000          | 1000        | **405.8√ó**    |
      | 100000          | 100000      | **66262.0√ó**  |

    - See [updated soft-cosine tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb) for more information and usage examples

* Add `python3.7` support ([@menshikh-iv](https://github.com/menshikh-iv), [#2211](https://github.com/RaRe-Technologies/gensim/pull/2211))
    - Wheels for Window, OSX and Linux platforms ([@menshikh-iv](https://github.com/menshikh-iv), [MacPython/gensim-wheels/#12](https://github.com/MacPython/gensim-wheels/pull/12))
    - Faster installation


### :+1: Improvements

##### Optimizations
* Reduce `Phraser` memory usage (drop frequencies) ([@jenishah](https://github.com/jenishah), [#2208](https://github.com/RaRe-Technologies/gensim/pull/2208))
* Reduce memory consumption of summarizer ([@horpto](https://github.com/horpto), [#2298](https://github.com/RaRe-Technologies/gensim/pull/2298))
* Replace inline slow equivalent of mean_absolute_difference with fast ([@horpto](https://github.com/horpto), [#2284](https://github.com/RaRe-Technologies/gensim/pull/2284))
* Reuse precalculated updated prior in `ldamodel.update_dir_prior` ([@horpto](https://github.com/horpto), [#2274](https://github.com/RaRe-Technologies/gensim/pull/2274))
* Improve `KeyedVector.wmdistance` ([@horpto](https://github.com/horpto), [#2326](https://github.com/RaRe-Technologies/gensim/pull/2326))
* Optimize `remove_unreachable_nodes` in `gensim.summarization` ([@horpto](https://github.com/horpto), [#2263](https://github.com/RaRe-Technologies/gensim/pull/2263))
* Optimize `mz_entropy` from `gensim.summarization` ([@horpto](https://github.com/horpto), [#2267](https://github.com/RaRe-Technologies/gensim/pull/2267))
* Improve `filter_extremes` methods in `Dictionary` and `HashDictionary` ([@horpto](https://github.com/horpto), [#2303](https://github.com/RaRe-Technologies/gensim/pull/2303))

##### Additions
* Add `KeyedVectors.relative_cosine_similarity` ([@rsdel2007](https://github.com/rsdel2007), [#2307](https://github.com/RaRe-Technologies/gensim/pull/2307))
* Add `random_seed` to `LdaMallet` ([@Zohaggie](https://github.com/Zohaggie) & [@menshikh-iv](https://github.com/menshikh-iv), [#2153](https://github.com/RaRe-Technologies/gensim/pull/2153))
* Add `common_terms` parameter to `sklearn_api.PhrasesTransformer` ([@pmlk](https://github.com/pmlk), [#2074](https://github.com/RaRe-Technologies/gensim/pull/2074))
* Add method for patch `corpora.Dictionary` based on special tokens ([@Froskekongen](https://github.com/Froskekongen), [#2200](https://github.com/RaRe-Technologies/gensim/pull/2200))

##### Cleanup
* Improve `six` usage (`xrange`, `map`, `zip`) ([@horpto](https://github.com/horpto), [#2264](https://github.com/RaRe-Technologies/gensim/pull/2264))
* Refactor `line2doc` methods of `LowCorpus` and `MalletCorpus` ([@horpto](https://github.com/horpto), [#2269](https://github.com/RaRe-Technologies/gensim/pull/2269))
* Get rid most of warnings in testing ([@menshikh-iv](https://github.com/menshikh-iv), [#2191](https://github.com/RaRe-Technologies/gensim/pull/2191))
* Fix non-deterministic test failures (pin `PYTHONHASHSEED`) ([@menshikh-iv](https://github.com/menshikh-iv), [#2196](https://github.com/RaRe-Technologies/gensim/pull/2196))
* Fix "aliasing chunkize to chunkize_serial" warning on Windows ([@aquatiko](https://github.com/aquatiko), [#2202](https://github.com/RaRe-Technologies/gensim/pull/2202))
* Remove `getitem` code duplication in `gensim.models.phrases` ([@jenishah](https://github.com/jenishah), [#2206](https://github.com/RaRe-Technologies/gensim/pull/2206))
* Add `flake8-rst` for docstring code examples ([@kataev](https://github.com/kataev), [#2192](https://github.com/RaRe-Technologies/gensim/pull/2192))
* Get rid `py26` stuff ([@menshikh-iv](https://github.com/menshikh-iv), [#2214](https://github.com/RaRe-Technologies/gensim/pull/2214))
* Use `itertools.chain` instead of `sum` to concatenate lists ([@Stigjb](https://github.com/Stigjb), [#2212](https://github.com/RaRe-Technologies/gensim/pull/2212))
* Fix flake8 warnings W605, W504 ([@horpto](https://github.com/horpto), [#2256](https://github.com/RaRe-Technologies/gensim/pull/2256))
* Remove unnecessary creations of lists at all ([@horpto](https://github.com/horpto), [#2261](https://github.com/RaRe-Technologies/gensim/pull/2261))
* Fix extra list creation in `utils.get_max_id` ([@horpto](https://github.com/horpto), [#2254](https://github.com/RaRe-Technologies/gensim/pull/2254))
* Fix deprecation warning `np.sum(generator)` ([@rsdel2007](https://github.com/rsdel2007), [#2296](https://github.com/RaRe-Technologies/gensim/pull/2296))
* Refactor `BM25` ([@horpto](https://github.com/horpto), [#2275](https://github.com/RaRe-Technologies/gensim/pull/2275))
* Fix pyemd import ([@ramprakash-94](https://github.com/ramprakash-94), [#2240](https://github.com/RaRe-Technologies/gensim/pull/2240))
* Set `metadata=True` for `make_wikicorpus` script by default ([@Xinyi2016](https://github.com/Xinyi2016), [#2245](https://github.com/RaRe-Technologies/gensim/pull/2245))
* Remove unimportant warning from `Phrases` ([@rsdel2007](https://github.com/rsdel2007), [#2331](https://github.com/RaRe-Technologies/gensim/pull/2331))
* Replace `open()` by `smart_open()` in `gensim.models.fasttext._load_fasttext_format` ([@rsdel2007](https://github.com/rsdel2007), [#2335](https://github.com/RaRe-Technologies/gensim/pull/2335))


### :red_circle: Bug fixes
* Fix overflow error for `*Vec` corpusfile-based training ([@bm371613](https://github.com/bm371613), [#2239](https://github.com/RaRe-Technologies/gensim/pull/2239))
* Fix `malletmodel2ldamodel` conversion ([@horpto](https://github.com/horpto), [#2288](https://github.com/RaRe-Technologies/gensim/pull/2288))
* Replace custom epsilons with numpy equivalent in `LdaModel` ([@horpto](https://github.com/horpto), [#2308](https://github.com/RaRe-Technologies/gensim/pull/2308))
* Add missing content to tarball ([@menshikh-iv](https://github.com/menshikh-iv), [#2194](https://github.com/RaRe-Technologies/gensim/pull/2194))
* Fixes divided by zero when w_star_count==0 ([@allenyllee](https://github.com/allenyllee), [#2259](https://github.com/RaRe-Technologies/gensim/pull/2259))
* Fix check for callbacks ([@allenyllee](https://github.com/allenyllee), [#2251](https://github.com/RaRe-Technologies/gensim/pull/2251))
* Fix `SvmLightCorpus.serialize` if `labels` instance of numpy.ndarray ([@aquatiko](https://github.com/aquatiko), [#2243](https://github.com/RaRe-Technologies/gensim/pull/2243))
* Fix poincate viz incompatibility with `plotly>=3.0.0` ([@jenishah](https://github.com/jenishah), [#2226](https://github.com/RaRe-Technologies/gensim/pull/2226))
* Fix `keep_n` behavior for `Dictionary.filter_extremes` ([@johann-petrak](https://github.com/johann-petrak), [#2232](https://github.com/RaRe-Technologies/gensim/pull/2232))
* Fix for `sphinx==1.8.1` (last r ([@menshikh-iv](https://github.com/menshikh-iv), [#None](https://github.com/RaRe-Technologies/gensim/pull/None))
* Fix `np.issubdtype` warnings ([@marioyc](https://github.com/marioyc), [#2210](https://github.com/RaRe-Technologies/gensim/pull/2210))
* Drop wrong key `-c` from `gensim.downloader` description ([@horpto](https://github.com/horpto), [#2262](https://github.com/RaRe-Technologies/gensim/pull/2262))
* Fix gensim build (docs & pyemd issues) ([@menshikh-iv](https://github.com/menshikh-iv), [#2318](https://github.com/RaRe-Technologies/gensim/pull/2318))
* Limit visdom version (avoid py2 issue from the latest visdom release) ([@menshikh-iv](https://github.com/menshikh-iv), [#2334](https://github.com/RaRe-Technologies/gensim/pull/2334))
* Fix visdom integration (using `viz.line()` instead of `viz.updatetrace()`) ([@allenyllee](https://github.com/allenyllee), [#2252](https://github.com/RaRe-Technologies/gensim/pull/2252))


### :books: Tutorial and doc improvements

* Add gensim-data repo to `gensim.downloader` & fix rendering of code examples ([@menshikh-iv](https://github.com/menshikh-iv), [#2327](https://github.com/RaRe-Technologies/gensim/pull/2327))
* Fix typos in `gensim.models` ([@rsdel2007](https://github.com/rsdel2007), [#2323](https://github.com/RaRe-Technologies/gensim/pull/2323))
* Fixed typos in notebooks ([@rsdel2007](https://github.com/rsdel2007), [#2322](https://github.com/RaRe-Technologies/gensim/pull/2322))
* Update `Doc2Vec` documentation: how tags are assigned in `corpus_file` mode ([@persiyanov](https://github.com/persiyanov), [#2320](https://github.com/RaRe-Technologies/gensim/pull/2320))
* Fix typos in `gensim/models/keyedvectors.py` ([@rsdel2007](https://github.com/rsdel2007), [#2290](https://github.com/RaRe-Technologies/gensim/pull/2290))
* Add documentation about ranges to scoring functions for `Phrases` ([@jenishah](https://github.com/jenishah), [#2242](https://github.com/RaRe-Technologies/gensim/pull/2242))
* Update return sections for `KeyedVectors.evaluate_word_*` ([@Stigjb](https://github.com/Stigjb), [#2205](https://github.com/RaRe-Technologies/gensim/pull/2205))
* Fix return type in `KeyedVector.evaluate_word_analogies` ([@Stigjb](https://github.com/Stigjb), [#2207](https://github.com/RaRe-Technologies/gensim/pull/2207))
* Fix `WmdSimilarity` documentation ([@jagmoreira](https://github.com/jagmoreira), [#2217](https://github.com/RaRe-Technologies/gensim/pull/2217))
* Replace `fify -> fifty` in `gensim.parsing.preprocessing.STOPWORDS` ([@coderwassananmol](https://github.com/coderwassananmol), [#2220](https://github.com/RaRe-Technologies/gensim/pull/2220))
* Remove `alpha="auto"` from `LdaMulticore` (not supported yet) ([@johann-petrak](https://github.com/johann-petrak), [#2225](https://github.com/RaRe-Technologies/gensim/pull/2225))
* Update Adopters in README ([@piskvorky](https://github.com/piskvorky), [#2234](https://github.com/RaRe-Technologies/gensim/pull/2234))
* Fix broken link in `tutorials.md` ([@rsdel2007](https://github.com/rsdel2007), [#2302](https://github.com/RaRe-Technologies/gensim/pull/2302))


### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.6.0, 2018-09-20

### :star2: New features
* File-based training for `*2Vec` models ([@persiyanov](https://github.com/persiyanov), [#2127](https://github.com/RaRe-Technologies/gensim/pull/2127) & [#2078](https://github.com/RaRe-Technologies/gensim/pull/2078) & [#2048](https://github.com/RaRe-Technologies/gensim/pull/2048))

  New training mode for `*2Vec` models (word2vec, doc2vec, fasttext) that allows model training to scale linearly with the number of cores (full GIL elimination). The result of our Google Summer of Code 2018 project by Dmitry Persiyanov.

  **Benchmark**
  - Dataset: `full English Wikipedia`
  - Cloud: `GCE`
  - CPU: `Intel(R) Xeon(R) CPU @ 2.30GHz 32 cores`
  - BLAS: `MKL`


  | Model | Queue-based version [sec] | File-based version [sec] | speed up | Accuracy (queue-based) | Accuracy (file-based) |
  |-------|------------|--------------------|----------|----------------|-----------------------|
  | Word2Vec | 9230 | **2437** | **3.79x** | 0.754 (¬± 0.003) | 0.750 (¬± 0.001) |
  | Doc2Vec | 18264 | **2889** | **6.32x** | 0.721 (¬± 0.002) | 0.683 (¬± 0.003) |
  | FastText | 16361 | **10625** | **1.54x** | 0.642 (¬± 0.002) | 0.660 (¬± 0.001) |

  Usage:

  ```python
  import gensim.downloader as api
  from multiprocessing import cpu_count
  from gensim.utils import save_as_line_sentence
  from gensim.test.utils import get_tmpfile
  from gensim.models import Word2Vec, Doc2Vec, FastText


  # Convert any corpus to the needed format: 1 document per line, words delimited by " "
  corpus = api.load("text8")
  corpus_fname = get_tmpfile("text8-file-sentence.txt")
  save_as_line_sentence(corpus, corpus_fname)

  # Choose num of cores that you want to use (let's use all, models scale linearly now!)
  num_cores = cpu_count()

  # Train models using all cores
  w2v_model = Word2Vec(corpus_file=corpus_fname, workers=num_cores)
  d2v_model = Doc2Vec(corpus_file=corpus_fname, workers=num_cores)
  ft_model = FastText(corpus_file=corpus_fname, workers=num_cores)

  ```
  [Read notebook tutorial with full description.](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Any2Vec_Filebased.ipynb)


### :+1: Improvements

* Add scikit-learn wrapper for `FastText` ([@mcemilg](https://github.com/mcemilg), [#2178](https://github.com/RaRe-Technologies/gensim/pull/2178))
* Add multiprocessing support for `BM25` ([@Shiki-H](https://github.com/Shiki-H), [#2146](https://github.com/RaRe-Technologies/gensim/pull/2146))
* Add `name_only` option for downloader api ([@aneesh-joshi](https://github.com/aneesh-joshi), [#2143](https://github.com/RaRe-Technologies/gensim/pull/2143))
* Make `word2vec2tensor` script compatible with `python3` ([@vsocrates](https://github.com/vsocrates), [#2147](https://github.com/RaRe-Technologies/gensim/pull/2147))
* Add custom filter for `Wikicorpus` ([@mattilyra](https://github.com/mattilyra), [#2089](https://github.com/RaRe-Technologies/gensim/pull/2089))
* Make `similarity_matrix` support non-contiguous dictionaries ([@Witiko](https://github.com/Witiko), [#2047](https://github.com/RaRe-Technologies/gensim/pull/2047))


### :red_circle: Bug fixes

* Fix memory consumption in `AuthorTopicModel` ([@philipphager](https://github.com/philipphager), [#2122](https://github.com/RaRe-Technologies/gensim/pull/2122))
* Correctly process empty documents in `AuthorTopicModel` ([@probinso](https://github.com/probinso), [#2133](https://github.com/RaRe-Technologies/gensim/pull/2133))
* Fix ZeroDivisionError `keywords` issue with short input ([@LShostenko](https://github.com/LShostenko), [#2154](https://github.com/RaRe-Technologies/gensim/pull/2154))
* Fix `min_count` handling in phrases detection using `npmi_scorer` ([@lopusz](https://github.com/lopusz), [#2072](https://github.com/RaRe-Technologies/gensim/pull/2072))
* Remove duplicate count from `Phraser` log message ([@robguinness](https://github.com/robguinness), [#2151](https://github.com/RaRe-Technologies/gensim/pull/2151))
* Replace `np.integer` -> `np.int` in `AuthorTopicModel` ([@menshikh-iv](https://github.com/menshikh-iv), [#2145](https://github.com/RaRe-Technologies/gensim/pull/2145))


### :books: Tutorial and doc improvements

* Update docstring with new analogy evaluation method ([@akutuzov](https://github.com/akutuzov), [#2130](https://github.com/RaRe-Technologies/gensim/pull/2130))
* Improve `prune_at` parameter description for `gensim.corpora.Dictionary` ([@yxonic](https://github.com/yxonic), [#2128](https://github.com/RaRe-Technologies/gensim/pull/2128))
* Fix `default` -> `auto` prior parameter in documentation for lda-related models ([@Laubeee](https://github.com/Laubeee), [#2156](https://github.com/RaRe-Technologies/gensim/pull/2156))
* Use heading instead of bold style in `gensim.models.translation_matrix` ([@nzw0301](https://github.com/nzw0301), [#2164](https://github.com/RaRe-Technologies/gensim/pull/2164))
* Fix quote of vocabulary from `gensim.models.Word2Vec` ([@nzw0301](https://github.com/nzw0301), [#2161](https://github.com/RaRe-Technologies/gensim/pull/2161))
* Replace deprecated parameters with new in docstring of `gensim.models.Doc2Vec` ([@xuhdev](https://github.com/xuhdev), [#2165](https://github.com/RaRe-Technologies/gensim/pull/2165))
* Fix formula in Mallet documentation ([@Laubeee](https://github.com/Laubeee), [#2186](https://github.com/RaRe-Technologies/gensim/pull/2186))
* Fix minor semantic issue in docs for `Phrases` ([@RunHorst](https://github.com/RunHorst), [#2148](https://github.com/RaRe-Technologies/gensim/pull/2148))
* Fix typo in documentation ([@KenjiOhtsuka](https://github.com/KenjiOhtsuka), [#2157](https://github.com/RaRe-Technologies/gensim/pull/2157))
* Additional documentation fixes ([@piskvorky](https://github.com/piskvorky), [#2121](https://github.com/RaRe-Technologies/gensim/pull/2121))

### :warning: Deprecations (will be removed in the next major release)

* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.5.0, 2018-07-06

This release comprises a glorious 38 pull requests from 28 contributors. Most of the effort went into improving the documentation‚Äîhence the release code name "Docs üí¨"!

Apart from the **massive overhaul of all Gensim documentation** (including docstring style and examples‚Äî[you asked for it](https://rare-technologies.com/gensim-survey-2018/)), we also managed to sneak in some new functionality and a number of bug fixes. As usual, see the notes below for a complete list, with links to pull requests for more details.

**Huge thanks to all contributors!** Nobody loves working on documentation. 3.5.0 is a result of several months of laborious, unglamorous, and sometimes invisible work. Enjoy!


### :books: Documentation improvements

* Overhaul documentation for `*2vec` models ([@steremma](https://github.com/steremma) & [@piskvorky](https://github.com/piskvorky) & [@menshikh-iv](https://github.com/menshikh-iv), [#1944](https://github.com/RaRe-Technologies/gensim/pull/1944), [#2087](https://github.com/RaRe-Technologies/gensim/pull/2087))
* Fix documentation for LDA-related models ([@steremma](https://github.com/steremma) & [@piskvorky](https://github.com/piskvorky) & [@menshikh-iv](https://github.com/menshikh-iv), [#2026](https://github.com/RaRe-Technologies/gensim/pull/2026))
* Fix documentation for utils, corpora, inferfaces ([@piskvorky](https://github.com/piskvorky) & [@menshikh-iv](https://github.com/menshikh-iv), [#2096](https://github.com/RaRe-Technologies/gensim/pull/2096))
* Update non-API docs (about, intro, license etc) ([@piskvorky](https://github.com/piskvorky) & [@menshikh-iv](https://github.com/menshikh-iv), [#2101](https://github.com/RaRe-Technologies/gensim/pull/2101))
* Refactor documentation for `gensim.models.phrases` ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1950](https://github.com/RaRe-Technologies/gensim/pull/1950))
* Fix HashDictionary documentation ([@piskvorky](https://github.com/piskvorky), [#2073](https://github.com/RaRe-Technologies/gensim/pull/2073))
* Fix docstrings for `gensim.models.AuthorTopicModel` ([@souravsingh](https://github.com/souravsingh) & [@menshikh-iv](https://github.com/menshikh-iv), [#1907](https://github.com/RaRe-Technologies/gensim/pull/1907))
* Fix docstrings for HdpModel, lda_worker & lda_dispatcher ([@gyanesh-m](https://github.com/gyanesh-m) & [@menshikh-iv](https://github.com/menshikh-iv), [#1912](https://github.com/RaRe-Technologies/gensim/pull/1912))
* Fix format & links for `gensim.similarities.docsim` ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#2030](https://github.com/RaRe-Technologies/gensim/pull/2030))
* Remove duplication of class documentation for `IndexedCorpus` ([@darindf](https://github.com/darindf), [#2033](https://github.com/RaRe-Technologies/gensim/pull/2033))
* Refactor documentation for `gensim.models.coherencemodel` ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1933](https://github.com/RaRe-Technologies/gensim/pull/1933))
* Fix docstrings for `gensim.sklearn_api` ([@steremma](https://github.com/steremma) & [@menshikh-iv](https://github.com/menshikh-iv), [#1895](https://github.com/RaRe-Technologies/gensim/pull/1895))
* Disable google-style docstring support ([@menshikh-iv](https://github.com/menshikh-iv), [#2106](https://github.com/RaRe-Technologies/gensim/pull/2106))
* Fix docstring of `gensim.models.KeyedVectors.similarity_matrix` ([@Witiko](https://github.com/Witiko), [#1971](https://github.com/RaRe-Technologies/gensim/pull/1971))
* Consistently use `smart_open()` instead of `open()` in notebooks ([@sharanry](https://github.com/sharanry), [#1812](https://github.com/RaRe-Technologies/gensim/pull/1812))


### :star2: New features:

* Add `add_entity` method to `KeyedVectors` to allow adding word vectors manually ([@persiyanov](https://github.com/persiyanov), [#1957](https://github.com/RaRe-Technologies/gensim/pull/1957))
* Add inference for new unseen author to `AuthorTopicModel` ([@Stamenov](https://github.com/Stamenov), [#1766](https://github.com/RaRe-Technologies/gensim/pull/1766))
* Add `evaluate_word_analogies` (will replace `accuracy`) method to `KeyedVectors` ([@akutuzov](https://github.com/akutuzov), [#1935](https://github.com/RaRe-Technologies/gensim/pull/1935))
* Add Pivot Normalization to `TfidfModel` ([@markroxor](https://github.com/markroxor), [#1780](https://github.com/RaRe-Technologies/gensim/pull/1780))



### :+1: Improvements

* Allow initialization with `max_final_vocab` in lieu of `min_count` in `Word2Vec`([@aneesh-joshi](https://github.com/aneesh-joshi), [#1915](https://github.com/RaRe-Technologies/gensim/pull/1915))
* Add `dtype` argument for `chunkize_serial` in `LdaModel` ([@darindf](https://github.com/darindf), [#2027](https://github.com/RaRe-Technologies/gensim/pull/2027))
* Increase performance in `Phrases.analyze_sentence` ([@JonathanHourany](https://github.com/JonathanHourany), [#2070](https://github.com/RaRe-Technologies/gensim/pull/2070))
* Add `ns_exponent` parameter to control the negative sampling distribution for `*2vec` models ([@fernandocamargoti](https://github.com/fernandocamargoti), [#2093](https://github.com/RaRe-Technologies/gensim/pull/2093))


### :red_circle: Bug fixes:


* Fix `Doc2Vec.infer_vector` + notebook cleanup ([@gojomo](https://github.com/gojomo), [#2103](https://github.com/RaRe-Technologies/gensim/pull/2103))
* Fix linear decay for learning rate in `Doc2Vec.infer_vector` ([@umangv](https://github.com/umangv), [#2063](https://github.com/RaRe-Technologies/gensim/pull/2063))
* Fix negative sampling floating-point error for `gensim.models.Poincare ([@jayantj](https://github.com/jayantj), [#1959](https://github.com/RaRe-Technologies/gensim/pull/1959))
* Fix loading `word2vec` and `doc2vec` models saved using old Gensim versions ([@manneshiva](https://github.com/manneshiva), [#2012](https://github.com/RaRe-Technologies/gensim/pull/2012))
* Fix `SoftCosineSimilarity.get_similarities` on corpora ssues/1955) ([@Witiko](https://github.com/Witiko), [#1972](https://github.com/RaRe-Technologies/gensim/pull/1972))
* Fix return dtype for `matutils.unitvec` according to input dtype ([@o-P-o](https://github.com/o-P-o), [#1992](https://github.com/RaRe-Technologies/gensim/pull/1992))
* Fix passing empty dictionary to `gensim.corpora.WikiCorpus` ([@steremma](https://github.com/steremma), [#2042](https://github.com/RaRe-Technologies/gensim/pull/2042))
* Fix bug in `Similarity.query_shards` in multiprocessing case ([@bohea](https://github.com/bohea), [#2044](https://github.com/RaRe-Technologies/gensim/pull/2044))
* Fix SMART from TfidfModel for case when `df == "n"` ([@PeteBleackley](https://github.com/PeteBleackley), [#2021](https://github.com/RaRe-Technologies/gensim/pull/2021))
* Fix OverflowError when loading a large term-document matrix in compiled MatrixMarket format ([@arlenk](https://github.com/arlenk), [#2001](https://github.com/RaRe-Technologies/gensim/pull/2001))
* Update rules for removing table markup from Wikipedia dumps ([@chaitaliSaini](https://github.com/chaitaliSaini), [#1954](https://github.com/RaRe-Technologies/gensim/pull/1954))
* Fix `_is_single` from `Phrases` for case when corpus is a NumPy array ([@rmalouf](https://github.com/rmalouf), [#1987](https://github.com/RaRe-Technologies/gensim/pull/1987))
* Fix tests for `EuclideanKeyedVectors.similarity_matrix` ([@Witiko](https://github.com/Witiko), [#1984](https://github.com/RaRe-Technologies/gensim/pull/1984))
* Fix deprecated parameters in `D2VTransformer` and `W2VTransformer`([@MritunjayMohitesh](https://github.com/MritunjayMohitesh), [#1945](https://github.com/RaRe-Technologies/gensim/pull/1945))
* Fix `Doc2Vec.infer_vector` after loading old `Doc2Vec` (`gensim<=3.2`)([@manneshiva](https://github.com/manneshiva), [#1974](https://github.com/RaRe-Technologies/gensim/pull/1974))
* Fix inheritance chain for `load_word2vec_format` ([@DennisChen0307](https://github.com/DennisChen0307), [#1968](https://github.com/RaRe-Technologies/gensim/pull/1968))
* Update Keras version (avoid bug from `keras==2.1.5`) ([@menshikh-iv](https://github.com/menshikh-iv), [#1963](https://github.com/RaRe-Technologies/gensim/pull/1963))



### :warning: Deprecations (will be removed in the next major release)
* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.4.0, 2018-03-01

### :star2: New features:
* Massive optimizations of `gensim.models.LdaModel`: much faster training, using Cython. ([@arlenk](https://github.com/arlenk), [#1767](https://github.com/RaRe-Technologies/gensim/pull/1767))
    - Training benchmark :boom:

      | dataset | old LDA [sec] | optimized LDA [sec] | speed up |
      |---------|---------------|---------------------|---------|
      | nytimes | 3473 | **1975** | **1.76x** |
      | enron   | 774 | **437** |  **1.77x** |

    - This change **affects all models that depend on `LdaModel`**, such as `LdaMulticore`, `LdaSeqModel`, `AuthorTopicModel`.
* Huge speed-ups to corpus I/O with `MmCorpus` (Cython) ([@arlenk](https://github.com/arlenk), [#1825](https://github.com/RaRe-Technologies/gensim/pull/1825))
    - File reading benchmark

      |     dataset   | file compressed? | old MmReader [sec] | optimized MmReader [sec] | speed up      |
      |---------------|:-----------:|:------------:|:------------------:|:-------------:|
      | enron         |      no     |      22.3    |     **2.6**        |    **8.7x**   |
      |               |     yes     |      37.3    |    **14.4**        |    **2.6x**   |
      | nytimes       |      no     |      419.3   |    **49.2**        |    **8.5x**   |
      |               |     yes     |      686.2   |    **275.1**       |    **2.5x**   |
      | text8         |      no     |      25.4    |     **2.5**        |   **10.1x**   |
      |               |     yes     |      41.9    |    **17.0**        |    **2.5x**   |

    - Overall, a **2.5x** speedup for compressed `.mm.gz` input and **8.5x** :fire::fire::fire: for uncompressed plaintext `.mm`.

* Performance and memory optimization to `gensim.models.FastText` :rocket: ([@jbaiter](https://github.com/jbaiter), [#1916](https://github.com/RaRe-Technologies/gensim/pull/1916))
    - Benchmark (first 500,000 articles from English Wikipedia)

      | Metric                 | old FastText         | optimized FastText  | improvement |
      | -----------------------| -----------------| -------------------|-------------|
      | Training time (1 epoch)     |      4823.4s (80.38 minutes)    |  **1873.6s (31.22 minutes)**  | **2.57x** |
      | Training time (full) | 1h 26min 13s | **36min 43s** | **2.35x** |
      | Training words/sec   |  72,781  | **187,366** | **2.57x** |
      | Training peak memory   | 5.2 GB  |  **3.7 GB** | **1.4x** |

    - Overall, a **2.5x** speedup & memory usage reduced by **30%**.

* Implemented [Soft Cosine Measure](https://en.wikipedia.org/wiki/Cosine_similarity#Soft_cosine_measure) ([@Witiko](https://github.com/Witiko), [#1827](https://github.com/RaRe-Technologies/gensim/pull/1827))
    - New method for assessing document similarity, a nice faster alternative to [WMD, Word Mover's Distance](http://proceedings.mlr.press/v37/kusnerb15.pdf)
    - Benchmark

      | Technique | MAP score | Duration     |
      |-----------|-----------|--------------|
      | softcossim| **45.99** | **1.24 sec** |
      | wmd-relax | 44.48     | 12.22 sec    |
      |  cossim   | 44.22     | 4.39 sec     |
      | wmd-gensim| 44.08     | 98.29 sec    |

    - [Soft Cosine notebook with detailed description, examples & benchmarks](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb)
    - Related papers:
        - [Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model](http://www.scielo.org.mx/pdf/cys/v18n3/v18n3a7.pdf)
        - [SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering](http://www.aclweb.org/anthology/S17-2051)
        - [Vector Space Representations in IR](https://github.com/witiko-masters-thesis/thesis/blob/master/main.pdf)


### :+1: Improvements:
* New method to show the Gensim installation parameters: `python -m gensim.scripts.package_info --info`. Use this when reporting problems, for easier debugging. Fix #1902 ([@sharanry](https://github.com/sharanry), [#1903](https://github.com/RaRe-Technologies/gensim/pull/1903))
* Added a flag to optionally skip network-related tests, to help maintainers avoid network issues with CI services ([@menshikh-iv](https://github.com/menshikh-iv), [#1930](https://github.com/RaRe-Technologies/gensim/pull/1930))
* Added `license` field to `setup.py`, allowing the use of tools like `pip-licenses` ([@nils-werner](https://github.com/nils-werner), [#1909](https://github.com/RaRe-Technologies/gensim/pull/1909))

### :red_circle: Bug fixes:
* Fix Python 3 compatibility for `gensim.corpora.UciCorpus.save_corpus` ([@darindf](https://github.com/darindf), [#1875](https://github.com/RaRe-Technologies/gensim/pull/1875))
* Add `wv` property to KeyedVectors for backward compatibility. Fix #1882 ([@manneshiva](https://github.com/manneshiva), [#1884](https://github.com/RaRe-Technologies/gensim/pull/1884))
* Fix deprecation warning from `inspect.getargspec`. Fix #1878 ([@aneesh-joshi](https://github.com/aneesh-joshi), [#1887](https://github.com/RaRe-Technologies/gensim/pull/1887))
* Add `LabeledSentence` to `gensim.models.doc2vec` for backward compatibility. Fix #1886 ([@manneshiva](https://github.com/manneshiva), [#1891](https://github.com/RaRe-Technologies/gensim/pull/1891))
* Fix empty output bug in `Phrases` (when using `model[tokens]` twice). Fix #1401 ([@sj29-innovate](https://github.com/sj29-innovate), [#1853](https://github.com/RaRe-Technologies/gensim/pull/1853))
* Fix type problems for `D2VTransformer.fit_transform`. Fix #1834 ([@Utkarsh-Mishra-CIC](https://github.com/Utkarsh-Mishra-CIC), [#1845](https://github.com/RaRe-Technologies/gensim/pull/1845))
* Fix `datatype` parameter for `KeyedVectors.load_word2vec_format`. Fix #1682 ([@pushpankar](https://github.com/pushpankar), [#1819](https://github.com/RaRe-Technologies/gensim/pull/1819))
* Fix deprecated parameters in `doc2vec-lee` notebook ([@TheFlash10](https://github.com/TheFlash10), [#1918](https://github.com/RaRe-Technologies/gensim/pull/1918))
* Fix file-like closing bug in `gensim.corpora.MmCorpus`. Fix #1869 ([@sj29-innovate](https://github.com/sj29-innovate), [#1911](https://github.com/RaRe-Technologies/gensim/pull/1911))
* Fix precision problem in `test_similarities.py`, no more FP fails. ([@menshikh-iv](https://github.com/menshikh-iv), [#1928](https://github.com/RaRe-Technologies/gensim/pull/1928))
* Fix encoding in Lee corpus reader. ([@menshikh-iv](https://github.com/menshikh-iv), [#1931](https://github.com/RaRe-Technologies/gensim/pull/1931))
* Fix OOV pairs counter in `WordEmbeddingsKeyedVectors.evaluate_word_pairs`. ([@akutuzov](https://github.com/akutuzov), [#1934](https://github.com/RaRe-Technologies/gensim/pull/1934))


### :books: Tutorial and doc improvements:
* Fix example block for `gensim.models.Word2Vec` ([@nzw0301](https://github.com/nzw0301), [#1870](https://github.com/RaRe-Technologies/gensim/pull/1876))
* Fix `doc2vec-lee` notebook ([@numericlee](https://github.com/numericlee), [#1870](https://github.com/RaRe-Technologies/gensim/pull/1870))
* Store images from `README.md` directly in repository. Fix #1849 ([@ibrahimsharaf](https://github.com/ibrahimsharaf), [#1861](https://github.com/RaRe-Technologies/gensim/pull/1861))
* Add windows venv activate command to `CONTRIBUTING.md` ([@aneesh-joshi](https://github.com/aneesh-joshi), [#1880](https://github.com/RaRe-Technologies/gensim/pull/1880))
* Add anaconda-cloud badge. Partial fix #1901 ([@sharanry](https://github.com/sharanry), [#1905](https://github.com/RaRe-Technologies/gensim/pull/1905))
* Fix docstrings for lsi-related code ([@steremma](https://github.com/steremma), [#1892](https://github.com/RaRe-Technologies/gensim/pull/1892))
* Fix parameter description of  `sg` parameter for `gensim.models.word2vec` ([@mdcclv](https://github.com/mdcclv), [#1919](https://github.com/RaRe-Technologies/gensim/pull/1919))
* Refactor documentation for `gensim.similarities.docsim` and `MmCorpus-related`. ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1910](https://github.com/RaRe-Technologies/gensim/pull/1910))
* Fix docstrings for `gensim.test.utils` ([@yurkai](https://github.com/yurkai) & [@menshikh-iv](https://github.com/menshikh-iv), [#1904](https://github.com/RaRe-Technologies/gensim/pull/1904))
* Refactor docstrings for `gensim.scripts`. Partial fix #1665 ([@yurkai](https://github.com/yurkai) & [@menshikh-iv](https://github.com/menshikh-iv), [#1792](https://github.com/RaRe-Technologies/gensim/pull/1792))
* Refactor API reference `gensim.corpora`. Partial fix #1671 ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1835](https://github.com/RaRe-Technologies/gensim/pull/1835))
* Fix documentation for `gensim.models.wrappers` ([@kakshay21](https://github.com/kakshay21) & [@menshikh-iv](https://github.com/menshikh-iv), [#1859](https://github.com/RaRe-Technologies/gensim/pull/1859))
* Fix docstrings for `gensim.interfaces` ([@yurkai](https://github.com/yurkai) & [@menshikh-iv](https://github.com/menshikh-iv), [#1913](https://github.com/RaRe-Technologies/gensim/pull/1913))


### :warning: Deprecations (will be removed in the next major release)
* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.3.0, 2018-02-02

:star2: New features:
* Re-designed all "*2vec" implementations ([@manneshiva](https://github.com/manneshiva), [#1777](https://github.com/RaRe-Technologies/gensim/pull/1777))
    - Modular organization of `Word2Vec`, `Doc2Vec`, `FastText`, etc ..., making it easier to add new models in the future and re-use code
    - Fully backward compatible (even with loading models stored by a previous Gensim version)
    - [Detailed documentation for the *2vec refactoring project](https://github.com/manneshiva/gensim/wiki/Any2Vec-Refactoring-Summary)

* Improve `gensim.scripts.segment_wiki` by retaining interwiki links. Fix #1712
 ([@steremma](https://github.com/steremma), [PR #1839](https://github.com/RaRe-Technologies/gensim/pull/1839))
    - Optionally extract interlinks from Wikipedia pages (use the `--include-interlinks` option). This will output one additional JSON dict for each article:
        ```
        {
            "interlinks": {
                "article title 1": "interlink text 1",
                "article title 2": "interlink text 2",
                ...
            }
        }
        ```

    - Example: extract the Wikipedia graph with article links as edges, from a raw Wikipedia dump:
        ```bash
        python -m gensim.scripts.segment_wiki --include-interlinks --file ~/Downloads/enwiki-latest-pages-articles.xml.bz2 --output ~/Desktop/enwiki-latest.jsonl.gz
        ```
        - Read this field from the `segment_wiki` output:

        ```python
        import json
        from smart_open import smart_open

        with smart_open("enwiki-latest.jsonl.gz") as infile:
            for doc in infile:
                doc = json.loads(doc)

                src_node = doc['title']
                dst_nodes = doc['interlinks'].keys()

                print(u"Source node: {}".format(src_node))
                print(u"Destination nodes: {}".format(u", ".join(dst_nodes)))
                break

        """
        OUTPUT:

        Source node: Anarchism
        Destination nodes: anarcha-feminist, Ivan Illich, Adolf Brand, Josiah Warren, will (philosophy), anarcha-feminism, Anarchism in Mexico, Lysander Spooner, English Civil War, G8, Sebastien Faure, Nihilist movement, S√©bastien Faure, Left-wing politics, imamate, Pierre Joseph Proudhon, anarchist communism, Universit√† popolare (Italian newspaper), 1848 Revolution, Synthesis anarchism, labour movement, anarchist communists, collectivist anarchism, polyamory, post-humanism, postcolonialism, anti war movement, State (polity), security culture, Catalan people, Stoicism, Progressive education, stateless society, Umberto I of Italy, German language, Anarchist schools of thought, NEFAC, Jacques Ellul, Spanish Communist Party, Crypto-anarchism, ruling class, non-violence, Platformist, The History of Sexuality, Revolutions of 1917‚Äì23, Federaci√≥n Anarquista Ib√©rica, propaganda of the deed, William B. Greene, Platformism, mutually exclusive, Fraye Arbeter Shtime, Adolf Hitler, oxymoron, Paris Commune, Anarchism in Italy#Postwar years and today, Oranienburg, abstentionism, Free Society, Henry David Thoreau, privative alpha, George I of Greece, communards, Gustav Landauer, Lucifer the Lightbearer, Moses Harman, coercion, regicide, rationalist, Resistance during World War II, Christ (title), Bohemianism, individualism, Crass, black bloc, Spanish Revolution of 1936, Erich M√ºhsam, Empress Elisabeth of Austria, Free association (communism and anarchism), general strike, Francesc Ferrer i Gu√†rdia, Catalan anarchist pedagogue and free-thinker, veganarchism, Traditional knowledge, Japanese Anarchist Federation, Diogenes of Sinope, Hierarchy, sexual revolution, Naturism, Bavarian Soviet Republic, February Revolution, Eugene Varlin, Renaissance humanism, Mexican Liberal Party, Friedrich Engels, Fernando Tarrida del M√°rmol, Caliphate, Marxism, Jesus, John Cage, Umanita Nova, Anarcho-pacifism, Peter Kropotkin, Religious anarchism, Anselme Bellegarrigue, civilisation, moral obligation, hedonist, Free Territory (Ukraine), -ism, neo-liberalism, Austrian School, philosophy, freethought, Joseph Goebbels, Conservatism, anarchist economics, Cavalier, Maximilien de Robespierre, Comstockery, Dorothy Day, Anarchism in France, F√©d√©ration anarchiste, World Economic Forum, Amparo Poch y Gasc√≥n, Sex Pistols, women's rights, collectivisation, Taoism, common ownership, William Batchelder Greene, Collective farming, popular education, biphobia, targeted killings, Protestant Christianity, state socialism, Marie Fran√ßois Sadi Carnot, Stephen Pearl Andrews, World Trade Organization, Communist Party of Spain (main), Pluto Press, Levante, Spain, Alexander Berkman, Wilhelm Weitling, Kharijites, Bolshevik, Liberty (1881‚Äì1908), Anarchist Aragon, social democrats, Dielo Truda, Post-left anarchy, Age of Enlightenment, Blanquism, Walden, mutual aid (organization), Far-left politics, privative, revolutions of 1848, anarchism and nationalism, punk rock, √âtienne de La Bo√©tie, Max Stirner, Jacobin (politics), agriculture, anarchy, Confederacion General del Trabajo de Espa√±a, toleration, reformism, International Anarchist Congress of Amsterdam, The Ego and Its Own, Ukraine, Civil Disobedience (Thoreau), Spanish Civil War, David Graeber, Anarchism and issues related to love and sex, James Guillaume, Insurrectionary anarchism, Political repression, International Workers' Association, Barcelona, Bulgaria, Voline, Zeno of Citium, anarcho-communists, organized religion, libertarianism, bisexuality, Ricardo Flores Mag√≥n, Henri Zisly, Eight-hour day, Freetown Christiania, heteronormativity, Mikhail Bakunin, Propagandaministerium, Ezra Heywood, individual reappropriation, Modern School (United States), archon, Conf√©d√©ration nationale du travail, socialist movement, History of Islam, Max Nettlau, Political Justice, Reichstag fire, Anti-Christianity, decentralised, Issues in anarchism#Communism, deschooling, Christian movement, squatter, Anarchism in Germany, Catalonia, Louise Michel, Solidarity Federation, What is Property?, European individualist anarchism, Pierre-Joseph Proudhon, Mexican Revolution, wikt:anarchism, Blackshirts, Jewish anarchism, Russian Civil War, property rights, anti-authoritarian, individual reclamation, propaganda by the deed, from each according to his ability, to each according to his need, Feminist movement, Confiscation, social anarchism, Anarchism in Russia, Daniel Gu√©rin, Uruguayan Anarchist Federation, Anarcha-feminism, Enrag√©s, Cynicism (philosophy), workers' council, The Word (free love), Allen Ginsberg, Campaign for Nuclear Disarmament, antimilitarism, Workers' self-management, Federaci√≥n Obrera Regional Argentina, self-governance, free market, Carlos I of Portugal, Simon Critchley, Anti-clericalism, heterosexual, Layla AbdelRahim, Mexican Anarchist Federation, Anarchism and Marxism, October Revolution, Anti-nuclear movement, Joseph D√©jacque, Bolsheviks, Luigi Fabbri, morality, Communist party, Sam Dolgoff, united front, Ammon Hennacy, social ecology, commune (intentional community), Oscar Wilde, French Revolution, egoist anarchism, Comintern, transphobia, anarchism without adjectives, social control, means of production, Michel Onfray, Anarchism in France#The Fourth Republic (1945‚Äì1958), syndicalism, Anarchism in Spain, Iberian Anarchist Federation, International of Anarchist Federations, Emma Goldman, Netherlands, anarchist free school, International Workingmen's Association, Queer anarchism, Cantonal Revolution, trade unionism, Karl Marx, LGBT community, humanism, Anti-fascism, Carrara, political philosophy, Anarcho-transhumanism, libertarian socialist, Russian Revolution (1917), Two Cheers for Anarchism: Six Easy Pieces on Autonomy, Dignity, and Meaningful Work and Play, Emile Armand, insurrectionary anarchism, individual, Zhuang Zhou, Free Territory, White movement, Greenwich Village, Virginia Bolten, transcendentalist, public choice theory, wikt:brigand, Issues in anarchism#Participation in statist democracy, free love, Mutualism (economic theory), Anarchist St. Imier International, censorship, federalist, 6 February 1934 crisis, biennio rosso, anti-clerical, centralism, Anarchism: A Documentary History of Libertarian Ideas, minarchism, James C. Scott, First International, homosexuality, political theology, spontaneous order, Oranienburg concentration camp, anarcho-communism, negative liberty, post-modernism, Anarchism in Italy, Leopold Kohr, union of egoists, counterculture, Miguel Gimenez Igualada, philosophical anarchism, International Libertarian Solidarity, homosexual, Counterculture of the 1960s, Errico Malatesta, strikebreaker, Workers' Party of Marxist Unification, Clifford Harper, Reification (fallacy), patriarchy, anarchist law, Apostle (Christian), market (economics), Summerhill School, positive liberty, socialism, feminism, Direct action, Melchor Rodr√≠guez Garc√≠a, William Godwin, Nazi concentration camps, Synthesist anarchism, Margaret Anderson, Han Ryner, Federation of Organized Trades and Labor Unions, technology, Workers Solidarity Movement, Edmund Burke, Encyclop√¶dia Britannica, state (polity), Herbert Read, Park G√ºell, utilitarian, far right leagues, Limited government, self-ownership, Pejorative, homophobia, Industrial Workers of the World, The Dispossessed, Hague Congress (1872), Stalinism, Reciprocity (cultural anthropology), Fernand Pelloutier, individualist anarchism in France, The False Principle of our Education, individualist anarchism, Pierre Monatte, Soviet Union, counter-economics, Rudolf Rocker, Anarchism and capitalism, Parma, Black Rose Books, lesbian, Arditi del Popolo, Emile Armand (1872‚Äì1962), who propounded the virtues of free love in the Parisian anarchist milieu of the early 20th century, collectivism, Development criticism, John Henry Mackay, Beno√Æt Broutchoux, Illegalism, Laozi, feminist, Christiaan Cornelissen, Syndicalist Workers' Federation, anarcho-syndicalism, Andalusia, Renzo Novatore, trade union, autonomist marxism, dictatorship of the proletariat, Mujeres Libres, Voltairine de Cleyre, Post-anarchism, participatory economics, Confederaci√≥n Nacional del Trabajo, Syncretic politics, direct democracy, Jean-Jacques Rousseau, Green anarchism, Surrealism, labour unions, A. S. Neill, christian anarchist, Bonnot Gang, Anti-capitalism, Anarchism in Brazil, simple living, enlightened self-interest, Conf√©d√©ration g√©n√©rale du travail, class conflict, International Workers' Day, H√©bertists, Gerrard Winstanley, Francoism, anarcho-pacifist, Andrej Grubacic, individualist anarchist and social anarchist thinkers., April Carter, private property, penal colonies, Libertarian socialism, Camillo Berneri, Christian anarchism, transhumanism, Lucifer, the Light-Bearer, Edna St. Vincent Millay, unschooling, Leo Tolstoy, M. E. Lazarus, Spanish Anarchists, Buddhist anarchism, ideology, William McKinley, anarcho-primitivism, Francesc Pi i Margall, :Category:Anarchism by country, International Workers Association, Anarcho-capitalism, Lois Waisbrooker, wikt:Solidarity, Baja California, social revolution, Unione Sindacale Italiana, Lev Chernyi, Alex Comfort, Sonnenburg, Leon Czolgosz, Volin, utopian, Argentine Libertarian Federation, Nudism, Left-wing market anarchism, insurrection, definitional concerns in anarchist theory, infinitive, affinity group, World Trade Organization Ministerial Conference of 1999 protest activity, class struggle, nonviolence, John Zerzan, poststructuralist, Noam Chomsky, Second Fitna, Julian Beck, Philadelphes, League of Peace and Freedom, F√©d√©ration Anarchiste, Kronstadt rebellion, Cold War, Andr√© Breton, Silvio Gesell, libertarian anarchism, voluntary association, anti-globalisation movement, birth control, L. Susan Brown, anarcho-naturism, personal property, Roundhead, Harold Barclay, The Joy of Sex, Council communism, Luc√≠a S√°nchez Saornil, tyrannicide, Neopaganism, lois sc√©l√©rates, Johann Most, Anarchist Catalonia, Albert Camus, Protests of 1968, Alexander II of Russia, Spain's economy, Federazione Anarchica Italiana, Cuba, German Revolution of 1918‚Äì1919, stirner, Property is theft, Situationist International, law and economics

        ```

* Add support for [SMART notation](https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html) for `TfidfModel`. Fix #1785 ([@markroxor](https://github.com/markroxor), [#1791](https://github.com/RaRe-Technologies/gensim/pull/1791))
    - Natural extension of `TfidfModel` to allow different weighting and normalization schemes
        ```python
        from gensim.corpora import Dictionary
        from gensim.models import TfidfModel
        import gensim.downloader as api

        data = api.load("text8")
        dct = Dictionary(data)
        corpus = [dct.doc2bow(line) for line in data]

        # Train Tfidf model using the SMART notation, smartirs="ntc" where
        # 'n' - natural term frequency
        # 't' - idf document frequency
        # 'c' - cosine normalization
        #
        # More information about possible values available in documentation or https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html

        model = TfidfModel(corpus, id2word=dct, smartirs="ntc")
        vectorized_corpus = list(model[corpus])

        ```
    - [SMART Information Retrieval System (wiki)](https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System)

* Add CircleCI for building Gensim documentation. Fix #1807 ([@menshikh-iv](https://github.com/menshikh-iv), [#1822](https://github.com/RaRe-Technologies/gensim/pull/1822))
    - An easy way to preview the rendered documentation (especially, if don't use Linux)
        - Go to "Details" link of CircleCI in your PR, click on the "Artifacts" tab, choose the HTML file that you want to view; a new tab will open with the rendered HTML page
    - Integration with Github, to see the documentation directly from the pull request page
        - Install a user-script plugin: [greasemonkey (for firefox)](https://addons.mozilla.org/en-US/firefox/addon/greasemonkey/) or [tampermonkey (for chrome)](https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en)
        - Add [this user-script](https://gist.github.com/menshikh-iv/bfe9b8ef2db10e9511aa9fe5935a7289) to the plugin
        - Now you‚Äôll see a new button "See CircleCI doc for this PR" in each PR in the Gensim repository. Click it to see the full rendered documentation.


:red_circle: Bug fixes:
* Fix import in `get_my_ip`. Fix #1771 ([@darindf](https://github.com/darindf), [#1772](https://github.com/RaRe-Technologies/gensim/pull/1772))
* Fix tox.ini/setup.cfg configuration ([@menshikh-iv](https://github.com/menshikh-iv), [#1815](https://github.com/RaRe-Technologies/gensim/pull/1815))
* Fix formula in `gensim.summarization.bm25`. Fix #1828 ([@sj29-innovate](https://github.com/sj29-innovate), [#1833](https://github.com/RaRe-Technologies/gensim/pull/1833))
* Fix the train method of `TranslationMatrix` ([@robotcator](https://github.com/robotcator), [#1838](https://github.com/RaRe-Technologies/gensim/pull/1838))
* Fix positional params used for `gensim.models.CoherenceModel` in `gensim.models.callbacks` ([@Alexjmsherman](https://github.com/Alexjmsherman), [#1823](https://github.com/RaRe-Technologies/gensim/pull/1823))
* Fix parameter setting for `FastText.train`. Fix #1818 ([@sj29-innovate](https://github.com/sj29-innovate), [#1837](https://github.com/RaRe-Technologies/gensim/pull/1837))
* Pin python2 explicitly for building documentation ([@menshikh-iv](https://github.com/menshikh-iv), [#1840](https://github.com/RaRe-Technologies/gensim/pull/1840))
* Remove dispatcher deadlock for distributed LDA ([@darindf](https://github.com/darindf), [#1817](https://github.com/RaRe-Technologies/gensim/pull/1817))
* Fix `score_function` from `LexicalEntailmentEvaluation`. Fix #1858 ([@hachibaka](https://github.com/hachibaka), [#1863](https://github.com/RaRe-Technologies/gensim/pull/1863))
* Fix symmetrical case for hellinger distance. Fix #1854 ([@caiyulun](https://github.com/caiyulun), [#1860](https://github.com/RaRe-Technologies/gensim/pull/1860))
* Remove wrong logging at import. Fix #1706 ([@menshikh-iv](https://github.com/menshikh-iv), [#1871](https://github.com/RaRe-Technologies/gensim/pull/1871))


:books: Tutorial and doc improvements:
* Refactor documentation API Reference for `gensim.summarization` ([@yurkai](https://github.com/yurkai) & [@menshikh-iv](https://github.com/menshikh-iv), [#1709](https://github.com/RaRe-Technologies/gensim/pull/1709))
* Fix docstrings for `gensim.similarities.index`. Partial fix #1666 ([@menshikh-iv](https://github.com/menshikh-iv), [#1681](https://github.com/RaRe-Technologies/gensim/pull/1681))
* Fix docstrings for `gensim.models.translation_matrix` ([@KokuKUSIAKU](https://github.com/KokuKUSIAKU) & [@menshikh-iv](https://github.com/menshikh-iv), [#1806](https://github.com/RaRe-Technologies/gensim/pull/1806))
* Fix docstrings for `gensim.models.rpmodel` ([@jazzmuesli](https://github.com/jazzmuesli) & [@menshikh-iv](https://github.com/menshikh-iv), [#1802](https://github.com/RaRe-Technologies/gensim/pull/1802))
* Fix docstrings for `gensim.utils` ([@kakshay21](https://github.com/kakshay21) & [@menshikh-iv](https://github.com/menshikh-iv), [#1797](https://github.com/RaRe-Technologies/gensim/pull/1797))
* Fix docstrings for `gensim.matutils` ([@Cheukting](https://github.com/Cheukting) & [@menshikh-iv](https://github.com/menshikh-iv), [#1804](https://github.com/RaRe-Technologies/gensim/pull/1804))
* Fix docstrings for `gensim.models.logentropy_model` ([@minggli](https://github.com/minggli) & [@menshikh-iv](https://github.com/menshikh-iv), [#1803](https://github.com/RaRe-Technologies/gensim/pull/1803))
* Fix docstrings for `gensim.models.normmodel` ([@AustenLamacraft](https://github.com/AustenLamacraft) & [@menshikh-iv](https://github.com/menshikh-iv), [#1805](https://github.com/RaRe-Technologies/gensim/pull/1805))
* Refactor API reference `gensim.topic_coherence`. Fix #1669 ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1714](https://github.com/RaRe-Technologies/gensim/pull/1714))
* Fix documentation for `gensim.corpora.dictionary` and `gensim.corpora.hashdictionary`. Partial fix #1671 ([@CLearERR](https://github.com/CLearERR) & [@menshikh-iv](https://github.com/menshikh-iv), [#1814](https://github.com/RaRe-Technologies/gensim/pull/1814))
* Fix documentation for `gensim.corpora`. Partial fix #1671 ([@anotherbugmaster](https://github.com/anotherbugmaster) & [@menshikh-iv](https://github.com/menshikh-iv), [#1729](https://github.com/RaRe-Technologies/gensim/pull/1729))
* Update banner in doc pages ([@piskvorky](https://github.com/piskvorky), [#1865](https://github.com/RaRe-Technologies/gensim/pull/1865))
* Fix errors in the doc2vec-lee notebook ([@PeterHamilton](https://github.com/PeterHamilton), [#1841](https://github.com/RaRe-Technologies/gensim/pull/1841))
* Add wordnet mammal train file for Poincare notebook ([@jayantj](https://github.com/jayantj), [#1781](https://github.com/RaRe-Technologies/gensim/pull/1781))
* Update Poincare notebooks (#1774) ([@jayantj](https://github.com/jayantj), [#1774](https://github.com/RaRe-Technologies/gensim/pull/1774))
* Update contributing guide. Fix #1786 ([@menshikh-iv](https://github.com/menshikh-iv), [#1793](https://github.com/RaRe-Technologies/gensim/pull/1793))
* Add `model_to_dict` one-liner to word2vec notebook. Fix #1269 ([@kakshay21](https://github.com/kakshay21), [#1776](https://github.com/RaRe-Technologies/gensim/pull/1776))
* Add word embedding viz to word2vec notebook. Fix #1419 ([@markroxor](https://github.com/markroxor), [#1800](https://github.com/RaRe-Technologies/gensim/pull/1800))
* Fix description of `sg` parameter for `gensim.models.FastText` ([@akutuzov](https://github.com/akutuzov), [#1801](https://github.com/RaRe-Technologies/gensim/pull/1801))
* Fix typo in `doc2vec-IMDB`. Fix #1788 ([@apoorvaeternity](https://github.com/apoorvaeternity), [#1796](https://github.com/RaRe-Technologies/gensim/pull/1796))
* Remove outdated bz2 examples from tutorials[2] ([@menshikh-iv](https://github.com/menshikh-iv), [#1868](https://github.com/RaRe-Technologies/gensim/pull/1868))
* Remove outdated `bz2` + `MmCorpus` examples from tutorials ([@menshikh-iv](https://github.com/menshikh-iv), [#1867](https://github.com/RaRe-Technologies/gensim/pull/1867))



:+1: Improvements:
* Refactor tests for `gensim.corpora.WikiCorpus` ([@steremma](https://github.com/steremma), [#1821](https://github.com/RaRe-Technologies/gensim/pull/1821))


:warning: Deprecations (will be removed in the next major release)
* Remove
    - `gensim.models.wrappers.fasttext` (obsoleted by the new native `gensim.models.fasttext` implementation)
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki` (all of these obsoleted by the new native  `gensim.scripts.segment_wiki` implementation)
    - "deprecated" functions and attributes

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.2.0, 2017-12-09

:star2: New features:

* New download API for corpora and pre-trained models ([@chaitaliSaini](https://github.com/chaitaliSaini) & [@menshikh-iv](https://github.com/menshikh-iv), [#1705](https://github.com/RaRe-Technologies/gensim/pull/1705) & [#1632](https://github.com/RaRe-Technologies/gensim/pull/1632) & [#1492](https://github.com/RaRe-Technologies/gensim/pull/1492))
    - Download large NLP datasets in one line of Python, then use with memory-efficient data streaming:
        ```python
        import gensim.downloader as api

        for article in api.load("wiki-english-20171001"):
            pass

        ```
    - Don‚Äôt waste time searching for good word embeddings, use the curated ones we included:
        ```python
        import gensim.downloader as api

        model = api.load("glove-twitter-25")
        model.most_similar("engineer")

        # [('specialist', 0.957542896270752),
        #  ('developer', 0.9548177123069763),
        #  ('administrator', 0.9432312846183777),
        #  ('consultant', 0.93915855884552),
        #  ('technician', 0.9368376135826111),
        #  ('analyst', 0.9342101216316223),
        #  ('architect', 0.9257484674453735),
        #  ('engineering', 0.9159940481185913),
        #  ('systems', 0.9123805165290833),
        #  ('consulting', 0.9112802147865295)]
        ```
    - [Blog post](https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/) introducing the API and design decisions.
  - [Notebook with examples](https://github.com/RaRe-Technologies/gensim/blob/be4500e4f0616ec2864c2ce70cb5d4db4b46512d/docs/notebooks/downloader_api_tutorial.ipynb)

* New model: Poincar√© embeddings ([@jayantj](https://github.com/jayantj), [#1696](https://github.com/RaRe-Technologies/gensim/pull/1696) & [#1700](https://github.com/RaRe-Technologies/gensim/pull/1700) & [#1757](https://github.com/RaRe-Technologies/gensim/pull/1757) & [#1734](https://github.com/RaRe-Technologies/gensim/pull/1734))
    - Embed a graph (taxonomy) in the same way as word2vec embeds words:
        ```python
        from gensim.models.poincare import PoincareRelations, PoincareModel
        from gensim.test.utils import datapath

        data = PoincareRelations(datapath('poincare_hypernyms.tsv'))
        model = PoincareModel(data)
        model.kv.most_similar("cat.n.01")

        # [('kangaroo.n.01', 0.010581353439700418),
        # ('gib.n.02', 0.011171531439892076),
        # ('striped_skunk.n.01', 0.012025106076442395),
        # ('metatherian.n.01', 0.01246679759214648),
        # ('mammal.n.01', 0.013281303506525968),
        # ('marsupial.n.01', 0.013941330203709653)]
        ```
    - [Tutorial notebook on Poincar√© embeddings](https://github.com/RaRe-Technologies/gensim/blob/920c029ca97f961c8df264672c34936607876694/docs/notebooks/Poincare%20Tutorial.ipynb)
    - [Model introduction and the journey of its implementation](https://rare-technologies.com/implementing-poincare-embeddings/)
    - [Original paper](https://arxiv.org/abs/1705.08039) on arXiv

* Optimized FastText ([@manneshiva](https://github.com/manneshiva), [#1742](https://github.com/RaRe-Technologies/gensim/pull/1742))
  - New fast multithreaded implementation of FastText, natively in Python/Cython. Deprecates the existing wrapper for Facebook‚Äôs C++ implementation.
    ```python
    import gensim.downloader as api
    from gensim.models import FastText

    model = FastText(api.load("text8"))
    model.most_similar("cat")

    # [('catnip', 0.8538144826889038),
    #  ('catwalk', 0.8136177062988281),
    #  ('catchy', 0.7828493118286133),
    #  ('caf', 0.7826495170593262),
    #  ('bobcat', 0.7745151519775391),
    #  ('tomcat', 0.7732658386230469),
    #  ('moat', 0.7728310823440552),
    #  ('caye', 0.7666271328926086),
    #  ('catv', 0.7651021480560303),
    #  ('caveat', 0.7643581628799438)]


    ```

* Binary pre-compiled wheels for Windows, OSX and Linux ([@menshikh-iv](https://github.com/menshikh-iv), [MacPython/gensim-wheels/#7](https://github.com/MacPython/gensim-wheels/pull/7))
    - Users no longer need to have a C compiler for using the fast (Cythonized) version of word2vec, doc2vec, etc.
    - Faster Gensim pip installation

* Added `DeprecationWarnings` to deprecated methods and parameters, with a clear schedule for removal.

:+1: Improvements:
* Add Montemurro and Zanette's entropy based keyword extraction algorithm. Fix #665 ([@PeteBleackley](https://github.com/PeteBleackley), [#1738](https://github.com/RaRe-Technologies/gensim/pull/1738))
* Fix flake8 E731, E402, refactor tests & sklearn API code. Partial fix #1644  ([@horpto](https://github.com/horpto), [#1689](https://github.com/RaRe-Technologies/gensim/pull/1689))
* Reduce distribution size. Fix #1698 ([@menshikh-iv](https://github.com/menshikh-iv), [#1699](https://github.com/RaRe-Technologies/gensim/pull/1699))
* Improve `scan_vocab` speed, `build_vocab_from_freq` method ([@jodevak](https://github.com/jodevak), [#1695](https://github.com/RaRe-Technologies/gensim/pull/1695))
* Improve `segment_wiki` script ([@piskvorky](https://github.com/piskvorky), [#1707](https://github.com/RaRe-Technologies/gensim/pull/1707))
* Add custom `dtype` support for `LdaModel`. Partially fix #1576 ([@xelez](https://github.com/xelez), [#1656](https://github.com/RaRe-Technologies/gensim/pull/1656))
* Add `doc2idx` method for `gensim.corpora.Dictionary`. Fix #1634 ([@roopalgarg](https://github.com/roopalgarg), [#1720](https://github.com/RaRe-Technologies/gensim/pull/1720))
* Add tox and pytest to gensim, integration with Travis and Appveyor. Fix #1613, #1644 ([@menshikh-iv](https://github.com/menshikh-iv), [#1721](https://github.com/RaRe-Technologies/gensim/pull/1721))
* Add flag for hiding outdated data for `gensim.downloader.info` ([@menshikh-iv](https://github.com/menshikh-iv), [#1736](https://github.com/RaRe-Technologies/gensim/pull/1736))
* Add reproducible order between python versions for `gensim.corpora.Dictionary` ([@formi23](https://github.com/formi23), [#1715](https://github.com/RaRe-Technologies/gensim/pull/1715))
* Update `tox.ini`, `setup.cfg`, `README.md` ([@menshikh-iv](https://github.com/menshikh-iv), [#1741](https://github.com/RaRe-Technologies/gensim/pull/1741))
* Add custom `logsumexp` for `LdaModel` ([@arlenk](https://github.com/arlenk), [#1745](https://github.com/RaRe-Technologies/gensim/pull/1745))

:red_circle: Bug fixes:
* Fix ranking formula in `gensim.summarization.bm25`. Fix #1718 ([@souravsingh](https://github.com/souravsingh), [#1726](https://github.com/RaRe-Technologies/gensim/pull/1726))
* Fixed incompatibility in persistence for `FastText` wrapper. Fix #1642 ([@chinmayapancholi13](https://github.com/chinmayapancholi13), [#1723](https://github.com/RaRe-Technologies/gensim/pull/1723))
* Fix `gensim.sklearn_api` bug with `documents_columns` parameter. Fix #1676 ([@chinmayapancholi13](https://github.com/chinmayapancholi13), [#1704](https://github.com/RaRe-Technologies/gensim/pull/1704))
* Fix slowdown of CI, remove pytest-cov ([@menshikh-iv](https://github.com/menshikh-iv), [#1728](https://github.com/RaRe-Technologies/gensim/pull/1728))
* Replace outdated packages in Dockerfile ([@rbahumi](https://github.com/rbahumi), [#1730](https://github.com/RaRe-Technologies/gensim/pull/1730))
* Replace `num_words` to `topn` in `LdaMallet.show_topics`. Fix #1747 ([@apoorvaeternity](https://github.com/apoorvaeternity), [#1749](https://github.com/RaRe-Technologies/gensim/pull/1749))
* Fix `os.rename` from `gensim.downloader` when 'src' and 'dst' on different partitions ([@anotherbugmaster](https://github.com/anotherbugmaster), [#1733](https://github.com/RaRe-Technologies/gensim/pull/1733))
* Fix `DeprecationWarning` from `logsumexp` ([@dreamgonfly](https://github.com/dreamgonfly), [#1703](https://github.com/RaRe-Technologies/gensim/pull/1703))
* Fix backward compatibility problem in `Phrases.load`. Fix #1751 ([@alexgarel](https://github.com/alexgarel), [#1758](https://github.com/RaRe-Technologies/gensim/pull/1758))
* Fix `load_word2vec_format` from `FastText`. Fix #1743 ([@manneshiva](https://github.com/manneshiva), [#1755](https://github.com/RaRe-Technologies/gensim/pull/1755))
* Fix ipython kernel version in `Dockerfile`. Fix #1762 ([@rbahumi](https://github.com/rbahumi), [#1764](https://github.com/RaRe-Technologies/gensim/pull/1764))
* Fix writing in `segment_wiki` ([@horpto](https://github.com/horpto), [#1763](https://github.com/RaRe-Technologies/gensim/pull/1763))
* Fix write method of file requires byte-like object in `segment_wiki` ([@horpto](https://github.com/horpto), [#1750](https://github.com/RaRe-Technologies/gensim/pull/1750))
* Fix incorrect vectors learned during online training for `FastText`. Fix #1752 ([@manneshiva](https://github.com/manneshiva), [#1756](https://github.com/RaRe-Technologies/gensim/pull/1756))
* Fix `dtype` of `model.wv.syn0_vocab` on updating `vocab` for `FastText`. Fix  #1759 ([@manneshiva](https://github.com/manneshiva), [#1760](https://github.com/RaRe-Technologies/gensim/pull/1760))
* Fix hashing-trick from `FastText.build_vocab`. Fix #1765 ([@manneshiva](https://github.com/manneshiva), [#1768](https://github.com/RaRe-Technologies/gensim/pull/1768))
* Add explicit `DeprecationWarning` for all outdated stuff. Fix #1753 ([@menshikh-iv](https://github.com/menshikh-iv), [#1769](https://github.com/RaRe-Technologies/gensim/pull/1769))
* Fix epsilon according to `dtype` in `LdaModel` ([@menshikh-iv](https://github.com/menshikh-iv), [#1770](https://github.com/RaRe-Technologies/gensim/pull/1770))

:books: Tutorial and doc improvements:
* Update perf numbers of `segment_wiki` ([@piskvorky](https://github.com/piskvorky), [#1708](https://github.com/RaRe-Technologies/gensim/pull/1708))
* Update docstring for `gensim.summarization.summarize`. Fix #1575 ([@fbarrios](https://github.com/fbarrios), [#1702](https://github.com/RaRe-Technologies/gensim/pull/1702))
* Refactor API Reference for `gensim.parsing`. Fix #1664 ([@CLearERR](https://github.com/CLearERR), [#1684](https://github.com/RaRe-Technologies/gensim/pull/1684))
* Fix typos in doc2vec-wikipedia notebook ([@youqad](https://github.com/youqad), [#1727](https://github.com/RaRe-Technologies/gensim/pull/1727))
* Fix PyPI long description rendering ([@edigaryev](https://github.com/edigaryev), [#1739](https://github.com/RaRe-Technologies/gensim/pull/1739))
* Fix twitter badge src  ([@menshikh-iv](https://github.com/menshikh-iv))
* Fix maillist badge color ([@menshikh-iv](https://github.com/menshikh-iv))

:warning: Deprecations (will be removed in the next major release)
* Remove
    - `gensim.examples`
    - `gensim.nosy`
    - `gensim.scripts.word2vec_standalone`
    - `gensim.scripts.make_wiki_lemma`
    - `gensim.scripts.make_wiki_online`
    - `gensim.scripts.make_wiki_online_lemma`
    - `gensim.scripts.make_wiki_online_nodebug`
    - `gensim.scripts.make_wiki`

* Move
    - `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
    - `gensim.summarization` ‚û° `gensim.models.summarization`
    - `gensim.topic_coherence` ‚û° `gensim.models._coherence`
    - `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
    - `gensim.parsing.*` ‚û° `gensim.utils.text_utils`


## 3.1.0, 2017-11-06


:star2: New features:
* Massive optimizations to LSI model training ([@isamaru](https://github.com/isamaru), [#1620](https://github.com/RaRe-Technologies/gensim/pull/1620) & [#1622](https://github.com/RaRe-Technologies/gensim/pull/1622))
  - LSI model allows use of single precision (float32), to consume  *40% less memory* while being *40% faster*.
  - LSI model can now also accept CSC matrix as input, for further memory and speed boost.
  - Overall, if your entire corpus fits in RAM: 3x faster LSI training (SVD) in 4x less memory!
    ```python
    # just an example; the corpus stream is up to you
    streaming_corpus = gensim.corpora.MmCorpus("my_tfidf_corpus.mm.gz")

    # convert your corpus to a CSC sparse matrix (assumes the entire corpus fits in RAM)
    in_memory_csc_matrix = gensim.matutils.corpus2csc(streaming_corpus, dtype=np.float32)

    # then pass the CSC to LsiModel directly
    model = LsiModel(corpus=in_memory_csc_matrix, num_topics=500, dtype=np.float32)
    ```
  - Even if you continue to use streaming corpora (your training dataset is too large for RAM), you should see significantly faster processing times and a lower memory footprint. In our experiments with a very large LSI model, we saw a drop from 29 GB peak RAM and 38 minutes (before) to 19 GB peak RAM and 26 minutes (now):
    ```python
    model = LsiModel(corpus=streaming_corpus, num_topics=500, dtype=np.float32)
    ```
* Add common terms to Phrases. Fix #1258 ([@alexgarel](https://github.com/alexgarel), [#1568](https://github.com/RaRe-Technologies/gensim/pull/1568))
  - Phrases allows to use common terms in bigrams. Before, if you are searching to reveal ngrams like `car_with_driver` and `car_without_driver`, you can either remove stop words before processing, but you will only find `car_driver`, or you won't find any of those forms (because they have three words, but also because high frequency of with will avoid them to be scored correctly), inspired by [ES common grams token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-common-grams-tokenfilter.html).
    ```python
    phr_old = Phrases(corpus)
    phr_new = Phrases(corpus, common_terms=stopwords.words('en'))

    print(phr_old[["we", "provide", "car", "with", "driver"]])  # ["we", "provide", "car_with", "driver"]
    print(phr_new[["we", "provide", "car", "with", "driver"]])  # ["we", "provide", "car_with_driver"]
    ```
* New [segment_wiki.py](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/scripts/segment_wiki.py) script ([@menshikh-iv](https://github.com/menshikh-iv), [#1483](https://github.com/RaRe-Technologies/gensim/pull/1483) & [#1694](https://github.com/RaRe-Technologies/gensim/pull/1694))
  - CLI script for processing a raw Wikipedia dump (the xml.bz2 format provided by WikiMedia) to extract its articles in a plain text format. It extracts each article's title, section names and section content and saves them as json-line:
    ```bash
    python -m gensim.scripts.segment_wiki -f enwiki-latest-pages-articles.xml.bz2 | gzip > enwiki-latest-pages-articles.json.gz
    ```
       Processing the entire English Wikipedia dump (13.5 GB, link [here](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2)) takes about 2.5 hours (i7-6700HQ, SSD).

       The output format is one article per line, serialized into JSON:
       ```python
          for line in smart_open('enwiki-latest-pages-articles.json.gz'):  # read the file we just created
              article = json.loads(line)
              print("Article title: %s" % article['title'])
              for section_title, section_text in zip(article['section_titles'], article['section_texts']):
                  print("Section title: %s" % section_title)
                  print("Section text: %s" % section_text)
        ```

:+1: Improvements:
* Speedup FastText tests ([@horpto](https://github.com/horpto), [#1686](https://github.com/RaRe-Technologies/gensim/pull/1686))
* Add optimization for `SlicedCorpus.len` ([@horpto](https://github.com/horpto), [#1679](https://github.com/RaRe-Technologies/gensim/pull/1679))
* Make `word_vec` return immutable vector. Fix #1651 ([@CLearERR](https://github.com/CLearERR), [#1662](https://github.com/RaRe-Technologies/gensim/pull/1662))
* Drop Win x32 support & add rolling builds ([@menshikh-iv](https://github.com/menshikh-iv), [#1652](https://github.com/RaRe-Technologies/gensim/pull/1652))
* Fix scoring function in Phrases. Fix #1533, #1635 ([@michaelwsherman](https://github.com/michaelwsherman), [#1573](https://github.com/RaRe-Technologies/gensim/pull/1573))
* Add configuration for flake8 to setup.cfg ([@mcobzarenco](https://github.com/mcobzarenco), [#1636](https://github.com/RaRe-Technologies/gensim/pull/1636))
* Add `build_vocab_from_freq` to Word2Vec, speedup scan\_vocab ([@jodevak](https://github.com/jodevak), [#1599](https://github.com/RaRe-Technologies/gensim/pull/1599))
* Add `most_similar_to_given` method for KeyedVectors ([@TheMathMajor](https://github.com/TheMathMajor), [#1582](https://github.com/RaRe-Technologies/gensim/pull/1582))
* Add `getitem` method to Sparse2Corpus to allow direct queries ([@isamaru](https://github.com/isamaru), [#1621](https://github.com/RaRe-Technologies/gensim/pull/1621))

:red_circle: Bug fixes:
* Add single core mode to CoherenceModel. Fix #1683 ([@horpto](https://github.com/horpto), [#1685](https://github.com/RaRe-Technologies/gensim/pull/1685))
* Fix ResourceWarnings in tests. Partially fix #1519 ([@horpto](https://github.com/horpto), [#1660](https://github.com/RaRe-Technologies/gensim/pull/1660))
* Fix DeprecationWarnings generated by deprecated assertEquals. Partial fix #1519 ([@poornagurram](https://github.com/poornagurram), [#1658](https://github.com/RaRe-Technologies/gensim/pull/1658))
* Fix DeprecationWarnings for regex string literals. Fix #1646 ([@franklsf95](https://github.com/franklsf95), [#1649](https://github.com/RaRe-Technologies/gensim/pull/1649))
* Fix pagerank algorithm. Fix #805 ([@xelez](https://github.com/xelez), [#1653](https://github.com/RaRe-Technologies/gensim/pull/1653))
* Fix FastText inconsistent dtype. Fix #1637 ([@mcobzarenco](https://github.com/mcobzarenco), [#1638](https://github.com/RaRe-Technologies/gensim/pull/1638))
* Fix `test_filename_filtering` test ([@nehaljwani](https://github.com/nehaljwani), [#1647](https://github.com/RaRe-Technologies/gensim/pull/1647))

:books: Tutorial and doc improvements:
* Fix code/docstring style ([@menshikh-iv](https://github.com/menshikh-iv), [#1650](https://github.com/RaRe-Technologies/gensim/pull/1650))
* Update error message for supervised FastText. Fix #1498 ([@ElSaico](https://github.com/ElSaico), [#1645](https://github.com/RaRe-Technologies/gensim/pull/1645))
* Add "DOI badge" to README. Fix #1610 ([@dphov](https://github.com/dphov), [#1639](https://github.com/RaRe-Technologies/gensim/pull/1639))
* Remove duplicate annoy notebook. Fix #1415 ([@Karamax](https://github.com/Karamax), [#1640](https://github.com/RaRe-Technologies/gensim/pull/1640))
* Fix duplication and wrong markup in docs ([@horpto](https://github.com/horpto), [#1633](https://github.com/RaRe-Technologies/gensim/pull/1633))
* Refactor dendrogram & topic network notebooks ([@parulsethi](https://github.com/parulsethi), [#1571](https://github.com/RaRe-Technologies/gensim/pull/1571))
* Fix release badge ([@menshikh-iv](https://github.com/menshikh-iv), [#1631](https://github.com/RaRe-Technologies/gensim/pull/1631))

:warning: Deprecation part (will come into force in the next major release)
* Remove
	- `gensim.examples`
	- `gensim.nosy`
	- `gensim.scripts.word2vec_standalone`
	- `gensim.scripts.make_wiki_lemma`
	- `gensim.scripts.make_wiki_online`
	- `gensim.scripts.make_wiki_online_lemma`
	- `gensim.scripts.make_wiki_online_nodebug`
	- `gensim.scripts.make_wiki`

* Move
	- `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
	- `gensim.summarization` ‚û° `gensim.models.summarization`
	- `gensim.topic_coherence` ‚û° `gensim.models._coherence`
	- `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
	- `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

Also, we'll create `experimental` subpackage for unstable models. Specific lists will be available in the next major release.


## 3.0.1, 2017-10-12


:red_circle: Bug fixes:
* Fix Keras import, speedup importing time. Fix #1614 (@menshikh-v, [#1615](https://github.com/RaRe-Technologies/gensim/pull/1615))
* Fix Sphinx warnings and retreive all missing .rst (@anotherbugmaster and @menshikh-iv, [#1612](https://github.com/RaRe-Technologies/gensim/pull/1612))
* Fix logger message in lsi_dispatcher (@lorosanu, [#1603](https://github.com/RaRe-Technologies/gensim/pull/1603))


:books: Tutorial and doc improvements:
* Fix spelling (@jberkel, [#1625](https://github.com/RaRe-Technologies/gensim/pull/1625))

:warning: Deprecation part (will come into force in the next release)
* Remove
	- `gensim.examples`
	- `gensim.nosy`
	- `gensim.scripts.word2vec_standalone`
	- `gensim.scripts.make_wiki_lemma`
	- `gensim.scripts.make_wiki_online`
	- `gensim.scripts.make_wiki_online_lemma`
	- `gensim.scripts.make_wiki_online_nodebug`
	- `gensim.scripts.make_wiki`

* Move
	- `gensim.scripts.make_wikicorpus` ‚û° `gensim.scripts.make_wiki.py`
	- `gensim.summarization` ‚û° `gensim.models.summarization`
	- `gensim.topic_coherence` ‚û° `gensim.models._coherence`
	- `gensim.utils` ‚û° `gensim.utils.utils` (old imports will continue to work)
	- `gensim.parsing.*` ‚û° `gensim.utils.text_utils`

Also, we'll create `experimental` subpackage for unstable models. Specific lists will be available in the next release.


## 3.0.0, 2017-09-27


:star2: New features:
* Add unsupervised FastText to Gensim (@chinmayapancholi13, [#1525](https://github.com/RaRe-Technologies/gensim/pull/1525))
* Add sklearn API for gensim models (@chinmayapancholi13, [#1462](https://github.com/RaRe-Technologies/gensim/pull/1462))
* Add callback metrics for LdaModel and integration with Visdom (@parulsethi, [#1399](https://github.com/RaRe-Technologies/gensim/pull/1399))
* Add TranslationMatrix model (@robotcator, [#1434](https://github.com/RaRe-Technologies/gensim/pull/1434))
* Add word2vec-based coherence. Fix #1380 (@macks22, [#1530](https://github.com/RaRe-Technologies/gensim/pull/1530))


:+1: Improvements:
* Add 'diagonal' parameter for LdaModel.diff (@parulsethi, [#1448](https://github.com/RaRe-Technologies/gensim/pull/1448))
* Add 'score' function for SklLdaModel (@chinmayapancholi13, [#1445](https://github.com/RaRe-Technologies/gensim/pull/1445))
* Update sklearn API for gensim models (@chinmayapancholi13, [#1473](https://github.com/RaRe-Technologies/gensim/pull/1473)) [:warning: breaks backward compatibility]
* Add CoherenceModel to LdaModel.top_topics. Fix #1128 (@macks22, [#1427](https://github.com/RaRe-Technologies/gensim/pull/1427))
* Add dendrogram viz for topics and JS metric (@parulsethi, [#1484](https://github.com/RaRe-Technologies/gensim/pull/1484))
* Add topic network viz (@parulsethi, [#1536](https://github.com/RaRe-Technologies/gensim/pull/1536))
* Replace viewitems to iteritems. Fix #1495 (@HodorTheCoder, [#1508](https://github.com/RaRe-Technologies/gensim/pull/1508))
* Fix Travis config and add style-checking for Ipython Notebooks. Fix #1518, #1520 (@menshikh-iv, [#1522](https://github.com/RaRe-Technologies/gensim/pull/1522))
* Remove mutable args from definitions. Fix #1561 (@zsef123, [#1562](https://github.com/RaRe-Technologies/gensim/pull/1562))
* Add Appveyour for all PRs. Fix #1565 (@menshikh-iv, [#1565](https://github.com/RaRe-Technologies/gensim/pull/1565))
* Refactor code by PEP8. Partially fix #1521 (@zsef123, [#1550](https://github.com/RaRe-Technologies/gensim/pull/1550))
* Refactor code by PEP8 with additional limitations. Fix #1521 (@menshikh-iv, [#1569](https://github.com/RaRe-Technologies/gensim/pull/1569))
* Update FastTextKeyedVectors.\_\_contains\_\_ (@ELind77, [#1499](https://github.com/RaRe-Technologies/gensim/pull/1499))
* Update WikiCorpus tokenization. Fix #1534 (@roopalgarg, [#1537](https://github.com/RaRe-Technologies/gensim/pull/1537))


:red_circle: Bug fixes:
* Remove round in LdaSeqModel.print_topic. Fix #1480 (@menshikh-iv, [#1547](https://github.com/RaRe-Technologies/gensim/pull/1547))
* Fix TextCorpus.samle_text (@menshikh-iv, [#1548](https://github.com/RaRe-Technologies/gensim/pull/1548))
* Fix Mallet wrapper and tests for HDPTransform (@menshikh-iv, [#1555](https://github.com/RaRe-Technologies/gensim/pull/1555))
* Fix incorrect initialization ShardedCorpus with a generator. Fix #1511 (@karkkainenk1, [#1512](https://github.com/RaRe-Technologies/gensim/pull/1512))
* Add verification when summarize_corpus returns null. Fix #1531 (@fbarrios, [#1570](https://github.com/RaRe-Technologies/gensim/pull/1570))
* Fix doctag unicode problem. Fix 1543 (@englhardt, [#1544](https://github.com/RaRe-Technologies/gensim/pull/1544))
* Fix Translation Matrix (@robotcator, [#1594](https://github.com/RaRe-Technologies/gensim/pull/1594))
* Add trainable flag to KeyedVectors.get_embedding_layer. Fix #1557 (@zsef123, [#1558](https://github.com/RaRe-Technologies/gensim/pull/1558))


:books: Tutorial and doc improvements:
* Update exception text in TextCorpus.samle_text. Partial fix #308 (@vlejd, [#1444](https://github.com/RaRe-Technologies/gensim/pull/1444))
* Remove extra filter_token from tutorial (@VorontsovIE, [#1502](https://github.com/RaRe-Technologies/gensim/pull/1502))
* Update Doc2Vec-IMDB notebook (@pahdo, [#1476](https://github.com/RaRe-Technologies/gensim/pull/1476))
* Add Google Tag Manager for site (@yardos, [#1556](https://github.com/RaRe-Technologies/gensim/pull/1556))
* Update docstring explaining lack of multistream support in WikiCopus. Fix #1496 (@polm and @menshikh-iv, [#1515](https://github.com/RaRe-Technologies/gensim/pull/1515))
* Fix PathLineSentences docstring (@gojomo)
* Fix typos from Translation Matrix notebook (@robotcator, [#1598](https://github.com/RaRe-Technologies/gensim/pull/1598))


## 2.3.0, 2017-07-25


:star2: New features:
* Add Dockerfile for gensim with external wrappers (@parulsethi, [#1368](https://github.com/RaRe-Technologies/gensim/pull/1368))
* Add sklearn wrapper for Word2Vec (@chinmayapancholi13, [#1437](https://github.com/RaRe-Technologies/gensim/pull/1437))
* Add loss function for Word2Vec. Fix #999 (@chinmayapancholi13, [#1201](https://github.com/RaRe-Technologies/gensim/pull/1201))
* Add sklearn wrapper for AuthorTopic model (@chinmayapancholi13, [#1403](https://github.com/RaRe-Technologies/gensim/pull/1403))


:+1: Improvements:
* Remove unittest2 (@souravsingh, [#1490](https://github.com/RaRe-Technologies/gensim/pull/1490))
* Add multiple scoring methods for Phrases. Partial fix #1363 (@michaelwsherman, [#1464](https://github.com/RaRe-Technologies/gensim/pull/1464))
* Add WordRank wrapper to Dockerfile (@parulsethi, [#1460](https://github.com/RaRe-Technologies/gensim/pull/1460))
* Add PathLineSentences. Fix #1364 (@michaelwsherman, [#1423](https://github.com/RaRe-Technologies/gensim/pull/1423))
* Add TextDirectoryCorpus and refactor TextCorpus. Fix #1387 (@macks22, [#1459](https://github.com/RaRe-Technologies/gensim/pull/1459))
* Add sparse input support with topn parameter in any2sparse. Fix #1294 (@manneshiva, [#1321](https://github.com/RaRe-Technologies/gensim/pull/1321))
* Add seed and length for sample_text. Partial fix #308 (@vlejd, [#1422](https://github.com/RaRe-Technologies/gensim/pull/1422))
* Add word_ngram parameter to FastText (@fsonntag, [#1432](https://github.com/RaRe-Technologies/gensim/pull/1432))


:red_circle: Bug fixes:
* Fix fastText loading from .bin file. Fix #1236 (@prakhar2b, [#1341](https://github.com/RaRe-Technologies/gensim/pull/1341))
* Fix paths in WordRank and running gensim version in Dockerfile (@parulsethi, [#1503](https://github.com/RaRe-Technologies/gensim/pull/1503))
* Fix commit version for gensim in Dockerfile (@parulsethi, [#1491](https://github.com/RaRe-Technologies/gensim/pull/1491))
* Fix encoding problems with tests on windows. Fix #1441 (@menshikh-iv, [#1469](https://github.com/RaRe-Technologies/gensim/pull/1469))
* Fix parameters in score_cbow_pair (@jmhessel, [#1468](https://github.com/RaRe-Technologies/gensim/pull/1468))
* Fix parameters in score_sentence_cbow (@jmhessel, [#1467](https://github.com/RaRe-Technologies/gensim/pull/1467))
* Fix TextDirectoryCorpus on windows (@macks22, [#1463](https://github.com/RaRe-Technologies/gensim/pull/1463))
* Fix gensim version in Dockerfile (@parulsethi, [#1456](https://github.com/RaRe-Technologies/gensim/pull/1456))
* Fix WordOccurenceAccumulator on windows. Fix #1441 (@macks22, [#1449](https://github.com/RaRe-Technologies/gensim/pull/1449))
* Fix scipy/numpy requirements (downgrade). Fix #1450 (@menshikh-iv, [#1450](https://github.com/RaRe-Technologies/gensim/pull/1450))


:books: Tutorial and doc improvements:
* Fix links and spaces in quick start guide (@iamsanten, [#1500](https://github.com/RaRe-Technologies/gensim/pull/1500))
* Fix error of ConcatedDoc2Vec in doc2vec-imdb notebook (@robocator, [#1377](https://github.com/RaRe-Technologies/gensim/pull/1377))
* Fix Sphinx warnings. Fix #1192 (@prerna135, [#1442](https://github.com/RaRe-Technologies/gensim/pull/1442))
* Fix typo in LdaModel.diff method (@parulsethi, [#1461](https://github.com/RaRe-Technologies/gensim/pull/1461))
* Add Tensorboard visualization for LDA (@parulsethi, [#1396](https://github.com/RaRe-Technologies/gensim/pull/1396))
* Update old and add new notebook with CoherenceModel (@macks22, [#1431](https://github.com/RaRe-Technologies/gensim/pull/1431))



## 2.2.0, 2017-06-21


:star2: New features:
* Add sklearn wrapper for RpModel (@chinmayapancholi13, [#1395](https://github.com/RaRe-Technologies/gensim/pull/1395))
* Add sklearn wrappers for LdaModel and LsiModel (@chinmayapancholi13, [#1398](https://github.com/RaRe-Technologies/gensim/pull/1398))
* Add sklearn wrapper for LdaSeq (@chinmayapancholi13, [#1405](https://github.com/RaRe-Technologies/gensim/pull/1405))
* Add keras wrapper for Word2Vec model (@chinmayapancholi13, [#1248](https://github.com/RaRe-Technologies/gensim/pull/1248))
* Add LdaModel.diff method (@menshikh-iv, [#1334](https://github.com/RaRe-Technologies/gensim/pull/1334))
* Allow use of truncated Dictionary for coherence measures. Fix #1342 (@macks22, [#1349](https://github.com/RaRe-Technologies/gensim/pull/1349))


:+1: Improvements:
* Fix save_as_text/load_as_text for Dictionary (@vlejd, [#1402](https://github.com/RaRe-Technologies/gensim/pull/1402))
* Add sampling support for corpus. Fix #308 (@vlejd, [#1408](https://github.com/RaRe-Technologies/gensim/pull/1408))
* Add napoleon extension to sphinx (@rasto2211, [#1411](https://github.com/RaRe-Technologies/gensim/pull/1411))
* Add KeyedVectors support to AnnoyIndexer (@quole, [#1318](https://github.com/RaRe-Technologies/gensim/pull/1318))
* Add BaseSklearnWrapper (@chinmayapancholi13, [#1383](https://github.com/RaRe-Technologies/gensim/pull/1383))
* Replace num_words to topn in model for unification. Fix #1198 (@prakhar2b, [#1200](https://github.com/RaRe-Technologies/gensim/pull/1200))
* Rename out_path to out_name & add logging for WordRank model. Fix #1310 (@parulsethi, [#1332](https://github.com/RaRe-Technologies/gensim/pull/1332))
* Remove multiple iterations of corpus in p_boolean_document (@danielchamberlain, [#1325](https://github.com/RaRe-Technologies/gensim/pull/1325))
* Fix codestyle in TfIdf (@piskvorky, [#1313](https://github.com/RaRe-Technologies/gensim/pull/1313))
* Fix warnings from Sphinx. Partial fix #1192 (@souravsingh, [#1330](https://github.com/RaRe-Technologies/gensim/pull/1330))
* Add test_env to setup.py (@menshikh-iv, [#1336](https://github.com/RaRe-Technologies/gensim/pull/1336))


:red_circle: Bug fixes:
* Add cleanup in annoy test (@prakhar2b, [#1420](https://github.com/RaRe-Technologies/gensim/pull/1420))
* Add cleanup in lda backprop test (@prakhar2b, [#1417](https://github.com/RaRe-Technologies/gensim/pull/1417))
* Fix out-of-vocab in FastText (@jayantj, [#1409](https://github.com/RaRe-Technologies/gensim/pull/1409))
* Add cleanup in WordRank test (@parulsethi, [#1410](https://github.com/RaRe-Technologies/gensim/pull/1410))
* Fix rest requirements in Travis. Partial fix #1393 (@ibrahimsharaf, @menshikh-iv, [#1400](https://github.com/RaRe-Technologies/gensim/pull/1400))
* Fix morfessor exception. Partial fix #1324 (@souravsingh, [#1406](https://github.com/RaRe-Technologies/gensim/pull/1406))
* Fix test for FastText (@prakhar2b, [#1371](https://github.com/RaRe-Technologies/gensim/pull/1371))
* Fix WikiCorpus (@alekol, [#1333](https://github.com/RaRe-Technologies/gensim/pull/1333))
* Fix backward incompatibility for LdaModel (@chinmayapancholi13, [#1327](https://github.com/RaRe-Technologies/gensim/pull/1327))
* Fix support for old and new FastText model format. Fix #1301 (@prakhar2b, [#1319](https://github.com/RaRe-Technologies/gensim/pull/1319))
* Fix wrapper tests. Fix #1323 (@shubhamjain74, [#1359](https://github.com/RaRe-Technologies/gensim/pull/1359))
* Update export_phrases method. Fix #794 (@toumorokoshi, [#1362](https://github.com/RaRe-Technologies/gensim/pull/1362))
* Fix sklearn exception in test (@souravsingh, [#1350](https://github.com/RaRe-Technologies/gensim/pull/1350))


:books: Tutorial and doc improvements:
* Fix incorrect link in tutorials (@aneesh-joshi, [#1426](https://github.com/RaRe-Technologies/gensim/pull/1426))
* Add notebook with sklearn wrapper examples (@chinmayapancholi13, [#1428](https://github.com/RaRe-Technologies/gensim/pull/1428))
* Replace absolute pathes to relative in notebooks (@vochicong, [#1414](https://github.com/RaRe-Technologies/gensim/pull/1414))
* Fix code-style in keras notebook (@chinmayapancholi13, [#1394](https://github.com/RaRe-Technologies/gensim/pull/1394))
* Replace absolute pathes to relative in notebooks (@vochicong, [#1407](https://github.com/RaRe-Technologies/gensim/pull/1407))
* Fix typo in quickstart guide (@vochicong, [#1404](https://github.com/RaRe-Technologies/gensim/pull/1404))
* Update docstring for WordRank. Fix #1384 (@parulsethi, [#1378](https://github.com/RaRe-Technologies/gensim/pull/1378))
* Update docstring for SkLdaModel (@chinmayapancholi13, [#1382](https://github.com/RaRe-Technologies/gensim/pull/1382))
* Update logic for updatetype in LdaModel (@chinmayapancholi13, [#1389](https://github.com/RaRe-Technologies/gensim/pull/1389))
* Update docstring for Doc2Vec (@jstol, [#1379](https://github.com/RaRe-Technologies/gensim/pull/1379))
* Fix docstring for KL-distance (@viciousstar, [#1373](https://github.com/RaRe-Technologies/gensim/pull/1373))
* Update Corpora_and_Vector_Spaces tutorial (@charliejharrison, [#1308](https://github.com/RaRe-Technologies/gensim/pull/1308))
* Add visualization for difference between LdaModel (@menshikh-iv, [#1374](https://github.com/RaRe-Technologies/gensim/pull/1374))
* Fix punctuation & typo in changelog (@piskvorky, @menshikh-iv, [#1366](https://github.com/RaRe-Technologies/gensim/pull/1366))
* Fix PEP8 & typo in several PRs (@menshikh-iv, [#1369](https://github.com/RaRe-Technologies/gensim/pull/1369))
* Update docstrings connected with backward compability in for LdaModel (@chinmayapancholi13, [#1365](https://github.com/RaRe-Technologies/gensim/pull/1365))
* Update Corpora_and_Vector_Spaces tutorial (@schuyler1d, [#1360](https://github.com/RaRe-Technologies/gensim/pull/1360))
* Fix typo in Doc2Vec doctsring (@fujiyuu75, [#1356](https://github.com/RaRe-Technologies/gensim/pull/1356))
* Update Annoy tutorial (@pmbaumgartner, [#1355](https://github.com/RaRe-Technologies/gensim/pull/1355))
* Update temp folder in tutorials (@yl2526, [#1352](https://github.com/RaRe-Technologies/gensim/pull/1352))
* Remove spaces after print in Topics_and_Transformation tutorial (@gsimore, [#1354](https://github.com/RaRe-Technologies/gensim/pull/1354))
* Update Dictionary docstring (@oonska, [#1347](https://github.com/RaRe-Technologies/gensim/pull/1347))
* Add section headings in word2vec notebook (@MikeTheReader, [#1348](https://github.com/RaRe-Technologies/gensim/pull/1348))
* Fix broken urls in starter tutorials (@ka7eh, [#1346](https://github.com/RaRe-Technologies/gensim/pull/1346))
* Update quick start notebook (@yardsale8, [#1345](https://github.com/RaRe-Technologies/gensim/pull/1345))
* Fix typo in quick start notebook (@MikeTheReader, [#1344](https://github.com/RaRe-Technologies/gensim/pull/1344))
* Fix docstring in keyedvectors (@chinmayapancholi13, [#1337](https://github.com/RaRe-Technologies/gensim/pull/1337))



## 2.1.0, 2017-05-12

:star2: New features:
* Add modified save_word2vec_format for Doc2Vec, to save document vectors. (@parulsethi, [#1256](https://github.com/RaRe-Technologies/gensim/pull/1256))


:+1: Improvements:
* Add automatic code style check limited only to the code modified in PR (@tmylk, [#1287](https://github.com/RaRe-Technologies/gensim/pull/1287))
* Replace `logger.warn` by `logger.warning` (@chinmayapancholi13, [#1295](https://github.com/RaRe-Technologies/gensim/pull/1295))
* Docs word2vec docstring improvement, deprecation labels (@shubhvachher, [#1274](https://github.com/RaRe-Technologies/gensim/pull/1274))
* Stop passing 'sentences' as parameter to Doc2Vec. Fix #511 (@gogokaradjov, [#1306](https://github.com/RaRe-Technologies/gensim/pull/1306))


:red_circle: Bug fixes:
* Allow indexing with np.int64 in doc2vec. Fix #1231 (@bogdanteleaga, [#1254](https://github.com/RaRe-Technologies/gensim/pull/1254))
* Update Doc2Vec docstring. Fix #1302 (@datapythonista, [#1307](https://github.com/RaRe-Technologies/gensim/pull/1307))
* Ignore rst and ipynb file in Travis flake8 validations (@datapythonista, [#1309](https://github.com/RaRe-Technologies/gensim/pull/1309))


:books: Tutorial and doc improvements:
* Update Tensorboard Doc2Vec notebook (@parulsethi, [#1286](https://github.com/RaRe-Technologies/gensim/pull/1286))
* Update Doc2Vec IMDB Notebook, replace codesc to smart_open (@robotcator, [#1278](https://github.com/RaRe-Technologies/gensim/pull/1278))
* Add explanation of `size` to Word2Vec Notebook (@jbcoe, [#1305](https://github.com/RaRe-Technologies/gensim/pull/1305))
* Add extra param to WordRank notebook. Fix #1276 (@parulsethi, [#1300](https://github.com/RaRe-Technologies/gensim/pull/1300))
* Update warning message in WordRank (@parulsethi, [#1299](https://github.com/RaRe-Technologies/gensim/pull/1299))


## 2.0.0, 2017-04-10

Breaking changes:

Any direct calls to method train() of Word2Vec/Doc2Vec now require an explicit epochs parameter and explicit estimate of corpus size. The most usual way to call `train` is `vec_model.train(sentences, total_examples=self.corpus_count, epochs=self.iter)`
See the [method documentation](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L766) for more information.


* Explicit epochs and corpus size in word2vec train(). (@gojomo, @robotcator, [#1139](https://github.com/RaRe-Technologies/gensim/pull/1139), [#1237](https://github.com/RaRe-Technologies/gensim/pull/1237))

New features:
* Add output word prediction in word2vec. Only for negative sampling scheme. See [ipynb]( https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb) (@chinmayapancholi13, [#1209](https://github.com/RaRe-Technologies/gensim/pull/1209))
* scikit_learn wrapper for LSI Model in Gensim (@chinmayapancholi13, [#1244](https://github.com/RaRe-Technologies/gensim/pull/1244))
* Add the 'keep_tokens' parameter to 'filter_extremes'. (@toliwa, [#1210](https://github.com/RaRe-Technologies/gensim/pull/1210))
* Load FastText models with specified encoding (@jayantj, [#1210](https://github.com/RaRe-Technologies/gensim/pull/1189))


Improvements:
* Fix loading large FastText models on Mac. (@jaksmid, [#1196](https://github.com/RaRe-Technologies/gensim/pull/1214))
* Sklearn LDA wrapper now works in sklearn pipeline (@kris-singh, [#1213](https://github.com/RaRe-Technologies/gensim/pull/1213))
* glove2word2vec conversion script refactoring (@parulsethi, [#1247](https://github.com/RaRe-Technologies/gensim/pull/1247))
* Word2vec error message when update called before train . Fix #1162 (@hemavakade, [#1205](https://github.com/RaRe-Technologies/gensim/pull/1205))
* Allow training if model is not modified by "_minimize_model". Add deprecation warning. (@chinmayapancholi13, [#1207](https://github.com/RaRe-Technologies/gensim/pull/1207))
* Update the warning text when building vocab on a trained w2v model (@prakhar2b, [#1190](https://github.com/RaRe-Technologies/gensim/pull/1190))

Bug fixes:

*  Fix word2vec reset_from bug in v1.0.1 Fix #1230. (@Kreiswolke, [#1234](https://github.com/RaRe-Technologies/gensim/pull/1234))

* Distributed LDA: checking the length of docs instead of the boolean value, plus int index conversion (@saparina, [#1191](https://github.com/RaRe-Technologies/gensim/pull/1191))

* syn0_lockf initialised with zero in intersect_word2vec_format() (@KiddoZhu, [#1267](https://github.com/RaRe-Technologies/gensim/pull/1267))

* Fix wordrank max_iter_dump calculation. Fix #1216 (@ajkl, [#1217](https://github.com/RaRe-Technologies/gensim/pull/1217))

* Make SgNegative test use skip-gram (@shubhvachher, [#1252](https://github.com/RaRe-Technologies/gensim/pull/1252))

* pep8/pycodestyle fixes for hanging indents in Summarization module (@SamriddhiJain, [#1202](https://github.com/RaRe-Technologies/gensim/pull/1202))

* WordRank and Mallet wrappers single vs double quote issue in windows. (@prakhar2b, [#1208](https://github.com/RaRe-Technologies/gensim/pull/1208))


* Fix #824 : no corpus in init, but trim_rule in init (@prakhar2b, [#1186](https://github.com/RaRe-Technologies/gensim/pull/1186))

* Hardcode version number. Fix #1138. (@tmylk, [#1138](https://github.com/RaRe-Technologies/gensim/pull/1138))

Tutorial and doc improvements:

* Color dictionary according to topic notebook update (@bhargavvader, [#1164](https://github.com/RaRe-Technologies/gensim/pull/1164))

* Fix hdp show_topic/s docstring (@parulsethi, [#1264](https://github.com/RaRe-Technologies/gensim/pull/1264))

* Add docstrings for word2vec.py forwarding functions (@shubhvachher, [#1251](https://github.com/RaRe-Technologies/gensim/pull/1251))

* updated description for worker_loop function used in score function (@chinmayapancholi13, [#1206](https://github.com/RaRe-Technologies/gensim/pull/1206))

## 1.0.1, 2017-03-03

* Rebuild cumulative table on load. Fix #1180. (@tmylk, [#1181](https://github.com/RaRe-Technologies/gensim/pull/893))
* most_similar_cosmul bug fix (@dkim010, [#1177](https://github.com/RaRe-Technologies/gensim/pull/1177))
* Fix loading old word2vec models pre-1.0.0  (@jayantj, [#1179](https://github.com/RaRe-Technologies/gensim/pull/1179))
* Load utf-8 words in fasttext  (@jayantj, [#1176](https://github.com/RaRe-Technologies/gensim/pull/1176))


## 1.0.0, 2017-02-24

New features:
* Add Author-topic modeling (@olavurmortensen, [#893](https://github.com/RaRe-Technologies/gensim/pull/893))
* Add FastText word embedding wrapper (@Jayantj, [#847](https://github.com/RaRe-Technologies/gensim/pull/847))
* Add WordRank word embedding  wrapper (@parulsethi, [#1066](https://github.com/RaRe-Technologies/gensim/pull/1066), [#1125](https://github.com/RaRe-Technologies/gensim/pull/1125))
* Add VarEmbed word embedding wrapper (@anmol01gulati, [#1067](https://github.com/RaRe-Technologies/gensim/pull/1067)))
* Add sklearn wrapper for LDAModel (@AadityaJ, [#932](https://github.com/RaRe-Technologies/gensim/pull/932))

Deprecated features:

* Move `load_word2vec_format` and `save_word2vec_format` out of Word2Vec class to KeyedVectors (@tmylk, [#1107](https://github.com/RaRe-Technologies/gensim/pull/1107))
* Move properties `syn0norm`, `syn0`, `vocab`, `index2word` from Word2Vec class to KeyedVectors (@tmylk,[#1147](https://github.com/RaRe-Technologies/gensim/pull/1147))
* Remove support for Python 2.6, 3.3 and 3.4 (@tmylk,[#1145](https://github.com/RaRe-Technologies/gensim/pull/1145))


Improvements:

* Python 3.6 support (@tmylk [#1077](https://github.com/RaRe-Technologies/gensim/pull/1077))
* Phrases and Phraser allow a generator corpus (ELind77 [#1099](https://github.com/RaRe-Technologies/gensim/pull/1099))
* Ignore DocvecsArray.doctag_syn0norm in save. Fix #789 (@accraze, [#1053](https://github.com/RaRe-Technologies/gensim/pull/1053))
* Fix bug in LsiModel that occurs when id2word is a Python 3 dictionary. (@cvangysel, [#1103](https://github.com/RaRe-Technologies/gensim/pull/1103)
* Fix broken link to paper in readme (@bhargavvader, [#1101](https://github.com/RaRe-Technologies/gensim/pull/1101))
* Lazy formatting in evaluate_word_pairs (@akutuzov, [#1084](https://github.com/RaRe-Technologies/gensim/pull/1084))
* Deacc option to keywords pre-processing (@bhargavvader, [#1076](https://github.com/RaRe-Technologies/gensim/pull/1076))
* Generate Deprecated exception when using Word2Vec.load_word2vec_format (@tmylk, [#1165](https://github.com/RaRe-Technologies/gensim/pull/1165))
* Fix hdpmodel constructor docstring for print_topics (#1152) (@toliwa, [#1152](https://github.com/RaRe-Technologies/gensim/pull/1152))
* Default to per_word_topics=False in LDA get_item for performance (@menshikh-iv, [#1154](https://github.com/RaRe-Technologies/gensim/pull/1154))
* Fix bound computation in Author Topic models. (@olavurmortensen, [#1156](https://github.com/RaRe-Technologies/gensim/pull/1156))
* Write UTF-8 byte strings in tensorboard conversion (@tmylk, [#1144](https://github.com/RaRe-Technologies/gensim/pull/1144))
* Make top_topics and sparse2full compatible with numpy 1.12 strictly int idexing (@tmylk, [#1146](https://github.com/RaRe-Technologies/gensim/pull/1146))

Tutorial and doc improvements:

* Clarifying comment in is_corpus func in utils.py (@greninja, [#1109](https://github.com/RaRe-Technologies/gensim/pull/1109))
* Tutorial Topics_and_Transformations fix markdown and add references (@lgmoneda, [#1120](https://github.com/RaRe-Technologies/gensim/pull/1120))
* Fix doc2vec-lee.ipynb results to match previous behavior (@bahbbc, [#1119](https://github.com/RaRe-Technologies/gensim/pull/1119))
* Remove Pattern lib dependency in News Classification tutorial (@luizcavalcanti, [#1118](https://github.com/RaRe-Technologies/gensim/pull/1118))
* Corpora_and_Vector_Spaces tutorial text clarification (@lgmoneda, [#1116](https://github.com/RaRe-Technologies/gensim/pull/1116))
* Update Transformation and Topics link from quick start notebook (@mariana393, [#1115](https://github.com/RaRe-Technologies/gensim/pull/1115))
* Quick Start Text clarification and typo correction (@luizcavalcanti, [#1114](https://github.com/RaRe-Technologies/gensim/pull/1114))
* Fix typos in Author-topic tutorial (@Fil, [#1102](https://github.com/RaRe-Technologies/gensim/pull/1102))
* Address benchmark inconsistencies in Annoy tutorial (@droudy, [#1113](https://github.com/RaRe-Technologies/gensim/pull/1113))
* Add note about Annoy speed depending on numpy BLAS setup in annoytutorial.ipynb (@greninja, [#1137](https://github.com/RaRe-Technologies/gensim/pull/1137))
* Fix dependencies description on doc2vec-IMDB notebook (@luizcavalcanti, [#1132](https://github.com/RaRe-Technologies/gensim/pull/1132))
* Add documentation for WikiCorpus metadata. (@kirit93, [#1163](https://github.com/RaRe-Technologies/gensim/pull/1163))


## 1.0.0RC2, 2017-02-16

* Add note about Annoy speed depending on numpy BLAS setup in annoytutorial.ipynb (@greninja, [#1137](https://github.com/RaRe-Technologies/gensim/pull/1137))
* Remove direct access to properties moved to KeyedVectors (@tmylk, [#1147](https://github.com/RaRe-Technologies/gensim/pull/1147))
* Remove support for Python 2.6, 3.3 and 3.4 (@tmylk, [#1145](https://github.com/RaRe-Technologies/gensim/pull/1145))
* Write UTF-8 byte strings in tensorboard conversion (@tmylk, [#1144](https://github.com/RaRe-Technologies/gensim/pull/1144))
* Make top_topics and sparse2full compatible with numpy 1.12 strictly int idexing (@tmylk, [#1146](https://github.com/RaRe-Technologies/gensim/pull/1146))

## 1.0.0RC1, 2017-01-31

New features:
* Add Author-topic modeling (@olavurmortensen, [#893](https://github.com/RaRe-Technologies/gensim/pull/893))
* Add FastText word embedding wrapper (@Jayantj, [#847](https://github.com/RaRe-Technologies/gensim/pull/847))
* Add WordRank word embedding  wrapper (@parulsethi, [#1066](https://github.com/RaRe-Technologies/gensim/pull/1066), [#1125](https://github.com/RaRe-Technologies/gensim/pull/1125))
* Add sklearn wrapper for LDAModel (@AadityaJ, [#932](https://github.com/RaRe-Technologies/gensim/pull/932))

Improvements:
* Python 3.6 support (@tmylk [#1077](https://github.com/RaRe-Technologies/gensim/pull/1077))
* Phrases and Phraser allow a generator corpus (ELind77 [#1099](https://github.com/RaRe-Technologies/gensim/pull/1099))
* Ignore DocvecsArray.doctag_syn0norm in save. Fix #789 (@accraze, [#1053](https://github.com/RaRe-Technologies/gensim/pull/1053))
* Move load and save word2vec_format out of word2vec class to KeyedVectors  (@tmylk, [#1107](https://github.com/RaRe-Technologies/gensim/pull/1107))
* Fix bug in LsiModel that occurs when id2word is a Python 3 dictionary. (@cvangysel, [#1103](https://github.com/RaRe-Technologies/gensim/pull/1103)
* Fix broken link to paper in readme (@bhargavvader, [#1101](https://github.com/RaRe-Technologies/gensim/pull/1101))
* Lazy formatting in evaluate_word_pairs (@akutuzov, [#1084](https://github.com/RaRe-Technologies/gensim/pull/1084))
* Deacc option to keywords pre-processing (@bhargavvader, [#1076](https://github.com/RaRe-Technologies/gensim/pull/1076))

Tutorial and doc improvements:

* Clarifying comment in is_corpus func in utils.py (@greninja, [#1109](https://github.com/RaRe-Technologies/gensim/pull/1109))
* Tutorial Topics_and_Transformations fix markdown and add references (@lgmoneda, [#1120](https://github.com/RaRe-Technologies/gensim/pull/1120))
* Fix doc2vec-lee.ipynb results to match previous behavior (@bahbbc, [#1119](https://github.com/RaRe-Technologies/gensim/pull/1119))
* Remove Pattern lib dependency in News Classification tutorial (@luizcavalcanti, [#1118](https://github.com/RaRe-Technologies/gensim/pull/1118))
* Corpora_and_Vector_Spaces tutorial text clarification (@lgmoneda, [#1116](https://github.com/RaRe-Technologies/gensim/pull/1116))
* Update Transformation and Topics link from quick start notebook (@mariana393, [#1115](https://github.com/RaRe-Technologies/gensim/pull/1115))
* Quick Start Text clarification and typo correction (@luizcavalcanti, [#1114](https://github.com/RaRe-Technologies/gensim/pull/1114))
* Fix typos in Author-topic tutorial (@Fil, [#1102](https://github.com/RaRe-Technologies/gensim/pull/1102))
* Address benchmark inconsistencies in Annoy tutorial (@droudy, [#1113](https://github.com/RaRe-Technologies/gensim/pull/1113))


## 0.13.4.1, 2017-01-04

* Disable direct access warnings on save and load of Word2vec/Doc2vec (@tmylk, [#1072](https://github.com/RaRe-Technologies/gensim/pull/1072))
* Making Default hs error explicit (@accraze, [#1054](https://github.com/RaRe-Technologies/gensim/pull/1054))
* Removed unnecessary numpy imports (@bhargavvader, [#1065](https://github.com/RaRe-Technologies/gensim/pull/1065))
* Utils and Matutils changes (@bhargavvader, [#1062](https://github.com/RaRe-Technologies/gensim/pull/1062))
* Tests for the evaluate_word_pairs function (@akutuzov, [#1061](https://github.com/RaRe-Technologies/gensim/pull/1061))

## 0.13.4, 2016-12-22

* Added suggested lda model method and print methods to HDP class (@bhargavvader, [#1055](https://github.com/RaRe-Technologies/gensim/pull/1055))
* New class KeyedVectors to store embedding separate from training code (@anmol01gulati and @droudy, [#980](https://github.com/RaRe-Technologies/gensim/pull/980))
* Evaluation of word2vec models against semantic similarity datasets like SimLex-999 (@akutuzov, [#1047](https://github.com/RaRe-Technologies/gensim/pull/1047))
* TensorBoard word embedding visualisation of Gensim Word2vec format (@loretoparisi, [#1051](https://github.com/RaRe-Technologies/gensim/pull/1051))
* Throw exception if load() is called on instance rather than the class in word2vec and doc2vec (@dust0x, [#889](https://github.com/RaRe-Technologies/gensim/pull/889))
* Loading and Saving LDA Models across Python 2 and 3. Fix #853 (@anmolgulati, [#913](https://github.com/RaRe-Technologies/gensim/pull/913), [#1093](https://github.com/RaRe-Technologies/gensim/pull/1093))
* Fix automatic learning of eta (prior over words) in LDA (@olavurmortensen, [#1024](https://github.com/RaRe-Technologies/gensim/pull/1024)).
    * eta should have dimensionality V (size of vocab) not K (number of topics). eta with shape K x V is still allowed, as the user may want to impose specific prior information to each topic.
    * eta is no longer allowed the "asymmetric" option. Asymmetric priors over words in general are fine (learned or user defined).
    * As a result, the eta update (`update_eta`) was simplified some. It also no longer logs eta when updated, because it is too large for that.
    * Unit tests were updated accordingly. The unit tests expect a different shape than before; some unit tests were redundant after the change; `eta='asymmetric'` now should raise an error.
* Optimise show_topics to only call get_lambda once. Fix #1006. (@bhargavvader, [#1028](https://github.com/RaRe-Technologies/gensim/pull/1028))
* HdpModel doc improvement. Inference and print_topics (@dsquareindia, [#1029](https://github.com/RaRe-Technologies/gensim/pull/1029))
* Removing Doc2Vec defaults so that it won't override Word2Vec defaults. Fix #795. (@markroxor, [#929](https://github.com/RaRe-Technologies/gensim/pull/929))
* Remove warning on gensim import "pattern not installed". Fix #1009 (@shashankg7, [#1018](https://github.com/RaRe-Technologies/gensim/pull/1018))
* Add delete_temporary_training_data() function to word2vec and doc2vec models. (@deepmipt-VladZhukov, [#987](https://github.com/RaRe-Technologies/gensim/pull/987))
* Documentation improvements (@IrinaGoloshchapova, [#1010](https://github.com/RaRe-Technologies/gensim/pull/1010), [#1011](https://github.com/RaRe-Technologies/gensim/pull/1011))
* LDA tutorial by Olavur, tips and tricks (@olavurmortensen, [#779](https://github.com/RaRe-Technologies/gensim/pull/779))
* Add double quote in commmand line to run on Windows (@akarazeev, [#1005](https://github.com/RaRe-Technologies/gensim/pull/1005))
* Fix directory names in notebooks to be OS-independent (@mamamot, [#1004](https://github.com/RaRe-Technologies/gensim/pull/1004))
* Respect clip_start, clip_end in most_similar. Fix #601. (@parulsethi, [#994](https://github.com/RaRe-Technologies/gensim/pull/994))
* Replace Python sigmoid function with scipy in word2vec & doc2vec (@markroxor, [#989](https://github.com/RaRe-Technologies/gensim/pull/989))
* WMD to return 0 instead of inf for sentences that contain a single word (@rbahumi, [#986](https://github.com/RaRe-Technologies/gensim/pull/986))
* Pass all the params through the apply call in lda.get_document_topics(), test case to use the per_word_topics through the corpus in test_ldamodel (@parthoiiitm, [#978](https://github.com/RaRe-Technologies/gensim/pull/978))
* Pyro annotations for lsi_worker (@markroxor, [#968](https://github.com/RaRe-Technologies/gensim/pull/968))


## 0.13.3, 2016-10-20

* Add vocabulary expansion feature to word2vec. (@isohyt, [#900](https://github.com/RaRe-Technologies/gensim/pull/900))
* Tutorial: Reproducing Doc2vec paper result on wikipedia. (@isohyt, [#654](https://github.com/RaRe-Technologies/gensim/pull/654))
* Add Save/Load interface to AnnoyIndexer for index persistence (@fortiema, [#845](https://github.com/RaRe-Technologies/gensim/pull/845))
* Fixed issue [#938](https://github.com/RaRe-Technologies/gensim/issues/938),Creating a unified base class for all topic models. ([@markroxor](https://github.com/markroxor), [#946](https://github.com/RaRe-Technologies/gensim/pull/946))
    -  breaking change in `HdpTopicFormatter.show_topics`
* Add Phraser for Phrases optimization. ( @gojomo & @anujkhare , [#837](https://github.com/RaRe-Technologies/gensim/pull/837))
* Fix issue #743, in word2vec's n_similarity method if at least one empty list is passed ZeroDivisionError is raised (@pranay360, [#883](https://github.com/RaRe-Technologies/gensim/pull/883))
* Change export_phrases in Phrases model. Fix issue #794 (@AadityaJ, [#879](https://github.com/RaRe-Technologies/gensim/pull/879))
    - bigram construction can now support multiple bigrams within one sentence
* Fix issue [#838](https://github.com/RaRe-Technologies/gensim/issues/838), RuntimeWarning: overflow encountered in exp ([@markroxor](https://github.com/markroxor), [#895](https://github.com/RaRe-Technologies/gensim/pull/895))
*  Change some log messages to warnings as suggested in issue #828. (@rhnvrm, [#884](https://github.com/RaRe-Technologies/gensim/pull/884))
*  Fix issue #851, In summarizer.py, RunTimeError is raised if single sentence input is provided to avoid ZeroDivionError. (@metalaman, #887)
* Fix issue [#791](https://github.com/RaRe-Technologies/gensim/issues/791), correct logic for iterating over SimilarityABC interface. ([@MridulS](https://github.com/MridulS), [#839](https://github.com/RaRe-Technologies/gensim/pull/839))
* Fix RP model loading for large Fortran-order arrays (@piskvorky, [#605](https://github.com/RaRe-Technologies/gensim/issues/938))
* Remove ShardedCorpus from init because of Theano dependency (@tmylk, [#919](https://github.com/RaRe-Technologies/gensim/pull/919))
* Documentation improvements ( @dsquareindia & @tmylk, [#914](https://github.com/RaRe-Technologies/gensim/pull/914), [#906](https://github.com/RaRe-Technologies/gensim/pull/906) )
* Add Annoy memory-mapping example (@harshul1610, [#899](https://github.com/RaRe-Technologies/gensim/pull/899))
* Fixed issue [#601](https://github.com/RaRe-Technologies/gensim/issues/601), correct docID in most_similar for clip range (@parulsethi, [#994](https://github.com/RaRe-Technologies/gensim/pull/994))

## 0.13.2, 2016-08-19

* wordtopics has changed to word_topics in ldamallet, and fixed issue #764. (@bhargavvader, [#771](https://github.com/RaRe-Technologies/gensim/pull/771))
  - assigning wordtopics value of word_topics to keep backward compatibility, for now
* topics, topn parameters changed to num_topics and num_words in show_topics() and print_topics() (@droudy, [#755](https://github.com/RaRe-Technologies/gensim/pull/755))
  - In hdpmodel and dtmmodel
  - NOT BACKWARDS COMPATIBLE!
* Added random_state parameter to LdaState initializer and check_random_state() (@droudy, [#113](https://github.com/RaRe-Technologies/gensim/pull/113))
* Topic coherence update with `c_uci`, `c_npmi` measures.  LdaMallet, LdaVowpalWabbit support. Add `topics` parameter to coherencemodel. Can now provide tokenized topics to calculate coherence value. Faster backtracking. (@dsquareindia, [#750](https://github.com/RaRe-Technologies/gensim/pull/750), [#793](https://github.com/RaRe-Technologies/gensim/pull/793))
* Added a check for empty (no words) documents before starting to run the DTM wrapper if model = "fixed" is used (DIM model) as this    causes the an error when such documents are reached in training. (@eickho, [#806](https://github.com/RaRe-Technologies/gensim/pull/806))
* New parameters `limit`, `datatype` for load_word2vec_format(); `lockf` for intersect_word2vec_format (@gojomo, [#817](https://github.com/RaRe-Technologies/gensim/pull/817))
* Changed `use_lowercase` option in word2vec accuracy to `case_insensitive` to account for case variations in training vocabulary (@jayantj, [#804](https://github.com/RaRe-Technologies/gensim/pull/804)
* Link to Doc2Vec on airline tweets example in tutorials page (@544895340, [#823](https://github.com/RaRe-Technologies/gensim/pull/823))
* Small error on Doc2vec notebook tutorial (@charlessutton, [#816](https://github.com/RaRe-Technologies/gensim/pull/816))
* Bugfix: Full2sparse clipped to use abs value (@tmylk, [#811](https://github.com/RaRe-Technologies/gensim/pull/811))
* WMD docstring: add tutorial link and query example (@tmylk, [#813](https://github.com/RaRe-Technologies/gensim/pull/813))
* Annoy integration to speed word2vec and doc2vec similarity. Tutorial update (@droudy, [#799](https://github.com/RaRe-Technologies/gensim/pull/799),[#792](https://github.com/RaRe-Technologies/gensim/pull/799) )
* Add converter of LDA model between Mallet, Vowpal Wabit and gensim (@dsquareindia, [#798](https://github.com/RaRe-Technologies/gensim/pull/798), [#766](https://github.com/RaRe-Technologies/gensim/pull/766))
* Distributed LDA in different network segments without broadcast (@menshikh-iv, [#782](https://github.com/RaRe-Technologies/gensim/pull/782))
* Update Corpora_and_Vector_Spaces.ipynb (@megansquire, [#772](https://github.com/RaRe-Technologies/gensim/pull/772))
* DTM wrapper bug fixes caused by renaming num_words in #755 (@bhargavvader, [#770](https://github.com/RaRe-Technologies/gensim/pull/770))
* Add LsiModel.docs_processed attribute (@hobson, [#763](https://github.com/RaRe-Technologies/gensim/pull/763))
* Dynamic Topic Modelling in Python. Google Summer of Code 2016 project. (@bhargavvader, [#739](https://github.com/RaRe-Technologies/gensim/pull/739), [#831](https://github.com/RaRe-Technologies/gensim/pull/831))

## 0.13.1, 2016-06-22

* Topic coherence C_v and U_mass (@dsquareindia, #710)

## 0.13.0, 2016-06-21

* Added Distance Metrics to matutils.pt (@bhargavvader, #656)
* Tutorials migrated from website to ipynb (@j9chan, #721), (@jesford, #733), (@jesford, #725), (@jesford, #716)
* New doc2vec intro tutorial (@seanlaw, #730)
* Gensim Quick Start Tutorial (@andrewjlm, #727)
* Add export_phrases(sentences) to model Phrases (hanabi1224 #588)
* SparseMatrixSimilarity returns a sparse matrix if `maintain_sparsity` is True (@davechallis, #590)
* added functionality for Topics of Words in document - i.e, dynamic topics. (@bhargavvader, #704)
  - also included tutorial which explains new functionalities, and document word-topic colring.
* Made normalization an explicit transformation. Added 'l1' norm support (@dsquareindia, #649)
* added term-topics API for most probable topic for word in vocab. (@bhargavvader, #706)
* build_vocab takes progress_per parameter for smaller output (@zer0n, #624)
* Control whether to use lowercase for computing word2vec accuracy. (@alantian, #607)
* Easy import of GloVe vectors using Gensim (Manas Ranjan Kar, #625)
  - Allow easy port of GloVe vectors into Gensim
  - Standalone script with command line arguments, compatible with Python>=2.6
  - Usage: python -m gensim.scripts.glove2word2vec -i glove_vectors.txt -o output_word2vec_compatible.txt
* Add `similar_by_word()` and `similar_by_vector()` to word2vec (@isohyt, #381)
* Convenience method for similarity of two out of training sentences to doc2vec (@ellolo, #707)
* Dynamic Topic Modelling Tutorial updated with Dynamic Influence Model (@bhargavvader, #689)
* Added function to filter 'n' most frequent words from the dictionary (@abhinavchawla, #718)
* Raise warnings if vocab is single character elements and if alpha is increased in word2vec/doc2vec (@dsquareindia, #705)
* Tests for wikidump (@jonmcoe, #723)
* Mallet wrapper sparse format support (@RishabGoel, #664)
* Doc2vec pre-processing script translated from bash to Python (@andrewjlm, #720)


## 0.12.4, 2016-01-29

* Better internal handling of job batching in word2vec (#535)
  - up to 300% speed up when training on very short documents (~tweets)
* Word2vec CLI in line with original word2vec.c (Andrey Kutuzov, #538)
  - Same default values. See diff https://github.com/akutuzov/gensim/commit/6456cbcd75e6f8720451766ba31cc046b4463ae2
  - Standalone script with command line arguments matching those of original C tool.
  - Usage: python -m gensim.scripts.word2vec_standalone -train data.txt -output trained_vec.txt -size 200 -window 2 -sample 1e-4
* Improved load_word2vec_format() performance (@svenkreiss, #555)
  - Remove `init_sims()` call for performance improvements when normalized vectors are not needed.
  - Remove `norm_only` parameter (API change). Call `init_sims(replace=True)` after the `load_word2vec_format()` call for the old `norm_only=True` behavior.
* Word2vec allows non-strict unicode error handling (ignore or replace) (Gordon Mohr, #466)
* Doc2Vec `model.docvecs[key]` now raises KeyError for unknown keys (Gordon Mohr, #520)
* Fix `DocvecsArray.index_to_doctag` so `most_similar()` returns string doctags (Gordon Mohr, #560)
* On-demand loading of the `pattern` library in utils.lemmatize (Jan Zikes, #461)
  - `utils.HAS_PATTERN` flag moved to `utils.has_pattern()`
* Threadsafe Word2Vec/Doc2Vec finish-check to avoid hang/unending Word2Vec/Doc2Vec training (Gordon Mohr, #571)
* Tuned `TestWord2VecModel.test_cbow_hs()` against random failures (Gordon Mohr, #531)
* Prevent ZeroDivisionError when `default_timer()` indicate no elapsed time (Gordon Mohr, #518)
* Forwards compatibility for NumPy > 1.10 (Matti Lyra, #494, #513)
  - LdaModel and LdaMulticore produce a large number of DeprecationWarnings from
    .inference() because the term ids in each chunk returned from utils.grouper
    are floats. This behaviour has been changed so that the term IDs are now ints.
  - utils.grouper returns a python list instead of a numpy array in .update() when
    LdaModel is called in non distributed mode
  - in distributed mode .update() will still call utils.grouper with as_numpy=True
    to save memory
  - LdaModel.update and LdaMulticore.update have a new keyword parameter
    chunks_as_numpy=True/False (defaults to False) that allows controlling
    this behaviour

## 0.12.3, 2015-11-05

* Make show_topics return value consistent across models (Christopher Corley, #448)
  - All models with the `show_topics` method should return a list of
    `(topic_number, topic)` tuples, where `topic` is a list of
    `(word, probability)` tuples.
  - This is a breaking change that affects users of the `LsiModel`, `LdaModel`,
  and `LdaMulticore` that may be reliant on the old tuple layout of
  `(probability, word)`.
* Mixed integer & string document-tags (keys to doc-vectors) will work (Gordon Mohr, #491)
  - DocvecsArray's `index2doctag` list is renamed/reinterpreted as `offset2doctag`
  - `offset2doctag` entries map to `doctag_syn0` indexes *after* last plain-int doctag (if any)
  - (If using only string doctags, `offset2doctag` may be interpreted same as `index2doctag`.)
* New Tutorials on Dynamic Topic Modelling and Classification via Word2Vec (@arttii #471, @mataddy #500)
* Auto-learning for the eta parameter on the LdaModel (Christopher Corley, #479)
* Python 3.5 support
* Speed improvements to keyword and summarisation methods (@erbas #441)
* OSX wheels (#504)
* Win build (#492)

## 0.12.2, 2015-09-19

* tutorial on text summarization (√ìlavur Mortensen, #436)
* more flexible vocabulary construction in word2vec & doc2vec (Philipp Dowling, #434)
* added support for sliced TransformedCorpus objects, so that after applying (for instance) TfidfModel the returned corpus remains randomly indexable. (Matti Lyra, #425)
* changed the LdaModel.save so that a custom `ignore` list can be passed in (Matti Lyra, #331)
* added support for NumPy style fancy indexing to corpus objects (Matti Lyra, #414)
* py3k fix in distributed LSI (spacecowboy, #433)
* Windows fix for setup.py (#428)
* fix compatibility for scipy 0.16.0 (#415)

## 0.12.1, 2015-07-20

* improvements to testing, switch to Travis CI containers
* support for loading old word2vec models (<=0.11.1) in 0.12+ (Gordon Mohr, #405)
* various bug fixes to word2vec, doc2vec (Gordon Mohr, #393, #386, #404)
* TextSummatization support for very short texts (Federico Barrios, #390)
* support for word2vec[['word1', 'word2'...]] convenience API calls (Satish Palaniappan, #395)
* MatrixSimilarity supports indexing generator corpora (single pass)

## 0.12.0, 2015-07-06

* complete API, performance, memory overhaul of doc2vec (Gordon Mohr, #356, #373, #380, #384)
  - fast infer_vector(); optional memory-mapped doc vectors; memory savings with int doc IDs
  - 'dbow_words' for combined DBOW & word skip-gram training; new 'dm_concat' mode
  - multithreading & negative-sampling optimizations (also benefitting word2vec)
  - API NOTE: doc vectors must now be accessed/compared through model's 'docvecs' field
    (eg: "model.docvecs['my_ID']" or "model.docvecs.most_similar('my_ID')")
  - https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb
* new "text summarization" module (PR #324: Federico Lopez, Federico Barrios)
  - https://github.com/summanlp/docs/raw/master/articulo/articulo-en.pdf
* new matutils.argsort with partial sort
  - performance speedups to all similarity queries (word2vec, Similarity classes...)
* word2vec can compute likelihood scores for classification (Mat Addy, #358)
  - http://arxiv.org/abs/1504.07295
  - http://nbviewer.ipython.org/github/taddylab/deepir/blob/master/w2v-inversion.ipynb
* word2vec supports "encoding" parameter when loading from C format, for non-utf8 models
* more memory-efficient word2vec training (#385)
* fixes to Python3 compatibility (Pavel Kalaidin #330, S-Eugene #369)
* enhancements to save/load format (Liang Bo Wang #363, Gordon Mohr #356)
  - pickle defaults to protocol=2 for better py3 compatibility
* fixes and improvements to wiki parsing (Lukas Elmer #357, Excellent5 #333)
* fix to phrases scoring (Ikuya Yamada, #353)
* speed up of phrases generation (Dave Challis, #349)
* changes to multipass LDA training (Christopher Corley, #298)
* various doc improvements and fixes (Matti Lyra #331, Hongjoo Lee #334)
* fixes and improvements to LDA (Christopher Corley #323)

## 0.11.0 = 0.11.1 = 0.11.1-1, 2015-04-10

* added "topic ranking" to sort topics by coherence in LdaModel (jtmcmc, #311)
* new fast ShardedCorpus out-of-core corpus (Jan Hajic jr., #284)
* utils.smart_open now uses the smart_open package (#316)
* new wrapper for LDA in Vowpal Wabbit (Dave Challis, #304)
* improvements to the DtmModel wrapper (Yang Han, #272, #277)
* move wrappers for external modeling programs into a submodule (Christopher Corley, #295)
* allow transparent compression of NumPy files in save/load (Christopher Corley, #248)
* save/load methods now accept file handles, in addition to file names (macks22, #292)
* fixes to LdaMulticore on Windows (Feng Mai, #305)
* lots of small fixes & py3k compatibility improvements (Chyi-Kwei Yau, Daniel Nouri, Timothy Emerick, Juarez Bochi, Christopher Corley, Chirag Nagpal, Jan Hajic jr., Fl√°vio Code√ßo Coelho)
* re-released as 0.11.1 and 0.11.1-1 because of a packaging bug

## 0.10.3, 2014-11-17

* added streamed phrases = collocation detection (Miguel Cabrera, #258)
* added param for multiple word2vec epochs (sebastienj, #243)
* added doc2vec (=paragraph2vec = extension of word2vec) model (Timothy Emerick, #231)
* initialize word2vec deterministically, for increased experiment reproducibility (KCzar, #240)
* all indexed corpora now allow full Python slicing syntax (Christopher Corley, #246)
* update distributed code for new Pyro4 API and py3k (Michael Brooks, Marco Bonzanini, #255, #249)
* fixes to six module version (Lars Buitinck, #259)
* fixes to setup.py (Maxim Avanov and Christopher Corley, #260, #251)
* ...and lots of minor fixes & updates all around

## 0.10.2, 2014-09-18

* new parallelized, LdaMulticore implementation (Jan Zikes, #232)
* Dynamic Topic Models (DTM) wrapper (Arttii, #205)
* word2vec compiled from bundled C file at install time: no more pyximport (#233)
* standardize show_/print_topics in LdaMallet (Benjamin Bray, #223)
* add new word2vec multiplicative objective (3CosMul) of Levy & Goldberg (Gordon Mohr, #224)
* preserve case in MALLET wrapper (mcburton, #222)
* support for matrix-valued topic/word prior eta in LdaModel (mjwillson, #208)
* py3k fix to SparseCorpus (Andreas Madsen, #234)
* fix to LowCorpus when switching dictionaries (Christopher Corley, #237)

## 0.10.1, 2014-07-22

* word2vec: new n_similarity method for comparing two sets of words (Fran√ßois Scharffe, #219)
* make LDA print/show topics parameters consistent with LSI (Bram Vandekerckhove, #201)
* add option for efficient word2vec subsampling (Gordon Mohr, #206)
* fix length calculation for corpora on empty files (Christopher Corley, #209)
* improve file cleanup of unit tests (Christopher Corley)
* more unit tests
* unicode now stored everywhere in gensim internally; accepted input stays either utf8 or unicode
* various fixes to the py3k ported code
* allow any dict-like input in Dictionary.from_corpus (Andreas Madsen)
* error checking improvements to the MALLET wrapper
* ignore non-articles during wiki parsig
* utils.lemmatize now (optionally) ignores stopwords

## 0.10.0 (aka "PY3K port"), 2014-06-04

* full Python 3 support (targeting 3.3+, #196)
* all internal methods now expect & store unicode, instead of utf8
* new optimized word2vec functionality: negative sampling, cbow (sebastien-j, #162)
* allow by-frequency sort in Dictionary.save_as_text (Renaud Richardet, #192)
* add topic printing to HDP model (Tiepes, #190)
* new gensim_addons package = optional install-time Cython compilations (Bj√∂rn Esser, #197)
* added py3.3 and 3.4 to Travis CI tests
* fix a cbow word2vec bug (Liang-Chi Hsieh)

## 0.9.1, 2014-04-12

* MmCorpus fix for Windows
* LdaMallet support for printing/showing topics
* fix LdaMallet bug when user specified a file prefix (Victor, #184)
* fix LdaMallet output when input is single vector (Suvir)
* added LdaMallet unit tests
* more py3k fixes (Lars Buitinck)
* change order of LDA topic printing (Fayimora Femi-Balogun, #188)

## 0.9.0, 2014-03-16

* save/load automatically single out large arrays + allow mmap
* allow .gz/.bz2 corpus filenames => transparently (de)compressed I/O
* CBOW model for word2vec (S√©bastien Jean, #176)
* new API for storing corpus metadata (Joseph Chang, #169)
* new LdaMallet class = train LDA using wrapped Mallet
* new MalletCorpus class for corpora in Mallet format (Christopher Corley, #179)
* better Wikipedia article parsing (Joseph Chang, #170)
* word2vec load_word2vec_format uses less memory (Yves Raimond, #164)
* load/store vocabulary files for word2vec C format (Yves Raimond, #172)
* HDP estimation on new documents (Elliot Kulakow, #153)
* store labels in SvmLight corpus (Ritesh, #152)
* fix word2vec binary load on Windows (Stephanus van Schalkwyk)
* replace numpy.svd with scipy.svd for more stability (Sven D√∂ring, #159)
* parametrize LDA constructor (Christopher Corley, #174)
* steps toward py3k compatibility (Lars Buitinck, #154)

## 0.8.9, 2013-12-26

* use travis-ci for continuous integration
* auto-optimize LDA asymmetric prior (Ben Trahan)
* update for new word2vec binary format (Daren Race)
* doc rendering fix (Dan Foreman-Mackey)
* better LDA perplexity logging
* fix Pyro thread leak in distributed algos (Brian Feeny)
* optimizations in word2vec (Bryan Rink)
* allow compressed input in LineSentence corpus (Eric Moyer)
* upgrade ez_setup, doc improvements, minor fixes etc.

## 0.8.8 (aka "word2vec release"), 2013-11-03

* python3 port by Parikshit Samant: https://github.com/samantp/gensimPy3
* massive optimizations to word2vec (cython, BLAS, multithreading): ~20x-300x speedup
* new word2vec functionality (thx to Ghassen Hamrouni, PR #124)
* new CSV corpus class (thx to Zygmunt ZajƒÖc)
* corpus serialization checks to prevent overwriting (by Ian Langmore, PR #125)
* add context manager support for older Python<=2.6 for gzip and bz2
* added unittests for word2vec

## 0.8.7, 2013-09-18

* initial version of word2vec, a neural network deep learning algo
* make distributed gensim compatible with the new Pyro
* allow merging dictionaries (by Florent Chandelier)
* new design for the gensim website!
* speed up handling of corner cases when returning top-n most similar
* make Random Projections compatible with new scipy (andrewjOc360, PR #110)
* allow "light" (faster) word lemmatization (by Karsten Jeschkies)
* save/load directly from bzip2 files (by Luis Pedro Coelho, PR #101)
* Blei corpus now tries harder to find its vocabulary file (by Luis Pedro Coelho, PR #100)
* sparse vector elements can now be a list (was: only a 2-tuple)
* simple_preprocess now optionally deaccents letters (≈ô/≈°/√∫=>r/s/u etc.)
* better serialization of numpy corpora
* print_topics() returns the topics, in addition to printing/logging
* fixes for more robust Windows multiprocessing
* lots of small fixes, data checks and documentation updates

## 0.8.6, 2012-09-15

* added HashDictionary (by Homer Strong)
* support for adding target classes in SVMlight format (by Corrado Monti)
* fixed problems with global lemmatizer object when running in parallel on Windows
* parallelization of Wikipedia processing + added script version that lemmatizes the input documents
* added class method to initialize Dictionary from an existing corpus (by Marko Burjek)

## 0.8.5, 2012-07-22

* improved performance of sharding (similarity queries)
* better Wikipedia parsing (thx to Alejandro Weinstein and Lars Buitinck)
* faster Porter stemmer (thx to Lars Buitinck)
* several minor fixes (in HDP model thx to Greg Ver Steeg)
* improvements to documentation

## 0.8.4, 2012-03-09

* better support for Pandas series input (thx to JT Bates)
* a new corpus format: UCI bag-of-words (thx to Jonathan Esterhazy)
* a new model, non-parametric bayes: HDP (thx to Jonathan Esterhazy; based on Chong Wang's code)
* improved support for new scipy versions (thx to Skipper Seabold)
* lemmatizer support for wikipedia parsing (via the `pattern` python package)
* extended the lemmatizer for multi-core processing, to improve its performance

## 0.8.3, 2011-12-02

* fixed Similarity sharding bug (issue #65, thx to Paul Rudin)
* improved LDA code (clarity & memory footprint)
* optimized efficiency of Similarity sharding

## 0.8.2, 2011-10-31

* improved gensim landing page
* improved accuracy of SVD (Latent Semantic Analysis) (thx to Mark Tygert)
* changed interpretation of LDA topics: github issue #57
* took out similarity server code introduced in 0.8.1 (will become a separate project)
* started using `tox` for testing
* + several smaller fixes and optimizations

## 0.8.1, 2011-10-10

* transactional similarity server: see docs/simserver.html
* website moved from university hosting to radimrehurek.com
* much improved speed of lsi[corpus] transformation:
* accuracy tests of incremental svd: test/svd_error.py and http://groups.google.com/group/gensim/browse_thread/thread/4b605b72f8062770
* further improvements to memory-efficiency of LDA and LSA
* improved wiki preprocessing (thx to Luca de Alfaro)
* model.print_topics() debug fncs now support std output, in addition to logging (thx to Homer Strong)
* several smaller fixes and improvements

## 0.8.0 (Armageddon), 2011-06-28

* changed all variable and function names to comply with PEP8 (numTopics->num_topics): BREAKS BACKWARD COMPATIBILITY!
* added support for similarity querying more documents at once (index[query_documents] in addition to index[query_document]; much faster)
* rewrote Similarity so that it is more efficient and scalable (using disk-based mmap'ed shards)
* simplified directory structure (src/gensim/ is now only gensim/)
* several small fixes and optimizations

## 0.7.8, 2011-03-26

* added `corpora.IndexedCorpus`, a base class for corpus serializers (thx to Dieter Plaetinck). This allows corpus formats that inherit from it (MmCorpus, SvmLightCorpus, BleiCorpus etc.) to retrieve individual documents by their id in O(1), e.g. `corpus[14]` returns document #14.
* merged new code from the LarKC.eu team (`corpora.textcorpus`, `models.logentropy_model`, lots of unit tests etc.)
* fixed a bug in `lda[bow]` transformation (was returning gamma distribution instead of theta). LDA model generation was not affected, only transforming new vectors.
* several small fixes and documentation updates

## 0.7.7, 2011-02-13

* new LDA implementation after Hoffman et al.: Online Learning for Latent Dirichlet Allocation
* distributed LDA
* updated LDA docs (wiki experiments, distributed tutorial)
* matrixmarket header now uses capital 'M's: MatrixMarket. (Andr√© Lynum reported than Matlab has trouble processing the lowercase version)
* moved code to github
* started gensim Google group

## 0.7.6, 2011-01-10

* added workaround for a bug in numpy: pickling a fortran-order array (e.g. LSA model) and then loading it back and using it results in segfault (thx to Brian Merrel)
* bundled a new version of ez_setup.py: old failed with Python2.6 when setuptools were missing (thx to Alan Salmoni).

## 0.7.5, 2010-11-03

* further optimization to LSA; this is the version used in my NIPS workshop paper
* got rid of SVDLIBC dependency (one-pass LSA now uses stochastic algo for base-base decompositions)

## 0.7.4

* sped up Latent Dirichlet ~10x (through scipy.weave, optional)
* finally, distributed LDA! scales almost linearly, but no tutorial yet. see the tutorial on distributed LSI, everything's completely analogous.
* several minor fixes and improvements; one nasty bug fixed (lsi[corpus] didn't work; thx to Danilo Spinelli)

## 0.7.3

* added stochastic SVD decomposition (faster than the current one-pass LSI algo, but needs two passes over the input corpus)
* published gensim on mloss.org

## 0.7.2

* added workaround for a numpy bug where SVD sometimes fails to converge for no good reason
* changed content of gensims's PyPi title page
* completed HTML tutorial on distributed LSA

## 0.7.1

* fixed a bug in LSA that occurred when the number of features was smaller than the number of topics (thx to Richard Berendsen)

## 0.7.0

* optimized vocabulary generation in gensim.corpora.dictionary (faster and less memory-intense)
* MmCorpus accepts compressed input (file-like objects such as GzipFile, BZ2File; to save disk space)
* changed sparse solver to SVDLIBC (sparsesvd on PyPi) for large document chunks
* added distributed LSA, updated tutorials (still experimental though)
* several minor bug fixes

## 0.6.0

* added option for online LSI training (yay!). the transformation can now be
  used after any amount of training, and training can be continued at any time
  with more data.
* optimized the tf-idf transformation, so that it is a strictly one-pass algorithm in all cases  (thx to Brian Merrell).
* fixed Windows-specific bug in handling binary files (thx to Sutee Sudprasert)
* fixed 1-based feature counting bug in SVMlight format (thx to Richard Berendsen)
* added 'Topic :: Text Processing :: Linguistic' to gensim's pypi classifiers
* change of sphinx documentation css and layout

## 0.5.0

* finished all tutorials, stable version

## 0.4.7

* tutorial on transformations

## 0.4.6

* added Random Projections (aka Random Indexing), as another transformation model.
* several DML-CZ specific updates

## 0.4.5

* updated documentation
* further memory optimizations in SVD (LSI)

## 0.4.4

* added missing test files to MANIFEST.in

## 0.4.3

* documentation changes
* added gensim reference to Wikipedia articles (SVD, LSI, LDA, TFIDF, ...)

## 0.4.2

* finally, a tutorial!
* similarity queries got their own package

## 0.4.1

* pdf documentation
* removed dependency on python2.5 (theoretically, gensim now runs on 2.6 and 2.7 as well).

## 0.4.0

* support for ``python setup.py test``
* fixing package metadata
* documentation clean-up

## 0.2.0

* First version
<!--
**IMPORTANT**:

- Use the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim) to ask general or usage questions. Github issues are only for bug reports.
- Check [Recipes&FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) first for common answers.

Github bug reports that do not include relevant information and context will be closed without an answer. Thanks!
-->

#### Problem description

What are you trying to achieve? What is the expected result? What are you seeing instead?

#### Steps/code/corpus to reproduce

Include full tracebacks, logs and datasets if necessary. Please keep the examples minimal ("minimal reproducible example").

If your problem is with a specific Gensim model (word2vec, lsimodel, doc2vec, fasttext, ldamodel etc), include the following:

```python
print(my_model.lifecycle_events)
```

#### Versions

Please provide the output of:

```python
import platform; print(platform.platform())
import sys; print("Python", sys.version)
import struct; print("Bits", 8 * struct.calcsize("P"))
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import gensim; print("gensim", gensim.__version__)
from gensim.models import word2vec;print("FAST_VERSION", word2vec.FAST_VERSION)
```
# :pizza: Hacktoberfest 2019 :beer:

It's that time of the year again!
[Hacktoberfest](https://hacktoberfest.digitalocean.com) is here, and `gensim` needs **your** help.
We've prepared a list of good issues to work on: [gensim hacktoberfest issues](https://github.com/RaRe-Technologies/gensim/labels/hacktoberfest).

If the learning curve for `gensim` is a bit steep, give the [smart_open](https://github.com/RaRe-Technologies/smart_open) repository a try.
`smart_open` is an important dependency of `gensim`: it performs file I/O over a variety of protocols and formats.
There's also a list of Hacktoberfest-friendly issues to work on: [smart_open hacktoberfest issues](https://github.com/RaRe-Technologies/smart_open/labels/hacktoberfest).

Of course, we welcome contributions on any of the existing issues, not just the ones labeled `hacktoberfest`.
If the issue is simple & quick, you can just submit your PR, with a proper reference to the issue it addresses.
If the issue requires a little more work, but you have a good idea of how to proceed & know when you'll be submitting some initial work, please post a short note about your plans to the issue, or a "work-in-progress" ("[WIP]") pull-request indicating work is underway, to help avoid wasted duplicate work.

Furthermore, we also welcome contributions not connected to an existing issue.
This includes things like fixing typos in documentation, docstrings, etc.
If you make such contributions, please make the motivation behind the contribution clear.
You could start such a contribution with a new pull-request, or if you think it requires other discussion beforehand, as a separate new issue.
Please avoid making innocuous changes without sufficient motivation (e.g. changing code formatting, etc).

## Before Contributing

Check out the following:

- [First-time contributors guide](https://github.com/firstcontributions/first-contributions): if this is your first time contributing on GitHub.
- [Hacktoberfest rules](https://hacktoberfest.digitalocean.com/faq#rules): read this in full
- [Developer page](https://github.com/RaRe-Technologies/gensim/wiki/Developer-page) on our Wiki: for the git flow, code style, etc.

## Questions

If you have a general question about Gensim, please ask on the [mailing list](https://groups.google.com/forum/#!forum/gensim).
If you have a question a about a specific issue or PR, just ask there directly, and we'll get back to you as soon as we can.
Otherwise, ping @mpenkov on [Twitter](https://twitter.com/mpenkov) or [Telegram](https://t.me/mpenkov).

Happy Hacking!!
gensim ‚Äì Topic Modelling in Python
==================================

<!--
The following image URLs are obfuscated = proxied and cached through
Google because of Github's proxying issues. See:
https://github.com/RaRe-Technologies/gensim/issues/2805
-->

[![Build Status](https://github.com/RaRe-Technologies/gensim/actions/workflows/tests.yml/badge.svg?branch=develop)](https://github.com/RaRe-Technologies/gensim/actions)
[![GitHub release](https://img.shields.io/github/release/rare-technologies/gensim.svg?maxAge=3600)](https://github.com/RaRe-Technologies/gensim/releases)
[![Downloads](https://img.shields.io/pypi/dm/gensim?color=blue)](https://pepy.tech/project/gensim/)
[![DOI](https://zenodo.org/badge/DOI/10.13140/2.1.2393.1847.svg)](https://doi.org/10.13140/2.1.2393.1847)
[![Mailing List](https://img.shields.io/badge/-Mailing%20List-blue.svg)](https://groups.google.com/forum/#!forum/gensim)
[![Follow](https://img.shields.io/twitter/follow/gensim_py.svg?style=social&style=flat&logo=twitter&label=Follow&color=blue)](https://twitter.com/gensim_py)

Gensim is a Python library for *topic modelling*, *document indexing*
and *similarity retrieval* with large corpora. Target audience is the
*natural language processing* (NLP) and *information retrieval* (IR)
community.

## ‚ö†Ô∏è  Please [sponsor Gensim](https://github.com/sponsors/piskvorky) to help sustain this open source project ‚ù§Ô∏è


Features
--------

-   All algorithms are **memory-independent** w.r.t. the corpus size
    (can process input larger than RAM, streamed, out-of-core),
-   **Intuitive interfaces**
    -   easy to plug in your own input corpus/datastream (trivial
        streaming API)
    -   easy to extend with other Vector Space algorithms (trivial
        transformation API)
-   Efficient multicore implementations of popular algorithms, such as
    online **Latent Semantic Analysis (LSA/LSI/SVD)**, **Latent
    Dirichlet Allocation (LDA)**, **Random Projections (RP)**,
    **Hierarchical Dirichlet Process (HDP)** or **word2vec deep
    learning**.
-   **Distributed computing**: can run *Latent Semantic Analysis* and
    *Latent Dirichlet Allocation* on a cluster of computers.
-   Extensive [documentation and Jupyter Notebook tutorials].

If this feature list left you scratching your head, you can first read
more about the [Vector Space Model] and [unsupervised document analysis]
on Wikipedia.

Installation
------------

This software depends on [NumPy and Scipy], two Python packages for
scientific computing. You must have them installed prior to installing
gensim.

It is also recommended you install a fast BLAS library before installing
NumPy. This is optional, but using an optimized BLAS such as MKL, [ATLAS] or
[OpenBLAS] is known to improve performance by as much as an order of
magnitude. On OSX, NumPy picks up its vecLib BLAS automatically,
so you don‚Äôt need to do anything special.

Install the latest version of gensim:

```bash
    pip install --upgrade gensim
```

Or, if you have instead downloaded and unzipped the [source tar.gz]
package:

```bash
    python setup.py install
```

For alternative modes of installation, see the [documentation].

Gensim is being [continuously tested](http://radimrehurek.com/gensim/#testing) under all
[supported Python versions](https://github.com/RaRe-Technologies/gensim/wiki/Gensim-And-Compatibility).
Support for Python 2.7 was dropped in gensim 4.0.0 ‚Äì install gensim 3.8.3 if you must use Python 2.7.

How come gensim is so fast and memory efficient? Isn‚Äôt it pure Python, and isn‚Äôt Python slow and greedy?
--------------------------------------------------------------------------------------------------------

Many scientific algorithms can be expressed in terms of large matrix
operations (see the BLAS note above). Gensim taps into these low-level
BLAS libraries, by means of its dependency on NumPy. So while
gensim-the-top-level-code is pure Python, it actually executes highly
optimized Fortran/C under the hood, including multithreading (if your
BLAS is so configured).

Memory-wise, gensim makes heavy use of Python‚Äôs built-in generators and
iterators for streamed data processing. Memory efficiency was one of
gensim‚Äôs [design goals], and is a central feature of gensim, rather than
something bolted on as an afterthought.

Documentation
-------------

-   [QuickStart]
-   [Tutorials]
-   [Official API Documentation]

  [QuickStart]: https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html
  [Tutorials]: https://radimrehurek.com/gensim/auto_examples/
  [Official Documentation and Walkthrough]: http://radimrehurek.com/gensim/
  [Official API Documentation]: http://radimrehurek.com/gensim/apiref.html

Support
-------

For commercial support, please see [Gensim sponsorship](https://github.com/sponsors/piskvorky).

Ask open-ended questions on the public [Gensim Mailing List](https://groups.google.com/forum/#!forum/gensim).

Raise bugs on [Github](https://github.com/RaRe-Technologies/gensim/blob/develop/CONTRIBUTING.md) but please **make sure you follow the [issue template](https://github.com/RaRe-Technologies/gensim/blob/develop/ISSUE_TEMPLATE.md)**. Issues that are not bugs or fail to provide the requested details will be closed without inspection.


---------

Adopters
--------

| Company | Logo | Industry | Use of Gensim |
|---------|------|----------|---------------|
| [RARE Technologies](http://rare-technologies.com) | ![rare](docs/src/readme_images/rare.png) | ML & NLP consulting | Creators of Gensim ‚Äì¬†this is us! |
| [Amazon](http://www.amazon.com/) |  ![amazon](docs/src/readme_images/amazon.png) | Retail |  Document similarity. |
| [National Institutes of Health](https://github.com/NIHOPA/pipeline_word2vec) | ![nih](docs/src/readme_images/nih.png) | Health | Processing grants and publications with word2vec. |
| [Cisco Security](http://www.cisco.com/c/en/us/products/security/index.html) | ![cisco](docs/src/readme_images/cisco.png) | Security |  Large-scale fraud detection. |
| [Mindseye](http://www.mindseyesolutions.com/) | ![mindseye](docs/src/readme_images/mindseye.png) | Legal | Similarities in legal documents. |
| [Channel 4](http://www.channel4.com/) | ![channel4](docs/src/readme_images/channel4.png) | Media | Recommendation engine. |
| [Talentpair](http://talentpair.com) | ![talent-pair](docs/src/readme_images/talent-pair.png) | HR | Candidate matching in high-touch recruiting. |
| [Juju](http://www.juju.com/)  | ![juju](docs/src/readme_images/juju.png) | HR | Provide non-obvious related job suggestions. |
| [Tailwind](https://www.tailwindapp.com/) | ![tailwind](docs/src/readme_images/tailwind.png) | Media | Post interesting and relevant content to Pinterest. |
| [Issuu](https://issuu.com/) | ![issuu](docs/src/readme_images/issuu.png) | Media | Gensim's LDA module lies at the very core of the analysis we perform on each uploaded publication to figure out what it's all about. |
| [Search Metrics](http://www.searchmetrics.com/) | ![search-metrics](docs/src/readme_images/search-metrics.png) | Content Marketing | Gensim word2vec used for entity disambiguation in Search Engine Optimisation. |
| [12K Research](https://12k.co/) | ![12k](docs/src/readme_images/12k.png)| Media |   Document similarity analysis on media articles. |
| [Stillwater Supercomputing](http://www.stillwater-sc.com/) | ![stillwater](docs/src/readme_images/stillwater.png) | Hardware | Document comprehension and association with word2vec. |
| [SiteGround](https://www.siteground.com/) |  ![siteground](docs/src/readme_images/siteground.png) | Web hosting | An ensemble search engine which uses different embeddings models and similarities, including word2vec, WMD, and LDA. |
| [Capital One](https://www.capitalone.com/) | ![capitalone](docs/src/readme_images/capitalone.png) | Finance | Topic modeling for customer complaints exploration. |

-------

Citing gensim
------------

When [citing gensim in academic papers and theses], please use this
BibTeX entry:

    @inproceedings{rehurek_lrec,
          title = {{Software Framework for Topic Modelling with Large Corpora}},
          author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
          booktitle = {{Proceedings of the LREC 2010 Workshop on New
               Challenges for NLP Frameworks}},
          pages = {45--50},
          year = 2010,
          month = May,
          day = 22,
          publisher = {ELRA},
          address = {Valletta, Malta},
          note={\url{http://is.muni.cz/publication/884893/en}},
          language={English}
    }

  [citing gensim in academic papers and theses]: https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:NaGl4SEjCO4C

  [design goals]: http://radimrehurek.com/gensim/about.html
  [RaRe Technologies]: http://rare-technologies.com/wp-content/uploads/2016/02/rare_image_only.png%20=10x20
  [rare\_tech]: //rare-technologies.com
  [Talentpair]: https://avatars3.githubusercontent.com/u/8418395?v=3&s=100
  [citing gensim in academic papers and theses]: https://scholar.google.cz/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:u-x6o8ySG0sC

  [documentation and Jupyter Notebook tutorials]: https://github.com/RaRe-Technologies/gensim/#documentation
  [Vector Space Model]: http://en.wikipedia.org/wiki/Vector_space_model
  [unsupervised document analysis]: http://en.wikipedia.org/wiki/Latent_semantic_indexing
  [NumPy and Scipy]: http://www.scipy.org/Download
  [ATLAS]: http://math-atlas.sourceforge.net/
  [OpenBLAS]: http://xianyi.github.io/OpenBLAS/
  [source tar.gz]: http://pypi.python.org/pypi/gensim
  [documentation]: http://radimrehurek.com/gensim/install.html
# Security Policy

## Supported Versions

Use this section to tell people about which versions of your project are
currently being supported with security updates.

| Version | Supported          |
| ------- | ------------------ |
| 4.  x   | :white_check_mark: |
| < 4.0   | :x:                |

## Reporting a Vulnerability

Open a ticket and add the "security" label to it.
Describe the vulnerability in general.
We'll reach out to you for specifics.
# How to submit an issue?

First, please see [contribution-guide.org](http://www.contribution-guide.org/) for the steps we expect from contributors before submitting an issue or bug report. Be as concrete as possible, include relevant logs, package versions etc.

Also, please check the [Gensim FAQ](https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ) page before posting.

**The proper place for open-ended questions is the [Gensim mailing list](https://groups.google.com/forum/#!forum/gensim).** Github is not the right place for research discussions or feature requests.

# How to add a new feature or create a pull request?

1. <a href="https://github.com/RaRe-Technologies/gensim/fork">Fork the Gensim repository</a>
2. Clone your fork: `git clone https://github.com/<YOUR_GITHUB_USERNAME>/gensim.git`
3. Create a new branch based on `develop`: `git checkout -b my-feature develop`
4. Setup your Python enviroment
   - Create a new [virtual environment](https://virtualenv.pypa.io/en/stable/): `pip install virtualenv; virtualenv gensim_env` and activate it:
      - For linux: `source gensim_env/bin/activate` 
      - For windows: `gensim_env\Scripts\activate`
   - Install Gensim and its test dependencies in [editable mode](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs): 
      - For linux: `pip install -e .[test]`
      - For windows: `pip install -e .[test-win]`
5. Implement your changes
6. Check that everything's OK in your branch:
   - Check it for PEP8: `tox -e flake8`
   - Build its documentation (works only for MacOS/Linux): `tox -e docs` (documentation stored in `docs/src/_build`)
   - Run unit tests: `tox -e py{version}-{os}`, for example `tox -e py35-linux` or `tox -e py36-win` where
      - `{version}` is one of `35`, `36`
      - `{os}` is either `win` or `linux`
7. Add files, commit and push: `git add ... ; git commit -m "my commit message"; git push origin my-feature`
8. [Create a PR](https://help.github.com/articles/creating-a-pull-request/) on Github. Write a **clear description** for your PR, including all the context and relevant information, such as:
   - The issue that you fixed, e.g. `Fixes #123`
   - Motivation: why did you create this PR? What functionality did you set out to improve? What was the problem + an overview of how you fixed it? Whom does it affect and how should people use it?
   - Any other useful information: links to other related Github or mailing list issues and discussions, benchmark graphs, academic papers‚Ä¶

P.S. for developers: see our [Developer Page](https://github.com/piskvorky/gensim/wiki/Developer-page#code-style) for details on the Gensim code style, CI, testing and similar.

**Thanks and let's improve the open source world together!**
Scripts to help when making new releases.

For more info, see [our Wiki page](https://github.com/RaRe-Technologies/gensim/wiki/Developer-page#make-a-new-release-for-maintainers).
Distributed Computing
=====================

Why distributed computing?
--------------------------

Need to build semantic representation of a corpus that is millions of documents large and it's taking forever? Have several idle machines at your disposal that you could use? [Distributed computing][1] tries to accelerate computations by splitting a given task into several smaller subtasks, passing them on to several computing nodes in parallel.

In the context of gensim, computing nodes are computers identified by their IP address/port, and communication happens over TCP/IP. The whole collection of available machines is called a *cluster*. The distribution is very coarse grained (not much communication going on), so the network is allowed to be of relatively high latency.

> **Warning**

> The primary reason for using distributed computing is making things run faster. In gensim, most of the time consuming stuff is done inside low-level routines for linear algebra, inside NumPy, independent of any gensim code. **Installing a fast** [BLAS (Basic Linear Algebra)][2] **library for NumPy can improve performance up to 15 times!** So before you start buying those extra computers, consider installing a fast, threaded BLAS that is optimized for your particular machine (as opposed to a generic, binary-distributed library). Options include your vendor's BLAS library (Intel's MKL, AMD's ACML, OS X's vecLib, Sun's Sunperf, ...) or some open-source alternative (GotoBLAS, ALTAS).
>
> To see what BLAS and LAPACK you are using, type into your shell:
>
>     $ python -c 'import scipy; scipy.show_config()'

Prerequisites
-------------

For communication between nodes, gensim uses [Pyro (PYthon Remote Objects)][3], version &gt;= `4.27`. This is a library for low-level socket communication and remote procedure calls (RPC) in Python. Pyro is a pure-Python library, so its installation is quite painless and only involves copying its `*.py` files somewhere onto your Python's import path:

    sudo easy_install Pyro4

You don't have to install Pyro to run gensim, but if you don't, you won't be able to access the distributed features (i.e., everything will always run in serial mode, the examples on this page don't apply).

Core concepts
-------------

As always, gensim strives for a clear and straightforward API (see [Features][4]). To this end, *you do not need to make any changes in your code at all* in order to run it over a cluster of computers!

What you need to do is run a [worker][5] script (*see below*) on each of your cluster nodes prior to starting your computation. Running this script tells gensim that it may use the node as a slave to delegate some work to it. During initialization, the algorithms inside gensim will try to look for and enslave all available worker nodes.

**Node**
A logical working unit. Can correspond to a single physical machine, but you can also run multiple workers on one machine, resulting in multiple logical nodes.

**Cluster**
Several nodes which communicate over TCP/IP. Nodes can lie in different [broadcast domains][6]. Here is how to [configure](https://github.com/RaRe-Technologies/gensim/pull/782).

**Worker**
A process which is created on each node. To remove a node from your cluster, simply kill its worker process.

**Dispatcher**
The dispatcher will be in charge of negotiating all computations, queueing and distributing ("dispatching") individual jobs to the workers. Computations never "talk" to worker nodes directly, only through this dispatcher. Unlike workers, there can only be one active dispatcher at a time in the cluster.

Available distributed algorithms
---------------------------------
* [Distributed Latent Semantic Analysis][7]
* [Distributed Latent Dirichlet Allocation][8]


[1]: http://en.wikipedia.org/wiki/Distributed_computing
[2]: http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms
[3]: http://pypi.python.org/pypi/Pyro4
[4]: http://radimrehurek.com/gensim/intro.html#design
[5]: http://radimrehurek.com/gensim/distributed.html#term-worker
[6]: http://en.wikipedia.org/wiki/Broadcast_domain
[7]: http://radimrehurek.com/gensim/dist_lsi.html
[8]: http://radimrehurek.com/gensim/dist_lda.html
:orphan:

.. _wiki:

Experiments on the English Wikipedia
============================================

To test `gensim` performance, we run it against the English version of Wikipedia.

This page describes the process of obtaining and processing Wikipedia, so that
anyone can reproduce the results. It is assumed you have `gensim` properly installed.



Preparing the corpus
----------------------

1. First, download the dump of all Wikipedia articles from http://download.wikimedia.org/enwiki/
   (you want the file `enwiki-latest-pages-articles.xml.bz2`, or `enwiki-YYYYMMDD-pages-articles.xml.bz2` for date-specific dumps). This file is about 8GB in size
   and contains (a compressed version of) all articles from the English Wikipedia.

2. Convert the articles to plain text (process Wiki markup) and store the result as
   sparse TF-IDF vectors. In Python, this is easy to do on-the-fly and we don't
   even need to uncompress the whole archive to disk. There is a script included in
   `gensim` that does just that, run::

   $ python -m gensim.scripts.make_wiki

.. note::
  This pre-processing step makes two passes over the 8.2GB compressed wiki dump (one to extract
  the dictionary, one to create and store the sparse vectors) and takes about
  9 hours on my laptop, so you may want to go have a coffee or two.

  Also, you will need about 35GB of free disk space to store the sparse output vectors.
  I recommend compressing these files immediately, e.g. with bzip2 (down to ~13GB). Gensim
  can work with compressed files directly, so this lets you save disk space.

Latent Semantic Analysis
--------------------------

First let's load the corpus iterator and dictionary, created in the second step above

.. sourcecode:: pycon

    >>> import logging
    >>> import gensim
    >>>
    >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    >>>
    >>> # load id->word mapping (the dictionary), one of the results of step 2 above
    >>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
    >>> # load corpus iterator
    >>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
    >>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output (recommended)
    >>>
    >>> print(mm)
    MmCorpus(3931787 documents, 100000 features, 756379027 non-zero entries)

We see that our corpus contains 3.9M documents, 100K features (distinct
tokens) and 0.76G non-zero entries in the sparse TF-IDF matrix. The Wikipedia corpus
contains about 2.24 billion tokens in total.

Now we're ready to compute LSA of the English Wikipedia:

.. sourcecode:: pycon

    >>> # extract 400 LSI topics; use the default one-pass algorithm
    >>> lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400)
    >>>
    >>> # print the most contributing words (both positively and negatively) for each of the first ten topics
    >>> lsi.print_topics(10)
    topic #0(332.762): 0.425*"utc" + 0.299*"talk" + 0.293*"page" + 0.226*"article" + 0.224*"delete" + 0.216*"discussion" + 0.205*"deletion" + 0.198*"should" + 0.146*"debate" + 0.132*"be"
    topic #1(201.852): 0.282*"link" + 0.209*"he" + 0.145*"com" + 0.139*"his" + -0.137*"page" + -0.118*"delete" + 0.114*"blacklist" + -0.108*"deletion" + -0.105*"discussion" + 0.100*"diff"
    topic #2(191.991): -0.565*"link" + -0.241*"com" + -0.238*"blacklist" + -0.202*"diff" + -0.193*"additions" + -0.182*"users" + -0.158*"coibot" + -0.136*"user" + 0.133*"he" + -0.130*"resolves"
    topic #3(141.284): -0.476*"image" + -0.255*"copyright" + -0.245*"fair" + -0.225*"use" + -0.173*"album" + -0.163*"cover" + -0.155*"resolution" + -0.141*"licensing" + 0.137*"he" + -0.121*"copies"
    topic #4(130.909): 0.264*"population" + 0.246*"age" + 0.243*"median" + 0.213*"income" + 0.195*"census" + -0.189*"he" + 0.184*"households" + 0.175*"were" + 0.167*"females" + 0.166*"males"
    topic #5(120.397): 0.304*"diff" + 0.278*"utc" + 0.213*"you" + -0.171*"additions" + 0.165*"talk" + -0.159*"image" + 0.159*"undo" + 0.155*"www" + -0.152*"page" + 0.148*"contribs"
    topic #6(115.414): -0.362*"diff" + -0.203*"www" + 0.197*"you" + -0.180*"undo" + -0.180*"kategori" + 0.164*"users" + 0.157*"additions" + -0.150*"contribs" + -0.139*"he" + -0.136*"image"
    topic #7(111.440): 0.429*"kategori" + 0.276*"categoria" + 0.251*"category" + 0.207*"kategorija" + 0.198*"kategorie" + -0.188*"diff" + 0.163*"–∫–∞—Ç–µ–≥–æ—Ä–∏—è" + 0.153*"categor√≠a" + 0.139*"kategoria" + 0.133*"categorie"
    topic #8(109.907): 0.385*"album" + 0.224*"song" + 0.209*"chart" + 0.204*"band" + 0.169*"released" + 0.151*"music" + 0.142*"diff" + 0.141*"vocals" + 0.138*"she" + 0.132*"guitar"
    topic #9(102.599): -0.237*"league" + -0.214*"he" + -0.180*"season" + -0.174*"football" + -0.166*"team" + 0.159*"station" + -0.137*"played" + -0.131*"cup" + 0.131*"she" + -0.128*"utc"

Creating the LSI model of Wikipedia takes about 4 hours and 9 minutes on my laptop [1]_.
That's about **16,000 documents per minute, including all I/O**.

.. note::
  If you need your results even faster, see the tutorial on :doc:`distributed`. Note
  that the BLAS libraries inside `gensim` make use of multiple cores transparently, so the same data
  will be processed faster on a multicore machine "for free", without any distributed setup.

We see that the total processing time is dominated by the preprocessing step of
preparing the TF-IDF corpus from a raw Wikipedia XML dump, which took 9h. [2]_

The algorithm used in `gensim` only needs to see each input document once, so it
is suitable for environments where the documents come as a non-repeatable stream,
or where the cost of storing/iterating over the corpus multiple times is too high.


Latent Dirichlet Allocation
----------------------------

As with Latent Semantic Analysis above, first load the corpus iterator and dictionary

.. sourcecode:: pycon

    >>> import logging
    >>> import gensim
    >>>
    >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    >>>
    >>> # load id->word mapping (the dictionary), one of the results of step 2 above
    >>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
    >>> # load corpus iterator
    >>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
    >>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output
    >>>
    >>> print(mm)
    MmCorpus(3931787 documents, 100000 features, 756379027 non-zero entries)

We will run online LDA (see Hoffman et al. [3]_), which is an algorithm that takes a chunk of documents,
updates the LDA model, takes another chunk, updates the model etc. Online LDA can be contrasted
with batch LDA, which processes the whole corpus (one full pass), then updates
the model, then another pass, another update... The difference is that given a
reasonably stationary document stream (not much topic drift), the online updates
over the smaller chunks (subcorpora) are pretty good in themselves, so that the
model estimation converges faster. As a result, we will perhaps only need a single full
pass over the corpus: if the corpus has 3 million articles, and we update once after
every 10,000 articles, this means we will have done 300 updates in one pass, quite likely
enough to have a very accurate topics estimate

.. sourcecode:: pycon

    >>> # extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)
    >>> lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, passes=1)
    using serial LDA version on this node
    running online LDA training, 100 topics, 1 passes over the supplied corpus of 3931787 documents, updating model once every 10000 documents
    ...

Unlike LSA, the topics coming from LDA are easier to interpret

.. sourcecode:: pycon

    >>> # print the most contributing words for 20 randomly selected topics
    >>> lda.print_topics(20)
    topic #0: 0.009*river + 0.008*lake + 0.006*island + 0.005*mountain + 0.004*area + 0.004*park + 0.004*antarctic + 0.004*south + 0.004*mountains + 0.004*dam
    topic #1: 0.026*relay + 0.026*athletics + 0.025*metres + 0.023*freestyle + 0.022*hurdles + 0.020*ret + 0.017*divis√£o + 0.017*athletes + 0.016*bundesliga + 0.014*medals
    topic #2: 0.002*were + 0.002*he + 0.002*court + 0.002*his + 0.002*had + 0.002*law + 0.002*government + 0.002*police + 0.002*patrolling + 0.002*their
    topic #3: 0.040*courcelles + 0.035*centimeters + 0.023*mattythewhite + 0.021*wine + 0.019*stamps + 0.018*oko + 0.017*perennial + 0.014*stubs + 0.012*ovate + 0.011*greyish
    topic #4: 0.039*al + 0.029*sysop + 0.019*iran + 0.015*pakistan + 0.014*ali + 0.013*arab + 0.010*islamic + 0.010*arabic + 0.010*saudi + 0.010*muhammad
    topic #5: 0.020*copyrighted + 0.020*northamerica + 0.014*uncopyrighted + 0.007*rihanna + 0.005*cloudz + 0.005*knowles + 0.004*gaga + 0.004*zombie + 0.004*wigan + 0.003*maccabi
    topic #6: 0.061*israel + 0.056*israeli + 0.030*sockpuppet + 0.025*jerusalem + 0.025*tel + 0.023*aviv + 0.022*palestinian + 0.019*ifk + 0.016*palestine + 0.014*hebrew
    topic #7: 0.015*melbourne + 0.014*rovers + 0.013*vfl + 0.012*australian + 0.012*wanderers + 0.011*afl + 0.008*dinamo + 0.008*queensland + 0.008*tracklist + 0.008*brisbane
    topic #8: 0.011*film + 0.007*her + 0.007*she + 0.004*he + 0.004*series + 0.004*his + 0.004*episode + 0.003*films + 0.003*television + 0.003*best
    topic #9: 0.019*wrestling + 0.013*ch√¢teau + 0.013*ligue + 0.012*discus + 0.012*estonian + 0.009*uci + 0.008*hockeyarchives + 0.008*wwe + 0.008*estonia + 0.007*reign
    topic #10: 0.078*edits + 0.059*notability + 0.035*archived + 0.025*clearer + 0.022*speedy + 0.021*deleted + 0.016*hook + 0.015*checkuser + 0.014*ron + 0.011*nominator
    topic #11: 0.013*admins + 0.009*acid + 0.009*molniya + 0.009*chemical + 0.007*ch + 0.007*chemistry + 0.007*compound + 0.007*anemone + 0.006*mg + 0.006*reaction
    topic #12: 0.018*india + 0.013*indian + 0.010*tamil + 0.009*singh + 0.008*film + 0.008*temple + 0.006*kumar + 0.006*hindi + 0.006*delhi + 0.005*bengal
    topic #13: 0.047*bwebs + 0.024*malta + 0.020*hobart + 0.019*basa + 0.019*columella + 0.019*huon + 0.018*tasmania + 0.016*popups + 0.014*tasmanian + 0.014*mod√®le
    topic #14: 0.014*jewish + 0.011*rabbi + 0.008*bgwhite + 0.008*lebanese + 0.007*lebanon + 0.006*homs + 0.005*beirut + 0.004*jews + 0.004*hebrew + 0.004*caligari
    topic #15: 0.025*german + 0.020*der + 0.017*von + 0.015*und + 0.014*berlin + 0.012*germany + 0.012*die + 0.010*des + 0.008*kategorie + 0.007*cross
    topic #16: 0.003*can + 0.003*system + 0.003*power + 0.003*are + 0.003*energy + 0.002*data + 0.002*be + 0.002*used + 0.002*or + 0.002*using
    topic #17: 0.049*indonesia + 0.042*indonesian + 0.031*malaysia + 0.024*singapore + 0.022*greek + 0.021*jakarta + 0.016*greece + 0.015*dord + 0.014*athens + 0.011*malaysian
    topic #18: 0.031*stakes + 0.029*webs + 0.018*futsal + 0.014*whitish + 0.013*hyun + 0.012*thoroughbred + 0.012*dnf + 0.012*jockey + 0.011*medalists + 0.011*racehorse
    topic #19: 0.119*oblast + 0.034*uploaded + 0.034*uploads + 0.033*nordland + 0.025*selsoviet + 0.023*raion + 0.022*krai + 0.018*okrug + 0.015*h√•logaland + 0.015*russiae + 0.020*manga + 0.017*dragon + 0.012*theme + 0.011*dvd + 0.011*super + 0.011*hunter + 0.009*ash + 0.009*dream + 0.009*angel
    >>>
    >>> import pickle  # noqa: E402
    >>>
    >>> # Get an article and its topic distribution
    >>> with open("wiki_en_bow.mm.metadata.cpickle", 'rb') as meta_file:
    ...     docno2metadata = pickle.load(meta_file)
    >>>
    >>> doc_num = 0
    >>> print("Title: {}".format(docno2metadata[doc_num][1]))  # take the first article as an example
    Title: Anarchism
    >>>
    >>> vec = mm[doc_num]  # get tf-idf vector
    >>> lda.get_document_topics(vec)
    [(1, 0.028828567), (10, 0.32766217), (36, 0.021675354), (55, 0.2521854), (57, 0.27154338)]

Creating this LDA model of Wikipedia takes about 6 hours and 20 minutes on my laptop [1]_.
If you need your results faster, consider running :doc:`dist_lda` on a cluster of
computers.

Note two differences between the LDA and LSA runs: we asked LSA
to extract 400 topics, LDA only 100 topics (so the difference in speed is in fact
even greater). Secondly, the LSA implementation in `gensim` is truly online: if the nature of the input
stream changes in time, LSA will re-orient itself to reflect these changes, in a reasonably
small amount of updates. In contrast, LDA is not truly online (the name of the [3]_
article notwithstanding), as the impact of later updates on the model gradually
diminishes. If there is topic drift in the input document stream, LDA will get
confused and be increasingly slower at adjusting itself to the new state of affairs.

In short, be careful if using LDA to incrementally add new documents to the model
over time. **Batch usage of LDA**, where the entire training corpus is either known beforehand or does
not exhibit topic drift, **is ok and not affected**.

To run batch LDA (not online), train `LdaModel` with:

.. sourcecode:: pycon

    >>> # extract 100 LDA topics, using 20 full passes, no online updates
    >>> lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, passes=20)

As usual, a trained model can used be to transform new, unseen documents (plain bag-of-words count vectors)
into LDA topic distributions:

.. sourcecode:: pycon

    >>> doc_lda = lda[doc_bow]

--------------------

.. [1] My laptop = MacBook Pro, Intel Core i7 2.3GHz, 16GB DDR3 RAM, OS X with `libVec`.

.. [2]
  Here we're mostly interested in performance, but it is interesting to look at the
  retrieved LSA concepts, too. I am no Wikipedia expert and don't see into Wiki's bowels,
  but Brian Mingus had this to say about the result::

    There appears to be a lot of noise in your dataset. The first three topics
    in your list appear to be meta topics, concerning the administration and
    cleanup of Wikipedia. These show up because you didn't exclude templates
    such as these, some of which are included in most articles for quality
    control: http://en.wikipedia.org/wiki/Wikipedia:Template_messages/Cleanup

    The fourth and fifth topics clearly shows the influence of bots that import
    massive databases of cities, countries, etc. and their statistics such as
    population, capita, etc.

    The sixth shows the influence of sports bots, and the seventh of music bots.

  So the top ten concepts are apparently dominated by Wikipedia robots and expanded
  templates; this is a good reminder that LSA is a powerful tool for data analysis,
  but no silver bullet. As always, it's `garbage in, garbage out
  <http://en.wikipedia.org/wiki/Garbage_In,_Garbage_Out>`_...
  By the way, improvements to the Wiki markup parsing code are welcome :-)

.. [3] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation
   [`pdf <https://papers.neurips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf>`_] [`code <https://github.com/blei-lab/onlineldavb/blob/master/onlineldavb.py>`_]

:mod:`interfaces` -- Core gensim interfaces
============================================

.. automodule:: gensim.interfaces
    :synopsis: Core gensim interfaces
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`utils` -- Various utility functions
==========================================

.. automodule:: gensim.utils
    :synopsis: Various utility functions
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
.. _dist_lda:

Distributed Latent Dirichlet Allocation
============================================


.. note::
  See :doc:`distributed` for an introduction to distributed computing in `gensim`.


Setting up the cluster
_______________________

See the tutorial on :doc:`dist_lsi`; setting up a cluster for LDA is completely
analogous, except you want to run `lda_worker` and `lda_dispatcher` scripts instead
of `lsi_worker` and `lsi_dispatcher`.

Running LDA
____________

Run LDA like you normally would, but turn on the `distributed=True` constructor
parameter

.. sourcecode:: pycon

    >>> # extract 100 LDA topics, using default parameters
    >>> lda = LdaModel(corpus=mm, id2word=id2word, num_topics=100, distributed=True)
    using distributed version with 4 workers
    running online LDA training, 100 topics, 1 passes over the supplied corpus of 3199665 documets, updating model once every 40000 documents
    ..


In serial mode (no distribution), creating this online LDA :doc:`model of Wikipedia <wiki>`
takes 10h56m on my laptop (OS X, C2D 2.53GHz, 4GB RAM with `libVec`).
In distributed mode with four workers (Linux, Xeons of 2Ghz, 4GB RAM
with `ATLAS <http://math-atlas.sourceforge.net/>`_), the wallclock time taken drops to 3h20m.

To run standard batch LDA (no online updates of mini-batches) instead, you would similarly
call

.. sourcecode:: pycon

    >>> lda = LdaModel(corpus=mm, id2word=id2token, num_topics=100, update_every=0, passes=20, distributed=True)
    using distributed version with 4 workers
    running batch LDA training, 100 topics, 20 passes over the supplied corpus of 3199665 documets, updating model once every 3199665 documents
    initializing workers
    iteration 0, dispatching documents up to #10000/3199665
    iteration 0, dispatching documents up to #20000/3199665
    ...

and then, some two days later::

    iteration 19, dispatching documents up to #3190000/3199665
    iteration 19, dispatching documents up to #3199665/3199665
    reached the end of input; now waiting for all remaining jobs to finish

.. sourcecode:: pycon

    >>> lda.print_topics(20)
    topic #0: 0.007*disease + 0.006*medical + 0.005*treatment + 0.005*cells + 0.005*cell + 0.005*cancer + 0.005*health + 0.005*blood + 0.004*patients + 0.004*drug
    topic #1: 0.024*king + 0.013*ii + 0.013*prince + 0.013*emperor + 0.008*duke + 0.008*empire + 0.007*son + 0.007*china + 0.007*dynasty + 0.007*iii
    topic #2: 0.031*film + 0.017*films + 0.005*movie + 0.005*directed + 0.004*man + 0.004*episode + 0.003*character + 0.003*cast + 0.003*father + 0.003*mother
    topic #3: 0.022*user + 0.012*edit + 0.009*wikipedia + 0.007*block + 0.007*my + 0.007*here + 0.007*edits + 0.007*blocked + 0.006*revert + 0.006*me
    topic #4: 0.045*air + 0.026*aircraft + 0.021*force + 0.018*airport + 0.011*squadron + 0.010*flight + 0.010*military + 0.008*wing + 0.007*aviation + 0.007*f
    topic #5: 0.025*sun + 0.022*star + 0.018*moon + 0.015*light + 0.013*stars + 0.012*planet + 0.011*camera + 0.010*mm + 0.009*earth + 0.008*lens
    topic #6: 0.037*radio + 0.026*station + 0.022*fm + 0.014*news + 0.014*stations + 0.014*channel + 0.013*am + 0.013*racing + 0.011*tv + 0.010*broadcasting
    topic #7: 0.122*image + 0.099*jpg + 0.046*file + 0.038*uploaded + 0.024*png + 0.014*contribs + 0.013*notify + 0.013*logs + 0.013*picture + 0.013*flag
    topic #8: 0.036*russian + 0.030*soviet + 0.028*polish + 0.024*poland + 0.022*russia + 0.013*union + 0.012*czech + 0.011*republic + 0.011*moscow + 0.010*finland
    topic #9: 0.031*language + 0.014*word + 0.013*languages + 0.009*term + 0.009*words + 0.008*example + 0.007*names + 0.007*meaning + 0.006*latin + 0.006*form
    topic #10: 0.029*w + 0.029*toronto + 0.023*l + 0.020*hockey + 0.019*nhl + 0.014*ontario + 0.012*calgary + 0.011*edmonton + 0.011*hamilton + 0.010*season
    topic #11: 0.110*wikipedia + 0.110*articles + 0.030*library + 0.029*wikiproject + 0.028*project + 0.019*data + 0.016*archives + 0.012*needing + 0.009*reference + 0.009*statements
    topic #12: 0.032*http + 0.030*your + 0.022*request + 0.017*sources + 0.016*archived + 0.016*modify + 0.015*changes + 0.015*creation + 0.014*www + 0.013*try
    topic #13: 0.011*your + 0.010*my + 0.009*we + 0.008*don + 0.008*get + 0.008*know + 0.007*me + 0.006*think + 0.006*question + 0.005*find
    topic #14: 0.073*r + 0.066*japanese + 0.062*japan + 0.018*tokyo + 0.008*prefecture + 0.005*osaka + 0.004*j + 0.004*sf + 0.003*kyoto + 0.003*manga
    topic #15: 0.045*da + 0.045*fr + 0.027*kategori + 0.026*pl + 0.024*nl + 0.021*pt + 0.017*en + 0.015*categoria + 0.014*es + 0.012*kategorie
    topic #16: 0.010*death + 0.005*died + 0.005*father + 0.004*said + 0.004*himself + 0.004*took + 0.004*son + 0.004*killed + 0.003*murder + 0.003*wife
    topic #17: 0.027*book + 0.021*published + 0.020*books + 0.014*isbn + 0.010*author + 0.010*magazine + 0.009*press + 0.009*novel + 0.009*writers + 0.008*story
    topic #18: 0.027*football + 0.024*players + 0.023*cup + 0.019*club + 0.017*fc + 0.017*footballers + 0.017*league + 0.011*season + 0.007*teams + 0.007*goals
    topic #19: 0.032*band + 0.024*album + 0.014*albums + 0.013*guitar + 0.013*rock + 0.011*records + 0.011*vocals + 0.009*live + 0.008*bass + 0.008*track



If you used the distributed LDA implementation in `gensim`, please let me know (my
email is at the bottom of this page). I would like to hear about your application and
the possible (inevitable?) issues that you encountered, to improve `gensim` in the future.
.. _dist_lsi:

Distributed Latent Semantic Analysis
============================================


.. note::
  See :doc:`distributed` for an introduction to distributed computing in `gensim`.


Setting up the cluster
_______________________

We will show how to run distributed Latent Semantic Analysis by means of an example.
Let's say we have 5 computers at our disposal, all on the same network segment (=reachable
by network broadcast). To start with, install `gensim` and set up `Pyro` on each computer with::

  $ sudo easy_install gensim[distributed]
  $ export PYRO_SERIALIZERS_ACCEPTED=pickle
  $ export PYRO_SERIALIZER=pickle

Then run Pyro‚Äôs name server on exactly one of the machines (doesn‚Äôt matter which one)::

  $ python -m Pyro4.naming -n 0.0.0.0 &

Let's say our example cluster consists of dual-core computers with loads of
memory. We will therefore run **two** worker scripts on four of the physical machines,
creating **eight** logical worker nodes::

  $ python -m gensim.models.lsi_worker &

This will execute `gensim`'s `lsi_worker.py` script (to be run twice on each of the
four computer).
This lets `gensim` know that it can run two jobs on each of the four computers in
parallel, so that the computation will be done faster, while also taking up twice
as much memory on each machine.

Next, pick one computer that will be a job scheduler in charge of worker
synchronization, and on it, run `LSA dispatcher`. In our example, we will use the
fifth computer to act as the dispatcher and from there run::

  $ python -m gensim.models.lsi_dispatcher &

In general, the dispatcher can be run on the same machine as one of the worker nodes, or it
can be another, distinct computer (within the same broadcast domain). The dispatcher
won't be doing much with CPU most of the time, but pick a computer with ample memory.

And that's it! The cluster is set up and running, ready to accept jobs. To remove
a worker later on, simply terminate its `lsi_worker` process. To add another worker, run another
`lsi_worker` (this will not affect a computation that is already running, the additions/deletions are not dynamic).
If you terminate `lsi_dispatcher`, you won't be able to run computations until you run it again
(surviving worker processes can be re-used though).


Running LSA
____________

So let's test our setup and run one computation of distributed LSA. Open a Python
shell on one of the five machines (again, this can be done on any computer
in the same `broadcast domain <http://en.wikipedia.org/wiki/Broadcast_domain>`_,
our choice is incidental) and try:

.. sourcecode:: pycon

    >>> from gensim import corpora, models
    >>> import logging
    >>>
    >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    >>>
    >>> corpus = corpora.MmCorpus('/tmp/deerwester.mm')  # load a corpus of nine documents, from the Tutorials
    >>> id2word = corpora.Dictionary.load('/tmp/deerwester.dict')
    >>>
    >>> # run distributed LSA on nine documents
    >>> lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True)

If you look at the log in your Python session, you should see a line similar to::

  2010-08-09 23:44:25,746 : INFO : using distributed version with 8 workers

which means all went well. You can also check the logs coming from your worker and dispatcher
processes --- this is especially helpful in case of problems.
To check the LSA results, let's print the first two latent topics:

.. sourcecode:: pycon

    >>> lsi.print_topics(num_topics=2, num_words=5)
    topic #0(3.341): 0.644*"system" + 0.404*"user" + 0.301*"eps" + 0.265*"time" + 0.265*"response"
    topic #1(2.542): 0.623*"graph" + 0.490*"trees" + 0.451*"minors" + 0.274*"survey" + -0.167*"system"

Success! But a corpus of nine documents is no challenge for our powerful cluster...
In fact, we had to lower the job size (`chunksize` parameter above) to a single document
at a time, otherwise all documents would be processed by a single worker all at once.

So let's run LSA on **one million documents** instead

.. sourcecode:: pycon

    >>> # inflate the corpus to 1M documents, by repeating its documents over&over
    >>> corpus1m = utils.RepeatCorpus(corpus, 1000000)
    >>> # run distributed LSA on 1 million documents
    >>> lsi1m = models.LsiModel(corpus1m, id2word=id2word, num_topics=200, chunksize=10000, distributed=True)
    >>>
    >>> lsi1m.print_topics(num_topics=2, num_words=5)
    topic #0(1113.628): 0.644*"system" + 0.404*"user" + 0.301*"eps" + 0.265*"time" + 0.265*"response"
    topic #1(847.233): 0.623*"graph" + 0.490*"trees" + 0.451*"minors" + 0.274*"survey" + -0.167*"system"

The log from 1M LSA should look like::

  2010-08-10 02:46:35,087 : INFO : using distributed version with 8 workers
  2010-08-10 02:46:35,087 : INFO : updating SVD with new documents
  2010-08-10 02:46:35,202 : INFO : dispatched documents up to #10000
  2010-08-10 02:46:35,296 : INFO : dispatched documents up to #20000
  ...
  2010-08-10 02:46:46,524 : INFO : dispatched documents up to #990000
  2010-08-10 02:46:46,694 : INFO : dispatched documents up to #1000000
  2010-08-10 02:46:46,694 : INFO : reached the end of input; now waiting for all remaining jobs to finish
  2010-08-10 02:46:47,195 : INFO : all jobs finished, downloading final projection
  2010-08-10 02:46:47,200 : INFO : decomposition complete

Due to the small vocabulary size and trivial structure of our "one-million corpus", the computation
of LSA still takes only 12 seconds. To really stress-test our cluster, let's do
Latent Semantic Analysis on the English Wikipedia.

Distributed LSA on Wikipedia
++++++++++++++++++++++++++++++

First, download and prepare the Wikipedia corpus as per :doc:`wiki`, then load
the corpus iterator with

.. sourcecode:: pycon

    >>> import logging
    >>> import gensim
    >>>
    >>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    >>>
    >>> # load id->word mapping (the dictionary)
    >>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')
    >>> # load corpus iterator
    >>> mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')
    >>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output
    >>>
    >>> print(mm)
    MmCorpus(3199665 documents, 100000 features, 495547400 non-zero entries)

Now we're ready to run distributed LSA on the English Wikipedia:

.. sourcecode:: pycon

    >>> # extract 400 LSI topics, using a cluster of nodes
    >>> lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400, chunksize=20000, distributed=True)
    >>>
    >>> # print the most contributing words (both positively and negatively) for each of the first ten topics
    >>> lsi.print_topics(10)
    2010-11-03 16:08:27,602 : INFO : topic #0(200.990): -0.475*"delete" + -0.383*"deletion" + -0.275*"debate" + -0.223*"comments" + -0.220*"edits" + -0.213*"modify" + -0.208*"appropriate" + -0.194*"subsequent" + -0.155*"wp" + -0.117*"notability"
    2010-11-03 16:08:27,626 : INFO : topic #1(143.129): -0.320*"diff" + -0.305*"link" + -0.199*"image" + -0.171*"www" + -0.162*"user" + 0.149*"delete" + -0.147*"undo" + -0.144*"contribs" + -0.122*"album" + 0.113*"deletion"
    2010-11-03 16:08:27,651 : INFO : topic #2(135.665): -0.437*"diff" + -0.400*"link" + -0.202*"undo" + -0.192*"user" + -0.182*"www" + -0.176*"contribs" + 0.168*"image" + -0.109*"added" + 0.106*"album" + 0.097*"copyright"
    2010-11-03 16:08:27,677 : INFO : topic #3(125.027): -0.354*"image" + 0.239*"age" + 0.218*"median" + -0.213*"copyright" + 0.204*"population" + -0.195*"fair" + 0.195*"income" + 0.167*"census" + 0.165*"km" + 0.162*"households"
    2010-11-03 16:08:27,701 : INFO : topic #4(116.927): -0.307*"image" + 0.195*"players" + -0.184*"median" + -0.184*"copyright" + -0.181*"age" + -0.167*"fair" + -0.162*"income" + -0.151*"population" + -0.136*"households" + -0.134*"census"
    2010-11-03 16:08:27,728 : INFO : topic #5(100.326): 0.501*"players" + 0.318*"football" + 0.284*"league" + 0.193*"footballers" + 0.142*"image" + 0.133*"season" + 0.119*"cup" + 0.113*"club" + 0.110*"baseball" + 0.103*"f"
    2010-11-03 16:08:27,754 : INFO : topic #6(92.298): -0.411*"album" + -0.275*"albums" + -0.217*"band" + -0.214*"song" + -0.184*"chart" + -0.163*"songs" + -0.160*"singles" + -0.149*"vocals" + -0.139*"guitar" + -0.129*"track"
    2010-11-03 16:08:27,780 : INFO : topic #7(83.811): -0.248*"wikipedia" + -0.182*"keep" + 0.180*"delete" + -0.167*"articles" + -0.152*"your" + -0.150*"my" + 0.144*"film" + -0.130*"we" + -0.123*"think" + -0.120*"user"
    2010-11-03 16:08:27,807 : INFO : topic #8(78.981): 0.588*"film" + 0.460*"films" + -0.130*"album" + -0.127*"station" + 0.121*"television" + 0.115*"poster" + 0.112*"directed" + 0.110*"actors" + -0.096*"railway" + 0.086*"movie"
    2010-11-03 16:08:27,834 : INFO : topic #9(78.620): 0.502*"kategori" + 0.282*"categoria" + 0.248*"kategorija" + 0.234*"kategorie" + 0.172*"–∫–∞—Ç–µ–≥–æ—Ä–∏—è" + 0.165*"categor√≠a" + 0.161*"kategoria" + 0.148*"categorie" + 0.126*"kateg√≥ria" + 0.121*"cat√©gorie"

In serial mode, creating the LSI model of Wikipedia with this **one-pass algorithm**
takes about 5.25h on my laptop (OS X, C2D 2.53GHz, 4GB RAM with `libVec`).
In distributed mode with four workers (Linux, dual-core Xeons of 2Ghz, 4GB RAM
with `ATLAS`), the wallclock time taken drops to 1 hour and 41 minutes. You can
read more about various internal settings and experiments in my `research
paper <http://nlp.fi.muni.cz/~xrehurek/nips/rehurek_nips.pdf>`_.

.. toctree::
   :hidden:
   :maxdepth: 1

   intro
   auto_examples/index
   apiref
   support
   people
.. _apiref:

API Reference
=============

Modules:

.. toctree::
    :maxdepth: 0

    interfaces
    utils
    matutils
    downloader
    corpora/bleicorpus
    corpora/csvcorpus
    corpora/dictionary
    corpora/hashdictionary
    corpora/indexedcorpus
    corpora/lowcorpus
    corpora/malletcorpus
    corpora/mmcorpus
    corpora/opinosiscorpus
    corpora/sharded_corpus
    corpora/svmlightcorpus
    corpora/textcorpus
    corpora/ucicorpus
    corpora/wikicorpus
    models/ldamodel
    models/ldamulticore
    models/ensemblelda
    models/nmf
    models/lsimodel
    models/ldaseqmodel
    models/tfidfmodel
    models/rpmodel
    models/hdpmodel
    models/logentropy_model
    models/normmodel
    models/translation_matrix
    models/lsi_dispatcher
    models/lsi_worker
    models/lda_dispatcher
    models/lda_worker
    models/atmodel
    models/word2vec
    models/keyedvectors
    models/doc2vec
    models/fasttext
    models/_fasttext_bin
    models/phrases
    models/poincare
    models/coherencemodel
    models/basemodel
    models/callbacks
    models/word2vec_inner
    models/doc2vec_inner
    models/fasttext_inner
    similarities/docsim
    similarities/termsim
    similarities/annoy
    similarities/nmslib
    similarities/levenshtein
    similarities/fastss
    test/utils
    topic_coherence/aggregation
    topic_coherence/direct_confirmation_measure
    topic_coherence/indirect_confirmation_measure
    topic_coherence/probability_estimation
    topic_coherence/segmentation
    topic_coherence/text_analysis
    scripts/package_info
    scripts/glove2word2vec
    scripts/make_wikicorpus
    scripts/word2vec_standalone
    scripts/make_wiki_online
    scripts/make_wiki_online_nodebug
    scripts/word2vec2tensor
    scripts/segment_wiki
    parsing/porter
    parsing/preprocessing
.. _people:

People behind Gensim
====================

.. _contributors:

Top Contributors
----------------

See the `full list of contributors on Github <https://github.com/RaRe-Technologies/gensim/graphs/contributors>`_. I'm thankful to all of them.

Let me also highlight the "core pillars" of Gensim here:

- Radim ≈òeh≈Ø≈ôek, `piskvorky <https://github.com/piskvorky>`_: Creator of Gensim AKA me, living in Prague. I prefer not to count the late-night hours volunteered on Gensim development & support since 2009.
- Gordon Mohr, `gojomo <https://github.com/gojomo>`_: Core Gensim contributor from SF USA, key developer of its doc2vec implementation. ‚ÄòOracle Open Source Developer of the Year‚Äô in 2006, creator of the Heritrix web crawler and the ‚Äòmagnet link‚Äô.
- Misha Penkov, `mpenkov <https://github.com/mpenkov>`_: Core maintainer and release manager of Gensim, Smart_open, SQLitedict, and other open source packages. Lives and works in Sapporo, Japan.
- Ivan Menshikh, `menshikh-iv <https://github.com/menshikh-iv>`_: Ex-maintainer and mentor in the RARE student incubator, from Yekaterinburg Russia.

.. figure:: _static/images/misha_radim.jpeg
   :width: 100%
   :alt: Misha (left) and Radim (right) got together in Prague for some open source and badminton :)

   RARE photo: Misha (left) and Radim (right) got together in Prague, for some open source hacking & badminton :)


The following sponsors help open source by `supporting Gensim financially <https://github.com/sponsors/piskvorky>`_:

.. _gold-sponsors:

Gold Sponsors
-------------

`You? <https://github.com/sponsors/piskvorky>`_

.. _silver-sponsors:

Silver Sponsors
---------------

.. figure:: _static/images/wilabs-logo.png
   :target: https://wilabs.com/
   :width: 50%
   :alt: WiLabs

.. _bronze-sponsors:

Bronze Sponsors
---------------

.. figure:: _static/images/eaccidents-logo.png
   :target: https://eaccidents.com/
   :width: 50%
   :alt: EAccidents

.. figure:: _static/images/techtarget-logo.png
   :target: https://www.techtarget.com/
   :width: 50%
   :alt: TechTarget
:orphan:

.. _distributed:

Distributed Computing
=====================

Why distributed computing?
---------------------------

Need to build semantic representation of a corpus that is millions of documents large and it's
taking forever? Have several idle machines at your disposal that you could use?
`Distributed computing <http://en.wikipedia.org/wiki/Distributed_computing>`_ tries
to accelerate computations by splitting a given task into several smaller subtasks,
passing them on to several computing nodes in parallel.

In the context of `gensim`, computing nodes are computers identified by their IP address/port,
and communication happens over TCP/IP. The whole collection of available machines is called
a *cluster*. The distribution is very coarse grained (not
much communication going on), so the network is allowed to be of relatively high latency.

.. warning::
  The primary reason for using distributed computing is making things run faster. In `gensim`,
  most of the time consuming stuff is done inside low-level routines for linear algebra, inside
  NumPy, independent of any `gensim` code.
  **Installing a fast** `BLAS (Basic Linear Algebra) <http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>`_ **library
  for NumPy can improve performance up to 15 times!** So before you start buying those extra computers,
  consider installing a fast, threaded BLAS that is optimized for your particular machine
  (as opposed to a generic, binary-distributed library).
  Options include your vendor's BLAS library (Intel's MKL,
  AMD's ACML, OS X's vecLib, Sun's Sunperf, ...) or some open-source alternative (GotoBLAS, ALTAS).

  To see what BLAS and LAPACK you are using, type into your shell::

    python -c 'import scipy; scipy.show_config()'

Prerequisites
-----------------

For communication between nodes, `gensim` uses `Pyro (PYthon Remote Objects)
<http://pypi.python.org/pypi/Pyro4>`_, version >= 4.27. This is a library for low-level socket communication
and remote procedure calls (RPC) in Python. `Pyro4` is a pure-Python library, so its
installation is quite painless and only involves copying its `*.py` files somewhere onto your Python's import path::

  pip install Pyro4

You don't have to install Pyro to run Gensim, but if you don't, you won't be able
to access the distributed features (i.e., everything will always run in serial mode,
the examples on this page don't apply).


Core concepts
-------------

As always, Gensim strives for a clear and straightforward API (see :ref:`design`).
To this end, *you do not need to make any changes in your code at all* in order to
run it over a cluster of computers!

What you need to do is run a :term:`worker` script (see below) on each of your cluster nodes prior
to starting your computation. Running this script tells `gensim` that it may use the node
as a slave to delegate some work to it. During initialization, the algorithms
inside `gensim` will try to look for and enslave all available worker nodes.

.. glossary::

  Node
    A logical working unit. Can correspond to a single physical machine, but you
    can also run multiple workers on one machine, resulting in multiple
    logical nodes.

  Cluster
    Several nodes which communicate over TCP/IP. Currently, network broadcasting
    is used to discover and connect all communicating nodes, so the nodes must lie
    within the same `broadcast domain <http://en.wikipedia.org/wiki/Broadcast_domain>`_.

  Worker
    A process which is created on each node. To remove a node from your cluster,
    simply kill its worker process.

  Dispatcher
    The dispatcher will be in charge of negotiating all computations, queueing and
    distributing ("dispatching") individual jobs to the workers. Computations never
    "talk" to worker nodes directly, only through this dispatcher. Unlike workers,
    there can only be one active dispatcher at a time in the cluster.


Available distributed algorithms
---------------------------------

.. toctree::
   :maxdepth: 1

   dist_lsi
   dist_lda
.. _support:

=======
Support
=======

**‚ö†Ô∏è Please don't send me private emails unless you have a substantial budget for commercial support (see below).**

Open source support
-------------------

The main communication channel is the free `Gensim mailing list <https://groups.google.com/group/gensim>`_.

This is the preferred way to ask for help, report problems and share insights with the community. Newbie questions are perfectly fine, as long as you've read the :ref:`tutorials <sphx_glr_auto_examples>` and `FAQ <https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ>`_.

FAQ and some useful snippets of code are maintained on GitHub: https://github.com/RARE-Technologies/gensim/wiki/Recipes-&-FAQ.

We're on `Twitter @gensim_py <https://twitter.com/gensim_py>`_. You can also try asking on StackOverflow, using the `gensim tag <http://stackoverflow.com/questions/tagged/gensim>`_, but the mailing list above will give you more authoritative answers, faster.


.. _Commercial support:

Commercial support
------------------

If your company needs commercial support, please consider `becoming a Gensim sponsor <https://github.com/sponsors/piskvorky>`_. How it works: you chip in, we prioritize your tickets.

Corporate sponsorship means sustainability. It allows us to dedicate our time keeping Gensim stable and performant for you.

The Gold Sponsor üëë tier also allows for a commercial non-LGPL license of Gensim.


For developers
--------------

Developers who want to contribute to Gensim are welcome ‚Äì Gensim is an open source project.

First propose your feature / fix on the `Gensim mailing list <https://groups.google.com/group/gensim>`_ and if there is consensus for accepting your contribution, read the `Developer page <https://github.com/RARE-Technologies/gensim/wiki/Developer-page>`_ and implement it. Thanks!

Note that Github is not a medium for asking open-ended questions. Please use the `Gensim mailing list <https://groups.google.com/group/gensim>`_ for that.
:mod:`matutils` -- Math utils
==============================

.. automodule:: gensim.matutils
    :synopsis: Math utils
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`downloader` -- Downloader API for gensim
==============================================

.. automodule:: gensim.downloader
    :synopsis: Downloader API for gensim
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
.. _intro:

===============
What is Gensim?
===============

Gensim is a free open-source Python library for representing
documents as semantic vectors, as efficiently (computer-wise) and painlessly (human-wise) as possible.

.. image:: _static/images/gensim_logo_positive_complete_tb.png
  :width: 600
  :alt: Gensim logo

Gensim is designed to process raw, unstructured digital texts ("*plain text*") using unsupervised machine learning algorithms.

The algorithms in Gensim, such as :class:`~gensim.models.word2vec.Word2Vec`, :class:`~gensim.models.fasttext.FastText`,
Latent Semantic Indexing (LSI, LSA, :class:`~gensim.models.lsimodel.LsiModel`), Latent Dirichlet
Allocation (LDA, :class:`~gensim.models.ldamodel.LdaModel`) etc, automatically discover the semantic
structure of documents by examining statistical
co-occurrence patterns within a corpus of training documents. These algorithms are **unsupervised**,
which means no human input is necessary -- you only need a corpus of plain text documents.

Once these statistical patterns are found, any plain text documents (sentence, phrase, word‚Ä¶) can be succinctly expressed in the new, semantic representation and queried for topical similarity against other documents (words, phrases‚Ä¶).

.. note::
   If the previous paragraphs left you confused, you can read more about the `Vector
   Space Model <http://en.wikipedia.org/wiki/Vector_space_model>`_ and `unsupervised
   document analysis <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_ on Wikipedia.

.. _design:

Design principles
-----------------

We built Gensim from scratch for:

* **Practicality** -- as industry experts, we focus on proven, battle-hardened algorithms to solve real industry problems. More focus on engineering, less on academia.
* **Memory independence** -- there is no need for the whole training corpus to
  reside fully in RAM at any one time. Can process large, web-scale corpora using data streaming.
* **Performance** ‚Äì¬†highly optimized implementations of popular vector space algorithms using C, BLAS and memory-mapping.

Who are "we"? Check the :ref:`people`.

Installation
------------

Gensim is a Python library, so you need `Python <https://www.python.org/downloads/>`_. Gensim supports all Python versions that haven't reached their `end-of-life <https://devguide.python.org/#status-of-python-branches>`_.

If you need with an older Python (such as Python 2.7), you must install an older version of Gensim (such as `Gensim 3.8.3 <https://github.com/RaRe-Technologies/gensim/releases/tag/3.8.3>`_).

To install gensim, simply run::

  pip install --upgrade gensim

Alternatively, you can download the source code from `Github <https://github.com/RARE-Technologies/gensim/>`__
or the `Python Package Index <http://pypi.python.org/pypi/gensim>`_.

After installation, learn how to use Gensim from its :ref:`sphx_glr_auto_examples_core_run_core_concepts.py` tutorials.


.. _Licensing:

Licensing
----------

Gensim is licensed under the OSI-approved `GNU LGPLv2.1 license <http://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html>`_.
This means that it's free for both personal and commercial use, but if you make any
modification to Gensim that you distribute to other people, you have to disclose
the source code of these modifications.

Apart from that, you are free to redistribute Gensim in any way you like, though you're
not allowed to modify its license (doh!).

If LGPL doesn't fit your bill, you can ask for :ref:`Commercial support`.

.. _Academic citing:

Academic citing
---------------

Gensim has been used in `over two thousand research papers and student theses <https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:NaGl4SEjCO4C>`_.

When citing Gensim, please use `this BibTeX entry <bibtex_gensim.bib>`_::

  @inproceedings{rehurek_lrec,
        title = {{Software Framework for Topic Modelling with Large Corpora}},
        author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
        booktitle = {{Proceedings of the LREC 2010 Workshop on New
             Challenges for NLP Frameworks}},
        pages = {45--50},
        year = 2010,
        month = May,
        day = 22,
        publisher = {ELRA},
        address = {Valletta, Malta},
        note={\url{http://is.muni.cz/publication/884893/en}},
        language={English}
  }

Gensim = "Generate Similar"
---------------------------

Historically, Gensim started off as a collection of Python scripts for the Czech Digital Mathematics Library `dml.cz <http://dml.cz/>`_ project, back in 2008. The scripts served to generate a short list of the most similar math articles to a given article.

I (Radim) also wanted to try these fancy "Latent Semantic Methods", but the libraries that realized the necessary computation were `not much fun to work with <http://soi.stanford.edu/~rmunk/PROPACK/>`_.

Naturally, I set out to reinvent the wheel. Our `2010 LREC publication <http://radimrehurek.com/lrec2010_final.pdf>`_ describes the initial design decisions behind Gensim: **clarity, efficiency and scalability**. It is fairly representative of how Gensim works even today.

Later versions of Gensim improved this efficiency and scalability tremendously. In fact, I made algorithmic scalability of distributional semantics the topic of my `PhD thesis <http://radimrehurek.com/phd_rehurek.pdf>`_.

By now, Gensim is---to my knowledge---the most robust, efficient and hassle-free piece
of software to realize unsupervised semantic modelling from plain text. It stands
in contrast to brittle homework-assignment-implementations that do not scale on one hand,
and robust java-esque projects that take forever just to run "hello world".

In 2011, I moved Gensim's source code to `Github <https://github.com/piskvorky/gensim>`__
and created the Gensim website. In 2013 Gensim got its current logo, and in 2020 a website redesign.
:orphan:



.. _sphx_glr_auto_examples:

Documentation
=============

We welcome contributions to our documentation via GitHub pull requests, whether it's fixing a typo or authoring an entirely new tutorial or guide.
If you're thinking about contributing documentation, please see :ref:`sphx_glr_auto_examples_howtos_run_doc.py`.


.. raw:: html

    <div class="sphx-glr-clear"></div>



.. _sphx_glr_auto_examples_core:

Core Tutorials: New Users Start Here!
-------------------------------------

If you're new to gensim, we recommend going through all core tutorials in order.
Understanding this functionality is vital for using gensim effectively.



.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This tutorial introduces Documents, Corpora, Vectors and Models: the basic concepts and terms n...">

.. only:: html

 .. figure:: /auto_examples/core/images/thumb/sphx_glr_run_core_concepts_thumb.png
     :alt: Core Concepts

     :ref:`sphx_glr_auto_examples_core_run_core_concepts.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/core/run_core_concepts

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates transforming text into a vector space representation.">

.. only:: html

 .. figure:: /auto_examples/core/images/thumb/sphx_glr_run_corpora_and_vector_spaces_thumb.png
     :alt: Corpora and Vector Spaces

     :ref:`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/core/run_corpora_and_vector_spaces

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces transformations and demonstrates their use on a toy corpus. ">

.. only:: html

 .. figure:: /auto_examples/core/images/thumb/sphx_glr_run_topics_and_transformations_thumb.png
     :alt: Topics and Transformations

     :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/core/run_topics_and_transformations

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates querying a corpus for similar documents. ">

.. only:: html

 .. figure:: /auto_examples/core/images/thumb/sphx_glr_run_similarity_queries_thumb.png
     :alt: Similarity Queries

     :ref:`sphx_glr_auto_examples_core_run_similarity_queries.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/core/run_similarity_queries
.. raw:: html

    <div class="sphx-glr-clear"></div>



.. _sphx_glr_auto_examples_tutorials:

Tutorials: Learning Oriented Lessons
------------------------------------

Learning-oriented lessons that introduce a particular gensim feature, e.g. a model (Word2Vec, FastText) or technique (similarity queries or text summarization).



.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces Gensim&#x27;s Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus &lt;http...">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_word2vec_thumb.png
     :alt: Word2Vec Model

     :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_word2vec

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces Gensim&#x27;s Doc2Vec model and demonstrates its use on the `Lee Corpus &lt;https://hekyll.s...">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_doc2vec_lee_thumb.png
     :alt: Doc2Vec Model

     :ref:`sphx_glr_auto_examples_tutorials_run_doc2vec_lee.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_doc2vec_lee

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces Gensim&#x27;s EnsembleLda model">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_ensemblelda_thumb.png
     :alt: Ensemble LDA

     :ref:`sphx_glr_auto_examples_tutorials_run_ensemblelda.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_ensemblelda

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces Gensim&#x27;s fastText model and demonstrates its use on the Lee Corpus. ">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_fasttext_thumb.png
     :alt: FastText Model

     :ref:`sphx_glr_auto_examples_tutorials_run_fasttext.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_fasttext

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces the Annoy library for similarity queries on top of vectors learned by Word2Vec. ">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_annoy_thumb.png
     :alt: Fast Similarity Queries with Annoy and Word2Vec

     :ref:`sphx_glr_auto_examples_tutorials_run_annoy.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_annoy

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduces Gensim&#x27;s LDA model and demonstrates its use on the NIPS corpus.">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_lda_thumb.png
     :alt: LDA Model

     :ref:`sphx_glr_auto_examples_tutorials_run_lda.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_lda

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates using Gensim&#x27;s implemenation of the SCM.">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_scm_thumb.png
     :alt: Soft Cosine Measure

     :ref:`sphx_glr_auto_examples_tutorials_run_scm.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_scm

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates using Gensim&#x27;s implemenation of the WMD.">

.. only:: html

 .. figure:: /auto_examples/tutorials/images/thumb/sphx_glr_run_wmd_thumb.png
     :alt: Word Mover's Distance

     :ref:`sphx_glr_auto_examples_tutorials_run_wmd.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/tutorials/run_wmd
.. raw:: html

    <div class="sphx-glr-clear"></div>



.. _sphx_glr_auto_examples_howtos:

How-to Guides: Solve a Problem
------------------------------

These **goal-oriented guides** demonstrate how to **solve a specific problem** using gensim.



.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates simple and quick access to common corpora and pretrained models. ">

.. only:: html

 .. figure:: /auto_examples/howtos/images/thumb/sphx_glr_run_downloader_api_thumb.png
     :alt: How to download pre-trained models and corpora

     :ref:`sphx_glr_auto_examples_howtos_run_downloader_api.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/howtos/run_downloader_api

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="How to author documentation for Gensim. ">

.. only:: html

 .. figure:: /auto_examples/howtos/images/thumb/sphx_glr_run_doc_thumb.png
     :alt: How to Author Gensim Documentation

     :ref:`sphx_glr_auto_examples_howtos_run_doc.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/howtos/run_doc

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Shows how to reproduce results of the &quot;Distributed Representation of Sentences and Documents&quot; p...">

.. only:: html

 .. figure:: /auto_examples/howtos/images/thumb/sphx_glr_run_doc2vec_imdb_thumb.png
     :alt: How to reproduce the doc2vec 'Paragraph Vector' paper

     :ref:`sphx_glr_auto_examples_howtos_run_doc2vec_imdb.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/howtos/run_doc2vec_imdb

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrates how you can visualize and compare trained topic models.">

.. only:: html

 .. figure:: /auto_examples/howtos/images/thumb/sphx_glr_run_compare_lda_thumb.png
     :alt: How to Compare LDA Models

     :ref:`sphx_glr_auto_examples_howtos_run_compare_lda.py`

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/howtos/run_compare_lda
.. raw:: html

    <div class="sphx-glr-clear"></div>



.. _sphx_glr_auto_examples_other:

Other Resources
---------------

Blog posts, tutorial videos, hackathons and other useful Gensim resources, from around the internet.

- *Use FastText or Word2Vec?* Comparison of embedding quality and performance. `Jupyter Notebook <https://github.com/RaRe-Technologies/gensim/blob/ba1ce894a5192fc493a865c535202695bb3c0424/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>`__
- Multiword phrases extracted from *How I Met Your Mother*. `Blog post by Mark Needham <http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/>`__
- *Using Gensim LDA for hierarchical document clustering*. `Jupyter notebook by Brandon Rose <http://brandonrose.org/clustering#Latent-Dirichlet-Allocation>`__
- *Evolution of Voldemort topic through the 7 Harry Potter books*. `Blog post <http://rare-technologies.com/understanding-and-coding-dynamic-topic-models/>`__
- *Movie plots by genre*: Document classification using various techniques: TF-IDF, word2vec averaging, Deep IR, Word Movers Distance and doc2vec. `Github repo <https://github.com/RaRe-Technologies/movie-plots-by-genre>`__
- *Word2vec: Faster than Google? Optimization lessons in Python*, talk by Radim ≈òeh≈Ø≈ôek at PyData Berlin 2014. `Youtube video <https://www.youtube.com/watch?v=vU4TlwZzTfU>`__
- *Word2vec & friends*, talk by Radim ≈òeh≈Ø≈ôek at MLMU.cz 7.1.2015. `Youtube video <https://www.youtube.com/watch?v=wTp3P2UnTfQ>`__

..
   - ? `Making an Impact with NLP <https://www.youtube.com/watch?v=oSSnDeOXTZQ>`__ -- Pycon 2016 Tutorial by Hobsons Lane
   - ? `NLP with NLTK and Gensim <https://www.youtube.com/watch?v=itKNpCPHq3I>`__ -- Pycon 2016 Tutorial by Tony Ojeda, Benjamin Bengfort, Laura Lorenz from District Data Labs
   - ? `Word Embeddings for Fun and Profit <https://www.youtube.com/watch?v=lfqW46u0UKc>`__ -- Talk at PyData London 2016 talk by Lev Konstantinovskiy. See accompanying `repo <https://github.com/RaRe-Technologies/movie-plots-by-genre>`__
   - ? English Wikipedia; TODO: convert to proper .py format
   - ? `Colouring words by topic in a document, print words in a
     topics <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb>`__
   - ? `Topic Coherence, a metric that correlates that human judgement on topic quality. <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence_tutorial.ipynb>`__
   - ? `America's Next Topic Model slides <https://speakerdeck.com/tmylk/americas-next-topic-model-at-pydata-berlin-august-2016?slide=7>`__
      - How to choose your next topic model, presented at Pydata Berlin 10 August 2016 by Lev Konstantinovsky
   - ?  `Dynamic Topic Modeling and Dynamic Influence Model Tutorial <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb>`__
   - ?  `Python Dynamic Topic Modelling Theory and Tutorial <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb>`__
   - ? `Word Movers Distance for Yelp Reviews tutorial <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb>`__
     - FIXME WMD superceded by soft cosine similarity = faster and better? any numbers / tutorials for that?
   - ? `Great illustration of corpus preparation <https://linanqiu.github.io/2015/10/07/word2vec-sentiment/>`__, `Code <https://github.com/linanqiu/word2vec-sentiments>`__
     - ? `Alternative <https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1#.nv2lxvbj1>`__,
     - ? `Alternative 2 <https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis>`__
   - ? `Doc2Vec on customer reviews <http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/>`__
   - ? `Doc2Vec on Airline Tweets Sentiment Analysis <https://www.zybuluo.com/HaomingJiang/note/462804>`__
   - ? `Deep Inverse Regression with Yelp Reviews <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb>`__ (Document Classification using Bayesian Inversion and several word2vec models, one for each class)


.. raw:: html

    <div class="sphx-glr-clear"></div>



.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-gallery


  .. container:: sphx-glr-download sphx-glr-download-python

    :download:`Download all examples in Python source code: auto_examples_python.zip <//Volumes/work/workspace/gensim/trunk/docs/src/auto_examples/auto_examples_python.zip>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

    :download:`Download all examples in Jupyter notebooks: auto_examples_jupyter.zip <//Volumes/work/workspace/gensim/trunk/docs/src/auto_examples/auto_examples_jupyter.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

:orphan:

.. _sphx_glr_auto_examples_core_sg_execution_times:

Computation times
=================
**00:03.242** total execution time for **auto_examples_core** files:

+--------------------------------------------------------------------------------------------------------------+-----------+---------+
| :ref:`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py` (``run_corpora_and_vector_spaces.py``)   | 00:03.242 | 48.2 MB |
+--------------------------------------------------------------------------------------------------------------+-----------+---------+
| :ref:`sphx_glr_auto_examples_core_run_core_concepts.py` (``run_core_concepts.py``)                           | 00:00.000 | 0.0 MB  |
+--------------------------------------------------------------------------------------------------------------+-----------+---------+
| :ref:`sphx_glr_auto_examples_core_run_similarity_queries.py` (``run_similarity_queries.py``)                 | 00:00.000 | 0.0 MB  |
+--------------------------------------------------------------------------------------------------------------+-----------+---------+
| :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py` (``run_topics_and_transformations.py``) | 00:00.000 | 0.0 MB  |
+--------------------------------------------------------------------------------------------------------------+-----------+---------+
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_core_run_core_concepts.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_core_run_core_concepts.py:


Core Concepts
=============

This tutorial introduces Documents, Corpora, Vectors and Models: the basic concepts and terms needed to understand and use gensim.


.. code-block:: default


    import pprint








The core concepts of ``gensim`` are:

1. :ref:`core_concepts_document`: some text.
2. :ref:`core_concepts_corpus`: a collection of documents.
3. :ref:`core_concepts_vector`: a mathematically convenient representation of a document.
4. :ref:`core_concepts_model`: an algorithm for transforming vectors from one representation to another.

Let's examine each of these in slightly more detail.

.. _core_concepts_document:

Document
--------

In Gensim, a *document* is an object of the `text sequence type <https://docs.python.org/3.7/library/stdtypes.html#text-sequence-type-str>`_ (commonly known as ``str`` in Python 3).
A document could be anything from a short 140 character tweet, a single
paragraph (i.e., journal article abstract), a news article, or a book.



.. code-block:: default

    document = "Human machine interface for lab abc computer applications"








.. _core_concepts_corpus:

Corpus
------

A *corpus* is a collection of :ref:`core_concepts_document` objects.
Corpora serve two roles in Gensim:

1. Input for training a :ref:`core_concepts_model`.
   During training, the models use this *training corpus* to look for common
   themes and topics, initializing their internal model parameters.

   Gensim focuses on *unsupervised* models so that no human intervention,
   such as costly annotations or tagging documents by hand, is required.

2. Documents to organize.
   After training, a topic model can be used to extract topics from new
   documents (documents not seen in the training corpus).

   Such corpora can be indexed for
   :ref:`sphx_glr_auto_examples_core_run_similarity_queries.py`,
   queried by semantic similarity, clustered etc.

Here is an example corpus.
It consists of 9 documents, where each document is a string consisting of a single sentence.



.. code-block:: default

    text_corpus = [
        "Human machine interface for lab abc computer applications",
        "A survey of user opinion of computer system response time",
        "The EPS user interface management system",
        "System and human system engineering testing of EPS",
        "Relation of user perceived response time to error measurement",
        "The generation of random binary unordered trees",
        "The intersection graph of paths in trees",
        "Graph minors IV Widths of trees and well quasi ordering",
        "Graph minors A survey",
    ]








.. Important::
  The above example loads the entire corpus into memory.
  In practice, corpora may be very large, so loading them into memory may be impossible.
  Gensim intelligently handles such corpora by *streaming* them one document at a time.
  See :ref:`corpus_streaming_tutorial` for details.

This is a particularly small example of a corpus for illustration purposes.
Another example could be a list of all the plays written by Shakespeare, list
of all wikipedia articles, or all tweets by a particular person of interest.

After collecting our corpus, there are typically a number of preprocessing
steps we want to undertake. We'll keep it simple and just remove some
commonly used English words (such as 'the') and words that occur only once in
the corpus. In the process of doing so, we'll tokenize our data.
Tokenization breaks up the documents into words (in this case using space as
a delimiter).

.. Important::
  There are better ways to perform preprocessing than just lower-casing and
  splitting by space.  Effective preprocessing is beyond the scope of this
  tutorial: if you're interested, check out the
  :py:func:`gensim.utils.simple_preprocess` function.



.. code-block:: default


    # Create a set of frequent words
    stoplist = set('for a of the and to in'.split(' '))
    # Lowercase each document, split it by white space and filter out stopwords
    texts = [[word for word in document.lower().split() if word not in stoplist]
             for document in text_corpus]

    # Count word frequencies
    from collections import defaultdict
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1

    # Only keep words that appear more than once
    processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]
    pprint.pprint(processed_corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [['human', 'interface', 'computer'],
     ['survey', 'user', 'computer', 'system', 'response', 'time'],
     ['eps', 'user', 'interface', 'system'],
     ['system', 'human', 'system', 'eps'],
     ['user', 'response', 'time'],
     ['trees'],
     ['graph', 'trees'],
     ['graph', 'minors', 'trees'],
     ['graph', 'minors', 'survey']]




Before proceeding, we want to associate each word in the corpus with a unique
integer ID. We can do this using the :py:class:`gensim.corpora.Dictionary`
class.  This dictionary defines the vocabulary of all words that our
processing knows about.



.. code-block:: default

    from gensim import corpora

    dictionary = corpora.Dictionary(processed_corpus)
    print(dictionary)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)




Because our corpus is small, there are only 12 different tokens in this
:py:class:`gensim.corpora.Dictionary`. For larger corpuses, dictionaries that
contains hundreds of thousands of tokens are quite common.


.. _core_concepts_vector:

Vector
------

To infer the latent structure in our corpus we need a way to represent
documents that we can manipulate mathematically. One approach is to represent
each document as a vector of *features*.
For example, a single feature may be thought of as a question-answer pair:

1. How many times does the word *splonge* appear in the document? Zero.
2. How many paragraphs does the document consist of? Two.
3. How many fonts does the document use? Five.

The question is usually represented only by its integer id (such as `1`, `2` and `3`).
The representation of this document then becomes a series of pairs like ``(1, 0.0), (2, 2.0), (3, 5.0)``.
This is known as a *dense vector*, because it contains an explicit answer to each of the above questions.

If we know all the questions in advance, we may leave them implicit
and simply represent the document as ``(0, 2, 5)``.
This sequence of answers is the **vector** for our document (in this case a 3-dimensional dense vector).
For practical purposes, only questions to which the answer is (or
can be converted to) a *single floating point number* are allowed in Gensim.

In practice, vectors often consist of many zero values.
To save memory, Gensim omits all vector elements with value 0.0.
The above example thus becomes ``(2, 2.0), (3, 5.0)``.
This is known as a *sparse vector* or *bag-of-words vector*.
The values of all missing features in this sparse representation can be unambiguously resolved to zero, ``0.0``.

Assuming the questions are the same, we can compare the vectors of two different documents to each other.
For example, assume we are given two vectors ``(0.0, 2.0, 5.0)`` and ``(0.1, 1.9, 4.9)``.
Because the vectors are very similar to each other, we can conclude that the documents corresponding to those vectors are similar, too.
Of course, the correctness of that conclusion depends on how well we picked the questions in the first place.

Another approach to represent a document as a vector is the *bag-of-words
model*.
Under the bag-of-words model each document is represented by a vector
containing the frequency counts of each word in the dictionary.
For example, assume we have a dictionary containing the words
``['coffee', 'milk', 'sugar', 'spoon']``.
A document consisting of the string ``"coffee milk coffee"`` would then
be represented by the vector ``[2, 1, 0, 0]`` where the entries of the vector
are (in order) the occurrences of "coffee", "milk", "sugar" and "spoon" in
the document. The length of the vector is the number of entries in the
dictionary. One of the main properties of the bag-of-words model is that it
completely ignores the order of the tokens in the document that is encoded,
which is where the name bag-of-words comes from.

Our processed corpus has 12 unique words in it, which means that each
document will be represented by a 12-dimensional vector under the
bag-of-words model. We can use the dictionary to turn tokenized documents
into these 12-dimensional vectors. We can see what these IDs correspond to:



.. code-block:: default

    pprint.pprint(dictionary.token2id)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {'computer': 0,
     'eps': 8,
     'graph': 10,
     'human': 1,
     'interface': 2,
     'minors': 11,
     'response': 3,
     'survey': 4,
     'system': 5,
     'time': 6,
     'trees': 9,
     'user': 7}




For example, suppose we wanted to vectorize the phrase "Human computer
interaction" (note that this phrase was not in our original corpus). We can
create the bag-of-word representation for a document using the ``doc2bow``
method of the dictionary, which returns a sparse representation of the word
counts:



.. code-block:: default


    new_doc = "Human computer interaction"
    new_vec = dictionary.doc2bow(new_doc.lower().split())
    print(new_vec)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 1), (1, 1)]




The first entry in each tuple corresponds to the ID of the token in the
dictionary, the second corresponds to the count of this token.

Note that "interaction" did not occur in the original corpus and so it was
not included in the vectorization. Also note that this vector only contains
entries for words that actually appeared in the document. Because any given
document will only contain a few words out of the many words in the
dictionary, words that do not appear in the vectorization are represented as
implicitly zero as a space saving measure.

We can convert our entire original corpus to a list of vectors:



.. code-block:: default

    bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]
    pprint.pprint(bow_corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [[(0, 1), (1, 1), (2, 1)],
     [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],
     [(2, 1), (5, 1), (7, 1), (8, 1)],
     [(1, 1), (5, 2), (8, 1)],
     [(3, 1), (6, 1), (7, 1)],
     [(9, 1)],
     [(9, 1), (10, 1)],
     [(9, 1), (10, 1), (11, 1)],
     [(4, 1), (10, 1), (11, 1)]]




Note that while this list lives entirely in memory, in most applications you
will want a more scalable solution. Luckily, ``gensim`` allows you to use any
iterator that returns a single document vector at a time. See the
documentation for more details.

.. Important::
  The distinction between a document and a vector is that the former is text,
  and the latter is a mathematically convenient representation of the text.
  Sometimes, people will use the terms interchangeably: for example, given
  some arbitrary document ``D``, instead of saying "the vector that
  corresponds to document ``D``", they will just say "the vector ``D``" or
  the "document ``D``".  This achieves brevity at the cost of ambiguity.

  As long as you remember that documents exist in document space, and that
  vectors exist in vector space, the above ambiguity is acceptable.

.. Important::
  Depending on how the representation was obtained, two different documents
  may have the same vector representations.

.. _core_concepts_model:

Model
-----

Now that we have vectorized our corpus we can begin to transform it using
*models*. We use model as an abstract term referring to a *transformation* from
one document representation to another. In ``gensim`` documents are
represented as vectors so a model can be thought of as a transformation
between two vector spaces. The model learns the details of this
transformation during training, when it reads the training
:ref:`core_concepts_corpus`.

One simple example of a model is `tf-idf
<https://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_.  The tf-idf model
transforms vectors from the bag-of-words representation to a vector space
where the frequency counts are weighted according to the relative rarity of
each word in the corpus.

Here's a simple example. Let's initialize the tf-idf model, training it on
our corpus and transforming the string "system minors":



.. code-block:: default


    from gensim import models

    # train the model
    tfidf = models.TfidfModel(bow_corpus)

    # transform the "system minors" string
    words = "system minors".lower().split()
    print(tfidf[dictionary.doc2bow(words)])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(5, 0.5898341626740045), (11, 0.8075244024440723)]




The ``tfidf`` model again returns a list of tuples, where the first entry is
the token ID and the second entry is the tf-idf weighting. Note that the ID
corresponding to "system" (which occurred 4 times in the original corpus) has
been weighted lower than the ID corresponding to "minors" (which only
occurred twice).

You can save trained models to disk and later load them back, either to
continue training on new training documents or to transform new documents.

``gensim`` offers a number of different models/transformations.
For more, see :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`.

Once you've created the model, you can do all sorts of cool stuff with it.
For example, to transform the whole corpus via TfIdf and index it, in
preparation for similarity queries:



.. code-block:: default

    from gensim import similarities

    index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)








and to query the similarity of our query document ``query_document`` against every document in the corpus:


.. code-block:: default

    query_document = 'system engineering'.split()
    query_bow = dictionary.doc2bow(query_document)
    sims = index[tfidf[query_bow]]
    print(list(enumerate(sims)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]




How to read this output?
Document 3 has a similarity score of 0.718=72%, document 2 has a similarity score of 42% etc.
We can make this slightly more readable by sorting:


.. code-block:: default


    for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):
        print(document_number, score)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    3 0.7184812
    2 0.41707572
    1 0.32448703
    0 0.0
    4 0.0
    5 0.0
    6 0.0
    7 0.0
    8 0.0




Summary
-------

The core concepts of ``gensim`` are:

1. :ref:`core_concepts_document`: some text.
2. :ref:`core_concepts_corpus`: a collection of documents.
3. :ref:`core_concepts_vector`: a mathematically convenient representation of a document.
4. :ref:`core_concepts_model`: an algorithm for transforming vectors from one representation to another.

We saw these concepts in action.
First, we started with a corpus of documents.
Next, we transformed these documents to a vector space representation.
After that, we created a model that transformed our original vector representation to TfIdf.
Finally, we used our model to calculate the similarity between some query document and all documents in the corpus.

What Next?
----------

There's still much more to learn about :ref:`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py`.


.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('run_core_concepts.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image:: /auto_examples/core/images/sphx_glr_run_core_concepts_001.png
    :alt: run core concepts
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.675 seconds)

**Estimated memory usage:**  37 MB


.. _sphx_glr_download_auto_examples_core_run_core_concepts.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_core_concepts.py <run_core_concepts.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_core_concepts.ipynb <run_core_concepts.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_core_run_topics_and_transformations.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_core_run_topics_and_transformations.py:


Topics and Transformations
===========================

Introduces transformations and demonstrates their use on a toy corpus.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








In this tutorial, I will show how to transform documents from one vector representation
into another. This process serves two goals:

1. To bring out hidden structure in the corpus, discover relationships between
   words and use them to describe the documents in a new and
   (hopefully) more semantic way.
2. To make the document representation more compact. This both improves efficiency
   (new representation consumes less resources) and efficacy (marginal data
   trends are ignored, noise-reduction).

Creating the Corpus
-------------------

First, we need to create a corpus to work with.
This step is the same as in the previous tutorial;
if you completed it, feel free to skip to the next section.


.. code-block:: default


    from collections import defaultdict
    from gensim import corpora

    documents = [
        "Human machine interface for lab abc computer applications",
        "A survey of user opinion of computer system response time",
        "The EPS user interface management system",
        "System and human system engineering testing of EPS",
        "Relation of user perceived response time to error measurement",
        "The generation of random binary unordered trees",
        "The intersection graph of paths in trees",
        "Graph minors IV Widths of trees and well quasi ordering",
        "Graph minors A survey",
    ]

    # remove common words and tokenize
    stoplist = set('for a of the and to in'.split())
    texts = [
        [word for word in document.lower().split() if word not in stoplist]
        for document in documents
    ]

    # remove words that appear only once
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1

    texts = [
        [token for token in text if frequency[token] > 1]
        for text in texts
    ]

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]








Creating a transformation
++++++++++++++++++++++++++

The transformations are standard Python objects, typically initialized by means of
a :dfn:`training corpus`:



.. code-block:: default

    from gensim import models

    tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model








We used our old corpus from tutorial 1 to initialize (train) the transformation model. Different
transformations may require different initialization parameters; in case of TfIdf, the
"training" consists simply of going through the supplied corpus once and computing document frequencies
of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet
Allocation, is much more involved and, consequently, takes much more time.

.. note::
  Transformations always convert between two specific vector
  spaces. The same vector space (= the same set of feature ids) must be used for training
  as well as for subsequent vector transformations. Failure to use the same input
  feature space, such as applying a different string preprocessing, using different
  feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will
  result in feature mismatch during transformation calls and consequently in either
  garbage output and/or runtime exceptions.


Transforming vectors
+++++++++++++++++++++

From now on, ``tfidf`` is treated as a read-only object that can be used to convert
any vector from the old representation (bag-of-words integer counts) to the new representation
(TfIdf real-valued weights):


.. code-block:: default


    doc_bow = [(0, 1), (1, 1)]
    print(tfidf[doc_bow])  # step 2 -- use the model to transform vectors





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.7071067811865476), (1, 0.7071067811865476)]




Or to apply a transformation to a whole corpus:


.. code-block:: default


    corpus_tfidf = tfidf[corpus]
    for doc in corpus_tfidf:
        print(doc)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]
    [(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]
    [(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]
    [(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]
    [(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]
    [(9, 1.0)]
    [(9, 0.7071067811865475), (10, 0.7071067811865475)]
    [(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]
    [(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]




In this particular case, we are transforming the same corpus that we used
for training, but this is only incidental. Once the transformation model has been initialized,
it can be used on any vectors (provided they come from the same vector space, of course),
even if they were not used in the training corpus at all. This is achieved by a process called
folding-in for LSA, by topic inference for LDA etc.

.. note::
  Calling ``model[corpus]`` only creates a wrapper around the old ``corpus``
  document stream -- actual conversions are done on-the-fly, during document iteration.
  We cannot convert the entire corpus at the time of calling ``corpus_transformed = model[corpus]``,
  because that would mean storing the result in main memory, and that contradicts gensim's objective of memory-indepedence.
  If you will be iterating over the transformed ``corpus_transformed`` multiple times, and the
  transformation is costly, :ref:`serialize the resulting corpus to disk first <corpus-formats>` and continue
  using that.

Transformations can also be serialized, one on top of another, in a sort of chain:


.. code-block:: default


    lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation
    corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi








Here we transformed our Tf-Idf corpus via `Latent Semantic Indexing <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_
into a latent 2-D space (2-D because we set ``num_topics=2``). Now you're probably wondering: what do these two latent
dimensions stand for? Let's inspect with :func:`models.LsiModel.print_topics`:


.. code-block:: default


    lsi_model.print_topics(2)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    [(0, '0.703*"trees" + 0.538*"graph" + 0.402*"minors" + 0.187*"survey" + 0.061*"system" + 0.060*"response" + 0.060*"time" + 0.058*"user" + 0.049*"computer" + 0.035*"interface"'), (1, '-0.460*"system" + -0.373*"user" + -0.332*"eps" + -0.328*"interface" + -0.320*"response" + -0.320*"time" + -0.293*"computer" + -0.280*"human" + -0.171*"survey" + 0.161*"trees"')]



(the topics are printed to log -- see the note at the top of this page about activating
logging)

It appears that according to LSI, "trees", "graph" and "minors" are all related
words (and contribute the most to the direction of the first topic), while the
second topic practically concerns itself with all the other words. As expected,
the first five documents are more strongly related to the second topic while the
remaining four documents to the first topic:


.. code-block:: default


    # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly
    for doc, as_text in zip(corpus_lsi, documents):
        print(doc, as_text)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.06600783396090518), (1, -0.520070330636184)] Human machine interface for lab abc computer applications
    [(0, 0.19667592859142694), (1, -0.7609563167700047)] A survey of user opinion of computer system response time
    [(0, 0.08992639972446678), (1, -0.72418606267525)] The EPS user interface management system
    [(0, 0.07585847652178407), (1, -0.6320551586003417)] System and human system engineering testing of EPS
    [(0, 0.10150299184980252), (1, -0.5737308483002961)] Relation of user perceived response time to error measurement
    [(0, 0.7032108939378307), (1, 0.16115180214025954)] The generation of random binary unordered trees
    [(0, 0.8774787673119826), (1, 0.16758906864659615)] The intersection graph of paths in trees
    [(0, 0.9098624686818572), (1, 0.14086553628719237)] Graph minors IV Widths of trees and well quasi ordering
    [(0, 0.6165825350569278), (1, -0.05392907566389235)] Graph minors A survey




Model persistency is achieved with the :func:`save` and :func:`load` functions:


.. code-block:: default

    import os
    import tempfile

    with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:
        lsi_model.save(tmp.name)  # same for tfidf, lda, ...

    loaded_lsi_model = models.LsiModel.load(tmp.name)

    os.unlink(tmp.name)








The next question might be: just how exactly similar are those documents to each other?
Is there a way to formalize the similarity, so that for a given input document, we can
order some other set of documents according to their similarity? Similarity queries
are covered in the next tutorial (:ref:`sphx_glr_auto_examples_core_run_similarity_queries.py`).

.. _transformations:

Available transformations
--------------------------

Gensim implements several popular Vector Space Model algorithms:

* `Term Frequency * Inverse Document Frequency, Tf-Idf <http://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_
  expects a bag-of-words (integer values) training corpus during initialization.
  During transformation, it will take a vector and return another vector of the
  same dimensionality, except that features which were rare in the training corpus
  will have their value increased.
  It therefore converts integer-valued vectors into real-valued ones, while leaving
  the number of dimensions intact. It can also optionally normalize the resulting
  vectors to (Euclidean) unit length.

 .. sourcecode:: pycon

    model = models.TfidfModel(corpus, normalize=True)

* `Latent Semantic Indexing, LSI (or sometimes LSA) <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_
  transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into
  a latent space of a lower dimensionality. For the toy corpus above we used only
  2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended
  as a "golden standard" [1]_.

  .. sourcecode:: pycon

    model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)

  LSI training is unique in that we can continue "training" at any point, simply
  by providing more training documents. This is done by incremental updates to
  the underlying model, in a process called `online training`. Because of this feature, the
  input document stream may even be infinite -- just keep feeding LSI new documents
  as they arrive, while using the computed transformation model as read-only in the meanwhile!

  .. sourcecode:: pycon

    model.add_documents(another_tfidf_corpus)  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus
    lsi_vec = model[tfidf_vec]  # convert some new document into the LSI space, without affecting the model

    model.add_documents(more_documents)  # tfidf_corpus + another_tfidf_corpus + more_documents
    lsi_vec = model[tfidf_vec]

  See the :mod:`gensim.models.lsimodel` documentation for details on how to make
  LSI gradually "forget" old observations in infinite streams. If you want to get dirty,
  there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical
  precision of the LSI algorithm.

  `gensim` uses a novel online incremental streamed distributed training algorithm (quite a mouthful!),
  which I published in [5]_. `gensim` also executes a stochastic multi-pass algorithm
  from Halko et al. [4]_ internally, to accelerate in-core part
  of the computations.
  See also :ref:`wiki` for further speed-ups by distributing the computation across
  a cluster of computers.

* `Random Projections, RP <http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf>`_ aim to
  reduce vector space dimensionality. This is a very efficient (both memory- and
  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.
  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.

  .. sourcecode:: pycon

    model = models.RpModel(tfidf_corpus, num_topics=500)

* `Latent Dirichlet Allocation, LDA <http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>`_
  is yet another transformation from bag-of-words counts into a topic space of lower
  dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA),
  so LDA's topics can be interpreted as probability distributions over words. These distributions are,
  just like with LSA, inferred automatically from a training corpus. Documents
  are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).

  .. sourcecode:: pycon

    model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)

  `gensim` uses a fast implementation of online LDA parameter estimation based on [2]_,
  modified to run in :ref:`distributed mode <distributed>` on a cluster of computers.

* `Hierarchical Dirichlet Process, HDP <http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>`_
  is a non-parametric bayesian method (note the missing number of requested topics):

  .. sourcecode:: pycon

    model = models.HdpModel(corpus, id2word=dictionary)

  `gensim` uses a fast, online implementation based on [3]_.
  The HDP model is a new addition to `gensim`, and still rough around its academic edges -- use with care.

Adding new :abbr:`VSM (Vector Space Model)` transformations (such as different weighting schemes) is rather trivial;
see the :ref:`apiref` or directly the `Python code <https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py>`_
for more info and examples.

It is worth repeating that these are all unique, **incremental** implementations,
which do not require the whole training corpus to be present in main memory all at once.
With memory taken care of, I am now improving :ref:`distributed`,
to improve CPU efficiency, too.
If you feel you could contribute by testing, providing use-cases or code, see the `Gensim Developer guide <https://github.com/RaRe-Technologies/gensim/wiki/Developer-page>`__.

What Next?
----------

Continue on to the next tutorial on :ref:`sphx_glr_auto_examples_core_run_similarity_queries.py`.

References
----------

.. [1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.

.. [2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.

.. [3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.

.. [4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.

.. [5] ≈òeh≈Ø≈ôek. 2011. Subspace tracking for Latent Semantic Analysis.


.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('run_topics_and_transformations.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image:: /auto_examples/core/images/sphx_glr_run_topics_and_transformations_001.png
    :alt: run topics and transformations
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.970 seconds)

**Estimated memory usage:**  7 MB


.. _sphx_glr_download_auto_examples_core_run_topics_and_transformations.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_topics_and_transformations.py <run_topics_and_transformations.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_topics_and_transformations.ipynb <run_topics_and_transformations.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_core_run_similarity_queries.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_core_run_similarity_queries.py:


Similarity Queries
==================

Demonstrates querying a corpus for similar documents.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








Creating the Corpus
-------------------

First, we need to create a corpus to work with.
This step is the same as in the previous tutorial;
if you completed it, feel free to skip to the next section.


.. code-block:: default


    from collections import defaultdict
    from gensim import corpora

    documents = [
        "Human machine interface for lab abc computer applications",
        "A survey of user opinion of computer system response time",
        "The EPS user interface management system",
        "System and human system engineering testing of EPS",
        "Relation of user perceived response time to error measurement",
        "The generation of random binary unordered trees",
        "The intersection graph of paths in trees",
        "Graph minors IV Widths of trees and well quasi ordering",
        "Graph minors A survey",
    ]

    # remove common words and tokenize
    stoplist = set('for a of the and to in'.split())
    texts = [
        [word for word in document.lower().split() if word not in stoplist]
        for document in documents
    ]

    # remove words that appear only once
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1

    texts = [
        [token for token in text if frequency[token] > 1]
        for text in texts
    ]

    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]








Similarity interface
--------------------

In the previous tutorials on
:ref:`sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py`
and
:ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`,
we covered what it means to create a corpus in the Vector Space Model and how
to transform it between different vector spaces. A common reason for such a
charade is that we want to determine **similarity between pairs of
documents**, or the **similarity between a specific document and a set of
other documents** (such as a user query vs. indexed documents).

To show how this can be done in gensim, let us consider the same corpus as in the
previous examples (which really originally comes from Deerwester et al.'s
`"Indexing by Latent Semantic Analysis" <http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf>`_
seminal 1990 article).
To follow Deerwester's example, we first use this tiny corpus to define a 2-dimensional
LSI space:


.. code-block:: default


    from gensim import models
    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)








For the purposes of this tutorial, there are only two things you need to know about LSI.
First, it's just another transformation: it transforms vectors from one space to another.
Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics.
Our LSI space is two-dimensional (`num_topics = 2`) so there are two topics, but this is arbitrary.
If you're interested, you can read more about LSI here: `Latent Semantic Indexing <https://en.wikipedia.org/wiki/Latent_semantic_indexing>`_:

Now suppose a user typed in the query `"Human computer interaction"`. We would
like to sort our nine corpus documents in decreasing order of relevance to this query.
Unlike modern search engines, here we only concentrate on a single aspect of possible
similarities---on apparent semantic relatedness of their texts (words). No hyperlinks,
no random-walk static ranks, just a semantic extension over the boolean keyword match:


.. code-block:: default


    doc = "Human computer interaction"
    vec_bow = dictionary.doc2bow(doc.lower().split())
    vec_lsi = lsi[vec_bow]  # convert the query to LSI space
    print(vec_lsi)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.4618210045327157), (1, -0.07002766527900028)]




In addition, we will be considering `cosine similarity <http://en.wikipedia.org/wiki/Cosine_similarity>`_
to determine the similarity of two vectors. Cosine similarity is a standard measure
in Vector Space Modeling, but wherever the vectors represent probability distributions,
`different similarity measures <http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence>`_
may be more appropriate.

Initializing query structures
++++++++++++++++++++++++++++++++

To prepare for similarity queries, we need to enter all documents which we want
to compare against subsequent queries. In our case, they are the same nine documents
used for training LSI, converted to 2-D LSA space. But that's only incidental, we
might also be indexing a different corpus altogether.


.. code-block:: default


    from gensim import similarities
    index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it








.. warning::
  The class :class:`similarities.MatrixSimilarity` is only appropriate when the whole
  set of vectors fits into memory. For example, a corpus of one million documents
  would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.

  Without 2GB of free RAM, you would need to use the :class:`similarities.Similarity` class.
  This class operates in fixed memory, by splitting the index across multiple files on disk, called shards.
  It uses :class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity` internally,
  so it is still fast, although slightly more complex.

Index persistency is handled via the standard :func:`save` and :func:`load` functions:


.. code-block:: default


    index.save('/tmp/deerwester.index')
    index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')








This is true for all similarity indexing classes (:class:`similarities.Similarity`,
:class:`similarities.MatrixSimilarity` and :class:`similarities.SparseMatrixSimilarity`).
Also in the following, `index` can be an object of any of these. When in doubt,
use :class:`similarities.Similarity`, as it is the most scalable version, and it also
supports adding more documents to the index later.

Performing queries
++++++++++++++++++

To obtain similarities of our query document against the nine indexed documents:


.. code-block:: default


    sims = index[vec_lsi]  # perform a similarity query against the corpus
    print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]




Cosine measure returns similarities in the range `<-1, 1>` (the greater, the more similar),
so that the first document has a score of 0.99809301 etc.

With some standard Python magic we sort these similarities into descending
order, and obtain the final answer to the query `"Human computer interaction"`:


.. code-block:: default


    sims = sorted(enumerate(sims), key=lambda item: -item[1])
    for doc_position, doc_score in sims:
        print(doc_score, documents[doc_position])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.9984453 The EPS user interface management system
    0.998093 Human machine interface for lab abc computer applications
    0.9865886 System and human system engineering testing of EPS
    0.93748635 A survey of user opinion of computer system response time
    0.90755945 Relation of user perceived response time to error measurement
    0.050041765 Graph minors A survey
    -0.09879464 Graph minors IV Widths of trees and well quasi ordering
    -0.10639259 The intersection graph of paths in trees
    -0.12416792 The generation of random binary unordered trees




The thing to note here is that documents no. 2 (``"The EPS user interface management system"``)
and 4 (``"Relation of user perceived response time to error measurement"``) would never be returned by
a standard boolean fulltext search, because they do not share any common words with ``"Human
computer interaction"``. However, after applying LSI, we can observe that both of
them received quite high similarity scores (no. 2 is actually the most similar!),
which corresponds better to our intuition of
them sharing a "computer-human" related topic with the query. In fact, this semantic
generalization is the reason why we apply transformations and do topic modelling
in the first place.

Where next?
------------

Congratulations, you have finished the tutorials -- now you know how gensim works :-)
To delve into more details, you can browse through the :ref:`apiref`,
see the :ref:`wiki` or perhaps check out :ref:`distributed` in `gensim`.

Gensim is a fairly mature package that has been used successfully by many individuals and companies, both for rapid prototyping and in production.
That doesn't mean it's perfect though:

* there are parts that could be implemented more efficiently (in C, for example), or make better use of parallelism (multiple machines cores)
* new algorithms are published all the time; help gensim keep up by `discussing them <http://groups.google.com/group/gensim>`_ and `contributing code <https://github.com/piskvorky/gensim/wiki/Developer-page>`_
* your **feedback is most welcome** and appreciated (and it's not just the code!):
  `bug reports <https://github.com/piskvorky/gensim/issues>`_ or
  `user stories and general questions <http://groups.google.com/group/gensim/topics>`_.

Gensim has no ambition to become an all-encompassing framework, across all NLP (or even Machine Learning) subfields.
Its mission is to help NLP practitioners try out popular topic modelling algorithms
on large datasets easily, and to facilitate prototyping of new algorithms for researchers.


.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('run_similarity_queries.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image:: /auto_examples/core/images/sphx_glr_run_similarity_queries_001.png
    :alt: run similarity queries
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.834 seconds)

**Estimated memory usage:**  7 MB


.. _sphx_glr_download_auto_examples_core_run_similarity_queries.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_similarity_queries.py <run_similarity_queries.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_similarity_queries.ipynb <run_similarity_queries.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/core/run_corpora_and_vector_spaces.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_core_run_corpora_and_vector_spaces.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_core_run_corpora_and_vector_spaces.py:


Corpora and Vector Spaces
=========================

Demonstrates transforming text into a vector space representation.

Also introduces corpus streaming and persistence to disk in various formats.

.. GENERATED FROM PYTHON SOURCE LINES 9-13

.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








.. GENERATED FROM PYTHON SOURCE LINES 14-23

First, let‚Äôs create a small corpus of nine short documents [1]_:

.. _second example:

From Strings to Vectors
------------------------

This time, let's start from documents represented as strings:


.. GENERATED FROM PYTHON SOURCE LINES 23-35

.. code-block:: default

    documents = [
        "Human machine interface for lab abc computer applications",
        "A survey of user opinion of computer system response time",
        "The EPS user interface management system",
        "System and human system engineering testing of EPS",
        "Relation of user perceived response time to error measurement",
        "The generation of random binary unordered trees",
        "The intersection graph of paths in trees",
        "Graph minors IV Widths of trees and well quasi ordering",
        "Graph minors A survey",
    ]








.. GENERATED FROM PYTHON SOURCE LINES 36-40

This is a tiny corpus of nine documents, each consisting of only a single sentence.

First, let's tokenize the documents, remove common words (using a toy stoplist)
as well as words that only appear once in the corpus:

.. GENERATED FROM PYTHON SOURCE LINES 40-64

.. code-block:: default


    from pprint import pprint  # pretty-printer
    from collections import defaultdict

    # remove common words and tokenize
    stoplist = set('for a of the and to in'.split())
    texts = [
        [word for word in document.lower().split() if word not in stoplist]
        for document in documents
    ]

    # remove words that appear only once
    frequency = defaultdict(int)
    for text in texts:
        for token in text:
            frequency[token] += 1

    texts = [
        [token for token in text if frequency[token] > 1]
        for text in texts
    ]

    pprint(texts)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [['human', 'interface', 'computer'],
     ['survey', 'user', 'computer', 'system', 'response', 'time'],
     ['eps', 'user', 'interface', 'system'],
     ['system', 'human', 'system', 'eps'],
     ['user', 'response', 'time'],
     ['trees'],
     ['graph', 'trees'],
     ['graph', 'minors', 'trees'],
     ['graph', 'minors', 'survey']]




.. GENERATED FROM PYTHON SOURCE LINES 65-87

Your way of processing the documents will likely vary; here, I only split on whitespace
to tokenize, followed by lowercasing each word. In fact, I use this particular
(simplistic and inefficient) setup to mimic the experiment done in Deerwester et al.'s
original LSA article [1]_.

The ways to process documents are so varied and application- and language-dependent that I
decided to *not* constrain them by any interface. Instead, a document is represented
by the features extracted from it, not by its "surface" string form: how you get to
the features is up to you. Below I describe one common, general-purpose approach (called
:dfn:`bag-of-words`), but keep in mind that different application domains call for
different features, and, as always, it's `garbage in, garbage out <http://en.wikipedia.org/wiki/Garbage_In,_Garbage_Out>`_...

To convert documents to vectors, we'll use a document representation called
`bag-of-words <http://en.wikipedia.org/wiki/Bag_of_words>`_. In this representation,
each document is represented by one vector where each vector element represents
a question-answer pair, in the style of:

- Question: How many times does the word `system` appear in the document?
- Answer: Once.

It is advantageous to represent the questions only by their (integer) ids. The mapping
between the questions and ids is called a dictionary:

.. GENERATED FROM PYTHON SOURCE LINES 87-93

.. code-block:: default


    from gensim import corpora
    dictionary = corpora.Dictionary(texts)
    dictionary.save('/tmp/deerwester.dict')  # store the dictionary, for future reference
    print(dictionary)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:56,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2021-06-01 10:34:56,824 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)
    2021-06-01 10:34:56,834 : INFO : Dictionary lifecycle event {'msg': "built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)", 'datetime': '2021-06-01T10:34:56.825003', 'gensim': '4.1.0.dev0', 'python': '3.8.5 (default, Jan 27 2021, 15:41:15) \n[GCC 9.3.0]', 'platform': 'Linux-5.4.0-73-generic-x86_64-with-glibc2.29', 'event': 'created'}
    2021-06-01 10:34:56,834 : INFO : Dictionary lifecycle event {'fname_or_handle': '/tmp/deerwester.dict', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-06-01T10:34:56.834300', 'gensim': '4.1.0.dev0', 'python': '3.8.5 (default, Jan 27 2021, 15:41:15) \n[GCC 9.3.0]', 'platform': 'Linux-5.4.0-73-generic-x86_64-with-glibc2.29', 'event': 'saving'}
    2021-06-01 10:34:56,834 : INFO : saved /tmp/deerwester.dict
    Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)




.. GENERATED FROM PYTHON SOURCE LINES 94-99

Here we assigned a unique integer id to all words appearing in the corpus with the
:class:`gensim.corpora.dictionary.Dictionary` class. This sweeps across the texts, collecting word counts
and relevant statistics. In the end, we see there are twelve distinct words in the
processed corpus, which means each document will be represented by twelve numbers (ie., by a 12-D vector).
To see the mapping between words and their ids:

.. GENERATED FROM PYTHON SOURCE LINES 99-102

.. code-block:: default


    print(dictionary.token2id)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}




.. GENERATED FROM PYTHON SOURCE LINES 103-104

To actually convert tokenized documents to vectors:

.. GENERATED FROM PYTHON SOURCE LINES 104-109

.. code-block:: default


    new_doc = "Human computer interaction"
    new_vec = dictionary.doc2bow(new_doc.lower().split())
    print(new_vec)  # the word "interaction" does not appear in the dictionary and is ignored





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 1), (1, 1)]




.. GENERATED FROM PYTHON SOURCE LINES 110-115

The function :func:`doc2bow` simply counts the number of occurrences of
each distinct word, converts the word to its integer word id
and returns the result as a sparse vector. The sparse vector ``[(0, 1), (1, 1)]``
therefore reads: in the document `"Human computer interaction"`, the words `computer`
(id 0) and `human` (id 1) appear once; the other ten dictionary words appear (implicitly) zero times.

.. GENERATED FROM PYTHON SOURCE LINES 115-120

.. code-block:: default


    corpus = [dictionary.doc2bow(text) for text in texts]
    corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)  # store to disk, for later use
    print(corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:57,074 : INFO : storing corpus in Matrix Market format to /tmp/deerwester.mm
    2021-06-01 10:34:57,075 : INFO : saving sparse matrix to /tmp/deerwester.mm
    2021-06-01 10:34:57,075 : INFO : PROGRESS: saving document #0
    2021-06-01 10:34:57,076 : INFO : saved 9x12 matrix, density=25.926% (28/108)
    2021-06-01 10:34:57,076 : INFO : saving MmCorpus index to /tmp/deerwester.mm.index
    [[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]




.. GENERATED FROM PYTHON SOURCE LINES 121-136

By now it should be clear that the vector feature with ``id=10`` stands for the question "How many
times does the word `graph` appear in the document?" and that the answer is "zero" for
the first six documents and "one" for the remaining three.

.. _corpus_streaming_tutorial:

Corpus Streaming -- One Document at a Time
-------------------------------------------

Note that `corpus` above resides fully in memory, as a plain Python list.
In this simple example, it doesn't matter much, but just to make things clear,
let's assume there are millions of documents in the corpus. Storing all of them in RAM won't do.
Instead, let's assume the documents are stored in a file on disk, one document per line. Gensim
only requires that a corpus must be able to return one document vector at a time:


.. GENERATED FROM PYTHON SOURCE LINES 136-145

.. code-block:: default

    from smart_open import open  # for transparently opening remote files


    class MyCorpus:
        def __iter__(self):
            for line in open('https://radimrehurek.com/mycorpus.txt'):
                # assume there's one document per line, tokens separated by whitespace
                yield dictionary.doc2bow(line.lower().split())








.. GENERATED FROM PYTHON SOURCE LINES 146-150

The full power of Gensim comes from the fact that a corpus doesn't have to be
a ``list``, or a ``NumPy`` array, or a ``Pandas`` dataframe, or whatever.
Gensim *accepts any object that, when iterated over, successively yields
documents*.

.. GENERATED FROM PYTHON SOURCE LINES 150-156

.. code-block:: default


    # This flexibility allows you to create your own corpus classes that stream the
    # documents directly from disk, network, database, dataframes... The models
    # in Gensim are implemented such that they don't require all vectors to reside
    # in RAM at once. You can even create the documents on the fly!








.. GENERATED FROM PYTHON SOURCE LINES 157-163

Download the sample `mycorpus.txt file here <https://radimrehurek.com/mycorpus.txt>`_. The assumption that
each document occupies one line in a single file is not important; you can mold
the `__iter__` function to fit your input format, whatever it is.
Walking directories, parsing XML, accessing the network...
Just parse your input to retrieve a clean list of tokens in each document,
then convert the tokens via a dictionary to their ids and yield the resulting sparse vector inside `__iter__`.

.. GENERATED FROM PYTHON SOURCE LINES 163-167

.. code-block:: default


    corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!
    print(corpus_memory_friendly)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <__main__.MyCorpus object at 0x7f389b5f8520>




.. GENERATED FROM PYTHON SOURCE LINES 168-171

Corpus is now an object. We didn't define any way to print it, so `print` just outputs address
of the object in memory. Not very useful. To see the constituent vectors, let's
iterate over the corpus and print each document vector (one at a time):

.. GENERATED FROM PYTHON SOURCE LINES 171-175

.. code-block:: default


    for vector in corpus_memory_friendly:  # load one vector into memory at a time
        print(vector)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(0, 1), (1, 1), (2, 1)]
    [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]
    [(2, 1), (5, 1), (7, 1), (8, 1)]
    [(1, 1), (5, 2), (8, 1)]
    [(3, 1), (6, 1), (7, 1)]
    [(9, 1)]
    [(9, 1), (10, 1)]
    [(9, 1), (10, 1), (11, 1)]
    [(4, 1), (10, 1), (11, 1)]




.. GENERATED FROM PYTHON SOURCE LINES 176-181

Although the output is the same as for the plain Python list, the corpus is now much
more memory friendly, because at most one vector resides in RAM at a time. Your
corpus can now be as large as you want.

Similarly, to construct the dictionary without loading all texts into memory:

.. GENERATED FROM PYTHON SOURCE LINES 181-195

.. code-block:: default


    # collect statistics about all tokens
    dictionary = corpora.Dictionary(line.lower().split() for line in open('https://radimrehurek.com/mycorpus.txt'))
    # remove stop words and words that appear only once
    stop_ids = [
        dictionary.token2id[stopword]
        for stopword in stoplist
        if stopword in dictionary.token2id
    ]
    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]
    dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once
    dictionary.compactify()  # remove gaps in id sequence after words that were removed
    print(dictionary)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:58,466 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2021-06-01 10:34:58,467 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)
    2021-06-01 10:34:58,467 : INFO : Dictionary lifecycle event {'msg': "built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)", 'datetime': '2021-06-01T10:34:58.467454', 'gensim': '4.1.0.dev0', 'python': '3.8.5 (default, Jan 27 2021, 15:41:15) \n[GCC 9.3.0]', 'platform': 'Linux-5.4.0-73-generic-x86_64-with-glibc2.29', 'event': 'created'}
    Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)




.. GENERATED FROM PYTHON SOURCE LINES 196-219

And that is all there is to it! At least as far as bag-of-words representation is concerned.
Of course, what we do with such a corpus is another question; it is not at all clear
how counting the frequency of distinct words could be useful. As it turns out, it isn't, and
we will need to apply a transformation on this simple representation first, before
we can use it to compute any meaningful document vs. document similarities.
Transformations are covered in the next tutorial
(:ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`),
but before that, let's briefly turn our attention to *corpus persistency*.

.. _corpus-formats:

Corpus Formats
---------------

There exist several file formats for serializing a Vector Space corpus (~sequence of vectors) to disk.
`Gensim` implements them via the *streaming corpus interface* mentioned earlier:
documents are read from (resp. stored to) disk in a lazy fashion, one document at
a time, without the whole corpus being read into main memory at once.

One of the more notable file formats is the `Market Matrix format <http://math.nist.gov/MatrixMarket/formats.html>`_.
To save a corpus in the Matrix Market format:

create a toy corpus of 2 documents, as a plain Python list

.. GENERATED FROM PYTHON SOURCE LINES 219-223

.. code-block:: default

    corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it

    corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:58,603 : INFO : storing corpus in Matrix Market format to /tmp/corpus.mm
    2021-06-01 10:34:58,604 : INFO : saving sparse matrix to /tmp/corpus.mm
    2021-06-01 10:34:58,604 : INFO : PROGRESS: saving document #0
    2021-06-01 10:34:58,604 : INFO : saved 2x2 matrix, density=25.000% (1/4)
    2021-06-01 10:34:58,604 : INFO : saving MmCorpus index to /tmp/corpus.mm.index




.. GENERATED FROM PYTHON SOURCE LINES 224-227

Other formats include `Joachim's SVMlight format <http://svmlight.joachims.org/>`_,
`Blei's LDA-C format <http://www.cs.princeton.edu/~blei/lda-c/>`_ and
`GibbsLDA++ format <http://gibbslda.sourceforge.net/>`_.

.. GENERATED FROM PYTHON SOURCE LINES 227-233

.. code-block:: default


    corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus)
    corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)
    corpora.LowCorpus.serialize('/tmp/corpus.low', corpus)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:58,653 : INFO : converting corpus to SVMlight format: /tmp/corpus.svmlight
    2021-06-01 10:34:58,654 : INFO : saving SvmLightCorpus index to /tmp/corpus.svmlight.index
    2021-06-01 10:34:58,654 : INFO : no word id mapping provided; initializing from corpus
    2021-06-01 10:34:58,654 : INFO : storing corpus in Blei's LDA-C format into /tmp/corpus.lda-c
    2021-06-01 10:34:58,654 : INFO : saving vocabulary of 2 words to /tmp/corpus.lda-c.vocab
    2021-06-01 10:34:58,654 : INFO : saving BleiCorpus index to /tmp/corpus.lda-c.index
    2021-06-01 10:34:58,707 : INFO : no word id mapping provided; initializing from corpus
    2021-06-01 10:34:58,708 : INFO : storing corpus in List-Of-Words format into /tmp/corpus.low
    2021-06-01 10:34:58,708 : WARNING : List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value
    2021-06-01 10:34:58,708 : INFO : saving LowCorpus index to /tmp/corpus.low.index




.. GENERATED FROM PYTHON SOURCE LINES 234-235

Conversely, to load a corpus iterator from a Matrix Market file:

.. GENERATED FROM PYTHON SOURCE LINES 235-238

.. code-block:: default


    corpus = corpora.MmCorpus('/tmp/corpus.mm')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:58,756 : INFO : loaded corpus index from /tmp/corpus.mm.index
    2021-06-01 10:34:58,757 : INFO : initializing cython corpus reader from /tmp/corpus.mm
    2021-06-01 10:34:58,757 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries




.. GENERATED FROM PYTHON SOURCE LINES 239-240

Corpus objects are streams, so typically you won't be able to print them directly:

.. GENERATED FROM PYTHON SOURCE LINES 240-243

.. code-block:: default


    print(corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    MmCorpus(2 documents, 2 features, 1 non-zero entries)




.. GENERATED FROM PYTHON SOURCE LINES 244-245

Instead, to view the contents of a corpus:

.. GENERATED FROM PYTHON SOURCE LINES 245-249

.. code-block:: default


    # one way of printing a corpus: load it entirely into memory
    print(list(corpus))  # calling list() will convert any sequence to a plain Python list





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [[(1, 0.5)], []]




.. GENERATED FROM PYTHON SOURCE LINES 250-251

or

.. GENERATED FROM PYTHON SOURCE LINES 251-256

.. code-block:: default


    # another way of doing it: print one document at a time, making use of the streaming interface
    for doc in corpus:
        print(doc)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [(1, 0.5)]
    []




.. GENERATED FROM PYTHON SOURCE LINES 257-261

The second way is obviously more memory-friendly, but for testing and development
purposes, nothing beats the simplicity of calling ``list(corpus)``.

To save the same Matrix Market document stream in Blei's LDA-C format,

.. GENERATED FROM PYTHON SOURCE LINES 261-264

.. code-block:: default


    corpora.BleiCorpus.serialize('/tmp/corpus.lda-c', corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-06-01 10:34:59,085 : INFO : no word id mapping provided; initializing from corpus
    2021-06-01 10:34:59,086 : INFO : storing corpus in Blei's LDA-C format into /tmp/corpus.lda-c
    2021-06-01 10:34:59,087 : INFO : saving vocabulary of 2 words to /tmp/corpus.lda-c.vocab
    2021-06-01 10:34:59,087 : INFO : saving BleiCorpus index to /tmp/corpus.lda-c.index




.. GENERATED FROM PYTHON SOURCE LINES 265-275

In this way, `gensim` can also be used as a memory-efficient **I/O format conversion tool**:
just load a document stream using one format and immediately save it in another format.
Adding new formats is dead easy, check out the `code for the SVMlight corpus
<https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/svmlightcorpus.py>`_ for an example.

Compatibility with NumPy and SciPy
----------------------------------

Gensim also contains `efficient utility functions <http://radimrehurek.com/gensim/matutils.html>`_
to help converting from/to numpy matrices

.. GENERATED FROM PYTHON SOURCE LINES 275-282

.. code-block:: default


    import gensim
    import numpy as np
    numpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an example
    corpus = gensim.matutils.Dense2Corpus(numpy_matrix)
    # numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)








.. GENERATED FROM PYTHON SOURCE LINES 283-284

and from/to `scipy.sparse` matrices

.. GENERATED FROM PYTHON SOURCE LINES 284-290

.. code-block:: default


    import scipy.sparse
    scipy_sparse_matrix = scipy.sparse.random(5, 2)  # random sparse matrix as example
    corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)
    scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)








.. GENERATED FROM PYTHON SOURCE LINES 291-304

What Next
---------

Read about :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`.

References
----------

For a complete reference (Want to prune the dictionary to a smaller size?
Optimize converting between corpora and NumPy/SciPy arrays?), see the :ref:`apiref`.

.. [1] This is the same corpus as used in
       `Deerwester et al. (1990): Indexing by Latent Semantic Analysis <http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf>`_, Table 2.

.. GENERATED FROM PYTHON SOURCE LINES 304-310

.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('run_corpora_and_vector_spaces.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image:: /auto_examples/core/images/sphx_glr_run_corpora_and_vector_spaces_001.png
    :alt: run corpora and vector spaces
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  3.242 seconds)

**Estimated memory usage:**  48 MB


.. _sphx_glr_download_auto_examples_core_run_corpora_and_vector_spaces.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_corpora_and_vector_spaces.py <run_corpora_and_vector_spaces.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_corpora_and_vector_spaces.ipynb <run_corpora_and_vector_spaces.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_wmd.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_wmd.py:


Word Mover's Distance
=====================

Demonstrates using Gensim's implemenation of the WMD.

Word Mover's Distance (WMD) is a promising new tool in machine learning that
allows us to submit a query and return the most relevant documents. This
tutorial introduces WMD and shows how you can compute the WMD distance
between two documents using ``wmdistance``.

WMD Basics
----------

WMD enables us to assess the "distance" between two documents in a meaningful
way even when they have no words in common. It uses `word2vec
<http://rare-technologies.com/word2vec-tutorial/>`_ [4] vector embeddings of
words. It been shown to outperform many of the state-of-the-art methods in
k-nearest neighbors classification [3].

WMD is illustrated below for two very similar sentences (illustration taken
from `Vlad Niculae's blog
<http://vene.ro/blog/word-movers-distance-in-python.html>`_). The sentences
have no words in common, but by matching the relevant words, WMD is able to
accurately measure the (dis)similarity between the two sentences. The method
also uses the bag-of-words representation of the documents (simply put, the
word's frequencies in the documents), noted as $d$ in the figure below. The
intuition behind the method is that we find the minimum "traveling distance"
between documents, in other words the most efficient way to "move" the
distribution of document 1 to the distribution of document 2.



.. code-block:: default


    # Image from https://vene.ro/images/wmd-obama.png
    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('wmd-obama.png')
    imgplot = plt.imshow(img)
    plt.axis('off')
    plt.show()




.. image:: /auto_examples/tutorials/images/sphx_glr_run_wmd_001.png
    :class: sphx-glr-single-img




This method was introduced in the article "From Word Embeddings To Document
Distances" by Matt Kusner et al. (\ `link to PDF
<http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf>`_\ ). It is inspired
by the "Earth Mover's Distance", and employs a solver of the "transportation
problem".

In this tutorial, we will learn how to use Gensim's WMD functionality, which
consists of the ``wmdistance`` method for distance computation, and the
``WmdSimilarity`` class for corpus based similarity queries.

.. Important::
   If you use Gensim's WMD functionality, please consider citing [1], [2] and [3].

Computing the Word Mover's Distance
-----------------------------------

To use WMD, you need some existing word embeddings.
You could train your own Word2Vec model, but that is beyond the scope of this tutorial
(check out :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py` if you're interested).
For this tutorial, we'll be using an existing Word2Vec model.

Let's take some sentences to compute the distance between.



.. code-block:: default


    # Initialize logging.
    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    sentence_obama = 'Obama speaks to the media in Illinois'
    sentence_president = 'The president greets the press in Chicago'







These sentences have very similar content, and as such the WMD should be low.
Before we compute the WMD, we want to remove stopwords ("the", "to", etc.),
as these do not contribute a lot to the information in the sentences.



.. code-block:: default


    # Import and download stopwords from NLTK.
    from nltk.corpus import stopwords
    from nltk import download
    download('stopwords')  # Download stopwords list.
    stop_words = stopwords.words('english')

    def preprocess(sentence):
        return [w for w in sentence.lower().split() if w not in stop_words]

    sentence_obama = preprocess(sentence_obama)
    sentence_president = preprocess(sentence_president)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      dtype=np.int):
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      method='lar', copy_X=True, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      method='lar', copy_X=True, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, positive=False):
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, positive=False):
    [nltk_data] Downloading package stopwords to /home/witiko/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



Now, as mentioned earlier, we will be using some downloaded pre-trained
embeddings. We load these into a Gensim Word2Vec model class.

.. Important::
  The embeddings we have chosen here require a lot of memory.



.. code-block:: default

    import gensim.downloader as api
    model = api.load('word2vec-google-news-300')







So let's compute WMD using the ``wmdistance`` method.



.. code-block:: default

    distance = model.wmdistance(sentence_obama, sentence_president)
    print('distance = %.4f' % distance)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    distance = 1.0175



Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.



.. code-block:: default

    sentence_orange = preprocess('Oranges are my favorite fruit')
    distance = model.wmdistance(sentence_obama, sentence_orange)
    print('distance = %.4f' % distance)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    distance = 1.3663



References
----------

1. Ofir Pele and Michael Werman, *A linear time histogram metric for improved SIFT matching*, 2008.
2. Ofir Pele and Michael Werman, *Fast and robust earth mover's distances*, 2009.
3. Matt Kusner et al. *From Embeddings To Document Distances*, 2015.
4. Tom√°≈° Mikolov et al. *Efficient Estimation of Word Representations in Vector Space*, 2013.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  55.983 seconds)

**Estimated memory usage:**  7537 MB


.. _sphx_glr_download_auto_examples_tutorials_run_wmd.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: run_wmd.py <run_wmd.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: run_wmd.ipynb <run_wmd.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_ensemblelda.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_ensemblelda.py:


Ensemble LDA
============

Introduces Gensim's EnsembleLda model



.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








This tutorial will explain how to use the EnsembleLDA model class.

EnsembleLda is a method of finding and generating stable topics from the results of multiple topic models,
it can be used to remove topics from your results that are noise and are not reproducible.


Corpus
------
We will use the gensim downloader api to get a small corpus for training our ensemble.

The preprocessing is similar to :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py`,
so it won't be explained again in detail.



.. code-block:: default


    import gensim.downloader as api
    from gensim.corpora import Dictionary
    from nltk.stem.wordnet import WordNetLemmatizer

    lemmatizer = WordNetLemmatizer()
    docs = api.load('text8')

    dictionary = Dictionary()
    for doc in docs:
        dictionary.add_documents([[lemmatizer.lemmatize(token) for token in doc]])
    dictionary.filter_extremes(no_below=20, no_above=0.5)

    corpus = [dictionary.doc2bow(doc) for doc in docs]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /Volumes/work/workspace/gensim/trunk/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
      warnings.warn(msg)
    2021-05-05 22:35:33,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2021-05-05 22:35:33,178 : INFO : built Dictionary(2312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1 documents (total 10000 corpus positions)
    2021-05-05 22:35:33,220 : INFO : adding document #0 to Dictionary(2312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,228 : INFO : built Dictionary(3906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 2 documents (total 20000 corpus positions)
    2021-05-05 22:35:33,270 : INFO : adding document #0 to Dictionary(3906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,278 : INFO : built Dictionary(5147 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 3 documents (total 30000 corpus positions)
    2021-05-05 22:35:33,320 : INFO : adding document #0 to Dictionary(5147 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,328 : INFO : built Dictionary(6182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 4 documents (total 40000 corpus positions)
    2021-05-05 22:35:33,371 : INFO : adding document #0 to Dictionary(6182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,378 : INFO : built Dictionary(7053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 5 documents (total 50000 corpus positions)
    2021-05-05 22:35:33,420 : INFO : adding document #0 to Dictionary(7053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,427 : INFO : built Dictionary(7993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 6 documents (total 60000 corpus positions)
    2021-05-05 22:35:33,470 : INFO : adding document #0 to Dictionary(7993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,476 : INFO : built Dictionary(8587 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 7 documents (total 70000 corpus positions)
    2021-05-05 22:35:33,519 : INFO : adding document #0 to Dictionary(8587 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,527 : INFO : built Dictionary(9306 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 8 documents (total 80000 corpus positions)
    2021-05-05 22:35:33,569 : INFO : adding document #0 to Dictionary(9306 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,576 : INFO : built Dictionary(10072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 9 documents (total 90000 corpus positions)
    2021-05-05 22:35:33,619 : INFO : adding document #0 to Dictionary(10072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,626 : INFO : built Dictionary(10770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 10 documents (total 100000 corpus positions)
    2021-05-05 22:35:33,669 : INFO : adding document #0 to Dictionary(10770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,677 : INFO : built Dictionary(11396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 11 documents (total 110000 corpus positions)
    2021-05-05 22:35:33,719 : INFO : adding document #0 to Dictionary(11396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,727 : INFO : built Dictionary(12149 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 12 documents (total 120000 corpus positions)
    2021-05-05 22:35:33,773 : INFO : adding document #0 to Dictionary(12149 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,781 : INFO : built Dictionary(12766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 13 documents (total 130000 corpus positions)
    2021-05-05 22:35:33,824 : INFO : adding document #0 to Dictionary(12766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,832 : INFO : built Dictionary(13310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 14 documents (total 140000 corpus positions)
    2021-05-05 22:35:33,875 : INFO : adding document #0 to Dictionary(13310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,883 : INFO : built Dictionary(13921 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 15 documents (total 150000 corpus positions)
    2021-05-05 22:35:33,926 : INFO : adding document #0 to Dictionary(13921 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,932 : INFO : built Dictionary(14485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 16 documents (total 160000 corpus positions)
    2021-05-05 22:35:33,975 : INFO : adding document #0 to Dictionary(14485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:33,983 : INFO : built Dictionary(15040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 17 documents (total 170000 corpus positions)
    2021-05-05 22:35:34,025 : INFO : adding document #0 to Dictionary(15040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,032 : INFO : built Dictionary(15482 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 18 documents (total 180000 corpus positions)
    2021-05-05 22:35:34,080 : INFO : adding document #0 to Dictionary(15482 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,088 : INFO : built Dictionary(16019 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 19 documents (total 190000 corpus positions)
    2021-05-05 22:35:34,131 : INFO : adding document #0 to Dictionary(16019 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,140 : INFO : built Dictionary(16997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 20 documents (total 200000 corpus positions)
    2021-05-05 22:35:34,186 : INFO : adding document #0 to Dictionary(16997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,194 : INFO : built Dictionary(17548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 21 documents (total 210000 corpus positions)
    2021-05-05 22:35:34,237 : INFO : adding document #0 to Dictionary(17548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,244 : INFO : built Dictionary(18074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 22 documents (total 220000 corpus positions)
    2021-05-05 22:35:34,287 : INFO : adding document #0 to Dictionary(18074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,294 : INFO : built Dictionary(18485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 23 documents (total 230000 corpus positions)
    2021-05-05 22:35:34,337 : INFO : adding document #0 to Dictionary(18485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,346 : INFO : built Dictionary(19411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 24 documents (total 240000 corpus positions)
    2021-05-05 22:35:34,391 : INFO : adding document #0 to Dictionary(19411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,398 : INFO : built Dictionary(19909 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 25 documents (total 250000 corpus positions)
    2021-05-05 22:35:34,441 : INFO : adding document #0 to Dictionary(19909 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,448 : INFO : built Dictionary(20332 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 26 documents (total 260000 corpus positions)
    2021-05-05 22:35:34,491 : INFO : adding document #0 to Dictionary(20332 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,499 : INFO : built Dictionary(20766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 27 documents (total 270000 corpus positions)
    2021-05-05 22:35:34,541 : INFO : adding document #0 to Dictionary(20766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,548 : INFO : built Dictionary(21174 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 28 documents (total 280000 corpus positions)
    2021-05-05 22:35:34,591 : INFO : adding document #0 to Dictionary(21174 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,598 : INFO : built Dictionary(21602 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 29 documents (total 290000 corpus positions)
    2021-05-05 22:35:34,641 : INFO : adding document #0 to Dictionary(21602 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,648 : INFO : built Dictionary(21878 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 30 documents (total 300000 corpus positions)
    2021-05-05 22:35:34,695 : INFO : adding document #0 to Dictionary(21878 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,702 : INFO : built Dictionary(22126 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 31 documents (total 310000 corpus positions)
    2021-05-05 22:35:34,746 : INFO : adding document #0 to Dictionary(22126 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,754 : INFO : built Dictionary(22522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 32 documents (total 320000 corpus positions)
    2021-05-05 22:35:34,798 : INFO : adding document #0 to Dictionary(22522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,806 : INFO : built Dictionary(23022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 33 documents (total 330000 corpus positions)
    2021-05-05 22:35:34,850 : INFO : adding document #0 to Dictionary(23022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,859 : INFO : built Dictionary(23512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 34 documents (total 340000 corpus positions)
    2021-05-05 22:35:34,902 : INFO : adding document #0 to Dictionary(23512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,911 : INFO : built Dictionary(24078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 35 documents (total 350000 corpus positions)
    2021-05-05 22:35:34,954 : INFO : adding document #0 to Dictionary(24078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:34,963 : INFO : built Dictionary(24518 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 36 documents (total 360000 corpus positions)
    2021-05-05 22:35:35,008 : INFO : adding document #0 to Dictionary(24518 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,018 : INFO : built Dictionary(25027 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 37 documents (total 370000 corpus positions)
    2021-05-05 22:35:35,067 : INFO : adding document #0 to Dictionary(25027 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,074 : INFO : built Dictionary(25452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 38 documents (total 380000 corpus positions)
    2021-05-05 22:35:35,117 : INFO : adding document #0 to Dictionary(25452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,125 : INFO : built Dictionary(26041 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 39 documents (total 390000 corpus positions)
    2021-05-05 22:35:35,168 : INFO : adding document #0 to Dictionary(26041 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,176 : INFO : built Dictionary(26389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 40 documents (total 400000 corpus positions)
    2021-05-05 22:35:35,220 : INFO : adding document #0 to Dictionary(26389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,228 : INFO : built Dictionary(26621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 41 documents (total 410000 corpus positions)
    2021-05-05 22:35:35,272 : INFO : adding document #0 to Dictionary(26621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,280 : INFO : built Dictionary(27013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 42 documents (total 420000 corpus positions)
    2021-05-05 22:35:35,326 : INFO : adding document #0 to Dictionary(27013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,335 : INFO : built Dictionary(27452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 43 documents (total 430000 corpus positions)
    2021-05-05 22:35:35,378 : INFO : adding document #0 to Dictionary(27452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,386 : INFO : built Dictionary(27868 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 44 documents (total 440000 corpus positions)
    2021-05-05 22:35:35,429 : INFO : adding document #0 to Dictionary(27868 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,437 : INFO : built Dictionary(28213 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 45 documents (total 450000 corpus positions)
    2021-05-05 22:35:35,480 : INFO : adding document #0 to Dictionary(28213 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,488 : INFO : built Dictionary(28596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 46 documents (total 460000 corpus positions)
    2021-05-05 22:35:35,535 : INFO : adding document #0 to Dictionary(28596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,541 : INFO : built Dictionary(28842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 47 documents (total 470000 corpus positions)
    2021-05-05 22:35:35,584 : INFO : adding document #0 to Dictionary(28842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,591 : INFO : built Dictionary(29183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 48 documents (total 480000 corpus positions)
    2021-05-05 22:35:35,635 : INFO : adding document #0 to Dictionary(29183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,643 : INFO : built Dictionary(29569 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 49 documents (total 490000 corpus positions)
    2021-05-05 22:35:35,689 : INFO : adding document #0 to Dictionary(29569 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,698 : INFO : built Dictionary(29905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 50 documents (total 500000 corpus positions)
    2021-05-05 22:35:35,742 : INFO : adding document #0 to Dictionary(29905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,754 : INFO : built Dictionary(30435 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 51 documents (total 510000 corpus positions)
    2021-05-05 22:35:35,799 : INFO : adding document #0 to Dictionary(30435 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,808 : INFO : built Dictionary(30852 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 52 documents (total 520000 corpus positions)
    2021-05-05 22:35:35,852 : INFO : adding document #0 to Dictionary(30852 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,860 : INFO : built Dictionary(31140 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 53 documents (total 530000 corpus positions)
    2021-05-05 22:35:35,904 : INFO : adding document #0 to Dictionary(31140 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,913 : INFO : built Dictionary(31611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 54 documents (total 540000 corpus positions)
    2021-05-05 22:35:35,957 : INFO : adding document #0 to Dictionary(31611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:35,965 : INFO : built Dictionary(32277 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 55 documents (total 550000 corpus positions)
    2021-05-05 22:35:36,009 : INFO : adding document #0 to Dictionary(32277 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,018 : INFO : built Dictionary(32761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 56 documents (total 560000 corpus positions)
    2021-05-05 22:35:36,061 : INFO : adding document #0 to Dictionary(32761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,069 : INFO : built Dictionary(33053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 57 documents (total 570000 corpus positions)
    2021-05-05 22:35:36,113 : INFO : adding document #0 to Dictionary(33053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,122 : INFO : built Dictionary(33393 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 58 documents (total 580000 corpus positions)
    2021-05-05 22:35:36,165 : INFO : adding document #0 to Dictionary(33393 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,173 : INFO : built Dictionary(33804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 59 documents (total 590000 corpus positions)
    2021-05-05 22:35:36,216 : INFO : adding document #0 to Dictionary(33804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,225 : INFO : built Dictionary(34233 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 60 documents (total 600000 corpus positions)
    2021-05-05 22:35:36,272 : INFO : adding document #0 to Dictionary(34233 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,281 : INFO : built Dictionary(34513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 61 documents (total 610000 corpus positions)
    2021-05-05 22:35:36,329 : INFO : adding document #0 to Dictionary(34513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,336 : INFO : built Dictionary(34814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 62 documents (total 620000 corpus positions)
    2021-05-05 22:35:36,379 : INFO : adding document #0 to Dictionary(34814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,386 : INFO : built Dictionary(35107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 63 documents (total 630000 corpus positions)
    2021-05-05 22:35:36,429 : INFO : adding document #0 to Dictionary(35107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,437 : INFO : built Dictionary(35446 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 64 documents (total 640000 corpus positions)
    2021-05-05 22:35:36,480 : INFO : adding document #0 to Dictionary(35446 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,487 : INFO : built Dictionary(35713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 65 documents (total 650000 corpus positions)
    2021-05-05 22:35:36,530 : INFO : adding document #0 to Dictionary(35713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,538 : INFO : built Dictionary(36124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 66 documents (total 660000 corpus positions)
    2021-05-05 22:35:36,582 : INFO : adding document #0 to Dictionary(36124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,590 : INFO : built Dictionary(36513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 67 documents (total 670000 corpus positions)
    2021-05-05 22:35:36,633 : INFO : adding document #0 to Dictionary(36513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,643 : INFO : built Dictionary(36825 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 68 documents (total 680000 corpus positions)
    2021-05-05 22:35:36,686 : INFO : adding document #0 to Dictionary(36825 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,694 : INFO : built Dictionary(37084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 69 documents (total 690000 corpus positions)
    2021-05-05 22:35:36,738 : INFO : adding document #0 to Dictionary(37084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,746 : INFO : built Dictionary(37333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 70 documents (total 700000 corpus positions)
    2021-05-05 22:35:36,789 : INFO : adding document #0 to Dictionary(37333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,797 : INFO : built Dictionary(37634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 71 documents (total 710000 corpus positions)
    2021-05-05 22:35:36,840 : INFO : adding document #0 to Dictionary(37634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,847 : INFO : built Dictionary(37919 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 72 documents (total 720000 corpus positions)
    2021-05-05 22:35:36,892 : INFO : adding document #0 to Dictionary(37919 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,900 : INFO : built Dictionary(38309 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 73 documents (total 730000 corpus positions)
    2021-05-05 22:35:36,944 : INFO : adding document #0 to Dictionary(38309 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:36,952 : INFO : built Dictionary(38690 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 74 documents (total 740000 corpus positions)
    2021-05-05 22:35:36,995 : INFO : adding document #0 to Dictionary(38690 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,003 : INFO : built Dictionary(39042 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 75 documents (total 750000 corpus positions)
    2021-05-05 22:35:37,050 : INFO : adding document #0 to Dictionary(39042 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,058 : INFO : built Dictionary(39344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 76 documents (total 760000 corpus positions)
    2021-05-05 22:35:37,101 : INFO : adding document #0 to Dictionary(39344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,108 : INFO : built Dictionary(39690 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 77 documents (total 770000 corpus positions)
    2021-05-05 22:35:37,153 : INFO : adding document #0 to Dictionary(39690 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,162 : INFO : built Dictionary(40100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 78 documents (total 780000 corpus positions)
    2021-05-05 22:35:37,205 : INFO : adding document #0 to Dictionary(40100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,212 : INFO : built Dictionary(40362 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 79 documents (total 790000 corpus positions)
    2021-05-05 22:35:37,255 : INFO : adding document #0 to Dictionary(40362 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,263 : INFO : built Dictionary(40614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 80 documents (total 800000 corpus positions)
    2021-05-05 22:35:37,308 : INFO : adding document #0 to Dictionary(40614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,317 : INFO : built Dictionary(41071 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 81 documents (total 810000 corpus positions)
    2021-05-05 22:35:37,364 : INFO : adding document #0 to Dictionary(41071 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,372 : INFO : built Dictionary(41378 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 82 documents (total 820000 corpus positions)
    2021-05-05 22:35:37,416 : INFO : adding document #0 to Dictionary(41378 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,425 : INFO : built Dictionary(41734 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 83 documents (total 830000 corpus positions)
    2021-05-05 22:35:37,469 : INFO : adding document #0 to Dictionary(41734 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,479 : INFO : built Dictionary(42113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 84 documents (total 840000 corpus positions)
    2021-05-05 22:35:37,524 : INFO : adding document #0 to Dictionary(42113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,532 : INFO : built Dictionary(42410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 85 documents (total 850000 corpus positions)
    2021-05-05 22:35:37,575 : INFO : adding document #0 to Dictionary(42410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,583 : INFO : built Dictionary(42735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 86 documents (total 860000 corpus positions)
    2021-05-05 22:35:37,626 : INFO : adding document #0 to Dictionary(42735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,634 : INFO : built Dictionary(43066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 87 documents (total 870000 corpus positions)
    2021-05-05 22:35:37,677 : INFO : adding document #0 to Dictionary(43066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,684 : INFO : built Dictionary(43358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 88 documents (total 880000 corpus positions)
    2021-05-05 22:35:37,727 : INFO : adding document #0 to Dictionary(43358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,739 : INFO : built Dictionary(43707 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 89 documents (total 890000 corpus positions)
    2021-05-05 22:35:37,782 : INFO : adding document #0 to Dictionary(43707 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,790 : INFO : built Dictionary(44209 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 90 documents (total 900000 corpus positions)
    2021-05-05 22:35:37,834 : INFO : adding document #0 to Dictionary(44209 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,841 : INFO : built Dictionary(44413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 91 documents (total 910000 corpus positions)
    2021-05-05 22:35:37,888 : INFO : adding document #0 to Dictionary(44413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,895 : INFO : built Dictionary(44687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 92 documents (total 920000 corpus positions)
    2021-05-05 22:35:37,939 : INFO : adding document #0 to Dictionary(44687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:37,948 : INFO : built Dictionary(45069 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 93 documents (total 930000 corpus positions)
    2021-05-05 22:35:37,992 : INFO : adding document #0 to Dictionary(45069 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,001 : INFO : built Dictionary(45372 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 94 documents (total 940000 corpus positions)
    2021-05-05 22:35:38,046 : INFO : adding document #0 to Dictionary(45372 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,054 : INFO : built Dictionary(45740 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 95 documents (total 950000 corpus positions)
    2021-05-05 22:35:38,097 : INFO : adding document #0 to Dictionary(45740 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,107 : INFO : built Dictionary(46116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 96 documents (total 960000 corpus positions)
    2021-05-05 22:35:38,152 : INFO : adding document #0 to Dictionary(46116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,163 : INFO : built Dictionary(46614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 97 documents (total 970000 corpus positions)
    2021-05-05 22:35:38,210 : INFO : adding document #0 to Dictionary(46614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,221 : INFO : built Dictionary(46993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 98 documents (total 980000 corpus positions)
    2021-05-05 22:35:38,264 : INFO : adding document #0 to Dictionary(46993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,274 : INFO : built Dictionary(47333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 99 documents (total 990000 corpus positions)
    2021-05-05 22:35:38,318 : INFO : adding document #0 to Dictionary(47333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,327 : INFO : built Dictionary(47651 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 100 documents (total 1000000 corpus positions)
    2021-05-05 22:35:38,371 : INFO : adding document #0 to Dictionary(47651 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,380 : INFO : built Dictionary(47971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 101 documents (total 1010000 corpus positions)
    2021-05-05 22:35:38,425 : INFO : adding document #0 to Dictionary(47971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,433 : INFO : built Dictionary(48158 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 102 documents (total 1020000 corpus positions)
    2021-05-05 22:35:38,476 : INFO : adding document #0 to Dictionary(48158 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,484 : INFO : built Dictionary(48417 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 103 documents (total 1030000 corpus positions)
    2021-05-05 22:35:38,527 : INFO : adding document #0 to Dictionary(48417 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,535 : INFO : built Dictionary(48761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 104 documents (total 1040000 corpus positions)
    2021-05-05 22:35:38,578 : INFO : adding document #0 to Dictionary(48761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,586 : INFO : built Dictionary(49112 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 105 documents (total 1050000 corpus positions)
    2021-05-05 22:35:38,634 : INFO : adding document #0 to Dictionary(49112 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,643 : INFO : built Dictionary(49389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 106 documents (total 1060000 corpus positions)
    2021-05-05 22:35:38,685 : INFO : adding document #0 to Dictionary(49389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,692 : INFO : built Dictionary(49713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 107 documents (total 1070000 corpus positions)
    2021-05-05 22:35:38,738 : INFO : adding document #0 to Dictionary(49713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,747 : INFO : built Dictionary(50011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 108 documents (total 1080000 corpus positions)
    2021-05-05 22:35:38,794 : INFO : adding document #0 to Dictionary(50011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,804 : INFO : built Dictionary(50348 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 109 documents (total 1090000 corpus positions)
    2021-05-05 22:35:38,854 : INFO : adding document #0 to Dictionary(50348 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,863 : INFO : built Dictionary(50686 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 110 documents (total 1100000 corpus positions)
    2021-05-05 22:35:38,907 : INFO : adding document #0 to Dictionary(50686 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,915 : INFO : built Dictionary(51015 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 111 documents (total 1110000 corpus positions)
    2021-05-05 22:35:38,959 : INFO : adding document #0 to Dictionary(51015 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:38,967 : INFO : built Dictionary(51249 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 112 documents (total 1120000 corpus positions)
    2021-05-05 22:35:39,012 : INFO : adding document #0 to Dictionary(51249 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,020 : INFO : built Dictionary(51548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 113 documents (total 1130000 corpus positions)
    2021-05-05 22:35:39,067 : INFO : adding document #0 to Dictionary(51548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,074 : INFO : built Dictionary(51723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 114 documents (total 1140000 corpus positions)
    2021-05-05 22:35:39,118 : INFO : adding document #0 to Dictionary(51723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,127 : INFO : built Dictionary(52062 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 115 documents (total 1150000 corpus positions)
    2021-05-05 22:35:39,170 : INFO : adding document #0 to Dictionary(52062 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,178 : INFO : built Dictionary(52293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 116 documents (total 1160000 corpus positions)
    2021-05-05 22:35:39,222 : INFO : adding document #0 to Dictionary(52293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,230 : INFO : built Dictionary(52463 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 117 documents (total 1170000 corpus positions)
    2021-05-05 22:35:39,274 : INFO : adding document #0 to Dictionary(52463 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,283 : INFO : built Dictionary(52883 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 118 documents (total 1180000 corpus positions)
    2021-05-05 22:35:39,326 : INFO : adding document #0 to Dictionary(52883 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,334 : INFO : built Dictionary(53129 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 119 documents (total 1190000 corpus positions)
    2021-05-05 22:35:39,377 : INFO : adding document #0 to Dictionary(53129 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,384 : INFO : built Dictionary(53356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 120 documents (total 1200000 corpus positions)
    2021-05-05 22:35:39,427 : INFO : adding document #0 to Dictionary(53356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,437 : INFO : built Dictionary(53643 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 121 documents (total 1210000 corpus positions)
    2021-05-05 22:35:39,482 : INFO : adding document #0 to Dictionary(53643 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,490 : INFO : built Dictionary(53991 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 122 documents (total 1220000 corpus positions)
    2021-05-05 22:35:39,534 : INFO : adding document #0 to Dictionary(53991 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,541 : INFO : built Dictionary(54237 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 123 documents (total 1230000 corpus positions)
    2021-05-05 22:35:39,584 : INFO : adding document #0 to Dictionary(54237 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,591 : INFO : built Dictionary(54478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 124 documents (total 1240000 corpus positions)
    2021-05-05 22:35:39,635 : INFO : adding document #0 to Dictionary(54478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,643 : INFO : built Dictionary(54729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 125 documents (total 1250000 corpus positions)
    2021-05-05 22:35:39,687 : INFO : adding document #0 to Dictionary(54729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,693 : INFO : built Dictionary(54920 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 126 documents (total 1260000 corpus positions)
    2021-05-05 22:35:39,737 : INFO : adding document #0 to Dictionary(54920 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,745 : INFO : built Dictionary(55154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 127 documents (total 1270000 corpus positions)
    2021-05-05 22:35:39,788 : INFO : adding document #0 to Dictionary(55154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,795 : INFO : built Dictionary(55312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 128 documents (total 1280000 corpus positions)
    2021-05-05 22:35:39,839 : INFO : adding document #0 to Dictionary(55312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,846 : INFO : built Dictionary(55447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 129 documents (total 1290000 corpus positions)
    2021-05-05 22:35:39,917 : INFO : adding document #0 to Dictionary(55447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,924 : INFO : built Dictionary(55546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 130 documents (total 1300000 corpus positions)
    2021-05-05 22:35:39,972 : INFO : adding document #0 to Dictionary(55546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:39,980 : INFO : built Dictionary(55725 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 131 documents (total 1310000 corpus positions)
    2021-05-05 22:35:40,027 : INFO : adding document #0 to Dictionary(55725 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,035 : INFO : built Dictionary(55983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 132 documents (total 1320000 corpus positions)
    2021-05-05 22:35:40,081 : INFO : adding document #0 to Dictionary(55983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,089 : INFO : built Dictionary(56128 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 133 documents (total 1330000 corpus positions)
    2021-05-05 22:35:40,135 : INFO : adding document #0 to Dictionary(56128 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,144 : INFO : built Dictionary(56299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 134 documents (total 1340000 corpus positions)
    2021-05-05 22:35:40,191 : INFO : adding document #0 to Dictionary(56299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,199 : INFO : built Dictionary(56401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 135 documents (total 1350000 corpus positions)
    2021-05-05 22:35:40,245 : INFO : adding document #0 to Dictionary(56401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,255 : INFO : built Dictionary(56673 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 136 documents (total 1360000 corpus positions)
    2021-05-05 22:35:40,300 : INFO : adding document #0 to Dictionary(56673 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,310 : INFO : built Dictionary(56854 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 137 documents (total 1370000 corpus positions)
    2021-05-05 22:35:40,376 : INFO : adding document #0 to Dictionary(56854 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,387 : INFO : built Dictionary(56986 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 138 documents (total 1380000 corpus positions)
    2021-05-05 22:35:40,450 : INFO : adding document #0 to Dictionary(56986 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,462 : INFO : built Dictionary(57177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 139 documents (total 1390000 corpus positions)
    2021-05-05 22:35:40,538 : INFO : adding document #0 to Dictionary(57177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,549 : INFO : built Dictionary(57377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 140 documents (total 1400000 corpus positions)
    2021-05-05 22:35:40,632 : INFO : adding document #0 to Dictionary(57377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,644 : INFO : built Dictionary(57564 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 141 documents (total 1410000 corpus positions)
    2021-05-05 22:35:40,711 : INFO : adding document #0 to Dictionary(57564 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,728 : INFO : built Dictionary(57814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 142 documents (total 1420000 corpus positions)
    2021-05-05 22:35:40,832 : INFO : adding document #0 to Dictionary(57814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,846 : INFO : built Dictionary(58037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 143 documents (total 1430000 corpus positions)
    2021-05-05 22:35:40,921 : INFO : adding document #0 to Dictionary(58037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:40,933 : INFO : built Dictionary(58170 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 144 documents (total 1440000 corpus positions)
    2021-05-05 22:35:41,140 : INFO : adding document #0 to Dictionary(58170 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,159 : INFO : built Dictionary(58427 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 145 documents (total 1450000 corpus positions)
    2021-05-05 22:35:41,231 : INFO : adding document #0 to Dictionary(58427 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,244 : INFO : built Dictionary(58737 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 146 documents (total 1460000 corpus positions)
    2021-05-05 22:35:41,316 : INFO : adding document #0 to Dictionary(58737 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,325 : INFO : built Dictionary(58925 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 147 documents (total 1470000 corpus positions)
    2021-05-05 22:35:41,379 : INFO : adding document #0 to Dictionary(58925 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,388 : INFO : built Dictionary(59102 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 148 documents (total 1480000 corpus positions)
    2021-05-05 22:35:41,435 : INFO : adding document #0 to Dictionary(59102 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,445 : INFO : built Dictionary(59274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 149 documents (total 1490000 corpus positions)
    2021-05-05 22:35:41,491 : INFO : adding document #0 to Dictionary(59274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,499 : INFO : built Dictionary(59504 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 150 documents (total 1500000 corpus positions)
    2021-05-05 22:35:41,544 : INFO : adding document #0 to Dictionary(59504 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,553 : INFO : built Dictionary(59712 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 151 documents (total 1510000 corpus positions)
    2021-05-05 22:35:41,598 : INFO : adding document #0 to Dictionary(59712 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,608 : INFO : built Dictionary(60001 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 152 documents (total 1520000 corpus positions)
    2021-05-05 22:35:41,662 : INFO : adding document #0 to Dictionary(60001 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,671 : INFO : built Dictionary(60298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 153 documents (total 1530000 corpus positions)
    2021-05-05 22:35:41,723 : INFO : adding document #0 to Dictionary(60298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,734 : INFO : built Dictionary(60490 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 154 documents (total 1540000 corpus positions)
    2021-05-05 22:35:41,783 : INFO : adding document #0 to Dictionary(60490 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,792 : INFO : built Dictionary(60838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 155 documents (total 1550000 corpus positions)
    2021-05-05 22:35:41,854 : INFO : adding document #0 to Dictionary(60838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,869 : INFO : built Dictionary(61147 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 156 documents (total 1560000 corpus positions)
    2021-05-05 22:35:41,930 : INFO : adding document #0 to Dictionary(61147 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,943 : INFO : built Dictionary(61440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 157 documents (total 1570000 corpus positions)
    2021-05-05 22:35:41,990 : INFO : adding document #0 to Dictionary(61440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:41,997 : INFO : built Dictionary(61659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 158 documents (total 1580000 corpus positions)
    2021-05-05 22:35:42,042 : INFO : adding document #0 to Dictionary(61659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,051 : INFO : built Dictionary(61915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 159 documents (total 1590000 corpus positions)
    2021-05-05 22:35:42,100 : INFO : adding document #0 to Dictionary(61915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,109 : INFO : built Dictionary(62163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 160 documents (total 1600000 corpus positions)
    2021-05-05 22:35:42,156 : INFO : adding document #0 to Dictionary(62163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,164 : INFO : built Dictionary(62295 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 161 documents (total 1610000 corpus positions)
    2021-05-05 22:35:42,211 : INFO : adding document #0 to Dictionary(62295 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,219 : INFO : built Dictionary(62495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 162 documents (total 1620000 corpus positions)
    2021-05-05 22:35:42,265 : INFO : adding document #0 to Dictionary(62495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,276 : INFO : built Dictionary(62821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 163 documents (total 1630000 corpus positions)
    2021-05-05 22:35:42,323 : INFO : adding document #0 to Dictionary(62821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,330 : INFO : built Dictionary(62975 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 164 documents (total 1640000 corpus positions)
    2021-05-05 22:35:42,375 : INFO : adding document #0 to Dictionary(62975 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,384 : INFO : built Dictionary(63237 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 165 documents (total 1650000 corpus positions)
    2021-05-05 22:35:42,437 : INFO : adding document #0 to Dictionary(63237 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,446 : INFO : built Dictionary(63517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 166 documents (total 1660000 corpus positions)
    2021-05-05 22:35:42,494 : INFO : adding document #0 to Dictionary(63517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,504 : INFO : built Dictionary(63860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 167 documents (total 1670000 corpus positions)
    2021-05-05 22:35:42,555 : INFO : adding document #0 to Dictionary(63860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,565 : INFO : built Dictionary(64072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 168 documents (total 1680000 corpus positions)
    2021-05-05 22:35:42,616 : INFO : adding document #0 to Dictionary(64072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,625 : INFO : built Dictionary(64198 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 169 documents (total 1690000 corpus positions)
    2021-05-05 22:35:42,679 : INFO : adding document #0 to Dictionary(64198 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,689 : INFO : built Dictionary(64384 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 170 documents (total 1700000 corpus positions)
    2021-05-05 22:35:42,741 : INFO : adding document #0 to Dictionary(64384 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,749 : INFO : built Dictionary(64537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 171 documents (total 1710000 corpus positions)
    2021-05-05 22:35:42,801 : INFO : adding document #0 to Dictionary(64537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,812 : INFO : built Dictionary(64748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 172 documents (total 1720000 corpus positions)
    2021-05-05 22:35:42,862 : INFO : adding document #0 to Dictionary(64748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,874 : INFO : built Dictionary(64997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 173 documents (total 1730000 corpus positions)
    2021-05-05 22:35:42,927 : INFO : adding document #0 to Dictionary(64997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,937 : INFO : built Dictionary(65321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 174 documents (total 1740000 corpus positions)
    2021-05-05 22:35:42,987 : INFO : adding document #0 to Dictionary(65321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:42,997 : INFO : built Dictionary(65527 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 175 documents (total 1750000 corpus positions)
    2021-05-05 22:35:43,047 : INFO : adding document #0 to Dictionary(65527 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,057 : INFO : built Dictionary(65821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 176 documents (total 1760000 corpus positions)
    2021-05-05 22:35:43,118 : INFO : adding document #0 to Dictionary(65821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,130 : INFO : built Dictionary(66074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 177 documents (total 1770000 corpus positions)
    2021-05-05 22:35:43,186 : INFO : adding document #0 to Dictionary(66074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,198 : INFO : built Dictionary(66275 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 178 documents (total 1780000 corpus positions)
    2021-05-05 22:35:43,247 : INFO : adding document #0 to Dictionary(66275 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,258 : INFO : built Dictionary(66562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 179 documents (total 1790000 corpus positions)
    2021-05-05 22:35:43,307 : INFO : adding document #0 to Dictionary(66562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,317 : INFO : built Dictionary(66761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 180 documents (total 1800000 corpus positions)
    2021-05-05 22:35:43,375 : INFO : adding document #0 to Dictionary(66761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,388 : INFO : built Dictionary(67013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 181 documents (total 1810000 corpus positions)
    2021-05-05 22:35:43,447 : INFO : adding document #0 to Dictionary(67013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,459 : INFO : built Dictionary(67260 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 182 documents (total 1820000 corpus positions)
    2021-05-05 22:35:43,512 : INFO : adding document #0 to Dictionary(67260 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,521 : INFO : built Dictionary(67506 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 183 documents (total 1830000 corpus positions)
    2021-05-05 22:35:43,567 : INFO : adding document #0 to Dictionary(67506 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,575 : INFO : built Dictionary(67701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 184 documents (total 1840000 corpus positions)
    2021-05-05 22:35:43,621 : INFO : adding document #0 to Dictionary(67701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,631 : INFO : built Dictionary(67914 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 185 documents (total 1850000 corpus positions)
    2021-05-05 22:35:43,681 : INFO : adding document #0 to Dictionary(67914 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,689 : INFO : built Dictionary(68096 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 186 documents (total 1860000 corpus positions)
    2021-05-05 22:35:43,739 : INFO : adding document #0 to Dictionary(68096 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,749 : INFO : built Dictionary(68319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 187 documents (total 1870000 corpus positions)
    2021-05-05 22:35:43,800 : INFO : adding document #0 to Dictionary(68319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,812 : INFO : built Dictionary(68621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 188 documents (total 1880000 corpus positions)
    2021-05-05 22:35:43,867 : INFO : adding document #0 to Dictionary(68621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,878 : INFO : built Dictionary(68875 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 189 documents (total 1890000 corpus positions)
    2021-05-05 22:35:43,936 : INFO : adding document #0 to Dictionary(68875 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:43,945 : INFO : built Dictionary(69130 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 190 documents (total 1900000 corpus positions)
    2021-05-05 22:35:43,996 : INFO : adding document #0 to Dictionary(69130 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,004 : INFO : built Dictionary(69306 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 191 documents (total 1910000 corpus positions)
    2021-05-05 22:35:44,055 : INFO : adding document #0 to Dictionary(69306 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,064 : INFO : built Dictionary(69479 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 192 documents (total 1920000 corpus positions)
    2021-05-05 22:35:44,118 : INFO : adding document #0 to Dictionary(69479 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,128 : INFO : built Dictionary(69741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 193 documents (total 1930000 corpus positions)
    2021-05-05 22:35:44,177 : INFO : adding document #0 to Dictionary(69741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,186 : INFO : built Dictionary(69934 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 194 documents (total 1940000 corpus positions)
    2021-05-05 22:35:44,236 : INFO : adding document #0 to Dictionary(69934 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,245 : INFO : built Dictionary(70090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 195 documents (total 1950000 corpus positions)
    2021-05-05 22:35:44,297 : INFO : adding document #0 to Dictionary(70090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,307 : INFO : built Dictionary(70442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 196 documents (total 1960000 corpus positions)
    2021-05-05 22:35:44,363 : INFO : adding document #0 to Dictionary(70442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,380 : INFO : built Dictionary(70634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 197 documents (total 1970000 corpus positions)
    2021-05-05 22:35:44,436 : INFO : adding document #0 to Dictionary(70634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,447 : INFO : built Dictionary(70951 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 198 documents (total 1980000 corpus positions)
    2021-05-05 22:35:44,500 : INFO : adding document #0 to Dictionary(70951 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,514 : INFO : built Dictionary(71175 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 199 documents (total 1990000 corpus positions)
    2021-05-05 22:35:44,563 : INFO : adding document #0 to Dictionary(71175 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,571 : INFO : built Dictionary(71354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 200 documents (total 2000000 corpus positions)
    2021-05-05 22:35:44,623 : INFO : adding document #0 to Dictionary(71354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,631 : INFO : built Dictionary(71464 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 201 documents (total 2010000 corpus positions)
    2021-05-05 22:35:44,682 : INFO : adding document #0 to Dictionary(71464 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,694 : INFO : built Dictionary(71697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 202 documents (total 2020000 corpus positions)
    2021-05-05 22:35:44,745 : INFO : adding document #0 to Dictionary(71697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,753 : INFO : built Dictionary(71804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 203 documents (total 2030000 corpus positions)
    2021-05-05 22:35:44,806 : INFO : adding document #0 to Dictionary(71804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,815 : INFO : built Dictionary(71947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 204 documents (total 2040000 corpus positions)
    2021-05-05 22:35:44,865 : INFO : adding document #0 to Dictionary(71947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,875 : INFO : built Dictionary(72137 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 205 documents (total 2050000 corpus positions)
    2021-05-05 22:35:44,928 : INFO : adding document #0 to Dictionary(72137 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:44,939 : INFO : built Dictionary(72359 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 206 documents (total 2060000 corpus positions)
    2021-05-05 22:35:44,990 : INFO : adding document #0 to Dictionary(72359 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,000 : INFO : built Dictionary(72547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 207 documents (total 2070000 corpus positions)
    2021-05-05 22:35:45,061 : INFO : adding document #0 to Dictionary(72547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,070 : INFO : built Dictionary(72768 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 208 documents (total 2080000 corpus positions)
    2021-05-05 22:35:45,128 : INFO : adding document #0 to Dictionary(72768 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,136 : INFO : built Dictionary(72963 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 209 documents (total 2090000 corpus positions)
    2021-05-05 22:35:45,189 : INFO : adding document #0 to Dictionary(72963 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,199 : INFO : built Dictionary(73150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 210 documents (total 2100000 corpus positions)
    2021-05-05 22:35:45,249 : INFO : adding document #0 to Dictionary(73150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,257 : INFO : built Dictionary(73299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 211 documents (total 2110000 corpus positions)
    2021-05-05 22:35:45,311 : INFO : adding document #0 to Dictionary(73299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,320 : INFO : built Dictionary(73442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 212 documents (total 2120000 corpus positions)
    2021-05-05 22:35:45,370 : INFO : adding document #0 to Dictionary(73442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,380 : INFO : built Dictionary(73613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 213 documents (total 2130000 corpus positions)
    2021-05-05 22:35:45,431 : INFO : adding document #0 to Dictionary(73613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,440 : INFO : built Dictionary(73755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 214 documents (total 2140000 corpus positions)
    2021-05-05 22:35:45,491 : INFO : adding document #0 to Dictionary(73755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,498 : INFO : built Dictionary(73889 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 215 documents (total 2150000 corpus positions)
    2021-05-05 22:35:45,550 : INFO : adding document #0 to Dictionary(73889 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,559 : INFO : built Dictionary(73986 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 216 documents (total 2160000 corpus positions)
    2021-05-05 22:35:45,611 : INFO : adding document #0 to Dictionary(73986 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,622 : INFO : built Dictionary(74149 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 217 documents (total 2170000 corpus positions)
    2021-05-05 22:35:45,674 : INFO : adding document #0 to Dictionary(74149 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,683 : INFO : built Dictionary(74300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 218 documents (total 2180000 corpus positions)
    2021-05-05 22:35:45,733 : INFO : adding document #0 to Dictionary(74300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,743 : INFO : built Dictionary(74430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 219 documents (total 2190000 corpus positions)
    2021-05-05 22:35:45,795 : INFO : adding document #0 to Dictionary(74430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,805 : INFO : built Dictionary(74596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 220 documents (total 2200000 corpus positions)
    2021-05-05 22:35:45,857 : INFO : adding document #0 to Dictionary(74596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,868 : INFO : built Dictionary(74840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 221 documents (total 2210000 corpus positions)
    2021-05-05 22:35:45,917 : INFO : adding document #0 to Dictionary(74840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,929 : INFO : built Dictionary(75063 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 222 documents (total 2220000 corpus positions)
    2021-05-05 22:35:45,980 : INFO : adding document #0 to Dictionary(75063 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:45,990 : INFO : built Dictionary(75278 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 223 documents (total 2230000 corpus positions)
    2021-05-05 22:35:46,040 : INFO : adding document #0 to Dictionary(75278 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,050 : INFO : built Dictionary(75512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 224 documents (total 2240000 corpus positions)
    2021-05-05 22:35:46,100 : INFO : adding document #0 to Dictionary(75512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,108 : INFO : built Dictionary(75707 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 225 documents (total 2250000 corpus positions)
    2021-05-05 22:35:46,155 : INFO : adding document #0 to Dictionary(75707 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,164 : INFO : built Dictionary(75892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 226 documents (total 2260000 corpus positions)
    2021-05-05 22:35:46,209 : INFO : adding document #0 to Dictionary(75892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,216 : INFO : built Dictionary(76033 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 227 documents (total 2270000 corpus positions)
    2021-05-05 22:35:46,268 : INFO : adding document #0 to Dictionary(76033 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,276 : INFO : built Dictionary(76216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 228 documents (total 2280000 corpus positions)
    2021-05-05 22:35:46,332 : INFO : adding document #0 to Dictionary(76216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,341 : INFO : built Dictionary(76357 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 229 documents (total 2290000 corpus positions)
    2021-05-05 22:35:46,388 : INFO : adding document #0 to Dictionary(76357 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,396 : INFO : built Dictionary(76492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 230 documents (total 2300000 corpus positions)
    2021-05-05 22:35:46,444 : INFO : adding document #0 to Dictionary(76492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,454 : INFO : built Dictionary(76682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 231 documents (total 2310000 corpus positions)
    2021-05-05 22:35:46,506 : INFO : adding document #0 to Dictionary(76682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,515 : INFO : built Dictionary(76947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 232 documents (total 2320000 corpus positions)
    2021-05-05 22:35:46,573 : INFO : adding document #0 to Dictionary(76947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,583 : INFO : built Dictionary(77189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 233 documents (total 2330000 corpus positions)
    2021-05-05 22:35:46,632 : INFO : adding document #0 to Dictionary(77189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,643 : INFO : built Dictionary(77451 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 234 documents (total 2340000 corpus positions)
    2021-05-05 22:35:46,691 : INFO : adding document #0 to Dictionary(77451 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,701 : INFO : built Dictionary(77719 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 235 documents (total 2350000 corpus positions)
    2021-05-05 22:35:46,749 : INFO : adding document #0 to Dictionary(77719 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,759 : INFO : built Dictionary(77989 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 236 documents (total 2360000 corpus positions)
    2021-05-05 22:35:46,805 : INFO : adding document #0 to Dictionary(77989 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,815 : INFO : built Dictionary(78233 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 237 documents (total 2370000 corpus positions)
    2021-05-05 22:35:46,860 : INFO : adding document #0 to Dictionary(78233 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,868 : INFO : built Dictionary(78374 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 238 documents (total 2380000 corpus positions)
    2021-05-05 22:35:46,914 : INFO : adding document #0 to Dictionary(78374 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,924 : INFO : built Dictionary(78577 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 239 documents (total 2390000 corpus positions)
    2021-05-05 22:35:46,969 : INFO : adding document #0 to Dictionary(78577 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:46,978 : INFO : built Dictionary(78758 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 240 documents (total 2400000 corpus positions)
    2021-05-05 22:35:47,034 : INFO : adding document #0 to Dictionary(78758 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,043 : INFO : built Dictionary(78896 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 241 documents (total 2410000 corpus positions)
    2021-05-05 22:35:47,098 : INFO : adding document #0 to Dictionary(78896 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,109 : INFO : built Dictionary(79128 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 242 documents (total 2420000 corpus positions)
    2021-05-05 22:35:47,162 : INFO : adding document #0 to Dictionary(79128 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,171 : INFO : built Dictionary(79261 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 243 documents (total 2430000 corpus positions)
    2021-05-05 22:35:47,221 : INFO : adding document #0 to Dictionary(79261 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,230 : INFO : built Dictionary(79438 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 244 documents (total 2440000 corpus positions)
    2021-05-05 22:35:47,281 : INFO : adding document #0 to Dictionary(79438 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,289 : INFO : built Dictionary(79545 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 245 documents (total 2450000 corpus positions)
    2021-05-05 22:35:47,344 : INFO : adding document #0 to Dictionary(79545 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,354 : INFO : built Dictionary(79654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 246 documents (total 2460000 corpus positions)
    2021-05-05 22:35:47,412 : INFO : adding document #0 to Dictionary(79654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,420 : INFO : built Dictionary(79769 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 247 documents (total 2470000 corpus positions)
    2021-05-05 22:35:47,470 : INFO : adding document #0 to Dictionary(79769 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,480 : INFO : built Dictionary(79887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 248 documents (total 2480000 corpus positions)
    2021-05-05 22:35:47,530 : INFO : adding document #0 to Dictionary(79887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,541 : INFO : built Dictionary(80129 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 249 documents (total 2490000 corpus positions)
    2021-05-05 22:35:47,592 : INFO : adding document #0 to Dictionary(80129 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,601 : INFO : built Dictionary(80286 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 250 documents (total 2500000 corpus positions)
    2021-05-05 22:35:47,650 : INFO : adding document #0 to Dictionary(80286 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,661 : INFO : built Dictionary(80379 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 251 documents (total 2510000 corpus positions)
    2021-05-05 22:35:47,715 : INFO : adding document #0 to Dictionary(80379 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,726 : INFO : built Dictionary(80648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 252 documents (total 2520000 corpus positions)
    2021-05-05 22:35:47,779 : INFO : adding document #0 to Dictionary(80648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,789 : INFO : built Dictionary(80937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 253 documents (total 2530000 corpus positions)
    2021-05-05 22:35:47,846 : INFO : adding document #0 to Dictionary(80937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,854 : INFO : built Dictionary(81090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 254 documents (total 2540000 corpus positions)
    2021-05-05 22:35:47,904 : INFO : adding document #0 to Dictionary(81090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,915 : INFO : built Dictionary(81273 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 255 documents (total 2550000 corpus positions)
    2021-05-05 22:35:47,967 : INFO : adding document #0 to Dictionary(81273 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:47,977 : INFO : built Dictionary(81547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 256 documents (total 2560000 corpus positions)
    2021-05-05 22:35:48,030 : INFO : adding document #0 to Dictionary(81547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,038 : INFO : built Dictionary(81698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 257 documents (total 2570000 corpus positions)
    2021-05-05 22:35:48,087 : INFO : adding document #0 to Dictionary(81698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,097 : INFO : built Dictionary(81927 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 258 documents (total 2580000 corpus positions)
    2021-05-05 22:35:48,146 : INFO : adding document #0 to Dictionary(81927 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,154 : INFO : built Dictionary(82024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 259 documents (total 2590000 corpus positions)
    2021-05-05 22:35:48,203 : INFO : adding document #0 to Dictionary(82024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,214 : INFO : built Dictionary(82218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 260 documents (total 2600000 corpus positions)
    2021-05-05 22:35:48,264 : INFO : adding document #0 to Dictionary(82218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,276 : INFO : built Dictionary(82461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 261 documents (total 2610000 corpus positions)
    2021-05-05 22:35:48,325 : INFO : adding document #0 to Dictionary(82461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,333 : INFO : built Dictionary(82558 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 262 documents (total 2620000 corpus positions)
    2021-05-05 22:35:48,383 : INFO : adding document #0 to Dictionary(82558 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,391 : INFO : built Dictionary(82649 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 263 documents (total 2630000 corpus positions)
    2021-05-05 22:35:48,441 : INFO : adding document #0 to Dictionary(82649 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,450 : INFO : built Dictionary(82786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 264 documents (total 2640000 corpus positions)
    2021-05-05 22:35:48,498 : INFO : adding document #0 to Dictionary(82786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,507 : INFO : built Dictionary(83056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 265 documents (total 2650000 corpus positions)
    2021-05-05 22:35:48,558 : INFO : adding document #0 to Dictionary(83056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,567 : INFO : built Dictionary(83269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 266 documents (total 2660000 corpus positions)
    2021-05-05 22:35:48,618 : INFO : adding document #0 to Dictionary(83269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,628 : INFO : built Dictionary(83411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 267 documents (total 2670000 corpus positions)
    2021-05-05 22:35:48,677 : INFO : adding document #0 to Dictionary(83411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,684 : INFO : built Dictionary(83536 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 268 documents (total 2680000 corpus positions)
    2021-05-05 22:35:48,737 : INFO : adding document #0 to Dictionary(83536 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,745 : INFO : built Dictionary(83762 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 269 documents (total 2690000 corpus positions)
    2021-05-05 22:35:48,792 : INFO : adding document #0 to Dictionary(83762 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,800 : INFO : built Dictionary(83922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 270 documents (total 2700000 corpus positions)
    2021-05-05 22:35:48,848 : INFO : adding document #0 to Dictionary(83922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,858 : INFO : built Dictionary(84113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 271 documents (total 2710000 corpus positions)
    2021-05-05 22:35:48,904 : INFO : adding document #0 to Dictionary(84113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,913 : INFO : built Dictionary(84205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 272 documents (total 2720000 corpus positions)
    2021-05-05 22:35:48,962 : INFO : adding document #0 to Dictionary(84205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:48,971 : INFO : built Dictionary(84339 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 273 documents (total 2730000 corpus positions)
    2021-05-05 22:35:49,018 : INFO : adding document #0 to Dictionary(84339 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,025 : INFO : built Dictionary(84411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 274 documents (total 2740000 corpus positions)
    2021-05-05 22:35:49,073 : INFO : adding document #0 to Dictionary(84411 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,081 : INFO : built Dictionary(84491 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 275 documents (total 2750000 corpus positions)
    2021-05-05 22:35:49,128 : INFO : adding document #0 to Dictionary(84491 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,135 : INFO : built Dictionary(84598 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 276 documents (total 2760000 corpus positions)
    2021-05-05 22:35:49,184 : INFO : adding document #0 to Dictionary(84598 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,192 : INFO : built Dictionary(84687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 277 documents (total 2770000 corpus positions)
    2021-05-05 22:35:49,239 : INFO : adding document #0 to Dictionary(84687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,248 : INFO : built Dictionary(84864 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 278 documents (total 2780000 corpus positions)
    2021-05-05 22:35:49,299 : INFO : adding document #0 to Dictionary(84864 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,308 : INFO : built Dictionary(85069 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 279 documents (total 2790000 corpus positions)
    2021-05-05 22:35:49,355 : INFO : adding document #0 to Dictionary(85069 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,364 : INFO : built Dictionary(85228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 280 documents (total 2800000 corpus positions)
    2021-05-05 22:35:49,414 : INFO : adding document #0 to Dictionary(85228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,421 : INFO : built Dictionary(85331 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 281 documents (total 2810000 corpus positions)
    2021-05-05 22:35:49,478 : INFO : adding document #0 to Dictionary(85331 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,488 : INFO : built Dictionary(85458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 282 documents (total 2820000 corpus positions)
    2021-05-05 22:35:49,540 : INFO : adding document #0 to Dictionary(85458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,549 : INFO : built Dictionary(85593 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 283 documents (total 2830000 corpus positions)
    2021-05-05 22:35:49,603 : INFO : adding document #0 to Dictionary(85593 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,612 : INFO : built Dictionary(85713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 284 documents (total 2840000 corpus positions)
    2021-05-05 22:35:49,663 : INFO : adding document #0 to Dictionary(85713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,674 : INFO : built Dictionary(85931 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 285 documents (total 2850000 corpus positions)
    2021-05-05 22:35:49,727 : INFO : adding document #0 to Dictionary(85931 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,738 : INFO : built Dictionary(86057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 286 documents (total 2860000 corpus positions)
    2021-05-05 22:35:49,790 : INFO : adding document #0 to Dictionary(86057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,799 : INFO : built Dictionary(86188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 287 documents (total 2870000 corpus positions)
    2021-05-05 22:35:49,848 : INFO : adding document #0 to Dictionary(86188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,858 : INFO : built Dictionary(86415 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 288 documents (total 2880000 corpus positions)
    2021-05-05 22:35:49,911 : INFO : adding document #0 to Dictionary(86415 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,919 : INFO : built Dictionary(86517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 289 documents (total 2890000 corpus positions)
    2021-05-05 22:35:49,970 : INFO : adding document #0 to Dictionary(86517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:49,982 : INFO : built Dictionary(86920 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 290 documents (total 2900000 corpus positions)
    2021-05-05 22:35:50,033 : INFO : adding document #0 to Dictionary(86920 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,041 : INFO : built Dictionary(87071 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 291 documents (total 2910000 corpus positions)
    2021-05-05 22:35:50,092 : INFO : adding document #0 to Dictionary(87071 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,101 : INFO : built Dictionary(87224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 292 documents (total 2920000 corpus positions)
    2021-05-05 22:35:50,157 : INFO : adding document #0 to Dictionary(87224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,167 : INFO : built Dictionary(87289 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 293 documents (total 2930000 corpus positions)
    2021-05-05 22:35:50,217 : INFO : adding document #0 to Dictionary(87289 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,226 : INFO : built Dictionary(87368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 294 documents (total 2940000 corpus positions)
    2021-05-05 22:35:50,278 : INFO : adding document #0 to Dictionary(87368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,300 : INFO : built Dictionary(87452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 295 documents (total 2950000 corpus positions)
    2021-05-05 22:35:50,357 : INFO : adding document #0 to Dictionary(87452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,369 : INFO : built Dictionary(87614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 296 documents (total 2960000 corpus positions)
    2021-05-05 22:35:50,426 : INFO : adding document #0 to Dictionary(87614 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,436 : INFO : built Dictionary(87766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 297 documents (total 2970000 corpus positions)
    2021-05-05 22:35:50,487 : INFO : adding document #0 to Dictionary(87766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,495 : INFO : built Dictionary(87901 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 298 documents (total 2980000 corpus positions)
    2021-05-05 22:35:50,541 : INFO : adding document #0 to Dictionary(87901 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,551 : INFO : built Dictionary(88063 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 299 documents (total 2990000 corpus positions)
    2021-05-05 22:35:50,601 : INFO : adding document #0 to Dictionary(88063 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,610 : INFO : built Dictionary(88333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 300 documents (total 3000000 corpus positions)
    2021-05-05 22:35:50,655 : INFO : adding document #0 to Dictionary(88333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,663 : INFO : built Dictionary(88408 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 301 documents (total 3010000 corpus positions)
    2021-05-05 22:35:50,710 : INFO : adding document #0 to Dictionary(88408 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,721 : INFO : built Dictionary(88589 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 302 documents (total 3020000 corpus positions)
    2021-05-05 22:35:50,768 : INFO : adding document #0 to Dictionary(88589 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,778 : INFO : built Dictionary(88800 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 303 documents (total 3030000 corpus positions)
    2021-05-05 22:35:50,827 : INFO : adding document #0 to Dictionary(88800 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,836 : INFO : built Dictionary(88944 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 304 documents (total 3040000 corpus positions)
    2021-05-05 22:35:50,882 : INFO : adding document #0 to Dictionary(88944 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,891 : INFO : built Dictionary(89095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 305 documents (total 3050000 corpus positions)
    2021-05-05 22:35:50,937 : INFO : adding document #0 to Dictionary(89095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,946 : INFO : built Dictionary(89272 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 306 documents (total 3060000 corpus positions)
    2021-05-05 22:35:50,991 : INFO : adding document #0 to Dictionary(89272 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:50,999 : INFO : built Dictionary(89396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 307 documents (total 3070000 corpus positions)
    2021-05-05 22:35:51,046 : INFO : adding document #0 to Dictionary(89396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,054 : INFO : built Dictionary(89505 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 308 documents (total 3080000 corpus positions)
    2021-05-05 22:35:51,104 : INFO : adding document #0 to Dictionary(89505 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,113 : INFO : built Dictionary(89656 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 309 documents (total 3090000 corpus positions)
    2021-05-05 22:35:51,162 : INFO : adding document #0 to Dictionary(89656 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,170 : INFO : built Dictionary(89740 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 310 documents (total 3100000 corpus positions)
    2021-05-05 22:35:51,217 : INFO : adding document #0 to Dictionary(89740 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,225 : INFO : built Dictionary(89822 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 311 documents (total 3110000 corpus positions)
    2021-05-05 22:35:51,269 : INFO : adding document #0 to Dictionary(89822 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,277 : INFO : built Dictionary(89940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 312 documents (total 3120000 corpus positions)
    2021-05-05 22:35:51,321 : INFO : adding document #0 to Dictionary(89940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,329 : INFO : built Dictionary(90078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 313 documents (total 3130000 corpus positions)
    2021-05-05 22:35:51,375 : INFO : adding document #0 to Dictionary(90078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,384 : INFO : built Dictionary(90168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 314 documents (total 3140000 corpus positions)
    2021-05-05 22:35:51,438 : INFO : adding document #0 to Dictionary(90168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,446 : INFO : built Dictionary(90275 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 315 documents (total 3150000 corpus positions)
    2021-05-05 22:35:51,490 : INFO : adding document #0 to Dictionary(90275 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,499 : INFO : built Dictionary(90457 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 316 documents (total 3160000 corpus positions)
    2021-05-05 22:35:51,552 : INFO : adding document #0 to Dictionary(90457 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,563 : INFO : built Dictionary(90610 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 317 documents (total 3170000 corpus positions)
    2021-05-05 22:35:51,609 : INFO : adding document #0 to Dictionary(90610 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,617 : INFO : built Dictionary(90730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 318 documents (total 3180000 corpus positions)
    2021-05-05 22:35:51,668 : INFO : adding document #0 to Dictionary(90730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,676 : INFO : built Dictionary(90831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 319 documents (total 3190000 corpus positions)
    2021-05-05 22:35:51,725 : INFO : adding document #0 to Dictionary(90831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,734 : INFO : built Dictionary(90966 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 320 documents (total 3200000 corpus positions)
    2021-05-05 22:35:51,780 : INFO : adding document #0 to Dictionary(90966 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,788 : INFO : built Dictionary(91088 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 321 documents (total 3210000 corpus positions)
    2021-05-05 22:35:51,838 : INFO : adding document #0 to Dictionary(91088 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,846 : INFO : built Dictionary(91214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 322 documents (total 3220000 corpus positions)
    2021-05-05 22:35:51,891 : INFO : adding document #0 to Dictionary(91214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,899 : INFO : built Dictionary(91323 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 323 documents (total 3230000 corpus positions)
    2021-05-05 22:35:51,944 : INFO : adding document #0 to Dictionary(91323 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:51,954 : INFO : built Dictionary(91523 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 324 documents (total 3240000 corpus positions)
    2021-05-05 22:35:51,999 : INFO : adding document #0 to Dictionary(91523 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,008 : INFO : built Dictionary(91717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 325 documents (total 3250000 corpus positions)
    2021-05-05 22:35:52,053 : INFO : adding document #0 to Dictionary(91717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,062 : INFO : built Dictionary(91857 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 326 documents (total 3260000 corpus positions)
    2021-05-05 22:35:52,114 : INFO : adding document #0 to Dictionary(91857 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,125 : INFO : built Dictionary(92032 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 327 documents (total 3270000 corpus positions)
    2021-05-05 22:35:52,169 : INFO : adding document #0 to Dictionary(92032 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,178 : INFO : built Dictionary(92163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 328 documents (total 3280000 corpus positions)
    2021-05-05 22:35:52,230 : INFO : adding document #0 to Dictionary(92163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,238 : INFO : built Dictionary(92311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 329 documents (total 3290000 corpus positions)
    2021-05-05 22:35:52,284 : INFO : adding document #0 to Dictionary(92311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,292 : INFO : built Dictionary(92409 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 330 documents (total 3300000 corpus positions)
    2021-05-05 22:35:52,337 : INFO : adding document #0 to Dictionary(92409 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,346 : INFO : built Dictionary(92533 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 331 documents (total 3310000 corpus positions)
    2021-05-05 22:35:52,393 : INFO : adding document #0 to Dictionary(92533 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,401 : INFO : built Dictionary(92621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 332 documents (total 3320000 corpus positions)
    2021-05-05 22:35:52,445 : INFO : adding document #0 to Dictionary(92621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,452 : INFO : built Dictionary(92730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 333 documents (total 3330000 corpus positions)
    2021-05-05 22:35:52,500 : INFO : adding document #0 to Dictionary(92730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,510 : INFO : built Dictionary(92999 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 334 documents (total 3340000 corpus positions)
    2021-05-05 22:35:52,555 : INFO : adding document #0 to Dictionary(92999 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,564 : INFO : built Dictionary(93146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 335 documents (total 3350000 corpus positions)
    2021-05-05 22:35:52,610 : INFO : adding document #0 to Dictionary(93146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,620 : INFO : built Dictionary(93326 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 336 documents (total 3360000 corpus positions)
    2021-05-05 22:35:52,669 : INFO : adding document #0 to Dictionary(93326 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,678 : INFO : built Dictionary(93485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 337 documents (total 3370000 corpus positions)
    2021-05-05 22:35:52,723 : INFO : adding document #0 to Dictionary(93485 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,731 : INFO : built Dictionary(93621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 338 documents (total 3380000 corpus positions)
    2021-05-05 22:35:52,778 : INFO : adding document #0 to Dictionary(93621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,786 : INFO : built Dictionary(93823 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 339 documents (total 3390000 corpus positions)
    2021-05-05 22:35:52,836 : INFO : adding document #0 to Dictionary(93823 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,846 : INFO : built Dictionary(93970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 340 documents (total 3400000 corpus positions)
    2021-05-05 22:35:52,898 : INFO : adding document #0 to Dictionary(93970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,909 : INFO : built Dictionary(94159 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 341 documents (total 3410000 corpus positions)
    2021-05-05 22:35:52,956 : INFO : adding document #0 to Dictionary(94159 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:52,965 : INFO : built Dictionary(94291 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 342 documents (total 3420000 corpus positions)
    2021-05-05 22:35:53,022 : INFO : adding document #0 to Dictionary(94291 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,033 : INFO : built Dictionary(94472 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 343 documents (total 3430000 corpus positions)
    2021-05-05 22:35:53,083 : INFO : adding document #0 to Dictionary(94472 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,092 : INFO : built Dictionary(94589 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 344 documents (total 3440000 corpus positions)
    2021-05-05 22:35:53,149 : INFO : adding document #0 to Dictionary(94589 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,159 : INFO : built Dictionary(94701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 345 documents (total 3450000 corpus positions)
    2021-05-05 22:35:53,221 : INFO : adding document #0 to Dictionary(94701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,232 : INFO : built Dictionary(94809 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 346 documents (total 3460000 corpus positions)
    2021-05-05 22:35:53,282 : INFO : adding document #0 to Dictionary(94809 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,292 : INFO : built Dictionary(94935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 347 documents (total 3470000 corpus positions)
    2021-05-05 22:35:53,338 : INFO : adding document #0 to Dictionary(94935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,347 : INFO : built Dictionary(95081 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 348 documents (total 3480000 corpus positions)
    2021-05-05 22:35:53,394 : INFO : adding document #0 to Dictionary(95081 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,402 : INFO : built Dictionary(95212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 349 documents (total 3490000 corpus positions)
    2021-05-05 22:35:53,451 : INFO : adding document #0 to Dictionary(95212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,460 : INFO : built Dictionary(95447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 350 documents (total 3500000 corpus positions)
    2021-05-05 22:35:53,505 : INFO : adding document #0 to Dictionary(95447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,514 : INFO : built Dictionary(95600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 351 documents (total 3510000 corpus positions)
    2021-05-05 22:35:53,558 : INFO : adding document #0 to Dictionary(95600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,567 : INFO : built Dictionary(95741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 352 documents (total 3520000 corpus positions)
    2021-05-05 22:35:53,613 : INFO : adding document #0 to Dictionary(95741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,622 : INFO : built Dictionary(95962 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 353 documents (total 3530000 corpus positions)
    2021-05-05 22:35:53,670 : INFO : adding document #0 to Dictionary(95962 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,679 : INFO : built Dictionary(96154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 354 documents (total 3540000 corpus positions)
    2021-05-05 22:35:53,725 : INFO : adding document #0 to Dictionary(96154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,734 : INFO : built Dictionary(96193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 355 documents (total 3550000 corpus positions)
    2021-05-05 22:35:53,780 : INFO : adding document #0 to Dictionary(96193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,790 : INFO : built Dictionary(96315 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 356 documents (total 3560000 corpus positions)
    2021-05-05 22:35:53,838 : INFO : adding document #0 to Dictionary(96315 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,849 : INFO : built Dictionary(96498 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 357 documents (total 3570000 corpus positions)
    2021-05-05 22:35:53,899 : INFO : adding document #0 to Dictionary(96498 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,908 : INFO : built Dictionary(96628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 358 documents (total 3580000 corpus positions)
    2021-05-05 22:35:53,955 : INFO : adding document #0 to Dictionary(96628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:53,965 : INFO : built Dictionary(96810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 359 documents (total 3590000 corpus positions)
    2021-05-05 22:35:54,018 : INFO : adding document #0 to Dictionary(96810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,028 : INFO : built Dictionary(96971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 360 documents (total 3600000 corpus positions)
    2021-05-05 22:35:54,074 : INFO : adding document #0 to Dictionary(96971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,083 : INFO : built Dictionary(97100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 361 documents (total 3610000 corpus positions)
    2021-05-05 22:35:54,128 : INFO : adding document #0 to Dictionary(97100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,137 : INFO : built Dictionary(97235 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 362 documents (total 3620000 corpus positions)
    2021-05-05 22:35:54,184 : INFO : adding document #0 to Dictionary(97235 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,194 : INFO : built Dictionary(97327 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 363 documents (total 3630000 corpus positions)
    2021-05-05 22:35:54,241 : INFO : adding document #0 to Dictionary(97327 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,249 : INFO : built Dictionary(97474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 364 documents (total 3640000 corpus positions)
    2021-05-05 22:35:54,295 : INFO : adding document #0 to Dictionary(97474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,304 : INFO : built Dictionary(97562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 365 documents (total 3650000 corpus positions)
    2021-05-05 22:35:54,351 : INFO : adding document #0 to Dictionary(97562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,359 : INFO : built Dictionary(97701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 366 documents (total 3660000 corpus positions)
    2021-05-05 22:35:54,409 : INFO : adding document #0 to Dictionary(97701 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,417 : INFO : built Dictionary(97828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 367 documents (total 3670000 corpus positions)
    2021-05-05 22:35:54,462 : INFO : adding document #0 to Dictionary(97828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,472 : INFO : built Dictionary(97993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 368 documents (total 3680000 corpus positions)
    2021-05-05 22:35:54,520 : INFO : adding document #0 to Dictionary(97993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,528 : INFO : built Dictionary(98111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 369 documents (total 3690000 corpus positions)
    2021-05-05 22:35:54,572 : INFO : adding document #0 to Dictionary(98111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,585 : INFO : built Dictionary(98325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 370 documents (total 3700000 corpus positions)
    2021-05-05 22:35:54,635 : INFO : adding document #0 to Dictionary(98325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,645 : INFO : built Dictionary(98510 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 371 documents (total 3710000 corpus positions)
    2021-05-05 22:35:54,689 : INFO : adding document #0 to Dictionary(98510 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,697 : INFO : built Dictionary(98608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 372 documents (total 3720000 corpus positions)
    2021-05-05 22:35:54,749 : INFO : adding document #0 to Dictionary(98608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,757 : INFO : built Dictionary(98746 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 373 documents (total 3730000 corpus positions)
    2021-05-05 22:35:54,802 : INFO : adding document #0 to Dictionary(98746 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,811 : INFO : built Dictionary(98886 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 374 documents (total 3740000 corpus positions)
    2021-05-05 22:35:54,857 : INFO : adding document #0 to Dictionary(98886 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,867 : INFO : built Dictionary(99025 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 375 documents (total 3750000 corpus positions)
    2021-05-05 22:35:54,911 : INFO : adding document #0 to Dictionary(99025 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,920 : INFO : built Dictionary(99177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 376 documents (total 3760000 corpus positions)
    2021-05-05 22:35:54,965 : INFO : adding document #0 to Dictionary(99177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:54,973 : INFO : built Dictionary(99310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 377 documents (total 3770000 corpus positions)
    2021-05-05 22:35:55,023 : INFO : adding document #0 to Dictionary(99310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,034 : INFO : built Dictionary(99510 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 378 documents (total 3780000 corpus positions)
    2021-05-05 22:35:55,079 : INFO : adding document #0 to Dictionary(99510 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,088 : INFO : built Dictionary(99687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 379 documents (total 3790000 corpus positions)
    2021-05-05 22:35:55,135 : INFO : adding document #0 to Dictionary(99687 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,144 : INFO : built Dictionary(99739 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 380 documents (total 3800000 corpus positions)
    2021-05-05 22:35:55,189 : INFO : adding document #0 to Dictionary(99739 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,200 : INFO : built Dictionary(99880 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 381 documents (total 3810000 corpus positions)
    2021-05-05 22:35:55,250 : INFO : adding document #0 to Dictionary(99880 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,259 : INFO : built Dictionary(99993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 382 documents (total 3820000 corpus positions)
    2021-05-05 22:35:55,307 : INFO : adding document #0 to Dictionary(99993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,315 : INFO : built Dictionary(100051 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 383 documents (total 3830000 corpus positions)
    2021-05-05 22:35:55,367 : INFO : adding document #0 to Dictionary(100051 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,376 : INFO : built Dictionary(100093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 384 documents (total 3840000 corpus positions)
    2021-05-05 22:35:55,421 : INFO : adding document #0 to Dictionary(100093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,430 : INFO : built Dictionary(100188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 385 documents (total 3850000 corpus positions)
    2021-05-05 22:35:55,477 : INFO : adding document #0 to Dictionary(100188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,485 : INFO : built Dictionary(100335 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 386 documents (total 3860000 corpus positions)
    2021-05-05 22:35:55,535 : INFO : adding document #0 to Dictionary(100335 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,545 : INFO : built Dictionary(100596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 387 documents (total 3870000 corpus positions)
    2021-05-05 22:35:55,589 : INFO : adding document #0 to Dictionary(100596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,599 : INFO : built Dictionary(100749 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 388 documents (total 3880000 corpus positions)
    2021-05-05 22:35:55,645 : INFO : adding document #0 to Dictionary(100749 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,654 : INFO : built Dictionary(100812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 389 documents (total 3890000 corpus positions)
    2021-05-05 22:35:55,712 : INFO : adding document #0 to Dictionary(100812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,721 : INFO : built Dictionary(100953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 390 documents (total 3900000 corpus positions)
    2021-05-05 22:35:55,769 : INFO : adding document #0 to Dictionary(100953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,777 : INFO : built Dictionary(101035 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 391 documents (total 3910000 corpus positions)
    2021-05-05 22:35:55,825 : INFO : adding document #0 to Dictionary(101035 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,833 : INFO : built Dictionary(101185 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 392 documents (total 3920000 corpus positions)
    2021-05-05 22:35:55,877 : INFO : adding document #0 to Dictionary(101185 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,885 : INFO : built Dictionary(101261 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 393 documents (total 3930000 corpus positions)
    2021-05-05 22:35:55,932 : INFO : adding document #0 to Dictionary(101261 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:55,940 : INFO : built Dictionary(101358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 394 documents (total 3940000 corpus positions)
    2021-05-05 22:35:55,987 : INFO : adding document #0 to Dictionary(101358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,002 : INFO : built Dictionary(101512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 395 documents (total 3950000 corpus positions)
    2021-05-05 22:35:56,059 : INFO : adding document #0 to Dictionary(101512 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,067 : INFO : built Dictionary(101667 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 396 documents (total 3960000 corpus positions)
    2021-05-05 22:35:56,117 : INFO : adding document #0 to Dictionary(101667 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,126 : INFO : built Dictionary(101752 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 397 documents (total 3970000 corpus positions)
    2021-05-05 22:35:56,171 : INFO : adding document #0 to Dictionary(101752 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,180 : INFO : built Dictionary(101874 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 398 documents (total 3980000 corpus positions)
    2021-05-05 22:35:56,225 : INFO : adding document #0 to Dictionary(101874 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,234 : INFO : built Dictionary(101971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 399 documents (total 3990000 corpus positions)
    2021-05-05 22:35:56,278 : INFO : adding document #0 to Dictionary(101971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,286 : INFO : built Dictionary(102121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 400 documents (total 4000000 corpus positions)
    2021-05-05 22:35:56,333 : INFO : adding document #0 to Dictionary(102121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,342 : INFO : built Dictionary(102210 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 401 documents (total 4010000 corpus positions)
    2021-05-05 22:35:56,387 : INFO : adding document #0 to Dictionary(102210 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,395 : INFO : built Dictionary(102325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 402 documents (total 4020000 corpus positions)
    2021-05-05 22:35:56,450 : INFO : adding document #0 to Dictionary(102325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,463 : INFO : built Dictionary(102396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 403 documents (total 4030000 corpus positions)
    2021-05-05 22:35:56,519 : INFO : adding document #0 to Dictionary(102396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,527 : INFO : built Dictionary(102479 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 404 documents (total 4040000 corpus positions)
    2021-05-05 22:35:56,574 : INFO : adding document #0 to Dictionary(102479 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,583 : INFO : built Dictionary(102659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 405 documents (total 4050000 corpus positions)
    2021-05-05 22:35:56,626 : INFO : adding document #0 to Dictionary(102659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,635 : INFO : built Dictionary(102804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 406 documents (total 4060000 corpus positions)
    2021-05-05 22:35:56,681 : INFO : adding document #0 to Dictionary(102804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,689 : INFO : built Dictionary(102870 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 407 documents (total 4070000 corpus positions)
    2021-05-05 22:35:56,735 : INFO : adding document #0 to Dictionary(102870 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,745 : INFO : built Dictionary(103057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 408 documents (total 4080000 corpus positions)
    2021-05-05 22:35:56,790 : INFO : adding document #0 to Dictionary(103057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,798 : INFO : built Dictionary(103200 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 409 documents (total 4090000 corpus positions)
    2021-05-05 22:35:56,843 : INFO : adding document #0 to Dictionary(103200 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,852 : INFO : built Dictionary(103354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 410 documents (total 4100000 corpus positions)
    2021-05-05 22:35:56,895 : INFO : adding document #0 to Dictionary(103354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,902 : INFO : built Dictionary(103486 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 411 documents (total 4110000 corpus positions)
    2021-05-05 22:35:56,946 : INFO : adding document #0 to Dictionary(103486 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:56,953 : INFO : built Dictionary(103592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 412 documents (total 4120000 corpus positions)
    2021-05-05 22:35:57,001 : INFO : adding document #0 to Dictionary(103592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,012 : INFO : built Dictionary(103681 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 413 documents (total 4130000 corpus positions)
    2021-05-05 22:35:57,058 : INFO : adding document #0 to Dictionary(103681 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,066 : INFO : built Dictionary(103785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 414 documents (total 4140000 corpus positions)
    2021-05-05 22:35:57,112 : INFO : adding document #0 to Dictionary(103785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,121 : INFO : built Dictionary(103982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 415 documents (total 4150000 corpus positions)
    2021-05-05 22:35:57,166 : INFO : adding document #0 to Dictionary(103982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,174 : INFO : built Dictionary(104123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 416 documents (total 4160000 corpus positions)
    2021-05-05 22:35:57,222 : INFO : adding document #0 to Dictionary(104123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,230 : INFO : built Dictionary(104199 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 417 documents (total 4170000 corpus positions)
    2021-05-05 22:35:57,273 : INFO : adding document #0 to Dictionary(104199 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,280 : INFO : built Dictionary(104405 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 418 documents (total 4180000 corpus positions)
    2021-05-05 22:35:57,323 : INFO : adding document #0 to Dictionary(104405 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,331 : INFO : built Dictionary(104538 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 419 documents (total 4190000 corpus positions)
    2021-05-05 22:35:57,376 : INFO : adding document #0 to Dictionary(104538 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,384 : INFO : built Dictionary(104731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 420 documents (total 4200000 corpus positions)
    2021-05-05 22:35:57,427 : INFO : adding document #0 to Dictionary(104731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,435 : INFO : built Dictionary(104809 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 421 documents (total 4210000 corpus positions)
    2021-05-05 22:35:57,479 : INFO : adding document #0 to Dictionary(104809 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,487 : INFO : built Dictionary(104946 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 422 documents (total 4220000 corpus positions)
    2021-05-05 22:35:57,531 : INFO : adding document #0 to Dictionary(104946 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,539 : INFO : built Dictionary(105011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 423 documents (total 4230000 corpus positions)
    2021-05-05 22:35:57,582 : INFO : adding document #0 to Dictionary(105011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,590 : INFO : built Dictionary(105224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 424 documents (total 4240000 corpus positions)
    2021-05-05 22:35:57,633 : INFO : adding document #0 to Dictionary(105224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,641 : INFO : built Dictionary(105377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 425 documents (total 4250000 corpus positions)
    2021-05-05 22:35:57,684 : INFO : adding document #0 to Dictionary(105377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,692 : INFO : built Dictionary(105511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 426 documents (total 4260000 corpus positions)
    2021-05-05 22:35:57,748 : INFO : adding document #0 to Dictionary(105511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,758 : INFO : built Dictionary(105677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 427 documents (total 4270000 corpus positions)
    2021-05-05 22:35:57,815 : INFO : adding document #0 to Dictionary(105677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,827 : INFO : built Dictionary(105814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 428 documents (total 4280000 corpus positions)
    2021-05-05 22:35:57,876 : INFO : adding document #0 to Dictionary(105814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,886 : INFO : built Dictionary(105952 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 429 documents (total 4290000 corpus positions)
    2021-05-05 22:35:57,938 : INFO : adding document #0 to Dictionary(105952 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:57,951 : INFO : built Dictionary(106160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 430 documents (total 4300000 corpus positions)
    2021-05-05 22:35:58,002 : INFO : adding document #0 to Dictionary(106160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,013 : INFO : built Dictionary(106311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 431 documents (total 4310000 corpus positions)
    2021-05-05 22:35:58,061 : INFO : adding document #0 to Dictionary(106311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,068 : INFO : built Dictionary(106382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 432 documents (total 4320000 corpus positions)
    2021-05-05 22:35:58,113 : INFO : adding document #0 to Dictionary(106382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,122 : INFO : built Dictionary(106493 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 433 documents (total 4330000 corpus positions)
    2021-05-05 22:35:58,166 : INFO : adding document #0 to Dictionary(106493 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,174 : INFO : built Dictionary(106597 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 434 documents (total 4340000 corpus positions)
    2021-05-05 22:35:58,225 : INFO : adding document #0 to Dictionary(106597 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,235 : INFO : built Dictionary(106714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 435 documents (total 4350000 corpus positions)
    2021-05-05 22:35:58,289 : INFO : adding document #0 to Dictionary(106714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,300 : INFO : built Dictionary(106906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 436 documents (total 4360000 corpus positions)
    2021-05-05 22:35:58,350 : INFO : adding document #0 to Dictionary(106906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,361 : INFO : built Dictionary(107095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 437 documents (total 4370000 corpus positions)
    2021-05-05 22:35:58,415 : INFO : adding document #0 to Dictionary(107095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,426 : INFO : built Dictionary(107144 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 438 documents (total 4380000 corpus positions)
    2021-05-05 22:35:58,475 : INFO : adding document #0 to Dictionary(107144 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,482 : INFO : built Dictionary(107262 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 439 documents (total 4390000 corpus positions)
    2021-05-05 22:35:58,528 : INFO : adding document #0 to Dictionary(107262 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,537 : INFO : built Dictionary(107413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 440 documents (total 4400000 corpus positions)
    2021-05-05 22:35:58,588 : INFO : adding document #0 to Dictionary(107413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,596 : INFO : built Dictionary(107543 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 441 documents (total 4410000 corpus positions)
    2021-05-05 22:35:58,646 : INFO : adding document #0 to Dictionary(107543 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,655 : INFO : built Dictionary(107657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 442 documents (total 4420000 corpus positions)
    2021-05-05 22:35:58,708 : INFO : adding document #0 to Dictionary(107657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,720 : INFO : built Dictionary(107778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 443 documents (total 4430000 corpus positions)
    2021-05-05 22:35:58,765 : INFO : adding document #0 to Dictionary(107778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,774 : INFO : built Dictionary(107918 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 444 documents (total 4440000 corpus positions)
    2021-05-05 22:35:58,816 : INFO : adding document #0 to Dictionary(107918 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,825 : INFO : built Dictionary(108205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 445 documents (total 4450000 corpus positions)
    2021-05-05 22:35:58,873 : INFO : adding document #0 to Dictionary(108205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,881 : INFO : built Dictionary(108398 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 446 documents (total 4460000 corpus positions)
    2021-05-05 22:35:58,925 : INFO : adding document #0 to Dictionary(108398 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,933 : INFO : built Dictionary(108505 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 447 documents (total 4470000 corpus positions)
    2021-05-05 22:35:58,984 : INFO : adding document #0 to Dictionary(108505 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:58,995 : INFO : built Dictionary(108741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 448 documents (total 4480000 corpus positions)
    2021-05-05 22:35:59,045 : INFO : adding document #0 to Dictionary(108741 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,056 : INFO : built Dictionary(108858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 449 documents (total 4490000 corpus positions)
    2021-05-05 22:35:59,101 : INFO : adding document #0 to Dictionary(108858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,114 : INFO : built Dictionary(108989 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 450 documents (total 4500000 corpus positions)
    2021-05-05 22:35:59,166 : INFO : adding document #0 to Dictionary(108989 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,174 : INFO : built Dictionary(109089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 451 documents (total 4510000 corpus positions)
    2021-05-05 22:35:59,227 : INFO : adding document #0 to Dictionary(109089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,236 : INFO : built Dictionary(109217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 452 documents (total 4520000 corpus positions)
    2021-05-05 22:35:59,279 : INFO : adding document #0 to Dictionary(109217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,287 : INFO : built Dictionary(109307 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 453 documents (total 4530000 corpus positions)
    2021-05-05 22:35:59,331 : INFO : adding document #0 to Dictionary(109307 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,340 : INFO : built Dictionary(109441 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 454 documents (total 4540000 corpus positions)
    2021-05-05 22:35:59,389 : INFO : adding document #0 to Dictionary(109441 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,397 : INFO : built Dictionary(109568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 455 documents (total 4550000 corpus positions)
    2021-05-05 22:35:59,441 : INFO : adding document #0 to Dictionary(109568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,451 : INFO : built Dictionary(109776 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 456 documents (total 4560000 corpus positions)
    2021-05-05 22:35:59,495 : INFO : adding document #0 to Dictionary(109776 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,502 : INFO : built Dictionary(109832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 457 documents (total 4570000 corpus positions)
    2021-05-05 22:35:59,545 : INFO : adding document #0 to Dictionary(109832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,552 : INFO : built Dictionary(109871 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 458 documents (total 4580000 corpus positions)
    2021-05-05 22:35:59,595 : INFO : adding document #0 to Dictionary(109871 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,602 : INFO : built Dictionary(109918 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 459 documents (total 4590000 corpus positions)
    2021-05-05 22:35:59,645 : INFO : adding document #0 to Dictionary(109918 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,652 : INFO : built Dictionary(110004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 460 documents (total 4600000 corpus positions)
    2021-05-05 22:35:59,700 : INFO : adding document #0 to Dictionary(110004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,709 : INFO : built Dictionary(110123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 461 documents (total 4610000 corpus positions)
    2021-05-05 22:35:59,756 : INFO : adding document #0 to Dictionary(110123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,766 : INFO : built Dictionary(110295 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 462 documents (total 4620000 corpus positions)
    2021-05-05 22:35:59,812 : INFO : adding document #0 to Dictionary(110295 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,820 : INFO : built Dictionary(110361 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 463 documents (total 4630000 corpus positions)
    2021-05-05 22:35:59,864 : INFO : adding document #0 to Dictionary(110361 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,874 : INFO : built Dictionary(110504 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 464 documents (total 4640000 corpus positions)
    2021-05-05 22:35:59,924 : INFO : adding document #0 to Dictionary(110504 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,933 : INFO : built Dictionary(110635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 465 documents (total 4650000 corpus positions)
    2021-05-05 22:35:59,976 : INFO : adding document #0 to Dictionary(110635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:35:59,983 : INFO : built Dictionary(110731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 466 documents (total 4660000 corpus positions)
    2021-05-05 22:36:00,032 : INFO : adding document #0 to Dictionary(110731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,042 : INFO : built Dictionary(110873 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 467 documents (total 4670000 corpus positions)
    2021-05-05 22:36:00,085 : INFO : adding document #0 to Dictionary(110873 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,092 : INFO : built Dictionary(110953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 468 documents (total 4680000 corpus positions)
    2021-05-05 22:36:00,135 : INFO : adding document #0 to Dictionary(110953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,143 : INFO : built Dictionary(111028 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 469 documents (total 4690000 corpus positions)
    2021-05-05 22:36:00,186 : INFO : adding document #0 to Dictionary(111028 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,193 : INFO : built Dictionary(111109 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 470 documents (total 4700000 corpus positions)
    2021-05-05 22:36:00,237 : INFO : adding document #0 to Dictionary(111109 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,247 : INFO : built Dictionary(111350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 471 documents (total 4710000 corpus positions)
    2021-05-05 22:36:00,294 : INFO : adding document #0 to Dictionary(111350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,301 : INFO : built Dictionary(111495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 472 documents (total 4720000 corpus positions)
    2021-05-05 22:36:00,356 : INFO : adding document #0 to Dictionary(111495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,368 : INFO : built Dictionary(111592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 473 documents (total 4730000 corpus positions)
    2021-05-05 22:36:00,439 : INFO : adding document #0 to Dictionary(111592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,448 : INFO : built Dictionary(111692 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 474 documents (total 4740000 corpus positions)
    2021-05-05 22:36:00,497 : INFO : adding document #0 to Dictionary(111692 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,507 : INFO : built Dictionary(111827 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 475 documents (total 4750000 corpus positions)
    2021-05-05 22:36:00,571 : INFO : adding document #0 to Dictionary(111827 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,584 : INFO : built Dictionary(111997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 476 documents (total 4760000 corpus positions)
    2021-05-05 22:36:00,634 : INFO : adding document #0 to Dictionary(111997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,643 : INFO : built Dictionary(112111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 477 documents (total 4770000 corpus positions)
    2021-05-05 22:36:00,696 : INFO : adding document #0 to Dictionary(112111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,705 : INFO : built Dictionary(112224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 478 documents (total 4780000 corpus positions)
    2021-05-05 22:36:00,760 : INFO : adding document #0 to Dictionary(112224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,771 : INFO : built Dictionary(112314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 479 documents (total 4790000 corpus positions)
    2021-05-05 22:36:00,838 : INFO : adding document #0 to Dictionary(112314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,851 : INFO : built Dictionary(112426 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 480 documents (total 4800000 corpus positions)
    2021-05-05 22:36:00,915 : INFO : adding document #0 to Dictionary(112426 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,923 : INFO : built Dictionary(112484 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 481 documents (total 4810000 corpus positions)
    2021-05-05 22:36:00,979 : INFO : adding document #0 to Dictionary(112484 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:00,988 : INFO : built Dictionary(112657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 482 documents (total 4820000 corpus positions)
    2021-05-05 22:36:01,040 : INFO : adding document #0 to Dictionary(112657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,053 : INFO : built Dictionary(112757 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 483 documents (total 4830000 corpus positions)
    2021-05-05 22:36:01,108 : INFO : adding document #0 to Dictionary(112757 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,118 : INFO : built Dictionary(112842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 484 documents (total 4840000 corpus positions)
    2021-05-05 22:36:01,170 : INFO : adding document #0 to Dictionary(112842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,180 : INFO : built Dictionary(112945 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 485 documents (total 4850000 corpus positions)
    2021-05-05 22:36:01,229 : INFO : adding document #0 to Dictionary(112945 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,239 : INFO : built Dictionary(113003 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 486 documents (total 4860000 corpus positions)
    2021-05-05 22:36:01,289 : INFO : adding document #0 to Dictionary(113003 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,300 : INFO : built Dictionary(113222 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 487 documents (total 4870000 corpus positions)
    2021-05-05 22:36:01,350 : INFO : adding document #0 to Dictionary(113222 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,362 : INFO : built Dictionary(113410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 488 documents (total 4880000 corpus positions)
    2021-05-05 22:36:01,416 : INFO : adding document #0 to Dictionary(113410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,426 : INFO : built Dictionary(113535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 489 documents (total 4890000 corpus positions)
    2021-05-05 22:36:01,482 : INFO : adding document #0 to Dictionary(113535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,493 : INFO : built Dictionary(113696 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 490 documents (total 4900000 corpus positions)
    2021-05-05 22:36:01,544 : INFO : adding document #0 to Dictionary(113696 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,552 : INFO : built Dictionary(113848 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 491 documents (total 4910000 corpus positions)
    2021-05-05 22:36:01,601 : INFO : adding document #0 to Dictionary(113848 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,611 : INFO : built Dictionary(113965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 492 documents (total 4920000 corpus positions)
    2021-05-05 22:36:01,668 : INFO : adding document #0 to Dictionary(113965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,679 : INFO : built Dictionary(114110 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 493 documents (total 4930000 corpus positions)
    2021-05-05 22:36:01,731 : INFO : adding document #0 to Dictionary(114110 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,740 : INFO : built Dictionary(114287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 494 documents (total 4940000 corpus positions)
    2021-05-05 22:36:01,788 : INFO : adding document #0 to Dictionary(114287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,797 : INFO : built Dictionary(114471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 495 documents (total 4950000 corpus positions)
    2021-05-05 22:36:01,844 : INFO : adding document #0 to Dictionary(114471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,853 : INFO : built Dictionary(114584 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 496 documents (total 4960000 corpus positions)
    2021-05-05 22:36:01,901 : INFO : adding document #0 to Dictionary(114584 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,909 : INFO : built Dictionary(114665 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 497 documents (total 4970000 corpus positions)
    2021-05-05 22:36:01,956 : INFO : adding document #0 to Dictionary(114665 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:01,966 : INFO : built Dictionary(114783 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 498 documents (total 4980000 corpus positions)
    2021-05-05 22:36:02,014 : INFO : adding document #0 to Dictionary(114783 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,025 : INFO : built Dictionary(115035 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 499 documents (total 4990000 corpus positions)
    2021-05-05 22:36:02,072 : INFO : adding document #0 to Dictionary(115035 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,081 : INFO : built Dictionary(115143 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 500 documents (total 5000000 corpus positions)
    2021-05-05 22:36:02,128 : INFO : adding document #0 to Dictionary(115143 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,137 : INFO : built Dictionary(115226 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 501 documents (total 5010000 corpus positions)
    2021-05-05 22:36:02,186 : INFO : adding document #0 to Dictionary(115226 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,197 : INFO : built Dictionary(115592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 502 documents (total 5020000 corpus positions)
    2021-05-05 22:36:02,245 : INFO : adding document #0 to Dictionary(115592 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,255 : INFO : built Dictionary(116153 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 503 documents (total 5030000 corpus positions)
    2021-05-05 22:36:02,301 : INFO : adding document #0 to Dictionary(116153 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,311 : INFO : built Dictionary(116297 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 504 documents (total 5040000 corpus positions)
    2021-05-05 22:36:02,360 : INFO : adding document #0 to Dictionary(116297 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,369 : INFO : built Dictionary(116466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 505 documents (total 5050000 corpus positions)
    2021-05-05 22:36:02,417 : INFO : adding document #0 to Dictionary(116466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,429 : INFO : built Dictionary(116658 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 506 documents (total 5060000 corpus positions)
    2021-05-05 22:36:02,477 : INFO : adding document #0 to Dictionary(116658 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,487 : INFO : built Dictionary(116770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 507 documents (total 5070000 corpus positions)
    2021-05-05 22:36:02,536 : INFO : adding document #0 to Dictionary(116770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,545 : INFO : built Dictionary(116879 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 508 documents (total 5080000 corpus positions)
    2021-05-05 22:36:02,593 : INFO : adding document #0 to Dictionary(116879 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,601 : INFO : built Dictionary(116952 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 509 documents (total 5090000 corpus positions)
    2021-05-05 22:36:02,652 : INFO : adding document #0 to Dictionary(116952 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,661 : INFO : built Dictionary(117066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 510 documents (total 5100000 corpus positions)
    2021-05-05 22:36:02,712 : INFO : adding document #0 to Dictionary(117066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,723 : INFO : built Dictionary(117251 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 511 documents (total 5110000 corpus positions)
    2021-05-05 22:36:02,772 : INFO : adding document #0 to Dictionary(117251 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,782 : INFO : built Dictionary(117365 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 512 documents (total 5120000 corpus positions)
    2021-05-05 22:36:02,830 : INFO : adding document #0 to Dictionary(117365 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,836 : INFO : built Dictionary(117443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 513 documents (total 5130000 corpus positions)
    2021-05-05 22:36:02,886 : INFO : adding document #0 to Dictionary(117443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,896 : INFO : built Dictionary(117581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 514 documents (total 5140000 corpus positions)
    2021-05-05 22:36:02,956 : INFO : adding document #0 to Dictionary(117581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:02,966 : INFO : built Dictionary(117780 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 515 documents (total 5150000 corpus positions)
    2021-05-05 22:36:03,020 : INFO : adding document #0 to Dictionary(117780 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,029 : INFO : built Dictionary(117905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 516 documents (total 5160000 corpus positions)
    2021-05-05 22:36:03,077 : INFO : adding document #0 to Dictionary(117905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,086 : INFO : built Dictionary(118005 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 517 documents (total 5170000 corpus positions)
    2021-05-05 22:36:03,150 : INFO : adding document #0 to Dictionary(118005 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,159 : INFO : built Dictionary(118125 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 518 documents (total 5180000 corpus positions)
    2021-05-05 22:36:03,208 : INFO : adding document #0 to Dictionary(118125 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,218 : INFO : built Dictionary(118231 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 519 documents (total 5190000 corpus positions)
    2021-05-05 22:36:03,265 : INFO : adding document #0 to Dictionary(118231 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,272 : INFO : built Dictionary(118324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 520 documents (total 5200000 corpus positions)
    2021-05-05 22:36:03,322 : INFO : adding document #0 to Dictionary(118324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,332 : INFO : built Dictionary(118436 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 521 documents (total 5210000 corpus positions)
    2021-05-05 22:36:03,380 : INFO : adding document #0 to Dictionary(118436 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,388 : INFO : built Dictionary(118537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 522 documents (total 5220000 corpus positions)
    2021-05-05 22:36:03,436 : INFO : adding document #0 to Dictionary(118537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,446 : INFO : built Dictionary(118708 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 523 documents (total 5230000 corpus positions)
    2021-05-05 22:36:03,496 : INFO : adding document #0 to Dictionary(118708 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,507 : INFO : built Dictionary(119299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 524 documents (total 5240000 corpus positions)
    2021-05-05 22:36:03,555 : INFO : adding document #0 to Dictionary(119299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,564 : INFO : built Dictionary(119447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 525 documents (total 5250000 corpus positions)
    2021-05-05 22:36:03,611 : INFO : adding document #0 to Dictionary(119447 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,619 : INFO : built Dictionary(119529 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 526 documents (total 5260000 corpus positions)
    2021-05-05 22:36:03,667 : INFO : adding document #0 to Dictionary(119529 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,676 : INFO : built Dictionary(119648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 527 documents (total 5270000 corpus positions)
    2021-05-05 22:36:03,722 : INFO : adding document #0 to Dictionary(119648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,732 : INFO : built Dictionary(119776 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 528 documents (total 5280000 corpus positions)
    2021-05-05 22:36:03,781 : INFO : adding document #0 to Dictionary(119776 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,789 : INFO : built Dictionary(119862 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 529 documents (total 5290000 corpus positions)
    2021-05-05 22:36:03,835 : INFO : adding document #0 to Dictionary(119862 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,844 : INFO : built Dictionary(120025 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 530 documents (total 5300000 corpus positions)
    2021-05-05 22:36:03,889 : INFO : adding document #0 to Dictionary(120025 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,898 : INFO : built Dictionary(120113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 531 documents (total 5310000 corpus positions)
    2021-05-05 22:36:03,942 : INFO : adding document #0 to Dictionary(120113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:03,950 : INFO : built Dictionary(120216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 532 documents (total 5320000 corpus positions)
    2021-05-05 22:36:03,994 : INFO : adding document #0 to Dictionary(120216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,002 : INFO : built Dictionary(120304 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 533 documents (total 5330000 corpus positions)
    2021-05-05 22:36:04,052 : INFO : adding document #0 to Dictionary(120304 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,061 : INFO : built Dictionary(120424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 534 documents (total 5340000 corpus positions)
    2021-05-05 22:36:04,107 : INFO : adding document #0 to Dictionary(120424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,117 : INFO : built Dictionary(120650 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 535 documents (total 5350000 corpus positions)
    2021-05-05 22:36:04,163 : INFO : adding document #0 to Dictionary(120650 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,173 : INFO : built Dictionary(120785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 536 documents (total 5360000 corpus positions)
    2021-05-05 22:36:04,216 : INFO : adding document #0 to Dictionary(120785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,225 : INFO : built Dictionary(120861 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 537 documents (total 5370000 corpus positions)
    2021-05-05 22:36:04,270 : INFO : adding document #0 to Dictionary(120861 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,279 : INFO : built Dictionary(120953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 538 documents (total 5380000 corpus positions)
    2021-05-05 22:36:04,322 : INFO : adding document #0 to Dictionary(120953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,332 : INFO : built Dictionary(121154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 539 documents (total 5390000 corpus positions)
    2021-05-05 22:36:04,376 : INFO : adding document #0 to Dictionary(121154 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,384 : INFO : built Dictionary(121251 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 540 documents (total 5400000 corpus positions)
    2021-05-05 22:36:04,427 : INFO : adding document #0 to Dictionary(121251 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,435 : INFO : built Dictionary(121323 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 541 documents (total 5410000 corpus positions)
    2021-05-05 22:36:04,478 : INFO : adding document #0 to Dictionary(121323 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,488 : INFO : built Dictionary(121515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 542 documents (total 5420000 corpus positions)
    2021-05-05 22:36:04,531 : INFO : adding document #0 to Dictionary(121515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,541 : INFO : built Dictionary(121631 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 543 documents (total 5430000 corpus positions)
    2021-05-05 22:36:04,584 : INFO : adding document #0 to Dictionary(121631 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,592 : INFO : built Dictionary(121753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 544 documents (total 5440000 corpus positions)
    2021-05-05 22:36:04,635 : INFO : adding document #0 to Dictionary(121753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,643 : INFO : built Dictionary(121850 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 545 documents (total 5450000 corpus positions)
    2021-05-05 22:36:04,693 : INFO : adding document #0 to Dictionary(121850 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,701 : INFO : built Dictionary(121942 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 546 documents (total 5460000 corpus positions)
    2021-05-05 22:36:04,754 : INFO : adding document #0 to Dictionary(121942 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,764 : INFO : built Dictionary(122030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 547 documents (total 5470000 corpus positions)
    2021-05-05 22:36:04,816 : INFO : adding document #0 to Dictionary(122030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,829 : INFO : built Dictionary(122281 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 548 documents (total 5480000 corpus positions)
    2021-05-05 22:36:04,878 : INFO : adding document #0 to Dictionary(122281 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,887 : INFO : built Dictionary(122423 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 549 documents (total 5490000 corpus positions)
    2021-05-05 22:36:04,935 : INFO : adding document #0 to Dictionary(122423 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:04,945 : INFO : built Dictionary(122540 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 550 documents (total 5500000 corpus positions)
    2021-05-05 22:36:04,993 : INFO : adding document #0 to Dictionary(122540 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,003 : INFO : built Dictionary(122676 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 551 documents (total 5510000 corpus positions)
    2021-05-05 22:36:05,054 : INFO : adding document #0 to Dictionary(122676 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,063 : INFO : built Dictionary(122750 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 552 documents (total 5520000 corpus positions)
    2021-05-05 22:36:05,110 : INFO : adding document #0 to Dictionary(122750 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,119 : INFO : built Dictionary(122853 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 553 documents (total 5530000 corpus positions)
    2021-05-05 22:36:05,164 : INFO : adding document #0 to Dictionary(122853 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,173 : INFO : built Dictionary(122992 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 554 documents (total 5540000 corpus positions)
    2021-05-05 22:36:05,217 : INFO : adding document #0 to Dictionary(122992 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,230 : INFO : built Dictionary(123074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 555 documents (total 5550000 corpus positions)
    2021-05-05 22:36:05,281 : INFO : adding document #0 to Dictionary(123074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,290 : INFO : built Dictionary(123212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 556 documents (total 5560000 corpus positions)
    2021-05-05 22:36:05,335 : INFO : adding document #0 to Dictionary(123212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,344 : INFO : built Dictionary(123363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 557 documents (total 5570000 corpus positions)
    2021-05-05 22:36:05,392 : INFO : adding document #0 to Dictionary(123363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,400 : INFO : built Dictionary(123482 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 558 documents (total 5580000 corpus positions)
    2021-05-05 22:36:05,447 : INFO : adding document #0 to Dictionary(123482 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,456 : INFO : built Dictionary(123643 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 559 documents (total 5590000 corpus positions)
    2021-05-05 22:36:05,499 : INFO : adding document #0 to Dictionary(123643 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,509 : INFO : built Dictionary(123711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 560 documents (total 5600000 corpus positions)
    2021-05-05 22:36:05,555 : INFO : adding document #0 to Dictionary(123711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,564 : INFO : built Dictionary(123815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 561 documents (total 5610000 corpus positions)
    2021-05-05 22:36:05,609 : INFO : adding document #0 to Dictionary(123815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,619 : INFO : built Dictionary(123910 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 562 documents (total 5620000 corpus positions)
    2021-05-05 22:36:05,664 : INFO : adding document #0 to Dictionary(123910 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,673 : INFO : built Dictionary(124116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 563 documents (total 5630000 corpus positions)
    2021-05-05 22:36:05,717 : INFO : adding document #0 to Dictionary(124116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,725 : INFO : built Dictionary(124250 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 564 documents (total 5640000 corpus positions)
    2021-05-05 22:36:05,770 : INFO : adding document #0 to Dictionary(124250 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,780 : INFO : built Dictionary(124396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 565 documents (total 5650000 corpus positions)
    2021-05-05 22:36:05,825 : INFO : adding document #0 to Dictionary(124396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,833 : INFO : built Dictionary(124522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 566 documents (total 5660000 corpus positions)
    2021-05-05 22:36:05,876 : INFO : adding document #0 to Dictionary(124522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,885 : INFO : built Dictionary(124838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 567 documents (total 5670000 corpus positions)
    2021-05-05 22:36:05,928 : INFO : adding document #0 to Dictionary(124838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,938 : INFO : built Dictionary(124977 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 568 documents (total 5680000 corpus positions)
    2021-05-05 22:36:05,982 : INFO : adding document #0 to Dictionary(124977 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:05,990 : INFO : built Dictionary(125082 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 569 documents (total 5690000 corpus positions)
    2021-05-05 22:36:06,035 : INFO : adding document #0 to Dictionary(125082 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,044 : INFO : built Dictionary(125157 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 570 documents (total 5700000 corpus positions)
    2021-05-05 22:36:06,087 : INFO : adding document #0 to Dictionary(125157 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,094 : INFO : built Dictionary(125219 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 571 documents (total 5710000 corpus positions)
    2021-05-05 22:36:06,138 : INFO : adding document #0 to Dictionary(125219 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,147 : INFO : built Dictionary(125539 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 572 documents (total 5720000 corpus positions)
    2021-05-05 22:36:06,192 : INFO : adding document #0 to Dictionary(125539 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,200 : INFO : built Dictionary(125661 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 573 documents (total 5730000 corpus positions)
    2021-05-05 22:36:06,247 : INFO : adding document #0 to Dictionary(125661 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,260 : INFO : built Dictionary(125840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 574 documents (total 5740000 corpus positions)
    2021-05-05 22:36:06,312 : INFO : adding document #0 to Dictionary(125840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,321 : INFO : built Dictionary(125965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 575 documents (total 5750000 corpus positions)
    2021-05-05 22:36:06,371 : INFO : adding document #0 to Dictionary(125965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,381 : INFO : built Dictionary(126068 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 576 documents (total 5760000 corpus positions)
    2021-05-05 22:36:06,435 : INFO : adding document #0 to Dictionary(126068 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,444 : INFO : built Dictionary(126171 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 577 documents (total 5770000 corpus positions)
    2021-05-05 22:36:06,493 : INFO : adding document #0 to Dictionary(126171 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,502 : INFO : built Dictionary(126271 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 578 documents (total 5780000 corpus positions)
    2021-05-05 22:36:06,552 : INFO : adding document #0 to Dictionary(126271 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,562 : INFO : built Dictionary(126344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 579 documents (total 5790000 corpus positions)
    2021-05-05 22:36:06,611 : INFO : adding document #0 to Dictionary(126344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,621 : INFO : built Dictionary(126436 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 580 documents (total 5800000 corpus positions)
    2021-05-05 22:36:06,665 : INFO : adding document #0 to Dictionary(126436 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,672 : INFO : built Dictionary(126538 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 581 documents (total 5810000 corpus positions)
    2021-05-05 22:36:06,719 : INFO : adding document #0 to Dictionary(126538 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,728 : INFO : built Dictionary(126600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 582 documents (total 5820000 corpus positions)
    2021-05-05 22:36:06,773 : INFO : adding document #0 to Dictionary(126600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,783 : INFO : built Dictionary(126716 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 583 documents (total 5830000 corpus positions)
    2021-05-05 22:36:06,834 : INFO : adding document #0 to Dictionary(126716 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,843 : INFO : built Dictionary(126805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 584 documents (total 5840000 corpus positions)
    2021-05-05 22:36:06,886 : INFO : adding document #0 to Dictionary(126805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,895 : INFO : built Dictionary(126906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 585 documents (total 5850000 corpus positions)
    2021-05-05 22:36:06,945 : INFO : adding document #0 to Dictionary(126906 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:06,954 : INFO : built Dictionary(126982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 586 documents (total 5860000 corpus positions)
    2021-05-05 22:36:06,998 : INFO : adding document #0 to Dictionary(126982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,008 : INFO : built Dictionary(127080 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 587 documents (total 5870000 corpus positions)
    2021-05-05 22:36:07,055 : INFO : adding document #0 to Dictionary(127080 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,063 : INFO : built Dictionary(127188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 588 documents (total 5880000 corpus positions)
    2021-05-05 22:36:07,108 : INFO : adding document #0 to Dictionary(127188 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,119 : INFO : built Dictionary(127299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 589 documents (total 5890000 corpus positions)
    2021-05-05 22:36:07,183 : INFO : adding document #0 to Dictionary(127299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,190 : INFO : built Dictionary(127382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 590 documents (total 5900000 corpus positions)
    2021-05-05 22:36:07,241 : INFO : adding document #0 to Dictionary(127382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,250 : INFO : built Dictionary(127575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 591 documents (total 5910000 corpus positions)
    2021-05-05 22:36:07,296 : INFO : adding document #0 to Dictionary(127575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,306 : INFO : built Dictionary(127702 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 592 documents (total 5920000 corpus positions)
    2021-05-05 22:36:07,352 : INFO : adding document #0 to Dictionary(127702 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,360 : INFO : built Dictionary(127770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 593 documents (total 5930000 corpus positions)
    2021-05-05 22:36:07,404 : INFO : adding document #0 to Dictionary(127770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,413 : INFO : built Dictionary(127866 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 594 documents (total 5940000 corpus positions)
    2021-05-05 22:36:07,457 : INFO : adding document #0 to Dictionary(127866 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,466 : INFO : built Dictionary(128040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 595 documents (total 5950000 corpus positions)
    2021-05-05 22:36:07,510 : INFO : adding document #0 to Dictionary(128040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,518 : INFO : built Dictionary(128141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 596 documents (total 5960000 corpus positions)
    2021-05-05 22:36:07,561 : INFO : adding document #0 to Dictionary(128141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,567 : INFO : built Dictionary(128287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 597 documents (total 5970000 corpus positions)
    2021-05-05 22:36:07,610 : INFO : adding document #0 to Dictionary(128287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,618 : INFO : built Dictionary(128386 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 598 documents (total 5980000 corpus positions)
    2021-05-05 22:36:07,661 : INFO : adding document #0 to Dictionary(128386 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,668 : INFO : built Dictionary(128481 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 599 documents (total 5990000 corpus positions)
    2021-05-05 22:36:07,712 : INFO : adding document #0 to Dictionary(128481 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,720 : INFO : built Dictionary(128573 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 600 documents (total 6000000 corpus positions)
    2021-05-05 22:36:07,763 : INFO : adding document #0 to Dictionary(128573 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,772 : INFO : built Dictionary(128699 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 601 documents (total 6010000 corpus positions)
    2021-05-05 22:36:07,815 : INFO : adding document #0 to Dictionary(128699 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,824 : INFO : built Dictionary(128817 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 602 documents (total 6020000 corpus positions)
    2021-05-05 22:36:07,869 : INFO : adding document #0 to Dictionary(128817 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,877 : INFO : built Dictionary(128905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 603 documents (total 6030000 corpus positions)
    2021-05-05 22:36:07,922 : INFO : adding document #0 to Dictionary(128905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,931 : INFO : built Dictionary(129057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 604 documents (total 6040000 corpus positions)
    2021-05-05 22:36:07,974 : INFO : adding document #0 to Dictionary(129057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:07,983 : INFO : built Dictionary(129246 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 605 documents (total 6050000 corpus positions)
    2021-05-05 22:36:08,028 : INFO : adding document #0 to Dictionary(129246 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,036 : INFO : built Dictionary(129370 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 606 documents (total 6060000 corpus positions)
    2021-05-05 22:36:08,080 : INFO : adding document #0 to Dictionary(129370 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,088 : INFO : built Dictionary(129483 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 607 documents (total 6070000 corpus positions)
    2021-05-05 22:36:08,131 : INFO : adding document #0 to Dictionary(129483 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,139 : INFO : built Dictionary(129548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 608 documents (total 6080000 corpus positions)
    2021-05-05 22:36:08,181 : INFO : adding document #0 to Dictionary(129548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,190 : INFO : built Dictionary(129698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 609 documents (total 6090000 corpus positions)
    2021-05-05 22:36:08,233 : INFO : adding document #0 to Dictionary(129698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,242 : INFO : built Dictionary(129807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 610 documents (total 6100000 corpus positions)
    2021-05-05 22:36:08,285 : INFO : adding document #0 to Dictionary(129807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,293 : INFO : built Dictionary(129869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 611 documents (total 6110000 corpus positions)
    2021-05-05 22:36:08,337 : INFO : adding document #0 to Dictionary(129869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,345 : INFO : built Dictionary(130017 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 612 documents (total 6120000 corpus positions)
    2021-05-05 22:36:08,390 : INFO : adding document #0 to Dictionary(130017 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,399 : INFO : built Dictionary(130108 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 613 documents (total 6130000 corpus positions)
    2021-05-05 22:36:08,456 : INFO : adding document #0 to Dictionary(130108 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,466 : INFO : built Dictionary(130177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 614 documents (total 6140000 corpus positions)
    2021-05-05 22:36:08,517 : INFO : adding document #0 to Dictionary(130177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,525 : INFO : built Dictionary(130236 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 615 documents (total 6150000 corpus positions)
    2021-05-05 22:36:08,569 : INFO : adding document #0 to Dictionary(130236 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,579 : INFO : built Dictionary(130386 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 616 documents (total 6160000 corpus positions)
    2021-05-05 22:36:08,626 : INFO : adding document #0 to Dictionary(130386 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,633 : INFO : built Dictionary(130448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 617 documents (total 6170000 corpus positions)
    2021-05-05 22:36:08,677 : INFO : adding document #0 to Dictionary(130448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,685 : INFO : built Dictionary(130558 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 618 documents (total 6180000 corpus positions)
    2021-05-05 22:36:08,728 : INFO : adding document #0 to Dictionary(130558 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,737 : INFO : built Dictionary(130659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 619 documents (total 6190000 corpus positions)
    2021-05-05 22:36:08,781 : INFO : adding document #0 to Dictionary(130659 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,789 : INFO : built Dictionary(130755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 620 documents (total 6200000 corpus positions)
    2021-05-05 22:36:08,833 : INFO : adding document #0 to Dictionary(130755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,840 : INFO : built Dictionary(130844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 621 documents (total 6210000 corpus positions)
    2021-05-05 22:36:08,884 : INFO : adding document #0 to Dictionary(130844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,891 : INFO : built Dictionary(130935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 622 documents (total 6220000 corpus positions)
    2021-05-05 22:36:08,935 : INFO : adding document #0 to Dictionary(130935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,942 : INFO : built Dictionary(131018 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 623 documents (total 6230000 corpus positions)
    2021-05-05 22:36:08,985 : INFO : adding document #0 to Dictionary(131018 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:08,993 : INFO : built Dictionary(131095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 624 documents (total 6240000 corpus positions)
    2021-05-05 22:36:09,041 : INFO : adding document #0 to Dictionary(131095 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,050 : INFO : built Dictionary(131189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 625 documents (total 6250000 corpus positions)
    2021-05-05 22:36:09,092 : INFO : adding document #0 to Dictionary(131189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,100 : INFO : built Dictionary(131293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 626 documents (total 6260000 corpus positions)
    2021-05-05 22:36:09,144 : INFO : adding document #0 to Dictionary(131293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,153 : INFO : built Dictionary(131390 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 627 documents (total 6270000 corpus positions)
    2021-05-05 22:36:09,196 : INFO : adding document #0 to Dictionary(131390 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,203 : INFO : built Dictionary(131459 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 628 documents (total 6280000 corpus positions)
    2021-05-05 22:36:09,253 : INFO : adding document #0 to Dictionary(131459 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,263 : INFO : built Dictionary(131616 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 629 documents (total 6290000 corpus positions)
    2021-05-05 22:36:09,306 : INFO : adding document #0 to Dictionary(131616 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,315 : INFO : built Dictionary(131742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 630 documents (total 6300000 corpus positions)
    2021-05-05 22:36:09,359 : INFO : adding document #0 to Dictionary(131742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,368 : INFO : built Dictionary(131953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 631 documents (total 6310000 corpus positions)
    2021-05-05 22:36:09,411 : INFO : adding document #0 to Dictionary(131953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,419 : INFO : built Dictionary(132057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 632 documents (total 6320000 corpus positions)
    2021-05-05 22:36:09,462 : INFO : adding document #0 to Dictionary(132057 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,471 : INFO : built Dictionary(132244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 633 documents (total 6330000 corpus positions)
    2021-05-05 22:36:09,519 : INFO : adding document #0 to Dictionary(132244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,529 : INFO : built Dictionary(132418 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 634 documents (total 6340000 corpus positions)
    2021-05-05 22:36:09,572 : INFO : adding document #0 to Dictionary(132418 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,581 : INFO : built Dictionary(132511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 635 documents (total 6350000 corpus positions)
    2021-05-05 22:36:09,623 : INFO : adding document #0 to Dictionary(132511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,631 : INFO : built Dictionary(132627 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 636 documents (total 6360000 corpus positions)
    2021-05-05 22:36:09,674 : INFO : adding document #0 to Dictionary(132627 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,682 : INFO : built Dictionary(132711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 637 documents (total 6370000 corpus positions)
    2021-05-05 22:36:09,729 : INFO : adding document #0 to Dictionary(132711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,739 : INFO : built Dictionary(132941 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 638 documents (total 6380000 corpus positions)
    2021-05-05 22:36:09,782 : INFO : adding document #0 to Dictionary(132941 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,791 : INFO : built Dictionary(133060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 639 documents (total 6390000 corpus positions)
    2021-05-05 22:36:09,838 : INFO : adding document #0 to Dictionary(133060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,847 : INFO : built Dictionary(133151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 640 documents (total 6400000 corpus positions)
    2021-05-05 22:36:09,890 : INFO : adding document #0 to Dictionary(133151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,898 : INFO : built Dictionary(133285 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 641 documents (total 6410000 corpus positions)
    2021-05-05 22:36:09,941 : INFO : adding document #0 to Dictionary(133285 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:09,948 : INFO : built Dictionary(133368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 642 documents (total 6420000 corpus positions)
    2021-05-05 22:36:09,992 : INFO : adding document #0 to Dictionary(133368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,000 : INFO : built Dictionary(133450 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 643 documents (total 6430000 corpus positions)
    2021-05-05 22:36:10,048 : INFO : adding document #0 to Dictionary(133450 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,058 : INFO : built Dictionary(133653 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 644 documents (total 6440000 corpus positions)
    2021-05-05 22:36:10,101 : INFO : adding document #0 to Dictionary(133653 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,109 : INFO : built Dictionary(133728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 645 documents (total 6450000 corpus positions)
    2021-05-05 22:36:10,153 : INFO : adding document #0 to Dictionary(133728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,161 : INFO : built Dictionary(133842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 646 documents (total 6460000 corpus positions)
    2021-05-05 22:36:10,205 : INFO : adding document #0 to Dictionary(133842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,213 : INFO : built Dictionary(134024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 647 documents (total 6470000 corpus positions)
    2021-05-05 22:36:10,256 : INFO : adding document #0 to Dictionary(134024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,264 : INFO : built Dictionary(134137 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 648 documents (total 6480000 corpus positions)
    2021-05-05 22:36:10,310 : INFO : adding document #0 to Dictionary(134137 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,319 : INFO : built Dictionary(134234 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 649 documents (total 6490000 corpus positions)
    2021-05-05 22:36:10,363 : INFO : adding document #0 to Dictionary(134234 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,372 : INFO : built Dictionary(134461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 650 documents (total 6500000 corpus positions)
    2021-05-05 22:36:10,416 : INFO : adding document #0 to Dictionary(134461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,424 : INFO : built Dictionary(134632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 651 documents (total 6510000 corpus positions)
    2021-05-05 22:36:10,466 : INFO : adding document #0 to Dictionary(134632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,473 : INFO : built Dictionary(134680 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 652 documents (total 6520000 corpus positions)
    2021-05-05 22:36:10,516 : INFO : adding document #0 to Dictionary(134680 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,523 : INFO : built Dictionary(134767 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 653 documents (total 6530000 corpus positions)
    2021-05-05 22:36:10,566 : INFO : adding document #0 to Dictionary(134767 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,576 : INFO : built Dictionary(134891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 654 documents (total 6540000 corpus positions)
    2021-05-05 22:36:10,625 : INFO : adding document #0 to Dictionary(134891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,634 : INFO : built Dictionary(135012 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 655 documents (total 6550000 corpus positions)
    2021-05-05 22:36:10,679 : INFO : adding document #0 to Dictionary(135012 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,688 : INFO : built Dictionary(135134 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 656 documents (total 6560000 corpus positions)
    2021-05-05 22:36:10,734 : INFO : adding document #0 to Dictionary(135134 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,742 : INFO : built Dictionary(135244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 657 documents (total 6570000 corpus positions)
    2021-05-05 22:36:10,790 : INFO : adding document #0 to Dictionary(135244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,799 : INFO : built Dictionary(135383 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 658 documents (total 6580000 corpus positions)
    2021-05-05 22:36:10,843 : INFO : adding document #0 to Dictionary(135383 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,850 : INFO : built Dictionary(135585 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 659 documents (total 6590000 corpus positions)
    2021-05-05 22:36:10,894 : INFO : adding document #0 to Dictionary(135585 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,903 : INFO : built Dictionary(135727 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 660 documents (total 6600000 corpus positions)
    2021-05-05 22:36:10,947 : INFO : adding document #0 to Dictionary(135727 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:10,955 : INFO : built Dictionary(135813 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 661 documents (total 6610000 corpus positions)
    2021-05-05 22:36:10,999 : INFO : adding document #0 to Dictionary(135813 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,009 : INFO : built Dictionary(135902 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 662 documents (total 6620000 corpus positions)
    2021-05-05 22:36:11,056 : INFO : adding document #0 to Dictionary(135902 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,064 : INFO : built Dictionary(136011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 663 documents (total 6630000 corpus positions)
    2021-05-05 22:36:11,107 : INFO : adding document #0 to Dictionary(136011 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,115 : INFO : built Dictionary(136172 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 664 documents (total 6640000 corpus positions)
    2021-05-05 22:36:11,159 : INFO : adding document #0 to Dictionary(136172 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,168 : INFO : built Dictionary(136293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 665 documents (total 6650000 corpus positions)
    2021-05-05 22:36:11,214 : INFO : adding document #0 to Dictionary(136293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,225 : INFO : built Dictionary(136353 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 666 documents (total 6660000 corpus positions)
    2021-05-05 22:36:11,272 : INFO : adding document #0 to Dictionary(136353 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,281 : INFO : built Dictionary(136461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 667 documents (total 6670000 corpus positions)
    2021-05-05 22:36:11,324 : INFO : adding document #0 to Dictionary(136461 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,335 : INFO : built Dictionary(136556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 668 documents (total 6680000 corpus positions)
    2021-05-05 22:36:11,385 : INFO : adding document #0 to Dictionary(136556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,398 : INFO : built Dictionary(136620 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 669 documents (total 6690000 corpus positions)
    2021-05-05 22:36:11,453 : INFO : adding document #0 to Dictionary(136620 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,462 : INFO : built Dictionary(136724 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 670 documents (total 6700000 corpus positions)
    2021-05-05 22:36:11,508 : INFO : adding document #0 to Dictionary(136724 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,517 : INFO : built Dictionary(136828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 671 documents (total 6710000 corpus positions)
    2021-05-05 22:36:11,560 : INFO : adding document #0 to Dictionary(136828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,568 : INFO : built Dictionary(136888 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 672 documents (total 6720000 corpus positions)
    2021-05-05 22:36:11,617 : INFO : adding document #0 to Dictionary(136888 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,624 : INFO : built Dictionary(136982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 673 documents (total 6730000 corpus positions)
    2021-05-05 22:36:11,667 : INFO : adding document #0 to Dictionary(136982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,676 : INFO : built Dictionary(137102 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 674 documents (total 6740000 corpus positions)
    2021-05-05 22:36:11,720 : INFO : adding document #0 to Dictionary(137102 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,728 : INFO : built Dictionary(137187 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 675 documents (total 6750000 corpus positions)
    2021-05-05 22:36:11,774 : INFO : adding document #0 to Dictionary(137187 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,783 : INFO : built Dictionary(137496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 676 documents (total 6760000 corpus positions)
    2021-05-05 22:36:11,831 : INFO : adding document #0 to Dictionary(137496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,843 : INFO : built Dictionary(137623 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 677 documents (total 6770000 corpus positions)
    2021-05-05 22:36:11,887 : INFO : adding document #0 to Dictionary(137623 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,897 : INFO : built Dictionary(137744 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 678 documents (total 6780000 corpus positions)
    2021-05-05 22:36:11,945 : INFO : adding document #0 to Dictionary(137744 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:11,953 : INFO : built Dictionary(137860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 679 documents (total 6790000 corpus positions)
    2021-05-05 22:36:11,996 : INFO : adding document #0 to Dictionary(137860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,004 : INFO : built Dictionary(138028 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 680 documents (total 6800000 corpus positions)
    2021-05-05 22:36:12,047 : INFO : adding document #0 to Dictionary(138028 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,055 : INFO : built Dictionary(138152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 681 documents (total 6810000 corpus positions)
    2021-05-05 22:36:12,098 : INFO : adding document #0 to Dictionary(138152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,108 : INFO : built Dictionary(138239 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 682 documents (total 6820000 corpus positions)
    2021-05-05 22:36:12,153 : INFO : adding document #0 to Dictionary(138239 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,163 : INFO : built Dictionary(138341 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 683 documents (total 6830000 corpus positions)
    2021-05-05 22:36:12,206 : INFO : adding document #0 to Dictionary(138341 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,215 : INFO : built Dictionary(138458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 684 documents (total 6840000 corpus positions)
    2021-05-05 22:36:12,259 : INFO : adding document #0 to Dictionary(138458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,267 : INFO : built Dictionary(138601 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 685 documents (total 6850000 corpus positions)
    2021-05-05 22:36:12,311 : INFO : adding document #0 to Dictionary(138601 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,319 : INFO : built Dictionary(138704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 686 documents (total 6860000 corpus positions)
    2021-05-05 22:36:12,363 : INFO : adding document #0 to Dictionary(138704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,371 : INFO : built Dictionary(138831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 687 documents (total 6870000 corpus positions)
    2021-05-05 22:36:12,420 : INFO : adding document #0 to Dictionary(138831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,429 : INFO : built Dictionary(139055 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 688 documents (total 6880000 corpus positions)
    2021-05-05 22:36:12,472 : INFO : adding document #0 to Dictionary(139055 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,480 : INFO : built Dictionary(139148 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 689 documents (total 6890000 corpus positions)
    2021-05-05 22:36:12,523 : INFO : adding document #0 to Dictionary(139148 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,531 : INFO : built Dictionary(139208 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 690 documents (total 6900000 corpus positions)
    2021-05-05 22:36:12,575 : INFO : adding document #0 to Dictionary(139208 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,583 : INFO : built Dictionary(139310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 691 documents (total 6910000 corpus positions)
    2021-05-05 22:36:12,626 : INFO : adding document #0 to Dictionary(139310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,635 : INFO : built Dictionary(139430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 692 documents (total 6920000 corpus positions)
    2021-05-05 22:36:12,678 : INFO : adding document #0 to Dictionary(139430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,686 : INFO : built Dictionary(139521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 693 documents (total 6930000 corpus positions)
    2021-05-05 22:36:12,730 : INFO : adding document #0 to Dictionary(139521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,738 : INFO : built Dictionary(139608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 694 documents (total 6940000 corpus positions)
    2021-05-05 22:36:12,783 : INFO : adding document #0 to Dictionary(139608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,792 : INFO : built Dictionary(139720 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 695 documents (total 6950000 corpus positions)
    2021-05-05 22:36:12,841 : INFO : adding document #0 to Dictionary(139720 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,852 : INFO : built Dictionary(139797 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 696 documents (total 6960000 corpus positions)
    2021-05-05 22:36:12,901 : INFO : adding document #0 to Dictionary(139797 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,912 : INFO : built Dictionary(139969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 697 documents (total 6970000 corpus positions)
    2021-05-05 22:36:12,967 : INFO : adding document #0 to Dictionary(139969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:12,975 : INFO : built Dictionary(140104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 698 documents (total 6980000 corpus positions)
    2021-05-05 22:36:13,019 : INFO : adding document #0 to Dictionary(140104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,031 : INFO : built Dictionary(140231 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 699 documents (total 6990000 corpus positions)
    2021-05-05 22:36:13,075 : INFO : adding document #0 to Dictionary(140231 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,083 : INFO : built Dictionary(140320 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 700 documents (total 7000000 corpus positions)
    2021-05-05 22:36:13,127 : INFO : adding document #0 to Dictionary(140320 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,135 : INFO : built Dictionary(140429 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 701 documents (total 7010000 corpus positions)
    2021-05-05 22:36:13,179 : INFO : adding document #0 to Dictionary(140429 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,187 : INFO : built Dictionary(140572 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 702 documents (total 7020000 corpus positions)
    2021-05-05 22:36:13,231 : INFO : adding document #0 to Dictionary(140572 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,241 : INFO : built Dictionary(140720 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 703 documents (total 7030000 corpus positions)
    2021-05-05 22:36:13,284 : INFO : adding document #0 to Dictionary(140720 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,293 : INFO : built Dictionary(140801 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 704 documents (total 7040000 corpus positions)
    2021-05-05 22:36:13,339 : INFO : adding document #0 to Dictionary(140801 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,348 : INFO : built Dictionary(140937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 705 documents (total 7050000 corpus positions)
    2021-05-05 22:36:13,393 : INFO : adding document #0 to Dictionary(140937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,402 : INFO : built Dictionary(141029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 706 documents (total 7060000 corpus positions)
    2021-05-05 22:36:13,450 : INFO : adding document #0 to Dictionary(141029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,459 : INFO : built Dictionary(141145 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 707 documents (total 7070000 corpus positions)
    2021-05-05 22:36:13,502 : INFO : adding document #0 to Dictionary(141145 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,509 : INFO : built Dictionary(141230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 708 documents (total 7080000 corpus positions)
    2021-05-05 22:36:13,553 : INFO : adding document #0 to Dictionary(141230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,564 : INFO : built Dictionary(141660 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 709 documents (total 7090000 corpus positions)
    2021-05-05 22:36:13,607 : INFO : adding document #0 to Dictionary(141660 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,617 : INFO : built Dictionary(141799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 710 documents (total 7100000 corpus positions)
    2021-05-05 22:36:13,666 : INFO : adding document #0 to Dictionary(141799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,675 : INFO : built Dictionary(141871 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 711 documents (total 7110000 corpus positions)
    2021-05-05 22:36:13,719 : INFO : adding document #0 to Dictionary(141871 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,728 : INFO : built Dictionary(141937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 712 documents (total 7120000 corpus positions)
    2021-05-05 22:36:13,777 : INFO : adding document #0 to Dictionary(141937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,784 : INFO : built Dictionary(141983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 713 documents (total 7130000 corpus positions)
    2021-05-05 22:36:13,828 : INFO : adding document #0 to Dictionary(141983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,838 : INFO : built Dictionary(142083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 714 documents (total 7140000 corpus positions)
    2021-05-05 22:36:13,881 : INFO : adding document #0 to Dictionary(142083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,889 : INFO : built Dictionary(142196 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 715 documents (total 7150000 corpus positions)
    2021-05-05 22:36:13,936 : INFO : adding document #0 to Dictionary(142196 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,945 : INFO : built Dictionary(142299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 716 documents (total 7160000 corpus positions)
    2021-05-05 22:36:13,989 : INFO : adding document #0 to Dictionary(142299 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:13,997 : INFO : built Dictionary(142356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 717 documents (total 7170000 corpus positions)
    2021-05-05 22:36:14,048 : INFO : adding document #0 to Dictionary(142356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,057 : INFO : built Dictionary(142453 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 718 documents (total 7180000 corpus positions)
    2021-05-05 22:36:14,101 : INFO : adding document #0 to Dictionary(142453 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,109 : INFO : built Dictionary(142562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 719 documents (total 7190000 corpus positions)
    2021-05-05 22:36:14,153 : INFO : adding document #0 to Dictionary(142562 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,161 : INFO : built Dictionary(142689 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 720 documents (total 7200000 corpus positions)
    2021-05-05 22:36:14,205 : INFO : adding document #0 to Dictionary(142689 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,217 : INFO : built Dictionary(142789 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 721 documents (total 7210000 corpus positions)
    2021-05-05 22:36:14,263 : INFO : adding document #0 to Dictionary(142789 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,272 : INFO : built Dictionary(142887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 722 documents (total 7220000 corpus positions)
    2021-05-05 22:36:14,317 : INFO : adding document #0 to Dictionary(142887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,325 : INFO : built Dictionary(143013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 723 documents (total 7230000 corpus positions)
    2021-05-05 22:36:14,369 : INFO : adding document #0 to Dictionary(143013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,376 : INFO : built Dictionary(143123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 724 documents (total 7240000 corpus positions)
    2021-05-05 22:36:14,420 : INFO : adding document #0 to Dictionary(143123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,428 : INFO : built Dictionary(143195 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 725 documents (total 7250000 corpus positions)
    2021-05-05 22:36:14,472 : INFO : adding document #0 to Dictionary(143195 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,479 : INFO : built Dictionary(143322 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 726 documents (total 7260000 corpus positions)
    2021-05-05 22:36:14,527 : INFO : adding document #0 to Dictionary(143322 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,536 : INFO : built Dictionary(143419 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 727 documents (total 7270000 corpus positions)
    2021-05-05 22:36:14,580 : INFO : adding document #0 to Dictionary(143419 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,588 : INFO : built Dictionary(143471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 728 documents (total 7280000 corpus positions)
    2021-05-05 22:36:14,638 : INFO : adding document #0 to Dictionary(143471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,647 : INFO : built Dictionary(143543 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 729 documents (total 7290000 corpus positions)
    2021-05-05 22:36:14,690 : INFO : adding document #0 to Dictionary(143543 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,697 : INFO : built Dictionary(143675 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 730 documents (total 7300000 corpus positions)
    2021-05-05 22:36:14,747 : INFO : adding document #0 to Dictionary(143675 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,757 : INFO : built Dictionary(143808 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 731 documents (total 7310000 corpus positions)
    2021-05-05 22:36:14,801 : INFO : adding document #0 to Dictionary(143808 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,810 : INFO : built Dictionary(143907 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 732 documents (total 7320000 corpus positions)
    2021-05-05 22:36:14,855 : INFO : adding document #0 to Dictionary(143907 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,864 : INFO : built Dictionary(144045 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 733 documents (total 7330000 corpus positions)
    2021-05-05 22:36:14,909 : INFO : adding document #0 to Dictionary(144045 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,917 : INFO : built Dictionary(144153 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 734 documents (total 7340000 corpus positions)
    2021-05-05 22:36:14,964 : INFO : adding document #0 to Dictionary(144153 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:14,972 : INFO : built Dictionary(144226 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 735 documents (total 7350000 corpus positions)
    2021-05-05 22:36:15,016 : INFO : adding document #0 to Dictionary(144226 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,025 : INFO : built Dictionary(144325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 736 documents (total 7360000 corpus positions)
    2021-05-05 22:36:15,074 : INFO : adding document #0 to Dictionary(144325 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,081 : INFO : built Dictionary(144419 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 737 documents (total 7370000 corpus positions)
    2021-05-05 22:36:15,131 : INFO : adding document #0 to Dictionary(144419 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,138 : INFO : built Dictionary(144518 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 738 documents (total 7380000 corpus positions)
    2021-05-05 22:36:15,181 : INFO : adding document #0 to Dictionary(144518 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,188 : INFO : built Dictionary(144615 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 739 documents (total 7390000 corpus positions)
    2021-05-05 22:36:15,235 : INFO : adding document #0 to Dictionary(144615 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,244 : INFO : built Dictionary(144823 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 740 documents (total 7400000 corpus positions)
    2021-05-05 22:36:15,287 : INFO : adding document #0 to Dictionary(144823 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,297 : INFO : built Dictionary(145022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 741 documents (total 7410000 corpus positions)
    2021-05-05 22:36:15,347 : INFO : adding document #0 to Dictionary(145022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,356 : INFO : built Dictionary(145076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 742 documents (total 7420000 corpus positions)
    2021-05-05 22:36:15,401 : INFO : adding document #0 to Dictionary(145076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,411 : INFO : built Dictionary(145168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 743 documents (total 7430000 corpus positions)
    2021-05-05 22:36:15,458 : INFO : adding document #0 to Dictionary(145168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,467 : INFO : built Dictionary(145555 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 744 documents (total 7440000 corpus positions)
    2021-05-05 22:36:15,510 : INFO : adding document #0 to Dictionary(145555 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,517 : INFO : built Dictionary(145596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 745 documents (total 7450000 corpus positions)
    2021-05-05 22:36:15,560 : INFO : adding document #0 to Dictionary(145596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,568 : INFO : built Dictionary(145704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 746 documents (total 7460000 corpus positions)
    2021-05-05 22:36:15,613 : INFO : adding document #0 to Dictionary(145704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,620 : INFO : built Dictionary(145814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 747 documents (total 7470000 corpus positions)
    2021-05-05 22:36:15,663 : INFO : adding document #0 to Dictionary(145814 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,670 : INFO : built Dictionary(145943 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 748 documents (total 7480000 corpus positions)
    2021-05-05 22:36:15,714 : INFO : adding document #0 to Dictionary(145943 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,722 : INFO : built Dictionary(146043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 749 documents (total 7490000 corpus positions)
    2021-05-05 22:36:15,766 : INFO : adding document #0 to Dictionary(146043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,774 : INFO : built Dictionary(146150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 750 documents (total 7500000 corpus positions)
    2021-05-05 22:36:15,819 : INFO : adding document #0 to Dictionary(146150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,829 : INFO : built Dictionary(146253 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 751 documents (total 7510000 corpus positions)
    2021-05-05 22:36:15,873 : INFO : adding document #0 to Dictionary(146253 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,882 : INFO : built Dictionary(146334 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 752 documents (total 7520000 corpus positions)
    2021-05-05 22:36:15,929 : INFO : adding document #0 to Dictionary(146334 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,939 : INFO : built Dictionary(146706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 753 documents (total 7530000 corpus positions)
    2021-05-05 22:36:15,982 : INFO : adding document #0 to Dictionary(146706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:15,988 : INFO : built Dictionary(146753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 754 documents (total 7540000 corpus positions)
    2021-05-05 22:36:16,041 : INFO : adding document #0 to Dictionary(146753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,050 : INFO : built Dictionary(146833 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 755 documents (total 7550000 corpus positions)
    2021-05-05 22:36:16,095 : INFO : adding document #0 to Dictionary(146833 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,102 : INFO : built Dictionary(146974 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 756 documents (total 7560000 corpus positions)
    2021-05-05 22:36:16,146 : INFO : adding document #0 to Dictionary(146974 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,153 : INFO : built Dictionary(147051 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 757 documents (total 7570000 corpus positions)
    2021-05-05 22:36:16,196 : INFO : adding document #0 to Dictionary(147051 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,204 : INFO : built Dictionary(147152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 758 documents (total 7580000 corpus positions)
    2021-05-05 22:36:16,248 : INFO : adding document #0 to Dictionary(147152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,255 : INFO : built Dictionary(147232 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 759 documents (total 7590000 corpus positions)
    2021-05-05 22:36:16,298 : INFO : adding document #0 to Dictionary(147232 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,305 : INFO : built Dictionary(147305 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 760 documents (total 7600000 corpus positions)
    2021-05-05 22:36:16,355 : INFO : adding document #0 to Dictionary(147305 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,363 : INFO : built Dictionary(147363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 761 documents (total 7610000 corpus positions)
    2021-05-05 22:36:16,409 : INFO : adding document #0 to Dictionary(147363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,418 : INFO : built Dictionary(147473 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 762 documents (total 7620000 corpus positions)
    2021-05-05 22:36:16,462 : INFO : adding document #0 to Dictionary(147473 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,470 : INFO : built Dictionary(147576 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 763 documents (total 7630000 corpus positions)
    2021-05-05 22:36:16,518 : INFO : adding document #0 to Dictionary(147576 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,527 : INFO : built Dictionary(147682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 764 documents (total 7640000 corpus positions)
    2021-05-05 22:36:16,571 : INFO : adding document #0 to Dictionary(147682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,579 : INFO : built Dictionary(147779 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 765 documents (total 7650000 corpus positions)
    2021-05-05 22:36:16,623 : INFO : adding document #0 to Dictionary(147779 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,632 : INFO : built Dictionary(147964 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 766 documents (total 7660000 corpus positions)
    2021-05-05 22:36:16,675 : INFO : adding document #0 to Dictionary(147964 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,683 : INFO : built Dictionary(148040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 767 documents (total 7670000 corpus positions)
    2021-05-05 22:36:16,727 : INFO : adding document #0 to Dictionary(148040 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,737 : INFO : built Dictionary(148431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 768 documents (total 7680000 corpus positions)
    2021-05-05 22:36:16,781 : INFO : adding document #0 to Dictionary(148431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,789 : INFO : built Dictionary(148526 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 769 documents (total 7690000 corpus positions)
    2021-05-05 22:36:16,833 : INFO : adding document #0 to Dictionary(148526 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,841 : INFO : built Dictionary(148588 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 770 documents (total 7700000 corpus positions)
    2021-05-05 22:36:16,883 : INFO : adding document #0 to Dictionary(148588 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,890 : INFO : built Dictionary(148663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 771 documents (total 7710000 corpus positions)
    2021-05-05 22:36:16,933 : INFO : adding document #0 to Dictionary(148663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,940 : INFO : built Dictionary(148735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 772 documents (total 7720000 corpus positions)
    2021-05-05 22:36:16,983 : INFO : adding document #0 to Dictionary(148735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:16,990 : INFO : built Dictionary(148835 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 773 documents (total 7730000 corpus positions)
    2021-05-05 22:36:17,038 : INFO : adding document #0 to Dictionary(148835 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,048 : INFO : built Dictionary(148970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 774 documents (total 7740000 corpus positions)
    2021-05-05 22:36:17,092 : INFO : adding document #0 to Dictionary(148970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,101 : INFO : built Dictionary(149064 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 775 documents (total 7750000 corpus positions)
    2021-05-05 22:36:17,149 : INFO : adding document #0 to Dictionary(149064 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,157 : INFO : built Dictionary(149141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 776 documents (total 7760000 corpus positions)
    2021-05-05 22:36:17,201 : INFO : adding document #0 to Dictionary(149141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,213 : INFO : built Dictionary(149304 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 777 documents (total 7770000 corpus positions)
    2021-05-05 22:36:17,262 : INFO : adding document #0 to Dictionary(149304 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,271 : INFO : built Dictionary(149522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 778 documents (total 7780000 corpus positions)
    2021-05-05 22:36:17,315 : INFO : adding document #0 to Dictionary(149522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,323 : INFO : built Dictionary(149586 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 779 documents (total 7790000 corpus positions)
    2021-05-05 22:36:17,368 : INFO : adding document #0 to Dictionary(149586 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,378 : INFO : built Dictionary(149693 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 780 documents (total 7800000 corpus positions)
    2021-05-05 22:36:17,422 : INFO : adding document #0 to Dictionary(149693 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,430 : INFO : built Dictionary(149781 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 781 documents (total 7810000 corpus positions)
    2021-05-05 22:36:17,474 : INFO : adding document #0 to Dictionary(149781 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,482 : INFO : built Dictionary(149947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 782 documents (total 7820000 corpus positions)
    2021-05-05 22:36:17,525 : INFO : adding document #0 to Dictionary(149947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,534 : INFO : built Dictionary(150037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 783 documents (total 7830000 corpus positions)
    2021-05-05 22:36:17,578 : INFO : adding document #0 to Dictionary(150037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,586 : INFO : built Dictionary(150111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 784 documents (total 7840000 corpus positions)
    2021-05-05 22:36:17,629 : INFO : adding document #0 to Dictionary(150111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,637 : INFO : built Dictionary(150267 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 785 documents (total 7850000 corpus positions)
    2021-05-05 22:36:17,680 : INFO : adding document #0 to Dictionary(150267 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,688 : INFO : built Dictionary(150338 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 786 documents (total 7860000 corpus positions)
    2021-05-05 22:36:17,732 : INFO : adding document #0 to Dictionary(150338 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,742 : INFO : built Dictionary(150683 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 787 documents (total 7870000 corpus positions)
    2021-05-05 22:36:17,788 : INFO : adding document #0 to Dictionary(150683 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,797 : INFO : built Dictionary(150838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 788 documents (total 7880000 corpus positions)
    2021-05-05 22:36:17,846 : INFO : adding document #0 to Dictionary(150838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,857 : INFO : built Dictionary(150934 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 789 documents (total 7890000 corpus positions)
    2021-05-05 22:36:17,902 : INFO : adding document #0 to Dictionary(150934 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,912 : INFO : built Dictionary(151047 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 790 documents (total 7900000 corpus positions)
    2021-05-05 22:36:17,957 : INFO : adding document #0 to Dictionary(151047 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:17,965 : INFO : built Dictionary(151134 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 791 documents (total 7910000 corpus positions)
    2021-05-05 22:36:18,014 : INFO : adding document #0 to Dictionary(151134 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,023 : INFO : built Dictionary(151214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 792 documents (total 7920000 corpus positions)
    2021-05-05 22:36:18,073 : INFO : adding document #0 to Dictionary(151214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,084 : INFO : built Dictionary(151349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 793 documents (total 7930000 corpus positions)
    2021-05-05 22:36:18,131 : INFO : adding document #0 to Dictionary(151349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,140 : INFO : built Dictionary(151467 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 794 documents (total 7940000 corpus positions)
    2021-05-05 22:36:18,183 : INFO : adding document #0 to Dictionary(151467 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,191 : INFO : built Dictionary(151541 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 795 documents (total 7950000 corpus positions)
    2021-05-05 22:36:18,239 : INFO : adding document #0 to Dictionary(151541 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,247 : INFO : built Dictionary(151637 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 796 documents (total 7960000 corpus positions)
    2021-05-05 22:36:18,291 : INFO : adding document #0 to Dictionary(151637 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,298 : INFO : built Dictionary(151731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 797 documents (total 7970000 corpus positions)
    2021-05-05 22:36:18,341 : INFO : adding document #0 to Dictionary(151731 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,349 : INFO : built Dictionary(151812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 798 documents (total 7980000 corpus positions)
    2021-05-05 22:36:18,391 : INFO : adding document #0 to Dictionary(151812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,400 : INFO : built Dictionary(151891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 799 documents (total 7990000 corpus positions)
    2021-05-05 22:36:18,444 : INFO : adding document #0 to Dictionary(151891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,452 : INFO : built Dictionary(151968 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 800 documents (total 8000000 corpus positions)
    2021-05-05 22:36:18,534 : INFO : adding document #0 to Dictionary(151968 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,570 : INFO : built Dictionary(152084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 801 documents (total 8010000 corpus positions)
    2021-05-05 22:36:18,640 : INFO : adding document #0 to Dictionary(152084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,648 : INFO : built Dictionary(152180 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 802 documents (total 8020000 corpus positions)
    2021-05-05 22:36:18,702 : INFO : adding document #0 to Dictionary(152180 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,710 : INFO : built Dictionary(152274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 803 documents (total 8030000 corpus positions)
    2021-05-05 22:36:18,762 : INFO : adding document #0 to Dictionary(152274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,769 : INFO : built Dictionary(152349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 804 documents (total 8040000 corpus positions)
    2021-05-05 22:36:18,822 : INFO : adding document #0 to Dictionary(152349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,834 : INFO : built Dictionary(152414 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 805 documents (total 8050000 corpus positions)
    2021-05-05 22:36:18,881 : INFO : adding document #0 to Dictionary(152414 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,891 : INFO : built Dictionary(153168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 806 documents (total 8060000 corpus positions)
    2021-05-05 22:36:18,934 : INFO : adding document #0 to Dictionary(153168 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,942 : INFO : built Dictionary(153234 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 807 documents (total 8070000 corpus positions)
    2021-05-05 22:36:18,987 : INFO : adding document #0 to Dictionary(153234 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:18,996 : INFO : built Dictionary(153330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 808 documents (total 8080000 corpus positions)
    2021-05-05 22:36:19,043 : INFO : adding document #0 to Dictionary(153330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,053 : INFO : built Dictionary(153674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 809 documents (total 8090000 corpus positions)
    2021-05-05 22:36:19,100 : INFO : adding document #0 to Dictionary(153674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,109 : INFO : built Dictionary(153770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 810 documents (total 8100000 corpus positions)
    2021-05-05 22:36:19,153 : INFO : adding document #0 to Dictionary(153770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,161 : INFO : built Dictionary(153832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 811 documents (total 8110000 corpus positions)
    2021-05-05 22:36:19,204 : INFO : adding document #0 to Dictionary(153832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,215 : INFO : built Dictionary(153892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 812 documents (total 8120000 corpus positions)
    2021-05-05 22:36:19,263 : INFO : adding document #0 to Dictionary(153892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,273 : INFO : built Dictionary(154030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 813 documents (total 8130000 corpus positions)
    2021-05-05 22:36:19,316 : INFO : adding document #0 to Dictionary(154030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,325 : INFO : built Dictionary(154151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 814 documents (total 8140000 corpus positions)
    2021-05-05 22:36:19,368 : INFO : adding document #0 to Dictionary(154151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,375 : INFO : built Dictionary(154274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 815 documents (total 8150000 corpus positions)
    2021-05-05 22:36:19,418 : INFO : adding document #0 to Dictionary(154274 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,425 : INFO : built Dictionary(154347 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 816 documents (total 8160000 corpus positions)
    2021-05-05 22:36:19,473 : INFO : adding document #0 to Dictionary(154347 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,480 : INFO : built Dictionary(154424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 817 documents (total 8170000 corpus positions)
    2021-05-05 22:36:19,523 : INFO : adding document #0 to Dictionary(154424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,531 : INFO : built Dictionary(154528 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 818 documents (total 8180000 corpus positions)
    2021-05-05 22:36:19,574 : INFO : adding document #0 to Dictionary(154528 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,583 : INFO : built Dictionary(154726 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 819 documents (total 8190000 corpus positions)
    2021-05-05 22:36:19,625 : INFO : adding document #0 to Dictionary(154726 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,633 : INFO : built Dictionary(155004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 820 documents (total 8200000 corpus positions)
    2021-05-05 22:36:19,676 : INFO : adding document #0 to Dictionary(155004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,683 : INFO : built Dictionary(155187 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 821 documents (total 8210000 corpus positions)
    2021-05-05 22:36:19,731 : INFO : adding document #0 to Dictionary(155187 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,738 : INFO : built Dictionary(155322 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 822 documents (total 8220000 corpus positions)
    2021-05-05 22:36:19,782 : INFO : adding document #0 to Dictionary(155322 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,790 : INFO : built Dictionary(155492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 823 documents (total 8230000 corpus positions)
    2021-05-05 22:36:19,834 : INFO : adding document #0 to Dictionary(155492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,843 : INFO : built Dictionary(155639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 824 documents (total 8240000 corpus positions)
    2021-05-05 22:36:19,910 : INFO : adding document #0 to Dictionary(155639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,920 : INFO : built Dictionary(155714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 825 documents (total 8250000 corpus positions)
    2021-05-05 22:36:19,966 : INFO : adding document #0 to Dictionary(155714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:19,975 : INFO : built Dictionary(155804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 826 documents (total 8260000 corpus positions)
    2021-05-05 22:36:20,021 : INFO : adding document #0 to Dictionary(155804 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,029 : INFO : built Dictionary(155863 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 827 documents (total 8270000 corpus positions)
    2021-05-05 22:36:20,074 : INFO : adding document #0 to Dictionary(155863 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,081 : INFO : built Dictionary(155938 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 828 documents (total 8280000 corpus positions)
    2021-05-05 22:36:20,128 : INFO : adding document #0 to Dictionary(155938 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,136 : INFO : built Dictionary(156110 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 829 documents (total 8290000 corpus positions)
    2021-05-05 22:36:20,183 : INFO : adding document #0 to Dictionary(156110 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,191 : INFO : built Dictionary(156355 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 830 documents (total 8300000 corpus positions)
    2021-05-05 22:36:20,238 : INFO : adding document #0 to Dictionary(156355 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,247 : INFO : built Dictionary(156457 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 831 documents (total 8310000 corpus positions)
    2021-05-05 22:36:20,294 : INFO : adding document #0 to Dictionary(156457 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,302 : INFO : built Dictionary(156535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 832 documents (total 8320000 corpus positions)
    2021-05-05 22:36:20,350 : INFO : adding document #0 to Dictionary(156535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,359 : INFO : built Dictionary(156674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 833 documents (total 8330000 corpus positions)
    2021-05-05 22:36:20,408 : INFO : adding document #0 to Dictionary(156674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,417 : INFO : built Dictionary(156751 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 834 documents (total 8340000 corpus positions)
    2021-05-05 22:36:20,465 : INFO : adding document #0 to Dictionary(156751 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,474 : INFO : built Dictionary(156824 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 835 documents (total 8350000 corpus positions)
    2021-05-05 22:36:20,523 : INFO : adding document #0 to Dictionary(156824 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,534 : INFO : built Dictionary(157156 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 836 documents (total 8360000 corpus positions)
    2021-05-05 22:36:20,581 : INFO : adding document #0 to Dictionary(157156 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,590 : INFO : built Dictionary(157333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 837 documents (total 8370000 corpus positions)
    2021-05-05 22:36:20,637 : INFO : adding document #0 to Dictionary(157333 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,645 : INFO : built Dictionary(157412 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 838 documents (total 8380000 corpus positions)
    2021-05-05 22:36:20,693 : INFO : adding document #0 to Dictionary(157412 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,701 : INFO : built Dictionary(157484 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 839 documents (total 8390000 corpus positions)
    2021-05-05 22:36:20,753 : INFO : adding document #0 to Dictionary(157484 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,761 : INFO : built Dictionary(157556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 840 documents (total 8400000 corpus positions)
    2021-05-05 22:36:20,810 : INFO : adding document #0 to Dictionary(157556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,818 : INFO : built Dictionary(157657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 841 documents (total 8410000 corpus positions)
    2021-05-05 22:36:20,867 : INFO : adding document #0 to Dictionary(157657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,877 : INFO : built Dictionary(157730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 842 documents (total 8420000 corpus positions)
    2021-05-05 22:36:20,929 : INFO : adding document #0 to Dictionary(157730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,938 : INFO : built Dictionary(157807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 843 documents (total 8430000 corpus positions)
    2021-05-05 22:36:20,984 : INFO : adding document #0 to Dictionary(157807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:20,993 : INFO : built Dictionary(158043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 844 documents (total 8440000 corpus positions)
    2021-05-05 22:36:21,041 : INFO : adding document #0 to Dictionary(158043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,052 : INFO : built Dictionary(158171 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 845 documents (total 8450000 corpus positions)
    2021-05-05 22:36:21,098 : INFO : adding document #0 to Dictionary(158171 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,107 : INFO : built Dictionary(158294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 846 documents (total 8460000 corpus positions)
    2021-05-05 22:36:21,153 : INFO : adding document #0 to Dictionary(158294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,162 : INFO : built Dictionary(158616 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 847 documents (total 8470000 corpus positions)
    2021-05-05 22:36:21,212 : INFO : adding document #0 to Dictionary(158616 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,222 : INFO : built Dictionary(158684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 848 documents (total 8480000 corpus positions)
    2021-05-05 22:36:21,271 : INFO : adding document #0 to Dictionary(158684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,279 : INFO : built Dictionary(158816 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 849 documents (total 8490000 corpus positions)
    2021-05-05 22:36:21,322 : INFO : adding document #0 to Dictionary(158816 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,331 : INFO : built Dictionary(158890 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 850 documents (total 8500000 corpus positions)
    2021-05-05 22:36:21,375 : INFO : adding document #0 to Dictionary(158890 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,383 : INFO : built Dictionary(158990 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 851 documents (total 8510000 corpus positions)
    2021-05-05 22:36:21,427 : INFO : adding document #0 to Dictionary(158990 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,434 : INFO : built Dictionary(159060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 852 documents (total 8520000 corpus positions)
    2021-05-05 22:36:21,477 : INFO : adding document #0 to Dictionary(159060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,486 : INFO : built Dictionary(159215 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 853 documents (total 8530000 corpus positions)
    2021-05-05 22:36:21,530 : INFO : adding document #0 to Dictionary(159215 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,538 : INFO : built Dictionary(159321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 854 documents (total 8540000 corpus positions)
    2021-05-05 22:36:21,585 : INFO : adding document #0 to Dictionary(159321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,594 : INFO : built Dictionary(159406 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 855 documents (total 8550000 corpus positions)
    2021-05-05 22:36:21,636 : INFO : adding document #0 to Dictionary(159406 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,643 : INFO : built Dictionary(159466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 856 documents (total 8560000 corpus positions)
    2021-05-05 22:36:21,687 : INFO : adding document #0 to Dictionary(159466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,694 : INFO : built Dictionary(159576 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 857 documents (total 8570000 corpus positions)
    2021-05-05 22:36:21,737 : INFO : adding document #0 to Dictionary(159576 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,746 : INFO : built Dictionary(159685 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 858 documents (total 8580000 corpus positions)
    2021-05-05 22:36:21,789 : INFO : adding document #0 to Dictionary(159685 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,796 : INFO : built Dictionary(159783 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 859 documents (total 8590000 corpus positions)
    2021-05-05 22:36:21,842 : INFO : adding document #0 to Dictionary(159783 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,849 : INFO : built Dictionary(159879 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 860 documents (total 8600000 corpus positions)
    2021-05-05 22:36:21,894 : INFO : adding document #0 to Dictionary(159879 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,901 : INFO : built Dictionary(159940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 861 documents (total 8610000 corpus positions)
    2021-05-05 22:36:21,945 : INFO : adding document #0 to Dictionary(159940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:21,953 : INFO : built Dictionary(160060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 862 documents (total 8620000 corpus positions)
    2021-05-05 22:36:21,996 : INFO : adding document #0 to Dictionary(160060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,005 : INFO : built Dictionary(160124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 863 documents (total 8630000 corpus positions)
    2021-05-05 22:36:22,052 : INFO : adding document #0 to Dictionary(160124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,060 : INFO : built Dictionary(160164 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 864 documents (total 8640000 corpus positions)
    2021-05-05 22:36:22,103 : INFO : adding document #0 to Dictionary(160164 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,110 : INFO : built Dictionary(160309 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 865 documents (total 8650000 corpus positions)
    2021-05-05 22:36:22,155 : INFO : adding document #0 to Dictionary(160309 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,163 : INFO : built Dictionary(160401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 866 documents (total 8660000 corpus positions)
    2021-05-05 22:36:22,206 : INFO : adding document #0 to Dictionary(160401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,214 : INFO : built Dictionary(160548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 867 documents (total 8670000 corpus positions)
    2021-05-05 22:36:22,259 : INFO : adding document #0 to Dictionary(160548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,267 : INFO : built Dictionary(160709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 868 documents (total 8680000 corpus positions)
    2021-05-05 22:36:22,312 : INFO : adding document #0 to Dictionary(160709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,321 : INFO : built Dictionary(160829 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 869 documents (total 8690000 corpus positions)
    2021-05-05 22:36:22,365 : INFO : adding document #0 to Dictionary(160829 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,374 : INFO : built Dictionary(160981 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 870 documents (total 8700000 corpus positions)
    2021-05-05 22:36:22,417 : INFO : adding document #0 to Dictionary(160981 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,425 : INFO : built Dictionary(161093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 871 documents (total 8710000 corpus positions)
    2021-05-05 22:36:22,469 : INFO : adding document #0 to Dictionary(161093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,479 : INFO : built Dictionary(161182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 872 documents (total 8720000 corpus positions)
    2021-05-05 22:36:22,521 : INFO : adding document #0 to Dictionary(161182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,528 : INFO : built Dictionary(161252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 873 documents (total 8730000 corpus positions)
    2021-05-05 22:36:22,571 : INFO : adding document #0 to Dictionary(161252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,579 : INFO : built Dictionary(161363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 874 documents (total 8740000 corpus positions)
    2021-05-05 22:36:22,622 : INFO : adding document #0 to Dictionary(161363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,630 : INFO : built Dictionary(161458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 875 documents (total 8750000 corpus positions)
    2021-05-05 22:36:22,672 : INFO : adding document #0 to Dictionary(161458 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,680 : INFO : built Dictionary(161575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 876 documents (total 8760000 corpus positions)
    2021-05-05 22:36:22,722 : INFO : adding document #0 to Dictionary(161575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,731 : INFO : built Dictionary(161691 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 877 documents (total 8770000 corpus positions)
    2021-05-05 22:36:22,778 : INFO : adding document #0 to Dictionary(161691 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,786 : INFO : built Dictionary(161832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 878 documents (total 8780000 corpus positions)
    2021-05-05 22:36:22,829 : INFO : adding document #0 to Dictionary(161832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,840 : INFO : built Dictionary(161936 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 879 documents (total 8790000 corpus positions)
    2021-05-05 22:36:22,891 : INFO : adding document #0 to Dictionary(161936 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,902 : INFO : built Dictionary(162022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 880 documents (total 8800000 corpus positions)
    2021-05-05 22:36:22,956 : INFO : adding document #0 to Dictionary(162022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:22,965 : INFO : built Dictionary(162163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 881 documents (total 8810000 corpus positions)
    2021-05-05 22:36:23,011 : INFO : adding document #0 to Dictionary(162163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,021 : INFO : built Dictionary(162287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 882 documents (total 8820000 corpus positions)
    2021-05-05 22:36:23,065 : INFO : adding document #0 to Dictionary(162287 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,073 : INFO : built Dictionary(162378 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 883 documents (total 8830000 corpus positions)
    2021-05-05 22:36:23,121 : INFO : adding document #0 to Dictionary(162378 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,129 : INFO : built Dictionary(162474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 884 documents (total 8840000 corpus positions)
    2021-05-05 22:36:23,173 : INFO : adding document #0 to Dictionary(162474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,180 : INFO : built Dictionary(162568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 885 documents (total 8850000 corpus positions)
    2021-05-05 22:36:23,224 : INFO : adding document #0 to Dictionary(162568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,231 : INFO : built Dictionary(162645 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 886 documents (total 8860000 corpus positions)
    2021-05-05 22:36:23,278 : INFO : adding document #0 to Dictionary(162645 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,285 : INFO : built Dictionary(162755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 887 documents (total 8870000 corpus positions)
    2021-05-05 22:36:23,333 : INFO : adding document #0 to Dictionary(162755 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,341 : INFO : built Dictionary(162815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 888 documents (total 8880000 corpus positions)
    2021-05-05 22:36:23,384 : INFO : adding document #0 to Dictionary(162815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,391 : INFO : built Dictionary(162882 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 889 documents (total 8890000 corpus positions)
    2021-05-05 22:36:23,435 : INFO : adding document #0 to Dictionary(162882 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,443 : INFO : built Dictionary(162984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 890 documents (total 8900000 corpus positions)
    2021-05-05 22:36:23,488 : INFO : adding document #0 to Dictionary(162984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,496 : INFO : built Dictionary(163136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 891 documents (total 8910000 corpus positions)
    2021-05-05 22:36:23,540 : INFO : adding document #0 to Dictionary(163136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,548 : INFO : built Dictionary(163311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 892 documents (total 8920000 corpus positions)
    2021-05-05 22:36:23,591 : INFO : adding document #0 to Dictionary(163311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,598 : INFO : built Dictionary(163421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 893 documents (total 8930000 corpus positions)
    2021-05-05 22:36:23,642 : INFO : adding document #0 to Dictionary(163421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,650 : INFO : built Dictionary(163481 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 894 documents (total 8940000 corpus positions)
    2021-05-05 22:36:23,693 : INFO : adding document #0 to Dictionary(163481 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,701 : INFO : built Dictionary(163537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 895 documents (total 8950000 corpus positions)
    2021-05-05 22:36:23,751 : INFO : adding document #0 to Dictionary(163537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,759 : INFO : built Dictionary(163697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 896 documents (total 8960000 corpus positions)
    2021-05-05 22:36:23,810 : INFO : adding document #0 to Dictionary(163697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,818 : INFO : built Dictionary(163771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 897 documents (total 8970000 corpus positions)
    2021-05-05 22:36:23,861 : INFO : adding document #0 to Dictionary(163771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,870 : INFO : built Dictionary(163950 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 898 documents (total 8980000 corpus positions)
    2021-05-05 22:36:23,915 : INFO : adding document #0 to Dictionary(163950 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,927 : INFO : built Dictionary(164922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 899 documents (total 8990000 corpus positions)
    2021-05-05 22:36:23,972 : INFO : adding document #0 to Dictionary(164922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:23,983 : INFO : built Dictionary(165570 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 900 documents (total 9000000 corpus positions)
    2021-05-05 22:36:24,027 : INFO : adding document #0 to Dictionary(165570 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,039 : INFO : built Dictionary(166285 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 901 documents (total 9010000 corpus positions)
    2021-05-05 22:36:24,084 : INFO : adding document #0 to Dictionary(166285 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,091 : INFO : built Dictionary(166408 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 902 documents (total 9020000 corpus positions)
    2021-05-05 22:36:24,136 : INFO : adding document #0 to Dictionary(166408 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,143 : INFO : built Dictionary(166513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 903 documents (total 9030000 corpus positions)
    2021-05-05 22:36:24,191 : INFO : adding document #0 to Dictionary(166513 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,199 : INFO : built Dictionary(166717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 904 documents (total 9040000 corpus positions)
    2021-05-05 22:36:24,255 : INFO : adding document #0 to Dictionary(166717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,264 : INFO : built Dictionary(166828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 905 documents (total 9050000 corpus positions)
    2021-05-05 22:36:24,320 : INFO : adding document #0 to Dictionary(166828 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,328 : INFO : built Dictionary(166933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 906 documents (total 9060000 corpus positions)
    2021-05-05 22:36:24,382 : INFO : adding document #0 to Dictionary(166933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,390 : INFO : built Dictionary(167004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 907 documents (total 9070000 corpus positions)
    2021-05-05 22:36:24,433 : INFO : adding document #0 to Dictionary(167004 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,441 : INFO : built Dictionary(167204 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 908 documents (total 9080000 corpus positions)
    2021-05-05 22:36:24,485 : INFO : adding document #0 to Dictionary(167204 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,492 : INFO : built Dictionary(167276 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 909 documents (total 9090000 corpus positions)
    2021-05-05 22:36:24,537 : INFO : adding document #0 to Dictionary(167276 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,545 : INFO : built Dictionary(167336 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 910 documents (total 9100000 corpus positions)
    2021-05-05 22:36:24,591 : INFO : adding document #0 to Dictionary(167336 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,599 : INFO : built Dictionary(167441 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 911 documents (total 9110000 corpus positions)
    2021-05-05 22:36:24,646 : INFO : adding document #0 to Dictionary(167441 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,654 : INFO : built Dictionary(167575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 912 documents (total 9120000 corpus positions)
    2021-05-05 22:36:24,698 : INFO : adding document #0 to Dictionary(167575 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,706 : INFO : built Dictionary(167628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 913 documents (total 9130000 corpus positions)
    2021-05-05 22:36:24,752 : INFO : adding document #0 to Dictionary(167628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,763 : INFO : built Dictionary(167802 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 914 documents (total 9140000 corpus positions)
    2021-05-05 22:36:24,806 : INFO : adding document #0 to Dictionary(167802 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,815 : INFO : built Dictionary(167869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 915 documents (total 9150000 corpus positions)
    2021-05-05 22:36:24,858 : INFO : adding document #0 to Dictionary(167869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,865 : INFO : built Dictionary(167935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 916 documents (total 9160000 corpus positions)
    2021-05-05 22:36:24,908 : INFO : adding document #0 to Dictionary(167935 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,920 : INFO : built Dictionary(168049 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 917 documents (total 9170000 corpus positions)
    2021-05-05 22:36:24,967 : INFO : adding document #0 to Dictionary(168049 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:24,975 : INFO : built Dictionary(168189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 918 documents (total 9180000 corpus positions)
    2021-05-05 22:36:25,024 : INFO : adding document #0 to Dictionary(168189 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,032 : INFO : built Dictionary(168214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 919 documents (total 9190000 corpus positions)
    2021-05-05 22:36:25,079 : INFO : adding document #0 to Dictionary(168214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,087 : INFO : built Dictionary(168282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 920 documents (total 9200000 corpus positions)
    2021-05-05 22:36:25,135 : INFO : adding document #0 to Dictionary(168282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,144 : INFO : built Dictionary(168394 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 921 documents (total 9210000 corpus positions)
    2021-05-05 22:36:25,189 : INFO : adding document #0 to Dictionary(168394 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,197 : INFO : built Dictionary(168480 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 922 documents (total 9220000 corpus positions)
    2021-05-05 22:36:25,241 : INFO : adding document #0 to Dictionary(168480 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,249 : INFO : built Dictionary(168548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 923 documents (total 9230000 corpus positions)
    2021-05-05 22:36:25,294 : INFO : adding document #0 to Dictionary(168548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,302 : INFO : built Dictionary(168600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 924 documents (total 9240000 corpus positions)
    2021-05-05 22:36:25,347 : INFO : adding document #0 to Dictionary(168600 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,355 : INFO : built Dictionary(168721 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 925 documents (total 9250000 corpus positions)
    2021-05-05 22:36:25,401 : INFO : adding document #0 to Dictionary(168721 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,409 : INFO : built Dictionary(168778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 926 documents (total 9260000 corpus positions)
    2021-05-05 22:36:25,453 : INFO : adding document #0 to Dictionary(168778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,460 : INFO : built Dictionary(168853 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 927 documents (total 9270000 corpus positions)
    2021-05-05 22:36:25,505 : INFO : adding document #0 to Dictionary(168853 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,513 : INFO : built Dictionary(168988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 928 documents (total 9280000 corpus positions)
    2021-05-05 22:36:25,557 : INFO : adding document #0 to Dictionary(168988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,565 : INFO : built Dictionary(169048 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 929 documents (total 9290000 corpus positions)
    2021-05-05 22:36:25,609 : INFO : adding document #0 to Dictionary(169048 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,618 : INFO : built Dictionary(169212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 930 documents (total 9300000 corpus positions)
    2021-05-05 22:36:25,662 : INFO : adding document #0 to Dictionary(169212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,671 : INFO : built Dictionary(169560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 931 documents (total 9310000 corpus positions)
    2021-05-05 22:36:25,716 : INFO : adding document #0 to Dictionary(169560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,724 : INFO : built Dictionary(169635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 932 documents (total 9320000 corpus positions)
    2021-05-05 22:36:25,771 : INFO : adding document #0 to Dictionary(169635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,779 : INFO : built Dictionary(169735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 933 documents (total 9330000 corpus positions)
    2021-05-05 22:36:25,826 : INFO : adding document #0 to Dictionary(169735 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,834 : INFO : built Dictionary(169786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 934 documents (total 9340000 corpus positions)
    2021-05-05 22:36:25,877 : INFO : adding document #0 to Dictionary(169786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,885 : INFO : built Dictionary(169915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 935 documents (total 9350000 corpus positions)
    2021-05-05 22:36:25,931 : INFO : adding document #0 to Dictionary(169915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,939 : INFO : built Dictionary(169988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 936 documents (total 9360000 corpus positions)
    2021-05-05 22:36:25,981 : INFO : adding document #0 to Dictionary(169988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:25,989 : INFO : built Dictionary(170052 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 937 documents (total 9370000 corpus positions)
    2021-05-05 22:36:26,036 : INFO : adding document #0 to Dictionary(170052 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,043 : INFO : built Dictionary(170183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 938 documents (total 9380000 corpus positions)
    2021-05-05 22:36:26,088 : INFO : adding document #0 to Dictionary(170183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,096 : INFO : built Dictionary(170302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 939 documents (total 9390000 corpus positions)
    2021-05-05 22:36:26,142 : INFO : adding document #0 to Dictionary(170302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,149 : INFO : built Dictionary(170383 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 940 documents (total 9400000 corpus positions)
    2021-05-05 22:36:26,194 : INFO : adding document #0 to Dictionary(170383 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,202 : INFO : built Dictionary(170473 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 941 documents (total 9410000 corpus positions)
    2021-05-05 22:36:26,245 : INFO : adding document #0 to Dictionary(170473 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,253 : INFO : built Dictionary(170632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 942 documents (total 9420000 corpus positions)
    2021-05-05 22:36:26,296 : INFO : adding document #0 to Dictionary(170632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,306 : INFO : built Dictionary(170831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 943 documents (total 9430000 corpus positions)
    2021-05-05 22:36:26,353 : INFO : adding document #0 to Dictionary(170831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,361 : INFO : built Dictionary(170883 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 944 documents (total 9440000 corpus positions)
    2021-05-05 22:36:26,406 : INFO : adding document #0 to Dictionary(170883 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,414 : INFO : built Dictionary(170973 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 945 documents (total 9450000 corpus positions)
    2021-05-05 22:36:26,457 : INFO : adding document #0 to Dictionary(170973 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,464 : INFO : built Dictionary(171010 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 946 documents (total 9460000 corpus positions)
    2021-05-05 22:36:26,507 : INFO : adding document #0 to Dictionary(171010 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,515 : INFO : built Dictionary(171090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 947 documents (total 9470000 corpus positions)
    2021-05-05 22:36:26,558 : INFO : adding document #0 to Dictionary(171090 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,567 : INFO : built Dictionary(171152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 948 documents (total 9480000 corpus positions)
    2021-05-05 22:36:26,614 : INFO : adding document #0 to Dictionary(171152 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,623 : INFO : built Dictionary(171217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 949 documents (total 9490000 corpus positions)
    2021-05-05 22:36:26,666 : INFO : adding document #0 to Dictionary(171217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,673 : INFO : built Dictionary(171282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 950 documents (total 9500000 corpus positions)
    2021-05-05 22:36:26,717 : INFO : adding document #0 to Dictionary(171282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,725 : INFO : built Dictionary(171327 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 951 documents (total 9510000 corpus positions)
    2021-05-05 22:36:26,770 : INFO : adding document #0 to Dictionary(171327 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,778 : INFO : built Dictionary(171401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 952 documents (total 9520000 corpus positions)
    2021-05-05 22:36:26,821 : INFO : adding document #0 to Dictionary(171401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,829 : INFO : built Dictionary(171521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 953 documents (total 9530000 corpus positions)
    2021-05-05 22:36:26,871 : INFO : adding document #0 to Dictionary(171521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,879 : INFO : built Dictionary(171655 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 954 documents (total 9540000 corpus positions)
    2021-05-05 22:36:26,926 : INFO : adding document #0 to Dictionary(171655 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,933 : INFO : built Dictionary(171709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 955 documents (total 9550000 corpus positions)
    2021-05-05 22:36:26,976 : INFO : adding document #0 to Dictionary(171709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:26,983 : INFO : built Dictionary(171774 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 956 documents (total 9560000 corpus positions)
    2021-05-05 22:36:27,028 : INFO : adding document #0 to Dictionary(171774 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,035 : INFO : built Dictionary(171840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 957 documents (total 9570000 corpus positions)
    2021-05-05 22:36:27,078 : INFO : adding document #0 to Dictionary(171840 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,086 : INFO : built Dictionary(171897 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 958 documents (total 9580000 corpus positions)
    2021-05-05 22:36:27,130 : INFO : adding document #0 to Dictionary(171897 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,137 : INFO : built Dictionary(171960 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 959 documents (total 9590000 corpus positions)
    2021-05-05 22:36:27,180 : INFO : adding document #0 to Dictionary(171960 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,187 : INFO : built Dictionary(172106 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 960 documents (total 9600000 corpus positions)
    2021-05-05 22:36:27,231 : INFO : adding document #0 to Dictionary(172106 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,239 : INFO : built Dictionary(172209 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 961 documents (total 9610000 corpus positions)
    2021-05-05 22:36:27,283 : INFO : adding document #0 to Dictionary(172209 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,290 : INFO : built Dictionary(172302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 962 documents (total 9620000 corpus positions)
    2021-05-05 22:36:27,335 : INFO : adding document #0 to Dictionary(172302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,345 : INFO : built Dictionary(172399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 963 documents (total 9630000 corpus positions)
    2021-05-05 22:36:27,390 : INFO : adding document #0 to Dictionary(172399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,397 : INFO : built Dictionary(172454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 964 documents (total 9640000 corpus positions)
    2021-05-05 22:36:27,445 : INFO : adding document #0 to Dictionary(172454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,452 : INFO : built Dictionary(172553 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 965 documents (total 9650000 corpus positions)
    2021-05-05 22:36:27,496 : INFO : adding document #0 to Dictionary(172553 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,503 : INFO : built Dictionary(172662 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 966 documents (total 9660000 corpus positions)
    2021-05-05 22:36:27,546 : INFO : adding document #0 to Dictionary(172662 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,555 : INFO : built Dictionary(172748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 967 documents (total 9670000 corpus positions)
    2021-05-05 22:36:27,603 : INFO : adding document #0 to Dictionary(172748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,610 : INFO : built Dictionary(172833 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 968 documents (total 9680000 corpus positions)
    2021-05-05 22:36:27,655 : INFO : adding document #0 to Dictionary(172833 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,662 : INFO : built Dictionary(172865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 969 documents (total 9690000 corpus positions)
    2021-05-05 22:36:27,707 : INFO : adding document #0 to Dictionary(172865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,715 : INFO : built Dictionary(172960 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 970 documents (total 9700000 corpus positions)
    2021-05-05 22:36:27,760 : INFO : adding document #0 to Dictionary(172960 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,768 : INFO : built Dictionary(173046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 971 documents (total 9710000 corpus positions)
    2021-05-05 22:36:27,811 : INFO : adding document #0 to Dictionary(173046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,820 : INFO : built Dictionary(173146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 972 documents (total 9720000 corpus positions)
    2021-05-05 22:36:27,867 : INFO : adding document #0 to Dictionary(173146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,875 : INFO : built Dictionary(173215 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 973 documents (total 9730000 corpus positions)
    2021-05-05 22:36:27,918 : INFO : adding document #0 to Dictionary(173215 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,926 : INFO : built Dictionary(173290 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 974 documents (total 9740000 corpus positions)
    2021-05-05 22:36:27,969 : INFO : adding document #0 to Dictionary(173290 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:27,977 : INFO : built Dictionary(173468 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 975 documents (total 9750000 corpus positions)
    2021-05-05 22:36:28,023 : INFO : adding document #0 to Dictionary(173468 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,031 : INFO : built Dictionary(173609 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 976 documents (total 9760000 corpus positions)
    2021-05-05 22:36:28,074 : INFO : adding document #0 to Dictionary(173609 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,081 : INFO : built Dictionary(173682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 977 documents (total 9770000 corpus positions)
    2021-05-05 22:36:28,125 : INFO : adding document #0 to Dictionary(173682 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,132 : INFO : built Dictionary(173723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 978 documents (total 9780000 corpus positions)
    2021-05-05 22:36:28,175 : INFO : adding document #0 to Dictionary(173723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,183 : INFO : built Dictionary(173808 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 979 documents (total 9790000 corpus positions)
    2021-05-05 22:36:28,232 : INFO : adding document #0 to Dictionary(173808 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,240 : INFO : built Dictionary(173997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 980 documents (total 9800000 corpus positions)
    2021-05-05 22:36:28,288 : INFO : adding document #0 to Dictionary(173997 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,296 : INFO : built Dictionary(174089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 981 documents (total 9810000 corpus positions)
    2021-05-05 22:36:28,339 : INFO : adding document #0 to Dictionary(174089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,346 : INFO : built Dictionary(174151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 982 documents (total 9820000 corpus positions)
    2021-05-05 22:36:28,390 : INFO : adding document #0 to Dictionary(174151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,397 : INFO : built Dictionary(174227 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 983 documents (total 9830000 corpus positions)
    2021-05-05 22:36:28,441 : INFO : adding document #0 to Dictionary(174227 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,447 : INFO : built Dictionary(174288 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 984 documents (total 9840000 corpus positions)
    2021-05-05 22:36:28,494 : INFO : adding document #0 to Dictionary(174288 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,504 : INFO : built Dictionary(174431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 985 documents (total 9850000 corpus positions)
    2021-05-05 22:36:28,549 : INFO : adding document #0 to Dictionary(174431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,557 : INFO : built Dictionary(174652 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 986 documents (total 9860000 corpus positions)
    2021-05-05 22:36:28,607 : INFO : adding document #0 to Dictionary(174652 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,642 : INFO : built Dictionary(174763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 987 documents (total 9870000 corpus positions)
    2021-05-05 22:36:28,690 : INFO : adding document #0 to Dictionary(174763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,699 : INFO : built Dictionary(174956 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 988 documents (total 9880000 corpus positions)
    2021-05-05 22:36:28,742 : INFO : adding document #0 to Dictionary(174956 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,751 : INFO : built Dictionary(175029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 989 documents (total 9890000 corpus positions)
    2021-05-05 22:36:28,798 : INFO : adding document #0 to Dictionary(175029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,807 : INFO : built Dictionary(175097 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 990 documents (total 9900000 corpus positions)
    2021-05-05 22:36:28,856 : INFO : adding document #0 to Dictionary(175097 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,864 : INFO : built Dictionary(175193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 991 documents (total 9910000 corpus positions)
    2021-05-05 22:36:28,911 : INFO : adding document #0 to Dictionary(175193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,919 : INFO : built Dictionary(175351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 992 documents (total 9920000 corpus positions)
    2021-05-05 22:36:28,968 : INFO : adding document #0 to Dictionary(175351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:28,975 : INFO : built Dictionary(175452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 993 documents (total 9930000 corpus positions)
    2021-05-05 22:36:29,025 : INFO : adding document #0 to Dictionary(175452 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,034 : INFO : built Dictionary(175521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 994 documents (total 9940000 corpus positions)
    2021-05-05 22:36:29,081 : INFO : adding document #0 to Dictionary(175521 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,089 : INFO : built Dictionary(175581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 995 documents (total 9950000 corpus positions)
    2021-05-05 22:36:29,135 : INFO : adding document #0 to Dictionary(175581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,143 : INFO : built Dictionary(175716 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 996 documents (total 9960000 corpus positions)
    2021-05-05 22:36:29,188 : INFO : adding document #0 to Dictionary(175716 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,196 : INFO : built Dictionary(175801 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 997 documents (total 9970000 corpus positions)
    2021-05-05 22:36:29,244 : INFO : adding document #0 to Dictionary(175801 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,253 : INFO : built Dictionary(175904 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 998 documents (total 9980000 corpus positions)
    2021-05-05 22:36:29,298 : INFO : adding document #0 to Dictionary(175904 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,306 : INFO : built Dictionary(175970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 999 documents (total 9990000 corpus positions)
    2021-05-05 22:36:29,351 : INFO : adding document #0 to Dictionary(175970 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,359 : INFO : built Dictionary(176030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1000 documents (total 10000000 corpus positions)
    2021-05-05 22:36:29,406 : INFO : adding document #0 to Dictionary(176030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,413 : INFO : built Dictionary(176089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1001 documents (total 10010000 corpus positions)
    2021-05-05 22:36:29,461 : INFO : adding document #0 to Dictionary(176089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,470 : INFO : built Dictionary(176186 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1002 documents (total 10020000 corpus positions)
    2021-05-05 22:36:29,518 : INFO : adding document #0 to Dictionary(176186 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,526 : INFO : built Dictionary(176324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1003 documents (total 10030000 corpus positions)
    2021-05-05 22:36:29,570 : INFO : adding document #0 to Dictionary(176324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,579 : INFO : built Dictionary(176552 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1004 documents (total 10040000 corpus positions)
    2021-05-05 22:36:29,622 : INFO : adding document #0 to Dictionary(176552 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,630 : INFO : built Dictionary(176635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1005 documents (total 10050000 corpus positions)
    2021-05-05 22:36:29,675 : INFO : adding document #0 to Dictionary(176635 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,684 : INFO : built Dictionary(176865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1006 documents (total 10060000 corpus positions)
    2021-05-05 22:36:29,731 : INFO : adding document #0 to Dictionary(176865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,740 : INFO : built Dictionary(176990 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1007 documents (total 10070000 corpus positions)
    2021-05-05 22:36:29,785 : INFO : adding document #0 to Dictionary(176990 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,792 : INFO : built Dictionary(177047 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1008 documents (total 10080000 corpus positions)
    2021-05-05 22:36:29,842 : INFO : adding document #0 to Dictionary(177047 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,850 : INFO : built Dictionary(177105 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1009 documents (total 10090000 corpus positions)
    2021-05-05 22:36:29,895 : INFO : adding document #0 to Dictionary(177105 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,904 : INFO : built Dictionary(177166 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1010 documents (total 10100000 corpus positions)
    2021-05-05 22:36:29,952 : INFO : adding document #0 to Dictionary(177166 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:29,960 : INFO : built Dictionary(177229 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1011 documents (total 10110000 corpus positions)
    2021-05-05 22:36:30,011 : INFO : adding document #0 to Dictionary(177229 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,021 : INFO : built Dictionary(177361 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1012 documents (total 10120000 corpus positions)
    2021-05-05 22:36:30,065 : INFO : adding document #0 to Dictionary(177361 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,073 : INFO : built Dictionary(177464 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1013 documents (total 10130000 corpus positions)
    2021-05-05 22:36:30,117 : INFO : adding document #0 to Dictionary(177464 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,124 : INFO : built Dictionary(177539 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1014 documents (total 10140000 corpus positions)
    2021-05-05 22:36:30,168 : INFO : adding document #0 to Dictionary(177539 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,177 : INFO : built Dictionary(177679 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1015 documents (total 10150000 corpus positions)
    2021-05-05 22:36:30,221 : INFO : adding document #0 to Dictionary(177679 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,229 : INFO : built Dictionary(177779 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1016 documents (total 10160000 corpus positions)
    2021-05-05 22:36:30,278 : INFO : adding document #0 to Dictionary(177779 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,287 : INFO : built Dictionary(177926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1017 documents (total 10170000 corpus positions)
    2021-05-05 22:36:30,332 : INFO : adding document #0 to Dictionary(177926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,340 : INFO : built Dictionary(178018 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1018 documents (total 10180000 corpus positions)
    2021-05-05 22:36:30,384 : INFO : adding document #0 to Dictionary(178018 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,392 : INFO : built Dictionary(178104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1019 documents (total 10190000 corpus positions)
    2021-05-05 22:36:30,440 : INFO : adding document #0 to Dictionary(178104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,448 : INFO : built Dictionary(178160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1020 documents (total 10200000 corpus positions)
    2021-05-05 22:36:30,492 : INFO : adding document #0 to Dictionary(178160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,500 : INFO : built Dictionary(178258 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1021 documents (total 10210000 corpus positions)
    2021-05-05 22:36:30,543 : INFO : adding document #0 to Dictionary(178258 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,551 : INFO : built Dictionary(178324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1022 documents (total 10220000 corpus positions)
    2021-05-05 22:36:30,594 : INFO : adding document #0 to Dictionary(178324 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,601 : INFO : built Dictionary(178437 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1023 documents (total 10230000 corpus positions)
    2021-05-05 22:36:30,645 : INFO : adding document #0 to Dictionary(178437 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,652 : INFO : built Dictionary(178621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1024 documents (total 10240000 corpus positions)
    2021-05-05 22:36:30,695 : INFO : adding document #0 to Dictionary(178621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,703 : INFO : built Dictionary(178799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1025 documents (total 10250000 corpus positions)
    2021-05-05 22:36:30,749 : INFO : adding document #0 to Dictionary(178799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,758 : INFO : built Dictionary(178921 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1026 documents (total 10260000 corpus positions)
    2021-05-05 22:36:30,803 : INFO : adding document #0 to Dictionary(178921 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,812 : INFO : built Dictionary(178987 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1027 documents (total 10270000 corpus positions)
    2021-05-05 22:36:30,856 : INFO : adding document #0 to Dictionary(178987 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,864 : INFO : built Dictionary(179136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1028 documents (total 10280000 corpus positions)
    2021-05-05 22:36:30,910 : INFO : adding document #0 to Dictionary(179136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,919 : INFO : built Dictionary(179194 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1029 documents (total 10290000 corpus positions)
    2021-05-05 22:36:30,961 : INFO : adding document #0 to Dictionary(179194 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:30,970 : INFO : built Dictionary(179426 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1030 documents (total 10300000 corpus positions)
    2021-05-05 22:36:31,017 : INFO : adding document #0 to Dictionary(179426 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,025 : INFO : built Dictionary(179492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1031 documents (total 10310000 corpus positions)
    2021-05-05 22:36:31,072 : INFO : adding document #0 to Dictionary(179492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,080 : INFO : built Dictionary(179582 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1032 documents (total 10320000 corpus positions)
    2021-05-05 22:36:31,124 : INFO : adding document #0 to Dictionary(179582 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,132 : INFO : built Dictionary(179647 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1033 documents (total 10330000 corpus positions)
    2021-05-05 22:36:31,175 : INFO : adding document #0 to Dictionary(179647 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,183 : INFO : built Dictionary(179754 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1034 documents (total 10340000 corpus positions)
    2021-05-05 22:36:31,226 : INFO : adding document #0 to Dictionary(179754 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,234 : INFO : built Dictionary(179865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1035 documents (total 10350000 corpus positions)
    2021-05-05 22:36:31,277 : INFO : adding document #0 to Dictionary(179865 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,285 : INFO : built Dictionary(179969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1036 documents (total 10360000 corpus positions)
    2021-05-05 22:36:31,329 : INFO : adding document #0 to Dictionary(179969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,337 : INFO : built Dictionary(180029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1037 documents (total 10370000 corpus positions)
    2021-05-05 22:36:31,380 : INFO : adding document #0 to Dictionary(180029 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,389 : INFO : built Dictionary(180177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1038 documents (total 10380000 corpus positions)
    2021-05-05 22:36:31,437 : INFO : adding document #0 to Dictionary(180177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,445 : INFO : built Dictionary(180300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1039 documents (total 10390000 corpus positions)
    2021-05-05 22:36:31,488 : INFO : adding document #0 to Dictionary(180300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,496 : INFO : built Dictionary(180374 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1040 documents (total 10400000 corpus positions)
    2021-05-05 22:36:31,539 : INFO : adding document #0 to Dictionary(180374 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,547 : INFO : built Dictionary(180448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1041 documents (total 10410000 corpus positions)
    2021-05-05 22:36:31,590 : INFO : adding document #0 to Dictionary(180448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,597 : INFO : built Dictionary(180502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1042 documents (total 10420000 corpus positions)
    2021-05-05 22:36:31,641 : INFO : adding document #0 to Dictionary(180502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,648 : INFO : built Dictionary(180604 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1043 documents (total 10430000 corpus positions)
    2021-05-05 22:36:31,690 : INFO : adding document #0 to Dictionary(180604 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,697 : INFO : built Dictionary(180663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1044 documents (total 10440000 corpus positions)
    2021-05-05 22:36:31,741 : INFO : adding document #0 to Dictionary(180663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,749 : INFO : built Dictionary(180710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1045 documents (total 10450000 corpus positions)
    2021-05-05 22:36:31,793 : INFO : adding document #0 to Dictionary(180710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,800 : INFO : built Dictionary(180784 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1046 documents (total 10460000 corpus positions)
    2021-05-05 22:36:31,844 : INFO : adding document #0 to Dictionary(180784 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,853 : INFO : built Dictionary(180913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1047 documents (total 10470000 corpus positions)
    2021-05-05 22:36:31,896 : INFO : adding document #0 to Dictionary(180913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,904 : INFO : built Dictionary(181036 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1048 documents (total 10480000 corpus positions)
    2021-05-05 22:36:31,948 : INFO : adding document #0 to Dictionary(181036 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:31,955 : INFO : built Dictionary(181158 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1049 documents (total 10490000 corpus positions)
    2021-05-05 22:36:31,999 : INFO : adding document #0 to Dictionary(181158 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,007 : INFO : built Dictionary(181243 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1050 documents (total 10500000 corpus positions)
    2021-05-05 22:36:32,052 : INFO : adding document #0 to Dictionary(181243 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,060 : INFO : built Dictionary(181319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1051 documents (total 10510000 corpus positions)
    2021-05-05 22:36:32,104 : INFO : adding document #0 to Dictionary(181319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,111 : INFO : built Dictionary(181418 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1052 documents (total 10520000 corpus positions)
    2021-05-05 22:36:32,159 : INFO : adding document #0 to Dictionary(181418 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,167 : INFO : built Dictionary(181515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1053 documents (total 10530000 corpus positions)
    2021-05-05 22:36:32,210 : INFO : adding document #0 to Dictionary(181515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,219 : INFO : built Dictionary(181595 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1054 documents (total 10540000 corpus positions)
    2021-05-05 22:36:32,264 : INFO : adding document #0 to Dictionary(181595 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,272 : INFO : built Dictionary(181669 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1055 documents (total 10550000 corpus positions)
    2021-05-05 22:36:32,318 : INFO : adding document #0 to Dictionary(181669 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,326 : INFO : built Dictionary(181767 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1056 documents (total 10560000 corpus positions)
    2021-05-05 22:36:32,369 : INFO : adding document #0 to Dictionary(181767 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,377 : INFO : built Dictionary(181806 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1057 documents (total 10570000 corpus positions)
    2021-05-05 22:36:32,421 : INFO : adding document #0 to Dictionary(181806 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,428 : INFO : built Dictionary(181838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1058 documents (total 10580000 corpus positions)
    2021-05-05 22:36:32,471 : INFO : adding document #0 to Dictionary(181838 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,481 : INFO : built Dictionary(181946 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1059 documents (total 10590000 corpus positions)
    2021-05-05 22:36:32,527 : INFO : adding document #0 to Dictionary(181946 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,535 : INFO : built Dictionary(182005 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1060 documents (total 10600000 corpus positions)
    2021-05-05 22:36:32,578 : INFO : adding document #0 to Dictionary(182005 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,586 : INFO : built Dictionary(182107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1061 documents (total 10610000 corpus positions)
    2021-05-05 22:36:32,635 : INFO : adding document #0 to Dictionary(182107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,645 : INFO : built Dictionary(182170 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1062 documents (total 10620000 corpus positions)
    2021-05-05 22:36:32,691 : INFO : adding document #0 to Dictionary(182170 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,699 : INFO : built Dictionary(182245 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1063 documents (total 10630000 corpus positions)
    2021-05-05 22:36:32,743 : INFO : adding document #0 to Dictionary(182245 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,753 : INFO : built Dictionary(182368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1064 documents (total 10640000 corpus positions)
    2021-05-05 22:36:32,798 : INFO : adding document #0 to Dictionary(182368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,806 : INFO : built Dictionary(182440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1065 documents (total 10650000 corpus positions)
    2021-05-05 22:36:32,854 : INFO : adding document #0 to Dictionary(182440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,862 : INFO : built Dictionary(182532 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1066 documents (total 10660000 corpus positions)
    2021-05-05 22:36:32,905 : INFO : adding document #0 to Dictionary(182532 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,913 : INFO : built Dictionary(182639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1067 documents (total 10670000 corpus positions)
    2021-05-05 22:36:32,956 : INFO : adding document #0 to Dictionary(182639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:32,963 : INFO : built Dictionary(182711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1068 documents (total 10680000 corpus positions)
    2021-05-05 22:36:33,007 : INFO : adding document #0 to Dictionary(182711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,016 : INFO : built Dictionary(182821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1069 documents (total 10690000 corpus positions)
    2021-05-05 22:36:33,062 : INFO : adding document #0 to Dictionary(182821 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,070 : INFO : built Dictionary(182922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1070 documents (total 10700000 corpus positions)
    2021-05-05 22:36:33,118 : INFO : adding document #0 to Dictionary(182922 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,126 : INFO : built Dictionary(183001 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1071 documents (total 10710000 corpus positions)
    2021-05-05 22:36:33,169 : INFO : adding document #0 to Dictionary(183001 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,177 : INFO : built Dictionary(183050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1072 documents (total 10720000 corpus positions)
    2021-05-05 22:36:33,227 : INFO : adding document #0 to Dictionary(183050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,240 : INFO : built Dictionary(183151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1073 documents (total 10730000 corpus positions)
    2021-05-05 22:36:33,292 : INFO : adding document #0 to Dictionary(183151 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,308 : INFO : built Dictionary(183292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1074 documents (total 10740000 corpus positions)
    2021-05-05 22:36:33,359 : INFO : adding document #0 to Dictionary(183292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,371 : INFO : built Dictionary(183360 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1075 documents (total 10750000 corpus positions)
    2021-05-05 22:36:33,427 : INFO : adding document #0 to Dictionary(183360 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,435 : INFO : built Dictionary(183431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1076 documents (total 10760000 corpus positions)
    2021-05-05 22:36:33,482 : INFO : adding document #0 to Dictionary(183431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,490 : INFO : built Dictionary(183517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1077 documents (total 10770000 corpus positions)
    2021-05-05 22:36:33,534 : INFO : adding document #0 to Dictionary(183517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,542 : INFO : built Dictionary(183590 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1078 documents (total 10780000 corpus positions)
    2021-05-05 22:36:33,585 : INFO : adding document #0 to Dictionary(183590 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,592 : INFO : built Dictionary(183672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1079 documents (total 10790000 corpus positions)
    2021-05-05 22:36:33,635 : INFO : adding document #0 to Dictionary(183672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,643 : INFO : built Dictionary(183713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1080 documents (total 10800000 corpus positions)
    2021-05-05 22:36:33,686 : INFO : adding document #0 to Dictionary(183713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,693 : INFO : built Dictionary(183790 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1081 documents (total 10810000 corpus positions)
    2021-05-05 22:36:33,737 : INFO : adding document #0 to Dictionary(183790 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,745 : INFO : built Dictionary(183877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1082 documents (total 10820000 corpus positions)
    2021-05-05 22:36:33,788 : INFO : adding document #0 to Dictionary(183877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,795 : INFO : built Dictionary(183953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1083 documents (total 10830000 corpus positions)
    2021-05-05 22:36:33,839 : INFO : adding document #0 to Dictionary(183953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,846 : INFO : built Dictionary(183980 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1084 documents (total 10840000 corpus positions)
    2021-05-05 22:36:33,890 : INFO : adding document #0 to Dictionary(183980 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,898 : INFO : built Dictionary(184116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1085 documents (total 10850000 corpus positions)
    2021-05-05 22:36:33,941 : INFO : adding document #0 to Dictionary(184116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:33,948 : INFO : built Dictionary(184269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1086 documents (total 10860000 corpus positions)
    2021-05-05 22:36:34,000 : INFO : adding document #0 to Dictionary(184269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,007 : INFO : built Dictionary(184310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1087 documents (total 10870000 corpus positions)
    2021-05-05 22:36:34,050 : INFO : adding document #0 to Dictionary(184310 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,057 : INFO : built Dictionary(184402 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1088 documents (total 10880000 corpus positions)
    2021-05-05 22:36:34,101 : INFO : adding document #0 to Dictionary(184402 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,108 : INFO : built Dictionary(184449 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1089 documents (total 10890000 corpus positions)
    2021-05-05 22:36:34,152 : INFO : adding document #0 to Dictionary(184449 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,159 : INFO : built Dictionary(184561 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1090 documents (total 10900000 corpus positions)
    2021-05-05 22:36:34,203 : INFO : adding document #0 to Dictionary(184561 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,210 : INFO : built Dictionary(184650 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1091 documents (total 10910000 corpus positions)
    2021-05-05 22:36:34,254 : INFO : adding document #0 to Dictionary(184650 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,262 : INFO : built Dictionary(184742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1092 documents (total 10920000 corpus positions)
    2021-05-05 22:36:34,305 : INFO : adding document #0 to Dictionary(184742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,313 : INFO : built Dictionary(184831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1093 documents (total 10930000 corpus positions)
    2021-05-05 22:36:34,356 : INFO : adding document #0 to Dictionary(184831 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,363 : INFO : built Dictionary(184940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1094 documents (total 10940000 corpus positions)
    2021-05-05 22:36:34,406 : INFO : adding document #0 to Dictionary(184940 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,414 : INFO : built Dictionary(184987 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1095 documents (total 10950000 corpus positions)
    2021-05-05 22:36:34,457 : INFO : adding document #0 to Dictionary(184987 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,465 : INFO : built Dictionary(185060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1096 documents (total 10960000 corpus positions)
    2021-05-05 22:36:34,508 : INFO : adding document #0 to Dictionary(185060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,515 : INFO : built Dictionary(185091 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1097 documents (total 10970000 corpus positions)
    2021-05-05 22:36:34,558 : INFO : adding document #0 to Dictionary(185091 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,564 : INFO : built Dictionary(185159 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1098 documents (total 10980000 corpus positions)
    2021-05-05 22:36:34,607 : INFO : adding document #0 to Dictionary(185159 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,615 : INFO : built Dictionary(185241 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1099 documents (total 10990000 corpus positions)
    2021-05-05 22:36:34,658 : INFO : adding document #0 to Dictionary(185241 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,665 : INFO : built Dictionary(185301 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1100 documents (total 11000000 corpus positions)
    2021-05-05 22:36:34,708 : INFO : adding document #0 to Dictionary(185301 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,715 : INFO : built Dictionary(185363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1101 documents (total 11010000 corpus positions)
    2021-05-05 22:36:34,758 : INFO : adding document #0 to Dictionary(185363 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,766 : INFO : built Dictionary(185393 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1102 documents (total 11020000 corpus positions)
    2021-05-05 22:36:34,813 : INFO : adding document #0 to Dictionary(185393 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,820 : INFO : built Dictionary(185519 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1103 documents (total 11030000 corpus positions)
    2021-05-05 22:36:34,868 : INFO : adding document #0 to Dictionary(185519 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,875 : INFO : built Dictionary(185617 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1104 documents (total 11040000 corpus positions)
    2021-05-05 22:36:34,918 : INFO : adding document #0 to Dictionary(185617 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,924 : INFO : built Dictionary(185654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1105 documents (total 11050000 corpus positions)
    2021-05-05 22:36:34,967 : INFO : adding document #0 to Dictionary(185654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:34,974 : INFO : built Dictionary(185693 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1106 documents (total 11060000 corpus positions)
    2021-05-05 22:36:35,031 : INFO : adding document #0 to Dictionary(185693 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,040 : INFO : built Dictionary(185845 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1107 documents (total 11070000 corpus positions)
    2021-05-05 22:36:35,090 : INFO : adding document #0 to Dictionary(185845 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,100 : INFO : built Dictionary(185981 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1108 documents (total 11080000 corpus positions)
    2021-05-05 22:36:35,148 : INFO : adding document #0 to Dictionary(185981 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,157 : INFO : built Dictionary(186066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1109 documents (total 11090000 corpus positions)
    2021-05-05 22:36:35,206 : INFO : adding document #0 to Dictionary(186066 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,215 : INFO : built Dictionary(186111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1110 documents (total 11100000 corpus positions)
    2021-05-05 22:36:35,260 : INFO : adding document #0 to Dictionary(186111 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,267 : INFO : built Dictionary(186195 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1111 documents (total 11110000 corpus positions)
    2021-05-05 22:36:35,311 : INFO : adding document #0 to Dictionary(186195 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,318 : INFO : built Dictionary(186415 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1112 documents (total 11120000 corpus positions)
    2021-05-05 22:36:35,366 : INFO : adding document #0 to Dictionary(186415 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,374 : INFO : built Dictionary(186557 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1113 documents (total 11130000 corpus positions)
    2021-05-05 22:36:35,423 : INFO : adding document #0 to Dictionary(186557 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,430 : INFO : built Dictionary(186602 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1114 documents (total 11140000 corpus positions)
    2021-05-05 22:36:35,473 : INFO : adding document #0 to Dictionary(186602 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,481 : INFO : built Dictionary(186785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1115 documents (total 11150000 corpus positions)
    2021-05-05 22:36:35,525 : INFO : adding document #0 to Dictionary(186785 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,533 : INFO : built Dictionary(186905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1116 documents (total 11160000 corpus positions)
    2021-05-05 22:36:35,577 : INFO : adding document #0 to Dictionary(186905 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,585 : INFO : built Dictionary(186993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1117 documents (total 11170000 corpus positions)
    2021-05-05 22:36:35,629 : INFO : adding document #0 to Dictionary(186993 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,637 : INFO : built Dictionary(187070 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1118 documents (total 11180000 corpus positions)
    2021-05-05 22:36:35,679 : INFO : adding document #0 to Dictionary(187070 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,687 : INFO : built Dictionary(187175 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1119 documents (total 11190000 corpus positions)
    2021-05-05 22:36:35,731 : INFO : adding document #0 to Dictionary(187175 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,738 : INFO : built Dictionary(187230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1120 documents (total 11200000 corpus positions)
    2021-05-05 22:36:35,781 : INFO : adding document #0 to Dictionary(187230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,789 : INFO : built Dictionary(187314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1121 documents (total 11210000 corpus positions)
    2021-05-05 22:36:35,837 : INFO : adding document #0 to Dictionary(187314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,844 : INFO : built Dictionary(187354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1122 documents (total 11220000 corpus positions)
    2021-05-05 22:36:35,891 : INFO : adding document #0 to Dictionary(187354 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,899 : INFO : built Dictionary(187475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1123 documents (total 11230000 corpus positions)
    2021-05-05 22:36:35,947 : INFO : adding document #0 to Dictionary(187475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:35,956 : INFO : built Dictionary(187596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1124 documents (total 11240000 corpus positions)
    2021-05-05 22:36:35,999 : INFO : adding document #0 to Dictionary(187596 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,007 : INFO : built Dictionary(187654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1125 documents (total 11250000 corpus positions)
    2021-05-05 22:36:36,052 : INFO : adding document #0 to Dictionary(187654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,062 : INFO : built Dictionary(187746 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1126 documents (total 11260000 corpus positions)
    2021-05-05 22:36:36,106 : INFO : adding document #0 to Dictionary(187746 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,114 : INFO : built Dictionary(187810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1127 documents (total 11270000 corpus positions)
    2021-05-05 22:36:36,157 : INFO : adding document #0 to Dictionary(187810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,164 : INFO : built Dictionary(187869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1128 documents (total 11280000 corpus positions)
    2021-05-05 22:36:36,207 : INFO : adding document #0 to Dictionary(187869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,216 : INFO : built Dictionary(187984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1129 documents (total 11290000 corpus positions)
    2021-05-05 22:36:36,258 : INFO : adding document #0 to Dictionary(187984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,266 : INFO : built Dictionary(188030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1130 documents (total 11300000 corpus positions)
    2021-05-05 22:36:36,309 : INFO : adding document #0 to Dictionary(188030 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,317 : INFO : built Dictionary(188116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1131 documents (total 11310000 corpus positions)
    2021-05-05 22:36:36,360 : INFO : adding document #0 to Dictionary(188116 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,367 : INFO : built Dictionary(188203 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1132 documents (total 11320000 corpus positions)
    2021-05-05 22:36:36,410 : INFO : adding document #0 to Dictionary(188203 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,418 : INFO : built Dictionary(188330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1133 documents (total 11330000 corpus positions)
    2021-05-05 22:36:36,461 : INFO : adding document #0 to Dictionary(188330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,468 : INFO : built Dictionary(188377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1134 documents (total 11340000 corpus positions)
    2021-05-05 22:36:36,511 : INFO : adding document #0 to Dictionary(188377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,518 : INFO : built Dictionary(188429 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1135 documents (total 11350000 corpus positions)
    2021-05-05 22:36:36,561 : INFO : adding document #0 to Dictionary(188429 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,569 : INFO : built Dictionary(188516 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1136 documents (total 11360000 corpus positions)
    2021-05-05 22:36:36,612 : INFO : adding document #0 to Dictionary(188516 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,620 : INFO : built Dictionary(188549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1137 documents (total 11370000 corpus positions)
    2021-05-05 22:36:36,663 : INFO : adding document #0 to Dictionary(188549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,670 : INFO : built Dictionary(188580 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1138 documents (total 11380000 corpus positions)
    2021-05-05 22:36:36,714 : INFO : adding document #0 to Dictionary(188580 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,722 : INFO : built Dictionary(188680 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1139 documents (total 11390000 corpus positions)
    2021-05-05 22:36:36,767 : INFO : adding document #0 to Dictionary(188680 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,775 : INFO : built Dictionary(188798 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1140 documents (total 11400000 corpus positions)
    2021-05-05 22:36:36,823 : INFO : adding document #0 to Dictionary(188798 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,831 : INFO : built Dictionary(188916 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1141 documents (total 11410000 corpus positions)
    2021-05-05 22:36:36,873 : INFO : adding document #0 to Dictionary(188916 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,881 : INFO : built Dictionary(189008 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1142 documents (total 11420000 corpus positions)
    2021-05-05 22:36:36,923 : INFO : adding document #0 to Dictionary(189008 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,930 : INFO : built Dictionary(189092 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1143 documents (total 11430000 corpus positions)
    2021-05-05 22:36:36,972 : INFO : adding document #0 to Dictionary(189092 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:36,980 : INFO : built Dictionary(189217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1144 documents (total 11440000 corpus positions)
    2021-05-05 22:36:37,025 : INFO : adding document #0 to Dictionary(189217 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,033 : INFO : built Dictionary(189321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1145 documents (total 11450000 corpus positions)
    2021-05-05 22:36:37,076 : INFO : adding document #0 to Dictionary(189321 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,085 : INFO : built Dictionary(189381 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1146 documents (total 11460000 corpus positions)
    2021-05-05 22:36:37,128 : INFO : adding document #0 to Dictionary(189381 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,136 : INFO : built Dictionary(189503 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1147 documents (total 11470000 corpus positions)
    2021-05-05 22:36:37,181 : INFO : adding document #0 to Dictionary(189503 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,189 : INFO : built Dictionary(189617 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1148 documents (total 11480000 corpus positions)
    2021-05-05 22:36:37,232 : INFO : adding document #0 to Dictionary(189617 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,240 : INFO : built Dictionary(189710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1149 documents (total 11490000 corpus positions)
    2021-05-05 22:36:37,283 : INFO : adding document #0 to Dictionary(189710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,291 : INFO : built Dictionary(189799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1150 documents (total 11500000 corpus positions)
    2021-05-05 22:36:37,333 : INFO : adding document #0 to Dictionary(189799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,341 : INFO : built Dictionary(189887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1151 documents (total 11510000 corpus positions)
    2021-05-05 22:36:37,384 : INFO : adding document #0 to Dictionary(189887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,392 : INFO : built Dictionary(189961 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1152 documents (total 11520000 corpus positions)
    2021-05-05 22:36:37,435 : INFO : adding document #0 to Dictionary(189961 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,442 : INFO : built Dictionary(190016 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1153 documents (total 11530000 corpus positions)
    2021-05-05 22:36:37,485 : INFO : adding document #0 to Dictionary(190016 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,493 : INFO : built Dictionary(190079 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1154 documents (total 11540000 corpus positions)
    2021-05-05 22:36:37,536 : INFO : adding document #0 to Dictionary(190079 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,543 : INFO : built Dictionary(190131 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1155 documents (total 11550000 corpus positions)
    2021-05-05 22:36:37,586 : INFO : adding document #0 to Dictionary(190131 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,594 : INFO : built Dictionary(190252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1156 documents (total 11560000 corpus positions)
    2021-05-05 22:36:37,636 : INFO : adding document #0 to Dictionary(190252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,642 : INFO : built Dictionary(190331 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1157 documents (total 11570000 corpus positions)
    2021-05-05 22:36:37,685 : INFO : adding document #0 to Dictionary(190331 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,693 : INFO : built Dictionary(190451 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1158 documents (total 11580000 corpus positions)
    2021-05-05 22:36:37,741 : INFO : adding document #0 to Dictionary(190451 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,749 : INFO : built Dictionary(190547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1159 documents (total 11590000 corpus positions)
    2021-05-05 22:36:37,792 : INFO : adding document #0 to Dictionary(190547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,799 : INFO : built Dictionary(190634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1160 documents (total 11600000 corpus positions)
    2021-05-05 22:36:37,847 : INFO : adding document #0 to Dictionary(190634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,854 : INFO : built Dictionary(190677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1161 documents (total 11610000 corpus positions)
    2021-05-05 22:36:37,899 : INFO : adding document #0 to Dictionary(190677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,909 : INFO : built Dictionary(190774 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1162 documents (total 11620000 corpus positions)
    2021-05-05 22:36:37,958 : INFO : adding document #0 to Dictionary(190774 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:37,971 : INFO : built Dictionary(190877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1163 documents (total 11630000 corpus positions)
    2021-05-05 22:36:38,022 : INFO : adding document #0 to Dictionary(190877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,030 : INFO : built Dictionary(191000 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1164 documents (total 11640000 corpus positions)
    2021-05-05 22:36:38,076 : INFO : adding document #0 to Dictionary(191000 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,085 : INFO : built Dictionary(191107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1165 documents (total 11650000 corpus positions)
    2021-05-05 22:36:38,132 : INFO : adding document #0 to Dictionary(191107 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,140 : INFO : built Dictionary(191212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1166 documents (total 11660000 corpus positions)
    2021-05-05 22:36:38,183 : INFO : adding document #0 to Dictionary(191212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,192 : INFO : built Dictionary(191312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1167 documents (total 11670000 corpus positions)
    2021-05-05 22:36:38,240 : INFO : adding document #0 to Dictionary(191312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,247 : INFO : built Dictionary(191401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1168 documents (total 11680000 corpus positions)
    2021-05-05 22:36:38,291 : INFO : adding document #0 to Dictionary(191401 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,298 : INFO : built Dictionary(191563 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1169 documents (total 11690000 corpus positions)
    2021-05-05 22:36:38,347 : INFO : adding document #0 to Dictionary(191563 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,355 : INFO : built Dictionary(191661 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1170 documents (total 11700000 corpus positions)
    2021-05-05 22:36:38,401 : INFO : adding document #0 to Dictionary(191661 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,408 : INFO : built Dictionary(191711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1171 documents (total 11710000 corpus positions)
    2021-05-05 22:36:38,452 : INFO : adding document #0 to Dictionary(191711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,459 : INFO : built Dictionary(191753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1172 documents (total 11720000 corpus positions)
    2021-05-05 22:36:38,507 : INFO : adding document #0 to Dictionary(191753 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,516 : INFO : built Dictionary(191872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1173 documents (total 11730000 corpus positions)
    2021-05-05 22:36:38,560 : INFO : adding document #0 to Dictionary(191872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,568 : INFO : built Dictionary(191957 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1174 documents (total 11740000 corpus positions)
    2021-05-05 22:36:38,612 : INFO : adding document #0 to Dictionary(191957 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,620 : INFO : built Dictionary(192034 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1175 documents (total 11750000 corpus positions)
    2021-05-05 22:36:38,665 : INFO : adding document #0 to Dictionary(192034 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,672 : INFO : built Dictionary(192144 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1176 documents (total 11760000 corpus positions)
    2021-05-05 22:36:38,720 : INFO : adding document #0 to Dictionary(192144 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,728 : INFO : built Dictionary(192256 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1177 documents (total 11770000 corpus positions)
    2021-05-05 22:36:38,775 : INFO : adding document #0 to Dictionary(192256 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,783 : INFO : built Dictionary(192355 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1178 documents (total 11780000 corpus positions)
    2021-05-05 22:36:38,831 : INFO : adding document #0 to Dictionary(192355 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,839 : INFO : built Dictionary(192448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1179 documents (total 11790000 corpus positions)
    2021-05-05 22:36:38,882 : INFO : adding document #0 to Dictionary(192448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,889 : INFO : built Dictionary(192549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1180 documents (total 11800000 corpus positions)
    2021-05-05 22:36:38,933 : INFO : adding document #0 to Dictionary(192549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,940 : INFO : built Dictionary(192642 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1181 documents (total 11810000 corpus positions)
    2021-05-05 22:36:38,989 : INFO : adding document #0 to Dictionary(192642 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:38,996 : INFO : built Dictionary(192728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1182 documents (total 11820000 corpus positions)
    2021-05-05 22:36:39,048 : INFO : adding document #0 to Dictionary(192728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,056 : INFO : built Dictionary(192761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1183 documents (total 11830000 corpus positions)
    2021-05-05 22:36:39,100 : INFO : adding document #0 to Dictionary(192761 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,107 : INFO : built Dictionary(192811 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1184 documents (total 11840000 corpus positions)
    2021-05-05 22:36:39,151 : INFO : adding document #0 to Dictionary(192811 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,158 : INFO : built Dictionary(192877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1185 documents (total 11850000 corpus positions)
    2021-05-05 22:36:39,202 : INFO : adding document #0 to Dictionary(192877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,213 : INFO : built Dictionary(193056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1186 documents (total 11860000 corpus positions)
    2021-05-05 22:36:39,256 : INFO : adding document #0 to Dictionary(193056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,263 : INFO : built Dictionary(193172 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1187 documents (total 11870000 corpus positions)
    2021-05-05 22:36:39,307 : INFO : adding document #0 to Dictionary(193172 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,315 : INFO : built Dictionary(193221 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1188 documents (total 11880000 corpus positions)
    2021-05-05 22:36:39,357 : INFO : adding document #0 to Dictionary(193221 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,365 : INFO : built Dictionary(193349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1189 documents (total 11890000 corpus positions)
    2021-05-05 22:36:39,408 : INFO : adding document #0 to Dictionary(193349 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,416 : INFO : built Dictionary(193410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1190 documents (total 11900000 corpus positions)
    2021-05-05 22:36:39,459 : INFO : adding document #0 to Dictionary(193410 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,466 : INFO : built Dictionary(193460 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1191 documents (total 11910000 corpus positions)
    2021-05-05 22:36:39,509 : INFO : adding document #0 to Dictionary(193460 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,517 : INFO : built Dictionary(193556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1192 documents (total 11920000 corpus positions)
    2021-05-05 22:36:39,560 : INFO : adding document #0 to Dictionary(193556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,568 : INFO : built Dictionary(193612 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1193 documents (total 11930000 corpus positions)
    2021-05-05 22:36:39,611 : INFO : adding document #0 to Dictionary(193612 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,618 : INFO : built Dictionary(193684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1194 documents (total 11940000 corpus positions)
    2021-05-05 22:36:39,661 : INFO : adding document #0 to Dictionary(193684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,668 : INFO : built Dictionary(193736 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1195 documents (total 11950000 corpus positions)
    2021-05-05 22:36:39,712 : INFO : adding document #0 to Dictionary(193736 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,720 : INFO : built Dictionary(193796 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1196 documents (total 11960000 corpus positions)
    2021-05-05 22:36:39,767 : INFO : adding document #0 to Dictionary(193796 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,775 : INFO : built Dictionary(193860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1197 documents (total 11970000 corpus positions)
    2021-05-05 22:36:39,818 : INFO : adding document #0 to Dictionary(193860 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,826 : INFO : built Dictionary(193933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1198 documents (total 11980000 corpus positions)
    2021-05-05 22:36:39,869 : INFO : adding document #0 to Dictionary(193933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,876 : INFO : built Dictionary(193972 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1199 documents (total 11990000 corpus positions)
    2021-05-05 22:36:39,920 : INFO : adding document #0 to Dictionary(193972 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,928 : INFO : built Dictionary(194050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1200 documents (total 12000000 corpus positions)
    2021-05-05 22:36:39,971 : INFO : adding document #0 to Dictionary(194050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:39,979 : INFO : built Dictionary(194113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1201 documents (total 12010000 corpus positions)
    2021-05-05 22:36:40,026 : INFO : adding document #0 to Dictionary(194113 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,034 : INFO : built Dictionary(194190 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1202 documents (total 12020000 corpus positions)
    2021-05-05 22:36:40,083 : INFO : adding document #0 to Dictionary(194190 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,090 : INFO : built Dictionary(194289 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1203 documents (total 12030000 corpus positions)
    2021-05-05 22:36:40,139 : INFO : adding document #0 to Dictionary(194289 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,147 : INFO : built Dictionary(194368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1204 documents (total 12040000 corpus positions)
    2021-05-05 22:36:40,192 : INFO : adding document #0 to Dictionary(194368 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,200 : INFO : built Dictionary(194437 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1205 documents (total 12050000 corpus positions)
    2021-05-05 22:36:40,244 : INFO : adding document #0 to Dictionary(194437 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,251 : INFO : built Dictionary(194487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1206 documents (total 12060000 corpus positions)
    2021-05-05 22:36:40,295 : INFO : adding document #0 to Dictionary(194487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,303 : INFO : built Dictionary(194537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1207 documents (total 12070000 corpus positions)
    2021-05-05 22:36:40,350 : INFO : adding document #0 to Dictionary(194537 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,359 : INFO : built Dictionary(194916 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1208 documents (total 12080000 corpus positions)
    2021-05-05 22:36:40,402 : INFO : adding document #0 to Dictionary(194916 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,409 : INFO : built Dictionary(195034 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1209 documents (total 12090000 corpus positions)
    2021-05-05 22:36:40,454 : INFO : adding document #0 to Dictionary(195034 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,463 : INFO : built Dictionary(195218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1210 documents (total 12100000 corpus positions)
    2021-05-05 22:36:40,513 : INFO : adding document #0 to Dictionary(195218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,521 : INFO : built Dictionary(195294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1211 documents (total 12110000 corpus positions)
    2021-05-05 22:36:40,565 : INFO : adding document #0 to Dictionary(195294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,573 : INFO : built Dictionary(195352 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1212 documents (total 12120000 corpus positions)
    2021-05-05 22:36:40,622 : INFO : adding document #0 to Dictionary(195352 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,631 : INFO : built Dictionary(195434 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1213 documents (total 12130000 corpus positions)
    2021-05-05 22:36:40,681 : INFO : adding document #0 to Dictionary(195434 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,688 : INFO : built Dictionary(195496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1214 documents (total 12140000 corpus positions)
    2021-05-05 22:36:40,734 : INFO : adding document #0 to Dictionary(195496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,741 : INFO : built Dictionary(195522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1215 documents (total 12150000 corpus positions)
    2021-05-05 22:36:40,789 : INFO : adding document #0 to Dictionary(195522 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,797 : INFO : built Dictionary(195714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1216 documents (total 12160000 corpus positions)
    2021-05-05 22:36:40,848 : INFO : adding document #0 to Dictionary(195714 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,856 : INFO : built Dictionary(195786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1217 documents (total 12170000 corpus positions)
    2021-05-05 22:36:40,901 : INFO : adding document #0 to Dictionary(195786 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,908 : INFO : built Dictionary(195836 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1218 documents (total 12180000 corpus positions)
    2021-05-05 22:36:40,954 : INFO : adding document #0 to Dictionary(195836 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:40,959 : INFO : built Dictionary(195880 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1219 documents (total 12190000 corpus positions)
    2021-05-05 22:36:41,005 : INFO : adding document #0 to Dictionary(195880 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,014 : INFO : built Dictionary(195945 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1220 documents (total 12200000 corpus positions)
    2021-05-05 22:36:41,060 : INFO : adding document #0 to Dictionary(195945 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,070 : INFO : built Dictionary(196013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1221 documents (total 12210000 corpus positions)
    2021-05-05 22:36:41,120 : INFO : adding document #0 to Dictionary(196013 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,128 : INFO : built Dictionary(196112 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1222 documents (total 12220000 corpus positions)
    2021-05-05 22:36:41,173 : INFO : adding document #0 to Dictionary(196112 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,182 : INFO : built Dictionary(196280 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1223 documents (total 12230000 corpus positions)
    2021-05-05 22:36:41,227 : INFO : adding document #0 to Dictionary(196280 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,236 : INFO : built Dictionary(196440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1224 documents (total 12240000 corpus positions)
    2021-05-05 22:36:41,285 : INFO : adding document #0 to Dictionary(196440 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,292 : INFO : built Dictionary(196500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1225 documents (total 12250000 corpus positions)
    2021-05-05 22:36:41,337 : INFO : adding document #0 to Dictionary(196500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,345 : INFO : built Dictionary(196588 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1226 documents (total 12260000 corpus positions)
    2021-05-05 22:36:41,389 : INFO : adding document #0 to Dictionary(196588 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,398 : INFO : built Dictionary(196728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1227 documents (total 12270000 corpus positions)
    2021-05-05 22:36:41,445 : INFO : adding document #0 to Dictionary(196728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,453 : INFO : built Dictionary(196797 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1228 documents (total 12280000 corpus positions)
    2021-05-05 22:36:41,497 : INFO : adding document #0 to Dictionary(196797 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,505 : INFO : built Dictionary(196887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1229 documents (total 12290000 corpus positions)
    2021-05-05 22:36:41,549 : INFO : adding document #0 to Dictionary(196887 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,556 : INFO : built Dictionary(196931 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1230 documents (total 12300000 corpus positions)
    2021-05-05 22:36:41,600 : INFO : adding document #0 to Dictionary(196931 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,607 : INFO : built Dictionary(197038 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1231 documents (total 12310000 corpus positions)
    2021-05-05 22:36:41,654 : INFO : adding document #0 to Dictionary(197038 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,662 : INFO : built Dictionary(197084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1232 documents (total 12320000 corpus positions)
    2021-05-05 22:36:41,707 : INFO : adding document #0 to Dictionary(197084 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,715 : INFO : built Dictionary(197205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1233 documents (total 12330000 corpus positions)
    2021-05-05 22:36:41,762 : INFO : adding document #0 to Dictionary(197205 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,771 : INFO : built Dictionary(197364 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1234 documents (total 12340000 corpus positions)
    2021-05-05 22:36:41,817 : INFO : adding document #0 to Dictionary(197364 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,826 : INFO : built Dictionary(197642 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1235 documents (total 12350000 corpus positions)
    2021-05-05 22:36:41,868 : INFO : adding document #0 to Dictionary(197642 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,876 : INFO : built Dictionary(197698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1236 documents (total 12360000 corpus positions)
    2021-05-05 22:36:41,919 : INFO : adding document #0 to Dictionary(197698 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,926 : INFO : built Dictionary(197722 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1237 documents (total 12370000 corpus positions)
    2021-05-05 22:36:41,969 : INFO : adding document #0 to Dictionary(197722 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:41,976 : INFO : built Dictionary(197820 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1238 documents (total 12380000 corpus positions)
    2021-05-05 22:36:42,020 : INFO : adding document #0 to Dictionary(197820 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,029 : INFO : built Dictionary(197877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1239 documents (total 12390000 corpus positions)
    2021-05-05 22:36:42,072 : INFO : adding document #0 to Dictionary(197877 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,079 : INFO : built Dictionary(197969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1240 documents (total 12400000 corpus positions)
    2021-05-05 22:36:42,123 : INFO : adding document #0 to Dictionary(197969 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,131 : INFO : built Dictionary(198043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1241 documents (total 12410000 corpus positions)
    2021-05-05 22:36:42,174 : INFO : adding document #0 to Dictionary(198043 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,182 : INFO : built Dictionary(198121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1242 documents (total 12420000 corpus positions)
    2021-05-05 22:36:42,227 : INFO : adding document #0 to Dictionary(198121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,236 : INFO : built Dictionary(198266 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1243 documents (total 12430000 corpus positions)
    2021-05-05 22:36:42,279 : INFO : adding document #0 to Dictionary(198266 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,287 : INFO : built Dictionary(198312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1244 documents (total 12440000 corpus positions)
    2021-05-05 22:36:42,330 : INFO : adding document #0 to Dictionary(198312 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,338 : INFO : built Dictionary(198389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1245 documents (total 12450000 corpus positions)
    2021-05-05 22:36:42,381 : INFO : adding document #0 to Dictionary(198389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,388 : INFO : built Dictionary(198421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1246 documents (total 12460000 corpus positions)
    2021-05-05 22:36:42,432 : INFO : adding document #0 to Dictionary(198421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,439 : INFO : built Dictionary(198483 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1247 documents (total 12470000 corpus positions)
    2021-05-05 22:36:42,482 : INFO : adding document #0 to Dictionary(198483 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,490 : INFO : built Dictionary(198535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1248 documents (total 12480000 corpus positions)
    2021-05-05 22:36:42,533 : INFO : adding document #0 to Dictionary(198535 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,540 : INFO : built Dictionary(198594 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1249 documents (total 12490000 corpus positions)
    2021-05-05 22:36:42,584 : INFO : adding document #0 to Dictionary(198594 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,591 : INFO : built Dictionary(198668 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1250 documents (total 12500000 corpus positions)
    2021-05-05 22:36:42,635 : INFO : adding document #0 to Dictionary(198668 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,644 : INFO : built Dictionary(198748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1251 documents (total 12510000 corpus positions)
    2021-05-05 22:36:42,687 : INFO : adding document #0 to Dictionary(198748 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,695 : INFO : built Dictionary(198815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1252 documents (total 12520000 corpus positions)
    2021-05-05 22:36:42,738 : INFO : adding document #0 to Dictionary(198815 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,747 : INFO : built Dictionary(198895 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1253 documents (total 12530000 corpus positions)
    2021-05-05 22:36:42,790 : INFO : adding document #0 to Dictionary(198895 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,797 : INFO : built Dictionary(198955 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1254 documents (total 12540000 corpus positions)
    2021-05-05 22:36:42,842 : INFO : adding document #0 to Dictionary(198955 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,851 : INFO : built Dictionary(199074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1255 documents (total 12550000 corpus positions)
    2021-05-05 22:36:42,895 : INFO : adding document #0 to Dictionary(199074 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,902 : INFO : built Dictionary(199106 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1256 documents (total 12560000 corpus positions)
    2021-05-05 22:36:42,945 : INFO : adding document #0 to Dictionary(199106 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:42,952 : INFO : built Dictionary(199200 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1257 documents (total 12570000 corpus positions)
    2021-05-05 22:36:42,996 : INFO : adding document #0 to Dictionary(199200 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,005 : INFO : built Dictionary(199294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1258 documents (total 12580000 corpus positions)
    2021-05-05 22:36:43,050 : INFO : adding document #0 to Dictionary(199294 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,058 : INFO : built Dictionary(199340 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1259 documents (total 12590000 corpus positions)
    2021-05-05 22:36:43,102 : INFO : adding document #0 to Dictionary(199340 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,111 : INFO : built Dictionary(199424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1260 documents (total 12600000 corpus positions)
    2021-05-05 22:36:43,155 : INFO : adding document #0 to Dictionary(199424 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,162 : INFO : built Dictionary(199487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1261 documents (total 12610000 corpus positions)
    2021-05-05 22:36:43,207 : INFO : adding document #0 to Dictionary(199487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,214 : INFO : built Dictionary(199534 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1262 documents (total 12620000 corpus positions)
    2021-05-05 22:36:43,259 : INFO : adding document #0 to Dictionary(199534 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,267 : INFO : built Dictionary(199615 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1263 documents (total 12630000 corpus positions)
    2021-05-05 22:36:43,311 : INFO : adding document #0 to Dictionary(199615 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,318 : INFO : built Dictionary(199663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1264 documents (total 12640000 corpus positions)
    2021-05-05 22:36:43,362 : INFO : adding document #0 to Dictionary(199663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,370 : INFO : built Dictionary(199729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1265 documents (total 12650000 corpus positions)
    2021-05-05 22:36:43,414 : INFO : adding document #0 to Dictionary(199729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,422 : INFO : built Dictionary(199810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1266 documents (total 12660000 corpus positions)
    2021-05-05 22:36:43,465 : INFO : adding document #0 to Dictionary(199810 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,473 : INFO : built Dictionary(199855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1267 documents (total 12670000 corpus positions)
    2021-05-05 22:36:43,516 : INFO : adding document #0 to Dictionary(199855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,523 : INFO : built Dictionary(199926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1268 documents (total 12680000 corpus positions)
    2021-05-05 22:36:43,567 : INFO : adding document #0 to Dictionary(199926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,578 : INFO : built Dictionary(199984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1269 documents (total 12690000 corpus positions)
    2021-05-05 22:36:43,621 : INFO : adding document #0 to Dictionary(199984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,630 : INFO : built Dictionary(200060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1270 documents (total 12700000 corpus positions)
    2021-05-05 22:36:43,674 : INFO : adding document #0 to Dictionary(200060 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,683 : INFO : built Dictionary(200177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1271 documents (total 12710000 corpus positions)
    2021-05-05 22:36:43,726 : INFO : adding document #0 to Dictionary(200177 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,735 : INFO : built Dictionary(200262 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1272 documents (total 12720000 corpus positions)
    2021-05-05 22:36:43,779 : INFO : adding document #0 to Dictionary(200262 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,788 : INFO : built Dictionary(200366 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1273 documents (total 12730000 corpus positions)
    2021-05-05 22:36:43,837 : INFO : adding document #0 to Dictionary(200366 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,845 : INFO : built Dictionary(200500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1274 documents (total 12740000 corpus positions)
    2021-05-05 22:36:43,889 : INFO : adding document #0 to Dictionary(200500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,897 : INFO : built Dictionary(200622 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1275 documents (total 12750000 corpus positions)
    2021-05-05 22:36:43,941 : INFO : adding document #0 to Dictionary(200622 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:43,950 : INFO : built Dictionary(200763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1276 documents (total 12760000 corpus positions)
    2021-05-05 22:36:43,995 : INFO : adding document #0 to Dictionary(200763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,005 : INFO : built Dictionary(200881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1277 documents (total 12770000 corpus positions)
    2021-05-05 22:36:44,057 : INFO : adding document #0 to Dictionary(200881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,065 : INFO : built Dictionary(200976 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1278 documents (total 12780000 corpus positions)
    2021-05-05 22:36:44,112 : INFO : adding document #0 to Dictionary(200976 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,122 : INFO : built Dictionary(201104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1279 documents (total 12790000 corpus positions)
    2021-05-05 22:36:44,173 : INFO : adding document #0 to Dictionary(201104 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,181 : INFO : built Dictionary(201192 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1280 documents (total 12800000 corpus positions)
    2021-05-05 22:36:44,226 : INFO : adding document #0 to Dictionary(201192 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,237 : INFO : built Dictionary(201319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1281 documents (total 12810000 corpus positions)
    2021-05-05 22:36:44,286 : INFO : adding document #0 to Dictionary(201319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,294 : INFO : built Dictionary(201438 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1282 documents (total 12820000 corpus positions)
    2021-05-05 22:36:44,340 : INFO : adding document #0 to Dictionary(201438 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,349 : INFO : built Dictionary(201515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1283 documents (total 12830000 corpus positions)
    2021-05-05 22:36:44,397 : INFO : adding document #0 to Dictionary(201515 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,408 : INFO : built Dictionary(201603 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1284 documents (total 12840000 corpus positions)
    2021-05-05 22:36:44,457 : INFO : adding document #0 to Dictionary(201603 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,465 : INFO : built Dictionary(201703 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1285 documents (total 12850000 corpus positions)
    2021-05-05 22:36:44,509 : INFO : adding document #0 to Dictionary(201703 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,518 : INFO : built Dictionary(201805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1286 documents (total 12860000 corpus positions)
    2021-05-05 22:36:44,563 : INFO : adding document #0 to Dictionary(201805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,572 : INFO : built Dictionary(201915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1287 documents (total 12870000 corpus positions)
    2021-05-05 22:36:44,621 : INFO : adding document #0 to Dictionary(201915 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,630 : INFO : built Dictionary(202010 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1288 documents (total 12880000 corpus positions)
    2021-05-05 22:36:44,675 : INFO : adding document #0 to Dictionary(202010 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,684 : INFO : built Dictionary(202058 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1289 documents (total 12890000 corpus positions)
    2021-05-05 22:36:44,732 : INFO : adding document #0 to Dictionary(202058 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,739 : INFO : built Dictionary(202244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1290 documents (total 12900000 corpus positions)
    2021-05-05 22:36:44,785 : INFO : adding document #0 to Dictionary(202244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,792 : INFO : built Dictionary(202351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1291 documents (total 12910000 corpus positions)
    2021-05-05 22:36:44,840 : INFO : adding document #0 to Dictionary(202351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,848 : INFO : built Dictionary(202399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1292 documents (total 12920000 corpus positions)
    2021-05-05 22:36:44,893 : INFO : adding document #0 to Dictionary(202399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,902 : INFO : built Dictionary(202476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1293 documents (total 12930000 corpus positions)
    2021-05-05 22:36:44,945 : INFO : adding document #0 to Dictionary(202476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:44,953 : INFO : built Dictionary(202532 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1294 documents (total 12940000 corpus positions)
    2021-05-05 22:36:44,997 : INFO : adding document #0 to Dictionary(202532 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,009 : INFO : built Dictionary(202593 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1295 documents (total 12950000 corpus positions)
    2021-05-05 22:36:45,057 : INFO : adding document #0 to Dictionary(202593 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,066 : INFO : built Dictionary(202681 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1296 documents (total 12960000 corpus positions)
    2021-05-05 22:36:45,120 : INFO : adding document #0 to Dictionary(202681 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,128 : INFO : built Dictionary(202745 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1297 documents (total 12970000 corpus positions)
    2021-05-05 22:36:45,177 : INFO : adding document #0 to Dictionary(202745 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,184 : INFO : built Dictionary(202802 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1298 documents (total 12980000 corpus positions)
    2021-05-05 22:36:45,228 : INFO : adding document #0 to Dictionary(202802 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,236 : INFO : built Dictionary(202869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1299 documents (total 12990000 corpus positions)
    2021-05-05 22:36:45,279 : INFO : adding document #0 to Dictionary(202869 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,287 : INFO : built Dictionary(202980 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1300 documents (total 13000000 corpus positions)
    2021-05-05 22:36:45,331 : INFO : adding document #0 to Dictionary(202980 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,339 : INFO : built Dictionary(203065 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1301 documents (total 13010000 corpus positions)
    2021-05-05 22:36:45,383 : INFO : adding document #0 to Dictionary(203065 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,393 : INFO : built Dictionary(203317 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1302 documents (total 13020000 corpus positions)
    2021-05-05 22:36:45,441 : INFO : adding document #0 to Dictionary(203317 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,448 : INFO : built Dictionary(203351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1303 documents (total 13030000 corpus positions)
    2021-05-05 22:36:45,496 : INFO : adding document #0 to Dictionary(203351 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,504 : INFO : built Dictionary(203470 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1304 documents (total 13040000 corpus positions)
    2021-05-05 22:36:45,550 : INFO : adding document #0 to Dictionary(203470 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,558 : INFO : built Dictionary(203508 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1305 documents (total 13050000 corpus positions)
    2021-05-05 22:36:45,607 : INFO : adding document #0 to Dictionary(203508 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,619 : INFO : built Dictionary(203629 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1306 documents (total 13060000 corpus positions)
    2021-05-05 22:36:45,665 : INFO : adding document #0 to Dictionary(203629 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,676 : INFO : built Dictionary(203742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1307 documents (total 13070000 corpus positions)
    2021-05-05 22:36:45,723 : INFO : adding document #0 to Dictionary(203742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,733 : INFO : built Dictionary(203908 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1308 documents (total 13080000 corpus positions)
    2021-05-05 22:36:45,778 : INFO : adding document #0 to Dictionary(203908 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,787 : INFO : built Dictionary(203971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1309 documents (total 13090000 corpus positions)
    2021-05-05 22:36:45,832 : INFO : adding document #0 to Dictionary(203971 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,840 : INFO : built Dictionary(204023 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1310 documents (total 13100000 corpus positions)
    2021-05-05 22:36:45,885 : INFO : adding document #0 to Dictionary(204023 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,895 : INFO : built Dictionary(204117 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1311 documents (total 13110000 corpus positions)
    2021-05-05 22:36:45,941 : INFO : adding document #0 to Dictionary(204117 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:45,951 : INFO : built Dictionary(204160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1312 documents (total 13120000 corpus positions)
    2021-05-05 22:36:45,997 : INFO : adding document #0 to Dictionary(204160 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,006 : INFO : built Dictionary(204282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1313 documents (total 13130000 corpus positions)
    2021-05-05 22:36:46,053 : INFO : adding document #0 to Dictionary(204282 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,063 : INFO : built Dictionary(204399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1314 documents (total 13140000 corpus positions)
    2021-05-05 22:36:46,113 : INFO : adding document #0 to Dictionary(204399 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,126 : INFO : built Dictionary(204443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1315 documents (total 13150000 corpus positions)
    2021-05-05 22:36:46,178 : INFO : adding document #0 to Dictionary(204443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,187 : INFO : built Dictionary(204495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1316 documents (total 13160000 corpus positions)
    2021-05-05 22:36:46,235 : INFO : adding document #0 to Dictionary(204495 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,246 : INFO : built Dictionary(204581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1317 documents (total 13170000 corpus positions)
    2021-05-05 22:36:46,291 : INFO : adding document #0 to Dictionary(204581 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,299 : INFO : built Dictionary(204660 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1318 documents (total 13180000 corpus positions)
    2021-05-05 22:36:46,347 : INFO : adding document #0 to Dictionary(204660 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,354 : INFO : built Dictionary(204710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1319 documents (total 13190000 corpus positions)
    2021-05-05 22:36:46,402 : INFO : adding document #0 to Dictionary(204710 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,409 : INFO : built Dictionary(204770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1320 documents (total 13200000 corpus positions)
    2021-05-05 22:36:46,454 : INFO : adding document #0 to Dictionary(204770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,462 : INFO : built Dictionary(204881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1321 documents (total 13210000 corpus positions)
    2021-05-05 22:36:46,508 : INFO : adding document #0 to Dictionary(204881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,516 : INFO : built Dictionary(204982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1322 documents (total 13220000 corpus positions)
    2021-05-05 22:36:46,561 : INFO : adding document #0 to Dictionary(204982 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,569 : INFO : built Dictionary(205053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1323 documents (total 13230000 corpus positions)
    2021-05-05 22:36:46,618 : INFO : adding document #0 to Dictionary(205053 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,627 : INFO : built Dictionary(205131 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1324 documents (total 13240000 corpus positions)
    2021-05-05 22:36:46,669 : INFO : adding document #0 to Dictionary(205131 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,678 : INFO : built Dictionary(205218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1325 documents (total 13250000 corpus positions)
    2021-05-05 22:36:46,725 : INFO : adding document #0 to Dictionary(205218 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,734 : INFO : built Dictionary(205292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1326 documents (total 13260000 corpus positions)
    2021-05-05 22:36:46,780 : INFO : adding document #0 to Dictionary(205292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,788 : INFO : built Dictionary(205387 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1327 documents (total 13270000 corpus positions)
    2021-05-05 22:36:46,831 : INFO : adding document #0 to Dictionary(205387 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,839 : INFO : built Dictionary(205444 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1328 documents (total 13280000 corpus positions)
    2021-05-05 22:36:46,882 : INFO : adding document #0 to Dictionary(205444 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,890 : INFO : built Dictionary(205489 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1329 documents (total 13290000 corpus positions)
    2021-05-05 22:36:46,937 : INFO : adding document #0 to Dictionary(205489 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,945 : INFO : built Dictionary(205546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1330 documents (total 13300000 corpus positions)
    2021-05-05 22:36:46,990 : INFO : adding document #0 to Dictionary(205546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:46,999 : INFO : built Dictionary(205648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1331 documents (total 13310000 corpus positions)
    2021-05-05 22:36:47,047 : INFO : adding document #0 to Dictionary(205648 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,055 : INFO : built Dictionary(205723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1332 documents (total 13320000 corpus positions)
    2021-05-05 22:36:47,098 : INFO : adding document #0 to Dictionary(205723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,106 : INFO : built Dictionary(205796 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1333 documents (total 13330000 corpus positions)
    2021-05-05 22:36:47,152 : INFO : adding document #0 to Dictionary(205796 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,160 : INFO : built Dictionary(205842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1334 documents (total 13340000 corpus positions)
    2021-05-05 22:36:47,205 : INFO : adding document #0 to Dictionary(205842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,214 : INFO : built Dictionary(205894 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1335 documents (total 13350000 corpus positions)
    2021-05-05 22:36:47,258 : INFO : adding document #0 to Dictionary(205894 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,266 : INFO : built Dictionary(205959 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1336 documents (total 13360000 corpus positions)
    2021-05-05 22:36:47,311 : INFO : adding document #0 to Dictionary(205959 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,319 : INFO : built Dictionary(206012 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1337 documents (total 13370000 corpus positions)
    2021-05-05 22:36:47,366 : INFO : adding document #0 to Dictionary(206012 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,376 : INFO : built Dictionary(206094 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1338 documents (total 13380000 corpus positions)
    2021-05-05 22:36:47,432 : INFO : adding document #0 to Dictionary(206094 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,440 : INFO : built Dictionary(206271 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1339 documents (total 13390000 corpus positions)
    2021-05-05 22:36:47,484 : INFO : adding document #0 to Dictionary(206271 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,492 : INFO : built Dictionary(206342 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1340 documents (total 13400000 corpus positions)
    2021-05-05 22:36:47,535 : INFO : adding document #0 to Dictionary(206342 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,543 : INFO : built Dictionary(206450 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1341 documents (total 13410000 corpus positions)
    2021-05-05 22:36:47,586 : INFO : adding document #0 to Dictionary(206450 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,594 : INFO : built Dictionary(206507 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1342 documents (total 13420000 corpus positions)
    2021-05-05 22:36:47,637 : INFO : adding document #0 to Dictionary(206507 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,645 : INFO : built Dictionary(206634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1343 documents (total 13430000 corpus positions)
    2021-05-05 22:36:47,688 : INFO : adding document #0 to Dictionary(206634 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,696 : INFO : built Dictionary(206694 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1344 documents (total 13440000 corpus positions)
    2021-05-05 22:36:47,739 : INFO : adding document #0 to Dictionary(206694 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,748 : INFO : built Dictionary(206732 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1345 documents (total 13450000 corpus positions)
    2021-05-05 22:36:47,792 : INFO : adding document #0 to Dictionary(206732 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,800 : INFO : built Dictionary(206807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1346 documents (total 13460000 corpus positions)
    2021-05-05 22:36:47,845 : INFO : adding document #0 to Dictionary(206807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,853 : INFO : built Dictionary(206867 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1347 documents (total 13470000 corpus positions)
    2021-05-05 22:36:47,897 : INFO : adding document #0 to Dictionary(206867 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,907 : INFO : built Dictionary(206924 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1348 documents (total 13480000 corpus positions)
    2021-05-05 22:36:47,956 : INFO : adding document #0 to Dictionary(206924 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:47,964 : INFO : built Dictionary(207045 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1349 documents (total 13490000 corpus positions)
    2021-05-05 22:36:48,016 : INFO : adding document #0 to Dictionary(207045 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,026 : INFO : built Dictionary(207123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1350 documents (total 13500000 corpus positions)
    2021-05-05 22:36:48,077 : INFO : adding document #0 to Dictionary(207123 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,086 : INFO : built Dictionary(207202 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1351 documents (total 13510000 corpus positions)
    2021-05-05 22:36:48,132 : INFO : adding document #0 to Dictionary(207202 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,140 : INFO : built Dictionary(207254 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1352 documents (total 13520000 corpus positions)
    2021-05-05 22:36:48,187 : INFO : adding document #0 to Dictionary(207254 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,196 : INFO : built Dictionary(207344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1353 documents (total 13530000 corpus positions)
    2021-05-05 22:36:48,243 : INFO : adding document #0 to Dictionary(207344 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,251 : INFO : built Dictionary(207430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1354 documents (total 13540000 corpus positions)
    2021-05-05 22:36:48,294 : INFO : adding document #0 to Dictionary(207430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,301 : INFO : built Dictionary(207497 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1355 documents (total 13550000 corpus positions)
    2021-05-05 22:36:48,345 : INFO : adding document #0 to Dictionary(207497 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,353 : INFO : built Dictionary(207608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1356 documents (total 13560000 corpus positions)
    2021-05-05 22:36:48,396 : INFO : adding document #0 to Dictionary(207608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,406 : INFO : built Dictionary(207763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1357 documents (total 13570000 corpus positions)
    2021-05-05 22:36:48,452 : INFO : adding document #0 to Dictionary(207763 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,460 : INFO : built Dictionary(207924 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1358 documents (total 13580000 corpus positions)
    2021-05-05 22:36:48,504 : INFO : adding document #0 to Dictionary(207924 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,512 : INFO : built Dictionary(207994 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1359 documents (total 13590000 corpus positions)
    2021-05-05 22:36:48,560 : INFO : adding document #0 to Dictionary(207994 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,567 : INFO : built Dictionary(208094 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1360 documents (total 13600000 corpus positions)
    2021-05-05 22:36:48,614 : INFO : adding document #0 to Dictionary(208094 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,622 : INFO : built Dictionary(208150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1361 documents (total 13610000 corpus positions)
    2021-05-05 22:36:48,666 : INFO : adding document #0 to Dictionary(208150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,674 : INFO : built Dictionary(208223 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1362 documents (total 13620000 corpus positions)
    2021-05-05 22:36:48,718 : INFO : adding document #0 to Dictionary(208223 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,727 : INFO : built Dictionary(208284 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1363 documents (total 13630000 corpus positions)
    2021-05-05 22:36:48,773 : INFO : adding document #0 to Dictionary(208284 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,780 : INFO : built Dictionary(208330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1364 documents (total 13640000 corpus positions)
    2021-05-05 22:36:48,824 : INFO : adding document #0 to Dictionary(208330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,832 : INFO : built Dictionary(208382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1365 documents (total 13650000 corpus positions)
    2021-05-05 22:36:48,874 : INFO : adding document #0 to Dictionary(208382 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,882 : INFO : built Dictionary(208526 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1366 documents (total 13660000 corpus positions)
    2021-05-05 22:36:48,929 : INFO : adding document #0 to Dictionary(208526 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,937 : INFO : built Dictionary(208669 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1367 documents (total 13670000 corpus positions)
    2021-05-05 22:36:48,979 : INFO : adding document #0 to Dictionary(208669 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:48,986 : INFO : built Dictionary(208724 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1368 documents (total 13680000 corpus positions)
    2021-05-05 22:36:49,037 : INFO : adding document #0 to Dictionary(208724 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,045 : INFO : built Dictionary(208794 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1369 documents (total 13690000 corpus positions)
    2021-05-05 22:36:49,089 : INFO : adding document #0 to Dictionary(208794 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,097 : INFO : built Dictionary(208858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1370 documents (total 13700000 corpus positions)
    2021-05-05 22:36:49,140 : INFO : adding document #0 to Dictionary(208858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,148 : INFO : built Dictionary(208909 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1371 documents (total 13710000 corpus positions)
    2021-05-05 22:36:49,191 : INFO : adding document #0 to Dictionary(208909 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,199 : INFO : built Dictionary(209014 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1372 documents (total 13720000 corpus positions)
    2021-05-05 22:36:49,242 : INFO : adding document #0 to Dictionary(209014 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,250 : INFO : built Dictionary(209091 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1373 documents (total 13730000 corpus positions)
    2021-05-05 22:36:49,295 : INFO : adding document #0 to Dictionary(209091 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,303 : INFO : built Dictionary(209214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1374 documents (total 13740000 corpus positions)
    2021-05-05 22:36:49,349 : INFO : adding document #0 to Dictionary(209214 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,358 : INFO : built Dictionary(209330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1375 documents (total 13750000 corpus positions)
    2021-05-05 22:36:49,402 : INFO : adding document #0 to Dictionary(209330 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,411 : INFO : built Dictionary(209476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1376 documents (total 13760000 corpus positions)
    2021-05-05 22:36:49,455 : INFO : adding document #0 to Dictionary(209476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,462 : INFO : built Dictionary(209546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1377 documents (total 13770000 corpus positions)
    2021-05-05 22:36:49,506 : INFO : adding document #0 to Dictionary(209546 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,514 : INFO : built Dictionary(209623 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1378 documents (total 13780000 corpus positions)
    2021-05-05 22:36:49,558 : INFO : adding document #0 to Dictionary(209623 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,566 : INFO : built Dictionary(209819 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1379 documents (total 13790000 corpus positions)
    2021-05-05 22:36:49,610 : INFO : adding document #0 to Dictionary(209819 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,617 : INFO : built Dictionary(209975 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1380 documents (total 13800000 corpus positions)
    2021-05-05 22:36:49,663 : INFO : adding document #0 to Dictionary(209975 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,671 : INFO : built Dictionary(210072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1381 documents (total 13810000 corpus positions)
    2021-05-05 22:36:49,716 : INFO : adding document #0 to Dictionary(210072 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,724 : INFO : built Dictionary(210191 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1382 documents (total 13820000 corpus positions)
    2021-05-05 22:36:49,767 : INFO : adding document #0 to Dictionary(210191 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,775 : INFO : built Dictionary(210364 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1383 documents (total 13830000 corpus positions)
    2021-05-05 22:36:49,819 : INFO : adding document #0 to Dictionary(210364 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,827 : INFO : built Dictionary(210448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1384 documents (total 13840000 corpus positions)
    2021-05-05 22:36:49,872 : INFO : adding document #0 to Dictionary(210448 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,880 : INFO : built Dictionary(210496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1385 documents (total 13850000 corpus positions)
    2021-05-05 22:36:49,923 : INFO : adding document #0 to Dictionary(210496 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,931 : INFO : built Dictionary(210608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1386 documents (total 13860000 corpus positions)
    2021-05-05 22:36:49,973 : INFO : adding document #0 to Dictionary(210608 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:49,982 : INFO : built Dictionary(210733 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1387 documents (total 13870000 corpus positions)
    2021-05-05 22:36:50,033 : INFO : adding document #0 to Dictionary(210733 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,042 : INFO : built Dictionary(210854 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1388 documents (total 13880000 corpus positions)
    2021-05-05 22:36:50,088 : INFO : adding document #0 to Dictionary(210854 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,097 : INFO : built Dictionary(210937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1389 documents (total 13890000 corpus positions)
    2021-05-05 22:36:50,141 : INFO : adding document #0 to Dictionary(210937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,151 : INFO : built Dictionary(211085 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1390 documents (total 13900000 corpus positions)
    2021-05-05 22:36:50,194 : INFO : adding document #0 to Dictionary(211085 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,204 : INFO : built Dictionary(211162 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1391 documents (total 13910000 corpus positions)
    2021-05-05 22:36:50,250 : INFO : adding document #0 to Dictionary(211162 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,259 : INFO : built Dictionary(211314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1392 documents (total 13920000 corpus positions)
    2021-05-05 22:36:50,302 : INFO : adding document #0 to Dictionary(211314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,310 : INFO : built Dictionary(211431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1393 documents (total 13930000 corpus positions)
    2021-05-05 22:36:50,354 : INFO : adding document #0 to Dictionary(211431 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,363 : INFO : built Dictionary(211579 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1394 documents (total 13940000 corpus positions)
    2021-05-05 22:36:50,407 : INFO : adding document #0 to Dictionary(211579 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,417 : INFO : built Dictionary(211706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1395 documents (total 13950000 corpus positions)
    2021-05-05 22:36:50,461 : INFO : adding document #0 to Dictionary(211706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,470 : INFO : built Dictionary(211933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1396 documents (total 13960000 corpus positions)
    2021-05-05 22:36:50,519 : INFO : adding document #0 to Dictionary(211933 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,527 : INFO : built Dictionary(212078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1397 documents (total 13970000 corpus positions)
    2021-05-05 22:36:50,571 : INFO : adding document #0 to Dictionary(212078 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,580 : INFO : built Dictionary(212183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1398 documents (total 13980000 corpus positions)
    2021-05-05 22:36:50,624 : INFO : adding document #0 to Dictionary(212183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,633 : INFO : built Dictionary(212313 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1399 documents (total 13990000 corpus positions)
    2021-05-05 22:36:50,676 : INFO : adding document #0 to Dictionary(212313 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,685 : INFO : built Dictionary(212395 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1400 documents (total 14000000 corpus positions)
    2021-05-05 22:36:50,729 : INFO : adding document #0 to Dictionary(212395 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,740 : INFO : built Dictionary(212502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1401 documents (total 14010000 corpus positions)
    2021-05-05 22:36:50,783 : INFO : adding document #0 to Dictionary(212502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,791 : INFO : built Dictionary(212584 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1402 documents (total 14020000 corpus positions)
    2021-05-05 22:36:50,838 : INFO : adding document #0 to Dictionary(212584 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,846 : INFO : built Dictionary(212723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1403 documents (total 14030000 corpus positions)
    2021-05-05 22:36:50,889 : INFO : adding document #0 to Dictionary(212723 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,896 : INFO : built Dictionary(212897 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1404 documents (total 14040000 corpus positions)
    2021-05-05 22:36:50,942 : INFO : adding document #0 to Dictionary(212897 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:50,951 : INFO : built Dictionary(213064 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1405 documents (total 14050000 corpus positions)
    2021-05-05 22:36:50,998 : INFO : adding document #0 to Dictionary(213064 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,007 : INFO : built Dictionary(213181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1406 documents (total 14060000 corpus positions)
    2021-05-05 22:36:51,054 : INFO : adding document #0 to Dictionary(213181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,062 : INFO : built Dictionary(213269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1407 documents (total 14070000 corpus positions)
    2021-05-05 22:36:51,108 : INFO : adding document #0 to Dictionary(213269 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,116 : INFO : built Dictionary(213380 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1408 documents (total 14080000 corpus positions)
    2021-05-05 22:36:51,162 : INFO : adding document #0 to Dictionary(213380 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,171 : INFO : built Dictionary(213478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1409 documents (total 14090000 corpus positions)
    2021-05-05 22:36:51,219 : INFO : adding document #0 to Dictionary(213478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,229 : INFO : built Dictionary(213549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1410 documents (total 14100000 corpus positions)
    2021-05-05 22:36:51,274 : INFO : adding document #0 to Dictionary(213549 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,283 : INFO : built Dictionary(213639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1411 documents (total 14110000 corpus positions)
    2021-05-05 22:36:51,326 : INFO : adding document #0 to Dictionary(213639 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,334 : INFO : built Dictionary(213722 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1412 documents (total 14120000 corpus positions)
    2021-05-05 22:36:51,377 : INFO : adding document #0 to Dictionary(213722 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,384 : INFO : built Dictionary(213773 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1413 documents (total 14130000 corpus positions)
    2021-05-05 22:36:51,428 : INFO : adding document #0 to Dictionary(213773 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,436 : INFO : built Dictionary(213874 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1414 documents (total 14140000 corpus positions)
    2021-05-05 22:36:51,479 : INFO : adding document #0 to Dictionary(213874 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,488 : INFO : built Dictionary(213947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1415 documents (total 14150000 corpus positions)
    2021-05-05 22:36:51,531 : INFO : adding document #0 to Dictionary(213947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,540 : INFO : built Dictionary(214026 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1416 documents (total 14160000 corpus positions)
    2021-05-05 22:36:51,593 : INFO : adding document #0 to Dictionary(214026 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,601 : INFO : built Dictionary(214130 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1417 documents (total 14170000 corpus positions)
    2021-05-05 22:36:51,648 : INFO : adding document #0 to Dictionary(214130 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,656 : INFO : built Dictionary(214223 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1418 documents (total 14180000 corpus positions)
    2021-05-05 22:36:51,702 : INFO : adding document #0 to Dictionary(214223 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,710 : INFO : built Dictionary(214277 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1419 documents (total 14190000 corpus positions)
    2021-05-05 22:36:51,756 : INFO : adding document #0 to Dictionary(214277 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,764 : INFO : built Dictionary(214302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1420 documents (total 14200000 corpus positions)
    2021-05-05 22:36:51,809 : INFO : adding document #0 to Dictionary(214302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,816 : INFO : built Dictionary(214379 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1421 documents (total 14210000 corpus positions)
    2021-05-05 22:36:51,861 : INFO : adding document #0 to Dictionary(214379 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,870 : INFO : built Dictionary(214502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1422 documents (total 14220000 corpus positions)
    2021-05-05 22:36:51,917 : INFO : adding document #0 to Dictionary(214502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,925 : INFO : built Dictionary(214556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1423 documents (total 14230000 corpus positions)
    2021-05-05 22:36:51,969 : INFO : adding document #0 to Dictionary(214556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:51,977 : INFO : built Dictionary(214603 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1424 documents (total 14240000 corpus positions)
    2021-05-05 22:36:52,022 : INFO : adding document #0 to Dictionary(214603 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,031 : INFO : built Dictionary(214706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1425 documents (total 14250000 corpus positions)
    2021-05-05 22:36:52,077 : INFO : adding document #0 to Dictionary(214706 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,086 : INFO : built Dictionary(214805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1426 documents (total 14260000 corpus positions)
    2021-05-05 22:36:52,135 : INFO : adding document #0 to Dictionary(214805 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,143 : INFO : built Dictionary(214850 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1427 documents (total 14270000 corpus positions)
    2021-05-05 22:36:52,192 : INFO : adding document #0 to Dictionary(214850 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,202 : INFO : built Dictionary(214904 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1428 documents (total 14280000 corpus positions)
    2021-05-05 22:36:52,249 : INFO : adding document #0 to Dictionary(214904 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,258 : INFO : built Dictionary(215055 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1429 documents (total 14290000 corpus positions)
    2021-05-05 22:36:52,302 : INFO : adding document #0 to Dictionary(215055 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,312 : INFO : built Dictionary(215216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1430 documents (total 14300000 corpus positions)
    2021-05-05 22:36:52,357 : INFO : adding document #0 to Dictionary(215216 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,365 : INFO : built Dictionary(215311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1431 documents (total 14310000 corpus positions)
    2021-05-05 22:36:52,411 : INFO : adding document #0 to Dictionary(215311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,419 : INFO : built Dictionary(215389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1432 documents (total 14320000 corpus positions)
    2021-05-05 22:36:52,462 : INFO : adding document #0 to Dictionary(215389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,469 : INFO : built Dictionary(215454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1433 documents (total 14330000 corpus positions)
    2021-05-05 22:36:52,513 : INFO : adding document #0 to Dictionary(215454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,520 : INFO : built Dictionary(215487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1434 documents (total 14340000 corpus positions)
    2021-05-05 22:36:52,563 : INFO : adding document #0 to Dictionary(215487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,570 : INFO : built Dictionary(215560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1435 documents (total 14350000 corpus positions)
    2021-05-05 22:36:52,613 : INFO : adding document #0 to Dictionary(215560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,620 : INFO : built Dictionary(215609 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1436 documents (total 14360000 corpus positions)
    2021-05-05 22:36:52,663 : INFO : adding document #0 to Dictionary(215609 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,671 : INFO : built Dictionary(215684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1437 documents (total 14370000 corpus positions)
    2021-05-05 22:36:52,715 : INFO : adding document #0 to Dictionary(215684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,723 : INFO : built Dictionary(215827 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1438 documents (total 14380000 corpus positions)
    2021-05-05 22:36:52,767 : INFO : adding document #0 to Dictionary(215827 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,774 : INFO : built Dictionary(215872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1439 documents (total 14390000 corpus positions)
    2021-05-05 22:36:52,818 : INFO : adding document #0 to Dictionary(215872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,827 : INFO : built Dictionary(215961 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1440 documents (total 14400000 corpus positions)
    2021-05-05 22:36:52,875 : INFO : adding document #0 to Dictionary(215961 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,883 : INFO : built Dictionary(216207 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1441 documents (total 14410000 corpus positions)
    2021-05-05 22:36:52,927 : INFO : adding document #0 to Dictionary(216207 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,935 : INFO : built Dictionary(216247 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1442 documents (total 14420000 corpus positions)
    2021-05-05 22:36:52,981 : INFO : adding document #0 to Dictionary(216247 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:52,990 : INFO : built Dictionary(216296 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1443 documents (total 14430000 corpus positions)
    2021-05-05 22:36:53,039 : INFO : adding document #0 to Dictionary(216296 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,050 : INFO : built Dictionary(216421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1444 documents (total 14440000 corpus positions)
    2021-05-05 22:36:53,092 : INFO : adding document #0 to Dictionary(216421 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,099 : INFO : built Dictionary(216470 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1445 documents (total 14450000 corpus positions)
    2021-05-05 22:36:53,143 : INFO : adding document #0 to Dictionary(216470 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,150 : INFO : built Dictionary(216542 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1446 documents (total 14460000 corpus positions)
    2021-05-05 22:36:53,193 : INFO : adding document #0 to Dictionary(216542 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,202 : INFO : built Dictionary(216626 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1447 documents (total 14470000 corpus positions)
    2021-05-05 22:36:53,248 : INFO : adding document #0 to Dictionary(216626 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,257 : INFO : built Dictionary(216674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1448 documents (total 14480000 corpus positions)
    2021-05-05 22:36:53,306 : INFO : adding document #0 to Dictionary(216674 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,314 : INFO : built Dictionary(216742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1449 documents (total 14490000 corpus positions)
    2021-05-05 22:36:53,361 : INFO : adding document #0 to Dictionary(216742 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,369 : INFO : built Dictionary(216807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1450 documents (total 14500000 corpus positions)
    2021-05-05 22:36:53,414 : INFO : adding document #0 to Dictionary(216807 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,422 : INFO : built Dictionary(216841 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1451 documents (total 14510000 corpus positions)
    2021-05-05 22:36:53,467 : INFO : adding document #0 to Dictionary(216841 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,478 : INFO : built Dictionary(216913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1452 documents (total 14520000 corpus positions)
    2021-05-05 22:36:53,525 : INFO : adding document #0 to Dictionary(216913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,533 : INFO : built Dictionary(216979 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1453 documents (total 14530000 corpus positions)
    2021-05-05 22:36:53,580 : INFO : adding document #0 to Dictionary(216979 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,591 : INFO : built Dictionary(217083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1454 documents (total 14540000 corpus positions)
    2021-05-05 22:36:53,642 : INFO : adding document #0 to Dictionary(217083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,649 : INFO : built Dictionary(217181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1455 documents (total 14550000 corpus positions)
    2021-05-05 22:36:53,696 : INFO : adding document #0 to Dictionary(217181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,705 : INFO : built Dictionary(217752 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1456 documents (total 14560000 corpus positions)
    2021-05-05 22:36:53,749 : INFO : adding document #0 to Dictionary(217752 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,757 : INFO : built Dictionary(217844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1457 documents (total 14570000 corpus positions)
    2021-05-05 22:36:53,802 : INFO : adding document #0 to Dictionary(217844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,810 : INFO : built Dictionary(217895 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1458 documents (total 14580000 corpus positions)
    2021-05-05 22:36:53,856 : INFO : adding document #0 to Dictionary(217895 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,864 : INFO : built Dictionary(218024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1459 documents (total 14590000 corpus positions)
    2021-05-05 22:36:53,909 : INFO : adding document #0 to Dictionary(218024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,918 : INFO : built Dictionary(218138 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1460 documents (total 14600000 corpus positions)
    2021-05-05 22:36:53,965 : INFO : adding document #0 to Dictionary(218138 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:53,973 : INFO : built Dictionary(218183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1461 documents (total 14610000 corpus positions)
    2021-05-05 22:36:54,019 : INFO : adding document #0 to Dictionary(218183 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,029 : INFO : built Dictionary(218273 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1462 documents (total 14620000 corpus positions)
    2021-05-05 22:36:54,079 : INFO : adding document #0 to Dictionary(218273 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,087 : INFO : built Dictionary(218347 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1463 documents (total 14630000 corpus positions)
    2021-05-05 22:36:54,131 : INFO : adding document #0 to Dictionary(218347 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,139 : INFO : built Dictionary(218454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1464 documents (total 14640000 corpus positions)
    2021-05-05 22:36:54,182 : INFO : adding document #0 to Dictionary(218454 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,190 : INFO : built Dictionary(218500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1465 documents (total 14650000 corpus positions)
    2021-05-05 22:36:54,234 : INFO : adding document #0 to Dictionary(218500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,241 : INFO : built Dictionary(218632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1466 documents (total 14660000 corpus positions)
    2021-05-05 22:36:54,285 : INFO : adding document #0 to Dictionary(218632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,293 : INFO : built Dictionary(218727 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1467 documents (total 14670000 corpus positions)
    2021-05-05 22:36:54,339 : INFO : adding document #0 to Dictionary(218727 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,347 : INFO : built Dictionary(218799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1468 documents (total 14680000 corpus positions)
    2021-05-05 22:36:54,393 : INFO : adding document #0 to Dictionary(218799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,401 : INFO : built Dictionary(218855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1469 documents (total 14690000 corpus positions)
    2021-05-05 22:36:54,448 : INFO : adding document #0 to Dictionary(218855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,457 : INFO : built Dictionary(219089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1470 documents (total 14700000 corpus positions)
    2021-05-05 22:36:54,500 : INFO : adding document #0 to Dictionary(219089 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,507 : INFO : built Dictionary(219139 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1471 documents (total 14710000 corpus positions)
    2021-05-05 22:36:54,551 : INFO : adding document #0 to Dictionary(219139 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,560 : INFO : built Dictionary(219502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1472 documents (total 14720000 corpus positions)
    2021-05-05 22:36:54,603 : INFO : adding document #0 to Dictionary(219502 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,611 : INFO : built Dictionary(219663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1473 documents (total 14730000 corpus positions)
    2021-05-05 22:36:54,655 : INFO : adding document #0 to Dictionary(219663 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,662 : INFO : built Dictionary(219717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1474 documents (total 14740000 corpus positions)
    2021-05-05 22:36:54,705 : INFO : adding document #0 to Dictionary(219717 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,712 : INFO : built Dictionary(219842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1475 documents (total 14750000 corpus positions)
    2021-05-05 22:36:54,761 : INFO : adding document #0 to Dictionary(219842 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,769 : INFO : built Dictionary(219954 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1476 documents (total 14760000 corpus positions)
    2021-05-05 22:36:54,817 : INFO : adding document #0 to Dictionary(219954 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,821 : INFO : built Dictionary(219965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1477 documents (total 14770000 corpus positions)
    2021-05-05 22:36:54,865 : INFO : adding document #0 to Dictionary(219965 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,876 : INFO : built Dictionary(220238 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1478 documents (total 14780000 corpus positions)
    2021-05-05 22:36:54,919 : INFO : adding document #0 to Dictionary(220238 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,926 : INFO : built Dictionary(220300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1479 documents (total 14790000 corpus positions)
    2021-05-05 22:36:54,970 : INFO : adding document #0 to Dictionary(220300 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:54,978 : INFO : built Dictionary(220377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1480 documents (total 14800000 corpus positions)
    2021-05-05 22:36:55,023 : INFO : adding document #0 to Dictionary(220377 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,031 : INFO : built Dictionary(220486 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1481 documents (total 14810000 corpus positions)
    2021-05-05 22:36:55,074 : INFO : adding document #0 to Dictionary(220486 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,082 : INFO : built Dictionary(220560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1482 documents (total 14820000 corpus positions)
    2021-05-05 22:36:55,124 : INFO : adding document #0 to Dictionary(220560 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,132 : INFO : built Dictionary(220628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1483 documents (total 14830000 corpus positions)
    2021-05-05 22:36:55,175 : INFO : adding document #0 to Dictionary(220628 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,183 : INFO : built Dictionary(220730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1484 documents (total 14840000 corpus positions)
    2021-05-05 22:36:55,227 : INFO : adding document #0 to Dictionary(220730 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,235 : INFO : built Dictionary(220846 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1485 documents (total 14850000 corpus positions)
    2021-05-05 22:36:55,283 : INFO : adding document #0 to Dictionary(220846 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,292 : INFO : built Dictionary(220978 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1486 documents (total 14860000 corpus positions)
    2021-05-05 22:36:55,336 : INFO : adding document #0 to Dictionary(220978 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,345 : INFO : built Dictionary(221087 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1487 documents (total 14870000 corpus positions)
    2021-05-05 22:36:55,388 : INFO : adding document #0 to Dictionary(221087 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,396 : INFO : built Dictionary(221146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1488 documents (total 14880000 corpus positions)
    2021-05-05 22:36:55,439 : INFO : adding document #0 to Dictionary(221146 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,447 : INFO : built Dictionary(221230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1489 documents (total 14890000 corpus positions)
    2021-05-05 22:36:55,491 : INFO : adding document #0 to Dictionary(221230 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,500 : INFO : built Dictionary(221311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1490 documents (total 14900000 corpus positions)
    2021-05-05 22:36:55,544 : INFO : adding document #0 to Dictionary(221311 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,552 : INFO : built Dictionary(221530 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1491 documents (total 14910000 corpus positions)
    2021-05-05 22:36:55,596 : INFO : adding document #0 to Dictionary(221530 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,604 : INFO : built Dictionary(221586 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1492 documents (total 14920000 corpus positions)
    2021-05-05 22:36:55,647 : INFO : adding document #0 to Dictionary(221586 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,655 : INFO : built Dictionary(221709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1493 documents (total 14930000 corpus positions)
    2021-05-05 22:36:55,703 : INFO : adding document #0 to Dictionary(221709 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,711 : INFO : built Dictionary(221782 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1494 documents (total 14940000 corpus positions)
    2021-05-05 22:36:55,756 : INFO : adding document #0 to Dictionary(221782 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,764 : INFO : built Dictionary(221836 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1495 documents (total 14950000 corpus positions)
    2021-05-05 22:36:55,808 : INFO : adding document #0 to Dictionary(221836 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,815 : INFO : built Dictionary(221913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1496 documents (total 14960000 corpus positions)
    2021-05-05 22:36:55,860 : INFO : adding document #0 to Dictionary(221913 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,870 : INFO : built Dictionary(222298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1497 documents (total 14970000 corpus positions)
    2021-05-05 22:36:55,914 : INFO : adding document #0 to Dictionary(222298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,923 : INFO : built Dictionary(222375 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1498 documents (total 14980000 corpus positions)
    2021-05-05 22:36:55,977 : INFO : adding document #0 to Dictionary(222375 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:55,985 : INFO : built Dictionary(222500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1499 documents (total 14990000 corpus positions)
    2021-05-05 22:36:56,032 : INFO : adding document #0 to Dictionary(222500 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,041 : INFO : built Dictionary(222611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1500 documents (total 15000000 corpus positions)
    2021-05-05 22:36:56,086 : INFO : adding document #0 to Dictionary(222611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,094 : INFO : built Dictionary(222684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1501 documents (total 15010000 corpus positions)
    2021-05-05 22:36:56,141 : INFO : adding document #0 to Dictionary(222684 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,149 : INFO : built Dictionary(222771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1502 documents (total 15020000 corpus positions)
    2021-05-05 22:36:56,194 : INFO : adding document #0 to Dictionary(222771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,202 : INFO : built Dictionary(222857 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1503 documents (total 15030000 corpus positions)
    2021-05-05 22:36:56,246 : INFO : adding document #0 to Dictionary(222857 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,255 : INFO : built Dictionary(222977 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1504 documents (total 15040000 corpus positions)
    2021-05-05 22:36:56,299 : INFO : adding document #0 to Dictionary(222977 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,307 : INFO : built Dictionary(223049 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1505 documents (total 15050000 corpus positions)
    2021-05-05 22:36:56,350 : INFO : adding document #0 to Dictionary(223049 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,358 : INFO : built Dictionary(223121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1506 documents (total 15060000 corpus positions)
    2021-05-05 22:36:56,403 : INFO : adding document #0 to Dictionary(223121 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,412 : INFO : built Dictionary(223196 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1507 documents (total 15070000 corpus positions)
    2021-05-05 22:36:56,456 : INFO : adding document #0 to Dictionary(223196 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,464 : INFO : built Dictionary(223254 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1508 documents (total 15080000 corpus positions)
    2021-05-05 22:36:56,508 : INFO : adding document #0 to Dictionary(223254 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,517 : INFO : built Dictionary(223413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1509 documents (total 15090000 corpus positions)
    2021-05-05 22:36:56,565 : INFO : adding document #0 to Dictionary(223413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,573 : INFO : built Dictionary(223475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1510 documents (total 15100000 corpus positions)
    2021-05-05 22:36:56,616 : INFO : adding document #0 to Dictionary(223475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,626 : INFO : built Dictionary(223574 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1511 documents (total 15110000 corpus positions)
    2021-05-05 22:36:56,673 : INFO : adding document #0 to Dictionary(223574 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,681 : INFO : built Dictionary(223677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1512 documents (total 15120000 corpus positions)
    2021-05-05 22:36:56,725 : INFO : adding document #0 to Dictionary(223677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,733 : INFO : built Dictionary(223704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1513 documents (total 15130000 corpus positions)
    2021-05-05 22:36:56,779 : INFO : adding document #0 to Dictionary(223704 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,787 : INFO : built Dictionary(223751 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1514 documents (total 15140000 corpus positions)
    2021-05-05 22:36:56,831 : INFO : adding document #0 to Dictionary(223751 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,840 : INFO : built Dictionary(223799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1515 documents (total 15150000 corpus positions)
    2021-05-05 22:36:56,886 : INFO : adding document #0 to Dictionary(223799 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,894 : INFO : built Dictionary(223958 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1516 documents (total 15160000 corpus positions)
    2021-05-05 22:36:56,938 : INFO : adding document #0 to Dictionary(223958 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,946 : INFO : built Dictionary(224046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1517 documents (total 15170000 corpus positions)
    2021-05-05 22:36:56,990 : INFO : adding document #0 to Dictionary(224046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:56,998 : INFO : built Dictionary(224100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1518 documents (total 15180000 corpus positions)
    2021-05-05 22:36:57,049 : INFO : adding document #0 to Dictionary(224100 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,059 : INFO : built Dictionary(224140 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1519 documents (total 15190000 corpus positions)
    2021-05-05 22:36:57,102 : INFO : adding document #0 to Dictionary(224140 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,109 : INFO : built Dictionary(224228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1520 documents (total 15200000 corpus positions)
    2021-05-05 22:36:57,153 : INFO : adding document #0 to Dictionary(224228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,160 : INFO : built Dictionary(224370 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1521 documents (total 15210000 corpus positions)
    2021-05-05 22:36:57,202 : INFO : adding document #0 to Dictionary(224370 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,210 : INFO : built Dictionary(224404 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1522 documents (total 15220000 corpus positions)
    2021-05-05 22:36:57,254 : INFO : adding document #0 to Dictionary(224404 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,262 : INFO : built Dictionary(224477 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1523 documents (total 15230000 corpus positions)
    2021-05-05 22:36:57,305 : INFO : adding document #0 to Dictionary(224477 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,313 : INFO : built Dictionary(224524 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1524 documents (total 15240000 corpus positions)
    2021-05-05 22:36:57,356 : INFO : adding document #0 to Dictionary(224524 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,364 : INFO : built Dictionary(224613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1525 documents (total 15250000 corpus positions)
    2021-05-05 22:36:57,407 : INFO : adding document #0 to Dictionary(224613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,415 : INFO : built Dictionary(224657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1526 documents (total 15260000 corpus positions)
    2021-05-05 22:36:57,459 : INFO : adding document #0 to Dictionary(224657 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,466 : INFO : built Dictionary(224728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1527 documents (total 15270000 corpus positions)
    2021-05-05 22:36:57,511 : INFO : adding document #0 to Dictionary(224728 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,519 : INFO : built Dictionary(224859 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1528 documents (total 15280000 corpus positions)
    2021-05-05 22:36:57,563 : INFO : adding document #0 to Dictionary(224859 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,570 : INFO : built Dictionary(224994 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1529 documents (total 15290000 corpus positions)
    2021-05-05 22:36:57,613 : INFO : adding document #0 to Dictionary(224994 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,622 : INFO : built Dictionary(225079 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1530 documents (total 15300000 corpus positions)
    2021-05-05 22:36:57,665 : INFO : adding document #0 to Dictionary(225079 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,673 : INFO : built Dictionary(225155 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1531 documents (total 15310000 corpus positions)
    2021-05-05 22:36:57,716 : INFO : adding document #0 to Dictionary(225155 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,726 : INFO : built Dictionary(225211 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1532 documents (total 15320000 corpus positions)
    2021-05-05 22:36:57,771 : INFO : adding document #0 to Dictionary(225211 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,779 : INFO : built Dictionary(225298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1533 documents (total 15330000 corpus positions)
    2021-05-05 22:36:57,823 : INFO : adding document #0 to Dictionary(225298 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,831 : INFO : built Dictionary(225389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1534 documents (total 15340000 corpus positions)
    2021-05-05 22:36:57,875 : INFO : adding document #0 to Dictionary(225389 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,883 : INFO : built Dictionary(225487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1535 documents (total 15350000 corpus positions)
    2021-05-05 22:36:57,929 : INFO : adding document #0 to Dictionary(225487 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,937 : INFO : built Dictionary(225611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1536 documents (total 15360000 corpus positions)
    2021-05-05 22:36:57,981 : INFO : adding document #0 to Dictionary(225611 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:57,988 : INFO : built Dictionary(225677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1537 documents (total 15370000 corpus positions)
    2021-05-05 22:36:58,032 : INFO : adding document #0 to Dictionary(225677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,041 : INFO : built Dictionary(225812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1538 documents (total 15380000 corpus positions)
    2021-05-05 22:36:58,085 : INFO : adding document #0 to Dictionary(225812 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,093 : INFO : built Dictionary(225881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1539 documents (total 15390000 corpus positions)
    2021-05-05 22:36:58,142 : INFO : adding document #0 to Dictionary(225881 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,152 : INFO : built Dictionary(225947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1540 documents (total 15400000 corpus positions)
    2021-05-05 22:36:58,196 : INFO : adding document #0 to Dictionary(225947 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,203 : INFO : built Dictionary(225988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1541 documents (total 15410000 corpus positions)
    2021-05-05 22:36:58,246 : INFO : adding document #0 to Dictionary(225988 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,252 : INFO : built Dictionary(226056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1542 documents (total 15420000 corpus positions)
    2021-05-05 22:36:58,296 : INFO : adding document #0 to Dictionary(226056 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,304 : INFO : built Dictionary(226197 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1543 documents (total 15430000 corpus positions)
    2021-05-05 22:36:58,348 : INFO : adding document #0 to Dictionary(226197 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,355 : INFO : built Dictionary(226293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1544 documents (total 15440000 corpus positions)
    2021-05-05 22:36:58,400 : INFO : adding document #0 to Dictionary(226293 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,407 : INFO : built Dictionary(226384 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1545 documents (total 15450000 corpus positions)
    2021-05-05 22:36:58,450 : INFO : adding document #0 to Dictionary(226384 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,459 : INFO : built Dictionary(226442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1546 documents (total 15460000 corpus positions)
    2021-05-05 22:36:58,505 : INFO : adding document #0 to Dictionary(226442 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,512 : INFO : built Dictionary(226548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1547 documents (total 15470000 corpus positions)
    2021-05-05 22:36:58,555 : INFO : adding document #0 to Dictionary(226548 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,562 : INFO : built Dictionary(226618 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1548 documents (total 15480000 corpus positions)
    2021-05-05 22:36:58,606 : INFO : adding document #0 to Dictionary(226618 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,613 : INFO : built Dictionary(226729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1549 documents (total 15490000 corpus positions)
    2021-05-05 22:36:58,656 : INFO : adding document #0 to Dictionary(226729 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,663 : INFO : built Dictionary(226777 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1550 documents (total 15500000 corpus positions)
    2021-05-05 22:36:58,712 : INFO : adding document #0 to Dictionary(226777 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,719 : INFO : built Dictionary(226844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1551 documents (total 15510000 corpus positions)
    2021-05-05 22:36:58,762 : INFO : adding document #0 to Dictionary(226844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,769 : INFO : built Dictionary(226907 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1552 documents (total 15520000 corpus positions)
    2021-05-05 22:36:58,813 : INFO : adding document #0 to Dictionary(226907 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,820 : INFO : built Dictionary(226979 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1553 documents (total 15530000 corpus positions)
    2021-05-05 22:36:58,864 : INFO : adding document #0 to Dictionary(226979 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,872 : INFO : built Dictionary(227117 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1554 documents (total 15540000 corpus positions)
    2021-05-05 22:36:58,920 : INFO : adding document #0 to Dictionary(227117 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,926 : INFO : built Dictionary(227203 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1555 documents (total 15550000 corpus positions)
    2021-05-05 22:36:58,970 : INFO : adding document #0 to Dictionary(227203 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:58,978 : INFO : built Dictionary(227302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1556 documents (total 15560000 corpus positions)
    2021-05-05 22:36:59,025 : INFO : adding document #0 to Dictionary(227302 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,034 : INFO : built Dictionary(227343 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1557 documents (total 15570000 corpus positions)
    2021-05-05 22:36:59,078 : INFO : adding document #0 to Dictionary(227343 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,087 : INFO : built Dictionary(227388 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1558 documents (total 15580000 corpus positions)
    2021-05-05 22:36:59,131 : INFO : adding document #0 to Dictionary(227388 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,139 : INFO : built Dictionary(227443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1559 documents (total 15590000 corpus positions)
    2021-05-05 22:36:59,186 : INFO : adding document #0 to Dictionary(227443 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,193 : INFO : built Dictionary(227511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1560 documents (total 15600000 corpus positions)
    2021-05-05 22:36:59,241 : INFO : adding document #0 to Dictionary(227511 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,250 : INFO : built Dictionary(227601 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1561 documents (total 15610000 corpus positions)
    2021-05-05 22:36:59,295 : INFO : adding document #0 to Dictionary(227601 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,303 : INFO : built Dictionary(227694 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1562 documents (total 15620000 corpus positions)
    2021-05-05 22:36:59,348 : INFO : adding document #0 to Dictionary(227694 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,356 : INFO : built Dictionary(227766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1563 documents (total 15630000 corpus positions)
    2021-05-05 22:36:59,399 : INFO : adding document #0 to Dictionary(227766 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,408 : INFO : built Dictionary(227861 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1564 documents (total 15640000 corpus positions)
    2021-05-05 22:36:59,452 : INFO : adding document #0 to Dictionary(227861 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,460 : INFO : built Dictionary(227972 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1565 documents (total 15650000 corpus positions)
    2021-05-05 22:36:59,503 : INFO : adding document #0 to Dictionary(227972 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,511 : INFO : built Dictionary(228037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1566 documents (total 15660000 corpus positions)
    2021-05-05 22:36:59,555 : INFO : adding document #0 to Dictionary(228037 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,563 : INFO : built Dictionary(228148 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1567 documents (total 15670000 corpus positions)
    2021-05-05 22:36:59,606 : INFO : adding document #0 to Dictionary(228148 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,616 : INFO : built Dictionary(228314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1568 documents (total 15680000 corpus positions)
    2021-05-05 22:36:59,658 : INFO : adding document #0 to Dictionary(228314 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,665 : INFO : built Dictionary(228358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1569 documents (total 15690000 corpus positions)
    2021-05-05 22:36:59,709 : INFO : adding document #0 to Dictionary(228358 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,717 : INFO : built Dictionary(228433 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1570 documents (total 15700000 corpus positions)
    2021-05-05 22:36:59,764 : INFO : adding document #0 to Dictionary(228433 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,772 : INFO : built Dictionary(228492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1571 documents (total 15710000 corpus positions)
    2021-05-05 22:36:59,816 : INFO : adding document #0 to Dictionary(228492 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,824 : INFO : built Dictionary(228556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1572 documents (total 15720000 corpus positions)
    2021-05-05 22:36:59,867 : INFO : adding document #0 to Dictionary(228556 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,875 : INFO : built Dictionary(228620 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1573 documents (total 15730000 corpus positions)
    2021-05-05 22:36:59,925 : INFO : adding document #0 to Dictionary(228620 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,933 : INFO : built Dictionary(228677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1574 documents (total 15740000 corpus positions)
    2021-05-05 22:36:59,981 : INFO : adding document #0 to Dictionary(228677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:36:59,989 : INFO : built Dictionary(228771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1575 documents (total 15750000 corpus positions)
    2021-05-05 22:37:00,035 : INFO : adding document #0 to Dictionary(228771 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,043 : INFO : built Dictionary(228834 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1576 documents (total 15760000 corpus positions)
    2021-05-05 22:37:00,087 : INFO : adding document #0 to Dictionary(228834 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,096 : INFO : built Dictionary(228891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1577 documents (total 15770000 corpus positions)
    2021-05-05 22:37:00,140 : INFO : adding document #0 to Dictionary(228891 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,148 : INFO : built Dictionary(228984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1578 documents (total 15780000 corpus positions)
    2021-05-05 22:37:00,193 : INFO : adding document #0 to Dictionary(228984 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,201 : INFO : built Dictionary(229022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1579 documents (total 15790000 corpus positions)
    2021-05-05 22:37:00,249 : INFO : adding document #0 to Dictionary(229022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,257 : INFO : built Dictionary(229076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1580 documents (total 15800000 corpus positions)
    2021-05-05 22:37:00,301 : INFO : adding document #0 to Dictionary(229076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,309 : INFO : built Dictionary(229136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1581 documents (total 15810000 corpus positions)
    2021-05-05 22:37:00,355 : INFO : adding document #0 to Dictionary(229136 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,363 : INFO : built Dictionary(229212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1582 documents (total 15820000 corpus positions)
    2021-05-05 22:37:00,405 : INFO : adding document #0 to Dictionary(229212 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,413 : INFO : built Dictionary(229281 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1583 documents (total 15830000 corpus positions)
    2021-05-05 22:37:00,459 : INFO : adding document #0 to Dictionary(229281 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,469 : INFO : built Dictionary(229356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1584 documents (total 15840000 corpus positions)
    2021-05-05 22:37:00,515 : INFO : adding document #0 to Dictionary(229356 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,524 : INFO : built Dictionary(229427 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1585 documents (total 15850000 corpus positions)
    2021-05-05 22:37:00,568 : INFO : adding document #0 to Dictionary(229427 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,575 : INFO : built Dictionary(229466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1586 documents (total 15860000 corpus positions)
    2021-05-05 22:37:00,619 : INFO : adding document #0 to Dictionary(229466 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,627 : INFO : built Dictionary(229520 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1587 documents (total 15870000 corpus positions)
    2021-05-05 22:37:00,672 : INFO : adding document #0 to Dictionary(229520 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,681 : INFO : built Dictionary(229613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1588 documents (total 15880000 corpus positions)
    2021-05-05 22:37:00,725 : INFO : adding document #0 to Dictionary(229613 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,733 : INFO : built Dictionary(229697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1589 documents (total 15890000 corpus positions)
    2021-05-05 22:37:00,776 : INFO : adding document #0 to Dictionary(229697 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,784 : INFO : built Dictionary(229759 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1590 documents (total 15900000 corpus positions)
    2021-05-05 22:37:00,827 : INFO : adding document #0 to Dictionary(229759 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,839 : INFO : built Dictionary(229872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1591 documents (total 15910000 corpus positions)
    2021-05-05 22:37:00,884 : INFO : adding document #0 to Dictionary(229872 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,892 : INFO : built Dictionary(229955 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1592 documents (total 15920000 corpus positions)
    2021-05-05 22:37:00,936 : INFO : adding document #0 to Dictionary(229955 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,944 : INFO : built Dictionary(230050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1593 documents (total 15930000 corpus positions)
    2021-05-05 22:37:00,989 : INFO : adding document #0 to Dictionary(230050 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:00,996 : INFO : built Dictionary(230114 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1594 documents (total 15940000 corpus positions)
    2021-05-05 22:37:01,048 : INFO : adding document #0 to Dictionary(230114 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,058 : INFO : built Dictionary(230202 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1595 documents (total 15950000 corpus positions)
    2021-05-05 22:37:01,101 : INFO : adding document #0 to Dictionary(230202 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,109 : INFO : built Dictionary(230258 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1596 documents (total 15960000 corpus positions)
    2021-05-05 22:37:01,156 : INFO : adding document #0 to Dictionary(230258 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,164 : INFO : built Dictionary(230350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1597 documents (total 15970000 corpus positions)
    2021-05-05 22:37:01,207 : INFO : adding document #0 to Dictionary(230350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,216 : INFO : built Dictionary(230430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1598 documents (total 15980000 corpus positions)
    2021-05-05 22:37:01,259 : INFO : adding document #0 to Dictionary(230430 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,267 : INFO : built Dictionary(230552 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1599 documents (total 15990000 corpus positions)
    2021-05-05 22:37:01,312 : INFO : adding document #0 to Dictionary(230552 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,320 : INFO : built Dictionary(230612 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1600 documents (total 16000000 corpus positions)
    2021-05-05 22:37:01,362 : INFO : adding document #0 to Dictionary(230612 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,370 : INFO : built Dictionary(230649 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1601 documents (total 16010000 corpus positions)
    2021-05-05 22:37:01,414 : INFO : adding document #0 to Dictionary(230649 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,422 : INFO : built Dictionary(230711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1602 documents (total 16020000 corpus positions)
    2021-05-05 22:37:01,465 : INFO : adding document #0 to Dictionary(230711 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,476 : INFO : built Dictionary(230832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1603 documents (total 16030000 corpus positions)
    2021-05-05 22:37:01,519 : INFO : adding document #0 to Dictionary(230832 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,526 : INFO : built Dictionary(230847 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1604 documents (total 16040000 corpus positions)
    2021-05-05 22:37:01,570 : INFO : adding document #0 to Dictionary(230847 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,579 : INFO : built Dictionary(230953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1605 documents (total 16050000 corpus positions)
    2021-05-05 22:37:01,622 : INFO : adding document #0 to Dictionary(230953 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,630 : INFO : built Dictionary(231024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1606 documents (total 16060000 corpus positions)
    2021-05-05 22:37:01,673 : INFO : adding document #0 to Dictionary(231024 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,683 : INFO : built Dictionary(231054 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1607 documents (total 16070000 corpus positions)
    2021-05-05 22:37:01,726 : INFO : adding document #0 to Dictionary(231054 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,734 : INFO : built Dictionary(231083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1608 documents (total 16080000 corpus positions)
    2021-05-05 22:37:01,777 : INFO : adding document #0 to Dictionary(231083 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,785 : INFO : built Dictionary(231150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1609 documents (total 16090000 corpus positions)
    2021-05-05 22:37:01,830 : INFO : adding document #0 to Dictionary(231150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,839 : INFO : built Dictionary(231292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1610 documents (total 16100000 corpus positions)
    2021-05-05 22:37:01,881 : INFO : adding document #0 to Dictionary(231292 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,890 : INFO : built Dictionary(231396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1611 documents (total 16110000 corpus positions)
    2021-05-05 22:37:01,933 : INFO : adding document #0 to Dictionary(231396 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,941 : INFO : built Dictionary(231475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1612 documents (total 16120000 corpus positions)
    2021-05-05 22:37:01,985 : INFO : adding document #0 to Dictionary(231475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:01,993 : INFO : built Dictionary(231527 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1613 documents (total 16130000 corpus positions)
    2021-05-05 22:37:02,037 : INFO : adding document #0 to Dictionary(231527 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,045 : INFO : built Dictionary(231578 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1614 documents (total 16140000 corpus positions)
    2021-05-05 22:37:02,089 : INFO : adding document #0 to Dictionary(231578 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,096 : INFO : built Dictionary(231621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1615 documents (total 16150000 corpus positions)
    2021-05-05 22:37:02,141 : INFO : adding document #0 to Dictionary(231621 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,149 : INFO : built Dictionary(231677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1616 documents (total 16160000 corpus positions)
    2021-05-05 22:37:02,193 : INFO : adding document #0 to Dictionary(231677 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,201 : INFO : built Dictionary(231794 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1617 documents (total 16170000 corpus positions)
    2021-05-05 22:37:02,245 : INFO : adding document #0 to Dictionary(231794 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,253 : INFO : built Dictionary(231892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1618 documents (total 16180000 corpus positions)
    2021-05-05 22:37:02,296 : INFO : adding document #0 to Dictionary(231892 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,304 : INFO : built Dictionary(231983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1619 documents (total 16190000 corpus positions)
    2021-05-05 22:37:02,351 : INFO : adding document #0 to Dictionary(231983 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,361 : INFO : built Dictionary(232099 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1620 documents (total 16200000 corpus positions)
    2021-05-05 22:37:02,404 : INFO : adding document #0 to Dictionary(232099 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,412 : INFO : built Dictionary(232182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1621 documents (total 16210000 corpus positions)
    2021-05-05 22:37:02,456 : INFO : adding document #0 to Dictionary(232182 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,463 : INFO : built Dictionary(232252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1622 documents (total 16220000 corpus positions)
    2021-05-05 22:37:02,507 : INFO : adding document #0 to Dictionary(232252 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,515 : INFO : built Dictionary(232329 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1623 documents (total 16230000 corpus positions)
    2021-05-05 22:37:02,558 : INFO : adding document #0 to Dictionary(232329 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,567 : INFO : built Dictionary(232478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1624 documents (total 16240000 corpus positions)
    2021-05-05 22:37:02,615 : INFO : adding document #0 to Dictionary(232478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,622 : INFO : built Dictionary(232516 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1625 documents (total 16250000 corpus positions)
    2021-05-05 22:37:02,665 : INFO : adding document #0 to Dictionary(232516 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,673 : INFO : built Dictionary(232568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1626 documents (total 16260000 corpus positions)
    2021-05-05 22:37:02,717 : INFO : adding document #0 to Dictionary(232568 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,725 : INFO : built Dictionary(232632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1627 documents (total 16270000 corpus positions)
    2021-05-05 22:37:02,769 : INFO : adding document #0 to Dictionary(232632 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,777 : INFO : built Dictionary(232672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1628 documents (total 16280000 corpus positions)
    2021-05-05 22:37:02,821 : INFO : adding document #0 to Dictionary(232672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,829 : INFO : built Dictionary(232713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1629 documents (total 16290000 corpus positions)
    2021-05-05 22:37:02,875 : INFO : adding document #0 to Dictionary(232713 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,883 : INFO : built Dictionary(232844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1630 documents (total 16300000 corpus positions)
    2021-05-05 22:37:02,931 : INFO : adding document #0 to Dictionary(232844 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,940 : INFO : built Dictionary(232908 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1631 documents (total 16310000 corpus positions)
    2021-05-05 22:37:02,983 : INFO : adding document #0 to Dictionary(232908 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:02,990 : INFO : built Dictionary(232962 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1632 documents (total 16320000 corpus positions)
    2021-05-05 22:37:03,038 : INFO : adding document #0 to Dictionary(232962 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,046 : INFO : built Dictionary(233022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1633 documents (total 16330000 corpus positions)
    2021-05-05 22:37:03,090 : INFO : adding document #0 to Dictionary(233022 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,097 : INFO : built Dictionary(233076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1634 documents (total 16340000 corpus positions)
    2021-05-05 22:37:03,141 : INFO : adding document #0 to Dictionary(233076 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,149 : INFO : built Dictionary(233211 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1635 documents (total 16350000 corpus positions)
    2021-05-05 22:37:03,193 : INFO : adding document #0 to Dictionary(233211 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,201 : INFO : built Dictionary(233276 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1636 documents (total 16360000 corpus positions)
    2021-05-05 22:37:03,246 : INFO : adding document #0 to Dictionary(233276 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,255 : INFO : built Dictionary(233413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1637 documents (total 16370000 corpus positions)
    2021-05-05 22:37:03,301 : INFO : adding document #0 to Dictionary(233413 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,310 : INFO : built Dictionary(233476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1638 documents (total 16380000 corpus positions)
    2021-05-05 22:37:03,354 : INFO : adding document #0 to Dictionary(233476 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,363 : INFO : built Dictionary(233582 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1639 documents (total 16390000 corpus positions)
    2021-05-05 22:37:03,410 : INFO : adding document #0 to Dictionary(233582 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,419 : INFO : built Dictionary(233673 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1640 documents (total 16400000 corpus positions)
    2021-05-05 22:37:03,463 : INFO : adding document #0 to Dictionary(233673 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,470 : INFO : built Dictionary(233772 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1641 documents (total 16410000 corpus positions)
    2021-05-05 22:37:03,515 : INFO : adding document #0 to Dictionary(233772 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,524 : INFO : built Dictionary(233858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1642 documents (total 16420000 corpus positions)
    2021-05-05 22:37:03,573 : INFO : adding document #0 to Dictionary(233858 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,581 : INFO : built Dictionary(233930 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1643 documents (total 16430000 corpus positions)
    2021-05-05 22:37:03,625 : INFO : adding document #0 to Dictionary(233930 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,632 : INFO : built Dictionary(233985 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1644 documents (total 16440000 corpus positions)
    2021-05-05 22:37:03,677 : INFO : adding document #0 to Dictionary(233985 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,686 : INFO : built Dictionary(234073 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1645 documents (total 16450000 corpus positions)
    2021-05-05 22:37:03,735 : INFO : adding document #0 to Dictionary(234073 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,744 : INFO : built Dictionary(234141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1646 documents (total 16460000 corpus positions)
    2021-05-05 22:37:03,791 : INFO : adding document #0 to Dictionary(234141 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,798 : INFO : built Dictionary(234193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1647 documents (total 16470000 corpus positions)
    2021-05-05 22:37:03,848 : INFO : adding document #0 to Dictionary(234193 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,855 : INFO : built Dictionary(234268 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1648 documents (total 16480000 corpus positions)
    2021-05-05 22:37:03,899 : INFO : adding document #0 to Dictionary(234268 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,908 : INFO : built Dictionary(234367 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1649 documents (total 16490000 corpus positions)
    2021-05-05 22:37:03,952 : INFO : adding document #0 to Dictionary(234367 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:03,959 : INFO : built Dictionary(234435 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1650 documents (total 16500000 corpus positions)
    2021-05-05 22:37:04,003 : INFO : adding document #0 to Dictionary(234435 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,012 : INFO : built Dictionary(234530 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1651 documents (total 16510000 corpus positions)
    2021-05-05 22:37:04,058 : INFO : adding document #0 to Dictionary(234530 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,066 : INFO : built Dictionary(234598 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1652 documents (total 16520000 corpus positions)
    2021-05-05 22:37:04,110 : INFO : adding document #0 to Dictionary(234598 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,119 : INFO : built Dictionary(234672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1653 documents (total 16530000 corpus positions)
    2021-05-05 22:37:04,163 : INFO : adding document #0 to Dictionary(234672 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,172 : INFO : built Dictionary(234770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1654 documents (total 16540000 corpus positions)
    2021-05-05 22:37:04,215 : INFO : adding document #0 to Dictionary(234770 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,224 : INFO : built Dictionary(234855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1655 documents (total 16550000 corpus positions)
    2021-05-05 22:37:04,266 : INFO : adding document #0 to Dictionary(234855 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,274 : INFO : built Dictionary(234937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1656 documents (total 16560000 corpus positions)
    2021-05-05 22:37:04,317 : INFO : adding document #0 to Dictionary(234937 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,326 : INFO : built Dictionary(235036 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1657 documents (total 16570000 corpus positions)
    2021-05-05 22:37:04,369 : INFO : adding document #0 to Dictionary(235036 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,376 : INFO : built Dictionary(235093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1658 documents (total 16580000 corpus positions)
    2021-05-05 22:37:04,423 : INFO : adding document #0 to Dictionary(235093 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,431 : INFO : built Dictionary(235150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1659 documents (total 16590000 corpus positions)
    2021-05-05 22:37:04,492 : INFO : adding document #0 to Dictionary(235150 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,501 : INFO : built Dictionary(235224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1660 documents (total 16600000 corpus positions)
    2021-05-05 22:37:04,553 : INFO : adding document #0 to Dictionary(235224 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,561 : INFO : built Dictionary(235268 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1661 documents (total 16610000 corpus positions)
    2021-05-05 22:37:04,607 : INFO : adding document #0 to Dictionary(235268 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,615 : INFO : built Dictionary(235318 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1662 documents (total 16620000 corpus positions)
    2021-05-05 22:37:04,663 : INFO : adding document #0 to Dictionary(235318 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,671 : INFO : built Dictionary(235392 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1663 documents (total 16630000 corpus positions)
    2021-05-05 22:37:04,718 : INFO : adding document #0 to Dictionary(235392 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,725 : INFO : built Dictionary(235471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1664 documents (total 16640000 corpus positions)
    2021-05-05 22:37:04,770 : INFO : adding document #0 to Dictionary(235471 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,779 : INFO : built Dictionary(235517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1665 documents (total 16650000 corpus positions)
    2021-05-05 22:37:04,828 : INFO : adding document #0 to Dictionary(235517 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,840 : INFO : built Dictionary(235654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1666 documents (total 16660000 corpus positions)
    2021-05-05 22:37:04,914 : INFO : adding document #0 to Dictionary(235654 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:04,928 : INFO : built Dictionary(235778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1667 documents (total 16670000 corpus positions)
    2021-05-05 22:37:04,998 : INFO : adding document #0 to Dictionary(235778 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,008 : INFO : built Dictionary(235926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1668 documents (total 16680000 corpus positions)
    2021-05-05 22:37:05,058 : INFO : adding document #0 to Dictionary(235926 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,068 : INFO : built Dictionary(236046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1669 documents (total 16690000 corpus positions)
    2021-05-05 22:37:05,128 : INFO : adding document #0 to Dictionary(236046 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,147 : INFO : built Dictionary(236267 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1670 documents (total 16700000 corpus positions)
    2021-05-05 22:37:05,218 : INFO : adding document #0 to Dictionary(236267 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,235 : INFO : built Dictionary(236350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1671 documents (total 16710000 corpus positions)
    2021-05-05 22:37:05,349 : INFO : adding document #0 to Dictionary(236350 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,368 : INFO : built Dictionary(236474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1672 documents (total 16720000 corpus positions)
    2021-05-05 22:37:05,436 : INFO : adding document #0 to Dictionary(236474 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,444 : INFO : built Dictionary(236554 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1673 documents (total 16730000 corpus positions)
    2021-05-05 22:37:05,494 : INFO : adding document #0 to Dictionary(236554 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,505 : INFO : built Dictionary(236726 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1674 documents (total 16740000 corpus positions)
    2021-05-05 22:37:05,554 : INFO : adding document #0 to Dictionary(236726 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,566 : INFO : built Dictionary(236813 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1675 documents (total 16750000 corpus positions)
    2021-05-05 22:37:05,618 : INFO : adding document #0 to Dictionary(236813 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,627 : INFO : built Dictionary(236859 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1676 documents (total 16760000 corpus positions)
    2021-05-05 22:37:05,677 : INFO : adding document #0 to Dictionary(236859 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,686 : INFO : built Dictionary(236938 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1677 documents (total 16770000 corpus positions)
    2021-05-05 22:37:05,731 : INFO : adding document #0 to Dictionary(236938 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,740 : INFO : built Dictionary(237039 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1678 documents (total 16780000 corpus positions)
    2021-05-05 22:37:05,790 : INFO : adding document #0 to Dictionary(237039 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,799 : INFO : built Dictionary(237124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1679 documents (total 16790000 corpus positions)
    2021-05-05 22:37:05,851 : INFO : adding document #0 to Dictionary(237124 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,860 : INFO : built Dictionary(237163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1680 documents (total 16800000 corpus positions)
    2021-05-05 22:37:05,910 : INFO : adding document #0 to Dictionary(237163 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,918 : INFO : built Dictionary(237228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1681 documents (total 16810000 corpus positions)
    2021-05-05 22:37:05,966 : INFO : adding document #0 to Dictionary(237228 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:05,976 : INFO : built Dictionary(237319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1682 documents (total 16820000 corpus positions)
    2021-05-05 22:37:06,021 : INFO : adding document #0 to Dictionary(237319 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,030 : INFO : built Dictionary(237365 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1683 documents (total 16830000 corpus positions)
    2021-05-05 22:37:06,073 : INFO : adding document #0 to Dictionary(237365 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,082 : INFO : built Dictionary(237475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1684 documents (total 16840000 corpus positions)
    2021-05-05 22:37:06,132 : INFO : adding document #0 to Dictionary(237475 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,142 : INFO : built Dictionary(237547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1685 documents (total 16850000 corpus positions)
    2021-05-05 22:37:06,190 : INFO : adding document #0 to Dictionary(237547 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,199 : INFO : built Dictionary(237604 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1686 documents (total 16860000 corpus positions)
    2021-05-05 22:37:06,244 : INFO : adding document #0 to Dictionary(237604 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,252 : INFO : built Dictionary(237692 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1687 documents (total 16870000 corpus positions)
    2021-05-05 22:37:06,300 : INFO : adding document #0 to Dictionary(237692 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,310 : INFO : built Dictionary(237745 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1688 documents (total 16880000 corpus positions)
    2021-05-05 22:37:06,360 : INFO : adding document #0 to Dictionary(237745 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,373 : INFO : built Dictionary(237824 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1689 documents (total 16890000 corpus positions)
    2021-05-05 22:37:06,435 : INFO : adding document #0 to Dictionary(237824 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,444 : INFO : built Dictionary(237889 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1690 documents (total 16900000 corpus positions)
    2021-05-05 22:37:06,499 : INFO : adding document #0 to Dictionary(237889 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,508 : INFO : built Dictionary(237942 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1691 documents (total 16910000 corpus positions)
    2021-05-05 22:37:06,558 : INFO : adding document #0 to Dictionary(237942 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,567 : INFO : built Dictionary(237985 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1692 documents (total 16920000 corpus positions)
    2021-05-05 22:37:06,613 : INFO : adding document #0 to Dictionary(237985 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,623 : INFO : built Dictionary(238081 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1693 documents (total 16930000 corpus positions)
    2021-05-05 22:37:06,672 : INFO : adding document #0 to Dictionary(238081 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,682 : INFO : built Dictionary(238181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1694 documents (total 16940000 corpus positions)
    2021-05-05 22:37:06,731 : INFO : adding document #0 to Dictionary(238181 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,742 : INFO : built Dictionary(238244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1695 documents (total 16950000 corpus positions)
    2021-05-05 22:37:06,806 : INFO : adding document #0 to Dictionary(238244 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,817 : INFO : built Dictionary(238315 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1696 documents (total 16960000 corpus positions)
    2021-05-05 22:37:06,876 : INFO : adding document #0 to Dictionary(238315 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,888 : INFO : built Dictionary(238375 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1697 documents (total 16970000 corpus positions)
    2021-05-05 22:37:06,948 : INFO : adding document #0 to Dictionary(238375 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:06,959 : INFO : built Dictionary(238425 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1698 documents (total 16980000 corpus positions)
    2021-05-05 22:37:07,023 : INFO : adding document #0 to Dictionary(238425 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:07,034 : INFO : built Dictionary(238478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1699 documents (total 16990000 corpus positions)
    2021-05-05 22:37:07,088 : INFO : adding document #0 to Dictionary(238478 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:07,098 : INFO : built Dictionary(238524 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1700 documents (total 17000000 corpus positions)
    2021-05-05 22:37:07,125 : INFO : adding document #0 to Dictionary(238524 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...)
    2021-05-05 22:37:07,134 : INFO : built Dictionary(238542 unique tokens: ['a', 'abacus', 'ability', 'able', 'abnormal']...) from 1701 documents (total 17005207 corpus positions)
    2021-05-05 22:37:07,538 : INFO : discarding 218466 tokens: [('a', 1701), ('ability', 934), ('able', 1202), ('about', 1687), ('above', 1327), ('abstention', 13), ('accepted', 945), ('according', 1468), ('account', 1113), ('act', 1312)]...
    2021-05-05 22:37:07,538 : INFO : keeping 20076 tokens which were in no less than 20 and no more than 850 (=50.0%) documents
    2021-05-05 22:37:07,649 : INFO : resulting dictionary: Dictionary(20076 unique tokens: ['abacus', 'abnormal', 'abolished', 'abolition', 'absence']...)




Training
--------

Training the ensemble works very similar to training a single model,

You can use any model that is based on LdaModel, such as LdaMulticore, to train the Ensemble.
In experiments, LdaMulticore showed better results.



.. code-block:: default


    from gensim.models import LdaModel
    topic_model_class = LdaModel








Any arbitrary number of models can be used, but it should be a multiple of your workers so that the
load can be distributed properly. In this example, 4 processes will train 8 models each.



.. code-block:: default


    ensemble_workers = 4
    num_models = 8








After training all the models, some distance computations are required which can take quite some
time as well. You can speed this up by using workers for that as well.



.. code-block:: default


    distance_workers = 4








All other parameters that are unknown to EnsembleLda are forwarded to each LDA Model, such as



.. code-block:: default

    num_topics = 20
    passes = 2








Now start the training

Since 20 topics were trained on each of the 8 models, we expect there to be 160 different topics.
The number of stable topics which are clustered from all those topics is smaller.



.. code-block:: default


    from gensim.models import EnsembleLda
    ensemble = EnsembleLda(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        passes=passes,
        num_models=num_models,
        topic_model_class=LdaModel,
        ensemble_workers=ensemble_workers,
        distance_workers=distance_workers
    )

    print(len(ensemble.ttda))
    print(len(ensemble.get_topics()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-05-05 22:37:17,535 : INFO : generating 8 topic models...
    2021-05-05 22:41:11,338 : INFO : generating a 160 x 160 asymmetric distance matrix...
    2021-05-05 22:41:13,465 : INFO : fitting the clustering model, using 4 for min_samples
    2021-05-05 22:41:13,529 : INFO : generating stable topics, using 3 for min_cores
    2021-05-05 22:41:13,530 : INFO : found 3 clusters
    2021-05-05 22:41:13,579 : INFO : found 1 stable topics
    2021-05-05 22:41:13,584 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-05-05 22:41:14,020 : INFO : using symmetric alpha at 1.0
    2021-05-05 22:41:14,020 : INFO : using symmetric eta at 1.0
    2021-05-05 22:41:14,025 : INFO : using serial LDA version on this node
    2021-05-05 22:41:14,028 : INFO : running online (multi-pass) LDA training, 1 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-05-05 22:41:14,028 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-05-05 22:41:14,039 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=20076, num_topics=1, decay=0.5, chunksize=2000) in 0.00s', 'datetime': '2021-05-05T22:41:14.029108', 'gensim': '4.1.0.dev0', 'python': '3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]', 'platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'event': 'created'}
    160
    1




Tuning
------

Different from LdaModel, the number of resulting topics varies greatly depending on the clustering parameters.

You can provide those in the ``recluster()`` function or the ``EnsembleLda`` constructor.

Play around until you get as many topics as you desire, which however may reduce their quality.
If your ensemble doesn't have enough topics to begin with, you should make sure to make it large enough.

Having an epsilon that is smaller than the smallest distance doesn't make sense.
Make sure to chose one that is within the range of values in ``asymmetric_distance_matrix``.



.. code-block:: default


    import numpy as np
    shape = ensemble.asymmetric_distance_matrix.shape
    without_diagonal = ensemble.asymmetric_distance_matrix[~np.eye(shape[0], dtype=bool)].reshape(shape[0], -1)
    print(without_diagonal.min(), without_diagonal.mean(), without_diagonal.max())

    ensemble.recluster(eps=0.09, min_samples=2, min_cores=2)

    print(len(ensemble.get_topics()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.006134824668422967 0.03832085602841112 0.20069650782257142
    2021-05-05 22:41:14,160 : INFO : fitting the clustering model
    2021-05-05 22:41:14,220 : INFO : generating stable topics
    2021-05-05 22:41:14,221 : INFO : found 3 clusters
    2021-05-05 22:41:14,299 : INFO : found 1 stable topics
    2021-05-05 22:41:14,304 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-05-05 22:41:14,309 : INFO : using symmetric alpha at 1.0
    2021-05-05 22:41:14,310 : INFO : using symmetric eta at 1.0
    2021-05-05 22:41:14,314 : INFO : using serial LDA version on this node
    2021-05-05 22:41:14,317 : INFO : running online (multi-pass) LDA training, 1 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-05-05 22:41:14,317 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-05-05 22:41:14,318 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=20076, num_topics=1, decay=0.5, chunksize=2000) in 0.00s', 'datetime': '2021-05-05T22:41:14.318036', 'gensim': '4.1.0.dev0', 'python': '3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]', 'platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'event': 'created'}
    1




Increasing the Size
-------------------

If you have some models lying around that were trained on a corpus based on the same dictionary,
they are compatible and you can add them to the ensemble.

By setting num_models of the EnsembleLda constructor to 0 you can also create an ensemble that is
entirely made out of your existing topic models with the following method.

Afterwards the number and quality of stable topics might be different depending on your added topics and parameters.



.. code-block:: default


    from gensim.models import LdaMulticore

    model1 = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=9,
        passes=4,
    )

    model2 = LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=11,
        passes=2,
    )

    # add_model supports various types of input, check out its docstring
    ensemble.add_model(model1)
    ensemble.add_model(model2)

    ensemble.recluster()

    print(len(ensemble.ttda))
    print(len(ensemble.get_topics()))




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-05-05 22:41:14,506 : INFO : using symmetric alpha at 0.1111111111111111
    2021-05-05 22:41:14,507 : INFO : using symmetric eta at 0.1111111111111111
    2021-05-05 22:41:14,512 : INFO : using serial LDA version on this node
    2021-05-05 22:41:14,538 : INFO : running online LDA training, 9 topics, 4 passes over the supplied corpus of 1701 documents, updating every 14000 documents, evaluating every ~1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-05-05 22:41:14,538 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-05-05 22:41:14,542 : INFO : training LDA model using 7 processes
    2021-05-05 22:41:14,574 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-05-05 22:41:26,440 : INFO : topic #5 (0.111): 0.033*"as" + 0.001*"km" + 0.001*"energy" + 0.001*"emperor" + 0.001*"actor" + 0.001*"economy" + 0.001*"china" + 0.001*"league" + 0.001*"bc" + 0.001*"soviet"
    2021-05-05 22:41:26,441 : INFO : topic #1 (0.111): 0.030*"as" + 0.001*"km" + 0.001*"est" + 0.001*"y" + 0.001*"minister" + 0.001*"actor" + 0.001*"female" + 0.001*"spanish" + 0.001*"bc" + 0.001*"league"
    2021-05-05 22:41:26,441 : INFO : topic #2 (0.111): 0.032*"as" + 0.001*"china" + 0.001*"israel" + 0.001*"india" + 0.001*"band" + 0.001*"software" + 0.001*"energy" + 0.001*"km" + 0.001*"league" + 0.001*"female"
    2021-05-05 22:41:26,442 : INFO : topic #3 (0.111): 0.033*"as" + 0.001*"league" + 0.001*"soviet" + 0.001*"software" + 0.001*"chinese" + 0.001*"km" + 0.001*"japanese" + 0.001*"energy" + 0.001*"spanish" + 0.001*"y"
    2021-05-05 22:41:26,443 : INFO : topic #7 (0.111): 0.029*"as" + 0.001*"league" + 0.001*"km" + 0.001*"actor" + 0.001*"soviet" + 0.001*"season" + 0.001*"band" + 0.001*"emperor" + 0.001*"jewish" + 0.001*"la"
    2021-05-05 22:41:26,443 : INFO : topic diff=1.026745, rho=1.000000
    2021-05-05 22:41:46,618 : INFO : -9.132 per-word bound, 561.0 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:41:46,619 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-05-05 22:41:58,023 : INFO : topic #0 (0.111): 0.038*"as" + 0.002*"jewish" + 0.001*"jesus" + 0.001*"israel" + 0.001*"australia" + 0.001*"y" + 0.001*"km" + 0.001*"software" + 0.001*"bible" + 0.001*"judaism"
    2021-05-05 22:41:58,024 : INFO : topic #2 (0.111): 0.032*"as" + 0.001*"israel" + 0.001*"china" + 0.001*"import" + 0.001*"kong" + 0.001*"hong" + 0.001*"india" + 0.001*"apollo" + 0.001*"band" + 0.001*"aircraft"
    2021-05-05 22:41:58,024 : INFO : topic #4 (0.111): 0.030*"as" + 0.001*"software" + 0.001*"philosophy" + 0.001*"india" + 0.001*"y" + 0.001*"indian" + 0.001*"lincoln" + 0.001*"dna" + 0.001*"scientific" + 0.001*"bc"
    2021-05-05 22:41:58,025 : INFO : topic #6 (0.111): 0.023*"as" + 0.002*"soviet" + 0.002*"km" + 0.002*"est" + 0.001*"economy" + 0.001*"russian" + 0.001*"chinese" + 0.001*"y" + 0.001*"africa" + 0.001*"minister"
    2021-05-05 22:41:58,025 : INFO : topic #7 (0.111): 0.028*"as" + 0.002*"league" + 0.002*"actor" + 0.002*"season" + 0.001*"baseball" + 0.001*"band" + 0.001*"singer" + 0.001*"football" + 0.001*"actress" + 0.001*"album"
    2021-05-05 22:41:58,026 : INFO : topic diff=0.163646, rho=0.592297
    2021-05-05 22:42:18,212 : INFO : -9.061 per-word bound, 534.0 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:42:18,213 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-05-05 22:42:29,923 : INFO : topic #6 (0.111): 0.024*"as" + 0.003*"soviet" + 0.002*"km" + 0.002*"est" + 0.002*"economy" + 0.001*"russian" + 0.001*"minister" + 0.001*"elected" + 0.001*"iraq" + 0.001*"election"
    2021-05-05 22:42:29,924 : INFO : topic #8 (0.111): 0.028*"as" + 0.003*"km" + 0.003*"est" + 0.002*"economy" + 0.002*"africa" + 0.002*"microsoft" + 0.002*"growth" + 0.001*"female" + 0.001*"soviet" + 0.001*"finland"
    2021-05-05 22:42:29,924 : INFO : topic #4 (0.111): 0.032*"as" + 0.001*"y" + 0.001*"philosophy" + 0.001*"software" + 0.001*"dna" + 0.001*"india" + 0.001*"energy" + 0.001*"scientific" + 0.001*"lincoln" + 0.001*"frac"
    2021-05-05 22:42:29,925 : INFO : topic #1 (0.111): 0.030*"as" + 0.001*"apple" + 0.001*"finalist" + 0.001*"mac" + 0.001*"km" + 0.001*"australian" + 0.001*"cuba" + 0.001*"os" + 0.001*"address" + 0.001*"software"
    2021-05-05 22:42:29,926 : INFO : topic #7 (0.111): 0.027*"as" + 0.003*"league" + 0.003*"actor" + 0.002*"season" + 0.002*"baseball" + 0.002*"football" + 0.002*"singer" + 0.002*"band" + 0.002*"actress" + 0.002*"album"
    2021-05-05 22:42:29,926 : INFO : topic diff=0.192558, rho=0.509614
    2021-05-05 22:42:50,260 : INFO : -9.000 per-word bound, 512.0 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:42:50,261 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #1701/1701, outstanding queue size 1
    2021-05-05 22:43:01,567 : INFO : topic #0 (0.111): 0.039*"as" + 0.004*"jewish" + 0.002*"jesus" + 0.002*"israel" + 0.002*"christ" + 0.002*"bible" + 0.002*"judaism" + 0.002*"hebrew" + 0.001*"orthodox" + 0.001*"holy"
    2021-05-05 22:43:01,567 : INFO : topic #4 (0.111): 0.032*"as" + 0.002*"y" + 0.002*"philosophy" + 0.001*"energy" + 0.001*"frac" + 0.001*"dna" + 0.001*"scientific" + 0.001*"cell" + 0.001*"software" + 0.001*"evolution"
    2021-05-05 22:43:01,568 : INFO : topic #2 (0.111): 0.031*"as" + 0.002*"apollo" + 0.002*"import" + 0.002*"kong" + 0.002*"hong" + 0.001*"aircraft" + 0.001*"china" + 0.001*"moon" + 0.001*"israel" + 0.001*"chinese"
    2021-05-05 22:43:01,569 : INFO : topic #6 (0.111): 0.024*"as" + 0.003*"soviet" + 0.003*"km" + 0.003*"est" + 0.002*"economy" + 0.002*"minister" + 0.002*"russian" + 0.002*"elected" + 0.002*"constitution" + 0.002*"election"
    2021-05-05 22:43:01,569 : INFO : topic #5 (0.111): 0.033*"as" + 0.003*"emperor" + 0.002*"energy" + 0.001*"engine" + 0.001*"bc" + 0.001*"japanese" + 0.001*"bass" + 0.001*"ford" + 0.001*"imperial" + 0.001*"instrument"
    2021-05-05 22:43:01,570 : INFO : topic diff=0.167185, rho=0.454053
    2021-05-05 22:43:21,983 : INFO : -8.965 per-word bound, 499.6 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:43:22,030 : INFO : LdaMulticore lifecycle event {'msg': 'trained LdaModel(num_terms=20076, num_topics=9, decay=0.5, chunksize=2000) in 127.49s', 'datetime': '2021-05-05T22:43:22.030248', 'gensim': '4.1.0.dev0', 'python': '3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]', 'platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'event': 'created'}
    2021-05-05 22:43:22,031 : INFO : using symmetric alpha at 0.09090909090909091
    2021-05-05 22:43:22,031 : INFO : using symmetric eta at 0.09090909090909091
    2021-05-05 22:43:22,035 : INFO : using serial LDA version on this node
    2021-05-05 22:43:22,063 : INFO : running online (multi-pass) LDA training, 11 topics, 2 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-05-05 22:43:22,064 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-05-05 22:43:42,751 : INFO : -10.507 per-word bound, 1455.6 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:43:42,751 : INFO : PROGRESS: pass 0, at document #1701/1701
    2021-05-05 22:43:52,019 : INFO : topic #1 (0.091): 0.026*"as" + 0.001*"league" + 0.001*"km" + 0.001*"india" + 0.001*"band" + 0.001*"software" + 0.001*"soviet" + 0.001*"israel" + 0.001*"jewish" + 0.001*"japanese"
    2021-05-05 22:43:52,020 : INFO : topic #7 (0.091): 0.030*"as" + 0.001*"japanese" + 0.001*"km" + 0.001*"soviet" + 0.001*"philosophy" + 0.001*"actor" + 0.001*"bc" + 0.001*"love" + 0.001*"russian" + 0.001*"energy"
    2021-05-05 22:43:52,021 : INFO : topic #3 (0.091): 0.023*"as" + 0.001*"km" + 0.001*"league" + 0.001*"bc" + 0.001*"season" + 0.001*"minister" + 0.001*"moon" + 0.001*"actor" + 0.001*"ball" + 0.001*"lincoln"
    2021-05-05 22:43:52,021 : INFO : topic #6 (0.091): 0.026*"as" + 0.001*"band" + 0.001*"energy" + 0.001*"software" + 0.001*"km" + 0.001*"minister" + 0.001*"est" + 0.001*"speed" + 0.001*"prime" + 0.001*"elected"
    2021-05-05 22:43:52,022 : INFO : topic #4 (0.091): 0.029*"as" + 0.001*"india" + 0.001*"km" + 0.001*"irish" + 0.001*"band" + 0.001*"africa" + 0.001*"jewish" + 0.001*"soviet" + 0.001*"minister" + 0.001*"italian"
    2021-05-05 22:43:52,022 : INFO : topic diff=1.054397, rho=1.000000
    2021-05-05 22:44:12,699 : INFO : -9.155 per-word bound, 569.9 perplexity estimate based on a held-out corpus of 1701 documents with 4222006 words
    2021-05-05 22:44:12,699 : INFO : PROGRESS: pass 1, at document #1701/1701
    2021-05-05 22:44:22,190 : INFO : topic #5 (0.091): 0.042*"as" + 0.001*"energy" + 0.001*"irish" + 0.001*"india" + 0.001*"australia" + 0.001*"soviet" + 0.001*"japanese" + 0.001*"y" + 0.001*"china" + 0.001*"australian"
    2021-05-05 22:44:22,190 : INFO : topic #10 (0.091): 0.030*"as" + 0.002*"soviet" + 0.002*"jewish" + 0.002*"km" + 0.001*"russian" + 0.001*"emperor" + 0.001*"israel" + 0.001*"actor" + 0.001*"minister" + 0.001*"league"
    2021-05-05 22:44:22,191 : INFO : topic #2 (0.091): 0.016*"as" + 0.003*"km" + 0.002*"est" + 0.002*"lebanon" + 0.002*"minister" + 0.002*"egypt" + 0.002*"israel" + 0.002*"prime" + 0.002*"energy" + 0.002*"marriage"
    2021-05-05 22:44:22,191 : INFO : topic #9 (0.091): 0.037*"as" + 0.001*"aircraft" + 0.001*"km" + 0.001*"software" + 0.001*"economy" + 0.001*"bridge" + 0.001*"israel" + 0.001*"minister" + 0.001*"spanish" + 0.001*"park"
    2021-05-05 22:44:22,192 : INFO : topic #4 (0.091): 0.030*"as" + 0.002*"india" + 0.001*"irish" + 0.001*"band" + 0.001*"ireland" + 0.001*"indian" + 0.001*"africa" + 0.001*"km" + 0.001*"love" + 0.001*"christmas"
    2021-05-05 22:44:22,192 : INFO : topic diff=0.195202, rho=0.577350
    2021-05-05 22:44:22,193 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=20076, num_topics=11, decay=0.5, chunksize=2000) in 60.13s', 'datetime': '2021-05-05T22:44:22.193045', 'gensim': '4.1.0.dev0', 'python': '3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]', 'platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'event': 'created'}
    2021-05-05 22:44:22,193 : INFO : ensemble contains 9 models and 160 topics now
    2021-05-05 22:44:22,203 : INFO : ensemble contains 10 models and 169 topics now
    2021-05-05 22:44:22,231 : INFO : asymmetric distance matrix is outdated due to add_model
    2021-05-05 22:44:22,231 : INFO : generating a 180 x 180 asymmetric distance matrix...
    2021-05-05 22:44:24,696 : INFO : fitting the clustering model, using 5 for min_samples
    2021-05-05 22:44:24,780 : INFO : generating stable topics, using 3 for min_cores
    2021-05-05 22:44:24,781 : INFO : found 3 clusters
    2021-05-05 22:44:24,843 : INFO : found 1 stable topics
    2021-05-05 22:44:24,851 : INFO : generating classic gensim model representation based on results from the ensemble
    2021-05-05 22:44:24,854 : INFO : using symmetric alpha at 1.0
    2021-05-05 22:44:24,854 : INFO : using symmetric eta at 1.0
    2021-05-05 22:44:24,859 : INFO : using serial LDA version on this node
    2021-05-05 22:44:24,863 : INFO : running online (multi-pass) LDA training, 1 topics, 0 passes over the supplied corpus of 1701 documents, updating model once every 1701 documents, evaluating perplexity every 1701 documents, iterating 50x with a convergence threshold of 0.001000
    2021-05-05 22:44:24,863 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy
    2021-05-05 22:44:24,863 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=20076, num_topics=1, decay=0.5, chunksize=2000) in 0.00s', 'datetime': '2021-05-05T22:44:24.863402', 'gensim': '4.1.0.dev0', 'python': '3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]', 'platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'event': 'created'}
    180
    1





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 8 minutes  55.221 seconds)

**Estimated memory usage:**  507 MB


.. _sphx_glr_download_auto_examples_tutorials_run_ensemblelda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_ensemblelda.py <run_ensemblelda.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_ensemblelda.ipynb <run_ensemblelda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

:orphan:

.. _sphx_glr_auto_examples_tutorials_sg_execution_times:

Computation times
=================
**08:55.221** total execution time for **auto_examples_tutorials** files:

+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_ensemblelda.py` (``run_ensemblelda.py``) | 08:55.221 | 506.6 MB |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_annoy.py` (``run_annoy.py``)             | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_doc2vec_lee.py` (``run_doc2vec_lee.py``) | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_fasttext.py` (``run_fasttext.py``)       | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_lda.py` (``run_lda.py``)                 | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_scm.py` (``run_scm.py``)                 | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_wmd.py` (``run_wmd.py``)                 | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
| :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py` (``run_word2vec.py``)       | 00:00.000 | 0.0 MB   |
+-------------------------------------------------------------------------------------+-----------+----------+
.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_scm.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_scm.py:


Soft Cosine Measure
===================

Demonstrates using Gensim's implemenation of the SCM.

Soft Cosine Measure (SCM) is a promising new tool in machine learning that
allows us to submit a query and return the most relevant documents. This
tutorial introduces SCM and shows how you can compute the SCM similarities
between two documents using the ``inner_product`` method.

Soft Cosine Measure basics
--------------------------

Soft Cosine Measure (SCM) is a method that allows us to assess the similarity
between two documents in a meaningful way, even when they have no words in
common. It uses a measure of similarity between words, which can be derived
[2] using [word2vec][] [4] vector embeddings of words. It has been shown to
outperform many of the state-of-the-art methods in the semantic text
similarity task in the context of community question answering [2].


SCM is illustrated below for two very similar sentences. The sentences have
no words in common, but by modeling synonymy, SCM is able to accurately
measure the similarity between the two sentences. The method also uses the
bag-of-words vector representation of the documents (simply put, the word's
frequencies in the documents). The intution behind the method is that we
compute standard cosine similarity assuming that the document vectors are
expressed in a non-orthogonal basis, where the angle between two basis
vectors is derived from the angle between the word2vec embeddings of the
corresponding words.



.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('scm-hello.png')
    imgplot = plt.imshow(img)
    plt.axis('off')
    plt.show()




.. image:: /auto_examples/tutorials/images/sphx_glr_run_scm_001.png
    :class: sphx-glr-single-img




This method was perhaps first introduced in the article ‚ÄúSoft Measure and
Soft Cosine Measure: Measure of Features in Vector Space Model‚Äù by Grigori
Sidorov, Alexander Gelbukh, Helena Gomez-Adorno, and David Pinto.

In this tutorial, we will learn how to use Gensim's SCM functionality, which
consists of the ``inner_product`` method for one-off computation, and the
``SoftCosineSimilarity`` class for corpus-based similarity queries.

.. Important::
   If you use Gensim's SCM functionality, please consider citing [1], [2] and [3].

Computing the Soft Cosine Measure
---------------------------------
To use SCM, you need some existing word embeddings.
You could train your own Word2Vec model, but that is beyond the scope of this tutorial
(check out :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py` if you're interested).
For this tutorial, we'll be using an existing Word2Vec model.

Let's take some sentences to compute the distance between.



.. code-block:: default


    # Initialize logging.
    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    sentence_obama = 'Obama speaks to the media in Illinois'
    sentence_president = 'The president greets the press in Chicago'
    sentence_orange = 'Oranges are my favorite fruit'







The first two sentences sentences have very similar content, and as such the
SCM should be high. By contrast, the third sentence is unrelated to the first
two and the SCM should be low.

Before we compute the SCM, we want to remove stopwords ("the", "to", etc.),
as these do not contribute a lot to the information in the sentences.



.. code-block:: default


    # Import and download stopwords from NLTK.
    from nltk.corpus import stopwords
    from nltk import download
    download('stopwords')  # Download stopwords list.
    stop_words = stopwords.words('english')

    def preprocess(sentence):
        return [w for w in sentence.lower().split() if w not in stop_words]

    sentence_obama = preprocess(sentence_obama)
    sentence_president = preprocess(sentence_president)
    sentence_orange = preprocess(sentence_orange)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      dtype=np.int):
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      method='lar', copy_X=True, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      method='lar', copy_X=True, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, positive=False):
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
    /home/witiko/.virtualenvs/gensim4/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, positive=False):
    [nltk_data] Downloading package stopwords to /home/witiko/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



Next, we will build a dictionary and a TF-IDF model, and we will convert the
sentences to the bag-of-words format.



.. code-block:: default

    from gensim.corpora import Dictionary
    documents = [sentence_obama, sentence_president, sentence_orange]
    dictionary = Dictionary(documents)

    sentence_obama = dictionary.doc2bow(sentence_obama)
    sentence_president = dictionary.doc2bow(sentence_president)
    sentence_orange = dictionary.doc2bow(sentence_orange)

    from gensim.models import TfidfModel
    documents = [sentence_obama, sentence_president, sentence_orange]
    tfidf = TfidfModel(documents)

    sentence_obama = tfidf[sentence_obama]
    sentence_president = tfidf[sentence_president]
    sentence_orange = tfidf[sentence_orange]







Now, as mentioned earlier, we will be using some downloaded pre-trained
embeddings. We load these into a Gensim Word2Vec model class and we build
a term similarity mextrix using the embeddings.

.. Important::
  The embeddings we have chosen here require a lot of memory.



.. code-block:: default

    import gensim.downloader as api
    model = api.load('word2vec-google-news-300')

    from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex
    termsim_index = WordEmbeddingSimilarityIndex(model)
    termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)







So let's compute SCM using the ``inner_product`` method.



.. code-block:: default

    similarity = termsim_matrix.inner_product(sentence_obama, sentence_president, normalized=(True, True))
    print('similarity = %.4f' % similarity)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    similarity = 0.2575



Let's try the same thing with two completely unrelated sentences.
Notice that the similarity is smaller.



.. code-block:: default

    similarity = termsim_matrix.inner_product(sentence_obama, sentence_orange, normalized=(True, True))
    print('similarity = %.4f' % similarity)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    similarity = 0.0000



References
----------

1. Grigori Sidorov et al. *Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model*, 2014.
2. Delphine Charlet and Geraldine Damnati, SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity between Questions for Community Question Answering, 2017.
3. V√≠t Novotn√Ω. *Implementation Notes for the Soft Cosine Measure*, 2018.
4. Tom√°≈° Mikolov et al. Efficient Estimation of Word Representations in Vector Space, 2013.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  56.707 seconds)

**Estimated memory usage:**  7701 MB


.. _sphx_glr_download_auto_examples_tutorials_run_scm.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: run_scm.py <run_scm.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: run_scm.ipynb <run_scm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_fasttext.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_fasttext.py:


FastText Model
==============

Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








Here, we'll learn to work with fastText library for training word-embedding
models, saving & loading them and performing similarity operations & vector
lookups analogous to Word2Vec.

When to use fastText?
---------------------

The main principle behind `fastText <https://github.com/facebookresearch/fastText>`_ is that the
morphological structure of a word carries important information about the meaning of the word.
Such structure is not taken into account by traditional word embeddings like Word2Vec, which
train a unique word embedding for every individual word.
This is especially significant for morphologically rich languages (German, Turkish) in which a
single word can have a large number of morphological forms, each of which might occur rarely,
thus making it hard to train good word embeddings.


fastText attempts to solve this by treating each word as the aggregation of its subwords.
For the sake of simplicity and language-independence, subwords are taken to be the character ngrams
of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.


According to a detailed comparison of Word2Vec and fastText in
`this notebook <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>`__,
fastText does significantly better on syntactic tasks as compared to the original Word2Vec,
especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText
on semantic tasks though. The differences grow smaller as the size of the training corpus increases.


fastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its
component char-ngrams, provided at least one of the char-ngrams was present in the training data.


Training models
---------------


For the following examples, we'll use the Lee Corpus (which you already have if you've installed Gensim) for training our model.





.. code-block:: default

    from pprint import pprint as print
    from gensim.models.fasttext import FastText
    from gensim.test.utils import datapath

    # Set file names for train and test data
    corpus_file = datapath('lee_background.cor')

    model = FastText(vector_size=100)

    # build the vocabulary
    model.build_vocab(corpus_file=corpus_file)

    # train the model
    model.train(
        corpus_file=corpus_file, epochs=model.epochs,
        total_examples=model.corpus_count, total_words=model.corpus_total_words,
    )

    print(model)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <gensim.models.fasttext.FastText object at 0x20ce0d390>




Training hyperparameters
^^^^^^^^^^^^^^^^^^^^^^^^


Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:

- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)
- vector_size: Dimensionality of vector embeddings to be learnt (Default 100)
- alpha: Initial learning rate (Default 0.025)
- window: Context window size (Default 5)
- min_count: Ignore words with number of occurrences below this (Default 5)
- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)
- sample: Threshold for downsampling higher-frequency words (Default 0.001)
- negative: Number of negative words to sample, for `ns` (Default 5)
- epochs: Number of epochs (Default 5)
- sorted_vocab: Sort vocab by descending frequency (Default 1)
- threads: Number of threads to use (Default 12)


In addition, fastText has three additional parameters:

- min_n: min length of char ngrams (Default 3)
- max_n: max length of char ngrams (Default 6)
- bucket: number of buckets used for hashing ngrams (Default 2000000)


Parameters ``min_n`` and ``max_n`` control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0, or to be lesser than ``min_n``\ , no character ngrams are used, and the model effectively reduces to Word2Vec.



To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the `Fowler-Noll-Vo hashing function <http://www.isthe.com/chongo/tech/comp/fnv>`_ (FNV-1a variant) is employed.


**Note:** You can continue to train your model while using Gensim's native implementation of fastText.


Saving/loading models
---------------------


Models can be saved and loaded via the ``load`` and ``save`` methods, just like
any other model in Gensim.



.. code-block:: default



    # Save a model trained via Gensim's fastText implementation to temp.
    import tempfile
    import os
    with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:
        model.save(tmp.name, separately=[])

    # Load back the same model.
    loaded_model = FastText.load(tmp.name)
    print(loaded_model)

    os.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <gensim.models.fasttext.FastText object at 0x20cc99d30>




The ``save_word2vec_format`` is also available for fastText models, but will
cause all vectors for ngrams to be lost.
As a result, a model loaded in this way will behave as a regular word2vec model.


Word vector lookup
------------------


All information necessary for looking up fastText words (incl. OOV words) is
contained in its ``model.wv`` attribute.

If you don't need to continue training your model, you can export & save this `.wv`
attribute and discard `model`, to save space and RAM.



.. code-block:: default

    wv = model.wv
    print(wv)

    #
    # FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.
    #
    print('night' in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    <gensim.models.fasttext.FastTextKeyedVectors object at 0x20ce0d828>
    True





.. code-block:: default

    print('nights' in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    False





.. code-block:: default

    print(wv['night'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    array([ 0.12453239, -0.26018462, -0.04087191,  0.2563215 ,  0.31401935,
            0.16155584,  0.39527607,  0.27404118, -0.45236284,  0.06942682,
            0.36584955,  0.51162827, -0.51161295, -0.192019  , -0.5068029 ,
           -0.07426998, -0.6276584 ,  0.22271585,  0.19990133,  0.2582401 ,
            0.14329399, -0.01959469, -0.45576197, -0.06447829,  0.1493489 ,
            0.17261286, -0.13472046,  0.26546794, -0.34596932,  0.5626187 ,
           -0.7038802 ,  0.15603925, -0.03104019, -0.06228801, -0.13480644,
           -0.0684596 ,  0.24728075,  0.55081636,  0.07330963,  0.32814154,
            0.1574982 ,  0.56742406, -0.31233737,  0.14195296,  0.0540203 ,
            0.01718009,  0.05519052, -0.04002226,  0.16157456, -0.5134223 ,
           -0.01033936,  0.05745083, -0.39208183,  0.52553374, -1.0542839 ,
            0.2145304 , -0.15234643, -0.35197273, -0.6215585 ,  0.01796502,
            0.21242104,  0.30762967,  0.2787644 , -0.19908747,  0.7144409 ,
            0.45586124, -0.21344525,  0.26920903, -0.651759  , -0.37096855,
           -0.16243419, -0.3085725 , -0.70485127, -0.04926324, -0.80278563,
           -0.24352737,  0.6427129 , -0.3530421 , -0.29960123,  0.01466726,
           -0.18253349, -0.2489397 ,  0.00648343,  0.18057272, -0.11812428,
           -0.49044088,  0.1847386 , -0.27946883,  0.3941279 , -0.39211616,
            0.26847798,  0.41468227, -0.3953728 , -0.25371104,  0.3390468 ,
           -0.16447693, -0.18722224,  0.2782088 , -0.0696249 ,  0.4313547 ],
          dtype=float32)





.. code-block:: default

    print(wv['nights'])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    array([ 0.10586783, -0.22489995, -0.03636307,  0.22263278,  0.27037606,
            0.1394871 ,  0.3411114 ,  0.2369042 , -0.38989475,  0.05935   ,
            0.31713557,  0.44301754, -0.44249156, -0.16652377, -0.4388366 ,
           -0.06266895, -0.5436303 ,  0.19294666,  0.17363031,  0.22459263,
            0.12532061, -0.01866964, -0.3936521 , -0.05507145,  0.12905194,
            0.14942174, -0.11657442,  0.22935589, -0.29934618,  0.4859668 ,
           -0.6073519 ,  0.13433163, -0.02491274, -0.05468523, -0.11884545,
           -0.06117092,  0.21444008,  0.4775469 ,  0.06227469,  0.28350767,
            0.13580805,  0.48993143, -0.27067345,  0.1252003 ,  0.04606731,
            0.01598426,  0.04640368, -0.03456376,  0.14138013, -0.44429192,
           -0.00865329,  0.05027836, -0.341311  ,  0.45402458, -0.91097856,
            0.1868968 , -0.13116683, -0.30361563, -0.5364188 ,  0.01603454,
            0.18146741,  0.26708448,  0.24074472, -0.17163375,  0.61906886,
            0.39530373, -0.18259627,  0.23319626, -0.5634787 , -0.31959867,
           -0.13945322, -0.269441  , -0.60941464, -0.0403638 , -0.69563633,
           -0.2098089 ,  0.5569868 , -0.30320194, -0.25840232,  0.01436759,
           -0.15632603, -0.21624804,  0.00434287,  0.15566474, -0.10228094,
           -0.4249678 ,  0.16197811, -0.24147548,  0.34205705, -0.3391568 ,
            0.23235887,  0.35860622, -0.34247142, -0.21777524,  0.29318404,
           -0.1407287 , -0.16115218,  0.24247572, -0.06217333,  0.37221798],
          dtype=float32)




Similarity operations
---------------------


Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**



.. code-block:: default



    print("nights" in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    False





.. code-block:: default

    print("night" in wv.key_to_index)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    True





.. code-block:: default

    print(wv.similarity("night", "nights"))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.9999929




Syntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. A detailed comparison is provided `here <Word2Vec_FastText_Comparison.ipynb>`_.


Other similarity operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only


.. code-block:: default

    print(wv.most_similar("nights"))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('night', 0.9999929070472717),
     ('night.', 0.9999895095825195),
     ('flights', 0.999988853931427),
     ('rights', 0.9999886751174927),
     ('residents', 0.9999884366989136),
     ('overnight', 0.9999883770942688),
     ('commanders', 0.999988317489624),
     ('reached', 0.9999881386756897),
     ('commander', 0.9999880790710449),
     ('leading', 0.999987781047821)]





.. code-block:: default

    print(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.9999402





.. code-block:: default

    print(wv.doesnt_match("breakfast cereal dinner lunch".split()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    'lunch'





.. code-block:: default

    print(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('attempt', 0.999660074710846),
     ('biggest', 0.9996545314788818),
     ('again', 0.9996527433395386),
     ('against', 0.9996523857116699),
     ('doubles', 0.9996522068977356),
     ('Royal', 0.9996512532234192),
     ('Airlines', 0.9996494054794312),
     ('forced', 0.9996494054794312),
     ('arrest', 0.9996492266654968),
     ('follows', 0.999649167060852)]





.. code-block:: default

    print(wv.evaluate_word_analogies(datapath('questions-words.txt')))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (0.24489795918367346,
     [{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},
      {'correct': [], 'incorrect': [], 'section': 'capital-world'},
      {'correct': [], 'incorrect': [], 'section': 'currency'},
      {'correct': [], 'incorrect': [], 'section': 'city-in-state'},
      {'correct': [],
       'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],
       'section': 'family'},
      {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},
      {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},
      {'correct': [('GOOD', 'BETTER', 'LOW', 'LOWER'),
                   ('GREAT', 'GREATER', 'LOW', 'LOWER'),
                   ('LONG', 'LONGER', 'LOW', 'LOWER')],
       'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                     ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'LONG', 'LONGER')],
       'section': 'gram3-comparative'},
      {'correct': [('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
                   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST')],
       'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                     ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),
                     ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],
       'section': 'gram4-superlative'},
      {'correct': [('GO', 'GOING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
                   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
                   ('PLAY', 'PLAYING', 'GO', 'GOING'),
                   ('SAY', 'SAYING', 'GO', 'GOING')],
       'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),
                     ('GO', 'GOING', 'PLAY', 'PLAYING'),
                     ('GO', 'GOING', 'RUN', 'RUNNING'),
                     ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                     ('RUN', 'RUNNING', 'GO', 'GOING'),
                     ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                     ('SAY', 'SAYING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'RUN', 'RUNNING')],
       'section': 'gram5-present-participle'},
      {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
                   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
                   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
                   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),
                   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN')],
       'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                     ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                     ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                     ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                     ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],
       'section': 'gram6-nationality-adjective'},
      {'correct': [],
       'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),
                     ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                     ('GOING', 'WENT', 'SAYING', 'SAID'),
                     ('GOING', 'WENT', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                     ('PAYING', 'PAID', 'SAYING', 'SAID'),
                     ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                     ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                     ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                     ('SAYING', 'SAID', 'GOING', 'WENT'),
                     ('SAYING', 'SAID', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'GOING', 'WENT'),
                     ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                     ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'SAYING', 'SAID')],
       'section': 'gram7-past-tense'},
      {'correct': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
                   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
                   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
                   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
                   ('MAN', 'MEN', 'CAR', 'CARS')],
       'incorrect': [('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                     ('CAR', 'CARS', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                     ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                     ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
       'section': 'gram8-plural'},
      {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},
      {'correct': [('GOOD', 'BETTER', 'LOW', 'LOWER'),
                   ('GREAT', 'GREATER', 'LOW', 'LOWER'),
                   ('LONG', 'LONGER', 'LOW', 'LOWER'),
                   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),
                   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),
                   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),
                   ('GO', 'GOING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),
                   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),
                   ('LOOK', 'LOOKING', 'GO', 'GOING'),
                   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),
                   ('PLAY', 'PLAYING', 'GO', 'GOING'),
                   ('SAY', 'SAYING', 'GO', 'GOING'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),
                   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),
                   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),
                   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),
                   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),
                   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),
                   ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),
                   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),
                   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),
                   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),
                   ('MAN', 'MEN', 'CAR', 'CARS')],
       'incorrect': [('HE', 'SHE', 'HIS', 'HER'),
                     ('HIS', 'HER', 'HE', 'SHE'),
                     ('GOOD', 'BETTER', 'GREAT', 'GREATER'),
                     ('GOOD', 'BETTER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'LONG', 'LONGER'),
                     ('GREAT', 'GREATER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'GOOD', 'BETTER'),
                     ('LONG', 'LONGER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'GOOD', 'BETTER'),
                     ('LOW', 'LOWER', 'GREAT', 'GREATER'),
                     ('LOW', 'LOWER', 'LONG', 'LONGER'),
                     ('BIG', 'BIGGEST', 'GOOD', 'BEST'),
                     ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'GREAT', 'GREATEST'),
                     ('GOOD', 'BEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),
                     ('GREAT', 'GREATEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),
                     ('LARGE', 'LARGEST', 'GOOD', 'BEST'),
                     ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),
                     ('GO', 'GOING', 'LOOK', 'LOOKING'),
                     ('GO', 'GOING', 'PLAY', 'PLAYING'),
                     ('GO', 'GOING', 'RUN', 'RUNNING'),
                     ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),
                     ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'SAY', 'SAYING'),
                     ('RUN', 'RUNNING', 'GO', 'GOING'),
                     ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),
                     ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'LOOK', 'LOOKING'),
                     ('SAY', 'SAYING', 'PLAY', 'PLAYING'),
                     ('SAY', 'SAYING', 'RUN', 'RUNNING'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),
                     ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),
                     ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),
                     ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),
                     ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),
                     ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),
                     ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),
                     ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),
                     ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),
                     ('GOING', 'WENT', 'PAYING', 'PAID'),
                     ('GOING', 'WENT', 'PLAYING', 'PLAYED'),
                     ('GOING', 'WENT', 'SAYING', 'SAID'),
                     ('GOING', 'WENT', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),
                     ('PAYING', 'PAID', 'SAYING', 'SAID'),
                     ('PAYING', 'PAID', 'TAKING', 'TOOK'),
                     ('PAYING', 'PAID', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),
                     ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),
                     ('PLAYING', 'PLAYED', 'GOING', 'WENT'),
                     ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'TAKING', 'TOOK'),
                     ('SAYING', 'SAID', 'GOING', 'WENT'),
                     ('SAYING', 'SAID', 'PAYING', 'PAID'),
                     ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'GOING', 'WENT'),
                     ('TAKING', 'TOOK', 'PAYING', 'PAID'),
                     ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),
                     ('TAKING', 'TOOK', 'SAYING', 'SAID'),
                     ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),
                     ('CAR', 'CARS', 'CHILD', 'CHILDREN'),
                     ('CAR', 'CARS', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'MAN', 'MEN'),
                     ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),
                     ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),
                     ('MAN', 'MEN', 'CHILD', 'CHILDREN')],
       'section': 'Total accuracy'}])




Word Movers distance
^^^^^^^^^^^^^^^^^^^^

You'll need the optional ``pyemd`` library for this section, ``pip install pyemd``.

Let's start with two sentences:


.. code-block:: default

    sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()
    sentence_president = 'The president greets the press in Chicago'.lower().split()









Remove their stopwords.



.. code-block:: default

    from gensim.parsing.preprocessing import STOPWORDS
    sentence_obama = [w for w in sentence_obama if w not in STOPWORDS]
    sentence_president = [w for w in sentence_president if w not in STOPWORDS]








Compute the Word Movers Distance between the two sentences.


.. code-block:: default

    distance = wv.wmdistance(sentence_obama, sentence_president)
    print(f"Word Movers Distance is {distance} (lower means closer)")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    'Word Movers Distance is 0.015923231075180694 (lower means closer)'




That's all! You've made it to the end of this tutorial.



.. code-block:: default

    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img = mpimg.imread('fasttext-logo-color-web.png')
    imgplot = plt.imshow(img)
    _ = plt.axis('off')



.. image:: /auto_examples/tutorials/images/sphx_glr_run_fasttext_001.png
    :alt: run fasttext
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  28.645 seconds)

**Estimated memory usage:**  2975 MB


.. _sphx_glr_download_auto_examples_tutorials_run_fasttext.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_fasttext.py <run_fasttext.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_fasttext.ipynb <run_fasttext.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/tutorials/run_lda.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_lda.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_tutorials_run_lda.py:


LDA Model
=========

Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.

.. GENERATED FROM PYTHON SOURCE LINES 8-12

.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








.. GENERATED FROM PYTHON SOURCE LINES 13-60

The purpose of this tutorial is to demonstrate how to train and tune an LDA model.

In this tutorial we will:

* Load input data.
* Pre-process that data.
* Transform documents into bag-of-words vectors.
* Train an LDA model.

This tutorial will **not**:

* Explain how Latent Dirichlet Allocation works
* Explain how the LDA model performs inference
* Teach you all the parameters and options for Gensim's LDA implementation

If you are not familiar with the LDA model or how to use it in Gensim, I (Olavur Mortensen)
suggest you read up on that before continuing with this tutorial. Basic
understanding of the LDA model should suffice. Examples:

* `Introduction to Latent Dirichlet Allocation <http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation>`_
* Gensim tutorial: :ref:`sphx_glr_auto_examples_core_run_topics_and_transformations.py`
* Gensim's LDA model API docs: :py:class:`gensim.models.LdaModel`

I would also encourage you to consider each step when applying the model to
your data, instead of just blindly applying my solution. The different steps
will depend on your data and possibly your goal with the model.

Data
----

I have used a corpus of NIPS papers in this tutorial, but if you're following
this tutorial just to learn about LDA I encourage you to consider picking a
corpus on a subject that you are familiar with. Qualitatively evaluating the
output of an LDA model is challenging and can require you to understand the
subject matter of your corpus (depending on your goal with the model).

NIPS (Neural Information Processing Systems) is a machine learning conference
so the subject matter should be well suited for most of the target audience
of this tutorial.  You can download the original data from Sam Roweis'
`website <http://www.cs.nyu.edu/~roweis/data.html>`_.  The code below will
also do that for you.

.. Important::
    The corpus contains 1740 documents, and not particularly long ones.
    So keep in mind that this tutorial is not geared towards efficiency, and be
    careful before applying the code to a large dataset.


.. GENERATED FROM PYTHON SOURCE LINES 60-78

.. code-block:: default


    import io
    import os.path
    import re
    import tarfile

    import smart_open

    def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):
        with smart_open.open(url, "rb") as file:
            with tarfile.open(fileobj=file) as tar:
                for member in tar.getmembers():
                    if member.isfile() and re.search(r'nipstxt/nips\d+/\d+\.txt', member.name):
                        member_bytes = tar.extractfile(member).read()
                        yield member_bytes.decode('utf-8', errors='replace')

    docs = list(extract_documents())








.. GENERATED FROM PYTHON SOURCE LINES 79-84

So we have a list of 1740 documents, where each document is a Unicode string.
If you're thinking about using your own corpus, then you need to make sure
that it's in the same format (list of Unicode strings) before proceeding
with the rest of this tutorial.


.. GENERATED FROM PYTHON SOURCE LINES 84-87

.. code-block:: default

    print(len(docs))
    print(docs[0][:500])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    1740
    387 
    Neural Net and Traditional Classifiers  
    William Y. Huang and Richard P. Lippmann 
    MIT Lincoln Laboratory 
    Lexington, MA 02173, USA 
    Abstract
    Previous work on nets with continuous-valued inputs led to generative 
    procedures to construct convex decision regions with two-layer percepttons (one hidden 
    layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). 
    Here we demonstrate that two-layer perceptton classifiers trained with back propagation 
    can form both c




.. GENERATED FROM PYTHON SOURCE LINES 88-107

Pre-process and vectorize the documents
---------------------------------------

As part of preprocessing, we will:

* Tokenize (split the documents into tokens).
* Lemmatize the tokens.
* Compute bigrams.
* Compute a bag-of-words representation of the data.

First we tokenize the text using a regular expression tokenizer from NLTK. We
remove numeric tokens and tokens that are only a single character, as they
don't tend to be useful, and the dataset contains a lot of them.

.. Important::

   This tutorial uses the nltk library for preprocessing, although you can
   replace it with something else if you want.


.. GENERATED FROM PYTHON SOURCE LINES 107-123

.. code-block:: default


    # Tokenize the documents.
    from nltk.tokenize import RegexpTokenizer

    # Split the documents into tokens.
    tokenizer = RegexpTokenizer(r'\w+')
    for idx in range(len(docs)):
        docs[idx] = docs[idx].lower()  # Convert to lowercase.
        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.

    # Remove numbers, but not words that contain numbers.
    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]

    # Remove words that are only one character.
    docs = [[token for token in doc if len(token) > 1] for doc in docs]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      dtype=np.int):
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, positive=False):
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, copy_X=True, positive=False):
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      precompute=False, eps=np.finfo(np.float).eps,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=np.finfo(np.float).eps, random_state=None,
    /home/jonaschn/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      eps=4 * np.finfo(np.float).eps, n_jobs=1,




.. GENERATED FROM PYTHON SOURCE LINES 124-128

We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a
stemmer in this case because it produces more readable words. Output that is
easy to read is very desirable in topic modelling.


.. GENERATED FROM PYTHON SOURCE LINES 128-135

.. code-block:: default


    # Lemmatize the documents.
    from nltk.stem.wordnet import WordNetLemmatizer

    lemmatizer = WordNetLemmatizer()
    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]








.. GENERATED FROM PYTHON SOURCE LINES 136-149

We find bigrams in the documents. Bigrams are sets of two adjacent words.
Using bigrams we can get phrases like "machine_learning" in our output
(spaces are replaced with underscores); without bigrams we would only get
"machine" and "learning".

Note that in the code below, we find bigrams and then add them to the
original data, because we would like to keep the words "machine" and
"learning" as well as the bigram "machine_learning".

.. Important::
    Computing n-grams of large dataset can be very computationally
    and memory intensive.


.. GENERATED FROM PYTHON SOURCE LINES 149-162

.. code-block:: default



    # Compute bigrams.
    from gensim.models import Phrases

    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).
    bigram = Phrases(docs, min_count=20)
    for idx in range(len(docs)):
        for token in bigram[docs[idx]]:
            if '_' in token:
                # Token is a bigram, add to document.
                docs[idx].append(token)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/jonaschn/Projects/gensim/gensim/similarities/__init__.py:11: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
      "The gensim.similarities.levenshtein submodule is disabled, because the optional "
    2021-03-19 14:09:53,817 : INFO : collecting all words and their counts
    2021-03-19 14:09:53,817 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types
    2021-03-19 14:09:59,172 : INFO : collected 1120198 token types (unigram + bigrams) from a corpus of 4629808 words and 1740 sentences
    2021-03-19 14:09:59,172 : INFO : merged Phrases<1120198 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>
    2021-03-19 14:09:59,190 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1120198 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 5.36s', 'datetime': '2021-03-19T14:09:59.189253', 'gensim': '4.0.0.rc1', 'python': '3.7.0 (default, Jun 28 2018, 13:15:42) \n[GCC 7.2.0]', 'platform': 'Linux-4.15.0-136-generic-x86_64-with-debian-buster-sid', 'event': 'created'}




.. GENERATED FROM PYTHON SOURCE LINES 163-168

We remove rare words and common words based on their *document frequency*.
Below we remove words that appear in less than 20 documents or in more than
50% of the documents. Consider trying to remove words only based on their
frequency, or maybe combining that with this approach.


.. GENERATED FROM PYTHON SOURCE LINES 168-178

.. code-block:: default


    # Remove rare and common tokens.
    from gensim.corpora import Dictionary

    # Create a dictionary representation of the documents.
    dictionary = Dictionary(docs)

    # Filter out words that occur less than 20 documents, or more than 50% of the documents.
    dictionary.filter_extremes(no_below=20, no_above=0.5)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-03-19 14:10:07,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2021-03-19 14:10:09,906 : INFO : built Dictionary(79429 unique tokens: ['1ooooo', '1st', '25oo', '2o00', '4ooo']...) from 1740 documents (total 4953968 corpus positions)
    2021-03-19 14:10:09,906 : INFO : Dictionary lifecycle event {'msg': "built Dictionary(79429 unique tokens: ['1ooooo', '1st', '25oo', '2o00', '4ooo']...) from 1740 documents (total 4953968 corpus positions)", 'datetime': '2021-03-19T14:10:09.906597', 'gensim': '4.0.0.rc1', 'python': '3.7.0 (default, Jun 28 2018, 13:15:42) \n[GCC 7.2.0]', 'platform': 'Linux-4.15.0-136-generic-x86_64-with-debian-buster-sid', 'event': 'created'}
    2021-03-19 14:10:10,101 : INFO : discarding 70785 tokens: [('1ooooo', 1), ('25oo', 2), ('2o00', 6), ('4ooo', 2), ('64k', 6), ('a', 1740), ('aaditional', 1), ('above', 1114), ('abstract', 1740), ('acase', 1)]...
    2021-03-19 14:10:10,102 : INFO : keeping 8644 tokens which were in no less than 20 and no more than 870 (=50.0%) documents
    2021-03-19 14:10:10,128 : INFO : resulting dictionary: Dictionary(8644 unique tokens: ['1st', '5oo', '7th', 'a2', 'a_well']...)




.. GENERATED FROM PYTHON SOURCE LINES 179-182

Finally, we transform the documents to a vectorized form. We simply compute
the frequency of each word, including the bigrams.


.. GENERATED FROM PYTHON SOURCE LINES 182-186

.. code-block:: default


    # Bag-of-words representation of the documents.
    corpus = [dictionary.doc2bow(doc) for doc in docs]








.. GENERATED FROM PYTHON SOURCE LINES 187-189

Let's see how many tokens and documents we have to train on.


.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: default


    print('Number of unique tokens: %d' % len(dictionary))
    print('Number of documents: %d' % len(corpus))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Number of unique tokens: 8644
    Number of documents: 1740




.. GENERATED FROM PYTHON SOURCE LINES 194-236

Training
--------

We are ready to train the LDA model. We will first discuss how to set some of
the training parameters.

First of all, the elephant in the room: how many topics do I need? There is
really no easy answer for this, it will depend on both your data and your
application. I have used 10 topics here because I wanted to have a few topics
that I could interpret and "label", and because that turned out to give me
reasonably good results. You might not need to interpret all your topics, so
you could use a large number of topics, for example 100.

``chunksize`` controls how many documents are processed at a time in the
training algorithm. Increasing chunksize will speed up training, at least as
long as the chunk of documents easily fit into memory. I've set ``chunksize =
2000``, which is more than the amount of documents, so I process all the
data in one go. Chunksize can however influence the quality of the model, as
discussed in Hoffman and co-authors [2], but the difference was not
substantial in this case.

``passes`` controls how often we train the model on the entire corpus.
Another word for passes might be "epochs". ``iterations`` is somewhat
technical, but essentially it controls how often we repeat a particular loop
over each document. It is important to set the number of "passes" and
"iterations" high enough.

I suggest the following way to choose iterations and passes. First, enable
logging (as described in many Gensim tutorials), and set ``eval_every = 1``
in ``LdaModel``. When training the model look for a line in the log that
looks something like this::

   2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations

If you set ``passes = 20`` you will see this line 20 times. Make sure that by
the final passes, most of the documents have converged. So you want to choose
both passes and iterations to be high enough for this to happen.

We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat
technical, but essentially we are automatically learning two parameters in
the model that we usually would have to specify explicitly.


.. GENERATED FROM PYTHON SOURCE LINES 236-264

.. code-block:: default



    # Train LDA model.
    from gensim.models import LdaModel

    # Set training parameters.
    num_topics = 10
    chunksize = 2000
    passes = 20
    iterations = 400
    eval_every = None  # Don't evaluate model perplexity, takes too much time.

    # Make a index to word dictionary.
    temp = dictionary[0]  # This is only to "load" the dictionary.
    id2word = dictionary.id2token

    model = LdaModel(
        corpus=corpus,
        id2word=id2word,
        chunksize=chunksize,
        alpha='auto',
        eta='auto',
        iterations=iterations,
        num_topics=num_topics,
        passes=passes,
        eval_every=eval_every
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-03-19 14:10:12,273 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
    2021-03-19 14:10:12,278 : INFO : using serial LDA version on this node
    2021-03-19 14:10:12,478 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000
    2021-03-19 14:10:12,482 : INFO : PROGRESS: pass 0, at document #1740/1740
    2021-03-19 14:10:27,000 : INFO : optimized alpha [0.06386429, 0.07352975, 0.10417274, 0.09618805, 0.09326739, 0.07658379, 0.05232423, 0.09257348, 0.05156824, 0.064680815]
    2021-03-19 14:10:27,050 : INFO : topic #8 (0.052): 0.004*"layer" + 0.004*"action" + 0.003*"generalization" + 0.003*"image" + 0.002*"dynamic" + 0.002*"sample" + 0.002*"optimal" + 0.002*"matrix" + 0.002*"net" + 0.002*"classifier"
    2021-03-19 14:10:27,051 : INFO : topic #6 (0.052): 0.006*"image" + 0.005*"hidden" + 0.004*"recognition" + 0.003*"component" + 0.003*"field" + 0.003*"dynamic" + 0.002*"map" + 0.002*"solution" + 0.002*"net" + 0.002*"generalization"
    2021-03-19 14:10:27,051 : INFO : topic #4 (0.093): 0.004*"class" + 0.003*"rule" + 0.003*"hidden" + 0.003*"neuron" + 0.003*"layer" + 0.003*"field" + 0.002*"noise" + 0.002*"net" + 0.002*"image" + 0.002*"node"
    2021-03-19 14:10:27,051 : INFO : topic #3 (0.096): 0.006*"image" + 0.003*"gaussian" + 0.003*"layer" + 0.003*"neuron" + 0.003*"field" + 0.003*"matrix" + 0.003*"circuit" + 0.003*"class" + 0.002*"threshold" + 0.002*"recognition"
    2021-03-19 14:10:27,051 : INFO : topic #2 (0.104): 0.005*"neuron" + 0.004*"image" + 0.004*"control" + 0.004*"layer" + 0.004*"hidden" + 0.003*"recognition" + 0.003*"object" + 0.003*"signal" + 0.003*"response" + 0.003*"class"
    2021-03-19 14:10:27,051 : INFO : topic diff=1.190941, rho=1.000000
    2021-03-19 14:10:27,063 : INFO : PROGRESS: pass 1, at document #1740/1740
    2021-03-19 14:10:36,200 : INFO : optimized alpha [0.05691391, 0.05848132, 0.0764488, 0.07592632, 0.07411411, 0.06465285, 0.046124753, 0.06826302, 0.043833494, 0.05291034]
    2021-03-19 14:10:36,207 : INFO : topic #8 (0.044): 0.007*"action" + 0.004*"robot" + 0.004*"control" + 0.003*"optimal" + 0.003*"policy" + 0.003*"reinforcement" + 0.003*"generalization" + 0.003*"dynamic" + 0.003*"layer" + 0.003*"trajectory"
    2021-03-19 14:10:36,207 : INFO : topic #6 (0.046): 0.007*"image" + 0.007*"hidden" + 0.005*"recognition" + 0.003*"hidden_unit" + 0.003*"energy" + 0.003*"component" + 0.003*"map" + 0.003*"generalization" + 0.003*"net" + 0.003*"layer"
    2021-03-19 14:10:36,207 : INFO : topic #4 (0.074): 0.005*"class" + 0.004*"rule" + 0.003*"hidden" + 0.003*"layer" + 0.003*"net" + 0.003*"classifier" + 0.002*"node" + 0.002*"word" + 0.002*"context" + 0.002*"architecture"
    2021-03-19 14:10:36,207 : INFO : topic #3 (0.076): 0.007*"image" + 0.004*"circuit" + 0.003*"layer" + 0.003*"field" + 0.003*"analog" + 0.003*"chip" + 0.003*"threshold" + 0.003*"gaussian" + 0.003*"class" + 0.003*"matrix"
    2021-03-19 14:10:36,208 : INFO : topic #2 (0.076): 0.005*"control" + 0.005*"recognition" + 0.005*"image" + 0.005*"object" + 0.004*"speech" + 0.004*"layer" + 0.004*"signal" + 0.004*"neuron" + 0.004*"hidden" + 0.003*"word"
    2021-03-19 14:10:36,208 : INFO : topic diff=0.297702, rho=0.577350
    2021-03-19 14:10:36,218 : INFO : PROGRESS: pass 2, at document #1740/1740
    2021-03-19 14:10:43,026 : INFO : optimized alpha [0.05407287, 0.051192053, 0.06480061, 0.06461501, 0.06359977, 0.05890888, 0.042885136, 0.056735355, 0.039943077, 0.04743726]
    2021-03-19 14:10:43,033 : INFO : topic #8 (0.040): 0.008*"action" + 0.006*"control" + 0.005*"robot" + 0.005*"reinforcement" + 0.005*"policy" + 0.004*"optimal" + 0.004*"dynamic" + 0.003*"trajectory" + 0.003*"reinforcement_learning" + 0.003*"controller"
    2021-03-19 14:10:43,033 : INFO : topic #6 (0.043): 0.008*"image" + 0.008*"hidden" + 0.005*"recognition" + 0.004*"hidden_unit" + 0.003*"energy" + 0.003*"layer" + 0.003*"net" + 0.003*"generalization" + 0.003*"map" + 0.003*"solution"
    2021-03-19 14:10:43,034 : INFO : topic #4 (0.064): 0.005*"class" + 0.004*"rule" + 0.004*"hidden" + 0.004*"layer" + 0.003*"net" + 0.003*"classifier" + 0.003*"node" + 0.003*"word" + 0.003*"context" + 0.002*"architecture"
    2021-03-19 14:10:43,034 : INFO : topic #3 (0.065): 0.008*"image" + 0.004*"circuit" + 0.004*"chip" + 0.004*"analog" + 0.004*"threshold" + 0.004*"layer" + 0.003*"field" + 0.003*"node" + 0.003*"class" + 0.003*"net"
    2021-03-19 14:10:43,034 : INFO : topic #2 (0.065): 0.006*"recognition" + 0.006*"speech" + 0.005*"control" + 0.005*"object" + 0.005*"image" + 0.005*"layer" + 0.005*"signal" + 0.004*"word" + 0.004*"hidden" + 0.003*"classification"
    2021-03-19 14:10:43,034 : INFO : topic diff=0.256329, rho=0.500000
    2021-03-19 14:10:43,044 : INFO : PROGRESS: pass 3, at document #1740/1740
    2021-03-19 14:10:48,846 : INFO : optimized alpha [0.053115886, 0.046841364, 0.05838778, 0.05814584, 0.05758646, 0.05547897, 0.040862918, 0.05055692, 0.037515096, 0.044183854]
    2021-03-19 14:10:48,853 : INFO : topic #8 (0.038): 0.010*"action" + 0.008*"control" + 0.006*"reinforcement" + 0.006*"robot" + 0.005*"policy" + 0.005*"optimal" + 0.004*"controller" + 0.004*"dynamic" + 0.004*"reinforcement_learning" + 0.004*"trajectory"
    2021-03-19 14:10:48,853 : INFO : topic #6 (0.041): 0.009*"hidden" + 0.008*"image" + 0.006*"recognition" + 0.004*"hidden_unit" + 0.004*"layer" + 0.004*"energy" + 0.003*"net" + 0.003*"generalization" + 0.003*"field" + 0.003*"map"
    2021-03-19 14:10:48,853 : INFO : topic #4 (0.058): 0.005*"class" + 0.005*"hidden" + 0.004*"rule" + 0.004*"layer" + 0.004*"net" + 0.004*"classifier" + 0.003*"node" + 0.003*"propagation" + 0.003*"architecture" + 0.003*"context"
    2021-03-19 14:10:48,854 : INFO : topic #3 (0.058): 0.009*"image" + 0.005*"chip" + 0.005*"circuit" + 0.005*"analog" + 0.004*"threshold" + 0.004*"layer" + 0.003*"field" + 0.003*"bit" + 0.003*"node" + 0.003*"net"
    2021-03-19 14:10:48,854 : INFO : topic #2 (0.058): 0.007*"recognition" + 0.007*"speech" + 0.006*"object" + 0.006*"image" + 0.005*"word" + 0.005*"layer" + 0.005*"control" + 0.005*"signal" + 0.004*"hidden" + 0.003*"face"
    2021-03-19 14:10:48,854 : INFO : topic diff=0.230126, rho=0.447214
    2021-03-19 14:10:48,864 : INFO : PROGRESS: pass 4, at document #1740/1740
    2021-03-19 14:10:54,097 : INFO : optimized alpha [0.052869715, 0.044183813, 0.0546517, 0.054109406, 0.053801704, 0.053375203, 0.0394719, 0.04672288, 0.035995413, 0.04192354]
    2021-03-19 14:10:54,105 : INFO : topic #8 (0.036): 0.010*"action" + 0.010*"control" + 0.007*"reinforcement" + 0.006*"robot" + 0.006*"policy" + 0.005*"optimal" + 0.005*"controller" + 0.005*"dynamic" + 0.004*"reinforcement_learning" + 0.004*"trajectory"
    2021-03-19 14:10:54,105 : INFO : topic #6 (0.039): 0.009*"hidden" + 0.008*"image" + 0.006*"recognition" + 0.005*"hidden_unit" + 0.004*"layer" + 0.004*"energy" + 0.003*"net" + 0.003*"digit" + 0.003*"field" + 0.003*"generalization"
    2021-03-19 14:10:54,105 : INFO : topic #4 (0.054): 0.005*"class" + 0.005*"hidden" + 0.005*"rule" + 0.005*"net" + 0.005*"layer" + 0.004*"classifier" + 0.004*"node" + 0.003*"propagation" + 0.003*"architecture" + 0.003*"sequence"
    2021-03-19 14:10:54,106 : INFO : topic #3 (0.054): 0.009*"image" + 0.006*"chip" + 0.006*"circuit" + 0.006*"analog" + 0.004*"threshold" + 0.004*"layer" + 0.003*"field" + 0.003*"bit" + 0.003*"node" + 0.003*"net"
    2021-03-19 14:10:54,106 : INFO : topic #2 (0.055): 0.008*"recognition" + 0.008*"speech" + 0.007*"object" + 0.006*"word" + 0.006*"image" + 0.005*"layer" + 0.005*"signal" + 0.005*"control" + 0.004*"hidden" + 0.004*"face"
    2021-03-19 14:10:54,106 : INFO : topic diff=0.214075, rho=0.408248
    2021-03-19 14:10:54,116 : INFO : PROGRESS: pass 5, at document #1740/1740
    2021-03-19 14:10:59,195 : INFO : optimized alpha [0.05290075, 0.042460088, 0.052235015, 0.051339325, 0.05138389, 0.05190376, 0.038578223, 0.044312876, 0.035001513, 0.040355477]
    2021-03-19 14:10:59,202 : INFO : topic #8 (0.035): 0.011*"control" + 0.011*"action" + 0.007*"reinforcement" + 0.006*"policy" + 0.006*"robot" + 0.005*"controller" + 0.005*"optimal" + 0.005*"dynamic" + 0.005*"reinforcement_learning" + 0.005*"trajectory"
    2021-03-19 14:10:59,202 : INFO : topic #6 (0.039): 0.010*"hidden" + 0.008*"image" + 0.006*"recognition" + 0.005*"hidden_unit" + 0.005*"layer" + 0.004*"energy" + 0.004*"digit" + 0.004*"character" + 0.004*"net" + 0.003*"field"
    2021-03-19 14:10:59,203 : INFO : topic #5 (0.052): 0.021*"neuron" + 0.012*"cell" + 0.007*"response" + 0.007*"spike" + 0.006*"synaptic" + 0.006*"stimulus" + 0.005*"activity" + 0.005*"firing" + 0.005*"signal" + 0.004*"memory"
    2021-03-19 14:10:59,203 : INFO : topic #2 (0.052): 0.009*"recognition" + 0.008*"speech" + 0.007*"object" + 0.007*"word" + 0.006*"image" + 0.006*"signal" + 0.005*"layer" + 0.004*"hidden" + 0.004*"control" + 0.004*"face"
    2021-03-19 14:10:59,203 : INFO : topic #0 (0.053): 0.005*"gaussian" + 0.005*"noise" + 0.005*"matrix" + 0.005*"hidden" + 0.004*"approximation" + 0.004*"sample" + 0.004*"estimate" + 0.004*"variance" + 0.004*"bayesian" + 0.003*"prior"
    2021-03-19 14:10:59,203 : INFO : topic diff=0.202368, rho=0.377964
    2021-03-19 14:10:59,214 : INFO : PROGRESS: pass 6, at document #1740/1740
    2021-03-19 14:11:04,013 : INFO : optimized alpha [0.053310633, 0.041254587, 0.050613035, 0.04936813, 0.049790192, 0.05083673, 0.038025398, 0.042830754, 0.034370847, 0.039269455]
    2021-03-19 14:11:04,020 : INFO : topic #8 (0.034): 0.012*"control" + 0.011*"action" + 0.008*"reinforcement" + 0.007*"policy" + 0.006*"robot" + 0.006*"controller" + 0.005*"optimal" + 0.005*"dynamic" + 0.005*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:04,020 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"recognition" + 0.005*"hidden_unit" + 0.005*"layer" + 0.004*"energy" + 0.004*"character" + 0.004*"digit" + 0.004*"net" + 0.004*"field"
    2021-03-19 14:11:04,021 : INFO : topic #2 (0.051): 0.010*"recognition" + 0.009*"speech" + 0.007*"word" + 0.007*"object" + 0.007*"image" + 0.006*"signal" + 0.006*"layer" + 0.004*"hidden" + 0.004*"face" + 0.004*"classification"
    2021-03-19 14:11:04,021 : INFO : topic #5 (0.051): 0.021*"neuron" + 0.012*"cell" + 0.007*"response" + 0.007*"spike" + 0.006*"synaptic" + 0.006*"stimulus" + 0.006*"activity" + 0.005*"firing" + 0.005*"signal" + 0.004*"frequency"
    2021-03-19 14:11:04,021 : INFO : topic #0 (0.053): 0.006*"gaussian" + 0.005*"noise" + 0.005*"matrix" + 0.005*"hidden" + 0.004*"approximation" + 0.004*"estimate" + 0.004*"sample" + 0.004*"bayesian" + 0.004*"variance" + 0.004*"prior"
    2021-03-19 14:11:04,021 : INFO : topic diff=0.192693, rho=0.353553
    2021-03-19 14:11:04,032 : INFO : PROGRESS: pass 7, at document #1740/1740
    2021-03-19 14:11:08,718 : INFO : optimized alpha [0.053891532, 0.040544394, 0.049499568, 0.047873296, 0.04881682, 0.0500006, 0.037689965, 0.04181969, 0.03393164, 0.038607482]
    2021-03-19 14:11:08,725 : INFO : topic #8 (0.034): 0.013*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.007*"policy" + 0.006*"robot" + 0.006*"controller" + 0.005*"dynamic" + 0.005*"optimal" + 0.005*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:08,725 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"recognition" + 0.005*"layer" + 0.005*"hidden_unit" + 0.005*"character" + 0.004*"energy" + 0.004*"digit" + 0.004*"net" + 0.004*"field"
    2021-03-19 14:11:08,726 : INFO : topic #2 (0.049): 0.011*"recognition" + 0.009*"speech" + 0.008*"word" + 0.007*"object" + 0.007*"image" + 0.006*"signal" + 0.006*"layer" + 0.004*"face" + 0.004*"hidden" + 0.004*"classification"
    2021-03-19 14:11:08,726 : INFO : topic #5 (0.050): 0.022*"neuron" + 0.012*"cell" + 0.007*"response" + 0.007*"spike" + 0.007*"synaptic" + 0.006*"stimulus" + 0.006*"activity" + 0.005*"firing" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:08,726 : INFO : topic #0 (0.054): 0.006*"gaussian" + 0.005*"noise" + 0.005*"matrix" + 0.004*"approximation" + 0.004*"hidden" + 0.004*"estimate" + 0.004*"sample" + 0.004*"bayesian" + 0.004*"variance" + 0.004*"prior"
    2021-03-19 14:11:08,726 : INFO : topic diff=0.183651, rho=0.333333
    2021-03-19 14:11:08,737 : INFO : PROGRESS: pass 8, at document #1740/1740
    2021-03-19 14:11:13,510 : INFO : optimized alpha [0.0545965, 0.040113404, 0.048812777, 0.0467447, 0.048271947, 0.049433745, 0.03755086, 0.04124074, 0.033623673, 0.038269136]
    2021-03-19 14:11:13,518 : INFO : topic #8 (0.034): 0.014*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.007*"policy" + 0.006*"controller" + 0.006*"robot" + 0.006*"dynamic" + 0.006*"optimal" + 0.005*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:13,518 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"recognition" + 0.005*"layer" + 0.005*"hidden_unit" + 0.005*"character" + 0.004*"energy" + 0.004*"digit" + 0.004*"net" + 0.004*"field"
    2021-03-19 14:11:13,518 : INFO : topic #2 (0.049): 0.011*"recognition" + 0.009*"speech" + 0.008*"word" + 0.008*"object" + 0.007*"image" + 0.006*"signal" + 0.006*"layer" + 0.004*"face" + 0.004*"classification" + 0.004*"hidden"
    2021-03-19 14:11:13,518 : INFO : topic #5 (0.049): 0.022*"neuron" + 0.013*"cell" + 0.008*"response" + 0.007*"spike" + 0.007*"synaptic" + 0.006*"stimulus" + 0.006*"activity" + 0.006*"firing" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:13,519 : INFO : topic #0 (0.055): 0.006*"gaussian" + 0.005*"noise" + 0.005*"matrix" + 0.004*"approximation" + 0.004*"estimate" + 0.004*"hidden" + 0.004*"sample" + 0.004*"bayesian" + 0.004*"likelihood" + 0.004*"variance"
    2021-03-19 14:11:13,519 : INFO : topic diff=0.175043, rho=0.316228
    2021-03-19 14:11:13,530 : INFO : PROGRESS: pass 9, at document #1740/1740
    2021-03-19 14:11:18,487 : INFO : optimized alpha [0.055368014, 0.039957594, 0.048399936, 0.045934383, 0.04802085, 0.049097233, 0.037513737, 0.040929828, 0.0334422, 0.038141657]
    2021-03-19 14:11:18,495 : INFO : topic #8 (0.033): 0.014*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.007*"policy" + 0.006*"controller" + 0.006*"robot" + 0.006*"dynamic" + 0.006*"optimal" + 0.005*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:18,495 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"layer" + 0.006*"recognition" + 0.005*"character" + 0.005*"hidden_unit" + 0.004*"digit" + 0.004*"energy" + 0.004*"field" + 0.004*"net"
    2021-03-19 14:11:18,496 : INFO : topic #2 (0.048): 0.012*"recognition" + 0.010*"speech" + 0.009*"word" + 0.008*"image" + 0.008*"object" + 0.006*"signal" + 0.006*"layer" + 0.004*"face" + 0.004*"classification" + 0.004*"trained"
    2021-03-19 14:11:18,496 : INFO : topic #5 (0.049): 0.022*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"stimulus" + 0.006*"activity" + 0.006*"firing" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:18,496 : INFO : topic #0 (0.055): 0.006*"gaussian" + 0.005*"noise" + 0.005*"matrix" + 0.005*"estimate" + 0.005*"approximation" + 0.004*"hidden" + 0.004*"sample" + 0.004*"bayesian" + 0.004*"likelihood" + 0.004*"prior"
    2021-03-19 14:11:18,496 : INFO : topic diff=0.166410, rho=0.301511
    2021-03-19 14:11:18,507 : INFO : PROGRESS: pass 10, at document #1740/1740
    2021-03-19 14:11:23,641 : INFO : optimized alpha [0.056234606, 0.039904997, 0.04814231, 0.045396697, 0.048054837, 0.048870783, 0.037563145, 0.04080154, 0.03336996, 0.03815883]
    2021-03-19 14:11:23,650 : INFO : topic #8 (0.033): 0.015*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.008*"policy" + 0.007*"controller" + 0.006*"robot" + 0.006*"dynamic" + 0.006*"optimal" + 0.005*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:23,651 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"layer" + 0.006*"character" + 0.005*"recognition" + 0.005*"hidden_unit" + 0.004*"digit" + 0.004*"energy" + 0.004*"field" + 0.004*"net"
    2021-03-19 14:11:23,651 : INFO : topic #2 (0.048): 0.012*"recognition" + 0.010*"speech" + 0.009*"word" + 0.008*"image" + 0.008*"object" + 0.006*"signal" + 0.006*"layer" + 0.005*"face" + 0.004*"classification" + 0.004*"trained"
    2021-03-19 14:11:23,651 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"activity" + 0.006*"stimulus" + 0.006*"firing" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:23,651 : INFO : topic #0 (0.056): 0.006*"gaussian" + 0.005*"noise" + 0.005*"estimate" + 0.005*"matrix" + 0.005*"approximation" + 0.004*"bayesian" + 0.004*"likelihood" + 0.004*"sample" + 0.004*"hidden" + 0.004*"prior"
    2021-03-19 14:11:23,651 : INFO : topic diff=0.157726, rho=0.288675
    2021-03-19 14:11:23,663 : INFO : PROGRESS: pass 11, at document #1740/1740
    2021-03-19 14:11:28,247 : INFO : optimized alpha [0.05706192, 0.039978355, 0.04797657, 0.044978894, 0.048209604, 0.048704833, 0.03767563, 0.04074631, 0.033347335, 0.038310345]
    2021-03-19 14:11:28,255 : INFO : topic #8 (0.033): 0.015*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.008*"policy" + 0.007*"controller" + 0.006*"robot" + 0.006*"dynamic" + 0.006*"optimal" + 0.006*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:28,256 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"layer" + 0.006*"character" + 0.005*"recognition" + 0.005*"hidden_unit" + 0.004*"digit" + 0.004*"energy" + 0.004*"field" + 0.004*"net"
    2021-03-19 14:11:28,256 : INFO : topic #4 (0.048): 0.008*"hidden" + 0.007*"net" + 0.006*"layer" + 0.006*"rule" + 0.005*"node" + 0.004*"classifier" + 0.004*"hidden_unit" + 0.004*"class" + 0.004*"propagation" + 0.004*"sequence"
    2021-03-19 14:11:28,256 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"activity" + 0.006*"firing" + 0.006*"stimulus" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:28,256 : INFO : topic #0 (0.057): 0.006*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"matrix" + 0.005*"approximation" + 0.004*"likelihood" + 0.004*"bayesian" + 0.004*"prior" + 0.004*"sample" + 0.004*"hidden"
    2021-03-19 14:11:28,256 : INFO : topic diff=0.149091, rho=0.277350
    2021-03-19 14:11:28,268 : INFO : PROGRESS: pass 12, at document #1740/1740
    2021-03-19 14:11:32,844 : INFO : optimized alpha [0.057841934, 0.040147286, 0.047984846, 0.04466845, 0.048510514, 0.048608452, 0.037831437, 0.04078982, 0.03338453, 0.038538743]
    2021-03-19 14:11:32,852 : INFO : topic #8 (0.033): 0.015*"control" + 0.012*"action" + 0.008*"reinforcement" + 0.008*"policy" + 0.007*"controller" + 0.006*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:32,852 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"layer" + 0.006*"character" + 0.005*"recognition" + 0.005*"hidden_unit" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"field"
    2021-03-19 14:11:32,853 : INFO : topic #4 (0.049): 0.008*"hidden" + 0.007*"net" + 0.006*"layer" + 0.006*"rule" + 0.005*"node" + 0.004*"hidden_unit" + 0.004*"classifier" + 0.004*"class" + 0.004*"propagation" + 0.004*"sequence"
    2021-03-19 14:11:32,853 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"firing" + 0.006*"activity" + 0.006*"stimulus" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:32,853 : INFO : topic #0 (0.058): 0.006*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"approximation" + 0.005*"matrix" + 0.004*"likelihood" + 0.004*"bayesian" + 0.004*"prior" + 0.004*"variance" + 0.004*"sample"
    2021-03-19 14:11:32,853 : INFO : topic diff=0.140596, rho=0.267261
    2021-03-19 14:11:32,865 : INFO : PROGRESS: pass 13, at document #1740/1740
    2021-03-19 14:11:37,447 : INFO : optimized alpha [0.058551796, 0.040399875, 0.048106886, 0.044424307, 0.04896659, 0.04858641, 0.03804483, 0.040931225, 0.03344661, 0.038809597]
    2021-03-19 14:11:37,455 : INFO : topic #8 (0.033): 0.016*"control" + 0.013*"action" + 0.008*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.006*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:37,455 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"layer" + 0.006*"character" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"field"
    2021-03-19 14:11:37,456 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"firing" + 0.006*"activity" + 0.006*"stimulus" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:37,456 : INFO : topic #4 (0.049): 0.008*"hidden" + 0.007*"net" + 0.006*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"classifier" + 0.004*"class" + 0.004*"sequence" + 0.004*"propagation"
    2021-03-19 14:11:37,456 : INFO : topic #0 (0.059): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"approximation" + 0.005*"matrix" + 0.005*"likelihood" + 0.004*"bayesian" + 0.004*"prior" + 0.004*"variance" + 0.004*"sample"
    2021-03-19 14:11:37,456 : INFO : topic diff=0.132327, rho=0.258199
    2021-03-19 14:11:37,467 : INFO : PROGRESS: pass 14, at document #1740/1740
    2021-03-19 14:11:41,536 : INFO : optimized alpha [0.05925279, 0.040705983, 0.04832607, 0.04427085, 0.049501013, 0.048644915, 0.038285527, 0.04113948, 0.03352695, 0.039150245]
    2021-03-19 14:11:41,544 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.006*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:41,544 : INFO : topic #6 (0.038): 0.010*"hidden" + 0.009*"image" + 0.006*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"net"
    2021-03-19 14:11:41,544 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.013*"cell" + 0.008*"spike" + 0.008*"response" + 0.007*"synaptic" + 0.006*"firing" + 0.006*"activity" + 0.006*"stimulus" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:41,545 : INFO : topic #4 (0.050): 0.008*"hidden" + 0.008*"net" + 0.006*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"sequence" + 0.004*"propagation" + 0.004*"architecture" + 0.004*"activation"
    2021-03-19 14:11:41,545 : INFO : topic #0 (0.059): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"approximation" + 0.005*"likelihood" + 0.005*"matrix" + 0.004*"prior" + 0.004*"bayesian" + 0.004*"variance" + 0.004*"density"
    2021-03-19 14:11:41,545 : INFO : topic diff=0.124371, rho=0.250000
    2021-03-19 14:11:41,556 : INFO : PROGRESS: pass 15, at document #1740/1740
    2021-03-19 14:11:45,592 : INFO : optimized alpha [0.05994643, 0.041028578, 0.048593685, 0.04419364, 0.05009154, 0.048734292, 0.03856185, 0.041424613, 0.033627965, 0.039535556]
    2021-03-19 14:11:45,600 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.007*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.005*"reinforcement_learning"
    2021-03-19 14:11:45,600 : INFO : topic #6 (0.039): 0.010*"hidden" + 0.009*"image" + 0.006*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"dynamic"
    2021-03-19 14:11:45,600 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.014*"cell" + 0.008*"spike" + 0.008*"response" + 0.008*"synaptic" + 0.006*"firing" + 0.006*"activity" + 0.006*"stimulus" + 0.005*"signal" + 0.005*"frequency"
    2021-03-19 14:11:45,600 : INFO : topic #4 (0.050): 0.008*"hidden" + 0.008*"net" + 0.007*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"sequence" + 0.004*"architecture" + 0.004*"propagation" + 0.004*"activation"
    2021-03-19 14:11:45,601 : INFO : topic #0 (0.060): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"likelihood" + 0.005*"approximation" + 0.005*"matrix" + 0.004*"prior" + 0.004*"bayesian" + 0.004*"variance" + 0.004*"density"
    2021-03-19 14:11:45,601 : INFO : topic diff=0.116794, rho=0.242536
    2021-03-19 14:11:45,611 : INFO : PROGRESS: pass 16, at document #1740/1740
    2021-03-19 14:11:49,737 : INFO : optimized alpha [0.06068379, 0.041378528, 0.048856508, 0.0441432, 0.05072476, 0.0488511, 0.038870405, 0.041741073, 0.03375229, 0.039979585]
    2021-03-19 14:11:49,745 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.007*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.006*"reinforcement_learning"
    2021-03-19 14:11:49,745 : INFO : topic #6 (0.039): 0.010*"hidden" + 0.009*"image" + 0.006*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"dynamic"
    2021-03-19 14:11:49,745 : INFO : topic #5 (0.049): 0.023*"neuron" + 0.014*"cell" + 0.008*"spike" + 0.008*"response" + 0.008*"synaptic" + 0.006*"firing" + 0.006*"activity" + 0.006*"stimulus" + 0.006*"signal" + 0.005*"frequency"
    2021-03-19 14:11:49,746 : INFO : topic #4 (0.051): 0.008*"hidden" + 0.008*"net" + 0.007*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"sequence" + 0.004*"architecture" + 0.004*"activation" + 0.004*"propagation"
    2021-03-19 14:11:49,746 : INFO : topic #0 (0.061): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"likelihood" + 0.005*"approximation" + 0.005*"prior" + 0.004*"bayesian" + 0.004*"matrix" + 0.004*"variance" + 0.004*"density"
    2021-03-19 14:11:49,746 : INFO : topic diff=0.109661, rho=0.235702
    2021-03-19 14:11:49,756 : INFO : PROGRESS: pass 17, at document #1740/1740
    2021-03-19 14:11:53,841 : INFO : optimized alpha [0.061406724, 0.04174132, 0.0491224, 0.044116188, 0.05141323, 0.049025778, 0.03920408, 0.04207979, 0.033907466, 0.04045379]
    2021-03-19 14:11:53,850 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.007*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.006*"reinforcement_learning"
    2021-03-19 14:11:53,850 : INFO : topic #6 (0.039): 0.010*"hidden" + 0.009*"image" + 0.007*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"dynamic"
    2021-03-19 14:11:53,850 : INFO : topic #2 (0.049): 0.014*"recognition" + 0.011*"speech" + 0.010*"word" + 0.010*"image" + 0.008*"object" + 0.006*"signal" + 0.005*"layer" + 0.005*"face" + 0.005*"classification" + 0.005*"trained"
    2021-03-19 14:11:53,851 : INFO : topic #4 (0.051): 0.009*"hidden" + 0.008*"net" + 0.007*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"architecture" + 0.004*"sequence" + 0.004*"activation" + 0.004*"propagation"
    2021-03-19 14:11:53,851 : INFO : topic #0 (0.061): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"likelihood" + 0.005*"approximation" + 0.005*"prior" + 0.005*"bayesian" + 0.004*"matrix" + 0.004*"variance" + 0.004*"density"
    2021-03-19 14:11:53,851 : INFO : topic diff=0.102938, rho=0.229416
    2021-03-19 14:11:53,862 : INFO : PROGRESS: pass 18, at document #1740/1740
    2021-03-19 14:11:57,816 : INFO : optimized alpha [0.062154472, 0.042110436, 0.04939213, 0.044109803, 0.05212181, 0.049227104, 0.039544087, 0.04246847, 0.03410476, 0.040957462]
    2021-03-19 14:11:57,823 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.007*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.006*"reinforcement_learning"
    2021-03-19 14:11:57,824 : INFO : topic #6 (0.040): 0.010*"hidden" + 0.008*"image" + 0.007*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"energy" + 0.004*"attractor" + 0.004*"dynamic"
    2021-03-19 14:11:57,824 : INFO : topic #2 (0.049): 0.014*"recognition" + 0.011*"speech" + 0.010*"word" + 0.010*"image" + 0.008*"object" + 0.006*"signal" + 0.005*"layer" + 0.005*"face" + 0.005*"classification" + 0.005*"trained"
    2021-03-19 14:11:57,824 : INFO : topic #4 (0.052): 0.009*"hidden" + 0.008*"net" + 0.007*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"architecture" + 0.004*"sequence" + 0.004*"activation" + 0.004*"propagation"
    2021-03-19 14:11:57,824 : INFO : topic #0 (0.062): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"likelihood" + 0.005*"approximation" + 0.005*"prior" + 0.005*"bayesian" + 0.004*"matrix" + 0.004*"density" + 0.004*"variance"
    2021-03-19 14:11:57,825 : INFO : topic diff=0.096678, rho=0.223607
    2021-03-19 14:11:57,835 : INFO : PROGRESS: pass 19, at document #1740/1740
    2021-03-19 14:12:01,856 : INFO : optimized alpha [0.06292996, 0.04251684, 0.049703237, 0.044167582, 0.052860808, 0.049467582, 0.039925203, 0.042864826, 0.03433462, 0.0415304]
    2021-03-19 14:12:01,864 : INFO : topic #8 (0.034): 0.016*"control" + 0.013*"action" + 0.009*"policy" + 0.008*"reinforcement" + 0.007*"controller" + 0.007*"dynamic" + 0.006*"robot" + 0.006*"optimal" + 0.006*"trajectory" + 0.006*"reinforcement_learning"
    2021-03-19 14:12:01,864 : INFO : topic #6 (0.040): 0.010*"hidden" + 0.008*"image" + 0.007*"character" + 0.006*"layer" + 0.005*"hidden_unit" + 0.005*"recognition" + 0.005*"digit" + 0.004*"attractor" + 0.004*"energy" + 0.004*"dynamic"
    2021-03-19 14:12:01,864 : INFO : topic #2 (0.050): 0.014*"recognition" + 0.011*"speech" + 0.010*"word" + 0.010*"image" + 0.008*"object" + 0.006*"signal" + 0.005*"layer" + 0.005*"classification" + 0.005*"face" + 0.005*"trained"
    2021-03-19 14:12:01,865 : INFO : topic #4 (0.053): 0.009*"hidden" + 0.008*"net" + 0.007*"layer" + 0.006*"rule" + 0.006*"node" + 0.005*"hidden_unit" + 0.004*"architecture" + 0.004*"activation" + 0.004*"sequence" + 0.004*"propagation"
    2021-03-19 14:12:01,865 : INFO : topic #0 (0.063): 0.007*"gaussian" + 0.006*"noise" + 0.005*"estimate" + 0.005*"likelihood" + 0.005*"approximation" + 0.005*"prior" + 0.005*"bayesian" + 0.004*"density" + 0.004*"mixture" + 0.004*"variance"
    2021-03-19 14:12:01,865 : INFO : topic diff=0.090853, rho=0.218218
    2021-03-19 14:12:01,877 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=8644, num_topics=10, decay=0.5, chunksize=2000) in 109.40s', 'datetime': '2021-03-19T14:12:01.877604', 'gensim': '4.0.0.rc1', 'python': '3.7.0 (default, Jun 28 2018, 13:15:42) \n[GCC 7.2.0]', 'platform': 'Linux-4.15.0-136-generic-x86_64-with-debian-buster-sid', 'event': 'created'}




.. GENERATED FROM PYTHON SOURCE LINES 265-280

We can compute the topic coherence of each topic. Below we display the
average topic coherence and print the topics in order of topic coherence.

Note that we use the "Umass" topic coherence measure here (see
:py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently
obtained an implementation of the "AKSW" topic coherence measure (see
accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).

If you are familiar with the subject of the articles in this dataset, you can
see that the topics below make a lot of sense. However, they are not without
flaws. We can see that there is substantial overlap between some topics,
others are hard to interpret, and most of them have at least some terms that
seem out of place. If you were able to do better, feel free to share your
methods on the blog at http://rare-technologies.com/lda-training-tips/ !


.. GENERATED FROM PYTHON SOURCE LINES 280-290

.. code-block:: default


    top_topics = model.top_topics(corpus) #, num_words=20)

    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.
    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics
    print('Average topic coherence: %.4f.' % avg_topic_coherence)

    from pprint import pprint
    pprint(top_topics)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2021-03-19 14:12:02,008 : INFO : CorpusAccumulator accumulated stats from 1000 documents
    Average topic coherence: -1.1072.
    [([(0.023360161, 'neuron'),
       (0.013864572, 'cell'),
       (0.0085508, 'spike'),
       (0.007835109, 'response'),
       (0.0077002184, 'synaptic'),
       (0.006420619, 'firing'),
       (0.0063291225, 'activity'),
       (0.005894408, 'stimulus'),
       (0.005635916, 'signal'),
       (0.005319338, 'frequency'),
       (0.0044079474, 'potential'),
       (0.0042212, 'connection'),
       (0.003969707, 'fig'),
       (0.0038775448, 'phase'),
       (0.0037467096, 'synapsis'),
       (0.0035546266, 'channel'),
       (0.0035464808, 'dynamic'),
       (0.0035111816, 'memory'),
       (0.003500412, 'simulation'),
       (0.0033668294, 'temporal')],
      -0.8843724877515563),
     ([(0.007043698, 'gaussian'),
       (0.0058810986, 'noise'),
       (0.005357382, 'estimate'),
       (0.005118217, 'likelihood'),
       (0.004725707, 'approximation'),
       (0.0047162576, 'prior'),
       (0.004589121, 'bayesian'),
       (0.0044163894, 'density'),
       (0.004383228, 'mixture'),
       (0.0043818722, 'variance'),
       (0.004343727, 'matrix'),
       (0.003920799, 'log'),
       (0.0039041233, 'sample'),
       (0.0038657538, 'posterior'),
       (0.0038494268, 'hidden'),
       (0.003747304, 'prediction'),
       (0.0035524433, 'generalization'),
       (0.003297515, 'em'),
       (0.0031830291, 'optimal'),
       (0.0029574349, 'estimation')],
      -0.9201121458749306),
     ([(0.013338742, 'visual'),
       (0.011440194, 'cell'),
       (0.010699649, 'field'),
       (0.009350259, 'image'),
       (0.008701173, 'motion'),
       (0.008576538, 'map'),
       (0.0077895345, 'direction'),
       (0.0073878667, 'orientation'),
       (0.006964441, 'eye'),
       (0.0066007036, 'response'),
       (0.0062312516, 'stimulus'),
       (0.006194355, 'spatial'),
       (0.0055934438, 'receptive'),
       (0.005137706, 'receptive_field'),
       (0.00512753, 'object'),
       (0.004664231, 'layer'),
       (0.0046304427, 'activity'),
       (0.0045092506, 'position'),
       (0.004168487, 'cortex'),
       (0.0040872716, 'location')],
      -0.9666086669197183),
     ([(0.009677556, 'hidden'),
       (0.008472348, 'image'),
       (0.0066851787, 'character'),
       (0.0064806826, 'layer'),
       (0.005060741, 'hidden_unit'),
       (0.004902215, 'recognition'),
       (0.004825573, 'digit'),
       (0.0043749292, 'attractor'),
       (0.0043325345, 'energy'),
       (0.00431843, 'dynamic'),
       (0.0038877935, 'matrix'),
       (0.003805258, 'net'),
       (0.003757226, 'field'),
       (0.0035065063, 'transformation'),
       (0.0034933372, 'dimensional'),
       (0.0034391459, 'distance'),
       (0.0031490896, 'gradient'),
       (0.0031419578, 'solution'),
       (0.002954112, 'map'),
       (0.0028736237, 'minimum')],
      -1.011100924928429),
     ([(0.010836434, 'circuit'),
       (0.009359381, 'chip'),
       (0.008903197, 'analog'),
       (0.00655248, 'neuron'),
       (0.006147317, 'threshold'),
       (0.0050505013, 'image'),
       (0.0048734145, 'bit'),
       (0.0048433533, 'voltage'),
       (0.004609887, 'memory'),
       (0.004231914, 'vlsi'),
       (0.0042090695, 'implementation'),
       (0.004113957, 'net'),
       (0.003907882, 'gate'),
       (0.0038376434, 'layer'),
       (0.0034949183, 'pp'),
       (0.003291277, 'element'),
       (0.0032199384, 'node'),
       (0.0030992834, 'signal'),
       (0.0029631325, 'design'),
       (0.0028471586, 'processor')],
      -1.0450720584710176),
     ([(0.008781833, 'hidden'),
       (0.008109003, 'net'),
       (0.0069496827, 'layer'),
       (0.006155399, 'rule'),
       (0.005891262, 'node'),
       (0.0051560537, 'hidden_unit'),
       (0.0041502067, 'architecture'),
       (0.0041317134, 'activation'),
       (0.0041251457, 'sequence'),
       (0.0040346556, 'propagation'),
       (0.0036248995, 'back'),
       (0.0035959794, 'recurrent'),
       (0.0031377305, 'class'),
       (0.0030542722, 'trained'),
       (0.0030384492, 'code'),
       (0.002923781, 'expert'),
       (0.0028879363, 'string'),
       (0.0027964872, 'learn'),
       (0.0027678378, 'table'),
       (0.0027654031, 'connection')],
      -1.122278491657109),
     ([(0.014161764, 'recognition'),
       (0.011104057, 'speech'),
       (0.010318562, 'word'),
       (0.010277273, 'image'),
       (0.00809512, 'object'),
       (0.0063050594, 'signal'),
       (0.0053472514, 'layer'),
       (0.005024713, 'classification'),
       (0.0050242324, 'face'),
       (0.004580911, 'trained'),
       (0.004409548, 'human'),
       (0.0043301815, 'context'),
       (0.0042581595, 'frame'),
       (0.0040203724, 'hidden'),
       (0.004008649, 'speaker'),
       (0.0035841789, 'class'),
       (0.0033736168, 'sequence'),
       (0.0032663026, 'hmm'),
       (0.0032505158, 'architecture'),
       (0.0031761383, 'view')],
      -1.1844643136695376),
     ([(0.0071913837, 'matrix'),
       (0.006639144, 'gradient'),
       (0.0058832015, 'kernel'),
       (0.0058791665, 'component'),
       (0.0047264574, 'class'),
       (0.0042780563, 'density'),
       (0.004226884, 'xi'),
       (0.004164046, 'convergence'),
       (0.0041592806, 'source'),
       (0.0040763966, 'loss'),
       (0.00392406, 'basis'),
       (0.0036241056, 'regression'),
       (0.0035536229, 'approximation'),
       (0.0033525354, 'independent'),
       (0.0032649476, 'bound'),
       (0.0031867179, 'mixture'),
       (0.0031306876, 'let'),
       (0.0030615225, 'signal'),
       (0.0030061873, 'support'),
       (0.0029361995, 'pca')],
      -1.2550214906161075),
     ([(0.012204602, 'tree'),
       (0.010181904, 'node'),
       (0.010171177, 'class'),
       (0.007966109, 'classifier'),
       (0.0075656017, 'decision'),
       (0.005655141, 'rule'),
       (0.0056041405, 'classification'),
       (0.0054354756, 'sample'),
       (0.0050921105, 'distance'),
       (0.0046420856, 'bound'),
       (0.0035473844, 'let'),
       (0.0032015098, 'measure'),
       (0.0031701634, 'cluster'),
       (0.0030615227, 'clustering'),
       (0.0030600468, 'graph'),
       (0.003044858, 'neighbor'),
       (0.0030077181, 'nearest'),
       (0.0029182513, 'call'),
       (0.0027482447, 'machine'),
       (0.0027105191, 'hypothesis')],
      -1.2831209969858721),
     ([(0.016391048, 'control'),
       (0.013031393, 'action'),
       (0.009197483, 'policy'),
       (0.008487638, 'reinforcement'),
       (0.0068111503, 'controller'),
       (0.0067618974, 'dynamic'),
       (0.006282514, 'robot'),
       (0.0061591244, 'optimal'),
       (0.005933612, 'trajectory'),
       (0.00556125, 'reinforcement_learning'),
       (0.004895806, 'environment'),
       (0.0044026882, 'goal'),
       (0.0042024464, 'reward'),
       (0.0037804258, 'position'),
       (0.0037499247, 'arm'),
       (0.003601292, 'motor'),
       (0.0034139594, 'sutton'),
       (0.0031908047, 'movement'),
       (0.003142896, 'td'),
       (0.0031323545, 'trial')],
      -1.4003243935908478)]




.. GENERATED FROM PYTHON SOURCE LINES 291-313

Things to experiment with
-------------------------

* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.
* Adding trigrams or even higher order n-grams.
* Consider whether using a hold-out set or cross-validation is the way to go for you.
* Try other datasets.

Where to go from here
---------------------

* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).
* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).
* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).
* If you haven't already, read [1] and [2] (see references).

References
----------

1. "Latent Dirichlet Allocation", Blei et al. 2003.
2. "Online Learning for Latent Dirichlet Allocation", Hoffman et al. 2010.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  47.007 seconds)

**Estimated memory usage:**  658 MB


.. _sphx_glr_download_auto_examples_tutorials_run_lda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_lda.py <run_lda.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_lda.ipynb <run_lda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_doc2vec_lee.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_doc2vec_lee.py:


Doc2Vec Model
=============

Introduces Gensim's Doc2Vec model and demonstrates its use on the
`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.



.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








Doc2Vec is a :ref:`core_concepts_model` that represents each
:ref:`core_concepts_document` as a :ref:`core_concepts_vector`.  This
tutorial introduces the model and demonstrates how to train and assess it.

Here's a list of what we'll be doing:

0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec
1. Load and preprocess the training and test corpora (see :ref:`core_concepts_corpus`)
2. Train a Doc2Vec :ref:`core_concepts_model` model using the training corpus
3. Demonstrate how the trained model can be used to infer a :ref:`core_concepts_vector`
4. Assess the model
5. Test the model on the test corpus

Review: Bag-of-words
--------------------

.. Note:: Feel free to skip these review sections if you're already familiar with the models.

You may be familiar with the `bag-of-words model
<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the
:ref:`core_concepts_vector` section.
This model transforms each document to a fixed-length vector of integers.
For example, given the sentences:

- ``John likes to watch movies. Mary likes movies too.``
- ``John also likes to watch football games. Mary hates football.``

The model outputs the vectors:

- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``
- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``

Each vector has 10 elements, where each element counts the number of times a
particular word occurred in the document.
The order of elements is arbitrary.
In the example above, the order of the elements corresponds to the words:
``["John", "likes", "to", "watch", "movies", "Mary", "too", "also", "football", "games", "hates"]``.

Bag-of-words models are surprisingly effective, but have several weaknesses.

First, they lose all information about word order: "John likes Mary" and
"Mary likes John" correspond to identical vectors. There is a solution: bag
of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__
models consider word phrases of length n to represent documents as
fixed-length vectors to capture local word order but suffer from data
sparsity and high dimensionality.

Second, the model does not attempt to learn the meaning of the underlying
words, and as a consequence, the distance between vectors doesn't always
reflect the difference in meaning.  The ``Word2Vec`` model addresses this
second problem.

Review: ``Word2Vec`` Model
--------------------------

``Word2Vec`` is a more recent model that embeds words in a lower-dimensional
vector space using a shallow neural network. The result is a set of
word-vectors where vectors close together in vector space have similar
meanings based on context, and word-vectors distant to each other have
differing meanings. For example, ``strong`` and ``powerful`` would be close
together and ``strong`` and ``Paris`` would be relatively far.

Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.

With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.
But what if we want to calculate a vector for the **entire document**\ ?
We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.
However, there is a better way...

Introducing: Paragraph Vector
-----------------------------

.. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.

Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,
which usually outperforms such simple-averaging of ``Word2Vec`` vectors.

The basic idea is: act as if a document has another floating word-like
vector, which contributes to all training predictions, and is updated like
other word-vectors, but we will call it a doc-vector. Gensim's
:py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.

There are two implementations:

1. Paragraph Vector - Distributed Memory (PV-DM)
2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)

.. Important::
  Don't let the implementation details below scare you.
  They're advanced material: if it's too much, then move on to the next section.

PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training
a neural network on the synthetic task of predicting a center word based an
average of both context word-vectors and the full document's doc-vector.

PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training
a neural network on the synthetic task of predicting a target word just from
the full document's doc-vector. (It is also common to combine this with
skip-gram testing, using both the doc-vector and nearby word-vectors to
predict a single target word, but only one at a time.)

Prepare the Training and Test Data
----------------------------------

For this tutorial, we'll be training our model using the `Lee Background
Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_
included in gensim. This corpus contains 314 documents selected from the
Australian Broadcasting Corporation‚Äôs news mail service, which provides text
e-mails of headline stories and covers a number of broad topics.

And we'll test our model by eye using the much shorter `Lee Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_
which contains 50 documents.



.. code-block:: default


    import os
    import gensim
    # Set file names for train and test data
    test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')
    lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')
    lee_test_file = os.path.join(test_data_dir, 'lee.cor')








Define a Function to Read and Preprocess Text
---------------------------------------------

Below, we define a function to:

- open the train/test file (with latin encoding)
- read the file line-by-line
- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)

The file we're reading is a **corpus**.
Each line of the file is a **document**.

.. Important::
  To train the model, we'll need to associate a tag/number with each document
  of the training corpus. In our case, the tag is simply the zero-based line
  number.



.. code-block:: default

    import smart_open

    def read_corpus(fname, tokens_only=False):
        with smart_open.open(fname, encoding="iso-8859-1") as f:
            for i, line in enumerate(f):
                tokens = gensim.utils.simple_preprocess(line)
                if tokens_only:
                    yield tokens
                else:
                    # For training data, add tags
                    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

    train_corpus = list(read_corpus(lee_train_file))
    test_corpus = list(read_corpus(lee_test_file, tokens_only=True))








Let's take a look at the training corpus



.. code-block:: default

    print(train_corpus[:2])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]




And the testing corpus looks like this:



.. code-block:: default

    print(test_corpus[:2])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]




Notice that the testing corpus is just a list of lists and does not contain
any tags.


Training the Model
------------------

Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and
iterating over the training corpus 40 times. We set the minimum word count to
2 in order to discard words with very few occurrences. (Without a variety of
representative examples, retaining such infrequent words can often make a
model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__
results, using 10s-of-thousands to millions of docs, are 10-20. More
iterations take more time and eventually reach a point of diminishing
returns.

However, this is a very very small dataset (300 documents) with shortish
documents (a few hundred words). Adding training passes can sometimes help
with such small datasets.



.. code-block:: default

    model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)








Build a vocabulary


.. code-block:: default

    model.build_vocab(train_corpus)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 21:08:55,026 : INFO : collecting all words and their counts
    2020-09-30 21:08:55,027 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
    2020-09-30 21:08:55,043 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words
    2020-09-30 21:08:55,043 : INFO : Loading a fresh vocabulary
    2020-09-30 21:08:55,064 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)
    2020-09-30 21:08:55,064 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)
    2020-09-30 21:08:55,098 : INFO : deleting the raw counts dictionary of 6981 items
    2020-09-30 21:08:55,100 : INFO : sample=0.001 downsamples 46 most-common words
    2020-09-30 21:08:55,100 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)
    2020-09-30 21:08:55,149 : INFO : estimated required memory for 3955 words and 50 dimensions: 3679500 bytes
    2020-09-30 21:08:55,149 : INFO : resetting layer weights




Essentially, the vocabulary is a list (accessible via
``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.
Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,
For example, to see how many times ``penalty`` appeared in the training corpus:



.. code-block:: default

    print(f"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Word 'penalty' appeared 4 times in the training corpus.




Next, train the model on the corpus.
If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.
If the BLAS library is not being used, this should take no more than 2
minutes, so use optimized Gensim with BLAS if you value your time.



.. code-block:: default

    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 21:08:55,553 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5
    2020-09-30 21:08:55,613 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,614 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,614 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,614 : INFO : EPOCH - 1 : training on 58152 raw words (42784 effective words) took 0.1s, 751479 effective words/s
    2020-09-30 21:08:55,664 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,666 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,666 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,666 : INFO : EPOCH - 2 : training on 58152 raw words (42745 effective words) took 0.1s, 845101 effective words/s
    2020-09-30 21:08:55,718 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,719 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,720 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,720 : INFO : EPOCH - 3 : training on 58152 raw words (42605 effective words) took 0.1s, 810845 effective words/s
    2020-09-30 21:08:55,781 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,783 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,784 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,784 : INFO : EPOCH - 4 : training on 58152 raw words (42723 effective words) took 0.1s, 677810 effective words/s
    2020-09-30 21:08:55,846 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,847 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,848 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,848 : INFO : EPOCH - 5 : training on 58152 raw words (42641 effective words) took 0.1s, 682513 effective words/s
    2020-09-30 21:08:55,903 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,905 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,905 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,905 : INFO : EPOCH - 6 : training on 58152 raw words (42654 effective words) took 0.1s, 760381 effective words/s
    2020-09-30 21:08:55,960 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:55,962 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:55,964 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:55,964 : INFO : EPOCH - 7 : training on 58152 raw words (42751 effective words) took 0.1s, 741994 effective words/s
    2020-09-30 21:08:56,018 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,020 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,020 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,020 : INFO : EPOCH - 8 : training on 58152 raw words (42692 effective words) took 0.1s, 773631 effective words/s
    2020-09-30 21:08:56,076 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,078 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,081 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,081 : INFO : EPOCH - 9 : training on 58152 raw words (42745 effective words) took 0.1s, 719453 effective words/s
    2020-09-30 21:08:56,137 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,137 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,137 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,138 : INFO : EPOCH - 10 : training on 58152 raw words (42733 effective words) took 0.1s, 770082 effective words/s
    2020-09-30 21:08:56,195 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,196 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,197 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,197 : INFO : EPOCH - 11 : training on 58152 raw words (42791 effective words) took 0.1s, 734171 effective words/s
    2020-09-30 21:08:56,253 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,255 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,255 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,255 : INFO : EPOCH - 12 : training on 58152 raw words (42773 effective words) took 0.1s, 745248 effective words/s
    2020-09-30 21:08:56,316 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,318 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,318 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,318 : INFO : EPOCH - 13 : training on 58152 raw words (42793 effective words) took 0.1s, 702300 effective words/s
    2020-09-30 21:08:56,369 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,371 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,373 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,373 : INFO : EPOCH - 14 : training on 58152 raw words (42637 effective words) took 0.1s, 802259 effective words/s
    2020-09-30 21:08:56,421 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,425 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,426 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,426 : INFO : EPOCH - 15 : training on 58152 raw words (42686 effective words) took 0.1s, 820787 effective words/s
    2020-09-30 21:08:56,475 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,478 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,479 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,479 : INFO : EPOCH - 16 : training on 58152 raw words (42799 effective words) took 0.1s, 829690 effective words/s
    2020-09-30 21:08:56,530 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,530 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,533 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,534 : INFO : EPOCH - 17 : training on 58152 raw words (42733 effective words) took 0.1s, 794744 effective words/s
    2020-09-30 21:08:56,583 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,585 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,587 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,587 : INFO : EPOCH - 18 : training on 58152 raw words (42703 effective words) took 0.1s, 813146 effective words/s
    2020-09-30 21:08:56,638 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,640 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,640 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,641 : INFO : EPOCH - 19 : training on 58152 raw words (42763 effective words) took 0.1s, 822300 effective words/s
    2020-09-30 21:08:56,696 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,700 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,700 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,700 : INFO : EPOCH - 20 : training on 58152 raw words (42649 effective words) took 0.1s, 733047 effective words/s
    2020-09-30 21:08:56,752 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,753 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,754 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,754 : INFO : EPOCH - 21 : training on 58152 raw words (42701 effective words) took 0.1s, 822006 effective words/s
    2020-09-30 21:08:56,803 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,805 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,805 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,805 : INFO : EPOCH - 22 : training on 58152 raw words (42714 effective words) took 0.1s, 848390 effective words/s
    2020-09-30 21:08:56,857 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,857 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,859 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,860 : INFO : EPOCH - 23 : training on 58152 raw words (42740 effective words) took 0.1s, 811758 effective words/s
    2020-09-30 21:08:56,907 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,909 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,910 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,910 : INFO : EPOCH - 24 : training on 58152 raw words (42754 effective words) took 0.0s, 873741 effective words/s
    2020-09-30 21:08:56,959 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:56,960 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:56,960 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:56,960 : INFO : EPOCH - 25 : training on 58152 raw words (42704 effective words) took 0.0s, 862291 effective words/s
    2020-09-30 21:08:57,009 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,010 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,011 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,011 : INFO : EPOCH - 26 : training on 58152 raw words (42741 effective words) took 0.0s, 868076 effective words/s
    2020-09-30 21:08:57,059 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,062 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,063 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,063 : INFO : EPOCH - 27 : training on 58152 raw words (42610 effective words) took 0.1s, 830699 effective words/s
    2020-09-30 21:08:57,112 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,114 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,115 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,116 : INFO : EPOCH - 28 : training on 58152 raw words (42747 effective words) took 0.1s, 835959 effective words/s
    2020-09-30 21:08:57,164 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,169 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,170 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,170 : INFO : EPOCH - 29 : training on 58152 raw words (42755 effective words) took 0.1s, 804348 effective words/s
    2020-09-30 21:08:57,219 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,222 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,224 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,224 : INFO : EPOCH - 30 : training on 58152 raw words (42760 effective words) took 0.1s, 808636 effective words/s
    2020-09-30 21:08:57,271 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,273 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,273 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,273 : INFO : EPOCH - 31 : training on 58152 raw words (42727 effective words) took 0.0s, 889118 effective words/s
    2020-09-30 21:08:57,323 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,326 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,327 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,327 : INFO : EPOCH - 32 : training on 58152 raw words (42786 effective words) took 0.1s, 819149 effective words/s
    2020-09-30 21:08:57,377 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,378 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,379 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,379 : INFO : EPOCH - 33 : training on 58152 raw words (42614 effective words) took 0.1s, 828217 effective words/s
    2020-09-30 21:08:57,427 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,430 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,431 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,431 : INFO : EPOCH - 34 : training on 58152 raw words (42757 effective words) took 0.1s, 848700 effective words/s
    2020-09-30 21:08:57,476 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,479 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,481 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,481 : INFO : EPOCH - 35 : training on 58152 raw words (42713 effective words) took 0.0s, 881912 effective words/s
    2020-09-30 21:08:57,530 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,530 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,532 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,532 : INFO : EPOCH - 36 : training on 58152 raw words (42632 effective words) took 0.1s, 843930 effective words/s
    2020-09-30 21:08:57,580 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,583 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,584 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,584 : INFO : EPOCH - 37 : training on 58152 raw words (42691 effective words) took 0.1s, 851268 effective words/s
    2020-09-30 21:08:57,632 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,634 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,635 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,635 : INFO : EPOCH - 38 : training on 58152 raw words (42667 effective words) took 0.1s, 850589 effective words/s
    2020-09-30 21:08:57,685 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,686 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,687 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,687 : INFO : EPOCH - 39 : training on 58152 raw words (42641 effective words) took 0.1s, 843857 effective words/s
    2020-09-30 21:08:57,736 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 21:08:57,737 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 21:08:57,741 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 21:08:57,741 : INFO : EPOCH - 40 : training on 58152 raw words (42721 effective words) took 0.1s, 807691 effective words/s
    2020-09-30 21:08:57,741 : INFO : training on a 2326080 raw words (1708575 effective words) took 2.2s, 781245 effective words/s




Now, we can use the trained model to infer a vector for any piece of text
by passing a list of words to the ``model.infer_vector`` function. This
vector can then be compared with other vectors via cosine similarity.



.. code-block:: default

    vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])
    print(vector)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [-0.08478509  0.05011684  0.0675064  -0.19926868 -0.1235586   0.01768214
     -0.12645927  0.01062329  0.06113973  0.35424358  0.01320948  0.07561274
     -0.01645093  0.0692549   0.08346193 -0.01599065  0.08287009 -0.0139379
     -0.17772709 -0.26271465  0.0442089  -0.04659882 -0.12873884  0.28799203
     -0.13040264  0.12478471 -0.14091878 -0.09698066 -0.07903259 -0.10124907
     -0.28239366  0.13270256  0.04445919 -0.24210942 -0.1907376  -0.07264525
     -0.14167067 -0.22816683 -0.00663796  0.23165748 -0.10436232 -0.01028251
     -0.04064698  0.08813146  0.01072008 -0.149789    0.05923386  0.16301566
      0.05815683  0.1258063 ]




Note that ``infer_vector()`` does *not* take a string, but rather a list of
string tokens, which should have already been tokenized the same way as the
``words`` property of original training document objects.

Also note that because the underlying training/inference algorithms are an
iterative approximation problem that makes use of internal randomization,
repeated inferences of the same text will return slightly different vectors.


Assessing the Model
-------------------

To assess our new model, we'll first infer new vectors for each document of
the training corpus, compare the inferred vectors with the training corpus,
and then returning the rank of the document based on self-similarity.
Basically, we're pretending as if the training corpus is some new unseen data
and then seeing how they compare with the trained model. The expectation is
that we've likely overfit our model (i.e., all of the ranks will be less than
2) and so we should be able to find similar documents very easily.
Additionally, we'll keep track of the second ranks for a comparison of less
similar documents.



.. code-block:: default

    ranks = []
    second_ranks = []
    for doc_id in range(len(train_corpus)):
        inferred_vector = model.infer_vector(train_corpus[doc_id].words)
        sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))
        rank = [docid for docid, sim in sims].index(doc_id)
        ranks.append(rank)

        second_ranks.append(sims[1])








Let's count how each document ranks with respect to the training corpus

NB. Results vary between runs due to random seeding and very small corpus


.. code-block:: default

    import collections

    counter = collections.Counter(ranks)
    print(counter)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Counter({0: 292, 1: 8})




Basically, greater than 95% of the inferred documents are found to be most
similar to itself and about 5% of the time it is mistakenly most similar to
another document. Checking the inferred-vector against a
training-vector is a sort of 'sanity check' as to whether the model is
behaving in a usefully consistent manner, though not a real 'accuracy' value.

This is great and not entirely surprising. We can take a look at an example:



.. code-block:: default

    print('Document ({}): ¬´{}¬ª\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
    for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
        print(u'%s %s: ¬´%s¬ª\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Document (299): ¬´australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well¬ª

    SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):

    MOST (299, 0.9482713341712952): ¬´australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well¬ª

    SECOND-MOST (104, 0.8029672503471375): ¬´australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said¬ª

    MEDIAN (238, 0.2635717988014221): ¬´centrelink is urging people affected by job cuts at regional pay tv operator austar and travel company traveland to seek information about their income support options traveland has announced it is shedding more than jobs around australia and austar is letting employees go centrelink finance information officer peter murray says those facing uncertain futures should head to centrelink in the next few days centrelink is the shopfront now for commonwealth services for income support and the employment network so that it is important if people haven been to us before they might get pleasant surprise at the range of services that we do offer to try and help them through situations where things might have changed for them mr murray said¬ª

    LEAST (243, -0.13247375190258026): ¬´four afghan factions have reached agreement on an interim cabinet during talks in germany the united nations says the administration which will take over from december will be headed by the royalist anti taliban commander hamed karzai it concludes more than week of negotiations outside bonn and is aimed at restoring peace and stability to the war ravaged country the year old former deputy foreign minister who is currently battling the taliban around the southern city of kandahar is an ally of the exiled afghan king mohammed zahir shah he will serve as chairman of an interim authority that will govern afghanistan for six month period before loya jirga or grand traditional assembly of elders in turn appoints an month transitional government meanwhile united states marines are now reported to have been deployed in eastern afghanistan where opposition forces are closing in on al qaeda soldiers reports from the area say there has been gun battle between the opposition and al qaeda close to the tora bora cave complex where osama bin laden is thought to be hiding in the south of the country american marines are taking part in patrols around the air base they have secured near kandahar but are unlikely to take part in any assault on the city however the chairman of the joint chiefs of staff general richard myers says they are prepared for anything they are prepared for engagements they re robust fighting force and they re absolutely ready to engage if that required he said¬ª





Notice above that the most similar document (usually the same text) is has a
similarity score approaching 1.0. However, the similarity score for the
second-ranked documents should be significantly lower (assuming the documents
are in fact different) and the reasoning becomes obvious when we examine the
text itself.

We can run the next cell repeatedly to see a sampling other target-document
comparisons.



.. code-block:: default


    # Pick a random document from the corpus and infer a vector from the model
    import random
    doc_id = random.randint(0, len(train_corpus) - 1)

    # Compare and print the second-most-similar document
    print('Train Document ({}): ¬´{}¬ª\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))
    sim_id = second_ranks[doc_id]
    print('Similar Document {}: ¬´{}¬ª\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Train Document (292): ¬´rival afghan factions are deadlocked over the shape of future government the northern alliance has demanded day adjournment of power sharing talks in germany after its president burhanuddin rabbani objected to the appointment system for an interim administration president rabbani has objected to the plans for an interim government to be drawn up by appointment as discussed in bonn saying the interim leaders should be voted in by afghans themselves he also says there is no real need for sizeable international security force president rabbani says he would prefer local afghan factions drew up their own internal security forces of around personnel but if the world insisted there should be an international security presence there should be no more than or personnel in their security forces he says president rabbani objections are likely to cast doubt on his delegation ability to commit the northern alliance to any course of action decided upon in bonn he now threatens to undermine the very process he claims to support in the quest for stable government in afghanistan¬ª

    Similar Document (13, 0.7867921590805054): ¬´talks between afghan and british officials in kabul have ended without final agreement on the deployment of international security force the lack of suitable translation of the document meant further delay authorities in kabul have been giving conflicting signals for weeks now over the number of peacekeepers they would allow and the role the international force would play the foreign minister dr abdullah appeared to be ending the confusion saying an agreement was about to be signed there is already the agreement so it was finalised he said but spokesman for the interior minister yunis kanooni emerged soon after to say there was no agreement and nothing to sign scores of british peacekeepers are already patrolling the streets of kabul in tandem with afghan police but proposals to enlarge the force to as many as international peacekeepers have been criticised by some commanders as tantamount to foreign occupation¬ª





Testing the Model
-----------------

Using the same approach above, we'll infer the vector for a randomly chosen
test document, and compare the document to our model by eye.



.. code-block:: default


    # Pick a random document from the test corpus and infer a vector from the model
    doc_id = random.randint(0, len(test_corpus) - 1)
    inferred_vector = model.infer_vector(test_corpus[doc_id])
    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))

    # Compare and print the most/median/least similar documents from the train corpus
    print('Test Document ({}): ¬´{}¬ª\n'.format(doc_id, ' '.join(test_corpus[doc_id])))
    print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
    for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
        print(u'%s %s: ¬´%s¬ª\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Test Document (49): ¬´labor needed to distinguish itself from the government on the issue of asylum seekers greens leader bob brown has said his senate colleague kerry nettle intends to move motion today on the first anniversary of the tampa crisis condemning the government over its refugee policy and calling for an end to mandatory detention we greens want to bring the government to book over its serial breach of international obligations as far as asylum seekers in this country are concerned senator brown said today¬ª

    SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):

    MOST (218, 0.8016394376754761): ¬´refugee support groups are strongly critical of federal government claims that the pacific solution program is working well the immigration minister philip ruddock says he is pleased with the program which uses pacific island nations to process asylum seekers wanting to come to australia president of the hazara ethnic society of australia hassan ghulam says the australian government is bullying smaller nations into accepting asylum seekers if the pacific countries wanted refugees they can clearly raise their voice in the united nations and say yes we are accepting refugees and why australia who gives this authority to the australian government to force the pacific countries to accept refugees in this form or in the other form he asked¬ª

    MEDIAN (204, 0.3319269120693207): ¬´an iraqi doctor being held at sydney villawood detention centre claims he was prevented from receiving human rights award dr aamer sultan had been awarded special commendation at yesterday human rights and equal opportunity commission awards in sydney but was not able to receive the honour in person dr sultan says he had been hoping to attend the ceremony but says the management at villawood stopped him from going submitted formal request to the centre manager who promised me that he will present the matter to migration management here who are the main authority here they also came back that unfortunately we can not fulfill this request for you but they didn give any explanation dr sultan says he was disappointed by the decision the immigration minister philip ruddock has written letter of complaint to the medical journal of australia about an article penned by dr sultan on the psychological state of detainees at villawood the journal has published research dr sultan conducted with former visiting psychologist to the centre kevin sullivan their survey of detainees over nine months found all but one displayed symptoms of psychological distress at some time the article says per cent acknowledged chronic depressive symptoms and close to half of the group had reached severe stages of depression¬ª

    LEAST (157, -0.10524928569793701): ¬´british man has been found guilty by unanimous verdict of the kidnap and murder of an eight year old schoolgirl whose death in july shocked britain and set off rampage of anti paedophile vigilantes roy whiting was sentenced to life imprisonment for the abduction and murder of eight year old sarah payne with recommendation by trial judge justice richard curtis that he never be released you are indeed an evil man you are in no way mentally unwell have seen you for month and in my view you are glib and cunning liar justice curtis said there were cheers of delight as the verdicts were read out by the foreman at lewes crown court the jury of nine men and three women had been deliberating for nine hours as soon as the verdicts were declared the court heard details of whiting previous conviction for the kidnap and indecent assault of nine year old girl in prosecutor timothy langdale told the jury how the defendant threw the child into the back of his dirty red ford sierra and locked the doors he had driven her somewhere she didn know where when she asked where they were going he said shut up because he had knife mr langdale said the defendant told the girl to take off her clothes when she refused he produced rope from his pocket and threatened to tie her up what he actually threatened was that he would tie her mouth up she took her clothes off as he had ordered her to do mr langdale then gave graphic details of the abuse to which whiting subjected the terrified child whiting was given four year jail sentence in june after admitting carrying out the attack in march that year but he was released in november despite warnings from probation officers who were convinced there was danger he would attack another child they set out their warnings in pre sentence report prepared after the first assault and in the parole report before he was released from prison he was kept under supervision for four months after his release but was not being monitored by july last year when eight year old sarah was abducted and killed whiting has been arrested three times in connection with the case but the first and second times was released without being charged sarah disappeared on july last year prompting massive police search her partially buried naked body was found days later in field and police believe she was strangled or suffocated¬ª





Conclusion
----------

Let's review what we've seen in this tutorial:

0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec
1. Load and preprocess the training and test corpora (see :ref:`core_concepts_corpus`)
2. Train a Doc2Vec :ref:`core_concepts_model` model using the training corpus
3. Demonstrate how the trained model can be used to infer a :ref:`core_concepts_vector`
4. Assess the model
5. Test the model on the test corpus

That's it! Doc2Vec is a great way to explore relationships between documents.

Additional Resources
--------------------

If you'd like to know more about the subject matter of this tutorial, check out the links below.

* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_
* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_
* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_
* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__
* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  7.863 seconds)

**Estimated memory usage:**  37 MB


.. _sphx_glr_download_auto_examples_tutorials_run_doc2vec_lee.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_doc2vec_lee.py <run_doc2vec_lee.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_doc2vec_lee.ipynb <run_doc2vec_lee.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_annoy.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_annoy.py:


Fast Similarity Queries with Annoy and Word2Vec
===============================================

Introduces the Annoy library for similarity queries on top of vectors learned by Word2Vec.


.. code-block:: default


    LOGS = False  # Set to True if you want to see progress in logs.
    if LOGS:
        import logging
        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








The `Annoy "Approximate Nearest Neighbors Oh Yeah"
<https://github.com/spotify/annoy>`_ library enables similarity queries with
a Word2Vec model.  The current implementation for finding k nearest neighbors
in a vector space in Gensim has linear complexity via brute force in the
number of indexed documents, although with extremely low constant factors.
The retrieved results are exact, which is an overkill in many applications:
approximate results retrieved in sub-linear time may be enough. Annoy can
find approximate nearest neighbors much faster.

Outline
-------

1. Download Text8 Corpus
2. Train the Word2Vec model
3. Construct AnnoyIndex with model & make a similarity query
4. Compare to the traditional indexer
5. Persist indices to disk
6. Save memory by via memory-mapping indices saved to disk
7. Evaluate relationship of ``num_trees`` to initialization time and accuracy
8. Work with Google's word2vec C formats


1. Download Text8 corpus
------------------------


.. code-block:: default

    import gensim.downloader as api
    text8_path = api.load('text8', return_path=True)
    print("Using corpus from", text8_path)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using corpus from /Users/kofola3/gensim-data/text8/text8.gz




2. Train the Word2Vec model
---------------------------

For more details, see :ref:`sphx_glr_auto_examples_tutorials_run_word2vec.py`.


.. code-block:: default

    from gensim.models import Word2Vec, KeyedVectors
    from gensim.models.word2vec import Text8Corpus

    # Using params from Word2Vec_FastText_Comparison
    params = {
        'alpha': 0.05,
        'vector_size': 100,
        'window': 5,
        'epochs': 5,
        'min_count': 5,
        'sample': 1e-4,
        'sg': 1,
        'hs': 0,
        'negative': 5,
    }
    model = Word2Vec(Text8Corpus(text8_path), **params)
    wv = model.wv
    print("Using trained model", wv)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using trained model <gensim.models.keyedvectors.KeyedVectors object at 0x2095fb0f0>




3. Construct AnnoyIndex with model & make a similarity query
------------------------------------------------------------

An instance of ``AnnoyIndexer`` needs to be created in order to use Annoy in Gensim.
The ``AnnoyIndexer`` class is located in ``gensim.similarities.annoy``.

``AnnoyIndexer()`` takes two parameters:

* **model**: A ``Word2Vec`` or ``Doc2Vec`` model.
* **num_trees**: A positive integer. ``num_trees`` effects the build
  time and the index size. **A larger value will give more accurate results,
  but larger indexes**. More information on what trees in Annoy do can be found
  `here <https://github.com/spotify/annoy#how-does-it-work>`__. The relationship
  between ``num_trees``\ , build time, and accuracy will be investigated later
  in the tutorial.

Now that we are ready to make a query, lets find the top 5 most similar words
to "science" in the Text8 corpus. To make a similarity query we call
``Word2Vec.most_similar`` like we would traditionally, but with an added
parameter, ``indexer``.

Apart from Annoy, Gensim also supports the NMSLIB indexer. NMSLIB is a similar library to
Annoy ‚Äì both support fast, approximate searches for similar vectors.



.. code-block:: default

    from gensim.similarities.annoy import AnnoyIndexer

    # 100 trees are being used in this example
    annoy_index = AnnoyIndexer(model, 100)
    # Derive the vector for the word "science" in our model
    vector = wv["science"]
    # The instance of AnnoyIndexer we just created is passed
    approximate_neighbors = wv.most_similar([vector], topn=11, indexer=annoy_index)
    # Neatly print the approximate_neighbors and their corresponding cosine similarity values
    print("Approximate Neighbors")
    for neighbor in approximate_neighbors:
        print(neighbor)

    normal_neighbors = wv.most_similar([vector], topn=11)
    print("\nExact Neighbors")
    for neighbor in normal_neighbors:
        print(neighbor)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Approximate Neighbors
    ('science', 1.0)
    ('fiction', 0.6577868759632111)
    ('crichton', 0.5896251797676086)
    ('interdisciplinary', 0.5887056291103363)
    ('astrobiology', 0.5863820314407349)
    ('multidisciplinary', 0.5813699960708618)
    ('protoscience', 0.5805026590824127)
    ('vinge', 0.5781905055046082)
    ('astronautics', 0.5768974423408508)
    ('aaas', 0.574912428855896)
    ('brookings', 0.5739299058914185)

    Exact Neighbors
    ('science', 1.0)
    ('fiction', 0.7657802700996399)
    ('crichton', 0.6631850600242615)
    ('interdisciplinary', 0.661673903465271)
    ('astrobiology', 0.6578403115272522)
    ('bimonthly', 0.6501255631446838)
    ('actuarial', 0.6495736837387085)
    ('multidisciplinary', 0.6494976878166199)
    ('protoscience', 0.6480439305305481)
    ('vinge', 0.6441534757614136)
    ('xenobiology', 0.6438207030296326)




The closer the cosine similarity of a vector is to 1, the more similar that
word is to our query, which was the vector for "science". There are some
differences in the ranking of similar words and the set of words included
within the 10 most similar words.

4. Compare to the traditional indexer
-------------------------------------


.. code-block:: default


    # Set up the model and vector that we are using in the comparison
    annoy_index = AnnoyIndexer(model, 100)

    # Dry run to make sure both indexes are fully in RAM
    normed_vectors = wv.get_normed_vectors()
    vector = normed_vectors[0]
    wv.most_similar([vector], topn=5, indexer=annoy_index)
    wv.most_similar([vector], topn=5)

    import time
    import numpy as np

    def avg_query_time(annoy_index=None, queries=1000):
        """Average query time of a most_similar method over 1000 random queries."""
        total_time = 0
        for _ in range(queries):
            rand_vec = normed_vectors[np.random.randint(0, len(wv))]
            start_time = time.process_time()
            wv.most_similar([rand_vec], topn=5, indexer=annoy_index)
            total_time += time.process_time() - start_time
        return total_time / queries

    queries = 1000

    gensim_time = avg_query_time(queries=queries)
    annoy_time = avg_query_time(annoy_index, queries=queries)
    print("Gensim (s/query):\t{0:.5f}".format(gensim_time))
    print("Annoy (s/query):\t{0:.5f}".format(annoy_time))
    speed_improvement = gensim_time / annoy_time
    print ("\nAnnoy is {0:.2f} times faster on average on this particular run".format(speed_improvement))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Gensim (s/query):       0.00585
    Annoy (s/query):        0.00052

    Annoy is 11.25 times faster on average on this particular run




**This speedup factor is by no means constant** and will vary greatly from
run to run and is particular to this data set, BLAS setup, Annoy
parameters(as tree size increases speedup factor decreases), machine
specifications, among other factors.

.. Important::
   Initialization time for the annoy indexer was not included in the times.
   The optimal knn algorithm for you to use will depend on how many queries
   you need to make and the size of the corpus. If you are making very few
   similarity queries, the time taken to initialize the annoy indexer will be
   longer than the time it would take the brute force method to retrieve
   results. If you are making many queries however, the time it takes to
   initialize the annoy indexer will be made up for by the incredibly fast
   retrieval times for queries once the indexer has been initialized

.. Important::
   Gensim's 'most_similar' method is using numpy operations in the form of
   dot product whereas Annoy's method isnt. If 'numpy' on your machine is
   using one of the BLAS libraries like ATLAS or LAPACK, it'll run on
   multiple cores (only if your machine has multicore support ). Check `SciPy
   Cookbook
   <http://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html>`_
   for more details.


5. Persisting indices to disk
-----------------------------

You can save and load your indexes from/to disk to prevent having to
construct them each time. This will create two files on disk, *fname* and
*fname.d*. Both files are needed to correctly restore all attributes. Before
loading an index, you will have to create an empty AnnoyIndexer object.



.. code-block:: default

    fname = '/tmp/mymodel.index'

    # Persist index to disk
    annoy_index.save(fname)

    # Load index back
    import os.path
    if os.path.exists(fname):
        annoy_index2 = AnnoyIndexer()
        annoy_index2.load(fname)
        annoy_index2.model = model

    # Results should be identical to above
    vector = wv["science"]
    approximate_neighbors2 = wv.most_similar([vector], topn=11, indexer=annoy_index2)
    for neighbor in approximate_neighbors2:
        print(neighbor)

    assert approximate_neighbors == approximate_neighbors2





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ('science', 1.0)
    ('fiction', 0.6577868759632111)
    ('crichton', 0.5896251797676086)
    ('interdisciplinary', 0.5887056291103363)
    ('astrobiology', 0.5863820314407349)
    ('multidisciplinary', 0.5813699960708618)
    ('protoscience', 0.5805026590824127)
    ('vinge', 0.5781905055046082)
    ('astronautics', 0.5768974423408508)
    ('aaas', 0.574912428855896)
    ('brookings', 0.5739299058914185)




Be sure to use the same model at load that was used originally, otherwise you
will get unexpected behaviors.


6. Save memory via memory-mapping indexes saved to disk
-------------------------------------------------------

Annoy library has a useful feature that indices can be memory-mapped from
disk. It saves memory when the same index is used by several processes.

Below are two snippets of code. First one has a separate index for each
process. The second snipped shares the index between two processes via
memory-mapping. The second example uses less total RAM as it is shared.



.. code-block:: default


    # Remove verbosity from code below (if logging active)
    if LOGS:
        logging.disable(logging.CRITICAL)

    from multiprocessing import Process
    import os
    import psutil








Bad example: two processes load the Word2vec model from disk and create their
own Annoy index from that model.



.. code-block:: default


    model.save('/tmp/mymodel.pkl')

    def f(process_id):
        print('Process Id: {}'.format(os.getpid()))
        process = psutil.Process(os.getpid())
        new_model = Word2Vec.load('/tmp/mymodel.pkl')
        vector = new_model.wv["science"]
        annoy_index = AnnoyIndexer(new_model, 100)
        approximate_neighbors = new_model.wv.most_similar([vector], topn=5, indexer=annoy_index)
        print('\nMemory used by process {}: {}\n---'.format(os.getpid(), process.memory_info()))

    # Create and run two parallel processes to share the same index file.
    p1 = Process(target=f, args=('1',))
    p1.start()
    p1.join()
    p2 = Process(target=f, args=('2',))
    p2.start()
    p2.join()








Good example: two processes load both the Word2vec model and index from disk
and memory-map the index.



.. code-block:: default


    model.save('/tmp/mymodel.pkl')

    def f(process_id):
        print('Process Id: {}'.format(os.getpid()))
        process = psutil.Process(os.getpid())
        new_model = Word2Vec.load('/tmp/mymodel.pkl')
        vector = new_model.wv["science"]
        annoy_index = AnnoyIndexer()
        annoy_index.load('/tmp/mymodel.index')
        annoy_index.model = new_model
        approximate_neighbors = new_model.wv.most_similar([vector], topn=5, indexer=annoy_index)
        print('\nMemory used by process {}: {}\n---'.format(os.getpid(), process.memory_info()))

    # Creating and running two parallel process to share the same index file.
    p1 = Process(target=f, args=('1',))
    p1.start()
    p1.join()
    p2 = Process(target=f, args=('2',))
    p2.start()
    p2.join()








7. Evaluate relationship of ``num_trees`` to initialization time and accuracy
-----------------------------------------------------------------------------



.. code-block:: default

    import matplotlib.pyplot as plt








Build dataset of initialization times and accuracy measures:



.. code-block:: default


    exact_results = [element[0] for element in wv.most_similar([normed_vectors[0]], topn=100)]

    x_values = []
    y_values_init = []
    y_values_accuracy = []

    for x in range(1, 300, 10):
        x_values.append(x)
        start_time = time.time()
        annoy_index = AnnoyIndexer(model, x)
        y_values_init.append(time.time() - start_time)
        approximate_results = wv.most_similar([normed_vectors[0]], topn=100, indexer=annoy_index)
        top_words = [result[0] for result in approximate_results]
        y_values_accuracy.append(len(set(top_words).intersection(exact_results)))








Plot results:


.. code-block:: default


    plt.figure(1, figsize=(12, 6))
    plt.subplot(121)
    plt.plot(x_values, y_values_init)
    plt.title("num_trees vs initalization time")
    plt.ylabel("Initialization time (s)")
    plt.xlabel("num_trees")
    plt.subplot(122)
    plt.plot(x_values, y_values_accuracy)
    plt.title("num_trees vs accuracy")
    plt.ylabel("%% accuracy")
    plt.xlabel("num_trees")
    plt.tight_layout()
    plt.show()




.. image:: /auto_examples/tutorials/images/sphx_glr_run_annoy_001.png
    :alt: num_trees vs initalization time, num_trees vs accuracy
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /Volumes/work/workspace/vew/gensim3.6/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning:

    Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.





From the above, we can see that the initialization time of the annoy indexer
increases in a linear fashion with num_trees. Initialization time will vary
from corpus to corpus. In the graph above we used the (tiny) Lee corpus.

Furthermore, in this dataset, the accuracy seems logarithmically related to
the number of trees. We see an improvement in accuracy with more trees, but
the relationship is nonlinear.


7. Work with Google's word2vec files
------------------------------------

Our model can be exported to a word2vec C format. There is a binary and a
plain text word2vec format. Both can be read with a variety of other
software, or imported back into Gensim as a ``KeyedVectors`` object.



.. code-block:: default


    # To export our model as text
    wv.save_word2vec_format('/tmp/vectors.txt', binary=False)

    from smart_open import open
    # View the first 3 lines of the exported file
    # The first line has the total number of entries and the vector dimension count.
    # The next lines have a key (a string) followed by its vector.
    with open('/tmp/vectors.txt', encoding='utf8') as myfile:
        for i in range(3):
            print(myfile.readline().strip())

    # To import a word2vec text model
    wv = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)

    # To export a model as binary
    wv.save_word2vec_format('/tmp/vectors.bin', binary=True)

    # To import a word2vec binary model
    wv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)

    # To create and save Annoy Index from a loaded `KeyedVectors` object (with 100 trees)
    annoy_index = AnnoyIndexer(wv, 100)
    annoy_index.save('/tmp/mymodel.index')

    # Load and test the saved word vectors and saved Annoy index
    wv = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)
    annoy_index = AnnoyIndexer()
    annoy_index.load('/tmp/mymodel.index')
    annoy_index.model = wv

    vector = wv["cat"]
    approximate_neighbors = wv.most_similar([vector], topn=11, indexer=annoy_index)
    # Neatly print the approximate_neighbors and their corresponding cosine similarity values
    print("Approximate Neighbors")
    for neighbor in approximate_neighbors:
        print(neighbor)

    normal_neighbors = wv.most_similar([vector], topn=11)
    print("\nExact Neighbors")
    for neighbor in normal_neighbors:
        print(neighbor)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    71290 100
    the 0.1645237 0.049031682 -0.11330697 0.097082675 -0.099474825 -0.08294691 0.007256336 -0.113704175 0.24664731 -0.062123552 -0.024763709 0.25688595 0.059356388 0.28822595 0.18409002 0.17533085 0.12412363 0.05312752 -0.10347493 0.07136696 0.050333817 0.03533254 0.07569087 -0.41796425 -0.13256022 0.30041444 0.26416314 -0.022389138 -0.20686609 -0.21565206 -0.25032488 -0.12548248 0.077188216 0.2432488 -0.1458781 -0.23084323 -0.13360116 -0.01887776 0.21207437 -0.0022163654 0.047225904 0.18978342 0.19625767 -0.02934954 0.005101277 0.11670754 0.11398655 0.33111402 -0.037173223 0.21018152 -0.07217948 -0.0045775156 -0.18228853 -0.065637104 0.16755614 0.20857134 0.1822439 -0.17496146 0.034775164 0.09327986 -0.011131699 -0.009912837 -0.18504283 -0.0043261787 0.03363841 -0.054994233 0.18313456 -0.22603175 0.15427239 0.22330661 0.026417818 0.09543534 0.09841086 -0.41345838 0.14082615 0.13712159 0.070771925 0.06285282 5.9063022e-05 -0.15651035 -0.016906142 0.14885448 0.07121329 -0.23360902 -0.09033932 -0.11270273 -0.0059097605 -0.04875052 -0.04409246 0.103411175 0.00074150774 -0.08402691 -0.07324047 -0.20355953 -0.091564305 -0.11138651 -0.18119322 0.21025972 -0.06939676 0.0016936468
    of 0.19971648 0.15359728 -0.1338489 0.12083505 -0.005847811 -0.085402876 -0.075938866 -0.13501053 0.18837708 -0.1259633 0.110350266 0.108376145 0.015276252 0.33608598 0.22733492 0.11238891 -0.053862635 0.073887356 -0.20558539 -0.099394076 -0.0069137346 -0.114128046 0.027444497 -0.35551408 0.007910002 0.23189865 0.2650087 0.03700684 -0.17699398 -0.35950723 -0.32789174 -0.30379272 0.02704152 0.21078588 -0.023837725 -0.21654394 -0.166978 -0.08431874 0.2691367 -0.0023258273 0.06707064 0.09761329 0.24171327 -0.093486875 0.12232643 0.096265465 0.12889618 0.17138048 0.015292533 0.013243989 -0.09338309 0.0905355 -0.26343557 -0.2523928 0.07358186 0.17042407 0.266381 -0.218722 0.059136674 -0.00048657134 -0.0690399 -0.03615013 -0.059233107 -0.066501416 0.04838442 -0.11165278 0.09096755 -0.18076046 0.20482069 0.34460145 0.03740757 0.019260708 0.03930956 -0.37160733 -0.10296658 0.075969525 0.09362528 0.04970148 -0.07688446 -0.12854671 -0.10089095 0.01764436 0.1420408 -0.17590913 -0.20053966 0.14636976 -0.18029185 -0.081263 -0.048385028 0.26456535 -0.055859976 -0.08821882 -0.15724823 -0.17458497 0.010780472 -0.13346615 -0.12641737 0.16775236 -0.20294443 -0.115340725
    Approximate Neighbors
    ('cat', 1.0)
    ('cats', 0.5968745350837708)
    ('meow', 0.5941576957702637)
    ('leopardus', 0.5938971042633057)
    ('prionailurus', 0.5928952395915985)
    ('felis', 0.5831491053104401)
    ('saimiri', 0.5817937552928925)
    ('rabbits', 0.5794903337955475)
    ('caracal', 0.5760406851768494)
    ('sighthound', 0.5754748582839966)
    ('oncifelis', 0.5718523561954498)

    Exact Neighbors
    ('cat', 1.0000001192092896)
    ('cats', 0.6749798059463501)
    ('meow', 0.6705840826034546)
    ('leopardus', 0.6701608896255493)
    ('prionailurus', 0.6685314774513245)
    ('felis', 0.6524706482887268)
    ('saimiri', 0.6502071619033813)
    ('rabbits', 0.6463432312011719)
    ('purr', 0.6449686288833618)
    ('caracal', 0.640516996383667)
    ('sighthound', 0.639556884765625)




Recap
-----

In this notebook we used the Annoy module to build an indexed approximation
of our word embeddings. To do so, we did the following steps:

1. Download Text8 Corpus
2. Train Word2Vec Model
3. Construct AnnoyIndex with model & make a similarity query
4. Persist indices to disk
5. Save memory by via memory-mapping indices saved to disk
6. Evaluate relationship of ``num_trees`` to initialization time and accuracy
7. Work with Google's word2vec C formats



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 13 minutes  35.756 seconds)

**Estimated memory usage:**  693 MB


.. _sphx_glr_download_auto_examples_tutorials_run_annoy.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_annoy.py <run_annoy.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_annoy.ipynb <run_annoy.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_tutorials_run_word2vec.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_tutorials_run_word2vec.py:


Word2Vec Model
==============

Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








In case you missed the buzz, Word2Vec is a widely used algorithm based on neural
networks, commonly referred to as "deep learning" (though word2vec itself is rather shallow).
Using large amounts of unannotated plain text, word2vec learns relationships
between words automatically. The output are vectors, one vector per word,
with remarkable linear relationships that allow us to do things like:

* vec("king") - vec("man") + vec("woman") =~ vec("queen")
* vec("Montreal Canadiens") ‚Äì vec("Montreal") + vec("Toronto") =~ vec("Toronto Maple Leafs").

Word2vec is very useful in `automatic text tagging
<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\ , recommender
systems and machine translation.

This tutorial:

#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words
#. Shows off a demo of ``Word2Vec`` using a pre-trained model
#. Demonstrates training a new model from your own data
#. Demonstrates loading and saving models
#. Introduces several training parameters and demonstrates their effect
#. Discusses memory requirements
#. Visualizes Word2Vec embeddings by applying dimensionality reduction

Review: Bag-of-words
--------------------

.. Note:: Feel free to skip these review sections if you're already familiar with the models.

You may be familiar with the `bag-of-words model
<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the
:ref:`core_concepts_vector` section.
This model transforms each document to a fixed-length vector of integers.
For example, given the sentences:

- ``John likes to watch movies. Mary likes movies too.``
- ``John also likes to watch football games. Mary hates football.``

The model outputs the vectors:

- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``
- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``

Each vector has 10 elements, where each element counts the number of times a
particular word occurred in the document.
The order of elements is arbitrary.
In the example above, the order of the elements corresponds to the words:
``["John", "likes", "to", "watch", "movies", "Mary", "too", "also", "football", "games", "hates"]``.

Bag-of-words models are surprisingly effective, but have several weaknesses.

First, they lose all information about word order: "John likes Mary" and
"Mary likes John" correspond to identical vectors. There is a solution: bag
of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__
models consider word phrases of length n to represent documents as
fixed-length vectors to capture local word order but suffer from data
sparsity and high dimensionality.

Second, the model does not attempt to learn the meaning of the underlying
words, and as a consequence, the distance between vectors doesn't always
reflect the difference in meaning.  The ``Word2Vec`` model addresses this
second problem.

Introducing: the ``Word2Vec`` Model
-----------------------------------

``Word2Vec`` is a more recent model that embeds words in a lower-dimensional
vector space using a shallow neural network. The result is a set of
word-vectors where vectors close together in vector space have similar
meanings based on context, and word-vectors distant to each other have
differing meanings. For example, ``strong`` and ``powerful`` would be close
together and ``strong`` and ``Paris`` would be relatively far.

The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`
class implements them both:

1. Skip-grams (SG)
2. Continuous-bag-of-words (CBOW)

.. Important::
  Don't let the implementation details below scare you.
  They're advanced material: if it's too much, then move on to the next section.

The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__
model, for example, takes in pairs (word1, word2) generated by moving a
window across text data, and trains a 1-hidden-layer neural network based on
the synthetic task of given an input word, giving us a predicted probability
distribution of nearby words to the input. A virtual `one-hot
<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words
goes through a 'projection layer' to the hidden layer; these projection
weights are later interpreted as the word embeddings. So if the hidden layer
has 300 neurons, this network will give us 300-dimensional word embeddings.

Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It
is also a 1-hidden-layer neural network. The synthetic training task now uses
the average of multiple input context words, rather than a single word as in
skip-gram, to predict the center word. Again, the projection weights that
turn one-hot words into averageable vectors, of the same width as the hidden
layer, are interpreted as the word embeddings.


Word2Vec Demo
-------------

To see what ``Word2Vec`` can do, let's download a pre-trained model and play
around with it. We will fetch the Word2Vec model trained on part of the
Google News dataset, covering approximately 3 million words and phrases. Such
a model can take hours to train, but since it's already available,
downloading and loading it with Gensim takes minutes.

.. Important::
  The model is approximately 2GB, so you'll need a decent network connection
  to proceed.  Otherwise, skip ahead to the "Training Your Own Model" section
  below.

You may also check out an `online word2vec demo
<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try
this vector algebra for yourself. That demo runs ``word2vec`` on the
**entire** Google News dataset, of **about 100 billion words**.



.. code-block:: default

    import gensim.downloader as api
    wv = api.load('word2vec-google-news-300')








A common operation is to retrieve the vocabulary of a model. That is trivial:


.. code-block:: default

    for index, word in enumerate(wv.index_to_key):
        if index == 10:
            break
        print(f"word #{index}/{len(wv.index_to_key)} is {word}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    word #0/3000000 is </s>
    word #1/3000000 is in
    word #2/3000000 is for
    word #3/3000000 is that
    word #4/3000000 is is
    word #5/3000000 is on
    word #6/3000000 is ##
    word #7/3000000 is The
    word #8/3000000 is with
    word #9/3000000 is said




We can easily obtain vectors for terms the model is familiar with:



.. code-block:: default

    vec_king = wv['king']








Unfortunately, the model is unable to infer vectors for unfamiliar words.
This is one limitation of Word2Vec: if this limitation matters to you, check
out the FastText model.



.. code-block:: default

    try:
        vec_cameroon = wv['cameroon']
    except KeyError:
        print("The word 'cameroon' does not appear in this model")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    The word 'cameroon' does not appear in this model




Moving on, ``Word2Vec`` supports several word similarity tasks out of the
box.  You can see how the similarity intuitively decreases as the words get
less and less similar.



.. code-block:: default

    pairs = [
        ('car', 'minivan'),   # a minivan is a kind of car
        ('car', 'bicycle'),   # still a wheeled vehicle
        ('car', 'airplane'),  # ok, no wheels, but still a vehicle
        ('car', 'cereal'),    # ... and so on
        ('car', 'communism'),
    ]
    for w1, w2 in pairs:
        print('%r\t%r\t%.2f' % (w1, w2, wv.similarity(w1, w2)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    'car'   'minivan'       0.69
    'car'   'bicycle'       0.54
    'car'   'airplane'      0.42
    'car'   'cereal'        0.14
    'car'   'communism'     0.06




Print the 5 most similar words to "car" or "minivan"


.. code-block:: default

    print(wv.most_similar(positive=['car', 'minivan'], topn=5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]




Which of the below does not belong in the sequence?


.. code-block:: default

    print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    car




Training Your Own Model
-----------------------

To start, you'll need some data for training the model. For the following
examples, we'll use the `Lee Evaluation Corpus
<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_
(which you `already have
<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_
if you've installed Gensim).

This corpus is small enough to fit entirely in memory, but we'll implement a
memory-friendly iterator that reads it line-by-line to demonstrate how you
would handle a larger corpus.



.. code-block:: default


    from gensim.test.utils import datapath
    from gensim import utils

    class MyCorpus:
        """An iterator that yields sentences (lists of str)."""

        def __iter__(self):
            corpus_path = datapath('lee_background.cor')
            for line in open(corpus_path):
                # assume there's one document per line, tokens separated by whitespace
                yield utils.simple_preprocess(line)








If we wanted to do any custom preprocessing, e.g. decode a non-standard
encoding, lowercase, remove numbers, extract named entities... All of this can
be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn‚Äôt need to
know. All that is required is that the input yields one sentence (list of
utf8 words) after another.

Let's go ahead and train a model on our corpus.  Don't worry about the
training parameters much for now, we'll revisit them later.



.. code-block:: default

    import gensim.models

    sentences = MyCorpus()
    model = gensim.models.Word2Vec(sentences=sentences)








Once we have our model, we can use it in the same way as in the demo above.

The main part of the model is ``model.wv``\ , where "wv" stands for "word vectors".



.. code-block:: default

    vec_king = model.wv['king']








Retrieving the vocabulary works the same way:


.. code-block:: default

    for index, word in enumerate(wv.index_to_key):
        if index == 10:
            break
        print(f"word #{index}/{len(wv.index_to_key)} is {word}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    word #0/3000000 is </s>
    word #1/3000000 is in
    word #2/3000000 is for
    word #3/3000000 is that
    word #4/3000000 is is
    word #5/3000000 is on
    word #6/3000000 is ##
    word #7/3000000 is The
    word #8/3000000 is with
    word #9/3000000 is said




Storing and loading models
--------------------------

You'll notice that training non-trivial models can take time.  Once you've
trained your model and it works as expected, you can save it to disk.  That
way, you don't have to spend time training it all over again later.

You can store/load models using the standard gensim methods:



.. code-block:: default

    import tempfile

    with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:
        temporary_filepath = tmp.name
        model.save(temporary_filepath)
        #
        # The model is now safely stored in the filepath.
        # You can copy it to other machines, share it with others, etc.
        #
        # To load a saved model:
        #
        new_model = gensim.models.Word2Vec.load(temporary_filepath)








which uses pickle internally, optionally ``mmap``\ ‚Äòing the model‚Äôs internal
large NumPy matrices into virtual memory directly from disk files, for
inter-process memory sharing.

In addition, you can load models created by the original C tool, both using
its text and binary formats::

  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)
  # using gzipped/bz2 input works too, no need to unzip
  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)


Training Parameters
-------------------

``Word2Vec`` accepts several parameters that affect both training speed and quality.

min_count
---------

``min_count`` is for pruning the internal dictionary. Words that appear only
once or twice in a billion-word corpus are probably uninteresting typos and
garbage. In addition, there‚Äôs not enough data to make any meaningful training
on those words, so it‚Äôs best to ignore them:

default value of min_count=5


.. code-block:: default

    model = gensim.models.Word2Vec(sentences, min_count=10)








vector_size
-----------

``vector_size`` is the number of dimensions (N) of the N-dimensional space that
gensim Word2Vec maps the words onto.

Bigger size values require more training data, but can lead to better (more
accurate) models. Reasonable values are in the tens to hundreds.



.. code-block:: default


    # The default value of vector_size is 100.
    model = gensim.models.Word2Vec(sentences, vector_size=200)








workers
-------

``workers`` , the last of the major parameters (full list `here
<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)
is for training parallelization, to speed up training:



.. code-block:: default


    # default value of workers=3 (tutorial says 1...)
    model = gensim.models.Word2Vec(sentences, workers=4)








The ``workers`` parameter only has an effect if you have `Cython
<http://cython.org/>`_ installed. Without Cython, you‚Äôll only be able to use
one core because of the `GIL
<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``
training will be `miserably slow
<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\ ).


Memory
------

At its core, ``word2vec`` model parameters are stored as matrices (NumPy
arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)
times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).

Three such matrices are held in RAM (work is underway to reduce that number
to two, or even one). So if your input contains 100,000 unique words, and you
asked for layer ``vector_size=200``\ , the model will require approx.
``100,000*200*4*3 bytes = ~229MB``.

There‚Äôs a little extra memory needed for storing the vocabulary tree (100,000 words would
take a few megabytes), but unless your words are extremely loooong strings, memory
footprint will be dominated by the three matrices above.


Evaluating
----------

``Word2Vec`` training is an unsupervised task, there‚Äôs no good way to
objectively evaluate the result. Evaluation depends on your end application.

Google has released their testing set of about 20,000 syntactic and semantic
test examples, following the ‚ÄúA is to B as C is to D‚Äù task. It is provided in
the 'datasets' folder.

For example a syntactic analogy of comparative type is ``bad:worse;good:?``.
There are total of 9 types of syntactic comparisons in the dataset like
plural nouns and nouns of opposite meaning.

The semantic questions contain five types of semantic analogies, such as
capital cities (``Paris:France;Tokyo:?``) or family members
(``brother:sister;dad:?``).


Gensim supports the same evaluation set, in exactly the same format:



.. code-block:: default

    model.wv.evaluate_word_analogies(datapath('questions-words.txt'))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (0.0, [{'section': 'capital-common-countries', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]}, {'section': 'capital-world', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]}, {'section': 'currency', 'correct': [], 'incorrect': []}, {'section': 'city-in-state', 'correct': [], 'incorrect': []}, {'section': 'family', 'correct': [], 'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER')]}, {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []}, {'section': 'gram2-opposite', 'correct': [], 'incorrect': []}, {'section': 'gram3-comparative', 'correct': [], 'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER')]}, {'section': 'gram4-superlative', 'correct': [], 'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]}, {'section': 'gram5-present-participle', 'correct': [], 'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING')]}, {'section': 'gram6-nationality-adjective', 'correct': [], 'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]}, {'section': 'gram7-past-tense', 'correct': [], 'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID')]}, {'section': 'gram8-plural', 'correct': [], 'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}, {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []}, {'section': 'Total accuracy', 'correct': [], 'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'), ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'), ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'), ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'), ('HE', 'SHE', 'HIS', 'HER'), ('HE', 'SHE', 'MAN', 'WOMAN'), ('HIS', 'HER', 'MAN', 'WOMAN'), ('HIS', 'HER', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HE', 'SHE'), ('MAN', 'WOMAN', 'HIS', 'HER'), ('GOOD', 'BETTER', 'GREAT', 'GREATER'), ('GOOD', 'BETTER', 'LONG', 'LONGER'), ('GOOD', 'BETTER', 'LOW', 'LOWER'), ('GOOD', 'BETTER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'LONG', 'LONGER'), ('GREAT', 'GREATER', 'LOW', 'LOWER'), ('GREAT', 'GREATER', 'SMALL', 'SMALLER'), ('GREAT', 'GREATER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'LOW', 'LOWER'), ('LONG', 'LONGER', 'SMALL', 'SMALLER'), ('LONG', 'LONGER', 'GOOD', 'BETTER'), ('LONG', 'LONGER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'SMALL', 'SMALLER'), ('LOW', 'LOWER', 'GOOD', 'BETTER'), ('LOW', 'LOWER', 'GREAT', 'GREATER'), ('LOW', 'LOWER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'GOOD', 'BETTER'), ('SMALL', 'SMALLER', 'GREAT', 'GREATER'), ('SMALL', 'SMALLER', 'LONG', 'LONGER'), ('SMALL', 'SMALLER', 'LOW', 'LOWER'), ('BIG', 'BIGGEST', 'GOOD', 'BEST'), ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'), ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'GREAT', 'GREATEST'), ('GOOD', 'BEST', 'LARGE', 'LARGEST'), ('GOOD', 'BEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'), ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'), ('GREAT', 'GREATEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'), ('LARGE', 'LARGEST', 'GOOD', 'BEST'), ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'), ('GO', 'GOING', 'LOOK', 'LOOKING'), ('GO', 'GOING', 'PLAY', 'PLAYING'), ('GO', 'GOING', 'RUN', 'RUNNING'), ('GO', 'GOING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'), ('LOOK', 'LOOKING', 'RUN', 'RUNNING'), ('LOOK', 'LOOKING', 'SAY', 'SAYING'), ('LOOK', 'LOOKING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'RUN', 'RUNNING'), ('PLAY', 'PLAYING', 'SAY', 'SAYING'), ('PLAY', 'PLAYING', 'GO', 'GOING'), ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'SAY', 'SAYING'), ('RUN', 'RUNNING', 'GO', 'GOING'), ('RUN', 'RUNNING', 'LOOK', 'LOOKING'), ('RUN', 'RUNNING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'GO', 'GOING'), ('SAY', 'SAYING', 'LOOK', 'LOOKING'), ('SAY', 'SAYING', 'PLAY', 'PLAYING'), ('SAY', 'SAYING', 'RUN', 'RUNNING'), ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'), ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'), ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'), ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'), ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'), ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'), ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'), ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'), ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'), ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'), ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'), ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'), ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'), ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'), ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'), ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'), ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'), ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'), ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'), ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'), ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'), ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'), ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'), ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'), ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'), ('GOING', 'WENT', 'PAYING', 'PAID'), ('GOING', 'WENT', 'PLAYING', 'PLAYED'), ('GOING', 'WENT', 'SAYING', 'SAID'), ('GOING', 'WENT', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'PLAYING', 'PLAYED'), ('PAYING', 'PAID', 'SAYING', 'SAID'), ('PAYING', 'PAID', 'TAKING', 'TOOK'), ('PAYING', 'PAID', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'SAYING', 'SAID'), ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'), ('PLAYING', 'PLAYED', 'GOING', 'WENT'), ('PLAYING', 'PLAYED', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'TAKING', 'TOOK'), ('SAYING', 'SAID', 'GOING', 'WENT'), ('SAYING', 'SAID', 'PAYING', 'PAID'), ('SAYING', 'SAID', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'GOING', 'WENT'), ('TAKING', 'TOOK', 'PAYING', 'PAID'), ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'), ('TAKING', 'TOOK', 'SAYING', 'SAID'), ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'), ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'), ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'), ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'), ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'CHILD', 'CHILDREN'), ('CAR', 'CARS', 'MAN', 'MEN'), ('CAR', 'CARS', 'ROAD', 'ROADS'), ('CAR', 'CARS', 'WOMAN', 'WOMEN'), ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'MAN', 'MEN'), ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'), ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'), ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'), ('CHILD', 'CHILDREN', 'CAR', 'CARS'), ('MAN', 'MEN', 'ROAD', 'ROADS'), ('MAN', 'MEN', 'WOMAN', 'WOMEN'), ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'), ('MAN', 'MEN', 'CAR', 'CARS'), ('MAN', 'MEN', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'), ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'), ('ROAD', 'ROADS', 'CAR', 'CARS'), ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'), ('ROAD', 'ROADS', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'), ('WOMAN', 'WOMEN', 'CAR', 'CARS'), ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'), ('WOMAN', 'WOMEN', 'MAN', 'MEN'), ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])



This ``evaluate_word_analogies`` method takes an `optional parameter
<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_
``restrict_vocab`` which limits which test examples are to be considered.


In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.

By default it uses an academic dataset WS-353 but one can create a dataset
specific to your business based on it. It contains word pairs together with
human-assigned similarity judgments. It measures the relatedness or
co-occurrence of two words. For example, 'coast' and 'shore' are very similar
as they appear in the same context. At the same time 'clothes' and 'closet'
are less similar because they are related but not interchangeable.



.. code-block:: default

    model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ((0.1014236962315867, 0.44065378924434523), SpearmanrResult(correlation=0.07441989763914543, pvalue=0.5719973648460552), 83.0028328611898)



.. Important::
  Good performance on Google's or WS-353 test set doesn‚Äôt mean word2vec will
  work well in your application, or vice versa. It‚Äôs always best to evaluate
  directly on your intended task. For an example of how to use word2vec in a
  classifier pipeline, see this `tutorial
  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.


Online training / Resuming training
-----------------------------------

Advanced users can load a model and continue training it with more sentences
and `new vocabulary words <online_w2v_tutorial.ipynb>`_:



.. code-block:: default

    model = gensim.models.Word2Vec.load(temporary_filepath)
    more_sentences = [
        ['Advanced', 'users', 'can', 'load', 'a', 'model',
         'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],
    ]
    model.build_vocab(more_sentences, update=True)
    model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)

    # cleaning up temporary file
    import os
    os.remove(temporary_filepath)








You may need to tweak the ``total_words`` parameter to ``train()``,
depending on what learning rate decay you want to simulate.

Note that it‚Äôs not possible to resume training with models generated by the C
tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for
querying/similarity, but information vital for training (the vocab tree) is
missing there.


Training Loss Computation
-------------------------

The parameter ``compute_loss`` can be used to toggle computation of loss
while training the Word2Vec model. The computed loss is stored in the model
attribute ``running_training_loss`` and can be retrieved using the function
``get_latest_training_loss`` as follows :



.. code-block:: default


    # instantiating and training the Word2Vec model
    model_with_loss = gensim.models.Word2Vec(
        sentences,
        min_count=1,
        compute_loss=True,
        hs=0,
        sg=1,
        seed=42,
    )

    # getting the training loss value
    training_loss = model_with_loss.get_latest_training_loss()
    print(training_loss)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    1369454.25




Benchmarks
----------

Let's run some benchmarks to see effect of the training loss computation code
on training time.

We'll use the following data for the benchmarks:

#. Lee Background corpus: included in gensim's test data
#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the
   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.



.. code-block:: default


    import io
    import os

    import gensim.models.word2vec
    import gensim.downloader as api
    import smart_open


    def head(path, size):
        with smart_open.open(path) as fin:
            return io.StringIO(fin.read(size))


    def generate_input_data():
        lee_path = datapath('lee_background.cor')
        ls = gensim.models.word2vec.LineSentence(lee_path)
        ls.name = '25kB'
        yield ls

        text8_path = api.load('text8').fn
        labels = ('1MB', '10MB', '50MB', '100MB')
        sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)
        for l, s in zip(labels, sizes):
            ls = gensim.models.word2vec.LineSentence(head(text8_path, s))
            ls.name = l
            yield ls


    input_data = list(generate_input_data())








We now compare the training time taken for different combinations of input
data and model training parameters like ``hs`` and ``sg``.

For each combination, we repeat the test several times to obtain the mean and
standard deviation of the test duration.



.. code-block:: default


    # Temporarily reduce logging verbosity
    logging.root.level = logging.ERROR

    import time
    import numpy as np
    import pandas as pd

    train_time_values = []
    seed_val = 42
    sg_values = [0, 1]
    hs_values = [0, 1]

    fast = True
    if fast:
        input_data_subset = input_data[:3]
    else:
        input_data_subset = input_data


    for data in input_data_subset:
        for sg_val in sg_values:
            for hs_val in hs_values:
                for loss_flag in [True, False]:
                    time_taken_list = []
                    for i in range(3):
                        start_time = time.time()
                        w2v_model = gensim.models.Word2Vec(
                            data,
                            compute_loss=loss_flag,
                            sg=sg_val,
                            hs=hs_val,
                            seed=seed_val,
                        )
                        time_taken_list.append(time.time() - start_time)

                    time_taken_list = np.array(time_taken_list)
                    time_mean = np.mean(time_taken_list)
                    time_std = np.std(time_taken_list)

                    model_result = {
                        'train_data': data.name,
                        'compute_loss': loss_flag,
                        'sg': sg_val,
                        'hs': hs_val,
                        'train_time_mean': time_mean,
                        'train_time_std': time_std,
                    }
                    print("Word2vec model #%i: %s" % (len(train_time_values), model_result))
                    train_time_values.append(model_result)

    train_times_table = pd.DataFrame(train_time_values)
    train_times_table = train_times_table.sort_values(
        by=['train_data', 'sg', 'hs', 'compute_loss'],
        ascending=[False, False, True, False],
    )
    print(train_times_table)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Word2vec model #0: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.25217413902282715, 'train_time_std': 0.020226552024939795}
    Word2vec model #1: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.25898512204488117, 'train_time_std': 0.026276375796854143}
    Word2vec model #2: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.4194076855977376, 'train_time_std': 0.0021983060310549808}
    Word2vec model #3: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.4308760166168213, 'train_time_std': 0.0009999532723555815}
    Word2vec model #4: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 0.47211599349975586, 'train_time_std': 0.015136686417800442}
    Word2vec model #5: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 0.4695216814676921, 'train_time_std': 0.0033446725418043747}
    Word2vec model #6: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 0.9502590497334799, 'train_time_std': 0.005153258425238986}
    Word2vec model #7: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 0.9424160321553549, 'train_time_std': 0.009776048211734903}
    Word2vec model #8: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.6441135406494141, 'train_time_std': 0.00934594899599891}
    Word2vec model #9: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.656217098236084, 'train_time_std': 0.02703627277086478}
    Word2vec model #10: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 1.3150715033213298, 'train_time_std': 0.09457246701267184}
    Word2vec model #11: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 1.205832560857137, 'train_time_std': 0.005158620074483131}
    Word2vec model #12: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 1.5065066814422607, 'train_time_std': 0.036966116484319765}
    Word2vec model #13: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 1.537813663482666, 'train_time_std': 0.01020688183426915}
    Word2vec model #14: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 3.302257219950358, 'train_time_std': 0.04523242606424026}
    Word2vec model #15: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 3.4928714434305825, 'train_time_std': 0.19327551634697}
    Word2vec model #16: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 7.446084260940552, 'train_time_std': 0.7894319693665308}
    Word2vec model #17: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 7.060012976328532, 'train_time_std': 0.2136692186366028}
    Word2vec model #18: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 14.277136087417603, 'train_time_std': 0.7441633349142932}
    Word2vec model #19: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 13.758649031321207, 'train_time_std': 0.37393987718126326}
    Word2vec model #20: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 20.35730775197347, 'train_time_std': 0.41241047454786994}
    Word2vec model #21: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 21.380844751993816, 'train_time_std': 1.6909472056783184}
    Word2vec model #22: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 44.4877184232076, 'train_time_std': 1.1314265197889173}
    Word2vec model #23: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 44.517534812291466, 'train_time_std': 1.4472790491207064}
        compute_loss  hs  sg train_data  train_time_mean  train_time_std
    4           True   0   1       25kB         0.472116        0.015137
    5          False   0   1       25kB         0.469522        0.003345
    6           True   1   1       25kB         0.950259        0.005153
    7          False   1   1       25kB         0.942416        0.009776
    0           True   0   0       25kB         0.252174        0.020227
    1          False   0   0       25kB         0.258985        0.026276
    2           True   1   0       25kB         0.419408        0.002198
    3          False   1   0       25kB         0.430876        0.001000
    12          True   0   1        1MB         1.506507        0.036966
    13         False   0   1        1MB         1.537814        0.010207
    14          True   1   1        1MB         3.302257        0.045232
    15         False   1   1        1MB         3.492871        0.193276
    8           True   0   0        1MB         0.644114        0.009346
    9          False   0   0        1MB         0.656217        0.027036
    10          True   1   0        1MB         1.315072        0.094572
    11         False   1   0        1MB         1.205833        0.005159
    20          True   0   1       10MB        20.357308        0.412410
    21         False   0   1       10MB        21.380845        1.690947
    22          True   1   1       10MB        44.487718        1.131427
    23         False   1   1       10MB        44.517535        1.447279
    16          True   0   0       10MB         7.446084        0.789432
    17         False   0   0       10MB         7.060013        0.213669
    18          True   1   0       10MB        14.277136        0.744163
    19         False   1   0       10MB        13.758649        0.373940




Visualising Word Embeddings
---------------------------

The word embeddings made by the model can be visualised by reducing
dimensionality of the words to 2 dimensions using tSNE.

Visualisations can be used to notice semantic and syntactic trends in the data.

Example:

* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by
* Syntactic: words like run, running or cut, cutting lie close together.

Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.

.. Important::
  The model used for the visualisation is trained on a small corpus. Thus
  some of the relations might not be so clear.



.. code-block:: default


    from sklearn.decomposition import IncrementalPCA    # inital reduction
    from sklearn.manifold import TSNE                   # final reduction
    import numpy as np                                  # array handling


    def reduce_dimensions(model):
        num_dimensions = 2  # final num dimensions (2D, 3D, etc)

        # extract the words & their vectors, as numpy arrays
        vectors = np.asarray(model.wv.vectors)
        labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings

        # reduce using t-SNE
        tsne = TSNE(n_components=num_dimensions, random_state=0)
        vectors = tsne.fit_transform(vectors)

        x_vals = [v[0] for v in vectors]
        y_vals = [v[1] for v in vectors]
        return x_vals, y_vals, labels


    x_vals, y_vals, labels = reduce_dimensions(model)

    def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):
        from plotly.offline import init_notebook_mode, iplot, plot
        import plotly.graph_objs as go

        trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)
        data = [trace]

        if plot_in_notebook:
            init_notebook_mode(connected=True)
            iplot(data, filename='word-embedding-plot')
        else:
            plot(data, filename='word-embedding-plot.html')


    def plot_with_matplotlib(x_vals, y_vals, labels):
        import matplotlib.pyplot as plt
        import random

        random.seed(0)

        plt.figure(figsize=(12, 12))
        plt.scatter(x_vals, y_vals)

        #
        # Label randomly subsampled 25 data points
        #
        indices = list(range(len(labels)))
        selected_indices = random.sample(indices, 25)
        for i in selected_indices:
            plt.annotate(labels[i], (x_vals[i], y_vals[i]))

    try:
        get_ipython()
    except Exception:
        plot_function = plot_with_matplotlib
    else:
        plot_function = plot_with_plotly

    plot_function(x_vals, y_vals, labels)




.. image:: /auto_examples/tutorials/images/sphx_glr_run_word2vec_001.png
    :alt: run word2vec
    :class: sphx-glr-single-img





Conclusion
----------

In this tutorial we learned how to train word2vec models on your custom data
and also how to evaluate it. Hope that you too will find this popular tool
useful in your Machine Learning tasks!

Links
-----

- API docs: :py:mod:`gensim.models.word2vec`
- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 11 minutes  26.674 seconds)

**Estimated memory usage:**  7177 MB


.. _sphx_glr_download_auto_examples_tutorials_run_word2vec.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_word2vec.py <run_word2vec.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_word2vec.ipynb <run_word2vec.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_howtos_run_compare_lda.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_howtos_run_compare_lda.py:


How to Compare LDA Models
=========================

Demonstrates how you can visualize and compare trained topic models.



.. code-block:: default


    # sphinx_gallery_thumbnail_number = 2
    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








First, clean up the 20 Newsgroups dataset. We will use it to fit LDA.
---------------------------------------------------------------------



.. code-block:: default


    from string import punctuation
    from nltk import RegexpTokenizer
    from nltk.stem.porter import PorterStemmer
    from nltk.corpus import stopwords
    from sklearn.datasets import fetch_20newsgroups


    newsgroups = fetch_20newsgroups()
    eng_stopwords = set(stopwords.words('english'))

    tokenizer = RegexpTokenizer(r'\s+', gaps=True)
    stemmer = PorterStemmer()
    translate_tab = {ord(p): u" " for p in punctuation}

    def text2tokens(raw_text):
        """Split the raw_text string into a list of stemmed tokens."""
        clean_text = raw_text.lower().translate(translate_tab)
        tokens = [token.strip() for token in tokenizer.tokenize(clean_text)]
        tokens = [token for token in tokens if token not in eng_stopwords]
        stemmed_tokens = [stemmer.stem(token) for token in tokens]
        return [token for token in stemmed_tokens if len(token) > 2]  # skip short tokens

    dataset = [text2tokens(txt) for txt in newsgroups['data']]  # convert a documents to list of tokens

    from gensim.corpora import Dictionary
    dictionary = Dictionary(documents=dataset, prune_at=None)
    dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)  # use Dictionary to remove un-relevant tokens
    dictionary.compactify()

    d2b_dataset = [dictionary.doc2bow(doc) for doc in dataset]  # convert list of tokens to bag of word representation








Second, fit two LDA models.
---------------------------



.. code-block:: default


    from gensim.models import LdaMulticore
    num_topics = 15

    lda_fst = LdaMulticore(
        corpus=d2b_dataset, num_topics=num_topics, id2word=dictionary,
        workers=4, eval_every=None, passes=10, batch=True,
    )

    lda_snd = LdaMulticore(
        corpus=d2b_dataset, num_topics=num_topics, id2word=dictionary,
        workers=4, eval_every=None, passes=20, batch=True,
    )








Time to visualize, yay!
-----------------------

We use two slightly different visualization methods depending on how you're running this tutorial.
If you're running via a Jupyter notebook, then you'll get a nice interactive Plotly heatmap.
If you're viewing the static version of the page, you'll get a similar matplotlib heatmap, but it won't be interactive.



.. code-block:: default


    def plot_difference_plotly(mdiff, title="", annotation=None):
        """Plot the difference between models.

        Uses plotly as the backend."""
        import plotly.graph_objs as go
        import plotly.offline as py

        annotation_html = None
        if annotation is not None:
            annotation_html = [
                [
                    "+++ {}<br>--- {}".format(", ".join(int_tokens), ", ".join(diff_tokens))
                    for (int_tokens, diff_tokens) in row
                ]
                for row in annotation
            ]

        data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)
        layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title="topic"), yaxis=dict(title="topic"))
        py.iplot(dict(data=[data], layout=layout))


    def plot_difference_matplotlib(mdiff, title="", annotation=None):
        """Helper function to plot difference between models.

        Uses matplotlib as the backend."""
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots(figsize=(18, 14))
        data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')
        plt.title(title)
        plt.colorbar(data)


    try:
        get_ipython()
        import plotly.offline as py
    except Exception:
        #
        # Fall back to matplotlib if we're not in a notebook, or if plotly is
        # unavailable for whatever reason.
        #
        plot_difference = plot_difference_matplotlib
    else:
        py.init_notebook_mode()
        plot_difference = plot_difference_plotly








Gensim can help you visualise the differences between topics. For this purpose, you can use the ``diff()`` method of LdaModel.

``diff()`` returns a matrix with distances **mdiff** and a matrix with annotations **annotation**. Read the docstring for more detailed info.

In each **mdiff[i][j]** cell you'll find a distance between **topic_i** from the first model and **topic_j** from the second model.

In each **annotation[i][j]** cell you'll find **[tokens from intersection, tokens from difference** between **topic_i** from first model and **topic_j** from the second model.



.. code-block:: default


    print(LdaMulticore.diff.__doc__)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Calculate the difference in topic distributions between two models: `self` and `other`.

            Parameters
            ----------
            other : :class:`~gensim.models.ldamodel.LdaModel`
                The model which will be compared against the current object.
            distance : {'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'}
                The distance metric to calculate the difference with.
            num_words : int, optional
                The number of most relevant words used if `distance == 'jaccard'`. Also used for annotating topics.
            n_ann_terms : int, optional
                Max number of words in intersection/symmetric difference between topics. Used for annotation.
            diagonal : bool, optional
                Whether we need the difference between identical topics (the diagonal of the difference matrix).
            annotation : bool, optional
                Whether the intersection or difference of words between two topics should be returned.
            normed : bool, optional
                Whether the matrix should be normalized or not.

            Returns
            -------
            numpy.ndarray
                A difference matrix. Each element corresponds to the difference between the two topics,
                shape (`self.num_topics`, `other.num_topics`)
            numpy.ndarray, optional
                Annotation matrix where for each pair we include the word from the intersection of the two topics,
                and the word from the symmetric difference of the two topics. Only included if `annotation == True`.
                Shape (`self.num_topics`, `other_model.num_topics`, 2).

            Examples
            --------
            Get the differences between each pair of topics inferred by two models

            .. sourcecode:: pycon

                >>> from gensim.models.ldamulticore import LdaMulticore
                >>> from gensim.test.utils import datapath
                >>>
                >>> m1 = LdaMulticore.load(datapath("lda_3_0_1_model"))
                >>> m2 = LdaMulticore.load(datapath("ldamodel_python_3_5"))
                >>> mdiff, annotation = m1.diff(m2)
                >>> topic_diff = mdiff  # get matrix with difference for each topic pair from `m1` and `m2`

        




Case 1: How topics within ONE model correlate with each other.
--------------------------------------------------------------


Short description:

* x-axis - topic;

* y-axis - topic;

.. role:: raw-html-m2r(raw)
   :format: html

* :raw-html-m2r:`<span style="color:red">almost red cell</span>` - strongly decorrelated topics;

.. role:: raw-html-m2r(raw)
   :format: html

* :raw-html-m2r:`<span style="color:blue">almost blue cell</span>` - strongly correlated topics.

In an ideal world, we would like to see different topics decorrelated between themselves.
In this case, our matrix would look like this:



.. code-block:: default



    import numpy as np

    mdiff = np.ones((num_topics, num_topics))
    np.fill_diagonal(mdiff, 0.)
    plot_difference(mdiff, title="Topic difference (one model) in ideal world")




.. image:: /auto_examples/howtos/images/sphx_glr_run_compare_lda_001.png
    :alt: Topic difference (one model) in ideal world
    :class: sphx-glr-single-img





Unfortunately, in real life, not everything is so good, and the matrix looks different.


Short description (interactive annotations only):

* ``+++ make, world, well`` - words from the intersection of topics = present in both topics;

* ``--- money, day, still`` - words from the symmetric difference of topics = present in one topic but not the other.



.. code-block:: default



    mdiff, annotation = lda_fst.diff(lda_fst, distance='jaccard', num_words=50)
    plot_difference(mdiff, title="Topic difference (one model) [jaccard distance]", annotation=annotation)




.. image:: /auto_examples/howtos/images/sphx_glr_run_compare_lda_002.png
    :alt: Topic difference (one model) [jaccard distance]
    :class: sphx-glr-single-img





If you compare a model with itself, you want to see as many red elements as
possible (except on the diagonal). With this picture, you can look at the
"not very red elements" and understand which topics in the model are very
similar and why (you can read annotation if you move your pointer to cell).

Jaccard is a stable and robust distance function, but sometimes not sensitive
enough. Let's try to use the Hellinger distance instead.



.. code-block:: default


    mdiff, annotation = lda_fst.diff(lda_fst, distance='hellinger', num_words=50)
    plot_difference(mdiff, title="Topic difference (one model)[hellinger distance]", annotation=annotation)




.. image:: /auto_examples/howtos/images/sphx_glr_run_compare_lda_003.png
    :alt: Topic difference (one model)[hellinger distance]
    :class: sphx-glr-single-img





You see that everything has become worse, but remember that everything depends on the task.

Choose a distance function that matches your upstream task better: what kind of "similarity" is
relevant to you. From my (Ivan's) experience, Jaccard is fine.


Case 2: How topics from DIFFERENT models correlate with each other.
-------------------------------------------------------------------


Sometimes, we want to look at the patterns between two different models and compare them.

You can do this by constructing a matrix with the difference.



.. code-block:: default



    mdiff, annotation = lda_fst.diff(lda_snd, distance='jaccard', num_words=50)
    plot_difference(mdiff, title="Topic difference (two models)[jaccard distance]", annotation=annotation)




.. image:: /auto_examples/howtos/images/sphx_glr_run_compare_lda_004.png
    :alt: Topic difference (two models)[jaccard distance]
    :class: sphx-glr-single-img





Looking at this matrix, you can find similar and different topics between the two models.
The plot also includes relevant tokens describing the topics' intersection and difference.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  55.324 seconds)

**Estimated memory usage:**  303 MB


.. _sphx_glr_download_auto_examples_howtos_run_compare_lda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_compare_lda.py <run_compare_lda.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_compare_lda.ipynb <run_compare_lda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

:orphan:

.. _sphx_glr_auto_examples_howtos_sg_execution_times:

Computation times
=================
**00:00.171** total execution time for **auto_examples_howtos** files:

+----------------------------------------------------------------------------------------+-----------+--------+
| :ref:`sphx_glr_auto_examples_howtos_run_doc.py` (``run_doc.py``)                       | 00:00.171 | 6.1 MB |
+----------------------------------------------------------------------------------------+-----------+--------+
| :ref:`sphx_glr_auto_examples_howtos_run_compare_lda.py` (``run_compare_lda.py``)       | 00:00.000 | 0.0 MB |
+----------------------------------------------------------------------------------------+-----------+--------+
| :ref:`sphx_glr_auto_examples_howtos_run_doc2vec_imdb.py` (``run_doc2vec_imdb.py``)     | 00:00.000 | 0.0 MB |
+----------------------------------------------------------------------------------------+-----------+--------+
| :ref:`sphx_glr_auto_examples_howtos_run_downloader_api.py` (``run_downloader_api.py``) | 00:00.000 | 0.0 MB |
+----------------------------------------------------------------------------------------+-----------+--------+
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_howtos_run_doc2vec_imdb.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_howtos_run_doc2vec_imdb.py:


How to reproduce the doc2vec 'Paragraph Vector' paper
=====================================================

Shows how to reproduce results of the "Distributed Representation of Sentences and Documents" paper by Le and Mikolov using Gensim.



.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








Introduction
------------

This guide shows you how to reproduce the results of the paper by `Le and
Mikolov 2014 <https://arxiv.org/pdf/1405.4053.pdf>`_ using Gensim. While the
entire paper is worth reading (it's only 9 pages), we will be focusing on
Section 3.2: "Beyond One Sentence - Sentiment Analysis with the IMDB
dataset".

This guide follows the following steps:

#. Load the IMDB dataset
#. Train a variety of Doc2Vec models on the dataset
#. Evaluate the performance of each model using a logistic regression
#. Examine some of the results directly:

When examining results, we will look for answers for the following questions:

#. Are inferred vectors close to the precalculated ones?
#. Do close documents seem more related than distant ones?
#. Do the word vectors show useful similarities?
#. Are the word vectors from this dataset any good at analogies?

Load corpus
-----------

Our data for the tutorial will be the `IMDB archive
<http://ai.stanford.edu/~amaas/data/sentiment/>`_.
If you're not familiar with this dataset, then here's a brief intro: it
contains several thousand movie reviews.

Each review is a single line of text containing multiple sentences, for example:

```
One of the best movie-dramas I have ever seen. We do a lot of acting in the
church and this is one that can be used as a resource that highlights all the
good things that actors can do in their work. I highly recommend this one,
especially for those who have an interest in acting, as a "must see."
```

These reviews will be the **documents** that we will work with in this tutorial.
There are 100 thousand reviews in total.

#. 25k reviews for training (12.5k positive, 12.5k negative)
#. 25k reviews for testing (12.5k positive, 12.5k negative)
#. 50k unlabeled reviews

Out of 100k reviews, 50k have a label: either positive (the reviewer liked
the movie) or negative.
The remaining 50k are unlabeled.

Our first task will be to prepare the dataset.

More specifically, we will:

#. Download the tar.gz file (it's only 84MB, so this shouldn't take too long)
#. Unpack it and extract each movie review
#. Split the reviews into training and test datasets

First, let's define a convenient datatype for holding data for a single document:

* words: The text of the document, as a ``list`` of words.
* tags: Used to keep the index of the document in the entire dataset.
* split: one of ``train``\ , ``test`` or ``extra``. Determines how the document will be used (for training, testing, etc).
* sentiment: either 1 (positive), 0 (negative) or None (unlabeled document).

This data type is helpful for later evaluation and reporting.
In particular, the ``index`` member will help us quickly and easily retrieve the vectors for a document from a model.



.. code-block:: default

    import collections

    SentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')








We can now proceed with loading the corpus.


.. code-block:: default

    import io
    import re
    import tarfile
    import os.path

    import smart_open
    import gensim.utils

    def download_dataset(url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'):
        fname = url.split('/')[-1]

        if os.path.isfile(fname):
           return fname

        # Download the file to local storage first.
        with smart_open.open(url, "rb", ignore_ext=True) as fin:
            with smart_open.open(fname, 'wb', ignore_ext=True) as fout:
                while True:
                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)
                    if not buf:
                        break
                    fout.write(buf)

        return fname

    def create_sentiment_document(name, text, index):
        _, split, sentiment_str, _ = name.split('/')
        sentiment = {'pos': 1.0, 'neg': 0.0, 'unsup': None}[sentiment_str]

        if sentiment is None:
            split = 'extra'

        tokens = gensim.utils.to_unicode(text).split()
        return SentimentDocument(tokens, [index], split, sentiment)

    def extract_documents():
        fname = download_dataset()

        index = 0

        with tarfile.open(fname, mode='r:gz') as tar:
            for member in tar.getmembers():
                if re.match(r'aclImdb/(train|test)/(pos|neg|unsup)/\d+_\d+.txt$', member.name):
                    member_bytes = tar.extractfile(member).read()
                    member_text = member_bytes.decode('utf-8', errors='replace')
                    assert member_text.count('\n') == 0
                    yield create_sentiment_document(member.name, member_text, index)
                    index += 1

    alldocs = list(extract_documents())








Here's what a single document looks like.


.. code-block:: default

    print(alldocs[27])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    SentimentDocument(words=['I', 'was', 'looking', 'forward', 'to', 'this', 'movie.', 'Trustworthy', 'actors,', 'interesting', 'plot.', 'Great', 'atmosphere', 'then', '?????', 'IF', 'you', 'are', 'going', 'to', 'attempt', 'something', 'that', 'is', 'meant', 'to', 'encapsulate', 'the', 'meaning', 'of', 'life.', 'First.', 'Know', 'it.', 'OK', 'I', 'did', 'not', 'expect', 'the', 'directors', 'or', 'writers', 'to', 'actually', 'know', 'the', 'meaning', 'but', 'I', 'thought', 'they', 'may', 'have', 'offered', 'crumbs', 'to', 'peck', 'at', 'and', 'treats', 'to', 'add', 'fuel', 'to', 'the', 'fire-Which!', 'they', 'almost', 'did.', 'Things', 'I', "didn't", 'get.', 'A', 'woman', 'wandering', 'around', 'in', 'dark', 'places', 'and', 'lonely', 'car', 'parks', 'alone-oblivious', 'to', 'the', 'consequences.', 'Great', 'riddles', 'that', 'fell', 'by', 'the', 'wayside.', 'The', 'promise', 'of', 'the', 'knowledge', 'therein', 'contained', 'by', 'the', 'original', 'so-called', 'criminal.', 'I', 'had', 'no', 'problem', 'with', 'the', 'budget', 'and', 'enjoyed', 'the', 'suspense.', 'I', 'understood', 'and', 'can', 'wax', 'lyrical', 'about', 'the', 'fool', 'and', 'found', 'Adrian', 'Pauls', 'role', 'crucial', 'and', 'penetrating', 'and', 'then', '?????', 'Basically', 'the', 'story', 'line', 'and', 'the', 'script', 'where', 'good', 'up', 'to', 'a', 'point', 'and', 'that', 'point', 'was', 'the', 'last', '10', 'minutes', 'or', 'so.', 'What?', 'Run', 'out', 'of', 'ideas!', 'Such', 'a', 'pity', 'that', 'this', 'movie', 'had', 'to', 'let', 'us', 'down', 'so', 'badly.', 'It', 'may', 'not', 'comprehend', 'the', 'meaning', 'and', 'I', 'really', 'did', 'not', 'expect', 'the', 'writers', 'to', 'understand', 'it', 'but', 'I', 'was', 'hoping', 'for', 'an', 'intellectual,', 'if', 'not', 'spiritual', 'ride', 'and', 'got', 'a', 'bump', 'in', 'the', 'road'], tags=[27], split='test', sentiment=0.0)




Extract our documents and split into training/test sets.


.. code-block:: default

    train_docs = [doc for doc in alldocs if doc.split == 'train']
    test_docs = [doc for doc in alldocs if doc.split == 'test']
    print(f'{len(alldocs)} docs: {len(train_docs)} train-sentiment, {len(test_docs)} test-sentiment')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    100000 docs: 25000 train-sentiment, 25000 test-sentiment




Set-up Doc2Vec Training & Evaluation Models
-------------------------------------------

We approximate the experiment of Le & Mikolov `"Distributed Representations
of Sentences and Documents"
<http://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_ with guidance from
Mikolov's `example go.sh
<https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ>`_::

    ./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1

We vary the following parameter choices:

* 100-dimensional vectors, as the 400-d vectors of the paper take a lot of
  memory and, in our tests of this task, don't seem to offer much benefit
* Similarly, frequent word subsampling seems to decrease sentiment-prediction
  accuracy, so it's left out
* ``cbow=0`` means skip-gram which is equivalent to the paper's 'PV-DBOW'
  mode, matched in gensim with ``dm=0``
* Added to that DBOW model are two DM models, one which averages context
  vectors (\ ``dm_mean``\ ) and one which concatenates them (\ ``dm_concat``\ ,
  resulting in a much larger, slower, more data-hungry model)
* A ``min_count=2`` saves quite a bit of model memory, discarding only words
  that appear in a single doc (and are thus no more expressive than the
  unique-to-each doc vectors themselves)



.. code-block:: default


    import multiprocessing
    from collections import OrderedDict

    import gensim.models.doc2vec
    assert gensim.models.doc2vec.FAST_VERSION > -1, "This will be painfully slow otherwise"

    from gensim.models.doc2vec import Doc2Vec

    common_kwargs = dict(
        vector_size=100, epochs=20, min_count=2,
        sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0,
    )

    simple_models = [
        # PV-DBOW plain
        Doc2Vec(dm=0, **common_kwargs),
        # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes
        Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_kwargs),
        # PV-DM w/ concatenation - big, slow, experimental mode
        # window=5 (both sides) approximates paper's apparent 10-word total window size
        Doc2Vec(dm=1, dm_concat=1, window=5, **common_kwargs),
    ]

    for model in simple_models:
        model.build_vocab(alldocs)
        print(f"{model} vocabulary scanned & state initialized")

    models_by_name = OrderedDict((str(model), model) for model in simple_models)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 22:37:03,794 : INFO : using concatenative 1100-dimensional layer1
    2020-09-30 22:37:03,797 : INFO : collecting all words and their counts
    2020-09-30 22:37:03,797 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
    2020-09-30 22:37:04,523 : INFO : PROGRESS: at example #10000, processed 2292381 words (3159292/s), 150816 word types, 0 tags
    2020-09-30 22:37:05,236 : INFO : PROGRESS: at example #20000, processed 4573645 words (3201510/s), 238497 word types, 0 tags
    2020-09-30 22:37:05,974 : INFO : PROGRESS: at example #30000, processed 6865575 words (3106937/s), 312348 word types, 0 tags
    2020-09-30 22:37:06,731 : INFO : PROGRESS: at example #40000, processed 9190019 words (3071781/s), 377231 word types, 0 tags
    2020-09-30 22:37:07,465 : INFO : PROGRESS: at example #50000, processed 11557847 words (3227463/s), 438729 word types, 0 tags
    2020-09-30 22:37:08,233 : INFO : PROGRESS: at example #60000, processed 13899883 words (3046947/s), 493913 word types, 0 tags
    2020-09-30 22:37:09,009 : INFO : PROGRESS: at example #70000, processed 16270094 words (3056103/s), 548474 word types, 0 tags
    2020-09-30 22:37:09,777 : INFO : PROGRESS: at example #80000, processed 18598876 words (3035248/s), 598272 word types, 0 tags
    2020-09-30 22:37:10,563 : INFO : PROGRESS: at example #90000, processed 20916044 words (2945690/s), 646082 word types, 0 tags
    2020-09-30 22:37:11,354 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words
    2020-09-30 22:37:11,354 : INFO : Loading a fresh vocabulary
    2020-09-30 22:37:13,167 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)
    2020-09-30 22:37:13,167 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)
    2020-09-30 22:37:15,301 : INFO : deleting the raw counts dictionary of 693922 items
    2020-09-30 22:37:15,317 : INFO : sample=0 downsamples 0 most-common words
    2020-09-30 22:37:15,317 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)
    2020-09-30 22:37:18,663 : INFO : estimated required memory for 265408 words and 100 dimensions: 405030400 bytes
    2020-09-30 22:37:18,663 : INFO : resetting layer weights
    Doc2Vec(dbow,d100,n5,mc2,t8) vocabulary scanned & state initialized
    2020-09-30 22:37:37,706 : INFO : collecting all words and their counts
    2020-09-30 22:37:37,706 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
    2020-09-30 22:37:38,202 : INFO : PROGRESS: at example #10000, processed 2292381 words (4629335/s), 150816 word types, 0 tags
    2020-09-30 22:37:38,761 : INFO : PROGRESS: at example #20000, processed 4573645 words (4082741/s), 238497 word types, 0 tags
    2020-09-30 22:37:39,283 : INFO : PROGRESS: at example #30000, processed 6865575 words (4388176/s), 312348 word types, 0 tags
    2020-09-30 22:37:39,843 : INFO : PROGRESS: at example #40000, processed 9190019 words (4151942/s), 377231 word types, 0 tags
    2020-09-30 22:37:40,396 : INFO : PROGRESS: at example #50000, processed 11557847 words (4287824/s), 438729 word types, 0 tags
    2020-09-30 22:37:40,939 : INFO : PROGRESS: at example #60000, processed 13899883 words (4311204/s), 493913 word types, 0 tags
    2020-09-30 22:37:41,498 : INFO : PROGRESS: at example #70000, processed 16270094 words (4242545/s), 548474 word types, 0 tags
    2020-09-30 22:37:42,032 : INFO : PROGRESS: at example #80000, processed 18598876 words (4360341/s), 598272 word types, 0 tags
    2020-09-30 22:37:42,582 : INFO : PROGRESS: at example #90000, processed 20916044 words (4217682/s), 646082 word types, 0 tags
    2020-09-30 22:37:43,152 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words
    2020-09-30 22:37:43,152 : INFO : Loading a fresh vocabulary
    2020-09-30 22:37:44,948 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)
    2020-09-30 22:37:44,948 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)
    2020-09-30 22:37:46,994 : INFO : deleting the raw counts dictionary of 693922 items
    2020-09-30 22:37:47,008 : INFO : sample=0 downsamples 0 most-common words
    2020-09-30 22:37:47,009 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)
    2020-09-30 22:37:50,336 : INFO : estimated required memory for 265408 words and 100 dimensions: 405030400 bytes
    2020-09-30 22:37:50,336 : INFO : resetting layer weights
    Doc2Vec(dm/m,d100,n5,w10,mc2,t8) vocabulary scanned & state initialized
    2020-09-30 22:38:09,628 : INFO : collecting all words and their counts
    2020-09-30 22:38:09,628 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags
    2020-09-30 22:38:10,145 : INFO : PROGRESS: at example #10000, processed 2292381 words (4432122/s), 150816 word types, 0 tags
    2020-09-30 22:38:10,697 : INFO : PROGRESS: at example #20000, processed 4573645 words (4131700/s), 238497 word types, 0 tags
    2020-09-30 22:38:11,250 : INFO : PROGRESS: at example #30000, processed 6865575 words (4147845/s), 312348 word types, 0 tags
    2020-09-30 22:38:11,826 : INFO : PROGRESS: at example #40000, processed 9190019 words (4040528/s), 377231 word types, 0 tags
    2020-09-30 22:38:12,399 : INFO : PROGRESS: at example #50000, processed 11557847 words (4129801/s), 438729 word types, 0 tags
    2020-09-30 22:38:12,975 : INFO : PROGRESS: at example #60000, processed 13899883 words (4069527/s), 493913 word types, 0 tags
    2020-09-30 22:38:13,556 : INFO : PROGRESS: at example #70000, processed 16270094 words (4079870/s), 548474 word types, 0 tags
    2020-09-30 22:38:14,145 : INFO : PROGRESS: at example #80000, processed 18598876 words (3956736/s), 598272 word types, 0 tags
    2020-09-30 22:38:14,726 : INFO : PROGRESS: at example #90000, processed 20916044 words (3986449/s), 646082 word types, 0 tags
    2020-09-30 22:38:15,292 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words
    2020-09-30 22:38:15,293 : INFO : Loading a fresh vocabulary
    2020-09-30 22:38:17,117 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)
    2020-09-30 22:38:17,117 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)
    2020-09-30 22:38:19,256 : INFO : deleting the raw counts dictionary of 693922 items
    2020-09-30 22:38:19,270 : INFO : sample=0 downsamples 0 most-common words
    2020-09-30 22:38:19,270 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)
    2020-09-30 22:38:22,593 : INFO : estimated required memory for 265408 words and 100 dimensions: 1466662400 bytes
    2020-09-30 22:38:22,593 : INFO : resetting layer weights
    Doc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary scanned & state initialized




Le and Mikolov note that combining a paragraph vector from Distributed Bag of
Words (DBOW) and Distributed Memory (DM) improves performance. We will
follow, pairing the models together for evaluation. Here, we concatenate the
paragraph vectors obtained from each model with the help of a thin wrapper
class included in a gensim test module. (Note that this a separate, later
concatenation of output-vectors than the kind of input-window-concatenation
enabled by the ``dm_concat=1`` mode above.)



.. code-block:: default

    from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
    models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])
    models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 22:38:42,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])
    2020-09-30 22:38:42,022 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)




Predictive Evaluation Methods
-----------------------------

Given a document, our ``Doc2Vec`` models output a vector representation of the document.
How useful is a particular model?
In case of sentiment analysis, we want the ouput vector to reflect the sentiment in the input document.
So, in vector space, positive documents should be distant from negative documents.

We train a logistic regression from the training set:

  - regressors (inputs): document vectors from the Doc2Vec model
  - target (outpus): sentiment labels

So, this logistic regression will be able to predict sentiment given a document vector.

Next, we test our logistic regression on the test set, and measure the rate of errors (incorrect predictions).
If the document vectors from the Doc2Vec model reflect the actual sentiment well, the error rate will be low.

Therefore, the error rate of the logistic regression is indication of *how well* the given Doc2Vec model represents documents as vectors.
We can then compare different ``Doc2Vec`` models by looking at their error rates.



.. code-block:: default


    import numpy as np
    import statsmodels.api as sm
    from random import sample

    def logistic_predictor_from_data(train_targets, train_regressors):
        """Fit a statsmodel logistic predictor on supplied data"""
        logit = sm.Logit(train_targets, train_regressors)
        predictor = logit.fit(disp=0)
        # print(predictor.summary())
        return predictor

    def error_rate_for_model(test_model, train_set, test_set):
        """Report error rate on test_doc sentiments, using supplied model and train_docs"""

        train_targets = [doc.sentiment for doc in train_set]
        train_regressors = [test_model.dv[doc.tags[0]] for doc in train_set]
        train_regressors = sm.add_constant(train_regressors)
        predictor = logistic_predictor_from_data(train_targets, train_regressors)

        test_regressors = [test_model.dv[doc.tags[0]] for doc in test_set]
        test_regressors = sm.add_constant(test_regressors)

        # Predict & evaluate
        test_predictions = predictor.predict(test_regressors)
        corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_set])
        errors = len(test_predictions) - corrects
        error_rate = float(errors) / len(test_predictions)
        return (error_rate, errors, len(test_predictions), predictor)








Bulk Training & Per-Model Evaluation
------------------------------------

Note that doc-vector training is occurring on *all* documents of the dataset,
which includes all TRAIN/TEST/DEV docs.  Because the native document-order
has similar-sentiment documents in large clumps ‚Äì which is suboptimal for
training ‚Äì we work with once-shuffled copy of the training set.

We evaluate each model's sentiment predictive power based on error rate, and
the evaluation is done for each model.

(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3
main models takes about an hour.)



.. code-block:: default

    from collections import defaultdict
    error_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved









.. code-block:: default

    from random import shuffle
    shuffled_alldocs = alldocs[:]
    shuffle(shuffled_alldocs)

    for model in simple_models:
        print(f"Training {model}")
        model.train(shuffled_alldocs, total_examples=len(shuffled_alldocs), epochs=model.epochs)

        print(f"\nEvaluating {model}")
        err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)
        error_rates[str(model)] = err_rate
        print("\n%f %s\n" % (err_rate, model))

    for model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]:
        print(f"\nEvaluating {model}")
        err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)
        error_rates[str(model)] = err_rate
        print(f"\n{err_rate} {model}\n")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Training Doc2Vec(dbow,d100,n5,mc2,t8)
    2020-09-30 22:38:43,643 : INFO : training model with 8 workers on 265408 vocabulary and 100 features, using sg=1 hs=0 sample=0 negative=5 window=5
    2020-09-30 22:38:44,654 : INFO : EPOCH 1 - PROGRESS: at 3.50% examples, 810634 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:38:45,665 : INFO : EPOCH 1 - PROGRESS: at 8.11% examples, 932060 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:46,674 : INFO : EPOCH 1 - PROGRESS: at 13.11% examples, 1003981 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:47,682 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 1056038 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:48,690 : INFO : EPOCH 1 - PROGRESS: at 23.57% examples, 1080969 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:49,705 : INFO : EPOCH 1 - PROGRESS: at 28.88% examples, 1102341 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:38:50,708 : INFO : EPOCH 1 - PROGRESS: at 34.36% examples, 1120959 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:51,715 : INFO : EPOCH 1 - PROGRESS: at 39.89% examples, 1138346 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:52,729 : INFO : EPOCH 1 - PROGRESS: at 45.30% examples, 1148552 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:53,729 : INFO : EPOCH 1 - PROGRESS: at 50.84% examples, 1158541 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:38:54,738 : INFO : EPOCH 1 - PROGRESS: at 56.30% examples, 1166666 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:55,740 : INFO : EPOCH 1 - PROGRESS: at 61.75% examples, 1174829 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:56,740 : INFO : EPOCH 1 - PROGRESS: at 67.20% examples, 1179766 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:57,740 : INFO : EPOCH 1 - PROGRESS: at 72.73% examples, 1186870 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:38:58,756 : INFO : EPOCH 1 - PROGRESS: at 78.14% examples, 1189792 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:38:59,760 : INFO : EPOCH 1 - PROGRESS: at 83.70% examples, 1194989 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:00,763 : INFO : EPOCH 1 - PROGRESS: at 89.28% examples, 1199066 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:01,764 : INFO : EPOCH 1 - PROGRESS: at 94.91% examples, 1202983 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:02,608 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:39:02,614 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:39:02,615 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:39:02,616 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:39:02,617 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:39:02,622 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:39:02,629 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:39:02,631 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:39:02,631 : INFO : EPOCH - 1 : training on 23279529 raw words (22951015 effective words) took 19.0s, 1209291 effective words/s
    2020-09-30 22:39:03,641 : INFO : EPOCH 2 - PROGRESS: at 5.81% examples, 1334892 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:04,642 : INFO : EPOCH 2 - PROGRESS: at 11.52% examples, 1332668 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:05,650 : INFO : EPOCH 2 - PROGRESS: at 17.41% examples, 1339624 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:06,654 : INFO : EPOCH 2 - PROGRESS: at 23.22% examples, 1335308 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:07,656 : INFO : EPOCH 2 - PROGRESS: at 28.97% examples, 1332332 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:08,673 : INFO : EPOCH 2 - PROGRESS: at 34.48% examples, 1314497 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:09,676 : INFO : EPOCH 2 - PROGRESS: at 40.24% examples, 1314296 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:10,698 : INFO : EPOCH 2 - PROGRESS: at 45.66% examples, 1302303 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:11,700 : INFO : EPOCH 2 - PROGRESS: at 51.23% examples, 1297227 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:12,702 : INFO : EPOCH 2 - PROGRESS: at 56.84% examples, 1297181 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:13,702 : INFO : EPOCH 2 - PROGRESS: at 62.60% examples, 1300534 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:14,707 : INFO : EPOCH 2 - PROGRESS: at 68.34% examples, 1300550 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:39:15,712 : INFO : EPOCH 2 - PROGRESS: at 73.81% examples, 1298506 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:16,720 : INFO : EPOCH 2 - PROGRESS: at 79.37% examples, 1295658 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:17,724 : INFO : EPOCH 2 - PROGRESS: at 85.01% examples, 1294762 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:18,744 : INFO : EPOCH 2 - PROGRESS: at 90.64% examples, 1292767 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:19,760 : INFO : EPOCH 2 - PROGRESS: at 96.32% examples, 1290914 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:20,357 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:39:20,358 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:39:20,361 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:39:20,365 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:39:20,373 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:39:20,374 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:39:20,379 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:39:20,381 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:39:20,381 : INFO : EPOCH - 2 : training on 23279529 raw words (22951015 effective words) took 17.7s, 1293152 effective words/s
    2020-09-30 22:39:21,385 : INFO : EPOCH 3 - PROGRESS: at 5.45% examples, 1266660 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:22,387 : INFO : EPOCH 3 - PROGRESS: at 11.09% examples, 1288270 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:39:23,387 : INFO : EPOCH 3 - PROGRESS: at 16.66% examples, 1287842 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:24,389 : INFO : EPOCH 3 - PROGRESS: at 22.22% examples, 1285359 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:25,397 : INFO : EPOCH 3 - PROGRESS: at 27.87% examples, 1283208 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:26,405 : INFO : EPOCH 3 - PROGRESS: at 33.33% examples, 1275169 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:27,408 : INFO : EPOCH 3 - PROGRESS: at 39.04% examples, 1279197 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:28,414 : INFO : EPOCH 3 - PROGRESS: at 44.64% examples, 1279209 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:29,416 : INFO : EPOCH 3 - PROGRESS: at 50.46% examples, 1283072 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:30,417 : INFO : EPOCH 3 - PROGRESS: at 56.09% examples, 1284428 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:31,418 : INFO : EPOCH 3 - PROGRESS: at 61.67% examples, 1285268 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:32,435 : INFO : EPOCH 3 - PROGRESS: at 66.94% examples, 1276443 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:33,444 : INFO : EPOCH 3 - PROGRESS: at 72.56% examples, 1277342 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:34,447 : INFO : EPOCH 3 - PROGRESS: at 77.98% examples, 1275080 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:35,452 : INFO : EPOCH 3 - PROGRESS: at 83.31% examples, 1271646 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:36,467 : INFO : EPOCH 3 - PROGRESS: at 89.05% examples, 1272677 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:37,469 : INFO : EPOCH 3 - PROGRESS: at 94.11% examples, 1265033 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:38,473 : INFO : EPOCH 3 - PROGRESS: at 99.28% examples, 1259790 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:38,547 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:39:38,551 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:39:38,553 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:39:38,555 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:39:38,559 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:39:38,560 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:39:38,567 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:39:38,570 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:39:38,570 : INFO : EPOCH - 3 : training on 23279529 raw words (22951015 effective words) took 18.2s, 1262025 effective words/s
    2020-09-30 22:39:39,578 : INFO : EPOCH 4 - PROGRESS: at 5.45% examples, 1260893 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:40,581 : INFO : EPOCH 4 - PROGRESS: at 11.13% examples, 1289836 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:41,582 : INFO : EPOCH 4 - PROGRESS: at 16.91% examples, 1304703 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:42,582 : INFO : EPOCH 4 - PROGRESS: at 22.61% examples, 1305466 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:43,585 : INFO : EPOCH 4 - PROGRESS: at 28.35% examples, 1306310 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:44,588 : INFO : EPOCH 4 - PROGRESS: at 33.85% examples, 1295655 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:45,609 : INFO : EPOCH 4 - PROGRESS: at 39.63% examples, 1296177 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:46,614 : INFO : EPOCH 4 - PROGRESS: at 44.97% examples, 1286945 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:47,617 : INFO : EPOCH 4 - PROGRESS: at 50.58% examples, 1284547 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:48,631 : INFO : EPOCH 4 - PROGRESS: at 55.88% examples, 1276301 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:49,634 : INFO : EPOCH 4 - PROGRESS: at 61.46% examples, 1277819 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:39:50,635 : INFO : EPOCH 4 - PROGRESS: at 67.16% examples, 1279326 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:51,638 : INFO : EPOCH 4 - PROGRESS: at 72.88% examples, 1282749 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:39:52,653 : INFO : EPOCH 4 - PROGRESS: at 78.43% examples, 1281163 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:53,665 : INFO : EPOCH 4 - PROGRESS: at 84.31% examples, 1284425 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:54,669 : INFO : EPOCH 4 - PROGRESS: at 90.11% examples, 1286674 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:55,675 : INFO : EPOCH 4 - PROGRESS: at 96.00% examples, 1288231 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:56,310 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:39:56,315 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:39:56,318 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:39:56,319 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:39:56,327 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:39:56,329 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:39:56,332 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:39:56,335 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:39:56,335 : INFO : EPOCH - 4 : training on 23279529 raw words (22951015 effective words) took 17.8s, 1292154 effective words/s
    2020-09-30 22:39:57,340 : INFO : EPOCH 5 - PROGRESS: at 5.58% examples, 1294342 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:39:58,348 : INFO : EPOCH 5 - PROGRESS: at 10.96% examples, 1268816 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:39:59,351 : INFO : EPOCH 5 - PROGRESS: at 16.21% examples, 1244571 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:00,364 : INFO : EPOCH 5 - PROGRESS: at 21.88% examples, 1259315 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:01,369 : INFO : EPOCH 5 - PROGRESS: at 27.41% examples, 1257507 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:02,371 : INFO : EPOCH 5 - PROGRESS: at 32.96% examples, 1259700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:03,385 : INFO : EPOCH 5 - PROGRESS: at 38.66% examples, 1262599 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:04,394 : INFO : EPOCH 5 - PROGRESS: at 44.34% examples, 1265326 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:05,401 : INFO : EPOCH 5 - PROGRESS: at 49.65% examples, 1259209 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:06,418 : INFO : EPOCH 5 - PROGRESS: at 55.33% examples, 1260120 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:07,431 : INFO : EPOCH 5 - PROGRESS: at 60.91% examples, 1263647 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:08,434 : INFO : EPOCH 5 - PROGRESS: at 66.57% examples, 1264442 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:09,439 : INFO : EPOCH 5 - PROGRESS: at 71.86% examples, 1260773 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:10,450 : INFO : EPOCH 5 - PROGRESS: at 77.20% examples, 1258923 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:11,450 : INFO : EPOCH 5 - PROGRESS: at 82.67% examples, 1257666 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:40:12,455 : INFO : EPOCH 5 - PROGRESS: at 88.22% examples, 1257992 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:13,456 : INFO : EPOCH 5 - PROGRESS: at 93.77% examples, 1258003 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:14,468 : INFO : EPOCH 5 - PROGRESS: at 99.32% examples, 1257390 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:14,547 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:40:14,554 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:40:14,557 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:40:14,558 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:40:14,559 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:40:14,562 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:40:14,564 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:40:14,565 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:40:14,565 : INFO : EPOCH - 5 : training on 23279529 raw words (22951015 effective words) took 18.2s, 1259124 effective words/s
    2020-09-30 22:40:15,572 : INFO : EPOCH 6 - PROGRESS: at 4.99% examples, 1157587 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:16,586 : INFO : EPOCH 6 - PROGRESS: at 10.29% examples, 1187334 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:17,587 : INFO : EPOCH 6 - PROGRESS: at 15.62% examples, 1197426 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:18,590 : INFO : EPOCH 6 - PROGRESS: at 20.71% examples, 1195503 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:19,604 : INFO : EPOCH 6 - PROGRESS: at 26.33% examples, 1208202 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:20,608 : INFO : EPOCH 6 - PROGRESS: at 31.77% examples, 1213675 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:21,612 : INFO : EPOCH 6 - PROGRESS: at 37.07% examples, 1210678 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:22,618 : INFO : EPOCH 6 - PROGRESS: at 42.66% examples, 1219476 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:23,623 : INFO : EPOCH 6 - PROGRESS: at 48.39% examples, 1228249 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:24,647 : INFO : EPOCH 6 - PROGRESS: at 54.05% examples, 1231281 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:25,651 : INFO : EPOCH 6 - PROGRESS: at 59.74% examples, 1240284 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:26,662 : INFO : EPOCH 6 - PROGRESS: at 65.22% examples, 1239158 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:27,675 : INFO : EPOCH 6 - PROGRESS: at 70.73% examples, 1240296 words/s, in_qsize 15, out_qsize 2
    2020-09-30 22:40:28,688 : INFO : EPOCH 6 - PROGRESS: at 76.10% examples, 1240448 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:29,693 : INFO : EPOCH 6 - PROGRESS: at 81.53% examples, 1239411 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:30,696 : INFO : EPOCH 6 - PROGRESS: at 86.95% examples, 1239166 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:31,697 : INFO : EPOCH 6 - PROGRESS: at 92.42% examples, 1239150 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:32,711 : INFO : EPOCH 6 - PROGRESS: at 98.00% examples, 1240013 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:33,022 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:40:33,022 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:40:33,024 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:40:33,026 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:40:33,030 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:40:33,031 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:40:33,034 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:40:33,040 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:40:33,040 : INFO : EPOCH - 6 : training on 23279529 raw words (22951015 effective words) took 18.5s, 1242469 effective words/s
    2020-09-30 22:40:34,056 : INFO : EPOCH 7 - PROGRESS: at 5.58% examples, 1280763 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:35,057 : INFO : EPOCH 7 - PROGRESS: at 10.85% examples, 1257239 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:36,060 : INFO : EPOCH 7 - PROGRESS: at 16.51% examples, 1268944 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:37,070 : INFO : EPOCH 7 - PROGRESS: at 22.10% examples, 1270911 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:38,081 : INFO : EPOCH 7 - PROGRESS: at 27.65% examples, 1267114 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:39,092 : INFO : EPOCH 7 - PROGRESS: at 33.05% examples, 1259463 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:40,093 : INFO : EPOCH 7 - PROGRESS: at 38.45% examples, 1255238 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:41,097 : INFO : EPOCH 7 - PROGRESS: at 44.21% examples, 1262013 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:40:42,101 : INFO : EPOCH 7 - PROGRESS: at 49.84% examples, 1264196 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:43,114 : INFO : EPOCH 7 - PROGRESS: at 55.52% examples, 1266020 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:44,122 : INFO : EPOCH 7 - PROGRESS: at 60.90% examples, 1265325 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:45,127 : INFO : EPOCH 7 - PROGRESS: at 65.80% examples, 1251392 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:40:46,144 : INFO : EPOCH 7 - PROGRESS: at 70.73% examples, 1240850 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:47,148 : INFO : EPOCH 7 - PROGRESS: at 75.98% examples, 1239689 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:48,149 : INFO : EPOCH 7 - PROGRESS: at 81.30% examples, 1237700 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:40:49,155 : INFO : EPOCH 7 - PROGRESS: at 86.92% examples, 1239847 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:50,155 : INFO : EPOCH 7 - PROGRESS: at 92.27% examples, 1238659 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:51,159 : INFO : EPOCH 7 - PROGRESS: at 97.87% examples, 1240246 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:51,492 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:40:51,493 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:40:51,497 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:40:51,498 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:40:51,504 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:40:51,506 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:40:51,511 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:40:51,513 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:40:51,513 : INFO : EPOCH - 7 : training on 23279529 raw words (22951015 effective words) took 18.5s, 1242577 effective words/s
    2020-09-30 22:40:52,518 : INFO : EPOCH 8 - PROGRESS: at 5.31% examples, 1237109 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:53,523 : INFO : EPOCH 8 - PROGRESS: at 10.77% examples, 1251883 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:54,533 : INFO : EPOCH 8 - PROGRESS: at 16.24% examples, 1246617 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:55,544 : INFO : EPOCH 8 - PROGRESS: at 21.06% examples, 1212978 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:56,550 : INFO : EPOCH 8 - PROGRESS: at 25.68% examples, 1177978 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:57,551 : INFO : EPOCH 8 - PROGRESS: at 30.77% examples, 1177925 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:40:58,554 : INFO : EPOCH 8 - PROGRESS: at 36.09% examples, 1180227 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:40:59,557 : INFO : EPOCH 8 - PROGRESS: at 41.39% examples, 1184834 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:00,560 : INFO : EPOCH 8 - PROGRESS: at 46.70% examples, 1186988 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:01,570 : INFO : EPOCH 8 - PROGRESS: at 52.31% examples, 1194996 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:02,573 : INFO : EPOCH 8 - PROGRESS: at 57.66% examples, 1198839 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:03,584 : INFO : EPOCH 8 - PROGRESS: at 63.25% examples, 1204897 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:04,598 : INFO : EPOCH 8 - PROGRESS: at 68.51% examples, 1203348 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:41:05,626 : INFO : EPOCH 8 - PROGRESS: at 73.81% examples, 1203637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:06,635 : INFO : EPOCH 8 - PROGRESS: at 78.48% examples, 1193759 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:07,635 : INFO : EPOCH 8 - PROGRESS: at 83.40% examples, 1189957 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:08,639 : INFO : EPOCH 8 - PROGRESS: at 87.64% examples, 1176213 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:09,640 : INFO : EPOCH 8 - PROGRESS: at 92.10% examples, 1167420 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:10,641 : INFO : EPOCH 8 - PROGRESS: at 96.91% examples, 1163172 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:11,370 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:41:11,371 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:41:11,373 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:41:11,386 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:41:11,389 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:41:11,392 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:41:11,395 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:41:11,400 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:41:11,400 : INFO : EPOCH - 8 : training on 23279529 raw words (22951015 effective words) took 19.9s, 1154250 effective words/s
    2020-09-30 22:41:12,408 : INFO : EPOCH 9 - PROGRESS: at 4.06% examples, 934517 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:41:13,415 : INFO : EPOCH 9 - PROGRESS: at 7.93% examples, 913759 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:14,421 : INFO : EPOCH 9 - PROGRESS: at 11.52% examples, 887053 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:15,437 : INFO : EPOCH 9 - PROGRESS: at 15.84% examples, 908247 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:16,449 : INFO : EPOCH 9 - PROGRESS: at 19.68% examples, 904688 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:17,449 : INFO : EPOCH 9 - PROGRESS: at 24.45% examples, 933015 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:18,477 : INFO : EPOCH 9 - PROGRESS: at 29.50% examples, 963829 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:19,482 : INFO : EPOCH 9 - PROGRESS: at 34.48% examples, 982678 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:20,494 : INFO : EPOCH 9 - PROGRESS: at 39.30% examples, 994877 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:21,516 : INFO : EPOCH 9 - PROGRESS: at 43.82% examples, 996557 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:22,520 : INFO : EPOCH 9 - PROGRESS: at 47.98% examples, 992704 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:23,527 : INFO : EPOCH 9 - PROGRESS: at 52.95% examples, 1002986 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:24,532 : INFO : EPOCH 9 - PROGRESS: at 58.20% examples, 1019197 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:25,537 : INFO : EPOCH 9 - PROGRESS: at 62.72% examples, 1020615 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:26,541 : INFO : EPOCH 9 - PROGRESS: at 67.70% examples, 1027724 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:27,549 : INFO : EPOCH 9 - PROGRESS: at 72.18% examples, 1027853 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:28,557 : INFO : EPOCH 9 - PROGRESS: at 77.03% examples, 1033423 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:29,567 : INFO : EPOCH 9 - PROGRESS: at 82.33% examples, 1042148 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:30,572 : INFO : EPOCH 9 - PROGRESS: at 87.64% examples, 1050648 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:31,575 : INFO : EPOCH 9 - PROGRESS: at 92.92% examples, 1057974 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:32,578 : INFO : EPOCH 9 - PROGRESS: at 98.21% examples, 1064734 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:32,865 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:41:32,868 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:41:32,871 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:41:32,871 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:41:32,872 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:41:32,872 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:41:32,882 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:41:32,886 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:41:32,886 : INFO : EPOCH - 9 : training on 23279529 raw words (22951015 effective words) took 21.5s, 1068336 effective words/s
    2020-09-30 22:41:33,891 : INFO : EPOCH 10 - PROGRESS: at 4.99% examples, 1160494 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:34,902 : INFO : EPOCH 10 - PROGRESS: at 10.09% examples, 1166681 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:35,903 : INFO : EPOCH 10 - PROGRESS: at 15.45% examples, 1186580 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:36,913 : INFO : EPOCH 10 - PROGRESS: at 20.71% examples, 1194965 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:37,914 : INFO : EPOCH 10 - PROGRESS: at 26.33% examples, 1211006 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:41:38,918 : INFO : EPOCH 10 - PROGRESS: at 31.52% examples, 1206396 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:39,925 : INFO : EPOCH 10 - PROGRESS: at 37.07% examples, 1212076 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:40,930 : INFO : EPOCH 10 - PROGRESS: at 42.57% examples, 1218468 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:41,937 : INFO : EPOCH 10 - PROGRESS: at 48.25% examples, 1226103 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:42,938 : INFO : EPOCH 10 - PROGRESS: at 53.63% examples, 1225374 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:43,941 : INFO : EPOCH 10 - PROGRESS: at 59.06% examples, 1229025 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:44,943 : INFO : EPOCH 10 - PROGRESS: at 64.33% examples, 1226391 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:45,946 : INFO : EPOCH 10 - PROGRESS: at 69.81% examples, 1228690 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:41:46,967 : INFO : EPOCH 10 - PROGRESS: at 75.14% examples, 1228313 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:47,967 : INFO : EPOCH 10 - PROGRESS: at 80.50% examples, 1228427 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:48,978 : INFO : EPOCH 10 - PROGRESS: at 85.68% examples, 1224206 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:49,982 : INFO : EPOCH 10 - PROGRESS: at 90.99% examples, 1223108 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:50,995 : INFO : EPOCH 10 - PROGRESS: at 96.32% examples, 1221126 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:51,610 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:41:51,611 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:41:51,613 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:41:51,615 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:41:51,621 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:41:51,622 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:41:51,626 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:41:51,633 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:41:51,633 : INFO : EPOCH - 10 : training on 23279529 raw words (22951015 effective words) took 18.7s, 1224422 effective words/s
    2020-09-30 22:41:52,647 : INFO : EPOCH 11 - PROGRESS: at 5.28% examples, 1216993 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:41:53,651 : INFO : EPOCH 11 - PROGRESS: at 10.85% examples, 1256436 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:54,667 : INFO : EPOCH 11 - PROGRESS: at 16.29% examples, 1244085 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:55,676 : INFO : EPOCH 11 - PROGRESS: at 21.67% examples, 1243136 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:56,680 : INFO : EPOCH 11 - PROGRESS: at 26.98% examples, 1235377 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:57,686 : INFO : EPOCH 11 - PROGRESS: at 32.37% examples, 1234038 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:58,686 : INFO : EPOCH 11 - PROGRESS: at 37.28% examples, 1216537 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:41:59,692 : INFO : EPOCH 11 - PROGRESS: at 42.58% examples, 1216230 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:00,723 : INFO : EPOCH 11 - PROGRESS: at 48.02% examples, 1215516 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:01,726 : INFO : EPOCH 11 - PROGRESS: at 53.51% examples, 1217515 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:02,737 : INFO : EPOCH 11 - PROGRESS: at 58.73% examples, 1216743 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:03,737 : INFO : EPOCH 11 - PROGRESS: at 63.84% examples, 1212750 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:04,741 : INFO : EPOCH 11 - PROGRESS: at 68.82% examples, 1206454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:05,742 : INFO : EPOCH 11 - PROGRESS: at 73.84% examples, 1204651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:06,755 : INFO : EPOCH 11 - PROGRESS: at 79.19% examples, 1204610 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:07,763 : INFO : EPOCH 11 - PROGRESS: at 84.66% examples, 1206784 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:08,767 : INFO : EPOCH 11 - PROGRESS: at 89.70% examples, 1203347 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:09,771 : INFO : EPOCH 11 - PROGRESS: at 94.95% examples, 1201992 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:10,677 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:42:10,682 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:42:10,685 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:42:10,687 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:42:10,690 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:42:10,692 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:42:10,700 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:42:10,701 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:42:10,701 : INFO : EPOCH - 11 : training on 23279529 raw words (22951015 effective words) took 19.1s, 1203849 effective words/s
    2020-09-30 22:42:11,710 : INFO : EPOCH 12 - PROGRESS: at 5.24% examples, 1213409 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:12,723 : INFO : EPOCH 12 - PROGRESS: at 10.65% examples, 1230132 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:13,730 : INFO : EPOCH 12 - PROGRESS: at 16.24% examples, 1242649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:14,731 : INFO : EPOCH 12 - PROGRESS: at 21.47% examples, 1234854 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:15,737 : INFO : EPOCH 12 - PROGRESS: at 26.72% examples, 1226460 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:16,752 : INFO : EPOCH 12 - PROGRESS: at 31.94% examples, 1218649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:17,752 : INFO : EPOCH 12 - PROGRESS: at 37.10% examples, 1211470 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:18,758 : INFO : EPOCH 12 - PROGRESS: at 42.17% examples, 1204578 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:19,765 : INFO : EPOCH 12 - PROGRESS: at 47.60% examples, 1208369 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:20,769 : INFO : EPOCH 12 - PROGRESS: at 52.86% examples, 1206189 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:21,769 : INFO : EPOCH 12 - PROGRESS: at 57.94% examples, 1204070 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:22,774 : INFO : EPOCH 12 - PROGRESS: at 63.24% examples, 1204773 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:23,776 : INFO : EPOCH 12 - PROGRESS: at 68.69% examples, 1207237 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:24,782 : INFO : EPOCH 12 - PROGRESS: at 73.77% examples, 1205684 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:25,786 : INFO : EPOCH 12 - PROGRESS: at 78.85% examples, 1202468 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:26,803 : INFO : EPOCH 12 - PROGRESS: at 83.61% examples, 1194484 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:27,803 : INFO : EPOCH 12 - PROGRESS: at 88.89% examples, 1194814 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:28,821 : INFO : EPOCH 12 - PROGRESS: at 93.62% examples, 1186590 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:29,825 : INFO : EPOCH 12 - PROGRESS: at 98.55% examples, 1183182 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:30,062 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:42:30,063 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:42:30,065 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:42:30,068 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:42:30,071 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:42:30,072 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:42:30,082 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:42:30,084 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:42:30,084 : INFO : EPOCH - 12 : training on 23279529 raw words (22951015 effective words) took 19.4s, 1184266 effective words/s
    2020-09-30 22:42:31,091 : INFO : EPOCH 13 - PROGRESS: at 5.08% examples, 1176516 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:32,097 : INFO : EPOCH 13 - PROGRESS: at 9.41% examples, 1086949 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:33,107 : INFO : EPOCH 13 - PROGRESS: at 13.73% examples, 1052705 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:34,112 : INFO : EPOCH 13 - PROGRESS: at 18.60% examples, 1071677 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:35,115 : INFO : EPOCH 13 - PROGRESS: at 22.74% examples, 1046719 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:36,118 : INFO : EPOCH 13 - PROGRESS: at 26.81% examples, 1026636 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:37,127 : INFO : EPOCH 13 - PROGRESS: at 31.60% examples, 1035861 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:38,133 : INFO : EPOCH 13 - PROGRESS: at 36.57% examples, 1045551 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:39,136 : INFO : EPOCH 13 - PROGRESS: at 41.69% examples, 1060274 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:40,141 : INFO : EPOCH 13 - PROGRESS: at 46.95% examples, 1073474 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:41,145 : INFO : EPOCH 13 - PROGRESS: at 51.87% examples, 1076896 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:42,159 : INFO : EPOCH 13 - PROGRESS: at 56.21% examples, 1069865 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:43,162 : INFO : EPOCH 13 - PROGRESS: at 60.91% examples, 1072153 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:44,168 : INFO : EPOCH 13 - PROGRESS: at 65.56% examples, 1069832 words/s, in_qsize 16, out_qsize 2
    2020-09-30 22:42:45,183 : INFO : EPOCH 13 - PROGRESS: at 70.86% examples, 1078767 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:46,187 : INFO : EPOCH 13 - PROGRESS: at 76.10% examples, 1087893 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:47,191 : INFO : EPOCH 13 - PROGRESS: at 81.61% examples, 1097075 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:48,205 : INFO : EPOCH 13 - PROGRESS: at 87.21% examples, 1106285 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:49,216 : INFO : EPOCH 13 - PROGRESS: at 92.80% examples, 1114172 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:50,217 : INFO : EPOCH 13 - PROGRESS: at 98.03% examples, 1118058 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:50,547 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:42:50,553 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:42:50,553 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:42:50,559 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:42:50,561 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:42:50,563 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:42:50,570 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:42:50,573 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:42:50,573 : INFO : EPOCH - 13 : training on 23279529 raw words (22951015 effective words) took 20.5s, 1120326 effective words/s
    2020-09-30 22:42:51,583 : INFO : EPOCH 14 - PROGRESS: at 4.71% examples, 1086772 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:52,586 : INFO : EPOCH 14 - PROGRESS: at 9.53% examples, 1101042 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:53,595 : INFO : EPOCH 14 - PROGRESS: at 14.79% examples, 1133043 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:42:54,606 : INFO : EPOCH 14 - PROGRESS: at 20.02% examples, 1154453 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:55,610 : INFO : EPOCH 14 - PROGRESS: at 25.47% examples, 1168383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:56,613 : INFO : EPOCH 14 - PROGRESS: at 30.61% examples, 1171078 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:42:57,614 : INFO : EPOCH 14 - PROGRESS: at 35.72% examples, 1167782 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:58,625 : INFO : EPOCH 14 - PROGRESS: at 40.83% examples, 1167887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:42:59,641 : INFO : EPOCH 14 - PROGRESS: at 46.38% examples, 1175771 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:00,641 : INFO : EPOCH 14 - PROGRESS: at 51.61% examples, 1177368 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:01,643 : INFO : EPOCH 14 - PROGRESS: at 57.00% examples, 1183653 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:02,652 : INFO : EPOCH 14 - PROGRESS: at 62.39% examples, 1188089 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:03,661 : INFO : EPOCH 14 - PROGRESS: at 67.46% examples, 1184433 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:04,675 : INFO : EPOCH 14 - PROGRESS: at 72.47% examples, 1181853 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:05,677 : INFO : EPOCH 14 - PROGRESS: at 77.61% examples, 1182258 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:06,689 : INFO : EPOCH 14 - PROGRESS: at 82.51% examples, 1177176 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:07,689 : INFO : EPOCH 14 - PROGRESS: at 87.59% examples, 1176299 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:08,690 : INFO : EPOCH 14 - PROGRESS: at 92.88% examples, 1177654 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:09,694 : INFO : EPOCH 14 - PROGRESS: at 98.21% examples, 1179264 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:09,974 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:43:09,979 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:43:09,981 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:43:09,981 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:43:09,983 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:43:09,984 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:43:09,998 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:43:10,000 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:43:10,000 : INFO : EPOCH - 14 : training on 23279529 raw words (22951015 effective words) took 19.4s, 1181581 effective words/s
    2020-09-30 22:43:11,006 : INFO : EPOCH 15 - PROGRESS: at 5.04% examples, 1169059 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:12,020 : INFO : EPOCH 15 - PROGRESS: at 10.33% examples, 1192786 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:13,025 : INFO : EPOCH 15 - PROGRESS: at 15.66% examples, 1199598 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:43:14,040 : INFO : EPOCH 15 - PROGRESS: at 21.02% examples, 1207847 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:15,045 : INFO : EPOCH 15 - PROGRESS: at 26.36% examples, 1208733 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:16,057 : INFO : EPOCH 15 - PROGRESS: at 31.37% examples, 1196431 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:17,059 : INFO : EPOCH 15 - PROGRESS: at 36.56% examples, 1192360 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:18,059 : INFO : EPOCH 15 - PROGRESS: at 41.57% examples, 1187426 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:19,065 : INFO : EPOCH 15 - PROGRESS: at 46.38% examples, 1176196 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:20,070 : INFO : EPOCH 15 - PROGRESS: at 51.31% examples, 1170374 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:21,070 : INFO : EPOCH 15 - PROGRESS: at 56.60% examples, 1174909 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:22,073 : INFO : EPOCH 15 - PROGRESS: at 61.88% examples, 1178982 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:23,082 : INFO : EPOCH 15 - PROGRESS: at 67.37% examples, 1183558 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:24,086 : INFO : EPOCH 15 - PROGRESS: at 72.81% examples, 1188717 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:25,089 : INFO : EPOCH 15 - PROGRESS: at 78.19% examples, 1191843 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:26,093 : INFO : EPOCH 15 - PROGRESS: at 83.23% examples, 1189713 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:27,101 : INFO : EPOCH 15 - PROGRESS: at 87.72% examples, 1179068 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:28,105 : INFO : EPOCH 15 - PROGRESS: at 91.76% examples, 1164566 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:29,107 : INFO : EPOCH 15 - PROGRESS: at 96.70% examples, 1161944 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:29,701 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:43:29,704 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:43:29,708 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:43:29,710 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:43:29,711 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:43:29,712 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:43:29,719 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:43:29,723 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:43:29,723 : INFO : EPOCH - 15 : training on 23279529 raw words (22951015 effective words) took 19.7s, 1163839 effective words/s
    2020-09-30 22:43:30,747 : INFO : EPOCH 16 - PROGRESS: at 5.03% examples, 1148006 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:31,753 : INFO : EPOCH 16 - PROGRESS: at 10.37% examples, 1191820 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:32,766 : INFO : EPOCH 16 - PROGRESS: at 15.66% examples, 1192330 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:33,783 : INFO : EPOCH 16 - PROGRESS: at 20.37% examples, 1165833 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:34,797 : INFO : EPOCH 16 - PROGRESS: at 24.71% examples, 1123999 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:35,805 : INFO : EPOCH 16 - PROGRESS: at 29.73% examples, 1129538 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:43:36,813 : INFO : EPOCH 16 - PROGRESS: at 34.72% examples, 1128446 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:37,819 : INFO : EPOCH 16 - PROGRESS: at 39.98% examples, 1136526 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:38,822 : INFO : EPOCH 16 - PROGRESS: at 44.84% examples, 1134569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:39,836 : INFO : EPOCH 16 - PROGRESS: at 49.74% examples, 1130700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:40,845 : INFO : EPOCH 16 - PROGRESS: at 54.25% examples, 1120524 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:43:41,848 : INFO : EPOCH 16 - PROGRESS: at 58.77% examples, 1114984 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:43:42,849 : INFO : EPOCH 16 - PROGRESS: at 63.77% examples, 1116806 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:43,864 : INFO : EPOCH 16 - PROGRESS: at 68.73% examples, 1116856 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:44,866 : INFO : EPOCH 16 - PROGRESS: at 73.77% examples, 1121043 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:45,877 : INFO : EPOCH 16 - PROGRESS: at 79.11% examples, 1126476 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:46,880 : INFO : EPOCH 16 - PROGRESS: at 84.36% examples, 1130656 words/s, in_qsize 15, out_qsize 1
    2020-09-30 22:43:47,886 : INFO : EPOCH 16 - PROGRESS: at 89.85% examples, 1137280 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:43:48,888 : INFO : EPOCH 16 - PROGRESS: at 94.71% examples, 1134564 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:43:49,828 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:43:49,834 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:43:49,835 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:43:49,839 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:43:49,842 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:43:49,843 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:43:49,850 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:43:49,853 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:43:49,853 : INFO : EPOCH - 16 : training on 23279529 raw words (22951015 effective words) took 20.1s, 1140322 effective words/s
    2020-09-30 22:43:50,867 : INFO : EPOCH 17 - PROGRESS: at 4.99% examples, 1148756 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:51,871 : INFO : EPOCH 17 - PROGRESS: at 10.02% examples, 1155615 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:52,879 : INFO : EPOCH 17 - PROGRESS: at 15.21% examples, 1163883 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:53,879 : INFO : EPOCH 17 - PROGRESS: at 20.50% examples, 1182766 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:54,887 : INFO : EPOCH 17 - PROGRESS: at 25.64% examples, 1176517 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:55,891 : INFO : EPOCH 17 - PROGRESS: at 30.94% examples, 1184077 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:56,895 : INFO : EPOCH 17 - PROGRESS: at 35.97% examples, 1175834 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:43:57,906 : INFO : EPOCH 17 - PROGRESS: at 40.83% examples, 1167651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:58,918 : INFO : EPOCH 17 - PROGRESS: at 45.66% examples, 1158895 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:43:59,934 : INFO : EPOCH 17 - PROGRESS: at 50.88% examples, 1159432 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:00,946 : INFO : EPOCH 17 - PROGRESS: at 55.88% examples, 1157545 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:01,950 : INFO : EPOCH 17 - PROGRESS: at 60.82% examples, 1157464 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:02,961 : INFO : EPOCH 17 - PROGRESS: at 66.11% examples, 1159071 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:03,965 : INFO : EPOCH 17 - PROGRESS: at 71.42% examples, 1163791 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:04,967 : INFO : EPOCH 17 - PROGRESS: at 76.67% examples, 1167344 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:44:05,967 : INFO : EPOCH 17 - PROGRESS: at 82.11% examples, 1171839 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:06,972 : INFO : EPOCH 17 - PROGRESS: at 87.38% examples, 1173229 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:07,977 : INFO : EPOCH 17 - PROGRESS: at 92.76% examples, 1175590 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:08,979 : INFO : EPOCH 17 - PROGRESS: at 97.83% examples, 1174413 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:09,329 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:44:09,329 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:44:09,331 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:44:09,334 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:44:09,338 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:44:09,339 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:44:09,351 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:44:09,352 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:44:09,352 : INFO : EPOCH - 17 : training on 23279529 raw words (22951015 effective words) took 19.5s, 1177191 effective words/s
    2020-09-30 22:44:10,358 : INFO : EPOCH 18 - PROGRESS: at 5.08% examples, 1177268 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:44:11,374 : INFO : EPOCH 18 - PROGRESS: at 10.09% examples, 1162965 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:12,375 : INFO : EPOCH 18 - PROGRESS: at 15.21% examples, 1164978 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:13,385 : INFO : EPOCH 18 - PROGRESS: at 20.37% examples, 1173518 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:44:14,392 : INFO : EPOCH 18 - PROGRESS: at 25.83% examples, 1184972 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:15,394 : INFO : EPOCH 18 - PROGRESS: at 31.18% examples, 1191451 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:16,395 : INFO : EPOCH 18 - PROGRESS: at 36.52% examples, 1193605 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:17,397 : INFO : EPOCH 18 - PROGRESS: at 41.87% examples, 1197806 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:18,401 : INFO : EPOCH 18 - PROGRESS: at 47.12% examples, 1197465 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:19,401 : INFO : EPOCH 18 - PROGRESS: at 52.54% examples, 1200688 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:44:20,405 : INFO : EPOCH 18 - PROGRESS: at 57.82% examples, 1203074 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:21,415 : INFO : EPOCH 18 - PROGRESS: at 63.11% examples, 1203290 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:22,447 : INFO : EPOCH 18 - PROGRESS: at 68.38% examples, 1200122 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:23,465 : INFO : EPOCH 18 - PROGRESS: at 73.77% examples, 1202919 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:44:24,470 : INFO : EPOCH 18 - PROGRESS: at 79.11% examples, 1203659 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:25,470 : INFO : EPOCH 18 - PROGRESS: at 84.13% examples, 1200456 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:26,479 : INFO : EPOCH 18 - PROGRESS: at 89.28% examples, 1198131 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:27,487 : INFO : EPOCH 18 - PROGRESS: at 94.56% examples, 1197356 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:28,462 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:44:28,467 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:44:28,469 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:44:28,470 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:44:28,473 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:44:28,475 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:44:28,479 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:44:28,489 : INFO : EPOCH 18 - PROGRESS: at 100.00% examples, 1199470 words/s, in_qsize 0, out_qsize 1
    2020-09-30 22:44:28,489 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:44:28,489 : INFO : EPOCH - 18 : training on 23279529 raw words (22951015 effective words) took 19.1s, 1199451 effective words/s
    2020-09-30 22:44:29,494 : INFO : EPOCH 19 - PROGRESS: at 5.20% examples, 1207824 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:30,502 : INFO : EPOCH 19 - PROGRESS: at 10.45% examples, 1211193 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:44:31,509 : INFO : EPOCH 19 - PROGRESS: at 15.99% examples, 1227394 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:32,515 : INFO : EPOCH 19 - PROGRESS: at 21.34% examples, 1228856 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:33,519 : INFO : EPOCH 19 - PROGRESS: at 26.54% examples, 1220169 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:34,531 : INFO : EPOCH 19 - PROGRESS: at 31.39% examples, 1199499 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:35,556 : INFO : EPOCH 19 - PROGRESS: at 36.74% examples, 1196410 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:44:36,566 : INFO : EPOCH 19 - PROGRESS: at 41.87% examples, 1193103 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:37,570 : INFO : EPOCH 19 - PROGRESS: at 46.75% examples, 1183500 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:38,578 : INFO : EPOCH 19 - PROGRESS: at 51.48% examples, 1172054 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:39,591 : INFO : EPOCH 19 - PROGRESS: at 56.17% examples, 1162790 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:40,600 : INFO : EPOCH 19 - PROGRESS: at 60.91% examples, 1157797 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:44:41,603 : INFO : EPOCH 19 - PROGRESS: at 66.11% examples, 1158603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:42,610 : INFO : EPOCH 19 - PROGRESS: at 71.33% examples, 1161750 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:44:43,627 : INFO : EPOCH 19 - PROGRESS: at 76.86% examples, 1168766 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:44,631 : INFO : EPOCH 19 - PROGRESS: at 82.47% examples, 1174692 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:45,636 : INFO : EPOCH 19 - PROGRESS: at 88.22% examples, 1182700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:46,637 : INFO : EPOCH 19 - PROGRESS: at 93.86% examples, 1187942 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:47,654 : INFO : EPOCH 19 - PROGRESS: at 99.61% examples, 1193267 words/s, in_qsize 9, out_qsize 0
    2020-09-30 22:44:47,682 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:44:47,682 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:44:47,685 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:44:47,686 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:44:47,690 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:44:47,692 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:44:47,699 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:44:47,703 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:44:47,703 : INFO : EPOCH - 19 : training on 23279529 raw words (22951015 effective words) took 19.2s, 1194667 effective words/s
    2020-09-30 22:44:48,709 : INFO : EPOCH 20 - PROGRESS: at 5.63% examples, 1303461 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:49,711 : INFO : EPOCH 20 - PROGRESS: at 10.49% examples, 1219230 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:50,723 : INFO : EPOCH 20 - PROGRESS: at 15.87% examples, 1217482 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:51,736 : INFO : EPOCH 20 - PROGRESS: at 21.06% examples, 1212412 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:52,753 : INFO : EPOCH 20 - PROGRESS: at 26.01% examples, 1190365 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:53,754 : INFO : EPOCH 20 - PROGRESS: at 31.22% examples, 1191444 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:54,771 : INFO : EPOCH 20 - PROGRESS: at 36.52% examples, 1189415 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:55,772 : INFO : EPOCH 20 - PROGRESS: at 41.78% examples, 1191921 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:56,786 : INFO : EPOCH 20 - PROGRESS: at 47.07% examples, 1191888 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:57,796 : INFO : EPOCH 20 - PROGRESS: at 52.62% examples, 1197368 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:44:58,799 : INFO : EPOCH 20 - PROGRESS: at 57.70% examples, 1195866 words/s, in_qsize 15, out_qsize 2
    2020-09-30 22:44:59,800 : INFO : EPOCH 20 - PROGRESS: at 63.25% examples, 1202264 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:00,804 : INFO : EPOCH 20 - PROGRESS: at 68.52% examples, 1201864 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:01,807 : INFO : EPOCH 20 - PROGRESS: at 73.81% examples, 1204338 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:02,815 : INFO : EPOCH 20 - PROGRESS: at 79.19% examples, 1205375 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:03,817 : INFO : EPOCH 20 - PROGRESS: at 84.36% examples, 1203806 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:04,818 : INFO : EPOCH 20 - PROGRESS: at 89.45% examples, 1201252 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:05,819 : INFO : EPOCH 20 - PROGRESS: at 94.32% examples, 1195924 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:06,820 : INFO : EPOCH 20 - PROGRESS: at 99.32% examples, 1192737 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:06,894 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:45:06,904 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:45:06,906 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:45:06,910 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:45:06,920 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:45:06,921 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:45:06,923 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:45:06,925 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:45:06,925 : INFO : EPOCH - 20 : training on 23279529 raw words (22951015 effective words) took 19.2s, 1194194 effective words/s
    2020-09-30 22:45:06,925 : INFO : training on a 465590580 raw words (459020300 effective words) took 383.3s, 1197567 effective words/s

    Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)

    0.105600 Doc2Vec(dbow,d100,n5,mc2,t8)

    Training Doc2Vec(dm/m,d100,n5,w10,mc2,t8)
    2020-09-30 22:45:07,659 : INFO : training model with 8 workers on 265408 vocabulary and 100 features, using sg=0 hs=0 sample=0 negative=5 window=10
    2020-09-30 22:45:08,666 : INFO : EPOCH 1 - PROGRESS: at 2.35% examples, 548427 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:09,682 : INFO : EPOCH 1 - PROGRESS: at 5.28% examples, 608661 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:10,688 : INFO : EPOCH 1 - PROGRESS: at 8.10% examples, 619824 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:11,697 : INFO : EPOCH 1 - PROGRESS: at 10.96% examples, 632160 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:12,725 : INFO : EPOCH 1 - PROGRESS: at 13.74% examples, 628037 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:13,730 : INFO : EPOCH 1 - PROGRESS: at 16.67% examples, 637295 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:14,743 : INFO : EPOCH 1 - PROGRESS: at 19.53% examples, 639311 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:15,743 : INFO : EPOCH 1 - PROGRESS: at 22.41% examples, 641702 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:16,745 : INFO : EPOCH 1 - PROGRESS: at 25.51% examples, 648623 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:17,761 : INFO : EPOCH 1 - PROGRESS: at 28.76% examples, 657878 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:18,773 : INFO : EPOCH 1 - PROGRESS: at 31.99% examples, 664091 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:19,775 : INFO : EPOCH 1 - PROGRESS: at 35.27% examples, 670543 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:20,779 : INFO : EPOCH 1 - PROGRESS: at 38.49% examples, 675356 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:21,790 : INFO : EPOCH 1 - PROGRESS: at 41.69% examples, 679084 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:22,808 : INFO : EPOCH 1 - PROGRESS: at 44.92% examples, 682633 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:23,809 : INFO : EPOCH 1 - PROGRESS: at 48.35% examples, 688218 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:24,810 : INFO : EPOCH 1 - PROGRESS: at 51.71% examples, 692167 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:25,824 : INFO : EPOCH 1 - PROGRESS: at 55.11% examples, 696745 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:26,828 : INFO : EPOCH 1 - PROGRESS: at 58.39% examples, 700651 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:27,834 : INFO : EPOCH 1 - PROGRESS: at 61.67% examples, 703037 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:28,844 : INFO : EPOCH 1 - PROGRESS: at 65.14% examples, 706601 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:29,849 : INFO : EPOCH 1 - PROGRESS: at 68.47% examples, 709055 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:30,852 : INFO : EPOCH 1 - PROGRESS: at 71.73% examples, 711014 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:31,861 : INFO : EPOCH 1 - PROGRESS: at 75.10% examples, 714131 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:32,873 : INFO : EPOCH 1 - PROGRESS: at 78.19% examples, 713170 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:33,880 : INFO : EPOCH 1 - PROGRESS: at 81.53% examples, 714986 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:34,901 : INFO : EPOCH 1 - PROGRESS: at 84.70% examples, 714837 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:35,927 : INFO : EPOCH 1 - PROGRESS: at 88.05% examples, 715983 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:36,928 : INFO : EPOCH 1 - PROGRESS: at 91.21% examples, 715976 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:37,947 : INFO : EPOCH 1 - PROGRESS: at 94.37% examples, 715603 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:38,947 : INFO : EPOCH 1 - PROGRESS: at 97.26% examples, 713526 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:39,725 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:45:39,731 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:45:39,732 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:45:39,739 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:45:39,741 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:45:39,742 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:45:39,756 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:45:39,757 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:45:39,757 : INFO : EPOCH - 1 : training on 23279529 raw words (22951015 effective words) took 32.1s, 715091 effective words/s
    2020-09-30 22:45:40,766 : INFO : EPOCH 2 - PROGRESS: at 3.00% examples, 692022 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:41,774 : INFO : EPOCH 2 - PROGRESS: at 6.05% examples, 696651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:42,786 : INFO : EPOCH 2 - PROGRESS: at 9.16% examples, 702760 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:43,797 : INFO : EPOCH 2 - PROGRESS: at 12.24% examples, 703596 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:44,804 : INFO : EPOCH 2 - PROGRESS: at 15.13% examples, 693553 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:45,806 : INFO : EPOCH 2 - PROGRESS: at 18.05% examples, 692473 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:46,811 : INFO : EPOCH 2 - PROGRESS: at 21.02% examples, 691519 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:47,819 : INFO : EPOCH 2 - PROGRESS: at 23.57% examples, 675702 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:48,836 : INFO : EPOCH 2 - PROGRESS: at 26.58% examples, 676807 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:49,841 : INFO : EPOCH 2 - PROGRESS: at 29.55% examples, 677251 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:50,863 : INFO : EPOCH 2 - PROGRESS: at 32.66% examples, 678347 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:51,882 : INFO : EPOCH 2 - PROGRESS: at 35.76% examples, 678809 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:52,900 : INFO : EPOCH 2 - PROGRESS: at 38.62% examples, 676355 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:53,901 : INFO : EPOCH 2 - PROGRESS: at 41.26% examples, 671628 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:45:54,918 : INFO : EPOCH 2 - PROGRESS: at 44.29% examples, 671793 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:55,926 : INFO : EPOCH 2 - PROGRESS: at 47.32% examples, 673054 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:56,952 : INFO : EPOCH 2 - PROGRESS: at 49.74% examples, 664932 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:57,952 : INFO : EPOCH 2 - PROGRESS: at 52.27% examples, 659843 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:58,987 : INFO : EPOCH 2 - PROGRESS: at 54.72% examples, 653579 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:45:59,995 : INFO : EPOCH 2 - PROGRESS: at 56.84% examples, 645398 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:01,038 : INFO : EPOCH 2 - PROGRESS: at 59.15% examples, 639229 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:02,067 : INFO : EPOCH 2 - PROGRESS: at 61.91% examples, 638337 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:03,074 : INFO : EPOCH 2 - PROGRESS: at 65.14% examples, 641954 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:04,093 : INFO : EPOCH 2 - PROGRESS: at 68.04% examples, 642498 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:05,125 : INFO : EPOCH 2 - PROGRESS: at 70.85% examples, 642015 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:06,142 : INFO : EPOCH 2 - PROGRESS: at 73.81% examples, 643690 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:07,143 : INFO : EPOCH 2 - PROGRESS: at 76.82% examples, 645613 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:08,148 : INFO : EPOCH 2 - PROGRESS: at 79.42% examples, 643238 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:09,156 : INFO : EPOCH 2 - PROGRESS: at 81.99% examples, 641282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:10,161 : INFO : EPOCH 2 - PROGRESS: at 84.87% examples, 641734 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:11,174 : INFO : EPOCH 2 - PROGRESS: at 87.88% examples, 642963 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:12,189 : INFO : EPOCH 2 - PROGRESS: at 90.11% examples, 638621 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:13,206 : INFO : EPOCH 2 - PROGRESS: at 92.92% examples, 638083 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:14,215 : INFO : EPOCH 2 - PROGRESS: at 95.45% examples, 635756 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:15,238 : INFO : EPOCH 2 - PROGRESS: at 98.07% examples, 634640 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:15,916 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:46:15,924 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:46:15,937 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:46:15,941 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:46:15,945 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:46:15,946 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:46:15,960 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:46:15,963 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:46:15,963 : INFO : EPOCH - 2 : training on 23279529 raw words (22951015 effective words) took 36.2s, 633941 effective words/s
    2020-09-30 22:46:17,015 : INFO : EPOCH 3 - PROGRESS: at 1.36% examples, 304582 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:18,015 : INFO : EPOCH 3 - PROGRESS: at 3.61% examples, 410865 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:46:19,051 : INFO : EPOCH 3 - PROGRESS: at 5.80% examples, 436241 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:20,055 : INFO : EPOCH 3 - PROGRESS: at 7.90% examples, 447092 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:21,056 : INFO : EPOCH 3 - PROGRESS: at 9.93% examples, 453686 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:22,058 : INFO : EPOCH 3 - PROGRESS: at 12.02% examples, 458457 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:23,064 : INFO : EPOCH 3 - PROGRESS: at 14.33% examples, 467007 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:24,065 : INFO : EPOCH 3 - PROGRESS: at 16.55% examples, 473911 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:25,083 : INFO : EPOCH 3 - PROGRESS: at 18.98% examples, 482773 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:26,097 : INFO : EPOCH 3 - PROGRESS: at 21.06% examples, 482215 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:46:27,109 : INFO : EPOCH 3 - PROGRESS: at 23.52% examples, 487862 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:28,114 : INFO : EPOCH 3 - PROGRESS: at 26.09% examples, 496102 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:29,114 : INFO : EPOCH 3 - PROGRESS: at 28.43% examples, 499456 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:30,116 : INFO : EPOCH 3 - PROGRESS: at 30.65% examples, 500280 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:31,123 : INFO : EPOCH 3 - PROGRESS: at 33.37% examples, 507158 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:32,126 : INFO : EPOCH 3 - PROGRESS: at 36.04% examples, 513351 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:33,132 : INFO : EPOCH 3 - PROGRESS: at 38.70% examples, 518888 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:34,139 : INFO : EPOCH 3 - PROGRESS: at 41.39% examples, 524222 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:35,153 : INFO : EPOCH 3 - PROGRESS: at 44.48% examples, 533314 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:36,192 : INFO : EPOCH 3 - PROGRESS: at 47.37% examples, 538454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:37,202 : INFO : EPOCH 3 - PROGRESS: at 50.41% examples, 545218 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:38,204 : INFO : EPOCH 3 - PROGRESS: at 53.08% examples, 548113 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:39,214 : INFO : EPOCH 3 - PROGRESS: at 55.72% examples, 550546 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:40,216 : INFO : EPOCH 3 - PROGRESS: at 58.35% examples, 553380 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:41,222 : INFO : EPOCH 3 - PROGRESS: at 61.05% examples, 556199 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:42,235 : INFO : EPOCH 3 - PROGRESS: at 63.25% examples, 553534 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:43,253 : INFO : EPOCH 3 - PROGRESS: at 65.93% examples, 555252 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:44,274 : INFO : EPOCH 3 - PROGRESS: at 68.73% examples, 557805 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:45,282 : INFO : EPOCH 3 - PROGRESS: at 71.38% examples, 559776 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:46,292 : INFO : EPOCH 3 - PROGRESS: at 74.05% examples, 561890 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:47,333 : INFO : EPOCH 3 - PROGRESS: at 76.82% examples, 563625 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:48,338 : INFO : EPOCH 3 - PROGRESS: at 79.57% examples, 565292 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:49,345 : INFO : EPOCH 3 - PROGRESS: at 82.43% examples, 567673 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:50,363 : INFO : EPOCH 3 - PROGRESS: at 85.01% examples, 568055 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:46:51,386 : INFO : EPOCH 3 - PROGRESS: at 87.88% examples, 570250 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:52,420 : INFO : EPOCH 3 - PROGRESS: at 90.74% examples, 571887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:53,440 : INFO : EPOCH 3 - PROGRESS: at 93.86% examples, 575202 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:54,443 : INFO : EPOCH 3 - PROGRESS: at 96.91% examples, 578150 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:55,333 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:46:55,347 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:46:55,349 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:46:55,352 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:46:55,363 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:46:55,365 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:46:55,370 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:46:55,377 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:46:55,377 : INFO : EPOCH - 3 : training on 23279529 raw words (22951015 effective words) took 39.4s, 582343 effective words/s
    2020-09-30 22:46:56,383 : INFO : EPOCH 4 - PROGRESS: at 2.87% examples, 666594 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:57,388 : INFO : EPOCH 4 - PROGRESS: at 5.76% examples, 665539 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:58,391 : INFO : EPOCH 4 - PROGRESS: at 8.21% examples, 632843 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:46:59,402 : INFO : EPOCH 4 - PROGRESS: at 10.85% examples, 629651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:00,418 : INFO : EPOCH 4 - PROGRESS: at 13.74% examples, 631361 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:01,433 : INFO : EPOCH 4 - PROGRESS: at 16.55% examples, 634195 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:02,439 : INFO : EPOCH 4 - PROGRESS: at 19.42% examples, 637339 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:03,439 : INFO : EPOCH 4 - PROGRESS: at 22.54% examples, 647170 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:04,445 : INFO : EPOCH 4 - PROGRESS: at 25.64% examples, 653128 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:05,446 : INFO : EPOCH 4 - PROGRESS: at 28.56% examples, 655333 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:06,455 : INFO : EPOCH 4 - PROGRESS: at 31.60% examples, 658491 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:07,460 : INFO : EPOCH 4 - PROGRESS: at 34.68% examples, 661245 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:08,470 : INFO : EPOCH 4 - PROGRESS: at 37.63% examples, 661227 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:09,489 : INFO : EPOCH 4 - PROGRESS: at 40.68% examples, 663539 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:10,491 : INFO : EPOCH 4 - PROGRESS: at 43.86% examples, 667583 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:11,521 : INFO : EPOCH 4 - PROGRESS: at 47.03% examples, 669945 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:12,526 : INFO : EPOCH 4 - PROGRESS: at 50.27% examples, 673579 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:13,535 : INFO : EPOCH 4 - PROGRESS: at 53.44% examples, 675662 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:14,536 : INFO : EPOCH 4 - PROGRESS: at 56.56% examples, 678314 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:15,551 : INFO : EPOCH 4 - PROGRESS: at 59.62% examples, 680066 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:47:16,567 : INFO : EPOCH 4 - PROGRESS: at 62.76% examples, 681323 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:17,582 : INFO : EPOCH 4 - PROGRESS: at 66.15% examples, 684631 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:18,582 : INFO : EPOCH 4 - PROGRESS: at 69.36% examples, 687279 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:19,600 : INFO : EPOCH 4 - PROGRESS: at 72.81% examples, 691197 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:20,617 : INFO : EPOCH 4 - PROGRESS: at 76.10% examples, 694067 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:21,617 : INFO : EPOCH 4 - PROGRESS: at 79.46% examples, 696387 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:22,626 : INFO : EPOCH 4 - PROGRESS: at 82.51% examples, 696184 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:23,629 : INFO : EPOCH 4 - PROGRESS: at 85.38% examples, 694826 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:24,634 : INFO : EPOCH 4 - PROGRESS: at 88.40% examples, 694455 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:25,661 : INFO : EPOCH 4 - PROGRESS: at 91.39% examples, 693306 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:26,677 : INFO : EPOCH 4 - PROGRESS: at 94.56% examples, 693748 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:27,689 : INFO : EPOCH 4 - PROGRESS: at 97.83% examples, 695156 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:28,306 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:47:28,312 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:47:28,321 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:47:28,322 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:47:28,330 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:47:28,330 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:47:28,330 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:47:28,349 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:47:28,349 : INFO : EPOCH - 4 : training on 23279529 raw words (22951015 effective words) took 33.0s, 696165 effective words/s
    2020-09-30 22:47:29,359 : INFO : EPOCH 5 - PROGRESS: at 2.83% examples, 653079 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:30,367 : INFO : EPOCH 5 - PROGRESS: at 5.80% examples, 667789 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:31,369 : INFO : EPOCH 5 - PROGRESS: at 8.76% examples, 673078 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:32,380 : INFO : EPOCH 5 - PROGRESS: at 11.72% examples, 676480 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:33,382 : INFO : EPOCH 5 - PROGRESS: at 14.91% examples, 685780 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:34,394 : INFO : EPOCH 5 - PROGRESS: at 18.01% examples, 691332 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:35,406 : INFO : EPOCH 5 - PROGRESS: at 21.06% examples, 692591 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:36,442 : INFO : EPOCH 5 - PROGRESS: at 24.06% examples, 686427 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:37,445 : INFO : EPOCH 5 - PROGRESS: at 27.07% examples, 687277 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:38,476 : INFO : EPOCH 5 - PROGRESS: at 30.19% examples, 688773 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:39,490 : INFO : EPOCH 5 - PROGRESS: at 33.37% examples, 690084 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:40,497 : INFO : EPOCH 5 - PROGRESS: at 36.56% examples, 692620 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:41,500 : INFO : EPOCH 5 - PROGRESS: at 39.79% examples, 696630 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:42,544 : INFO : EPOCH 5 - PROGRESS: at 42.88% examples, 695120 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:43,617 : INFO : EPOCH 5 - PROGRESS: at 46.03% examples, 693068 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:44,627 : INFO : EPOCH 5 - PROGRESS: at 49.13% examples, 694104 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:45,639 : INFO : EPOCH 5 - PROGRESS: at 52.23% examples, 693819 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:46,644 : INFO : EPOCH 5 - PROGRESS: at 54.67% examples, 686473 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:47,651 : INFO : EPOCH 5 - PROGRESS: at 57.66% examples, 686838 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:48,665 : INFO : EPOCH 5 - PROGRESS: at 60.62% examples, 686745 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:49,703 : INFO : EPOCH 5 - PROGRESS: at 63.11% examples, 679672 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:50,721 : INFO : EPOCH 5 - PROGRESS: at 65.84% examples, 676445 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:51,728 : INFO : EPOCH 5 - PROGRESS: at 68.69% examples, 675073 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:52,730 : INFO : EPOCH 5 - PROGRESS: at 71.28% examples, 672366 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:47:53,732 : INFO : EPOCH 5 - PROGRESS: at 73.88% examples, 669889 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:54,739 : INFO : EPOCH 5 - PROGRESS: at 76.82% examples, 669991 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:55,748 : INFO : EPOCH 5 - PROGRESS: at 79.84% examples, 670398 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:56,782 : INFO : EPOCH 5 - PROGRESS: at 82.64% examples, 668184 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:57,812 : INFO : EPOCH 5 - PROGRESS: at 85.72% examples, 668881 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:58,826 : INFO : EPOCH 5 - PROGRESS: at 88.75% examples, 669158 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:47:59,849 : INFO : EPOCH 5 - PROGRESS: at 91.72% examples, 668980 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:00,855 : INFO : EPOCH 5 - PROGRESS: at 94.56% examples, 667955 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:01,859 : INFO : EPOCH 5 - PROGRESS: at 97.55% examples, 668233 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:02,598 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:48:02,606 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:48:02,610 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:48:02,611 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:48:02,616 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:48:02,620 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:48:02,636 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:48:02,637 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:48:02,637 : INFO : EPOCH - 5 : training on 23279529 raw words (22951015 effective words) took 34.3s, 669411 effective words/s
    2020-09-30 22:48:03,647 : INFO : EPOCH 6 - PROGRESS: at 2.79% examples, 643214 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:04,679 : INFO : EPOCH 6 - PROGRESS: at 5.76% examples, 655007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:05,696 : INFO : EPOCH 6 - PROGRESS: at 8.55% examples, 648482 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:06,721 : INFO : EPOCH 6 - PROGRESS: at 11.26% examples, 641626 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:07,730 : INFO : EPOCH 6 - PROGRESS: at 13.95% examples, 634009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:08,748 : INFO : EPOCH 6 - PROGRESS: at 16.67% examples, 633106 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:09,771 : INFO : EPOCH 6 - PROGRESS: at 19.45% examples, 632048 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:10,771 : INFO : EPOCH 6 - PROGRESS: at 22.41% examples, 637689 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:11,774 : INFO : EPOCH 6 - PROGRESS: at 25.35% examples, 640747 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:12,784 : INFO : EPOCH 6 - PROGRESS: at 28.35% examples, 645383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:13,784 : INFO : EPOCH 6 - PROGRESS: at 31.26% examples, 647326 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:14,790 : INFO : EPOCH 6 - PROGRESS: at 34.32% examples, 650187 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:15,799 : INFO : EPOCH 6 - PROGRESS: at 37.41% examples, 653956 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:16,805 : INFO : EPOCH 6 - PROGRESS: at 40.36% examples, 655418 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:17,808 : INFO : EPOCH 6 - PROGRESS: at 43.43% examples, 658635 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:18,839 : INFO : EPOCH 6 - PROGRESS: at 46.38% examples, 657886 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:19,863 : INFO : EPOCH 6 - PROGRESS: at 49.38% examples, 659267 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:20,880 : INFO : EPOCH 6 - PROGRESS: at 52.36% examples, 659172 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:21,885 : INFO : EPOCH 6 - PROGRESS: at 55.33% examples, 660003 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:22,892 : INFO : EPOCH 6 - PROGRESS: at 58.24% examples, 661180 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:23,897 : INFO : EPOCH 6 - PROGRESS: at 61.05% examples, 660803 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:24,924 : INFO : EPOCH 6 - PROGRESS: at 64.14% examples, 661602 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:25,942 : INFO : EPOCH 6 - PROGRESS: at 67.12% examples, 661790 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:26,954 : INFO : EPOCH 6 - PROGRESS: at 70.03% examples, 661798 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:27,962 : INFO : EPOCH 6 - PROGRESS: at 72.77% examples, 660678 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:28,969 : INFO : EPOCH 6 - PROGRESS: at 75.14% examples, 656712 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:48:29,972 : INFO : EPOCH 6 - PROGRESS: at 77.57% examples, 652820 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:30,983 : INFO : EPOCH 6 - PROGRESS: at 80.25% examples, 651404 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:31,994 : INFO : EPOCH 6 - PROGRESS: at 83.26% examples, 652429 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:33,018 : INFO : EPOCH 6 - PROGRESS: at 86.47% examples, 654048 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:34,022 : INFO : EPOCH 6 - PROGRESS: at 89.63% examples, 656227 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:35,026 : INFO : EPOCH 6 - PROGRESS: at 92.72% examples, 657465 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:36,031 : INFO : EPOCH 6 - PROGRESS: at 95.86% examples, 658918 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:37,048 : INFO : EPOCH 6 - PROGRESS: at 98.92% examples, 659995 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:37,303 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:48:37,307 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:48:37,311 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:48:37,318 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:48:37,322 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:48:37,324 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:48:37,344 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:48:37,346 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:48:37,346 : INFO : EPOCH - 6 : training on 23279529 raw words (22951015 effective words) took 34.7s, 661277 effective words/s
    2020-09-30 22:48:38,357 : INFO : EPOCH 7 - PROGRESS: at 2.92% examples, 671649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:39,367 : INFO : EPOCH 7 - PROGRESS: at 6.01% examples, 690448 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:40,385 : INFO : EPOCH 7 - PROGRESS: at 9.07% examples, 694103 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:41,404 : INFO : EPOCH 7 - PROGRESS: at 12.37% examples, 707772 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:42,408 : INFO : EPOCH 7 - PROGRESS: at 15.45% examples, 706833 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:43,422 : INFO : EPOCH 7 - PROGRESS: at 18.69% examples, 713393 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:44,437 : INFO : EPOCH 7 - PROGRESS: at 21.80% examples, 712436 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:45,469 : INFO : EPOCH 7 - PROGRESS: at 25.02% examples, 711421 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:46,472 : INFO : EPOCH 7 - PROGRESS: at 27.95% examples, 707251 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:47,473 : INFO : EPOCH 7 - PROGRESS: at 31.22% examples, 711629 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:48,488 : INFO : EPOCH 7 - PROGRESS: at 34.36% examples, 710048 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:49,491 : INFO : EPOCH 7 - PROGRESS: at 37.63% examples, 712774 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:50,493 : INFO : EPOCH 7 - PROGRESS: at 40.64% examples, 711449 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:51,506 : INFO : EPOCH 7 - PROGRESS: at 43.90% examples, 713189 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:52,506 : INFO : EPOCH 7 - PROGRESS: at 46.87% examples, 710743 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:48:53,508 : INFO : EPOCH 7 - PROGRESS: at 50.02% examples, 711029 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:54,531 : INFO : EPOCH 7 - PROGRESS: at 53.08% examples, 709366 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:55,548 : INFO : EPOCH 7 - PROGRESS: at 56.34% examples, 711216 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:56,579 : INFO : EPOCH 7 - PROGRESS: at 59.50% examples, 711786 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:57,589 : INFO : EPOCH 7 - PROGRESS: at 62.72% examples, 712657 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:58,592 : INFO : EPOCH 7 - PROGRESS: at 65.89% examples, 712725 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:48:59,619 : INFO : EPOCH 7 - PROGRESS: at 69.06% examples, 712514 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:00,620 : INFO : EPOCH 7 - PROGRESS: at 72.18% examples, 713126 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:01,620 : INFO : EPOCH 7 - PROGRESS: at 75.26% examples, 713614 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:02,631 : INFO : EPOCH 7 - PROGRESS: at 78.43% examples, 713454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:03,668 : INFO : EPOCH 7 - PROGRESS: at 81.61% examples, 712930 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:04,690 : INFO : EPOCH 7 - PROGRESS: at 84.70% examples, 712156 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:05,693 : INFO : EPOCH 7 - PROGRESS: at 87.97% examples, 713284 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:06,704 : INFO : EPOCH 7 - PROGRESS: at 91.08% examples, 712810 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:07,717 : INFO : EPOCH 7 - PROGRESS: at 94.32% examples, 713291 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:08,731 : INFO : EPOCH 7 - PROGRESS: at 97.34% examples, 711930 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:09,493 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:49:09,501 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:49:09,505 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:49:09,507 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:49:09,510 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:49:09,514 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:49:09,527 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:49:09,529 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:49:09,529 : INFO : EPOCH - 7 : training on 23279529 raw words (22951015 effective words) took 32.2s, 713188 effective words/s
    2020-09-30 22:49:10,535 : INFO : EPOCH 8 - PROGRESS: at 2.87% examples, 665635 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:11,539 : INFO : EPOCH 8 - PROGRESS: at 5.98% examples, 689634 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:12,549 : INFO : EPOCH 8 - PROGRESS: at 9.03% examples, 695436 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:13,550 : INFO : EPOCH 8 - PROGRESS: at 12.29% examples, 709441 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:14,573 : INFO : EPOCH 8 - PROGRESS: at 15.42% examples, 707581 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:15,589 : INFO : EPOCH 8 - PROGRESS: at 18.64% examples, 713731 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:16,592 : INFO : EPOCH 8 - PROGRESS: at 21.70% examples, 712612 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:17,611 : INFO : EPOCH 8 - PROGRESS: at 25.02% examples, 714942 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:18,615 : INFO : EPOCH 8 - PROGRESS: at 28.11% examples, 714544 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:19,625 : INFO : EPOCH 8 - PROGRESS: at 31.18% examples, 712909 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:20,634 : INFO : EPOCH 8 - PROGRESS: at 34.36% examples, 712468 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:21,651 : INFO : EPOCH 8 - PROGRESS: at 37.62% examples, 714104 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:22,662 : INFO : EPOCH 8 - PROGRESS: at 40.71% examples, 713678 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:23,676 : INFO : EPOCH 8 - PROGRESS: at 43.94% examples, 714489 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:24,690 : INFO : EPOCH 8 - PROGRESS: at 47.08% examples, 713963 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:49:25,692 : INFO : EPOCH 8 - PROGRESS: at 50.36% examples, 715845 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:26,712 : INFO : EPOCH 8 - PROGRESS: at 53.62% examples, 716728 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:27,720 : INFO : EPOCH 8 - PROGRESS: at 56.84% examples, 718051 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:28,723 : INFO : EPOCH 8 - PROGRESS: at 59.82% examples, 717243 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:29,742 : INFO : EPOCH 8 - PROGRESS: at 63.12% examples, 718031 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:30,768 : INFO : EPOCH 8 - PROGRESS: at 66.23% examples, 716591 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:31,776 : INFO : EPOCH 8 - PROGRESS: at 69.36% examples, 716839 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:32,778 : INFO : EPOCH 8 - PROGRESS: at 72.56% examples, 717637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:33,811 : INFO : EPOCH 8 - PROGRESS: at 75.81% examples, 718575 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:34,823 : INFO : EPOCH 8 - PROGRESS: at 79.07% examples, 718954 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:35,862 : INFO : EPOCH 8 - PROGRESS: at 82.30% examples, 718542 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:36,880 : INFO : EPOCH 8 - PROGRESS: at 85.38% examples, 717667 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:37,915 : INFO : EPOCH 8 - PROGRESS: at 88.40% examples, 715722 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:38,925 : INFO : EPOCH 8 - PROGRESS: at 91.54% examples, 715524 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:39,939 : INFO : EPOCH 8 - PROGRESS: at 94.56% examples, 713995 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:40,942 : INFO : EPOCH 8 - PROGRESS: at 97.83% examples, 715001 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:41,557 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:49:41,558 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:49:41,564 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:49:41,569 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:49:41,570 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:49:41,571 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:49:41,577 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:49:41,582 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:49:41,582 : INFO : EPOCH - 8 : training on 23279529 raw words (22951015 effective words) took 32.1s, 716082 effective words/s
    2020-09-30 22:49:42,599 : INFO : EPOCH 9 - PROGRESS: at 3.05% examples, 697016 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:43,600 : INFO : EPOCH 9 - PROGRESS: at 6.27% examples, 720746 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:44,618 : INFO : EPOCH 9 - PROGRESS: at 9.40% examples, 720241 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:45,644 : INFO : EPOCH 9 - PROGRESS: at 12.71% examples, 726004 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:46,659 : INFO : EPOCH 9 - PROGRESS: at 15.91% examples, 725848 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:47,683 : INFO : EPOCH 9 - PROGRESS: at 19.12% examples, 726525 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:49:48,700 : INFO : EPOCH 9 - PROGRESS: at 22.41% examples, 728896 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:49,728 : INFO : EPOCH 9 - PROGRESS: at 25.68% examples, 728237 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:50,730 : INFO : EPOCH 9 - PROGRESS: at 28.88% examples, 729664 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:51,739 : INFO : EPOCH 9 - PROGRESS: at 32.15% examples, 730401 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:49:52,758 : INFO : EPOCH 9 - PROGRESS: at 35.39% examples, 729524 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:53,785 : INFO : EPOCH 9 - PROGRESS: at 38.66% examples, 729307 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:54,796 : INFO : EPOCH 9 - PROGRESS: at 41.87% examples, 729150 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:55,817 : INFO : EPOCH 9 - PROGRESS: at 45.11% examples, 729228 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:56,825 : INFO : EPOCH 9 - PROGRESS: at 48.39% examples, 729806 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:57,832 : INFO : EPOCH 9 - PROGRESS: at 51.57% examples, 728729 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:58,841 : INFO : EPOCH 9 - PROGRESS: at 54.81% examples, 729387 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:49:59,843 : INFO : EPOCH 9 - PROGRESS: at 57.94% examples, 729703 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:00,851 : INFO : EPOCH 9 - PROGRESS: at 61.18% examples, 730641 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:01,851 : INFO : EPOCH 9 - PROGRESS: at 64.28% examples, 728955 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:02,869 : INFO : EPOCH 9 - PROGRESS: at 67.62% examples, 730020 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:03,879 : INFO : EPOCH 9 - PROGRESS: at 70.73% examples, 729156 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:04,880 : INFO : EPOCH 9 - PROGRESS: at 73.84% examples, 729426 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:05,888 : INFO : EPOCH 9 - PROGRESS: at 77.03% examples, 729418 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:06,907 : INFO : EPOCH 9 - PROGRESS: at 80.24% examples, 729133 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:07,940 : INFO : EPOCH 9 - PROGRESS: at 83.62% examples, 729637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:08,942 : INFO : EPOCH 9 - PROGRESS: at 86.83% examples, 729490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:09,947 : INFO : EPOCH 9 - PROGRESS: at 90.02% examples, 729523 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:10,953 : INFO : EPOCH 9 - PROGRESS: at 93.36% examples, 729997 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:11,962 : INFO : EPOCH 9 - PROGRESS: at 96.58% examples, 729761 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:12,950 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:50:12,956 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:50:12,961 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:50:12,962 : INFO : EPOCH 9 - PROGRESS: at 99.81% examples, 730237 words/s, in_qsize 4, out_qsize 1
    2020-09-30 22:50:12,962 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:50:12,966 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:50:12,972 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:50:12,977 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:50:12,979 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:50:12,979 : INFO : EPOCH - 9 : training on 23279529 raw words (22951015 effective words) took 31.4s, 731056 effective words/s
    2020-09-30 22:50:14,009 : INFO : EPOCH 10 - PROGRESS: at 2.95% examples, 668769 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:15,012 : INFO : EPOCH 10 - PROGRESS: at 6.05% examples, 691135 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:16,028 : INFO : EPOCH 10 - PROGRESS: at 9.07% examples, 691904 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:17,031 : INFO : EPOCH 10 - PROGRESS: at 12.33% examples, 706421 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:18,045 : INFO : EPOCH 10 - PROGRESS: at 15.49% examples, 708224 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:19,069 : INFO : EPOCH 10 - PROGRESS: at 18.69% examples, 711852 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:20,077 : INFO : EPOCH 10 - PROGRESS: at 21.70% examples, 709062 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:21,108 : INFO : EPOCH 10 - PROGRESS: at 25.02% examples, 710803 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:22,115 : INFO : EPOCH 10 - PROGRESS: at 28.20% examples, 712801 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:23,118 : INFO : EPOCH 10 - PROGRESS: at 31.56% examples, 718415 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:24,136 : INFO : EPOCH 10 - PROGRESS: at 34.72% examples, 716960 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:25,149 : INFO : EPOCH 10 - PROGRESS: at 38.08% examples, 720074 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:26,154 : INFO : EPOCH 10 - PROGRESS: at 41.17% examples, 719558 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:27,159 : INFO : EPOCH 10 - PROGRESS: at 44.48% examples, 721740 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:28,212 : INFO : EPOCH 10 - PROGRESS: at 47.69% examples, 720163 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:29,216 : INFO : EPOCH 10 - PROGRESS: at 50.92% examples, 720350 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:30,217 : INFO : EPOCH 10 - PROGRESS: at 54.01% examples, 719488 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:31,220 : INFO : EPOCH 10 - PROGRESS: at 57.22% examples, 720901 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:32,224 : INFO : EPOCH 10 - PROGRESS: at 60.28% examples, 720898 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:33,235 : INFO : EPOCH 10 - PROGRESS: at 63.60% examples, 721729 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:34,243 : INFO : EPOCH 10 - PROGRESS: at 66.82% examples, 722101 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:35,276 : INFO : EPOCH 10 - PROGRESS: at 70.07% examples, 722196 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:36,305 : INFO : EPOCH 10 - PROGRESS: at 73.16% examples, 721471 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:37,319 : INFO : EPOCH 10 - PROGRESS: at 76.28% examples, 721273 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:38,366 : INFO : EPOCH 10 - PROGRESS: at 79.23% examples, 717852 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:39,367 : INFO : EPOCH 10 - PROGRESS: at 82.16% examples, 715942 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:40,367 : INFO : EPOCH 10 - PROGRESS: at 85.10% examples, 714184 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:41,374 : INFO : EPOCH 10 - PROGRESS: at 88.27% examples, 714483 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:42,389 : INFO : EPOCH 10 - PROGRESS: at 91.39% examples, 713874 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:43,393 : INFO : EPOCH 10 - PROGRESS: at 94.60% examples, 714219 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:44,398 : INFO : EPOCH 10 - PROGRESS: at 97.88% examples, 715161 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:44,985 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:50:44,992 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:50:44,995 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:50:44,996 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:50:45,000 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:50:45,001 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:50:45,016 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:50:45,020 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:50:45,020 : INFO : EPOCH - 10 : training on 23279529 raw words (22951015 effective words) took 32.0s, 716354 effective words/s
    2020-09-30 22:50:46,039 : INFO : EPOCH 11 - PROGRESS: at 2.91% examples, 666098 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:47,069 : INFO : EPOCH 11 - PROGRESS: at 6.05% examples, 685636 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:48,070 : INFO : EPOCH 11 - PROGRESS: at 9.07% examples, 691604 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:49,071 : INFO : EPOCH 11 - PROGRESS: at 12.15% examples, 696932 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:50,074 : INFO : EPOCH 11 - PROGRESS: at 15.17% examples, 694565 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:51,091 : INFO : EPOCH 11 - PROGRESS: at 18.36% examples, 701278 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:52,103 : INFO : EPOCH 11 - PROGRESS: at 21.58% examples, 706333 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:53,107 : INFO : EPOCH 11 - PROGRESS: at 24.87% examples, 709850 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:54,117 : INFO : EPOCH 11 - PROGRESS: at 27.87% examples, 707297 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:50:55,135 : INFO : EPOCH 11 - PROGRESS: at 31.03% examples, 708640 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:56,162 : INFO : EPOCH 11 - PROGRESS: at 34.28% examples, 708296 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:57,164 : INFO : EPOCH 11 - PROGRESS: at 37.41% examples, 708820 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:50:58,175 : INFO : EPOCH 11 - PROGRESS: at 40.60% examples, 710242 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:50:59,188 : INFO : EPOCH 11 - PROGRESS: at 43.86% examples, 712081 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:00,192 : INFO : EPOCH 11 - PROGRESS: at 47.03% examples, 712782 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:01,222 : INFO : EPOCH 11 - PROGRESS: at 50.36% examples, 714106 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:02,233 : INFO : EPOCH 11 - PROGRESS: at 53.43% examples, 712696 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:03,254 : INFO : EPOCH 11 - PROGRESS: at 56.60% examples, 713177 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:04,276 : INFO : EPOCH 11 - PROGRESS: at 59.60% examples, 712418 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:05,321 : INFO : EPOCH 11 - PROGRESS: at 62.76% examples, 711110 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:06,331 : INFO : EPOCH 11 - PROGRESS: at 65.93% examples, 711018 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:07,354 : INFO : EPOCH 11 - PROGRESS: at 69.06% examples, 710543 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:08,372 : INFO : EPOCH 11 - PROGRESS: at 72.18% examples, 710742 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:51:09,382 : INFO : EPOCH 11 - PROGRESS: at 75.31% examples, 711452 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:10,386 : INFO : EPOCH 11 - PROGRESS: at 78.52% examples, 711934 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:11,392 : INFO : EPOCH 11 - PROGRESS: at 81.78% examples, 713080 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:12,393 : INFO : EPOCH 11 - PROGRESS: at 84.87% examples, 712802 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:13,444 : INFO : EPOCH 11 - PROGRESS: at 88.05% examples, 712044 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:51:14,448 : INFO : EPOCH 11 - PROGRESS: at 91.26% examples, 712443 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:15,452 : INFO : EPOCH 11 - PROGRESS: at 94.37% examples, 712194 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:16,452 : INFO : EPOCH 11 - PROGRESS: at 97.47% examples, 711794 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:17,188 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:51:17,197 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:51:17,198 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:51:17,200 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:51:17,202 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:51:17,207 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:51:17,218 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:51:17,219 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:51:17,219 : INFO : EPOCH - 11 : training on 23279529 raw words (22951015 effective words) took 32.2s, 712836 effective words/s
    2020-09-30 22:51:18,223 : INFO : EPOCH 12 - PROGRESS: at 2.95% examples, 686168 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:51:19,224 : INFO : EPOCH 12 - PROGRESS: at 6.05% examples, 700997 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:51:20,266 : INFO : EPOCH 12 - PROGRESS: at 9.37% examples, 714515 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:21,272 : INFO : EPOCH 12 - PROGRESS: at 12.54% examples, 718077 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:22,284 : INFO : EPOCH 12 - PROGRESS: at 15.62% examples, 714103 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:23,334 : INFO : EPOCH 12 - PROGRESS: at 18.76% examples, 712133 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:24,343 : INFO : EPOCH 12 - PROGRESS: at 21.97% examples, 714635 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:25,349 : INFO : EPOCH 12 - PROGRESS: at 25.02% examples, 710774 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:26,357 : INFO : EPOCH 12 - PROGRESS: at 28.15% examples, 711565 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:27,366 : INFO : EPOCH 12 - PROGRESS: at 31.26% examples, 711249 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:28,391 : INFO : EPOCH 12 - PROGRESS: at 34.40% examples, 709010 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:29,412 : INFO : EPOCH 12 - PROGRESS: at 37.63% examples, 709941 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:30,428 : INFO : EPOCH 12 - PROGRESS: at 40.71% examples, 709561 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:51:31,433 : INFO : EPOCH 12 - PROGRESS: at 43.94% examples, 711113 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:32,441 : INFO : EPOCH 12 - PROGRESS: at 47.08% examples, 711067 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:33,465 : INFO : EPOCH 12 - PROGRESS: at 50.44% examples, 713361 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:34,467 : INFO : EPOCH 12 - PROGRESS: at 53.59% examples, 713490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:35,482 : INFO : EPOCH 12 - PROGRESS: at 56.51% examples, 710984 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:36,493 : INFO : EPOCH 12 - PROGRESS: at 59.53% examples, 710779 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:37,501 : INFO : EPOCH 12 - PROGRESS: at 62.72% examples, 711302 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:38,518 : INFO : EPOCH 12 - PROGRESS: at 65.81% examples, 710086 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:39,522 : INFO : EPOCH 12 - PROGRESS: at 68.86% examples, 709389 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:40,525 : INFO : EPOCH 12 - PROGRESS: at 71.95% examples, 709628 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:41,527 : INFO : EPOCH 12 - PROGRESS: at 75.02% examples, 710212 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:42,537 : INFO : EPOCH 12 - PROGRESS: at 78.14% examples, 709861 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:43,569 : INFO : EPOCH 12 - PROGRESS: at 81.26% examples, 709262 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:51:44,584 : INFO : EPOCH 12 - PROGRESS: at 84.54% examples, 710225 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:45,589 : INFO : EPOCH 12 - PROGRESS: at 87.72% examples, 710637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:46,605 : INFO : EPOCH 12 - PROGRESS: at 90.99% examples, 711478 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:47,625 : INFO : EPOCH 12 - PROGRESS: at 94.11% examples, 710887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:48,628 : INFO : EPOCH 12 - PROGRESS: at 97.26% examples, 710768 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:49,470 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:51:49,473 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:51:49,477 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:51:49,477 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:51:49,482 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:51:49,485 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:51:49,502 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:51:49,502 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:51:49,502 : INFO : EPOCH - 12 : training on 23279529 raw words (22951015 effective words) took 32.3s, 710980 effective words/s
    2020-09-30 22:51:50,513 : INFO : EPOCH 13 - PROGRESS: at 3.00% examples, 690974 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:51,515 : INFO : EPOCH 13 - PROGRESS: at 6.01% examples, 693466 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:52,529 : INFO : EPOCH 13 - PROGRESS: at 9.03% examples, 693700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:53,532 : INFO : EPOCH 13 - PROGRESS: at 12.11% examples, 698318 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:54,557 : INFO : EPOCH 13 - PROGRESS: at 15.25% examples, 698352 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:55,563 : INFO : EPOCH 13 - PROGRESS: at 18.36% examples, 702485 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:56,580 : INFO : EPOCH 13 - PROGRESS: at 21.42% examples, 701455 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:51:57,597 : INFO : EPOCH 13 - PROGRESS: at 24.66% examples, 703133 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:58,610 : INFO : EPOCH 13 - PROGRESS: at 27.69% examples, 702249 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:51:59,625 : INFO : EPOCH 13 - PROGRESS: at 30.81% examples, 703266 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:00,651 : INFO : EPOCH 13 - PROGRESS: at 33.99% examples, 701851 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:01,654 : INFO : EPOCH 13 - PROGRESS: at 37.07% examples, 701969 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:02,667 : INFO : EPOCH 13 - PROGRESS: at 40.24% examples, 703241 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:03,674 : INFO : EPOCH 13 - PROGRESS: at 43.31% examples, 703059 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:04,674 : INFO : EPOCH 13 - PROGRESS: at 46.42% examples, 703201 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:05,684 : INFO : EPOCH 13 - PROGRESS: at 49.47% examples, 703007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:06,685 : INFO : EPOCH 13 - PROGRESS: at 52.62% examples, 703234 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:07,704 : INFO : EPOCH 13 - PROGRESS: at 55.67% examples, 702742 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:08,717 : INFO : EPOCH 13 - PROGRESS: at 58.73% examples, 702985 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:09,739 : INFO : EPOCH 13 - PROGRESS: at 61.88% examples, 703282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:10,740 : INFO : EPOCH 13 - PROGRESS: at 65.09% examples, 704365 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:11,747 : INFO : EPOCH 13 - PROGRESS: at 68.03% examples, 702941 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:12,754 : INFO : EPOCH 13 - PROGRESS: at 71.17% examples, 703762 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:13,779 : INFO : EPOCH 13 - PROGRESS: at 74.14% examples, 702789 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:14,784 : INFO : EPOCH 13 - PROGRESS: at 77.33% examples, 703937 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:15,791 : INFO : EPOCH 13 - PROGRESS: at 80.25% examples, 702395 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:16,794 : INFO : EPOCH 13 - PROGRESS: at 83.53% examples, 703934 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:17,796 : INFO : EPOCH 13 - PROGRESS: at 86.59% examples, 703336 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:18,811 : INFO : EPOCH 13 - PROGRESS: at 89.70% examples, 703382 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:19,816 : INFO : EPOCH 13 - PROGRESS: at 92.76% examples, 702807 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:20,820 : INFO : EPOCH 13 - PROGRESS: at 95.91% examples, 702922 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:21,860 : INFO : EPOCH 13 - PROGRESS: at 98.97% examples, 702191 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:22,084 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:52:22,091 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:52:22,095 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:52:22,096 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:52:22,105 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:52:22,106 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:52:22,117 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:52:22,119 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:52:22,119 : INFO : EPOCH - 13 : training on 23279529 raw words (22951015 effective words) took 32.6s, 703714 effective words/s
    2020-09-30 22:52:23,134 : INFO : EPOCH 14 - PROGRESS: at 3.00% examples, 687998 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:24,138 : INFO : EPOCH 14 - PROGRESS: at 6.01% examples, 691254 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:25,150 : INFO : EPOCH 14 - PROGRESS: at 9.07% examples, 696072 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:26,160 : INFO : EPOCH 14 - PROGRESS: at 12.15% examples, 698753 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:27,160 : INFO : EPOCH 14 - PROGRESS: at 15.34% examples, 704003 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:28,208 : INFO : EPOCH 14 - PROGRESS: at 18.45% examples, 702261 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:29,248 : INFO : EPOCH 14 - PROGRESS: at 21.67% examples, 704569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:30,264 : INFO : EPOCH 14 - PROGRESS: at 24.71% examples, 699976 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:31,266 : INFO : EPOCH 14 - PROGRESS: at 27.73% examples, 700235 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:32,278 : INFO : EPOCH 14 - PROGRESS: at 30.81% examples, 700797 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:33,288 : INFO : EPOCH 14 - PROGRESS: at 33.99% examples, 700523 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:34,317 : INFO : EPOCH 14 - PROGRESS: at 37.07% examples, 699314 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:35,317 : INFO : EPOCH 14 - PROGRESS: at 40.24% examples, 701489 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:36,324 : INFO : EPOCH 14 - PROGRESS: at 43.26% examples, 700715 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:37,331 : INFO : EPOCH 14 - PROGRESS: at 46.38% examples, 700724 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:38,337 : INFO : EPOCH 14 - PROGRESS: at 49.42% examples, 700846 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:39,351 : INFO : EPOCH 14 - PROGRESS: at 52.54% examples, 700071 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:40,353 : INFO : EPOCH 14 - PROGRESS: at 55.52% examples, 699379 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:41,358 : INFO : EPOCH 14 - PROGRESS: at 58.60% examples, 700625 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:42,366 : INFO : EPOCH 14 - PROGRESS: at 61.67% examples, 700513 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:43,387 : INFO : EPOCH 14 - PROGRESS: at 64.84% examples, 700637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:44,388 : INFO : EPOCH 14 - PROGRESS: at 67.91% examples, 700850 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:45,402 : INFO : EPOCH 14 - PROGRESS: at 71.01% examples, 701189 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:46,422 : INFO : EPOCH 14 - PROGRESS: at 73.84% examples, 699248 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:47,423 : INFO : EPOCH 14 - PROGRESS: at 76.86% examples, 699128 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:48,424 : INFO : EPOCH 14 - PROGRESS: at 79.96% examples, 699371 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:49,428 : INFO : EPOCH 14 - PROGRESS: at 83.09% examples, 699933 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:50,433 : INFO : EPOCH 14 - PROGRESS: at 86.12% examples, 699076 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:51,437 : INFO : EPOCH 14 - PROGRESS: at 89.28% examples, 699865 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:52,448 : INFO : EPOCH 14 - PROGRESS: at 92.42% examples, 699893 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:52:53,458 : INFO : EPOCH 14 - PROGRESS: at 95.56% examples, 699964 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:54,480 : INFO : EPOCH 14 - PROGRESS: at 98.58% examples, 699418 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:54,882 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:52:54,891 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:52:54,898 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:52:54,900 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:52:54,901 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:52:54,902 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:52:54,916 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:52:54,922 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:52:54,923 : INFO : EPOCH - 14 : training on 23279529 raw words (22951015 effective words) took 32.8s, 699694 effective words/s
    2020-09-30 22:52:55,931 : INFO : EPOCH 15 - PROGRESS: at 2.70% examples, 624746 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:56,943 : INFO : EPOCH 15 - PROGRESS: at 5.45% examples, 628334 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:57,967 : INFO : EPOCH 15 - PROGRESS: at 8.55% examples, 651732 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:58,978 : INFO : EPOCH 15 - PROGRESS: at 11.64% examples, 667759 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:52:59,985 : INFO : EPOCH 15 - PROGRESS: at 14.74% examples, 674127 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:00,990 : INFO : EPOCH 15 - PROGRESS: at 17.80% examples, 680707 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:01,993 : INFO : EPOCH 15 - PROGRESS: at 20.98% examples, 688603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:02,996 : INFO : EPOCH 15 - PROGRESS: at 24.06% examples, 688036 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:04,001 : INFO : EPOCH 15 - PROGRESS: at 27.16% examples, 690733 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:05,020 : INFO : EPOCH 15 - PROGRESS: at 30.19% examples, 690764 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:06,030 : INFO : EPOCH 15 - PROGRESS: at 33.33% examples, 691330 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:07,032 : INFO : EPOCH 15 - PROGRESS: at 36.48% examples, 693240 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:08,036 : INFO : EPOCH 15 - PROGRESS: at 39.59% examples, 694884 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:09,062 : INFO : EPOCH 15 - PROGRESS: at 42.53% examples, 692406 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:10,066 : INFO : EPOCH 15 - PROGRESS: at 45.61% examples, 693083 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:11,095 : INFO : EPOCH 15 - PROGRESS: at 48.71% examples, 692625 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:12,107 : INFO : EPOCH 15 - PROGRESS: at 51.83% examples, 692500 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:13,108 : INFO : EPOCH 15 - PROGRESS: at 54.89% examples, 693309 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:14,115 : INFO : EPOCH 15 - PROGRESS: at 57.94% examples, 694263 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:15,126 : INFO : EPOCH 15 - PROGRESS: at 61.05% examples, 695396 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:16,133 : INFO : EPOCH 15 - PROGRESS: at 64.14% examples, 695209 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:17,138 : INFO : EPOCH 15 - PROGRESS: at 67.29% examples, 695978 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:18,169 : INFO : EPOCH 15 - PROGRESS: at 70.40% examples, 696027 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:53:19,170 : INFO : EPOCH 15 - PROGRESS: at 73.48% examples, 697257 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:20,178 : INFO : EPOCH 15 - PROGRESS: at 76.53% examples, 697432 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:21,179 : INFO : EPOCH 15 - PROGRESS: at 79.69% examples, 698130 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:22,180 : INFO : EPOCH 15 - PROGRESS: at 82.72% examples, 697694 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:23,191 : INFO : EPOCH 15 - PROGRESS: at 85.84% examples, 698154 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:24,216 : INFO : EPOCH 15 - PROGRESS: at 88.74% examples, 696161 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:25,216 : INFO : EPOCH 15 - PROGRESS: at 91.72% examples, 695616 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:26,222 : INFO : EPOCH 15 - PROGRESS: at 94.87% examples, 695870 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:27,231 : INFO : EPOCH 15 - PROGRESS: at 98.00% examples, 696383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:27,812 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:53:27,816 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:53:27,817 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:53:27,822 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:53:27,825 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:53:27,826 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:53:27,844 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:53:27,846 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:53:27,846 : INFO : EPOCH - 15 : training on 23279529 raw words (22951015 effective words) took 32.9s, 697158 effective words/s
    2020-09-30 22:53:28,860 : INFO : EPOCH 16 - PROGRESS: at 3.00% examples, 688568 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:29,893 : INFO : EPOCH 16 - PROGRESS: at 6.05% examples, 686386 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:30,904 : INFO : EPOCH 16 - PROGRESS: at 9.20% examples, 699415 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:31,916 : INFO : EPOCH 16 - PROGRESS: at 12.28% examples, 700809 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:32,919 : INFO : EPOCH 16 - PROGRESS: at 15.41% examples, 703469 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:33,951 : INFO : EPOCH 16 - PROGRESS: at 18.45% examples, 700564 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:34,951 : INFO : EPOCH 16 - PROGRESS: at 21.39% examples, 697439 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:35,973 : INFO : EPOCH 16 - PROGRESS: at 24.40% examples, 693131 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:36,975 : INFO : EPOCH 16 - PROGRESS: at 27.52% examples, 696324 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:37,989 : INFO : EPOCH 16 - PROGRESS: at 30.57% examples, 696226 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:39,005 : INFO : EPOCH 16 - PROGRESS: at 33.66% examples, 695067 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:40,013 : INFO : EPOCH 16 - PROGRESS: at 36.74% examples, 694697 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:41,015 : INFO : EPOCH 16 - PROGRESS: at 39.89% examples, 697111 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:42,023 : INFO : EPOCH 16 - PROGRESS: at 42.88% examples, 696028 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:43,031 : INFO : EPOCH 16 - PROGRESS: at 46.03% examples, 696839 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:44,039 : INFO : EPOCH 16 - PROGRESS: at 49.09% examples, 697138 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:45,040 : INFO : EPOCH 16 - PROGRESS: at 52.15% examples, 696579 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:46,051 : INFO : EPOCH 16 - PROGRESS: at 55.20% examples, 696208 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:47,053 : INFO : EPOCH 16 - PROGRESS: at 58.28% examples, 697749 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:48,056 : INFO : EPOCH 16 - PROGRESS: at 61.32% examples, 698001 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:49,088 : INFO : EPOCH 16 - PROGRESS: at 64.50% examples, 697831 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:50,089 : INFO : EPOCH 16 - PROGRESS: at 67.54% examples, 697717 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:51,092 : INFO : EPOCH 16 - PROGRESS: at 70.61% examples, 698119 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:52,094 : INFO : EPOCH 16 - PROGRESS: at 73.57% examples, 698009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:53,096 : INFO : EPOCH 16 - PROGRESS: at 76.74% examples, 699459 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:54,115 : INFO : EPOCH 16 - PROGRESS: at 79.76% examples, 698536 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:53:55,129 : INFO : EPOCH 16 - PROGRESS: at 82.80% examples, 697744 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:56,133 : INFO : EPOCH 16 - PROGRESS: at 85.89% examples, 698027 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:57,177 : INFO : EPOCH 16 - PROGRESS: at 89.05% examples, 697897 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:53:58,201 : INFO : EPOCH 16 - PROGRESS: at 92.27% examples, 698340 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:53:59,207 : INFO : EPOCH 16 - PROGRESS: at 95.49% examples, 698856 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:00,228 : INFO : EPOCH 16 - PROGRESS: at 98.58% examples, 698977 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:00,602 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:54:00,609 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:54:00,610 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:54:00,611 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:54:00,618 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:54:00,620 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:54:00,633 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:54:00,636 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:54:00,637 : INFO : EPOCH - 16 : training on 23279529 raw words (22951015 effective words) took 32.8s, 699972 effective words/s
    2020-09-30 22:54:01,641 : INFO : EPOCH 17 - PROGRESS: at 2.87% examples, 666568 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:02,647 : INFO : EPOCH 17 - PROGRESS: at 6.01% examples, 694213 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:03,661 : INFO : EPOCH 17 - PROGRESS: at 9.03% examples, 694199 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:04,688 : INFO : EPOCH 17 - PROGRESS: at 12.02% examples, 689782 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:05,703 : INFO : EPOCH 17 - PROGRESS: at 15.09% examples, 689026 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:06,726 : INFO : EPOCH 17 - PROGRESS: at 18.09% examples, 689490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:07,728 : INFO : EPOCH 17 - PROGRESS: at 21.19% examples, 693268 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:08,734 : INFO : EPOCH 17 - PROGRESS: at 24.32% examples, 693258 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:09,747 : INFO : EPOCH 17 - PROGRESS: at 27.41% examples, 694675 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:10,762 : INFO : EPOCH 17 - PROGRESS: at 30.57% examples, 697416 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:11,772 : INFO : EPOCH 17 - PROGRESS: at 33.66% examples, 696529 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:12,775 : INFO : EPOCH 17 - PROGRESS: at 36.78% examples, 697202 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:13,782 : INFO : EPOCH 17 - PROGRESS: at 39.72% examples, 695407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:14,806 : INFO : EPOCH 17 - PROGRESS: at 42.88% examples, 696381 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:15,832 : INFO : EPOCH 17 - PROGRESS: at 46.03% examples, 696381 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:16,834 : INFO : EPOCH 17 - PROGRESS: at 49.13% examples, 697554 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:17,851 : INFO : EPOCH 17 - PROGRESS: at 52.27% examples, 697422 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:18,857 : INFO : EPOCH 17 - PROGRESS: at 55.45% examples, 698825 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:19,879 : INFO : EPOCH 17 - PROGRESS: at 58.39% examples, 697953 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:20,886 : INFO : EPOCH 17 - PROGRESS: at 61.41% examples, 697618 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:21,928 : INFO : EPOCH 17 - PROGRESS: at 64.50% examples, 696238 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:22,957 : INFO : EPOCH 17 - PROGRESS: at 67.70% examples, 697065 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:23,992 : INFO : EPOCH 17 - PROGRESS: at 70.85% examples, 697374 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:24,998 : INFO : EPOCH 17 - PROGRESS: at 73.92% examples, 698364 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:26,003 : INFO : EPOCH 17 - PROGRESS: at 77.07% examples, 699300 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:27,009 : INFO : EPOCH 17 - PROGRESS: at 80.17% examples, 699424 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:28,023 : INFO : EPOCH 17 - PROGRESS: at 83.26% examples, 699399 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:29,056 : INFO : EPOCH 17 - PROGRESS: at 86.38% examples, 698549 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:30,073 : INFO : EPOCH 17 - PROGRESS: at 89.41% examples, 698041 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:31,083 : INFO : EPOCH 17 - PROGRESS: at 92.60% examples, 698495 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:32,095 : INFO : EPOCH 17 - PROGRESS: at 95.70% examples, 698237 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:33,111 : INFO : EPOCH 17 - PROGRESS: at 98.92% examples, 699364 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:33,380 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:54:33,390 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:54:33,399 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:54:33,400 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:54:33,404 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:54:33,404 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:54:33,420 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:54:33,423 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:54:33,423 : INFO : EPOCH - 17 : training on 23279529 raw words (22951015 effective words) took 32.8s, 700071 effective words/s
    2020-09-30 22:54:34,439 : INFO : EPOCH 18 - PROGRESS: at 2.79% examples, 639168 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:35,450 : INFO : EPOCH 18 - PROGRESS: at 5.92% examples, 679215 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:36,473 : INFO : EPOCH 18 - PROGRESS: at 8.89% examples, 675879 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:37,487 : INFO : EPOCH 18 - PROGRESS: at 11.68% examples, 668776 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:38,539 : INFO : EPOCH 18 - PROGRESS: at 14.41% examples, 652013 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:39,561 : INFO : EPOCH 18 - PROGRESS: at 17.11% examples, 647596 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:40,576 : INFO : EPOCH 18 - PROGRESS: at 19.80% examples, 642547 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:54:41,589 : INFO : EPOCH 18 - PROGRESS: at 22.66% examples, 642339 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:42,611 : INFO : EPOCH 18 - PROGRESS: at 25.68% examples, 645557 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:43,616 : INFO : EPOCH 18 - PROGRESS: at 28.73% examples, 650996 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:44,631 : INFO : EPOCH 18 - PROGRESS: at 31.77% examples, 654206 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:45,634 : INFO : EPOCH 18 - PROGRESS: at 34.89% examples, 658246 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:46,639 : INFO : EPOCH 18 - PROGRESS: at 37.98% examples, 661594 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:47,660 : INFO : EPOCH 18 - PROGRESS: at 41.00% examples, 663091 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:54:48,670 : INFO : EPOCH 18 - PROGRESS: at 43.64% examples, 658543 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:49,677 : INFO : EPOCH 18 - PROGRESS: at 46.38% examples, 655791 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:50,693 : INFO : EPOCH 18 - PROGRESS: at 49.05% examples, 653096 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:51,705 : INFO : EPOCH 18 - PROGRESS: at 51.71% examples, 649329 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:52,714 : INFO : EPOCH 18 - PROGRESS: at 54.25% examples, 645947 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:53,718 : INFO : EPOCH 18 - PROGRESS: at 56.84% examples, 643619 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:54,719 : INFO : EPOCH 18 - PROGRESS: at 59.53% examples, 643289 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:54:55,720 : INFO : EPOCH 18 - PROGRESS: at 62.18% examples, 641354 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:56,725 : INFO : EPOCH 18 - PROGRESS: at 64.80% examples, 639067 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:57,739 : INFO : EPOCH 18 - PROGRESS: at 67.62% examples, 639048 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:58,743 : INFO : EPOCH 18 - PROGRESS: at 70.69% examples, 641699 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:54:59,750 : INFO : EPOCH 18 - PROGRESS: at 73.53% examples, 642519 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:00,763 : INFO : EPOCH 18 - PROGRESS: at 76.74% examples, 645988 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:01,805 : INFO : EPOCH 18 - PROGRESS: at 79.84% examples, 647184 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:02,814 : INFO : EPOCH 18 - PROGRESS: at 82.83% examples, 648039 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:03,814 : INFO : EPOCH 18 - PROGRESS: at 85.38% examples, 645866 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:04,825 : INFO : EPOCH 18 - PROGRESS: at 88.18% examples, 645442 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:05,830 : INFO : EPOCH 18 - PROGRESS: at 90.64% examples, 642736 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:06,864 : INFO : EPOCH 18 - PROGRESS: at 93.53% examples, 642287 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:07,895 : INFO : EPOCH 18 - PROGRESS: at 96.58% examples, 643113 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:08,902 : INFO : EPOCH 18 - PROGRESS: at 99.65% examples, 644799 words/s, in_qsize 8, out_qsize 0
    2020-09-30 22:55:08,921 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:55:08,926 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:55:08,927 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:55:08,927 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:55:08,941 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:55:08,942 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:55:08,950 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:55:08,955 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:55:08,955 : INFO : EPOCH - 18 : training on 23279529 raw words (22951015 effective words) took 35.5s, 645962 effective words/s
    2020-09-30 22:55:09,973 : INFO : EPOCH 19 - PROGRESS: at 2.92% examples, 667894 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:11,004 : INFO : EPOCH 19 - PROGRESS: at 6.05% examples, 686248 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:12,010 : INFO : EPOCH 19 - PROGRESS: at 9.07% examples, 690883 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:13,018 : INFO : EPOCH 19 - PROGRESS: at 12.24% examples, 700009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:14,020 : INFO : EPOCH 19 - PROGRESS: at 15.34% examples, 700928 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:15,024 : INFO : EPOCH 19 - PROGRESS: at 18.36% examples, 701698 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:16,029 : INFO : EPOCH 19 - PROGRESS: at 21.11% examples, 692465 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:17,036 : INFO : EPOCH 19 - PROGRESS: at 24.18% examples, 691167 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:18,041 : INFO : EPOCH 19 - PROGRESS: at 27.33% examples, 694584 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:19,065 : INFO : EPOCH 19 - PROGRESS: at 30.53% examples, 697638 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:20,072 : INFO : EPOCH 19 - PROGRESS: at 33.62% examples, 696896 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:21,083 : INFO : EPOCH 19 - PROGRESS: at 36.74% examples, 697079 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:22,087 : INFO : EPOCH 19 - PROGRESS: at 39.84% examples, 698456 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:23,098 : INFO : EPOCH 19 - PROGRESS: at 42.92% examples, 698485 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:24,117 : INFO : EPOCH 19 - PROGRESS: at 46.03% examples, 697997 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:25,129 : INFO : EPOCH 19 - PROGRESS: at 49.09% examples, 698011 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:26,133 : INFO : EPOCH 19 - PROGRESS: at 52.15% examples, 697298 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:27,136 : INFO : EPOCH 19 - PROGRESS: at 55.24% examples, 697716 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:55:28,149 : INFO : EPOCH 19 - PROGRESS: at 58.31% examples, 698816 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:29,151 : INFO : EPOCH 19 - PROGRESS: at 61.41% examples, 699481 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:30,168 : INFO : EPOCH 19 - PROGRESS: at 64.50% examples, 698831 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:31,177 : INFO : EPOCH 19 - PROGRESS: at 67.58% examples, 698901 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:32,182 : INFO : EPOCH 19 - PROGRESS: at 70.66% examples, 699172 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:33,185 : INFO : EPOCH 19 - PROGRESS: at 73.77% examples, 700597 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:34,207 : INFO : EPOCH 19 - PROGRESS: at 76.78% examples, 699842 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:35,233 : INFO : EPOCH 19 - PROGRESS: at 79.88% examples, 699410 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:36,235 : INFO : EPOCH 19 - PROGRESS: at 82.94% examples, 699296 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:37,251 : INFO : EPOCH 19 - PROGRESS: at 86.07% examples, 699232 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:38,258 : INFO : EPOCH 19 - PROGRESS: at 89.05% examples, 698619 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:39,259 : INFO : EPOCH 19 - PROGRESS: at 92.06% examples, 697976 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:40,276 : INFO : EPOCH 19 - PROGRESS: at 95.17% examples, 697601 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:41,289 : INFO : EPOCH 19 - PROGRESS: at 98.30% examples, 697944 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:41,757 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:55:41,759 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:55:41,765 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:55:41,766 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:55:41,768 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:55:41,773 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:55:41,787 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:55:41,791 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:55:41,791 : INFO : EPOCH - 19 : training on 23279529 raw words (22951015 effective words) took 32.8s, 699046 effective words/s
    2020-09-30 22:55:42,798 : INFO : EPOCH 20 - PROGRESS: at 3.00% examples, 693893 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:43,819 : INFO : EPOCH 20 - PROGRESS: at 6.04% examples, 692915 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:44,823 : INFO : EPOCH 20 - PROGRESS: at 9.20% examples, 705231 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:45,829 : INFO : EPOCH 20 - PROGRESS: at 12.15% examples, 699174 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:46,831 : INFO : EPOCH 20 - PROGRESS: at 15.21% examples, 698414 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:47,836 : INFO : EPOCH 20 - PROGRESS: at 18.23% examples, 699395 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:48,847 : INFO : EPOCH 20 - PROGRESS: at 21.34% examples, 700869 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:49,854 : INFO : EPOCH 20 - PROGRESS: at 24.45% examples, 699767 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:50,861 : INFO : EPOCH 20 - PROGRESS: at 27.57% examples, 701948 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:51,863 : INFO : EPOCH 20 - PROGRESS: at 30.61% examples, 702048 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:52,863 : INFO : EPOCH 20 - PROGRESS: at 33.81% examples, 703137 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:53,871 : INFO : EPOCH 20 - PROGRESS: at 36.87% examples, 702140 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:54,889 : INFO : EPOCH 20 - PROGRESS: at 39.89% examples, 700894 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:55,899 : INFO : EPOCH 20 - PROGRESS: at 42.84% examples, 698704 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:56,945 : INFO : EPOCH 20 - PROGRESS: at 46.03% examples, 698265 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:57,951 : INFO : EPOCH 20 - PROGRESS: at 49.09% examples, 698564 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:55:58,970 : INFO : EPOCH 20 - PROGRESS: at 52.27% examples, 698875 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:55:59,971 : INFO : EPOCH 20 - PROGRESS: at 55.41% examples, 699848 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:00,972 : INFO : EPOCH 20 - PROGRESS: at 58.48% examples, 701197 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:01,979 : INFO : EPOCH 20 - PROGRESS: at 61.46% examples, 700180 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:02,996 : INFO : EPOCH 20 - PROGRESS: at 64.50% examples, 699042 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:04,041 : INFO : EPOCH 20 - PROGRESS: at 67.70% examples, 699260 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:05,055 : INFO : EPOCH 20 - PROGRESS: at 70.73% examples, 698822 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:06,077 : INFO : EPOCH 20 - PROGRESS: at 73.81% examples, 699319 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:07,079 : INFO : EPOCH 20 - PROGRESS: at 76.86% examples, 699554 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:08,117 : INFO : EPOCH 20 - PROGRESS: at 79.88% examples, 698096 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:09,133 : INFO : EPOCH 20 - PROGRESS: at 82.94% examples, 697667 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:10,153 : INFO : EPOCH 20 - PROGRESS: at 86.07% examples, 697557 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:11,154 : INFO : EPOCH 20 - PROGRESS: at 89.23% examples, 698457 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:12,156 : INFO : EPOCH 20 - PROGRESS: at 92.27% examples, 698116 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:13,175 : INFO : EPOCH 20 - PROGRESS: at 95.30% examples, 697082 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:14,180 : INFO : EPOCH 20 - PROGRESS: at 98.25% examples, 696440 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:14,687 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:56:14,702 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:56:14,703 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:56:14,708 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:56:14,712 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:56:14,714 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:56:14,727 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:56:14,730 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:56:14,731 : INFO : EPOCH - 20 : training on 23279529 raw words (22951015 effective words) took 32.9s, 696805 effective words/s
    2020-09-30 22:56:14,731 : INFO : training on a 465590580 raw words (459020300 effective words) took 667.1s, 688091 effective words/s

    Evaluating Doc2Vec(dm/m,d100,n5,w10,mc2,t8)

    0.168400 Doc2Vec(dm/m,d100,n5,w10,mc2,t8)

    Training Doc2Vec(dm/c,d100,n5,w5,mc2,t8)
    2020-09-30 22:56:15,472 : INFO : training model with 8 workers on 265409 vocabulary and 1100 features, using sg=0 hs=0 sample=0 negative=5 window=5
    2020-09-30 22:56:16,514 : INFO : EPOCH 1 - PROGRESS: at 0.37% examples, 83752 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:17,544 : INFO : EPOCH 1 - PROGRESS: at 1.94% examples, 219284 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:18,607 : INFO : EPOCH 1 - PROGRESS: at 3.74% examples, 278086 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:19,646 : INFO : EPOCH 1 - PROGRESS: at 5.50% examples, 306209 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:20,668 : INFO : EPOCH 1 - PROGRESS: at 7.31% examples, 326068 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:21,674 : INFO : EPOCH 1 - PROGRESS: at 9.20% examples, 344658 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:22,690 : INFO : EPOCH 1 - PROGRESS: at 10.90% examples, 352182 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:23,721 : INFO : EPOCH 1 - PROGRESS: at 12.76% examples, 358528 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:24,744 : INFO : EPOCH 1 - PROGRESS: at 14.66% examples, 365839 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:25,780 : INFO : EPOCH 1 - PROGRESS: at 16.37% examples, 367683 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:56:26,783 : INFO : EPOCH 1 - PROGRESS: at 18.18% examples, 372860 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:27,789 : INFO : EPOCH 1 - PROGRESS: at 20.03% examples, 377763 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:28,838 : INFO : EPOCH 1 - PROGRESS: at 21.97% examples, 380846 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:29,882 : INFO : EPOCH 1 - PROGRESS: at 24.02% examples, 384766 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:30,904 : INFO : EPOCH 1 - PROGRESS: at 25.93% examples, 388059 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:31,907 : INFO : EPOCH 1 - PROGRESS: at 27.82% examples, 390873 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:32,964 : INFO : EPOCH 1 - PROGRESS: at 29.86% examples, 394288 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:33,975 : INFO : EPOCH 1 - PROGRESS: at 31.86% examples, 397284 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:35,005 : INFO : EPOCH 1 - PROGRESS: at 33.66% examples, 397027 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:36,019 : INFO : EPOCH 1 - PROGRESS: at 35.68% examples, 399598 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:37,027 : INFO : EPOCH 1 - PROGRESS: at 37.63% examples, 401569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:38,040 : INFO : EPOCH 1 - PROGRESS: at 39.59% examples, 403726 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:39,044 : INFO : EPOCH 1 - PROGRESS: at 41.48% examples, 405028 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:40,068 : INFO : EPOCH 1 - PROGRESS: at 43.47% examples, 406613 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:41,092 : INFO : EPOCH 1 - PROGRESS: at 45.35% examples, 407353 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:42,095 : INFO : EPOCH 1 - PROGRESS: at 47.49% examples, 410190 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:43,144 : INFO : EPOCH 1 - PROGRESS: at 49.52% examples, 411438 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:44,174 : INFO : EPOCH 1 - PROGRESS: at 51.39% examples, 411191 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:45,204 : INFO : EPOCH 1 - PROGRESS: at 53.26% examples, 411295 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:46,213 : INFO : EPOCH 1 - PROGRESS: at 55.24% examples, 412602 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:47,213 : INFO : EPOCH 1 - PROGRESS: at 57.13% examples, 413651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:48,220 : INFO : EPOCH 1 - PROGRESS: at 59.11% examples, 415096 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:49,248 : INFO : EPOCH 1 - PROGRESS: at 60.99% examples, 415641 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:50,256 : INFO : EPOCH 1 - PROGRESS: at 62.98% examples, 416399 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:51,263 : INFO : EPOCH 1 - PROGRESS: at 65.05% examples, 417667 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:52,267 : INFO : EPOCH 1 - PROGRESS: at 66.95% examples, 418084 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:53,275 : INFO : EPOCH 1 - PROGRESS: at 68.86% examples, 418505 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:54,278 : INFO : EPOCH 1 - PROGRESS: at 70.73% examples, 418936 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:55,280 : INFO : EPOCH 1 - PROGRESS: at 72.70% examples, 419826 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:56,287 : INFO : EPOCH 1 - PROGRESS: at 74.52% examples, 420125 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:57,292 : INFO : EPOCH 1 - PROGRESS: at 76.67% examples, 421845 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:56:58,296 : INFO : EPOCH 1 - PROGRESS: at 78.53% examples, 421695 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:56:59,328 : INFO : EPOCH 1 - PROGRESS: at 80.50% examples, 422357 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:57:00,331 : INFO : EPOCH 1 - PROGRESS: at 82.47% examples, 422647 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:01,337 : INFO : EPOCH 1 - PROGRESS: at 84.41% examples, 423102 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:02,338 : INFO : EPOCH 1 - PROGRESS: at 86.38% examples, 423578 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:03,353 : INFO : EPOCH 1 - PROGRESS: at 88.31% examples, 423888 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:04,389 : INFO : EPOCH 1 - PROGRESS: at 90.15% examples, 423601 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:05,412 : INFO : EPOCH 1 - PROGRESS: at 92.06% examples, 423508 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:06,419 : INFO : EPOCH 1 - PROGRESS: at 94.03% examples, 423863 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:07,428 : INFO : EPOCH 1 - PROGRESS: at 96.19% examples, 424997 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:08,442 : INFO : EPOCH 1 - PROGRESS: at 98.25% examples, 425831 words/s, in_qsize 15, out_qsize 1
    2020-09-30 22:57:09,238 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:57:09,257 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:57:09,263 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:57:09,277 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:57:09,278 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:57:09,280 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:57:09,306 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:57:09,308 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:57:09,308 : INFO : EPOCH - 1 : training on 23279529 raw words (22951015 effective words) took 53.8s, 426333 effective words/s
    2020-09-30 22:57:10,316 : INFO : EPOCH 2 - PROGRESS: at 1.77% examples, 413667 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:11,326 : INFO : EPOCH 2 - PROGRESS: at 3.66% examples, 422914 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:12,347 : INFO : EPOCH 2 - PROGRESS: at 5.54% examples, 424094 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:13,359 : INFO : EPOCH 2 - PROGRESS: at 7.69% examples, 439786 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:14,391 : INFO : EPOCH 2 - PROGRESS: at 9.61% examples, 439339 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:15,416 : INFO : EPOCH 2 - PROGRESS: at 11.64% examples, 443335 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:16,464 : INFO : EPOCH 2 - PROGRESS: at 13.70% examples, 443245 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:17,480 : INFO : EPOCH 2 - PROGRESS: at 15.71% examples, 445007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:18,488 : INFO : EPOCH 2 - PROGRESS: at 17.58% examples, 444604 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:19,496 : INFO : EPOCH 2 - PROGRESS: at 19.49% examples, 443569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:20,538 : INFO : EPOCH 2 - PROGRESS: at 21.50% examples, 443767 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:21,577 : INFO : EPOCH 2 - PROGRESS: at 23.62% examples, 444817 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:22,605 : INFO : EPOCH 2 - PROGRESS: at 25.68% examples, 446084 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:23,622 : INFO : EPOCH 2 - PROGRESS: at 27.75% examples, 447477 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:24,662 : INFO : EPOCH 2 - PROGRESS: at 29.86% examples, 449229 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:25,678 : INFO : EPOCH 2 - PROGRESS: at 32.03% examples, 451460 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:26,692 : INFO : EPOCH 2 - PROGRESS: at 34.20% examples, 452853 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:27,701 : INFO : EPOCH 2 - PROGRESS: at 36.31% examples, 454288 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:28,733 : INFO : EPOCH 2 - PROGRESS: at 38.34% examples, 454151 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:57:29,734 : INFO : EPOCH 2 - PROGRESS: at 40.56% examples, 456946 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:30,750 : INFO : EPOCH 2 - PROGRESS: at 42.58% examples, 457025 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:31,809 : INFO : EPOCH 2 - PROGRESS: at 44.64% examples, 456544 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:32,848 : INFO : EPOCH 2 - PROGRESS: at 46.62% examples, 455249 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:57:33,853 : INFO : EPOCH 2 - PROGRESS: at 48.51% examples, 454382 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:34,855 : INFO : EPOCH 2 - PROGRESS: at 50.53% examples, 454406 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:35,887 : INFO : EPOCH 2 - PROGRESS: at 52.50% examples, 453523 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:36,899 : INFO : EPOCH 2 - PROGRESS: at 54.52% examples, 453765 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:37,904 : INFO : EPOCH 2 - PROGRESS: at 56.56% examples, 454417 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:38,904 : INFO : EPOCH 2 - PROGRESS: at 58.43% examples, 454101 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:39,938 : INFO : EPOCH 2 - PROGRESS: at 60.53% examples, 454860 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:40,973 : INFO : EPOCH 2 - PROGRESS: at 62.89% examples, 456814 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:41,998 : INFO : EPOCH 2 - PROGRESS: at 64.92% examples, 456430 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:43,003 : INFO : EPOCH 2 - PROGRESS: at 67.03% examples, 457150 words/s, in_qsize 16, out_qsize 1
    2020-09-30 22:57:44,019 : INFO : EPOCH 2 - PROGRESS: at 69.06% examples, 457190 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:45,024 : INFO : EPOCH 2 - PROGRESS: at 70.97% examples, 456824 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:46,055 : INFO : EPOCH 2 - PROGRESS: at 72.97% examples, 456655 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:47,078 : INFO : EPOCH 2 - PROGRESS: at 75.02% examples, 457071 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:48,132 : INFO : EPOCH 2 - PROGRESS: at 77.03% examples, 456648 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:49,151 : INFO : EPOCH 2 - PROGRESS: at 79.11% examples, 456670 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:50,181 : INFO : EPOCH 2 - PROGRESS: at 80.72% examples, 454382 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:51,190 : INFO : EPOCH 2 - PROGRESS: at 82.50% examples, 452918 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:52,232 : INFO : EPOCH 2 - PROGRESS: at 84.17% examples, 450968 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:53,245 : INFO : EPOCH 2 - PROGRESS: at 85.72% examples, 448527 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:54,276 : INFO : EPOCH 2 - PROGRESS: at 87.25% examples, 445972 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:57:55,318 : INFO : EPOCH 2 - PROGRESS: at 88.93% examples, 444275 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:56,331 : INFO : EPOCH 2 - PROGRESS: at 90.59% examples, 442755 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:57:57,359 : INFO : EPOCH 2 - PROGRESS: at 92.27% examples, 441154 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:58,373 : INFO : EPOCH 2 - PROGRESS: at 93.84% examples, 439337 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:57:59,396 : INFO : EPOCH 2 - PROGRESS: at 95.65% examples, 438336 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:00,452 : INFO : EPOCH 2 - PROGRESS: at 97.17% examples, 436130 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:01,504 : INFO : EPOCH 2 - PROGRESS: at 98.17% examples, 431787 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:02,506 : INFO : EPOCH 2 - PROGRESS: at 99.00% examples, 427282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:03,203 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:58:03,209 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:58:03,222 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:58:03,227 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:58:03,242 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:58:03,248 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:58:03,248 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:58:03,252 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:58:03,252 : INFO : EPOCH - 2 : training on 23279529 raw words (22951015 effective words) took 53.9s, 425487 effective words/s
    2020-09-30 22:58:04,285 : INFO : EPOCH 3 - PROGRESS: at 1.23% examples, 282713 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:05,308 : INFO : EPOCH 3 - PROGRESS: at 3.00% examples, 339393 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:06,311 : INFO : EPOCH 3 - PROGRESS: at 4.87% examples, 370909 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:07,337 : INFO : EPOCH 3 - PROGRESS: at 6.78% examples, 384094 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:08,341 : INFO : EPOCH 3 - PROGRESS: at 8.64% examples, 393649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:09,344 : INFO : EPOCH 3 - PROGRESS: at 10.69% examples, 409423 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:10,414 : INFO : EPOCH 3 - PROGRESS: at 12.76% examples, 413014 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:11,434 : INFO : EPOCH 3 - PROGRESS: at 14.96% examples, 423034 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:12,446 : INFO : EPOCH 3 - PROGRESS: at 16.70% examples, 421878 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:13,459 : INFO : EPOCH 3 - PROGRESS: at 18.49% examples, 419911 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:14,488 : INFO : EPOCH 3 - PROGRESS: at 20.37% examples, 421071 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:58:15,524 : INFO : EPOCH 3 - PROGRESS: at 22.74% examples, 428935 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:16,529 : INFO : EPOCH 3 - PROGRESS: at 24.90% examples, 433010 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:17,540 : INFO : EPOCH 3 - PROGRESS: at 26.94% examples, 435487 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:18,563 : INFO : EPOCH 3 - PROGRESS: at 29.05% examples, 438450 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:19,595 : INFO : EPOCH 3 - PROGRESS: at 31.08% examples, 439171 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:20,641 : INFO : EPOCH 3 - PROGRESS: at 33.14% examples, 439362 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:21,646 : INFO : EPOCH 3 - PROGRESS: at 35.24% examples, 441161 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:22,703 : INFO : EPOCH 3 - PROGRESS: at 37.45% examples, 443008 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:23,708 : INFO : EPOCH 3 - PROGRESS: at 39.35% examples, 442629 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:24,711 : INFO : EPOCH 3 - PROGRESS: at 41.53% examples, 445373 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:25,718 : INFO : EPOCH 3 - PROGRESS: at 43.77% examples, 448193 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:26,723 : INFO : EPOCH 3 - PROGRESS: at 45.75% examples, 448361 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:58:27,765 : INFO : EPOCH 3 - PROGRESS: at 47.82% examples, 448684 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:28,781 : INFO : EPOCH 3 - PROGRESS: at 49.93% examples, 449383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:29,786 : INFO : EPOCH 3 - PROGRESS: at 52.06% examples, 450658 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:30,789 : INFO : EPOCH 3 - PROGRESS: at 54.09% examples, 451100 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:31,791 : INFO : EPOCH 3 - PROGRESS: at 56.13% examples, 451933 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:32,799 : INFO : EPOCH 3 - PROGRESS: at 58.02% examples, 451621 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:33,814 : INFO : EPOCH 3 - PROGRESS: at 60.11% examples, 452681 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:34,814 : INFO : EPOCH 3 - PROGRESS: at 62.04% examples, 452153 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:35,816 : INFO : EPOCH 3 - PROGRESS: at 64.23% examples, 453407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:36,826 : INFO : EPOCH 3 - PROGRESS: at 66.53% examples, 455336 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:37,833 : INFO : EPOCH 3 - PROGRESS: at 68.77% examples, 456944 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:38,849 : INFO : EPOCH 3 - PROGRESS: at 70.74% examples, 456714 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:39,860 : INFO : EPOCH 3 - PROGRESS: at 72.74% examples, 456803 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:40,881 : INFO : EPOCH 3 - PROGRESS: at 74.74% examples, 457002 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:41,895 : INFO : EPOCH 3 - PROGRESS: at 76.49% examples, 455559 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:42,934 : INFO : EPOCH 3 - PROGRESS: at 78.61% examples, 455582 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:58:43,959 : INFO : EPOCH 3 - PROGRESS: at 80.91% examples, 457195 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:58:44,965 : INFO : EPOCH 3 - PROGRESS: at 82.94% examples, 457306 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:45,985 : INFO : EPOCH 3 - PROGRESS: at 85.22% examples, 458416 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:46,987 : INFO : EPOCH 3 - PROGRESS: at 87.47% examples, 459647 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:48,005 : INFO : EPOCH 3 - PROGRESS: at 89.51% examples, 459569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:49,013 : INFO : EPOCH 3 - PROGRESS: at 91.59% examples, 459852 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:50,029 : INFO : EPOCH 3 - PROGRESS: at 93.69% examples, 460007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:51,045 : INFO : EPOCH 3 - PROGRESS: at 95.78% examples, 459993 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:52,077 : INFO : EPOCH 3 - PROGRESS: at 97.83% examples, 460020 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:53,036 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:58:53,052 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:58:53,057 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:58:53,076 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:58:53,086 : INFO : EPOCH 3 - PROGRESS: at 99.86% examples, 460015 words/s, in_qsize 3, out_qsize 1
    2020-09-30 22:58:53,086 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:58:53,092 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:58:53,113 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:58:53,122 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:58:53,123 : INFO : EPOCH - 3 : training on 23279529 raw words (22951015 effective words) took 49.9s, 460242 effective words/s
    2020-09-30 22:58:54,171 : INFO : EPOCH 4 - PROGRESS: at 1.69% examples, 379235 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:55,198 : INFO : EPOCH 4 - PROGRESS: at 3.70% examples, 415908 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:56,216 : INFO : EPOCH 4 - PROGRESS: at 5.73% examples, 429233 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:57,221 : INFO : EPOCH 4 - PROGRESS: at 8.02% examples, 453518 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:58,284 : INFO : EPOCH 4 - PROGRESS: at 10.22% examples, 460796 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:58:59,289 : INFO : EPOCH 4 - PROGRESS: at 12.37% examples, 465781 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:00,331 : INFO : EPOCH 4 - PROGRESS: at 14.58% examples, 468065 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:01,345 : INFO : EPOCH 4 - PROGRESS: at 16.62% examples, 469375 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:02,381 : INFO : EPOCH 4 - PROGRESS: at 18.73% examples, 469271 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:03,412 : INFO : EPOCH 4 - PROGRESS: at 21.02% examples, 474068 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:04,414 : INFO : EPOCH 4 - PROGRESS: at 23.01% examples, 471349 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:05,437 : INFO : EPOCH 4 - PROGRESS: at 24.90% examples, 466884 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:06,448 : INFO : EPOCH 4 - PROGRESS: at 26.98% examples, 467683 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:07,456 : INFO : EPOCH 4 - PROGRESS: at 28.97% examples, 466979 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:08,471 : INFO : EPOCH 4 - PROGRESS: at 31.13% examples, 468292 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:09,488 : INFO : EPOCH 4 - PROGRESS: at 33.19% examples, 467438 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:10,490 : INFO : EPOCH 4 - PROGRESS: at 35.52% examples, 471102 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:11,518 : INFO : EPOCH 4 - PROGRESS: at 37.82% examples, 473221 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:12,532 : INFO : EPOCH 4 - PROGRESS: at 39.89% examples, 472993 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:13,565 : INFO : EPOCH 4 - PROGRESS: at 42.09% examples, 473686 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:14,607 : INFO : EPOCH 4 - PROGRESS: at 44.17% examples, 472728 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:15,625 : INFO : EPOCH 4 - PROGRESS: at 46.22% examples, 471989 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:16,630 : INFO : EPOCH 4 - PROGRESS: at 48.16% examples, 471148 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:17,633 : INFO : EPOCH 4 - PROGRESS: at 50.41% examples, 472439 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:18,651 : INFO : EPOCH 4 - PROGRESS: at 52.71% examples, 474104 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:19,670 : INFO : EPOCH 4 - PROGRESS: at 54.94% examples, 475276 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:20,682 : INFO : EPOCH 4 - PROGRESS: at 57.00% examples, 475373 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:21,691 : INFO : EPOCH 4 - PROGRESS: at 59.11% examples, 475847 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:22,713 : INFO : EPOCH 4 - PROGRESS: at 61.27% examples, 476407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:23,719 : INFO : EPOCH 4 - PROGRESS: at 63.60% examples, 477813 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:24,754 : INFO : EPOCH 4 - PROGRESS: at 65.68% examples, 477214 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:25,758 : INFO : EPOCH 4 - PROGRESS: at 67.71% examples, 476748 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:26,773 : INFO : EPOCH 4 - PROGRESS: at 69.81% examples, 476811 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:27,773 : INFO : EPOCH 4 - PROGRESS: at 71.81% examples, 476456 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:28,822 : INFO : EPOCH 4 - PROGRESS: at 74.10% examples, 477629 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:29,827 : INFO : EPOCH 4 - PROGRESS: at 76.36% examples, 478821 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:30,835 : INFO : EPOCH 4 - PROGRESS: at 78.61% examples, 479384 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:31,839 : INFO : EPOCH 4 - PROGRESS: at 80.63% examples, 479199 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:32,865 : INFO : EPOCH 4 - PROGRESS: at 82.94% examples, 479983 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:33,892 : INFO : EPOCH 4 - PROGRESS: at 85.14% examples, 480031 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:34,910 : INFO : EPOCH 4 - PROGRESS: at 87.21% examples, 479689 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:35,978 : INFO : EPOCH 4 - PROGRESS: at 89.28% examples, 478786 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:36,982 : INFO : EPOCH 4 - PROGRESS: at 91.39% examples, 478689 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:37,999 : INFO : EPOCH 4 - PROGRESS: at 93.27% examples, 477340 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:39,008 : INFO : EPOCH 4 - PROGRESS: at 95.40% examples, 477212 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:40,025 : INFO : EPOCH 4 - PROGRESS: at 97.70% examples, 478267 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:41,005 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 22:59:41,041 : INFO : EPOCH 4 - PROGRESS: at 99.73% examples, 477797 words/s, in_qsize 6, out_qsize 1
    2020-09-30 22:59:41,041 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 22:59:41,046 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 22:59:41,051 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 22:59:41,057 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 22:59:41,060 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 22:59:41,081 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 22:59:41,095 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 22:59:41,095 : INFO : EPOCH - 4 : training on 23279529 raw words (22951015 effective words) took 48.0s, 478456 effective words/s
    2020-09-30 22:59:42,103 : INFO : EPOCH 5 - PROGRESS: at 2.05% examples, 480861 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:43,127 : INFO : EPOCH 5 - PROGRESS: at 4.38% examples, 501164 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:44,169 : INFO : EPOCH 5 - PROGRESS: at 6.53% examples, 491634 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:45,175 : INFO : EPOCH 5 - PROGRESS: at 8.55% examples, 486266 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:46,190 : INFO : EPOCH 5 - PROGRESS: at 10.53% examples, 481953 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:47,220 : INFO : EPOCH 5 - PROGRESS: at 12.63% examples, 478324 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:48,220 : INFO : EPOCH 5 - PROGRESS: at 14.75% examples, 478924 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:49,235 : INFO : EPOCH 5 - PROGRESS: at 16.75% examples, 477700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:50,239 : INFO : EPOCH 5 - PROGRESS: at 19.03% examples, 482513 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:51,245 : INFO : EPOCH 5 - PROGRESS: at 21.11% examples, 482451 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:59:52,248 : INFO : EPOCH 5 - PROGRESS: at 23.05% examples, 478069 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:53,295 : INFO : EPOCH 5 - PROGRESS: at 25.28% examples, 478266 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:54,297 : INFO : EPOCH 5 - PROGRESS: at 27.41% examples, 479354 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:55,308 : INFO : EPOCH 5 - PROGRESS: at 29.77% examples, 483936 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:56,348 : INFO : EPOCH 5 - PROGRESS: at 31.77% examples, 480723 words/s, in_qsize 14, out_qsize 1
    2020-09-30 22:59:57,350 : INFO : EPOCH 5 - PROGRESS: at 33.99% examples, 481353 words/s, in_qsize 16, out_qsize 0
    2020-09-30 22:59:58,360 : INFO : EPOCH 5 - PROGRESS: at 36.23% examples, 482826 words/s, in_qsize 15, out_qsize 0
    2020-09-30 22:59:59,375 : INFO : EPOCH 5 - PROGRESS: at 38.50% examples, 484728 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:00:00,407 : INFO : EPOCH 5 - PROGRESS: at 40.68% examples, 484805 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:01,430 : INFO : EPOCH 5 - PROGRESS: at 42.84% examples, 484749 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:02,448 : INFO : EPOCH 5 - PROGRESS: at 45.01% examples, 485186 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:03,512 : INFO : EPOCH 5 - PROGRESS: at 47.32% examples, 485448 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:04,517 : INFO : EPOCH 5 - PROGRESS: at 49.38% examples, 484851 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:05,574 : INFO : EPOCH 5 - PROGRESS: at 51.65% examples, 484542 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:00:06,599 : INFO : EPOCH 5 - PROGRESS: at 54.01% examples, 486295 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:07,613 : INFO : EPOCH 5 - PROGRESS: at 55.97% examples, 484899 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:08,625 : INFO : EPOCH 5 - PROGRESS: at 58.10% examples, 485405 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:09,662 : INFO : EPOCH 5 - PROGRESS: at 60.32% examples, 485986 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:10,691 : INFO : EPOCH 5 - PROGRESS: at 62.47% examples, 485478 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:11,716 : INFO : EPOCH 5 - PROGRESS: at 64.73% examples, 485692 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:12,721 : INFO : EPOCH 5 - PROGRESS: at 66.91% examples, 486129 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:13,726 : INFO : EPOCH 5 - PROGRESS: at 69.10% examples, 486629 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:14,737 : INFO : EPOCH 5 - PROGRESS: at 71.13% examples, 486123 words/s, in_qsize 15, out_qsize 1
    2020-09-30 23:00:15,739 : INFO : EPOCH 5 - PROGRESS: at 73.28% examples, 486607 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:16,748 : INFO : EPOCH 5 - PROGRESS: at 75.39% examples, 486667 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:17,792 : INFO : EPOCH 5 - PROGRESS: at 77.56% examples, 486282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:18,823 : INFO : EPOCH 5 - PROGRESS: at 79.84% examples, 486863 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:19,828 : INFO : EPOCH 5 - PROGRESS: at 81.87% examples, 486009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:20,836 : INFO : EPOCH 5 - PROGRESS: at 83.66% examples, 484146 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:21,844 : INFO : EPOCH 5 - PROGRESS: at 85.86% examples, 484322 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:22,845 : INFO : EPOCH 5 - PROGRESS: at 87.92% examples, 484062 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:23,853 : INFO : EPOCH 5 - PROGRESS: at 89.91% examples, 483258 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:24,855 : INFO : EPOCH 5 - PROGRESS: at 91.88% examples, 482436 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:25,866 : INFO : EPOCH 5 - PROGRESS: at 93.99% examples, 482130 words/s, in_qsize 15, out_qsize 1
    2020-09-30 23:00:26,890 : INFO : EPOCH 5 - PROGRESS: at 96.19% examples, 482180 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:27,946 : INFO : EPOCH 5 - PROGRESS: at 97.82% examples, 479400 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:28,831 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:00:28,852 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:00:28,855 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:00:28,865 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:00:28,896 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:00:28,914 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:00:28,915 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:00:28,916 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:00:28,916 : INFO : EPOCH - 5 : training on 23279529 raw words (22951015 effective words) took 47.8s, 479966 effective words/s
    2020-09-30 23:00:29,959 : INFO : EPOCH 6 - PROGRESS: at 1.82% examples, 409020 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:31,004 : INFO : EPOCH 6 - PROGRESS: at 4.19% examples, 464356 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:32,050 : INFO : EPOCH 6 - PROGRESS: at 6.05% examples, 448376 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:33,058 : INFO : EPOCH 6 - PROGRESS: at 8.18% examples, 457905 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:34,065 : INFO : EPOCH 6 - PROGRESS: at 10.15% examples, 458317 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:35,083 : INFO : EPOCH 6 - PROGRESS: at 12.24% examples, 460966 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:36,107 : INFO : EPOCH 6 - PROGRESS: at 14.37% examples, 462518 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:37,153 : INFO : EPOCH 6 - PROGRESS: at 16.51% examples, 464985 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:38,188 : INFO : EPOCH 6 - PROGRESS: at 18.82% examples, 470664 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:39,233 : INFO : EPOCH 6 - PROGRESS: at 20.94% examples, 470929 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:40,246 : INFO : EPOCH 6 - PROGRESS: at 22.91% examples, 467995 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:41,296 : INFO : EPOCH 6 - PROGRESS: at 24.98% examples, 465955 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:42,307 : INFO : EPOCH 6 - PROGRESS: at 26.85% examples, 463232 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:43,324 : INFO : EPOCH 6 - PROGRESS: at 29.01% examples, 465239 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:44,348 : INFO : EPOCH 6 - PROGRESS: at 31.04% examples, 464485 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:45,389 : INFO : EPOCH 6 - PROGRESS: at 33.27% examples, 465536 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:46,405 : INFO : EPOCH 6 - PROGRESS: at 35.53% examples, 467829 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:47,446 : INFO : EPOCH 6 - PROGRESS: at 37.78% examples, 469248 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:48,451 : INFO : EPOCH 6 - PROGRESS: at 40.12% examples, 472419 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:49,481 : INFO : EPOCH 6 - PROGRESS: at 42.22% examples, 472263 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:50,481 : INFO : EPOCH 6 - PROGRESS: at 44.37% examples, 473209 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:00:51,506 : INFO : EPOCH 6 - PROGRESS: at 46.50% examples, 473130 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:52,533 : INFO : EPOCH 6 - PROGRESS: at 48.46% examples, 471824 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:53,543 : INFO : EPOCH 6 - PROGRESS: at 50.67% examples, 472579 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:54,545 : INFO : EPOCH 6 - PROGRESS: at 52.67% examples, 471850 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:55,549 : INFO : EPOCH 6 - PROGRESS: at 54.84% examples, 473019 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:56,571 : INFO : EPOCH 6 - PROGRESS: at 56.96% examples, 473379 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:57,603 : INFO : EPOCH 6 - PROGRESS: at 59.23% examples, 474884 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:00:58,604 : INFO : EPOCH 6 - PROGRESS: at 61.46% examples, 476124 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:00:59,619 : INFO : EPOCH 6 - PROGRESS: at 63.60% examples, 476148 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:00,645 : INFO : EPOCH 6 - PROGRESS: at 65.55% examples, 474814 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:01,663 : INFO : EPOCH 6 - PROGRESS: at 67.71% examples, 475119 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:02,670 : INFO : EPOCH 6 - PROGRESS: at 69.67% examples, 474472 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:01:03,680 : INFO : EPOCH 6 - PROGRESS: at 71.65% examples, 473782 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:01:04,721 : INFO : EPOCH 6 - PROGRESS: at 73.65% examples, 473245 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:05,778 : INFO : EPOCH 6 - PROGRESS: at 75.74% examples, 472808 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:01:06,780 : INFO : EPOCH 6 - PROGRESS: at 77.78% examples, 472580 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:07,785 : INFO : EPOCH 6 - PROGRESS: at 80.05% examples, 473797 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:08,787 : INFO : EPOCH 6 - PROGRESS: at 82.12% examples, 473587 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:09,787 : INFO : EPOCH 6 - PROGRESS: at 84.17% examples, 473613 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:10,796 : INFO : EPOCH 6 - PROGRESS: at 86.16% examples, 472872 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:11,808 : INFO : EPOCH 6 - PROGRESS: at 88.22% examples, 472752 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:12,818 : INFO : EPOCH 6 - PROGRESS: at 90.37% examples, 473117 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:01:13,829 : INFO : EPOCH 6 - PROGRESS: at 92.60% examples, 473497 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:14,840 : INFO : EPOCH 6 - PROGRESS: at 94.69% examples, 473437 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:15,857 : INFO : EPOCH 6 - PROGRESS: at 96.96% examples, 474150 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:16,863 : INFO : EPOCH 6 - PROGRESS: at 99.05% examples, 474285 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:17,139 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:01:17,147 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:01:17,153 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:01:17,167 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:01:17,169 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:01:17,191 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:01:17,203 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:01:17,210 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:01:17,210 : INFO : EPOCH - 6 : training on 23279529 raw words (22951015 effective words) took 48.3s, 475265 effective words/s
    2020-09-30 23:01:18,235 : INFO : EPOCH 7 - PROGRESS: at 2.01% examples, 463404 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:19,235 : INFO : EPOCH 7 - PROGRESS: at 4.19% examples, 478685 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:20,254 : INFO : EPOCH 7 - PROGRESS: at 6.40% examples, 487075 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:21,255 : INFO : EPOCH 7 - PROGRESS: at 8.30% examples, 476092 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:22,267 : INFO : EPOCH 7 - PROGRESS: at 10.34% examples, 476019 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:23,272 : INFO : EPOCH 7 - PROGRESS: at 12.46% examples, 477017 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:24,293 : INFO : EPOCH 7 - PROGRESS: at 14.54% examples, 474959 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:01:25,295 : INFO : EPOCH 7 - PROGRESS: at 16.62% examples, 477353 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:26,298 : INFO : EPOCH 7 - PROGRESS: at 18.69% examples, 477015 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:27,314 : INFO : EPOCH 7 - PROGRESS: at 20.84% examples, 478939 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:28,346 : INFO : EPOCH 7 - PROGRESS: at 23.18% examples, 481424 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:29,350 : INFO : EPOCH 7 - PROGRESS: at 25.55% examples, 486214 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:30,359 : INFO : EPOCH 7 - PROGRESS: at 27.56% examples, 484199 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:31,374 : INFO : EPOCH 7 - PROGRESS: at 29.89% examples, 487680 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:32,387 : INFO : EPOCH 7 - PROGRESS: at 32.07% examples, 487504 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:33,389 : INFO : EPOCH 7 - PROGRESS: at 34.20% examples, 486598 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:34,389 : INFO : EPOCH 7 - PROGRESS: at 36.08% examples, 483578 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:35,401 : INFO : EPOCH 7 - PROGRESS: at 38.03% examples, 481188 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:36,403 : INFO : EPOCH 7 - PROGRESS: at 40.12% examples, 480848 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:37,409 : INFO : EPOCH 7 - PROGRESS: at 42.22% examples, 480829 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:38,416 : INFO : EPOCH 7 - PROGRESS: at 44.25% examples, 479862 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:39,422 : INFO : EPOCH 7 - PROGRESS: at 46.43% examples, 480324 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:40,461 : INFO : EPOCH 7 - PROGRESS: at 48.55% examples, 480084 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:41,495 : INFO : EPOCH 7 - PROGRESS: at 50.84% examples, 480852 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:42,504 : INFO : EPOCH 7 - PROGRESS: at 52.86% examples, 480028 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:43,505 : INFO : EPOCH 7 - PROGRESS: at 55.15% examples, 481649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:44,507 : INFO : EPOCH 7 - PROGRESS: at 57.43% examples, 483519 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:45,543 : INFO : EPOCH 7 - PROGRESS: at 59.50% examples, 483171 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:46,544 : INFO : EPOCH 7 - PROGRESS: at 61.67% examples, 483522 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:47,578 : INFO : EPOCH 7 - PROGRESS: at 63.81% examples, 482987 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:48,602 : INFO : EPOCH 7 - PROGRESS: at 66.07% examples, 483612 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:01:49,613 : INFO : EPOCH 7 - PROGRESS: at 68.47% examples, 485556 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:50,657 : INFO : EPOCH 7 - PROGRESS: at 70.44% examples, 484052 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:51,687 : INFO : EPOCH 7 - PROGRESS: at 72.56% examples, 483919 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:52,745 : INFO : EPOCH 7 - PROGRESS: at 74.70% examples, 483651 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:53,771 : INFO : EPOCH 7 - PROGRESS: at 77.03% examples, 484915 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:54,804 : INFO : EPOCH 7 - PROGRESS: at 79.46% examples, 486040 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:55,815 : INFO : EPOCH 7 - PROGRESS: at 81.40% examples, 484864 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:56,820 : INFO : EPOCH 7 - PROGRESS: at 83.44% examples, 484533 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:57,829 : INFO : EPOCH 7 - PROGRESS: at 85.51% examples, 483964 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:58,842 : INFO : EPOCH 7 - PROGRESS: at 87.76% examples, 484505 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:01:59,850 : INFO : EPOCH 7 - PROGRESS: at 89.91% examples, 484604 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:00,852 : INFO : EPOCH 7 - PROGRESS: at 92.14% examples, 485057 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:01,857 : INFO : EPOCH 7 - PROGRESS: at 94.42% examples, 485657 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:02,892 : INFO : EPOCH 7 - PROGRESS: at 96.44% examples, 484653 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:03,897 : INFO : EPOCH 7 - PROGRESS: at 98.56% examples, 484608 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:04,441 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:02:04,457 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:02:04,460 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:02:04,487 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:02:04,503 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:02:04,504 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:02:04,516 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:02:04,527 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:02:04,527 : INFO : EPOCH - 7 : training on 23279529 raw words (22951015 effective words) took 47.3s, 485084 effective words/s
    2020-09-30 23:02:05,537 : INFO : EPOCH 8 - PROGRESS: at 1.85% examples, 431863 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:06,561 : INFO : EPOCH 8 - PROGRESS: at 4.23% examples, 481472 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:07,572 : INFO : EPOCH 8 - PROGRESS: at 6.44% examples, 490143 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:08,582 : INFO : EPOCH 8 - PROGRESS: at 8.51% examples, 486913 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:02:09,597 : INFO : EPOCH 8 - PROGRESS: at 10.69% examples, 491967 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:10,601 : INFO : EPOCH 8 - PROGRESS: at 12.90% examples, 491810 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:11,622 : INFO : EPOCH 8 - PROGRESS: at 15.26% examples, 497478 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:02:12,641 : INFO : EPOCH 8 - PROGRESS: at 17.45% examples, 499466 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:02:13,654 : INFO : EPOCH 8 - PROGRESS: at 19.61% examples, 498231 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:14,658 : INFO : EPOCH 8 - PROGRESS: at 21.66% examples, 495787 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:15,660 : INFO : EPOCH 8 - PROGRESS: at 23.93% examples, 496372 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:16,692 : INFO : EPOCH 8 - PROGRESS: at 25.76% examples, 489128 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:17,694 : INFO : EPOCH 8 - PROGRESS: at 27.82% examples, 487947 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:02:18,707 : INFO : EPOCH 8 - PROGRESS: at 29.99% examples, 488492 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:19,716 : INFO : EPOCH 8 - PROGRESS: at 32.03% examples, 486510 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:20,732 : INFO : EPOCH 8 - PROGRESS: at 34.15% examples, 485217 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:21,763 : INFO : EPOCH 8 - PROGRESS: at 36.27% examples, 484211 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:22,773 : INFO : EPOCH 8 - PROGRESS: at 38.41% examples, 484562 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:02:23,783 : INFO : EPOCH 8 - PROGRESS: at 40.60% examples, 485220 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:24,815 : INFO : EPOCH 8 - PROGRESS: at 42.62% examples, 483504 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:25,839 : INFO : EPOCH 8 - PROGRESS: at 44.76% examples, 483399 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:26,841 : INFO : EPOCH 8 - PROGRESS: at 46.95% examples, 483768 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:27,863 : INFO : EPOCH 8 - PROGRESS: at 49.13% examples, 484168 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:28,875 : INFO : EPOCH 8 - PROGRESS: at 51.35% examples, 484347 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:29,880 : INFO : EPOCH 8 - PROGRESS: at 53.63% examples, 485732 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:30,900 : INFO : EPOCH 8 - PROGRESS: at 55.80% examples, 486094 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:31,923 : INFO : EPOCH 8 - PROGRESS: at 57.86% examples, 485665 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:02:32,951 : INFO : EPOCH 8 - PROGRESS: at 59.87% examples, 484669 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:33,965 : INFO : EPOCH 8 - PROGRESS: at 62.04% examples, 484785 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:34,995 : INFO : EPOCH 8 - PROGRESS: at 63.93% examples, 482358 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:35,998 : INFO : EPOCH 8 - PROGRESS: at 66.02% examples, 482093 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:37,024 : INFO : EPOCH 8 - PROGRESS: at 68.09% examples, 481468 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:38,026 : INFO : EPOCH 8 - PROGRESS: at 70.15% examples, 481281 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:02:39,078 : INFO : EPOCH 8 - PROGRESS: at 72.18% examples, 480367 words/s, in_qsize 13, out_qsize 2
    2020-09-30 23:02:40,079 : INFO : EPOCH 8 - PROGRESS: at 74.35% examples, 481248 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:41,101 : INFO : EPOCH 8 - PROGRESS: at 76.53% examples, 481593 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:42,106 : INFO : EPOCH 8 - PROGRESS: at 78.85% examples, 482620 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:43,107 : INFO : EPOCH 8 - PROGRESS: at 81.04% examples, 483168 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:44,121 : INFO : EPOCH 8 - PROGRESS: at 83.28% examples, 483740 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:45,139 : INFO : EPOCH 8 - PROGRESS: at 85.47% examples, 483811 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:46,140 : INFO : EPOCH 8 - PROGRESS: at 87.34% examples, 482400 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:47,147 : INFO : EPOCH 8 - PROGRESS: at 89.51% examples, 482564 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:48,164 : INFO : EPOCH 8 - PROGRESS: at 91.67% examples, 482685 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:49,177 : INFO : EPOCH 8 - PROGRESS: at 93.81% examples, 482569 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:50,216 : INFO : EPOCH 8 - PROGRESS: at 96.00% examples, 482240 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:51,217 : INFO : EPOCH 8 - PROGRESS: at 98.00% examples, 481875 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:52,060 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:02:52,065 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:02:52,080 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:02:52,085 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:02:52,096 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:02:52,110 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:02:52,125 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:02:52,137 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:02:52,137 : INFO : EPOCH - 8 : training on 23279529 raw words (22951015 effective words) took 47.6s, 482090 effective words/s
    2020-09-30 23:02:53,208 : INFO : EPOCH 9 - PROGRESS: at 2.01% examples, 443560 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:54,228 : INFO : EPOCH 9 - PROGRESS: at 4.04% examples, 449797 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:55,229 : INFO : EPOCH 9 - PROGRESS: at 6.32% examples, 473491 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:56,257 : INFO : EPOCH 9 - PROGRESS: at 8.55% examples, 481694 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:57,266 : INFO : EPOCH 9 - PROGRESS: at 10.73% examples, 488318 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:58,342 : INFO : EPOCH 9 - PROGRESS: at 13.06% examples, 487757 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:02:59,369 : INFO : EPOCH 9 - PROGRESS: at 15.34% examples, 490799 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:00,381 : INFO : EPOCH 9 - PROGRESS: at 17.45% examples, 491632 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:03:01,394 : INFO : EPOCH 9 - PROGRESS: at 19.61% examples, 491296 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:02,407 : INFO : EPOCH 9 - PROGRESS: at 21.75% examples, 491000 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:03,414 : INFO : EPOCH 9 - PROGRESS: at 23.92% examples, 490046 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:04,429 : INFO : EPOCH 9 - PROGRESS: at 26.25% examples, 493595 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:05,442 : INFO : EPOCH 9 - PROGRESS: at 28.27% examples, 490880 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:06,465 : INFO : EPOCH 9 - PROGRESS: at 30.57% examples, 492897 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:07,482 : INFO : EPOCH 9 - PROGRESS: at 32.92% examples, 494787 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:08,493 : INFO : EPOCH 9 - PROGRESS: at 34.94% examples, 492037 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:09,531 : INFO : EPOCH 9 - PROGRESS: at 36.87% examples, 487654 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:10,550 : INFO : EPOCH 9 - PROGRESS: at 39.26% examples, 490740 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:11,567 : INFO : EPOCH 9 - PROGRESS: at 41.17% examples, 487925 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:12,599 : INFO : EPOCH 9 - PROGRESS: at 43.47% examples, 488809 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:13,637 : INFO : EPOCH 9 - PROGRESS: at 45.75% examples, 489510 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:14,639 : INFO : EPOCH 9 - PROGRESS: at 48.07% examples, 491368 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:03:15,669 : INFO : EPOCH 9 - PROGRESS: at 50.44% examples, 492494 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:16,686 : INFO : EPOCH 9 - PROGRESS: at 52.70% examples, 493014 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:17,734 : INFO : EPOCH 9 - PROGRESS: at 54.84% examples, 492184 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:18,745 : INFO : EPOCH 9 - PROGRESS: at 57.25% examples, 494585 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:19,745 : INFO : EPOCH 9 - PROGRESS: at 59.38% examples, 494817 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:20,761 : INFO : EPOCH 9 - PROGRESS: at 61.50% examples, 494177 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:21,795 : INFO : EPOCH 9 - PROGRESS: at 63.81% examples, 494563 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:22,809 : INFO : EPOCH 9 - PROGRESS: at 66.15% examples, 495613 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:23,840 : INFO : EPOCH 9 - PROGRESS: at 68.34% examples, 495364 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:24,853 : INFO : EPOCH 9 - PROGRESS: at 70.48% examples, 495161 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:25,909 : INFO : EPOCH 9 - PROGRESS: at 72.52% examples, 493744 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:26,973 : INFO : EPOCH 9 - PROGRESS: at 74.82% examples, 494212 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:27,988 : INFO : EPOCH 9 - PROGRESS: at 76.91% examples, 493736 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:29,019 : INFO : EPOCH 9 - PROGRESS: at 79.19% examples, 493853 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:30,044 : INFO : EPOCH 9 - PROGRESS: at 81.39% examples, 493801 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:31,101 : INFO : EPOCH 9 - PROGRESS: at 83.57% examples, 493317 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:32,110 : INFO : EPOCH 9 - PROGRESS: at 85.73% examples, 493015 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:33,118 : INFO : EPOCH 9 - PROGRESS: at 87.88% examples, 492918 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:34,139 : INFO : EPOCH 9 - PROGRESS: at 89.98% examples, 492444 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:35,151 : INFO : EPOCH 9 - PROGRESS: at 92.14% examples, 492158 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:36,163 : INFO : EPOCH 9 - PROGRESS: at 94.03% examples, 490525 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:37,190 : INFO : EPOCH 9 - PROGRESS: at 96.19% examples, 490141 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:38,197 : INFO : EPOCH 9 - PROGRESS: at 98.34% examples, 490155 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:38,851 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:03:38,894 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:03:38,900 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:03:38,903 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:03:38,908 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:03:38,917 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:03:38,925 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:03:38,928 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:03:38,928 : INFO : EPOCH - 9 : training on 23279529 raw words (22951015 effective words) took 46.8s, 490538 effective words/s
    2020-09-30 23:03:39,951 : INFO : EPOCH 10 - PROGRESS: at 2.01% examples, 464928 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:40,963 : INFO : EPOCH 10 - PROGRESS: at 4.19% examples, 476471 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:41,978 : INFO : EPOCH 10 - PROGRESS: at 6.44% examples, 489429 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:43,026 : INFO : EPOCH 10 - PROGRESS: at 8.76% examples, 496051 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:44,030 : INFO : EPOCH 10 - PROGRESS: at 11.05% examples, 504236 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:45,046 : INFO : EPOCH 10 - PROGRESS: at 13.15% examples, 497855 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:46,056 : INFO : EPOCH 10 - PROGRESS: at 15.21% examples, 493882 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:47,076 : INFO : EPOCH 10 - PROGRESS: at 17.41% examples, 496242 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:48,108 : INFO : EPOCH 10 - PROGRESS: at 19.77% examples, 499588 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:49,132 : INFO : EPOCH 10 - PROGRESS: at 22.02% examples, 499850 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:50,149 : INFO : EPOCH 10 - PROGRESS: at 24.15% examples, 496841 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:51,150 : INFO : EPOCH 10 - PROGRESS: at 26.40% examples, 499595 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:52,152 : INFO : EPOCH 10 - PROGRESS: at 28.57% examples, 498924 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:53,155 : INFO : EPOCH 10 - PROGRESS: at 30.77% examples, 499739 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:54,167 : INFO : EPOCH 10 - PROGRESS: at 33.18% examples, 502000 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:03:55,175 : INFO : EPOCH 10 - PROGRESS: at 35.63% examples, 504826 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:03:56,185 : INFO : EPOCH 10 - PROGRESS: at 37.78% examples, 503891 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:57,209 : INFO : EPOCH 10 - PROGRESS: at 40.07% examples, 504323 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:58,227 : INFO : EPOCH 10 - PROGRESS: at 42.22% examples, 503280 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:03:59,229 : INFO : EPOCH 10 - PROGRESS: at 44.37% examples, 502692 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:00,245 : INFO : EPOCH 10 - PROGRESS: at 46.57% examples, 502311 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:01,281 : INFO : EPOCH 10 - PROGRESS: at 48.84% examples, 502414 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:02,291 : INFO : EPOCH 10 - PROGRESS: at 51.10% examples, 502315 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:03,306 : INFO : EPOCH 10 - PROGRESS: at 53.51% examples, 504023 words/s, in_qsize 15, out_qsize 1
    2020-09-30 23:04:04,306 : INFO : EPOCH 10 - PROGRESS: at 55.52% examples, 502516 words/s, in_qsize 16, out_qsize 2
    2020-09-30 23:04:05,339 : INFO : EPOCH 10 - PROGRESS: at 57.58% examples, 501221 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:06,367 : INFO : EPOCH 10 - PROGRESS: at 59.66% examples, 500323 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:07,399 : INFO : EPOCH 10 - PROGRESS: at 62.00% examples, 500912 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:08,425 : INFO : EPOCH 10 - PROGRESS: at 64.14% examples, 499912 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:09,437 : INFO : EPOCH 10 - PROGRESS: at 66.28% examples, 499195 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:10,442 : INFO : EPOCH 10 - PROGRESS: at 68.61% examples, 500209 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:11,444 : INFO : EPOCH 10 - PROGRESS: at 70.81% examples, 500597 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:12,477 : INFO : EPOCH 10 - PROGRESS: at 72.84% examples, 499315 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:13,497 : INFO : EPOCH 10 - PROGRESS: at 74.90% examples, 498585 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:14,500 : INFO : EPOCH 10 - PROGRESS: at 76.99% examples, 498134 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:15,505 : INFO : EPOCH 10 - PROGRESS: at 79.37% examples, 499029 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:16,532 : INFO : EPOCH 10 - PROGRESS: at 81.53% examples, 498540 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:17,534 : INFO : EPOCH 10 - PROGRESS: at 83.87% examples, 499658 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:18,585 : INFO : EPOCH 10 - PROGRESS: at 86.12% examples, 499138 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:19,588 : INFO : EPOCH 10 - PROGRESS: at 88.27% examples, 498963 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:20,589 : INFO : EPOCH 10 - PROGRESS: at 90.32% examples, 498337 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:21,590 : INFO : EPOCH 10 - PROGRESS: at 92.60% examples, 498495 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:22,598 : INFO : EPOCH 10 - PROGRESS: at 94.71% examples, 497876 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:23,639 : INFO : EPOCH 10 - PROGRESS: at 96.74% examples, 496717 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:24,657 : INFO : EPOCH 10 - PROGRESS: at 98.97% examples, 496868 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:25,029 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:04:25,039 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:04:25,041 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:04:25,052 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:04:25,054 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:04:25,059 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:04:25,067 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:04:25,081 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:04:25,081 : INFO : EPOCH - 10 : training on 23279529 raw words (22951015 effective words) took 46.1s, 497323 effective words/s
    2020-09-30 23:04:26,093 : INFO : EPOCH 11 - PROGRESS: at 2.18% examples, 507648 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:27,152 : INFO : EPOCH 11 - PROGRESS: at 4.38% examples, 491727 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:28,170 : INFO : EPOCH 11 - PROGRESS: at 6.49% examples, 486148 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:29,186 : INFO : EPOCH 11 - PROGRESS: at 8.85% examples, 499805 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:30,191 : INFO : EPOCH 11 - PROGRESS: at 10.90% examples, 497584 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:31,196 : INFO : EPOCH 11 - PROGRESS: at 13.11% examples, 496472 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:32,210 : INFO : EPOCH 11 - PROGRESS: at 15.13% examples, 491023 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:33,253 : INFO : EPOCH 11 - PROGRESS: at 17.32% examples, 492407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:34,281 : INFO : EPOCH 11 - PROGRESS: at 19.57% examples, 493243 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:35,337 : INFO : EPOCH 11 - PROGRESS: at 21.58% examples, 487845 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:36,384 : INFO : EPOCH 11 - PROGRESS: at 23.80% examples, 486270 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:37,440 : INFO : EPOCH 11 - PROGRESS: at 25.88% examples, 483844 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:38,465 : INFO : EPOCH 11 - PROGRESS: at 27.73% examples, 478559 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:39,477 : INFO : EPOCH 11 - PROGRESS: at 29.55% examples, 474409 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:40,490 : INFO : EPOCH 11 - PROGRESS: at 31.56% examples, 472702 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:41,498 : INFO : EPOCH 11 - PROGRESS: at 33.45% examples, 469502 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:42,510 : INFO : EPOCH 11 - PROGRESS: at 35.52% examples, 469441 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:43,521 : INFO : EPOCH 11 - PROGRESS: at 37.63% examples, 469431 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:44,533 : INFO : EPOCH 11 - PROGRESS: at 39.44% examples, 466474 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:45,590 : INFO : EPOCH 11 - PROGRESS: at 41.48% examples, 465547 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:46,593 : INFO : EPOCH 11 - PROGRESS: at 43.68% examples, 467207 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:47,602 : INFO : EPOCH 11 - PROGRESS: at 45.84% examples, 468138 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:48,694 : INFO : EPOCH 11 - PROGRESS: at 48.03% examples, 467829 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:49,745 : INFO : EPOCH 11 - PROGRESS: at 50.27% examples, 468308 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:50,779 : INFO : EPOCH 11 - PROGRESS: at 52.67% examples, 470567 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:04:51,788 : INFO : EPOCH 11 - PROGRESS: at 54.63% examples, 469889 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:52,822 : INFO : EPOCH 11 - PROGRESS: at 56.51% examples, 468069 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:53,841 : INFO : EPOCH 11 - PROGRESS: at 58.39% examples, 466979 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:54,847 : INFO : EPOCH 11 - PROGRESS: at 60.15% examples, 465107 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:55,850 : INFO : EPOCH 11 - PROGRESS: at 62.13% examples, 464447 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:56,852 : INFO : EPOCH 11 - PROGRESS: at 63.81% examples, 461660 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:57,882 : INFO : EPOCH 11 - PROGRESS: at 66.11% examples, 463138 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:04:58,914 : INFO : EPOCH 11 - PROGRESS: at 68.39% examples, 464455 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:04:59,948 : INFO : EPOCH 11 - PROGRESS: at 70.85% examples, 467115 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:00,950 : INFO : EPOCH 11 - PROGRESS: at 72.88% examples, 467278 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:01,955 : INFO : EPOCH 11 - PROGRESS: at 74.90% examples, 467413 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:02,973 : INFO : EPOCH 11 - PROGRESS: at 77.07% examples, 468128 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:03,974 : INFO : EPOCH 11 - PROGRESS: at 79.46% examples, 469803 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:04,974 : INFO : EPOCH 11 - PROGRESS: at 81.44% examples, 469453 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:05,974 : INFO : EPOCH 11 - PROGRESS: at 83.44% examples, 469319 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:06,976 : INFO : EPOCH 11 - PROGRESS: at 85.84% examples, 471079 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:08,000 : INFO : EPOCH 11 - PROGRESS: at 88.09% examples, 471786 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:09,010 : INFO : EPOCH 11 - PROGRESS: at 89.89% examples, 470386 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:10,016 : INFO : EPOCH 11 - PROGRESS: at 92.14% examples, 471111 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:11,017 : INFO : EPOCH 11 - PROGRESS: at 94.24% examples, 471180 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:12,024 : INFO : EPOCH 11 - PROGRESS: at 96.50% examples, 471844 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:05:13,031 : INFO : EPOCH 11 - PROGRESS: at 98.72% examples, 472650 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:13,460 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:05:13,463 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:05:13,464 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:05:13,470 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:05:13,477 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:05:13,496 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:05:13,506 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:05:13,516 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:05:13,517 : INFO : EPOCH - 11 : training on 23279529 raw words (22951015 effective words) took 48.4s, 473876 effective words/s
    2020-09-30 23:05:14,576 : INFO : EPOCH 12 - PROGRESS: at 2.28% examples, 503825 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:15,582 : INFO : EPOCH 12 - PROGRESS: at 4.42% examples, 497770 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:16,596 : INFO : EPOCH 12 - PROGRESS: at 6.67% examples, 500279 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:17,623 : INFO : EPOCH 12 - PROGRESS: at 8.81% examples, 497305 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:18,656 : INFO : EPOCH 12 - PROGRESS: at 10.99% examples, 498604 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:19,658 : INFO : EPOCH 12 - PROGRESS: at 13.11% examples, 494338 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:20,660 : INFO : EPOCH 12 - PROGRESS: at 15.18% examples, 491397 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:21,660 : INFO : EPOCH 12 - PROGRESS: at 17.16% examples, 489371 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:22,687 : INFO : EPOCH 12 - PROGRESS: at 19.37% examples, 489627 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:23,706 : INFO : EPOCH 12 - PROGRESS: at 21.54% examples, 490081 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:05:24,706 : INFO : EPOCH 12 - PROGRESS: at 23.97% examples, 494708 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:25,745 : INFO : EPOCH 12 - PROGRESS: at 26.05% examples, 492175 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:26,761 : INFO : EPOCH 12 - PROGRESS: at 28.07% examples, 489500 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:27,789 : INFO : EPOCH 12 - PROGRESS: at 30.15% examples, 488065 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:28,807 : INFO : EPOCH 12 - PROGRESS: at 32.62% examples, 492104 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:29,842 : INFO : EPOCH 12 - PROGRESS: at 35.07% examples, 494737 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:30,876 : INFO : EPOCH 12 - PROGRESS: at 37.45% examples, 496406 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:31,883 : INFO : EPOCH 12 - PROGRESS: at 39.71% examples, 497749 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:05:32,891 : INFO : EPOCH 12 - PROGRESS: at 41.66% examples, 494791 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:33,899 : INFO : EPOCH 12 - PROGRESS: at 43.94% examples, 495908 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:34,933 : INFO : EPOCH 12 - PROGRESS: at 46.12% examples, 495017 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:35,964 : INFO : EPOCH 12 - PROGRESS: at 48.42% examples, 495993 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:36,994 : INFO : EPOCH 12 - PROGRESS: at 50.44% examples, 493644 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:38,063 : INFO : EPOCH 12 - PROGRESS: at 52.55% examples, 491490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:39,065 : INFO : EPOCH 12 - PROGRESS: at 54.77% examples, 492355 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:40,094 : INFO : EPOCH 12 - PROGRESS: at 57.14% examples, 494046 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:41,105 : INFO : EPOCH 12 - PROGRESS: at 59.22% examples, 493791 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:42,107 : INFO : EPOCH 12 - PROGRESS: at 61.54% examples, 495082 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:43,140 : INFO : EPOCH 12 - PROGRESS: at 63.69% examples, 494155 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:44,151 : INFO : EPOCH 12 - PROGRESS: at 66.03% examples, 495282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:45,177 : INFO : EPOCH 12 - PROGRESS: at 68.38% examples, 496334 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:05:46,203 : INFO : EPOCH 12 - PROGRESS: at 70.73% examples, 497389 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:05:47,217 : INFO : EPOCH 12 - PROGRESS: at 73.16% examples, 499383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:48,289 : INFO : EPOCH 12 - PROGRESS: at 75.48% examples, 499555 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:49,297 : INFO : EPOCH 12 - PROGRESS: at 77.89% examples, 500660 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:50,303 : INFO : EPOCH 12 - PROGRESS: at 80.09% examples, 500901 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:51,305 : INFO : EPOCH 12 - PROGRESS: at 82.43% examples, 501476 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:05:52,312 : INFO : EPOCH 12 - PROGRESS: at 84.53% examples, 500973 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:53,342 : INFO : EPOCH 12 - PROGRESS: at 86.80% examples, 500914 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:54,362 : INFO : EPOCH 12 - PROGRESS: at 88.89% examples, 500221 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:55,390 : INFO : EPOCH 12 - PROGRESS: at 91.26% examples, 500696 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:56,405 : INFO : EPOCH 12 - PROGRESS: at 93.63% examples, 501281 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:57,426 : INFO : EPOCH 12 - PROGRESS: at 95.70% examples, 500250 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:58,476 : INFO : EPOCH 12 - PROGRESS: at 97.79% examples, 499361 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:05:59,311 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:05:59,326 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:05:59,326 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:05:59,351 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:05:59,401 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:05:59,402 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:05:59,406 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:05:59,408 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:05:59,408 : INFO : EPOCH - 12 : training on 23279529 raw words (22951015 effective words) took 45.9s, 500151 effective words/s
    2020-09-30 23:06:00,434 : INFO : EPOCH 13 - PROGRESS: at 2.01% examples, 463668 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:01,467 : INFO : EPOCH 13 - PROGRESS: at 4.27% examples, 480695 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:02,491 : INFO : EPOCH 13 - PROGRESS: at 6.63% examples, 496838 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:03,517 : INFO : EPOCH 13 - PROGRESS: at 8.89% examples, 501875 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:04,535 : INFO : EPOCH 13 - PROGRESS: at 10.86% examples, 494222 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:06:05,547 : INFO : EPOCH 13 - PROGRESS: at 13.06% examples, 493067 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:06,552 : INFO : EPOCH 13 - PROGRESS: at 15.21% examples, 492821 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:07,593 : INFO : EPOCH 13 - PROGRESS: at 17.32% examples, 491638 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:06:08,622 : INFO : EPOCH 13 - PROGRESS: at 19.61% examples, 493671 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:09,626 : INFO : EPOCH 13 - PROGRESS: at 21.66% examples, 491648 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:10,635 : INFO : EPOCH 13 - PROGRESS: at 23.83% examples, 490526 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:11,638 : INFO : EPOCH 13 - PROGRESS: at 26.12% examples, 493754 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:12,676 : INFO : EPOCH 13 - PROGRESS: at 28.40% examples, 494361 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:13,677 : INFO : EPOCH 13 - PROGRESS: at 30.65% examples, 496262 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:14,693 : INFO : EPOCH 13 - PROGRESS: at 32.84% examples, 495511 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:15,733 : INFO : EPOCH 13 - PROGRESS: at 35.15% examples, 495940 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:16,735 : INFO : EPOCH 13 - PROGRESS: at 37.19% examples, 494019 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:17,766 : INFO : EPOCH 13 - PROGRESS: at 39.52% examples, 495328 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:06:18,797 : INFO : EPOCH 13 - PROGRESS: at 41.35% examples, 490995 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:19,813 : INFO : EPOCH 13 - PROGRESS: at 43.31% examples, 488293 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:20,834 : INFO : EPOCH 13 - PROGRESS: at 45.43% examples, 488070 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:06:21,870 : INFO : EPOCH 13 - PROGRESS: at 47.78% examples, 489278 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:22,878 : INFO : EPOCH 13 - PROGRESS: at 50.15% examples, 490917 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:23,884 : INFO : EPOCH 13 - PROGRESS: at 52.58% examples, 493305 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:06:24,893 : INFO : EPOCH 13 - PROGRESS: at 54.77% examples, 493592 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:25,909 : INFO : EPOCH 13 - PROGRESS: at 56.96% examples, 494021 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:26,969 : INFO : EPOCH 13 - PROGRESS: at 59.11% examples, 493271 words/s, in_qsize 13, out_qsize 2
    2020-09-30 23:06:27,985 : INFO : EPOCH 13 - PROGRESS: at 61.32% examples, 493672 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:28,990 : INFO : EPOCH 13 - PROGRESS: at 63.47% examples, 493264 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:06:29,996 : INFO : EPOCH 13 - PROGRESS: at 65.51% examples, 492249 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:31,014 : INFO : EPOCH 13 - PROGRESS: at 67.66% examples, 491993 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:32,046 : INFO : EPOCH 13 - PROGRESS: at 70.04% examples, 493115 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:33,060 : INFO : EPOCH 13 - PROGRESS: at 72.40% examples, 494645 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:34,060 : INFO : EPOCH 13 - PROGRESS: at 74.48% examples, 494602 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:35,115 : INFO : EPOCH 13 - PROGRESS: at 76.53% examples, 493313 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:36,142 : INFO : EPOCH 13 - PROGRESS: at 78.65% examples, 492435 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:37,158 : INFO : EPOCH 13 - PROGRESS: at 80.68% examples, 491734 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:38,172 : INFO : EPOCH 13 - PROGRESS: at 82.87% examples, 491616 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:39,181 : INFO : EPOCH 13 - PROGRESS: at 85.05% examples, 491582 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:40,188 : INFO : EPOCH 13 - PROGRESS: at 87.13% examples, 491082 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:41,213 : INFO : EPOCH 13 - PROGRESS: at 89.63% examples, 492687 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:06:42,235 : INFO : EPOCH 13 - PROGRESS: at 91.81% examples, 492506 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:43,236 : INFO : EPOCH 13 - PROGRESS: at 94.11% examples, 493193 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:06:44,243 : INFO : EPOCH 13 - PROGRESS: at 96.25% examples, 492760 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:45,249 : INFO : EPOCH 13 - PROGRESS: at 98.55% examples, 493562 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:45,801 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:06:45,842 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:06:45,847 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:06:45,871 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:06:45,871 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:06:45,873 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:06:45,886 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:06:45,892 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:06:45,892 : INFO : EPOCH - 13 : training on 23279529 raw words (22951015 effective words) took 46.5s, 493784 effective words/s
    2020-09-30 23:06:46,995 : INFO : EPOCH 14 - PROGRESS: at 2.37% examples, 501401 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:47,998 : INFO : EPOCH 14 - PROGRESS: at 4.67% examples, 515838 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:49,011 : INFO : EPOCH 14 - PROGRESS: at 6.75% examples, 500124 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:50,017 : INFO : EPOCH 14 - PROGRESS: at 8.90% examples, 499882 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:51,025 : INFO : EPOCH 14 - PROGRESS: at 10.96% examples, 497353 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:52,025 : INFO : EPOCH 14 - PROGRESS: at 13.06% examples, 493469 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:53,039 : INFO : EPOCH 14 - PROGRESS: at 15.21% examples, 492584 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:54,041 : INFO : EPOCH 14 - PROGRESS: at 17.58% examples, 500898 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:55,069 : INFO : EPOCH 14 - PROGRESS: at 19.99% examples, 506126 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:56,073 : INFO : EPOCH 14 - PROGRESS: at 22.23% examples, 505760 words/s, in_qsize 15, out_qsize 1
    2020-09-30 23:06:57,088 : INFO : EPOCH 14 - PROGRESS: at 24.71% examples, 509266 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:06:58,091 : INFO : EPOCH 14 - PROGRESS: at 26.90% examples, 509339 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:06:59,093 : INFO : EPOCH 14 - PROGRESS: at 29.05% examples, 508504 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:00,127 : INFO : EPOCH 14 - PROGRESS: at 31.26% examples, 506955 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:01,138 : INFO : EPOCH 14 - PROGRESS: at 33.62% examples, 508138 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:07:02,173 : INFO : EPOCH 14 - PROGRESS: at 36.09% examples, 510266 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:03,185 : INFO : EPOCH 14 - PROGRESS: at 38.49% examples, 512420 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:04,196 : INFO : EPOCH 14 - PROGRESS: at 40.60% examples, 510457 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:05,206 : INFO : EPOCH 14 - PROGRESS: at 42.49% examples, 506407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:06,227 : INFO : EPOCH 14 - PROGRESS: at 44.53% examples, 503776 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:07,240 : INFO : EPOCH 14 - PROGRESS: at 46.75% examples, 503383 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:08,245 : INFO : EPOCH 14 - PROGRESS: at 49.09% examples, 505028 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:09,251 : INFO : EPOCH 14 - PROGRESS: at 51.23% examples, 503596 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:10,280 : INFO : EPOCH 14 - PROGRESS: at 53.30% examples, 501874 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:11,325 : INFO : EPOCH 14 - PROGRESS: at 55.80% examples, 504060 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:12,338 : INFO : EPOCH 14 - PROGRESS: at 57.86% examples, 503118 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:13,358 : INFO : EPOCH 14 - PROGRESS: at 60.20% examples, 504416 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:07:14,364 : INFO : EPOCH 14 - PROGRESS: at 62.60% examples, 505689 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:15,432 : INFO : EPOCH 14 - PROGRESS: at 65.09% examples, 506409 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:16,442 : INFO : EPOCH 14 - PROGRESS: at 67.28% examples, 506148 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:17,466 : INFO : EPOCH 14 - PROGRESS: at 69.41% examples, 505402 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:18,470 : INFO : EPOCH 14 - PROGRESS: at 71.69% examples, 505882 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:19,476 : INFO : EPOCH 14 - PROGRESS: at 73.69% examples, 504858 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:20,489 : INFO : EPOCH 14 - PROGRESS: at 75.69% examples, 503490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:21,493 : INFO : EPOCH 14 - PROGRESS: at 77.83% examples, 502906 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:22,498 : INFO : EPOCH 14 - PROGRESS: at 79.92% examples, 502305 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:23,501 : INFO : EPOCH 14 - PROGRESS: at 82.25% examples, 502862 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:24,514 : INFO : EPOCH 14 - PROGRESS: at 84.31% examples, 501965 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:25,555 : INFO : EPOCH 14 - PROGRESS: at 86.51% examples, 501246 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:26,573 : INFO : EPOCH 14 - PROGRESS: at 88.75% examples, 501323 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:27,586 : INFO : EPOCH 14 - PROGRESS: at 91.17% examples, 502395 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:28,590 : INFO : EPOCH 14 - PROGRESS: at 93.32% examples, 501921 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:29,599 : INFO : EPOCH 14 - PROGRESS: at 95.58% examples, 501898 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:30,613 : INFO : EPOCH 14 - PROGRESS: at 97.70% examples, 501587 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:31,548 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:07:31,561 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:07:31,575 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:07:31,587 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:07:31,601 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:07:31,615 : INFO : EPOCH 14 - PROGRESS: at 99.91% examples, 501592 words/s, in_qsize 2, out_qsize 1
    2020-09-30 23:07:31,615 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:07:31,621 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:07:31,626 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:07:31,627 : INFO : EPOCH - 14 : training on 23279529 raw words (22951015 effective words) took 45.7s, 501871 effective words/s
    2020-09-30 23:07:32,638 : INFO : EPOCH 15 - PROGRESS: at 2.09% examples, 488539 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:33,642 : INFO : EPOCH 15 - PROGRESS: at 4.38% examples, 505310 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:34,655 : INFO : EPOCH 15 - PROGRESS: at 6.58% examples, 502207 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:07:35,656 : INFO : EPOCH 15 - PROGRESS: at 8.89% examples, 511723 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:36,670 : INFO : EPOCH 15 - PROGRESS: at 11.13% examples, 513878 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:37,671 : INFO : EPOCH 15 - PROGRESS: at 13.39% examples, 513540 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:38,734 : INFO : EPOCH 15 - PROGRESS: at 15.67% examples, 510319 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:39,746 : INFO : EPOCH 15 - PROGRESS: at 17.89% examples, 511120 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:40,753 : INFO : EPOCH 15 - PROGRESS: at 19.99% examples, 508903 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:41,757 : INFO : EPOCH 15 - PROGRESS: at 22.19% examples, 507359 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:42,795 : INFO : EPOCH 15 - PROGRESS: at 24.37% examples, 503494 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:43,813 : INFO : EPOCH 15 - PROGRESS: at 26.49% examples, 502660 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:07:44,814 : INFO : EPOCH 15 - PROGRESS: at 28.84% examples, 505394 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:45,818 : INFO : EPOCH 15 - PROGRESS: at 30.94% examples, 503706 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:46,820 : INFO : EPOCH 15 - PROGRESS: at 33.23% examples, 504130 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:47,826 : INFO : EPOCH 15 - PROGRESS: at 35.72% examples, 507490 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:07:48,831 : INFO : EPOCH 15 - PROGRESS: at 37.78% examples, 505385 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:07:49,837 : INFO : EPOCH 15 - PROGRESS: at 40.20% examples, 507872 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:50,840 : INFO : EPOCH 15 - PROGRESS: at 42.35% examples, 507011 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:51,845 : INFO : EPOCH 15 - PROGRESS: at 44.45% examples, 505702 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:52,868 : INFO : EPOCH 15 - PROGRESS: at 46.54% examples, 503603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:53,903 : INFO : EPOCH 15 - PROGRESS: at 48.88% examples, 504561 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:54,932 : INFO : EPOCH 15 - PROGRESS: at 51.10% examples, 503535 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:55,936 : INFO : EPOCH 15 - PROGRESS: at 53.13% examples, 501889 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:56,943 : INFO : EPOCH 15 - PROGRESS: at 55.41% examples, 502583 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:57,948 : INFO : EPOCH 15 - PROGRESS: at 57.63% examples, 503287 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:07:58,949 : INFO : EPOCH 15 - PROGRESS: at 59.95% examples, 504936 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:07:59,950 : INFO : EPOCH 15 - PROGRESS: at 62.39% examples, 506620 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:00,970 : INFO : EPOCH 15 - PROGRESS: at 64.55% examples, 505517 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:01,985 : INFO : EPOCH 15 - PROGRESS: at 66.91% examples, 506436 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:03,004 : INFO : EPOCH 15 - PROGRESS: at 69.06% examples, 505770 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:04,015 : INFO : EPOCH 15 - PROGRESS: at 71.24% examples, 505843 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:05,021 : INFO : EPOCH 15 - PROGRESS: at 73.20% examples, 504245 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:06,029 : INFO : EPOCH 15 - PROGRESS: at 75.27% examples, 503513 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:07,051 : INFO : EPOCH 15 - PROGRESS: at 77.42% examples, 502933 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:08,062 : INFO : EPOCH 15 - PROGRESS: at 79.65% examples, 502824 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:09,070 : INFO : EPOCH 15 - PROGRESS: at 81.91% examples, 503004 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:10,090 : INFO : EPOCH 15 - PROGRESS: at 84.08% examples, 502761 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:11,098 : INFO : EPOCH 15 - PROGRESS: at 86.47% examples, 503434 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:12,120 : INFO : EPOCH 15 - PROGRESS: at 88.89% examples, 504573 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:13,125 : INFO : EPOCH 15 - PROGRESS: at 91.31% examples, 505448 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:14,137 : INFO : EPOCH 15 - PROGRESS: at 93.40% examples, 504589 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:15,143 : INFO : EPOCH 15 - PROGRESS: at 95.90% examples, 505876 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:08:16,156 : INFO : EPOCH 15 - PROGRESS: at 97.87% examples, 504615 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:16,969 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:08:16,996 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:08:17,016 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:08:17,017 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:08:17,025 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:08:17,032 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:08:17,033 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:08:17,047 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:08:17,047 : INFO : EPOCH - 15 : training on 23279529 raw words (22951015 effective words) took 45.4s, 505339 effective words/s
    2020-09-30 23:08:18,083 : INFO : EPOCH 16 - PROGRESS: at 2.01% examples, 458709 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:19,085 : INFO : EPOCH 16 - PROGRESS: at 4.27% examples, 485413 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:20,091 : INFO : EPOCH 16 - PROGRESS: at 6.50% examples, 493328 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:21,094 : INFO : EPOCH 16 - PROGRESS: at 8.72% examples, 499947 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:22,109 : INFO : EPOCH 16 - PROGRESS: at 11.01% examples, 506167 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:23,120 : INFO : EPOCH 16 - PROGRESS: at 13.48% examples, 514301 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:24,161 : INFO : EPOCH 16 - PROGRESS: at 15.58% examples, 507073 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:25,165 : INFO : EPOCH 16 - PROGRESS: at 17.84% examples, 509965 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:08:26,188 : INFO : EPOCH 16 - PROGRESS: at 20.08% examples, 510232 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:08:27,189 : INFO : EPOCH 16 - PROGRESS: at 22.37% examples, 510534 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:08:28,224 : INFO : EPOCH 16 - PROGRESS: at 24.75% examples, 510981 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:29,234 : INFO : EPOCH 16 - PROGRESS: at 26.98% examples, 511380 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:30,234 : INFO : EPOCH 16 - PROGRESS: at 29.17% examples, 511244 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:31,248 : INFO : EPOCH 16 - PROGRESS: at 31.56% examples, 512950 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:32,252 : INFO : EPOCH 16 - PROGRESS: at 33.95% examples, 513986 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:33,254 : INFO : EPOCH 16 - PROGRESS: at 36.27% examples, 514974 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:34,293 : INFO : EPOCH 16 - PROGRESS: at 38.62% examples, 515481 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:35,311 : INFO : EPOCH 16 - PROGRESS: at 40.68% examples, 512632 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:36,318 : INFO : EPOCH 16 - PROGRESS: at 42.75% examples, 510529 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:37,324 : INFO : EPOCH 16 - PROGRESS: at 45.23% examples, 513323 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:38,348 : INFO : EPOCH 16 - PROGRESS: at 47.52% examples, 513171 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:08:39,349 : INFO : EPOCH 16 - PROGRESS: at 49.93% examples, 514427 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:40,364 : INFO : EPOCH 16 - PROGRESS: at 52.15% examples, 513671 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:41,378 : INFO : EPOCH 16 - PROGRESS: at 54.46% examples, 514173 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:42,382 : INFO : EPOCH 16 - PROGRESS: at 56.64% examples, 513671 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:43,415 : INFO : EPOCH 16 - PROGRESS: at 59.03% examples, 514834 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:44,429 : INFO : EPOCH 16 - PROGRESS: at 61.13% examples, 513776 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:08:45,470 : INFO : EPOCH 16 - PROGRESS: at 63.47% examples, 513351 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:46,471 : INFO : EPOCH 16 - PROGRESS: at 65.61% examples, 512356 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:47,472 : INFO : EPOCH 16 - PROGRESS: at 67.83% examples, 512358 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:48,490 : INFO : EPOCH 16 - PROGRESS: at 70.02% examples, 511834 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:08:49,500 : INFO : EPOCH 16 - PROGRESS: at 72.40% examples, 512897 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:50,516 : INFO : EPOCH 16 - PROGRESS: at 74.48% examples, 512074 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:08:51,546 : INFO : EPOCH 16 - PROGRESS: at 76.67% examples, 511385 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:52,553 : INFO : EPOCH 16 - PROGRESS: at 78.77% examples, 510251 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:53,562 : INFO : EPOCH 16 - PROGRESS: at 81.17% examples, 511288 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:54,570 : INFO : EPOCH 16 - PROGRESS: at 83.44% examples, 511476 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:55,588 : INFO : EPOCH 16 - PROGRESS: at 85.72% examples, 511324 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:56,627 : INFO : EPOCH 16 - PROGRESS: at 88.05% examples, 511344 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:57,643 : INFO : EPOCH 16 - PROGRESS: at 90.37% examples, 511645 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:58,671 : INFO : EPOCH 16 - PROGRESS: at 92.80% examples, 512064 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:08:59,713 : INFO : EPOCH 16 - PROGRESS: at 95.11% examples, 511856 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:00,722 : INFO : EPOCH 16 - PROGRESS: at 97.55% examples, 512716 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:01,696 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:09:01,706 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:09:01,718 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:09:01,726 : INFO : EPOCH 16 - PROGRESS: at 99.81% examples, 512875 words/s, in_qsize 4, out_qsize 1
    2020-09-30 23:09:01,726 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:09:01,734 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:09:01,745 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:09:01,755 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:09:01,756 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:09:01,757 : INFO : EPOCH - 16 : training on 23279529 raw words (22951015 effective words) took 44.7s, 513374 effective words/s
    2020-09-30 23:09:02,771 : INFO : EPOCH 17 - PROGRESS: at 2.09% examples, 487384 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:03,786 : INFO : EPOCH 17 - PROGRESS: at 4.43% examples, 506745 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:04,872 : INFO : EPOCH 17 - PROGRESS: at 6.90% examples, 513106 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:09:05,907 : INFO : EPOCH 17 - PROGRESS: at 9.07% examples, 508349 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:06,922 : INFO : EPOCH 17 - PROGRESS: at 11.42% examples, 514917 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:07,968 : INFO : EPOCH 17 - PROGRESS: at 13.56% examples, 506022 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:08,969 : INFO : EPOCH 17 - PROGRESS: at 15.83% examples, 508233 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:09,979 : INFO : EPOCH 17 - PROGRESS: at 17.97% examples, 507086 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:10,982 : INFO : EPOCH 17 - PROGRESS: at 20.50% examples, 516068 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:11,984 : INFO : EPOCH 17 - PROGRESS: at 22.70% examples, 513809 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:09:12,990 : INFO : EPOCH 17 - PROGRESS: at 25.28% examples, 519490 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:13,998 : INFO : EPOCH 17 - PROGRESS: at 27.49% examples, 518580 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:15,016 : INFO : EPOCH 17 - PROGRESS: at 29.64% examples, 516574 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:16,022 : INFO : EPOCH 17 - PROGRESS: at 31.95% examples, 516732 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:17,031 : INFO : EPOCH 17 - PROGRESS: at 34.40% examples, 518604 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:18,048 : INFO : EPOCH 17 - PROGRESS: at 36.74% examples, 518882 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:19,072 : INFO : EPOCH 17 - PROGRESS: at 38.79% examples, 515644 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:09:20,075 : INFO : EPOCH 17 - PROGRESS: at 40.88% examples, 513777 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:21,114 : INFO : EPOCH 17 - PROGRESS: at 43.00% examples, 511274 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:22,124 : INFO : EPOCH 17 - PROGRESS: at 45.23% examples, 511079 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:23,129 : INFO : EPOCH 17 - PROGRESS: at 47.58% examples, 511910 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:24,135 : INFO : EPOCH 17 - PROGRESS: at 49.69% examples, 510490 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:09:25,157 : INFO : EPOCH 17 - PROGRESS: at 51.91% examples, 509413 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:26,160 : INFO : EPOCH 17 - PROGRESS: at 54.05% examples, 508630 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:27,177 : INFO : EPOCH 17 - PROGRESS: at 56.34% examples, 509282 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:28,186 : INFO : EPOCH 17 - PROGRESS: at 58.73% examples, 511100 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:29,202 : INFO : EPOCH 17 - PROGRESS: at 61.05% examples, 511897 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:09:30,211 : INFO : EPOCH 17 - PROGRESS: at 63.42% examples, 512444 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:09:31,246 : INFO : EPOCH 17 - PROGRESS: at 65.64% examples, 511544 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:32,252 : INFO : EPOCH 17 - PROGRESS: at 67.87% examples, 511471 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:33,266 : INFO : EPOCH 17 - PROGRESS: at 70.23% examples, 512307 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:09:34,268 : INFO : EPOCH 17 - PROGRESS: at 72.36% examples, 511682 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:35,272 : INFO : EPOCH 17 - PROGRESS: at 74.34% examples, 510481 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:36,276 : INFO : EPOCH 17 - PROGRESS: at 76.67% examples, 511089 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:37,297 : INFO : EPOCH 17 - PROGRESS: at 78.76% examples, 509783 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:09:38,326 : INFO : EPOCH 17 - PROGRESS: at 80.55% examples, 506806 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:39,340 : INFO : EPOCH 17 - PROGRESS: at 82.51% examples, 504728 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:40,357 : INFO : EPOCH 17 - PROGRESS: at 84.71% examples, 504489 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:41,357 : INFO : EPOCH 17 - PROGRESS: at 86.92% examples, 504488 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:42,379 : INFO : EPOCH 17 - PROGRESS: at 89.18% examples, 504637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:43,413 : INFO : EPOCH 17 - PROGRESS: at 91.59% examples, 505172 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:44,419 : INFO : EPOCH 17 - PROGRESS: at 93.81% examples, 505053 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:45,423 : INFO : EPOCH 17 - PROGRESS: at 96.28% examples, 506150 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:46,426 : INFO : EPOCH 17 - PROGRESS: at 98.58% examples, 506706 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:46,935 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:09:46,952 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:09:46,958 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:09:46,964 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:09:46,964 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:09:46,993 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:09:47,004 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:09:47,005 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:09:47,006 : INFO : EPOCH - 17 : training on 23279529 raw words (22951015 effective words) took 45.2s, 507256 effective words/s
    2020-09-30 23:09:48,014 : INFO : EPOCH 18 - PROGRESS: at 2.01% examples, 471454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:49,023 : INFO : EPOCH 18 - PROGRESS: at 4.19% examples, 480747 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:50,199 : INFO : EPOCH 18 - PROGRESS: at 6.03% examples, 437065 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:51,223 : INFO : EPOCH 18 - PROGRESS: at 7.51% examples, 413217 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:52,225 : INFO : EPOCH 18 - PROGRESS: at 9.37% examples, 417051 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:53,231 : INFO : EPOCH 18 - PROGRESS: at 11.17% examples, 417822 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:54,237 : INFO : EPOCH 18 - PROGRESS: at 13.02% examples, 417087 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:55,270 : INFO : EPOCH 18 - PROGRESS: at 15.09% examples, 422362 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:56,275 : INFO : EPOCH 18 - PROGRESS: at 16.75% examples, 419486 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:57,308 : INFO : EPOCH 18 - PROGRESS: at 18.41% examples, 414143 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:58,320 : INFO : EPOCH 18 - PROGRESS: at 19.89% examples, 407919 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:09:59,336 : INFO : EPOCH 18 - PROGRESS: at 21.88% examples, 411291 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:00,381 : INFO : EPOCH 18 - PROGRESS: at 23.22% examples, 401526 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:01,383 : INFO : EPOCH 18 - PROGRESS: at 24.98% examples, 401214 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:02,392 : INFO : EPOCH 18 - PROGRESS: at 26.72% examples, 401231 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:03,403 : INFO : EPOCH 18 - PROGRESS: at 28.61% examples, 402913 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:10:04,410 : INFO : EPOCH 18 - PROGRESS: at 30.27% examples, 401835 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:05,415 : INFO : EPOCH 18 - PROGRESS: at 32.18% examples, 403455 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:06,420 : INFO : EPOCH 18 - PROGRESS: at 33.19% examples, 394005 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:07,427 : INFO : EPOCH 18 - PROGRESS: at 34.27% examples, 386430 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:08,433 : INFO : EPOCH 18 - PROGRESS: at 35.16% examples, 377794 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:09,479 : INFO : EPOCH 18 - PROGRESS: at 36.40% examples, 372657 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:10,484 : INFO : EPOCH 18 - PROGRESS: at 37.41% examples, 366603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:11,499 : INFO : EPOCH 18 - PROGRESS: at 38.33% examples, 360141 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:12,512 : INFO : EPOCH 18 - PROGRESS: at 39.56% examples, 356842 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:13,597 : INFO : EPOCH 18 - PROGRESS: at 40.72% examples, 352447 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:14,639 : INFO : EPOCH 18 - PROGRESS: at 41.83% examples, 348287 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:15,740 : INFO : EPOCH 18 - PROGRESS: at 42.92% examples, 343731 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:16,806 : INFO : EPOCH 18 - PROGRESS: at 43.86% examples, 338505 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:10:17,820 : INFO : EPOCH 18 - PROGRESS: at 44.97% examples, 335872 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:18,843 : INFO : EPOCH 18 - PROGRESS: at 46.38% examples, 334792 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:19,857 : INFO : EPOCH 18 - PROGRESS: at 47.61% examples, 333317 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:20,883 : INFO : EPOCH 18 - PROGRESS: at 48.97% examples, 332339 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:21,935 : INFO : EPOCH 18 - PROGRESS: at 50.44% examples, 331771 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:22,966 : INFO : EPOCH 18 - PROGRESS: at 51.95% examples, 331717 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:24,036 : INFO : EPOCH 18 - PROGRESS: at 53.39% examples, 331009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:25,049 : INFO : EPOCH 18 - PROGRESS: at 54.43% examples, 328554 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:26,064 : INFO : EPOCH 18 - PROGRESS: at 55.76% examples, 327950 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:27,065 : INFO : EPOCH 18 - PROGRESS: at 56.96% examples, 326778 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:28,073 : INFO : EPOCH 18 - PROGRESS: at 58.16% examples, 325626 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:29,081 : INFO : EPOCH 18 - PROGRESS: at 59.42% examples, 324887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:30,095 : INFO : EPOCH 18 - PROGRESS: at 60.62% examples, 323772 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:31,144 : INFO : EPOCH 18 - PROGRESS: at 62.04% examples, 323308 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:32,394 : INFO : EPOCH 18 - PROGRESS: at 63.30% examples, 320590 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:33,474 : INFO : EPOCH 18 - PROGRESS: at 64.55% examples, 319201 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:34,505 : INFO : EPOCH 18 - PROGRESS: at 65.72% examples, 317981 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:10:35,587 : INFO : EPOCH 18 - PROGRESS: at 66.67% examples, 315256 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:36,615 : INFO : EPOCH 18 - PROGRESS: at 67.62% examples, 313221 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:10:37,682 : INFO : EPOCH 18 - PROGRESS: at 68.78% examples, 311803 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:38,695 : INFO : EPOCH 18 - PROGRESS: at 69.86% examples, 310582 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:10:39,781 : INFO : EPOCH 18 - PROGRESS: at 70.86% examples, 308594 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:40,835 : INFO : EPOCH 18 - PROGRESS: at 71.56% examples, 305603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:41,856 : INFO : EPOCH 18 - PROGRESS: at 72.77% examples, 305035 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:42,913 : INFO : EPOCH 18 - PROGRESS: at 73.89% examples, 304116 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:43,967 : INFO : EPOCH 18 - PROGRESS: at 75.02% examples, 303062 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:44,986 : INFO : EPOCH 18 - PROGRESS: at 76.15% examples, 302265 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:46,021 : INFO : EPOCH 18 - PROGRESS: at 76.94% examples, 300066 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:10:47,048 : INFO : EPOCH 18 - PROGRESS: at 77.89% examples, 298336 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:48,059 : INFO : EPOCH 18 - PROGRESS: at 78.69% examples, 296417 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:49,070 : INFO : EPOCH 18 - PROGRESS: at 79.77% examples, 295644 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:50,127 : INFO : EPOCH 18 - PROGRESS: at 80.73% examples, 294215 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:51,230 : INFO : EPOCH 18 - PROGRESS: at 81.75% examples, 292636 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:52,234 : INFO : EPOCH 18 - PROGRESS: at 82.47% examples, 290650 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:53,308 : INFO : EPOCH 18 - PROGRESS: at 83.28% examples, 288867 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:54,349 : INFO : EPOCH 18 - PROGRESS: at 84.26% examples, 287717 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:55,427 : INFO : EPOCH 18 - PROGRESS: at 85.26% examples, 286438 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:56,443 : INFO : EPOCH 18 - PROGRESS: at 86.12% examples, 285049 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:57,474 : INFO : EPOCH 18 - PROGRESS: at 86.95% examples, 283608 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:58,522 : INFO : EPOCH 18 - PROGRESS: at 88.09% examples, 283120 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:10:59,545 : INFO : EPOCH 18 - PROGRESS: at 89.05% examples, 282185 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:00,593 : INFO : EPOCH 18 - PROGRESS: at 90.06% examples, 281318 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:01,674 : INFO : EPOCH 18 - PROGRESS: at 91.17% examples, 280517 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:02,712 : INFO : EPOCH 18 - PROGRESS: at 92.18% examples, 279737 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:03,747 : INFO : EPOCH 18 - PROGRESS: at 92.93% examples, 278111 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:04,765 : INFO : EPOCH 18 - PROGRESS: at 93.75% examples, 276835 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:05,809 : INFO : EPOCH 18 - PROGRESS: at 94.46% examples, 275269 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:06,812 : INFO : EPOCH 18 - PROGRESS: at 95.53% examples, 274733 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:07,899 : INFO : EPOCH 18 - PROGRESS: at 96.19% examples, 272964 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:11:09,046 : INFO : EPOCH 18 - PROGRESS: at 96.87% examples, 271045 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:10,126 : INFO : EPOCH 18 - PROGRESS: at 97.92% examples, 270431 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:11,213 : INFO : EPOCH 18 - PROGRESS: at 98.97% examples, 269813 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:11,940 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:11:11,944 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:11:11,949 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:11:12,011 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:11:12,030 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:11:12,033 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:11:12,049 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:11:12,068 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:11:12,069 : INFO : EPOCH - 18 : training on 23279529 raw words (22951015 effective words) took 85.1s, 269818 effective words/s
    2020-09-30 23:11:13,082 : INFO : EPOCH 19 - PROGRESS: at 0.86% examples, 203295 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:14,132 : INFO : EPOCH 19 - PROGRESS: at 1.78% examples, 202215 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:15,193 : INFO : EPOCH 19 - PROGRESS: at 2.91% examples, 217435 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:16,221 : INFO : EPOCH 19 - PROGRESS: at 3.86% examples, 217234 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:17,388 : INFO : EPOCH 19 - PROGRESS: at 4.47% examples, 195142 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:18,401 : INFO : EPOCH 19 - PROGRESS: at 5.32% examples, 195981 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:19,442 : INFO : EPOCH 19 - PROGRESS: at 5.85% examples, 184040 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:20,535 : INFO : EPOCH 19 - PROGRESS: at 6.54% examples, 178521 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:21,632 : INFO : EPOCH 19 - PROGRESS: at 7.35% examples, 178253 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:22,687 : INFO : EPOCH 19 - PROGRESS: at 8.34% examples, 182239 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:23,690 : INFO : EPOCH 19 - PROGRESS: at 9.32% examples, 186454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:24,714 : INFO : EPOCH 19 - PROGRESS: at 10.02% examples, 184291 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:11:25,800 : INFO : EPOCH 19 - PROGRESS: at 10.66% examples, 180962 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:26,812 : INFO : EPOCH 19 - PROGRESS: at 11.55% examples, 182346 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:11:27,819 : INFO : EPOCH 19 - PROGRESS: at 12.62% examples, 185972 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:28,822 : INFO : EPOCH 19 - PROGRESS: at 13.43% examples, 185844 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:29,822 : INFO : EPOCH 19 - PROGRESS: at 14.33% examples, 186807 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:11:30,873 : INFO : EPOCH 19 - PROGRESS: at 15.54% examples, 191307 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:31,873 : INFO : EPOCH 19 - PROGRESS: at 16.68% examples, 195364 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:32,956 : INFO : EPOCH 19 - PROGRESS: at 17.62% examples, 195867 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:33,994 : INFO : EPOCH 19 - PROGRESS: at 18.57% examples, 196373 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:35,006 : INFO : EPOCH 19 - PROGRESS: at 19.34% examples, 195327 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:36,019 : INFO : EPOCH 19 - PROGRESS: at 20.28% examples, 196735 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:11:37,038 : INFO : EPOCH 19 - PROGRESS: at 21.06% examples, 195726 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:38,064 : INFO : EPOCH 19 - PROGRESS: at 21.98% examples, 195825 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:39,126 : INFO : EPOCH 19 - PROGRESS: at 22.74% examples, 194549 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:40,160 : INFO : EPOCH 19 - PROGRESS: at 23.62% examples, 194274 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:41,160 : INFO : EPOCH 19 - PROGRESS: at 24.41% examples, 193623 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:42,185 : INFO : EPOCH 19 - PROGRESS: at 25.51% examples, 195670 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:43,204 : INFO : EPOCH 19 - PROGRESS: at 26.29% examples, 195170 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:44,237 : INFO : EPOCH 19 - PROGRESS: at 27.20% examples, 195236 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:45,345 : INFO : EPOCH 19 - PROGRESS: at 27.99% examples, 194235 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:46,413 : INFO : EPOCH 19 - PROGRESS: at 29.01% examples, 195178 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:47,455 : INFO : EPOCH 19 - PROGRESS: at 29.94% examples, 195470 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:48,511 : INFO : EPOCH 19 - PROGRESS: at 30.95% examples, 196161 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:49,559 : INFO : EPOCH 19 - PROGRESS: at 32.16% examples, 197873 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:11:50,610 : INFO : EPOCH 19 - PROGRESS: at 33.19% examples, 198477 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:51,627 : INFO : EPOCH 19 - PROGRESS: at 34.23% examples, 199257 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:52,747 : INFO : EPOCH 19 - PROGRESS: at 35.40% examples, 200413 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:53,751 : INFO : EPOCH 19 - PROGRESS: at 36.52% examples, 201628 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:54,757 : INFO : EPOCH 19 - PROGRESS: at 37.55% examples, 202324 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:55,838 : INFO : EPOCH 19 - PROGRESS: at 38.70% examples, 203551 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:56,847 : INFO : EPOCH 19 - PROGRESS: at 39.70% examples, 204146 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:57,873 : INFO : EPOCH 19 - PROGRESS: at 40.64% examples, 204195 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:11:58,949 : INFO : EPOCH 19 - PROGRESS: at 41.61% examples, 204271 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:11:59,954 : INFO : EPOCH 19 - PROGRESS: at 42.71% examples, 205260 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:12:00,960 : INFO : EPOCH 19 - PROGRESS: at 43.90% examples, 206556 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:01,991 : INFO : EPOCH 19 - PROGRESS: at 44.88% examples, 206942 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:03,000 : INFO : EPOCH 19 - PROGRESS: at 45.61% examples, 206068 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:12:04,041 : INFO : EPOCH 19 - PROGRESS: at 46.43% examples, 205285 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:05,145 : INFO : EPOCH 19 - PROGRESS: at 47.32% examples, 205035 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:06,184 : INFO : EPOCH 19 - PROGRESS: at 48.30% examples, 205203 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:07,209 : INFO : EPOCH 19 - PROGRESS: at 49.09% examples, 204726 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:08,257 : INFO : EPOCH 19 - PROGRESS: at 49.88% examples, 204003 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:09,442 : INFO : EPOCH 19 - PROGRESS: at 50.70% examples, 203017 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:10,449 : INFO : EPOCH 19 - PROGRESS: at 51.62% examples, 203007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:11,466 : INFO : EPOCH 19 - PROGRESS: at 52.37% examples, 202454 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:12,525 : INFO : EPOCH 19 - PROGRESS: at 53.35% examples, 202599 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:13,542 : INFO : EPOCH 19 - PROGRESS: at 54.39% examples, 203183 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:14,550 : INFO : EPOCH 19 - PROGRESS: at 55.45% examples, 203789 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:15,566 : INFO : EPOCH 19 - PROGRESS: at 56.47% examples, 204340 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:16,568 : INFO : EPOCH 19 - PROGRESS: at 57.38% examples, 204479 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:17,622 : INFO : EPOCH 19 - PROGRESS: at 58.39% examples, 204880 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:18,683 : INFO : EPOCH 19 - PROGRESS: at 59.11% examples, 204070 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:19,685 : INFO : EPOCH 19 - PROGRESS: at 60.11% examples, 204609 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:20,709 : INFO : EPOCH 19 - PROGRESS: at 61.27% examples, 205374 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:21,717 : INFO : EPOCH 19 - PROGRESS: at 62.26% examples, 205603 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:22,718 : INFO : EPOCH 19 - PROGRESS: at 63.20% examples, 205706 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:12:23,760 : INFO : EPOCH 19 - PROGRESS: at 64.09% examples, 205543 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:24,827 : INFO : EPOCH 19 - PROGRESS: at 64.77% examples, 204542 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:25,971 : INFO : EPOCH 19 - PROGRESS: at 65.56% examples, 203859 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:27,015 : INFO : EPOCH 19 - PROGRESS: at 66.33% examples, 203334 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:28,041 : INFO : EPOCH 19 - PROGRESS: at 67.03% examples, 202753 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:29,220 : INFO : EPOCH 19 - PROGRESS: at 67.75% examples, 201792 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:30,356 : INFO : EPOCH 19 - PROGRESS: at 68.43% examples, 200847 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:31,429 : INFO : EPOCH 19 - PROGRESS: at 68.98% examples, 199724 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:32,437 : INFO : EPOCH 19 - PROGRESS: at 69.45% examples, 198668 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:33,506 : INFO : EPOCH 19 - PROGRESS: at 70.40% examples, 198683 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:34,671 : INFO : EPOCH 19 - PROGRESS: at 70.81% examples, 197054 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:12:35,832 : INFO : EPOCH 19 - PROGRESS: at 71.38% examples, 195934 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:36,984 : INFO : EPOCH 19 - PROGRESS: at 71.99% examples, 194877 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:37,986 : INFO : EPOCH 19 - PROGRESS: at 72.52% examples, 194071 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:39,014 : INFO : EPOCH 19 - PROGRESS: at 72.89% examples, 192775 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:40,174 : INFO : EPOCH 19 - PROGRESS: at 73.33% examples, 191451 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:41,309 : INFO : EPOCH 19 - PROGRESS: at 73.77% examples, 190205 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:42,388 : INFO : EPOCH 19 - PROGRESS: at 74.27% examples, 189217 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:43,414 : INFO : EPOCH 19 - PROGRESS: at 74.65% examples, 188046 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:44,540 : INFO : EPOCH 19 - PROGRESS: at 75.13% examples, 187007 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:45,593 : INFO : EPOCH 19 - PROGRESS: at 75.78% examples, 186459 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:46,689 : INFO : EPOCH 19 - PROGRESS: at 76.32% examples, 185637 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:47,692 : INFO : EPOCH 19 - PROGRESS: at 77.03% examples, 185401 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:48,715 : INFO : EPOCH 19 - PROGRESS: at 77.87% examples, 185347 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:49,733 : INFO : EPOCH 19 - PROGRESS: at 78.65% examples, 185206 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:50,744 : INFO : EPOCH 19 - PROGRESS: at 79.42% examples, 185075 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:51,756 : INFO : EPOCH 19 - PROGRESS: at 80.25% examples, 185224 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:52,766 : INFO : EPOCH 19 - PROGRESS: at 81.04% examples, 185111 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:12:53,829 : INFO : EPOCH 19 - PROGRESS: at 82.08% examples, 185461 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:54,851 : INFO : EPOCH 19 - PROGRESS: at 82.55% examples, 184649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:55,901 : INFO : EPOCH 19 - PROGRESS: at 83.49% examples, 184927 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:56,916 : INFO : EPOCH 19 - PROGRESS: at 84.31% examples, 184900 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:57,933 : INFO : EPOCH 19 - PROGRESS: at 85.05% examples, 184675 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:12:58,995 : INFO : EPOCH 19 - PROGRESS: at 85.81% examples, 184484 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:00,166 : INFO : EPOCH 19 - PROGRESS: at 86.42% examples, 183739 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:01,174 : INFO : EPOCH 19 - PROGRESS: at 87.20% examples, 183718 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:13:02,204 : INFO : EPOCH 19 - PROGRESS: at 87.84% examples, 183318 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:03,206 : INFO : EPOCH 19 - PROGRESS: at 88.53% examples, 183060 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:04,247 : INFO : EPOCH 19 - PROGRESS: at 89.19% examples, 182735 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:05,253 : INFO : EPOCH 19 - PROGRESS: at 89.74% examples, 182221 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:06,379 : INFO : EPOCH 19 - PROGRESS: at 90.37% examples, 181703 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:07,649 : INFO : EPOCH 19 - PROGRESS: at 91.12% examples, 181143 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:08,803 : INFO : EPOCH 19 - PROGRESS: at 91.72% examples, 180517 words/s, in_qsize 16, out_qsize 1
    2020-09-30 23:13:09,810 : INFO : EPOCH 19 - PROGRESS: at 92.51% examples, 180451 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:10,853 : INFO : EPOCH 19 - PROGRESS: at 93.23% examples, 180252 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:11,866 : INFO : EPOCH 19 - PROGRESS: at 93.86% examples, 179940 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:12,895 : INFO : EPOCH 19 - PROGRESS: at 94.56% examples, 179700 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:13,945 : INFO : EPOCH 19 - PROGRESS: at 95.35% examples, 179583 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:14,959 : INFO : EPOCH 19 - PROGRESS: at 96.24% examples, 179764 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:15,986 : INFO : EPOCH 19 - PROGRESS: at 96.87% examples, 179453 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:17,005 : INFO : EPOCH 19 - PROGRESS: at 97.62% examples, 179386 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:18,012 : INFO : EPOCH 19 - PROGRESS: at 98.51% examples, 179563 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:19,025 : INFO : EPOCH 19 - PROGRESS: at 99.45% examples, 179809 words/s, in_qsize 13, out_qsize 0
    2020-09-30 23:13:19,445 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:13:19,463 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:13:19,464 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:13:19,468 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:13:19,572 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:13:19,645 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:13:19,654 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:13:19,661 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:13:19,662 : INFO : EPOCH - 19 : training on 23279529 raw words (22951015 effective words) took 127.6s, 179887 effective words/s
    2020-09-30 23:13:21,047 : INFO : EPOCH 20 - PROGRESS: at 0.70% examples, 120671 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:13:22,048 : INFO : EPOCH 20 - PROGRESS: at 1.47% examples, 146590 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:23,065 : INFO : EPOCH 20 - PROGRESS: at 2.27% examples, 156740 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:24,093 : INFO : EPOCH 20 - PROGRESS: at 2.91% examples, 153298 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:25,187 : INFO : EPOCH 20 - PROGRESS: at 3.61% examples, 152704 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:26,299 : INFO : EPOCH 20 - PROGRESS: at 4.27% examples, 149022 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:13:27,340 : INFO : EPOCH 20 - PROGRESS: at 4.95% examples, 150333 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:28,362 : INFO : EPOCH 20 - PROGRESS: at 5.63% examples, 150425 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:29,412 : INFO : EPOCH 20 - PROGRESS: at 6.40% examples, 152050 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:30,430 : INFO : EPOCH 20 - PROGRESS: at 6.95% examples, 149326 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:13:31,532 : INFO : EPOCH 20 - PROGRESS: at 7.56% examples, 147666 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:13:32,577 : INFO : EPOCH 20 - PROGRESS: at 8.14% examples, 146135 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:33,618 : INFO : EPOCH 20 - PROGRESS: at 8.67% examples, 144263 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:34,892 : INFO : EPOCH 20 - PROGRESS: at 9.20% examples, 140407 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:36,012 : INFO : EPOCH 20 - PROGRESS: at 9.82% examples, 139559 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:37,099 : INFO : EPOCH 20 - PROGRESS: at 10.49% examples, 140272 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:38,183 : INFO : EPOCH 20 - PROGRESS: at 11.17% examples, 140458 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:39,330 : INFO : EPOCH 20 - PROGRESS: at 11.81% examples, 139628 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:40,430 : INFO : EPOCH 20 - PROGRESS: at 12.41% examples, 138758 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:41,507 : INFO : EPOCH 20 - PROGRESS: at 12.98% examples, 137618 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:42,568 : INFO : EPOCH 20 - PROGRESS: at 13.69% examples, 138480 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:43,624 : INFO : EPOCH 20 - PROGRESS: at 14.29% examples, 138024 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:44,759 : INFO : EPOCH 20 - PROGRESS: at 14.96% examples, 137916 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:45,810 : INFO : EPOCH 20 - PROGRESS: at 15.67% examples, 138694 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:46,812 : INFO : EPOCH 20 - PROGRESS: at 16.40% examples, 139985 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:47,837 : INFO : EPOCH 20 - PROGRESS: at 17.04% examples, 140406 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:48,886 : INFO : EPOCH 20 - PROGRESS: at 17.70% examples, 140655 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:49,943 : INFO : EPOCH 20 - PROGRESS: at 18.45% examples, 141230 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:13:50,964 : INFO : EPOCH 20 - PROGRESS: at 19.17% examples, 141900 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:13:52,008 : INFO : EPOCH 20 - PROGRESS: at 19.89% examples, 142687 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:53,038 : INFO : EPOCH 20 - PROGRESS: at 20.63% examples, 143520 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:54,045 : INFO : EPOCH 20 - PROGRESS: at 21.20% examples, 142984 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:55,059 : INFO : EPOCH 20 - PROGRESS: at 21.89% examples, 143270 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:13:56,113 : INFO : EPOCH 20 - PROGRESS: at 22.78% examples, 144682 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:57,155 : INFO : EPOCH 20 - PROGRESS: at 23.50% examples, 144781 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:58,162 : INFO : EPOCH 20 - PROGRESS: at 24.37% examples, 146055 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:13:59,171 : INFO : EPOCH 20 - PROGRESS: at 25.06% examples, 146496 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:00,197 : INFO : EPOCH 20 - PROGRESS: at 25.76% examples, 146814 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:01,217 : INFO : EPOCH 20 - PROGRESS: at 26.49% examples, 147401 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:02,289 : INFO : EPOCH 20 - PROGRESS: at 27.45% examples, 148692 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:03,315 : INFO : EPOCH 20 - PROGRESS: at 28.19% examples, 149153 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:04,343 : INFO : EPOCH 20 - PROGRESS: at 29.01% examples, 150027 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:05,371 : INFO : EPOCH 20 - PROGRESS: at 29.73% examples, 150268 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:06,450 : INFO : EPOCH 20 - PROGRESS: at 30.36% examples, 149898 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:07,464 : INFO : EPOCH 20 - PROGRESS: at 31.26% examples, 150957 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:08,597 : INFO : EPOCH 20 - PROGRESS: at 31.94% examples, 150635 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:09,696 : INFO : EPOCH 20 - PROGRESS: at 32.62% examples, 150384 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:10,810 : INFO : EPOCH 20 - PROGRESS: at 33.49% examples, 150887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:11,818 : INFO : EPOCH 20 - PROGRESS: at 34.44% examples, 152061 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:12,959 : INFO : EPOCH 20 - PROGRESS: at 34.98% examples, 151170 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:14,032 : INFO : EPOCH 20 - PROGRESS: at 35.57% examples, 150665 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:15,056 : INFO : EPOCH 20 - PROGRESS: at 36.14% examples, 150144 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:16,094 : INFO : EPOCH 20 - PROGRESS: at 36.79% examples, 149962 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:17,144 : INFO : EPOCH 20 - PROGRESS: at 37.33% examples, 149410 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:18,165 : INFO : EPOCH 20 - PROGRESS: at 37.90% examples, 149134 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:19,195 : INFO : EPOCH 20 - PROGRESS: at 38.62% examples, 149325 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:20,227 : INFO : EPOCH 20 - PROGRESS: at 39.38% examples, 149662 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:21,229 : INFO : EPOCH 20 - PROGRESS: at 39.93% examples, 149270 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:22,311 : INFO : EPOCH 20 - PROGRESS: at 40.52% examples, 148832 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:23,312 : INFO : EPOCH 20 - PROGRESS: at 41.09% examples, 148632 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:24,333 : INFO : EPOCH 20 - PROGRESS: at 41.74% examples, 148527 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:25,433 : INFO : EPOCH 20 - PROGRESS: at 42.22% examples, 147670 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:26,444 : INFO : EPOCH 20 - PROGRESS: at 42.70% examples, 147170 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:27,482 : INFO : EPOCH 20 - PROGRESS: at 43.47% examples, 147475 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:28,498 : INFO : EPOCH 20 - PROGRESS: at 44.21% examples, 147685 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:29,595 : INFO : EPOCH 20 - PROGRESS: at 44.64% examples, 146896 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:30,753 : INFO : EPOCH 20 - PROGRESS: at 45.66% examples, 147760 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:31,884 : INFO : EPOCH 20 - PROGRESS: at 46.70% examples, 148657 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:32,906 : INFO : EPOCH 20 - PROGRESS: at 47.37% examples, 148713 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:33,915 : INFO : EPOCH 20 - PROGRESS: at 47.98% examples, 148644 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:34,999 : INFO : EPOCH 20 - PROGRESS: at 48.55% examples, 148173 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:36,028 : INFO : EPOCH 20 - PROGRESS: at 49.14% examples, 147954 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:37,144 : INFO : EPOCH 20 - PROGRESS: at 49.69% examples, 147438 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:38,200 : INFO : EPOCH 20 - PROGRESS: at 50.10% examples, 146571 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:39,220 : INFO : EPOCH 20 - PROGRESS: at 50.71% examples, 146408 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:40,246 : INFO : EPOCH 20 - PROGRESS: at 51.19% examples, 145867 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:41,249 : INFO : EPOCH 20 - PROGRESS: at 51.60% examples, 145261 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:42,419 : INFO : EPOCH 20 - PROGRESS: at 52.15% examples, 144725 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:43,614 : INFO : EPOCH 20 - PROGRESS: at 52.83% examples, 144510 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:44,640 : INFO : EPOCH 20 - PROGRESS: at 53.46% examples, 144474 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:45,726 : INFO : EPOCH 20 - PROGRESS: at 54.00% examples, 144106 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:46,924 : INFO : EPOCH 20 - PROGRESS: at 54.55% examples, 143584 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:48,203 : INFO : EPOCH 20 - PROGRESS: at 55.24% examples, 143259 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:49,260 : INFO : EPOCH 20 - PROGRESS: at 55.84% examples, 143185 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:50,266 : INFO : EPOCH 20 - PROGRESS: at 56.21% examples, 142560 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:51,301 : INFO : EPOCH 20 - PROGRESS: at 56.84% examples, 142538 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:52,340 : INFO : EPOCH 20 - PROGRESS: at 57.22% examples, 141887 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:53,409 : INFO : EPOCH 20 - PROGRESS: at 57.79% examples, 141725 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:54,447 : INFO : EPOCH 20 - PROGRESS: at 58.24% examples, 141291 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:55,497 : INFO : EPOCH 20 - PROGRESS: at 58.60% examples, 140649 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:56,548 : INFO : EPOCH 20 - PROGRESS: at 59.15% examples, 140409 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:57,623 : INFO : EPOCH 20 - PROGRESS: at 59.70% examples, 140234 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:14:58,670 : INFO : EPOCH 20 - PROGRESS: at 60.19% examples, 139926 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:14:59,846 : INFO : EPOCH 20 - PROGRESS: at 60.74% examples, 139552 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:15:00,945 : INFO : EPOCH 20 - PROGRESS: at 61.32% examples, 139276 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:01,974 : INFO : EPOCH 20 - PROGRESS: at 62.00% examples, 139387 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:03,025 : INFO : EPOCH 20 - PROGRESS: at 62.47% examples, 139009 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:04,066 : INFO : EPOCH 20 - PROGRESS: at 63.25% examples, 139288 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:05,106 : INFO : EPOCH 20 - PROGRESS: at 63.81% examples, 139097 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:06,189 : INFO : EPOCH 20 - PROGRESS: at 64.46% examples, 139061 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:07,229 : INFO : EPOCH 20 - PROGRESS: at 65.09% examples, 139066 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:08,333 : INFO : EPOCH 20 - PROGRESS: at 65.68% examples, 138902 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:15:09,364 : INFO : EPOCH 20 - PROGRESS: at 66.25% examples, 138737 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:10,377 : INFO : EPOCH 20 - PROGRESS: at 66.82% examples, 138684 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:11,455 : INFO : EPOCH 20 - PROGRESS: at 67.42% examples, 138565 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:12,568 : INFO : EPOCH 20 - PROGRESS: at 68.00% examples, 138402 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:13,603 : INFO : EPOCH 20 - PROGRESS: at 68.69% examples, 138512 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:14,712 : INFO : EPOCH 20 - PROGRESS: at 69.33% examples, 138524 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:15,768 : INFO : EPOCH 20 - PROGRESS: at 69.68% examples, 137937 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:16,802 : INFO : EPOCH 20 - PROGRESS: at 70.19% examples, 137718 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:17,841 : INFO : EPOCH 20 - PROGRESS: at 70.73% examples, 137567 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:18,843 : INFO : EPOCH 20 - PROGRESS: at 71.33% examples, 137626 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:19,927 : INFO : EPOCH 20 - PROGRESS: at 72.03% examples, 137676 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:20,941 : INFO : EPOCH 20 - PROGRESS: at 72.47% examples, 137406 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:22,095 : INFO : EPOCH 20 - PROGRESS: at 73.16% examples, 137453 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:23,147 : INFO : EPOCH 20 - PROGRESS: at 73.80% examples, 137538 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:24,236 : INFO : EPOCH 20 - PROGRESS: at 74.52% examples, 137650 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:15:25,253 : INFO : EPOCH 20 - PROGRESS: at 75.06% examples, 137536 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:15:26,307 : INFO : EPOCH 20 - PROGRESS: at 75.69% examples, 137541 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:27,409 : INFO : EPOCH 20 - PROGRESS: at 76.39% examples, 137652 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:28,487 : INFO : EPOCH 20 - PROGRESS: at 76.99% examples, 137544 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:29,501 : INFO : EPOCH 20 - PROGRESS: at 77.75% examples, 137740 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:30,660 : INFO : EPOCH 20 - PROGRESS: at 78.53% examples, 137857 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:31,679 : INFO : EPOCH 20 - PROGRESS: at 79.37% examples, 138260 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:32,691 : INFO : EPOCH 20 - PROGRESS: at 79.97% examples, 138293 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:33,795 : INFO : EPOCH 20 - PROGRESS: at 80.68% examples, 138386 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:34,835 : INFO : EPOCH 20 - PROGRESS: at 81.41% examples, 138475 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:35,875 : INFO : EPOCH 20 - PROGRESS: at 82.21% examples, 138764 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:36,997 : INFO : EPOCH 20 - PROGRESS: at 82.91% examples, 138826 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:38,022 : INFO : EPOCH 20 - PROGRESS: at 83.53% examples, 138851 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:39,117 : INFO : EPOCH 20 - PROGRESS: at 84.09% examples, 138668 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:15:40,173 : INFO : EPOCH 20 - PROGRESS: at 84.71% examples, 138588 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:41,266 : INFO : EPOCH 20 - PROGRESS: at 85.38% examples, 138617 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:15:42,271 : INFO : EPOCH 20 - PROGRESS: at 86.07% examples, 138730 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:43,272 : INFO : EPOCH 20 - PROGRESS: at 86.83% examples, 138974 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:44,432 : INFO : EPOCH 20 - PROGRESS: at 87.56% examples, 138994 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:45,477 : INFO : EPOCH 20 - PROGRESS: at 88.40% examples, 139329 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:46,540 : INFO : EPOCH 20 - PROGRESS: at 89.10% examples, 139432 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:15:47,554 : INFO : EPOCH 20 - PROGRESS: at 89.86% examples, 139653 words/s, in_qsize 14, out_qsize 1
    2020-09-30 23:15:48,586 : INFO : EPOCH 20 - PROGRESS: at 90.59% examples, 139800 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:49,648 : INFO : EPOCH 20 - PROGRESS: at 91.38% examples, 139976 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:50,765 : INFO : EPOCH 20 - PROGRESS: at 92.19% examples, 140161 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:51,800 : INFO : EPOCH 20 - PROGRESS: at 93.05% examples, 140481 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:52,896 : INFO : EPOCH 20 - PROGRESS: at 93.81% examples, 140613 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:53,928 : INFO : EPOCH 20 - PROGRESS: at 94.56% examples, 140747 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:54,931 : INFO : EPOCH 20 - PROGRESS: at 95.25% examples, 140838 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:55,984 : INFO : EPOCH 20 - PROGRESS: at 96.04% examples, 141009 words/s, in_qsize 16, out_qsize 0
    2020-09-30 23:15:57,047 : INFO : EPOCH 20 - PROGRESS: at 96.74% examples, 141107 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:58,139 : INFO : EPOCH 20 - PROGRESS: at 97.55% examples, 141298 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:15:59,177 : INFO : EPOCH 20 - PROGRESS: at 98.30% examples, 141469 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:16:00,234 : INFO : EPOCH 20 - PROGRESS: at 99.05% examples, 141623 words/s, in_qsize 15, out_qsize 0
    2020-09-30 23:16:01,085 : INFO : worker thread finished; awaiting finish of 7 more threads
    2020-09-30 23:16:01,119 : INFO : worker thread finished; awaiting finish of 6 more threads
    2020-09-30 23:16:01,130 : INFO : worker thread finished; awaiting finish of 5 more threads
    2020-09-30 23:16:01,190 : INFO : worker thread finished; awaiting finish of 4 more threads
    2020-09-30 23:16:01,201 : INFO : worker thread finished; awaiting finish of 3 more threads
    2020-09-30 23:16:01,315 : INFO : EPOCH 20 - PROGRESS: at 99.92% examples, 141871 words/s, in_qsize 2, out_qsize 1
    2020-09-30 23:16:01,316 : INFO : worker thread finished; awaiting finish of 2 more threads
    2020-09-30 23:16:01,316 : INFO : worker thread finished; awaiting finish of 1 more threads
    2020-09-30 23:16:01,352 : INFO : worker thread finished; awaiting finish of 0 more threads
    2020-09-30 23:16:01,353 : INFO : EPOCH - 20 : training on 23279529 raw words (22951015 effective words) took 161.7s, 141953 effective words/s
    2020-09-30 23:16:01,353 : INFO : training on a 465590580 raw words (459020300 effective words) took 1185.9s, 387059 effective words/s

    Evaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8)

    0.304920 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)


    Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/m,d100,n5,w10,mc2,t8)

    0.1042 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/m,d100,n5,w10,mc2,t8)


    Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)

    0.10632 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)





Achieved Sentiment-Prediction Accuracy
--------------------------------------
Compare error rates achieved, best-to-worst


.. code-block:: default

    print("Err_rate Model")
    for rate, name in sorted((rate, name) for name, rate in error_rates.items()):
        print(f"{rate} {name}")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Err_rate Model
    0.1042 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/m,d100,n5,w10,mc2,t8)
    0.1056 Doc2Vec(dbow,d100,n5,mc2,t8)
    0.10632 Doc2Vec(dbow,d100,n5,mc2,t8)+Doc2Vec(dm/c,d100,n5,w5,mc2,t8)
    0.1684 Doc2Vec(dm/m,d100,n5,w10,mc2,t8)
    0.30492 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)




In our testing, contrary to the results of the paper, on this problem,
PV-DBOW alone performs as good as anything else. Concatenating vectors from
different models only sometimes offers a tiny predictive improvement ‚Äì and
stays generally close to the best-performing solo model included.

The best results achieved here are just around 10% error rate, still a long
way from the paper's reported 7.42% error rate.

(Other trials not shown, with larger vectors and other changes, also don't
come close to the paper's reported value. Others around the net have reported
a similar inability to reproduce the paper's best numbers. The PV-DM/C mode
improves a bit with many more training epochs ‚Äì but doesn't reach parity with
PV-DBOW.)


Examining Results
-----------------

Let's look for answers to the following questions:

#. Are inferred vectors close to the precalculated ones?
#. Do close documents seem more related than distant ones?
#. Do the word vectors show useful similarities?
#. Are the word vectors from this dataset any good at analogies?


Are inferred vectors close to the precalculated ones?
-----------------------------------------------------


.. code-block:: default

    doc_id = np.random.randint(len(simple_models[0].dv))  # Pick random doc; re-run cell for more examples
    print(f'for doc {doc_id}...')
    for model in simple_models:
        inferred_docvec = model.infer_vector(alldocs[doc_id].words)
        print(f'{model}:\n {model.dv.most_similar([inferred_docvec], topn=3)}')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    for doc 43085...
    Doc2Vec(dbow,d100,n5,mc2,t8):
     [(43085, 0.9763993620872498), (60169, 0.6021242737770081), (19410, 0.6011009812355042)]
    Doc2Vec(dm/m,d100,n5,w10,mc2,t8):
     [(43085, 0.8938843011856079), (95363, 0.5461075305938721), (14357, 0.5367367267608643)]
    Doc2Vec(dm/c,d100,n5,w5,mc2,t8):
     [(43085, 0.8192846775054932), (35174, 0.48924335837364197), (63795, 0.4852815568447113)]




(Yes, here the stored vector from 20 epochs of training is usually one of the
closest to a freshly-inferred vector for the same words. Defaults for
inference may benefit from tuning for each dataset or model parameters.)


Do close documents seem more related than distant ones?
-------------------------------------------------------


.. code-block:: default

    import random

    doc_id = np.random.randint(len(simple_models[0].dv))  # pick random doc, re-run cell for more examples
    model = random.choice(simple_models)  # and a random model
    sims = model.dv.most_similar(doc_id, topn=len(model.dv))  # get *all* similar documents
    print(f'TARGET ({doc_id}): ¬´{" ".join(alldocs[doc_id].words)}¬ª\n')
    print(f'SIMILAR/DISSIMILAR DOCS PER MODEL {model}%s:\n')
    for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
        s = sims[index]
        i = sims[index][0]
        words = ' '.join(alldocs[i].words)
        print(f'{label} {s}: ¬´{words}¬ª\n')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    TARGET (94198): ¬´Believe it or not this is a good movie. I don't hate this at all. Why? It has horrific dialog, plot holes large enough so you can plow a cruise ship thorough, and not much going on in the plot department. I like this movie for its setting(so exotic, so alive, and up to date) In fact, so up to date that this is one of the first movies that had modern cellphones in them. Believe it or not the hero in this movie takes the damn bus! THE BUS!!! Sharon Stone and Slyvester Stallone make a great couple especially in "sex in the hotel shower" scene. Talk about chemistry. James Woods makes adds some hostility and a pulse to this melodrama with his wisecracks, outbursts, and foul-mouthed banter as the villain. But what's with the late Rod Stieger and Eric Roberts fronting as Cuban mobsters? C'mon, here. The strange thing is, they make it work. The action sequences such as the Sly kicking ass and taking names while encountering a couple of thugs on a bus and throwing one out the window, a hotel room explodes and collapses into the ocean with the bad guys still in it, and the funky way Sly sets traps for the bad guys gives this movie redemption. However, the characters in question could've been a little bolder, a little more gunplay is needed, and I have a major beef with discrepancies in the May Munro story. She looks 35, now if her parents were killed when she was a little girl, shouldn't Tomas be a tad older(he looked the same age in her flashback) The Specialist would've been a great movie if the plot was better explained and more intense.¬ª

    SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc2,t8)%s:

    MOST (41214, 0.625079333782196): ¬´Back in the forties, when movies touched on matters not yet admissible in "polite" society, they resorted to codes which supposedly floated over the heads of most of the audience while alerting those in the know to just what was up. Probably no film of the decade was so freighted with innuendo as the oddly obscure Desert Fury, set in a small gambling oasis called Chuckawalla somewhere in the California desert. Proprietress of the Purple Sage saloon and casino is the astonishing Mary Astor, in slacks and sporting a cigarette holder; into town drives her handful-of-a-daughter, Lizabeth Scott, looking, in Technicolor, like 20-million bucks. But listen to the dialogue between them, which suggests an older Lesbian and her young, restless companion (one can only wonder if A.I. Bezzerides' original script made this relationship explicit). Even more blatant are John Hodiak as a gangster and Wendell Corey as his insanely jealous torpedo. Add Burt Lancaster as the town sheriff, stir, and sit back. Both Lancaster and (surprisingly) Hodiak fall for Scott. It seems, however, that Hodiak not only has a past with Astor, but had a wife who died under suspicious circumstances. The desert sun heats these ingredients up to a hard boil, with face-slappings aplenty and empurpled exchanges. Don't pass up this hothouse melodrama, chock full of creepily exotic blooms, if it comes your way; it's a remarkable movie.¬ª

    MEDIAN (62786, 0.2857852280139923): ¬´This is a superb storyline and has excellent music. Set in the background of Rajasthan, this picture gives a beautiful insight into the Rajasthani culture and way of life. One wonders why more such enjoyable pictures are not made. More such pictures will enhance the stock of Bollywood films.¬ª

    LEAST (82757, -0.04075651988387108): ¬´Taj Mahal Badalandabad (played by Kal Penn) had a few funny moments in the movie (i.e. jokes regarding the former British aristocracy in India), but the movie almost nothing but sex jokes for the entire movie. This sort of humor included nearly a dozen different euphemisms for the female anatomy (I lost count), and countless jokes about how to "score" with the opposite gender. I laughed at the first few sex jokes, but by the end of the movie, I was really tired of hearing crude jokes.<br /><br />If this sort of humor is appealing to you, then you'll enjoy this movie. Otherwise, don't bother seeing it. Overall, I felt "Harold and Kumar" was much more humorous. I'm a fan of Kal Penn's, but unfortunately, I cannot recommend this movie.<br /><br />Brief Plot Synopsis (no spoilers below): Taj heads to Camford University (name taken from Cambridge and Oxford) in England to pursue his higher education and to follow in his fathers footsteps. He believes that he has been accepted to the prestigious fraternal guild "Fox and Hounds". However, upon arriving at the University, he is told by a "Fox and Hounds" member (nicknamed "Pip") that he was mistakenly sent this acceptance letter.<br /><br />Taj is forced to become a teacher's assistant at the "Barn" residence, which contains a group of social outcasts. Pip and the other "Fox and Hounds" members treat Taj and his house mates very poorly. Determined to help the members of the Barn residence gain acceptance, Taj starts the "Cock and Bulls" fraternal guild. He hopes that the "Cock and Bulls" guild will win the Camford Cup, which is an annual academic, social, and athletic competition between fraternities. The bitterness between Taj and Pip only increases as the fraternities compete in several competitions to earn points towards the Camford Cup.<br /><br />I cannot say anything more about the plot, or else I'll spoil it for you.¬ª





Somewhat, in terms of reviewer tone, movie genre, etc... the MOST
cosine-similar docs usually seem more like the TARGET than the MEDIAN or
LEAST... especially if the MOST has a cosine-similarity > 0.5. Re-run the
cell to try another random target document.


Do the word vectors show useful similarities?
---------------------------------------------



.. code-block:: default

    import random

    word_models = simple_models[:]

    def pick_random_word(model, threshold=10):
        # pick a random word with a suitable number of occurences
        while True:
            word = random.choice(model.wv.index_to_key)
            if model.wv.get_vecattr(word, "count") > threshold:
                return word

    target_word = pick_random_word(word_models[0])
    # or uncomment below line, to just pick a word from the relevant domain:
    # target_word = 'comedy/drama'

    for model in word_models:
        print(f'target_word: {repr(target_word)} model: {model} similar words:')
        for i, (word, sim) in enumerate(model.wv.most_similar(target_word, topn=10), 1):
            print(f'    {i}. {sim:.2f} {repr(word)}')
        print()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    target_word: 'GAY' model: Doc2Vec(dbow,d100,n5,mc2,t8) similar words:
        1. 0.46 'Penn'
        2. 0.45 'diverts'
        3. 0.43 '"Gas-s-s-s"'
        4. 0.42 'Lance."'
        5. 0.42 "1930's;"
        6. 0.42 "aren't?"
        7. 0.40 'Halder'
        8. 0.40 'dates.<br'
        9. 0.40 'INDEPENDENCE'
        10. 0.40 'Airplane'

    target_word: 'GAY' model: Doc2Vec(dm/m,d100,n5,w10,mc2,t8) similar words:
        1. 0.50 'judge.<br'
        2. 0.48 'crappy!<br'
        3. 0.46 '/>OUR'
        4. 0.46 '/>Girl'
        5. 0.45 'esteem.<br'
        6. 0.45 'proclivities,'
        7. 0.45 'brains.<br'
        8. 0.45 'arrested.<br'
        9. 0.45 'true),'
        10. 0.45 'origins)'

    target_word: 'GAY' model: Doc2Vec(dm/c,d100,n5,w5,mc2,t8) similar words:
        1. 0.59 "'every"
        2. 0.59 'Venezuelan'
        3. 0.58 'challenged)'
        4. 0.56 'deceiving'
        5. 0.55 'best-looking'
        6. 0.55 "'lone"
        7. 0.54 'Noona'
        8. 0.54 '"peace"'
        9. 0.54 'fruitiest'
        10. 0.54 'tele'





Do the DBOW words look meaningless? That's because the gensim DBOW model
doesn't train word vectors ‚Äì they remain at their random initialized values ‚Äì
unless you ask with the ``dbow_words=1`` initialization parameter. Concurrent
word-training slows DBOW mode significantly, and offers little improvement
(and sometimes a little worsening) of the error rate on this IMDB
sentiment-prediction task, but may be appropriate on other tasks, or if you
also need word-vectors.

Words from DM models tend to show meaningfully similar words when there are
many examples in the training data (as with 'plot' or 'actor'). (All DM modes
inherently involve word-vector training concurrent with doc-vector training.)


Are the word vectors from this dataset any good at analogies?
-------------------------------------------------------------


.. code-block:: default


    from gensim.test.utils import datapath
    questions_filename = datapath('questions-words.txt')

    # Note: this analysis takes many minutes
    for model in word_models:
        score, sections = model.wv.evaluate_word_analogies(questions_filename)
        correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])
        print(f'{model}: {float(correct*100)/(correct+incorrect):0.2f}%% correct ({correct} of {correct+incorrect}')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-09-30 23:16:20,972 : INFO : Evaluating word analogies for top 300000 words in the model on /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/questions-words.txt
    2020-09-30 23:16:29,726 : INFO : capital-common-countries: 0.0% (0/420)
    2020-09-30 23:16:49,935 : INFO : capital-world: 0.0% (0/902)
    2020-09-30 23:16:51,662 : INFO : currency: 0.0% (0/86)
    2020-09-30 23:17:21,119 : INFO : city-in-state: 0.0% (0/1510)
    2020-09-30 23:17:30,185 : INFO : family: 0.0% (0/506)
    2020-09-30 23:17:48,710 : INFO : gram1-adjective-to-adverb: 0.0% (0/992)
    2020-09-30 23:18:02,794 : INFO : gram2-opposite: 0.0% (0/756)
    2020-09-30 23:18:27,825 : INFO : gram3-comparative: 0.0% (0/1332)
    2020-09-30 23:18:47,756 : INFO : gram4-superlative: 0.0% (0/1056)
    2020-09-30 23:19:06,340 : INFO : gram5-present-participle: 0.0% (0/992)
    2020-09-30 23:19:32,225 : INFO : gram6-nationality-adjective: 0.0% (0/1445)
    2020-09-30 23:19:58,736 : INFO : gram7-past-tense: 0.0% (0/1560)
    2020-09-30 23:20:33,145 : INFO : gram8-plural: 0.0% (0/1190)
    2020-09-30 23:20:49,979 : INFO : gram9-plural-verbs: 0.0% (0/870)
    2020-09-30 23:20:49,980 : INFO : Quadruplets with out-of-vocabulary words: 30.3%
    2020-09-30 23:20:49,998 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use "dummy4unknown=True"
    2020-09-30 23:20:49,998 : INFO : Total accuracy: 0.0% (0/13617)
    Doc2Vec(dbow,d100,n5,mc2,t8): 0.00%% correct (0 of 13617
    2020-09-30 23:20:50,933 : INFO : Evaluating word analogies for top 300000 words in the model on /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/questions-words.txt
    2020-09-30 23:20:58,733 : INFO : capital-common-countries: 3.8% (16/420)
    2020-09-30 23:21:16,437 : INFO : capital-world: 0.9% (8/902)
    2020-09-30 23:21:17,872 : INFO : currency: 0.0% (0/86)
    2020-09-30 23:21:43,204 : INFO : city-in-state: 0.3% (4/1510)
    2020-09-30 23:21:52,906 : INFO : family: 39.7% (201/506)
    2020-09-30 23:22:11,367 : INFO : gram1-adjective-to-adverb: 3.2% (32/992)
    2020-09-30 23:22:24,694 : INFO : gram2-opposite: 5.7% (43/756)
    2020-09-30 23:22:49,286 : INFO : gram3-comparative: 49.3% (657/1332)
    2020-09-30 23:23:08,496 : INFO : gram4-superlative: 25.2% (266/1056)
    2020-09-30 23:23:27,399 : INFO : gram5-present-participle: 21.9% (217/992)
    2020-09-30 23:23:51,888 : INFO : gram6-nationality-adjective: 2.8% (41/1445)
    2020-09-30 23:24:20,544 : INFO : gram7-past-tense: 28.5% (445/1560)
    2020-09-30 23:24:38,103 : INFO : gram8-plural: 19.3% (230/1190)
    2020-09-30 23:24:53,145 : INFO : gram9-plural-verbs: 45.3% (394/870)
    2020-09-30 23:24:53,147 : INFO : Quadruplets with out-of-vocabulary words: 30.3%
    2020-09-30 23:24:53,147 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use "dummy4unknown=True"
    2020-09-30 23:24:53,147 : INFO : Total accuracy: 18.8% (2554/13617)
    Doc2Vec(dm/m,d100,n5,w10,mc2,t8): 18.76%% correct (2554 of 13617
    2020-09-30 23:24:53,978 : INFO : Evaluating word analogies for top 300000 words in the model on /Volumes/work/workspace/gensim/trunk/gensim/test/test_data/questions-words.txt
    2020-09-30 23:25:01,268 : INFO : capital-common-countries: 1.9% (8/420)
    2020-09-30 23:25:20,800 : INFO : capital-world: 0.6% (5/902)
    2020-09-30 23:25:22,770 : INFO : currency: 0.0% (0/86)
    2020-09-30 23:25:50,052 : INFO : city-in-state: 0.3% (4/1510)
    2020-09-30 23:25:58,619 : INFO : family: 39.3% (199/506)
    2020-09-30 23:26:15,657 : INFO : gram1-adjective-to-adverb: 6.9% (68/992)
    2020-09-30 23:26:30,514 : INFO : gram2-opposite: 4.6% (35/756)
    2020-09-30 23:26:54,775 : INFO : gram3-comparative: 33.9% (452/1332)
    2020-09-30 23:27:17,559 : INFO : gram4-superlative: 24.3% (257/1056)
    2020-09-30 23:27:38,425 : INFO : gram5-present-participle: 37.0% (367/992)
    2020-09-30 23:28:01,531 : INFO : gram6-nationality-adjective: 1.9% (27/1445)
    2020-09-30 23:28:26,685 : INFO : gram7-past-tense: 26.5% (414/1560)
    2020-09-30 23:28:46,289 : INFO : gram8-plural: 8.5% (101/1190)
    2020-09-30 23:28:58,855 : INFO : gram9-plural-verbs: 46.2% (402/870)
    2020-09-30 23:28:58,858 : INFO : Quadruplets with out-of-vocabulary words: 30.3%
    2020-09-30 23:28:58,859 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use "dummy4unknown=True"
    2020-09-30 23:28:58,859 : INFO : Total accuracy: 17.2% (2339/13617)
    Doc2Vec(dm/c,d100,n5,w5,mc2,t8): 17.18%% correct (2339 of 13617




Even though this is a tiny, domain-specific dataset, it shows some meager
capability on the general word analogies ‚Äì at least for the DM/mean and
DM/concat models which actually train word vectors. (The untrained
random-initialized words of the DBOW model of course fail miserably.)



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 52 minutes  12.903 seconds)

**Estimated memory usage:**  3494 MB


.. _sphx_glr_download_auto_examples_howtos_run_doc2vec_imdb.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_doc2vec_imdb.py <run_doc2vec_imdb.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_doc2vec_imdb.ipynb <run_doc2vec_imdb.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_howtos_run_downloader_api.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_howtos_run_downloader_api.py:


How to download pre-trained models and corpora
==============================================

Demonstrates simple and quick access to common corpora and pretrained models.


.. code-block:: default


    import logging
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)








One of Gensim's features is simple and easy access to common data.
The `gensim-data <https://github.com/RaRe-Technologies/gensim-data>`_ project stores a
variety of corpora and pretrained models.
Gensim has a :py:mod:`gensim.downloader` module for programmatically accessing this data.
This module leverages a local cache (in user's home folder, by default) that
ensures data is downloaded at most once.

This tutorial:

* Downloads the text8 corpus, unless it is already on your local machine
* Trains a Word2Vec model from the corpus (see :ref:`sphx_glr_auto_examples_tutorials_run_doc2vec_lee.py` for a detailed tutorial)
* Leverages the model to calculate word similarity
* Demonstrates using the API to load other models and corpora

Let's start by importing the api module.



.. code-block:: default

    import gensim.downloader as api








Now, let's download the text8 corpus and load it as a Python object
that supports streamed access.



.. code-block:: default

    corpus = api.load('text8')








In this case, our corpus is an iterable.
If you look under the covers, it has the following definition:


.. code-block:: default


    import inspect
    print(inspect.getsource(corpus.__class__))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    class Dataset(object):
        def __init__(self, fn):
            self.fn = fn

        def __iter__(self):
            corpus = Text8Corpus(self.fn)
            for doc in corpus:
                yield doc





For more details, look inside the file that defines the Dataset class for your particular resource.



.. code-block:: default

    print(inspect.getfile(corpus.__class__))






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /Users/kofola3/gensim-data/text8/__init__.py




With the corpus has been downloaded and loaded, let's use it to train a word2vec model.



.. code-block:: default


    from gensim.models.word2vec import Word2Vec
    model = Word2Vec(corpus)








Now that we have our word2vec model, let's find words that are similar to 'tree'.



.. code-block:: default



    print(model.wv.most_similar('tree'))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [('trees', 0.7091131806373596), ('bark', 0.673214316368103), ('leaf', 0.6706242561340332), ('flower', 0.6195512413978577), ('bird', 0.6081331372261047), ('nest', 0.602649450302124), ('avl', 0.5914573669433594), ('garden', 0.5712863206863403), ('egg', 0.5702848434448242), ('beetle', 0.5701731443405151)]




You can use the API to download several different corpora and pretrained models.
Here's how to list all resources available in gensim-data:



.. code-block:: default



    import json
    info = api.info()
    print(json.dumps(info, indent=4))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {
        "corpora": {
            "semeval-2016-2017-task3-subtaskBC": {
                "num_records": -1,
                "record_format": "dict",
                "file_size": 6344358,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py",
                "license": "All files released for the task are free for general research use",
                "fields": {
                    "2016-train": [
                        "..."
                    ],
                    "2016-dev": [
                        "..."
                    ],
                    "2017-test": [
                        "..."
                    ],
                    "2016-test": [
                        "..."
                    ]
                },
                "description": "SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section \u201cPapers\u201d of https://github.com/RaRe-Technologies/gensim-data/issues/18.",
                "checksum": "701ea67acd82e75f95e1d8e62fb0ad29",
                "file_name": "semeval-2016-2017-task3-subtaskBC.gz",
                "read_more": [
                    "http://alt.qcri.org/semeval2017/task3/",
                    "http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf",
                    "https://github.com/RaRe-Technologies/gensim-data/issues/18",
                    "https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english"
                ],
                "parts": 1
            },
            "semeval-2016-2017-task3-subtaskA-unannotated": {
                "num_records": 189941,
                "record_format": "dict",
                "file_size": 234373151,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py",
                "license": "These datasets are free for general research use.",
                "fields": {
                    "THREAD_SEQUENCE": "",
                    "RelQuestion": {
                        "RELQ_CATEGORY": "question category, according to the Qatar Living taxonomy",
                        "RELQ_DATE": "date of posting",
                        "RELQ_ID": "question indentifier",
                        "RELQ_USERID": "identifier of the user asking the question",
                        "RELQ_USERNAME": "name of the user asking the question",
                        "RelQBody": "body of question",
                        "RelQSubject": "subject of question"
                    },
                    "RelComments": [
                        {
                            "RelCText": "text of answer",
                            "RELC_USERID": "identifier of the user posting the comment",
                            "RELC_ID": "comment identifier",
                            "RELC_USERNAME": "name of the user posting the comment",
                            "RELC_DATE": "date of posting"
                        }
                    ]
                },
                "description": "SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.",
                "checksum": "2de0e2f2c4f91c66ae4fcf58d50ba816",
                "file_name": "semeval-2016-2017-task3-subtaskA-unannotated.gz",
                "read_more": [
                    "http://alt.qcri.org/semeval2016/task3/",
                    "http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf",
                    "https://github.com/RaRe-Technologies/gensim-data/issues/18",
                    "https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english"
                ],
                "parts": 1
            },
            "patent-2017": {
                "num_records": 353197,
                "record_format": "dict",
                "file_size": 3087262469,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py",
                "license": "not found",
                "description": "Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.",
                "checksum-0": "818501f0b9af62d3b88294d86d509f8f",
                "checksum-1": "66c05635c1d3c7a19b4a335829d09ffa",
                "file_name": "patent-2017.gz",
                "read_more": [
                    "http://patents.reedtech.com/pgrbft.php"
                ],
                "parts": 2
            },
            "quora-duplicate-questions": {
                "num_records": 404290,
                "record_format": "dict",
                "file_size": 21684784,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py",
                "license": "probably https://www.quora.com/about/tos",
                "fields": {
                    "question1": "the full text of each question",
                    "question2": "the full text of each question",
                    "qid1": "unique ids of each question",
                    "qid2": "unique ids of each question",
                    "id": "the id of a training set question pair",
                    "is_duplicate": "the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise"
                },
                "description": "Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.",
                "checksum": "d7cfa7fbc6e2ec71ab74c495586c6365",
                "file_name": "quora-duplicate-questions.gz",
                "read_more": [
                    "https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs"
                ],
                "parts": 1
            },
            "wiki-english-20171001": {
                "num_records": 4924894,
                "record_format": "dict",
                "file_size": 6516051717,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py",
                "license": "https://dumps.wikimedia.org/legal.html",
                "fields": {
                    "section_texts": "list of body of sections",
                    "section_titles": "list of titles of sections",
                    "title": "Title of wiki article"
                },
                "description": "Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`",
                "checksum-0": "a7d7d7fd41ea7e2d7fa32ec1bb640d71",
                "checksum-1": "b2683e3356ffbca3b6c2dca6e9801f9f",
                "checksum-2": "c5cde2a9ae77b3c4ebce804f6df542c2",
                "checksum-3": "00b71144ed5e3aeeb885de84f7452b81",
                "file_name": "wiki-english-20171001.gz",
                "read_more": [
                    "https://dumps.wikimedia.org/enwiki/20171001/"
                ],
                "parts": 4
            },
            "text8": {
                "num_records": 1701,
                "record_format": "list of str (tokens)",
                "file_size": 33182058,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py",
                "license": "not found",
                "description": "First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.",
                "checksum": "68799af40b6bda07dfa47a32612e5364",
                "file_name": "text8.gz",
                "read_more": [
                    "http://mattmahoney.net/dc/textdata.html"
                ],
                "parts": 1
            },
            "fake-news": {
                "num_records": 12999,
                "record_format": "dict",
                "file_size": 20102776,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py",
                "license": "https://creativecommons.org/publicdomain/zero/1.0/",
                "fields": {
                    "crawled": "date the story was archived",
                    "ord_in_thread": "",
                    "published": "date published",
                    "participants_count": "number of participants",
                    "shares": "number of Facebook shares",
                    "replies_count": "number of replies",
                    "main_img_url": "image from story",
                    "spam_score": "data from webhose.io",
                    "uuid": "unique identifier",
                    "language": "data from webhose.io",
                    "title": "title of story",
                    "country": "data from webhose.io",
                    "domain_rank": "data from webhose.io",
                    "author": "author of story",
                    "comments": "number of Facebook comments",
                    "site_url": "site URL from BS detector",
                    "text": "text of story",
                    "thread_title": "",
                    "type": "type of website (label from BS detector)",
                    "likes": "number of Facebook likes"
                },
                "description": "News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.",
                "checksum": "5e64e942df13219465927f92dcefd5fe",
                "file_name": "fake-news.gz",
                "read_more": [
                    "https://www.kaggle.com/mrisdal/fake-news"
                ],
                "parts": 1
            },
            "20-newsgroups": {
                "num_records": 18846,
                "record_format": "dict",
                "file_size": 14483581,
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py",
                "license": "not found",
                "fields": {
                    "topic": "name of topic (20 variant of possible values)",
                    "set": "marker of original split (possible values 'train' and 'test')",
                    "data": "",
                    "id": "original id inferred from folder name"
                },
                "description": "The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.",
                "checksum": "c92fd4f6640a86d5ba89eaad818a9891",
                "file_name": "20-newsgroups.gz",
                "read_more": [
                    "http://qwone.com/~jason/20Newsgroups/"
                ],
                "parts": 1
            },
            "__testing_matrix-synopsis": {
                "description": "[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.",
                "checksum": "1767ac93a089b43899d54944b07d9dc5",
                "file_name": "__testing_matrix-synopsis.gz",
                "read_more": [
                    "http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis"
                ],
                "parts": 1
            },
            "__testing_multipart-matrix-synopsis": {
                "description": "[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.",
                "checksum-0": "c8b0c7d8cf562b1b632c262a173ac338",
                "checksum-1": "5ff7fc6818e9a5d9bc1cf12c35ed8b96",
                "checksum-2": "966db9d274d125beaac7987202076cba",
                "file_name": "__testing_multipart-matrix-synopsis.gz",
                "read_more": [
                    "http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis"
                ],
                "parts": 3
            }
        },
        "models": {
            "fasttext-wiki-news-subwords-300": {
                "num_records": 999999,
                "file_size": 1005007116,
                "base_dataset": "Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py",
                "license": "https://creativecommons.org/licenses/by-sa/3.0/",
                "parameters": {
                    "dimension": 300
                },
                "description": "1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).",
                "read_more": [
                    "https://fasttext.cc/docs/en/english-vectors.html",
                    "https://arxiv.org/abs/1712.09405",
                    "https://arxiv.org/abs/1607.01759"
                ],
                "checksum": "de2bb3a20c46ce65c9c131e1ad9a77af",
                "file_name": "fasttext-wiki-news-subwords-300.gz",
                "parts": 1
            },
            "conceptnet-numberbatch-17-06-300": {
                "num_records": 1917247,
                "file_size": 1225497562,
                "base_dataset": "ConceptNet, word2vec, GloVe, and OpenSubtitles 2016",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py",
                "license": "https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt",
                "parameters": {
                    "dimension": 300
                },
                "description": "ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.",
                "read_more": [
                    "http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972",
                    "https://github.com/commonsense/conceptnet-numberbatch",
                    "http://conceptnet.io/"
                ],
                "checksum": "fd642d457adcd0ea94da0cd21b150847",
                "file_name": "conceptnet-numberbatch-17-06-300.gz",
                "parts": 1
            },
            "word2vec-ruscorpora-300": {
                "num_records": 184973,
                "file_size": 208427381,
                "base_dataset": "Russian National Corpus (about 250M words)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py",
                "license": "https://creativecommons.org/licenses/by/4.0/deed.en",
                "parameters": {
                    "dimension": 300,
                    "window_size": 10
                },
                "description": "Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.",
                "preprocessing": "The corpus was lemmatized and tagged with Universal PoS",
                "read_more": [
                    "https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models",
                    "http://rusvectores.org/en/",
                    "https://github.com/RaRe-Technologies/gensim-data/issues/3"
                ],
                "checksum": "9bdebdc8ae6d17d20839dd9b5af10bc4",
                "file_name": "word2vec-ruscorpora-300.gz",
                "parts": 1
            },
            "word2vec-google-news-300": {
                "num_records": 3000000,
                "file_size": 1743563840,
                "base_dataset": "Google News (about 100 billion words)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py",
                "license": "not found",
                "parameters": {
                    "dimension": 300
                },
                "description": "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).",
                "read_more": [
                    "https://code.google.com/archive/p/word2vec/",
                    "https://arxiv.org/abs/1301.3781",
                    "https://arxiv.org/abs/1310.4546",
                    "https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf"
                ],
                "checksum": "a5e5354d40acb95f9ec66d5977d140ef",
                "file_name": "word2vec-google-news-300.gz",
                "parts": 1
            },
            "glove-wiki-gigaword-50": {
                "num_records": 400000,
                "file_size": 69182535,
                "base_dataset": "Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 50
                },
                "description": "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "c289bc5d7f2f02c6dc9f2f9b67641813",
                "file_name": "glove-wiki-gigaword-50.gz",
                "parts": 1
            },
            "glove-wiki-gigaword-100": {
                "num_records": 400000,
                "file_size": 134300434,
                "base_dataset": "Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 100
                },
                "description": "Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "40ec481866001177b8cd4cb0df92924f",
                "file_name": "glove-wiki-gigaword-100.gz",
                "parts": 1
            },
            "glove-wiki-gigaword-200": {
                "num_records": 400000,
                "file_size": 264336934,
                "base_dataset": "Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 200
                },
                "description": "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "59652db361b7a87ee73834a6c391dfc1",
                "file_name": "glove-wiki-gigaword-200.gz",
                "parts": 1
            },
            "glove-wiki-gigaword-300": {
                "num_records": 400000,
                "file_size": 394362229,
                "base_dataset": "Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 300
                },
                "description": "Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "29e9329ac2241937d55b852e8284e89b",
                "file_name": "glove-wiki-gigaword-300.gz",
                "parts": 1
            },
            "glove-twitter-25": {
                "num_records": 1193514,
                "file_size": 109885004,
                "base_dataset": "Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 25
                },
                "description": "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "50db0211d7e7a2dcd362c6b774762793",
                "file_name": "glove-twitter-25.gz",
                "parts": 1
            },
            "glove-twitter-50": {
                "num_records": 1193514,
                "file_size": 209216938,
                "base_dataset": "Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 50
                },
                "description": "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "c168f18641f8c8a00fe30984c4799b2b",
                "file_name": "glove-twitter-50.gz",
                "parts": 1
            },
            "glove-twitter-100": {
                "num_records": 1193514,
                "file_size": 405932991,
                "base_dataset": "Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 100
                },
                "description": "Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "b04f7bed38756d64cf55b58ce7e97b15",
                "file_name": "glove-twitter-100.gz",
                "parts": 1
            },
            "glove-twitter-200": {
                "num_records": 1193514,
                "file_size": 795373100,
                "base_dataset": "Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)",
                "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py",
                "license": "http://opendatacommons.org/licenses/pddl/",
                "parameters": {
                    "dimension": 200
                },
                "description": "Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).",
                "preprocessing": "Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.",
                "read_more": [
                    "https://nlp.stanford.edu/projects/glove/",
                    "https://nlp.stanford.edu/pubs/glove.pdf"
                ],
                "checksum": "e52e8392d1860b95d5308a525817d8f9",
                "file_name": "glove-twitter-200.gz",
                "parts": 1
            },
            "__testing_word2vec-matrix-synopsis": {
                "description": "[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.",
                "parameters": {
                    "dimensions": 50
                },
                "preprocessing": "Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.",
                "read_more": [],
                "checksum": "534dcb8b56a360977a269b7bfc62d124",
                "file_name": "__testing_word2vec-matrix-synopsis.gz",
                "parts": 1
            }
        }
    }




There are two types of data resources: corpora and models.


.. code-block:: default

    print(info.keys())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    dict_keys(['corpora', 'models'])




Let's have a look at the available corpora:


.. code-block:: default

    for corpus_name, corpus_data in sorted(info['corpora'].items()):
        print(
            '%s (%d records): %s' % (
                corpus_name,
                corpus_data.get('num_records', -1),
                corpus_data['description'][:40] + '...',
            )
        )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    20-newsgroups (18846 records): The notorious collection of approximatel...
    __testing_matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Synopsis of t...
    __testing_multipart-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Synopsis of t...
    fake-news (12999 records): News dataset, contains text and metadata...
    patent-2017 (353197 records): Patent Grant Full Text. Contains the ful...
    quora-duplicate-questions (404290 records): Over 400,000 lines of potential question...
    semeval-2016-2017-task3-subtaskA-unannotated (189941 records): SemEval 2016 / 2017 Task 3 Subtask A una...
    semeval-2016-2017-task3-subtaskBC (-1 records): SemEval 2016 / 2017 Task 3 Subtask B and...
    text8 (1701 records): First 100,000,000 bytes of plain text fr...
    wiki-english-20171001 (4924894 records): Extracted Wikipedia dump from October 20...




... and the same for models:


.. code-block:: default

    for model_name, model_data in sorted(info['models'].items()):
        print(
            '%s (%d records): %s' % (
                model_name,
                model_data.get('num_records', -1),
                model_data['description'][:40] + '...',
            )
        )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    __testing_word2vec-matrix-synopsis (-1 records): [THIS IS ONLY FOR TESTING] Word vecrors ...
    conceptnet-numberbatch-17-06-300 (1917247 records): ConceptNet Numberbatch consists of state...
    fasttext-wiki-news-subwords-300 (999999 records): 1 million word vectors trained on Wikipe...
    glove-twitter-100 (1193514 records): Pre-trained vectors based on  2B tweets,...
    glove-twitter-200 (1193514 records): Pre-trained vectors based on 2B tweets, ...
    glove-twitter-25 (1193514 records): Pre-trained vectors based on 2B tweets, ...
    glove-twitter-50 (1193514 records): Pre-trained vectors based on 2B tweets, ...
    glove-wiki-gigaword-100 (400000 records): Pre-trained vectors based on Wikipedia 2...
    glove-wiki-gigaword-200 (400000 records): Pre-trained vectors based on Wikipedia 2...
    glove-wiki-gigaword-300 (400000 records): Pre-trained vectors based on Wikipedia 2...
    glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia 2...
    word2vec-google-news-300 (3000000 records): Pre-trained vectors trained on a part of...
    word2vec-ruscorpora-300 (184973 records): Word2vec Continuous Skipgram vectors tra...




If you want to get detailed information about a model/corpus, use:



.. code-block:: default



    fake_news_info = api.info('fake-news')
    print(json.dumps(fake_news_info, indent=4))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {
        "num_records": 12999,
        "record_format": "dict",
        "file_size": 20102776,
        "reader_code": "https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py",
        "license": "https://creativecommons.org/publicdomain/zero/1.0/",
        "fields": {
            "crawled": "date the story was archived",
            "ord_in_thread": "",
            "published": "date published",
            "participants_count": "number of participants",
            "shares": "number of Facebook shares",
            "replies_count": "number of replies",
            "main_img_url": "image from story",
            "spam_score": "data from webhose.io",
            "uuid": "unique identifier",
            "language": "data from webhose.io",
            "title": "title of story",
            "country": "data from webhose.io",
            "domain_rank": "data from webhose.io",
            "author": "author of story",
            "comments": "number of Facebook comments",
            "site_url": "site URL from BS detector",
            "text": "text of story",
            "thread_title": "",
            "type": "type of website (label from BS detector)",
            "likes": "number of Facebook likes"
        },
        "description": "News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.",
        "checksum": "5e64e942df13219465927f92dcefd5fe",
        "file_name": "fake-news.gz",
        "read_more": [
            "https://www.kaggle.com/mrisdal/fake-news"
        ],
        "parts": 1
    }




Sometimes, you do not want to load a model into memory. Instead, you can request
just the filesystem path to the model. For that, use:



.. code-block:: default



    print(api.load('glove-wiki-gigaword-50', return_path=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /Users/kofola3/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz




If you want to load the model to memory, then:



.. code-block:: default



    model = api.load("glove-wiki-gigaword-50")
    model.most_similar("glass")





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    [('plastic', 0.79425048828125), ('metal', 0.7708716988563538), ('walls', 0.7700635194778442), ('marble', 0.7638523578643799), ('wood', 0.7624280452728271), ('ceramic', 0.7602593302726746), ('pieces', 0.7589112520217896), ('stained', 0.7528817653656006), ('tile', 0.748193621635437), ('furniture', 0.7463858723640442)]



For corpora, the corpus is never loaded to memory, all corpora are iterables wrapped in
a special class ``Dataset``, with an ``__iter__`` method.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  39.422 seconds)

**Estimated memory usage:**  297 MB


.. _sphx_glr_download_auto_examples_howtos_run_downloader_api.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_downloader_api.py <run_downloader_api.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_downloader_api.ipynb <run_downloader_api.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_howtos_run_doc.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_howtos_run_doc.py:


How to Author Gensim Documentation
==================================

How to author documentation for Gensim.

Background
----------

Gensim is a large project with a wide range of functionality.
Unfortunately, not all of this functionality is documented **well**, and some of it is not documented at all.
Without good documentation, users are unable to unlock Gensim's full potential.
Therefore, authoring new documentation and improving existing documentation is of great value to the Gensim project.

If you implement new functionality in Gensim, please include **helpful** documentation.
By "helpful", we mean that your documentation answers questions that Gensim users may have.
For example:

- What is this new functionality?
- **Why** is it important?
- **How** is it relevant to Gensim?
- **What** can I do with it? What are some real-world applications?
- **How** do I use it to achieve those things?
- ... and others (if you can think of them, please add them here)

Before you author documentation, I suggest reading
`"What nobody tells you about documentation" <https://www.divio.com/blog/documentation/>`__
or watching its `accompanying video <https://www.youtube.com/watch?v=t4vKPhjcMZg>`__
(or even both, if you're really keen).

The summary of the above presentation is: there are four distinct kinds of documentation, and you really need them all:

1. Tutorials
2. Howto guides
3. Explanations
4. References

Each kind has its own intended audience, purpose, and writing style.
When you make a PR with new functionality, please consider authoring each kind of documentation.
At the very least, you will (indirectly) author reference documentation through module, class and function docstrings.

Mechanisms
----------

We keep our documentation as individual Python scripts.
These scripts live under :file:`docs/src/gallery` in one of several subdirectories:

- core: core tutorials.  We try to keep this part small, avoid putting stuff here.
- tutorials: tutorials.
- howtos: howto guides.

Pick a subdirectory and save your script under it.
Prefix the name of the script with ``run_``: this way, the the documentation builder will run your script each time it builds our docs.

The contents of the script are straightforward.
At the very top, you need a docstring describing what your script does.


.. code-block:: default


    r"""
    Title
    =====

    Brief description.
    """





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    '\nTitle\n=====\n\nBrief description.\n'



The title is what will show up in the gallery.
Keep this short and descriptive.

The description will appear as a tooltip in the gallery.
When people mouse-over the title, they will see the description.
Keep this short too.


The rest of the script is Python, formatted in a special way so that Sphinx Gallery can parse it.
The most important properties of this format are:

- Sphinx Gallery will split your script into blocks
- A block can be Python source or RST-formatted comments
- To indicate that a block is in RST, prefix it with a line of 80 hash (#) characters.
- All other blocks will be interpreted as Python source

Read `this link <https://sphinx-gallery.github.io/syntax.html>`__ for more details.
If you need further examples, check out other ``gensim`` tutorials and guides.
All of them (including this one!) have a download link at the bottom of the page, which exposes the Python source they were generated from.

You should be able to run your script directly from the command line::

  python myscript.py

and it should run to completion without error, occasionally printing stuff to standard output.


Authoring Workflow
------------------

There are several ways to author documentation.
The simplest and most straightforward is to author your ``script.py`` from scratch.
You'll have the following cycle:

1. Make changes
2. Run ``python script.py``
3. Check standard output, standard error and return code
4. If everything works well, stop.
5. Otherwise, go back to step 1).

If the above is not your cup of tea, you can also author your documentation as a Jupyter notebook.
This is a more flexible approach that enables you to tweak parts of the documentation and re-run them as necessary.

Once you're happy with the notebook, convert it to a script.py.
There's a helpful `script <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/src/tools/to_python.py>`__ that will do it for you.
To use it::

    python to_python.py < notebook.ipynb > script.py

You may have to touch up the resulting ``script.py``.
More specifically:

- Update the title
- Update the description
- Fix any issues that the markdown-to-RST converter could not deal with

Once your script.py works, put it in a suitable subdirectory.
Please don't include your original Jupyter notebook in the repository - we won't be using it.

Correctness
-----------

Incorrect documentation can be worse than no documentation at all.
Take the following steps to ensure correctness:

- Run Python's doctest module on your docstrings
- Run your documentation scripts from scratch, removing any temporary files/results

Using data in your documentation
--------------------------------

Some parts of the documentation require real-world data to be useful.
For example, you may need more than just a toy example to demonstrate the benefits of one model over another.
This subsection provides some tips for including data in your documentation.

If possible, use data available via Gensim's
`downloader API <https://radimrehurek.com/gensim/gensim_numfocus/auto_examples/010_tutorials/run_downloader_api.html>`__.
This will reduce the risk of your documentation becoming obsolete because required data is no longer available.

Use the smallest possible dataset: avoid making people unnecessarily load large datasets and models.
This will make your documentation faster to run and easier for people to use (they can modify your examples and re-run them quickly).

Finalizing your contribution
----------------------------

First, get Sphinx Gallery to build your documentation::

    make --directory docs/src html

This can take a while if your documentation uses a large dataset, or if you've changed many other tutorials or guides.
Once this completes successfully, open ``docs/auto_examples/index.html`` in your browser.
You should see your new tutorial or guide in the gallery.

Once your documentation script is working correctly, it's time to add it to the git repository::

    git add docs/src/gallery/tutorials/run_example.py
    git add docs/src/auto_examples/tutorials/run_example.{py,py.md5,rst,ipynb}
    git add docs/src/auto_examples/howtos/sg_execution_times.rst
    git commit -m "enter a helpful commit message here"
    git push origin branchname

.. Note::
  You may be wondering what all those other files are.
  Sphinx Gallery puts a copy of your Python script in ``auto_examples/tutorials``.
  The .md5 contains MD5 hash of the script to enable easy detection of modifications.
  Gallery also generates .rst (RST for Sphinx) and .ipynb (Jupyter notebook) files from the script.
  Finally, ``sg_execution_times.rst`` contains the time taken to run each example.

Finally, open a PR at `github <https://github.com/RaRe-Technologies/gensim>`__.
One of our friendly maintainers will review it, make suggestions, and eventually merge it.
Your documentation will then appear in the `gallery <https://radimrehurek.com/gensim/auto_examples/index.html>`__,
alongside the rest of the examples. Thanks a lot!


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.171 seconds)

**Estimated memory usage:**  6 MB


.. _sphx_glr_download_auto_examples_howtos_run_doc.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_doc.py <run_doc.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_doc.ipynb <run_doc.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
:mod:`similarities.annoy` -- Approximate Vector Search using Annoy
==================================================================

.. automodule:: gensim.similarities.annoy
    :synopsis: Fast Approximate Nearest Neighbor Similarity with the Annoy package
    :members:
    :inherited-members:

:mod:`similarities.fastss` -- Fast Levenshtein edit distance
==================================================================

.. automodule:: gensim.similarities.fastss
    :synopsis: Fast fuzzy search between strings, using the Levenshtein edit distance
    :members:
    :inherited-members:

:mod:`similarities.levenshtein` -- Fast soft-cosine semantic similarity search
==============================================================================

.. automodule:: gensim.similarities.levenshtein
    :synopsis: Fast fuzzy search between strings, using the Soft-Cosine Semantic Similarity
    :members:
    :inherited-members:

:mod:`similarities.nmslib` -- Approximate Vector Search using NMSLIB
====================================================================

.. automodule:: gensim.similarities.nmslib
    :synopsis: Fast Approximate Nearest Neighbor Similarity with the NMSLIB package
    :members:
    :inherited-members:

:mod:`similarities.termsim` -- Term similarity queries
========================================================================

.. automodule:: gensim.similarities.termsim
    :synopsis: Term similarity queries
    :members:
    :inherited-members:

:mod:`similarities.docsim` -- Document similarity queries
========================================================================

.. automodule:: gensim.similarities.docsim
    :synopsis: Document similarity queries
    :members:
    :inherited-members:

:mod:`parsing.preprocessing` -- Functions to preprocess raw text
================================================================

.. automodule:: gensim.parsing.preprocessing
    :synopsis: Functions to preprocess raw text
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`parsing.porter` -- Porter Stemming Algorithm
==================================================

.. automodule:: gensim.parsing.porter
    :synopsis: Porter Stemming Algorithm
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`topic_coherence.text_analysis` -- Analyzing the texts of a corpus to accumulate statistical information about word occurrences
====================================================================================================================================

.. automodule:: gensim.topic_coherence.text_analysis
    :synopsis: Analyzing the texts of a corpus to accumulate statistical information about word occurrences
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
    :special-members: __getitem__
:mod:`topic_coherence.probability_estimation` -- Probability estimation module
==============================================================================

.. automodule:: gensim.topic_coherence.probability_estimation
    :synopsis: Probability estimation module
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`topic_coherence.indirect_confirmation_measure` -- Indirect confirmation measure module
============================================================================================

.. automodule:: gensim.topic_coherence.indirect_confirmation_measure
    :synopsis: Indirect confirmation measure module
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`topic_coherence.segmentation` -- Segmentation module
==========================================================

.. automodule:: gensim.topic_coherence.segmentation
    :synopsis: Segmentation module
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`topic_coherence.direct_confirmation_measure` -- Direct confirmation measure module
========================================================================================

.. automodule:: gensim.topic_coherence.direct_confirmation_measure
    :synopsis: Direct confirmation measure module
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`topic_coherence.aggregation` -- Aggregation module
========================================================

.. automodule:: gensim.topic_coherence.aggregation
    :synopsis: Aggregation module
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.hdpmodel` -- Hierarchical Dirichlet Process
========================================================

.. automodule:: gensim.models.hdpmodel
    :synopsis: Hierarchical Dirichlet Process
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.lsi_dispatcher` -- Dispatcher for distributed LSI
===============================================================

.. automodule:: gensim.models.lsi_dispatcher
    :synopsis: Dispatcher for distributed LSI
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.ldaseqmodel` -- Dynamic Topic Modeling in Python 
=============================================================

.. automodule:: gensim.models.ldaseqmodel
    :synopsis: Dynamic Topic Modeling in Python 
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.lsi_worker` -- Worker for distributed LSI
======================================================

.. automodule:: gensim.models.lsi_worker
    :synopsis: Worker for distributed LSI
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.callbacks` -- Callbacks for track and viz LDA train process
========================================================================

.. automodule:: gensim.models.callbacks
    :synopsis: Callbacks for track and viz LDA train process
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.ldamodel` -- Latent Dirichlet Allocation
======================================================

.. automodule:: gensim.models.ldamodel
    :synopsis: Latent Dirichlet Allocation
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.lda_dispatcher` -- Dispatcher for distributed LDA
================================================================

.. automodule:: gensim.models.lda_dispatcher
    :synopsis: Dispatcher for distributed LDA
    :members:
    :inherited-members:

:mod:`models.lda_worker` -- Worker for distributed LDA
======================================================

.. automodule:: gensim.models.lda_worker
    :synopsis: Worker for distributed LDA
    :members:
    :inherited-members:

:mod:`models.word2vec_inner` -- Cython routines for training Word2Vec models
============================================================================

.. automodule:: gensim.models.word2vec_inner
    :synopsis: Optimized Cython routines for training Word2Vec models
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.normmodel` -- Normalization model
===============================================

.. automodule:: gensim.models.normmodel
    :synopsis: Normalization model
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
    :special-members: __getitem__
:mod:`models._fasttext_bin` -- Facebook's fastText I/O
======================================================

.. automodule:: gensim.models._fasttext_bin
    :synopsis: I/O routines for Facebook's fastText format
    :members:
    :inherited-members:
    :special-members: __getitem__
    :undoc-members:
    :show-inheritance:
:mod:`models.doc2vec_inner` -- Cython routines for training Doc2Vec models
==========================================================================

.. automodule:: gensim.models.doc2vec_inner
    :synopsis: Optimized Cython routines for training Doc2Vec models
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.atmodel` -- Author-topic models
======================================================

.. automodule:: gensim.models.atmodel
    :synopsis: Author-topic model
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.tfidfmodel` -- TF-IDF model
========================================

.. automodule:: gensim.models.tfidfmodel
    :synopsis: TF-IDF model
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
    :special-members: __getitem__
:mod:`models.translation_matrix` -- Translation Matrix model
=============================================================

.. automodule:: gensim.models.translation_matrix
    :synopsis: Translation Matrix
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.fasttext` -- FastText model
========================================

.. automodule:: gensim.models.fasttext
    :synopsis: FastText model
    :members:
    :inherited-members:
    :special-members: __getitem__
    :undoc-members:
    :show-inheritance:
:mod:`models.coherencemodel` -- Topic coherence pipeline
========================================================

.. automodule:: gensim.models.coherencemodel
    :synopsis: Topic coherence pipeline
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.word2vec` -- Word2vec embeddings
=============================================

.. automodule:: gensim.models.word2vec
    :synopsis: Word2vec embeddings
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.rpmodel` -- Random Projections
======================================================

.. automodule:: gensim.models.rpmodel
    :synopsis: Random Projections
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
    :special-members: __getitem__
:mod:`models.lsimodel` -- Latent Semantic Indexing
======================================================

.. automodule:: gensim.models.lsimodel
    :synopsis: Latent Semantic Indexing
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:orphan:

:mod:`models` -- Package for transformation models
======================================================

.. automodule:: gensim.models
    :synopsis: Package for transformation models
    :members:
    :inherited-members:

:mod:`models.fasttext_inner` -- Cython routines for training FastText models
============================================================================

.. automodule:: gensim.models.fasttext_inner
    :synopsis: Optimized Cython routines for training FastText models
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.ensembelda` -- Ensemble Latent Dirichlet Allocation
================================================================

.. automodule:: gensim.models.ensemblelda
    :synopsis: Ensemble Latent Dirichlet Allocation
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.logentropy_model` -- LogEntropy model
======================================================

.. automodule:: gensim.models.logentropy_model
    :synopsis: LogEntropy model
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.nmf` -- Non-Negative Matrix factorization
======================================================

.. automodule:: gensim.models.nmf
    :synopsis: Non-Negative Matrix Factorization
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.poincare` -- Train and use Poincare embeddings
=============================================================

.. automodule:: gensim.models.poincare
    :synopsis: Train and use Poincare embeddings
    :members:
    :inherited-members:
    :special-members: __iter__, __getitem__, __contains__
    :undoc-members:
    :show-inheritance:
:mod:`models.doc2vec` -- Doc2vec paragraph embeddings
=====================================================

.. automodule:: gensim.models.doc2vec
    :synopsis: Doc2vec paragraph embeddings
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.basemodel` -- Core TM interface
============================================

.. automodule:: gensim.models.basemodel
    :synopsis: Core TM interface
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.phrases` -- Phrase (collocation) detection
=======================================================

.. automodule:: gensim.models.phrases
    :synopsis: Phrase (collocation) detection
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.keyedvectors` -- Store and query word vectors
==========================================================

.. automodule:: gensim.models.keyedvectors
    :synopsis: Store and query word vectors
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`models.ldamulticore` -- parallelized Latent Dirichlet Allocation
======================================================================

.. automodule:: gensim.models.ldamulticore
    :synopsis: Latent Dirichlet Allocation
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
Gensim Sphinx Gallery README
============================

This README.rst file describes the mechanisms behind our documentation examples.
The intended audience is Gensim developers and documentation authors trying to understand how this stuff works.

Overview
--------

We use `Sphinx Gallery <https://sphinx-gallery.github.io/index.html>`__.
The top-level ``README.txt`` describes the gallery.
Each subdirectory is a gallery of examples, each with its own ``README.txt`` file.
Each example is a Python script.

Sphinx Gallery reads these scripts and renders them as HTML and Jupyter
notebooks.  If the script is unchanged from the previous run, then Sphinx skips
rendering and running it.  This saves considerable time, as running all the
examples can take several hours.

Subdirectories
--------------

There are three important subdirectories for the gallery:

1. ``docs/src/gallery`` contains Python scripts
2. ``docs/src/auto_examples`` contains Jupyter notebooks and RST rendered from the Python scripts
3. ``docs/auto_examples`` contains HTML rendered from the Python scripts

We keep all 1) and 2) under version control in our git repository.
The rendering takes a fair bit of time (one or two hours to run everything) so it's worth keeping the result.
On the contrary, it doesn't take a lot of time to generate 3) from 2), so we don't keep 3) under version control.

.. Note::
    I'm not sure if there's some way we can merge 2) and 3) - that may make more
    sense than keeping them separate.

File naming
-----------

Each example file is a Python script.
We prefix each script with a ``run_`` prefix: this tells Gallery that it should run the file in order to render HTML and Jupyter notebooks.

If we remove that prefix from a file, then Sphinx will skip running it and just render it.
This is helpful when the example fails to run for some reason, and we want to temporarily skip running it.
It's best to avoid this unless absolutely necessary, because running the script ensures the example is still correct and valid.

Configuration
-------------

The Gallery relies on the general Sphinx configuration script in ``docs/src/conf.py``.
Within that script, there is a ``sphinx_gallery_conf`` dictionary that contains all the config options.
If you go tweaking those options, see the `Sphinx Gallery Documentation <https://sphinx-gallery.github.io/configuration.html>`__.

The order in which subgalleries and examples appear is an important part of the configuration.
For the subgalleries, we list them explicitly using the ``subsection_order`` parameter.
We do a similar thing for examples, except we need to write a little bit of code to make that happen.
See the ``sort_key`` function in ``conf.py`` for details.
:mod:`test.utils` -- Internal testing functions
===============================================

.. automodule:: gensim.test.utils
    :synopsis: Common utils used in testing Gensim internally
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.word2vec_standalone` -- Train word2vec on text file CORPUS
========================================================================

.. automodule:: gensim.scripts.word2vec_standalone
    :synopsis: Train word2vec on text file CORPUS
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.word2vec2tensor` -- Convert the word2vec format to Tensorflow 2D tensor
=====================================================================================

.. automodule:: gensim.scripts.word2vec2tensor
    :synopsis: Convert the word2vec format to Tensorflow 2D tensor
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.package_info` -- Information about gensim package
===============================================================

.. automodule:: gensim.scripts.package_info
    :synopsis: Information about gensim package.
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.make_wiki_online` -- Convert articles from a Wikipedia dump
=========================================================================

.. automodule:: gensim.scripts.make_wiki_online
    :synopsis: Convert articles from a Wikipedia dump
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.make_wiki_online_nodebug` -- Convert articles from a Wikipedia dump
=================================================================================

.. automodule:: gensim.scripts.make_wiki_online_nodebug
    :synopsis: Convert articles from a Wikipedia dump
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.make_wikicorpus` -- Convert articles from a Wikipedia dump to vectors. 
====================================================================================

.. automodule:: gensim.scripts.make_wikicorpus
    :synopsis: Convert articles from a Wikipedia dump to vectors. 
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.segment_wiki` -- Convert wikipedia dump to json-line format
=========================================================================

.. automodule:: gensim.scripts.segment_wiki
    :synopsis: Convert wikipedia dump to json-line format.
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`scripts.glove2word2vec` -- Convert glove format to word2vec
=================================================================

.. automodule:: gensim.scripts.glove2word2vec
    :synopsis: Convert glove format to word2vec
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.hashdictionary` -- Construct word<->id mappings
=============================================================

.. automodule:: gensim.corpora.hashdictionary
    :synopsis: Construct word<->id mappings on the fly (the "hashing trick")
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.malletcorpus` -- Corpus in Mallet format
======================================================

.. automodule:: gensim.corpora.malletcorpus
    :synopsis: Corpus in Mallet format.
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.csvcorpus` -- Corpus in CSV format
==========================================================

.. automodule:: gensim.corpora.csvcorpus
    :synopsis: Corpus in CSV format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.wikicorpus` -- Corpus from a Wikipedia dump
==========================================================

.. automodule:: gensim.corpora.wikicorpus
    :synopsis: Corpus from a Wikipedia dump
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.bleicorpus` -- Corpus in Blei's LDA-C format
==========================================================

.. automodule:: gensim.corpora.bleicorpus
    :synopsis: Corpus in Blei's LDA-C format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.ucicorpus` -- Corpus in UCI format
================================================

.. automodule:: gensim.corpora.ucicorpus
    :synopsis: Corpus in UCI format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.opinosiscorpus` -- Topic related review sentences
===============================================================

.. automodule:: gensim.corpora.opinosiscorpus
    :synopsis: Topic related review sentences
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.lowcorpus` -- Corpus in GibbsLda++ format
=======================================================

.. automodule:: gensim.corpora.lowcorpus
    :synopsis: Corpus in GibbsLda++ format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.textcorpus` -- Tools for building corpora with dictionaries
=========================================================================

.. automodule:: gensim.corpora.textcorpus
    :synopsis: Tools for building corpora with dictionaries
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.dictionary` -- Construct word<->id mappings
==========================================================

.. automodule:: gensim.corpora.dictionary
    :synopsis: Construct word<->id mappings
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.mmcorpus` -- Corpus in Matrix Market format
==========================================================

.. automodule:: gensim.corpora.mmcorpus
    :synopsis: Corpus in Matrix Market format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.svmlightcorpus` -- Corpus in SVMlight format
==================================================================

.. automodule:: gensim.corpora.svmlightcorpus
    :synopsis: Corpus in SVMlight format
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
:mod:`corpora.indexedcorpus` -- Random access to corpus documents
=================================================================

.. automodule:: gensim.corpora.indexedcorpus
    :synopsis: Random access to corpus documents
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:

:orphan:

:mod:`corpora` -- Package for corpora I/O
==========================================

.. automodule:: gensim.corpora
    :synopsis: Package for corpora I/O
    :members:
    :inherited-members:

:mod:`corpora.sharded_corpus` -- Corpus stored in separate files
================================================================

.. automodule:: gensim.corpora.sharded_corpus
    :synopsis: Numpy arrays on disk for iterative processing 
    :members:
    :inherited-members:
    :undoc-members:
    :show-inheritance:
